[
 {
  "id": "008d5261ee7385a2b7e39772938f51_0",
  "x": "The window length was motivated by recent research (Qazvinian and Radev, 2010) which shows the best score for a four-sentence boundary when detecting non-explicit citation. The rest of the sentences were marked either positive (p), negative (n) or objective/neutral (o). A total of 1,741 citations were annotated. Although this annotation was performed by the first author only, we know from previous work that similar styles of annotation can achieve acceptable interannotator agreement (Teufel et al., 2006 ). An example annotation for Smadja (1993) is given in Figure  2 , where the first column shows the line number and the second one shows the class label. To compare our work with <cite>Athar (2011)</cite> , we also applied a three-class annotation scheme. In this method of annotation, we merge the citation context into a single sentence.",
  "y": "uses"
 },
 {
  "id": "008d5261ee7385a2b7e39772938f51_1",
  "x": "This setup has been shown to produce good results earlier as well (Pang et al., 2002; <cite>Athar, 2011</cite>) . The first set of experiments focuses on simultaneous detection of sentiment and context sentences. For this purpose, we use the four-class annotated corpus described earlier. While the original annotations were performed for a window of length 4, we also experiment with asymmetrical windows of l sentences preceding the citation and r sentences succeeding it. The detailed results are given in Table 2 . Table 2 : Results for joint context and sentiment detection. Because of the skewed class distribution, we use both the F macro and F micro scores with 10-fold cross-validation.",
  "y": "background"
 },
 {
  "id": "008d5261ee7385a2b7e39772938f51_2",
  "x": "The baseline score, shown in bold, is obtained with no context window and is comparable to the results reported by <cite>Athar (2011)</cite> . However, we can observe that the F scores decrease as more context is introduced. This may be attributed to the increase in the vocabulary size of the n-grams and a consequent reduction in the discriminating power of the decision boundaries. These results show that the task of jointly detecting sentiment and context is a hard problem. For our second set of experiments, we use the three-class annotation scheme. We merge the text of the sentences in the context windows as well as their dependency triplets to obtain the features. The results are reported in Table 3 with best results in bold.",
  "y": "similarities"
 },
 {
  "id": "008d5261ee7385a2b7e39772938f51_3",
  "x": "This may be attributed to the increase in the vocabulary size of the n-grams and a consequent reduction in the discriminating power of the decision boundaries. These results show that the task of jointly detecting sentiment and context is a hard problem. For our second set of experiments, we use the three-class annotation scheme. We merge the text of the sentences in the context windows as well as their dependency triplets to obtain the features. The results are reported in Table 3 with best results in bold. Although these results are not better than the context-less baseline, the reason might be data sparsity since existing work on citation sentiment analysis uses more data <cite>(Athar, 2011)</cite> . ----------------------------------",
  "y": "extends background"
 },
 {
  "id": "008d5261ee7385a2b7e39772938f51_4",
  "x": "---------------------------------- **RELATED WORK** While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000) , the only recent work on citation sentiment detection using a relatively large corpus is by <cite>Athar (2011)</cite> . However, <cite>this work</cite> does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003) . However, such approaches have been found to be highly topic dependent (Engstr\u00f6m, 2004; Gamon and Aue, 2005; Blitzer et al., 2007) .",
  "y": "background"
 },
 {
  "id": "008d5261ee7385a2b7e39772938f51_5",
  "x": "**RELATED WORK** While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000) , the only recent work on citation sentiment detection using a relatively large corpus is by <cite>Athar (2011)</cite> . However, <cite>this work</cite> does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003) . However, such approaches have been found to be highly topic dependent (Engstr\u00f6m, 2004; Gamon and Aue, 2005; Blitzer et al., 2007) . Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme.",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_0",
  "x": "This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also from middle layers. Our proposed method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by <cite>Yang et al. (2018)</cite> . The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to two application tasks: machine translation and headline generation. Our code is publicly available at: https://github.com/nttcslabnlp/doc lm. ---------------------------------- **INTRODUCTION**",
  "y": "uses"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_1",
  "x": "Previous researches demonstrated that RNN language models achieve high performance by using several regularizations and selecting appropriate hyperparameters (Melis et al., 2018; Merity et al., 2018) . However, <cite>Yang et al. (2018)</cite> proved that existing RNN language models have low expressive power due to the Softmax bottleneck, which means the output matrix of RNN language models is low rank when we interpret the training of RNN language models as a matrix factorization problem. To solve the Softmax bottleneck, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS), which increases the rank of the matrix by combining multiple probability distributions computed from the encoded fixed-length vector. In this study, we propose Direct Output Connection (DOC) as a generalization of MoS. For stacked RNNs, DOC computes the probability distributions from the middle layers including input embeddings. In addition to raising the rank, the proposed method helps weaken the vanishing gradient problem in backpropagation because DOC provides a shortcut connection to the output. We conduct experiments on standard benchmark datasets for language modeling: the Penn Treebank and WikiText-2. Our experiments demonstrate that DOC outperforms MoS and achieves state-of-theart perplexities on each dataset.",
  "y": "motivation"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_2",
  "x": "( 1) p(w 1 ) is generally assumed to be 1 in this literature, that is, p(w 1 ) = 1, and thus we can ignore its calculation. See the implementation of Zaremba et al. (2014) 1 , for an example. RNN language models obtain conditional probability p(w t+1 |w 1:t ) from the probability distribution of each word. To compute the probability distribution, RNN language models encode sequence w 1:t into a fixed-length vector and apply a transformation matrix and the softmax function. Previous researches demonstrated that RNN language models achieve high performance by using several regularizations and selecting appropriate hyperparameters (Melis et al., 2018; Merity et al., 2018) . However, <cite>Yang et al. (2018)</cite> proved that existing RNN language models have low expressive power due to the Softmax bottleneck, which means the output matrix of RNN language models is low rank when we interpret the training of RNN language models as a matrix factorization problem. To solve the Softmax bottleneck, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS), which increases the rank of the matrix by combining multiple probability distributions computed from the encoded fixed-length vector.",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_3",
  "x": "To solve the Softmax bottleneck, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS), which increases the rank of the matrix by combining multiple probability distributions computed from the encoded fixed-length vector. In this study, we propose Direct Output Connection (DOC) as a generalization of MoS. For stacked RNNs, DOC computes the probability distributions from the middle layers including input embeddings. In addition to raising the rank, the proposed method helps weaken the vanishing gradient problem in backpropagation because DOC provides a shortcut connection to the output. We conduct experiments on standard benchmark datasets for language modeling: the Penn Treebank and WikiText-2. Our experiments demonstrate that DOC outperforms MoS and achieves state-of-theart perplexities on each dataset. Moreover, we investigate the effect of DOC on two applications: machine translation and headline generation. We indicate that DOC can improve the performance of an encoder-decoder with an attention mechanism, which is a strong baseline for such applications.",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_4",
  "x": "See the implementation of Zaremba et al. (2014) 1 , for an example. RNN language models obtain conditional probability p(w t+1 |w 1:t ) from the probability distribution of each word. To compute the probability distribution, RNN language models encode sequence w 1:t into a fixed-length vector and apply a transformation matrix and the softmax function. Previous researches demonstrated that RNN language models achieve high performance by using several regularizations and selecting appropriate hyperparameters (Melis et al., 2018; Merity et al., 2018) . However, <cite>Yang et al. (2018)</cite> proved that existing RNN language models have low expressive power due to the Softmax bottleneck, which means the output matrix of RNN language models is low rank when we interpret the training of RNN language models as a matrix factorization problem. To solve the Softmax bottleneck, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS), which increases the rank of the matrix by combining multiple probability distributions computed from the encoded fixed-length vector. In this study, we propose Direct Output Connection (DOC) as a generalization of MoS. For stacked RNNs, DOC computes the probability distributions from the middle layers including input embeddings.",
  "y": "extends"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_5",
  "x": "Let V be the vocabulary size and let P t \u2208 R V be the probability distribution of the vocabulary at timestep t. Moreover, let D h n be the dimension of the hidden state of the n-th RNN, and let D e be the dimensions of the embedding vectors. Then the RNN language models predict probability distribution P t+1 by the following equation: where W \u2208 R V \u00d7D h N is a weight matrix 2 , E \u2208 R De\u00d7V is a word embedding matrix, x t \u2208 {0, 1} V is a one-hot vector of input word w t at timestep t, and h n t \u2208 R D h n is the hidden state of the n-th RNN at timestep t. We define h n t at timestep t = 0 as a zero vector: h n 0 = 0. Let f (\u00b7) represent an abstract function of an RNN, which might be the Elman network (Elman, 1990) , the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) , the Recurrent Highway Network (RHN) (Zilly et al., 2017) , or any other RNN variant. In this research, we stack three LSTM layers based on Merity et al. (2018) because they achieved high performance. 3 Language Modeling as Matrix Factorization <cite>Yang et al. (2018)</cite> indicated that the training of language models can be interpreted as a matrix 2 Actually, we apply a bias term in addition to the weight matrix but we omit it to simplify the following discussion. factorization problem.",
  "y": "extends"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_6",
  "x": "<cite>Yang et al. (2018)</cite> also argued that rank(A ) is as high as vocabulary size V based on the following two assumptions: 1. Natural language is highly context-dependent. In addition, since we can imagine many kinds of contexts, it is difficult to assume a basis that represents a conditional probability distribution for any contexts. In other words, compressing U is difficult. 2. Since we also have many kinds of semantic meanings, it is difficult to assume basic meanings that can create all other semantic meanings by such simple operations as addition and subtraction; compressing V is difficult. In summary, <cite>Yang et al. (2018)</cite> indicated that D h N is much smaller than rank(A) because its scale is usually 10 2 and vocabulary size V is at least 10 4 . ----------------------------------",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_7",
  "x": "2. Since we also have many kinds of semantic meanings, it is difficult to assume basic meanings that can create all other semantic meanings by such simple operations as addition and subtraction; compressing V is difficult. In summary, <cite>Yang et al. (2018)</cite> indicated that D h N is much smaller than rank(A) because its scale is usually 10 2 and vocabulary size V is at least 10 4 . ---------------------------------- **PROPOSED METHOD: DIRECT OUTPUT CONNECTION** To construct a high-rank matrix, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS). MoS computes multiple probability distributions from the hidden state of final RNN layer h N and regards the weighted average of the probability distributions as the final distribution. In this study, we propose Direct Output Connection (DOC), which is a generalization method of MoS. DOC computes probability distributions from the middle layers in addition to the final layer.",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_8",
  "x": "In summary, <cite>Yang et al. (2018)</cite> indicated that D h N is much smaller than rank(A) because its scale is usually 10 2 and vocabulary size V is at least 10 4 . ---------------------------------- **PROPOSED METHOD: DIRECT OUTPUT CONNECTION** To construct a high-rank matrix, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS). MoS computes multiple probability distributions from the hidden state of final RNN layer h N and regards the weighted average of the probability distributions as the final distribution. In this study, we propose Direct Output Connection (DOC), which is a generalization method of MoS. DOC computes probability distributions from the middle layers in addition to the final layer. In other words, DOC directly connects the middle layers to the output.",
  "y": "extends"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_9",
  "x": "The top row ( \u2020) represents MoS scores reported in <cite>Yang et al. (2018)</cite> as a baseline. \u2021 represents the perplexity obtained by the implementation of <cite>Yang et al. (2018)</cite> 6 with identical hyperparameters except for i 3 . dropout rate for vector k j,ct and the non-monotone interval. Since we found that the dropout rate for vector k j,ct greatly influences \u03b2 in Equation 13, we varied it from 0.3 to 0.6 with 0.1 intervals. We selected 0.6 because this value achieved the best score on the PTB validation dataset. For the nonmonotone interval, we adopted the same value as Zolna et al. (2018) . Table 2 summarizes the hyperparameters of our experiments.",
  "y": "uses"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_10",
  "x": "The top row ( \u2020) represents MoS scores reported in <cite>Yang et al. (2018)</cite> as a baseline. \u2021 represents the perplexity obtained by the implementation of <cite>Yang et al. (2018)</cite> 6 with identical hyperparameters except for i 3 . dropout rate for vector k j,ct and the non-monotone interval. Since we found that the dropout rate for vector k j,ct greatly influences \u03b2 in Equation 13, we varied it from 0.3 to 0.6 with 0.1 intervals. We selected 0.6 because this value achieved the best score on the PTB validation dataset. For the nonmonotone interval, we adopted the same value as Zolna et al. (2018) . Table 2 summarizes the hyperparameters of our experiments.",
  "y": "uses"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_11",
  "x": "Moreover, the top row of Table 3 shows the perplexity of AWD-LSTM with MoS reported in <cite>Yang et al. (2018)</cite> for comparison. Table 3 indicates that language models using middle layers outperformed one using only the final layer. In addition, Table  3 shows that increasing the distributions from the final layer (i 3 = 20) degraded the score from the language model with i 3 = 15 (the top row of Table 3). Thus, to obtain a superior language model, we should not increase the number of distributions from the final layer; we should instead use the middle layers, as with our proposed DOC. Table 3 shows that the i 3 = 15, i 2 = 5 setting achieved the best performance and the other settings with shallow layers have a little effect. This result implies that we need some layers to output accurate distributions. In fact, most previous studies adopted two LSTM layers for language modeling.",
  "y": "uses"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_12",
  "x": "In other words, Table 5 describes\u00c3 in Equation 9 for each method. As shown by this table, the output of AWD-LSTM is restricted to D 3 7 . In contrast, AWD-LSTM-MoS<cite> (Yang et al., 2018)</cite> and AWD-LSTM-DOC outputted matrices whose ranks equal the vocabulary size. This fact indicates that DOC (including MoS) can output the same matrix as the true distributions in view of a rank. Figure 2 illustrates the learning curves of each method on PTB. This figure contains the validation scores of AWD-LSTM, AWD-LSTM-MoS, and AWD-LSTM-DOC at each training epoch. We trained AWD-LSTM and AWD-LSTM-MoS by setting the non-monotone interval to 60, as with AWD-LSTM-DOC.",
  "y": "similarities"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_13",
  "x": "**MODELS** We compare AWD-LSTM-DOC with AWD-LSTM (Merity et al., 2018) and AWD-LSTMMoS<cite> (Yang et al., 2018)</cite> . We trained each model with the same hyperparameters from our language modeling experiments (Section 5). We selected the model that achieved the best perplexity on the validation set during the training. State-of-the-art results Dyer et al. (2016) 91.7 93.3 Fried et al. (2017) (ensemble) 92.72 94.25 Suzuki et al. (2018) (ensemble) 92.74 94.32 Kitaev and Klein (2018) 95.13 - Moreover, AWD-LSTM-DOC outperformed AWD-LSTM and AWD-LSTM-MoS. These results correspond to the performance on the language modeling task (Section 5.3). The middle part shows that AWD-LSTM-DOC also outperformed AWD-LSTM and AWD-LSTMMoS in the ensemble setting. In addition, we can improve the performance by exchanging the base parser with a stronger one.",
  "y": "uses"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_14",
  "x": "Zilly et al. (2017) proposed recurrent highway networks that use highway layers (Srivastava et al., 2015) to deepen recurrent connections. Zoph and Le (2017) adopted reinforcement learning to construct the best RNN structure. However, as mentioned, Melis et al. (2018) established that the standard LSTM is superior to these architectures. Apart from RNN architecture, proposed the input-tooutput gate (IOG), which boosts the performance of trained language models. As described in Section 3, <cite>Yang et al. (2018)</cite> interpreted training language modeling as matrix factorization and improved performance by computing multiple probability distributions. In this study, we generalized their approach to use the middle layers of RNNs. Finally, our proposed method, DOC, achieved the state-of-the-art score on the standard benchmark datasets.",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_15",
  "x": "Zilly et al. (2017) proposed recurrent highway networks that use highway layers (Srivastava et al., 2015) to deepen recurrent connections. Zoph and Le (2017) adopted reinforcement learning to construct the best RNN structure. However, as mentioned, Melis et al. (2018) established that the standard LSTM is superior to these architectures. Apart from RNN architecture, proposed the input-tooutput gate (IOG), which boosts the performance of trained language models. As described in Section 3, <cite>Yang et al. (2018)</cite> interpreted training language modeling as matrix factorization and improved performance by computing multiple probability distributions. In this study, we generalized their approach to use the middle layers of RNNs. Finally, our proposed method, DOC, achieved the state-of-the-art score on the standard benchmark datasets.",
  "y": "extends"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_16",
  "x": "Although these methods might also improve the performance of DOC, we omitted such investigation to focus on comparisons among methods trained only on the training set. ---------------------------------- **CONCLUSION** We proposed Direct Output Connection (DOC), a generalization method of MoS introduced by <cite>Yang et al. (2018)</cite> . DOC raises the expressive power of RNN language models and improves quality of the model. DOC outperformed MoS and achieved the best perplexities on the standard benchmark datasets of language modeling: PTB and WikiText-2. Moreover, we investigated its effectiveness on machine translation and headline generation.",
  "y": "extends"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_17",
  "x": "Krause et al. (2017) proposed dynamic evaluation that updates parameters based on a recent sequence during testing. Although these methods might also improve the performance of DOC, we omitted such investigation to focus on comparisons among methods trained only on the training set. ---------------------------------- **CONCLUSION** We proposed Direct Output Connection (DOC), a generalization method of MoS introduced by <cite>Yang et al. (2018)</cite> . DOC raises the expressive power of RNN language models and improves quality of the model. DOC outperformed MoS and achieved the best perplexities on the standard benchmark datasets of language modeling: PTB and WikiText-2.",
  "y": "uses"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_0",
  "x": "Word embeddings are a crucial component in many NLP approaches (Mikolov et al., 2013; Pennington et al., 2014) since they capture latent semantics of words and thus allow models to better train and generalize. Recent work has moved away from the original \"one word, one embedding\" paradigm to investigate contextualized embedding models (Peters et al., 2017 (Peters et al., , 2018<cite> Akbik et al., 2018)</cite> . Such approaches produce different embeddings for the same word depending on its context and are thus capable of capturing latent contextualized semantics of ambiguous words. Recently, <cite>Akbik et al. (2018)</cite> proposed a character-level contextualized embeddings ap- context. This leads to an underspecified contextual word embedding for the string \"Indra\" that ultimately causes a misclassification of \"Indra\" as an organization (ORG) instead of person (PER) in a downstream NER task. proach they refer to as contextual string embeddings. They leverage pre-trained character-level language models from which they extract hidden states at the beginning and end character positions of each word to produce embeddings for any string of characters in a sentential context.",
  "y": "background"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_1",
  "x": "---------------------------------- **INTRODUCTION** Word embeddings are a crucial component in many NLP approaches (Mikolov et al., 2013; Pennington et al., 2014) since they capture latent semantics of words and thus allow models to better train and generalize. Recent work has moved away from the original \"one word, one embedding\" paradigm to investigate contextualized embedding models (Peters et al., 2017 (Peters et al., , 2018<cite> Akbik et al., 2018)</cite> . Such approaches produce different embeddings for the same word depending on its context and are thus capable of capturing latent contextualized semantics of ambiguous words. Recently, <cite>Akbik et al. (2018)</cite> proposed a character-level contextualized embeddings ap- context. This leads to an underspecified contextual word embedding for the string \"Indra\" that ultimately causes a misclassification of \"Indra\" as an organization (ORG) instead of person (PER) in a downstream NER task.",
  "y": "background"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_2",
  "x": "---------------------------------- **INTRODUCTION** Word embeddings are a crucial component in many NLP approaches (Mikolov et al., 2013; Pennington et al., 2014) since they capture latent semantics of words and thus allow models to better train and generalize. Recent work has moved away from the original \"one word, one embedding\" paradigm to investigate contextualized embedding models (Peters et al., 2017 (Peters et al., , 2018<cite> Akbik et al., 2018)</cite> . Such approaches produce different embeddings for the same word depending on its context and are thus capable of capturing latent contextualized semantics of ambiguous words. Recently, <cite>Akbik et al. (2018)</cite> proposed a character-level contextualized embeddings ap- context. This leads to an underspecified contextual word embedding for the string \"Indra\" that ultimately causes a misclassification of \"Indra\" as an organization (ORG) instead of person (PER) in a downstream NER task.",
  "y": "motivation"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_3",
  "x": "We evaluate our proposed embedding approach on the task of named entity recognition on the CONLL-03 (English, German and Dutch) and WNUT datasets. In all cases, we find that our approach outperforms previous approaches and yields new state-of-the-art scores. We contribute our approach and all pre-trained models to the open source FLAIR 1 framework, to ensure reproducibility of these results. ---------------------------------- **METHOD** Our proposed approach dynamically builds up a \"memory\" of contextualized embeddings and applies a pooling operation to distill a global contextualized embedding for each word. It requires an embed() function that produces a contextualized embedding for a given word in a sentence context (see <cite>Akbik et al. (2018)</cite> ).",
  "y": "uses"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_4",
  "x": "Our approach thus produces evolving word representations that change over time as more instances of the same word are observed in the data. We evaluate our proposed embedding approach on the task of named entity recognition on the CONLL-03 (English, German and Dutch) and WNUT datasets. In all cases, we find that our approach outperforms previous approaches and yields new state-of-the-art scores. We contribute our approach and all pre-trained models to the open source FLAIR 1 framework (Akbik et al., 2019) , to ensure reproducibility of these results. Our proposed approach (see Figure 2 ) dynamically builds up a \"memory\" of contextualized embeddings and applies a pooling operation to distill a global contextualized embedding for each word. It requires an embed() function that produces a contextualized embedding for a given word in a 1 https://github.com/zalandoresearch/flair sentence context (see <cite>Akbik et al. (2018)</cite> ). It also requires a memory that records for each unique word all previous contextual embeddings, and a pool() operation to pool embedding vectors.",
  "y": "uses"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_5",
  "x": "It implements the standard BiLSTM-CRF sequence labeling architecture (Huang et al., 2015) and includes pre-trained contextual string embeddings for many languages. To FLAIR, we add an implementation of our proposed pooled contextualized embeddings. Hyperparameters. For our experiments, we follow the training and evaluation procedure outlined in <cite>Akbik et al. (2018)</cite> and follow most hyperparameter suggestions as given by the in-depth study presented in Reimers and Gurevych (2017) . That is, we use an LSTM with 256 hidden states and one layer (Hochreiter and Schmidhuber, 1997) , a locked dropout value of 0.5, a word dropout of 0.05, and train using SGD with an annealing rate of 0.5 and a patience of 3. We perform model selection over the learning rate \u2208 {0.01, 0.05, 0.1} and mini-batch size \u2208 {8, 16, 32}, choosing the model with the best F-measure on the validation set. Following Peters et al. (2017) , we then repeat the experiment 5 times with different random seeds, and train using both train and development set, reporting both average performance and standard deviation over these runs on the test set as final performance.",
  "y": "uses"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_6",
  "x": "Standard word embeddings. The default setup of <cite>Akbik et al. (2018)</cite> recommends contextual string embeddings to be used in combination with standard word embeddings. We use GLOVE embeddings (Pennington et al., 2014) for the English tasks and FASTTEXT embeddings (Bojanowski et al., 2017) for all newswire tasks. Baselines. Our baseline are contextual string embeddings without pooling, i.e. the original setup proposed in <cite>Akbik et al. (2018)</cite> 2 . By comparing against this baseline, we isolate the impact of our proposed pooled contextualized embeddings. Table 2 : Ablation experiment using contextual string embeddings without word embeddings.",
  "y": "background"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_7",
  "x": "We perform model selection over the learning rate \u2208 {0.01, 0.05, 0.1} and mini-batch size \u2208 {8, 16, 32}, choosing the model with the best F-measure on the validation set. Following Peters et al. (2017) , we then repeat the experiment 5 times with different random seeds, and train using both train and development set, reporting both average performance and standard deviation over these runs on the test set as final performance. Standard word embeddings. The default setup of <cite>Akbik et al. (2018)</cite> recommends contextual string embeddings to be used in combination with standard word embeddings. We use GLOVE embeddings (Pennington et al., 2014) for the English tasks and FASTTEXT embeddings (Bojanowski et al., 2017) for all newswire tasks. Baselines. Our baseline are contextual string embeddings without pooling, i.e. the original setup proposed in <cite>Akbik et al. (2018)</cite> 2 .",
  "y": "similarities uses"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_8",
  "x": "Standard word embeddings. The default setup of <cite>Akbik et al. (2018)</cite> recommends contextual string embeddings to be used in combination with standard word embeddings. We use GLOVE embeddings (Pennington et al., 2014) for the English tasks and FASTTEXT embeddings (Bojanowski et al., 2017) for all newswire tasks. Baselines. Our baseline are contextual string embeddings without pooling, i.e. the original setup proposed in <cite>Akbik et al. (2018)</cite> 2 . By comparing against this baseline, we isolate the impact of our proposed pooled contextualized embeddings. Table 2 : Ablation experiment using contextual string embeddings without word embeddings.",
  "y": "uses"
 },
 {
  "id": "0143619c1c54129702aafb585463d2_0",
  "x": "Citation texts usually highlight certain contributions of the referenced paper and a set of citation texts to a reference paper can provide useful information about that paper. Therefore, citation texts have been previously used to enhance many downstream tasks in IR/NLP such as search and summarization (e.g. [2, 15, 16] ). While useful, citation texts might lack the appropriate context from the reference article<cite> [4,</cite> 5, 18] . For example, details of the methods, assumptions or conditions for the obtained results are often not mentioned. Furthermore, in many cases the citing author might misunderstand or misquote the referenced paper and ascribe contributions to it that are not intended in that form. Hence, sometimes the citation text is not su ciently informative or in other cases, even inaccurate [17] . This problem is more serious in life sciences where accurate dissemination of knowledge has direct impact on human lives.",
  "y": "background"
 },
 {
  "id": "0143619c1c54129702aafb585463d2_1",
  "x": "---------------------------------- **EXPERIMENTS** Data. We use the TAC 201<cite>4</cite> Biomedical Summarization benchmark 3 . This dataset contains 220 scienti c biomedical journal articles and 313 total citation texts where the relevant contexts for each citation text are annotated by <cite>4</cite> experts. Baselines. To our knowledge, the only published results on TAC 201<cite>4</cite> is<cite> [4]</cite> , where the authors utilized query reformulation (QR) based on UMLS ontology.",
  "y": "background"
 },
 {
  "id": "0143619c1c54129702aafb585463d2_2",
  "x": "**EXPERIMENTS** Data. We use the TAC 201<cite>4</cite> Biomedical Summarization benchmark 3 . This dataset contains 220 scienti c biomedical journal articles and 313 total citation texts where the relevant contexts for each citation text are annotated by <cite>4</cite> experts. Baselines. To our knowledge, the only published results on TAC 201<cite>4</cite> is<cite> [4]</cite> , where the authors utilized query reformulation (QR) based on UMLS ontology. In addition to<cite> [4]</cite> , we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM: Vector Space Model that was used in<cite> [4]</cite> ; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12] ; and <cite>4</cite>) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10] .",
  "y": "extends differences"
 },
 {
  "id": "0143619c1c54129702aafb585463d2_3",
  "x": "The best baseline performance is the query reformulation (QR) method by<cite> [4]</cite> which improves over other baselines. We observe that using general domain embeddings does not provide much advantage in comparison with the best baseline (compare WE wiki and QR in the Table) . However, using the domain speci c embeddings (WE Bio ) results in 10% c-F improvement over the best baseline. This is expected since word relations in the biomedical context are better captured with biomedical embeddings. In Table 2 an illustrative word \"expression\" gives better intuition why is that the case. As shown, using general embeddings (left column in the table), the most similar words to \"expression\" are those related to 5 https://tac.nist.gov/201<cite>4</cite>/BiomedSumm/guidelines.html the general meaning of it. However, many of these related words are not relevant in the biomedical context.",
  "y": "background"
 },
 {
  "id": "0143619c1c54129702aafb585463d2_4",
  "x": "Embeddings have been recently used in general information retrieval models. Vuli\u0107 and Moens [19] proposed an architecture for learning word embeddings in multilingual settings and used them in document and query representation. Mitra et al. [12] proposed dual embedded space model that predicts document aboutness by comparing the centroid of word vectors to query terms. Ganguly et al. [7] used embeddings to transform term weights in a translation model for retrieval. Their model uses embeddings to expand documents and use co-occurrences for estimation. Unlike these works, we directly use embeddings in estimating the likelihood of query given documents; we furthermore incorporate ways to utilize domain speci c knowledge in our model. The most relevant prior work to ours is<cite> [4]</cite> where the authors approached the problem using a vector space model similarity ranking and query reformulations.",
  "y": "similarities"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_0",
  "x": "The open RE task is also related to open information extraction (open IE) (Banko et al., 2007; Del Corro and Gemulla, 2013) , which extracts large amounts of surface relations and their arguments from natural language text; e.g., \"critizices\" (\"Dante\", \"Catholic Church\") . 1 Although open IE is a domain-independent approach, the extracted surface relations are purely syntactic and often ambiguous or noisy. Moreover, open IE methods usually do not \"predict\" facts that have not been explicitly observed in the input data. Open RE combines the above tasks by predicting new facts for an open set of relations. The key challenge in open RE is to reason jointly over the universal schema consisting of KB relations and surface relations<cite> (Riedel et al., 2013)</cite> . A number of matrix or tensor factorization models have recently been proposed in the context of relation extraction<cite> Riedel et al., 2013</cite>; Huang et al., 2014; . These models use the available data to learn latent semantic representations of entities (or entity pairs) and relations in a domain-independent way; the latent representations are subsequently used to predict new facts.",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_1",
  "x": "The key challenge in open RE is to reason jointly over the universal schema consisting of KB relations and surface relations<cite> (Riedel et al., 2013)</cite> . A number of matrix or tensor factorization models have recently been proposed in the context of relation extraction<cite> Riedel et al., 2013</cite>; Huang et al., 2014; . These models use the available data to learn latent semantic representations of entities (or entity pairs) and relations in a domain-independent way; the latent representations are subsequently used to predict new facts. Existing models often focus on either targeted IE or open RE. Targeted models are used for within-KB reasoning; they rely on the closed-world assumption and often do not scale with the number of relations. Open RE models use the open-world assumption, which is more suitable for the open RE task because the available data is often highly incomplete. In this paper, we propose CORE, a novel open RE factorization model that incorporates and exploits contextual information to improve prediction performance.",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_2",
  "x": "Note that the prediction of the fact \"is band member of\"(TP, MM) is facilitated if we make use of a KB that knows that TP is a musician and MM is a music band. If TP and/or MM are not present in the knowledge base, however, such a reasoning does not apply. In our work, we consider both linked entities (in-KB) and non-linked entity mentions (out of-KB). Since KB are often incomplete, this open approach to handle named entities allows us to extract facts for all entities, even if they do not appear in the KB. In this paper, we propose CORE, a flexible open RE model that leverages contextual information. CORE is inspired by the combined factorization and entity model (FE) of<cite> Riedel et al. (2013)</cite> . As FE, CORE associates latent semantic representations with entities, relations, and arguments.",
  "y": "extends"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_3",
  "x": "For this reason,<cite> Riedel et al. (2013)</cite> argued and experimentally validated that open RE models can outperform targeted IE methods. Open IE. In contrast to targeted IE, the goal of open IE is to extract all (or most) relations expressed in natural-language text, whether or not these relations are defined in a KB (Banko et al., 2007; Fader et al., 2011; Del Corro and Gemulla, 2013) . The facts obtained by open IE methods are often not disambiguated, i.e., the entities and/or the relation are not linked to a knowledge base; e.g., \"criticizes\"(\"Dante\", \"Catholic Church\"). The goal of our work is to reason about extracted open-IE facts and their contextual information. Our method is oblivious to the actual open IE method being used. Relation clustering.",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_4",
  "x": "This assumption generally does not hold for the surface relations extracted by open IE systems<cite> (Riedel et al., 2013)</cite> ; examples of other types of relationships between relations include implication or mutual exclusion. Tensor factorization. Matrix or tensor factorization approaches try to address the above problem: instead of clustering relations, they directly predict facts. Both matrix and tensor models learn and make use of semantic representations of relations and their arguments. The semantic representations ideally captures all the information present in the data; it does not, however, establish a direct relationship (such as synonymy) between different KB or surface relations. Tensor factorization models conceptually model the input data as a subject\u00d7relation\u00d7object tensor, in which non-zero values correspond to input facts. The tensor is factored to construct a new tensor in which predicted facts take large non-zero values.",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_5",
  "x": "For this reason, matrix factorization models are not suited for in-KB reasoning; an individual pair of entities usually does not occur in more than one KB relation. In the open RE context, however, input relations are semantically related so that many subject-object pairs belong to multiple relations. The key advantage of matrix methods is (1) that this restriction allows them to use additional features-such as features for each subjectobject pair-and (2) that they scale much better with the number of relations. Examples of such matrix factorization models include (Tresp et al., 2009; Jiang et al., 2012; Fan et al., 2014; Huang et al., 2014) . have also shown that a combination of matrix and tensor factorization models can be fruitful. Closest to our work is the \"universal schema\" matrix factorization approach of<cite> Riedel et al. (2013)</cite> , which combines a latent features model, a neighborhood model and an entity model but does not incorporate context. Our CORE model follows the universal schema idea, but uses a more general factorization model, which includes the information captured by the latent features and entity model (but not the neighborhood model), and incorporates contextual information.",
  "y": "similarities"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_6",
  "x": "This closed-world approach essentially assumes that all unobserved facts are false, which may not be a suitable assumption for the sparsely observed relations of open RE. Following<cite> Riedel et al. (2013)</cite> , we adopt the open-world assumption instead, i.e., we treat each unobserved facts as unknown. Since factorization machines originally require explicit target values (e.g., feedback in recommender systems), we need to adapt parameter estimation to the open-world setting. In more detail, we employ a variant of the Bayesian personalized ranking (BPR) optimization criterion (Rendle et al., 2009 ). We associate with each training point x a set of negative samples X \u2212 x . Each negative sample x \u2212 \u2208 X \u2212 x is an unobserved fact with its associated context (constructed as described in the prediction section above). Generally, the negative samples x \u2212 should be chosen such that they are \"less likely\" to be true than fact x. We maximize the following optimization criterion:",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_7",
  "x": "Sampling negative evidence. To make BPR effective, the set of negative samples needs to be chosen carefully. A naive approach is to take the set of all unobserved facts between each relation r \u2208 R and each tuple t \u2208 T (or E \u00d7 E) as the set X \u2212 x . The reasoning is that, after all, we expect \"random\" unobserved facts to be less likely to be true than observed facts. This naive approach is problematic, however, because the set of negative samples is independent of x and thus not sufficiently informative (i.e., it contains many irrelevant samples). To overcome this problem, the negative sample set needs to be related to x in some way. Since we ultimately use our model to rank tuples for each relation individually, we consider as negative evidence for x only unobserved facts from the same relation<cite> (Riedel et al., 2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_8",
  "x": "**EXPERIMENTS** We conducted an experimental study on realworld data to compare our CORE model with other state-of-the-art approaches. 2 Our experimental study closely follows the one of<cite> Riedel et al. (2013)</cite> . ---------------------------------- **EXPERIMENTAL SETUP** Dataset. We made use of the dataset of<cite> Riedel et al. (2013)</cite> , but extended it with contextual information.",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_9",
  "x": "We made use of the dataset of<cite> Riedel et al. (2013)</cite> , but extended it with contextual information. The dataset consisted of 2.5M surface facts extracted from the New York Times corpus (Sandhaus, 2008) , as well as 16k facts from Freebase. Surface facts have been obtained by using a named-entity recognizer, which additionally labeled each named entity mention with its coarse-grained type (i.e., person, organization, location, miscellaneous). For each pair of entities found within a sentence, the shortest dependency path between these pairs was taken as surface relation. The entity mentions in each surface fact were linked to Freebase using a simple string matching ---------------------------------- **100**",
  "y": "extends"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_10",
  "x": "method. If no match was found, the entity mention was kept as is. There were around 2.2M tuples (distinct entity pairs) in this dataset, out of which 580k were fully linked to Freebase. For each of these tuples, the dataset additionally included all of the corresponding facts from Freebase. Using the metadata 3 of each New York Times article, we enriched each surface fact by the following contextual information: news desk (e.g., sports desk, foreign desk), descriptors (e.g., finances, elections), online section (e.g., sports, business), section (e.g., a, d), publication year, and bag-of-words of the sentence from which the surface fact has been extracted. Training data. From the raw dataset described above, we filtered out all surface relations with less than 10 instances, and all tuples with less than two instances, as in<cite> Riedel et al. (2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_11",
  "x": "Here we considered a fact or tuple as linked if both of its entities were linked to Freebase, as partiallylinked if only one of its entities was linked, and as non-linked otherwise. In contrast to previous work<cite> (Riedel et al., 2013</cite>; , we retain partially-linked and non-linked facts in our dataset. 3 Further information can be found at htps:// catalog.ldc.upenn.edu/LDC2008T19. Evaluation set. Open RE models produce predictions for all relations and all tuples. To keep the experimental study feasible and comparable to previous studies, we use the full training data but evaluate each model's predictions on only the subsample of 10k tuples (\u2248 6% of all tuples) of<cite> Riedel et al. (2013)</cite> . The subsample consisted of 20% linked, 40% partially-linked and 40% non-linked tuples.",
  "y": "differences"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_12",
  "x": "1 summarizes statistics of the resulting dataset. Here we considered a fact or tuple as linked if both of its entities were linked to Freebase, as partiallylinked if only one of its entities was linked, and as non-linked otherwise. In contrast to previous work<cite> (Riedel et al., 2013</cite>; , we retain partially-linked and non-linked facts in our dataset. 3 Further information can be found at htps:// catalog.ldc.upenn.edu/LDC2008T19. Evaluation set. Open RE models produce predictions for all relations and all tuples. To keep the experimental study feasible and comparable to previous studies, we use the full training data but evaluate each model's predictions on only the subsample of 10k tuples (\u2248 6% of all tuples) of<cite> Riedel et al. (2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_13",
  "x": "Our study focused on these two factorization models because they outperformed other models (including nonfactorization models) in previous studies<cite> (Riedel et al., 2013</cite>; . All models were trained with the full training data described above. PITF (Drumond et al., 2012) . PITF is a recent tensor factorization method designed for within-KB reasoning. PITF is based on factorization machines so that we used our scalable CORE implementation for training the model. NFE<cite> (Riedel et al., 2013)</cite> . NFE is the full model proposed in the \"universal schema\" work of<cite> Riedel et al. (2013)</cite> .",
  "y": "differences"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_15",
  "x": "PITF is a recent tensor factorization method designed for within-KB reasoning. PITF is based on factorization machines so that we used our scalable CORE implementation for training the model. NFE<cite> (Riedel et al., 2013)</cite> . NFE is the full model proposed in the \"universal schema\" work of<cite> Riedel et al. (2013)</cite> . It uses a linear combination of three component models: a neighborhood # (in parentheses) in the top-100 evaluation-set tuples for surface relations. We consider as context the article metadata (m), the tuple types (t) and the bag-of-words (w). Best value per relation in bold (unique winner) or italic (multiple winners).",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_16",
  "x": "model (N), a matrix factorization model (F), and an entity model (E). The F and E models together are similar (but not equal) to our CORE model without context. The NFE model outperformed tensor models as well as clustering methods and distantly supervised methods in the experimental study of<cite> Riedel et al. (2013)</cite> for open RE tasks. We use the original source code of<cite> Riedel et al. (2013)</cite> for training. CORE. We include multiple variants of our model in the experimental study, each differing by the amount of context being used. We consider as context the article metadata (m), the tuple types (t) and the bag-of-words (w).",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_17",
  "x": "model (N), a matrix factorization model (F), and an entity model (E). The F and E models together are similar (but not equal) to our CORE model without context. The NFE model outperformed tensor models as well as clustering methods and distantly supervised methods in the experimental study of<cite> Riedel et al. (2013)</cite> for open RE tasks. We use the original source code of<cite> Riedel et al. (2013)</cite> for training. CORE. We include multiple variants of our model in the experimental study, each differing by the amount of context being used. We consider as context the article metadata (m), the tuple types (t) and the bag-of-words (w).",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_18",
  "x": "NFE<cite> (Riedel et al., 2013)</cite> . NFE is the full model proposed in the \"universal schema\" work of<cite> Riedel et al. (2013)</cite> . The NFE model outperformed tensor models as well as clustering methods and distantly supervised methods in the experimental study of<cite> Riedel et al. (2013)</cite> for open RE tasks. We use the original source code of<cite> Riedel et al. (2013)</cite> for training.",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_19",
  "x": "We additionally consider the CORE+t, CORE+w, CORE+mt, and CORE+mtw models, where the suffix indicates which contextual information has been included. The total number of variables in the resulting models varied between 300k (CORE) to 350k (CORE+mtw). We used a modified version of libfm for training. 4 Our version adds support for BPR and parallelizes the training algorithm. Methodology. To evaluate the prediction performance of each method, we followed<cite> Riedel et al. (2013)</cite> . We considered a collection of 19 Freebase relations (Tab. 2) and 10 surface relations (Tab. 3) and restrict predictions to tuples in the evaluation set.",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_20",
  "x": "Evaluation metrics. For each relation and method, we computed the top-100 evaluation set predictions and labeled them manually. We used as evaluation metrics the mean average precision defined as: where indicator I k takes value 1 if the k-th prediction is true and 0 otherwise, and # denotes the number of true tuples for the relation in the top-100 predictions of all models. The denominator is included to account for the fact that the evaluation set may include less than 100 true facts. MAP 100 # reflects how many true facts are found by each method as well as their ranking. If all # facts are found and ranked top, then MAP 100 # = 1. Note that our definition of MAP 100 # differs slightly from<cite> Riedel et al. (2013)</cite> ; our metric is more robust because it is based on completely labeled evaluation data.",
  "y": "differences"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_21",
  "x": "Parameters. For all systems, we used d = 100 latent factors, \u03bb = 0.01 for all variables, a constant learning rate of \u03b7 = 0.05, and ran 1000 epochs of stochastic gradient ascent. These choices correspond to the ones of<cite> Riedel et al. (2013)</cite> ; no further tuning was performed. ---------------------------------- **RESULTS.** Prediction performance. The results of our experimental study are summarized in Tab.",
  "y": "uses"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_0",
  "x": "Coreference resolution forms an important component for natural language processing and information extraction pipelines due to its utility in relation extraction, cross-document coreference, text summarization, and question answering. The task of coreference is challenging for automated systems as the local information contained in the document is often not enough to accurately disambiguate mentions, for example, coreferencing (m 1 , m 2 ) requires identifying that George W. Bush (m 1 ) is the governor of Texas (m 2 ), and similarly for (m 3 , m 4 ). External knowledge-bases such as FrameNet (Baker et al., 1998) , Wikipedia, Yago (Suchanek et al., 2007) , and Freebase (Bollacker et al., 2008) , can be used to provide global context, and there is a strong need for coreference resolution systems to accurately use such sources for disambiguation. Incorporating external knowledge bases into coreference has been the subject of active recent research. Ponzetto and Strube (2006) and <cite>Ratinov and Roth (2012)</cite> precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of mentions to the external entities is itself a difficult task, these systems favor high-precision linking.",
  "y": "background"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_1",
  "x": "External knowledge-bases such as FrameNet (Baker et al., 1998) , Wikipedia, Yago (Suchanek et al., 2007) , and Freebase (Bollacker et al., 2008) , can be used to provide global context, and there is a strong need for coreference resolution systems to accurately use such sources for disambiguation. Incorporating external knowledge bases into coreference has been the subject of active recent research. Ponzetto and Strube (2006) and <cite>Ratinov and Roth (2012)</cite> precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of mentions to the external entities is itself a difficult task, these systems favor high-precision linking. Unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer (such as the non-transcript documents in <cite>Ratinov and Roth (2012)</cite> ). Alternatively, Rahman and Ng (2011) link each mention to multiple entities in the knowledge base, improving recall at the cost of lower precision; the attributes of all the linked entities are aggregated as features.",
  "y": "background"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_2",
  "x": "The large set of surface string variations and constant reranking of the entity candidates during inference allows our approach to correct mistakes in alignment and makes external information applicable to a wider variety of mentions. Our paper provides the following contributions: (1) an approach that jointly reasons about both within-doc entities and their alignment to KBentities by dynamically adjusting a ranked list of candidate alignments, during coreference, (2) Utilization of a larger set of surface string variations for each entity candidate by using links that appear all over the web (Spitkovsky and Chang, 2012) , (3) A combination of these approaches that improves upon a competitive baseline without a knowledge base by 1.09 B 3 F1 points on the ACE 2004 data, and outperforms the state-of-the-art coreference system (Stoyanov and Eisner, 2012) by 0.41 B 3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in <cite>Ratinov and Roth (2012)</cite> , and documents that contain a large number of mentions. ---------------------------------- **BASELINE PAIRWISE SYSTEM** In this section we describe a variant of a commonlyused coreference resolution system that does not utilize external knowledge sources. This widely adopted model casts the problem as a series of binary classifications (Soon et al., 2001; Ng and Cardie, 2002; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Stoyanov et al., 2010) . Given a document with its mentions, the system iteratively checks each mention m j for coreference with preceding mentions using a classifier.",
  "y": "uses"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_3",
  "x": "These queries return scored, ranked lists of entity candidates (Wikipedia articles), which we associate with each proper noun mention, leaving the rest of the candidate lists empty. Linking is often noisy, so only selecting the high-precision links as in <cite>Ratinov and Roth (2012)</cite> results in too few matches, while picking an aggregation of all links results in more noise due to lower precision (Rahman and Ng, 2011) . Additionally, since linking is often performed in pre-processing, two mentions that are determined coreferent during inference could still be linked to different KB entities. To avoid these problems, we keep a list of candidate links for each mention, merging the lists when two mentions are determined coreferent, and rerank this list during inference. ---------------------------------- **POPULATING ENTITY ATTRIBUTES** After linking to Wikipedia, we have a list of candidate KB entities for each mention.",
  "y": "background"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_4",
  "x": "**LINKING TO WIKIPEDIA** To create the initial entity candidate lists for proper noun mentions, we query a knowledge base searcher (Dalton and Dietz, 2013) with the text of these mentions. These queries return scored, ranked lists of entity candidates (Wikipedia articles), which we associate with each proper noun mention, leaving the rest of the candidate lists empty. Linking is often noisy, so only selecting the high-precision links as in <cite>Ratinov and Roth (2012)</cite> results in too few matches, while picking an aggregation of all links results in more noise due to lower precision (Rahman and Ng, 2011) . Additionally, since linking is often performed in pre-processing, two mentions that are determined coreferent during inference could still be linked to different KB entities. To avoid these problems, we keep a list of candidate links for each mention, merging the lists when two mentions are determined coreferent, and rerank this list during inference. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_5",
  "x": "**POPULATING ENTITY ATTRIBUTES** After linking to Wikipedia, we have a list of candidate KB entities for each mention. Each entity has access to external information keyed on the Wikipedia article, but this information could more generally come from any knowledge base. Given these entities, there are many possible features that may be used for disambiguation of the mentions, such as gender and fine-grained Wikipedia categories as used by <cite>Ratinov and Roth (2012)</cite> , however most of these features may not be relevant to the task of within-document coreference. Instead, an important resource for linking non-proper mentions of an entity is to identify the possible name variations of the entity. For example, it would be useful to know that Massachusetts is also referred to as \"The 6th State\", however this information is not readily available from Wikipedia. 1 We instead use the corpus described in Spitkovsky and Chang (2012) that consists of anchor texts of links to Wikipedia that appear on web pages.",
  "y": "background"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_6",
  "x": "After linking to Wikipedia, we have a list of candidate KB entities for each mention. Each entity has access to external information keyed on the Wikipedia article, but this information could more generally come from any knowledge base. Given these entities, there are many possible features that may be used for disambiguation of the mentions, such as gender and fine-grained Wikipedia categories as used by <cite>Ratinov and Roth (2012)</cite> , however most of these features may not be relevant to the task of within-document coreference. Instead, an important resource for linking non-proper mentions of an entity is to identify the possible name variations of the entity. For example, it would be useful to know that Massachusetts is also referred to as \"The 6th State\", however this information is not readily available from Wikipedia. 1 We instead use the corpus described in Spitkovsky and Chang (2012) that consists of anchor texts of links to Wikipedia that appear on web pages. This collection of anchor texts is sufficiently extensive to cover many common misspellings of entity names, as well as many name variations missing from Wikipedia.",
  "y": "differences"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_7",
  "x": "Baseline: Our implementation of a pairwise model that is similar to the approach in Bengston and Roth (2008) with the differences described in Section 2. This is our baseline system that performs coreference without the use of external knowledge. Incidentally, it outperforms Bengston and Roth (2008) . Dynamic linking: This is our complete system as described in Section 3, in which the list of candidates associated with each mention is reranked and modified during inference. Static linking: Identical to dynamic linking except that entity candidate lists are not merged during inference (i.e., Algorithm 1 without line 17). This approach is comparable to the fixed alignment model, as in the approaches of Ponzetto and Strube (2006) and <cite>Ratinov and Roth (2012)</cite> . ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_8",
  "x": "---------------------------------- **PERFORMANCE ON TRANSCRIPTS** The quality of alignment and the coreference predictions for a document is influenced by the quality of the mentions in the document. In particular, Table 2 : Evaluation on the ACE test data, with the system trained on the train and development sets. ACE contains a large number of broadcast news documents, many of which consist of transcribed data containing noise in the form of incomplete sentences and disfluencies. Since these transcripts provide an additional challenge for alignment and coreference, <cite>Ratinov and Roth (2012)</cite> only use the set of non-transcripts for their evaluation. Using dynamic linking and a large set of surface string variations, our approach may be able to provide an improvement even on the transcripts.",
  "y": "differences"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_9",
  "x": "In particular, Table 2 : Evaluation on the ACE test data, with the system trained on the train and development sets. ACE contains a large number of broadcast news documents, many of which consist of transcribed data containing noise in the form of incomplete sentences and disfluencies. Since these transcripts provide an additional challenge for alignment and coreference, <cite>Ratinov and Roth (2012)</cite> only use the set of non-transcripts for their evaluation. Using dynamic linking and a large set of surface string variations, our approach may be able to provide an improvement even on the transcripts. To identify the transcripts in the test set, we use the approximation from <cite>Ratinov and Roth (2012)</cite> that considers a document to be non-transcribed if it contains proper noun mentions and at least a third of those start with a capital letter. The performance is shown in Table 3 , while the improvement over our baseline is shown in Figure 3 . Our static linking matches the performance of <cite>Ratinov and Roth (2012)</cite> on the non-transcripts.",
  "y": "uses"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_10",
  "x": "The performance is shown in Table 3 , while the improvement over our baseline is shown in Figure 3 . Our static linking matches the performance of <cite>Ratinov and Roth (2012)</cite> on the non-transcripts. Further, the improvement of static linking on the transcripts over the baseline is lower than that on the non-transcript data, suggesting that noisy mentions and text result in poor quality alignment. Dynamic linking, on the other hand, not only outperforms all other systems, but also shows a higher improvement over the baseline on the transcripts than on non-transcripts. This indicates that dynamic linking approach is robust to noise, and its wider variety of surface strings and flexible alignments are especially useful for transcripts. ---------------------------------- **ONTONOTES**",
  "y": "similarities"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_11",
  "x": "Our method differs in that we draw such features from entity candidates during inference, and also maintain and update a set of candidate entity links instead of selecting only one. Rahman and Ng (2011) introduce similar features from a more extensive set of knowledge sources (such as YAGO and FrameNet) into a cluster-based model whose features change as inference proceeds. However, the features for each cluster come from a combination of all entities aligned to the cluster mentions. We improve upon this approach by maintaining a list of the candidate entities for each mention cluster, modifying this list during the course of inference, and using features from only the top-ranked candidate at any time. Further, they do not provide a comparison on a standard dataset. <cite>Ratinov and Roth (2012)</cite> extend the multi-sieve coreference model (Raghunathan et al., 2010) by identifying at most a single candidate for each mention, and incorporating high-precision attributes extracted from Wikipedia. The high-precision mention-candidate pairings are precomputed and fixed; additionally, the features for an entity are based on the predictions of the previous sieves, thus fixed while a sieve is applied.",
  "y": "background"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_12",
  "x": "We improve upon this approach by maintaining a list of the candidate entities for each mention cluster, modifying this list during the course of inference, and using features from only the top-ranked candidate at any time. Further, they do not provide a comparison on a standard dataset. <cite>Ratinov and Roth (2012)</cite> extend the multi-sieve coreference model (Raghunathan et al., 2010) by identifying at most a single candidate for each mention, and incorporating high-precision attributes extracted from Wikipedia. The high-precision mention-candidate pairings are precomputed and fixed; additionally, the features for an entity are based on the predictions of the previous sieves, thus fixed while a sieve is applied. With these restrictions, they show improvements over the state-ofthe-art on a subset of ACE mentions that are more easily aligned to Wikipedia, while our approach demonstrates improvements on the complete set of mentions including the tougher to link mentions from the transcripts. There are a number of approaches that provide an alignment from mentions in a document to Wikipedia. Wikifier (Ratinov et al., 2011) analyzes the context around the mentions and the entities jointly, and was used to align mentions for coreference in <cite>Ratinov and Roth (2012)</cite> .",
  "y": "differences"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_13",
  "x": "We improve upon this approach by maintaining a list of the candidate entities for each mention cluster, modifying this list during the course of inference, and using features from only the top-ranked candidate at any time. Further, they do not provide a comparison on a standard dataset. <cite>Ratinov and Roth (2012)</cite> extend the multi-sieve coreference model (Raghunathan et al., 2010) by identifying at most a single candidate for each mention, and incorporating high-precision attributes extracted from Wikipedia. The high-precision mention-candidate pairings are precomputed and fixed; additionally, the features for an entity are based on the predictions of the previous sieves, thus fixed while a sieve is applied. With these restrictions, they show improvements over the state-ofthe-art on a subset of ACE mentions that are more easily aligned to Wikipedia, while our approach demonstrates improvements on the complete set of mentions including the tougher to link mentions from the transcripts. There are a number of approaches that provide an alignment from mentions in a document to Wikipedia. Wikifier (Ratinov et al., 2011) analyzes the context around the mentions and the entities jointly, and was used to align mentions for coreference in <cite>Ratinov and Roth (2012)</cite> .",
  "y": "background"
 },
 {
  "id": "04461d946dadc759e4be1207655159_0",
  "x": "****HEAD-DRIVEN HIERARCHICAL PHRASE-BASED TRANSLATION**** **ABSTRACT** This paper presents an extension of Chiang's hierarchical phrase-based (HPB) model, called Head-Driven HPB (HD-HPB), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang's model with average gains of 1.91 points absolute in BLEU. ---------------------------------- **INTRODUCTION** Chiang's hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; <cite>Chiang, 2007)</cite> and has been widely adopted in statistical machine translation (SMT).",
  "y": "background"
 },
 {
  "id": "04461d946dadc759e4be1207655159_1",
  "x": "Like Chiang (2005) and<cite> Chiang (2007)</cite> , our HD-HPB translation model adopts a synchronous context free grammar, a rewriting system which generates source and target side string pairs simultaneously using a context-free grammar. Instead of collapsing all non-terminals in the source language into a single symbol X as in<cite> Chiang (2007)</cite> , given a word sequence f i j from position i to position j, we first find heads and then concatenate the POS tags of these heads as f i j 's non-terminal symbol. Specifically, we adopt unlabeled dependency structure to derive heads, which are defined as: is regarded as a head if it is dominated by a word outside of this sequence. Note that this definition (i) allows for a word sequence to have one or more heads (largely due to the fact that a word sequence is not necessarily linguistically constrained) and (ii) ensures that heads are always the highest heads in the sequence from a dependency structure perspective. For example, the word sequence ouzhou baguo lianming in Figure 1 has two heads (i.e., baguo and lianming, ouzhou is not a head of this sequence since its headword baguo falls within this sequence) and the non-terminal corresponding to the sequence is thus labeled as NN-AD. It is worth noting that in this paper we only refine non-terminal X on the source side to headinformed ones, while still using X on the target side.",
  "y": "similarities"
 },
 {
  "id": "04461d946dadc759e4be1207655159_2",
  "x": "Like Chiang (2005) and<cite> Chiang (2007)</cite> , our HD-HPB translation model adopts a synchronous context free grammar, a rewriting system which generates source and target side string pairs simultaneously using a context-free grammar. Instead of collapsing all non-terminals in the source language into a single symbol X as in<cite> Chiang (2007)</cite> , given a word sequence f i j from position i to position j, we first find heads and then concatenate the POS tags of these heads as f i j 's non-terminal symbol. Specifically, we adopt unlabeled dependency structure to derive heads, which are defined as: is regarded as a head if it is dominated by a word outside of this sequence. Note that this definition (i) allows for a word sequence to have one or more heads (largely due to the fact that a word sequence is not necessarily linguistically constrained) and (ii) ensures that heads are always the highest heads in the sequence from a dependency structure perspective. For example, the word sequence ouzhou baguo lianming in Figure 1 has two heads (i.e., baguo and lianming, ouzhou is not a head of this sequence since its headword baguo falls within this sequence) and the non-terminal corresponding to the sequence is thus labeled as NN-AD. It is worth noting that in this paper we only refine non-terminal X on the source side to headinformed ones, while still using X on the target side.",
  "y": "differences"
 },
 {
  "id": "04461d946dadc759e4be1207655159_3",
  "x": "Specifically, we adopt unlabeled dependency structure to derive heads, which are defined as: is regarded as a head if it is dominated by a word outside of this sequence. Note that this definition (i) allows for a word sequence to have one or more heads (largely due to the fact that a word sequence is not necessarily linguistically constrained) and (ii) ensures that heads are always the highest heads in the sequence from a dependency structure perspective. For example, the word sequence ouzhou baguo lianming in Figure 1 has two heads (i.e., baguo and lianming, ouzhou is not a head of this sequence since its headword baguo falls within this sequence) and the non-terminal corresponding to the sequence is thus labeled as NN-AD. It is worth noting that in this paper we only refine non-terminal X on the source side to headinformed ones, while still using X on the target side. According to the occurrence of terminals in translation rules, we group rules in the HD-HPB model into two categories: head-driven hierarchical rules (HD-HRs) and non-terminal reordering rules (NRRs), where the former have at least one terminal on both source and target sides and the later have no terminals. For rule extraction, we first identify initial phrase pairs on word-aligned sentence pairs by using the same criterion as most phrase-based translation models (Och and Ney, 2004 ) and Chiang's HPB model (Chiang, 2005; <cite>Chiang, 2007)</cite> .",
  "y": "similarities"
 },
 {
  "id": "04461d946dadc759e4be1207655159_4",
  "x": "This is the same as the hierarchical rules defined in Chiang's HPB model<cite> (Chiang, 2007)</cite> , except that we use head POSinformed non-terminal symbols in the source language. We look for initial phrase pairs that contain other phrases and then replace sub-phrases with POS tags corresponding to their heads. Given the word alignment in Figure 1 , Table 1 demonstrates the difference between hierarchical rules in<cite> Chiang (2007)</cite> and HD-HRs defined here. Similar to Chiang's HPB model, our HD-HPB model will result in a large number of rules causing problems in decoding. To alleviate these problems, we filter our HD-HRs according to the same constraints as described in<cite> Chiang (2007)</cite> . Moreover, we discard rules that have non-terminals with more than four heads. ----------------------------------",
  "y": "differences similarities"
 },
 {
  "id": "04461d946dadc759e4be1207655159_5",
  "x": "We extract HD-HRs and NRRs based on initial phrase pairs, respectively. ---------------------------------- **HD-HRS: HEAD-DRIVEN HIERARCHICAL RULES** As mentioned, a HD-HR has at least one terminal on both source and target sides. This is the same as the hierarchical rules defined in Chiang's HPB model<cite> (Chiang, 2007)</cite> , except that we use head POSinformed non-terminal symbols in the source language. We look for initial phrase pairs that contain other phrases and then replace sub-phrases with POS tags corresponding to their heads. Given the word alignment in Figure 1 , Table 1 demonstrates the difference between hierarchical rules in<cite> Chiang (2007)</cite> and HD-HRs defined here.",
  "y": "differences"
 },
 {
  "id": "04461d946dadc759e4be1207655159_6",
  "x": "Given the word alignment in Figure 1 , Table 1 demonstrates the difference between hierarchical rules in<cite> Chiang (2007)</cite> and HD-HRs defined here. Similar to Chiang's HPB model, our HD-HPB model will result in a large number of rules causing problems in decoding. To alleviate these problems, we filter our HD-HRs according to the same constraints as described in<cite> Chiang (2007)</cite> . Moreover, we discard rules that have non-terminals with more than four heads. ---------------------------------- **NRRS: NON-TERMINAL REORDERING RULES** NRRs are translation rules without terminals.",
  "y": "similarities uses"
 },
 {
  "id": "04461d946dadc759e4be1207655159_8",
  "x": "To speed up decoding, we currently (i) only use monotone and swap NRRs and (ii) limit the number of non-terminals in a NRR to 2. ---------------------------------- **FEATURES AND DECODING** Given e for the translation output in the target language, s and t for strings of terminals and nonterminals on the source and target side, respectively, we use a feature set analogous to the default feature set of<cite> Chiang (2007)</cite> , including: \u2022 P hd-hr (t|s) and P hd-hr (s|t), translation probabilities for HD-HRs; \u2022 P lex (t|s) and P lex (s|t), lexical translation probabilities for HD-HRs; \u2022 P ty hd-hr = exp (\u22121), rule penalty for HD-HRs;",
  "y": "similarities"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_0",
  "x": "Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a) , summarization (Zhou et al., 2006 ), text coherence (Lapata and Barzilay, 2005) , tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011) , etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012) , resulting in inadequate evidence to generalize to robust sentential semantics. <cite>Weighted Textual Matrix Factorization [WTMF]</cite> (<cite>Guo and Diab, 2012b</cite> ) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large margin in the SS task, yielding state-of-the-art performance on the LI06 (Li et al., 2006 ) SS dataset. However, all of these models make harsh simplifying assumptions on how a token is generated: (1) in LSA/<cite>WTMF</cite>, a token is generated by the inner product of the word latent vector and the document latent vector; (2) in LDA, all the tokens in a document are sampled from the same document level topic distribution. Under this framework, they ignore rich linguistic phenomena such as inter-word dependency, semantic scope of words, etc. This is a result of simply using document IDs as features to represent a word. Modeling quality lexical semantics in latent variable models does not draw enough attention in the community, since people usually apply dimension reduction techniques for documents, which have abundant words for extracting the document level semantics.",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_1",
  "x": "<cite>Weighted Textual Matrix Factorization [WTMF]</cite> (<cite>Guo and Diab, 2012b</cite> ) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large margin in the SS task, yielding state-of-the-art performance on the LI06 (Li et al., 2006 ) SS dataset. However, all of these models make harsh simplifying assumptions on how a token is generated: (1) in LSA/<cite>WTMF</cite>, a token is generated by the inner product of the word latent vector and the document latent vector; (2) in LDA, all the tokens in a document are sampled from the same document level topic distribution. Under this framework, they ignore rich linguistic phenomena such as inter-word dependency, semantic scope of words, etc. This is a result of simply using document IDs as features to represent a word. Modeling quality lexical semantics in latent variable models does not draw enough attention in the community, since people usually apply dimension reduction techniques for documents, which have abundant words for extracting the document level semantics. However, in the SS setting, it is crucial to make good use of each word, given the limited number of words in a sentence. We believe a reasonable word generation story will avoid introducing noise in sentential semantics, encouraging robust lexical semantics which can further boost the sentential semantics.",
  "y": "motivation"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_2",
  "x": "We believe a reasonable word generation story will avoid introducing noise in sentential semantics, encouraging robust lexical semantics which can further boost the sentential semantics. In this paper, we explicitly encode lexical semantics, both corpus-based and knowledge-based information, in the <cite>WTMF</cite> model, by which we are able to achieve even better results in SS task. The additional corpus-based information we exploit is selectional preference semantics (Resnik, 1997) , a feature already existing in the data yet ignored by most latent variable models. Selectional preference focuses on the admissible arguments for a word, thus capturing more nuanced semantics than the sentence IDs (when applied to a corpus of sentences as opposed to documents). Consider the following example: In <cite>WTMF</cite>/LSA/LDA, a word will receive semantics from all the other words in a sentence, hence, the word oil, in the above example, will be assigned the incorrect finance topic that reflects the sentence level semantics. Moreover, the problem worsens for adjectives, adverbs and verbs, which have a much narrower semantic scope than the whole sentence. For example, the verb say should only be associated with analyst (only receiving semantics from analyst), as it is not related to other words in the sentence.",
  "y": "uses motivation"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_3",
  "x": "We believe a reasonable word generation story will avoid introducing noise in sentential semantics, encouraging robust lexical semantics which can further boost the sentential semantics. In this paper, we explicitly encode lexical semantics, both corpus-based and knowledge-based information, in the <cite>WTMF</cite> model, by which we are able to achieve even better results in SS task. The additional corpus-based information we exploit is selectional preference semantics (Resnik, 1997) , a feature already existing in the data yet ignored by most latent variable models. Selectional preference focuses on the admissible arguments for a word, thus capturing more nuanced semantics than the sentence IDs (when applied to a corpus of sentences as opposed to documents). Consider the following example: In <cite>WTMF</cite>/LSA/LDA, a word will receive semantics from all the other words in a sentence, hence, the word oil, in the above example, will be assigned the incorrect finance topic that reflects the sentence level semantics. Moreover, the problem worsens for adjectives, adverbs and verbs, which have a much narrower semantic scope than the whole sentence. For example, the verb say should only be associated with analyst (only receiving semantics from analyst), as it is not related to other words in the sentence.",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_4",
  "x": "Moreover, the problem worsens for adjectives, adverbs and verbs, which have a much narrower semantic scope than the whole sentence. For example, the verb say should only be associated with analyst (only receiving semantics from analyst), as it is not related to other words in the sentence. In contrast, oil, according to its selectional preference, should be associated with crude indicating the resource topic. We believe modeling selectional preference capturing local evidence completes the semantic picture for words, hence further rendering better sentential semantics. To our best knowledge, this is the first work to model selectional preference for sentence/document semantics. We also integrate knowledge-based semantics in the <cite>WTMF</cite> framework. Knowledge-based semantics, a human-annotated clean resource, is an important complement to corpus-based noisy cooccurrence information.",
  "y": "extends"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_5",
  "x": "---------------------------------- **WEIGHTED TEXTUAL MATRIX FACTORIZATION** <cite>Our</cite> previous work (<cite>Guo and Diab, 2012b</cite> ) models the sentences in the weighted matrix factorization framework ( Figure 1 ). The corpus is stored in an M \u00d7 N matrix X, with each cell containing the TF-IDF values of words. The rows of X are M distinct words and columns are N sentences. As in Figure  1 , X is approximated by the product of a K \u00d7 M matrix P and a K \u00d7 N matrix Q. Accordingly, each sentence s j is represented by a K dimensional latent vector Q \u00b7,j . Similarly a word w i is generalized by P \u00b7,i .",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_6",
  "x": "Leveraging these pairs, an infrequent word such as purchase can exploit robust latent vectors from its synonyms such as buy. Similar words pairs can be seamlessly modeled in <cite>WTMF</cite>, since in the matrix factorization framework a latent vector profile is explicitly created for each word, while in LDA all the data structures are designed for documents/sentences. We construct a graph to connect words according to the extracted similar word pairs, to encourage similar words to share similar latent vector profiles. We will refer to our proposed novel model as WTMF+PK. ---------------------------------- **WEIGHTED TEXTUAL MATRIX FACTORIZATION** <cite>Our</cite> previous work (<cite>Guo and Diab, 2012b</cite> ) models the sentences in the weighted matrix factorization framework ( Figure 1 ).",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_7",
  "x": "To our best knowledge, this is the first work to model selectional preference for sentence/document semantics. We also integrate knowledge-based semantics in the <cite>WTMF</cite> framework. Knowledge-based semantics, a human-annotated clean resource, is an important complement to corpus-based noisy cooccurrence information. We extract similar word pairs from Wordnet (Fellbaum, 1998) . Leveraging these pairs, an infrequent word such as purchase can exploit robust latent vectors from its synonyms such as buy. Similar words pairs can be seamlessly modeled in <cite>WTMF</cite>, since in the matrix factorization framework a latent vector profile is explicitly created for each word, while in LDA all the data structures are designed for documents/sentences. We construct a graph to connect words according to the extracted similar word pairs, to encourage similar words to share similar latent vector profiles.",
  "y": "motivation uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_8",
  "x": "P and Q is optimized by minimize the objective function: where \u03bb is a regularization term. Missing tokens are modeled by assigning a different weight w m for each 0 cell in the matrix X. We can see the inner product of a word vector P \u00b7,i and a sentence vector Q \u00b7,j is used to approximate the cell X ij . The graphical model of <cite>WTMF</cite> is illustrated in Figure 2a . A w i /s j node is a latent vector P \u00b7,i /Q \u00b7,j , corresponding to a word/sentence, respectively. A shaded node is a non-zero cell in X, representing an observed token in a sentence. For simplicity, the missing tokens and weights are not shown in the graph.",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_9",
  "x": "The Figure 2c shows the final WTMF+PK model. ---------------------------------- **INFERENCE** In (<cite>Guo and Diab, 2012b</cite>) <cite>we</cite> use Alternating Least Square [ALS] for inference, which is to set the derivative of equation 1 for P/Q to 0 and iteratively compute P/Q by fixing the other matrix (Srebro and Jaakkola, 2003) . However, it is no longer applicable with the new term (equation 2) involving the length of word vectors |P \u00b7,i |. Therefore we approximate the objective function by treating the vector length |P \u00b7,i | as fixed values during the ALS iterations: where P \u00b7,s(i) are the latent vectors of similar words of word i; the length of these vectors in the current iteration are stored in L s(i) (similarly L i is the current length of P \u00b7,i ) (cf. (Steck, 2010; <cite>Guo and Diab, 2012b</cite>) for optimization details).",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_10",
  "x": "|P \u00b7,i | denotes the length of the vector P \u00b7,i . The coefficient \u03b4, analogous to \u03b3, denotes the importance of the knowledge-based evidence. The Figure 2c shows the final WTMF+PK model. ---------------------------------- **INFERENCE** In (<cite>Guo and Diab, 2012b</cite>) <cite>we</cite> use Alternating Least Square [ALS] for inference, which is to set the derivative of equation 1 for P/Q to 0 and iteratively compute P/Q by fixing the other matrix (Srebro and Jaakkola, 2003) . However, it is no longer applicable with the new term (equation 2) involving the length of word vectors |P \u00b7,i |.",
  "y": "motivation"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_11",
  "x": "**INFERENCE** In (<cite>Guo and Diab, 2012b</cite>) <cite>we</cite> use Alternating Least Square [ALS] for inference, which is to set the derivative of equation 1 for P/Q to 0 and iteratively compute P/Q by fixing the other matrix (Srebro and Jaakkola, 2003) . However, it is no longer applicable with the new term (equation 2) involving the length of word vectors |P \u00b7,i |. Therefore we approximate the objective function by treating the vector length |P \u00b7,i | as fixed values during the ALS iterations: where P \u00b7,s(i) are the latent vectors of similar words of word i; the length of these vectors in the current iteration are stored in L s(i) (similarly L i is the current length of P \u00b7,i ) (cf. (Steck, 2010; <cite>Guo and Diab, 2012b</cite>) for optimization details). ---------------------------------- **EXPERIMENTAL SETTING**",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_12",
  "x": "**INFERENCE** In (<cite>Guo and Diab, 2012b</cite>) <cite>we</cite> use Alternating Least Square [ALS] for inference, which is to set the derivative of equation 1 for P/Q to 0 and iteratively compute P/Q by fixing the other matrix (Srebro and Jaakkola, 2003) . However, it is no longer applicable with the new term (equation 2) involving the length of word vectors |P \u00b7,i |. Therefore we approximate the objective function by treating the vector length |P \u00b7,i | as fixed values during the ALS iterations: where P \u00b7,s(i) are the latent vectors of similar words of word i; the length of these vectors in the current iteration are stored in L s(i) (similarly L i is the current length of P \u00b7,i ) (cf. (Steck, 2010; <cite>Guo and Diab, 2012b</cite>) for optimization details). ---------------------------------- **EXPERIMENTAL SETTING**",
  "y": "extends"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_13",
  "x": "---------------------------------- **EXPERIMENTAL SETTING** We build the model WTMF+PK on the same corpora as used in <cite>our</cite> previous work (<cite>Guo and Diab, 2012b</cite>) , comprising the following: Brown corpus (each sentence is treated as a document), sense definitions from Wiktionary and Wordnet (only definitions without target words and usage examples). We follow the preprocessing steps in (Guo and Diab, 2012c) : tokenization, pos-tagging, lemmatization and further merge lemmas. The corpus is used for building matrix X. The evaluation datasets are LI06 dataset and Semeval-2012 STS [STS12] (Agirre et al., 2012) dataset. LI06 consists of 30 sentence pairs (dictionary definitions). For STS12, 1 the training data (2000 pairs) are used as the tuning set for setting the parameters of our models.",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_14",
  "x": "Pearson correlation between the system's answer and gold standard similarity scores is used as the evaluation metric. We include three baselines LSA, LDA and <cite>WTMF</cite> using the setting described in (<cite>Guo and Diab, 2012b</cite>) . We run Gibbs Sampling based LDA for 2000 iterations and average the model over the last 10 iterations. For <cite>WTMF</cite>, we run 20 iterations and fix the missing words weight at w m = 0.01 with a regularization coefficient set at \u03bb = 20, which is the best condition found in (<cite>Guo and Diab, 2012b</cite>) . Table 1 summarizes the results at dimension K = 100 (the dimension of latent vectors). To remove randomness, each reported number is the averaged results of 10 runs. Based on the STS tuning set, we experiment with different values for the selectional preference weight (\u03b3 = {0, 1, 2}), and likewise for the similar word pairs weight varying the \u03b4 value as follows \u03b4 = {0, 0.1, 0.3, 0.5, 0.7}. The performance on STS12 tuning and test dataset as well as on the LI06 dataset are illustrated in Figures 3a,  3b and 3d .",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_15",
  "x": "We run Gibbs Sampling based LDA for 2000 iterations and average the model over the last 10 iterations. For <cite>WTMF</cite>, we run 20 iterations and fix the missing words weight at w m = 0.01 with a regularization coefficient set at \u03bb = 20, which is the best condition found in (<cite>Guo and Diab, 2012b</cite>) . Table 1 summarizes the results at dimension K = 100 (the dimension of latent vectors). To remove randomness, each reported number is the averaged results of 10 runs. Based on the STS tuning set, we experiment with different values for the selectional preference weight (\u03b3 = {0, 1, 2}), and likewise for the similar word pairs weight varying the \u03b4 value as follows \u03b4 = {0, 0.1, 0.3, 0.5, 0.7}. The performance on STS12 tuning and test dataset as well as on the LI06 dataset are illustrated in Figures 3a,  3b and 3d . The parameters of model 6 in Table 1 (\u03b3 = 2, \u03b4 = 0.3) are the chosen values based on tuning set performance. Table 1 shows <cite>WTMF</cite> is already a very strong baseline: it outperforms LSA and LDA by a large margin.",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_16",
  "x": "We include three baselines LSA, LDA and <cite>WTMF</cite> using the setting described in (<cite>Guo and Diab, 2012b</cite>) . Table 1 shows <cite>WTMF</cite> is already a very strong baseline: it outperforms LSA and LDA by a large margin.",
  "y": "background uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_17",
  "x": "For <cite>WTMF</cite>, we run 20 iterations and fix the missing words weight at w m = 0.01 with a regularization coefficient set at \u03bb = 20, which is the best condition found in (<cite>Guo and Diab, 2012b</cite>) . Table 1 summarizes the results at dimension K = 100 (the dimension of latent vectors). To remove randomness, each reported number is the averaged results of 10 runs. Based on the STS tuning set, we experiment with different values for the selectional preference weight (\u03b3 = {0, 1, 2}), and likewise for the similar word pairs weight varying the \u03b4 value as follows \u03b4 = {0, 0.1, 0.3, 0.5, 0.7}. The performance on STS12 tuning and test dataset as well as on the LI06 dataset are illustrated in Figures 3a,  3b and 3d . The parameters of model 6 in Table 1 (\u03b3 = 2, \u03b4 = 0.3) are the chosen values based on tuning set performance. Table 1 shows <cite>WTMF</cite> is already a very strong baseline: it outperforms LSA and LDA by a large margin. Same as in (<cite>Guo and Diab, 2012b</cite>) , LSA performance degrades dramatically when trained on a corpus of sentence sized documents, yielding results worse than the surface words baseline 31% (Agirre et al., 2012) .",
  "y": "differences uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_18",
  "x": "To remove randomness, each reported number is the averaged results of 10 runs. Based on the STS tuning set, we experiment with different values for the selectional preference weight (\u03b3 = {0, 1, 2}), and likewise for the similar word pairs weight varying the \u03b4 value as follows \u03b4 = {0, 0.1, 0.3, 0.5, 0.7}. The performance on STS12 tuning and test dataset as well as on the LI06 dataset are illustrated in Figures 3a,  3b and 3d . The parameters of model 6 in Table 1 (\u03b3 = 2, \u03b4 = 0.3) are the chosen values based on tuning set performance. Table 1 shows <cite>WTMF</cite> is already a very strong baseline: it outperforms LSA and LDA by a large margin. Same as in (<cite>Guo and Diab, 2012b</cite>) , LSA performance degrades dramatically when trained on a corpus of sentence sized documents, yielding results worse than the surface words baseline 31% (Agirre et al., 2012) . Using corpus-based selectional preference semantics alone (model 4 WTMF+P in Table  1 ) boosts the performance of <cite>WTMF</cite> by +1.17% on the test set, while using knowledge-based semantics alone (model 5 WTMF+K) improves the over the <cite>WTMF</cite> results by an absolute +2.31%. Combining them (model 6 WTMF+PK) yields the best results, with an absolute increase of +3.39%, which suggests that the two sources of semantic evidence are useful, but more importantly, they are complementary for each other.",
  "y": "similarities"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_19",
  "x": "Based on the STS tuning set, we experiment with different values for the selectional preference weight (\u03b3 = {0, 1, 2}), and likewise for the similar word pairs weight varying the \u03b4 value as follows \u03b4 = {0, 0.1, 0.3, 0.5, 0.7}. The performance on STS12 tuning and test dataset as well as on the LI06 dataset are illustrated in Figures 3a,  3b and 3d . The parameters of model 6 in Table 1 (\u03b3 = 2, \u03b4 = 0.3) are the chosen values based on tuning set performance. Table 1 shows <cite>WTMF</cite> is already a very strong baseline: it outperforms LSA and LDA by a large margin. Same as in (<cite>Guo and Diab, 2012b</cite>) , LSA performance degrades dramatically when trained on a corpus of sentence sized documents, yielding results worse than the surface words baseline 31% (Agirre et al., 2012) . Using corpus-based selectional preference semantics alone (model 4 WTMF+P in Table  1 ) boosts the performance of <cite>WTMF</cite> by +1.17% on the test set, while using knowledge-based semantics alone (model 5 WTMF+K) improves the over the <cite>WTMF</cite> results by an absolute +2.31%. Combining them (model 6 WTMF+PK) yields the best results, with an absolute increase of +3.39%, which suggests that the two sources of semantic evidence are useful, but more importantly, they are complementary for each other. Table 1 also presents the performance on each individual dataset.",
  "y": "differences extends"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_20",
  "x": "Observing the performance using different values of weights in figure 3a and 3b, we can conclude that the selectional preference and similar word pairs yield very promising results. The trends hold in different parameter conditions with a consistent improvement. Figure 3c illustrates the impact of dimension K = {50, 75, 100, 125, 150} on <cite>WTMF</cite> and WTMF+PK. Generally a larger K leads to a higher Pearson correlation, but the improvement is tiny when K \u2265 100 (0.1% increase). Compared to all the unsupervised systems that participated in Semeval STS 2012 task, WTMF+PK yields state-of-the-art performance (70.70%). 2 In (Guo and Diab, 2012c) we also apply <cite>WTMF</cite> (K = 100) on STS12, achieving a correlation of 69.5%. However, additional data is incorporated in the training corpora: (1) STS12 tuning set; (2) for WordNet and Wiktionary data, the target words are also included in the definitions (hence synonym pairs were used); (3) the usage examples of target words were also appended to the definitions.",
  "y": "uses differences"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_21",
  "x": "Compared to all the unsupervised systems that participated in Semeval STS 2012 task, WTMF+PK yields state-of-the-art performance (70.70%). 2 In (Guo and Diab, 2012c) we also apply <cite>WTMF</cite> (K = 100) on STS12, achieving a correlation of 69.5%. However, additional data is incorporated in the training corpora: (1) STS12 tuning set; (2) for WordNet and Wiktionary data, the target words are also included in the definitions (hence synonym pairs were used); (3) the usage examples of target words were also appended to the definitions. 3 While trained with this experimental setting, our model WTMF+PK (\u03b3 = 2, \u03b4 = 0.3, K = 100) is able to reach an even higher correlation of 72.0%. Figure 3d presents the results obtained on the LI06 data set at different weight values for the corpusbased selectional preference semantics \u03b3 and for the knowledge-based semantics \u03b4. Our previous experiments (<cite>Guo and Diab, 2012b</cite>) show that <cite>WTMF</cite> is the state-of-the-art model on LI06. With lexical semantics explicitly modeled, WTMF+PK yields better results than <cite>WTMF</cite> (see Table 1 ).",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_22",
  "x": "However, additional data is incorporated in the training corpora: (1) STS12 tuning set; (2) for WordNet and Wiktionary data, the target words are also included in the definitions (hence synonym pairs were used); (3) the usage examples of target words were also appended to the definitions. 3 While trained with this experimental setting, our model WTMF+PK (\u03b3 = 2, \u03b4 = 0.3, K = 100) is able to reach an even higher correlation of 72.0%. Figure 3d presents the results obtained on the LI06 data set at different weight values for the corpusbased selectional preference semantics \u03b3 and for the knowledge-based semantics \u03b4. Our previous experiments (<cite>Guo and Diab, 2012b</cite>) show that <cite>WTMF</cite> is the state-of-the-art model on LI06. With lexical semantics explicitly modeled, WTMF+PK yields better results than <cite>WTMF</cite> (see Table 1 ). It should be noted that LI06 prefers a smaller similar word pair weight ( a \u03b4 = 0.1 yields the best performance around of 90.75%), yet in almost all conditions WTMF+PK outperforms <cite>WTMF</cite> as shown in Figure 3d . ----------------------------------",
  "y": "background differences"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_23",
  "x": "We will refer to our proposed novel model as WTMF+PK. With lexical semantics explicitly modeled, WTMF+PK yields better results than <cite>WTMF</cite> (see Table 1 ).",
  "y": "differences"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_24",
  "x": "Figure 3d presents the results obtained on the LI06 data set at different weight values for the corpusbased selectional preference semantics \u03b3 and for the knowledge-based semantics \u03b4. Our previous experiments (<cite>Guo and Diab, 2012b</cite>) show that <cite>WTMF</cite> is the state-of-the-art model on LI06. With lexical semantics explicitly modeled, WTMF+PK yields better results than <cite>WTMF</cite> (see Table 1 ). It should be noted that LI06 prefers a smaller similar word pair weight ( a \u03b4 = 0.1 yields the best performance around of 90.75%), yet in almost all conditions WTMF+PK outperforms <cite>WTMF</cite> as shown in Figure 3d . ---------------------------------- **RELATED WORK** SS has progressed immensely in recent years, especially with the establishment of the Semantic Textual Similarity task in SEMEVAL 2012.",
  "y": "background differences"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_25",
  "x": "We will refer to our proposed novel model as WTMF+PK. It should be noted that LI06 prefers a smaller similar word pair weight ( a \u03b4 = 0.1 yields the best performance around of 90.75%), yet in almost all conditions WTMF+PK outperforms <cite>WTMF</cite> as shown in Figure 3d .",
  "y": "differences"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_26",
  "x": "**RELATED WORK** SS has progressed immensely in recent years, especially with the establishment of the Semantic Textual Similarity task in SEMEVAL 2012. Early work in SS focused on word pair similarity in the high dimensional space (Li et al., 2006; Liu et al., 2007; Islam and Inkpen, 2008; Tsatsaronis et al., 2010; Ho et al., 2010) , where co-occurrence information was not efficiently exploited. Researchers (O'Shea et al., 2008) find LSA does not yield good performance. In (<cite>Guo and Diab, 2012b</cite>; Guo and Diab, 2012c) , we show the superiority of the latent space approach in <cite>WTMF</cite>. In this paper, we improve the <cite>WTMF</cite> model and achieve state-of-the-art Pearson correlation on two standard SS datasets. There are latent variable models designed for lexical semantics, such as word senses (Boyd-Graber et al., 2007; Guo and Diab, 2011) , function words (Griffiths et al., 2005) , selectional preference (Ritter et al., 2010) , synonyms and antonyms (Yih et al., 2012) , etc.",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_27",
  "x": "SS has progressed immensely in recent years, especially with the establishment of the Semantic Textual Similarity task in SEMEVAL 2012. Early work in SS focused on word pair similarity in the high dimensional space (Li et al., 2006; Liu et al., 2007; Islam and Inkpen, 2008; Tsatsaronis et al., 2010; Ho et al., 2010) , where co-occurrence information was not efficiently exploited. Researchers (O'Shea et al., 2008) find LSA does not yield good performance. In (<cite>Guo and Diab, 2012b</cite>; Guo and Diab, 2012c) , we show the superiority of the latent space approach in <cite>WTMF</cite>. In this paper, we improve the <cite>WTMF</cite> model and achieve state-of-the-art Pearson correlation on two standard SS datasets. There are latent variable models designed for lexical semantics, such as word senses (Boyd-Graber et al., 2007; Guo and Diab, 2011) , function words (Griffiths et al., 2005) , selectional preference (Ritter et al., 2010) , synonyms and antonyms (Yih et al., 2012) , etc. However little improvement is shown on document/sentence level semantics: (Ritter et al., 2010) and (Yih et al., 2012) focus on selectional preference and antonym identification, respectively; in (Griffiths et al., 2005 ) the LDA performance degrades in the text categorization task including the modeling of function words.",
  "y": "differences extends"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_28",
  "x": "Researchers (O'Shea et al., 2008) find LSA does not yield good performance. In (<cite>Guo and Diab, 2012b</cite>; Guo and Diab, 2012c) , we show the superiority of the latent space approach in <cite>WTMF</cite>. In this paper, we improve the <cite>WTMF</cite> model and achieve state-of-the-art Pearson correlation on two standard SS datasets. There are latent variable models designed for lexical semantics, such as word senses (Boyd-Graber et al., 2007; Guo and Diab, 2011) , function words (Griffiths et al., 2005) , selectional preference (Ritter et al., 2010) , synonyms and antonyms (Yih et al., 2012) , etc. However little improvement is shown on document/sentence level semantics: (Ritter et al., 2010) and (Yih et al., 2012) focus on selectional preference and antonym identification, respectively; in (Griffiths et al., 2005 ) the LDA performance degrades in the text categorization task including the modeling of function words. Rather, we concentrate on nuanced lexical semantics phenomena that could benefit sentential semantics. ----------------------------------",
  "y": "differences extends"
 },
 {
  "id": "053ce92029e643bfade157b3172c05_0",
  "x": "In order to ease the process of engineering such a large grammar, we have made use of the lexical knowledge representation language DATR (Evans & Gazdar, 1996) to compactly encode the elementary trees <cite>(Evans et al., 1995</cite>; Smets & Evans, 1998) . In Section 5 we present some figures that show how the size of the encoding of the grammar has increased during the grammar development process as the number and complexity of elementary trees has grown. We have addressed problems that result from trying to parse with such a large grammar by using a technique proposed by (Evans & Weir, 1997) and (Evans & Weir, 1998) in which all the trees that each word can anchor are compactly represented using a collection of finite state automata. In Section 6 we give some data that shows the extent to which this technique is successful in compacting the grammar. ---------------------------------- **COVERAGE OF THE LEXSYS GRAMMAR** The LEXSYS grammar has roughly the same coverage as the Alvey NL Tools grammar (Grover et al., 1993) , and adopts the same set of subcategorization frames as in the Alvey lexicon.",
  "y": "uses"
 },
 {
  "id": "053ce92029e643bfade157b3172c05_1",
  "x": "Moreover, when a lexical item anchors a tree, features get grounded, and different feature instantiations characterize different trees. For example, get can be followed by one of 12 different prepositions which means that there are at least 12 33 trees for the single subcategorization # trees # sets # merged # minimized ratio merged / in set states (mean) states (mean) minimized 1-10 112 17.9 6.9 2.6 11-20 83 53.9 13. ---------------------------------- **ENCODING FOR GRAMMAR DEVELOPMENT** Following<cite> (Evans et al., 1995)</cite> and (Smets & Evans, 1998 ) the LEXSYS grammar is encoded using DATR, a non-monotonic knowledge representation language. In 1998, the grammar contained 620 trees organized into 44 tree families and produced using 35 rules. This grammar was encoded in 2200 DATR statements, giving an average of 3:55 DATR statements per tree.",
  "y": "similarities uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_0",
  "x": "****DEEPNNNER: APPLYING BLSTM-CNNS AND EXTENDED LEXICONS TO NAMED ENTITY RECOGNITION IN TWEETS.**** **ABSTRACT** In this paper, we describe the DeepNNNER entry to The 2nd Workshop on Noisy User-generated Text (WNUT) Shared Task #2: Named Entity Recognition in Twitter. Our shared task submission adopts the bidirectional LSTM-CNN model of<cite> Chiu and Nichols (2016)</cite>, as it has been shown to perform well on both newswire and Web texts. It uses word embeddings trained on large-scale Web text collections together with text normalization to cope with the diversity in Web texts, and lexicons for target named entity classes constructed from publicly-available sources. Extended evaluation comparing the effectiveness of various word embeddings, text normalization, and lexicon settings shows that our system achieves a maximum F1-score of 47.24, performance surpassing that of the shared task's second-ranked system. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_1",
  "x": "---------------------------------- **INTRODUCTION** Named entity recognition (NER) is an important part of natural language processing. It is a challenging task that requires robust recognition to detect common entities over a large variety of expressions and vocabularies. These problems are intensified when targeting Web texts because of challenges such as differences in spelling and punctuation conventions, neologisms, and Web markup (Baldwin et al., 2015) . Traditional approaches to NER on newswire texts has been dominated by machine learning methods that rely heavily on manual feature engineering and external knowledge sources (Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014) . Recently, neural network models -especially those that use recursive models -have shown that state of the art performance can be achieved with little feature engineering (Collobert et al., 2011; Santos et al., 2015; <cite>Chiu and Nichols, 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_2",
  "x": "In this paper, we present the DeepNNNER entry to the WNUT 2016 Shared Task #2: Named Entity Recognition in Twitter. Our shared task submission is based on the model of<cite> Chiu and Nichols (2016)</cite> , a hybrid model of bidirectional long short-term memory (BLSTM) networks and convolutional neural networks (CNN) that automatically learns both character-and word-level features, and which holds the current state-of-the-art on both newswire texts (CoNLL 2003) and diverse corpora including Web texts (OntoNotes 5.0). In contrast to CRFs, FFNNs, and other windowed models, the BLSTM gives our model effectively infinite context on both sides of a word during sequential labeling. The character-level CNN allows our model to learn relevant features from the orthography of words, which is important in task where unseen words are commonplace. Finally, it also encodes partial lexicon matches in neural networks, allowing it to make effective use of lexical knowledge. Our primary contribution is adapting the model of<cite> Chiu and Nichols (2016)</cite> to Twitter data by developing a text normalization method to effectively apply word embeddings to large vocabulary Web texts and automatically constructing lexicons for the shared task's target NE classes from publicly-available sources. The rest of our paper is organized as follows.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_3",
  "x": "Finally, it also encodes partial lexicon matches in neural networks, allowing it to make effective use of lexical knowledge. Our primary contribution is adapting the model of<cite> Chiu and Nichols (2016)</cite> to Twitter data by developing a text normalization method to effectively apply word embeddings to large vocabulary Web texts and automatically constructing lexicons for the shared task's target NE classes from publicly-available sources. The rest of our paper is organized as follows. In Section 2, we describe the adaptations made to<cite> Chiu and Nichols (2016)</cite> 's model. In Section 3, we describe the evaluation methodology. In Section 4, we discuss the results and present an error analysis. In Section 5, we summarize related research.",
  "y": "extends"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_4",
  "x": "In contrast to CRFs, FFNNs, and other windowed models, the BLSTM gives our model effectively infinite context on both sides of a word during sequential labeling. The character-level CNN allows our model to learn relevant features from the orthography of words, which is important in task where unseen words are commonplace. Finally, it also encodes partial lexicon matches in neural networks, allowing it to make effective use of lexical knowledge. Our primary contribution is adapting the model of<cite> Chiu and Nichols (2016)</cite> to Twitter data by developing a text normalization method to effectively apply word embeddings to large vocabulary Web texts and automatically constructing lexicons for the shared task's target NE classes from publicly-available sources. The rest of our paper is organized as follows. In Section 2, we describe the adaptations made to<cite> Chiu and Nichols (2016)</cite> 's model. In Section 3, we describe the evaluation methodology.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_5",
  "x": "**MODEL** In this section, we describe the architecture of our shared task submission. An overview is given Figure 1 . Our system is based on the BLSTM-CNN model of<cite> Chiu and Nichols (2016)</cite> , and, unless otherwise noted, follows their training and tagging methodology, which the reader is referred to for more details. ---------------------------------- **FEATURES** Feature embeddings for words are constructed by concatenating together the features listed here.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_6",
  "x": "**WORD EMBEDDINGS** Word embeddings are critical for high-performance neural networks in NLP tasks (Turian et al., 2010) . In this paper, we compare six publicly available pre-trained word embeddings. The embeddings are described in detail in Table 3 . The neural embeddings of Collobert et al. (2011) were chosen because<cite> Chiu and Nichols (2016)</cite> reported them to be the highest performing on both CoNLL-2003 and OntoNotes 5.0 datasets. To evaluate embeddings trained on data closer to the WNUT dataset, we also selected the GloVe embeddings of Pennington et al. (2014) , trained on both Web text and tweets, and word2vec embeddings trained on Google News data (Mikolov et al., 2013 ) and on tweets (Godin et al., 2015) . Preliminary evaluation on the Dev1 data showed that GloVe 27B outperformed Collobert's embeddings (see Table 5 ) and word2vec 3B, so they were used in our submission.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_7",
  "x": "**CNN-EXTRACTED CHARACTER FEATURES** Following<cite> Chiu and Nichols (2016)</cite> , we use a CNN to extract features from 25 dim. character embeddings randomly-initialized from a uniform distribution between -0.5 and 0.5. To accommodate text normalization, we added embeddings for the normalization symbols described in Section 2.2, namely <url>, <user>, <smile>, <lolface>, <sadface>, <neutralface>, <heart>, <number> and <hashtag>. All experiments were conducted with the same character embeddings. ---------------------------------- **LEXICON FEATURES** Prior knowledge in the form of lexicons (also known as \"gazetteers\") has been shown to be essential to NER (Ratinov and Roth, 2009; Passos et al., 2014) .",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_8",
  "x": "Finally, in order to maximize coverage, the Product lexicon is a combination of the subtype Device from the DBpedia ontology and the lexicon product distributed with WNUT dataset. Every other category is as described in Table 1 . To generate lexicon features, we apply the partial matching algorithm of<cite> Chiu and Nichols (2016)</cite> to the input text, as shown in Figure 2 . Each lexicon and match type (BIOES) is associated with a randomly-initialized 5 dim. embedding. The embeddings for all lexicons are concatenated together to produce the lexicon feature for each word in the input. To facilitate matching, all entries were stripped of parentheses and tokenized with the Penn Treebank tokenization script.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_9",
  "x": "The embeddings for all lexicons are concatenated together to produce the lexicon feature for each word in the input. To facilitate matching, all entries were stripped of parentheses and tokenized with the Penn Treebank tokenization script. ---------------------------------- **CAPITALIZATION FEATURE** Following<cite> Chiu and Nichols (2016)</cite>, we used different symbols for word-level capitalization feature each assigned a randomly initialized embedding: allCaps, upperInitial, lowercase, mixedCaps and noinfo. Similar symbols were used for character-level (upper case, lower case, punctuation, other). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_10",
  "x": "In this case, we prioritize entity over non-entity tags, and keep the most frequent tag. Prioritizing entity over non-entity tags was meant to improve recall, albeit at the expense of precision. Initial experiments on Dev1 comparing word2vec 3B, Collobert, and GloVe 27B embeddings showed that text normalization improved performance for word2vec 3B and GloVe 27B but not Collobert 3 (Table  5) ; that word type coverage increased drastically for all embeddings; and that while word token coverage greatly increased for GloVe 27B, it slightly decreased for other embeddings (see Table 4 ). We thus selected GloVe 27B embeddings for our submission due to their superior performance and coverage. ---------------------------------- **TRAINING AND INFERENCE** We follow the training and inference methodology of<cite> Chiu and Nichols (2016)</cite> , training our neural network to maximize the sentence-level log-likelihood from Collobert et al. (2011) .",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_11",
  "x": "---------------------------------- **RELATED RESEARCH** Named entity recognition is a task with a long history, dating back to MUC-7 (Chinchor and Robinson, 1997) . In this section, we describe the NER research that influenced our system and give an overview of the work on NER for Twitter. For a more detailed survey, see<cite> (Chiu and Nichols, 2016)</cite> . Most recent approaches to NER have been characterized by the use of CRF, SVM, and perceptron models, where performance is heavily dependent on feature engineering. Ratinov and Roth (2009) used non-local features, a gazetteer extracted from Wikipedia, and Brown-cluster-like word representations.",
  "y": "background"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_12",
  "x": "The WNUT 2015 Shared Task included text normalization and named entity tokenization and detection tasks (Baldwin et al., 2015) , with most systems using machine learning methods like CRF together with a variety of features including lexicons, orthographic features, and distributional information. In contrast with conventional NER, there was only one neural network entry (Godin et al., 2015) , and most systems tended to prefer Brown clusters to word embeddings. The state of the art at WNUT 2015 used a cascaded model of entity tokenization, followed by linking to knowledge bases, and, finally, classification with random forests (Yamada et al., 2015) . Our system adopts the architecture of<cite> Chiu and Nichols (2016)</cite> , which combined BLSTMs to maximize context over the tagged word sequence and word-level CNNs to automatically generate characterlevel features with a partial-matching lexicon to achieve the state-of-the-art for NER on both CoNLL 2003 and OntoNotes datasets. Our system can be viewed as an investigation into how well state-of-theart neural approaches adapt to the challenges of NER on noisy Web data. ---------------------------------- **CONCLUSION**",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_13",
  "x": "The state of the art at WNUT 2015 used a cascaded model of entity tokenization, followed by linking to knowledge bases, and, finally, classification with random forests (Yamada et al., 2015) . Our system adopts the architecture of<cite> Chiu and Nichols (2016)</cite> , which combined BLSTMs to maximize context over the tagged word sequence and word-level CNNs to automatically generate characterlevel features with a partial-matching lexicon to achieve the state-of-the-art for NER on both CoNLL 2003 and OntoNotes datasets. Our system can be viewed as an investigation into how well state-of-theart neural approaches adapt to the challenges of NER on noisy Web data. ---------------------------------- **CONCLUSION** In this paper, we described the DeepNNNER entry to the WNUT 2016 Shared Task #2: Named Entity Recognition in Twitter, which adopted the BLSTM-CNN model of<cite> Chiu and Nichols (2016)</cite> . Extensive evaluation showed that high word type coverage for word embeddings is crucial to NER performance, likely due to rare words in entities, and that both text normalization and partial matching on lexicons constructed from DBpedia (Auer et al., 2007) contribute significantly to performance.",
  "y": "uses"
 },
 {
  "id": "062e9348de5fda68e61fff3ca4f186_0",
  "x": "If w(s i , e j ) = 1, then this edge indicates that there is a mention of e j in sentence s i . In order to realize the insight from Grosz et al. (1995) that certain syntactic roles are more important than others, the syntactic role of e j in s i can be mapped to an integer value (Guinaudeau and Strube, 2013): w(s i , e j ) = 3 if ej is subject in si 2 if ej is object in si 1 otherwise Figure 1 illustrates a weighted entity graph for three sentences. Three types of one-mode projections capture relations between sentences, P U , P W and P Acc . P U creates an edge between two sentences if they share at least one entity. P W captures the intuition that the connection between two sentences is stronger the more entities they share by means of weighted edges, where the weights equal the number of entities shared by sentences (Newman, 2004) . The third type of projection, P Acc , integrates syntactic information in the edge weights calculated by the following formula: While the entity grid <cite>(Barzilay and Lapata, 2008)</cite> uses information about sentences which do not share entities by means of the \"--\" transition, the entity graph cannot employ this negative information. Here, we propose a normalization for the entity graph and its corresponding one-mode projections which is based on the relative importance of entities and, in turn, the relative importance of sentences.",
  "y": "motivation background"
 },
 {
  "id": "062e9348de5fda68e61fff3ca4f186_1",
  "x": "Results for Guinaudeau and Strube (2013) , G&S, are reproduced, results for<cite> Barzilay and Lapata (2008)</cite> , B&L, and Elsner and Charniak (2011) , E&C, were reproduced by Guinaudeau and Strube (2013) . The unweighted graph, P U , does not need normalization. Hence the results for the entity graph and the normalized entity graph are identical. Normalization improves the results for the weighted graphs P W and P Acc with P Acc outperforming B&L considerably and closely approaching E&L. Sentence insertion is more difficult than discrimination. Following Elsner and Charniak (2011), we use two measures for evaluation: Accuracy (Acc.) and the average proportion of correct insertions per document (Ins.). Table 3 : Summary Coherence Rating, B&L and entity graph vs. normalized entity graph Table 2 shows that the normalized entity graph outperforms the entity graph for P W and P Acc (again, no difference for P U ).",
  "y": "background"
 },
 {
  "id": "062e9348de5fda68e61fff3ca4f186_2",
  "x": "**SUMMARY COHERENCE RATING** We follow<cite> Barzilay and Lapata (2008)</cite> for evaluating whether the normalized entity graph can decide whether automatic or human summaries are more coherent (80 pairs of summaries extracted from DUC 2003). Human coherence scores are associated with each pair of summarized documents <cite>(Barzilay and Lapata, 2008)</cite> . Table 3 displays reported results of B&L and reproduced results of the entity graph and our normalized entity graph. Normalizing significantly improves the results for P W and P Acc . P U is still slightly better than both, but in contrast to the entity graph, this difference is not statistically significant. We believe that better weighting schemes based on linguistic insights eventually will outperform P U and B&L (left for future work).",
  "y": "uses"
 },
 {
  "id": "062e9348de5fda68e61fff3ca4f186_3",
  "x": "Human coherence scores are associated with each pair of summarized documents <cite>(Barzilay and Lapata, 2008)</cite> . Table 3 displays reported results of B&L and reproduced results of the entity graph and our normalized entity graph. Normalizing significantly improves the results for P W and P Acc . P U is still slightly better than both, but in contrast to the entity graph, this difference is not statistically significant. We believe that better weighting schemes based on linguistic insights eventually will outperform P U and B&L (left for future work). Distance information always degrades the results for this task (see Guinaudeau and Strube (2013) ). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "062e9348de5fda68e61fff3ca4f186_4",
  "x": "In experiments,<cite> Barzilay and Lapata (2008)</cite> assume that articles taken from Encyclopedia Britannica are more difficult to read (less coherent) than the corresponding articles from Encyclopedia Britannica Elementary, its version for children. We follow them with regard to data (107 article pairs), experimental setup and evaluation. Sentences in the Britannica Elementary are simpler and shorter than in the Encyclopedia Britannica. The entity graph does not take into account the effect of entities not shared between sentences while the normalized entity graph assigns a lower weight if there are more of these entities. Hence, Britannica Elementary receives a higher cohesion score than Encyclopedia Britannica in our model. Adding grammatical information, does not help, because of the influence of the number of entities (shared and not shared) outweighs the influence of syntactic roles. The normalized entity graph (P W , Dist) does not only outperform the entity graph (significantly) and B&L but also S&O and the combination B&L + S&O.",
  "y": "background uses motivation"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_0",
  "x": "The paper evaluates the two approaches to argument span extraction on Penn Discourse Treebank explicit relations; and the problem is cast as token-level sequence labeling. We show that processing intra-and inter-sentential relations separately, reduces the task complexity and significantly outperforms the single model approach. ---------------------------------- **INTRODUCTION** Discourse analysis is one of the most challenging tasks in Natural Language Processing, that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc. (see (Webber et al., 2011) and (Taboada and Mann, 2006) for detailed review). With the availability of annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008) , statistical discourse parsers were developed (Lin et al., 2012;<cite> Ghosh et al., 2011</cite>; Xu et al., 2012) .",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_1",
  "x": "Finally, relation sense classification is the annotation of relations with the senses from PDTB. Since arguments of explicit discourse relations can appear in the same sentence or in different ones (i.e. relations can be intra-or inter-sentential); there are two approaches to argument span extraction. In the first approach the parser decision is not conditioned on whether the relation is intra-or inter-sentential (e.g.<cite> (Ghosh et al., 2011)</cite> ). In the second approach relations are parsed separately for each class (e.g. (Lin et al., 2012; Xu et al., 2012) ). In the former approach argument span extraction is applied right after discourse connective detection, while the latter approach also requires argument position classification. The decision on argument span can be made on different levels: from token-level to sentence-level. In<cite> (Ghosh et al., 2011</cite> ) the decision is made on tokenlevel, and the problem is cast as sequence labeling using conditional random fields (CRFs) (Lafferty et al., 2001) .",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_2",
  "x": "In<cite> (Ghosh et al., 2011</cite> ) the decision is made on tokenlevel, and the problem is cast as sequence labeling using conditional random fields (CRFs) (Lafferty et al., 2001) . In this paper we focus on argument span extraction, and extend the token-level sequence labeling approach of<cite> (Ghosh et al., 2011)</cite> with the separate models for arguments of intra-sentential and intersentential explicit discourse relations. To compare to the other approaches (i.e. (Lin et al., 2012) and (Xu et al., 2012) ) we adopt the immediately previous sentence heuristic to select a candidate Arg1 sentence for the inter-sentential relations. Additionally to the heuristic, we train and test CRF argument span extraction models to extract exact argument spans. The paper is structured as follows. In Section 2 we briefly present the corpus that was used in the experiments -Penn Discourse Treebank. Section 3 describes related works.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_3",
  "x": "In the first approach the parser decision is not conditioned on whether the relation is intra-or inter-sentential (e.g.<cite> (Ghosh et al., 2011)</cite> ). In the second approach relations are parsed separately for each class (e.g. (Lin et al., 2012; Xu et al., 2012) ). In the former approach argument span extraction is applied right after discourse connective detection, while the latter approach also requires argument position classification. The decision on argument span can be made on different levels: from token-level to sentence-level. In<cite> (Ghosh et al., 2011</cite> ) the decision is made on tokenlevel, and the problem is cast as sequence labeling using conditional random fields (CRFs) (Lafferty et al., 2001) . In this paper we focus on argument span extraction, and extend the token-level sequence labeling approach of<cite> (Ghosh et al., 2011)</cite> with the separate models for arguments of intra-sentential and intersentential explicit discourse relations. To compare to the other approaches (i.e. (Lin et al., 2012) and (Xu et al., 2012) ) we adopt the immediately previous sentence heuristic to select a candidate Arg1 sentence for the inter-sentential relations.",
  "y": "extends"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_4",
  "x": "Ghosh et al. (2011 ) (and further (Ghosh et al., 2012a Ghosh et al., 2012b) ), on the other hand, cast the problem as tokenlevel sequence labeling. In this paper we follows the approach of<cite> (Ghosh et al., 2011</cite> (Prasad et al., 2008) ); and distribution of Arg2 with respect to extent in inter-sentential explicit discourse relations. SS = same sentence as the connective; IPS = immediately previous sentence; NAPS = non-adjacent previous sentence; FS = some sentence following the sentence containing the connective; SingFull = Single Full sentence; SingPart = Part of single sentence; MultFull = Multiple full sentences; MultPart = Parts of multiple sentences. ---------------------------------- **IMMEDIATELY PREVIOUS SENTENCE HEURISTIC** According to Prasad et al. (2008) 's analysis of explicit discourse relations annotated in PDTB, out of 18,459 relations, 11,236 (60.9%) have both of the arguments in the same sentence (SS case), 7,215 (39.1%) have Arg1 in the sentences preceding the Arg2 (PS case), and only 8 instances have Arg1 in the sentences following Arg2 (FS case). Since FS case has too few instances it is usually ignored.",
  "y": "uses"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_5",
  "x": "Since FS case has too few instances it is usually ignored. For the PS case, the Arg1 is located either in Immediately Previous Sentences (IPS: 30.1%) or in some Non-Adjacent Previous Sentences (NAPS: 9.0%). CRF-based discourse parser of<cite> Ghosh et al. (2011)</cite> , which processes SS and PS cases with the same model, uses \u00b12 sentence window as a hypothesis space (5 sentences: 1 sentence containing the connective, 2 preceding and 2 following sentences). The window size is motivated by the observation that it entirely covers arguments of 94% of all explicit relations. The authors also report that the performance of the parser on inter-sentential relations (i.e. mainly PS case) has F-measure of 36.0. However, since in 44.2% of inter-sentential explicit discourse relations Arg1 fully covers the sentence immediately preceding Arg2 (see Table 1 partially copied from (Prasad et al., 2008) ), the heuristic that selects the immediately previous sentence and tags all of its tokens as Arg1 already yields F-measure of 44.2 over all PDTB (the performance on the test set may vary). The same heuristic is mentioned in (Lin et al., 2012 ) and (Xu et al., 2012) as a majority classifier for the relations with Arg1 in previous sentences.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_6",
  "x": "Even though low, there is still an ambiguity: e.g. for sentence-medial adverbials also, therefore, still, instead, in fact, etc. Arg1 appears in SS and PS cases evenly. Consequently, assigning the position of the Arg1 considering the discourse connective, together with its syntactic category and its position in the sentence, for PDTB will be correct in more than 95% of instances. In the literature, the task of argument position classification was addressed by several researchers (e.g. (Prasad et al., 2010) , (Lin et al., 2012) ). Lin et al. (2012) , for instance, report F 1 of 97.94% for a classifier trained on PDTB sections 02-21, and tested on section 23. The task has a very high baseline and even higher performance on supervised machine learning, Table 3 : Feature sets for Arg2 and Arg1 argument span extraction in<cite> (Ghosh et al., 2011)</cite> which is an additional motivation to process intra-and inter-sentential relations separately. ----------------------------------",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_7",
  "x": "The task has a very high baseline and even higher performance on supervised machine learning, Table 3 : Feature sets for Arg2 and Arg1 argument span extraction in<cite> (Ghosh et al., 2011)</cite> which is an additional motivation to process intra-and inter-sentential relations separately. ---------------------------------- **PARSING MODELS** We replicate and evaluate the discourse parser of<cite> (Ghosh et al., 2011)</cite> , then modify it to process intraand inter-sentential explicit relations separately. This is achieved by integrating Argument Position Classification and Immediately Previous Sentence heuristic into the parsing pipe-line. Since the features used to train argument span extraction models for both approaches are the same, we first describe them in Subsection 5.1. Then we proceed with the description of the single model discourse parser (our baseline) and separate models discourse parser, Subsections 5.2 and 5.3, respectively.",
  "y": "extends"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_8",
  "x": "For instance in the sentence Prices collapsed when the news flashed, the main verb is collapsed; thus, its BMV feature is '1', whereas for the rest of tokens it is '0'. Previous Sentence Feature (PREV) signals if a sentence immediately precedes the sentence starting with a connective, and its value is the first token of the connective<cite> (Ghosh et al., 2011)</cite> . For instance, if some sentence A is followed by a sentence B starting with discourse connective On the other hand, all the tokens of the sentence A have the PREV feature value 'On'. The feature is similar to a heuristic to select the sentence immediately preceding a sentence starting with a connective as a candidate for Arg1. Arg2 Label (ARG2) is an output of Arg2 span extraction model, and it is used as a feature for Arg1 span extraction. Since for sequence labeling we use IOBE (Inside, Out, Begin, End) notation, the possible values of ARG2 are IOBE-tagged labels, i.e. 'ARG2-B' -if a word is the first word of Arg2, 'ARG2-I' -if a word is inside the argument span, 'ARG2-E' -if a word is in the last word of Arg2, and 'O' otherwise. CRF++ 2 -conditional random field implementation we use -allows definition of feature templates.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_9",
  "x": "The feature is similar to a heuristic to select the sentence immediately preceding a sentence starting with a connective as a candidate for Arg1. Arg2 Label (ARG2) is an output of Arg2 span extraction model, and it is used as a feature for Arg1 span extraction. Since for sequence labeling we use IOBE (Inside, Out, Begin, End) notation, the possible values of ARG2 are IOBE-tagged labels, i.e. 'ARG2-B' -if a word is the first word of Arg2, 'ARG2-I' -if a word is inside the argument span, 'ARG2-E' -if a word is in the last word of Arg2, and 'O' otherwise. CRF++ 2 -conditional random field implementation we use -allows definition of feature templates. Via templates these features are enriched with ngrams: tokens with 2-grams in the window of \u00b11 to- Figure 1: Single model discourse parser architecture of<cite> (Ghosh et al., 2011)</cite> . CRF argument span extraction models are in bold. kens, and the rest of the features with 2 & 3-grams in the window of \u00b12 tokens.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_10",
  "x": "**SINGLE MODEL DISCOURSE PARSER** The discourse parser of<cite> (Ghosh et al., 2011</cite> ) is a cascade of CRF models to sequentially label Arg2 and Arg1 spans (since Arg2 label is a feature for Arg1 model) (see Figure 1 ). There is no distinction between intra-and inter-sentential relations, rather the single model jointly decides on the position and the span of an argument (either Arg1 or Arg2, not both together) in the window of \u00b12 sentences (the parser will be further abbreviated as W5P -Window 5 Parser). The single model parser achieves F-measure of 81.7 for Arg2 and 60.3 for Arg1 using CONNL evaluation script. The performance is higher than<cite> (Ghosh et al., 2011</cite> ) -Arg2: F 1 of 79.1 and Arg1: F 1 of 57.3 -due to improvements in feature and instance extraction, such as the treatment of multi-word connectives. These models are the baseline for comparison with separate models architecture. However, we change the evaluation method (see Section 6).",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_11",
  "x": "**SINGLE MODEL DISCOURSE PARSER** The discourse parser of<cite> (Ghosh et al., 2011</cite> ) is a cascade of CRF models to sequentially label Arg2 and Arg1 spans (since Arg2 label is a feature for Arg1 model) (see Figure 1 ). There is no distinction between intra-and inter-sentential relations, rather the single model jointly decides on the position and the span of an argument (either Arg1 or Arg2, not both together) in the window of \u00b12 sentences (the parser will be further abbreviated as W5P -Window 5 Parser). The single model parser achieves F-measure of 81.7 for Arg2 and 60.3 for Arg1 using CONNL evaluation script. The performance is higher than<cite> (Ghosh et al., 2011</cite> ) -Arg2: F 1 of 79.1 and Arg1: F 1 of 57.3 -due to improvements in feature and instance extraction, such as the treatment of multi-word connectives. These models are the baseline for comparison with separate models architecture. However, we change the evaluation method (see Section 6).",
  "y": "differences"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_12",
  "x": "Finally, we compare the results of the separate model parser to (Lin et al., 2012) and (Xu et al., 2012) . ---------------------------------- **EVALUATION** There are two important aspects regarding the evaluation. First, in this paper it is different from<cite> (Ghosh et al., 2011)</cite> ; thus, we first describe it and evaluate the difference. Second, in order to compare the baseline single and separate model parsers, the error from argument position classification has to be propagated for the latter one; and the process is described in 6.1.2. Since both versions of the parser are affected by automatic features, the evaluation is on gold features only.",
  "y": "differences"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_13",
  "x": "Since both versions of the parser are affected by automatic features, the evaluation is on gold features only. The exception is for Arg2 label; since it is generated within the segment of the pipeline we are in-terested in. Unless stated otherwise, all the results for Arg1 are reported for automatic Arg2 labels as a feature. Following<cite> (Ghosh et al., 2011)</cite> PDTB is split as Sections 02-22 for training, 00-01 for development, and 23-24 for testing. ---------------------------------- **CONLL VS. STRING-BASED EVALUATION** <cite>Ghosh et al. (2011)</cite> report using CONLL-based evaluation script.",
  "y": "uses"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_14",
  "x": "**CONLL VS. STRING-BASED EVALUATION** <cite>Ghosh et al. (2011)</cite> report using CONLL-based evaluation script. However, it is not well suited for the evaluation of argument spans because the unit of evaluation is a chunk -a segment delimited by any outof-chunk token or a sentence boundary. However, in PDTB arguments can (1) span over several sentences, (2) be non-contiguous in the same sentence. Thus, CONLL-based evaluation yields incorrect number of test instances:<cite> Ghosh et al. (2011)</cite> report 1,028 SS and 617 PS test instances for PDTB sections 23-24 (see caption of Table 7 in the original paper), which is 1,645 in total; whereas there is only 1,595 explicit relations in these sections. In this paper, the evaluation is string-based; i.e. an argument span is correct, if it matches the whole reference string. Following<cite> (Ghosh et al., 2011)</cite> and (Lin et al., 2012) , argument initial and final punctuation marks are removed; and precision (p), recall (r) and F 1 score are computed using the equations 1 -3.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_15",
  "x": "Following<cite> (Ghosh et al., 2011)</cite> PDTB is split as Sections 02-22 for training, 00-01 for development, and 23-24 for testing. ---------------------------------- **CONLL VS. STRING-BASED EVALUATION** <cite>Ghosh et al. (2011)</cite> report using CONLL-based evaluation script. However, it is not well suited for the evaluation of argument spans because the unit of evaluation is a chunk -a segment delimited by any outof-chunk token or a sentence boundary. However, in PDTB arguments can (1) span over several sentences, (2) be non-contiguous in the same sentence. Thus, CONLL-based evaluation yields incorrect number of test instances:<cite> Ghosh et al. (2011)</cite> report 1,028 SS and 617 PS test instances for PDTB sections 23-24 (see caption of Table 7 in the original paper), which is 1,645 in total; whereas there is only 1,595 explicit relations in these sections.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_16",
  "x": "However, in PDTB arguments can (1) span over several sentences, (2) be non-contiguous in the same sentence. Thus, CONLL-based evaluation yields incorrect number of test instances:<cite> Ghosh et al. (2011)</cite> report 1,028 SS and 617 PS test instances for PDTB sections 23-24 (see caption of Table 7 in the original paper), which is 1,645 in total; whereas there is only 1,595 explicit relations in these sections. In this paper, the evaluation is string-based; i.e. an argument span is correct, if it matches the whole reference string. Following<cite> (Ghosh et al., 2011)</cite> and (Lin et al., 2012) , argument initial and final punctuation marks are removed; and precision (p), recall (r) and F 1 score are computed using the equations 1 -3. In the equations, Exact Match is the count of correctly tagged argument spans; No Match is the count of argument spans that do not match the reference string exactly (even one token difference is counted as an error); and References in Gold is the total number of arguments in the reference. String-based evaluation of the single model discourse parser with gold features reduces F 1 for Arg2 from 81.7 to 77.8 and for Arg1 from 60.33 to 55.33. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_17",
  "x": "Since all systems have different components up the pipe-line, the only possible comparison is without error propagation. From the results in Table 8 , we can observe that all the systems perform well on Arg2. As expected, for the harder case of Arg1, performances are lower. ---------------------------------- **CONCLUSION** In this paper we compare two strategies for the argument span extraction: to process intra-and intersentential explicit relations by a single model, or separate ones. We extend the approach of<cite> (Ghosh et al., 2011)</cite> to argument span extraction cast as token-level sequence labeling using CRFs and integrate argument position classification and immediately previous sentence heuristic.",
  "y": "extends"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_0",
  "x": "Sequence-to-sequence (Seq2seq) models have successfully improved many well-studied NLP tasks, especially for natural language generation (NLG) tasks, such as machine translation (MT) (Sutskever et al., 2014; Cho et al., 2014) and abstractive summarization (Rush et al., 2015) . Seq2seq models have also been applied to constituency parsing <cite>(Vinyals et al., 2015)</cite> and provided a fairly good result. However one obvious, intuitive drawback of Seq2seq models when they are applied to constituency parsing is that they have no explicit architecture to model latent nested relationships among the words and phrases in constituency parse trees, Thus, models that directly model them, such as RNNG (Dyer et al., 2016) , are an intuitively more promising approach. In fact, RNNG and its extensions (Kuncoro et al., 2017; Fried et al., 2017) provide the current stateof-the-art performance. Sec2seq models are currently considered a simple baseline of neuralbased constituency parsing. After the first proposal of an Seq2seq constituency parser, many task-independent techniques have been developed, mainly in the NLG research area. Our aim is to update the Seq2seq approach proposed in<cite> Vinyals et al. (2015)</cite> as a stronger baseline of constituency parsing.",
  "y": "background"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_1",
  "x": "However one obvious, intuitive drawback of Seq2seq models when they are applied to constituency parsing is that they have no explicit architecture to model latent nested relationships among the words and phrases in constituency parse trees, Thus, models that directly model them, such as RNNG (Dyer et al., 2016) , are an intuitively more promising approach. In fact, RNNG and its extensions (Kuncoro et al., 2017; Fried et al., 2017) provide the current stateof-the-art performance. Sec2seq models are currently considered a simple baseline of neuralbased constituency parsing. After the first proposal of an Seq2seq constituency parser, many task-independent techniques have been developed, mainly in the NLG research area. Our aim is to update the Seq2seq approach proposed in<cite> Vinyals et al. (2015)</cite> as a stronger baseline of constituency parsing. Our motivation is basically identical to that described in Denkowski and Neubig (2017) . A strong baseline is crucial for reporting reliable experimental results.",
  "y": "uses"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_2",
  "x": "The contributions of this paper are summarized as follows: (1) a strong baseline for constituency parsing based on general purpose Seq2seq models 1 , (2) an empirical investigation of several generic techniques that can (or cannot) contribute to improve the parser performance, (3) empirical evidence that Seq2seq models implicitly learn parse tree structures well without knowing taskspecific and explicit tree structure information. ---------------------------------- **CONSTITUENCY PARSING BY SEQ2SEQ** Our starting point is an RNN-based Seq2seq model with an attention mechanism that was applied to constituency parsing <cite>(Vinyals et al., 2015)</cite> . We omit detailed descriptions due to space limitations, but note that our model architecture is identical to the one introduced in Luong et al. (2015a) 2 . A key trick for applying Seq2seq models to constituency parsing is the linearization of parse 1 Our code and experimental configurations for reproducing our experiments are publicly available: https://github.com/nttcslab-nlp/strong s2s baseline parser 2 More specifically, our Seq2seq model follows the one implemented in seq2seq-attn (https://github.com/harvardnlp/seq2seq-attn), which is the alpha-version of the OpenNMT tool (http://opennmt.net). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_4",
  "x": "3.4 Output length controlling As described in<cite> Vinyals et al. (2015)</cite> , not all the outputs (predicted linearized parse trees) obtained from the Seq2seq parser are valid (well-formed) as a parse tree. Toward guaranteeing that every output is a valid tree, we introduce a simple extension of the method for controlling the Seq2seq output length (Kikuchi et al., 2016) . First, we introduce an additional bias term b in the decoder output layer to prevent the selection of certain output words: If we set a large negative value at the m-th element in b, namely b m \u2248 \u2212\u221e, then the m-th element in p j becomes approximately 0, namely p j,m \u2248 0, regardless of the value of the k-th element in o j . We refer to this operation to set value \u2212\u221e in b as a mask. Since this naive masking approach is harmless to GPU-friendly processing, we can still exploit GPU parallelization.",
  "y": "motivation background"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_5",
  "x": "From this general knowledge, we re-consider jointly estimating POS-tags by incorporating the linearized forms without the POS-tag normalization as an auxiliary task. In detail, the linearized forms with and without the POS-tag normalization are independently and simultaneously estimated as o j and q j , respectively, in the decoder output layer by following equation: 3.4 Output length controlling As described in<cite> Vinyals et al. (2015)</cite> , not all the outputs (predicted linearized parse trees) obtained from the Seq2seq parser are valid (well-formed) as a parse tree. Toward guaranteeing that every output is a valid tree, we introduce a simple extension of the method for controlling the Seq2seq output length (Kikuchi et al., 2016) . First, we introduce an additional bias term b in the decoder output layer to prevent the selection of certain output words: If we set a large negative value at the m-th element in b, namely b m \u2248 \u2212\u221e, then the m-th element in p j becomes approximately 0, namely p j,m \u2248 0, regardless of the value of the k-th element in o j .",
  "y": "motivation"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_6",
  "x": "The pre-trained word embeddings obtained from a large external corpora often boost the final task performance even if they only initialize the input embedding layer. In constituency parsing, several systems also incorporate pre-trained word embeddings, such as<cite> Vinyals et al. (2015)</cite> ; Durrett and Klein (2015) . To maintain as much reproducibility of our experiments as possible, we simply applied publicly available pre-trained word embeddings, i.e., glove.840B.300d 7 , as initial values of the encoder embedding layer. ---------------------------------- **MODEL ENSEMBLE** Ensembling several independently trained models together significantly improves many NLP tasks. In the ensembling process, we predict the output tokens using the arithmetic mean of predicted probabilities computed by each model:",
  "y": "background"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_7",
  "x": "If both conditions (1) and (2) are satisfied, then the decoding process is finished. The additional cost for controlling the mask is to count the number of XX-tags and the open and closed brackets so far generated in the decoding process. The pre-trained word embeddings obtained from a large external corpora often boost the final task performance even if they only initialize the input embedding layer. In constituency parsing, several systems also incorporate pre-trained word embeddings, such as<cite> Vinyals et al. (2015)</cite> ; Durrett and Klein (2015) . To maintain as much reproducibility of our experiments as possible, we simply applied publicly available pre-trained word embeddings, i.e., glove.840B.300d 7 , as initial values of the encoder embedding layer. ---------------------------------- **MODEL ENSEMBLE**",
  "y": "motivation"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_8",
  "x": "---------------------------------- **EXPERIMENTS** Our experiments used the English Penn Treebank data (Marcus et al., 1994) , which are the most widely used benchmark data in the literature. We used the standard split of training (Sec.02-21), development (Sec.22), and test data (Sec.23) and strictly followed the instructions for the evaluation settings explained in<cite> Vinyals et al. (2015)</cite> . For data pre-processing, all the parse trees were transformed into linearized forms, which include standard UNK replacement for OOV words and POS-tag normalization by XX-tags. As explained in<cite> Vinyals et al. (2015)</cite> , we did not apply any parse tree binarization or special unary treatment, which were used as common techniques in the literature. Table 7 : List of bracketing F-measures on test data (PTB Sec.23) reported in recent top-notch systems: scores with bold font represent our scores.",
  "y": "uses"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_9",
  "x": "For data pre-processing, all the parse trees were transformed into linearized forms, which include standard UNK replacement for OOV words and POS-tag normalization by XX-tags. As explained in<cite> Vinyals et al. (2015)</cite> , we did not apply any parse tree binarization or special unary treatment, which were used as common techniques in the literature. Table 7 : List of bracketing F-measures on test data (PTB Sec.23) reported in recent top-notch systems: scores with bold font represent our scores. ments unless otherwise specified. The baseline Seq2seq models, (a) and (f), produced the malformed parse trees. We postprocessed such malformed parse trees by simple rules introduced in <cite>(Vinyals et al., 2015)</cite> . On the other hand, we confirmed that all the results applying the technique explained in Sec. 3.4 produced no malformed parse trees.",
  "y": "uses"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_10",
  "x": "We used the standard split of training (Sec.02-21), development (Sec.22), and test data (Sec.23) and strictly followed the instructions for the evaluation settings explained in<cite> Vinyals et al. (2015)</cite> . For data pre-processing, all the parse trees were transformed into linearized forms, which include standard UNK replacement for OOV words and POS-tag normalization by XX-tags. As explained in<cite> Vinyals et al. (2015)</cite> , we did not apply any parse tree binarization or special unary treatment, which were used as common techniques in the literature. Table 7 : List of bracketing F-measures on test data (PTB Sec.23) reported in recent top-notch systems: scores with bold font represent our scores. ments unless otherwise specified. The baseline Seq2seq models, (a) and (f), produced the malformed parse trees. We postprocessed such malformed parse trees by simple rules introduced in <cite>(Vinyals et al., 2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_0",
  "x": "In <cite>(S\u00e1nchez and Bened\u00ed, 2006b</cite> ), SITGs were used for obtaining word phrases, reporting preliminary results on the EuroParl corpus. In this work, we extend that work by using bracketed corpora for estimating the STIGs. In section 2, we will briefly review the phrase-based SMT approach. Next, in section 3, we will sum up the grounds of SITGs and the modifications proposed in (S\u00e1nchez and Bened\u00ed, 2006a) . In section 4, we present the translation results on the Europarl corpus, obtained when applying one learning iteration on SITGs with several number of nonterminals. ---------------------------------- **PHRASE-BASED MODELS**",
  "y": "background"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_1",
  "x": "In this work, we will be following this last approach, which relies on Stochastic Inverse Transduction Grammars (SITGs) (Wu, 1997) for phrase extraction. SITGs constitute a restricted subset of syntax directed stochastic grammars for translation, and are very related to context-free grammars. These can be used to analyse two strings simultaneously, which makes them specially useful for extracting bilingual segments from a parallel corpus in a syntax-oriented manner. In <cite>(S\u00e1nchez and Bened\u00ed, 2006b</cite> ), SITGs were used for obtaining word phrases, reporting preliminary results on the EuroParl corpus. In this work, we extend that work by using bracketed corpora for estimating the STIGs. In section 2, we will briefly review the phrase-based SMT approach. Next, in section 3, we will sum up the grounds of SITGs and the modifications proposed in (S\u00e1nchez and Bened\u00ed, 2006a) .",
  "y": "extends"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_2",
  "x": "However, if the corpus has been previously parsed with a syntactical parser and is given in a bracketed form, (S\u00e1nchez and Bened\u00ed, 2006a) suggest the use of a version of the algorithm by (Wu, 1997) which is more efficient while performing the analysis, achieving a time complexity of O(|x||y||R|) when x and y are fully bracketed. In this work, we will be taking profit of bracketing information provided by freely available parsing toolkits in order to achieve an important increase of speed within the estimation algorithm. ---------------------------------- **EXPERIMENTS** ---------------------------------- **SITGS FOR PHRASE EXTRACTION** First, we built an initial SITG by following the method described in <cite>(S\u00e1nchez and Bened\u00ed, 2006b</cite> ).",
  "y": "uses"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_3",
  "x": "Following common knowledge in SMT, we computed both the inverse and direct translation probabilities of each segment pair according to the formulae where C(s, t) is the number of times segments s and t were extracted throughout the whole corpus. This phrase-table was fed to Moses (Philipp Koehn, 2007) for producing the final translation. Initial SITGs with increasing number of non-terminal symbols were built and then estimated. The purpose of building SITGs with several non-terminal symbols was to analyse whether augmenting the number of non-terminals would improve word reorderings between both input and output languages. Adding non-terminal symbols may provide more complexity to the grammar built, and hence increases its expressive power. <cite>(S\u00e1nchez and Bened\u00ed, 2006b</cite> ) Translation results of this setup can be seen in Table 1 .",
  "y": "uses"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_4",
  "x": "If we marginalise for the input side of the word segments and for the output side of the segments, then we get: and In this way we obtain these two new syntax-based models: As can be seen in Table 2 , adding these new syntax based models produces a consistent improvement of approximately one point of BLEU. ---------------------------------- **DISCUSSION** Comparatively, the best result that<cite> (S\u00e1nchez and Bened\u00ed, 2006b)</cite> reported in the Spanish-English task was a BLEU score of 23.0, which they obtained by combining segments extracted from both the bracketed and the non-bracketed corpus.",
  "y": "background"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_5",
  "x": "In this way we obtain these two new syntax-based models: As can be seen in Table 2 , adding these new syntax based models produces a consistent improvement of approximately one point of BLEU. ---------------------------------- **DISCUSSION** Comparatively, the best result that<cite> (S\u00e1nchez and Bened\u00ed, 2006b)</cite> reported in the Spanish-English task was a BLEU score of 23.0, which they obtained by combining segments extracted from both the bracketed and the non-bracketed corpus. We have widely exceeded this baseline. On the other hand, the Moses toolkit (Philipp Koehn, 2007) , which is a state of the art statistical machine translation system, obtains in this task a score of 31.0 BLEU.",
  "y": "differences"
 },
 {
  "id": "0860b08831b01e7e98c66ced63b256_0",
  "x": "The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather The 2016 Conference on Computational Linguistics and Speech Processing ROCLING 2016, pp. 100-102 \uf0d3 The Association for Computational Linguistics and Chinese Language Processing 100 than window contexts in word2vec. Bansal et al. [8] and Melamud et al. [11] show the benefits of such modified-context embeddings in dependency parsing task. The dependency-based word embedding can relieve the problem of data sparseness, since even without occurrence of dependency word pairs in a corpus, dependency scores can be still calculated by word embeddings <cite>[12]</cite> . In this paper, we proposed a rescoring approach for parsing, based on a combination of original parsing scores and dependency word embedding scores to assist the determination of the best parse tree among the n-best parse trees.",
  "y": "motivation"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_0",
  "x": "Previous work on bridging anaphora resolution (Poesio et al., 2004;<cite> Hou et al., 2013b)</cite> use syntactic preposition patterns to calculate word relatedness. However, such patterns only consider NPs' head nouns and hence do not fully capture the semantics of NPs. Recently, Hou (2018) created word embeddings (embeddings PP) to capture associative similarity (i.e., relatedness) between nouns by exploring the syntactic structure of noun phrases. But embeddings PP only contains word representations for nouns. In this paper, we create new word vectors by combining embeddings PP with GloVe. This new word embeddings (embeddings bridging) are a more general lexical knowledge resource for bridging and allow us to represent the meaning of an NP beyond its head easily. We therefore develop a deterministic approach for bridging anaphora resolution, which represents the semantics of an NP based on its head noun and modifications. We show that this simple approach achieves the competitive results compared to the best system in<cite> Hou et al. (2013b)</cite> which explores Markov Logic Networks to model the problem.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_1",
  "x": "Previous work on bridging anaphora resolution (Poesio et al., 2004;<cite> Hou et al., 2013b)</cite> use syntactic preposition patterns to calculate word relatedness. However, such patterns only consider NPs' head nouns and hence do not fully capture the semantics of NPs. Recently, Hou (2018) created word embeddings (embeddings PP) to capture associative similarity (i.e., relatedness) between nouns by exploring the syntactic structure of noun phrases. But embeddings PP only contains word representations for nouns. In this paper, we create new word vectors by combining embeddings PP with GloVe. This new word embeddings (embeddings bridging) are a more general lexical knowledge resource for bridging and allow us to represent the meaning of an NP beyond its head easily. We therefore develop a deterministic approach for bridging anaphora resolution, which represents the semantics of an NP based on its head noun and modifications. We show that this simple approach achieves the competitive results compared to the best system in<cite> Hou et al. (2013b)</cite> which explores Markov Logic Networks to model the problem.",
  "y": "motivation background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_2",
  "x": "Recently, Hou (2018) created word embeddings (embeddings PP) to capture associative similarity (i.e., relatedness) between nouns by exploring the syntactic structure of noun phrases. But embeddings PP only contains word representations for nouns. In this paper, we create new word vectors by combining embeddings PP with GloVe. This new word embeddings (embeddings bridging) are a more general lexical knowledge resource for bridging and allow us to represent the meaning of an NP beyond its head easily. We therefore develop a deterministic approach for bridging anaphora resolution, which represents the semantics of an NP based on its head noun and modifications. We show that this simple approach achieves the competitive results compared to the best system in<cite> Hou et al. (2013b)</cite> which explores Markov Logic Networks to model the problem. Additionally, we further improve the results for bridging anaphora resolution reported in Hou (2018) by combining our simple deterministic approach with<cite> Hou et al. (2013b)</cite>'s best system MLN II. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_3",
  "x": "This new word embeddings (embeddings bridging) are a more general lexical knowledge resource for bridging and allow us to represent the meaning of an NP beyond its head easily. We therefore develop a deterministic approach for bridging anaphora resolution, which represents the semantics of an NP based on its head noun and modifications. We show that this simple approach achieves the competitive results compared to the best system in<cite> Hou et al. (2013b)</cite> which explores Markov Logic Networks to model the problem. Additionally, we further improve the results for bridging anaphora resolution reported in Hou (2018) by combining our simple deterministic approach with<cite> Hou et al. (2013b)</cite>'s best system MLN II. ---------------------------------- **INTRODUCTION** Anaphora plays a major role in discourse comprehension and accounts for the coherence of a text.",
  "y": "extends"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_4",
  "x": "In contrast to identity anaphora which indicates that a noun phrase refers back to the same entity introduced by previous descriptions in the discourse, bridging anaphora or associative anaphora links anaphors and antecedents via lexico-semantic, frame or encyclopedic relations. Bridging resolution is the task to recognize bridging anaphors (e.g., distribution arrangements in Example 1 1 ) and find links to their antecedents (e.g., dialysis products in Example 1). (1) While the discussions between Delmed and National Medical Care have been discontinued, Delmed will continue to supply dialysis products through National Medical after their exclusive agreement ends in March 1990, Delmed said. In addition, Delmed is exploring distribution arrangements with Fresenius USA, Delmed said. Most previous empirical research on bridging (Poesio and Vieira, 1998; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011;<cite> Hou et al., 2013b)</cite> focus on bridging anaphora resolution, a subtask of bridging resolution that aims to choose the antecedents for bridging anaphors. For this substask, most previous work (Poesio et al., 2004; Lassalle and Denis, 2011;<cite> Hou et al., 2013b)</cite> calculate semantic relatedness between an anaphor and its antecedent based on word co-occurrence counts using certain syntactic patterns. However, such patterns only consider head noun knowledge and hence are not sufficient for bridging relations which require the semantics of modification.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_5",
  "x": "Anaphora plays a major role in discourse comprehension and accounts for the coherence of a text. In contrast to identity anaphora which indicates that a noun phrase refers back to the same entity introduced by previous descriptions in the discourse, bridging anaphora or associative anaphora links anaphors and antecedents via lexico-semantic, frame or encyclopedic relations. Bridging resolution is the task to recognize bridging anaphors (e.g., distribution arrangements in Example 1 1 ) and find links to their antecedents (e.g., dialysis products in Example 1). (1) While the discussions between Delmed and National Medical Care have been discontinued, Delmed will continue to supply dialysis products through National Medical after their exclusive agreement ends in March 1990, Delmed said. In addition, Delmed is exploring distribution arrangements with Fresenius USA, Delmed said. Most previous empirical research on bridging (Poesio and Vieira, 1998; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011;<cite> Hou et al., 2013b)</cite> focus on bridging anaphora resolution, a subtask of bridging resolution that aims to choose the antecedents for bridging anaphors. For this substask, most previous work (Poesio et al., 2004; Lassalle and Denis, 2011;<cite> Hou et al., 2013b)</cite> calculate semantic relatedness between an anaphor and its antecedent based on word co-occurrence counts using certain syntactic patterns.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_6",
  "x": "Compared to embeddings PP, the coverage of lexicon in embeddings bridging is much larger. Also the word representations for nouns without the suffix \" PP\" are more accurate because they are trained on many more instances in the vanilla GloVe. Based on this general vector space, we develop a deterministic algorithm to select antecedents for bridging anaphors. Our approach combines the semantics of an NP's head with the semantics of its modifications by vector average using embeddings bridging. We show that this simple, efficient method achieves the competitive results on ISNotes for the task of bridging anaphora resolution compared to the best system in<cite> Hou et al. (2013b)</cite> which explores Markov Logic Networks to model the problem. The main contributions of our work are: (1) a general word representation resource 2 for bridging; and (2) a simple yet competitive deterministic approach for bridging anaphora resolution which models the meaning of an NP based on its head noun and modifications. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_7",
  "x": "Hou (2016) proposed an attention-based LSTM model with pre-trained word embeddings for information status classification and reported moderate results for bridging recognition. Pre-vious work on bridging anaphora resolution (Poesio et al., 2004; Lassalle and Denis, 2011;<cite> Hou et al., 2013b</cite> ) explored word co-occurrence counts in certain syntactic preposition patterns to calculate word relatedness. For instance, the big hit counts of the query \"the door of the house\" in large corpora could indicate that door and house stand in a part-of relation. These patterns encode associative relations between nouns which cover a variety of bridging relations. Unlike previous work which only consider a small number of prepositions per anaphor, the PP context model (Hou, 2018) uses all prepositions for all nouns in big corpora. It also includes the possessive structure of NPs. In this paper, we further improve Hou (2018)'s embeddings PP by combining it with the vanilla GloVe.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_8",
  "x": "Based on this corpus,<cite> Hou et al. (2013b)</cite> proposed a joint inference framework for bridging anaphora resolution using Markov logic networks (Domingos and Lowd, 2009 framework resolves all bridging anaphors in one document together by modeling that semantically related anaphors are likely to share the same antecedent. ISNotes is a challenging corpus for bridging. First, bridging anaphors are not limited to definite NPs as in previous work (Poesio et al., 1997 (Poesio et al., , 2004 Lassalle and Denis, 2011) . Also in ISNotes, the semantic relations between anaphor and antecedent are not restricted to meronymic relations. We therefore choose ISNotes to evaluate our algorithm for bridging anaphora resolution. Our approach is deterministic and simple, but achieves the competitive results compared to the advanced machine learning-based approach <cite>(Hou et al., 2013b)</cite> . We also improve the result reported in Hou (2018) on the same corpus by combining our deterministic approach with the best system from<cite> Hou et al. (2013b)</cite> .",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_9",
  "x": "ISNotes is a challenging corpus for bridging. First, bridging anaphors are not limited to definite NPs as in previous work (Poesio et al., 1997 (Poesio et al., , 2004 Lassalle and Denis, 2011) . Also in ISNotes, the semantic relations between anaphor and antecedent are not restricted to meronymic relations. We therefore choose ISNotes to evaluate our algorithm for bridging anaphora resolution. Our approach is deterministic and simple, but achieves the competitive results compared to the advanced machine learning-based approach <cite>(Hou et al., 2013b)</cite> . We also improve the result reported in Hou (2018) on the same corpus by combining our deterministic approach with the best system from<cite> Hou et al. (2013b)</cite> . Just recently, two new corpora (R\u00f6siger, 2018a; Poesio et al., 2018) with bridging annotations have become available and we notice that the definitions of bridging in these corpora are different from the bridging definition in ISNotes.",
  "y": "differences"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_10",
  "x": "We also improve the result reported in Hou (2018) on the same corpus by combining our deterministic approach with the best system from<cite> Hou et al. (2013b)</cite> . Just recently, two new corpora (R\u00f6siger, 2018a; Poesio et al., 2018) with bridging annotations have become available and we notice that the definitions of bridging in these corpora are different from the bridging definition in ISNotes. We apply our algorithm with small adaptations to select antecedents for bridging anaphors on these corpora. The moderate results demonstrate that embeddings bridging is a general word representation resource for bridging. (Parker et al., 2011; Napoles et al., 2012) . The author generates these noun pairs by exploring the syntactic prepositional and possessive structures of noun phrases. These two structures encode a variety of bridging relations between anaphors and their antecedents.",
  "y": "extends"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_11",
  "x": "79% of anaphors have \"other\" relation with their antecedents. This includes encyclopedic or frame relations such as restaurant -the waiter as well as context-specific relations such as palms -the thieves. In Example 1, \"dialysis products\" is the \"theme\" of the distribution arrangements. More specifically, \"dialysis products\" belongs to the frame element \"Individuals\" in the \"Dispersal\" frame that is triggered by \"distribution arrangements\". ---------------------------------- **EXPERIMENTAL SETUP** Following<cite> Hou et al. (2013b)</cite> 's experimental setup, we resolve bridging anaphors to entity antecedents.",
  "y": "uses"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_12",
  "x": "We also use the OntoNotes named entity annotation to assign 5 http://www.h-its.org/en/research/nlp/isnotes-corpus NPs the semantic type \"time\" if their entity types are \"date\" or \"time\". In<cite> Hou et al. (2013b)</cite> , features are extracted by using entity information. For instance, the raw hit counts of the preposition pattern query (e.g., arrangements of products) for a bridging anaphor a and its antecedent candidate e is the maximum count among all instantiations of e. In our experiments, we simply extend the list of antecedent candidates E a (described in Section 4) to include all instantiations of the original entities in E a . Note that our simple antecedent candidate selection strategy (described in Section 4) allows us to include 76% of NP antecedents compared to 77% in pairwise model III from<cite> Hou et al. (2013b)</cite> where they add top 10% salient entities as additional antecedent candidates. In<cite> Hou et al. (2013b)</cite> , salient entities on each text are measured through the lengths of the coreference chains based on the gold coreference annotation. Following<cite> Hou et al. (2013b)</cite> , we measure accuracy on the number of bridging anaphors, instead of on all links between bridging anaphors and their antecedent instantiations. We calculate how many bridging anaphors are correctly resolved among all bridging anaphors.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_13",
  "x": "In<cite> Hou et al. (2013b)</cite> , features are extracted by using entity information. For instance, the raw hit counts of the preposition pattern query (e.g., arrangements of products) for a bridging anaphor a and its antecedent candidate e is the maximum count among all instantiations of e. In our experiments, we simply extend the list of antecedent candidates E a (described in Section 4) to include all instantiations of the original entities in E a . Note that our simple antecedent candidate selection strategy (described in Section 4) allows us to include 76% of NP antecedents compared to 77% in pairwise model III from<cite> Hou et al. (2013b)</cite> where they add top 10% salient entities as additional antecedent candidates. In<cite> Hou et al. (2013b)</cite> , salient entities on each text are measured through the lengths of the coreference chains based on the gold coreference annotation. Following<cite> Hou et al. (2013b)</cite> , we measure accuracy on the number of bridging anaphors, instead of on all links between bridging anaphors and their antecedent instantiations. We calculate how many bridging anaphors are correctly resolved among all bridging anaphors. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_14",
  "x": "Entity information is based on the OntoNotes coreference annotation. We also use the OntoNotes named entity annotation to assign 5 http://www.h-its.org/en/research/nlp/isnotes-corpus NPs the semantic type \"time\" if their entity types are \"date\" or \"time\". In<cite> Hou et al. (2013b)</cite> , features are extracted by using entity information. For instance, the raw hit counts of the preposition pattern query (e.g., arrangements of products) for a bridging anaphor a and its antecedent candidate e is the maximum count among all instantiations of e. In our experiments, we simply extend the list of antecedent candidates E a (described in Section 4) to include all instantiations of the original entities in E a . Note that our simple antecedent candidate selection strategy (described in Section 4) allows us to include 76% of NP antecedents compared to 77% in pairwise model III from<cite> Hou et al. (2013b)</cite> where they add top 10% salient entities as additional antecedent candidates. In<cite> Hou et al. (2013b)</cite> , salient entities on each text are measured through the lengths of the coreference chains based on the gold coreference annotation. Following<cite> Hou et al. (2013b)</cite> , we measure accuracy on the number of bridging anaphors, instead of on all links between bridging anaphors and their antecedent instantiations.",
  "y": "differences"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_15",
  "x": "In<cite> Hou et al. (2013b)</cite> , salient entities on each text are measured through the lengths of the coreference chains based on the gold coreference annotation. Following<cite> Hou et al. (2013b)</cite> , we measure accuracy on the number of bridging anaphors, instead of on all links between bridging anaphors and their antecedent instantiations. We calculate how many bridging anaphors are correctly resolved among all bridging anaphors. ---------------------------------- **USING NP HEAD ALONE** Given an anaphor a and its antecedent candidate list E a , we predict the most related NP among all NPs in E a as the antecedent for a 6 . The relatedness is measured via cosine similarity between the head of the anaphor (plus the postfix \" PP\" if the anaphor is not a time expression) and the head of the candidate.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_16",
  "x": "We also use the OntoNotes named entity annotation to assign 5 http://www.h-its.org/en/research/nlp/isnotes-corpus NPs the semantic type \"time\" if their entity types are \"date\" or \"time\". In<cite> Hou et al. (2013b)</cite> , features are extracted by using entity information. For instance, the raw hit counts of the preposition pattern query (e.g., arrangements of products) for a bridging anaphor a and its antecedent candidate e is the maximum count among all instantiations of e. In our experiments, we simply extend the list of antecedent candidates E a (described in Section 4) to include all instantiations of the original entities in E a . Note that our simple antecedent candidate selection strategy (described in Section 4) allows us to include 76% of NP antecedents compared to 77% in pairwise model III from<cite> Hou et al. (2013b)</cite> where they add top 10% salient entities as additional antecedent candidates. In<cite> Hou et al. (2013b)</cite> , salient entities on each text are measured through the lengths of the coreference chains based on the gold coreference annotation. Following<cite> Hou et al. (2013b)</cite> , we measure accuracy on the number of bridging anaphors, instead of on all links between bridging anaphors and their antecedent instantiations. We calculate how many bridging anaphors are correctly resolved among all bridging anaphors.",
  "y": "uses"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_17",
  "x": "Using embeddings bridging further improves the result by 1.8%. Although the improvement is not significant, we suspect that the representations for words without the suffix \" PP\" in embeddings bridging are more accurate because they are trained on many more instances in the vanilla GloVe vectors (GloVe GigaWiki14). ---------------------------------- **USING NP HEAD + MODIFIERS** We carried out experiments using the deterministic algorithm described in Section 4 together with different word embeddings. Again we do not add the suffix \" PP\" to the bridging anaphors for GloVe GigaWiki14 and GloVe Giga. Table 6 lists the best results of the two models for bridging anaphora resolution from<cite> Hou et al. (2013b)</cite> .",
  "y": "uses"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_18",
  "x": "However, if we apply embeddings PP to a bridging anaphor's head and modifiers, and only apply embeddings PP to the head noun of an antecedent candidate, we get an due to the improved antecedent candidate selection strategy described in Section 4. acc models from<cite> Hou et al. (2013b)</cite> Table 6 : Results of using NP head plus modifications in different word representations for bridging anaphora resolution compared to the best results of two models from<cite> Hou et al. (2013b)</cite> . Bold indicates statistically significant differences over the other models (two-sided paired approximate randomization test, p < 0.01). accuracy of 34.53%. Although the differences are not significant, it confirms that the information from the modifiers of the antecedent candidates in embeddings PP hurts the performance. This corresponds to our observations in the previous section that the representations for words without the suffix \" PP\" in embeddings PP are not as good as in embeddings bridging due to less training instances. Finally, our method based on embeddings bridging achieves an accuracy of 39.52%, which is competitive to the best result (41.32%) reported in<cite> Hou et al. (2013b)</cite> .",
  "y": "uses"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_19",
  "x": "accuracy of 34.53%. Although the differences are not significant, it confirms that the information from the modifiers of the antecedent candidates in embeddings PP hurts the performance. This corresponds to our observations in the previous section that the representations for words without the suffix \" PP\" in embeddings PP are not as good as in embeddings bridging due to less training instances. Finally, our method based on embeddings bridging achieves an accuracy of 39.52%, which is competitive to the best result (41.32%) reported in<cite> Hou et al. (2013b)</cite> . There is no significant difference between NP head + modifiers based on embeddings bridging and MLN model II (randomization test with p < 0.01). To gain an insight into the contribution of embeddings bridging on different relation types, we analyze the results of our method using embeddings bridging on three relation types: set-of, part-of, and other. The accuracies on these three relation types are 17.78%, 50.0%, and 39.16%, respectively.",
  "y": "similarities"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_20",
  "x": "For bridging anaphora resolution, Hou (2018) integrates a much simpler deterministic approach by combining an NP head with its noun modifiers (appearing before the head) based on embeddings PP into the MLN II system <cite>(Hou et al., 2013b)</cite> . Similarly, we add a constraint on top of MLN II using our deterministic approach (NP head + modifiers) based on embeddings bridging. Table 8 lists the results of different systems 8 for bridging anaphora resolution in ISNotes. It shows that combining our deterministic approach (NP Head + modifiers) with MLN II slightly improves the result compared to Hou (2018) . Although combining NP Head + modifiers with MLN II achieves significant improvement over NP 8 We also reimplement the algorithms from Schulte im Walde (1998) and Poesio et al. (2004) as baselines (Table 8) . Schulte im Walde (1998) resolved bridging anaphors to the closest antecedent candidate in a high-dimensional space. We use the 2,000 most frequent words (adjectives, common nouns, proper nouns, and lexical verbs) from Gigaword as the context words.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_21",
  "x": "**NOUN MODIFIERS).** On the other hand, ed/ing participle modifiers only have a small positive impact over NP head + noun modifiers when combining with noun modifiers. ---------------------------------- **COMBINING NP HEAD + MODIFIERS WITH MLN II** For bridging anaphora resolution, Hou (2018) integrates a much simpler deterministic approach by combining an NP head with its noun modifiers (appearing before the head) based on embeddings PP into the MLN II system <cite>(Hou et al., 2013b)</cite> . Similarly, we add a constraint on top of MLN II using our deterministic approach (NP head + modifiers) based on embeddings bridging. Table 8 lists the results of different systems 8 for bridging anaphora resolution in ISNotes.",
  "y": "background uses"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_22",
  "x": "Based on embeddings bridging, we propose a deterministic approach for choosing antecedents for bridging anaphors. We show that this simple and efficient method achieves the competitive result on bridging anaphora resolution compared to the advanced machine learning-based approach in<cite> Hou et al. (2013b)</cite> which is heavily dependent on a lot of carefully designed complex features. We also demonstrate that using embeddings bridging yields better results than using embeddings PP for bridging anaphora resolution. For the task of bridging anaphora resolution,<cite> Hou et al. (2013b)</cite> pointed out that considering only head noun knowledge is not enough and future work needs to explore wider context to resolve context-specific bridging relations. In this work we explore the context within NPs-that is, we combine the semantics of certain modifications and the head by vector average using embeddings bridging. But in some cases, knowledge about NPs themselves is not enough for resolving bridging. For instance, in Example 3, knowing that any loosening has the ability to \"rekindle inflation\" from the context of the second sentence can help us to find its antecedent \"the high rates\" (which is used to against inflation).",
  "y": "differences"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_23",
  "x": "The resulting word embeddings (embeddings bridging) are a more general word representation resource for bridging. Based on embeddings bridging, we propose a deterministic approach for choosing antecedents for bridging anaphors. We show that this simple and efficient method achieves the competitive result on bridging anaphora resolution compared to the advanced machine learning-based approach in<cite> Hou et al. (2013b)</cite> which is heavily dependent on a lot of carefully designed complex features. We also demonstrate that using embeddings bridging yields better results than using embeddings PP for bridging anaphora resolution. For the task of bridging anaphora resolution,<cite> Hou et al. (2013b)</cite> pointed out that considering only head noun knowledge is not enough and future work needs to explore wider context to resolve context-specific bridging relations. In this work we explore the context within NPs-that is, we combine the semantics of certain modifications and the head by vector average using embeddings bridging. But in some cases, knowledge about NPs themselves is not enough for resolving bridging.",
  "y": "future_work"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_0",
  "x": "<cite>Wang et al. (2014a)</cite> rely on Wikipedia anchors, making the applicable scope quite limited. In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors. We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description. Extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of <cite>Wang et al. (2014a)</cite> , which is encouraging as we do not use any anchor information. ---------------------------------- **INTRODUCTION** Knowledge base embedding has attracted surging interest recently.",
  "y": "motivation"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_1",
  "x": "<cite>Wang et al. (2014a)</cite> rely on Wikipedia anchors, making the applicable scope quite limited. In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors. We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description. Extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of <cite>Wang et al. (2014a)</cite> , which is encouraging as we do not use any anchor information. ---------------------------------- **INTRODUCTION** Knowledge base embedding has attracted surging interest recently.",
  "y": "differences"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_2",
  "x": "Although seeming encouraging, the approaches in the aforementioned literature suffer from two common issues: (1) Embeddings are exclusive to entities/relations within KBs. Computation between KBs and text cannot be handled, which are prevalent in practice. For example, in fact extraction, a candidate value may be just a phrase in text. (2) KB sparsity. The above approaches are only based on structured facts of KBs, and thus cannot work well on entities with few facts. An important milestone, the approach of <cite>Wang et al. (2014a)</cite> solves issue (1) by jointly embedding entities, relations, and words into the same vector space and hence is able to deal with words/phrases beyond entities in KBs. The key component is the so-called alignment model, which makes sure the embeddings of entities, relations, and words are in the same space.",
  "y": "background"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_3",
  "x": "To fully address the two issues, this paper proposes a new alignment method, aligning by entity descriptions. We only assume some entities in KBs have text descriptions, which almost always holds in practice. We require the embedding of an entity not only fits the structured constraints in KBs but also equals the vector computed from the text description. Meanwhile, if an entity has few facts, the description will provide information for embedding, thus the issue of KB sparsity is also well handled. We conduct extensive experiments on the tasks of triplet classification, link prediction, relational fact extraction, and analogical reasoning to compare with the previous approach <cite>(Wang et al., 2014a)</cite> . Results show that our approach consistently achieves better or comparable performance. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_4",
  "x": "---------------------------------- **RELATED WORK** TransE This is a representative knowledge embedding model proposed by . For a fact (h, r, t) in KBs, where h is the head entity, r is the relation, and t is the tail entity, TransE models the relation r as a translation vector r connecting the embeddings h and t of the two entities, i.e., h + r is close to t. The model is simple, effective and efficient. Most knowledge embedding models thereafter including this paper are variants of this model (Wang et al., 2014b; <cite>Wang et al., 2014a</cite>; Lin et al., 2015) . Skip-gram This is an efficient word embedding method proposed by Mikolov et al. (2013a) , which learns word embeddings from word concurrencies in text windows. Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as 'King' \u2212 'Queen' \u2248 'Man' \u2212 'Women'.",
  "y": "uses background"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_5",
  "x": "Most knowledge embedding models thereafter including this paper are variants of this model (Wang et al., 2014b; <cite>Wang et al., 2014a</cite>; Lin et al., 2015) . Skip-gram This is an efficient word embedding method proposed by Mikolov et al. (2013a) , which learns word embeddings from word concurrencies in text windows. Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as 'King' \u2212 'Queen' \u2248 'Man' \u2212 'Women'. However, as it is unsupervised, it cannot tell the exact relation between two words. <cite>Wang et al. (2014a)</cite> combines knowledge embedding and word embedding in a joint framework so that the entities/relations and words are in the same vector space and hence operators like inner product (similarity) between them are meaningful. This brings convenience to tasks requiring computation between knowledge bases and text. Meanwhile, jointly embedding utilizes information from both structured KBs and unstructured text and hence the knowledge embedding and word embedding can be enhanced by each other.",
  "y": "background"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_6",
  "x": "We try to learn embeddings e i , r j and w l for each entity e i , relation r j and word w l respectively. The vocabulary of words is V. The union vocabulary of entities and words together is I = E \u222a V. In this paper \"word(s)\" refers to \"word(s)/phrase(s)\". We follow the jointly embedding framework of <cite>(Wang et al., 2014a)</cite> , i.e., learning optimal embeddings by minimizing the following loss where L K , L T and L A are the component loss functions of the knowledge model, text model and alignment model respectively. Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in <cite>(Wang et al., 2014a)</cite> . However, to make the content self-contained, we still need to briefly explain L K and L T . Knowledge Model Describes the plausibility of a triplet (h, r, t) by defining",
  "y": "uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_7",
  "x": "We follow the jointly embedding framework of <cite>(Wang et al., 2014a)</cite> , i.e., learning optimal embeddings by minimizing the following loss where L K , L T and L A are the component loss functions of the knowledge model, text model and alignment model respectively. Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in <cite>(Wang et al., 2014a)</cite> . However, to make the content self-contained, we still need to briefly explain L K and L T . Knowledge Model Describes the plausibility of a triplet (h, r, t) by defining where z(h, r, t) = b \u2212 0.5 \u00b7 h + r \u2212 t 2 2 , b = 7 as suggested by <cite>Wang et al. (2014a)</cite> . Pr(r|h, t) and Pr(t|h, r) are defined in the same way.",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_8",
  "x": "However, to make the content self-contained, we still need to briefly explain L K and L T . Knowledge Model Describes the plausibility of a triplet (h, r, t) by defining where z(h, r, t) = b \u2212 0.5 \u00b7 h + r \u2212 t 2 2 , b = 7 as suggested by <cite>Wang et al. (2014a)</cite> . Pr(r|h, t) and Pr(t|h, r) are defined in the same way. The loss function of knowledge model is then defined as log Pr(h|r, t) + log Pr(t|h, r) + log Pr(r|h, t) (4) Text Model Defines the probability of a pair of words w and v co-occurring in a text window:",
  "y": "uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_9",
  "x": "The loss function of knowledge model is then defined as log Pr(h|r, t) + log Pr(t|h, r) + log Pr(r|h, t) (4) Text Model Defines the probability of a pair of words w and v co-occurring in a text window: where Then the loss function of text model is Alignment Model This part is different from <cite>Wang et al. (2014a)</cite> . For each word w in the description of entity e, we define Pr(w|e), the conditional probability of predicting w given e:",
  "y": "differences"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_10",
  "x": "---------------------------------- **EXPERIMENTS** We conduct experiments on the following tasks: link prediction , triplet classification (Socher et al., 2013) , relational fact extraction , and analogical reasoning (Mikolov et al., 2013b) . The last one evaluates quality of word embeddings. We try to study whether the proposed alignment model, without using any anchor information, is able to achieve comparable or better performance than alignment by anchors. As to the methods, \"Separately\" denotes the method of separately embedding knowledge bases and text. \"Jointly(anchor)\" and \"Jointly(name)\" denote the jointly embedding methods based on Alignment by Wikipedia Anchors and Alignment by Entity Names in <cite>(Wang et al., 2014a)</cite> respectively. \"Jointly(desp)\" is the joint embedding method based on alignment by entity descriptions.",
  "y": "uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_11",
  "x": "\"Jointly(desp)\" is the joint embedding method based on alignment by entity descriptions. Data For link prediction, FB15K from ) is used as the knowledge base. For triplet classification, a large dataset provided by <cite>(Wang et al., 2014a )</cite> is used as the knowledge base. Both sets are subsets of Freebase. For all tasks, Wikipedia articles are used as the text corpus. As many Wikipedia articles can be mapped to Freebase entities, we regard a Wikipedia article as the description for the corresponding entity in Freebase. Following the settings in <cite>(Wang et al., 2014a)</cite> , we apply the same preprocessing steps, including sentence segmentation, tokenization, and named entity recognition.",
  "y": "uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_12",
  "x": "As many Wikipedia articles can be mapped to Freebase entities, we regard a Wikipedia article as the description for the corresponding entity in Freebase. Following the settings in <cite>(Wang et al., 2014a)</cite> , we apply the same preprocessing steps, including sentence segmentation, tokenization, and named entity recognition. We combine the consecutive tokens covered by an anchor or identically tagged as \"Location/Person/Organization\" and regard them as phrases. Link Prediction This task aims to complete a fact (h, r, t) in absence of h or t, simply based on h + r \u2212 t . We follow the same protocol in . We directly copy the results of the baseline (TransE) from and implement \"Jointly(anchor)\". The results are in Table 1 . \"MEAN\" is the average rank of the true absent entity.",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_13",
  "x": "(2) Under the setting of \"Raw\", \"Jointly(desp)\" and \"Jointly(anchor)\" are comparable. In other settings \"Jointly(desp)\" wins. Triplet Classification This is a binary classification task, predicting whether a candidate triplet (h, r, t) is a correct fact or not. It is used in (Socher et al., 2013; Wang et al., 2014b; <cite>Wang et al., 2014a)</cite> . We follow the same protocol in <cite>(Wang et al., 2014a)</cite> . We train their models via our own implementation on our dataset. The results are in Table 2 . \"e-e\" means both sides of a triplet (h, r, t) are entities in KB, \"e-w\" means the tail side is a word out of KB entity vocabulary, similarly for \"w-e\" and \"w-w\".",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_14",
  "x": "In other settings \"Jointly(desp)\" wins. Triplet Classification This is a binary classification task, predicting whether a candidate triplet (h, r, t) is a correct fact or not. It is used in (Socher et al., 2013; Wang et al., 2014b; <cite>Wang et al., 2014a)</cite> . We follow the same protocol in <cite>(Wang et al., 2014a)</cite> . We train their models via our own implementation on our dataset. The results are in Table 2 . \"e-e\" means both sides of a triplet (h, r, t) are entities in KB, \"e-w\" means the tail side is a word out of KB entity vocabulary, similarly for \"w-e\" and \"w-w\". The best configurations of the models are: k = 150, \u03b1 = 0.025, c = 10, s = 5 and traversing the text corpus with 6 epochs.",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_15",
  "x": "This task is to extract facts (h, r, t) from plain text. show that combing scores from TransE and some text side base extractor achieved much better precision-recall curve compared to the base extractor. <cite>Wang et al. (2014a)</cite> confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as <cite>(Wang et al., 2014a)</cite> to investigate the performance of our new alignment model. We use the same public dataset NYT+FB, released by Riedel et al. (2010) and used in and <cite>(Wang et al., 2014a)</cite> . We use Mintz (Mintz et al., 2009 ) and MIML (Surdeanu et al., 2012) as our base extractors. In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment. Since both Mintz and MIML are probabilistic models, we use the same method in <cite>(Wang et al., 2014a)</cite> to linearly combine the scores. The precision-recall curves are plot in Fig. (1) .",
  "y": "motivation uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_16",
  "x": "Using entity's description information is a more straightforward and effective way to align entity embeddings and word embeddings. ---------------------------------- **RELATIONAL FACT EXTRACTION** This task is to extract facts (h, r, t) from plain text. show that combing scores from TransE and some text side base extractor achieved much better precision-recall curve compared to the base extractor. <cite>Wang et al. (2014a)</cite> confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as <cite>(Wang et al., 2014a)</cite> to investigate the performance of our new alignment model. We use the same public dataset NYT+FB, released by Riedel et al. (2010) and used in and <cite>(Wang et al., 2014a)</cite> . We use Mintz (Mintz et al., 2009 ) and MIML (Surdeanu et al., 2012) as our base extractors.",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_17",
  "x": "This task is to extract facts (h, r, t) from plain text. show that combing scores from TransE and some text side base extractor achieved much better precision-recall curve compared to the base extractor. <cite>Wang et al. (2014a)</cite> confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as <cite>(Wang et al., 2014a)</cite> to investigate the performance of our new alignment model. We use the same public dataset NYT+FB, released by Riedel et al. (2010) and used in and <cite>(Wang et al., 2014a)</cite> . We use Mintz (Mintz et al., 2009 ) and MIML (Surdeanu et al., 2012) as our base extractors. In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment. Since both Mintz and MIML are probabilistic models, we use the same method in <cite>(Wang et al., 2014a)</cite> to linearly combine the scores. The precision-recall curves are plot in Fig. (1) .",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_18",
  "x": "Moreover, \"Jointly(desp)\" is slightly better than \"Jointly(anchor)\", which is in accordance with the results from the link prediction experiment and the triplet classification experiment. Analogical Reasoning This task evaluates the quality of word embeddings (Mikolov et al., 2013b) . We use the original dataset released by (Mikolov et al., 2013b) and follow the same evaluation protocol of <cite>(Wang et al., 2014a)</cite> . For a true analogical pair like (\"France\", \"Paris\") and (\"China\", \"Beijing\"), we hide \"Beijing\" and predict it by selecting the word from the vocabulary whose vector has highest similarity with the vector of \"China\" + \"Paris\" -\"France\". We use the word embeddings learned for the triplet classification experiment and conduct the analogical reasoning experiment for \"Skip-gram\", \"Jointly(anchor)\", \"Jointly(name)\" and \"Jointly(desp)\". Results are presented in Table 3 . \"Acc\" is the accuracy of the predicted word. \"HITS@10\" is the accuracy of the top 10 candidates containing the ground truth.",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_19",
  "x": "\"HITS@10\" is the accuracy of the top 10 candidates containing the ground truth. The evaluation analogical pairs are organized into two groups, \"Words\" and \"Phrases\", by whether an analogical pair contains phrases (i.e., multiple words). From the table we observe that: (1) Both \"Jointly(anchor)\" and \"Jointly(desp)\" outperform \"Skip-gram\". (2) \"Jointly(desp)\" achieves the best results, especially for the case of \"Phrases\". Both \"Jointly(anchor)\" and \"Skip-gram\" only consider the context of words, while \"Jointly(desp)\" not only consider the context but also use the whole document to disambiguate words. Intuitively, the whole document is also a valuable resource to disambiguate words. (3) We further verify that \"Jointly(name)\", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in <cite>(Wang et al., 2014a)</cite> .",
  "y": "similarities"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_20",
  "x": "\"HITS@10\" is the accuracy of the top 10 candidates containing the ground truth. The evaluation analogical pairs are organized into two groups, \"Words\" and \"Phrases\", by whether an analogical pair contains phrases (i.e., multiple words). From the table we observe that: (1) Both \"Jointly(anchor)\" and \"Jointly(desp)\" outperform \"Skip-gram\". (2) \"Jointly(desp)\" achieves the best results, especially for the case of \"Phrases\". Both \"Jointly(anchor)\" and \"Skip-gram\" only consider the context of words, while \"Jointly(desp)\" not only consider the context but also use the whole document to disambiguate words. Intuitively, the whole document is also a valuable resource to disambiguate words. (3) We further verify that \"Jointly(name)\", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in <cite>(Wang et al., 2014a)</cite> .",
  "y": "differences"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_0",
  "x": "**INTRODUCTION** Non-projective dependency trees are those containing crossing edges. They account for 12.59% of all training sentences in the annotated Universal Dependencies (UD) 2.1 data (Nivre et al., 2017) , and more than 20% in each of 10 languages among the 54 in UD 2.1 with training treebanks. But modeling non-projectivity is computationally costly (McDonald and Satta, 2007) . Some transition-based dependency parsers have deduction systems that use dynamic programming to enable exact inference in polynomial time and space (Huang and Sagae, 2010; Kuhlmann et al., 2011) . For non-projective parsing, though, the only tabularization of a transition-based parser is, to our knowledge, that of Cohen et al. (2011) . They define a deduction system for (an isomorphic variant of)<cite> Attardi's (2006)</cite> transition system, which covers a subset of non-projective trees.",
  "y": "background"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_4",
  "x": "We now introduce the widely-used<cite> Attardi (2006)</cite> system, which includes transitions that create arcs between non-consecutive subtrees, thus allowing it to produce some non-projective trees. To simplify exposition, here we present Cohen et al.'s (2011) isomorphic version. The set of transitions consists of a shift transition (sh) and four reduce transitions (re). A shift moves the first buffer item onto the stack: A reduce transition re h,m creates a dependency arc between h (head) and m (modifier) and reduces m. For example, re s 0 ,s 1 rp\u03c3|s 1 |s 0 , \u03b2, Aqs \" p\u03c3|s 0 , \u03b2, A Y tps 0 , s 1 quq . Row 1 of Fig. 1 depicts the four Attardi reduces. The distance between h and m in a re h,m transition is called its degree.",
  "y": "background"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_5",
  "x": "As shown in Fig. 1 ,<cite> Attardi's (2006)</cite> system has two degree-2 transitions (re s 0 ,s 2 and re s 2 ,s 0 ) that allow it to cover 87.24% of the nonprojective trees in UD 2.1. More generally, an Attardi system of degree D adds re s 0 ,s D and re s D ,s 0 to the system of degree D\u00b41. ---------------------------------- **IMPROVING COVERAGE** A key observation is that a degree-D Attardi system does not contain all possible transitions of degree within D. Since prior empirical work has ascertained that transition systems using more transitions with degree greater than 1 can handle more non-projective treebank trees <cite>(Attardi, 2006</cite>; G\u00f3mez-Rodr\u00edguez, 2016) , we hypothesize that adding some of these \"missing\" reduce transitions into the system's inventory should increase coverage. The challenge is to simultaneously maintain run-time guarantees, as there exists a known trade-off between coverage and complexity (G\u00f3mez-Rodr\u00edguez, 2016 Cohen et al. (2011) , rather than Opn 3\u00a83`1 q; and (ii) another has degree 2 but better runtime than Cohen et al.'s (2011) system. Here, we first sketch the existing exact inference algorithm, 3 and then present our variants.",
  "y": "background"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_6",
  "x": "More generally, an Attardi system of degree D adds re s 0 ,s D and re s D ,s 0 to the system of degree D\u00b41. ---------------------------------- **IMPROVING COVERAGE** A key observation is that a degree-D Attardi system does not contain all possible transitions of degree within D. Since prior empirical work has ascertained that transition systems using more transitions with degree greater than 1 can handle more non-projective treebank trees <cite>(Attardi, 2006</cite>; G\u00f3mez-Rodr\u00edguez, 2016) , we hypothesize that adding some of these \"missing\" reduce transitions into the system's inventory should increase coverage. The challenge is to simultaneously maintain run-time guarantees, as there exists a known trade-off between coverage and complexity (G\u00f3mez-Rodr\u00edguez, 2016 Cohen et al. (2011) , rather than Opn 3\u00a83`1 q; and (ii) another has degree 2 but better runtime than Cohen et al.'s (2011) system. Here, we first sketch the existing exact inference algorithm, 3 and then present our variants. ----------------------------------",
  "y": "background"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_7",
  "x": "With adaptation of the \"hook trick\" described in Eisner and Satta (1999) , we can reduce the running time to Opn 7 q. ---------------------------------- **OUR NEW VARIANTS** In this section, we modify Cohen et al.'s (2011) set of reduce deduction rules to improve coverage or time complexity. Since each such deduction rule corresponds to a reduce transition, each revision to the deduction system yields a variant of<cite> Attardi's (2006)</cite> parser. In other words, generalization of the deduction system gives rise to a family of nonprojective transition-based dependency parsers. We first explain why there are exactly nine reduce transitions R \" tre s 0 ,s 1 , re s 1 ,s 0 , re s 0 ,s 2 , re s 2 ,s 0 , re s 1 ,s 2 , re s 2 ,s 1 , re b 0 ,s 0 , re b 0 ,s 1 , re b 0 ,s 2 u that can be used in Cohen et al.'s (2011) exact inference algorithm, without allowing a reduction with head b i for i \u011b 1. 6 (Note that Cohen et al.'s (2011) reduce rules are precisely the first four elements of R.) From Fig. 3 we infer that the concatenation of I-computations rh 1 , i, h 2 , h 3 , ks and rh 3 , k, h 4 , h 5 , js yields a configuration of the form p\u03c3|h 2 |h 4 |h 5 , j|\u03b2, Aq.",
  "y": "background"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_8",
  "x": "They are given in Fig. 1 , along with their time complexities and empirical coverage statistics. The latter is computed using static oracles (Cohen et al., 2012) on the UD 2.1 dataset (Nivre et al., 2017) . 7 We report the global coverage over the 76,084 non-projective sentences from all the training treebanks. One might assume that adding more degree-1 transitions wouldn't improve coverage of trees with non-crossing edges. On the other hand, since their addition doesn't affect the asymptotic run-time, we define ALLDEG1 to include all five degree-1 transitions from R into the<cite> Attardi (2006)</cite> system. Surprisingly, using ALLDEG1 improves non-projective coverage from 87.24% to 93.32%. Furthermore, recall that we argued above that,",
  "y": "uses"
 },
 {
  "id": "09493a62815b4b826248d6d9be47cb_0",
  "x": "However, while these tools have proven to be effective for patient records and research papers, they achieve moderate results on social media texts (Nikfarjam et al., 2015;<cite> Limsopatham and Collier, 2016)</cite> . Recent works go beyond string matching: these works have tried to view the problem of matching a one-or multi-word expression against a knowledge base as a supervised sequence labeling problem. <cite>Limsopatham and Collier (2016)</cite> utilized convolutional neural networks (CNNs) for phrase normalization in user reviews, while Tutubalina et al. (2018) , Han et al. (2017) , and Belousov et al. (2017) applied recurrent neural networks (RNNs) to UGTs, achieving similar results. These works were among the first applications of deep learning techniques to medical concept normalization. The goal of this work is to study the use of deep neural models, i.e., contextualized word representation model BERT (Devlin et al., 2018) and Gated Recurrent Units (GRU) (Cho et al., 2014) with an attention mechanism, paired with word2vec word embeddings and contextualized ELMo embeddings (Peters et al., 2018) . We investigate if a joint architecture with special provisions for domain knowledge can further improve the mapping of entity mentions from UGTs to medical concepts. We combine the representation of an entity mention constructed by a neural model and distance-like similarity features using vectors of an entity mention and concepts from the UMLS.",
  "y": "motivation"
 },
 {
  "id": "09493a62815b4b826248d6d9be47cb_1",
  "x": "DNorm adopts a pairwise learning-torank technique using vectors of query mentions and candidate concept terms. This model outperforms MetaMap significantly, increasing the macro-averaged F-measure by 25% on an NCBI disease dataset. However, while these tools have proven to be effective for patient records and research papers, they achieve moderate results on social media texts (Nikfarjam et al., 2015;<cite> Limsopatham and Collier, 2016)</cite> . Recent works go beyond string matching: these works have tried to view the problem of matching a one-or multi-word expression against a knowledge base as a supervised sequence labeling problem. <cite>Limsopatham and Collier (2016)</cite> utilized convolutional neural networks (CNNs) for phrase normalization in user reviews, while Tutubalina et al. (2018) , Han et al. (2017) , and Belousov et al. (2017) applied recurrent neural networks (RNNs) to UGTs, achieving similar results. These works were among the first applications of deep learning techniques to medical concept normalization. The goal of this work is to study the use of deep neural models, i.e., contextualized word representation model BERT (Devlin et al., 2018) and Gated Recurrent Units (GRU) (Cho et al., 2014) with an attention mechanism, paired with word2vec word embeddings and contextualized ELMo embeddings (Peters et al., 2018) .",
  "y": "background"
 },
 {
  "id": "09493a62815b4b826248d6d9be47cb_2",
  "x": "Huang and Lu (2015) survey the work done in the organization of biomedical NLP (BioNLP) challenge evaluations up to 2014. These tasks are devoted to the normalization of (1)<cite> (Limsopatham and Collier, 2016)</cite> 73.39 ----CNN<cite> (Limsopatham and Collier, 2016)</cite> 81.41 ----RNN<cite> (Limsopatham and Collier, 2016)</cite> 79.98 ----Attentional Char-CNN (Niu et al., 2018) 84.65 ----Hierarchical Char-CNN (Han et al., 2017) - Table 2 : The performance of the proposed models and the state-of-the-art methods in terms of accuracy. (4) diseases from clinical reports (ShARe/CLEF eHealth 2013; SemEval 2014 task 7). Similarly, the CLEF Health 2016 and 2017 labs addressed the problem of ICD coding of freeform death certificates (without specified entity mentions). Traditionally, linguistic approaches based on dictionaries, association measures, and syntactic properties have been used to map texts to a concept from a controlled vocabulary (Aronson, 2001; Van Mulligen et al., 2016; Mottin et al., 2016; Ghiasvand and Kate, 2014; Tang et al., 2014) . Leaman et al. (2013) proposed the DNorm system based on a pairwise learningto-rank technique using vectors of query mentions and candidate concept terms. These vectors are obtained from a tf-idf representation of all tokens from training mentions and concept terms.",
  "y": "background"
 },
 {
  "id": "0984f12a6fea858c7f18263cc2fb01_0",
  "x": "We outline how the data can be used for multilingual image description and multimodal machine translation, but we anticipate the data will be useful for a broader range of tasks. ---------------------------------- **INTRODUCTION** Image description is one of the core challenges at the intersection of Natural Language Processing (NLP) and Computer Vision (CV) (Bernardi et al., 2016) . This task has only received attention in a monolingual English setting, helped by the availability of English datasets, e.g. Flickr8K (Hodosh et al., 2013) , Flickr30K <cite>(Young et al., 2014)</cite> , and MS COCO (Chen et al., 2015) . However, the possible applications of image description are useful for all languages, such as searching for images using natural language, or providing alternative-description text for visually impaired Web users. We introduce a large-scale dataset of images paired with sentences in English and German as an initial step towards studying the value and the characteristics of multilingual-multimodal data.",
  "y": "background"
 },
 {
  "id": "0984f12a6fea858c7f18263cc2fb01_1",
  "x": "However, the possible applications of image description are useful for all languages, such as searching for images using natural language, or providing alternative-description text for visually impaired Web users. We introduce a large-scale dataset of images paired with sentences in English and German as an initial step towards studying the value and the characteristics of multilingual-multimodal data. Multi30K is an extension of the Flickr30K dataset <cite>(Young et al., 2014)</cite> with 31,014 German translations of English descriptions and 155,070 independently collected German descriptions. The translations were collected from professionally contracted translators, whereas the descriptions were collected from untrained crowdworkers. The key difference between these corpora is the relationship between the sentences in different languages. In the translated corpus, we know there is a strong correspondence between the sentences in both languages. In the descriptions corpus, we only know that the sentences, regardless of the language, are supposed to describe the same image.",
  "y": "extends"
 },
 {
  "id": "0984f12a6fea858c7f18263cc2fb01_2",
  "x": "**THE MULTI30K DATASET** The Flickr30K Dataset contains 31,014 images sourced from online photo-sharing websites <cite>(Young et al., 2014)</cite> . Each image is paired with five English descriptions, which were collected from Amazon Mechanical Turk 1 . The dataset contains 145,000 training, 5,070 development, and 5,000 test descriptions. The Multi30K dataset extends the Flickr30K dataset with translated and independent German sentences. ---------------------------------- **TRANSLATIONS**",
  "y": "background"
 },
 {
  "id": "0984f12a6fea858c7f18263cc2fb01_3",
  "x": "---------------------------------- **THE MULTI30K DATASET** The Flickr30K Dataset contains 31,014 images sourced from online photo-sharing websites <cite>(Young et al., 2014)</cite> . Each image is paired with five English descriptions, which were collected from Amazon Mechanical Turk 1 . The dataset contains 145,000 training, 5,070 development, and 5,000 test descriptions. The Multi30K dataset extends the Flickr30K dataset with translated and independent German sentences. ----------------------------------",
  "y": "extends background"
 },
 {
  "id": "0984f12a6fea858c7f18263cc2fb01_4",
  "x": "The descriptions were collected as similarly as possible to the original Flickr30K dataset by translating the instructions used by<cite> Young et al. (2014)</cite> into German. The translations were collected without showing the images to the translators to keep it as close to a standard translation task as possible. There are substantial differences between the translated and the description datasets. The translations contain approximately the same number of tokens and have sentences of approximately the same length in both languages. These properties make them suited to machine translations models. The description datasets are very different in terms of average sentence lengths and the number of word types per language. This is likely to cause different engineering and scientific challenges because the descriptions are independently collected corpora instead of a sentence-level aligned corpus.",
  "y": "uses"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_0",
  "x": "---------------------------------- **INTRODUCTION** Syntactic analysis of search queries is important for a variety of tasks including better query refinement, improved matching and better ad targeting<cite> (Barr et al., 2008)</cite> . However, search queries differ substantially from traditional forms of written language (e.g., no capitalization, few function words, fairly free word order, etc.), and are therefore difficult to process with natural language processing tools trained on standard corpora<cite> (Barr et al., 2008)</cite> . In this paper we focus on part-of-speech (POS) tagging queries entered into commercial search engines and compare different strategies for learning from search logs. The search logs consist of user queries and relevant search results retrieved by a search engine. We use a supervised POS tagger to label the result snippets and then transfer the tags to the queries, producing a set of noisy labeled queries.",
  "y": "background"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_1",
  "x": "**INTRODUCTION** Syntactic analysis of search queries is important for a variety of tasks including better query refinement, improved matching and better ad targeting<cite> (Barr et al., 2008)</cite> . However, search queries differ substantially from traditional forms of written language (e.g., no capitalization, few function words, fairly free word order, etc.), and are therefore difficult to process with natural language processing tools trained on standard corpora<cite> (Barr et al., 2008)</cite> . In this paper we focus on part-of-speech (POS) tagging queries entered into commercial search engines and compare different strategies for learning from search logs. The search logs consist of user queries and relevant search results retrieved by a search engine. We use a supervised POS tagger to label the result snippets and then transfer the tags to the queries, producing a set of noisy labeled queries. These labeled queries are then added to the training data and the tagger is retrained.",
  "y": "motivation"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_2",
  "x": "We additionally refine the annotation to cover 14 POS tags comprising the 12 universal tags of Petrov et al. (2012) , as well as proper nouns and a special tag for search operator symbols such as \"-\" (for excluding the subsequent word). We refer to this evaluation set as MS-251 in our experiments. We had two annotators annotate the whole of the MS-251 data set. Before arbitration, the inter-annotator agreement was 90.2%. As a reference, <cite>Barr et al. (2008)</cite> report 79.3% when annotating queries with 19 POS tags. We then examined all the instances where the annotators disagreed, and corrected the discrepancy. Our annotations are available at http://code.google.com/p/query-syntax/.",
  "y": "uses"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_3",
  "x": "We refer to this evaluation set as MS-251 in our experiments. We had two annotators annotate the whole of the MS-251 data set. Before arbitration, the inter-annotator agreement was 90.2%. As a reference, <cite>Barr et al. (2008)</cite> report 79.3% when annotating queries with 19 POS tags. We then examined all the instances where the annotators disagreed, and corrected the discrepancy. Our annotations are available at http://code.google.com/p/query-syntax/. The second evaluation set consists of 500 so called \"long-tail\" queries.",
  "y": "differences"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_4",
  "x": "Our best system achieves a 21.2% relative reduction in error on their annotations. Some other trends become appar- ent in Table 2 . Firstly, a large part of the benefit of transfer has to do with case information that is available in the snippets but is missing in the query. The uncased tagger is insensitive to this mismatch and achieves significantly better results than the cased taggers. However, transferring information from the snippets provides additional benefits, significantly improving even the uncased baseline taggers. This is consistent with the analysis in <cite>Barr et al. (2008)</cite> . Finally, we see that the direct transfer method from Section 2 significantly outperforms the method described in Bendersky et al. (2010) .",
  "y": "similarities"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_5",
  "x": "For comparison, Bendersky et al. (2010) report 91.6% for their final system, which is comparable to our implementation of their system when the baseline tagger is trained on just the WSJ corpus. Our best system achieves a 21.2% relative reduction in error on their annotations. Some other trends become appar- ent in Table 2 . Firstly, a large part of the benefit of transfer has to do with case information that is available in the snippets but is missing in the query. The uncased tagger is insensitive to this mismatch and achieves significantly better results than the cased taggers. However, transferring information from the snippets provides additional benefits, significantly improving even the uncased baseline taggers. This is consistent with the analysis in <cite>Barr et al. (2008)</cite> .",
  "y": "similarities"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_6",
  "x": "Table 3 : Precision and recall of the NNP tag on the longtail data for the best baseline method and the three transfer methods using that baseline. 5 Related Work <cite>Barr et al. (2008)</cite> manually annotate a corpus of 2722 queries with 19 POS tags and use it to train and evaluate POS taggers, and also describe the linguistic structures they find. Unfortunately their data is not available so we cannot use it to compare to their results. R\u00fcd et al. (2011) create features based on search engine results, that they use in an NER system applied to queries. They report report significant improvements when incorporating features from the snippets. In particular, they exploit capitalization and query terms matching URL components; both of which we have used in this work. use clicks in a product data base to train a tagger for product queries, but they do not use snippets and do not annotate syntax.",
  "y": "background"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_7",
  "x": "In 32 cases, the errors in the query tagging could be traced back to errors in the snippet tagging. A better snippet tagger could alleviate that problem. In the remaining 8 cases there were problems with the matching -either the mis-tagged word was not found at all, or it was matched incorrectly. For example one of the results for the query \"bell helmet\" had a snippet containing \"Bell cycling helmets\" and we failed to match helmet to helmets. Table 3 : Precision and recall of the NNP tag on the longtail data for the best baseline method and the three transfer methods using that baseline. 5 Related Work <cite>Barr et al. (2008)</cite> manually annotate a corpus of 2722 queries with 19 POS tags and use it to train and evaluate POS taggers, and also describe the linguistic structures they find. Unfortunately their data is not available so we cannot use it to compare to their results.",
  "y": "background"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_0",
  "x": [
   "The projected target dependency parses are not always fully connected to be useful for training traditional dependency parsers. In this paper, we present a greedy non-directional parsing algorithm which doesn't need a fully connected parse and can learn from partial parses by utilizing available structural and syntactic information in them. Our parser achieved statistically significant improvements over a baseline system that trains on only fully connected parses for Bulgarian, Spanish and Hindi. It also gave a significant improvement over previously reported results for Bulgarian and set a benchmark for Hindi. ---------------------------------- **INTRODUCTION** Parallel corpora have been used to transfer information from source to target languages for Part-Of-Speech (POS) tagging, word sense disambiguation (Yarowsky et al., 2001) , syntactic parsing (Hwa et al., 2005; Ganchev et al., 2009; Jiang and Liu, 2010) and machine translation (Koehn, 2005; Tiedemann, 2002) ."
  ],
  "y": "background"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_1",
  "x": [
   "Equipped with a source language parser and a word alignment tool, parallel data can be used to build an automatic treebank for a target language. The parse trees given by the parser on the source sentences in the parallel data are projected onto the target sentence using the word alignments from the alignment tool. Due to the usage of automatic source parses, automatic word alignments and differences in the annotation schemes of source and target languages, the projected parses are not always fully connected and can have edges missing (Hwa et al., 2005; Ganchev et al., 2009 ). Nonliteral translations and divergences in the syntax of the two languages also lead to incomplete projected parse trees. Figure 1 shows an English-Hindi parallel sentence with correct source parse, alignments and target dependency parse. For the same sentence, Figure 2 is a sample partial dependency parse projected using an automatic source parser on aligned text. This parse is not fully connected with the words banaa, kottaige and dikhataa left without any parents."
  ],
  "y": "motivation background"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_2",
  "x": "Projectivity can be relaxed in some parsers (McDonald et al., 2005; Nivre, 2009) . But these parsers can not directly be used to learn from partially connected parses (Hwa et al., 2005; Ganchev et al., 2009 ). In the projected Hindi treebank (section 4) that was extracted from English-Hindi parallel text, only 5.9% of the sentences had full trees. In Spanish and Bulgarian projected data extracted by<cite> Ganchev et al. (2009)</cite> In this paper, we present a dependency parsing algorithm which can train on partial projected parses and can take rich syntactic information as features for learning. The parsing algorithm constructs the partial parses in a bottom-up manner by performing a greedy search over all possible relations and choosing the best one at each step without following either left-to-right or right-to-left traversal. The algorithm is inspired by earlier nondirectional parsing works of Shen and Joshi (2008) and Goldberg and Elhadad (2010) . We also propose an extended partial parsing algorithm that can learn from partial parses whose yields are partially contiguous.",
  "y": "motivation background"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_3",
  "x": "In Spanish and Bulgarian projected data extracted by<cite> Ganchev et al. (2009)</cite> In this paper, we present a dependency parsing algorithm which can train on partial projected parses and can take rich syntactic information as features for learning. The parsing algorithm constructs the partial parses in a bottom-up manner by performing a greedy search over all possible relations and choosing the best one at each step without following either left-to-right or right-to-left traversal. The algorithm is inspired by earlier nondirectional parsing works of Shen and Joshi (2008) and Goldberg and Elhadad (2010) . We also propose an extended partial parsing algorithm that can learn from partial parses whose yields are partially contiguous. Apart from bitext projections, this work can be extended to other cases where learning from partial structures is required. For example, while bootstrapping parsers high confidence parses are extracted and trained upon (Steedman et al., 2003; Reichart and Rappoport, 2007) . In cases where these parses are few, learning from partial parses might be beneficial.",
  "y": "background"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_4",
  "x": "Learning from partial parses has been dealt in different ways in the literature. Hwa et al. (2005) used post-projection completion/transformation rules to get full parse trees from the projections and train Collin's parser (Collins, 1999) on them. Ganchev et al. (2009) handle partial projected parses by avoiding committing to entire projected tree during training. The posterior regularization based framework constrains the projected syntactic relations to hold approximately and only in expectation. Jiang and Liu (2010) refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees. They deal with partial projections by breaking down the projected parse into a set of edges and training on the set of projected relations rather than on trees. While Hwa et al. (2005) requires full projected parses to train their parser,<cite> Ganchev et al. (2009)</cite> and Jiang and Liu (2010) can learn from partially projected trees.",
  "y": "motivation background"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_5",
  "x": "They deal with partial projections by breaking down the projected parse into a set of edges and training on the set of projected relations rather than on trees. While Hwa et al. (2005) requires full projected parses to train their parser,<cite> Ganchev et al. (2009)</cite> and Jiang and Liu (2010) can learn from partially projected trees. However, the discriminative training in<cite> (Ganchev et al., 2009</cite> ) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse. By treating each relation in the projected dependency data independently as a classification instance for parsing, Jiang and Liu (2010) sacrifice the context of the relations such as global structural context, neighboring relations that are crucial for dependency analysis. Due to this, they report that the parser suffers from local optimization during training. The parser proposed in this work (section 3) learns from partial trees by using the available structural information in it and also in neighboring partial parses. We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in<cite> (Ganchev et al., 2009</cite> ) for comparison.",
  "y": "motivation background"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_6",
  "x": "While Hwa et al. (2005) requires full projected parses to train their parser,<cite> Ganchev et al. (2009)</cite> and Jiang and Liu (2010) can learn from partially projected trees. However, the discriminative training in<cite> (Ganchev et al., 2009</cite> ) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse. By treating each relation in the projected dependency data independently as a classification instance for parsing, Jiang and Liu (2010) sacrifice the context of the relations such as global structural context, neighboring relations that are crucial for dependency analysis. Due to this, they report that the parser suffers from local optimization during training. The parser proposed in this work (section 3) learns from partial trees by using the available structural information in it and also in neighboring partial parses. We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in<cite> (Ganchev et al., 2009</cite> ) for comparison. The same could not be carried out for Chinese (which was the language (Jiang and Liu, 2010 ) worked on) due to the unavailability of projected data used in their work.",
  "y": "extends"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_7",
  "x": "**EXPERIMENTS** We carried out all our experiments on parallel corpora belonging to English-Hindi, EnglishBulgarian and English-Spanish language pairs. While the Hindi projected treebank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in<cite> (Ganchev et al., 2009)</cite> . The datasets of Bulgarian and Spanish that contributed to the best accuracies for<cite> Ganchev et al. (2009)</cite> were used in our work (7 rules dataset for Bulgarian and 3 rules dataset for Spanish). The Hindi, Bulgarian and Spanish projected dependency treebanks have 44760, 39516 and 76958 sentences respectively. Since we don't have confidence scores for the projections on the sentences, we picked 10,000 sentences randomly in each of the three datasets for training the parsers 2 . Other methods of choosing the 10K sentences such as those with the max.",
  "y": "uses"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_8",
  "x": "While the Hindi projected treebank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in<cite> (Ganchev et al., 2009)</cite> . The datasets of Bulgarian and Spanish that contributed to the best accuracies for<cite> Ganchev et al. (2009)</cite> were used in our work (7 rules dataset for Bulgarian and 3 rules dataset for Spanish). The Hindi, Bulgarian and Spanish projected dependency treebanks have 44760, 39516 and 76958 sentences respectively. Since we don't have confidence scores for the projections on the sentences, we picked 10,000 sentences randomly in each of the three datasets for training the parsers 2 . Other methods of choosing the 10K sentences such as those with the max. no. of relations, those with least no. of unconnected words, those with max. no. of contiguous partial trees that can be learned by GNPPA parser etc. were tried out.",
  "y": "uses"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_9",
  "x": "\u2020 denotes significance over GNPPA centage that satisfies the partially contiguous constraint. E-GNPPA parser learns around 2-5% more no. of relations than GNPPA due to the relaxation in the constraints. The Hindi test data that was released as part of the ICON-2010 Shared Task (Husain et al., 2010) was used for evaluation. For Bulgarian and Spanish, we used the same test data that was used in the work of<cite> Ganchev et al. (2009)</cite> . These test datasets had sentences from the training section of the CoNLL Shared Task (Nivre et al., 2007) that had lengths less than or equal to 10. All the test datasets have gold POS tags. A baseline parser was built to compare learning from partial parses with learning from fully connected parses.",
  "y": "uses background"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_0",
  "x": "Emojis are the evolution of characterbased emoticons (Pavalanathan and Eisenstein, 2015) , and are extensively used, not only as sentiment carriers or boosters, but more importantly, to express ideas about a myriad of topics, e.g., mood ( ), food ( ), sports ( ) or scenery ( ). Emoji modeling and prediction is, therefore, an important problem towards the end goal of properly capturing the intended meaning of a social media message. In fact, emoji prediction, i.e., given a (usually short) message, predict its most likely associated emoji(s), may help to improve different NLP tasks (Novak et al., 2015) , such as information retrieval, generation of emojienriched social media content or suggestion of emojis when writing text messages or sharing pictures online. It has furthermore proven to be useful for sentiment analysis, emotion recognition and irony detection <cite>(Felbo et al., 2017)</cite> . The problem of emoji prediction, albeit recent, has already seen important developments. For example, Barbieri et al. (2017) describe an LSTM model which outperforms a logistic regression baseline based on word vector averaging, and even human judgement in some scenarios. The above contributions, in addition to emoji similarity datasets (Barbieri et al., 2016; Wijeratne et al., 2017) or emoji sentiment lexicons (Novak et al., 2015; Wijeratne et al., 2016; Kimura and Katsurai, 2017; Rodrigues et al., 2018) , have paved the way for better understanding the semantics of emojis.",
  "y": "background"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_1",
  "x": "We observed a performance improvement over competitive baselines such as FastText (FT) (Joulin et al., 2017) and Deepmoji <cite>(Felbo et al., 2017)</cite> , which is most noticeable in the case of infrequent emojis. This suggests that an attentive mechanism can be leveraged to make neural architectures more sensitive to instances of underrepresented classes. ---------------------------------- **METHODOLOGY** Our base architecture is the Deepmoji model <cite>(Felbo et al., 2017)</cite> , which is based on two stacked word-based bi-directional LSTM recurrent neural networks with skip connections between the first and the second LSTM. The model also includes an attention module to increase its sensitivity to individual words during prediction. In general, attention mechanisms allow the model to focus on specific words of the input (Yang et al., 2016) , instead of having to memorize all the important features in a fixed-length vector.",
  "y": "differences"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_2",
  "x": "Our base architecture is the Deepmoji model <cite>(Felbo et al., 2017)</cite> , which is based on two stacked word-based bi-directional LSTM recurrent neural networks with skip connections between the first and the second LSTM. The model also includes an attention module to increase its sensitivity to individual words during prediction. In general, attention mechanisms allow the model to focus on specific words of the input (Yang et al., 2016) , instead of having to memorize all the important features in a fixed-length vector. The main architectural difference with respect to the typical attention is illustrated in Figure 1 . In<cite> Felbo et al. (2017)</cite> , attention is computed as follows: Here h i \u2208 R d is the hidden representation of the LSTM corresponding to the i th word, with N the total number of words in the sentence. The weight vector w a \u2208 R d and bias term b a \u2208 R map this hidden representation to a value that reflects the importance of this state for the considered classification problem.",
  "y": "uses"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_3",
  "x": "Our base architecture is the Deepmoji model <cite>(Felbo et al., 2017)</cite> , which is based on two stacked word-based bi-directional LSTM recurrent neural networks with skip connections between the first and the second LSTM. The model also includes an attention module to increase its sensitivity to individual words during prediction. In general, attention mechanisms allow the model to focus on specific words of the input (Yang et al., 2016) , instead of having to memorize all the important features in a fixed-length vector. The main architectural difference with respect to the typical attention is illustrated in Figure 1 . In<cite> Felbo et al. (2017)</cite> , attention is computed as follows: Here h i \u2208 R d is the hidden representation of the LSTM corresponding to the i th word, with N the total number of words in the sentence. The weight vector w a \u2208 R d and bias term b a \u2208 R map this hidden representation to a value that reflects the importance of this state for the considered classification problem.",
  "y": "background"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_4",
  "x": "Our base architecture is the Deepmoji model <cite>(Felbo et al., 2017)</cite> , which is based on two stacked word-based bi-directional LSTM recurrent neural networks with skip connections between the first and the second LSTM. The model also includes an attention module to increase its sensitivity to individual words during prediction. In general, attention mechanisms allow the model to focus on specific words of the input (Yang et al., 2016) , instead of having to memorize all the important features in a fixed-length vector. The main architectural difference with respect to the typical attention is illustrated in Figure 1 . In<cite> Felbo et al. (2017)</cite> , attention is computed as follows: Here h i \u2208 R d is the hidden representation of the LSTM corresponding to the i th word, with N the total number of words in the sentence. The weight vector w a \u2208 R d and bias term b a \u2208 R map this hidden representation to a value that reflects the importance of this state for the considered classification problem.",
  "y": "uses"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_5",
  "x": "These extended experiments are performed on a corpus of around 100M tweets geolocalized in the United States and posted between October 2015 and May 2018. Models. In order to put our proposed labelwise attention mechanism in context, we compare its performance with a set of baselines: (1 <cite>(Felbo et al., 2017)</cite> . Finally, we denote as 2-BiLSTMs l our proposed label-wise attentive Bi-LSTM architecture. Results. Table 1 shows the results of our model and the baselines in the emoji prediction task for the different evaluation splits. The evaluation metrics used are: F1, Accuracy@k (A@k, where k \u2208 {1, 5}), and Coverage Error (CE 1 ) (Tsoumakas et al., 2009) .",
  "y": "uses"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_0",
  "x": "Since CWS was firstly treated as a characterbased tagging task (we call it \"CT\" for short hereafter) in (Xue and Converse, 2002) , this method has been widely accepted and further developed by researchers (Peng et al., 2004) , (Tseng et al., 2005) , (Low et al., 2005) , (Zhao et al., 2006) . Relatively to dictionary-based segmentation (we call it \"DS\" for short hereafter), CT method can achieve a higher accuracy on OOV word recognition and a better performance of segmentation in whole. Thus, CT has drawn more and more attention and became the dominant method in the Bakeoff 2005 and 2006. Although CT has shown its merits in word segmentation task, some researchers still hold the belief that on IV words DS can perform better than CT even in the restriction of Bakeoff closed test. Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005) , <cite>(Zhang et al., 2006a)</cite> . Among these strategies, the confidence measure used to combine the results of CT and DS is a straight-forward one, which is introduced in <cite>(Zhang et al., 2006a)</cite> . The basic assumption of such combination is that DS method performs better on IV words and <cite>Zhang derives</cite> this belief from the fact that DS achieves higher IV recall rate as Table 1 shows.",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_1",
  "x": "We give a detailed explanation about this issue as following. Since CWS was firstly treated as a characterbased tagging task (we call it \"CT\" for short hereafter) in (Xue and Converse, 2002) , this method has been widely accepted and further developed by researchers (Peng et al., 2004) , (Tseng et al., 2005) , (Low et al., 2005) , (Zhao et al., 2006) . Relatively to dictionary-based segmentation (we call it \"DS\" for short hereafter), CT method can achieve a higher accuracy on OOV word recognition and a better performance of segmentation in whole. Thus, CT has drawn more and more attention and became the dominant method in the Bakeoff 2005 and 2006. Although CT has shown its merits in word segmentation task, some researchers still hold the belief that on IV words DS can perform better than CT even in the restriction of Bakeoff closed test. Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005) , <cite>(Zhang et al., 2006a)</cite> . Among these strategies, the confidence measure used to combine the results of CT and DS is a straight-forward one, which is introduced in <cite>(Zhang et al., 2006a)</cite> .",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_2",
  "x": "Thus, CT has drawn more and more attention and became the dominant method in the Bakeoff 2005 and 2006. Although CT has shown its merits in word segmentation task, some researchers still hold the belief that on IV words DS can perform better than CT even in the restriction of Bakeoff closed test. Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005) , <cite>(Zhang et al., 2006a)</cite> . Among these strategies, the confidence measure used to combine the results of CT and DS is a straight-forward one, which is introduced in <cite>(Zhang et al., 2006a)</cite> . The basic assumption of such combination is that DS method performs better on IV words and <cite>Zhang derives</cite> this belief from the fact that DS achieves higher IV recall rate as Table 1 shows. In which AS, CityU, MSRA and PKU are four corpora used in Bakeoff 2005 (also see Table 2 for detail). We provide a more detailed evaluation metric to analyze these two methods, including precision and F measure of IV and OOV respectively and our experiments show that CT outperforms DS on both IV and OOV words within Bakeoff closed test.",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_3",
  "x": "The basic assumption of such combination is that DS method performs better on IV words and <cite>Zhang derives</cite> this belief from the fact that DS achieves higher IV recall rate as Table 1 shows. In which AS, CityU, MSRA and PKU are four corpora used in Bakeoff 2005 (also see Table 2 for detail). We provide a more detailed evaluation metric to analyze these two methods, including precision and F measure of IV and OOV respectively and our experiments show that CT outperforms DS on both IV and OOV words within Bakeoff closed test. The precision and F measure are existing metrics and the definitions of them are clear. Here we just employ them to evaluate segmentation results. Furthermore, our error analysis on the results of combination reveals that confidence measure in <cite>(Zhang et al., 2006a)</cite> has a representation flaw and we propose an EIV tag method to revise it. Finally, we give an empirical comparison between existing pure CT method and combination, which shows that pure CT method can produce state-of-the-art results on both IV word and overall segmentation.",
  "y": "motivation"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_6",
  "x": "At each stage, the next incoming character is combined with an existing candidate in two different ways to generate new candidates: it is either appended to the last word in the candidate, or taken as the start of a new word. This method guarantees exhaustive generation of possible segmentations for any input sentence. However, the exponential time and space of the length of the input sentence are needed for such a search and it is always intractable in practice. Thus, we use the trigram language model to select top B (B is a constant predefined before search and in our experiment 3 is used) best candidates with highest probability at each stage so that the search algorithm can work in practice. Finally, when the whole sentence has been read, the best candidate with the highest probability will be selected as the segmentation result. Here, the term \"dictionary-based\" is exactly the method implemented in <cite>(Zhang et al., 2006a)</cite> , it does not mean the generative language model in general. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_7",
  "x": "Here, the term \"dictionary-based\" is exactly the method implemented in <cite>(Zhang et al., 2006a)</cite> , it does not mean the generative language model in general. ---------------------------------- **CHARACTER-BASED TAGGING** Under CT scheme, each character in one sentence is labeled as \"B\" if it is the beginning of a word, \"O\" tag means the current character is a single-character word, other character is labeled as \"I\". For example, \"\u5168\u4e2d\u56fd (whole China)\" is labeled as \" In <cite>(Zhang et al., 2006a)</cite> , the above CT method is developed as subword-based tagging. First, the most frequent multi-character words and all single characters in training corpus are collected as subwords.",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_10",
  "x": "Thus, in order to judge the results of each experiment, a more detailed evaluation with precision and F measure of both IV and OOV words included is used. To calculate the IV and OOV precision and recall, we firstly divide words of the segmenter\"s output and gold data into IV word and OOV word sets respectively with the dictionary collected from the training corpus. Then, for IV and OOV word sets respectively, the IV (or OOV) recall is the proportion of the correctly segmented IV (or OOV) word tokens to all IV (or OOV) word tokens in the gold data, and IV (or OOV) precision is the proportion of the correctly segmented IV (or OOV) word tokens to all IV (or OOV) word tokens in the segmenter\"s output. One thing have to be emphasized is that the single character in test corpus will be defined as OOV if it does not appear in training corpus. We will see later in this section, by this evaluation, some facts covered by the bakeoff evaluation can be illustrated by our new evaluation metric. Here, we repeat two experiments described in <cite>(Zhang et al., 2006a)</cite> , namely dictionary-based approach and subword-based tagging. For CT method, top 2000 most frequent multi-character words and all single characters in training corpus are selected as subwords and the feature templates used for CRF model is listed in Table 3 .",
  "y": "uses"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_11",
  "x": "One thing have to be emphasized is that the single character in test corpus will be defined as OOV if it does not appear in training corpus. We will see later in this section, by this evaluation, some facts covered by the bakeoff evaluation can be illustrated by our new evaluation metric. Here, we repeat two experiments described in <cite>(Zhang et al., 2006a)</cite> , namely dictionary-based approach and subword-based tagging. For CT method, top 2000 most frequent multi-character words and all single characters in training corpus are selected as subwords and the feature templates used for CRF model is listed in Table 3 . We present all the segmentation results in Table  6 to see the strength and weakness of each method conveniently. Based on IV and OOV recall as we show in Table 1 , <cite>Zhang argues</cite> that the DS performs better on IV word identification while CT performs better on OOV words. But we can see from the results in Table 6 (the lines about DS and CT), the IV precision of DS approach is much lower than that of CT on all the four corpora, which also causes a lower F measure of IV. The reason for low IV precision of DS is that many OOV words are segmented into two IV words by DS.",
  "y": "differences"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_12",
  "x": "We choose the confidence measure to study because it is straight-forward. We show in this section that there is a representation flaw in the formula of confidence measure in <cite>(Zhang et al., 2006a )</cite>. And we propose an \"EIV\" tag method to solve this problem. Our experiments show that confidence measure with EIV tag outperforms CT and DS alone. ---------------------------------- **CONFIDENCE MEASURE** Confidence Measure (CM) means to seek an optimal tradeoff between performance on IV and OOV words.",
  "y": "motivation"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_13",
  "x": "Confidence Measure (CM) means to seek an optimal tradeoff between performance on IV and OOV words. The basic idea of CM comes from the belief that CT performs better on OOV words while DS performs better on IV words. When both results of CT and DS are available, the CM can be calculated according to the following formula in <cite>(Zhang et al., 2006a)</cite> : Here, w is a subword, iob t is \"IOB\" tag given by CT and w t is \"IOB\" tag generated by DS. In the first term of the right hand side of the formula, is the marginal probability of iob t (we call this marginal probability \"MP\" for short will be kept, otherwise it will be replaced with w t . Thus, the CM ultimately is the marginal probability of the \"IOB\" tag (MP).",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_14",
  "x": "In the first term of the right hand side of the formula, is the marginal probability of iob t (we call this marginal probability \"MP\" for short will be kept, otherwise it will be replaced with w t . Thus, the CM ultimately is the marginal probability of the \"IOB\" tag (MP). In the experiment of this paper, MP is used as CM because it is equivalent to <cite>Zhang\"s CM</cite> but more convenient to express. ---------------------------------- **EXPERIMENTS AND ERROR ANALYSIS ABOUT COMBINATION** We repeat the experiments about CM in <cite>Zhang\"s paper</cite> <cite>(Zhang et al., 2006a)</cite> and show that there is a representation flaw in the CM formula.",
  "y": "similarities"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_15",
  "x": "We repeat the experiments about CM in <cite>Zhang\"s paper</cite> <cite>(Zhang et al., 2006a)</cite> and show that there is a representation flaw in the CM formula. Furthermore, we propose an EIV tag method to make CM yield a better result. In this paper, \uf061 = 0.8 and t = 0.7 (Parameters in two papers, <cite>Zhang et al. 2006a</cite> and Zhang et al. 2006b , are different. And our parameters are consistent with Zhang et al. 2006b which is confirmed by Dr Zhang through email) are used in CM, namely MP= 0.875 is the threshold. Here, in Table 4 , we provide some statistics on the results of CT when MP is less than 0.875. From Table 4 we can see that even with MP less than 0.875, most of the subwords are still tagged correctly by CT and should not be revised by DS result. Besides, lots of the subwords with low MP contained by OOV words in test data, especially for the corpus whose OOV rate is high (i.e. on CityU corpus more than one third subwords with low MP belong to OOV word) and performance on OOV recognition is the advantage of CT rather than that of DS approach. Thus when combining the results of the two methods, it is the iob t should be maintained if the subword is contained by an OOV word.",
  "y": "uses motivation"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_16",
  "x": "In the experiment of this paper, MP is used as CM because it is equivalent to <cite>Zhang\"s CM</cite> but more convenient to express. ---------------------------------- **EXPERIMENTS AND ERROR ANALYSIS ABOUT COMBINATION** We repeat the experiments about CM in <cite>Zhang\"s paper</cite> <cite>(Zhang et al., 2006a)</cite> and show that there is a representation flaw in the CM formula. Furthermore, we propose an EIV tag method to make CM yield a better result. In this paper, \uf061 = 0.8 and t = 0.7 (Parameters in two papers, <cite>Zhang et al. 2006a</cite> and Zhang et al. 2006b , are different. And our parameters are consistent with Zhang et al. 2006b which is confirmed by Dr Zhang through email) are used in CM, namely MP= 0.875 is the threshold. Here, in Table 4 , we provide some statistics on the results of CT when MP is less than 0.875.",
  "y": "differences"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_17",
  "x": "Therefore, the CM formula seems somewhat unreasonable. The error analysis about how many original errors are eliminated and how many new errors are introduced by CM is provided in Table 5 (the columns about CM). Table 5 illustrates that, after combining the two results, most original errors on IV words are corrected because DS can achieve higher IV recall as described in <cite>Zhang\"s paper</cite>. But on OOV part, more new errors are introduced by CM and these new errors decrease the precision of the IV words. For example, the OOV words \"\u8b66\u536b\u961f\u5458 (guard member)\" and \" \u8bbe\u8ba1\u8d39 (design fee)\" is recognized correctly by CT but with low CM. In the combining procedure, these words are wrongly split as IV errors: \"\u8b66\u536b (guard) \u961f\u5458 (member)\" and \"\u8bbe\u8ba1 (design) \u8d39 (fee)\". Thus, for two corpora (i.e. CityU and AS), F measure of IV and overall F measure decreases since there are more new errors introduced than original ones eliminated and only on the other two corpora (MSRA and PKU), overall F measure of combination method is higher than CT alone, which is shown in Table 6 by the lines about combination. ----------------------------------",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_18",
  "x": "After this procedure, when combining Table 5 Error analysis of confidence measure with and without EIV tag the two results, only the CT tag with EIV tags and low MP will be replaced by DS tag, otherwise the original CT tag will be maintained. Under this condition the errors introduced by OOV will not happen and enhanced results are listed in Table 6 lines about EIV. We can see that on all four corpora the overall F measure of EIV result is higher than that of CT alone, which show that our EIV method works well. Now, let\"s check what changes happened in the number of error tags after EIV condition added into the CM. We can see from the Table 5 columns about EIV, there are more errors eliminated than the new errors introduced after EIV condition added into CM and most CT tags of subwords contained in OOV words maintained unchanged as we supposed. And then, our results (in Table  6 lines about EIV) are comparable with that in <cite>Zhang\"s paper</cite>. Thus, there may be some similar strategies in <cite>Zhang\"s CM</cite> too but not presented in <cite>Zhang\"s paper</cite>. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_20",
  "x": "We can see from the Table 6 this pure CT approach achieves the state-of-the-art results on all the corpora. On three of the four corpora (AS, MSRA and PKU) this pure CT method gets the best result. Even on IV word, this pure CT approach outperforms <cite>Zhang\"s CT method</cite> and produces comparable results with combination with EIV tags, which shows that pure CT method can perform well on IV words too. Moreover, this character-based tagging approach is more clear and simple than the confidence measure method. Although character-based tagging became mainstream approach in the last two Bakeoffs, it does not mean that word information is valueless in Chinese word segmentation. A word-based perceptron algorithm is proposed recently (Zhang and Clark, 2007) , which views Chinese word segmentation task from a new angle instead of character-based tagging and gets comparable results with the best results of Bakeoff. Table 6 Results of different approach used in our experiments (White background lines are the results we repeat <cite>Zhang\"s methods</cite> and they have some trivial difference with Table 1. ) Therefore, the most important thing worth to pay attention in future study is how to integrate linguistic information into the statistical model effectively, no matter character or word information.",
  "y": "differences"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_21",
  "x": "On three of the four corpora (AS, MSRA and PKU) this pure CT method gets the best result. Even on IV word, this pure CT approach outperforms <cite>Zhang\"s CT method</cite> and produces comparable results with combination with EIV tags, which shows that pure CT method can perform well on IV words too. Moreover, this character-based tagging approach is more clear and simple than the confidence measure method. Although character-based tagging became mainstream approach in the last two Bakeoffs, it does not mean that word information is valueless in Chinese word segmentation. A word-based perceptron algorithm is proposed recently (Zhang and Clark, 2007) , which views Chinese word segmentation task from a new angle instead of character-based tagging and gets comparable results with the best results of Bakeoff. Table 6 Results of different approach used in our experiments (White background lines are the results we repeat <cite>Zhang\"s methods</cite> and they have some trivial difference with Table 1. ) Therefore, the most important thing worth to pay attention in future study is how to integrate linguistic information into the statistical model effectively, no matter character or word information. ----------------------------------",
  "y": "uses differences"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_22",
  "x": "A word-based perceptron algorithm is proposed recently (Zhang and Clark, 2007) , which views Chinese word segmentation task from a new angle instead of character-based tagging and gets comparable results with the best results of Bakeoff. Table 6 Results of different approach used in our experiments (White background lines are the results we repeat <cite>Zhang\"s methods</cite> and they have some trivial difference with Table 1. ) Therefore, the most important thing worth to pay attention in future study is how to integrate linguistic information into the statistical model effectively, no matter character or word information. ---------------------------------- **CONCLUSIONS AND FUTURE WORK** In this paper, we first provided a detailed evaluation metric, which provides the necessary information to judge the performance of each method on IV and OOV word identification. Second, by this evaluation metric, we show that characterbased tagging outperforms dictionary-based segmentation not only on OOV words but also on IV words within Bakeoff closed tests. Furthermore, our experiments show that confidence measure in <cite>Zhang\"s paper</cite> has a representation flaw and we propose an EIV tag method to revise the combination.",
  "y": "motivation"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_0",
  "x": "However, convolutional models must be significantly deeper to retrieve the same temporal receptive field [23] . Recently, the mechanism of self-attention<cite> [22,</cite> 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time. Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer<cite> [22]</cite> ), sentiment analysis [25] , and other tasks. These successes have motivated preliminary work in self-attention for ASR. Time-restricted self-attention was used as a drop-in replacement for individual layers in the state-of-theart lattice-free MMI model [26] , an HMM-NN system. Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) [27] , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems. In this work, we propose and evaluate fully self-attentional networks for CTC (SAN-CTC).",
  "y": "background"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_1",
  "x": "**** later works proposed partially-or purely-convolutional CTC models [8] [9] [10] [11] and convolution-heavy encoder-decoder models [16] for ASR. However, convolutional models must be significantly deeper to retrieve the same temporal receptive field [23] . Recently, the mechanism of self-attention<cite> [22,</cite> 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time. Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer<cite> [22]</cite> ), sentiment analysis [25] , and other tasks. These successes have motivated preliminary work in self-attention for ASR. Time-restricted self-attention was used as a drop-in replacement for individual layers in the state-of-theart lattice-free MMI model [26] , an HMM-NN system.",
  "y": "background"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_2",
  "x": "---------------------------------- **MOTIVATING THE SELF-ATTENTION LAYER** We now replace recurrent and convolutional layers for CTC with self-attention [24] . Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder<cite> [22]</cite> , previous explorations of self-attention in ASR [19, 27] , and defined in Section 2.3. The other stages are downsampling, which reduces input length T via methods like those in Section 2.4; embedding, which learns a dh-dim. embedding that also describes token position (Section 2.5); and projection, where each final representation is mapped framewise to logits over the intermediate alphabet L . The first implements self-attention, where the success of attention in CTC and encoder-decoder models [14, 31] is parallelized by using each position's representation to attend to all others, giving a contextualized representation for that position. Hence, the full receptive field is immediately available at the cost of O(T 2 ) inner products (Table 1) , enabling richer representations in fewer layers.",
  "y": "similarities uses"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_3",
  "x": "Hence, the full receptive field is immediately available at the cost of O(T 2 ) inner products (Table 1) , enabling richer representations in fewer layers. ---------------------------------- **MODEL** Operations per layer ---------------------------------- **SEQUENTIAL OPERATIONS** Maximum path length Table 1 : Operation complexity of each layer type, based on<cite> [22]</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_4",
  "x": "In particular, multi-head attention is akin to having a number of infinitely-wide filters whose weights adapt to the content (allowing fewer \"filters\" to suffice). One can also assign interpretations; for example, [27] argue their LAS self-attention heads are differentiated phoneme detectors. Further inductive biases like filter widths and causality could be expressed through time-restricted self-attention [26] and directed self-attention [25] , respectively. ---------------------------------- **FORMULATION** Let H \u2208 R T \u00d7d h denote a sublayer's input. The first sublayer performs multi-head, scaled dot-product, self-attention<cite> [22]</cite> .",
  "y": "background"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_5",
  "x": "The first sublayer performs multi-head, scaled dot-product, self-attention<cite> [22]</cite> . For each head i of nhds, we learn linear maps W , and values V (i) of the i-th head, which combine to give where \u03c3 is row-wise softmax. Heads are concatenated along the dh/nhds axis to give MltHdAtt = [HdAtt (1) , . . . , HdAtt (n hds ) ]. The second sublayer is a position-wise feed-forward network<cite> [22]</cite> FFN(H) = ReLU(HW1 + b1)W2 + b2 where parameters with the biases b1, b2 broadcasted over all T positions.",
  "y": "background"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_6",
  "x": "**POSITION** Self-attention is inherently content-based<cite> [22]</cite> , and so one often encodes position into the post-embedding vectors. We use standard trigonometric embeddings, where for 0 \u2264 i \u2264 demb/2, we define for position t. We consider three approaches: content-only [21] , which forgoes position encodings; additive [19] , which takes demb = dh and adds the encoding to the embedding; and concatenative, where one takes demb = 40 and concatenates it to the embedding. The latter was found necessary for self-attentional LAS [27] , as additive encodings did not give convergence. However, the monotonicity of CTC is a further positional inductive bias, which may enable the success of content-only and additive encodings. ----------------------------------",
  "y": "background"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_7",
  "x": "Our self-attention code is based on GluonNLP's implementation. At train time, utterances are sorted by length: we exclude those longer than 1800 frames ( 1% of each training set). We take a window of 25ms, a hop of 10ms, and concatenate cepstral mean-variance normalized features with temporal first-and second-order differences. 1 We downsample by a factor of k = 3 (this also gave an ideal T /k \u2248 dh for our data; see Table 1 ). We perform Nesterov-accelerated gradient descent on batches of 20 utterances. As self-attention architectures can be unstable in early training, we clip gradients to a global norm of 1 and use the standard linear warmup period before inverse square decay associated with these architectures [19, <cite>22]</cite> . Let n denote the global step number of the batch (across epochs); the learning rate is given by",
  "y": "extends differences"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_0",
  "x": "Recently, deep neural network (DNN) has shown to provide good results for modeling acoustic and textual information for emotion identification. In [5] , textual and acoustic information of the utterance are used by a DNN to obtain hidden feature representations for both the modality. These features are then concatenated to represent the utterance and subsequently used to classify the emotion of the speaker. Experimental evidence shows the potential of the approach. In our previous work <cite>[7]</cite> , we applied a dual RNN in order to obtain a richer representation by blending the content and acoustic knowledge. In this paper, we improve upon our earlier work by incorporating an attention mechanism in the emotion recognition framework. The proposed attention mechanism is trained to exploit both textual and acoustic information in tandem.",
  "y": "background"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_1",
  "x": "In [5] , textual and acoustic information of the utterance are used by a DNN to obtain hidden feature representations for both the modality. These features are then concatenated to represent the utterance and subsequently used to classify the emotion of the speaker. Experimental evidence shows the potential of the approach. In our previous work <cite>[7]</cite> , we applied a dual RNN in order to obtain a richer representation by blending the content and acoustic knowledge. In this paper, we improve upon our earlier work by incorporating an attention mechanism in the emotion recognition framework. The proposed attention mechanism is trained to exploit both textual and acoustic information in tandem. We refer to this attention method as the multi-hop.",
  "y": "extends"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_2",
  "x": "Recently,<cite> [7,</cite> 18] combined acoustic information and conversation transcripts using a neural network-based model to improve emotion classification accuracy. However, none of these studies utilized attention method over audio and text modality in tandem for contextual understanding of the emotion in audio recording. ---------------------------------- **MODEL** This section describes the methodologies that are applied to the speech emotion recognition task. We start by introducing a baseline model, the bidirectional recurrent encoder, for encoding the audio and text modalities individually. We then propose an approach to exploit both audio and text data in tandem.",
  "y": "background"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_3",
  "x": "We then propose an approach to exploit both audio and text data in tandem. In this technique, multihop attention is proposed to obtain relevant parts of audio and text data automatically. ---------------------------------- **BIDIRECTIONAL RECURRENT ENCODER** Motivated by the architecture used in<cite> [7,</cite> 17, 19] , we train a recurrent encoder to predict the categorical class of a given audio signal. To model the sequential nature of the speech signal, we use a bidirectional recurrent encoder (BRE) as shown in the Figure 1 (a). We also added a residual connection to the model for promoting convergence during training [20] .",
  "y": "motivation"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_4",
  "x": "The hidden representations ( \u2212 \u2192 h t, \u2190 \u2212 h t) from forward/backward LSTMs are concatenated for produce the feature, ot. To follow previous research <cite>[7]</cite> , we also add another prosodic feature vector, p, with each ot to generate a more informative vector representation of the signal, o A t . Finally, an emotion class is predicted from the acoustic signal by applying a softmax function to the final hidden representation at the last time step, o A last . We refer this model as audio-BRE with the objective function as follows: where yi,c is the true label vector, and\u0177i,c is the predicted probability distribution from the softmax layer. The W and the bias b are learned model parameters. C is the total number of classes, and N is the total number of samples used in training.",
  "y": "uses"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_5",
  "x": "We refer this model as text-BRE. The training objective for the text-BRE is same as the audio-BRE in equation (2). ---------------------------------- **PROPOSED MULTI-HOP ATTENTION** We propose a novel multi-hop attention method to predict the importance of audio and text, referred to multi-hop attention (MHA). Figure 1 shows the architecture of the proposed MHA model. Previous research used multi-modal information independently using neural network model by concatenating features from each modality<cite> [7,</cite> 21] .",
  "y": "background"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_6",
  "x": "Previous research used multi-modal information independently using neural network model by concatenating features from each modality<cite> [7,</cite> 21] . As opposed to this approach, we propose a neural network architecture that exploits information in each modality by extracting relevant segments of the speech data using information from the lexical content (and vice-versa). First, the acoustic and textual data are encoded with the audio-BRE and text-BRE, respectively, using equation (1). We then consider the final hidden representation of audio-BRE, o A last , as a context vector and apply attention method to the textual sequence, o T t . As this model is developed with a single attention method, we refer to the model as MHA-1. The final hidden representation of the MHA-1 model, H, is calculated as follows: The H 1 (equation 3) is a new hidden representation for textual information with consideration of audio modality.",
  "y": "differences"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_7",
  "x": "**DATASET AND EXPERIMENTAL SETUP** To train and evaluate our model, we use the Interactive Emotional Dyadic Motion Capture (IEMOCAP) [8] dataset, which includes five sessions of utterances between two speakers (one male and one female). Total 10 unique speakers participated in this work. For consistent comparison with previous works<cite> [7,</cite> 18] , all utterances labeled \"excitement\" are merged with those labeled \"happiness\". We assign single categorical emotion to the utterance with majority of annotators agreed on the emotion labels. The final dataset contains 5,531 utterances in total (1,636 happy, 1,084 sad, 1,103 angry and 1,708 neutral). In the training process, we perform 10-fold cross-validation where each 8, 1, 1 folds are used for the train set, development set, and test set, respectively.",
  "y": "uses"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_8",
  "x": "**FEATURE EXTRACTION AND IMPLEMENTATION DETAILS** As this research is extended work from previous research <cite>[7]</cite> , we use the same feature extraction method as done in our previous work. After extracting 40-dimensional Mel-frequency cepstral coefficients (MFCC) feature (frame size is set to 25 ms at a rate of 10 ms with the Hamming window) using Kaldi [22] , we concatenate it with its first, second order derivates, making the feature dimension to 120. We also extract prosodic features by using OpenSMILE toolkit [23] and appending it to the audio feature vector. In preparing the textual dataset, we first use the ground-truth transcripts of the IEMOCAP dataset. In a practical scenario where we may not access to transcripts of the audio, we obtain all of the transcripts from the speech signal using a commercial ASR system [24] (The performance of the ASR system is word error rate (WER) of 5.53%). We apply word-tokenizer to the transcripts and obtain sequential data for textual input.",
  "y": "extends"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_9",
  "x": "We minimize the cross-entropy loss function using (equation (2)) the Adam optimizer [26] with a learning rate of 1e-3 and gradients clipped with a norm value of 1. For the purposes of regularization, we apply the dropout method, 30%. The number of hidden units and the number of layers in the RNN for each model (BRE and MHA) are optimized on the development set. ---------------------------------- **PERFORMANCE EVALUATION** To measure the performance of systems, we report the weighted accuracy (WA) and unweighted accuracy (UA) averaging over the 10-fold cross-validation experiments. We use the same dataset and features as other researchers<cite> [7,</cite> 18] .",
  "y": "uses"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_10",
  "x": "The \"A\" and \"T\" in modality indicate \"Audio\" and \"Text\", receptively. (MHA-2-ASR) outperforms the best baseline system (MDRE) by 1.6% relative (0.718 to 0.730) in terms of WA. Figure 2 shows the confusion matrices of the proposed systems. In audio-BRE ( Fig. 2(a) ), most of the emotion labels are frequently misclassified as neutral class, supporting the claims of<cite> [7,</cite> 25] . The text-BRE shows improvement in classifying most of the labels in Fig. 2(b) . In particular, angry and happy classes are correctly classified by 32% (57.14 to 75.41) and 63% (40.21 to 65.56) relative in accuracy with respect to audio-BRE, receptively. However, it incorrectly predicted instances of the happy class as sad class in 10% of the time, even though these emotional states are opposites of one another.",
  "y": "similarities"
 },
 {
  "id": "0fd87fbdbe64e7d002ca31783448fb_0",
  "x": "Different from the traditional framework, Mappa Mundi offers opportunities to enable AI and artist together during art making. Mappa Mundi enables voice input from human and extracts the keywords for vivid expansion on myriad levels of ---------------------------------- **RELATED WORK AND CHALLENGES** Nowadays, AI demonstrates stronger potential for art creation. Many researches have been conducted to involve AI into poem generation [Zhang and Lapata, 2014; Cheng et al., 2018] , creation of classical or pop music<cite> [Manzelli et al., 2018</cite>; Hadjeres et al., 2017] and automatic images generation [van den Oord et al., 2016; Yan et al., 2016; Xu et al., 2018] . Whereas there are few researches exploring the possibility of artificial imagination for artwork creation.",
  "y": "background"
 },
 {
  "id": "0fd87fbdbe64e7d002ca31783448fb_1",
  "x": "Automatic Speech Recognition (ASR) engine is designed for interaction. Both English and Chinese languages are supported. We also performed lexicon and language model adaption to enhance the recognition of words in art domain. After that, we developed a keyword extraction function to extract meaningful words from the converted text for further expansion. We adopted open-source software jieba 2 for Chinese and Stanford parser<cite> [Toutanova and Manning, 2000]</cite> for English POS tagging. Only informative words such as noun, verb and adjective words are kept and TFIDF weights are calculated for further filtering. The output of this module is the keywords.",
  "y": "uses"
 },
 {
  "id": "0fd87fbdbe64e7d002ca31783448fb_2",
  "x": "Only informative words such as noun, verb and adjective words are kept and TFIDF weights are calculated for further filtering. The output of this module is the keywords. ---------------------------------- **TOPIC EXPANSION** As imagination is the soul for artistic Mind Map, Mappa Mundi employs several features to increase information variety<cite> [Liu et al., 2019]</cite> during topic expansion. It firstly uses word embeddings to find candidates based on semantic similarity <cite>[Mikolov et al., 2013</cite>;<cite> Pennington et al., 2014</cite>;<cite> Peters et al., 2018]</cite> . To enrich linguistic information of expansions, it also takes the morphological and phonological features into account.",
  "y": "background"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_0",
  "x": "In this work, we offer a probabilistic twist on the task, developing a novel discriminative latent-variable model that outperforms previous work. Our proposed model is a bridge between current state-of-the-art methods in bilingual lexicon induction that take advantage of word embeddings, e.g., the embeddings induced by Mikolov et al. (2013b) 's skip-gram objective, and older ideas in the literature that build explicit probabilistic models for the task. We propose a discriminative probability model, inspired by Irvine and Callison-Burch * The first two authors contributed equally. 1 The code used to run the experiments is available at https://github.com/sebastianruder/ latent-variable-vecmap. (2013), infused with the bipartite matching dictionary prior of Haghighi et al. (2008) . However, like more recent approaches <cite>(Artetxe et al., 2017)</cite> , our model operates directly over pretrained word embeddings, induces a joint cross-lingual embedding space, and scales to large vocabulary sizes. To train our model, we derive a generalized expectationmaximization algorithm (EM; Neal and Hinton, 1998) and employ an efficient matching algorithm.",
  "y": "similarities"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_1",
  "x": "Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bilingual dictionary, as well as analyzing the resulting bilingual word embeddings themselves. The latent-variable model yields gains over several previous approaches across language pairs. It also enables us to make implicit modeling assumptions explicit. To this end, we provide a reinterpretation of<cite> Artetxe et al. (2017)</cite> as a latent-variable model with an IBM Model 1-style (Brown et al., 1993 ) dictionary prior, which allows a clean side-by-side analytical comparison. Viewed in this light, the difference between our approach and<cite> Artetxe et al. (2017)</cite> , the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons. Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over<cite> Artetxe et al. (2017)</cite> .",
  "y": "extends"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_2",
  "x": "The latent-variable model yields gains over several previous approaches across language pairs. It also enables us to make implicit modeling assumptions explicit. To this end, we provide a reinterpretation of<cite> Artetxe et al. (2017)</cite> as a latent-variable model with an IBM Model 1-style (Brown et al., 1993 ) dictionary prior, which allows a clean side-by-side analytical comparison. Viewed in this light, the difference between our approach and<cite> Artetxe et al. (2017)</cite> , the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons. Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over<cite> Artetxe et al. (2017)</cite> . ---------------------------------- **BACKGROUND: BILINGUAL LEXICON INDUCTION AND WORD EMBEDDINGS**",
  "y": "differences"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_3",
  "x": "Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bilingual dictionary, as well as analyzing the resulting bilingual word embeddings themselves. The latent-variable model yields gains over several previous approaches across language pairs. It also enables us to make implicit modeling assumptions explicit. To this end, we provide a reinterpretation of<cite> Artetxe et al. (2017)</cite> as a latent-variable model with an IBM Model 1-style (Brown et al., 1993 ) dictionary prior, which allows a clean side-by-side analytical comparison. Viewed in this light, the difference between our approach and<cite> Artetxe et al. (2017)</cite> , the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons. Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over<cite> Artetxe et al. (2017)</cite> .",
  "y": "differences"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_4",
  "x": "4 (ii) There exists an orthogonal transformation, after which the embedding spaces are more or less equivalent. Assumption (i) may be true for related languages, but is likely false for morphologically rich languages that have a many-to-many relationship between the words in their respective lexicons. We propose to ameliorate this using a rank constraint that only considers the top n most frequent words in both lexicons for matching in \u00a76. In addition, we experiment with priors that express different matchings in \u00a77. As for assumption (ii), previous work (Xing et al., 2015;<cite> Artetxe et al., 2017)</cite> has achieved some success using an orthogonal transformation; recently, however, demonstrated that monolingual embedding spaces are not approximately isomorphic and that there is a complex relationship between word form and meaning, which is only inadequately modeled by current approaches, which for example cannot model polysemy. Nevertheless, we will show that imbuing our model with these assumptions helps empirically in \u00a76, giving them practical utility. ----------------------------------",
  "y": "background"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_5",
  "x": "Assumption (i) may be true for related languages, but is likely false for morphologically rich languages that have a many-to-many relationship between the words in their respective lexicons. We propose to ameliorate this using a rank constraint that only considers the top n most frequent words in both lexicons for matching in \u00a76. In addition, we experiment with priors that express different matchings in \u00a77. As for assumption (ii), previous work (Xing et al., 2015;<cite> Artetxe et al., 2017)</cite> has achieved some success using an orthogonal transformation; recently, however, demonstrated that monolingual embedding spaces are not approximately isomorphic and that there is a complex relationship between word form and meaning, which is only inadequately modeled by current approaches, which for example cannot model polysemy. Nevertheless, we will show that imbuing our model with these assumptions helps empirically in \u00a76, giving them practical utility. ---------------------------------- **WHY IT WORKS:**",
  "y": "uses background"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_6",
  "x": "Given an optimal matching m computed in \u00a74.1, we search for a matrix \u2126 \u2208 R d\u00d7d . We additionally enforce the constraint that \u2126 is a real orthogonal matrix, i.e., \u2126 \u2126 = I. Previous work (Xing et al., 2015;<cite> Artetxe et al., 2017)</cite> found that the orthogonality constraint leads to noticeable improvements. Our M-step optimizes two objectives independently. First, making use of the result in equation (6), we optimize the following: with respect to \u2126 subject to \u2126 \u2126 = I. (Note we may ignore the constant C during the optimization.) Second, we optimize the objective with respect to the mean parameter \u00b5, which is simply an average. Note, again, we may ignore the constant D during optimization.",
  "y": "uses background"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_7",
  "x": "Namely, we compute U \u03a3V = T m S m . Then, we directly arrive at the optimum: \u2126 = U V . Optimizing equation (9) can also been done in closed form; the point which minimizes distance to the data points (thereby maximizing the log-probability) is the centroid: \u00b5 = 1 /|utrg| \u00b7 i\u2208utrg t i . ---------------------------------- **REINTERPRETATION OF ARTETXE ET AL. (2017) AS A LATENT-VARIABLE MODEL** The self-training method of<cite> Artetxe et al. (2017)</cite> , our strongest baseline in \u00a76, may also be interpreted as a latent-variable model in the spirit of our exposition in \u00a73. Indeed, we only need to change the edge-set prior p(m) to allow for edge sets other than those that are matchings.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_8",
  "x": "**REINTERPRETATION OF ARTETXE ET AL. (2017) AS A LATENT-VARIABLE MODEL** The self-training method of<cite> Artetxe et al. (2017)</cite> , our strongest baseline in \u00a76, may also be interpreted as a latent-variable model in the spirit of our exposition in \u00a73. Indeed, we only need to change the edge-set prior p(m) to allow for edge sets other than those that are matchings. Specifically, a matching enforces a one-to-one alignment between types in the respective lexicons. Artetxe et al. (2017) , on the other hand, allow for one-to-many alignments. 2011; Mena et al., 2018 ) may have been a computationally more effective manner to deal with the latent matchings. We show how this corresponds to an alignment distribution that is equivalent to IBM Model 1 (Brown et al., 1993) , and that<cite> Artetxe et al. (2017)</cite> 's selftraining method is actually a form of Viterbi EM.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_9",
  "x": "Artetxe et al. (2017) , on the other hand, allow for one-to-many alignments. 2011; Mena et al., 2018 ) may have been a computationally more effective manner to deal with the latent matchings. We show how this corresponds to an alignment distribution that is equivalent to IBM Model 1 (Brown et al., 1993) , and that<cite> Artetxe et al. (2017)</cite> 's selftraining method is actually a form of Viterbi EM. To formalize<cite> Artetxe et al. (2017)</cite> 's contribution as a latent-variable model, we lay down some more notation. Let A = {1, . . . , n src + 1} ntrg , where we define (n src + 1) to be none, a distinguished symbol indicating unalignment. The set A is to be interpreted as the set of all one-to-many alignments a on the bipartite vertex set V = V trg \u222a V src such that a i = j means the i th vertex in V trg is aligned to the j th vertex in V src . Note that a i = (n src + 1) = none means that the i th element of V trg is unaligned.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_10",
  "x": "Step In the context of Viterbi EM, it means the max over A will decompose additively s thus, we can simply find a component-wise as follows: Artetxe et al. (2017) 's M-step The M-step remains unchanged from the exposition in \u00a73 with the exception that we fit \u2126 given matrices S a and T a formed from a one-to-many alignment a, rather than a matching m. Why a Reinterpretation? The reinterpretation of<cite> Artetxe et al. (2017)</cite> as a probabilistic model yields a clear analytical comparison between our method and theirs. The only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce. ---------------------------------- **EXPERIMENTS**",
  "y": "extends"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_11",
  "x": "thus, we can simply find a component-wise as follows: Artetxe et al. (2017) 's M-step The M-step remains unchanged from the exposition in \u00a73 with the exception that we fit \u2126 given matrices S a and T a formed from a one-to-many alignment a, rather than a matching m. Why a Reinterpretation? The reinterpretation of<cite> Artetxe et al. (2017)</cite> as a probabilistic model yields a clear analytical comparison between our method and theirs. The only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce. ---------------------------------- **EXPERIMENTS** We first conduct experiments on bilingual dictionary induction and cross-lingual word similarity on three standard language pairs, English-Italian, English-German, and English-Finnish.",
  "y": "extends differences"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_12",
  "x": "The only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce. ---------------------------------- **EXPERIMENTS** We first conduct experiments on bilingual dictionary induction and cross-lingual word similarity on three standard language pairs, English-Italian, English-German, and English-Finnish. ---------------------------------- **EXPERIMENTAL DETAILS** Datasets For bilingual dictionary induction, we use the English-Italian dataset by and the English-German and English-Finnish datasets by<cite> Artetxe et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_13",
  "x": "**MONOLINGUAL EMBEDDINGS** We follow<cite> Artetxe et al. (2017)</cite> and train monolingual embeddings with word2vec, CBOW, and negative sampling (Mikolov et al., 2013a ) on a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC), a 1.6 billion word corpus for Italian (itWaC), a 0.9 billion word corpus for German (SdeWaC), and a 2.8 billion word corpus for Finnish (Common Crawl). Seed dictionaries Following<cite> Artetxe et al. (2017)</cite>, we use dictionaries of 5,000 words, 25 words, and a numeral dictionary consisting of words matching the [0-9]+ regular expression in both vocabularies. 10 In line with , we additionally use a dictionary of identically spelled strings in both vocabularies. Implementation details Similar to<cite> Artetxe et al. (2017)</cite> , we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 \u00d7 10 \u22126 between succeeding iterations. Unless stated otherwise, we induce a dictionary of 200,000 source and 200,000 target words as in previous work (Mikolov et al., 2013c; Artetxe et al., 2016) . For optimal 1:1 alignment, we have observed the best results by keeping the top k = 3 most similar target words.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_14",
  "x": "**EXPERIMENTAL DETAILS** Datasets For bilingual dictionary induction, we use the English-Italian dataset by and the English-German and English-Finnish datasets by<cite> Artetxe et al. (2017)</cite> . For cross-lingual word similarity, we use the RG-65 and WordSim-353 cross-lingual datasets for English-German and the WordSim-353 cross-lingual dataset for EnglishItalian by Camacho-Collados et al. (2015) . ---------------------------------- **MONOLINGUAL EMBEDDINGS** We follow<cite> Artetxe et al. (2017)</cite> and train monolingual embeddings with word2vec, CBOW, and negative sampling (Mikolov et al., 2013a ) on a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC), a 1.6 billion word corpus for Italian (itWaC), a 0.9 billion word corpus for German (SdeWaC), and a 2.8 billion word corpus for Finnish (Common Crawl). Seed dictionaries Following<cite> Artetxe et al. (2017)</cite>, we use dictionaries of 5,000 words, 25 words, and a numeral dictionary consisting of words matching the [0-9]+ regular expression in both vocabularies.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_15",
  "x": "---------------------------------- **MONOLINGUAL EMBEDDINGS** We follow<cite> Artetxe et al. (2017)</cite> and train monolingual embeddings with word2vec, CBOW, and negative sampling (Mikolov et al., 2013a ) on a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC), a 1.6 billion word corpus for Italian (itWaC), a 0.9 billion word corpus for German (SdeWaC), and a 2.8 billion word corpus for Finnish (Common Crawl). Seed dictionaries Following<cite> Artetxe et al. (2017)</cite>, we use dictionaries of 5,000 words, 25 words, and a numeral dictionary consisting of words matching the [0-9]+ regular expression in both vocabularies. 10 In line with , we additionally use a dictionary of identically spelled strings in both vocabularies. Implementation details Similar to<cite> Artetxe et al. (2017)</cite> , we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 \u00d7 10 \u22126 between succeeding iterations. Unless stated otherwise, we induce a dictionary of 200,000 source and 200,000 target words as in previous work (Mikolov et al., 2013c; Artetxe et al., 2016) .",
  "y": "similarities"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_16",
  "x": "Baselines We compare our approach with and without the rank constraint to the original bilingual mapping approach by Mikolov et al. (2013c) . In addition, we compare with Zhang et al. (2016) and Xing et al. (2015) who augment the former with an orthogonality constraint and normalization and an orthogonality constraint respectively. Finally, we compare with Artetxe et al. (2016) who add dimension-wise mean centering to Xing et al. (2015) , and<cite> Artetxe et al. (2017)</cite> . Both Mikolov et al. (2013c) and<cite> Artetxe et al. (2017)</cite> are special cases of our famework and comparisons to these approaches thus act as an ablation study. Specifically, Mikolov et al. (2013c) does not employ orthogonal Procrustes, but rather allows the learned matrix \u2126 to range freely. Likewise, as discussed in \u00a75,<cite> Artetxe et al. (2017)</cite> make use of a Viterbi EM style algorithm with a different prior over edge sets. 13",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_17",
  "x": "For optimal 1:1 alignment, we have observed the best results by keeping the top k = 3 most similar target words. If using a rank constraint, we restrict the matching in the Estep to the top 40,000 words in both languages. 11 Finding an optimal alignment on the 200,000 \u00d7 200,000 graph takes about 25 minutes on CPU; 12 with a rank constraint, matching takes around three minutes. Baselines We compare our approach with and without the rank constraint to the original bilingual mapping approach by Mikolov et al. (2013c) . In addition, we compare with Zhang et al. (2016) and Xing et al. (2015) who augment the former with an orthogonality constraint and normalization and an orthogonality constraint respectively. Finally, we compare with Artetxe et al. (2016) who add dimension-wise mean centering to Xing et al. (2015) , and<cite> Artetxe et al. (2017)</cite> . Both Mikolov et al. (2013c) and<cite> Artetxe et al. (2017)</cite> are special cases of our famework and comparisons to these approaches thus act as an ablation study.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_18",
  "x": "11 Finding an optimal alignment on the 200,000 \u00d7 200,000 graph takes about 25 minutes on CPU; 12 with a rank constraint, matching takes around three minutes. Baselines We compare our approach with and without the rank constraint to the original bilingual mapping approach by Mikolov et al. (2013c) . In addition, we compare with Zhang et al. (2016) and Xing et al. (2015) who augment the former with an orthogonality constraint and normalization and an orthogonality constraint respectively. Finally, we compare with Artetxe et al. (2016) who add dimension-wise mean centering to Xing et al. (2015) , and<cite> Artetxe et al. (2017)</cite> . Both Mikolov et al. (2013c) and<cite> Artetxe et al. (2017)</cite> are special cases of our famework and comparisons to these approaches thus act as an ablation study. Specifically, Mikolov et al. (2013c) does not employ orthogonal Procrustes, but rather allows the learned matrix \u2126 to range freely. Likewise, as discussed in \u00a75,<cite> Artetxe et al. (2017)</cite> make use of a Viterbi EM style algorithm with a different prior over edge sets.",
  "y": "background"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_19",
  "x": "We hypothesize that the prior imposes too strong of a constraint to find a good solution for a distant language pair from a poor initialization. With a better-but still weakly supervised-starting point using identical strings, our approach finds a good solution. Alternatively, we can mitigate this deficiency effectively using a rank constraint, which allows our model to converge to good solutions even with a 25 word or numerals seed lexicon. The rank constraint gen- 11 We validated both values with identical strings using the 5,000 word lexicon as validation set on English-Italian. 12 Training takes a similar amount of time as <cite>(Artetxe et al., 2017)</cite> due to faster convergence. 13 Other recent improvements such as symmetric reweighting (Artetxe et al., 2018) are orthogonal to our method, which is why we do not explicitly compare to them here. 14 Note that results are not directly comparable to (Conneau et al., 2018) due to the use of embeddings trained on different monolingual corpora (WaCKy vs. Wikipedia).",
  "y": "similarities"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_20",
  "x": "All approaches do better with identical strings compared to numerals, indicating that the former may be generally suitable as a default weakly-supervised seed lexicon. ---------------------------------- **ENGLISH-ITALIAN** On cross-lingual word similarity, our approach yields the best performance on WordSim-353 and RG-65 for English-German and is only outperformed by<cite> Artetxe et al. (2017)</cite> on English-Italian Wordsim-353. ---------------------------------- **ANALYSIS** Vocabulary sizes The beneficial contribution of the rank constraint demonstrates that in similar languages, many frequent words will have one-to-one matchings, while it may be harder to find direct matches for infrequent words.",
  "y": "differences"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_21",
  "x": "(18) -'hypocrisy' (13) jorge (17) ahmed (13) mohammed (17) ideologie -'ideology' (13) gewi\u00df eduardo (13) -'certainly' (17) The 2:2 prior performs best for small vocabulary sizes. As solving the linear assignment problem for larger vocabularies becomes progressively more challenging, the differences between the priors become obscured and their performance converges. Hubness problem We analyze empirically whether the prior helps with the hubness problem. Following , we define the hubness N k (y) at k of a target word y as follows: where Q is a set of query source language words and NN k (x, G) denotes the k nearest neighbors of x in the graph G. 16 In accordance with Lazaridou et al. (2015), we set k = 20 and use the words in the evaluation dictionary as query terms. We show the target language words with the highest hubness using our method and<cite> Artetxe et al. (2017)</cite> for English-German with a 5,000 seed lexicon and the full vocabulary in Table 3 . 17 Hubs are fewer and occur less often with our method, demonstrating that the prior-to some en-tr en-bn en-hi et-fi<cite> Artetxe et al. (2017)</cite> extent-aids with resolving hubness.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_22",
  "x": "We show the target language words with the highest hubness using our method and<cite> Artetxe et al. (2017)</cite> for English-German with a 5,000 seed lexicon and the full vocabulary in Table 3 . 17 Hubs are fewer and occur less often with our method, demonstrating that the prior-to some en-tr en-bn en-hi et-fi<cite> Artetxe et al. (2017)</cite> extent-aids with resolving hubness. Interestingly, compared to , hubs seem to occur less often and are more meaningful in current cross-lingual word embedding models. 18 For instance, the neighbors of 'gleichg\u00fcltigkeit' all relate to indifference and words appearing close to 'luis' or 'jorge' are Spanish names. This suggests that the prior might also be beneficial in other ways, e.g. by enforcing more reliable translation pairs for subsequent iterations. Low-resource languages Cross-lingual embeddings are particularly promising for low-resource languages, where few labeled examples are typically available, but are not adequately reflected in current benchmarks (besides the English-Finnish language pair). We perform experiments with our method with and without a rank constraint and<cite> Artetxe et al. (2017)</cite> for three truly lowresource language pairs, English-{Turkish, Bengali, Hindi}. We additionally conduct an experiment for Estonian-Finnish, similarly to .",
  "y": "differences"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_23",
  "x": "This suggests that the prior might also be beneficial in other ways, e.g. by enforcing more reliable translation pairs for subsequent iterations. Low-resource languages Cross-lingual embeddings are particularly promising for low-resource languages, where few labeled examples are typically available, but are not adequately reflected in current benchmarks (besides the English-Finnish language pair). We perform experiments with our method with and without a rank constraint and<cite> Artetxe et al. (2017)</cite> for three truly lowresource language pairs, English-{Turkish, Bengali, Hindi}. We additionally conduct an experiment for Estonian-Finnish, similarly to . For all languages, we use fastText embeddings (Bojanowski et al., 2017) trained on Wikipedia, the evaluation dictionaries provided by Conneau et al. (2018) , and a seed lexicon based on identical strings to reflect a realistic use case. We note that English does not share scripts with Bengali and Hindi, making this even more challenging. We show results in Table 4 . Surprisingly, the method by<cite> Artetxe et al. (2017)</cite> a similar self-learning method that uses word embeddings, with an implicit one-to-many alignment based on nearest neighbor queries.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_24",
  "x": "For all languages, we use fastText embeddings (Bojanowski et al., 2017) trained on Wikipedia, the evaluation dictionaries provided by Conneau et al. (2018) , and a seed lexicon based on identical strings to reflect a realistic use case. We note that English does not share scripts with Bengali and Hindi, making this even more challenging. We show results in Table 4 . Surprisingly, the method by<cite> Artetxe et al. (2017)</cite> a similar self-learning method that uses word embeddings, with an implicit one-to-many alignment based on nearest neighbor queries. Vuli\u0107 and Korhonen (2016) proposed a more strict one-to-many alignment based on symmetric translation pairs, which is also used by Conneau et al. (2018) . Our method bridges the gap between early latent variable and word embedding-based approaches and explicitly allows us to reason over its prior. Hubness problem The hubness problem is an intrinsic problem in high-dimensional vector spaces (Radovanovi\u0107 et al., 2010) .",
  "y": "similarities"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_25",
  "x": "Hubness problem The hubness problem is an intrinsic problem in high-dimensional vector spaces (Radovanovi\u0107 et al., 2010) . first observed it for cross-lingual embedding spaces and proposed to address it by re-ranking neighbor lists. proposed a max-marging objective as a solution, while more recent approaches proposed to modify the nearest neighbor retrieval by inverting the softmax (Smi, 2017) or scaling the similarity values (Conneau et al., 2018) . ---------------------------------- **CONCLUSION** We have presented a novel latent-variable model for bilingual lexicon induction, building on the work of<cite> Artetxe et al. (2017)</cite> . Our model combines the prior over bipartite matchings inspired by Haghighi et al. (2008) and the discriminative, rather than generative, approach inspired by Irvine and CallisonBurch (2013) .",
  "y": "extends"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_0",
  "x": "The model exploits annotation projection, instance selection, tag dictionaries, morphological lexicons, and distributed representations, all in a uniform framework. The approach is simple, yet surprisingly effective, resulting in a new state of the art without access to any gold annotated data. ---------------------------------- **INTRODUCTION** Low-resource languages lack manually annotated data to learn even the most basic models such as part-of-speech (POS) taggers. To compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: -aligned parallel corpora to project POS annotations to target languages (Yarowsky et al., 2001; Agi\u0107 et al., 2015; Fang and Cohn, 2016) , -noisy tag dictionaries for type-level approximation of full supervision<cite> (Li et al., 2012)</cite> , -combination of projection and type constraints (Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013) , -rapid annotation of seed training data . However, only one or two compatible sources of distant supervision are typically employed.",
  "y": "background"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_1",
  "x": "**INTRODUCTION** Low-resource languages lack manually annotated data to learn even the most basic models such as part-of-speech (POS) taggers. To compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: -aligned parallel corpora to project POS annotations to target languages (Yarowsky et al., 2001; Agi\u0107 et al., 2015; Fang and Cohn, 2016) , -noisy tag dictionaries for type-level approximation of full supervision<cite> (Li et al., 2012)</cite> , -combination of projection and type constraints (Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013) , -rapid annotation of seed training data . However, only one or two compatible sources of distant supervision are typically employed. In reality severely under-resourced languages may require a more pragmatic \"take what you can get\" viewpoint. Our results suggest that combining supervision sources is the way to go about creating viable low-resource taggers. We propose a method to strike a balance between model simplicity and the capacity to easily integrate heterogeneous learning signals.",
  "y": "motivation"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_2",
  "x": "Dictionaries. Dictionaries are a useful source for distant supervision<cite> (Li et al., 2012</cite>; T\u00e4ckstr\u00f6m et al., 2013) . There are several ways to exploit such information: i) as type constraints during encoding (T\u00e4ckstr\u00f6m et al., 2013) , ii) to guide unsupervised learning<cite> (Li et al., 2012)</cite> , or iii) as additional signal at training. We focus on the latter and evaluate two ways to integrate lexical knowledge into neural models, while comparing to the former two: a) by representing lexicon properties as n-hot vector (e.g., if a word has two properties according to lexicon src, it results in a 2-hot vector, if the word is not present in src, a zero vector), with m the number of lexicon properties; b) by embedding the lexical features, i.e., e src is a lexicon src embedded into an l-dimensional space. We represent e src as concatenation of all embedded m properties of length l, and a zero vector otherwise. Tuning on the dev set, we found the second embedding approach to perform best, and simple concatenation outperformed mean vector representations. We evaluate two dictionary sources, motivated by ease of accessibility to many languages: WIK-TIONARY, a word type dictionary that maps tokens to one of the 12 Universal POS tags<cite> (Li et al., 2012</cite>; Petrov et al., 2012) ; and UNIMORPH, a morphological dictionary that provides inflectional paradigms across 350 languages (Kirov et al., 2016) .",
  "y": "background"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_3",
  "x": "We show that this simple approach to instance selection offers substantial improvements: across all languages, we learn better taggers with significantly fewer training instances. Dictionaries. Dictionaries are a useful source for distant supervision<cite> (Li et al., 2012</cite>; T\u00e4ckstr\u00f6m et al., 2013) . There are several ways to exploit such information: i) as type constraints during encoding (T\u00e4ckstr\u00f6m et al., 2013) , ii) to guide unsupervised learning<cite> (Li et al., 2012)</cite> , or iii) as additional signal at training. We focus on the latter and evaluate two ways to integrate lexical knowledge into neural models, while comparing to the former two: a) by representing lexicon properties as n-hot vector (e.g., if a word has two properties according to lexicon src, it results in a 2-hot vector, if the word is not present in src, a zero vector), with m the number of lexicon properties; b) by embedding the lexical features, i.e., e src is a lexicon src embedded into an l-dimensional space. We represent e src as concatenation of all embedded m properties of length l, and a zero vector otherwise. Tuning on the dev set, we found the second embedding approach to perform best, and simple concatenation outperformed mean vector representations.",
  "y": "background"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_4",
  "x": "We represent e src as concatenation of all embedded m properties of length l, and a zero vector otherwise. Tuning on the dev set, we found the second embedding approach to perform best, and simple concatenation outperformed mean vector representations. We evaluate two dictionary sources, motivated by ease of accessibility to many languages: WIK-TIONARY, a word type dictionary that maps tokens to one of the 12 Universal POS tags<cite> (Li et al., 2012</cite>; Petrov et al., 2012) ; and UNIMORPH, a morphological dictionary that provides inflectional paradigms across 350 languages (Kirov et al., 2016) . For Wiktionary, we use the freely available dictionaries from <cite>Li et al. (2012)</cite> and . The size of the dictionaries ranges from a few thousands (e.g., Hindi and Bulgarian) to 2M (Finnish UniMorph). Sizes are provided in Table 1 , first columns. UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively).",
  "y": "uses"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_5",
  "x": "Tuning on the dev set, we found the second embedding approach to perform best, and simple concatenation outperformed mean vector representations. We evaluate two dictionary sources, motivated by ease of accessibility to many languages: WIK-TIONARY, a word type dictionary that maps tokens to one of the 12 Universal POS tags<cite> (Li et al., 2012</cite>; Petrov et al., 2012) ; and UNIMORPH, a morphological dictionary that provides inflectional paradigms across 350 languages (Kirov et al., 2016) . For Wiktionary, we use the freely available dictionaries from <cite>Li et al. (2012)</cite> and . The size of the dictionaries ranges from a few thousands (e.g., Hindi and Bulgarian) to 2M (Finnish UniMorph). Sizes are provided in Table 1 , first columns. UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively). Word embeddings.",
  "y": "uses"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_6",
  "x": "We compare to the following weaklysupervised POS taggers: -AGIC: Multi-source annotation projection with Bible parallel data by Agi\u0107 et al. (2015) . -DAS: The label propagation approach by Das and Petrov (2011) over Europarl data. -GARRETTE: The approach by that works with projections, dictionaries, and unlabeled target text. -LI: Wiktionary supervision<cite> (Li et al., 2012)</cite> . Data. Our set of 25 languages is motivated by accessibility to embeddings and dictionaries. In all experiments we work with the 12 Universal POS tags (Petrov et al., 2012) .",
  "y": "uses"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_7",
  "x": "It helps the most on morphological rich languages such as Uralic. On the test sets (Table 4 , right) DSDS reaches 87.2 over 8 test languages intersecting <cite>Li et al. (2012)</cite> and Agi\u0107 et al. (2016) . It reaches 86.2 over the more commonly used 8 languages of Das and Petrov (2011) , compared to their 83.4. This shows that our novel \"soft\" inclusion of noisy dictionaries is superior to a hard decoding restriction, and including lexicons in neural taggers helps. We did not assume any gold data to further enrich the lexicons, nor fix possible tagset divergences. ---------------------------------- **DISCUSSION**",
  "y": "similarities"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_8",
  "x": "We show: i) LI peaks at 10 iterations for their test languages, and at 35 iterations for all the rest. This is in slight contrast to 50 iterations that <cite>Li et al. (2012)</cite> recommend, although selecting 50 does not dramatically hurt the scores; ii) Our replication falls \u223c5 points short of their 84.9 accuracy. There is a large 33-point accuracy gap between the scores of <cite>Li et al. (2012)</cite> , where the dictionaries are large, and the other languages in Figure 4 , with smaller dictionaries. Compared to DAS, our tagger clearly benefits from pre-trained word embeddings, while theirs relies on label propagation through Europarl, a much cleaner corpus that lacks the coverage of the noisier WTC. Similar applies to T\u00e4ckstr\u00f6m et al. (2013) , as they use 1-5M near-perfect parallel sentences. Even if we use much smaller and noisier data sources, DSDS is almost on par: 86.2 vs. 87.3 for the 8 languages from Das and Petrov (2011) , and we even outperform theirs on four languages: Czech, French, Italian, and Spanish. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_9",
  "x": "This is in slight contrast to 50 iterations that <cite>Li et al. (2012)</cite> recommend, although selecting 50 does not dramatically hurt the scores; ii) Our replication falls \u223c5 points short of their 84.9 accuracy. There is a large 33-point accuracy gap between the scores of <cite>Li et al. (2012)</cite> , where the dictionaries are large, and the other languages in Figure 4 , with smaller dictionaries. Compared to DAS, our tagger clearly benefits from pre-trained word embeddings, while theirs relies on label propagation through Europarl, a much cleaner corpus that lacks the coverage of the noisier WTC. Similar applies to T\u00e4ckstr\u00f6m et al. (2013) , as they use 1-5M near-perfect parallel sentences. Even if we use much smaller and noisier data sources, DSDS is almost on par: 86.2 vs. 87.3 for the 8 languages from Das and Petrov (2011) , and we even outperform theirs on four languages: Czech, French, Italian, and Spanish. ---------------------------------- **RELATED WORK**",
  "y": "background"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_10",
  "x": "Even if we use much smaller and noisier data sources, DSDS is almost on par: 86.2 vs. 87.3 for the 8 languages from Das and Petrov (2011) , and we even outperform theirs on four languages: Czech, French, Italian, and Spanish. ---------------------------------- **RELATED WORK** Most successful work on low-resource POS tagging is based on projection (Yarowsky et al., 2001) , tag dictionaries<cite> (Li et al., 2012)</cite> , annotation of seed training data or even more recently some combination of these, e.g., via multi-task learning (Fang and not <cite>Li et al. (2012)</cite> Figure 4: The performance of LI with our dictionary data over EM iterations, separate for the languages from <cite>Li et al. (2012)</cite> and all the remaining languages in Table 1 . Cohn, 2016; Kann et al., 2018) . Our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed. Most prior work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are unnecessary for deep learning methods. They rely on end-to-end training without resorting to additional linguistic resources.",
  "y": "background"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_11",
  "x": "Compared to DAS, our tagger clearly benefits from pre-trained word embeddings, while theirs relies on label propagation through Europarl, a much cleaner corpus that lacks the coverage of the noisier WTC. Similar applies to T\u00e4ckstr\u00f6m et al. (2013) , as they use 1-5M near-perfect parallel sentences. Even if we use much smaller and noisier data sources, DSDS is almost on par: 86.2 vs. 87.3 for the 8 languages from Das and Petrov (2011) , and we even outperform theirs on four languages: Czech, French, Italian, and Spanish. ---------------------------------- **RELATED WORK** Most successful work on low-resource POS tagging is based on projection (Yarowsky et al., 2001) , tag dictionaries<cite> (Li et al., 2012)</cite> , annotation of seed training data or even more recently some combination of these, e.g., via multi-task learning (Fang and not <cite>Li et al. (2012)</cite> Figure 4: The performance of LI with our dictionary data over EM iterations, separate for the languages from <cite>Li et al. (2012)</cite> and all the remaining languages in Table 1 . Cohn, 2016; Kann et al., 2018) . Our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed.",
  "y": "differences extends"
 },
 {
  "id": "10acbeba830b2f8b3feb30de542c56_0",
  "x": "**ABSTRACT** The Touchdown dataset <cite>(Chen et al., 2019)</cite> provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively with the Touchdown tasks, we are publicly releasing the 29k raw Street View panoramas needed for Touchdown. We follow the process used for the StreetLearn data release (Mirowski et al., 2019) to check panoramas for personally identifiable information and blur them as necessary. These have been added to the StreetLearn dataset and can be obtained via the same process as used previously for StreetLearn. We also provide a reference implementation for both of the Touchdown tasks: vision and language navigation (VLN) and spatial description resolution (SDR). We compare our model results to those given in<cite> Chen et al. (2019)</cite> and show that the panoramas we have added to StreetLearn fully support both Touchdown tasks and can be used effectively for further research and comparison.",
  "y": "background"
 },
 {
  "id": "10acbeba830b2f8b3feb30de542c56_2",
  "x": "---------------------------------- **INTRODUCTION** Following natural language navigation instructions in visual environments requires addressing multiple challenges in dynamic, continuously changing environments, including language understanding, object recognition, grounding and spatial reasoning. Until recently, the most commonly studied domains were map-based (Thompson et al., 1993) or game-like (Macmahon et al., 2006; Misra et al., 2017 Misra et al., , 2018 Hermann et al., 2017; Hill et al., 2017) . These environments enabled substantial progress, but the complexity and diversity of the visual input they provide is limited. This greatly simplifies both the language and vision challenges. To address this, recent tasks based on simulated environments include photo-realistic visual input, such as Room-to-Room (R2R; Anderson et al., 2018) , Talk-the-Walk (de Vries et al., 2018) and Touchdown <cite>(Chen et al., 2019)</cite> , all of which rely on panorama photos.",
  "y": "background"
 },
 {
  "id": "10acbeba830b2f8b3feb30de542c56_3",
  "x": "Regions containing PII were marked as bounding boxes by annotators, and we blurred all of these regions for the final images. ---------------------------------- **EXPERIMENTS** We re-implement the best-reported models on the navigation and spatial description resolution tasks from<cite> Chen et al. (2019)</cite> to compare performance with our data release to the original Touchdown paper. The key difference between the two settings is that our released panoramas contain additional blurred patches (Section 2). Another minor difference is that we use a word-piece tokenizer (Devlin et al., 2019) instead of a full-word tokenizer. Spatial Description Resolution.",
  "y": "uses"
 },
 {
  "id": "10acbeba830b2f8b3feb30de542c56_4",
  "x": "**EXPERIMENTS** We re-implement the best-reported models on the navigation and spatial description resolution tasks from<cite> Chen et al. (2019)</cite> to compare performance with our data release to the original Touchdown paper. The key difference between the two settings is that our released panoramas contain additional blurred patches (Section 2). Another minor difference is that we use a word-piece tokenizer (Devlin et al., 2019) instead of a full-word tokenizer. Spatial Description Resolution. SDR results are given in Table 1 . Following<cite> Chen et al. (2019)</cite> , we report mean distance error and accuracy with different thresholds (40px, 80px, and 120px), which measures the proportion of evaluation items where the pixel chosen by the model is within the specified pixel distance.",
  "y": "uses"
 },
 {
  "id": "10acbeba830b2f8b3feb30de542c56_6",
  "x": "\u2022 Shortest-path distance (SPD): the mean of the distances over all executions of the agent's final panorama position and the goal panorama. \u2022 Success weighted by Edit Distance (SED): normalized graph edit distance between the agent path and true path, with points only awarded for successful paths. \u2022 Normalized Dynamic Time Warping (nDTW): a minimized cumulative distance between the agent path and true path, normalized by path length. \u2022 Success weighted Dynamic Time Warping (SDTW): nDTW, with points awarded only for successful paths. TC, SPD, and SED are defined in<cite> Chen et al. (2019)</cite> and nDTW and SDTW are defined in Ilharco et al. (2019) . VLN results are given in Table 2 . Our Retouchdown reimplementation of the RCONCAT model improves over the results given in<cite> Chen et al. (2019)</cite> for all metrics.",
  "y": "background"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_0",
  "x": "Besides temporal variations, word embeddings can also used to analyze geographic ones, e.g., the distinction between US American and British English variants (Kulkarni et al., 2016) . Most of these studies were performed with algorithms from the word2vec family, respectively GloVe in Jo (2016), and are thus likely to be affected by the same systematic reliability problems on which we focus here. Only Hamilton et al. (2016) used SVD PPMI in some of their very recent experiments and showed it to be adequate for exploring historical semantics. The Google Books Ngram corpus (GBN; Michel et al. (2011 ), Lin et al. (2012 ) is used in most of the studies we already mentioned, including our current study and its predecessor <cite>(Hellrich and Hahn, 2016a)</cite> . It contains about 6% of all books published between 1500 and 2009 in the form of n-grams (up to pentagrams), together with their frequency for each year. This corpus has often been criticized for its opaque sampling strategy, as its constituent books remain unknown and can be shown to form an unbalanced collection (Pechenick et al., 2015) . GBN is multilingual, with its English part being subdivided into regional segments (British, US) and topic categories (general language and fiction texts).",
  "y": "background uses"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_1",
  "x": "Neural word embeddings (Mikolov et al., 2013) are probably the most influential among all embedding types (see Section 2.1). Yet, we gathered evidence that the inherent randomness involved in their generation affects the reliability of word neighborhood judgments and demonstrate how this hampers qualitative conclusions based on such models. Our investigation was performed on both historical (for the time span of 1900 to 1904) and contemporary texts (for the time span of 2005 to 2009) in two languages, English and German. It is thus a continuation of prior work, in which we investigated historical English texts only <cite>(Hellrich and Hahn, 2016a)</cite> , and also influenced by the design decisions of Kim et al. (2014) and Kulkarni et al. (2015) which were the first to use word embeddings in diachronic studies. Our results cast doubt on the reproducibility of such experiments where neighborhoods between words in embedding space are taken as a computationally valid indicator for properly capturing lexical meaning (and, consequently, meaning shifts). linguistics. The word2vec family of algorithms, developed from heavily trimmed artificial neural networks, is a widely used and robust way to generate such embeddings (Mikolov et al., 2013; Levy et al., 2015) .",
  "y": "background"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_2",
  "x": "linguistics. The word2vec family of algorithms, developed from heavily trimmed artificial neural networks, is a widely used and robust way to generate such embeddings (Mikolov et al., 2013; Levy et al., 2015) . Its skip-gram variant predicts plausible contexts for a given word, whereas the alternative continuous bag-of-words variant tries to predict words from contexts; we focus on the former as it is generally reported to be superior (see e.g., Levy et al. (2015) ). There are two strategies for managing the huge number of potential contexts a word can appear in. Skip-gram hierarchical softmax (SGHS) uses a binary tree to more efficiently represent the vocabulary, whereas skip-gram negative sampling (SGNS) updates only a limited number of word vectors during each training step. SGNS is preferred in general, yet SGHS showed slight benefits in some reliability scenarios in our prior investigations <cite>(Hellrich and Hahn, 2016a)</cite> . There are two sources of randomness involved in the training of neural word embeddings: First, the random initialization of all word vectors before any examples are processed.",
  "y": "background"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_3",
  "x": "These models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor (Kim et al., 2014; Hellrich and Hahn, 2016b) , or a mapping between models for different points in time must be calculated (Kulkarni et al., 2015; Hamilton et al., 2016) . The first approach cannot be performed in parallel and is thus rather time-consuming, if texts are not subsampled. We nevertheless discourage using samples instead of full corpora, as we observed extremely low reliability values between different samples <cite>(Hellrich and Hahn, 2016a)</cite> . Word embeddings can also be used in diachronic studies without any kind of mapping to track clusters of similar words over time and, thus, model the evolution of topics (Kenter et al., 2015) or compare neighborhoods in embedding space for preselected words (Jo, 2016) . Besides temporal variations, word embeddings can also used to analyze geographic ones, e.g., the distinction between US American and British English variants (Kulkarni et al., 2016) . Most of these studies were performed with algorithms from the word2vec family, respectively GloVe in Jo (2016), and are thus likely to be affected by the same systematic reliability problems on which we focus here. Only Hamilton et al. (2016) used SVD PPMI in some of their very recent experiments and showed it to be adequate for exploring historical semantics.",
  "y": "motivation background"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_5",
  "x": "The German part was not only taken as is, but also orthographically normalized using the CAB service (Jurish, 2013) . 7 We incorporated this step because major changes in German orthography occurred during the 20th century, an issue that could hamper diachronic comparisons, e.g., archaic 'Gem\u00fcth' (in English: \"mind, emotional disposition\") became modern 'Gem\u00fct'. Table 1 shows the resulting reduction in the number of types, bringing the morphologically richer German to levels below English (yet this reduction is in line with the respective corpus sizes). ---------------------------------- **TRAINING** We used the PYTHON-based GENSIM 8 implementation of word2vec to independently train word embeddings for each time span with 200 dimensions, a context window of 4 (limited by the 5-gram size), a minimum frequency of 10, and 10 \u22125 as the threshold for downsampling frequent words. We processed the full subcorpora for each time span, due to the extremely low reliability values between samples we observed in previous investigations <cite>(Hellrich and Hahn, 2016a)</cite> .",
  "y": "motivation"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_6",
  "x": "Reliability at different top-n cut-offs is very similar for all languages and time spans under scrutiny, confirming previous observations in<cite> Hellrich and Hahn (2016a)</cite> and strengthening the suggestion to use only top-1 reliability for evaluation. Figure 2 illustrates this phenomenon with an SGNS trained on 1900-1904 English Fiction data. We assume this to be connected with the general decrease in word2vec embedding utility for high values of n already observed by Schnabel et al. (2015) . Influence of Word Frequency. Figures 3 and 4 depict the influence of word frequency (as percentile ranks) for English, as well as orthographically normalized German. Negative sampling is overall more reliable, especially for words with low or medium frequency. Word frequency has a less pronounced effect on reliability for German and negative sampling is again preferable, especially for low or medium frequency words.",
  "y": "similarities"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_0",
  "x": "**ABSTRACT** Stories generated with neural language models have shown promise in grammatical and stylistic consistency. However, the generated stories are still lacking in common sense reasoning, e.g., they often contain sentences deprived of world knowledge. We propose a simple multi-task learning scheme to achieve quantitatively better common sense reasoning in language models by leveraging auxiliary training signals from datasets designed to provide common sense grounding. When combined with our two-stage fine-tuning pipeline, our method achieves improved common sense reasoning and state-of-the-art perplexity on the WritingPrompts<cite> (Fan et al., 2018)</cite> story generation dataset. ---------------------------------- **INTRODUCTION**",
  "y": "uses"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_1",
  "x": "When fine-tuning is combined with multi-task learning in a two-stage pipeline, we improve the model's CSR and outperform state-of-the-art perplexity on the WritingPrompts<cite> (Fan et al., 2018)</cite> Our primary task is to perform language modeling (Elman, 1990; Bengio et al., 2003; Dai and Le, 2015) on the WritingPrompts dataset. A language model learns to assign the probability of a text sequence X = x 1 , . . . , x T using the conditional probability factorization: We train our model using a standard cross-entropy loss between next-step true tokens and predicted probabilities given current tokens. WritingPrompts<cite> (Fan et al., 2018</cite> ) is a dataset of prompts and short stories crawled from Reddit. The aim of the dataset is to produce a story given a free-text prompt. We reduce this conditional text generation task into a generic language modeling task by simply concatenating the prompt before the story and treating a prompt-story pair as one input to the Transformer decoder model. This human-readable format (example in Figure 1 ) is chosen because GPT2 may have been trained on similarly formatted text from the web.",
  "y": "uses"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_2",
  "x": "A language model learns to assign the probability of a text sequence X = x 1 , . . . , x T using the conditional probability factorization: We train our model using a standard cross-entropy loss between next-step true tokens and predicted probabilities given current tokens. WritingPrompts<cite> (Fan et al., 2018</cite> ) is a dataset of prompts and short stories crawled from Reddit. The aim of the dataset is to produce a story given a free-text prompt. We reduce this conditional text generation task into a generic language modeling task by simply concatenating the prompt before the story and treating a prompt-story pair as one input to the Transformer decoder model. This human-readable format (example in Figure 1 ) is chosen because GPT2 may have been trained on similarly formatted text from the web. When sampling, we can either seed the model with a prompt or allow it to generate its own prompt.",
  "y": "background"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_3",
  "x": "---------------------------------- **EVALUATION** We perform three types of evaluation on the model to assess its readability, reliance on the prompt (prompt ranking) and CSR. Readability is measured in terms of model perplexity on the test set of WritingPrompts. Because GPT2 uses subword tokenization (Sennrich et al., 2016) , it is not directly comparable to the wordlevel perplexity obtained in<cite> Fan et al. (2018)</cite> . We estimate the corresponding word-level perplexity by taking the product of each subword's probabilities to obtain probabilities for each word. Both sub-word perplexity and word-level perplexities are reported in our experiments.",
  "y": "background"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_4",
  "x": "We perform three types of evaluation on the model to assess its readability, reliance on the prompt (prompt ranking) and CSR. Readability is measured in terms of model perplexity on the test set of WritingPrompts. Because GPT2 uses subword tokenization (Sennrich et al., 2016) , it is not directly comparable to the wordlevel perplexity obtained in<cite> Fan et al. (2018)</cite> . We estimate the corresponding word-level perplexity by taking the product of each subword's probabilities to obtain probabilities for each word. Both sub-word perplexity and word-level perplexities are reported in our experiments. Prompt ranking<cite> (Fan et al., 2018)</cite> assesses how well a model matches a story to its given prompt. This is measured by computing the likelihood of stories conditioned under ten different prompts, nine of which are randomly sampled and one is the true prompt.",
  "y": "background"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_5",
  "x": "Both sub-word perplexity and word-level perplexities are reported in our experiments. Prompt ranking<cite> (Fan et al., 2018)</cite> assesses how well a model matches a story to its given prompt. This is measured by computing the likelihood of stories conditioned under ten different prompts, nine of which are randomly sampled and one is the true prompt. Following<cite> Fan et al. (2018)</cite> , we count a random story sample as correct when it ranks the true prompt with the lowest perplexity. We compute the accuracy from 1000 random samples. CSR is evaluated on two multiple choice datasets -SWAG and Story Cloze (Mostafazadeh et al., 2016) . We rank the perplexity computed by the model for each example and count it as correct if the lowest perplexity matches the answer.",
  "y": "uses"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_7",
  "x": "Other metrics are negligibly affected by the auxiliary tasks. ---------------------------------- **RELATED WORK** Story Generation: Recent work in neural story generation (Kiros et al., 2015; Roemmele, 2016) has shown success in using hierarchical methods (Yao et al., 2018; <cite>Fan et al., 2018)</cite> to generate stories. In these schemes, a neural architecture is engineered to first generate an outline or a prompt, then to expand the prompt into a full-length story. Our work performs hierarchical generation, but our main focus is on achieving better common sense in the generated text rather than engineering task-specific architectures. Common Sense Reasoning: Common sense reasoning (CSR) has been studied through many benchmarks such as SWAG (Zellers et al., 2018) , Story Cloze (Mostafazadeh et al., 2016) , the Winograd Schema Challenge (Levesque et al., 2012) , and CommonsenseQA (Talmor et al., 2018) .",
  "y": "background"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_0",
  "x": "For this set of experiments, the parser hyper-parameters are taken directly from the best settings in the Web Domain experiments. The results are shown in Table 3 , together with some state-of-the-art deterministic parsers. Comparing the L, NN and This models, the observations are consistent with the web domain. Honnibal et al. (2013) 91.30 90.00 Ma et al. (2014a) 91.32 - Table 3 : Main results on WSJ. All systems are deterministic. Our combined parser gives accuracies competitive to state-of-the-art deterministic parsers in the literature. In particular, the method of <cite>Chen and Manning (2014)</cite> is the same as our NN baseline.",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_1",
  "x": "For this set of experiments, the parser hyper-parameters are taken directly from the best settings in the Web Domain experiments. The results are shown in Table 3 , together with some state-of-the-art deterministic parsers. Comparing the L, NN and This models, the observations are consistent with the web domain. Honnibal et al. (2013) 91.30 90.00 Ma et al. (2014a) 91.32 - Table 3 : Main results on WSJ. All systems are deterministic. Our combined parser gives accuracies competitive to state-of-the-art deterministic parsers in the literature. In particular, the method of <cite>Chen and Manning (2014)</cite> is the same as our NN baseline.",
  "y": "similarities"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_2",
  "x": "Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014) . Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G\u00f3mez-Rodr\u00edguez and Fern\u00e1ndez-Gonz\u00e1lez, 2015) . They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, <cite>Chen and Manning (2014)</cite> use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 A neural network model takes continuous vector representations of words as inputs, which can be pre-trained using large amounts of unlabeled data, thus containing more information. In addition, using an extra hidden layer, a neural network is capable of learning non-linear relations between automatic features, achieving feature combinations automatically.",
  "y": "background"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_3",
  "x": "1 We correlate and compare all the five systems in Figure 1 empirically, using the SANCL 2012 data (Petrov and McDonald, 2012) and the standard Penn Treebank data. Results show that the method of this paper gives higher accuracies than the other methods. In addition, the method of Guo et al. (2014) gives slightly better accuracies compared to the method of Turian et al. (2010) for parsing task, consistent with Guo et al's observation on named entity recognition (NER). We make our C++ code publicly available under GPL at https://github.com/ SUTDNLP/NNTransitionParser. ---------------------------------- **PARSER** We take <cite>Chen and Manning (2014)</cite> , which uses the arc-standard transition system (Nivre, 2008) .",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_4",
  "x": "We take <cite>Chen and Manning (2014)</cite> , which uses the arc-standard transition system (Nivre, 2008) . Given an POS-tagged input sentence, it builds a projective output y by performing a sequence of state transition actions using greedy search. <cite>Chen and Manning (2014)</cite> can be viewed as a neutral alternative of MaltParser (Nivre, 2008) . Although not giving state-of-the-art accuracies, deterministic parsing is attractive for its high parsing speed (1000+ sentences per second). Our incorporation of discrete features does not harm the overall speed significantly. In addition, deterministic parsers use standard neural classifiers, which allows isolated study of feature influences. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_5",
  "x": "<cite>Chen and Manning (2014)</cite> can be viewed as a neutral alternative of MaltParser (Nivre, 2008) . Although not giving state-of-the-art accuracies, deterministic parsing is attractive for its high parsing speed (1000+ sentences per second). Our incorporation of discrete features does not harm the overall speed significantly. In addition, deterministic parsers use standard neural classifiers, which allows isolated study of feature influences. ---------------------------------- **MODELS** Following <cite>Chen and Manning (2014)</cite> , training of all the models using a cross-entropy loss objective with a L2-regularization, and mini-batched AdaGrad (Duchi et al., 2011) .",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_6",
  "x": "The score of an action a is defined by where \u03c3 represents the sigmoid activation function, \u2212 \u2192 \u03b8 d is the set of model parameters, denoting the feature weights with respect to actions, a can be SHIFT, LEFT(l) and RIGHT(l). ---------------------------------- **BASELINE NEURAL (NN)** We take the Neural model of <cite>Chen and Manning (2014)</cite> as another baseline (Figure 1(b) ). Given a parsing state x, the words are first mapped into continuous vectors by using a set of pre-trained word embeddings. Denote the mapping as \u03a6 e (x).",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_7",
  "x": "Denote the mapping as \u03a6 e (x). In addition, denote the hidden layer as \u03a6 h , and the ith node in the hidden as \u03a6 h,i (0 \u2264 i \u2264 |\u03a6 h |). The hidden layer is defined as where \u2212 \u2192 \u03b8 h is the set of parameters between the input and hidden layers. The score of an action a is defined as where \u2212 \u2192 \u03b8 c,a is the set of parameters between the hidden and output layers. We use the arc-standard features \u03a6 e as <cite>Chen and Manning (2014)</cite> , which is also based on the arc-eager templates of Zhang and Nivre (2011) , similar to those of the baseline model L.",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_8",
  "x": "Following <cite>Chen and Manning (2014)</cite> , we use the pre-trained word embedding released by Collobert et al. (2011) , and set h = 200 for the hidden layer size, \u03bb = 10 \u22128 for L2 regularization, and \u03b1 = 0.01 for the initial learning rate of Adagrad. ---------------------------------- **DEVELOPMENT RESULTS** Fine-tuning of embeddings. <cite>Chen and Manning (2014)</cite> fine-tune word embeddings in supervised training, consistent with Socher et al. (2013) . Intuitively, fine-tuning embeddings allows in-vocabulary words to join the parameter space, thereby giving better fitting to in-domain data. However, it also forfeits the benefit of large-scale pre-training, because out-of-vocabulary (OOV) words do not have their embeddings fine-tuned.",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_9",
  "x": "**DEVELOPMENT RESULTS** Fine-tuning of embeddings. <cite>Chen and Manning (2014)</cite> fine-tune word embeddings in supervised training, consistent with Socher et al. (2013) . Intuitively, fine-tuning embeddings allows in-vocabulary words to join the parameter space, thereby giving better fitting to in-domain data. However, it also forfeits the benefit of large-scale pre-training, because out-of-vocabulary (OOV) words do not have their embeddings fine-tuned. In this sense, the method of Chen and Manning resembles a traditional supervised sparse linear model, which can be weak on OOV. On the other hand, the semi-supervised learning methods such as Turian et al. (2010) Table 2 : Main results on SANCL.",
  "y": "background"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_10",
  "x": "Interestingly, our combined model gives consistently better accuracies with fine-tuning. We attribute this to the use of sparse discrete features, which allows the model to benefit from large-scale pre-trained embeddings without sacrificing in-domain performance. The observation on Turian is similar. For the final experiments, we apply fine-tuning on the NN model, but not to the Turian and This. Note also tat for all experiments, the POS and label embedding features of <cite>Chen and Manning (2014)</cite> are fine-tuned, consistent with their original method. Dropout rate. We test the effect of dropout (Hinton et al., 2012) during training, using a default ratio of 0.5 according to <cite>Chen and Manning (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_11",
  "x": "For the final experiments, we apply fine-tuning on the NN model, but not to the Turian and This. Note also tat for all experiments, the POS and label embedding features of <cite>Chen and Manning (2014)</cite> are fine-tuned, consistent with their original method. Dropout rate. We test the effect of dropout (Hinton et al., 2012) during training, using a default ratio of 0.5 according to <cite>Chen and Manning (2014)</cite> . In our experiments, we find that the dense NN model and our combined model achieve better performances by using dropout, but the other models do not benefit from dropout. ---------------------------------- **FINAL RESULTS**",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_12",
  "x": "We test the effect of dropout (Hinton et al., 2012) during training, using a default ratio of 0.5 according to <cite>Chen and Manning (2014)</cite> . In our experiments, we find that the dense NN model and our combined model achieve better performances by using dropout, but the other models do not benefit from dropout. ---------------------------------- **FINAL RESULTS** The final results across web domains are shown in Table 2 . Our logistic regression linear parser and re-implementation of <cite>Chen and Manning (2014)</cite> give comparable accuracies to the perceptron ZPar 2 and Stanford NN Parser 3 , respectively. It can be seen from the table that both Turian and Guo 4 outperform L by incorporating embed-ding features.",
  "y": "similarities uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_13",
  "x": "Interestingly, NN gives higher accuracies on web domain out-of-embeddingvocabulary (OOE) words, out of which 54% are in-vocabulary. Note that the accuracies of our parsers are lower than the best systems in the SANCL shared task, which use ensemble models. Our parser enjoys the fast speed of deterministic parsers, and in particular the baseline NN parser<cite> (Chen and Manning, 2014)</cite> . ---------------------------------- **WSJ EXPERIMENTS** For comparison with related work, we conduct experiments on Penn Treebank corpus also. We use the WSJ sections 2-21 for training, section 22 for development and section 23 for testing.",
  "y": "similarities"
 },
 {
  "id": "1781b27c13dca15752cb6aa8a9fc38_0",
  "x": "This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (Saon and Chien, 2012; Chan et al., 2016) to document summarization (Chang and Chien, 2009 ), text classification (Blei et al., 2003; Zhang et al., 2015) , text segmentation (Chien and Chueh, 2012) , information extraction (Narasimhan et al., 2016) , image caption generation (Vinyals et al., 2015; Xu et al., 2015) , sentence generation (Li et al., 2016b) , dialogue control (Zhao and Eskenazi, 2016; Li et al., 2016a) , sentiment classification, recommendation system, question answering (Sukhbaatar et al., 2015) and machine translation , to name a few. Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model. The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs. The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (Blei et al., 2010) , hierarchical Pitman-Yor process (Teh, 2006) , Indian buffet process (Ghahramani and Griffiths, 2005) , recurrent neural network (Mikolov et al., 2010; Van Den Oord et al., 2016) , long short-term memory (Hochreiter and Schmidhuber, 1997; , sequence-to-sequence model (Sutskever et al., 2014), variational auto-encoder (Kingma and Welling, 2014) , generative adversarial network (Goodfellow et al., 2014) , attention mechanism (Chorowski et al., 2015; Seo et al., 2016) , memory-augmented neural network (Graves et al., 2014; Graves et al., 2014) , stochastic neural network <cite>Miao et al., 2016)</cite> , predictive state neural network (Downey et al., 2017) , policy gradient (Yu et al., 2017) and reinforcement learning (Mnih et al., 2015) . We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language. The variational inference and sampling method are formulated to tackle the optimization for complicated models (Rezende et al., 2014) .",
  "y": "background uses"
 },
 {
  "id": "1786b6c1c6532d5baa092cca40e389_0",
  "x": "In particular, data-driven models for lexical semantics require the creation of broad-coverage, hand-annotated corpora with predicateargument information, i.e. rich information about words expressing a semantic relation having argument slots filled by the interpretations of their grammatical complements. Corpora combining semantic and syntactic annotations constitute the backbone for the development of probabilistic models that automatically identify the semantic relationships, or semantic roles, conveyed by sentential constituents<cite> (Gildea and Jurafsky, 2002)</cite> . That is, given an input sentence and a target predicator the system labels constituents with general roles like Agent, Patient, Theme, etc., or more specific roles, as in (1) . (1) 1 The task of automatic semantic role labelling (or shallow semantic parsing) is a first step towards text understanding and has found use in a variety of NLP applications including information extraction (Surdeanu et al., 2003), machine translation (Boas, 2002) , question answering (Narayanan and Harabagiu, 2004) , summarisation (Melli et al., 2005) , recognition of textual entailment relations (Burchardt and Frank, 2006) , etc. Corpora with semantic role labels additionally lend themselves to extraction of linguistic knowledge at the syntax-semantics interface. The range of semantic and syntactic combinatorial properties (valences) of each word in each of its senses is documented in terms of annotated corpus attestations. For instance, the valence pattern for the use of admire in (1) is shown in (2).",
  "y": "background"
 },
 {
  "id": "1786b6c1c6532d5baa092cca40e389_1",
  "x": "Conceived entities that are peripheral to the essential relation lexicalised by the predicate are associated with a more specific property termed Conceived background state of affairs (Conceived bsoa). These arguments receive less focus in the meaning of the predicate, in a sense that they are not absolutely necessary to understand the predicate's meaning. The representation of test (8), stereotype (<cite>10</cite>), and find out (11) in terms of two Notion relations, one of which is treated as more salient, reifies the concept of relative significance of Proto-Role properties in the verbal semantics. This concept is related to the weighting of entailments in the overall semantics of a verb, which plays a critical role in determining the syntactic patterns in which the verb appears (i.e. the grammatical realisations of its arguments). <cite>10</cite> The verbs in (8) and (9) involve an additional entailment of Intentionality. This is used to mark entities characterised by conscious choice, decision, or control over the course of inherently intentional actions. Intentional participants necessarily have a notion/perception of some event participant(s).",
  "y": "uses"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_0",
  "x": "Then, we devise a new pretraining task called label-aware masked language model (LA-MLM) consisting of two subtasks: 1) word knowledge recovering given the sentence-level label; 2) sentence-level label prediction with linguistic knowledge enhanced context. Experiments show that Sen-tiLR achieves state-of-the-art performance on several sentence-level / aspect-level sentiment analysis tasks by fine-tuning, and also obtain comparative results on general language understanding tasks. ---------------------------------- **INTRODUCTION** Recently, pre-trained language representation models such as GPT (Radford et al., 2018 (Radford et al., , 2019 , ELMo (Peters et al., 2018) , BERT<cite> (Devlin et al., 2019)</cite> and XLNet have achieved promising results in NLP tasks, including reading comprehension (Rajpurkar et al., 2016) , natural language inference (Bowman et al., 2015; Williams et al., 2018) and sentiment classification (Socher et al., 2013) . These models capture contextual information from large-scale unlabelled corpora via well-designed pre-training * Equal contribution \u2020 Corresponding author: Minlie Huang tasks. The literature has commonly reported that pre-trained models can be used as effective feature extractors and achieve state-of-the-art performance on various downstream tasks .",
  "y": "background"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_1",
  "x": "These models capture contextual information from large-scale unlabelled corpora via well-designed pre-training * Equal contribution \u2020 Corresponding author: Minlie Huang tasks. The literature has commonly reported that pre-trained models can be used as effective feature extractors and achieve state-of-the-art performance on various downstream tasks . Although pre-trained language representation models have achieved transformative performance, the pre-training tasks like masked language model and next sentence prediction<cite> (Devlin et al., 2019)</cite> neglect to consider the linguistic knowledge. We argue that such knowledge is important for some NLP tasks, particularly for sentiment analysis. For instance, existing work has shown that linguistic knowledge including part-ofspeech tag (Qian et al., 2015; and prior sentiment polarity of each word is closely related to the sentiment of longer texts such as sentences and paragraphs. We argue that pre-trained models enriched with the linguistic knowledge of words will benefit the understanding of the sentiment of the whole texts, thereby resulting in better performance on sentiment analysis. Although directly introducing the linguistic knowledge from external linguistic resources is feasible, it remains a challenge for the model to learn beneficial knowledge-aware representation that promotes the downstream tasks in sentiment analysis.",
  "y": "motivation"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_2",
  "x": "**PRE-TRAINED LANGUAGE REPRESENTATION MODEL** Early work on pre-trained language representation models mainly focuses on distributed word representations, such as word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) . Since the distributed word representation is independent of context, it's challenging for such representation to model the complex word characteristics under different contexts. Thus contextual language representation based on pre-trained models including CoVe (McCann et al., 2017) , ELMo (Peters et al., 2018) , GPT (Radford et al., 2018 (Radford et al., , 2019 and BERT<cite> (Devlin et al., 2019)</cite> becomes prevalent recently. These models use deep LSTM (Hochreiter and Schmidhuber, 1997) or Transformer (Vaswani et al., 2017) as the encoder to acquire contextual language representation. Various pre-training tasks were explored including traditional NLP tasks like machine translation (Mc-Cann et al., 2017) and language model (Peters et al., 2018; Radford et al., 2018 Radford et al., , 2019 , or other tasks such as masked language model and next sentence prediction<cite> (Devlin et al., 2019)</cite> . With the advent of BERT<cite> (Devlin et al., 2019)</cite> achieving state-of-the-art performances on various NLP tasks, many variants of BERT have been proposed.",
  "y": "background"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_3",
  "x": "Since the distributed word representation is independent of context, it's challenging for such representation to model the complex word characteristics under different contexts. Thus contextual language representation based on pre-trained models including CoVe (McCann et al., 2017) , ELMo (Peters et al., 2018) , GPT (Radford et al., 2018 (Radford et al., , 2019 and BERT<cite> (Devlin et al., 2019)</cite> becomes prevalent recently. These models use deep LSTM (Hochreiter and Schmidhuber, 1997) or Transformer (Vaswani et al., 2017) as the encoder to acquire contextual language representation. Various pre-training tasks were explored including traditional NLP tasks like machine translation (Mc-Cann et al., 2017) and language model (Peters et al., 2018; Radford et al., 2018 Radford et al., , 2019 , or other tasks such as masked language model and next sentence prediction<cite> (Devlin et al., 2019)</cite> . With the advent of BERT<cite> (Devlin et al., 2019)</cite> achieving state-of-the-art performances on various NLP tasks, many variants of BERT have been proposed. Due to the important role of entities in language understanding, two heuristic ways have been studied to make the pre-trained model aware of entities, i.e. introducing knowledge graph (Zhang et al., 2019) / knowledge base (Peters et al., 2019) explicitly and designing entity-specific masking strategies during pretraining (Sun et al., 2019a,b) . Considering the implicit relationship among different NLP tasks, post-training approaches Li et al., 2019) conduct supervised training on the pretrained BERT with transfer tasks which are related to target tasks, in order to get a better initialization for target tasks.",
  "y": "background"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_4",
  "x": "Thus contextual language representation based on pre-trained models including CoVe (McCann et al., 2017) , ELMo (Peters et al., 2018) , GPT (Radford et al., 2018 (Radford et al., , 2019 and BERT<cite> (Devlin et al., 2019)</cite> becomes prevalent recently. These models use deep LSTM (Hochreiter and Schmidhuber, 1997) or Transformer (Vaswani et al., 2017) as the encoder to acquire contextual language representation. Various pre-training tasks were explored including traditional NLP tasks like machine translation (Mc-Cann et al., 2017) and language model (Peters et al., 2018; Radford et al., 2018 Radford et al., , 2019 , or other tasks such as masked language model and next sentence prediction<cite> (Devlin et al., 2019)</cite> . With the advent of BERT<cite> (Devlin et al., 2019)</cite> achieving state-of-the-art performances on various NLP tasks, many variants of BERT have been proposed. Due to the important role of entities in language understanding, two heuristic ways have been studied to make the pre-trained model aware of entities, i.e. introducing knowledge graph (Zhang et al., 2019) / knowledge base (Peters et al., 2019) explicitly and designing entity-specific masking strategies during pretraining (Sun et al., 2019a,b) . Considering the implicit relationship among different NLP tasks, post-training approaches Li et al., 2019) conduct supervised training on the pretrained BERT with transfer tasks which are related to target tasks, in order to get a better initialization for target tasks. The model structure and the pre-training tasks of BERT are also worth exploring.",
  "y": "background"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_5",
  "x": "In this formulation, d indicates the dimension of the representation vector. Figure 1 shows the overview of our model pipeline which contains three stages: 1) Acquiring the prior sentiment polarity for each word with its corresponding part-of-speech tag; 2) Conducting pre-training via two tasks i.e. label-aware masked language modeling and next sentence prediction; 3) Fine-tuning on sentiment analysis tasks with different settings. Compared with the vanilla pretrained models like BERT<cite> (Devlin et al., 2019)</cite> , our model enriches the input sequence with its linguistic knowledge including part-of-speech tags and sentiment polarity labels, and utilizes a modified masked language model to capture the relationship between sentence-level sentiment labels and word-level knowledge in addition to context dependency. ---------------------------------- **LINGUISTIC KNOWLEDGE ACQUISITION** This module obtains the sentiment polarity for each word with its part-of-speech tag. The input of this module is a sequence of tuples X = ((x 1 , pos 1 ), (x 2 , pos 2 ), \u00b7 \u00b7 \u00b7 , (x n , pos n )) containing words and part-of-speech labels tagged by external tools such as NLTK 1 .",
  "y": "differences"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_6",
  "x": "**PRE-TRAINING TASKS** During pre-training, Label-aware masked language model (LA-MLM) and next sentence prediction (NSP) are adopted as the pre-training tasks where the setting of NSP is identical to the one proposed by<cite> Devlin et al. (2019)</cite> . Label-aware masked language model is designed to utilize the linguistic knowledge to grasp the implicit dependency between sentence-level sentiment labels and words in addition to context dependency. It contains two separate sub-tasks, both of which take the position embedding, token embedding and segment embedding as the input. The goal of sub-task#1 is to recover the masked sequence conditioned on the sentence-level label, as shown in Figure 2 . In this setting, we add the sentence-level sentiment embedding to the inputs and the model is required to predict the word, partof-speech tag and word-level sentiment polarity individually using the hidden states at the masked positions. This sub-task explicitly exerts the impact of the high-level sentiment label on the words and the linguistic knowledge of words, enhancing the ability of our model to explore the complex connection among them.",
  "y": "similarities uses"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_7",
  "x": "We follow the fine-tuning setting of the existing work <cite>(Devlin et al., 2019</cite>; : Sentence-level Sentiment Classification: The input of this task is a text sequence ([CLS], x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x n , [SEP]). The sentiment label is obtained based on the hidden state of [CLS]. Aspect-level Sentiment Classification: In addition to the text sequence, the input additionally contains an aspect term / aspect category sequence (a 1 , \u00b7 \u00b7 \u00b7 , a l ). The sentiment label is also acquired based on the hidden state of [CLS] . Figure 4 illustrates the fine-tuning settings. ---------------------------------- **EXPERIMENT**",
  "y": "uses"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_8",
  "x": "This dataset contains 6,685,900 reviews with 5-class review-level sentiment labels. Each review consists of 8.1 sentences and 127.8 words on average. Since our method can adapt to all the BERTstyle pre-training models, we used vanilla BERT<cite> (Devlin et al., 2019)</cite> as the base framework to construct Transformer blocks in this paper and leave the exploration of other models like RoBERTa as future work. The hyperparameters of the Transformer blocks were set to be the same as BERT-Base due to the limited computational power. Considering the large cost of training from scratch, we utilized the parameters of pre-trained BERT 3 to initialize our model. We also followed BERT to use WordPiece vocabulary (Wu et al., 2016) with a vocabulary size of 30,522. The maximum sequence length in the pre-training phase was 128, while the batch size was 512.",
  "y": "uses"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_10",
  "x": "We followed the existing work to leave 150 examples from the training sets for validation. ---------------------------------- **ASPECT-LEVEL SENTIMENT CLASSIFICATION** We present the results of aspect-level sentiment classification in Table 5 . We can see that Sen-tiLR outperforms the baselines in both accuracy and Macro-F1 on these datasets, indicating that our model can successfully grasp the sentiment of the given aspects. Since the improvement of Macro-F1 is more notable than that of accuracy, it is convinced that our model actually does better in all the three sentiment classes. Due to the sparsity of aspect terms compared with aspect categories, our model improved a larger margin on the task of aspect category sentiment classification than the<cite> (Devlin et al., 2019)</cite> .",
  "y": "differences"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_11",
  "x": "To explore whether the performance of Sen-tiLR on common NLP tasks will improve or degrade, we evaluated our model on General Language Understanding Evaluation (GLUE) benchmark , which collects diverse language understanding tasks. We fine-tuned Sen-tiLR on each task of GLUE respectively, and compared its performance with vanilla BERT. Since the test sets of GLUE are not publicly available, we reported the results on development sets in Table 6 . Note that we directly used the results of BERT on SST-2, MNLI, QNLI and MRPC which are reported by<cite> Devlin et al. (2019)</cite> and reimplemented the BERT model fine-tuned on the rest of the tasks by ourselves. From Table 6 , SentiLR surely gets better results on the tasks in sentiment analysis like SST-2. We also observe that our model outperforms BERT on CoLA, MRPC, QNLI tasks, and gets comparative results on the other datasets. Among these datasets, CoLA requires fine-grained grammaticality distinction for complex syntactic structures (Warstadt and Bowman, 2019) , which may be aided by part-of-speech tag information.",
  "y": "uses"
 },
 {
  "id": "19233e4954d7e75ac01112a4c07e64_0",
  "x": "Multi-task learning involves sharing parameters between related tasks, whereby each task benefits from extra information in the training signals of the related tasks, and also improves its generalization performance. Luong et al. (2016) showed improvements on translation, captioning, and parsing in a shared multi-task setting. Recently, Pasunuru and Bansal (2017) extend this idea to video captioning with two related tasks: video completion and entailment generation. We demonstrate that abstractive text summarization models can also be improved by sharing parameters with an entailment generation task. ---------------------------------- **MODELS** First, we discuss our baseline model which is similar to the machine translation encoder-alignerdecoder model of Luong et al. (2015) , and presented by<cite> Chopra et al. (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "19233e4954d7e75ac01112a4c07e64_1",
  "x": "First, we discuss our baseline model which is similar to the machine translation encoder-alignerdecoder model of Luong et al. (2015) , and presented by<cite> Chopra et al. (2016)</cite> . Next, we introduce our multi-task learning approach of sharing the parameters between abstractive summarization and entailment generation models. ---------------------------------- **BASELINE MODEL** Our baseline model is a strong, multi-layered encoder-attention-decoder model with bilinear attention, similar to Luong et al. (2015) and following the details in<cite> Chopra et al. (2016)</cite> . Here, we encode the source document with a two-layered LSTM-RNN and generate the summary using another two-layered LSTM-RNN decoder. The word probability distribution at time step t of the decoder is defined as follows:",
  "y": "uses"
 },
 {
  "id": "19233e4954d7e75ac01112a4c07e64_2",
  "x": "**EVALUATION** Following previous work (Nallapati et al., 2016; <cite>Chopra et al., 2016</cite>; Rush et al., 2015) , we use the full-length F1 variant of Rouge (Lin, 2004) for the Gigaword results, and the 75-bytes length limited Recall variant of Rouge for DUC. Additionally, we also report other standard language generation metrics (as motivated recently by See et al. (2017) ): METEOR (Denkowski and Lavie, 2014) , BLEU-4 (Papineni et al., 2002) , and CIDEr-D , based on the MS-COCO evaluation script (Chen et al., 2015) . ---------------------------------- **TRAINING DETAILS** We use the following simple settings for all the models, unless otherwise specified. We unroll the encoder RNN's to a maximum of 50 time steps and decoder RNN's to a maximum of 30 time steps.",
  "y": "uses"
 },
 {
  "id": "19233e4954d7e75ac01112a4c07e64_4",
  "x": "We always tune hyperparameters on the validation set of the corresponding dataset, where applicable. For multi-task learning, we tried a few mixing ratios and found 1 : 0.05 to work better, i.e., 100 mini-batches of summarization with 5 mini-batches of entailment generation task in alternate training rounds. ---------------------------------- **RESULTS AND ANALYSIS** ---------------------------------- **SUMMARIZATION RESULTS: GIGAWORD** Baseline Results and Previous Work Our baseline is a strong encoder-attention-decoder model based on Luong et al. (2015) and presented by<cite> Chopra et al. (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "19233e4954d7e75ac01112a4c07e64_5",
  "x": "This is especially promising given that the domain of the entailment dataset (image captions) is very different from the domain of the summarization datasets (news), suggesting that the model might be learning some domain-agnostic inference skills. ---------------------------------- **SUMMARIZATION RESULTS: DUC** Here, we directly use the Gigaword-trained model to test on the DUC-2004 dataset (see tuning discussion in Sec. 4.1). In Table 2 , we again see that et al. (2015) 28.18 8.49 23.81<cite> Chopra et al. (2016)</cite> 28.97 8.26 24.06 Nallapati et al. (2016) our Luong et al. (2015) baseline model achieves competitive performance with previous work, esp. on Rouge-2 and Rouge-L. Next, we show promising multi-task improvements over this baseline of around 0.4% across all metrics, despite being a test-only setting and also with the mismatch between the summarization and entailment domains. Figure 3 shows some additional interesting output examples of our multi-task model and how it generates summaries that are better at being logically entailed by the input document, whereas the baseline model contains some crucial contradictory or unrelated information.",
  "y": "similarities"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_0",
  "x": "****FASTFUSIONNET: NEW STATE-OF-THE-ART FOR DAWNBENCH SQUAD**** **ABSTRACT** In this technical report, we introduce FastFusionNet, an efficient variant of FusionNet <cite>[12]</cite> . FusionNet is a high performing reading comprehension architecture, which was designed primarily for maximum retrieval accuracy with less regard towards computational requirements. For FastFusionNets we remove the expensive CoVe layers [21] and substitute the BiLSTMs with far more efficient SRU layers [19] . The resulting architecture obtains state-of-the-art results on DAWNBench [5] while achieving the lowest training and inference time on SQuAD [25] to-date. The code is available at https://github.com/felixgwu/FastFusionNet.",
  "y": "extends differences"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_1",
  "x": "The resulting architecture obtains state-of-the-art results on DAWNBench [5] while achieving the lowest training and inference time on SQuAD [25] to-date. The code is available at https://github.com/felixgwu/FastFusionNet. ---------------------------------- **** In this technical report, we analyze the inference bottlenecks of FusionNet <cite>[12]</cite> and introduce FastFusionNet that tackles them. In our experiments, we show that FastFusionNet achieves new state-of-the-art training and inference time on SQuAD based on the metrics of DAWNBench. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_2",
  "x": "**DRQA** DrQA [2] is one of the simplest reading comprehension model, which employs a variety of features including pre-trained word vectors, term frequencies, part-of-speech tags, name entity relations, and the fact that whether a context word is in the question or not, encodes the features with RNNs, and predicts the start and end of an answer with a PointerNet-like module [34] . ---------------------------------- **ANALYSIS OF FUSIONNET** FusionNet <cite>[12]</cite> is reading comprehension model built on top of DrQA by introducing Fully-aware attention layers (context-question attention and context self-attention), contextual embeddings [21] , and more RNN layers. Their proposed fully-aware attention mechanism uses the concatenation of layers of hidden representations as the query and the key to compute attention weights, which shares a similar intuition as DenseNet [11] . FusionNet was the state-of-the-art reading comprehension model at the time of writing (Oct. 4th 2017).",
  "y": "background"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_3",
  "x": "Figure 2 provides an analysis of the individual components of FusionNet that the contextual embedding layer, i.e. CoVe [21] , with several layers of wide LSTMs, takes up to 35.5% of the inference time while only contributing a 1.1% improvement of F1 Score (from 82.5% to 83.6%) Huang et al. <cite>[12]</cite> . Additionally, the LSTM layers contribute to 58.8% [19] , GRU [3] , LSTM [10] , QANet Encoding block (with 2 conv layers and a 8-head attention) [39] , 5 Convolution layers with gated linear unit (GLU) [6, 35] . All input and hidden sizes are 128. of the inference time. Therefore, we propose to remove the contextual embedding layer and replace each bidirectional LSTM layer with two layers of bidirectional SRU [19] . Figure 3 shows that SRU is faster than LSTM [10] , GRU [3] , QANet Encoder [39] , and 5-layer CNN w/ GLU [6, 35] . We time a 5-layer CNN since it matches the performance of one layer SRU.",
  "y": "background"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_4",
  "x": "Figure 3 shows that SRU is faster than LSTM [10] , GRU [3] , QANet Encoder [39] , and 5-layer CNN w/ GLU [6, 35] . We time a 5-layer CNN since it matches the performance of one layer SRU. ---------------------------------- **FASTFUSIONNET** Here we introduce FastFusionNet which addresses the inference bottlenecks of FusionNet <cite>[12]</cite> . There are two differences compared to FusionNet: i) the CoVe [21] layers are removed and ii) each BiLSTM layer is replaced with two BiSRU layers. We closely follow the implementation of Huang et al. <cite>[12]</cite> described in their paper except for the changes above.",
  "y": "extends differences"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_5",
  "x": "We time a 5-layer CNN since it matches the performance of one layer SRU. ---------------------------------- **FASTFUSIONNET** Here we introduce FastFusionNet which addresses the inference bottlenecks of FusionNet <cite>[12]</cite> . There are two differences compared to FusionNet: i) the CoVe [21] layers are removed and ii) each BiLSTM layer is replaced with two BiSRU layers. We closely follow the implementation of Huang et al. <cite>[12]</cite> described in their paper except for the changes above. Following Huang et al. <cite>[12]</cite> , the hidden size of each SRU is set to 125, resulting in a 250-d output feature of each BiSRU regardless of the input size.",
  "y": "extends differences"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_6",
  "x": "Following Huang et al. <cite>[12]</cite> , the hidden size of each SRU is set to 125, resulting in a 250-d output feature of each BiSRU regardless of the input size. In the following explanation, we use [A; B] to represent concatenation in the feature dimension. Attn(Q, K, V) represents the attention mechanism taking the query Q, the key K, and the value V as inputs. Assuming O being the output, we have Input Features. Following Chen et al. [2] , we use 300-dim GloVe [24] vectors, term-frequency, part-of-speech (POS) tags, and named entity recognition (NER) tags as features for each word in the context or the question. We fine-tune the embedding vector of the padding token, the unknown word token, and the top 1000 most frequent words in the training set.",
  "y": "similarities uses"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_7",
  "x": "In the following explanation, we use [A; B] to represent concatenation in the feature dimension. Attn(Q, K, V) represents the attention mechanism taking the query Q, the key K, and the value V as inputs. Assuming O being the output, we have Input Features. Following Chen et al. [2] , we use 300-dim GloVe [24] vectors, term-frequency, part-of-speech (POS) tags, and named entity recognition (NER) tags as features for each word in the context or the question. We fine-tune the embedding vector of the padding token, the unknown word token, and the top 1000 most frequent words in the training set. Like others <cite>[12]</cite> we use a randomly initialized the trainable embedding layer with 12 dimensions for POS tags and 8 dimensions for NER.",
  "y": "similarities"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_8",
  "x": "where The Question-Context Attention Layer is a fully-aware attention module <cite>[12]</cite> which takes the history (concatenation of GloVe, low-level, and high-level features) of each context word and question words as query and key for three attention modules, and represents each context word as three different vectors: ---------------------------------- **S). ANOTHER 2-LAYER SRU PROCESSES THE CONCATENATION OF ALL PREVIOUS CONTEXT WORD VECTORS** To be specific, The Context Self-Attention Layer is another fully-aware attention module that treats the history of words (GloVe vectors, Answer Prediction Layer.",
  "y": "similarities"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_9",
  "x": "---------------------------------- **DAWNBENCH RESULTS** We report the performance of our FastFusionNet on DAWNBench [5] . We consider three baselines: i) FusionNet ii) FusionNet without CoVE, and iii) BERT-base. For BERT-base, we use the open source code 3 . Our FastFusionNet reaches F1 75% in 4 epochs and achieves at F1 82.5% at the end which matches the reported F1 82.5% of FusionNet without CoVe on SQuAD development set <cite>[12]</cite> . The training time track aims to minimize the time to train a model up to at least 75% F1 score on SQuAD development set.",
  "y": "differences"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_10",
  "x": "Table 1 shows that our FastFusionNet reaches F1 75.0% within 20 minutes (after 4 epochs), which gives a 45% speedup compared to the winner DrQA(ParlAI) on the leaderboard. Notably, we use an Nvidia GTX-1080 GPU which is about 22% slower than their Nvidia RTX-2080 GPU. When controlling the generation of GPUs and comparing our model with a DrQA (ParlAI) trained on an Nvidia V100, our model achieves a 3.1\u00d7 speedup. Compared to FusionNet, FastFusionNet is 23% faster to reach 75% F1 score; however, in terms of the training time per epoch, it is in fact 2.6\u00d7 as fast as FusionNet. <cite>[12]</cite> here since our reimplementation is about 0.5% F1 score worse. The inference time track evaluates the average 1-example inference latency of a model with an F1 score at least 75%. Our FastFusionNet reduces the 1-example latency down to 7.9 ms, which is 2.8\u00d7 as fast as a BERT-base and 12.7\u00d7 over BiDAF.",
  "y": "differences"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_0",
  "x": "As intelligent systems/robots are brought out of the laboratory and into the physical world, they must become capable of natural everyday conversation with their human users about their physical surroundings. Among other competencies, this involves the ability to learn and adapt mappings between words, phrases, and sentences in Natural Language (NL) and perceptual aspects of the external environment -this is widely known as the grounding problem. Our work is similar in spirit to e.g. (Roy, 2002; Skocaj et al., 2011) but advances it in several aspects <cite>(Yu et al., 2016)</cite> . In this demo paper, we present a dialogue agent that learns visually grounded word meanings interactively from a human tutor, which we call: VOILA (Visually Optimised Interactive Learning Agent). Our goal is to enable this agent to learn to identify and describe objects/attributes (colour 1 http://www.furhatrobotics.com/ and shape in this case) in its immediate visual environment through interaction with human users, incrementally, over time. Unlike a lot of past work (Silberer and Lapata, 2014; Thomason et al., 2016; Matuszek et al., 2014) , here we assume that the agent is in the position of a child, who does not have any prior knowledge of perceptual categories. Hence, the agent must learn from scratch: (1) the perceptual/visual categories themselves; and (2) how NL expressions map to these; and in addition, (3) as a standard conversational agent, the agent much also learn to conduct natural, spontaneous conversations with real humans.",
  "y": "uses"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_1",
  "x": "Our goal is to enable this agent to learn to identify and describe objects/attributes (colour 1 http://www.furhatrobotics.com/ and shape in this case) in its immediate visual environment through interaction with human users, incrementally, over time. Unlike a lot of past work (Silberer and Lapata, 2014; Thomason et al., 2016; Matuszek et al., 2014) , here we assume that the agent is in the position of a child, who does not have any prior knowledge of perceptual categories. Hence, the agent must learn from scratch: (1) the perceptual/visual categories themselves; and (2) how NL expressions map to these; and in addition, (3) as a standard conversational agent, the agent much also learn to conduct natural, spontaneous conversations with real humans. In this demonstration, VOILA plays the role of an interactive, concept learning agent that takes initiative in the dialogues and actively learns novel visual knowledge from the feedback from the human tutor. What sets VOILA apart from other work in this area is: \u2022 VOILA's dialogue strategy is optimised via Reinforcement Learning to achieve an optimal trade-off between the accuracy of the concepts it learns/has learnt from users, and the effort that the dialogues incur on the users: this is a form of active learning where the agent only asks about something if it doesn't already know the answer with some appropriate confidence (see <cite>(Yu et al., 2016)</cite> for more detail). \u2022 VOILA is trained on a corpus of real HumanHuman conversations (Yu et al., 2017) , and is thus able to process natural human dialogue, which contains phenomena such as self-corrections, repetitions and restarts, pauses, fillers, and continuations VOILA is deployed onto Furhat, a humanlike robot head with a custom back-projected face, built-in stereo microphones, and a Microsoft",
  "y": "background"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_2",
  "x": "What sets VOILA apart from other work in this area is: \u2022 VOILA's dialogue strategy is optimised via Reinforcement Learning to achieve an optimal trade-off between the accuracy of the concepts it learns/has learnt from users, and the effort that the dialogues incur on the users: this is a form of active learning where the agent only asks about something if it doesn't already know the answer with some appropriate confidence (see <cite>(Yu et al., 2016)</cite> for more detail). \u2022 VOILA is trained on a corpus of real HumanHuman conversations (Yu et al., 2017) , and is thus able to process natural human dialogue, which contains phenomena such as self-corrections, repetitions and restarts, pauses, fillers, and continuations VOILA is deployed onto Furhat, a humanlike robot head with a custom back-projected face, built-in stereo microphones, and a Microsoft ---------------------------------- **INTERACTIVE MULTIMODAL FRAMEWORK** We developed a multimodal framework in support of building an interactive learning system, which loosely follows that of<cite> Yu et al. (2016)</cite> . The framework consists of two core modules:",
  "y": "differences"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_3",
  "x": "**INTERACTIVE MULTIMODAL FRAMEWORK** We developed a multimodal framework in support of building an interactive learning system, which loosely follows that of<cite> Yu et al. (2016)</cite> . The framework consists of two core modules: Vision Module The vision module produces visual attribute predictions, using two base feature categories: the HSV colour space for colour attributes, and a 'bag of visual words' (i.e. PHOW descriptors) for the object shapes/class. It consists of a set of binary classifiers -Logistic Regression SVM classifiers with Stochastic Gradient Descent (SGD) (Zhang, 2004) -to incrementally learn attribute predictions. The visual classifiers ground visual attribute words such as 'red', 'circle' etc. that appear as parameters of the Dialogue Acts used in the system.",
  "y": "similarities uses"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_4",
  "x": "The first MDP performs a kind of active learning: the learner/agent only acquires the feedback from humans about a visual attribute if it is not confident enough already about its own predictions. Following previous work <cite>(Yu et al., 2016)</cite> , here we use a positive confidence threshold, which determines when the agent believes its own predictions. For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as<cite> Yu et al. (2016)</cite> point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent. We therefore assign a separate but dependent component MDP for adjusting the threshold dynamically in order to optimise the trade-off between accuracy and cost. Note now that the adjusted confidence threshold will affect the agent's dialogue behaviour, modeled in the other MDP presented in the next section (natural interaction with humans). ---------------------------------- **HOW TO LEARN: NATURAL INTERACTION WITH HUMANS**",
  "y": "uses"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_5",
  "x": "Following previous work <cite>(Yu et al., 2016)</cite> , here we use a positive confidence threshold, which determines when the agent believes its own predictions. For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as<cite> Yu et al. (2016)</cite> point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent. We therefore assign a separate but dependent component MDP for adjusting the threshold dynamically in order to optimise the trade-off between accuracy and cost. Note now that the adjusted confidence threshold will affect the agent's dialogue behaviour, modeled in the other MDP presented in the next section (natural interaction with humans). ---------------------------------- **HOW TO LEARN: NATURAL INTERACTION WITH HUMANS** The second MDP, as a purely conversational agent, aims at managing natural, spontaneous conversation with human partners or other agents to achieve the final goal, i.e. gain useful information about visual attributes.",
  "y": "motivation"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_6",
  "x": "Here, we divide this interactive learning task into two sub-tasks, modeled as a hierarchical Markov Decision Process, consisting of two interdependent MDPs in charge of decisions about: \"when to learn\" and \"how to learn\". ---------------------------------- **WHEN TO LEARN: ADAPTIVE CONFIDENCE THRESHOLD** The first MDP performs a kind of active learning: the learner/agent only acquires the feedback from humans about a visual attribute if it is not confident enough already about its own predictions. Following previous work <cite>(Yu et al., 2016)</cite> , here we use a positive confidence threshold, which determines when the agent believes its own predictions. For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as<cite> Yu et al. (2016)</cite> point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent. We therefore assign a separate but dependent component MDP for adjusting the threshold dynamically in order to optimise the trade-off between accuracy and cost.",
  "y": "extends uses differences"
 },
 {
  "id": "1a8c7d22709cae34fbc1eb70fe5189_0",
  "x": "The method has been applied to a wide range of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. ---------------------------------- **INTRODUCTION** This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b) , which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007) , dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012) , context free grammar (CFG) parsing <cite>(Collins and Roark</cite>, 2004; Zhang and Clark, 2009; Zhu et al., 2013) , combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013) , achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010) , joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012) , joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013) , and joint segmentation, POS-tagging and parsing . In addition to the aforementioned tasks, the framework can be applied to all structural prediction tasks for which the output can be constructed using an incremental process.",
  "y": "uses"
 },
 {
  "id": "1a8c7d22709cae34fbc1eb70fe5189_1",
  "x": "Second, free from DPstyle constraints and Markov-style independence assumptions, the framework allows arbitrary features to be defined to capture structural patterns. In addition to feature advantages, the high accuracies of this framework are also enabled by direct interactions between learning and search (Daum\u00e9 III and Marcu, 2005; Huang et al., 2012; Zhang and Nivre, 2012) . ---------------------------------- **TUTORIAL OVERVIEW** In this tutorial, we make an introduction to the framework, illustrating how it can be applied to a range of NLP problems, giving theoretical discussions and demonstrating a software implementation. We start with a detailed introduction of the framework, describing the averaged perceptron algorithm (Collins, 2002) and its efficient implementation issues (Zhang and Clark, 2007) , as well as beam-search and the early-update strategy <cite>(Collins and Roark, 2004)</cite> . We then illustrate how the framework can be applied to NLP tasks, including word segmentation, joint segmentation & POS-tagging, labeled and unlabeled dependency parsing, joint POS-tagging and dependency parsing, CFG parsing, CCG parsing, and joint segmentation, POS-tagging and parsing.",
  "y": "uses"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_0",
  "x": "<cite>Levy et al. (2017)</cite> present a reformulation of RE, where the task is framed as reading comprehension. In this formulation, each relation type (e.g. author, occupation) is mapped to at least one natural language question template (e.g. \"Who is the author of x?\"), where x is filled with an entity (e.g. \"Inferno\"). The model is then tasked with finding an answer (\"Dante Alighieri\") to this question with respect to a given context. They show that this formulation of the problem both outperforms off-the-shelf RE systems in the typical RE setting and, in addition, enables generalization to unspecified and unseen types of relations. X-WikiRE enables exploration of this reformulation of RE in a multilingual setting. Contributions We introduce a new, largescale multilingual dataset (X-WikiRE) of reading comprehension-based RE for English, German, French, Spanish, and Italian, facilitating research on multilingual methods for RE. Our dataset covers more languages (five) and is at least an order of magnitude larger than existing multilingual RE datasets, e.g., TAC 2016 (Ellis et al., 2015) , which covers three languages and consists of \u2248 90k examples.",
  "y": "motivation background"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_1",
  "x": "In an open-domain setting, however, we are interested in the far more difficult problem of extracting unseen relation types. Open RE methods (Yates et al., 2007; Fader et al., 2011) do not require relationspecific data, but treat different phrasings of the same relation as different relations and rely on a combination of syntactic features (e.g. dependency parses) and normalisation rules, and so have limited generalization capacity. Zero-shot relation extraction <cite>Levy et al. (2017)</cite> propose a novel approach towards achieving this generalization by transforming relations into natural language question templates. For instance, the relation born in(x, y) can be expressed as \"Where was x born?\" or \"In which place was x born?\". Then, a reading comprehension model (Seo et al., 2016; Chen et al., 2017) can be trained on question, answer, and context examples where the x slot is filled with an entity and the y slot is either an answer if the answer is present in the context, or NIL. The model is then able to extract relation instances (given expressions of the relations as questions) from raw text. To test this \"harsh zero-shot\" setting of relation extraction, they build a dataset for RE as machine comprehension from WikiReading (Hewlett et al., 2016) ing comprehension model is able to use linguistic cues to identify relation paraphrases and lexicosyntactic patterns of textual deviation from questions to answers, enabling it to identify instances of new relations.",
  "y": "motivation background"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_2",
  "x": "We perform data integration on Wikidata, as described by Hewlett et al. (2016) : for each entity in Wikipedia we take the corresponding Wikidata document, add the Wikipedia page text, and denormalize the statements. This consists of replacing the property and value ids of each statement in the document with the text label for values which are entities, and with the human readable form for numeric values (e.g. timestamps are converted to natural forms like \"25 May 1994\") obtaining a tuple (property, entity). 2 Slot-filling data To extract the contexts for each triple in our dataset we use the distant supervision method described by <cite>Levy et al. (2017)</cite> . For each Wikidata document belonging to a given entity 1 we take all the denormalized tuples (property, entity 2 ) and extract the first sentence in the text containing both entity 1 and entity 2 . Negatives (contexts without answers) are constructed by finding pairs of triples with common entity 2 type (to ensure they contain good distractors), swapping their context if entity 2 is not present in the context of the other triple. Querification <cite>Levy et al. (2017)</cite> created 1192 question templates for 120 Wikidata properties. A template contains a placeholder for an entity x (e.g. for property \"author\", some templates are \"Who wrote the novel x?\" and \"Who is the author of x?\"), which can be automatically filled in to create questions so that question \u2248 template(property, x)).",
  "y": "uses"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_3",
  "x": "Querification <cite>Levy et al. (2017)</cite> created 1192 question templates for 120 Wikidata properties. A template contains a placeholder for an entity x (e.g. for property \"author\", some templates are \"Who wrote the novel x?\" and \"Who is the author of x?\"), which can be automatically filled in to create questions so that question \u2248 template(property, x)). For our multilingual dataset, we had these templates translated by human translators. The translators attempted to translate each of the original 1192 templates. If a template was difficult to translate, they were in- structed to discard it. They were also instructed to create their own templates, paraphrasing the original ones when possible. This resulted in a varying number of templates for each of the properties across languages. In addition to the entity placeholder, some languages with richer morphology (Spanish, Italian, and German) required extra placeholders in the templates because of agreement phenomena (gender). We added a placeholder for definite articles, as well as one for gender-dependent filler words. The gender is automatically inferred from the Wikipedia page statistics and a few heuristics.",
  "y": "extends"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_5",
  "x": "3 For BERT, we take the contexualized word representations from the final layer as input to our machine comprehension model's question and context Bi-LSTM encoders. We do not fine-tune the pre-trained model. ---------------------------------- **EXPERIMENTS** Following <cite>Levy et al. (2017)</cite> , we distinguish between the traditional RE setting where the aim is to generalize to unseen entities (UnENT) and the zero-shot setting (UnREL) where the aim is to do so for unseen relation types (see Section 2). Our goal is to answer these three questions: A) how well can RE models be transferred across languages? B) in the difficult UnREL setting, can the variance between languages in the number of instances of relations (see Figure 2 ) be exploited to enable more robust RE ? C) can one jointly-trained multilingual model which performs RE in multiple languages perform comparably to or outperform its individual monolingual counterparts? For all experiments, we take the multiple templates approach where a model sees different paraphrases of the same question during training.",
  "y": "motivation similarities"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_6",
  "x": "Following <cite>Levy et al. (2017)</cite> , we distinguish between the traditional RE setting where the aim is to generalize to unseen entities (UnENT) and the zero-shot setting (UnREL) where the aim is to do so for unseen relation types (see Section 2). Our goal is to answer these three questions: A) how well can RE models be transferred across languages? B) in the difficult UnREL setting, can the variance between languages in the number of instances of relations (see Figure 2 ) be exploited to enable more robust RE ? C) can one jointly-trained multilingual model which performs RE in multiple languages perform comparably to or outperform its individual monolingual counterparts? For all experiments, we take the multiple templates approach where a model sees different paraphrases of the same question during training. This approach was shown by <cite>Levy et al. (2017)</cite> to have significantly better paraphrasing abilities than when only one question template or simpler relation descriptions are employed. Evaluation Our evaluation methodology follows <cite>Levy et al. (2017)</cite> . We compute precision, recall and F1 by comparing spans predicted by the 3 https://github.com/google-research/ bert/blob/master/multilingual.md models with gold answers. Precision is equal to the true positives divided by total number of nonnil answers predicted by a system.",
  "y": "uses"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_7",
  "x": "Evaluation Our evaluation methodology follows <cite>Levy et al. (2017)</cite> . We compute precision, recall and F1 by comparing spans predicted by the 3 https://github.com/google-research/ bert/blob/master/multilingual.md models with gold answers. Precision is equal to the true positives divided by total number of nonnil answers predicted by a system. Recall is equal to the true positives divided by the total number of instances that are non-nil in the ground truth answers. Word order and punctuation are not considered. 4 ----------------------------------",
  "y": "uses"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_11",
  "x": "All monolingual models' word embeddings were initialised using fastText embeddings trained on each language's Wikipedia and common crawl corpora, 7 except for the comparison experiments described in sub-section 5.1 where GloVe (Pennington et al., 2014) was used for comparability with <cite>Levy et al. (2017)</cite> . ---------------------------------- **RELATED WORK** Multilingual NLU Advances in natural language understanding tasks have been as impressive as they have been fast-paced. Until recently, however, the multilingual aspect of such tasks has not received as much attention. This is pri-Lang. UnENT UnREL <cite>Levy et al. (2017)</cite> Faruqui and Kumar (2015) employed a pipeline of machine translation systems to translate to English, then Open RE systems to perform RE on the translated text, followed by crosslingual projection back to source language.",
  "y": "background"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_12",
  "x": "This is pri-Lang. UnENT UnREL <cite>Levy et al. (2017)</cite> Faruqui and Kumar (2015) employed a pipeline of machine translation systems to translate to English, then Open RE systems to perform RE on the translated text, followed by crosslingual projection back to source language. Verga et al. (2016) apply the universal schema framework (Riedel et al., 2013) on top of multilingual embeddings to extract relations from Spanish text without using Spanish training data. This approach, however, only enables generalization to unseen entities and does not have the flexibility to predict unseen relations. Furthermore, both of these works faced a fundamental difficulty with evaluation. The former resort to manual annotation of a small number of examples (1000) in each language and the latter use the 2012 TAC Spanish slot-filling evaluation dataset in which \"the coverage of facts in the available annotation is very small\". With the introduction of X-WikiRE, this work provides the first large-scale dataset and benchmark for the evaluation of multilingual RE spanning five languages.",
  "y": "background"
 },
 {
  "id": "1b424cab4d7008997a31be8c2e5198_0",
  "x": "In many tasks in natural language processing, it is necessary to match or compare two distributed representations. These representations may refer to whole sentences, word contexts or any other construct. For concreteness, let u and v be two representations we want to match. In order to facilitate the matching, it is often beneficial to explicitly create new features like element-wise absolute difference (|u\u2212v|) and element-wise product (u \u00b7 v) that augment u and v. The combined feature vector is then processed by further layers in the task specific neural network. For example, Tai et al. (2015) use these heuristics to improve semantic representations. Most notably, for the natural language inference task, augmenting the hypothesis (u) and premise (v) representations with |u\u2212v| and u \u00b7 v considerably improves performance in a siamese architecture <cite>Mou et al. (2016)</cite> . This is also used in the more sophisticated models of Chen et al. (2017) , where u and v represent word contexts.",
  "y": "background"
 },
 {
  "id": "1b424cab4d7008997a31be8c2e5198_1",
  "x": "Most notably, for the natural language inference task, augmenting the hypothesis (u) and premise (v) representations with |u\u2212v| and u \u00b7 v considerably improves performance in a siamese architecture <cite>Mou et al. (2016)</cite> . This is also used in the more sophisticated models of Chen et al. (2017) , where u and v represent word contexts. Several of these approaches are explored in the Compare-and-Aggregate framework by Wang & Jiang (2017) . In this paper we focus on polynomial features like u \u00b7 v for the natural language inference task, where it is trying to capture similarity between u and v. It is also a monomial of degree 2. We investigate two aspects of such terms -the use of scaling and the use of higher degree polynomials. The motivation for the former is the following. The values taken by individual elements of u and u \u00b7 v will in general have slightly different statistical distributions.",
  "y": "background uses"
 },
 {
  "id": "1b424cab4d7008997a31be8c2e5198_2",
  "x": "This is observed both for models that only use encodings of whole sentences and more complex ones. ---------------------------------- **SCALED POLYNOMIAL FEATURES** We present our work in the context of two baseline models for the natural language inference task. In this task, given a pair of sentences (premise and hypothesis), it needs to be classified into one of three categories -entailment, contradiction and neutral. In the first model, both the premise and the hypothesis sentence are encoded using a bidirectional LSTM Hochreiter & Schmidhuber (1997) and the intermediate states are max-pooled to get the respective representations u and v. We refer to this as the InferSent model Conneau et al. (2017) . The standard matching feature of <cite>Mou et al. (2016)</cite> uses a concatenation of u, v, |u\u2212v| and u \u00b7 v. We define the following new matching feature vector that scales the multiplicative term by a constant factor \u03b7 > 0.",
  "y": "background uses"
 },
 {
  "id": "1b424cab4d7008997a31be8c2e5198_3",
  "x": "and In w poly3 , the additional term is the sum of the two possible monomials of degree 3 involving both u and v. In w poly4 , the fourth degree term is the sum of the 3 possible monomials of degree 4 involving both u and v. Note that we scale the 3rd degree terms by \u03b7 2 and the 4th degree terms by \u03b7 3 . If the dimension of u and v is d, the dimensions of w poly2 , w poly3 and w poly4 are all 4d. In each case, the feature vector is fed into a fully connected layer(s), before computing the 3-way softmax in the classification layer. It is possible to use each of the degree 3 and 4 terms separately as a feature, but this did not make our models substantially more accurate. Choosing \u03b7 = 1 in w poly2 reduces it to the matching feature vector proposed by <cite>Mou et al. (2016)</cite> . The same procedure is repeated for the other baseline model, namely ESIM Chen et al. (2017) .",
  "y": "uses"
 },
 {
  "id": "1bcd442a685e5fb2d0f3f44d3c66c3_0",
  "x": "---------------------------------- **INTRODUCTION** Recent advances in deep learning have shown exceptional results in language-related tasks such as machine translation, question answering, or sentiment analysis. However, the supervised approaches that capture the underlying statistical patterns in language are not sufficient in perceiving the interactive nature of communication and how humans use it for coordination. It is thus crucial to learn to communicate by interaction, i.e., communication must emerge out of necessity. Such study gives further insights into how communication protocols emerge for successful coordination and the ability of a learner to understand the emerged language. Several recent works<cite> (Lazaridou, Peysakhovich, and Baroni 2016</cite>; Havrylov and Titov 2017; Lazaridou et al. 2018; <cite>Mordatch and Abbeel 2018)</cite> , have shown that in multi-agent cooperative setting of referential games, deep reinforcement learning can successfully induce communication protocols.",
  "y": "motivation background"
 },
 {
  "id": "1bcd442a685e5fb2d0f3f44d3c66c3_1",
  "x": "Such study gives further insights into how communication protocols emerge for successful coordination and the ability of a learner to understand the emerged language. Several recent works<cite> (Lazaridou, Peysakhovich, and Baroni 2016</cite>; Havrylov and Titov 2017; Lazaridou et al. 2018; <cite>Mordatch and Abbeel 2018)</cite> , have shown that in multi-agent cooperative setting of referential games, deep reinforcement learning can successfully induce communication protocols. In these games, communication success is the only supervision during learning, and the meaning of the emergent messages gets grounded during the game. In<cite> (Lazaridou, Peysakhovich, and Baroni 2016)</cite> , the authors have restricted the message to be a single symbol token picked from a fixed vocabulary while in (Havrylov and Titov 2017) , the message is considered to be a sequence of symbols. (Lazaridou et al. 2018) demonstrates that successful communication can also emerge in environments which present raw pixel input. Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
  "y": "background"
 },
 {
  "id": "1bcd442a685e5fb2d0f3f44d3c66c3_2",
  "x": "(<cite>Mordatch and Abbeel 2018)</cite> further extends the scope of mode of communication by also studying the emergence of non-verbal communication. While these works have studied a wide variety of game setups as well as variations in communication rules, none of them have considered written language system as a mode of communication. Historically, written language systems have shown complex patterns in evolution over time. Moreover, the process of writing requires sophisticated graphomotor skills which involves both linguistic and non-linguistic factors. Thus writing systems can be considered crucial for understanding autonomous system development. We are further motivated by the work in (Ganin et al. 2018) , where the authors demonstrate that artificial agents can produce visual representations similar to those created by humans. This can only be achieved by giving them access to the same tools that we use to recreate the world around us.",
  "y": "background"
 },
 {
  "id": "1bcd442a685e5fb2d0f3f44d3c66c3_3",
  "x": "We extend this idea to study emergence of writing systems. ---------------------------------- **REFERENTIAL GAME FRAMEWORK** In our work, we have used two referential game setups that are slight modifications to the ones used in<cite> (Lazaridou, Peysakhovich, and Baroni 2016</cite>; Lazaridou et al. 2018 ). There are two players, a sender and a receiver. From a given set of images where the sender only has access to the target image t; Distractor Aware (D-Aware): where the sender has access to the candidate set C = t \u222a D. In both these variations, the sender has to come up with a message M l = {m j } l j=1 , which is a sequence of l brushstrokes.",
  "y": "extends differences"
 },
 {
  "id": "1bcd442a685e5fb2d0f3f44d3c66c3_4",
  "x": "For both the agents, we pose the learning of communication protocols as maximization of the expected return Er[R(r)], where R is the reward function. The payoff is 1 for both the agents iff R \u03c6 (f s (S \u03b8 (R(M i ), h i , V )), U ) = t , where i is the last timestep of the episode. In all other cases and intermediate timesteps, the payoff is 0. Because of the high dimensional search space introduced due to brushstrokes, we use Proximal Policy Optimization (PPO)<cite> (Schulman et al. 2017)</cite> for optimizing the weights of sender and receiver agents. ---------------------------------- **IMAGES** We have used CIFAR-10 dataset (Krizhevsky, Hinton, and others 2009) , as a source of images. From the test set of CIFAR-10, we randomly sample 100 images from each class and represent them as outputs from relu7 layer of pretrained VGG-16 convNet (Simonyan and Zisserman 2014).",
  "y": "background"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_0",
  "x": "****ARE YOU A RACIST OR AM I SEEING THINGS? ANNOTATOR INFLUENCE ON HATE SPEECH DETECTION ON TWITTER.**** **ABSTRACT** Hate speech in the form of racism and sexism is commonplace on the internet <cite>(Waseem and Hovy, 2016)</cite> . For this reason, there has been both an academic and an industry interest in detection of hate speech. The volume of data to be reviewed for creating data sets encourages a use of crowd sourcing for the annotation efforts. In this paper, we provide an examination of the influence of annotator knowledge of hate speech on classification models by comparing classification results obtained from training on expert and amateur annotations. We provide an evaluation on our own data set and run our models on <cite>the data set</cite> released by <cite>Waseem and Hovy (2016)</cite>.",
  "y": "background"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_1",
  "x": "The volume of data to be reviewed for creating data sets encourages a use of crowd sourcing for the annotation efforts. In this paper, we provide an examination of the influence of annotator knowledge of hate speech on classification models by comparing classification results obtained from training on expert and amateur annotations. We provide an evaluation on our own data set and run our models on <cite>the data set</cite> released by <cite>Waseem and Hovy (2016)</cite>. We find that amateur annotators are more likely than expert annotators to label items as hate speech, and that systems trained on expert annotations outperform systems trained on amateur annotations. ---------------------------------- **INTRODUCTION** Large amounts of hate speech on exists on platforms that allow for user generated documents, which creates a need to detect and filter it (Nobata et al., 2016) , and to create data sets that contain hate speech and are annotated for the occurrence of hate speech.",
  "y": "uses"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_2",
  "x": "In addition, it is important to understand how different manners of obtaining labeling can influence the classification models and how it is possible to obtain good annotations, while ensuring that annotators are not likely to experience adverse effects of annotating hate speech. Our contribution We provide annotations of 6, 909 tweets for hate speech by annotators from CrowdFlower and annotators that have a theoretical and applied knowledge of hate speech, henceforth amateur and expert annotators 1 . Our data set extends the <cite>Waseem and Hovy (2016)</cite> data set by 4, 033 tweets. We also illustrate, how amateur and expert annotations influence classification efforts. Finally, we show the effects of allowing majority voting on classification and agreement between the amateur and expert annotators. ---------------------------------- **DATA**",
  "y": "extends"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_3",
  "x": "Our data set is obtained by sampling tweets from the 130k tweets extracted by <cite>Waseem and Hovy (2016)</cite> . The order of the tweets is selected by our database connection, thus allowing for an overlap with the data set released by <cite>Waseem and Hovy (2016)</cite> . We find that there is an overlap of 2, 876 tweets (see Table 1) between the two data sets. Racism Sexism Neither Count 1 95 2780 Given the distribution of the labels in <cite>Waseem and Hovy (2016)</cite> and our annotated data set (see Table  2 ), it is to be expected the largest overlap occurs with tweets annotated as negative for hate speech. Observing Table 2 , we see that the label distribution in our data set generally differs from the distribution in <cite>Waseem and Hovy (2016)</cite> . In fact, we see that the amateur majority voted labels is the only distribution that tends towards a label distribution similar to <cite>Waseem and Hovy (2016)</cite> Our annotation effort deviates from <cite>Waseem and Hovy (2016)</cite> . In addition to \"racism\", \"sexism\", and \"neither\", we add the label \"both\" for tweets that contain both racism and sexism.",
  "y": "uses"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_4",
  "x": "Our data set extends the <cite>Waseem and Hovy (2016)</cite> data set by 4, 033 tweets. We also illustrate, how amateur and expert annotations influence classification efforts. Finally, we show the effects of allowing majority voting on classification and agreement between the amateur and expert annotators. ---------------------------------- **DATA** Our data set is obtained by sampling tweets from the 130k tweets extracted by <cite>Waseem and Hovy (2016)</cite> . The order of the tweets is selected by our database connection, thus allowing for an overlap with the data set released by <cite>Waseem and Hovy (2016)</cite> .",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_5",
  "x": "Our data set is obtained by sampling tweets from the 130k tweets extracted by <cite>Waseem and Hovy (2016)</cite> . The order of the tweets is selected by our database connection, thus allowing for an overlap with the data set released by <cite>Waseem and Hovy (2016)</cite> . We find that there is an overlap of 2, 876 tweets (see Table 1) between the two data sets. Racism Sexism Neither Count 1 95 2780 Given the distribution of the labels in <cite>Waseem and Hovy (2016)</cite> and our annotated data set (see Table  2 ), it is to be expected the largest overlap occurs with tweets annotated as negative for hate speech. Observing Table 2 , we see that the label distribution in our data set generally differs from the distribution in <cite>Waseem and Hovy (2016)</cite> . In fact, we see that the amateur majority voted labels is the only distribution that tends towards a label distribution similar to <cite>Waseem and Hovy (2016)</cite> Our annotation effort deviates from <cite>Waseem and Hovy (2016)</cite> . In addition to \"racism\", \"sexism\", and \"neither\", we add the label \"both\" for tweets that contain both racism and sexism.",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_6",
  "x": "---------------------------------- **DATA** Our data set is obtained by sampling tweets from the 130k tweets extracted by <cite>Waseem and Hovy (2016)</cite> . The order of the tweets is selected by our database connection, thus allowing for an overlap with the data set released by <cite>Waseem and Hovy (2016)</cite> . We find that there is an overlap of 2, 876 tweets (see Table 1) between the two data sets. Racism Sexism Neither Count 1 95 2780 Given the distribution of the labels in <cite>Waseem and Hovy (2016)</cite> and our annotated data set (see Table  2 ), it is to be expected the largest overlap occurs with tweets annotated as negative for hate speech. Observing Table 2 , we see that the label distribution in our data set generally differs from the distribution in <cite>Waseem and Hovy (2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_7",
  "x": "The order of the tweets is selected by our database connection, thus allowing for an overlap with the data set released by <cite>Waseem and Hovy (2016)</cite> . We find that there is an overlap of 2, 876 tweets (see Table 1) between the two data sets. Racism Sexism Neither Count 1 95 2780 Given the distribution of the labels in <cite>Waseem and Hovy (2016)</cite> and our annotated data set (see Table  2 ), it is to be expected the largest overlap occurs with tweets annotated as negative for hate speech. Observing Table 2 , we see that the label distribution in our data set generally differs from the distribution in <cite>Waseem and Hovy (2016)</cite> . In fact, we see that the amateur majority voted labels is the only distribution that tends towards a label distribution similar to <cite>Waseem and Hovy (2016)</cite> Our annotation effort deviates from <cite>Waseem and Hovy (2016)</cite> . In addition to \"racism\", \"sexism\", and \"neither\", we add the label \"both\" for tweets that contain both racism and sexism. We add this label, as the intersection of multiple oppressions can differ from the forms of oppression it consists of (Crenshaw, 1989) , and as such becomes a unique form of oppression.",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_8",
  "x": "We present the annotators with <cite>the tests</cite> from <cite>Waseem and Hovy (2016)</cite> . If a tweet fails any of <cite>the tests</cite>, the annotators are instructed to label it as the relevant form of hate speech. Expert annotators are given the choice of skipping tweets, if they are not confident in which label to assign, and a \"Noise\" label in case the annotators are presented with non-English tweets. Due to privacy concerns, all expert annotators are treated as a single entity. Amateur Annotations Amateur annotators are recruited on CrowdFlower without any selection, to mitigate selection biases. They are presented with 6, 909 tweets that have been annotated by the expert annotators. The amateur annotators are not provided with the option to skip tweets, as they are not presented tweets the experts had skipped or labeled as \"Noise\".",
  "y": "uses"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_9",
  "x": "They are presented with 6, 909 tweets that have been annotated by the expert annotators. The amateur annotators are not provided with the option to skip tweets, as they are not presented tweets the experts had skipped or labeled as \"Noise\". Annotator agreement Considering annotator agreement, we find that the inter-annotator agreement among the amateur annotators is \u03ba = 0.57 (\u03c3 = 0.08). Majority Vote Full Agreement Expert 0.34 0.70 The low agreement in Table 2 provides further evidence to the claim by Ross et al. (2016) that annotation of hate speech is a hard task. Table 2 suggests that if only cases of full agreement are considered, it is possible to obtain good annotations using crowdsourcing. Overlap Considering the overlap with the <cite>Waseem and Hovy (2016)</cite>, we see that the agreement is extremely low (mean pairwise \u03ba = 0.14 between all annotator groups and <cite>Waseem and Hovy (2016)</cite> ). Interestingly, we see that the vast majority of disagreements between our annotators and <cite>Waseem and Hovy (2016)</cite> , are disagreements where our annotators do not find hate speech but <cite>Waseem and Hovy (2016)</cite> the influence of the features listed in Table 4 for each annotator group.",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_10",
  "x": "They are presented with 6, 909 tweets that have been annotated by the expert annotators. The amateur annotators are not provided with the option to skip tweets, as they are not presented tweets the experts had skipped or labeled as \"Noise\". Annotator agreement Considering annotator agreement, we find that the inter-annotator agreement among the amateur annotators is \u03ba = 0.57 (\u03c3 = 0.08). Majority Vote Full Agreement Expert 0.34 0.70 The low agreement in Table 2 provides further evidence to the claim by Ross et al. (2016) that annotation of hate speech is a hard task. Table 2 suggests that if only cases of full agreement are considered, it is possible to obtain good annotations using crowdsourcing. Overlap Considering the overlap with the <cite>Waseem and Hovy (2016)</cite>, we see that the agreement is extremely low (mean pairwise \u03ba = 0.14 between all annotator groups and <cite>Waseem and Hovy (2016)</cite> ). Interestingly, we see that the vast majority of disagreements between our annotators and <cite>Waseem and Hovy (2016)</cite> , are disagreements where our annotators do not find hate speech but <cite>Waseem and Hovy (2016)</cite> the influence of the features listed in Table 4 for each annotator group.",
  "y": "differences"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_11",
  "x": "We obtain up to 3200 tweets for each user in our data set, calculate the TF-IDF scores, and identify the top 100 terms. We then add a binary feature signifying the occurrence of each of these 100 terms. Interestingly, this feature performs worse than any other feature. Particularly when trained on expert annotations, suggesting that hate speech may be more situational or that users engaging in hate speech, do not only, or even primarily engage in hate speech. Gender Following the indication that gender can positively influence classification scores <cite>(Waseem and Hovy, 2016)</cite> , we compute the gender of the users in our data set. To counteract the low coverage in <cite>Waseem and Hovy (2016)</cite> , we use a lexicon trained on Twitter (Sap et al., 2014) to calculate the probability of gender. Using these probabilities we assign binary gender.",
  "y": "uses"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_12",
  "x": "Interestingly, this feature performs worse than any other feature. Particularly when trained on expert annotations, suggesting that hate speech may be more situational or that users engaging in hate speech, do not only, or even primarily engage in hate speech. Gender Following the indication that gender can positively influence classification scores <cite>(Waseem and Hovy, 2016)</cite> , we compute the gender of the users in our data set. To counteract the low coverage in <cite>Waseem and Hovy (2016)</cite> , we use a lexicon trained on Twitter (Sap et al., 2014) to calculate the probability of gender. Using these probabilities we assign binary gender. Both the probability of a gender for a user and the binary gender are used as individual features. We find that using gender information only contributes to the classification score for amateur annotators.",
  "y": "extends"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_13",
  "x": "Minority Class Misclassification We find that some features trained on expert and amateur annotations result in misclassification on the minority classes, including identifying no instances of the mi- nority classes (see Table 4 ). These misclassifications of the minority classes are largely due to the small number of instances in those classes. In spite of this, we do not believe that only boosting the size of the minority classes is a good approach, as we should seek to mimic reality in our data sets for hate speech detection. Results Running our system on the <cite>Waseem and Hovy (2016)</cite> data set, we find that our best performing system does not substantially outperform on the binary classification task <cite>Waseem and Hovy (2016</cite> Interestingly, the main cause of error is false positives. This holds true using both amateur and expert annotations. We mitigate personal bias in our annotations, as multiple people have participated in the annotation process. <cite>Waseem and Hovy (2016)</cite> may suffer from personal bias, as the only the authors annotated, and only the annotations positive for hate speech were reviewed by one other person.",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_14",
  "x": "We mitigate personal bias in our annotations, as multiple people have participated in the annotation process. <cite>Waseem and Hovy (2016)</cite> may suffer from personal bias, as the only the authors annotated, and only the annotations positive for hate speech were reviewed by one other person. It is our contention that hate speech corpora should reflect real life, in that hate speech is a rare occurrence comparatively. Given that some of our features obtain high F1-scores, in spite of not classifying for the minority classes, we suggest that the unweighted F1-score may not be an appropriate metric to evaluate classification on hate speech corpora. ---------------------------------- **RELATED WORK** Most related work in the field of abusive language detection has focused on detecting profanity using list-based methods to identify offensive words (Sood et al., 2012; Chen et al., 2012) .",
  "y": "differences"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_15",
  "x": "Nobata et al. (2016) address this, by using comprehensive lists of slurs obtained from Hatebase 4 . <cite>Waseem and Hovy (2016)</cite> and Ross et al. (2016) focus on building corpora which <cite>they annotate</cite> for containing hate speech. Our work closely resembles <cite>Waseem and Hovy (2016)</cite> , as <cite>they also run</cite> classification experiments on a hate speech data set. <cite>Waseem and Hovy (2016)</cite> obtain an F1-score of 73.91 on <cite>their data set</cite>, using character n-grams and gender information. Nobata et al. (2016) employ a wide array of features for abusive language detection, including but not limited to POS tags, the number of blacklisted words in a document, n-gram features including token and character n-grams and length features. The primary challenge this paper presents, is the need for good annotation guidelines, if one wishes to detect specific subsets of abusive language. ----------------------------------",
  "y": "background"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_16",
  "x": "These methods traditionally suffer from a poor recall and do not address hate speech. While Sood et al. (2012) incorporate edit distances to find variants of slurs, they are not able to find terms that do not occur in these lists. Nobata et al. (2016) address this, by using comprehensive lists of slurs obtained from Hatebase 4 . <cite>Waseem and Hovy (2016)</cite> and Ross et al. (2016) focus on building corpora which <cite>they annotate</cite> for containing hate speech. Our work closely resembles <cite>Waseem and Hovy (2016)</cite> , as <cite>they also run</cite> classification experiments on a hate speech data set. <cite>Waseem and Hovy (2016)</cite> obtain an F1-score of 73.91 on <cite>their data set</cite>, using character n-grams and gender information. Nobata et al. (2016) employ a wide array of features for abusive language detection, including but not limited to POS tags, the number of blacklisted words in a document, n-gram features including token and character n-grams and length features.",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_17",
  "x": "<cite>Waseem and Hovy (2016)</cite> and Ross et al. (2016) focus on building corpora which <cite>they annotate</cite> for containing hate speech. Our work closely resembles <cite>Waseem and Hovy (2016)</cite> , as <cite>they also run</cite> classification experiments on a hate speech data set. <cite>Waseem and Hovy (2016)</cite> obtain an F1-score of 73.91 on <cite>their data set</cite>, using character n-grams and gender information. Nobata et al. (2016) employ a wide array of features for abusive language detection, including but not limited to POS tags, the number of blacklisted words in a document, n-gram features including token and character n-grams and length features. The primary challenge this paper presents, is the need for good annotation guidelines, if one wishes to detect specific subsets of abusive language. ---------------------------------- **CONCLUSION**",
  "y": "background"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_18",
  "x": "The primary challenge this paper presents, is the need for good annotation guidelines, if one wishes to detect specific subsets of abusive language. ---------------------------------- **CONCLUSION** We find that using expert annotations can produce models that perform comparably to previous classification efforts. Our best model is on par with previous work on the <cite>Waseem and Hovy (2016)</cite> data set for the binary classification task but under-performs for the multi-class classification task. We suggest that a weighted F1-score be applied in evaluation of classification efforts on hate speech corpora, such that misclassification on minority classes is penalized. Our annotation and classification results expand on the claim of Ross et al. (2016) that hate speech is hard to annotate without intimate knowledge of hate speech.",
  "y": "differences similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_19",
  "x": "Our best model is on par with previous work on the <cite>Waseem and Hovy (2016)</cite> data set for the binary classification task but under-performs for the multi-class classification task. We suggest that a weighted F1-score be applied in evaluation of classification efforts on hate speech corpora, such that misclassification on minority classes is penalized. Our annotation and classification results expand on the claim of Ross et al. (2016) that hate speech is hard to annotate without intimate knowledge of hate speech. Furthermore, we find that considering only cases of full agreement among amateur annota-tors can produce relatively good annotations as compared to expert annotators. This can allow for a significant decrease in the annotations burden of expert annotators by asking them to primarily consider the cases in which amateur annotators have disagreed. Future Work We will seek to further investigate the socio-linguistic features such as gender and location. Furthermore, we will expand to more forms of hate speech.",
  "y": "motivation"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_0",
  "x": "2. Ambiguity -for written text, a change in context may require a different normalization. For example, \"2/3\" can be verbalized as a date or fraction depending on the meaning of the sentence. Traditionally, the task of NSW normalization has been approached by manually authoring grammars in the form of finite-state transducers (Sproat, 1996; Roark et al., 2012) such as integer grammars (e.g., \"26\" \u2192 \"twenty six\") or time grammars (e.g., \"5:26\" \u2192 \"five twenty six\"). Constructing such grammars is time consuming and error-prone and requires extensive linguistic knowledge and programming proficiency. Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field<cite> (Sproat and Jaitly, 2016)</cite> . In this paper, we present our approach to nonstandard text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively. ----------------------------------",
  "y": "background"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_1",
  "x": "Traditionally, the task of NSW normalization has been approached by manually authoring grammars in the form of finite-state transducers (Sproat, 1996; Roark et al., 2012) such as integer grammars (e.g., \"26\" \u2192 \"twenty six\") or time grammars (e.g., \"5:26\" \u2192 \"five twenty six\"). Constructing such grammars is time consuming and error-prone and requires extensive linguistic knowledge and programming proficiency. Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field<cite> (Sproat and Jaitly, 2016)</cite> . In this paper, we present our approach to nonstandard text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively. ---------------------------------- **RELATED WORK 2.1 FINITE STATE TRANSDUCER** Normalizing written-form text to its spoken form has been approached by authoring weighted finite state transducer (WFST) grammars to handle individual categories of NSW (e.g., time, date) and subsequently join them together (Sproat, 1996; Roark et al., 2012) .",
  "y": "motivation"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_2",
  "x": "---------------------------------- **DATA-DRIVEN APPROACHES** Recently, methods based on neural networks have been applied to TN and ITN<cite> (Sproat and Jaitly, 2016</cite>; Pusateri et al., 2017; Yolchuyeva et al., 2018) . To overcome one of the biggest problems -a lack of supervision, WFSTs have been used to transform large amounts of written-form text to its spoken form. Researchers hope a vast amount of such data can counteract the errors inherited in WFST-based models. Recent data-driven approaches examine window-based sequence-to-sequence (seq2seq) models and convolutional neural networks (CNN) to normalize a central piece of text with the help of context<cite> (Sproat and Jaitly, 2016</cite>; Yolchuyeva et al., 2018) . Window-based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special <self> token.",
  "y": "background"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_3",
  "x": "Recently, methods based on neural networks have been applied to TN and ITN<cite> (Sproat and Jaitly, 2016</cite>; Pusateri et al., 2017; Yolchuyeva et al., 2018) . To overcome one of the biggest problems -a lack of supervision, WFSTs have been used to transform large amounts of written-form text to its spoken form. Researchers hope a vast amount of such data can counteract the errors inherited in WFST-based models. Recent data-driven approaches examine window-based sequence-to-sequence (seq2seq) models and convolutional neural networks (CNN) to normalize a central piece of text with the help of context<cite> (Sproat and Jaitly, 2016</cite>; Yolchuyeva et al., 2018) . Window-based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special <self> token. Hybrid neural/WFST models have also been proposed and applied to the text normalization problem (Pusateri et al., 2017; Yolchuyeva et al., 2018) . Tokens in the input are first tagged with labels using machine learned models whereupon a handcrafted grammar corresponding to each label conducts conversion.",
  "y": "background"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_4",
  "x": "Our seq2seq model does not require the aforementioned tagger (although could benefit from the tagger as we will show later) and directly translates a written-form sentence to its spoken form without grammars. ---------------------------------- **MODEL** ---------------------------------- **BASELINE MODELS** Following <cite>Sproat and Jaitly (2016)</cite>, we implement a seq2seq model trained on window-based data. Table 1 illustrates the window-based model's training examples corresponding to one sentence \"wake me up at 8 AM .\" which is broken down into 6 pairs.",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_5",
  "x": "---------------------------------- **BASELINE MODELS** Following <cite>Sproat and Jaitly (2016)</cite>, we implement a seq2seq model trained on window-based data. Table 1 illustrates the window-based model's training examples corresponding to one sentence \"wake me up at 8 AM .\" which is broken down into 6 pairs. <n> and </n> indicate the center of the window. A window center might contain 1 or more words (e.g., \"8 AM\") and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as TIME, DATE, ORDINAL<cite> (Sproat and Jaitly, 2016)</cite> . The model outputs tokens which correspond to the center of the window.",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_6",
  "x": "The data for the window-based seq2seq model and full sentence seq2seq were generated from the publicly available release of parallel written/speech formatted text from <cite>Sproat and Jaitly (2016)</cite> . The set consists of Wikipedia text which was processed through Google TTS's Kestrel text normalization system relying primarily on handcrafted rules to produce speech-formatted text. Although a large parallel dataset is available for English, we consider the feasibility of developing neural models for other languages which may not have text normalization systems in place. Therefore, we choose to scale the training data size to a limited set of text which could be generated by annotators in a reasonable time frame. As summarized in Table 2 , both window-based and sentencebased models are trained with 500K training instances. Our datasets were randomly sampled from a set of 4.9M sentences in the training data portion of the <cite>Sproat and Jaitly (2016)</cite> data release and split into training, validation, and test data. However, the training data for window-based and sentencebased models are not identical due to differences in input configurations.",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_7",
  "x": "Our datasets were randomly sampled from a set of 4.9M sentences in the training data portion of the <cite>Sproat and Jaitly (2016)</cite> data release and split into training, validation, and test data. However, the training data for window-based and sentencebased models are not identical due to differences in input configurations. While the window-based model uses 500K randomly sampled windows, the sentence-based models use 500K sentences. For testing, 62.5K identical test sentences are used across all models. In order to decode sentences with the window-based model, sentences are first segmented into windows before inference. Among 16 edit labels available in the dataset release, we found the normalization target for Table 2 : Size of training, validation, and test datasets. For the window-baseline, the data are pairs of windows and the normalization of the central piece of the window.",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_8",
  "x": "---------------------------------- **MODEL** Train ELECTRONIC text is not suitable for our system as it primarily reads out URLs letter by letter, e.g., \"Forbes.com\" \u2192 \"f o r b e s dot c o m\" (as opposed to \"forbes dot com\"). Therefore, we exclude ELECTRONIC data in our experiments. There are large numbers of <self> tokens present in the dataset. We follow <cite>Sproat and Jaitly (2016)</cite> in down-sampling window-based training data to constrain the proportion of \"<self>\" tokens to 10% of the data. For training sentence-based models, the source sentence is segmented into characters while the target sentence is broken into tokens.",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_9",
  "x": "For the subword model, both the source and target sentences are segmented into subword sequences. Subword units are concatenated to words for evaluation. ---------------------------------- **BASELINE MODEL SETUP** Our first approach replicates the window-based seq2seq model of <cite>Sproat and Jaitly (2016)</cite> . The model encodes the central piece of text (1 or more tokens) including its context of N previous and following tokens at the character level. The output is a target token or a sequence of tokens.",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_10",
  "x": "Validation occurred every 10K timesteps and the number of timesteps was chosen based on maximum accuracy on the validation data. The learning rate was tuned to 1.0 for the window-based model and 0.5 for sentence-based models to achieve optimal performance. Learning rate decayed at a rate of 0.5 if perplexity on the validation set did not decrease or after 50K steps. A dropout of 0.3 was used across all models. Figure 2: Evaluation of the window-based model. Categories are sorted by frequency. * TELEPHONE is not reported in <cite>Sproat and Jaitly (2016)</cite> but included in the dataset; ** we removed ELECTRONIC category.",
  "y": "differences"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_11",
  "x": "* TELEPHONE is not reported in <cite>Sproat and Jaitly (2016)</cite> but included in the dataset; ** we removed ELECTRONIC category. As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with <cite>Sproat and Jaitly (2016)</cite> , considering our training set is much smaller. There are 16 different edit labels shown. Data with TELEPHONE labels were not included in the initial analysis of <cite>Sproat and Jaitly (2016)</cite> , but were made available in the dataset release. For our second baseline model which operates on whole sentences, on the input side, we still use 250 common characters. However, due to the removal of the <self> token, the output space is drastically extended from 1K tokens to 45K tokens. Thus, it becomes increasingly difficult for the model to learn and predict.",
  "y": "similarities"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_12",
  "x": "Figure 2: Evaluation of the window-based model. Categories are sorted by frequency. * TELEPHONE is not reported in <cite>Sproat and Jaitly (2016)</cite> but included in the dataset; ** we removed ELECTRONIC category. As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with <cite>Sproat and Jaitly (2016)</cite> , considering our training set is much smaller. There are 16 different edit labels shown. Data with TELEPHONE labels were not included in the initial analysis of <cite>Sproat and Jaitly (2016)</cite> , but were made available in the dataset release. For our second baseline model which operates on whole sentences, on the input side, we still use 250 common characters.",
  "y": "differences"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_13",
  "x": "Edit labels are the most expensive to obtain in real life. Our labels are generated directly from the Google FST<cite> (Sproat and Jaitly, 2016)</cite> . Each type of feature is represented by a one-hot encoding. To combine linguistic features with subword units, one can add or concatenate each subword's embedding with its corresponding linguistic feature embedding and feed a combined embedding to the bi-LSTM encoder. Or, a multi-layer perceptron (MLP) can be applied to combine information in a non-linear way. Our experiments find that concatenation outperforms the other two methods. In Table 4 we can see that the subword model with linguistic features produces the lowest SER (0.78%) and WER (0.17%).",
  "y": "uses"
 },
 {
  "id": "1e232f9dfa7d499d1ba39fcebf3d1a_0",
  "x": "**ABSTRACT** GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers. 1 ---------------------------------- **INTRODUCTION** Systems, such as treebank-based parsers<cite> (Charniak, 2001</cite>; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008) , are trained and tested on hand-annotated data.",
  "y": "background"
 },
 {
  "id": "1e232f9dfa7d499d1ba39fcebf3d1a_1",
  "x": "For example, given a sentence like I bought three acres of land in California, the PP in California can be attached to either acres or land with no difference in meaning. While annotation guidelines may direct a human annotator to prefer, for example, high attachment, systems output may have other preferences, e.g., the probability that land is modified by a PP (headed by in) versus the probability that acres can be so modified. Even if the manual annotation for a particular corpus is consistent when it comes to other factors such as tokenization or part of speech, developers of parsers sometimes change these guidelines to suit their needs. For example, users of the Charniak parser<cite> (Charniak, 2001)</cite> should add the AUX category to the PTB parts of speech and adjust their systems to account for the conversion of the word ain't into the tokens IS and n't. Similarly, tokenization decisions with respect to hyphens vary among different versions of the Penn Treebank, as well as different parsers based on these treebanks. Thus if a system uses multiple parsers, such differences must be accounted for. Differences that are not important for a particular application should be ignored (e.g., by merging alternative analyses).",
  "y": "background"
 },
 {
  "id": "1e232f9dfa7d499d1ba39fcebf3d1a_2",
  "x": "Japanese: 20 sentences from the Kyoto Corpus (KYO) (Kurohashi and Nagao, 1998) ---------------------------------- **RUNNING THE GLARFER PROGRAMS** We use Charniak, UMD and KNP parsers<cite> (Charniak, 2001</cite>; Huang and Harper, 2009; Kurohashi and Nagao, 1998) , JET Named Entity tagger (Grishman et al., 2005; Ji and Grishman, 2006) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper. English GLARFer rules use Comlex (Macleod et al., 1998a) and the various NomBank lexicons (http:// nlp.cs.nyu.edu/meyers/nombank/) for lexical lookup. The GLARF rules implemented vary by language as follows. English: correcting/standardizing phrase boundaries and part of speech (POS); recognizing multiword expressions; marking subconstituents; labeling relations; incorporating NEs; regularizing infinitival, passives, relatives, VP deletion, predicative and numerous other constructions.",
  "y": "uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_0",
  "x": "****RECOGNIZING HUMOUR USING WORD ASSOCIATIONS AND HUMOUR ANCHOR EXTRACTION**** **ABSTRACT** This paper attempts to marry the interpretability of statistical machine learning approaches with the more robust models of joke structure and joke semantics capable of being learned by neural models. Specifically, we explore the use of semantic relatedness features based on word associations, rather than the more common Word2Vec similarity, on a binary humour identification task and identify several factors that make word associations a better fit for humour. We also explore the effects of using joke structure, in the form of humour anchors<cite> (Yang et al., 2015)</cite> , for improving the performance of semantic features and show that, while an intriguing idea, humour anchors contain several pitfalls that can hurt performance. ---------------------------------- **INTRODUCTION**",
  "y": "uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_1",
  "x": "This makes computational humour a challenging yet very intriguing problem for natural language processing (NLP). As such, it is no surprise that computational humour, and humour recognition in particular, has received increased attention from the NLP community with SemEval-2017 devoting two tasks to it: ranking humorous tweets (Potash et al., 2017) and interpreting English puns (Miller et al., 2017) . This recent attention has lead to advancements such as sequence-based neural humour models capable of implicitly learning a joke structure and semantic features (Bertero and Fung, 2016a; Donahue et al., 2017) . While these approaches offer good performance, their reliance on complicated neural architectures over explicitly engineered features present a problem for interpretability which may make it difficult to diagnose problems if results go wrong. Works which take a more interpretable statistical machine learning approach have their own drawbacks. For example, the representation of joke semantics has been fairly basic, typically computing word embedding similarities between all word pairs in a document<cite> (Yang et al., 2015)</cite> , and bear little resemblance to the way humans actually interpret humour. Additionally, many works fail to take advantage of joke structure, treating texts as unordered bags-of-words (Bertero and Fung, 2016b; Mihalcea and Strapparava, 2005; Yan and Pedersen, 2017) .",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_2",
  "x": "Works which take a more interpretable statistical machine learning approach have their own drawbacks. For example, the representation of joke semantics has been fairly basic, typically computing word embedding similarities between all word pairs in a document<cite> (Yang et al., 2015)</cite> , and bear little resemblance to the way humans actually interpret humour. Additionally, many works fail to take advantage of joke structure, treating texts as unordered bags-of-words (Bertero and Fung, 2016b; Mihalcea and Strapparava, 2005; Yan and Pedersen, 2017) . This paper aims to marry the interpretability of statistical machine learning approaches with the more nuanced models of joke structure and joke semantics of neural approaches. Specifically, we explore the effectiveness modelling joke semantics using semantic relatedness features based on word associations, rather than the more common semantic similarity features based on word embeddings. We present evidence not only that relatedness in general is better suited than similarity for computational humour tasks This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ due to its broader nature, but also that word associations are particularly well suited due to their ability to map more nuanced relationships (De Deyne and Storms, 2008) and asymmetric nature.",
  "y": "uses background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_3",
  "x": "This paper aims to marry the interpretability of statistical machine learning approaches with the more nuanced models of joke structure and joke semantics of neural approaches. Specifically, we explore the effectiveness modelling joke semantics using semantic relatedness features based on word associations, rather than the more common semantic similarity features based on word embeddings. We present evidence not only that relatedness in general is better suited than similarity for computational humour tasks This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ due to its broader nature, but also that word associations are particularly well suited due to their ability to map more nuanced relationships (De Deyne and Storms, 2008) and asymmetric nature. While such features have been explored in the past (Cattle and Ma, 2016; Cattle and Ma, 2017b) , this work presents a more in depth analysis, focusing on a more fundamental task (binary humour classification vs. relative humour ranking) on with a dataset that better represents natural language (oneliners and puns vs. Twitter hashtag games), and is the first work to incorporate interpolated word association strengths. Furthermore, we introduce a novel method for targetting our semantic features using joke structure to help reduce noise and increase reliability. Specifically, we experiment with integrating the extraction of humour anchors, the \"meaningful, complete, minimal set of word spans\"<cite> (Yang et al., 2015)</cite> that allow humour to occur, into the humour classification process itself, the first work to do so.",
  "y": "uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_4",
  "x": "As overlap and incongruity are difficult to measure directly, one common approach is instead to use word embeddings, such as Word2Vec (Mikolov et al., 2013) , to calculate the cosine similarities between pairs of vectors representing words in a document <cite>(Yang et al., 2015</cite>; Shahaf et al., 2015; Kukova\u010dec et al., 2017) . However, measuring incongruity and overlap in terms of similarity is a rather odd choice. Just because two scripts overlap does not imply they are similar. In the \"doctor\" example in Section 2, the two scripts, namely [the patient is seeking medical advice] and [the patient is having an affair with the doctor's wife], overlap in terms of the people and locations involved but otherwise are quite different. Similarly, incongruent scripts such as [dog bites man] and [man bites dog] are very similar in all but the assignment of the roles. Compared with similarity, relatedness is a much broader concept. It is easy to think of concepts that are related but not similar. For example, \"beer\" and \"glass\" are not similar, describing very different concepts, but are quite closely related in that beer often comes in glasses (Ma, 2013) .",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_5",
  "x": "As mentioned in Section 1, joke structure and joke semantics are not entirely independent. This means poor models of joke structure can affect the performance of features designed to capture joke semantics. Despite the issues mentioned in Section 2.1,<cite> Yang et al. (2015)</cite> 's \"incongruity\" feature set, maximum and minimum word embedding similarities between pairs of words in a document, perform fairly well. Cattle and Ma (2017b) takes a similar approach with their word association features. The problems comes from the fact that both works compute these values across all possible pairs of words in a document. This can introduce noise as not all word pairs are meaningful (e.g. pairs of stopwords) and internally-cohesive setups and punchlines can bias maximum similarity scores. While this can be somewhat alleviated by judicial filtering of stopwords, this does not guarantee meaningful word pairs either.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_6",
  "x": "Cattle and Ma (2017b) takes a similar approach with their word association features. The problems comes from the fact that both works compute these values across all possible pairs of words in a document. This can introduce noise as not all word pairs are meaningful (e.g. pairs of stopwords) and internally-cohesive setups and punchlines can bias maximum similarity scores. While this can be somewhat alleviated by judicial filtering of stopwords, this does not guarantee meaningful word pairs either. Yang et al. (2015) , in addition to their humour classifier, also introduces a method for identifying jokes' humour anchors (HAs), the \"meaningful, complete, minimal set of word spans\" that allow humour to occur. While this is slightly different from identifying a joke's setup and punchline, focusing only on pairs of HAs would help reducing noise by increasing the precision of meaningful word pairs selection without sacrificing recall. However,<cite> Yang et al. (2015)</cite> does not use their extracted HAs to improve their humour classification performance.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_7",
  "x": "Furthermore, the quality of the extracted HAs is directly tied to the quality of the humour model. However, both issues could have been alleviated using some form of co-training or bootstrapping between the overall humour prediction model and the HA extractor's internal humour prediction model. The HA extractor works by generating a list of HA candidates for each document following a heuristic method. After removing various combinations of HA candidates from the original document, these modified documents are fed into a trained humour prediction model, with the HAs being the combination of HA candidates which causes the largest drop in humour score. Since the humour prediction model is by design robust against word order, words can be freely omitted with few side effects. It should be noted that sequence-based humour models such as Bertero and Fung (2016b) or Donahue et al. (2017) should theoretically be capable of implicitly learning HAs, especially Bertero and Fung (2016a)'s Long Short-Term Memory-based approach. However, these models are much more complex than<cite> Yang et al. (2015)</cite> 's approach, require more training data, and suffer from a lack of interpretability.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_8",
  "x": "It should be noted that sequence-based humour models such as Bertero and Fung (2016b) or Donahue et al. (2017) should theoretically be capable of implicitly learning HAs, especially Bertero and Fung (2016a)'s Long Short-Term Memory-based approach. However, these models are much more complex than<cite> Yang et al. (2015)</cite> 's approach, require more training data, and suffer from a lack of interpretability. ---------------------------------- **METHODOLOGY** ---------------------------------- **DATASETS** We evaluate our classifiers across two separate datasets: Pun of the Day (PotD), collected in<cite> Yang et al. (2015)</cite> , and 16000 One-Liner (OL), collected in Mihalcea and Strapparava (2005) .",
  "y": "uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_9",
  "x": "---------------------------------- **DATASETS** We evaluate our classifiers across two separate datasets: Pun of the Day (PotD), collected in<cite> Yang et al. (2015)</cite> , and 16000 One-Liner (OL), collected in Mihalcea and Strapparava (2005) . PotD consists of positive examples collected from the Pun of the Day website 2 and negative examples collected from a combination of news sources, question/answer forums, and lists of proverbs<cite> (Yang et al., 2015)</cite> . OL consists of positive examples scraped from humour websites and negative examples taken from a combination of new headlines, sentences from the British National Corpus, and proverbs. ---------------------------------- **BASELINE**",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_10",
  "x": "---------------------------------- **DATASETS** We evaluate our classifiers across two separate datasets: Pun of the Day (PotD), collected in<cite> Yang et al. (2015)</cite> , and 16000 One-Liner (OL), collected in Mihalcea and Strapparava (2005) . PotD consists of positive examples collected from the Pun of the Day website 2 and negative examples collected from a combination of news sources, question/answer forums, and lists of proverbs<cite> (Yang et al., 2015)</cite> . OL consists of positive examples scraped from humour websites and negative examples taken from a combination of new headlines, sentences from the British National Corpus, and proverbs. ---------------------------------- **BASELINE**",
  "y": "uses background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_11",
  "x": "**DATASETS** We evaluate our classifiers across two separate datasets: Pun of the Day (PotD), collected in<cite> Yang et al. (2015)</cite> , and 16000 One-Liner (OL), collected in Mihalcea and Strapparava (2005) . PotD consists of positive examples collected from the Pun of the Day website 2 and negative examples collected from a combination of news sources, question/answer forums, and lists of proverbs<cite> (Yang et al., 2015)</cite> . OL consists of positive examples scraped from humour websites and negative examples taken from a combination of new headlines, sentences from the British National Corpus, and proverbs. ---------------------------------- **BASELINE** For our baseline we implemented our own version of<cite> Yang et al. (2015)</cite> 's highest performing classifier.",
  "y": "extends"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_12",
  "x": "We then train a Random Forest Classifier using the scikit-learn 4 Python library with 100 estimators but otherwise default settings. All Word2Vec features, including those described below, use Google's pre-trained 300 dimension Word2Vec embeddings 5 . ---------------------------------- **SEMANTIC FEATURES** Similar to<cite> Yang et al. (2015)</cite> , we compute the minimum, maximum, and average Word2Vec similarity between ordered word pairs. Not only is this a common humour recognition feature <cite>(Yang et al., 2015</cite>; Shahaf et al., 2015; Kukova\u010dec et al., 2017) , but it also acts as a point of comparison for word association strength. For our word association features we compute the minimum, maximum, and average association strength between ordered word pairs, which we refer to this as the forward strength.",
  "y": "similarities"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_13",
  "x": "**SEMANTIC FEATURES** Similar to<cite> Yang et al. (2015)</cite> , we compute the minimum, maximum, and average Word2Vec similarity between ordered word pairs. Not only is this a common humour recognition feature <cite>(Yang et al., 2015</cite>; Shahaf et al., 2015; Kukova\u010dec et al., 2017) , but it also acts as a point of comparison for word association strength. For our word association features we compute the minimum, maximum, and average association strength between ordered word pairs, which we refer to this as the forward strength. Since, as described in Section 2.1, word associations are directional, we also compute the minimum, maximum, and average associations strengths between the reverse ordered word pairs, which we refer to as the backward strength. Following Cattle and Ma (2016) , we also compute the difference between these two values on both a micro (per word) and macro (per document) level. We refer to these sets of features as the diff strengths.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_14",
  "x": "Similar to<cite> Yang et al. (2015)</cite> , we compute the minimum, maximum, and average Word2Vec similarity between ordered word pairs. Not only is this a common humour recognition feature <cite>(Yang et al., 2015</cite>; Shahaf et al., 2015; Kukova\u010dec et al., 2017) , but it also acts as a point of comparison for word association strength. For our word association features we compute the minimum, maximum, and average association strength between ordered word pairs, which we refer to this as the forward strength. Since, as described in Section 2.1, word associations are directional, we also compute the minimum, maximum, and average associations strengths between the reverse ordered word pairs, which we refer to as the backward strength. Following Cattle and Ma (2016) , we also compute the difference between these two values on both a micro (per word) and macro (per document) level. We refer to these sets of features as the diff strengths. Forward, backward, and diff strengths are extracted for both the EAT and USF word association datasets.",
  "y": "similarities background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_15",
  "x": "By default, word association and Word2Vec features are computed across all word pairs in a document. However, we also experiment computing these features only across pairs of humour anchor (HA) words. HAs are extracted using the method described in<cite> Yang et al. (2015)</cite> using the same baseline humour model described in Section 3.2 for anchor candidate evaluation. HA extraction's requirement of a fully trained anchor candidate evaluator raises the problem of what data that evaluator should be trained on. Given that, as described in Section 3.1, we experiment on two separate humour datasets, we train the anchor candidate evaluator on the opposite dataset from the overall humour classifier. This is to avoid overfitting or biasing the anchor candidate evaluator by training on the test data. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_16",
  "x": "Given that, as described in Section 3.1, we experiment on two separate humour datasets, we train the anchor candidate evaluator on the opposite dataset from the overall humour classifier. This is to avoid overfitting or biasing the anchor candidate evaluator by training on the test data. ---------------------------------- **RESULTS AND DISCUSSION** The results of our experiments are reported in Table 1 . In general, our model performs slightly worse than the<cite> Yang et al. (2015)</cite> baseline. One interesting aspect to note is that our model uses only 28 feature dimensions compared to<cite> Yang et al. (2015)</cite> 's 318.",
  "y": "differences"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_17",
  "x": "The results of our experiments are reported in Table 1 . In general, our model performs slightly worse than the<cite> Yang et al. (2015)</cite> baseline. One interesting aspect to note is that our model uses only 28 feature dimensions compared to<cite> Yang et al. (2015)</cite> 's 318. While this is not exactly a fair comparison in the case of our ML-based word association strengths (our ML strength predictor takes 415 feature dimensions as input), graph-based associations perform similarly and do truly use only 28 dimensions. Overall performance is similar across both datasets with the only notable exception being graph-based USF performing better on OL than PotD. This is likely due to OL being better suited than PotD to USF's relatively smaller set of associations (72,176 pairs and 10,617 unique words versus EAT's 325,588 and 23,218). ---------------------------------- **WORD ASSOCIATION STRENGTHS**",
  "y": "differences"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_18",
  "x": "This meant it was difficult to know if a specific word pair represented setup to punchline, punchline to setup, or even setup to setup or punchline to punchline. Even using humour anchors (HAs) does not solve this problem since, as referenced in Section 2.2, identifying HAs is slightly different from identifying setups and punchlines. While labelling a word span as a setup or a punchline gives us some insight into its purpose in the joke (i.e. whether it is meant to establish context or to trigger a reframing, respectively), HAs do not include this information. ---------------------------------- **HUMOUR ANCHORS** As mentioned in Section 2.2, using HAs for humour recognition is an appealing notion and would allow semantic features to be targeted only to meaningful word pairs, potentially increasing their effectiveness. The wonderfully simple extraction method described in<cite> Yang et al. (2015)</cite> only makes HAs more intriguing.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_19",
  "x": "As mentioned in Section 2.2, using HAs for humour recognition is an appealing notion and would allow semantic features to be targeted only to meaningful word pairs, potentially increasing their effectiveness. The wonderfully simple extraction method described in<cite> Yang et al. (2015)</cite> only makes HAs more intriguing. Unfortunately, as can be seen in Table 1 , HA targetting actually hurts the performance of our humour model. One obvious suspect for this drop in performance is the quality of the extracted HAs, a sample of which is shown in extracted anchors were either incomplete, as is the case with Dark and Santa, or nonsensical, like profectionist. As described in Section 2.2,<cite> Yang et al. (2015)</cite> 's HA extraction algorithm requires a fully trained humour model, the accuracy of which undoubtedly affects the quality of the extracted HAs. For this reason we also experimented with training our anchor candidate scorer using the test data, to maximize its performance. While this approach is problematic, it does provide an upper bound for our HA extraction performance.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_20",
  "x": "As mentioned in Section 2.2, using HAs for humour recognition is an appealing notion and would allow semantic features to be targeted only to meaningful word pairs, potentially increasing their effectiveness. The wonderfully simple extraction method described in<cite> Yang et al. (2015)</cite> only makes HAs more intriguing. Unfortunately, as can be seen in Table 1 , HA targetting actually hurts the performance of our humour model. One obvious suspect for this drop in performance is the quality of the extracted HAs, a sample of which is shown in extracted anchors were either incomplete, as is the case with Dark and Santa, or nonsensical, like profectionist. As described in Section 2.2,<cite> Yang et al. (2015)</cite> 's HA extraction algorithm requires a fully trained humour model, the accuracy of which undoubtedly affects the quality of the extracted HAs. For this reason we also experimented with training our anchor candidate scorer using the test data, to maximize its performance. While this approach is problematic, it does provide an upper bound for our HA extraction performance.",
  "y": "similarities background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_21",
  "x": "One obvious suspect for this drop in performance is the quality of the extracted HAs, a sample of which is shown in extracted anchors were either incomplete, as is the case with Dark and Santa, or nonsensical, like profectionist. As described in Section 2.2,<cite> Yang et al. (2015)</cite> 's HA extraction algorithm requires a fully trained humour model, the accuracy of which undoubtedly affects the quality of the extracted HAs. For this reason we also experimented with training our anchor candidate scorer using the test data, to maximize its performance. While this approach is problematic, it does provide an upper bound for our HA extraction performance. While using such HA targetted models did result in increased humour classification performance (ACC = 0.740, P = 0.735, R = 0.754, F 1 = 0.744 on PotD. ACC = 0.706, P = 0.678, R = 0.783, F 1 = 0.727 on OL.), it still failed to exceed our non-HA models. We chose our baseline<cite> Yang et al. (2015)</cite> humour classifier as our anchor candidate scorer for simplicity but their HA extraction algorithm is able to work with any humour recognition model so long as it is robust to word order and capable of generating a humour score (in our case, we used humour probability). Therefore, using a more accurate humour model may have led to better performance.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_0",
  "x": "****A CONTEXT-AWARE CONVOLUTIONAL NATURAL LANGUAGE GENERATION MODEL FOR DIALOGUE SYSTEMS**** **ABSTRACT** Natural language generation (NLG) is an important component in spoken dialog systems (SDSs). A model for NLG involves sequence to sequence learning. State-of-the-art NLG models are built using recurrent neural network (RNN) based sequence to sequence models<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> . Convolutional sequence to sequence based models have been used in the domain of machine translation but their application as natural language generators in dialogue systems is still unexplored. In this work, we propose a novel approach to NLG using convolutional neural network (CNN) based sequence to sequence learning.",
  "y": "background"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_1",
  "x": "Most of the approaches for incorporating entrainment are rule-based models. Recent advances have been in the direction of developing a fully trainable context aware NLG model<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> . However, all these approaches are based on recurrent sequence to sequence architecture. Convolutional neural networks are largely unexplored in the domain of NLG for SDS inspite of having several advantages (Waibel et al., 1989; LeCun and Bengio, 1995) . Recurrent networks depend on the computations of previous time step and thus inhibits parallelization within a sequence. Convolutional networks on the other hand, allows parallelization within a sequence resulting in efficient use of GPUs and other computational resources (Gehring et al., 2017) . Multi-block (multilayer) convolutional networks enable controlling the upper bound on the effective context size and form a hierarchical structure.",
  "y": "background"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_2",
  "x": "In the decoding phase, beam search is implemented and nbest natural language responses are chosen. The n-best beam search responses from ConvSeq2Seq generator may have some missing and/or irrelevant information. To address this, we propose to rank the n-best outputs from ConvSeq2Seq generator using convolutional reranker (CNN reranker). CNN reranker implements one dimensional convolution on beam search responses and generates binary vectors. These binary vectors are used to penalize the responses having missing and/or irrelevant information. We evaluate our model on the Alex Context natural language generation (NLG) dataset of <cite>Du\u0161ek and Jurcicek (2016a)</cite> and demonstrate that our model outperforms the RNNbased model of <cite>Du\u0161ek and Jurcicek (2016a)</cite> (TGen model) in automatic metrics. Training time of proposed model is observed to be significantly lower than TGen model.",
  "y": "differences uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_3",
  "x": "This leads to generation of more informative response. Model proposed by <cite>Du\u0161ek and Jurcicek (2016a)</cite> serves as a baseline sequence to sequence generation model (TGen model) for SDS which takes into account the context. The model takes into account the preceding user utterance while generating natural language output. The model implemented three modifications to the model proposed by Du\u0161ek and Jurcicek (2016b) . The first modification was prepending context to the input DAs. The second modification was implementing a separate encoder for user utterances/contexts. The third modification was implementing a N-gram match reranker.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_4",
  "x": "Model proposed by <cite>Du\u0161ek and Jurcicek (2016a)</cite> serves as a baseline sequence to sequence generation model (TGen model) for SDS which takes into account the context. The model takes into account the preceding user utterance while generating natural language output. The model implemented three modifications to the model proposed by Du\u0161ek and Jurcicek (2016b) . The first modification was prepending context to the input DAs. The second modification was implementing a separate encoder for user utterances/contexts. The third modification was implementing a N-gram match reranker. This reranker is based on n-gram precision scores and promotes responses having phrase overlaps with user utterances<cite> (Du\u0161ek and Jurcicek, 2016a</cite> ).",
  "y": "background"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_5",
  "x": "The first modification was prepending context to the input DAs. The second modification was implementing a separate encoder for user utterances/contexts. The third modification was implementing a N-gram match reranker. This reranker is based on n-gram precision scores and promotes responses having phrase overlaps with user utterances<cite> (Du\u0161ek and Jurcicek, 2016a</cite> ). In the next section, we present the proposed CNN-based sequence to sequence generator for NLG. ---------------------------------- **PROPOSED APPROACH**",
  "y": "uses background"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_6",
  "x": "Here, \u03c9 and W are constants. We implement the N-gram match reranker as given by <cite>Du\u0161ek and Jurcicek (2016a)</cite> . We describe the proposed convolutional sequence to sequence generator in Section 3.1 and convolutional reranker in Section 3.2. ---------------------------------- **CONVSEQ2SEQ GENERATOR** The proposed sequence to sequence generator is based on convolutional sequence to sequence approach proposed by Gehring et al. (2017) 1 . It is a CNN-based encoder decoder architecture.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_7",
  "x": "The n-best beam search responses from ConvSeq2Seq model may have missing information and/or irrelevant information. CNN reranker reranks the n-best beam search responses and heavily penalizes those responses which are not semantically in correspondence with the input DA. Responses having missing information and/or irrelevant information are heavily penalized. Convolutional networks are excellent feature extractors and have achieved state-of-the-art results in many text classification and sentence-level classification tasks such as sentiment analysis, question classification, etc (Kim, 2014; Kalchbrenner et al., 2014) . This classifier takes as input a natural language response and outputs a binary vector. Each element of binary vector is a binary decision on the presence of DA type or slot-value combinations. For the dataset which we have used<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> , there are 19 such classes of DA types and slot-value combinations.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_8",
  "x": "This classifier takes as input a natural language response and outputs a binary vector. Each element of binary vector is a binary decision on the presence of DA type or slot-value combinations. For the dataset which we have used<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> , there are 19 such classes of DA types and slot-value combinations. These 19 classes are shown in Figure 3 . Input DAs are converted to similar binary vector. Hamming distance between the classifier output and binary vector representation of input DA is considered as reranking penalty. The weighted reranking penalties of all the n-best responses are subtracted from their log-probabilities similar to <cite>Du\u0161ek and Jurcicek (2016a)</cite> .",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_9",
  "x": "Thus, proposed CNN reranker requires lesser number of computations. ---------------------------------- **EXPERIMENTAL STUDIES** The studies in this work are performed on Alex Context natural language generation (NLG) dataset<cite> (Du\u0161ek and Jurcicek, 2016a</cite> ). This dataset is intended for fully trainable NLG systems in task-oriented spoken dialogue systems (SDS). It is in the domain of public transport information and has four dialogue act (DA) types namely request, inform, iconfirm and inform no match. It contains 1859 data instances each having 3 target responses.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_10",
  "x": "Data is delexicalized and split into training, validation and test sets as done by <cite>Du\u0161ek and Jurcicek (2016a)</cite> . For training and validation, the three paraphrases are used as separate instances. For evaluation they are used as three target references. Input to our ConvSeq2Seq generator is a DA prepended with user utterance. This allows entrainment of the model to the user utterances. A single dictionary is used for context utterances and DA tokens. Our model is trained by minimizing cross-entropy error using Nesterov Accelerated Gradient (NAG) optimizer (Nesterov, 1983) .",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_11",
  "x": "Misclassification penalty for CNN reranker is set to 100. Based on our experiments, we choose embedding dimension 128, filter sizes (3, 5, 7, 9) , number of filters 64, dropout keep probability 0.5, batch size 100, number of epochs 100 and L2 regularization, \u03bb=0.05. The performance of the proposed ConvSeq2Seq model for NLG is compared with that of TGen model<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> . For comparison, we have considered NIST (Doddington, 2002) , BLEU (Papineni et al., 2002) , METEOR (Denkowski and Lavie, 2014) , ROUGE L (Lin, 2004) and CIDEr metrics (Vedantam et al., 2015) . For this study, we have considered script \"mtevalv13a-sig.pl\" (version 13a) that implements these metrics. This script was used for E2E NLG challenge (Novikova et al., 2017) . We focus on the evaluations using this version.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_12",
  "x": "The performance of the proposed ConvSeq2Seq model for NLG is compared with that of TGen model<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> . For comparison, we have considered NIST (Doddington, 2002) , BLEU (Papineni et al., 2002) , METEOR (Denkowski and Lavie, 2014) , ROUGE L (Lin, 2004) and CIDEr metrics (Vedantam et al., 2015) . For this study, we have considered script \"mtevalv13a-sig.pl\" (version 13a) that implements these metrics. This script was used for E2E NLG challenge (Novikova et al., 2017) . We focus on the evaluations using this version. Our model has also been evaluated using the metric script \"mtevalv11b.pl\" (version 11b) to compare our results with those stated in<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> . The 13a version takes into account the closest reference length with respect to candidate length for calculation of brevity penalty.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_13",
  "x": "We see an improvement of 6.7 BLEU points when using N-gram match reranker with \u03c9 set to 5. A decrease in scores of other metrics is seen. These inconsistencies are due to the way brevity penalty is calculated for computing BLEU scores in 11b version of metric implementation. BLEU and NIST scores of the TGen model given in Table 2 match with that represented in<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> . The scores of our model shows slight improvement over TGen model. The studies done to compare the proposed model with the TGen model, show the effectiveness of considering the CNN-based approach to NLG. Studies also show that CNN reranker outperforms the RNN reranker.",
  "y": "similarities"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_0",
  "x": "This approach achieves competitive performance to the current state of the art model with less computation. In our experiments, we empirically study different kernel construction strategies on two widely used tasks: neural machine translation and sequence prediction. ---------------------------------- **INTRODUCTION** Transformer (Vaswani et al., 2017 ) is a relative new architecture which outperforms traditional deep learning models such as Recurrent Neural Networks (RNNs) (Sutskever et al., 2014) and Temporal Convolutional Networks (TCNs) (Bai et al., 2018) for sequence modeling tasks across neural machine translations (Vaswani et al., 2017) , language understanding (Devlin et al., 2018) , sequence prediction<cite> (Dai et al., 2019)</cite> , image generation (Child et al., 2019) , video activity classification (Wang et al., 2018) , music generation (Huang et al., 2018a) , and multimodal sentiment analysis (Tsai et al., 2019a) . Instead of performing recurrence (e.g., RNN) or convolution (e.g., TCN) over the sequences, Transformer is a feed-forward model that concurrently processes the entire sequence. At the core of the Transformer is its attention mechanism, which is proposed to integrate the dependencies between the inputs.",
  "y": "background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_1",
  "x": "At the core of the Transformer is its attention mechanism, which is proposed to integrate the dependencies between the inputs. There are up to three types of attention within the full Transformer model as exemplified with neural machine translation application (Vaswani et al., 2017) : 1) Encoder self-attention considers the source sentence as input, generating a sequence of encoded representations, where each encoded token has a global dependency with other tokens in the input sequence. 2) Decoder self-attention considers the target sentence (e.g., predicted target sequence for translation) as input, generating a sequence of decoded representations 1 , where each decoded token depends on previous decoded tokens. 3) Decoder-encoder attention considers both encoded and decoded sequences, generating a sequence with the same length as the decoded sequence. It should be noted that some applications has only the decoder self-attention such as sequence prediction<cite> (Dai et al., 2019)</cite> . In all cases, the Transformer's attentions follow the same general mechanism. At the high level, the attention can be seen as a weighted combination of the input sequence, where the weights are determined by the similarities between elements of the input sequence.",
  "y": "background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_2",
  "x": "In all cases, the Transformer's attentions follow the same general mechanism. At the high level, the attention can be seen as a weighted combination of the input sequence, where the weights are determined by the similarities between elements of the input sequence. We note that this operation is orderagnostic to the permutation in the input se-quence (order is encoded with extra positional embedding (Vaswani et al., 2017; Shaw et al., 2018;<cite> Dai et al., 2019)</cite> ). The above observation inspires us to connect Transformer's attention to kernel learning (Scholkopf and Smola, 2001) : they both concurrently and order-agnostically process all inputs by calculating the similarity between the inputs. Therefore, in the paper, we present a new formulation for Transformer's attention via the lens of kernel. To be more precise, the new formulation can be interpreted as a kernel smoother (Wasserman, 2006) over the inputs in a sequence, where the kernel measures how similar two different inputs are. The main advantage of connecting attention to kernel is that it opens up a new family of attention mechanisms that can relate to the well-established literature in kernel learning (Scholkopf and Smola, 2001) .",
  "y": "background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_3",
  "x": "We note that this operation is orderagnostic to the permutation in the input se-quence (order is encoded with extra positional embedding (Vaswani et al., 2017; Shaw et al., 2018;<cite> Dai et al., 2019)</cite> ). The above observation inspires us to connect Transformer's attention to kernel learning (Scholkopf and Smola, 2001) : they both concurrently and order-agnostically process all inputs by calculating the similarity between the inputs. Therefore, in the paper, we present a new formulation for Transformer's attention via the lens of kernel. To be more precise, the new formulation can be interpreted as a kernel smoother (Wasserman, 2006) over the inputs in a sequence, where the kernel measures how similar two different inputs are. The main advantage of connecting attention to kernel is that it opens up a new family of attention mechanisms that can relate to the well-established literature in kernel learning (Scholkopf and Smola, 2001) . As a result, we develop a new variant of attention which simply considers a product of symmetric kernels when modeling non-positional and positional embedding. Furthermore, our proposed formulation highlights naturally the main components of Transformer's attention, enabling a better understanding of this mechanism: recent variants of Transformers (Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019; Wang et al., 2018; Tsai et al., 2019a) can be expressed through these individual components.",
  "y": "motivation background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_4",
  "x": "We note that this operation is orderagnostic to the permutation in the input se-quence (order is encoded with extra positional embedding (Vaswani et al., 2017; Shaw et al., 2018;<cite> Dai et al., 2019)</cite> ). The above observation inspires us to connect Transformer's attention to kernel learning (Scholkopf and Smola, 2001) : they both concurrently and order-agnostically process all inputs by calculating the similarity between the inputs. Therefore, in the paper, we present a new formulation for Transformer's attention via the lens of kernel. To be more precise, the new formulation can be interpreted as a kernel smoother (Wasserman, 2006) over the inputs in a sequence, where the kernel measures how similar two different inputs are. The main advantage of connecting attention to kernel is that it opens up a new family of attention mechanisms that can relate to the well-established literature in kernel learning (Scholkopf and Smola, 2001) . As a result, we develop a new variant of attention which simply considers a product of symmetric kernels when modeling non-positional and positional embedding. Furthermore, our proposed formulation highlights naturally the main components of Transformer's attention, enabling a better understanding of this mechanism: recent variants of Transformers (Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019; Wang et al., 2018; Tsai et al., 2019a) can be expressed through these individual components.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_5",
  "x": "We empirically study multiple kernel forms and the ways to integrate positional embedding in neural machine translation (NMT) using IWSLT'14 GermanEnglish (De-En) dataset (Edunov et al., 2017) and sequence prediction (SP) using WikiText-103 dataset (Merity et al., 2016) . ---------------------------------- **ATTENTION** This section aims at providing an understanding of attention in Transformer via the lens of kernel. The inspiration for connecting the kernel (Scholkopf and Smola, 2001 ) and attention instantiates from the observation: both operations concurrently processes all inputs and calculate the similarity between the inputs. We first introduce the background (i.e., the original formulation) of attention and then provide a new reformulation within the class of kernel smoothers (Wasserman, 2006) . Next, we show that this new formulation allows us to explore new family of attention while at the same time offering a framework to categorize previous attention variants (Vaswani et al., 2017; Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019; Wang et al., 2018; Tsai et al., 2019a) .",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_6",
  "x": "As a result, Transformer (Vaswani et al., 2017) introduced positional embedding to indicate the positional relation for the inputs. Formally, a sequence x = [x 1 , x 2 , \u22ef, x T ] defines each element as x i = (f i , t i ) with f i \u2208 F being the nontemporal feature at time i and t i \u2208 T as an temporal feature (or we called it positional embedding). Note that f i can be the word representation (in neural machine translation (Vaswani et al., 2017) ), a pixel in a frame (in video activity recognition (Wang et al., 2018) ), or a music unit (in music generation (Huang et al., 2018b) ). t i can be a mixture of sine and cosine functions (Vaswani et al., 2017) or parameters that can be learned during back-propagation<cite> (Dai et al., 2019</cite>; Ott et al., 2019) . The feature vector are defined over a joint space X \u2236= (F \u00d7 T ). The resulting permutationinvariant set is: Followed the definition by Vaswani et al. (2017) , we use queries(q)/keys(k)/values(v) to represent the inputs for the attention.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_7",
  "x": "Recent work (Shaw et al., 2018; <cite>Dai et al., 2019</cite>; Huang et al., 2018b; Child et al., 2019; Parmar et al., 2018; Tsai et al., 2019a) proposed modifications to the Transformer for the purpose of better modeling inputs positional relation (Shaw et al., 2018; Huang et al., 2018b;<cite> Dai et al., 2019)</cite> , appending additional keys in S x k<cite> (Dai et al., 2019)</cite> , modifying the mask applied to Eq. (1) (Child et al., 2019) , or applying to distinct feature types Parmar et al., 2018; Tsai et al., 2019a) . These works adopt different designs of attention as comparing to the original form (Eq. (1)). In our paper, we aim at providing an unified view via the lens of kernel. ---------------------------------- **REFORMULATION VIA THE LENS OF KERNEL** We now provide the intuition to reformulate Eq. (1) via the lens of kernel.",
  "y": "background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_8",
  "x": "the weight, and d k being the feature dimension of x k W k . Decoder self-attention further introduces a mask to block the visibility of elements in S x k to x q . Particularly, decoder self-attention considers the decoded sequence as inputs (x k = x q ), where the decoded token at time t is not allowed to access the future decoded tokens (i.e., tokens decoded at time greater than t). On the contrary, encoder selfattention and decoder-encoder attention consider no additional mask to Eq. (1). Recent work (Shaw et al., 2018; <cite>Dai et al., 2019</cite>; Huang et al., 2018b; Child et al., 2019; Parmar et al., 2018; Tsai et al., 2019a) proposed modifications to the Transformer for the purpose of better modeling inputs positional relation (Shaw et al., 2018; Huang et al., 2018b;<cite> Dai et al., 2019)</cite> , appending additional keys in S x k<cite> (Dai et al., 2019)</cite> , modifying the mask applied to Eq. (1) (Child et al., 2019) , or applying to distinct feature types Parmar et al., 2018; Tsai et al., 2019a) . These works adopt different designs of attention as comparing to the original form (Eq. (1)). In our paper, we aim at providing an unified view via the lens of kernel.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_9",
  "x": "is a probability function depends on k and N when k(\u22c5, \u22c5) is always positive. In the prior work (Vaswani et al., 2017) Note that the kernel form k(x q , x k ) in the original Transformer (Vaswani et al., 2017 ) is a asymmetric exponential kernel with additional mapping W q and W k (Wilson et al., 2016; Li et al., 2017) 2 . The new formulation defines a larger space for composing attention by manipulating its individual components, and at the same time it is able to categorize different variants of attention in prior work (Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019; Wang et al., 2018; Tsai et al., 2019a) . In the following, we study these components by dissecting Eq. (2) into: 1) kernel feature space X , 2) kernel construction k(\u22c5, \u22c5), 3) value function v(\u22c5), and 4) set filtering function M (\u22c5, \u22c5). 2.2.1 Kernel Feature Space X In Eq. (2), to construct a kernel on X , the first thing is to identify the kernel feature space X . In addition to modeling sequences like word sentences (Vaswani et al., 2017) or music signals (Huang et al., 2018b) , the Transformer can also be applied to images (Parmar et al., 2018) , sets , and multimodal sequences (Tsai et al., 2019a) .",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_10",
  "x": "Due to distinct data types, these applications admit various kernel feature space: (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> : with F being non-positional feature space and T being the positional embedding space of the position in the sequence. (ii) Image Transformer (Parmar et al., 2018) : with F being non-positional feature space, H being the positional space of the height in an image, and W being the positional space of the width in an image. (iii) Set Transformer and NonLocal Neural Networks (Wang et al., 2018) : with no any positional information present. (iv) Multimodal Transformer (Tsai et al., 2019a) :",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_11",
  "x": "(iv) Multimodal Transformer (Tsai et al., 2019a) : with F \u2113 representing the language feature space, F v representing the vision feature space, F a representing the audio feature space, and T representing the temporal indicator space. For the rest of the paper, we will focus on the setting for sequence Transformer X = (F \u00d7 T ) and discuss the kernel construction on it. ---------------------------------- **KERNEL CONSTRUCTION AND THE ROLE OF** Positional Embedding k(\u22c5, \u22c5) The kernel construction on X = (F \u00d7 T ) has distinct design in variants of Transformers (Vaswani et al., 2017; <cite>Dai et al., 2019</cite>; Huang et al., 2018b; Shaw et al., 2018; Child et al., 2019) . Since now the kernel feature space considers a joint space, we will first discuss the kernel construction on F (the non-positional feature space) and then discuss how different variants integrate the positional embedding (with the positional feature space T ) into the kernel.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_12",
  "x": "The designs for integrating the positional embedding t q and t k are listed in the following. (i) Absolute Positional Embedding (Vaswani et al., 2017; <cite>Dai et al., 2019</cite>; Ott et al., 2019) : For the original Transformer (Vaswani et al., 2017) , each t i is represented by a vector with each dimension being sine or cosine functions. For learned positional embedding<cite> (Dai et al., 2019</cite>; Ott et al., 2019) , each t i is a learned parameter and is fixed for the same position for different sequences. These works defines the feature space as the direct sum of its temporal and non-temporal space: X = F \u2295 T . Via the lens of kernel, the kernel similarity is defined as (ii) Relative Positional Embedding in Transformer-XL<cite> (Dai et al., 2019)</cite> : t represents the indicator of the position in the sequence, and the kernel is chosen to be asymmetric of mixing sine and cosine functions: with k fq t q , t k being an asymmetric kernel with coefficients inferred by f q : log k fq t q , t k = \u2211 (iii) Relative Positional Embedding of Shaw et al. (2018) and Music Transformer (Huang et al., 2018b) : t \u22c5 represents the indicator of the position in the sequence, and the kernel is modified to be indexed by a look-up table:",
  "y": "uses background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_13",
  "x": "Kernel construction on X = (F \u00d7 T ). The designs for integrating the positional embedding t q and t k are listed in the following. (i) Absolute Positional Embedding (Vaswani et al., 2017; <cite>Dai et al., 2019</cite>; Ott et al., 2019) : For the original Transformer (Vaswani et al., 2017) , each t i is represented by a vector with each dimension being sine or cosine functions. For learned positional embedding<cite> (Dai et al., 2019</cite>; Ott et al., 2019) , each t i is a learned parameter and is fixed for the same position for different sequences. These works defines the feature space as the direct sum of its temporal and non-temporal space: X = F \u2295 T . Via the lens of kernel, the kernel similarity is defined as (ii) Relative Positional Embedding in Transformer-XL<cite> (Dai et al., 2019)</cite> : t represents the indicator of the position in the sequence, and the kernel is chosen to be asymmetric of mixing sine and cosine functions:",
  "y": "uses background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_14",
  "x": "Kernel construction on X = (F \u00d7 T ). The designs for integrating the positional embedding t q and t k are listed in the following. (i) Absolute Positional Embedding (Vaswani et al., 2017; <cite>Dai et al., 2019</cite>; Ott et al., 2019) : For the original Transformer (Vaswani et al., 2017) , each t i is represented by a vector with each dimension being sine or cosine functions. For learned positional embedding<cite> (Dai et al., 2019</cite>; Ott et al., 2019) , each t i is a learned parameter and is fixed for the same position for different sequences. These works defines the feature space as the direct sum of its temporal and non-temporal space: X = F \u2295 T . Via the lens of kernel, the kernel similarity is defined as (ii) Relative Positional Embedding in Transformer-XL<cite> (Dai et al., 2019)</cite> : t represents the indicator of the position in the sequence, and the kernel is chosen to be asymmetric of mixing sine and cosine functions:",
  "y": "uses background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_16",
  "x": "Dai et al. (2019) showed that the way to integrate positional embedding is better through Eq. (5) than through Eq. (6) and is better through Eq. (6) than through Eq. (4). We argue the reason is that if viewing f i and t i as two distinct spaces X \u2236= (F \u00d7 T ) , the direct sum x i = f i + t i may not be optimal when considering the kernel score between x q and x k . In contrast, Eq. (5) represents the kernel as a product of two kernels (one for f i and another for t i ), which is able to capture the similarities for both temporal and non-temporal components. ---------------------------------- **VALUE FUNCTION V(\u22c5)** The current Transformers consider two different value function construction: (Vaswani et al., 2017) and Sparse Transformer (Child et al., 2019) : (ii) Transformer-XL<cite> (Dai et al., 2019)</cite> , Music Transformer (Huang et al., 2018b) , Self-Attention with Relative Positional Embedding (Shaw et al., 2018) :",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_17",
  "x": "Since the decoded sequence is the output for previous timestep, the query at position i can only observe the keys being the tokens that are decoded with position < i. For convenience, let us define S 1 as the set returned by original Transformer (Vaswani et al., 2017 ) from M (x q , S x k ), which we will use it later. (iv) Decoder Self-Attention in Transformer-XL<cite> (Dai et al., 2019)</cite> : For each query x q in the decoded sequence, M (x q , S x k ) returns a set containing S 1 and additional memories (M (x q , S x k ) = S 1 + S mem , M (x q , S x k ) \u2283 S 1 ). S mem refers to additional memories. (v) Decoder Self-Attention in Sparse Transformer (Child et al., 2019) : For each query x q in the decoded sentence, M (x q , S x k ) returns a subset of S 1 (M (x q , S x k ) \u2282 S 1 ). To compare the differences for various designs, we see the computation time is inversely proportional to the number of elements in M (x q , S x k ). For performance-wise comparisons, Transformer-XL<cite> (Dai et al., 2019)</cite> showed that, the additional memories in M (x q , S x k ) are able to capture longer-term dependency than the original Transformer (Vaswani et al., 2017) and hence results in better performance. Sparse Transformer (Child et al., 2019) showed that although having much fewer elements in M (x q , S x k ), if the elements are carefully chosen, the attention can still reach the same performance as Transformer-XL<cite> (Dai et al., 2019)</cite> .",
  "y": "uses background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_18",
  "x": "For performance-wise comparisons, Transformer-XL<cite> (Dai et al., 2019)</cite> showed that, the additional memories in M (x q , S x k ) are able to capture longer-term dependency than the original Transformer (Vaswani et al., 2017) and hence results in better performance. Sparse Transformer (Child et al., 2019) showed that although having much fewer elements in M (x q , S x k ), if the elements are carefully chosen, the attention can still reach the same performance as Transformer-XL<cite> (Dai et al., 2019)</cite> . ---------------------------------- **EXPLORING THE DESIGN OF ATTENTION** So far, we see how Eq. (2) connects to the variants of Transformers. By changing the kernel construction in Section 2.2.2, we can define a larger space for composing attention. In this paper, we present a new form of attention with a kernel that is 1) valid (i.e., a kernel that is symmetric and positive semi-definite) and 2) delicate in the sense of constructing a kernel on a joint space (i.e., X = (F \u00d7 T )):",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_19",
  "x": "(v) Decoder Self-Attention in Sparse Transformer (Child et al., 2019) : For each query x q in the decoded sentence, M (x q , S x k ) returns a subset of S 1 (M (x q , S x k ) \u2282 S 1 ). To compare the differences for various designs, we see the computation time is inversely proportional to the number of elements in M (x q , S x k ). For performance-wise comparisons, Transformer-XL<cite> (Dai et al., 2019)</cite> showed that, the additional memories in M (x q , S x k ) are able to capture longer-term dependency than the original Transformer (Vaswani et al., 2017) and hence results in better performance. Sparse Transformer (Child et al., 2019) showed that although having much fewer elements in M (x q , S x k ), if the elements are carefully chosen, the attention can still reach the same performance as Transformer-XL<cite> (Dai et al., 2019)</cite> . ---------------------------------- **EXPLORING THE DESIGN OF ATTENTION** So far, we see how Eq. (2) connects to the variants of Transformers.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_20",
  "x": "By changing the kernel construction in Section 2.2.2, we can define a larger space for composing attention. In this paper, we present a new form of attention with a kernel that is 1) valid (i.e., a kernel that is symmetric and positive semi-definite) and 2) delicate in the sense of constructing a kernel on a joint space (i.e., X = (F \u00d7 T )): where W F and W T are weight matrices. The new form considers product of kernels with the first kernel measuring similarity between non-temporal features and the second kernel measuring similarity between temporal features. Both kernels are symmetric exponential kernel. Note that t i here is chosen as the mixture of sine and cosine functions as in the prior work (Vaswani et al., 2017; Ott et al., 2019) . In our experiment, we find it reaching competitive performance as comparing to the current state-of-the-art designs (Eq. (5) by<cite> Dai et al. (2019)</cite> ).",
  "y": "similarities uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_21",
  "x": "Q4. Is positional embedding required in value function? We conduct experiments on neural machine translation (NMT) and sequence prediction (SP) tasks since these two tasks are commonly chosen for studying Transformers (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> . Note that NMT has three different types of attentions (e.g., encoder selfattention, decoder-encoder attention, decoder selfattention) and SP has only one type of attention (e.g., decoder self-attention). For the choice of datasets, we pick IWSLT'14 German-English (De-En) dataset (Edunov et al., 2017) for NMT and WikiText-103 dataset (Merity et al., 2016) for SP as suggested by Edunov et al. (Edunov et al., 2017) and<cite> Dai et al. (Dai et al., 2019)</cite> . For fairness of comparisons, we train five random initializations and report test accuracy with the highest validation score. We fix the position-wise operations in Transformer 3 and only change the attention mechanism.",
  "y": "uses background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_22",
  "x": "We conduct experiments on neural machine translation (NMT) and sequence prediction (SP) tasks since these two tasks are commonly chosen for studying Transformers (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> . Note that NMT has three different types of attentions (e.g., encoder selfattention, decoder-encoder attention, decoder selfattention) and SP has only one type of attention (e.g., decoder self-attention). For the choice of datasets, we pick IWSLT'14 German-English (De-En) dataset (Edunov et al., 2017) for NMT and WikiText-103 dataset (Merity et al., 2016) for SP as suggested by Edunov et al. (Edunov et al., 2017) and<cite> Dai et al. (Dai et al., 2019)</cite> . For fairness of comparisons, we train five random initializations and report test accuracy with the highest validation score. We fix the position-wise operations in Transformer 3 and only change the attention mechanism. Similar to prior work (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> , we report BLEU score for NMT and perplexity for SP. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_23",
  "x": "Note that NMT has three different types of attentions (e.g., encoder selfattention, decoder-encoder attention, decoder selfattention) and SP has only one type of attention (e.g., decoder self-attention). For the choice of datasets, we pick IWSLT'14 German-English (De-En) dataset (Edunov et al., 2017) for NMT and WikiText-103 dataset (Merity et al., 2016) for SP as suggested by Edunov et al. (Edunov et al., 2017) and<cite> Dai et al. (Dai et al., 2019)</cite> . For fairness of comparisons, we train five random initializations and report test accuracy with the highest validation score. We fix the position-wise operations in Transformer 3 and only change the attention mechanism. Similar to prior work (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> , we report BLEU score for NMT and perplexity for SP. ---------------------------------- **INCORPORATING POSITIONAL EMBEDDING**",
  "y": "similarities"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_24",
  "x": "Referring to Sections 2.2.2 and 2.3, we consider four cases: 1) PE as direct sum in the feature space (see Eq. (4)), 2) PE as a look-up table (see Eq. (6)), 3) PE in product kernel with asymmetric kernel (see Eq. (5)), and 4) PE in product kernel with symmetric kernel (see Eq. (9)). We present the results in Table 1 . First, we see that by having PE as a look-up (Edunov et al., 2017) and SP stands for sequence prediction on WikiText-103 dataset (Merity et al., 2016) . \u2191 means the upper the better and \u2193 means the lower the better. Table 2 : Kernel Types. Other than manipulating the kernel choice of the non-positional features, we fix the configuration by Vaswani et al. (2017) for NMT and the configuration by<cite> Dai et al. (2019)</cite> for SP. 34.14 24.13 24.21 table, it outperforms the case with having PE as direct-sum in feature space, especially for SP task.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_25",
  "x": "Other than manipulating the kernel choice of the non-positional features, we fix the configuration by Vaswani et al. (2017) for NMT and the configuration by<cite> Dai et al. (2019)</cite> for SP. 34.14 24.13 24.21 table, it outperforms the case with having PE as direct-sum in feature space, especially for SP task. Note that the look-up table is indexed by the relative position (i.e., t q \u2212 t k ) instead of absolute position. Second, we see that PE in the product kernel proposed by<cite> Dai et al. (Dai et al., 2019)</cite> may not constantly outperform the other integration types (it has lower BLEU score for NMT). Our proposed product kernel reaches the best result in NMT and is competitive to the best result in SP. ---------------------------------- **KERNEL TYPES**",
  "y": "differences uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_26",
  "x": "---------------------------------- **KERNEL TYPES** To find the best kernel form in the attention mechanism, in addition to the exponential kernel (see Eq. (3)), we compare different kernel forms (i.e., linear, polynomial, and rbf kernel) for the non-positional features. We also provide the results for changing asymmetric to the symmetric kernel, when forcing W q = W k , so that the resulting kernel is a valid kernel (Scholkopf and Smola, 2001) . The numbers are shown in Table 2 . Note that, for fairness, other than manipulating the kernel choice of the non-positional features, we fix the configuration by Vaswani et al. (Vaswani et al., 2017) for NMT and the configuration by<cite> Dai et al. (Dai et al., 2019)</cite> for SP. We first observe that the linear kernel does not converge for both NMT and SP.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_27",
  "x": "The need of the positional embedding (PE) in the attention mechanism is based on the argument that the attention mechanism is an order-agnostic (or, permutation equivariant) operation (Vaswani et al., 2017; Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019) . However, we show that, for decoder self-attention, the operation is not order-agnostic. For clarification, we are not attacking the claim made by the prior work (Vaswani et al., 2017; Shaw et al., 2018;  Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019 ), but we aim at providing a new look at the order-invariance problem when considering the attention mechanism with masks (masks refer to the set filtering function in our kernel formulation). In other words, previous work did not consider the mask between queries and keys when discussing the order-invariance problem (P\u00e9rez et al., 2019) . To put it formally, we first present the definition by for a permutation equivariance function: Definition 2. Denote \u03a0 as the set of all permutations over [n] = {1, \u22ef, n}. A function f unc \u2236 X n \u2192 Y n is permutation equivariant iff for any permutation \u03c0 \u2208 \u03a0, f unc(\u03c0x) = \u03c0f unc(x). showed that the standard attention (encoder self-attention (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> ) is permutation equivariant.",
  "y": "background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_28",
  "x": "---------------------------------- **ORDER-INVARIANCE IN ATTENTION** The need of the positional embedding (PE) in the attention mechanism is based on the argument that the attention mechanism is an order-agnostic (or, permutation equivariant) operation (Vaswani et al., 2017; Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019) . However, we show that, for decoder self-attention, the operation is not order-agnostic. For clarification, we are not attacking the claim made by the prior work (Vaswani et al., 2017; Shaw et al., 2018;  Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019 ), but we aim at providing a new look at the order-invariance problem when considering the attention mechanism with masks (masks refer to the set filtering function in our kernel formulation). In other words, previous work did not consider the mask between queries and keys when discussing the order-invariance problem (P\u00e9rez et al., 2019) . To put it formally, we first present the definition by for a permutation equivariance function:",
  "y": "differences background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_29",
  "x": "Also, we see no much performance difference when comparing asymmetric to symmetric kernel. In the experiment, we fix the size of W \u22c5 in the kernel, and thus adopting the symmetric kernel benefits us from saving parameters. ---------------------------------- **ORDER-INVARIANCE IN ATTENTION** The need of the positional embedding (PE) in the attention mechanism is based on the argument that the attention mechanism is an order-agnostic (or, permutation equivariant) operation (Vaswani et al., 2017; Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019) . However, we show that, for decoder self-attention, the operation is not order-agnostic. For clarification, we are not attacking the claim made by the prior work (Vaswani et al., 2017; Shaw et al., 2018;  Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019 ), but we aim at providing a new look at the order-invariance problem when considering the attention mechanism with masks (masks refer to the set filtering function in our kernel formulation).",
  "y": "differences"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_30",
  "x": "However, we show that, for decoder self-attention, the operation is not order-agnostic. For clarification, we are not attacking the claim made by the prior work (Vaswani et al., 2017; Shaw et al., 2018;  Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019 ), but we aim at providing a new look at the order-invariance problem when considering the attention mechanism with masks (masks refer to the set filtering function in our kernel formulation). In other words, previous work did not consider the mask between queries and keys when discussing the order-invariance problem (P\u00e9rez et al., 2019) . To put it formally, we first present the definition by for a permutation equivariance function: Definition 2. Denote \u03a0 as the set of all permutations over [n] = {1, \u22ef, n}. A function f unc \u2236 X n \u2192 Y n is permutation equivariant iff for any permutation \u03c0 \u2208 \u03a0, f unc(\u03c0x) = \u03c0f unc(x). showed that the standard attention (encoder self-attention (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> ) is permutation equivariant. Here, we present the non-permutation-equivariant problem on the decoder self-attention: (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> is not permutation equivariant.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_31",
  "x": "However, we show that, for decoder self-attention, the operation is not order-agnostic. For clarification, we are not attacking the claim made by the prior work (Vaswani et al., 2017; Shaw et al., 2018;  Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019 ), but we aim at providing a new look at the order-invariance problem when considering the attention mechanism with masks (masks refer to the set filtering function in our kernel formulation). In other words, previous work did not consider the mask between queries and keys when discussing the order-invariance problem (P\u00e9rez et al., 2019) . To put it formally, we first present the definition by for a permutation equivariance function: Definition 2. Denote \u03a0 as the set of all permutations over [n] = {1, \u22ef, n}. A function f unc \u2236 X n \u2192 Y n is permutation equivariant iff for any permutation \u03c0 \u2208 \u03a0, f unc(\u03c0x) = \u03c0f unc(x). showed that the standard attention (encoder self-attention (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> ) is permutation equivariant. Here, we present the non-permutation-equivariant problem on the decoder self-attention: (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> is not permutation equivariant.",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_0",
  "x": "---------------------------------- **INTRODUCTION** The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009) , attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically<cite> (Plank and Moschitti, 2013)</cite> . This is where we need to resort to domain adaptation techniques (DA) to adapt a model trained on one domain (the source domain) into a new model which can perform well on new domains (the target domains).",
  "y": "motivation"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_1",
  "x": "This is where we need to resort to domain adaptation techniques (DA) to adapt a model trained on one domain (the source domain) into a new model which can perform well on new domains (the target domains). The consequences of linguistic variation between training and testing data on NLP tools have been studied extensively in the last couple of years for various NLP tasks such as Part-of-Speech tagging (Blitzer et al., 2006; Huang and Yates, 2010; Schnabel and Sch\u00fctze, 2014) , named entity recognition (Daum\u00e9 III, 2007) and sentiment analysis (Blitzer et al., 2007; Daum\u00e9 III, 2007; Daum\u00e9 III et al., 2010; Blitzer et al., 2011) , etc. Unfortunately, there is very little work on domain adaptation for RE. The only study explicitly targeting this problem so far is by<cite> Plank and Moschitti (2013)</cite> who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels. Although this idea is interesting, it suffers from two major limitations: + It does not incorporate word cluster information at different levels of granularity. In fact,<cite> Plank and Moschitti (2013)</cite> only use the 10-bit cluster prefix in their study. We will demonstrate later that the adaptability of relation extractors can benefit significantly from the addition of word cluster features at various granularities.",
  "y": "background"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_2",
  "x": "Unfortunately, there is very little work on domain adaptation for RE. The only study explicitly targeting this problem so far is by<cite> Plank and Moschitti (2013)</cite> who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels. Although this idea is interesting, it suffers from two major limitations: + It does not incorporate word cluster information at different levels of granularity. In fact,<cite> Plank and Moschitti (2013)</cite> only use the 10-bit cluster prefix in their study. We will demonstrate later that the adaptability of relation extractors can benefit significantly from the addition of word cluster features at various granularities. + It is unclear if this approach can encode realvalued features of words (such as word embeddings (Mnih and Hinton, 2007; Collobert and Weston, 2008) ) effectively. As the real-valued features are able to capture latent yet useful properties of words, the augmentation of lexical terms with these features is desirable to provide a more general representation, potentially helping relation extractors perform more robustly across domains.",
  "y": "motivation background"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_3",
  "x": "In fact,<cite> Plank and Moschitti (2013)</cite> only use the 10-bit cluster prefix in their study. We will demonstrate later that the adaptability of relation extractors can benefit significantly from the addition of word cluster features at various granularities. + It is unclear if this approach can encode realvalued features of words (such as word embeddings (Mnih and Hinton, 2007; Collobert and Weston, 2008) ) effectively. As the real-valued features are able to capture latent yet useful properties of words, the augmentation of lexical terms with these features is desirable to provide a more general representation, potentially helping relation extractors perform more robustly across domains. In this work, we propose to avoid these limitations by applying a feature-based approach for RE which allows us to integrate various word features of generalization into a single system more natu-rally and effectively. The application of word representations such as word clusters in domain adaptation of RE <cite>(Plank and Moschitti, 2013</cite> ) is motivated by its successes in semi-supervised methods (Chan and Roth, 2010; Sun et al., 2011) where word representations help to reduce data-sparseness of lexical information in the training data. In DA terms, since the vocabularies of the source and target domains are usually different, word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains, hence bridge the gap between domains.",
  "y": "background"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_4",
  "x": "Given the more general representations provided by word representations above, how can we learn a relation extractor from the labeled source domain data that generalizes well to new domains? In traditional machine learning where the challenge is to utilize the training data to make predictions on unseen data points (generated from the same distribution as the training data), the classifier with a good generalization performance is the one that not only fits the training data, but also avoids ovefitting over it. This is often obtained via regularization methods to penalize complexity of classifiers. Exploiting the shared interest in generalization performance with traditional machine learning, in domain adaptation for RE, we would prefer the relation extractor that fits the source domain data, but also circumvents the overfitting problem over this source domain 1 so that it could generalize well on new domains. Eventually, regularization methods can be considered naturally as a simple yet general technique to cope with DA problems. Following<cite> Plank and Moschitti (2013)</cite> , we assume that we only have labeled data in a single source domain but no labeled as well as unlabeled target data. Moreover, we consider the singlesystem DA setting where we construct a single system able to work robustly with different but related domains (multiple target domains).",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_5",
  "x": "Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al., 2010) . We investigate word embeddings induced by two typical language models: Collobert and Weston (2008) embeddings (C&W) (Collobert and Weston, 2008; Turian et al., 2010) and Hierarchical log-bilinear embeddings (HLBL) (Mnih and Hinton, 2007; Mnih and Hinton, 2009; Turian et al., 2010) . 5 Feature Set 5.1 Baseline Feature Set Sun et al. (2011) utilize the full feature set from (Zhou et al., 2005) plus some additional features and achieve the state-of-the-art feature-based RE system. Unfortunately, this feature set includes the human-annotated (gold-standard) information on entity and mention types which is often missing or noisy in reality<cite> (Plank and Moschitti, 2013)</cite> . This issue becomes more serious in our setting of single-system DA where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available. Therefore, following the settings of<cite> Plank and Moschitti (2013)</cite> , we will only assume entity boundaries and not rely on the gold standard information in the experiments. We apply the same feature set as Sun et al. (2011) but remove the entity and mention type information 2 .",
  "y": "motivation uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_6",
  "x": "While word clusters can be recognized as an one-hot vector representation over a small vocabulary, word embeddings are dense, lowdimensional, and real-valued vectors (distributed representations). Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al., 2010) . We investigate word embeddings induced by two typical language models: Collobert and Weston (2008) embeddings (C&W) (Collobert and Weston, 2008; Turian et al., 2010) and Hierarchical log-bilinear embeddings (HLBL) (Mnih and Hinton, 2007; Mnih and Hinton, 2009; Turian et al., 2010) . 5 Feature Set 5.1 Baseline Feature Set Sun et al. (2011) utilize the full feature set from (Zhou et al., 2005) plus some additional features and achieve the state-of-the-art feature-based RE system. Unfortunately, this feature set includes the human-annotated (gold-standard) information on entity and mention types which is often missing or noisy in reality<cite> (Plank and Moschitti, 2013)</cite> . This issue becomes more serious in our setting of single-system DA where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available. Therefore, following the settings of<cite> Plank and Moschitti (2013)</cite> , we will only assume entity boundaries and not rely on the gold standard information in the experiments.",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_7",
  "x": "Let M1 and M2 be the first and second mentions in the relation. to facilitate system comparison later. We evaluate C&W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al. (2010) and can be downloaded here 4 . The fact that we utilize the large, general and unbiased resources generated from the previous works for evaluation not only helps to verify the effectiveness of the resources across different tasks and settings but also supports our setting of single-system DA. We use the ACE 2005 corpus for DA experiments (as in<cite> Plank and Moschitti (2013)</cite> ). It involves 6 relation types and 6 domains: broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversation (cts), weblogs (wl) and usenet (un). We follow the standard practices on ACE<cite> (Plank and Moschitti, 2013)</cite> and use news (the union of bn and nw) as the source domain and bc, cts and wl as our target domains.",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_8",
  "x": "We follow the standard practices on ACE<cite> (Plank and Moschitti, 2013)</cite> and use news (the union of bn and nw) as the source domain and bc, cts and wl as our target domains. We take half of bc as the only target development set, and use the remaining data and domains for testing purposes (as they are small already). As noted in<cite> Plank and Moschitti (2013)</cite> , the distributions of relations as well as the vocabularies of the domains are quite different. ---------------------------------- **EVALUATION OF WORD EMBEDDING FEATURES** We investigate the effectiveness of word embeddings on lexical features by following the procedure described in Section 5.2. We test our system on two scenarios: In-domain: the system is trained and evaluated on the source domain (bn+nw, 5-fold cross validation); Out-of-domain: the system is trained on the source domain and evaluated on the target development set of bc (bc dev).",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_9",
  "x": "We use the ACE 2005 corpus for DA experiments (as in<cite> Plank and Moschitti (2013)</cite> ). It involves 6 relation types and 6 domains: broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversation (cts), weblogs (wl) and usenet (un). We follow the standard practices on ACE<cite> (Plank and Moschitti, 2013)</cite> and use news (the union of bn and nw) as the source domain and bc, cts and wl as our target domains. We take half of bc as the only target development set, and use the remaining data and domains for testing purposes (as they are small already). As noted in<cite> Plank and Moschitti (2013)</cite> , the distributions of relations as well as the vocabularies of the domains are quite different. ---------------------------------- **EVALUATION OF WORD EMBEDDING FEATURES**",
  "y": "background"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_10",
  "x": "For word clusters, we experiment with two possibilities: (i) only using a single prefix length of 10 (as<cite> Plank and Moschitti (2013)</cite> did) (denoted by WC10) and (ii) applying multiple prefix lengths of 4, 6, 8, 10 together with the full string (denoted by WC). (i): The baseline system achieves a performance of 51.4% within its own domain while the performance on target domains bc, cts, wl drops to 49.7%, 41.5% and 36.6% respectively. Our baseline performance is worse than that of<cite> Plank and Moschitti (2013)</cite> only on the target domain cts and better in the other cases. This might be explained by the difference between our baseline feature set and the feature set underlying their kernel-based system. However, the performance order across domains of the two baselines are the same. Besides, the baseline performance is improved over all target domains when the system is enriched with word cluster features of the 10 prefix length only (row 2). (ii): Over all the target domains, the performance of the system augmented with word cluster features of various granularities (row 3) is superior to that when only cluster features for the prefix length 10 are added (row 2).",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_11",
  "x": "(i): The baseline system achieves a performance of 51.4% within its own domain while the performance on target domains bc, cts, wl drops to 49.7%, 41.5% and 36.6% respectively. Our baseline performance is worse than that of<cite> Plank and Moschitti (2013)</cite> only on the target domain cts and better in the other cases. This might be explained by the difference between our baseline feature set and the feature set underlying their kernel-based system. However, the performance order across domains of the two baselines are the same. Besides, the baseline performance is improved over all target domains when the system is enriched with word cluster features of the 10 prefix length only (row 2). (ii): Over all the target domains, the performance of the system augmented with word cluster features of various granularities (row 3) is superior to that when only cluster features for the prefix length 10 are added (row 2). This is significant (at confidence level \u2265 95%) for domains bc and wl and verifies our assumption that various granularities for word cluster features are more effective than a single granularity for domain adaptation of RE.",
  "y": "differences"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_12",
  "x": "(iii): Row 4 shows that word embedding itself is also very useful for domain adaptation in RE since it improves the baseline system for all the target domains. (iv): In row 5, we see that the addition of both word cluster and word embedding features improves the system further and results in the best performance over all target domains (this is significant with confidence level \u2265 95% in domains bc and wl). The result suggests that word embeddings seem to capture different information from word clusters and their combination would be effective to generalize relation extractors across domains. However, in domain cts, the improvement that word embeddings provide for word clusters is modest. This is because the RCV1 corpus used to induce the word embeddings (Turian et al., 2010) does not cover spoken language words in cts very well. (v): Finally, the in-domain performance is also improved consistently demonstrating the robustness of word representations<cite> (Plank and Moschitti, 2013)</cite> . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_0",
  "x": "---------------------------------- **INTRODUCTION** Dependency parsers can recover much of the predicate-argument structure of a sentence, while being relatively efficient to train and extremely fast at parsing. Dependency parsers have been gaining in popularity in recent times due to the availability of large dependency treebanks for several languages and parsing shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a; Bharati et al., 2012) . <cite>Ambati et al. (2013)</cite> showed that the performance of Malt (Nivre et al., 2007b) on the free word order language, Hindi, is improved by using lexical categories from Combinatory Categorial Grammar (CCG) (Steedman, 2000) . In this paper, we extend <cite>this work</cite> and show that CCG categories are useful even in the case of English, a typologically different language, where parsing accuracy of dependency parsers is already extremely high. In addition, we also demonstrate the utility of CCG categories to MST (McDonald et al., 2005) for both languages.",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_1",
  "x": "<cite>Ambati et al. (2013)</cite> showed that the performance of Malt (Nivre et al., 2007b) on the free word order language, Hindi, is improved by using lexical categories from Combinatory Categorial Grammar (CCG) (Steedman, 2000) . In this paper, we extend <cite>this work</cite> and show that CCG categories are useful even in the case of English, a typologically different language, where parsing accuracy of dependency parsers is already extremely high. In addition, we also demonstrate the utility of CCG categories to MST (McDonald et al., 2005) for both languages. CCG lexical categories contain subcategorization information regarding the dependencies of predicates, including longdistance dependencies. We show that providing this subcategorization information in the form of CCG categories can help both Malt and MST on precisely those dependencies for which they are known to have weak rates of recovery. The result is particularly interesting for Malt, the fast greedy parser, as the improvement in Malt comes without significantly compromising its speed, so that it can be practically applied in web scale parsing. Our results apply both to English, a fixed word order and morphologically simple language, and to Hindi, a free word order and morphologically rich language, indicating that CCG categories from a supertagger are an easy and robust way of introducing lexicalized subcategorization information into dependency parsers.",
  "y": "extends"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_2",
  "x": "features from a dependency parser to a re-ranker with an improvement of 0.35% in labelled F-score of the CCGbank test set. Conversely, <cite>Ambati et al. (2013)</cite> showed that a Hindi dependency parser (Malt) could be improved by using CCG categories. Using an algorithm similar to Cakici (2005) and Uematsu et al. (2013) , <cite>they first created</cite> a <cite>Hindi CCGbank</cite> from a Hindi dependency treebank and built a supertagger. <cite>They provided</cite> CCG categories from a supertagger as features to Malt and obtained overall improvements of 0.3% and 0.4% in unlabelled and labelled attachment scores respectively. Figure 1 shows a CCG derivation with CCG lexical categories for each word and Stanford scheme dependencies (De Marneffe et al., 2006) for an example English sentence. (Details of CCG and dependency parsing are given by Steedman (2000) and K\u00fcbler et al. (2009) ----------------------------------",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_3",
  "x": "Figure 1: A CCG derivation and the Stanford scheme dependencies for an example sentence. features from a dependency parser to a re-ranker with an improvement of 0.35% in labelled F-score of the CCGbank test set. Conversely, <cite>Ambati et al. (2013)</cite> showed that a Hindi dependency parser (Malt) could be improved by using CCG categories. Using an algorithm similar to Cakici (2005) and Uematsu et al. (2013) , <cite>they first created</cite> a <cite>Hindi CCGbank</cite> from a Hindi dependency treebank and built a supertagger. <cite>They provided</cite> CCG categories from a supertagger as features to Malt and obtained overall improvements of 0.3% and 0.4% in unlabelled and labelled attachment scores respectively. Figure 1 shows a CCG derivation with CCG lexical categories for each word and Stanford scheme dependencies (De Marneffe et al., 2006) for an example English sentence. (Details of CCG and dependency parsing are given by Steedman (2000) and K\u00fcbler et al. (2009)",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_4",
  "x": "They extracted n-best parses from a CCG parser and provided dependency Pierre Vinken will join the board as a nonexecutive director Nov. 29 Figure 1: A CCG derivation and the Stanford scheme dependencies for an example sentence. features from a dependency parser to a re-ranker with an improvement of 0.35% in labelled F-score of the CCGbank test set. Conversely, <cite>Ambati et al. (2013)</cite> showed that a Hindi dependency parser (Malt) could be improved by using CCG categories. Using an algorithm similar to Cakici (2005) and Uematsu et al. (2013) , <cite>they first created</cite> a <cite>Hindi CCGbank</cite> from a Hindi dependency treebank and built a supertagger. <cite>They provided</cite> CCG categories from a supertagger as features to Malt and obtained overall improvements of 0.3% and 0.4% in unlabelled and labelled attachment scores respectively. Figure 1 shows a CCG derivation with CCG lexical categories for each word and Stanford scheme dependencies (De Marneffe et al., 2006) for an example English sentence.",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_5",
  "x": "For Hindi, we worked with the Hindi Dependency Treebank (HDT) released as part of Coling 2012 Shared Task (Bharati et al., 2012) . HDT contains 12,041 training, 1,233 development and 1,828 testing sentences. We used the English (Hockenmaier and Steedman, 2007) and <cite>Hindi CCGbanks</cite> (Ambati et al., 1 http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html 2013) for our experiments. For Hindi we used two lexicons: a fine-grained one (with morphological information) and a coarse-grained one (without morphological information). ---------------------------------- **SUPERTAGGERS** We used Clark and Curran (2004) 's supertagger for English, and <cite>Ambati et al. (2013)</cite> 's supertagger for Hindi.",
  "y": "uses"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_6",
  "x": "For Hindi we used two lexicons: a fine-grained one (with morphological information) and a coarse-grained one (without morphological information). ---------------------------------- **SUPERTAGGERS** We used Clark and Curran (2004) 's supertagger for English, and <cite>Ambati et al. (2013)</cite> 's supertagger for Hindi. Both are Maximum Entropy based CCG supertaggers. The Clark and Curran (2004) supertagger uses different features like word, partof-speech, and contextual and complex bi-gram features to obtain a 1-best accuracy of 91.5% on the development set. In addition to the above mentioned features, <cite>Ambati et al. (2013)</cite> employed morphological features useful for Hindi.",
  "y": "uses"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_7",
  "x": "Both are Maximum Entropy based CCG supertaggers. The Clark and Curran (2004) supertagger uses different features like word, partof-speech, and contextual and complex bi-gram features to obtain a 1-best accuracy of 91.5% on the development set. In addition to the above mentioned features, <cite>Ambati et al. (2013)</cite> employed morphological features useful for Hindi. The 1-best accuracy of Hindi supertagger for finegrained and coarse-grained lexicon is 82.92% and 84.40% respectively. ---------------------------------- **DEPENDENCY PARSERS** There has been a significant amount of work on parsing English and Hindi using the Malt and MST parsers in the recent past (Nivre et al., 2007a; Bharati et al., 2012) .",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_8",
  "x": "In the case of English, Malt uses arc-standard and stack-projective parsing algorithms for CoNLL and Stanford schemes respectively and LIBLIN-EAR learner (Fan et al., 2008) for both the schemes. MST uses 1st-order features, and a projective parsing algorithm with 5-best MIRA training for both the schemes. For Hindi, Malt uses the arc-standard parsing algorithm with a LIBLIN-EAR learner. MST uses 2nd-order features, nonprojective algorithm with 5-best MIRA training. For English, we assigned POS-tags using a perceptron tagger (Collins, 2002) . For Hindi, we also did all our experiments using automatic features <cite>Ambati et al. (2013)</cite> ). (POS, chunk and morphological information) extracted using a Hindi shallow parser 2 .",
  "y": "uses"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_9",
  "x": "Following <cite>Ambati et al. (2013)</cite> , we used supertags which occurred at least K times in the training data, and backed off to coarse POS-tags otherwise. For English K=1, i.e., when we use CCG categories for all words, gave the best results. K=15 gave the best results for Hindi due to sparsity issues, as the data for Hindi is small. We provided a supertag as an atomic symbol similar to a POS tag and didn't split it into a list of argument and result categories. We explored both Stanford and CoNLL schemes for English and fine and coarsegrained CCG categories for Hindi. All feature and parser tuning was done on the development data. We assigned automatic POS-tags and supertags to the training data.",
  "y": "uses"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_10",
  "x": "**ANALYSIS: HINDI** In the case of Hindi, for MST, providing CCG categories gave an increment of 0.5%, 0.4% and 0.3% for ROOT, SUBJ and OBJ labels respectively in F-score over the baseline. <cite>Ambati et al. (2013)</cite> showed that for Hindi, providing CCG categories as features improved Malt in better handling of long distance dependencies. The percentage of dependencies in the 1\u22125, 6\u221210 and >10 distance ranges are 82.2%, 8.6% and 9.2% respectively out of the total of around 40,000 dependencies. Similar to English, there was very slight improvement for short distance dependencies (1\u22125). But for longer distances, 6\u221210, and >10, there was significant improvement of 1.3% and 1.3% respectively for MST. <cite>Ambati et al. (2013)</cite> reported similar improvements for Malt as well.",
  "y": "similarities"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_11",
  "x": "**IMPACT ON WEB SCALE PARSING** Greedy parsers such as Malt are very fast and are practically useful in large-scale applications such as parsing the web. ---------------------------------- **CONCLUSION** We have shown that informative CCG categories, which contain both local subcategorization information and capture long distance dependencies elegantly, improve the performance of two dependency parsers, Malt and MST, by helping in recovering long distance relations for Malt and local verbal arguments for MST. This is true both in the case of English (a fixed word order language) and Hindi (free word order and morphologically richer language), extending the result of <cite>Ambati et al. (2013)</cite> . The result is particularly interesting in the case of Malt which cannot directly use valency information, which CCG categories provide indirectly.",
  "y": "extends"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_12",
  "x": "**ANALYSIS: HINDI** In the case of Hindi, for MST, providing CCG categories gave an increment of 0.5%, 0.4% and 0.3% for ROOT, SUBJ and OBJ labels respectively in F-score over the baseline. <cite>Ambati et al. (2013)</cite> showed that for Hindi, providing CCG categories as features improved Malt in better handling of long distance dependencies. The percentage of dependencies in the 1\u22125, 6\u221210 and >10 distance ranges are 82.2%, 8.6% and 9.2% respectively out of the total of around 40,000 dependencies. Similar to English, there was very slight improvement for short distance dependencies (1\u22125). But for longer distances, 6\u221210, and >10, there was significant improvement of 1.3% and 1.3% respectively for MST. <cite>Ambati et al. (2013)</cite> reported similar improvements for Malt as well.",
  "y": "uses"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_13",
  "x": "For English, in case of Malt, we achieved 0.3% improvement in both UAS and LAS for Stanford scheme. For CoNLL scheme, these improvements were 0.4% and 0.5% in UAS and LAS respectively. For MST, we got around 0.5% improvements in all cases. In case of Hindi, fine-grained supertags gave larger improvements for MST. We got final improvements of 0.5% and 0.3% in UAS and LAS respectively. In contrast, for Malt, <cite>Ambati et al. (2013)</cite> had shown that coarse-grained supertags gave larger improvements of 0.3% and 0.4% in UAS and LAS respectively. Due to better handling of error propagation in MST, the richer information in fine-grained categories may have surpassed the slightly lower supertagger performance, compared to coarse-grained categories.",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_14",
  "x": "For English, in case of Malt, we achieved 0.3% improvement in both UAS and LAS for Stanford scheme. For CoNLL scheme, these improvements were 0.4% and 0.5% in UAS and LAS respectively. For MST, we got around 0.5% improvements in all cases. In case of Hindi, fine-grained supertags gave larger improvements for MST. We got final improvements of 0.5% and 0.3% in UAS and LAS respectively. In contrast, for Malt, <cite>Ambati et al. (2013)</cite> had shown that coarse-grained supertags gave larger improvements of 0.3% and 0.4% in UAS and LAS respectively. Due to better handling of error propagation in MST, the richer information in fine-grained categories may have surpassed the slightly lower supertagger performance, compared to coarse-grained categories.",
  "y": "differences"
 },
 {
  "id": "2292b2c0366ef12a5dd25e544f6b2d_0",
  "x": "In this paper, we compare different ways of scaling up state-of-the-art semantic parsers for Freebase by adding synonyms and paraphrases. First, we consider Berant and Liang (2014) 's own extension of the semantic parser of <cite>Berant et al. (2013)</cite> by using paraphrases. Second, we apply WordNet synonyms (Miller, 1995) for selected parts of speech to the queries in the Free917 dataset. The new pairs of queries and logical forms are added to the dataset on which the semantic parsers are retrained. We find that both techniques of enhancing the lexical coverage of the semantic parsers result in improved parsing performance, and that the improvements add up nicely. However, improved parsing performance does not correspond to improved F1-score in answer retrieval when using the respective parser in a response-based learning framework. We show that in order to produce helpful feedback for responsebased learning, parser performance on incorrect En-glish queries needs to be taken into account, which is standardly ignored in parser evaluation.",
  "y": "uses"
 },
 {
  "id": "2292b2c0366ef12a5dd25e544f6b2d_1",
  "x": "Our work is most closely related to Riezler et al. (2014) . We extend their application of responsebased learning for SMT to a larger and lexically more diverse dataset and show how to perform model selection in the environment from which response signals are obtained. In contrast to their work where a monolingual SMT-based approach (Andreas et al., 2013 ) is used as semantic parser, our work builds on existing parsers for Freebase, with a focus on exploiting paraphrasing and synonym extension for scaling semantic parsers to open-domain database queries. Response-based learning has been applied in previous work to semantic parsing itself (Kwiatowski et al. (2013) , <cite>Berant et al. (2013)</cite> , Goldwasser and Roth (2013) , inter alia). In these works, extrinsic responses in form of correct answers from a database are used to alleviate the problem of manual data annotation in semantic parsing. Saluja et al. (2012) integrate human binary feedback on the quality of an SMT system output into a discriminative learner. Further work on learning from weak supervision signals has been presented in the machine learning community, e.g., in form of coactive learning (Shivaswamy and Joachims, 2012) , reinforcement learning (Sutton and Barto, 1998) , or online learning with limited feedback (Cesa-Bianchi and Lugosi, 2006) .",
  "y": "background"
 },
 {
  "id": "2292b2c0366ef12a5dd25e544f6b2d_2",
  "x": "In that case, y \u2212 needs to be computed in order to perform the stochastic gradient descent update of the weight vector. If the feedback is negative, the prediction is treated as y \u2212 and y + needs to be computed for the update. If either y + or y \u2212 cannot be computed, the example is skipped. ---------------------------------- **SCALING SEMANTIC PARSING TO OPEN-DOMAIN DATABASE QUERIES** The main challenge of grounding SMT in semantic parsing for Freebase lies in scaling the semantic parser to the lexical diversity of the open-domain database. Our baseline system is the parser of <cite>Berant et al. (2013)</cite> , called SEMPRE.",
  "y": "uses"
 },
 {
  "id": "2292b2c0366ef12a5dd25e544f6b2d_3",
  "x": "The SMT framework used is CDEC (Dyer et al., 2010) with standard dense features and additional sparse features as described in Simianer et al. (2012) 4 . Training of the baseline SMT system was performed on the COMMON CRAWL 5 (Smith et al., 2013 ) dataset consisting of 7.5M parallel English-German segments extracted from the web. Response-based learning for SMT uses the code described in Riezler et al. (2014) 6 . For semantic parsing we use the SEMPRE and PARASEMPRE tools of <cite>Berant et al. (2013)</cite> and Berant and Liang (2014) which were trained on the training portion of the FREE917 corpus 7 . Further models use the training data enhanced with synonyms from WordNet as described in Section 4. Following Jones et al. (2012), we evaluate semantic parsers according to precision, defined as the percentage of correctly answered examples out of those for which a parse could be produced, recall, defined as the percentage of total examples answered correctly, and F1-score, defined as harmonic mean of precision and recall. Furthermore, we report false discovery rate (FDR) on the combined set of 276 correct and 166 incorrect database queries.",
  "y": "uses"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_0",
  "x": "UKP-Athene <cite>(Hanselowski et al., 2018)</cite> , the highest document retrieval scoring team, uses MediaWiki API 1 to search the Wikipedia database for the claims noun phrases. ---------------------------------- **SENTENCE RETRIEVAL** In order to extract evidence sentences, (Thorne et al., 2018 ) use a TF-IDF approach similar to their document retrieval. The UCL team (Yoneda et al., 2018) trains a logistic regression model on a heuristically set of features. Enhanced Sequential Inference Model (ESIM) (Chen et al., 2016) with some small modifications has been used in (Nie et al., 2019;<cite> Hanselowski et al., 2018)</cite> . ESIM encodes premises and hypotheses using one Bidirectional Long Short-Term Memory (BiLSTM) with shared weights.",
  "y": "background"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_1",
  "x": "The UCL team (Yoneda et al., 2018) trains a logistic regression model on a heuristically set of features. Enhanced Sequential Inference Model (ESIM) (Chen et al., 2016) with some small modifications has been used in (Nie et al., 2019;<cite> Hanselowski et al., 2018)</cite> . ESIM encodes premises and hypotheses using one Bidirectional Long Short-Term Memory (BiLSTM) with shared weights. The encoded sentences are later aligned by a bidirectional attention mechanism. The encoded and aligned sentences are combined, and another shared BiL-STM matches the two representations. Finally, a softmax layer classifies the max and mean pooled representations of the second BiLSTM. The UKP-Athene team <cite>(Hanselowski et al., 2018)</cite> achieved the highest sentence retrieval recall using ESIM and pairwise training.",
  "y": "background"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_2",
  "x": "The encoded sentences are later aligned by a bidirectional attention mechanism. The encoded and aligned sentences are combined, and another shared BiL-STM matches the two representations. Finally, a softmax layer classifies the max and mean pooled representations of the second BiLSTM. The UKP-Athene team <cite>(Hanselowski et al., 2018)</cite> achieved the highest sentence retrieval recall using ESIM and pairwise training. Their model takes a claim and a pair of positive and negative sentences and predicts a similarity score for each sentence. To train the model, they use a modified Hinge loss function and a random neg-ative sampling strategy. In other words, positive samples are trained against five randomly selected negative sentences from the top retrieved pages for each claim.",
  "y": "background"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_3",
  "x": "---------------------------------- **CLAIM VERIFICATION** Decomposable Attention (DA) (Parikh et al., 2016) , which compares and aggregates softaligned words in sentence pairs, is used in the FEVER benchmark paper (Thorne et al., 2018) . The Papelo team (Malon, 2018) employs transformer networks with pre-trained weights (Radford et al., 2018) . ESIM has been widely used among the FEVER challenge participants (Nie et al., 2019; Yoneda et al., 2018;<cite> Hanselowski et al., 2018)</cite> . UNC (Nie et al., 2019) , the winner of the competition, proposes a modified ESIM that takes the concatenation of the retrieved evidence sentences and claim along with ELMo embedding and three additional token-level features: Word-Net, number embedding, and semantic relatedness score from the document retrieval and sentence retrieval steps. Dream (Zhong et al., 2019) has the state of the art FEVER score.",
  "y": "background"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_4",
  "x": "**DOCUMENT RETRIEVAL** In the document retrieval step, the Wikipedia documents containing the evidence supporting or refuting the claim are retrieved. Following the UKP-Athene promising document retrieval component <cite>(Hanselowski et al., 2018)</cite> , which results in more than 93% development set document recall, we exactly use their method to collect a set of top documents D c l top for the claim c l . ---------------------------------- **SENTENCE RETRIEVAL** The sentence retrieval step extracts the top five potential evidence sentences S c l top for the claim c l . The training set consists of about 145K claims and all the sentences (S d i ) from the documents retrieved at the previous step (D c l top ) corresponding to the claim c l (S c l all = {S d i |d i \u2208 D c l top }).",
  "y": "uses"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_5",
  "x": "In the pairwise approach, a pair of positive and negative samples are compared against each other (Figure 4 (right) ). We use the Ranknet loss function (Burges et al., 2005) : where the mapping from the positive sample o pos and negative sample output o neg to probabilities are calculated using the softmax function p i = e opos\u2212oneg /(1 + e opos\u2212oneg ). Note that we do not force the positive and negative samples to be selected from the same claims because the number of sentences per claim is significantly different and this difference might result in biasing on the claims with higher number of sentences. In addition, we experiment with the modified Hinge loss functions like <cite>(Hanselowski et al., 2018)</cite> : At testing time, for both pairwise loss functions, we sort the sentences by their output value o and similarly choose S c l top for the claim c l . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_8",
  "x": "This is more clear in Figure 5 , which plots the recall-precision trade-off by applying a decision threshold on the output scores. The pointwise Model FEVER Score(%) Label Accuracy(%) DREAM (Zhong et al., 2019) 70 (Nie et al., 2019) 64.21 68.21 UCL (Yoneda et al., 2018) 62.52 67.62 UKP-Athene <cite>(Hanselowski et al., 2018)</cite> 61.58 65.46 Figure 5 : Recall and precision results on the development set. x shows the UNC, UCL, UPK-Athene, DREAM XLNet, and DREAM RoBERTa scores (Nie et al., 2019; Yoneda et al., 2018;<cite> Hanselowski et al., 2018</cite>; Zhong et al., 2019) methods surpass the pairwise methods in terms of recall-precision performance. Figure 5 also shows that HNM enhances both pairwise methods trained by the Ranknet and Hinge loss functions and preserves the pointwise performance. In Table 2 , we compare the development set results of the state of the art methods with the BERT model trained on different retrieved evidence sets. The BERT claim verification system even if it is trained on the UKP-Athene sentence retrieval component <cite>(Hanselowski et al., 2018)</cite> , the state of the art method with the highest recall, improves both label accuracy and FEVER score. Training based on the BERT sentence retrieval predic-tions significantly enhances the verification results because while it explicitly improves the FEVER score by providing more correct evidence sentences, it provides a better training set for the verification system.",
  "y": "differences"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_9",
  "x": "This is more clear in Figure 5 , which plots the recall-precision trade-off by applying a decision threshold on the output scores. The pointwise Model FEVER Score(%) Label Accuracy(%) DREAM (Zhong et al., 2019) 70 (Nie et al., 2019) 64.21 68.21 UCL (Yoneda et al., 2018) 62.52 67.62 UKP-Athene <cite>(Hanselowski et al., 2018)</cite> 61.58 65.46 Figure 5 : Recall and precision results on the development set. x shows the UNC, UCL, UPK-Athene, DREAM XLNet, and DREAM RoBERTa scores (Nie et al., 2019; Yoneda et al., 2018;<cite> Hanselowski et al., 2018</cite>; Zhong et al., 2019) methods surpass the pairwise methods in terms of recall-precision performance. Figure 5 also shows that HNM enhances both pairwise methods trained by the Ranknet and Hinge loss functions and preserves the pointwise performance. In Table 2 , we compare the development set results of the state of the art methods with the BERT model trained on different retrieved evidence sets. The BERT claim verification system even if it is trained on the UKP-Athene sentence retrieval component <cite>(Hanselowski et al., 2018)</cite> , the state of the art method with the highest recall, improves both label accuracy and FEVER score. Training based on the BERT sentence retrieval predic-tions significantly enhances the verification results because while it explicitly improves the FEVER score by providing more correct evidence sentences, it provides a better training set for the verification system.",
  "y": "differences"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_0",
  "x": "However, this treatment is not optimal because it ignores the fact that words can share similar morphemes which can be exploited to estimate the OOV word embedding better. Meanwhile, word representation models based on subword units, such as characters or word segments, have been shown to perform well in many NLP tasks such as POS tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015) , language modeling (Ling et al., 2015; Kim et al., 2016; Vania and Lopez, 2017) , machine translation (Vylomova et al., 2016; Lee et al., 2016; Sennrich et al., 2016) , dependency parsing (Ballesteros et al., 2015) , and sequence labeling<cite> (Rei et al., 2016</cite>; Lample et al., 2016) . These representations are effective because they can represent OOV words better by leveraging the orthographic similarity among words. As for Indonesian NER, the earliest work was done by Budi et al. (2005) which relied on a rulebased approach. More recent research mainly used machine learning methods such as conditional random fields (CRF) (Luthfi et al., 2014; Leonandya et al., 2015; Taufik et al., 2016) and support vector machines (Suwarningsih et al., 2014; Aryoyudanta et al., 2016) . The most commonly used datasets are news articles (Budi et al., 2005) , Wikipedia/DBPedia articles (Luthfi et al., 2014; Leonandya et al., 2015; Aryoyudanta et al., 2016) , medical texts (Suwarningsih et al., 2014) , and Twitter data (Taufik et al., 2016) . To the best of our knowledge, there has been no work that used neural networks for Indonesian NER nor NER for Indonesian conversational texts.",
  "y": "background"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_1",
  "x": "More recent research mainly used machine learning methods such as conditional random fields (CRF) (Luthfi et al., 2014; Leonandya et al., 2015; Taufik et al., 2016) and support vector machines (Suwarningsih et al., 2014; Aryoyudanta et al., 2016) . The most commonly used datasets are news articles (Budi et al., 2005) , Wikipedia/DBPedia articles (Luthfi et al., 2014; Leonandya et al., 2015; Aryoyudanta et al., 2016) , medical texts (Suwarningsih et al., 2014) , and Twitter data (Taufik et al., 2016) . To the best of our knowledge, there has been no work that used neural networks for Indonesian NER nor NER for Indonesian conversational texts. In this paper, we report the ability of a neural network-based approach for Indonesian NER in conversational data. We employed the neural sequence labeling model of<cite> (Rei et al., 2016)</cite> and experimented with two word representation models: word-level and character-level. We evaluated all models on relatively large, manually annotated Indonesian conversational texts. We aim to address the following questions: 1) How do the character models perform compared to word embedding-only models on NER in Indonesian conversational texts?",
  "y": "uses"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_2",
  "x": "For the CRF model, we used an implementation provided by Okazaki (2007) 3 . Neural architectures for sequence labeling are pretty similar. They usually employ a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with CRF as the output layer, and a CNN (Ma and Hovy, 2016) or LSTM (Lample et al., 2016;<cite> Rei et al., 2016)</cite> composes the character embeddings. Also, we do not try to achieve state-of-theart results but only are interested whether neural sequence labeling models with character embedding can handle the OOV problem well. Therefore, for the neural models, we just picked the implementation provided in<cite> (Rei et al., 2016)</cite> . 4 In their implementation, all the LSTMs have only one layer. Dropout (Srivastava et al., 2014 ) is used as the regularizer but only applied to the final word embedding as opposed to the LSTM outputs as proposed by Zaremba et al. (2015) .",
  "y": "background"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_3",
  "x": "Some features that we did not employ were POS tags, lookup list, and non-standard word list as we did not have POS tags in our data nor access to the lists Taufik et al. (2016) used. For the CRF model, we used an implementation provided by Okazaki (2007) 3 . Neural architectures for sequence labeling are pretty similar. They usually employ a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with CRF as the output layer, and a CNN (Ma and Hovy, 2016) or LSTM (Lample et al., 2016;<cite> Rei et al., 2016)</cite> composes the character embeddings. Also, we do not try to achieve state-of-theart results but only are interested whether neural sequence labeling models with character embedding can handle the OOV problem well. Therefore, for the neural models, we just picked the implementation provided in<cite> (Rei et al., 2016)</cite> . 4 In their implementation, all the LSTMs have only one layer.",
  "y": "uses"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_4",
  "x": "Neural architectures for sequence labeling are pretty similar. They usually employ a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with CRF as the output layer, and a CNN (Ma and Hovy, 2016) or LSTM (Lample et al., 2016;<cite> Rei et al., 2016)</cite> composes the character embeddings. Also, we do not try to achieve state-of-theart results but only are interested whether neural sequence labeling models with character embedding can handle the OOV problem well. Therefore, for the neural models, we just picked the implementation provided in<cite> (Rei et al., 2016)</cite> . 4 In their implementation, all the LSTMs have only one layer. Dropout (Srivastava et al., 2014 ) is used as the regularizer but only applied to the final word embedding as opposed to the LSTM outputs as proposed by Zaremba et al. (2015) . The loss function contains not only the log likelihood of the training data and the similarity score but also a language modeling loss, which is not mentioned in<cite> (Rei et al., 2016)</cite> but discussed in the subsequent work (Rei, 2017) .",
  "y": "background"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_5",
  "x": "Therefore, for the neural models, we just picked the implementation provided in<cite> (Rei et al., 2016)</cite> . 4 In their implementation, all the LSTMs have only one layer. Dropout (Srivastava et al., 2014 ) is used as the regularizer but only applied to the final word embedding as opposed to the LSTM outputs as proposed by Zaremba et al. (2015) . The loss function contains not only the log likelihood of the training data and the similarity score but also a language modeling loss, which is not mentioned in<cite> (Rei et al., 2016)</cite> but discussed in the subsequent work (Rei, 2017) . Thus, their implementation essentially does multi-task learning with sequence labeling as the primary task and language modeling as the auxiliary task. We used an almost identical setting to<cite> Rei et al. (2016)</cite> : words are lowercased, but characters are not, digits are replaced with zeros, singleton words in the training set are converted into unknown tokens, word and character embedding sizes are 300 and 50 respectively. The character embeddings were initialized randomly and learned during training.",
  "y": "similarities uses"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_6",
  "x": "Going from left to right, as the OOV rate increases, the character models performance does not seem to degrade much. Remarkably, this is true even when OOV rate is as high as 90%, even approaching 100%, whereas the word embeddingonly model already has a significant drop in performance when the OOV rate is just around 70%. This finding confirms that character embedding is useful to mitigate the OOV problem and robust against different OOV rates. We also observe that there seems no perceptible difference between the concatenation and attention model. ---------------------------------- **CONCLUSION AND FUTURE WORK** We reported an empirical evaluation of neural sequence labeling models by<cite> Rei et al. (2016)</cite> on NER in Indonesian conversational texts.",
  "y": "uses"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_0",
  "x": "---------------------------------- **INTRODUCTION** Word Sense Disambiguation (WSD) is one of the most widely investigated problems of Natural Language Processing (NLP). Previous works have shown that supervised approaches to Word Sense Disambiguation which rely on sense annotated corpora (Ng and Lee, 1996; Lee et al., 2004) outperform unsupervised (Veronis, 2004 ) and knowledge based approaches (Mihalcea, 2005) . However, creation of sense marked corpora has always remained a costly proposition, especially for some of the resource deprived languages. To circumvent this problem,<cite> Khapra et al. (2009)</cite> proposed a WSD method that can be applied to a language even when no sense tagged corpus for that language is available. This is achieved by projecting Wordnet and corpus parameters from another language to the language in question.",
  "y": "motivation"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_1",
  "x": "Next, we show that by injecting examples for most frequent hard-to-disambiguate words from the target domain one can achieve higher accuracies at optimal cost of annotation. Finally, we propose a measure for cost-benefit analysis which identifies the optimal point of balance between these three related entities, viz., cross-linking, sense annotation and accuracy of disambiguation. The remainder of this paper is organized as follows. In section 2 we present related work. In section 3 we describe the Synset based multilingual dictionary which enables parameter projection. In section 4 we discuss the work of<cite> Khapra et al. (2009)</cite> on parameter projection for multilingual WSD. Section 5 is on the economics of multilingual WSD.",
  "y": "background"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_2",
  "x": "In section 10 we present the results followed by discussion in section 11. Section 12 concludes the paper. ---------------------------------- **RELATED WORK** Knowledge based approaches to WSD such as Lesk's algorithm (Lesk, 1986 ), Walker's algorithm (Walker and Amsler, 1986) , Conceptual Density (Agirre and Rigau, 1996) and PageRank (Mihalcea, 2005) are less demanding in terms of resources but fail to deliver good results. Supervised approaches like SVM (Lee et al., 2004) and k-NN (Ng and Lee, 1996) , on the other hand, give better accuracies, but the requirement of large annotated corpora renders them unsuitable for resource scarce languages. Recent work by<cite> Khapra et al. (2009)</cite> has shown that it is possible to project the parameters learnt from the annotation work of one language to another language provided aligned Wordnets for two languages are available.",
  "y": "background"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_3",
  "x": "The third factor is the accuracy of WSD The first two factors in some sense relate to the cost of purchasing a commodity and the third factor relates to the commodity itself. The work of<cite> Khapra et al. (2009)</cite> as described above does not attempt to reach an optimal costbenefit point in this economic system. They place their bets on manual cross-linking only and settle for the accuracy achieved thereof. Specifically, they do not explore the inclusion of small amount of annotated data from the target language to boost the accuracy (as mentioned earlier, supervised systems which use annotated data from the target language are known to perform better). Further, it is conceivable that with respect to accuracy-cost trade-off, there obtains a case for balancing one cost against the other, viz., the cost of cross-linking and the cost of annotation. In some cases bilingual lexicographers (needed for manual cross-linking) may be more expensive compared to monolingual annotators. There it makes sense to place fewer bets on manual crosslinking and more on collecting annotated corpora.",
  "y": "background"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_4",
  "x": "In some cases bilingual lexicographers (needed for manual cross-linking) may be more expensive compared to monolingual annotators. There it makes sense to place fewer bets on manual crosslinking and more on collecting annotated corpora. On the other hand if manual cross-linking is cheap then a very small amount of annotated corpora can be used in conjunction with full manual crosslinking to boost the accuracy. Based on the above discussion, if k a is the cost of sense annotating one word, k c is the cost of manually cross-linking a word and A is the accuracy desired then the problem of multilingual WSD can be cast as an optimization problem: Accuracy \u2265 A where, w c and w a are the number of words to be manually cross linked and annotated respectively. Ours is thus a 3-factor economic model (crosslinking, annotation and accuracy) as opposed to the 2-factor model (cross-linking, accuracy) proposed by<cite> Khapra et al. (2009)</cite> . ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_5",
  "x": "The model proposed by <cite>Khapra et al. (2009</cite> ) is a deterministic model where the expected count for (Sense S, Marathi Word W ), i.e., the number of times the word W appears in sense S is approximated by the count for the corresponding cross linked Hindi word. Such a model assumes that each Marathi word links to appropriate Hindi word(s) as identified manually by a lexicographer. Instead, we propose a probabilistic model where a Marathi word can link to every word in the corresponding Hindi synset with some probability. The expected count for (S, W ) can then be estimated as: where, P (h i |W, S) is the probability that the word h i from the corresponding Hindi synset is the correct cross-linked word for the given Marathi word. For example, one of the senses of the Marathi word maan is {neck} i.e. \"the body part which connects the head to the rest of the body\". The corresponding Hindi synset has 10 words {gardan, gala, greeva, halak, kandhar and so on}. Thus, using Equation (2), the expected count, E[C({neck}, maan)], is calculated as:",
  "y": "background"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_6",
  "x": "Let, ---------------------------------- **EXPERIMENTAL SETUP** We used Hindi as the source language (S L ) and trained a WSD engine using Hindi sense tagged corpus. The parameters thus learnt were then projected using the MultiDict (refer section 3 and 4) to build a resource conscious Marathi (T L ) WSD engine. We used the same dataset as described in<cite> Khapra et al. (2009)</cite> for all our experiments. The data was collected from two domains, viz., Tourism and Health.",
  "y": "similarities uses"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_0",
  "x": "****ON THE CORRESPONDENCE BETWEEN COMPOSITIONAL MATRIX-SPACE MODELS OF LANGUAGE AND WEIGHTED AUTOMATA**** **ABSTRACT** Compositional matrix-space models of language were recently proposed for the task of meaning representation of complex text structures in natural language processing. These models have been shown to be a theoretically elegant way to model compositionality in natural language. However, in practical cases, appropriate methods are required to learn such models by automatically acquiring the necessary token-to-matrix assignments. In this paper, we introduce graded matrix grammars of natural language, a variant of the matrix grammars proposed by <cite>Rudolph and Giesbrecht (2010)</cite> , and show a close correspondence between this matrix-space model and weighted finite automata. We conclude that the problem of learning compositional matrix-space models can be mapped to the problem of learning weighted finite automata over the real numbers.",
  "y": "extends"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_1",
  "x": "Mitchell and Lapata (2010) propose a framework for vector-based semantic composition. They define additive or multiplicative function for the composition of two vectors and show that compositional approaches generally outperform non-compositional approaches which treat the phrase as the union of single lexical items. However, VSMs still have some limitations in the task of modeling complex conceptual text structures. For example, in the bag-of-words model, the words order and therefore the structure of the language is lost. To overcome the limitations of VSMs, <cite>Rudolph and Giesbrecht (2010)</cite> proposed Compositional Matrix-Space Models (CMSM) as a recent alternative model to work with distributional approaches. These models employ matrices instead of vectors and make use of iterated matrix multiplication as the only composition operation. They show that these models are powerful enough to subsume many known models, both quantitative (vector-space models with diverse composition operations) and qualitative ones (such as regular languages).",
  "y": "background"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_2",
  "x": "Therefore, methods for training such models should be developed e.g. by leveraging appropriate machine learning methods. In this paper, we are concerned with Graded Matrix Grammars, a variant of the Matrix Grammars of <cite>Rudolph and Giesbrecht (2010)</cite> , where instead of the \"yes or no\" decision, if a sequence is part of a language, a real-valued score is assigned. This is a popular task in NLP, used, e.g., in sentiment analysis settings (Yessenalina and Cardie, 2011) . Generally, in many tasks of NLP, we need to estimate functions which map arbitrary sequence of words (e.g. sentences) to some semantical space. Using Weighted Finite Automata (WFA), an extensive class of these functions can be defined, which assign values to these sequences (Balle and Mohri, 2012) . Herein, inspired by the definition of weighted finite automata (Sakarovitch, 2009) and their applications in NLP (Knight and May, 2009 ), we show a tight correspondence between graded matrix grammars and weighted finite automata. Hence, we argue that the problem of learning CMSMs can be mapped to the problem of learning WFA.",
  "y": "extends"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_4",
  "x": "More formally, according to <cite>Rudolph and Giesbrecht (2010)</cite> , the underlying idea can be described as follows: \"Given a mapping \u00b7 : \u03a3 \u2192 S from a set of tokens in \u03a3 into some semantical space S, the composition operation is defined by mapping sequences of meanings to meanings: : S \u2192 S. So, the meaning of the sequence of tokens \u03c3 1 \u00b7 \u00b7 \u00b7 \u03c3 n can be obtained by first applying the function \u00b7 to each token and then to the sequence \u03c3 1 \u00b7 \u00b7 \u00b7 \u03c3 n , as shown in Figure 2 \". Figure 2: Principle of compositionality, illustration taken from <cite>Rudolph and Giesbrecht (2010)</cite> In compositional matrix-space models, this general idea is instantiated as follows: we have S = R n\u00d7n , i.e., the semantical space consists of quadratic matrices of real numbers. The mapping function \u00b7 maps the tokens into matrices so that the semantics of simple tokens is expressed by matrices. Then, using the standard matrix multiplication as the only composition operation , the semantics of complex phrases are also described by matrices. <cite>Rudolph and Giesbrecht (2010)</cite> showed theoretically that by employing matrices instead of vectors, CMSMs subsume a wide range of linguistic models such as statistical models (vector-space models and word space models). ---------------------------------- **GRADED MATRIX GRAMMARS AND WEIGHTED FINITE AUTOMATA**",
  "y": "background"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_5",
  "x": "Figure 1 illustrates an example of WFA over \u03a3 = {a, b}. Inside each state there is a tuple of the name, initial and final weight of the state, respectively. As an example, for x = ab we have: ---------------------------------- **COMPOSITIONALITY AND COMPOSITIONAL MATRIX-SPACE MODEL** The general principle of compositionality is that the meaning of a complex expression is a function of the meaning of its constituent tokens and some rules used to combine them (Frege, 1884) . More formally, according to <cite>Rudolph and Giesbrecht (2010)</cite> , the underlying idea can be described as follows: \"Given a mapping \u00b7 : \u03a3 \u2192 S from a set of tokens in \u03a3 into some semantical space S, the composition operation is defined by mapping sequences of meanings to meanings: : S \u2192 S. So, the meaning of the sequence of tokens \u03c3 1 \u00b7 \u00b7 \u00b7 \u03c3 n can be obtained by first applying the function \u00b7 to each token and then to the sequence \u03c3 1 \u00b7 \u00b7 \u00b7 \u03c3 n , as shown in Figure 2 \". Figure 2: Principle of compositionality, illustration taken from <cite>Rudolph and Giesbrecht (2010)</cite> In compositional matrix-space models, this general idea is instantiated as follows: we have S = R n\u00d7n , i.e., the semantical space consists of quadratic matrices of real numbers.",
  "y": "uses"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_6",
  "x": "**COMPOSITIONALITY AND COMPOSITIONAL MATRIX-SPACE MODEL** The general principle of compositionality is that the meaning of a complex expression is a function of the meaning of its constituent tokens and some rules used to combine them (Frege, 1884) . More formally, according to <cite>Rudolph and Giesbrecht (2010)</cite> , the underlying idea can be described as follows: \"Given a mapping \u00b7 : \u03a3 \u2192 S from a set of tokens in \u03a3 into some semantical space S, the composition operation is defined by mapping sequences of meanings to meanings: : S \u2192 S. So, the meaning of the sequence of tokens \u03c3 1 \u00b7 \u00b7 \u00b7 \u03c3 n can be obtained by first applying the function \u00b7 to each token and then to the sequence \u03c3 1 \u00b7 \u00b7 \u00b7 \u03c3 n , as shown in Figure 2 \". Figure 2: Principle of compositionality, illustration taken from <cite>Rudolph and Giesbrecht (2010)</cite> In compositional matrix-space models, this general idea is instantiated as follows: we have S = R n\u00d7n , i.e., the semantical space consists of quadratic matrices of real numbers. The mapping function \u00b7 maps the tokens into matrices so that the semantics of simple tokens is expressed by matrices. Then, using the standard matrix multiplication as the only composition operation , the semantics of complex phrases are also described by matrices. <cite>Rudolph and Giesbrecht (2010)</cite> showed theoretically that by employing matrices instead of vectors, CMSMs subsume a wide range of linguistic models such as statistical models (vector-space models and word space models).",
  "y": "background"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_7",
  "x": "<cite>Rudolph and Giesbrecht (2010)</cite> showed theoretically that by employing matrices instead of vectors, CMSMs subsume a wide range of linguistic models such as statistical models (vector-space models and word space models). ---------------------------------- **GRADED MATRIX GRAMMARS AND WEIGHTED FINITE AUTOMATA** In some applications of NLP, we need to derive the meaning of a sequence of words in a language, which can be done with CMSMs as described in Section 2.2. In this section, we introduce the notion of a graded matrix grammar which constitutes a slight variation of matrix grammars as introduced by <cite>Rudolph and Giesbrecht (2010)</cite> . Definition 1 (Graded Matrix Grammars). Let \u03a3 be an alphabet.",
  "y": "extends"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_8",
  "x": "They proposed a learning-based approach for phraselevel sentiment analysis. Inspired by the work of <cite>Rudolph and Giesbrecht (2010)</cite> they use CMSMs to model composition, and present an algorithm for learning a matrix for each word via ordered logistic regression, which is evaluated with promising results. However, it is not trivial to learn a matrix-space model. Since the final optimization problem is non-convex, the matrix initialization for this method is not done perfectly. Socher et al. (2012) introduce a matrix-vector recursive neural network (MV-RNN) model that learns compositional vector representations for phrases and sentences. The model assigns a vector and a matrix to every node in a parse tree. The vector represents the meaning of the constituent, while the matrix captures how it affects the meaning of neighboring constituent.",
  "y": "background"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_0",
  "x": "Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for pro (dropped pronouns or zero pronoun) has begun to receive attention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the problem as classifying each pair of the location of empty category and its head word in the dependency structure. Wang et al. (2015) proposed a joint embedding of empty categories and their contexts on dependency structure. <cite>Xiang et al. (2013)</cite> formalized the problem as classifying each IP node (roughly corresponds to S and SBAR in Penn Treebank) in the phrase structure. In this paper, we propose a novel method for empty category detection for Japanese that uses conjunction features on phrase structure and word embeddings. We use the Keyaki Treebank (Butler et al., 2012) , which is a recent development.",
  "y": "background"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_1",
  "x": "We use the Keyaki Treebank (Butler et al., 2012) , which is a recent development. As it has annotations for pro and trace, we show our method has substantial improvements over the state-of-the-art machine learning-based method<cite> (Xiang et al., 2013)</cite> for Chinese empty category detection as well as linguistically-motivated manually written rule-based method similar to (Campbell, 2004 ). ---------------------------------- **BASELINE SYSTEMS** The Keyaki Treebank annotates the phrase structure with functional information for Japanese sentences following a scheme adapted from the Annotation manual for the Penn Historical Corpora and Figure 1: An annotation example of (*pro* brought back a daughter who ran away from home.) in Keyaki Treebank. (The left tree is the original tree and the right tree is a converted tree based on Xiang et al.'s (2013) formalism) the PCEEC (Santorini, 2010) . There are some major changes: the VP level of structure is typically absent, function is marked on all clausal nodes (such as IP-REL and CP-THT) and all NPs that are clause level constituents (such as NP-SBJ).",
  "y": "differences"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_2",
  "x": "Disambiguation tags are also used for clarifying the functions of its immediately preceding node, such as NP-OBJ * *(wo) for PP, however, we removed them in our experiment. Keyaki Treebank has annotation for trace markers of relative clauses (*T*) and dropped pronouns (*pro*), however, it deliberately has no annotation for control dependencies (PRO) (Butler et al., 2015) . It has also fine grained empty categories of *pro* such as *speaker* and *hearer*, but we unified them into *pro* in our experiment. HARUNIWA (Fang et al., 2014 ) is a Japanese phrase structure parser trained on the treebank. It has a rule-based post-processor for adding empty categories, which is similar to (Campbell, 2004) . We call it RULE in later sections and use it as one of two baselines. We also use<cite> Xiang et al's (2013)</cite> model as another baseline.",
  "y": "uses"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_3",
  "x": "Let T = t 1 t 2 \u00b7 \u00b7 \u00b7 t n be the sequence of nodes produced by the post-order traversal from root node, and e i be the empty category tag associated with t i . The probability model of<cite> (Xiang et al., 2013)</cite> is formulated as MaxEnt model: where \u03c6 is a feature vector, \u03b8 is a weight vector to \u03c6 and Z is normalization factor: where E represents the set of all empty category types to be detected. <cite>Xiang et al. (2013)</cite> grouped their features into four types: tree label features, lexical features, empty category features and conjunction features as shown in Table 1 . As the features for<cite> (Xiang et al., 2013)</cite> were developed for Chinese Penn Treebank, we modify their features for Keyaki Treebank: First, the traversal order is changed from post-order (bottom-up) to pre-order (top-down). As PROs are implicit in Keyaki Treebank, the decisions on IPs in lower levels depend on those on higher levels in the tree.",
  "y": "background"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_4",
  "x": "where \u03c6 is a feature vector, \u03b8 is a weight vector to \u03c6 and Z is normalization factor: where E represents the set of all empty category types to be detected. <cite>Xiang et al. (2013)</cite> grouped their features into four types: tree label features, lexical features, empty category features and conjunction features as shown in Table 1 . As the features for<cite> (Xiang et al., 2013)</cite> were developed for Chinese Penn Treebank, we modify their features for Keyaki Treebank: First, the traversal order is changed from post-order (bottom-up) to pre-order (top-down). As PROs are implicit in Keyaki Treebank, the decisions on IPs in lower levels depend on those on higher levels in the tree. Second, empty category features are extracted from ancestor IP nodes, not from descendant IP nodes, in accordance with the first change. Table 2 shows the accuracies of Japanese empty category detection, using the original and our modification of the<cite> (Xiang et al., 2013)</cite> with ablation test.",
  "y": "background"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_5",
  "x": "As we can uniquely decode them from the extended IP labels, the problem is to predict the labels for the input tree that has no empty nodes. Let T = t 1 t 2 \u00b7 \u00b7 \u00b7 t n be the sequence of nodes produced by the post-order traversal from root node, and e i be the empty category tag associated with t i . The probability model of<cite> (Xiang et al., 2013)</cite> is formulated as MaxEnt model: where \u03c6 is a feature vector, \u03b8 is a weight vector to \u03c6 and Z is normalization factor: where E represents the set of all empty category types to be detected. <cite>Xiang et al. (2013)</cite> grouped their features into four types: tree label features, lexical features, empty category features and conjunction features as shown in Table 1 . As the features for<cite> (Xiang et al., 2013)</cite> were developed for Chinese Penn Treebank, we modify their features for Keyaki Treebank: First, the traversal order is changed from post-order (bottom-up) to pre-order (top-down).",
  "y": "extends"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_6",
  "x": "Second, empty category features are extracted from ancestor IP nodes, not from descendant IP nodes, in accordance with the first change. Table 2 shows the accuracies of Japanese empty category detection, using the original and our modification of the<cite> (Xiang et al., 2013)</cite> with ablation test. We find that the conjunction features left-sibling label or POS tag (up to two siblings) 10 right-sibling label or POS tag (up to two siblings) Lexical features 11 left-most word under the current node 12 right-most word under the current node 13 word immediately left to the span of the current node 14 word immediately right to the span of the current node 15 head word of the current node 16 head word of the parent node 17 is the current node head child of its parent? (binary) Empty category features 18 predicted empty categories of the left sibling 19* the set of detected empty categories of ancestor nodes Conjunction features 20 current node label with parent node label 21* current node label with features computed from ancestor nodes 22 current node label with features computed from leftsibling nodes 23 current node label with lexical features<cite> (Xiang et al., 2013)</cite> 68.2 \u22120.40 modified<cite> (Xiang et al., 2013)</cite> 68.6 -\u2212 Tree label 68.6 \u22120.00 \u2212 Empty category 68.3 \u22120.30 \u2212 Lexicon 68.6 \u22120.00 \u2212 Conjunction 58.5 \u221210.1 Table 2 : Ablation result of<cite> (Xiang et al., 2013)</cite> are highly effective compared to the three other features. This observation leads to the model proposed in the next section. ---------------------------------- **PROPOSED MODEL**",
  "y": "uses"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_8",
  "x": "By using the development set, we set the dimension of word embedding and the window size for co-occurrence counts as 200 and 10, respectively. ---------------------------------- **RESULT AND DISCUSSION** We tested in two conditions: gold parse and system parse. In gold parse condition, we used the trees of Keyaki Treebank without empty categories as input to the systems. In system parse condition, we used the output of the Berkeley Parser model of HARUNIWA before rule-based empty category detection 1 . We evaluated them using the word-position-level identification metrics described in<cite> (Xiang et al., 2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_9",
  "x": "In the gold parse condition, the two baselines, the rule-based method (RULE) and the modified<cite> (Xiang et al., 2013)</cite> method, achieved the F-measure of 62.6% and 68.6% respectively. We also implemented the third baseline based on (Johnson, 2002) . Minimal unlexicalized tree fragments from empty node to its antecedent were extracted as pattern rules based on corpus statistics. For *pro*, which has no antecedent, we used the statistics from empty node to the root. Although the precision of the method is high, the recall is very low, which results in the F-measure of 38.1%. Among the proposed models, the combination of path feature and child feature (PATH \u00d7 CHILD) even outperformed the baselines. It reached 73.2% with all features.",
  "y": "differences"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_10",
  "x": "DIST is a vector of co-occurrence counts for each case particle, which can be thought of an unsmoothed version of our DCF. Table 4 shows the accuracies of various empty category detection methods, for both gold parse and system parse. In the gold parse condition, the two baselines, the rule-based method (RULE) and the modified<cite> (Xiang et al., 2013)</cite> method, achieved the F-measure of 62.6% and 68.6% respectively. We also implemented the third baseline based on (Johnson, 2002) . Minimal unlexicalized tree fragments from empty node to its antecedent were extracted as pattern rules based on corpus statistics. For *pro*, which has no antecedent, we used the statistics from empty node to the root. Although the precision of the method is high, the recall is very low, which results in the F-measure of 38.1%.",
  "y": "uses"
 },
 {
  "id": "27aeffca1f7a9a6b40743284a2871d_0",
  "x": "After explaining this connection, I go on to describe extensions which identify topical multiword collocations and automatically learn the internal structure of namedentity phrases. The adaptor grammar framework is a nonparametric extension of probabilistic context-free grammars (Johnson et al., 2007) , which was initially intended to allow fast prototyping of models of unsupervised language acquisition (Johnson, 2008), but it has been shown to have applications in text data mining and information retrieval as well <cite>Hardisty et al., 2010</cite>) . We'll see how learning the referents of words and learning the roles of social cues in language acquisition (Johnson et al., 2012) can be viewed as a kind of topic modelling problem that can be reduced to a grammatical inference problem using the techniques described in this talk. ---------------------------------- **ABSTRACT** Context-free grammars have been a cornerstone of theoretical computer science and computational linguistics since their inception over half a century ago. Topic models are a newer development in machine learning that play an important role in document analysis and information retrieval.",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_0",
  "x": "Consider the second paragraph shown in Figure 1 . Only detecting the argument components (a claim in sentence 2 and two premises in sentences 3 and 4) does not give a complete picture of the argumentation. By looking for relations between these components, one can also see that the two premises together justify the claim. The argumentation structure of the text in Figure 1 is illustrated in Figure 2 . Our current study proposes a novel approach for argumentative relation mining that makes use of contextual features extracted from surrounding sentences of source and target components as well as from topic information of the writings. Prior argumentative relation mining studies have often used features extracted from argument components to model different aspects of the relations between the components, e.g., relative distance, word pairs, semantic similarity, textual entailment (Cabrio and Villata, 2012;<cite> Stab and Gurevych, 2014b</cite>; Boltu\u017ei\u0107 and\u0160najder, 2014; Peldszus and Stede, 2015b) . Features extracted from the text surrounding the components have been less explored, e.g., using words and their part-of-speech from adjacent sentences (Peldszus, 2014) .",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_1",
  "x": "Experimental results show that our proposed contextual features help significantly improve performance in two argumentative relation classification tasks. ---------------------------------- **RELATED WORK** Unlike argument component identification where textual inputs are typically sentences or clauses (Moens et al., 2007;<cite> Stab and Gurevych, 2014b</cite>; Levy et al., 2014; Lippi and Torroni, 2015) , textual inputs of argumentative relation mining vary from clauses (Stab and Gurevych, 2014b; Peldszus, 2014 ) to multiple-sentences (Biran and Rambow, 2011; Cabrio and Villata, 2012; Boltu\u017ei\u0107 an\u010f Snajder, 2014) . Studying claim justification between user comments, Biran and Rambow (2011) proposed that the argumentation in justification of a claim can be characterized with discourse structure in the justification. They however only considered discourse markers but not discourse relations. Cabrio et al. (2013) conducted a corpus analysis and found certain similarity between Penn Discourse TreeBank relations (Prasad et al., 2008) and argumentation schemes (Walton et al., 2008) .",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_2",
  "x": "Qazvinian and Radev (2010) used the term \"context sentence\" to refer to sentences surrounding a citation that contained information about the cited source but did not explicitly cite it. In our study, we only require that the context sentences of an argument component must be in the same paragraph and adjacent to the component. Prior work in argumentative relation mining has used argument component labels to provide constraints during relation identification. For example, when an annotation scheme (e.g., (Peldszus and Stede, 2013; Stab and Gurevych, 2014a) ) does not allow relations from claim to premise, no relations are inferred during relation mining for any argument component pair where the source is a claim and the target is a premise. In our work, we follow <cite>Stab and Gurevych (2014b)</cite> and use the predicted labels of argument components as features during argumentative relation mining. We, however, take advantage of an enhanced argument component model (Nguyen and Litman, 2016 ) to obtain more reliable argument component labels than in<cite> (Stab and Gurevych, 2014b)</cite> . Argument mining research has studied different data-driven approaches for separating organizational content (shell) from topical content to improve argument component identification, e.g., supervised sequence model (Madnani et al., 2012) , unsupervised probabilistic topic models (S\u00e9aghdha and Teufel, 2014; Du et al., 2014) .",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_3",
  "x": "We, however, take advantage of an enhanced argument component model (Nguyen and Litman, 2016 ) to obtain more reliable argument component labels than in<cite> (Stab and Gurevych, 2014b)</cite> . Argument mining research has studied different data-driven approaches for separating organizational content (shell) from topical content to improve argument component identification, e.g., supervised sequence model (Madnani et al., 2012) , unsupervised probabilistic topic models (S\u00e9aghdha and Teufel, 2014; Du et al., 2014) . Nguyen and Litman (2015) post-processed LDA (Blei et al., 2003) output to extract a lexicon of argument and domain words from development data. Their semi-supervised approach exploits the topic context through essay titles to guide the extraction. Finally, prior research has explored predicting different argumentative relationship labels between pairs of argument components, e.g., attachment (Peldszus and Stede, 2015a) , support vs. non-support (Biran and Rambow, 2011; Cabrio and Villata, 2012;<cite> Stab and Gurevych, 2014b)</cite> , {implicit, explicit}\u00d7{support, attack} (Boltu\u017ei\u0107 and\u0160najder, 2014) , verifiability of support (Park and Cardie, 2014) . Our experiments use two such argumentative relation classification tasks (Support vs. Non-support, Support vs. Attack) to evaluate the effectiveness of our proposed features. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_4",
  "x": "In our work, we follow <cite>Stab and Gurevych (2014b)</cite> and use the predicted labels of argument components as features during argumentative relation mining. We, however, take advantage of an enhanced argument component model (Nguyen and Litman, 2016 ) to obtain more reliable argument component labels than in<cite> (Stab and Gurevych, 2014b)</cite> . Argument mining research has studied different data-driven approaches for separating organizational content (shell) from topical content to improve argument component identification, e.g., supervised sequence model (Madnani et al., 2012) , unsupervised probabilistic topic models (S\u00e9aghdha and Teufel, 2014; Du et al., 2014) . Nguyen and Litman (2015) post-processed LDA (Blei et al., 2003) output to extract a lexicon of argument and domain words from development data. Their semi-supervised approach exploits the topic context through essay titles to guide the extraction. Finally, prior research has explored predicting different argumentative relationship labels between pairs of argument components, e.g., attachment (Peldszus and Stede, 2015a) , support vs. non-support (Biran and Rambow, 2011; Cabrio and Villata, 2012;<cite> Stab and Gurevych, 2014b)</cite> , {implicit, explicit}\u00d7{support, attack} (Boltu\u017ei\u0107 and\u0160najder, 2014) , verifiability of support (Park and Cardie, 2014) . Our experiments use two such argumentative relation classification tasks (Support vs. Non-support, Support vs. Attack) to evaluate the effectiveness of our proposed features.",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_5",
  "x": "Finally, prior research has explored predicting different argumentative relationship labels between pairs of argument components, e.g., attachment (Peldszus and Stede, 2015a) , support vs. non-support (Biran and Rambow, 2011; Cabrio and Villata, 2012;<cite> Stab and Gurevych, 2014b)</cite> , {implicit, explicit}\u00d7{support, attack} (Boltu\u017ei\u0107 and\u0160najder, 2014) , verifiability of support (Park and Cardie, 2014) . Our experiments use two such argumentative relation classification tasks (Support vs. Non-support, Support vs. Attack) to evaluate the effectiveness of our proposed features. ---------------------------------- **PERSUASIVE ESSAY CORPUS** Stab and Gurevych (2014a) compiled the Persuasive Essay Corpus consisting of 90 student argumentative essays and made it publicly available. 3 Because the corpus has been utilized for different argument mining tasks (Stab and Gurevych, 2014b; Nguyen and Litman, 2015; Nguyen and Litman, 2016) , we use this corpus to demonstrate our context-aware argumentative relation mining approach, and adapt the model developed by <cite>Stab and Gurevych (2014b)</cite> to serve as the baseline for evaluating our proposed approach. Three experts identified possible argument components of three types within each sentence in the corpus (MajorClaim -writer's stance toward the writing topic, Claim -controversial statements that support or attack MajorClaim, and Premiseevidence used to underpin the validity of Claim), and also connected the argument components using two argumentative relations (Support and Attack).",
  "y": "similarities"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_6",
  "x": "---------------------------------- **PERSUASIVE ESSAY CORPUS** Stab and Gurevych (2014a) compiled the Persuasive Essay Corpus consisting of 90 student argumentative essays and made it publicly available. 3 Because the corpus has been utilized for different argument mining tasks (Stab and Gurevych, 2014b; Nguyen and Litman, 2015; Nguyen and Litman, 2016) , we use this corpus to demonstrate our context-aware argumentative relation mining approach, and adapt the model developed by <cite>Stab and Gurevych (2014b)</cite> to serve as the baseline for evaluating our proposed approach. Three experts identified possible argument components of three types within each sentence in the corpus (MajorClaim -writer's stance toward the writing topic, Claim -controversial statements that support or attack MajorClaim, and Premiseevidence used to underpin the validity of Claim), and also connected the argument components using two argumentative relations (Support and Attack). According to the annotation manual, each essay has exactly one MajorClaim. A sentence can have one or more argument components (Argumentative sentences).",
  "y": "extends"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_7",
  "x": "The three experts achieved inter-rater accuracy 0.88 for component labels and Krippendorff's \u03b1 U 0.72 for component boundaries. Given the annotated argument components, the three experts obtained Krippendorff's \u03b1 0.81 for relation labels. The number of relations are shown in Table 1. 4 Argumentative Relation Tasks 4.1 Task 1: Support vs. Non-support Our first task follows<cite> (Stab and Gurevych, 2014b)</cite> : given a pair of source and target argument components, identify whether the source argumentatively supports the target or not. Note that when a support relation does not hold, the source may attack or has no relation with the target compo- <cite>Stab and Gurevych (2014b)</cite> split the corpus into an 80% training set and a 20% test set which have similar label distributions. We use this split to train and test our proposed models, and directly compare our models' performance to the reported performance in<cite> (Stab and Gurevych, 2014b)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_8",
  "x": "Given the annotated argument components, the three experts obtained Krippendorff's \u03b1 0.81 for relation labels. The number of relations are shown in Table 1. 4 Argumentative Relation Tasks 4.1 Task 1: Support vs. Non-support Our first task follows<cite> (Stab and Gurevych, 2014b)</cite> : given a pair of source and target argument components, identify whether the source argumentatively supports the target or not. Note that when a support relation does not hold, the source may attack or has no relation with the target compo- <cite>Stab and Gurevych (2014b)</cite> split the corpus into an 80% training set and a 20% test set which have similar label distributions. We use this split to train and test our proposed models, and directly compare our models' performance to the reported performance in<cite> (Stab and Gurevych, 2014b)</cite> . ---------------------------------- **TASK 2: SUPPORT VS. ATTACK**",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_9",
  "x": "To further evaluate the effectiveness of our approach, we conduct an additional task that classifies an argumentative relation as Support or Attack. For this task, we assume that the relation (i.e., attachment (Peldszus, 2014) ) between two components is given, and aim at identifying the argumentative function of the relation. Because we remove the paragraph constraint in this task, we obtain more Support relations than in Task 1. As shown in Table 1 , of the total 1473 relations, we have 1312 (89%) Support and 161 (11%) Attack relations. Because this task was not studied in<cite> (Stab and Gurevych, 2014b)</cite> , we adapt Stab and Gurevych's model to use as the baseline. ---------------------------------- **ARGUMENTATIVE RELATION MODELS**",
  "y": "extends"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_10",
  "x": "**ARGUMENTATIVE RELATION MODELS** ---------------------------------- **BASELINE** We adapt (Stab and Gurevych, 2014b ) to use as a baseline for evaluating our approach. Given a pair of argument components, we follow<cite> (Stab and Gurevych, 2014b)</cite> by first extracting 3 feature sets: structural (e.g., word counts, sentence position), lexical (e.g., word pairs, first words), and grammatical production rules (e.g., S\u2192NP,VP). Because a sentence may have more than one argument component, the relative component positions might provide useful information (Peldszus, 2014) . Thus, we also include 8 new component position features: whether the source and target components are the whole sentences or the beginning/end components of the sentences; if the source is before or after the target component; and the absolute difference of their positions.",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_11",
  "x": "<cite>Stab and Gurevych (2014b)</cite> used a 55-discourse marker set to extract indicator features. We expand their discourse maker set by combining them with a 298-discourse marker set developed in (Biran and Rambow, 2011). We expect the expanded set of discourse markers will represent better possible discourse relations in the texts. <cite>Stab and Gurevych (2014b)</cite> used predicted label of argument components as features for both training and testing their argumentation structure identification model. 5 As their predicted labels are not available to us, we adapt this feature set by using the argument component model in (Nguyen and Litman, 2016) which was shown to outperform the corresponding model of Stab and Gurevych. For later presentation purposes, we name the set of all features from this section except word pairs and production rules as the common features. While word pairs and grammatical production rules were the most predictive features in<cite> (Stab and Gurevych, 2014b)</cite> , we hypothesize that this large and sparse feature space may have negative impact on model robustness (Nguyen and Litman, 2015) .",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_12",
  "x": "Given a pair of argument components, we follow<cite> (Stab and Gurevych, 2014b)</cite> by first extracting 3 feature sets: structural (e.g., word counts, sentence position), lexical (e.g., word pairs, first words), and grammatical production rules (e.g., S\u2192NP,VP). Because a sentence may have more than one argument component, the relative component positions might provide useful information (Peldszus, 2014) . Thus, we also include 8 new component position features: whether the source and target components are the whole sentences or the beginning/end components of the sentences; if the source is before or after the target component; and the absolute difference of their positions. <cite>Stab and Gurevych (2014b)</cite> used a 55-discourse marker set to extract indicator features. We expand their discourse maker set by combining them with a 298-discourse marker set developed in (Biran and Rambow, 2011). We expect the expanded set of discourse markers will represent better possible discourse relations in the texts. <cite>Stab and Gurevych (2014b)</cite> used predicted label of argument components as features for both training and testing their argumentation structure identification model.",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_13",
  "x": "Thus, we also include 8 new component position features: whether the source and target components are the whole sentences or the beginning/end components of the sentences; if the source is before or after the target component; and the absolute difference of their positions. <cite>Stab and Gurevych (2014b)</cite> used a 55-discourse marker set to extract indicator features. We expand their discourse maker set by combining them with a 298-discourse marker set developed in (Biran and Rambow, 2011). We expect the expanded set of discourse markers will represent better possible discourse relations in the texts. <cite>Stab and Gurevych (2014b)</cite> used predicted label of argument components as features for both training and testing their argumentation structure identification model. 5 As their predicted labels are not available to us, we adapt this feature set by using the argument component model in (Nguyen and Litman, 2016) which was shown to outperform the corresponding model of Stab and Gurevych. For later presentation purposes, we name the set of all features from this section except word pairs and production rules as the common features.",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_14",
  "x": "We expect the expanded set of discourse markers will represent better possible discourse relations in the texts. <cite>Stab and Gurevych (2014b)</cite> used predicted label of argument components as features for both training and testing their argumentation structure identification model. 5 As their predicted labels are not available to us, we adapt this feature set by using the argument component model in (Nguyen and Litman, 2016) which was shown to outperform the corresponding model of Stab and Gurevych. For later presentation purposes, we name the set of all features from this section except word pairs and production rules as the common features. While word pairs and grammatical production rules were the most predictive features in<cite> (Stab and Gurevych, 2014b)</cite> , we hypothesize that this large and sparse feature space may have negative impact on model robustness (Nguyen and Litman, 2015) . Most of our proposed models replace word pairs and production rules with different combinations of new contextual features. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_16",
  "x": "Each PDTB relation has sense label defined in a 3-layered (class, type, subtype), e.g., CONTINGENCY.Cause.result. While there are only four semantic class labels at the class-level which may not cover well different aspects of argumentative relation, subtype-level output is not available given the discourse parser we use. Thus, we use relations at type-level as features. For RST-DTB relations, we use only relation labels, but ignore the nucleus and satellite labels of components as they do not provide more information given the component order in the pair. Because temporal relations were shown not helpful for argument mining tasks (Biran and Rambow, 2011;<cite> Stab and Gurevych, 2014b)</cite> , we exclude them here. Discourse marker: while the baseline model only considers discourse markers within the argument components, we define a boolean feature for each discourse marker classifying whether the marker is present before the covering sentence of the source and target components or not. This implementation aims to characterize the discourse of the preceding and following text segments of each argument component separately.",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_17",
  "x": "We use the training set as determined in<cite> (Stab and Gurevych, 2014b)</cite> to train/test 9 the models using LibLINEAR algorithm (Fan et al., 2008) without parameter or feature optimization. Cross-validations are conducted using Weka (Hall et al., 2009) . We use Stanford parser (Klein and Manning, 2003) to perform text processing. As shown in Figure 4 , while increasing the window-size from 2 to 3 improves performance (significantly), using window-sizes greater than 3 does not gain further improvement. We hypothesize that after a certain limit, larger context windows will produce more noise than helpful information for the prediction. Therefore, we set the window-size to 3 in all of our experiments involving Window-context model (all with a separate test set). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_18",
  "x": "We hypothesize that after a certain limit, larger context windows will produce more noise than helpful information for the prediction. Therefore, we set the window-size to 3 in all of our experiments involving Window-context model (all with a separate test set). ---------------------------------- **PERFORMANCE ON TEST SET** We train all models using the training set and report their performances on the test set in Table 2 . We also compare our baseline to the reported performance (REPORT) for Support vs. Non-support classification in<cite> (Stab and Gurevych, 2014b)</cite> . The learning algorithm with parameters are kept the same as in the window-size tuning experiment.",
  "y": "uses"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_0",
  "x": "Recent advances in computer vision and natural language processing have led to an upsurge of research on tasks involving both vision and language. State of the art visual detectors have made it possible to hypothesise what is in an image (Guillaumin et al., 2009; Felzenszwalb et al., 2010) , paving the way for automatic image description systems. The aim of such systems is to extract and reason about visual aspects of images to generate a humanlike description. An example of the type of image and gold-standard descriptions available can be seen in Figure 1 . Recent approaches to this task have been based on slot-filling (Yang et al., 2011;<cite> Elliott and Keller, 2013)</cite> , combining web-scale ngrams , syntactic tree substitution (Mitchell et al., 2012) , and description-by-retrieval (Farhadi et al., 2010; Ordonez et al., 2011; Hodosh et al., 2013) . Image description has been compared to translating an image into text or summarising an image 1. An older woman with a small dog in the snow.",
  "y": "background"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_1",
  "x": "Spearman's \u03c1 is a non-parametric correlation coefficient that restricts the ability of outlier data points to skew the co-efficient value. The automatic measures are calculated on the sentence level and correlated against human judgements of semantic correctness. ---------------------------------- **DATA** We perform the correlation analysis on the Flickr8K data set of Hodosh et al. (2013) , and the data set of <cite>Elliott and Keller (2013)</cite> . The test data of the Flickr8K data set contains 1,000 images paired with five reference descriptions. The images were retrieved from Flickr, the reference descriptions were collected from Mechanical Turk, and the human judgements were collected from expert annotators as follows: each image in the test data was paired with the highest scoring sentence(s) retrieved from all possible test sentences by the TRI5SEM model in Hodosh et al. (2013) .",
  "y": "uses"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_2",
  "x": "We perform the correlation analysis on the Flickr8K data set of Hodosh et al. (2013) , and the data set of <cite>Elliott and Keller (2013)</cite> . The test data of the Flickr8K data set contains 1,000 images paired with five reference descriptions. The images were retrieved from Flickr, the reference descriptions were collected from Mechanical Turk, and the human judgements were collected from expert annotators as follows: each image in the test data was paired with the highest scoring sentence(s) retrieved from all possible test sentences by the TRI5SEM model in Hodosh et al. (2013) . Each image-description pairing in the test data was judged for semantic correctness by three expert human judges on a scale of 1-4. We calculate automatic measures for each image-retrieved sentence pair against the five reference descriptions for the original image. The test data of <cite>Elliott and Keller (2013)</cite> contains 101 images paired with three reference descriptions. The images were taken from the PAS-CAL VOC Action Recognition Task, the reference descriptions were collected from Mechanical Turk, and the judgements were also collected from Mechanical Turk.",
  "y": "background"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_3",
  "x": "The test data of <cite>Elliott and Keller (2013)</cite> contains 101 images paired with three reference descriptions. The images were taken from the PAS-CAL VOC Action Recognition Task, the reference descriptions were collected from Mechanical Turk, and the judgements were also collected from Mechanical Turk. <cite>Elliott and Keller (2013)</cite> generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1-5 for each imagedescription pair, resulting in a total of 2,042 human judgement-description pairings. In this analysis, we use only the first sentence of the description, which describes the event depicted in the image. ---------------------------------- **AUTOMATIC EVALUATION MEASURES** BLEU measures the effective overlap between a reference sentence X and a candidate sentence Y .",
  "y": "background"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_4",
  "x": "The test data of the Flickr8K data set contains 1,000 images paired with five reference descriptions. The images were retrieved from Flickr, the reference descriptions were collected from Mechanical Turk, and the human judgements were collected from expert annotators as follows: each image in the test data was paired with the highest scoring sentence(s) retrieved from all possible test sentences by the TRI5SEM model in Hodosh et al. (2013) . Each image-description pairing in the test data was judged for semantic correctness by three expert human judges on a scale of 1-4. We calculate automatic measures for each image-retrieved sentence pair against the five reference descriptions for the original image. The test data of <cite>Elliott and Keller (2013)</cite> contains 101 images paired with three reference descriptions. The images were taken from the PAS-CAL VOC Action Recognition Task, the reference descriptions were collected from Mechanical Turk, and the judgements were also collected from Mechanical Turk. <cite>Elliott and Keller (2013)</cite> generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1-5 for each imagedescription pair, resulting in a total of 2,042 human judgement-description pairings.",
  "y": "uses"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_5",
  "x": "BLEU measures the effective overlap between a reference sentence X and a candidate sentence Y . It is defined as the geometric mean of the effective n-gram precision scores, multiplied by the brevity penalty factor BP to penalise short translations. p n measures the effective overlap by calculating the proportion of the maximum number of n-grams co-occurring between a candidate and a reference and the total number of n-grams in the candidate text. More formally, Unigram BLEU without a brevity penalty has been reported by ), Ordonez et al. (2011 , and Kuznetsova et al. (2012) ; to the best of our knowledge, the only image description work to use higher-order n-grams with BLEU is <cite>Elliott and Keller (2013)</cite> . In this paper we use the smoothed BLEU implementation of Clark et al. (2011) to perform a sentence-level analysis, setting n = 1 and no brevity penalty to get the unigram BLEU measure, or n = 4 with the brevity penalty to get the Smoothed BLEU measure. We note that a higher BLEU score is better.",
  "y": "background"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_6",
  "x": "TER is only weakly correlated with human judgements but could prove useful in comparing the types of differences between models. An analysis of the distribution of TER scores in Figure 2 (a) shows that differences in candidate and reference length are prevalent in the image description task. Unigram BLEU is also only weakly correlated against human judgements, even though it has been reported extensively for this task. Finally, Meteor is most strongly correlated measure against human judgements. A similar pattern is observed in the <cite>Elliott and Keller (2013)</cite> data set, though the correlations are lower across all measures. This could be caused by the smaller sample size or because the descriptions were generated by a computer, and not retrieved from a collection of human-written descriptions containing the goldstandard text, as in the Flickr8K data set. Figure 3 shows two images from the test collection of the Flickr8K data set with a low Meteor score and a maximum human judgement of semantic correctness.",
  "y": "similarities"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_7",
  "x": "In contrast to the results presented here, Reiter and Belz (2009) found no significant correlations of automatic evaluation measures against human judgements of the accuracy of machine-generated weather forecasts. They did, however, find significant correlations of automatic measures against fluency judgements. There are no fluency judgements available for Flickr8K, but <cite>Elliott and Keller (2013)</cite> report grammaticality judgements for their data, which are comparable to fluency ratings. We failed to find significant correlations between grammatlicality judgements and any of the automatic measures on the <cite>Elliott and Keller (2013)</cite> data. This discrepancy could be explained in terms of the differences between the weather forecast generation and image description tasks, or because the image description data sets contain thousands of texts and a few human judgements per text, whereas the data sets of Reiter and Belz (2009) included hundreds of texts with 30 human judges. ---------------------------------- **CONCLUSIONS**",
  "y": "uses"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_8",
  "x": "In contrast to the results presented here, Reiter and Belz (2009) found no significant correlations of automatic evaluation measures against human judgements of the accuracy of machine-generated weather forecasts. They did, however, find significant correlations of automatic measures against fluency judgements. There are no fluency judgements available for Flickr8K, but <cite>Elliott and Keller (2013)</cite> report grammaticality judgements for their data, which are comparable to fluency ratings. We failed to find significant correlations between grammatlicality judgements and any of the automatic measures on the <cite>Elliott and Keller (2013)</cite> data. This discrepancy could be explained in terms of the differences between the weather forecast generation and image description tasks, or because the image description data sets contain thousands of texts and a few human judgements per text, whereas the data sets of Reiter and Belz (2009) included hundreds of texts with 30 human judges. ---------------------------------- **CONCLUSIONS**",
  "y": "uses"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_0",
  "x": "Examples include summarization (Barzilay and Elhadad, 1999) , question answering (Lin and Pantel, 2001) , and textual entailment (Mirkin et al., 2006) . Graph-based methods have been successfully applied to evaluate word similarity using available ontologies, where the underlying graph included word senses and semantic relationships between them (Hughes and Ramage, 2007) . Another line of research aims at eliciting semantic similarity measures directly from freely available corpora, based on the distributional similarity assumption (Harria, 1968) . In this domain, vector-space methods give state-ofthe-art performance (Pad\u00f3 and Lapata, 2007) . Previously, a graph based framework has been proposed that models word semantic similarity from parsed text <cite>(Minkov and Cohen, 2008)</cite> . The underlying graph in this case describes a text corpus as connected dependency structures, according to the schema shown in Figure 1 . The toy graph shown includes the dependency analysis of two sentences: \"a major environmental disaster is under way\", and \"combat the environmental catastrophe\".",
  "y": "background"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_1",
  "x": "An advantage of graph-based similarity approaches is that they produce similarity scores that reflect structural infor-mation in the graph (Liben-Nowell and Kleinberg, 2003) . Semantically similar terms are expected to share connectivity patterns with the query term in the graph, and thus appear at the top of the list. Notably, different edge types, as well as the paths traversed, may have varying importance for different types of similarity sought. For example, in the parsed text domain, noun similarity and verb similarity are associated with different syntactic phenomena (Resnik and Diab, 2000) . To this end, we consider a path constrained graph walk (PCW) algorithm, which allows one to learn meaningful paths given a small number of labeled examples and incorporates this information in assessing node relatedness in the graph <cite>(Minkov and Cohen, 2008)</cite> . PCW have been successfully applied to the extraction of named entity coordinate terms, including city and person names, from graphs representing newswire text <cite>(Minkov and Cohen, 2008)</cite> , where the specialized measures learned outperformed the state-ofthe-art dependency vectors method (Pad\u00f3 and Lapata, 2007) for small-and medium-sized corpora. In this work, we apply the path constrained graph walk method to the task of eliciting general word relatedness from parsed text, conducting a set of experiments on the task of synonym extraction.",
  "y": "uses"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_2",
  "x": "Notably, different edge types, as well as the paths traversed, may have varying importance for different types of similarity sought. For example, in the parsed text domain, noun similarity and verb similarity are associated with different syntactic phenomena (Resnik and Diab, 2000) . To this end, we consider a path constrained graph walk (PCW) algorithm, which allows one to learn meaningful paths given a small number of labeled examples and incorporates this information in assessing node relatedness in the graph <cite>(Minkov and Cohen, 2008)</cite> . PCW have been successfully applied to the extraction of named entity coordinate terms, including city and person names, from graphs representing newswire text <cite>(Minkov and Cohen, 2008)</cite> , where the specialized measures learned outperformed the state-ofthe-art dependency vectors method (Pad\u00f3 and Lapata, 2007) for small-and medium-sized corpora. In this work, we apply the path constrained graph walk method to the task of eliciting general word relatedness from parsed text, conducting a set of experiments on the task of synonym extraction. While the tasks of named entity extraction and synonym extraction from text have been treated separately in the literature, this work shows that both tasks can be addressed using the same general framework. Our results are encouraging: the PCW model yields superior results to the dependency vectors approach.",
  "y": "background"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_3",
  "x": "PCW have been successfully applied to the extraction of named entity coordinate terms, including city and person names, from graphs representing newswire text <cite>(Minkov and Cohen, 2008)</cite> , where the specialized measures learned outperformed the state-ofthe-art dependency vectors method (Pad\u00f3 and Lapata, 2007) for small-and medium-sized corpora. In this work, we apply the path constrained graph walk method to the task of eliciting general word relatedness from parsed text, conducting a set of experiments on the task of synonym extraction. While the tasks of named entity extraction and synonym extraction from text have been treated separately in the literature, this work shows that both tasks can be addressed using the same general framework. Our results are encouraging: the PCW model yields superior results to the dependency vectors approach. Further, we show that learning specialized similarity measures per word type (nouns, verbs and adjectives) is preferable to applying a uniform model for all word types. ---------------------------------- **PATH CONSTRAINED GRAPH WALKS**",
  "y": "uses"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_4",
  "x": "In this work, we apply the path constrained graph walk method to the task of eliciting general word relatedness from parsed text, conducting a set of experiments on the task of synonym extraction. While the tasks of named entity extraction and synonym extraction from text have been treated separately in the literature, this work shows that both tasks can be addressed using the same general framework. Our results are encouraging: the PCW model yields superior results to the dependency vectors approach. Further, we show that learning specialized similarity measures per word type (nouns, verbs and adjectives) is preferable to applying a uniform model for all word types. ---------------------------------- **PATH CONSTRAINED GRAPH WALKS** PCW is a graph walk variant proposed recently that is intended to bias the random walk process to follow meaningful edge sequences (paths) <cite>(Minkov and Cohen, 2008)</cite> .",
  "y": "background"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_5",
  "x": "Rather than rank all words (terms) in response to a query, we use available (noisy) part of speech information to narrow down the search to the terms of the same type as the query term, e.g. for the query \"film\" we retrieve nodes of type \u03c4 =noun. We applied the PCW method to learn separate models for noun, verb and adjective queries. The path trees were constructed using the paths leading to the node known to be a correct answer, as well as to the otherwise irrelevant top-ranked 10 terms. We required the paths considered by PCW to include exactly 6 segments (edges). Such paths represent distributional similarity phenomena, allowing a direct comparison against the DV method. In conducting the constrained walk, we applied a threshold of 0.5 to truncate paths associated with lower probability of reaching a relevant response, following on previous work <cite>(Minkov and Cohen, 2008)</cite> . We implemented DV using code made available by its authors, 3 where we converted the syntactic patterns specified to Stanford dependency parser conventions.",
  "y": "uses"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_0",
  "x": "Most of the existing studies [6] - [9] have used conventional Machine Learning (ML) models to detect cyberbullying incidents. Recently Deep Neural Network Based (DNN) models have also been applied for detection of cyberbullying [10] , <cite>[11]</cite> . In <cite>[11]</cite> , authors have used DNN models for detection of cyberbullying and have expanded their models across multiple social media platforms. Based on their reported results, their models outperform traditional ML models, and most importantly authors have stated that they have applied transfer learning which means their developed models for detection of cyberbullying can be adapted and used on other datasets. In this contribution, we begin by reproducing and validating the <cite>[11]</cite> proposed models and their results on the three datasets, Formspring [12] , Twitter [13] and Wikipedia [14] , which have been used by the authors. Cyberbullying takes place in almost all of the online social networks; therefore, developing a detection model which is adaptable and transferable to different social networks is of great value. We expand our work by re-implementing the models on a new dataset.",
  "y": "background"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_1",
  "x": "There are also many research conducted on automatic detection and prevention of cyberbullying, which we will address in more details in the next section, but this problem is still far from resolved and there is the need for further improvements towards having a concrete solution. Most of the existing studies [6] - [9] have used conventional Machine Learning (ML) models to detect cyberbullying incidents. Recently Deep Neural Network Based (DNN) models have also been applied for detection of cyberbullying [10] , <cite>[11]</cite> . In <cite>[11]</cite> , authors have used DNN models for detection of cyberbullying and have expanded their models across multiple social media platforms. Based on their reported results, their models outperform traditional ML models, and most importantly authors have stated that they have applied transfer learning which means their developed models for detection of cyberbullying can be adapted and used on other datasets. In this contribution, we begin by reproducing and validating the <cite>[11]</cite> proposed models and their results on the three datasets, Formspring [12] , Twitter [13] and Wikipedia [14] , which have been used by the authors. Cyberbullying takes place in almost all of the online social networks; therefore, developing a detection model which is adaptable and transferable to different social networks is of great value.",
  "y": "background"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_2",
  "x": "In <cite>[11]</cite> , authors have used DNN models for detection of cyberbullying and have expanded their models across multiple social media platforms. Based on their reported results, their models outperform traditional ML models, and most importantly authors have stated that they have applied transfer learning which means their developed models for detection of cyberbullying can be adapted and used on other datasets. In this contribution, we begin by reproducing and validating the <cite>[11]</cite> proposed models and their results on the three datasets, Formspring [12] , Twitter [13] and Wikipedia [14] , which have been used by the authors. Cyberbullying takes place in almost all of the online social networks; therefore, developing a detection model which is adaptable and transferable to different social networks is of great value. We expand our work by re-implementing the models on a new dataset. For this purpose, we have used a YouTube dataset which has been extensively used in cyberbullying studies [6] , [15] , [16] . The ultimate aim was to investigate the interoperability and the performance of the reproduced models on new datasets, to see how adaptable they are to different social media platforms and to what extent models trained on a dataset (i.e., social network) can be transferred to another one.",
  "y": "uses"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_3",
  "x": "Most of the existing studies [6] - [9] have used conventional Machine Learning (ML) models to detect cyberbullying incidents. Recently Deep Neural Network Based (DNN) models have also been applied for detection of cyberbullying [10] , <cite>[11]</cite> . In <cite>[11]</cite> , authors have used DNN models for detection of cyberbullying and have expanded their models across multiple social media platforms. Based on their reported results, their models outperform traditional ML models, and most importantly authors have stated that they have applied transfer learning which means their developed models for detection of cyberbullying can be adapted and used on other datasets. In this contribution, we begin by reproducing and validating the <cite>[11]</cite> proposed models and their results on the three datasets, Formspring [12] , Twitter [13] and Wikipedia [14] , which have been used by the authors. Cyberbullying takes place in almost all of the online social networks; therefore, developing a detection model which is adaptable and transferable to different social networks is of great value. We expand our work by re-implementing the models on a new dataset.",
  "y": "extends"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_4",
  "x": "In the remainder of this paper, we reported the reproduced experimental setup, datasets and findings (Section 2), we investigated the adaptability of the methods on the YouTube dataset (Section 3), and in Section 4 we discussed our findings, compared our results with previous attempts on the same dataset, and pointed out potential future works. ---------------------------------- **REPRODUCED EXPERIMENTAL SETUP** In this study, we have first reproduced the experiments conducted in <cite>[11]</cite> on the datasets used by the authors namely, Formspring [12] , Wikipedia [14] , and Twitter [13] . We have used the same models and experimental setup for our implementations. In this section, we have briefly introduced the datasets and explained the models and other experiment components. For further details please see the reference literature.",
  "y": "uses"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_5",
  "x": "---------------------------------- **TRANSFER LEARNING** Transfer learning is the process of using a model which has been trained on one task for another related task. Following <cite>[11]</cite> we also implemented the transfer learning procedure to evaluate to what extent the DNN models trained on a social network, here Twitter, Formspring, and Wiki, can successfully detect cyberbullying posts in another social network, i.e., YouTube. For this purpose we used the BLSTM with attention model and experimented with three different approaches. \uf0b7 Complete Transfer Learning. In this approach, a model trained on one dataset is directly used in other datasets without any extra training.",
  "y": "uses"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_6",
  "x": "This might be due to the similarity of the nature of these social networks. YouTube, Formspring and (Table 6) was not significant compared to the feature level learning approach. This indicates that the transfer of network weights is not as essential to cyberbullying detection as the learned word embeddings. ---------------------------------- **CONCLUSION AND FUTURE WORK** In this study, we successfully reproduced the reference literature <cite>[11]</cite> for detection of cyberbullying incidents in social media platforms using DNN based models. The source codes and materials were mostly well organized and accessible.",
  "y": "uses"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_0",
  "x": "<cite>Prabhumoye et al. (2018)</cite> propose to transfer style through backtranslation. The latter method is simpler to train and it attains the state-of-the-art performance in style transfer accuracy, confirming the efficacy of back-translation in grounding meaning. The goal of the current study is to investigate alternative back-translation setups that attain a better balance between meaning preservation and style transfer. We introduce two approaches which extend the back-translation models proposed by <cite>Prabhumoye et al. (2018)</cite> exploring back-translation setups that preserve the content of the sentence better. The first approach explores multilingual pivoting, hypothesizing that transfer through several languages will help ground meaning better than transfer through one language. We follow Johnson et al.'s (2017) setup. The second approach is an investigation to include a term in the loss function which corresponds to preserving semantic content of the sentence: we add a feedback loss to the generative models.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_1",
  "x": "The latter method is simpler to train and it attains the state-of-the-art performance in style transfer accuracy, confirming the efficacy of back-translation in grounding meaning. The goal of the current study is to investigate alternative back-translation setups that attain a better balance between meaning preservation and style transfer. We introduce two approaches which extend the back-translation models proposed by <cite>Prabhumoye et al. (2018)</cite> exploring back-translation setups that preserve the content of the sentence better. The first approach explores multilingual pivoting, hypothesizing that transfer through several languages will help ground meaning better than transfer through one language. We follow Johnson et al.'s (2017) setup. The second approach is an investigation to include a term in the loss function which corresponds to preserving semantic content of the sentence: we add a feedback loss to the generative models. We evaluate our models along three dimensions: style transfer accuracy, fluency and preservation of meaning.",
  "y": "extends"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_3",
  "x": "We find that both extensions improve the accuracy of style transfer without reduction in preservation of meaning. ---------------------------------- **GROUNDING MEANING IN BACK-TRANSLATION** While the previous work (<cite>Prabhumoye et al., 2018</cite>) focuses on creating a representation by translating to a pivot language, preserving meaning in the generated sentences is still an unsolved question. In this work, we try to tackle this question by extending their model in two directions: (1) To improve the latent representation such that it grounds the meaning better and (2) Providing the generative models with a feedback which represents how good the generator performs in preserving the meaning. Both the extensions are marked in Figure 1 .",
  "y": "motivation"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_4",
  "x": "We denote the sentences of X 1 transferred to style s 2 asX 12 = {x (1) 12 , . . . ,x (n) 12 } and the sentences of X 2 transferred to style s 1 b\u0177 Style transfer through Back-translation. <cite>Prabhumoye et al. (2018)</cite> introduces the technique of back-translation to perform style transfer. <cite>They</cite> first transfer a sentence to one pivot language and use the encoding of the sentence in the pivot language to train the generative models corresponding to the two styles. <cite>They</cite> also use feedback from a pre-trained classifier to guide the generators to generate the desired style. The objective function of the generative models is: This model is denoted as <cite>Back-translated Style Transfer</cite> (<cite>BST</cite>) in the future.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_5",
  "x": "Let the set of sentences in X which belong to s 1 be X 1 = {x 1 } and the sentences which belong to s 2 be X 2 = {x We denote the sentences of X 1 transferred to style s 2 asX 12 = {x (1) 12 , . . . ,x (n) 12 } and the sentences of X 2 transferred to style s 1 b\u0177 Style transfer through Back-translation. <cite>Prabhumoye et al. (2018)</cite> introduces the technique of back-translation to perform style transfer. <cite>They</cite> first transfer a sentence to one pivot language and use the encoding of the sentence in the pivot language to train the generative models corresponding to the two styles. <cite>They</cite> also use feedback from a pre-trained classifier to guide the generators to generate the desired style.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_6",
  "x": "<cite>Prabhumoye et al. (2018)</cite> introduces the technique of back-translation to perform style transfer. <cite>They</cite> first transfer a sentence to one pivot language and use the encoding of the sentence in the pivot language to train the generative models corresponding to the two styles. <cite>They</cite> also use feedback from a pre-trained classifier to guide the generators to generate the desired style. The objective function of the generative models is: This model is denoted as <cite>Back-translated Style Transfer</cite> (<cite>BST</cite>) in the future. Grounding meaning with multilingual backtranslation. Johnson et al. (2017) showed that multi-lingual neural machine translation systems using one-to-many and many-to-one frameworks can perform zero-shot learning.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_7",
  "x": "<cite>They</cite> also use feedback from a pre-trained classifier to guide the generators to generate the desired style. The objective function of the generative models is: This model is denoted as <cite>Back-translated Style Transfer</cite> (<cite>BST</cite>) in the future. Grounding meaning with multilingual backtranslation. Johnson et al. (2017) showed that multi-lingual neural machine translation systems using one-to-many and many-to-one frameworks can perform zero-shot learning. We want to leverage this approach to ground meaning in style transfer using multiple pivot languages. We have trained a one to many translation system (Johnson et al., 2017) where we have a encoder-decoder network for one source language and two target languages.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_8",
  "x": "We also train a many to one translation system (Johnson et al., 2017) where we have a encoder-decoder network for two source languages and one target language. We use these translation systems for training the style specific decoders following the procedure in (<cite>Prabhumoye et al., 2018</cite>) . Specifically, a sentence is first translated from English to two pivot languages. We create the latent representation of the sentence by encoding the sentence in both pivot languages using the many to one translation system. where, Encoder mo is the encoder of the many to one translate system, x l 1 is the sentence in pivot language 1 and x l 2 is the sentence in pivot language 2. The final representation is given by elementwise average of the two representations. This model is denoted as Multi-lingual Back-translated Style Transfer (MBST) in the future.",
  "y": "uses"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_9",
  "x": "This model is denoted as Multi-lingual Back-translated Style Transfer + Feedback (MBST+F) in the future. ---------------------------------- **EXPERIMENTS** ---------------------------------- **STYLE TRANSFER TASKS** We use three tasks described in (<cite>Prabhumoye et al., 2018</cite>) to evaluate our models. The three tasks correspond to: (1) gender transfer: we transfer the style of writing reviews of Yelp from male and female authors (Reddy and Knight, 2016) .",
  "y": "uses"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_11",
  "x": "**EVALUATION TASKS** Style Transfer Accuracy. We measure the accuracy of style transfer as described in (Shen et al., 2017) . We have reproduced the classifiers described in (<cite>Prabhumoye et al., 2018</cite>) . The classifier has an accuracy of 82% for the gender- annotated corpus, 92% accuracy for the political slant dataset and 93.23% accuracy for the sentiment dataset. We use these classifiers to test the generated sentences for the desired style. Perplexity.",
  "y": "uses"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_12",
  "x": "We reuse the instructions provided by (<cite>Prabhumoye et al., 2018</cite>) for the three tasks. But unlike (<cite>Prabhumoye et al., 2018</cite>), we perform our evaluation on Amazon Mechanical Turk. We annotate 200 samples per task and ask 3 unique workers to annotate each sample. We take the majority vote as the final label. The results in (<cite>Prabhumoye et al., 2018</cite>) were reproduced for comparing the CAE model with the <cite>BST</cite> model. As reported by <cite>them</cite>, the <cite>BST</cite> model performs better in preservation of meaning for the tasks of gender and political slant transfer. We present the results for the comparison between <cite>BST</cite> and MBST models; and the MBST and the MBST+F models. Fluency.",
  "y": "differences uses"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_14",
  "x": "As reported by <cite>them</cite>, the <cite>BST</cite> model performs better in preservation of meaning for the tasks of gender and political slant transfer. We present the results for the comparison between <cite>BST</cite> and MBST models; and the MBST and the MBST+F models. Fluency. We asked human annotators on Mechanical Turk to measure the fluency of the generated sentences on a scale of 1 to 4. 1 is unreadable and 4 is perfectly readable. We annotate 120 samples for each model and each sample is annotated by three unique workers. The 120 samples of each model has an equal distribution of samples from the three tasks. ----------------------------------",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_16",
  "x": "Table 3 shows the results for human evaluation of the models MBST and MBST+F for preservation of meaning. Perhaps confusingly, these results show no clear preference between the models. This is a positive result as it means that these extensions do not degrade the meaning, in spite of them improving the style transfer accuracy. Although we observe that MBST may be slightly preferred over MBST+F. Table 3 : Human preference for meaning preservation % the four models for the three tasks. We averaged the scores over the 120 samples and 3 annotators per sample. MBST+F performs better than the other models in 2 out of 3 tasks and MBST performs the best in one task -political slant. The over-all averaged scores for the two models MBST and MBST+F is the same 3.08, whereas it is much lower 2.79 for <cite>BST</cite> and 2.57 for CAE.",
  "y": "differences"
 },
 {
  "id": "2915e49791d14f5b802225d10f33fb_0",
  "x": "Machine Learning methods have been widely applied for sentiment analysis (Pang et al. 2008;<cite> Pang et al. 2002</cite>; Tan et al. 2008 ). Pang and Lee (2004) experimented with various features like unigrams, bi-grams and adjectives for sentiment classification of movie reviews using different machine learning algorithms namely Na\u00efve Bayes (NB), Support Vector Machines (SVM), and Maximum-Entropy (ME). Feature selection methods improve the performance of sentiment classification by eliminating the noisy and irrelevant features from feature vector. Tan et al. (2008) investigated with various feature selection methods with different machine learning algorithm for sentiment classification. Their experimental results show that IG performs better as compared to other feature selection methods and SVM is best machine learning algorithms. Categorical Probability Proportion Difference (CPPD) feature selection method is proposed which computes the importance of a feature based on its class discriminating ability for sentiment classification (Agarwal et al. 2012) . Various features are extracted from the text for sentiment classification.",
  "y": "background"
 },
 {
  "id": "2915e49791d14f5b802225d10f33fb_1",
  "x": "Documents are initially pre-processed as follows: (i) Negation handling is performed as<cite> Pang et al. (2002)</cite> , \"NOT_\" is added to every words occurring after the negation word (no, not, isn't, can't, never, couldn't, didn't, wouldn't, don't) and first punctuation mark in the sentence. (ii) Words occurring in less than 3 documents are removed from the feature set. Binary weighting scheme has been identified as a better weighting scheme as compared to frequency based schemes for sentiment classification<cite> (Pang et al. 2002)</cite> ; therefore we also used binary weighting method for representing text. In addition, there is no need of using separate discretisation method in case of binary weighting scheme as required by RSAR feature selection algorithm. Noisy and irrelevant features are eliminated from the feature vector generated after pre-processing using various feature selection methods discussed before. Further, prominent feature vector is used by machine learning algorithms. Support Vector Machine (SVM) and Na\u00efve Bayes (NB) classifiers are the mostly used for sentiment classification <cite>(Pang et al. 2002</cite>; Tan et al. 2008) .",
  "y": "similarities uses"
 },
 {
  "id": "2915e49791d14f5b802225d10f33fb_2",
  "x": "Each domain has 1000 positive and 1000 negative labelled reviews. Documents are initially pre-processed as follows: (i) Negation handling is performed as<cite> Pang et al. (2002)</cite> , \"NOT_\" is added to every words occurring after the negation word (no, not, isn't, can't, never, couldn't, didn't, wouldn't, don't) and first punctuation mark in the sentence. (ii) Words occurring in less than 3 documents are removed from the feature set. Binary weighting scheme has been identified as a better weighting scheme as compared to frequency based schemes for sentiment classification<cite> (Pang et al. 2002)</cite> ; therefore we also used binary weighting method for representing text. In addition, there is no need of using separate discretisation method in case of binary weighting scheme as required by RSAR feature selection algorithm. Noisy and irrelevant features are eliminated from the feature vector generated after pre-processing using various feature selection methods discussed before. Further, prominent feature vector is used by machine learning algorithms.",
  "y": "motivation background"
 },
 {
  "id": "2915e49791d14f5b802225d10f33fb_3",
  "x": "Support Vector Machine (SVM) and Na\u00efve Bayes (NB) classifiers are the mostly used for sentiment classification <cite>(Pang et al. 2002</cite>; Tan et al. 2008) . Therefore, we report the classification results of SVM and NB classifier for classifying review documents into positive or negative sentiment polarity. For the evaluation of proposed methods 10 fold cross validation method is used. Fmeasure value is reported as a performance measure of various classifiers (Agarwal et al. 2013) ---------------------------------- **EXPERIMENTAL RESULTS AND DISCUSSIONS** Initially, unigram features are extracted from the review documents.",
  "y": "background"
 },
 {
  "id": "2abfa447cea31af26d06d4325c94ac_0",
  "x": "Recent successes in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees (Collins, 1999; Charniak, 2000;<cite> Henderson, 2003)</cite> have brought the hope that the same approach could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence. Moving towards a shallow semantic level of representation has immediate applications in question-answering and information extraction. For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to New York will need to know that from Geneva indicates the origin of the flight and to New York the destination. (Gildea and Jurafsky, 2002 ) define this shallow semantic task as a classification problem where the semantic role to be assigned to each constituent is inferred on the basis of probability distributions of syntactic features extracted from parse trees. They use learning features such as phrase type, position, voice, and parse tree path. Consider, for example, a sentence such as The authority dropped at midnight Tuesday to $ 2.80 trillion (taken from section 00 of PropBank (Palmer et al., 2005) ). The fact that to $ 2.80 trillion receives a direction semantic label is highly correlated to the fact that it is a Prepositional Phrase (PP), that it follows the verb dropped, a verb of change of state requiring an end point, that the verb is in the active voice, and that the PP is in a certain tree configuration with the governing verb.",
  "y": "background"
 },
 {
  "id": "2abfa447cea31af26d06d4325c94ac_1",
  "x": "The assumption that syntactic distributions will be predictive of semantic role assignments is based on linking theory. Linking theory assumes the existence of a hierarchy of semantic roles which are mapped by default on a hierarchy of syntactic positions. It also shows that regular mappings from the semantic to the syntactic level can be posited even for those verbs whose arguments can take several syntactic positions, such as psychological verbs, locatives, or datives, requiring a more complex theory. (See (Hale and Keyser, 1993; Levin and Rappaport Hovav, 1995) among many others.) If the internal semantics of a predicate determines the syntactic expressions of constituents bearing a semantic role, it is then reasonable to expect that knowledge about semantic roles in a sentence will be informative of its syntactic structure, and that learning semantic role labels at the same time as parsing will be beneficial to parsing accuracy. We present work to test the hypothesis that a current statistical parser <cite>(Henderson, 2003)</cite> can output rich information comprising both a parse tree and semantic role labels robustly, that is without any significant degradation of the parser's accuracy on the original parsing task. We achieve promising results both on the simple parsing task, where the accuracy of the parser is measured on the standard Parseval measures, and also on the parsing task where more complex labels comprising both syntactic labels and semantic roles are taken into account. These results have several consequences.",
  "y": "uses"
 },
 {
  "id": "2abfa447cea31af26d06d4325c94ac_2",
  "x": "In this section we describe the augmentations to our base parsing models necessary to tackle the joint learning of parse tree and semantic role labels. PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank (Marcus et al., 1993) . Verbal predicates in the Penn Treebank (PTB) receive a label REL and their arguments are annotated with abstract semantic role labels A0-A5 or AA for those complements of the predicative verb that are considered arguments while those complements of the verb labelled with a semantic functional label in the original PTB receive the composite semantic role label AM-X, where X stands for labels such as LOC, TMP or ADV, for locative, temporal and adverbial modifiers respectively. PropBank uses two levels of granularity in its annotation, at least conceptually. Arguments receiving labels A0-A5 or AA do not express consistent semantic roles and are specific to a verb, while arguments receiving an AM-X label are supposed to be adjuncts, and the roles they express are consistent across all verbs. To achieve the complex task of assigning semantic role labels while parsing, we use a family of state-of-the-art history-based statistical parsers, the Simple Synchrony Network (SSN) parsers <cite>(Henderson, 2003)</cite> , which use a form of left-corner parse strategy to map parse trees to sequences of derivation steps. These parsers do not impose any a priori independence assumptions, but instead smooth their parameters by means of the novel SSN neural network architecture.",
  "y": "uses"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_0",
  "x": "Mohammad et al. (2017) define stance detection as \" [.. .] the task of automatically determining from text whether the author of the text is in favor of, against, or neutral toward a proposition or target\". They distinguish stance detection from the better known task of sentiment analysis by that \"in stance detection, systems are to determine favorability toward a given (pre-chosen) target of interest\", whereas sentiment analysis is the task of \"determining whether a piece of text is positive, negative, or neutral, or determining from text the speakers opinion and the target of the opinion\". For instance, the utterance \"The diseases that vaccination can protect you from are horrible\" expresses a stance for the pre-chosen target \"vaccination\", while expressing a negative sentiment towards the sentiment-target \"diseases\". ---------------------------------- **BACKGROUND** This definition of stance is used in several stance detection studies. For instance, in studies performed on the text genres web debate forums (Somasundaran and Wiebe, 2010; Anand et al., 2011; Walker et al., 2012; Hasan and Ng, 2013) , news paper text (Ferreira and Vlachos, 2016; Fake News Challenge, 2017) and tweets (Augenstein et al., 2016;<cite> Mohammad et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_1",
  "x": "For instance, in studies performed on the text genres web debate forums (Somasundaran and Wiebe, 2010; Anand et al., 2011; Walker et al., 2012; Hasan and Ng, 2013) , news paper text (Ferreira and Vlachos, 2016; Fake News Challenge, 2017) and tweets (Augenstein et al., 2016;<cite> Mohammad et al., 2017)</cite> . Stance detection is generally considered more difficult than sentiment analysis and thereby a task for which currently available methods achieve lower results. This was, for instance, shown by a recent shared task on three-category stance classification of tweets, where an F-score of 0.59 was achieved by a classifier that outperformed submissions from 19 shared task teams <cite>(Mohammad et al., 2017)</cite> . For the task of stance classification of posts of two-sided discussion threads, an F-score of 0.70 is the best result we have been able to find in previous research (Hasan and Ng, 2013) Previous studies on attitudes towards vaccination do not make use of the term stance, but discuss negative/positive sentiment towards vaccination. There are a number of such sentiment detection studies conducted on tweets, while studies on online forums, to the best of our knowledge, are limited to the task of topic modelling (Tangherlini et al., 2016) . Most vaccination sentiment studies have been conducted on tweets that contain keywords related to HPV (human papillomavirus) vaccination.",
  "y": "background"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_2",
  "x": "Following the principle of the guidelines by<cite> Mohammad et al. (2017)</cite> , we classified the posts as taking a stance against or for vaccination, or to be undecided. The third category was applied to posts in which the debater explicitly declared to be undecided, as well as to posts for which the debater's stance towards vaccination could not be determined. The post did not have to explicitly support or oppose vaccination to be classified according to the categories against or for, but it was enough if an opposition or support could be inferred from the post. The stance taken could, for instance, be conveyed without actually mentioning vaccination, as in \"You are very lucky to live in the west, and you are free to make that decision because the majority are giving you herd immunity. \" It could also be conveyed through an agreement or disagreement with a known proponent/opponent of vaccination, as the statement \"Andrew Wakefield is a proven liar and a profiteer -therefore his \"research\" is irrelevant to any sane, rational discussion [...]\" Web links to external resources were, however, not included in the classification decision, even when a stance towards vaccination could be inferred from the name of the URL. Mohammad et al. (2017) did not specify in detail how to distinguish between stance against and for the targets included in the study. However, several of the posts in our data did not express a clear positive or clear negative stance towards vaccination in general, and we therefore needed more detailed guidelines for how to draw the line between against and for.",
  "y": "similarities uses"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_3",
  "x": "A standard text classification approach, in the form of a linear support vector machine model, was applied to the task of automatically classifying the debate posts. This follows the approach of<cite> Mohammad et al. (2017)</cite> , as well as of many of the previously performed vaccine sentiment studies. The model was trained on all tokens in the training data, as well as on 2-, 3-and 4-grams that occurred at least twice in the data. The standard NLTK stop word list for English was used for removing non-content words when constructing one set of n-grams. An additional set of n-grams was generated with a reduced version of this stop word list, which mainly consisted of articles, forms of copula, and forms of \"it\", \"have\" and \"do\". The reason for using a reduced list was that negations, pronouns etc. that were included in the standard NLTK stop word list can be important cues for classifying argumentative text.",
  "y": "similarities uses"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_4",
  "x": "Our decision, to classify debate posts as against vaccination when they opposed an official vaccination policy, was based on that debaters often implicitly argue against such a policy. In addition, a system for surveilling increases in vaccine hesitancy is likely to take an official policy as its point of departure. The eight annotators that classified each tweet in the study by<cite> Mohammad et al. (2017)</cite> were employed through a crowdsourcing platform, which was made possible by that the stance targets were chosen with the criterion that they should be commonly known in the United States. For annotating stance on vaccination, however, annotators with some amount of prior knowledge of vaccine debate topics and vaccine controversies might be preferred. Crowdsourcing might therefore not be a viable option for this annotation task. 40 randomly selected posts that did not fulfil the criterion of containing any of the selected vaccine-related filter terms, and which therefore had been excluded from the study, were also annotated. Although this set is too small to make any definite conclusions, the relatively large proportion of posts in the set that expressed a stance against or for vaccination (25%) indicates that the filtering criterion used was too crude and led to the exclusion of relevant posts.",
  "y": "background"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_5",
  "x": "40 randomly selected posts that did not fulfil the criterion of containing any of the selected vaccine-related filter terms, and which therefore had been excluded from the study, were also annotated. Although this set is too small to make any definite conclusions, the relatively large proportion of posts in the set that expressed a stance against or for vaccination (25%) indicates that the filtering criterion used was too crude and led to the exclusion of relevant posts. Future studies should therefore either apply a better filtering criterion, or include all discussion thread posts in an annotation and machine learning study. ---------------------------------- **MACHINE LEARNING DECISIONS** The choice of machine learning model was primarily based on that a linear support vector machine was successful on data from the previously mentioned shared task of stance detection of tweets <cite>(Mohammad et al., 2017)</cite> . This model outperformed submissions from teams that used methods which might intuitively be better adapted to the task, i.e., an LSTM classifier (Augenstein et al., 2016) .",
  "y": "similarities uses"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_6",
  "x": "For instance, Hasan and Ng (2013) improved their stance classification results by expanding their feature set to also include an unsupervised estimation of the stance of each sentence in the debate post. In addition, it is likely that even if applied on a sentence-level, the use of n-grams would not capture the full complexity of the argumentative text genre. Instead, the structure of the words in the sentences might need to be incorporated in the feature set. A classifier trained on a token-level and where neighbouring tokens in a large context window are incorporated as features could be one such approach. Another possibility would be the previously mentioned approach of stance detection using an LSTM classifier (Augenstein et al., 2016) . Previous studies have also been able to improve results, at least for some targets, by incorporating other features than n-grams. For instance, features constructed using an arguing lexicon (Somasun-daran and Wiebe, 2010), or word embeddings constructed in an unsupervised fashion using a large corpus from the same text genre as the text to classify <cite>(Mohammad et al., 2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_0",
  "x": "All but a few of these models rely on Recurrent Neural Networks (RNNs), which currently dominate the stateof-the-art in most Natural Language Processing (NLP) tasks. However, RNNs do have some drawbacks, of which the most relevant to real-world applications is the high number of sequential operations, which increases the processing time of both learning and inference. To address these limitations, Vaswani et al. have proposed the Transformer, a machine translation model that introduces a new deep learning architecture solely based on \"attention\" mechanisms <cite>[2]</cite> . We later clarify the meaning of attention in this context. Inspired by the positive results of Vaswani et al. in machine translation, we have applied a similar architecture to the domain of question-answering, a model that we have named Fully Attention-Based Information Retriever (FABIR). Our goal then was to verify how much performance we can get exclusively from the attention mechanism, without combining it with several other techniques. We validated our model in the SQuAD dataset, which proved that FABIR not only achieves competitive results (F1:77.6%, EM:67.7%) but also has fewer parameters and is faster at both training and testing times than competing methods.",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_1",
  "x": "In SQuAD, each problem instance consists of a passage P and a question Q. A QA system must then provide an answer A by selecting a snippet from P. That format reduces the complexity of the task and also facilitates training, as one can learn a probability distribution over the words that compose the passage. Since its publication in 2016, SQuAD has been targeted by many research groups, and the proposed models are gradually approaching (even overcoming) human-level performances. All but a few of these models rely on Recurrent Neural Networks (RNNs), which currently dominate the stateof-the-art in most Natural Language Processing (NLP) tasks. However, RNNs do have some drawbacks, of which the most relevant to real-world applications is the high number of sequential operations, which increases the processing time of both learning and inference. To address these limitations, Vaswani et al. have proposed the Transformer, a machine translation model that introduces a new deep learning architecture solely based on \"attention\" mechanisms <cite>[2]</cite> . We later clarify the meaning of attention in this context. Inspired by the positive results of Vaswani et al. in machine translation, we have applied a similar architecture to the domain of question-answering, a model that we have named Fully Attention-Based Information Retriever (FABIR).",
  "y": "extends"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_2",
  "x": "Our goal then was to verify how much performance we can get exclusively from the attention mechanism, without combining it with several other techniques. We validated our model in the SQuAD dataset, which proved that FABIR not only achieves competitive results (F1:77.6%, EM:67.7%) but also has fewer parameters and is faster at both training and testing times than competing methods. Besides the development of a new architecture, we identify three major contributions of our work that have made these results possible: \u2022 Convolutional attention: a novel attention mechanism that encodes many-to-many relationships between words, enabling richer contextual representations. \u2022 Reduction layer: a new layer design that fits the pipeline proposed by Vaswani et al. <cite>[2]</cite> and compresses the input embedding size for subsequent layers (this is especially beneficial when employing pre-trained embeddings). \u2022 Column-wise cross-attention: we modify the crossattention operation by <cite>[2]</cite> and propose a new technique that is better suited to question-answering. This article is organized as follows.",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_3",
  "x": "We validated our model in the SQuAD dataset, which proved that FABIR not only achieves competitive results (F1:77.6%, EM:67.7%) but also has fewer parameters and is faster at both training and testing times than competing methods. Besides the development of a new architecture, we identify three major contributions of our work that have made these results possible: \u2022 Convolutional attention: a novel attention mechanism that encodes many-to-many relationships between words, enabling richer contextual representations. \u2022 Reduction layer: a new layer design that fits the pipeline proposed by Vaswani et al. <cite>[2]</cite> and compresses the input embedding size for subsequent layers (this is especially beneficial when employing pre-trained embeddings). \u2022 Column-wise cross-attention: we modify the crossattention operation by <cite>[2]</cite> and propose a new technique that is better suited to question-answering. This article is organized as follows. We first introduce some of the related work in question-answering and then present FABIR's architecture and its basic design choices.",
  "y": "extends"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_4",
  "x": "Vaswani et al. were the first to apply attention directly over the word-embeddings, and thus derived a new neural network architecture which, without any recurrence, achieved state-ofthe-art results in machine translation <cite>[2]</cite> . In this section, we briefly discuss both types of attention models. ---------------------------------- **A. TRADITIONAL ATTENTION MECHANISMS** In recent years, attention mechanisms have been used with success in a variety of NLP tasks, such as machine translation <cite>[2]</cite> , [27] and natural language inference [28] , [29] . Indeed, most models that target the SQuAD dataset use some form of attention to model the relationship between question and passage. Attention can be defined as a mechanism that gives a score \u03b1 i to a vector p i from a set P = [p 1 , ..., p m ] with respect to a vector q j from Q = [q 1 , ..., q n ].",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_5",
  "x": "In recent years, attention mechanisms have been used with success in a variety of NLP tasks, such as machine translation <cite>[2]</cite> , [27] and natural language inference [28] , [29] . Indeed, most models that target the SQuAD dataset use some form of attention to model the relationship between question and passage. Attention can be defined as a mechanism that gives a score \u03b1 i to a vector p i from a set P = [p 1 , ..., p m ] with respect to a vector q j from Q = [q 1 , ..., q n ]. This score is a function of both P and Q and is shown in its most general form in (1) . where s i and \u03b1 i are scalars and f is a score function that measures the importance of p i relative to q j . Intuitively, a large weight \u03b1 i means that the vector p i is somehow strongly related to Q. In the literature, two alternatives for f have been proposed, additive [27] and multiplicative [30] attentions: where W 1 , W 2 and W 3 are learnable parameters and g is a elementwise nonlinear function.",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_6",
  "x": "**B. GOOGLE'S TRANSFORMER** The Transformer is a machine translation model introduced in <cite>[2]</cite> that achieved state-of-the-art results by combining feedforward neural networks with a multiplicative attention mechanism applied over position-encoded embedding vectors. It defines three different matrices U, K and V that are associated with queries, keys, and values, respectively. Every attention operation in the Transformer is performed by multiplying these matrices as shown in (4) . where W U K , W V \u2208 R d model \u00d7d model are weight matrices and d model is the embedding size of each word. Additionally, Vaswani et al. <cite>[2]</cite> suggest a multi-head attention, in which U, K and V are divided into n heads heads and the attention in the i th head is computed as where W U,i , W K,i , W V,i \u2208 R d model \u00d7d head are again learnable weight matrices and d head is the embedding dimension of each head.",
  "y": "uses background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_7",
  "x": "where W U K , W V \u2208 R d model \u00d7d model are weight matrices and d model is the embedding size of each word. Additionally, Vaswani et al. <cite>[2]</cite> suggest a multi-head attention, in which U, K and V are divided into n heads heads and the attention in the i th head is computed as where W U,i , W K,i , W V,i \u2208 R d model \u00d7d head are again learnable weight matrices and d head is the embedding dimension of each head. Finally, attention is computed by the concatenation of every head attention att i , followed by an affine transformation: where W O \u2208 R n heads * d head \u00d7d model . If one wants to model the interdependence of words within a single piece of text, U, K and V are all equal and consist of the text of interest embedded in some vectorial space. This type of attention is often called \"self-attention\" or \"self-alignment\" <cite>[2]</cite> , [21] .",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_8",
  "x": "where W U K , W V \u2208 R d model \u00d7d model are weight matrices and d model is the embedding size of each word. Additionally, Vaswani et al. <cite>[2]</cite> suggest a multi-head attention, in which U, K and V are divided into n heads heads and the attention in the i th head is computed as where W U,i , W K,i , W V,i \u2208 R d model \u00d7d head are again learnable weight matrices and d head is the embedding dimension of each head. Finally, attention is computed by the concatenation of every head attention att i , followed by an affine transformation: where W O \u2208 R n heads * d head \u00d7d model . If one wants to model the interdependence of words within a single piece of text, U, K and V are all equal and consist of the text of interest embedded in some vectorial space. This type of attention is often called \"self-attention\" or \"self-alignment\" <cite>[2]</cite> , [21] .",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_9",
  "x": "Conversely, if one seeks the relationship between words from two different passages, then U represents one, while K and V represent the other. In that case, we talk about \"cross-attention\". ---------------------------------- **C. OTHER RNN-FREE MODELS** We also identified another QA model [32] that is inspired by the architecture introduced by Vaswani et al. <cite>[2]</cite> . Their model differs from ours in that it heavily relies on convolutions (46 layers against 2 in FABIR), which approximates it to other CNN NLP models [33] , rather than purely attention based models. Although they report high F1 and EM scores (82.7% and 73.3%), our model is almost twice as fast in inference (259 samples/s against 440 in FABIR).",
  "y": "extends"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_10",
  "x": "In particular, we introduce the convolutional attention, the column-wise cross-attention, and the reduction layer, which build on the Transformer model <cite>[2]</cite> to enable its application to question-answering. ---------------------------------- **A. EMBEDDINGS** We model each piece of text at the level of a word, i.e., sentences are defined by a sequence of vectors \u03c9, each one representing a word in a vectorial space R dinput . Thus, we build a new representation of question and passage to which we will refer as \u2126 Q and \u2126 P , respectively. where Q len and P len denote the number of tokens in the question and the passage, respectively. These embeddings are composed by word-level and character-level representations.",
  "y": "extends"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_11",
  "x": "Given a word with length l, C = [c 1 , c 2 , ..., c l ], in which c i \u2208 R 8 are learned character embeddings, we compute \u03c9 c by convolving C with kernel H \u2208 R 1\u00d75\u00d78\u00d7100 and applying max-over time pooling [35] . Finally, we squeeze \u03c9 c values to [\u22121, 1] using a hyperbolic tangent activation function and pass the concatenation of \u03c9 w and tanh(\u03c9 c ) through a two-layer Highway-Network [36] to obtain the final representation of a word. ---------------------------------- **B. ENCODER** In contrast to an RNN, FABIR does not process words in sequence, and hence needs to model the position of each word in a sentence differently. We add positional information to each word embedding using a trigonometric encoder as proposed in <cite>[2]</cite> . Therefore, given a sequence of embedding vectors of even size d model , the position of the i th word is encoded in a vector e i as follows:",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_12",
  "x": "where f k are scalars, which were chosen according to <cite>[2]</cite> . The encoding of an embedding matrix \u2126 is represented by E and the whole operation can be summarized as where d model is the size of each position encoding, which is not necessarily equal to d input . The encoding E can be summed to \u2126 to include the information of the position of each word in the text. Indeed, in <cite>[2]</cite> , the final vectorial representation of a piece of text is defined by the sum of the embeddings \u2126 with the position encoding E, which would require d model = d input . However, we introduce a layer that processes embeddings and encodings separately before summing them up. Because we also use this layer to reduce the embedding size from d input to d model , we named it \"reduction layer\".",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_13",
  "x": "where d model is the size of each position encoding, which is not necessarily equal to d input . The encoding E can be summed to \u2126 to include the information of the position of each word in the text. Indeed, in <cite>[2]</cite> , the final vectorial representation of a piece of text is defined by the sum of the embeddings \u2126 with the position encoding E, which would require d model = d input . However, we introduce a layer that processes embeddings and encodings separately before summing them up. Because we also use this layer to reduce the embedding size from d input to d model , we named it \"reduction layer\". The architecture of this type of layer is addressed further on. ----------------------------------",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_14",
  "x": "The encoding of an embedding matrix \u2126 is represented by E and the whole operation can be summarized as where d model is the size of each position encoding, which is not necessarily equal to d input . The encoding E can be summed to \u2126 to include the information of the position of each word in the text. Indeed, in <cite>[2]</cite> , the final vectorial representation of a piece of text is defined by the sum of the embeddings \u2126 with the position encoding E, which would require d model = d input . However, we introduce a layer that processes embeddings and encodings separately before summing them up. Because we also use this layer to reduce the embedding size from d input to d model , we named it \"reduction layer\". The architecture of this type of layer is addressed further on.",
  "y": "differences"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_15",
  "x": "However, we introduce a layer that processes embeddings and encodings separately before summing them up. Because we also use this layer to reduce the embedding size from d input to d model , we named it \"reduction layer\". The architecture of this type of layer is addressed further on. ---------------------------------- **C. CONVOLUTIONAL ATTENTION** In FABIR the attention mechanism is inspired by the Transformer model introduced in <cite>[2]</cite> . However, we hypothesize the word-to-word relationship in (1a) fails to capture the complexity of expressions involving groups of words.",
  "y": "extends"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_16",
  "x": "In FABIR, self-attention is a sublayer that applies such operation via convolutional attention and is defined as att self (P ) = att conv (P, P, P ). 2) Column-wise Cross-attention: Cross-attention (att cross ) differs from other types of attention by relating two different pieces of text. Given P and Q, cross-attention of Q over P is defined as In contrast to Vaswani et al. <cite>[2]</cite> , where the softmax in (12d) is applied in a row-wise manner, we suggest column-wise cross-attention. More precisely, we sum over i instead of j in (1b). Row-wise softmax is inadequate in QA because, in practice, it computes a weighted average of the question words for every passage word, and thus cannot model the likely scenario in which not every word in the passage is related to the question. In contrast, the column-wise softmax attributes greater weights to passage words that are more closely related to the respective question word, which seems appropriate for the SQuAD task.",
  "y": "differences"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_17",
  "x": "Many question-answering models employ cross-attention in both directions: att cross (P, Q) and att cross (Q, P ) [7] , [21] , [22] . However, in FABIR we have observed better results when only the former is used. 3) Feedforward: The feedforward sublayer is solely composed of a neural network with a single hidden layer, which is applied vector-wise. Following the architecture suggested by Vaswani et al. <cite>[2]</cite> , the feedforward sublayer is implemented in (15) with a two-layer neural network: where hidden and b 2 \u2208 R 1\u00d7d model are all trainable parameters, d hidden is the dimension of the hidden layer in (15) and ReLU(x) = max(0, x). Reduction Layer Processing Layer Fig. 1 .",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_18",
  "x": "Not counting the pre-processing and answer selector layers, our best performing model was composed of four layers, of which the last three are processing layers and the first is a reduction layer. ---------------------------------- **F. REDUCTION LAYER** The SQuAD dataset is relatively small for the training of word embeddings, and pre-trained word vectors have been favored in the literature [21] . Nonetheless, we observed that the new architecture introduced by Vaswani et al. <cite>[2]</cite> is more susceptible to overfitting than RNNs when presented with large embedding sizes. Hence, we needed a method to compress the word representations, and thus facilitate and speed up training by reducing the number of parameters. A straightforward method to reduce the input embedding size is to multiply it by a matrix with the required dimensions:",
  "y": "motivation"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_19",
  "x": "We have trained our FABIR model during 54 epochs with a batch size of 75 in a GPU NVidia Titan X with 12 GB of RAM. We developed our model in Tensorflow [39] and made it available at https://worksheets.codalab.org/worksheets/ 0xee647ea284674396831ecb5aae9ca297/ for replicability. We pre-processed the texts with the NLTK Tokenizer [40] . As suggested in <cite>[2]</cite> , we have chosen the Adam optimizer [41] with the same hyperparameters, except for the learning rate, which was divided by two in our implementation. For regularization, we applied residual and attention dropout <cite>[2]</cite> of 0.9 in processing layers and of 0.8 in the reduction layer. In the character-level embedding process, a dropout of 0.75 was added before the convolution. Additionally, a dropout of 0.8 was added before each convolutional layer in the answer selector.",
  "y": "differences uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_20",
  "x": "We developed our model in Tensorflow [39] and made it available at https://worksheets.codalab.org/worksheets/ 0xee647ea284674396831ecb5aae9ca297/ for replicability. We pre-processed the texts with the NLTK Tokenizer [40] . As suggested in <cite>[2]</cite> , we have chosen the Adam optimizer [41] with the same hyperparameters, except for the learning rate, which was divided by two in our implementation. For regularization, we applied residual and attention dropout <cite>[2]</cite> of 0.9 in processing layers and of 0.8 in the reduction layer. In the character-level embedding process, a dropout of 0.75 was added before the convolution. Additionally, a dropout of 0.8 was added before each convolutional layer in the answer selector. We set processing layers dimension d model to 100, the number of heads n heads in each attention sublayer to 4, the feed-forward hidden size to 200 in processing layers and 400 in the reduction layer.",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_21",
  "x": "Table I show the results of these experiments regarding the F1 and EM scores, and the Training Time (TT) over 18-epoch runs. This analysis confirms the effectiveness of char-embeddings, as its addition increased the F1 and EM scores, by 2.7% and 3.1%, respectively. Most importantly, when the convolutional attention was replaced by the standard attention mechanism proposed in <cite>[2]</cite> , the performance dropped by 2.4% in F1 and 2.5% in EM. That validates the contribution of this new attention method in building elaborate contextual representations. Moreover, the tests also indicate that the reduction layer is capable of producing useful word representations when compressing the embeddings. Indeed, when we replaced that layer by a standard feedforward layer with the same reduction ratio, there was a drop of 2.1% and 2.5% in the F1 and EM scores, respectively. Finally, we observed that the column-wise cross-attention outperforms its row-wise counterpart by 2.0% and 1.9% in F1 and EM, respectively.",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_22",
  "x": "These results strengthen some of FABIR's compelling advantages, notably, an architecture that is both more parallelizable and lighter, with half of the number of parameters in comparison to BiDAF [21] . FABIR also brings three significant contributions to this new class of neural network architectures. The convolutional attention, the reduction layer, and the column-wise crossattention individually increased the model's F1 and EM scores by more than 2%. Moreover, being thoroughly compatible with the Transformer <cite>[2]</cite> , these new mechanisms are valuable assets to further developments in attention models. In fact, an intriguing line for future research is to evaluate their impact on other NLP tasks, such as machine translation or parsing. Although FABIR is still far from surpassing the models at the top of the SQuAD leaderboard (Table III) , we believe that its faster and lighter architecture already make it an attractive alternative to RNN-based models, especially for applications with limited processing power or that require low-latency. Also, being a distinct technique, FABIR might have low correlation with existing RNN-based models, increasing the potential of ensemble methods.",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_0",
  "x": "Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks. ---------------------------------- **INTRODUCTION** There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> .",
  "y": "background"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_1",
  "x": "So far, neural word segmentors have given comparable accuracies to the best statictical models. With respect to non-sparse representation, character embeddings have been exploited as a foundation of neural word segmentors. They serve to reduce sparsity of character ngrams, allowing, for example, \"\u732b(cat) \u8eba(lie) \u5728(in) \u5899\u89d2(corner)\" to be connected with \"\u72d7(dog) \u8e72(sit) \u5728(in) \u5899 * Equal contribution. \u89d2(corner)\" (Zheng et al., 2013) , which is infeasible by using sparse one-hot character features. In addition to character embeddings, distributed representations of character bigrams Pei et al., 2014) and words (Morita et al., 2015;<cite> Zhang et al., 2016b)</cite> have also been shown to improve segmentation accuracies. With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a) , as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> . For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> .",
  "y": "background"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_2",
  "x": "With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a) , as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> . For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> . Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016) . Pretraining can be regarded as one way of leveraging external resources to improve accuracies, which is practically highly useful and has become a standard practice in neural NLP. On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011) , and making use of selfpredictions (Wang et al., 2011; Liu and Zhang, 2012) . It has also utilised heterogenous annotations such as POS (Ng and Low, 2004; Zhang and Clark, 2008) standards (Jiang et al., 2009 ). To our knowledge, such rich external information has not been systematically investigated for neural segmentation.",
  "y": "background"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_3",
  "x": "In addition to character embeddings, distributed representations of character bigrams Pei et al., 2014) and words (Morita et al., 2015;<cite> Zhang et al., 2016b)</cite> have also been shown to improve segmentation accuracies. With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a) , as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> . For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> . Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016) . Pretraining can be regarded as one way of leveraging external resources to improve accuracies, which is practically highly useful and has become a standard practice in neural NLP. On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011) , and making use of selfpredictions (Wang et al., 2011; Liu and Zhang, 2012) . It has also utilised heterogenous annotations such as POS (Ng and Low, 2004; Zhang and Clark, 2008) standards (Jiang et al., 2009 ).",
  "y": "background"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_4",
  "x": "It has also utilised heterogenous annotations such as POS (Ng and Low, 2004; Zhang and Clark, 2008) standards (Jiang et al., 2009 ). To our knowledge, such rich external information has not been systematically investigated for neural segmentation. We fill this gap by investigating rich external pretraining for neural segmentation. Following Cai and Zhao (2016) and<cite> Zhang et al. (2016b)</cite> , we adopt a globally optimised beam-search framework for neural structured prediction (Andor et al., 2016; Zhou et al., 2015; Wiseman and Rush, 2016) , which allows word information to be modelled explicitly. Different from previous work, we make our model conceptually simple and modular, so that the most important sub module, namely a five-character window context, can be pretrained using external data. We adopt a multi-task learning strategy (Collobert et al., 2011) , casting each external source of information as a auxiliary classification task, sharing a five-character window network. After pretraining, the character window network is used to initialize the corresponding module in our segmentor.",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_5",
  "x": "Similar to<cite> Zhang et al. (2016b)</cite> and Cai and Zhao (2016) , we use word context on top of character context. However, words play a relatively less important role in our model, and we find that word LSTM, which has been used by all previous neural segmentation work, is unnecessary for our model. Our model is conceptually simpler and more modularised compared with Figure 1 : Overall model. Zhang et al. (2016b) and Cai and Zhao (2016) , allowing a central sub module, namely a fivecharacter context window, to be pretrained. ---------------------------------- **MODEL** Our segmentor works incrementally from left to right, as the example shown in Figure 1 .",
  "y": "similarities"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_6",
  "x": "Our results show a similar degree of error reduction compared to theirs by using external data. Our model inherits from previous findings on context representations, such as character windows Pei et al., 2014; Chen et al., 2015a) and LSTMs (Chen et al., 2015b; Xu and Sun, 2016) . Similar to<cite> Zhang et al. (2016b)</cite> and Cai and Zhao (2016) , we use word context on top of character context. However, words play a relatively less important role in our model, and we find that word LSTM, which has been used by all previous neural segmentation work, is unnecessary for our model. Our model is conceptually simpler and more modularised compared with Figure 1 : Overall model. Zhang et al. (2016b) and Cai and Zhao (2016) , allowing a central sub module, namely a fivecharacter context window, to be pretrained. ----------------------------------",
  "y": "similarities differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_7",
  "x": "At each step, a decision is made on c 0 , either appending it as a part of P , or seperating it as the beginning of a new word. The incremental process repeats until C is empty and P is null again (C = [ ], P = \u03c6). Formally, the process can be regarded as a state-transition process, where a state is a tuple S = W, P, C , and the transition actions include SEP (seperate) and APP (append), as shown by the deduction system in Figure 2 2 . In the figure, V denotes the score of a state, given by a neural network model. The score of the initial state (i.e. axiom) is 0, and the score of a non-axiom state is the sum of scores of all incremental decisions resulting in the state. Similar to<cite> Zhang et al. (2016b)</cite> and Cai and Zhao (2016) , our model is a global structural model, using the overall score to disambiguate states, which correspond to sequences of inter-dependent transition actions. Different from previous work, the structure of",
  "y": "similarities"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_8",
  "x": "**REPRESENTATION LEARNING** Characters. We investigate two different approaches to encode incoming characters, namely a window approach and an LSTM approach. For the former, we follow prior methods (Xue et al., 2003; Pei et al., 2014) , using five-character window [c \u22122 , c \u22121 , c 0 , c 1 , c 2 ] to represent incoming characters. Shown in Figure 3 , a multi-layer perceptron (MLP) is employed to derive a five-character window vector D C from single-character vector rep- For the latter, we follow recent work (Chen et al., 2015b;<cite> Zhang et al., 2016b)</cite> , using a bidirectional LSTM to encode input character sequence. 3 In particular, the bi-directional LSTM",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_9",
  "x": "Word. Similar to the character case, we investigate two different approaches to encoding incoming characters, namely a window approach and an LSTM approach. For the former, we follow prior methods (Zhang and Clark, 2007; Sun, 2010) , using the two-word window [w \u22122 , w \u22121 ] to represent recognized words. A hidden layer is employed to derive a two-word vector X W from single word embeddings e w (w \u22122 ) and e w (w \u22121 ). For the latter, we follow<cite> Zhang et al. (2016b)</cite> and Cai and Zhao (2016) , using an uni-directional LSTM on words that have been recognized. ---------------------------------- **PRETRAINING**",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_10",
  "x": "Pseudocode for the online learning algorithm is shown in Algorithm 1. We use Adagrad (Duchi et al., 2011) to optimize model parameters, with an initial learning rate \u03b1. L2 regularization and dropout (Srivastava et al., 2014) on input are used to reduce overfitting, with a L2 weight \u03bb and a dropout rate p. All the parameters in our model are randomly initialized to a value (\u2212r, r), where r = 6.0 f an in +f anout (Bengio, 2012). We fine-tune character and character bigram embeddings, but not word embeddings, acccording to<cite> Zhang et al. (2016b)</cite> . ---------------------------------- **EXPERIMENTS** ----------------------------------",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_11",
  "x": "Word Context. The influence of various word contexts are shown in Table 5 . Without using word information, our segmentor gives an F-score of 95.66% on the development data. Using a context of only w \u22121 (1-word window), the F-measure increases to 95.78%. This shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word-based segmentors <cite>(Zhang et al., 2016b</cite>; Cai and Zhao, 2016) . This is likely due to the difference in our neural network structures, and that we fine-tune both character and character bigram embeddings, which significantly enlarges the adjustable parameter space as compared with<cite> Zhang et al. (2016b)</cite> . The fact that word contexts can contribute relatively less than characters in a word is also not surprising in the sense that word-based neural segmentors do not outperform the best character-based models by large margins.",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_12",
  "x": "Word Context. The influence of various word contexts are shown in Table 5 . Without using word information, our segmentor gives an F-score of 95.66% on the development data. Using a context of only w \u22121 (1-word window), the F-measure increases to 95.78%. This shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word-based segmentors <cite>(Zhang et al., 2016b</cite>; Cai and Zhao, 2016) . This is likely due to the difference in our neural network structures, and that we fine-tune both character and character bigram embeddings, which significantly enlarges the adjustable parameter space as compared with<cite> Zhang et al. (2016b)</cite> . The fact that word contexts can contribute relatively less than characters in a word is also not surprising in the sense that word-based neural segmentors do not outperform the best character-based models by large margins.",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_13",
  "x": "With automatically-segmented data 6 , heterogenous segmentation and POS information, the F-score increases to 96.26%, 96.27% and 96.22%, respectively, showing the relevance of all information sources to neural segmentation, which is consistent with observations made for statistical word segmentation (Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2013) . Finally, by integrating all above information via multi-task learning, the F-score is further improved to 96.48%, with a 15.0% relative error reduction. Zhang et al. (2016b) Both our model and<cite> Zhang et al. (2016b)</cite> use global learning and beam search, but our network is different. Zhang et al. (2016b) utilizes the action history with LSTM encoder, while we use partial word rather than action information. Besides, the character and character bigram embeddings are fine-tuned in our model while<cite> Zhang et al. (2016b)</cite> set the embeddings fixed during training. We study the F-measure distribution with respect to sentence length on our baseline model, multitask pretraining model and<cite> Zhang et al. (2016b)</cite> . ----------------------------------",
  "y": "differences similarities"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_14",
  "x": "Besides, the character and character bigram embeddings are fine-tuned in our model while<cite> Zhang et al. (2016b)</cite> set the embeddings fixed during training. We study the F-measure distribution with respect to sentence length on our baseline model, multitask pretraining model and<cite> Zhang et al. (2016b)</cite> . ---------------------------------- **PRETRAINING RESULTS** ---------------------------------- **COMPARISION WITH** In particular, we cluster the sentences in the development dataset into 6 categories based on their length and evaluate their F1-values, respectively.",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_15",
  "x": "Besides, the character and character bigram embeddings are fine-tuned in our model while<cite> Zhang et al. (2016b)</cite> set the embeddings fixed during training. We study the F-measure distribution with respect to sentence length on our baseline model, multitask pretraining model and<cite> Zhang et al. (2016b)</cite> . ---------------------------------- **PRETRAINING RESULTS** ---------------------------------- **COMPARISION WITH** In particular, we cluster the sentences in the development dataset into 6 categories based on their length and evaluate their F1-values, respectively.",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_16",
  "x": "---------------------------------- **COMPARISION WITH** In particular, we cluster the sentences in the development dataset into 6 categories based on their length and evaluate their F1-values, respectively. As shown in Figure 5 , the models give different error distributions, with our models being more robust to the sentence length compared with<cite> Zhang et al. (2016b)</cite> . Their model is better on very short sentences, but worse on all other cases. This shows the relative advantages of our model. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_17",
  "x": "This shows the relative advantages of our model. ---------------------------------- **FINAL RESULTS** Our final results on CTB6 are shown in Table 7 , which lists the results of several current state-ofthe-art methods. Without multitask pretraining, our model gives an F-score of 95.44%, which is higher than the neural segmentor of<cite> Zhang et al. (2016b)</cite> , which gives the best accuracies among pure neural segments on this dataset. By using multitask pretraining, the result increases to 96.21%, with a relative error reduction of 16.9%. In comparison, Sun and Xu (2011) investigated heterogenous semi-supervised learning on a stateof-the-art statistical model, obtaining a relative error reduction of 13.8%.",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_18",
  "x": "In addition, it also outperforms the best neural models, in particular<cite> Zhang et al. (2016b)</cite> Table 7 : Main results on CTB6. ual discrete features into their word-based neural model. We achieve the best reported F-score on this dataset. To our knowledge, this is the first time a pure neural network model outperforms all existing methods on this dataset, allowing the use of external data 7 . We also evaluate our model pretrained only on punctuation and auto-segmented data, which do not include additional manual labels. The results on CTB test data show the accuracy of 95.8% and 95.7%, respectivley, which are comparable with those statistical semi-supervised methods (Sun and Xu, 2011; Wang et al., 2011) . They are also among the top performance methods in Table 7 .",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_19",
  "x": "Similar to Table 7 , our method gives the best accuracies on all corpora except for MSR, where it underperforms the hybrid model of<cite> Zhang et al. (2016b)</cite> by 0.2%. To our knowledge, we are the first to report results for a neural segmentor on more than 3 datasets, with competitive results consistently. It verifies that knowledge learned from a certain set of resources can be used to enhance cross-domain robustness in training a neural segmentor for different datasets, which is of practical importance. ---------------------------------- **CONCLUSION** We investigated rich external resources for enhancing neural word segmentation, by building a globally optimised beam-search model that leverages both character and word contexts. Taking each type of external resource as an auxiliary classification task, we use neural multi-task learning to pre-train a set of shared parameters for character contexts.",
  "y": "differences"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_0",
  "x": "While research in building automatically negotiating agents has primarily focused on agent-agent negotiations (Williams et al., 2012; Lin et al., 2014) , there is a recent interest in agent-human negotiations (Gratch et al., 2015) as well. Such agents may act as mediators or can be helpful for pedagogical purposes (Johnson et al., 2019) . Efforts in agent-human negotiations involving free-form natural language as a means of communication are rather sparse. Researchers<cite> (He et al., 2018)</cite> recently studied natural language negotiations in buyer-seller bargaining setup, which is comparatively less restricted than previously studied game environments (Asher et al., 2016; Lewis et al., 2017) . Lack of a well-defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario. Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work. We focus on buyer-seller negotiations<cite> (He et al., 2018)</cite> where two individuals negotiate the price of a given product.",
  "y": "background"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_1",
  "x": "While research in building automatically negotiating agents has primarily focused on agent-agent negotiations (Williams et al., 2012; Lin et al., 2014) , there is a recent interest in agent-human negotiations (Gratch et al., 2015) as well. Such agents may act as mediators or can be helpful for pedagogical purposes (Johnson et al., 2019) . Efforts in agent-human negotiations involving free-form natural language as a means of communication are rather sparse. Researchers<cite> (He et al., 2018)</cite> recently studied natural language negotiations in buyer-seller bargaining setup, which is comparatively less restricted than previously studied game environments (Asher et al., 2016; Lewis et al., 2017) . Lack of a well-defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario. Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work. We focus on buyer-seller negotiations<cite> (He et al., 2018)</cite> where two individuals negotiate the price of a given product.",
  "y": "motivation background"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_2",
  "x": "Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work. We focus on buyer-seller negotiations<cite> (He et al., 2018)</cite> where two individuals negotiate the price of a given product. Leveraging the recent advancements (Vaswani et al., 2017; Devlin et al., 2019) in pre-trained language encoders, we attempt to predict negotiation outcomes early on in the conversation, in a completely data-driven manner ( Figure  1 ). Early prediction of outcomes is essential for effective planning of an automatically negotiating agent. Although there have been attempts to gain insights into negotiations (Adair et al., 2001; Koit, 2018) , to the best of our knowledge, we are the first to study early natural language cues through a datadriven neural system (Section 3). Our evaluations show that natural language allows the models to make better predictions by looking at only a fraction of the negotiation. Rather than just realizing the strategy in natural language, our empirical results suggest that language can be crucial in the planning as well.",
  "y": "uses"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_3",
  "x": "Rather than just realizing the strategy in natural language, our empirical results suggest that language can be crucial in the planning as well. We provide a sample negotiation from the test set<cite> (He et al., 2018</cite> ) along with our model predictions in Table 1 . ---------------------------------- **PROBLEM SETUP** We study human-human negotiations in the buyerseller bargaining scenario, which has been a key research area in the literature (Williams et al., 2012) . In this section, we first describe our problem setup and key terminologies by discussing the dataset used. Later, we formalize our problem definition.",
  "y": "uses"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_4",
  "x": "We study human-human negotiations in the buyerseller bargaining scenario, which has been a key research area in the literature (Williams et al., 2012) . In this section, we first describe our problem setup and key terminologies by discussing the dataset used. Later, we formalize our problem definition. Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by<cite> He et al. (2018)</cite> . Instead of focusing on the previously studied game environments (Asher et al., 2016; Lewis et al., 2017) , the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist 1 . The dataset consists of 6682 dialogues between a buyer and a seller who converse in natural language to negotiate the price of a given product (sample in Table 1 ). In total, 1402 product ad postings were scraped from Craigslist, belonging to six categories: phones, bikes, housing, furniture, car and electronics.",
  "y": "uses"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_5",
  "x": "This number is lower than average for Housing, Bike and Car, resulting in relative better performance of Priceonly model for these categories over others. The models also show evidence of capturing buyer interest. By constructing artificial negotiations, we observe that the model predictions at f =0.2 increase when the buyer shows more interest in the product, indicating more willingness to pay. With the capability to incorporate cues from natural language, such a framework can be used in the future to get negotiation feedback, in order to guide the planning of a negotiating agent. This can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning (Lewis et al., 2017; <cite>He et al., 2018)</cite> . ---------------------------------- **CONCLUSION**",
  "y": "future_work"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_0",
  "x": "Much work that followed improved upon this strategy, by improving the features (Ng and Cardie, 2002b) , the type of classifier<cite> (Denis and Baldridge, 2007)</cite> , and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b) . This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. Ng and Cardie (2002a) and Ng (2004) highlight the problem of determining whether or not common noun phrases are anaphoric. They use two classifiers, an anaphoricity classifier, which decides if a mention should have an antecedent and a pairwise classifier similar those just discussed, which are combined in a cascaded manner. More recently,<cite> Denis and Baldridge (2007)</cite> utilized an integer linear programming (ILP) solver to better combine the decisions made by these two complementary classifiers, by finding the globally optimal solution according to both classifiers. However, when encoding constraints into their ILP solver, they did not enforce transitivity. The goal of the present work is simply to show that transitivity constraints are a useful source of information, which can and should be incorporated into an ILP-based coreference system.",
  "y": "background"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_1",
  "x": "Transitive closure in this model was done implicitly. If John Smith was labeled coreferent with Smith, and Smith with Jane Smith, then John Smith and Jane Smith were also coreferent regardless of the classifier's evaluation of that pair. Much work that followed improved upon this strategy, by improving the features (Ng and Cardie, 2002b) , the type of classifier<cite> (Denis and Baldridge, 2007)</cite> , and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b) . This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. Ng and Cardie (2002a) and Ng (2004) highlight the problem of determining whether or not common noun phrases are anaphoric. They use two classifiers, an anaphoricity classifier, which decides if a mention should have an antecedent and a pairwise classifier similar those just discussed, which are combined in a cascaded manner. More recently,<cite> Denis and Baldridge (2007)</cite> utilized an integer linear programming (ILP) solver to better combine the decisions made by these two complementary classifiers, by finding the globally optimal solution according to both classifiers.",
  "y": "background"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_2",
  "x": "**COREFERENCE RESOLUTION** For this task we are given a document which is annotated with a set of mentions, and the goal is to cluster the mentions which refer to the same entity. When describing our model, we build upon the notation used by<cite> Denis and Baldridge (2007)</cite> . ---------------------------------- **PAIRWISE CLASSIFICATION** Our baseline systems are based on a logistic classifier over pairs of mentions. The probability of a pair of mentions takes the standard logistic form:",
  "y": "uses"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_3",
  "x": "( 1) where m i and m j correspond to mentions i and j respectively; f (m i , m j ) is a feature function over a pair of mentions; \u03b8 are the feature weights we wish to learn; and x i,j is a boolean variable which takes value 1 if m i and m j are coreferent, and 0 if they are not. The log likelihood of a document is the sum of the log likelihoods of all pairs of mentions: (2) where m is the set of mentions in the document, and x is the set of variables representing each pairwise coreference decision x i,j . Note that this model is degenerate, because it assigns probability mass to nonsensical clusterings. Specifically, it will allow Prior work (Soon et al., 2001; <cite>Denis and Baldridge, 2007)</cite> has generated training data for pairwise classifiers in the following manner. For each mention, work backwards through the preceding mentions in the document until you come to a true coreferent mention. Create negative examples for all intermediate mentions, and a positive example for the mention and its correct antecedent.",
  "y": "background"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_4",
  "x": "For each mention, work backwards through the preceding mentions in the document until you come to a true coreferent mention. Create negative examples for all intermediate mentions, and a positive example for the mention and its correct antecedent. This approach made sense for Soon et al. (2001) because testing proceeded in a similar manner: for each mention, work backwards until you find a previous mention which the classifier thinks is coreferent, add a link, and terminate the search. The COREF-ILP model of<cite> Denis and Baldridge (2007)</cite> took a different approach at test time: for each mention they would work backwards and add a link for all previous mentions which the classifier deemed coreferent. This is equivalent to finding the most likely assignment to each x i,j in Equation 2. As noted, these assignments may not be a legal clustering because there is no guarantee of transitivity. The transitive closure happens in an ad-hoc manner after this assignment is found: any two mentions linked through other mentions are determined to be coreferent.",
  "y": "background"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_5",
  "x": "This is equivalent to finding the most likely assignment to each x i,j in Equation 2. As noted, these assignments may not be a legal clustering because there is no guarantee of transitivity. The transitive closure happens in an ad-hoc manner after this assignment is found: any two mentions linked through other mentions are determined to be coreferent. Our SOON-STYLE baseline used the same training and testing regimen as Soon et al. (2001) . Our D&B-STYLE baseline used the same test time method as<cite> Denis and Baldridge (2007)</cite> , however at training time we created data for all mention pairs. ---------------------------------- **INTEGER LINEAR PROGRAMMING TO ENFORCE TRANSITIVITY**",
  "y": "uses"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_6",
  "x": "**RESULTS** Our results are summarized in Table 1 . We show performance for both baseline classifiers, as well as our ILP-based classifier, which finds the most probable legal assignment to the variables representing coreference decisions over pairs of mentions. For comparison, we also give the results of the COREF-ILP system of<cite> Denis and Baldridge (2007)</cite> , which was also based on a na\u00efve pairwise classifier. They used an ILP solver to find an assignment for the variables, but as they note at the end of Section 5.1, it is equivalent to taking all links for which the classifier returns a probability \u2265 0.5, and so the ILP solver is not really necessary. We also include their JOINT-ILP numbers, however that system makes use of an additional anaphoricity classifier. For all three corpora, the ILP model beat both baselines for the cluster f-score, Rand index, and variation of information metrics.",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_0",
  "x": "---------------------------------- **INTRODUCTION** Lexical simplification is the task to find and substitute a complex word or phrase in a sentence with its simpler synonymous expression. We define complex word as a word that has lexical and subjective difficulty in a sentence. It can help in reading comprehension for children and language learners (De Belder and Moens, 2010) . This task is a rather easier task which prepare a pair of complex and simple representations than a challenging task which changes the substitute pair in a given context (Specia et al., 2012; <cite>Kajiwara and Yamamoto, 2015</cite>) . Construction of a benchmark dataset is important to ensure the reliability and reproducibility of evaluation.",
  "y": "uses differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_1",
  "x": "This task is a rather easier task which prepare a pair of complex and simple representations than a challenging task which changes the substitute pair in a given context (Specia et al., 2012; <cite>Kajiwara and Yamamoto, 2015</cite>) . Construction of a benchmark dataset is important to ensure the reliability and reproducibility of evaluation. However, few resources are available for the automatic evaluation of lexical simplification. Specia et al. (2012) and De Belder and Moens (2010) created benchmark datasets for evaluating English lexical simplification. In addition, Horn et al. (2014) extracted simplification candidates and constructed an evaluation dataset using English Wikipedia and Simple English Wikipedia. In contrast, such a parallel corpus does not exist in Japanese. <cite>Kajiwara and Yamamoto (2015)</cite> constructed an evaluation dataset for Japanese lexical simplification 1 in languages other than English.",
  "y": "background"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_2",
  "x": "<cite>Kajiwara and Yamamoto (2015)</cite> constructed an evaluation dataset for Japanese lexical simplification 1 in languages other than English. However, there are four drawbacks in the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> : (1) <cite>they</cite> extracted sentences only from a newswire corpus; (2) <cite>they</cite> substituted only a single target word; (3) <cite>they</cite> did not allow ties; and (4) <cite>they</cite> did not integrate simplification ranking considering the quality. Hence, we propose a new dataset addressing the problems in <cite>the dataset</cite> of <cite>Kajiwara and Yamamoto (2015)</cite> . The main contributions of our study are as follows: \u2022 It is the first controlled and balanced dataset for Japanese lexical simplification. We extract sentences from a balanced corpus and control sentences to have only one complex word. Experimental results show that our dataset is more suitable than previous datasets for evaluating systems with respect to correlation with human judgment.",
  "y": "motivation background"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_3",
  "x": "However, there are four drawbacks in the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> : (1) <cite>they</cite> extracted sentences only from a newswire corpus; (2) <cite>they</cite> substituted only a single target word; (3) <cite>they</cite> did not allow ties; and (4) <cite>they</cite> did not integrate simplification ranking considering the quality. Hence, we propose a new dataset addressing the problems in <cite>the dataset</cite> of <cite>Kajiwara and Yamamoto (2015)</cite> . The main contributions of our study are as follows: \u2022 It is the first controlled and balanced dataset for Japanese lexical simplification. We extract sentences from a balanced corpus and control sentences to have only one complex word. Experimental results show that our dataset is more suitable than previous datasets for evaluating systems with respect to correlation with human judgment. \u2022 The consistency of simplification ranking is greatly improved by allowing candidates to have ties and by considering the reliability of annotators.",
  "y": "extends"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_4",
  "x": "However, there are four drawbacks in the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> : (1) <cite>they</cite> extracted sentences only from a newswire corpus; (2) <cite>they</cite> substituted only a single target word; (3) <cite>they</cite> did not allow ties; and (4) <cite>they</cite> did not integrate simplification ranking considering the quality. Hence, we propose a new dataset addressing the problems in <cite>the dataset</cite> of <cite>Kajiwara and Yamamoto (2015)</cite> . The main contributions of our study are as follows: \u2022 It is the first controlled and balanced dataset for Japanese lexical simplification. We extract sentences from a balanced corpus and control sentences to have only one complex word. Experimental results show that our dataset is more suitable than previous datasets for evaluating systems with respect to correlation with human judgment. \u2022 The consistency of simplification ranking is greatly improved by allowing candidates to have ties and by considering the reliability of annotators.",
  "y": "motivation extends"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_5",
  "x": "**RELATED WORK** The evaluation dataset for the English Lexical Simplification task (Specia et al., 2012) Figure 1: A part of the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . notated on top of the evaluation dataset for English lexical substitution (McCarthy and Navigli, 2007) . They asked university students to rerank substitutes according to simplification ranking. Sentences in their dataset do not always contain complex words, and it is not appropriate to evaluate simplification systems if a test sentence does not include any complex words. In addition, De Belder and Moens (2012) built an evaluation dataset for English lexical simplification based on that developed by McCarthy and Navigli (2007) . They used Amazon's Mechanical Turk to rank substitutes and employed the reliability of annotators to remove outlier annotators and/or downweight unreliable annotators.",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_6",
  "x": "Again, it is not adequate for the automatic evaluation of lexical simplification because the human ranking of the resulting simplification might be affected by the context containing complex words. Furthermore, De Belder and Moens' (2012) dataset is too small to be used for achieving a reliable evaluation of lexical simplification systems. 3 Problems in previous datasets for Japanese lexical simplification <cite>Kajiwara and Yamamoto (2015)</cite> followed Specia et al. (2012) to construct an evaluation dataset for Japanese lexical simplification. Namely, <cite>they</cite> split the data creation process into two steps: substitute extraction and simplification ranking. During the substitute extraction task, <cite>they</cite> collected substitutes of each target word in 10 different contexts. These contexts were randomly selected from a newswire corpus. The target word was a content word (noun, verb, adjective, or adverb) , and was neither a simple word nor part of any compound words.",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_7",
  "x": "Furthermore, De Belder and Moens' (2012) dataset is too small to be used for achieving a reliable evaluation of lexical simplification systems. 3 Problems in previous datasets for Japanese lexical simplification <cite>Kajiwara and Yamamoto (2015)</cite> followed Specia et al. (2012) to construct an evaluation dataset for Japanese lexical simplification. Namely, <cite>they</cite> split the data creation process into two steps: substitute extraction and simplification ranking. During the substitute extraction task, <cite>they</cite> collected substitutes of each target word in 10 different contexts. These contexts were randomly selected from a newswire corpus. The target word was a content word (noun, verb, adjective, or adverb) , and was neither a simple word nor part of any compound words. <cite>They</cite> gathered substitutes from five annotators using crowdsourcing.",
  "y": "motivation"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_8",
  "x": "The target word was a content word (noun, verb, adjective, or adverb) , and was neither a simple word nor part of any compound words. <cite>They</cite> gathered substitutes from five annotators using crowdsourcing. These procedures were the same as for De Belder and Moens (2012) . During the simplification ranking task, annotators were asked to reorder the target word and its substitutes in a single order without allowing ties. <cite>They</cite> used crowdsourcing to find five annotators different from those who performed the substitute extraction task. Simplification ranking was integrated on the basis of the average of the simplification ranking from each annotator to generate a gold-standard ranking that might include ties. During the substitute extraction task, agreement among the annotators was 0.664, whereas during the simplification ranking task, Spearman's rank correlation coefficient score was 0.332.",
  "y": "background"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_9",
  "x": "The target word was a content word (noun, verb, adjective, or adverb) , and was neither a simple word nor part of any compound words. <cite>They</cite> gathered substitutes from five annotators using crowdsourcing. These procedures were the same as for De Belder and Moens (2012) . During the simplification ranking task, annotators were asked to reorder the target word and its substitutes in a single order without allowing ties. <cite>They</cite> used crowdsourcing to find five annotators different from those who performed the substitute extraction task. Simplification ranking was integrated on the basis of the average of the simplification ranking from each annotator to generate a gold-standard ranking that might include ties. During the substitute extraction task, agreement among the annotators was 0.664, whereas during the simplification ranking task, Spearman's rank correlation coefficient score was 0.332.",
  "y": "background"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_10",
  "x": "Thus, there was a big blur between annotators, and the simplification ranking collected using crowdsourcing tended to have a lower quality. Figure 1 shows a part of the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . Our discussion in this paper is based on this example. Domain of the dataset is limited. Because <cite>Kajiwara and Yamamoto (2015)</cite> extracted sentences from a newswire corpus, <cite>their</cite> dataset has a poor variety of expression. English lexical simplification datasets (Specia et al., 2012; De Belder and Moens, 2012) do not have this problem because both of them use a balanced corpus of English (Sharoff, 2006) . Complex words might exist in context.",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_11",
  "x": "Figure 1 shows a part of the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . Our discussion in this paper is based on this example. Domain of the dataset is limited. Because <cite>Kajiwara and Yamamoto (2015)</cite> extracted sentences from a newswire corpus, <cite>their</cite> dataset has a poor variety of expression. English lexical simplification datasets (Specia et al., 2012; De Belder and Moens, 2012) do not have this problem because both of them use a balanced corpus of English (Sharoff, 2006) . Complex words might exist in context. In Figure 1, even when a target word such as \" (feel exalted)\" is simplified, another complex word \" (skill)\" is left in a sentence.",
  "y": "motivation"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_12",
  "x": "Domain of the dataset is limited. Because <cite>Kajiwara and Yamamoto (2015)</cite> extracted sentences from a newswire corpus, <cite>their</cite> dataset has a poor variety of expression. English lexical simplification datasets (Specia et al., 2012; De Belder and Moens, 2012) do not have this problem because both of them use a balanced corpus of English (Sharoff, 2006) . Complex words might exist in context. In Figure 1, even when a target word such as \" (feel exalted)\" is simplified, another complex word \" (skill)\" is left in a sentence. Lexical simplification is a task of simplifying complex words in a sentence. Previous datasets may include multiple complex words in a sentence but target only one complex word.",
  "y": "motivation background"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_13",
  "x": "Previous datasets may include multiple complex words in a sentence but target only one complex word. Not only the target word but also other complex words should be considered as well, but annotation of substitutes and simplification ranking to all complex words in a sentence produces a huge number of patterns, therefore takes a very high cost of annotation. For example, when three complex words which have 10 substitutes each in a sentence, annotators should consider 10 3 patterns. Thus, it is desired that a sentence includes only simple words after the target word is substituted. Therefore, in this work, we extract sentences containing only one complex word. Ties are not permitted in simplification ranking. When each annotator assigns a simplification ranking to a substitution list, a tie cannot be assigned in previous datasets (Specia et al., 2012; <cite>Kajiwara and Yamamoto, 2015</cite>) .",
  "y": "differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_14",
  "x": "Ties are not permitted in simplification ranking. When each annotator assigns a simplification ranking to a substitution list, a tie cannot be assigned in previous datasets (Specia et al., 2012; <cite>Kajiwara and Yamamoto, 2015</cite>) . This deteriorates ranking consistency if some substitutes have a similar simplicity. De Belder and Moens (2012) allow ties in simplification ranking and report considerably higher agreement among annotators than Specia et al. (2012) . The method of ranking integration is na\u00efve. <cite>Kajiwara and Yamamoto (2015)</cite> and Specia et al. (2012) use an average score to integrate rankings, but it might be biased by outliers. De Belder and Moens (2012) report a slight increase in agreement by greedily removing annotators to maximize the agreement score.",
  "y": "motivation"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_15",
  "x": "**BALANCED DATASET FOR EVALUATION OF JAPANESE LEXICAL SIMPLIFICATION** We create a balanced dataset for the evaluation of Japanese lexical simplification. Figure 2 illustrates how we constructed the dataset. It follows the data creation procedure of <cite>Kajiwara and Yamamoto's (2015)</cite> dataset with improvements to resolve the problems described in Section 3. We use a crowdsourcing application, Lancers, 3 3 http://www.lancers.jp/ Figure 3 : Example of annotation of extracting substitutes. Annotators are provided with substitutes that preserve the meaning of target word which is shown bold in the sentence. In addition, annotators can write a substitute including particles.",
  "y": "extends uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_16",
  "x": "This method estimates the reliability of annotators in addition to determining the true order of rankings. We applied the reliability score to exclude extraordinary annotators. Table 1 shows the characteristics of our dataset. It is about the same size as previous work (Specia et al., 2012; <cite>Kajiwara and Yamamoto, 2015</cite>) . Our dataset has two advantages: (1) improved correlation with human judgment by making a controlled and balanced dataset, and (2) enhanced consistency by allowing ties in ranking and removing outlier annotators. In the following subsections, we evaluate our dataset in detail. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_17",
  "x": "In the following subsections, we evaluate our dataset in detail. ---------------------------------- **RESULT** ---------------------------------- **INTRINSIC EVALUATION** To evaluate the quality of the ranking integration, the Spearman rank correlation coefficient was calculated. The baseline integration ranking used an average score (<cite>Kajiwara and Yamamoto, 2015</cite>) .",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_18",
  "x": "On the contrary, Table 3 : Detail of sentences and substitutes in our dataset. (BCCWJ comprise three main subcorpora: publication (P), library (L), special-purpose (O). PB = book, PM = magazine, PN = newswire, LB = book, OW = white paper, OT = textbook, OP =PR paper, OB = bestselling books, OC = Yahoo! Answers, OY = Yahoo! Blogs, OL = Law, OM = Magazine) baseline outlier removal Average 0.541 0.580 Table 2 : Correlation of ranking integration. the Spearman rank correlation coefficient of the substitute ranking phase was 0.522. This score is higher than that from <cite>Kajiwara and Yamamoto (2015)</cite> by 0.190. This clearly shows the importance of allowing ties during the substitute ranking task. Table 2 shows the results of the ranking integration.",
  "y": "differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_19",
  "x": "(BCCWJ comprise three main subcorpora: publication (P), library (L), special-purpose (O). PB = book, PM = magazine, PN = newswire, LB = book, OW = white paper, OT = textbook, OP =PR paper, OB = bestselling books, OC = Yahoo! Answers, OY = Yahoo! Blogs, OL = Law, OM = Magazine) baseline outlier removal Average 0.541 0.580 Table 2 : Correlation of ranking integration. the Spearman rank correlation coefficient of the substitute ranking phase was 0.522. This score is higher than that from <cite>Kajiwara and Yamamoto (2015)</cite> by 0.190. This clearly shows the importance of allowing ties during the substitute ranking task. Table 2 shows the results of the ranking integration. Our method achieved better accuracy in ranking integration than previous methods (Specia et al., 2012; <cite>Kajiwara and Yamamoto, 2015</cite>) and is similar to the results from De Belder and Moens (2012) .",
  "y": "differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_20",
  "x": "Thus, it is necessary to consider annotation method. ---------------------------------- **EXTRINSIC EVALUATION** In this section, we evaluate our dataset using five simple lexical simplification methods. We calcu- late 1-best accuracy in our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . Annotated data is collected by our and <cite>Kajiwara and Yamamoto (2015)</cite>'s work in ranking substitutes task, and which size is 21,700 ((2010 + 2330) 5) rankings. Then, we calculate correlation between the accuracies of annotated data and either those of <cite>Kajiwara and Yamamoto (2015)</cite> or those of our dataset.",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_21",
  "x": "**EXTRINSIC EVALUATION** In this section, we evaluate our dataset using five simple lexical simplification methods. We calcu- late 1-best accuracy in our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . Annotated data is collected by our and <cite>Kajiwara and Yamamoto (2015)</cite>'s work in ranking substitutes task, and which size is 21,700 ((2010 + 2330) 5) rankings. Then, we calculate correlation between the accuracies of annotated data and either those of <cite>Kajiwara and Yamamoto (2015)</cite> or those of our dataset. ---------------------------------- **LEXICAL SIMPLIFICATION SYSTEMS**",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_22",
  "x": "---------------------------------- **EXTRINSIC EVALUATION** In this section, we evaluate our dataset using five simple lexical simplification methods. We calcu- late 1-best accuracy in our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . Annotated data is collected by our and <cite>Kajiwara and Yamamoto (2015)</cite>'s work in ranking substitutes task, and which size is 21,700 ((2010 + 2330) 5) rankings. Then, we calculate correlation between the accuracies of annotated data and either those of <cite>Kajiwara and Yamamoto (2015)</cite> or those of our dataset. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_23",
  "x": "**EVALUATION** We ranked substitutes according to the metrics, and calculated the 1-best accuracy for each target word. Finally, to compare two datasets, we used the Pearson product-moment correlation coefficient between our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> against the annotated data. Table 4 shows the result of this experiment. The Pearson coefficient shows that our dataset correlates with human annotation better than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> , possibly because we controlled each sentence to include only one complex word. Because our dataset is balanced, the accuracy of Web corpus-based metrics (Frequency and Number of Users) closer than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_24",
  "x": "**EVALUATION** We ranked substitutes according to the metrics, and calculated the 1-best accuracy for each target word. Finally, to compare two datasets, we used the Pearson product-moment correlation coefficient between our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> against the annotated data. Table 4 shows the result of this experiment. The Pearson coefficient shows that our dataset correlates with human annotation better than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> , possibly because we controlled each sentence to include only one complex word. Because our dataset is balanced, the accuracy of Web corpus-based metrics (Frequency and Number of Users) closer than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_25",
  "x": "We choose the lowest level word using levels of JLPT. These levels are a scale of one to five. ---------------------------------- **EVALUATION** We ranked substitutes according to the metrics, and calculated the 1-best accuracy for each target word. Finally, to compare two datasets, we used the Pearson product-moment correlation coefficient between our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> against the annotated data. Table 4 shows the result of this experiment.",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_26",
  "x": "These levels are a scale of one to five. ---------------------------------- **EVALUATION** We ranked substitutes according to the metrics, and calculated the 1-best accuracy for each target word. Finally, to compare two datasets, we used the Pearson product-moment correlation coefficient between our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> against the annotated data. Table 4 shows the result of this experiment. The Pearson coefficient shows that our dataset correlates with human annotation better than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> , possibly because we controlled each sentence to include only one complex word.",
  "y": "differences extends"
 },
 {
  "id": "2e636754342e9bb857068922519dbc_0",
  "x": "More specifically, these techniques enhanced NLP algorithms through the use of contextualized text embeddings at word, sentence, and paragraph levels (Mikolov et al., 2013; Le and Mikolov, 2014; Peters et al., 2017; <cite>Devlin et al., 2018</cite>; Logeswaran and Lee, 2018; Radford et al., 2018) . More recently, Jin and Szolovits (2018) components from PubMed abstracts. To our knowledge, that study was the first in which a deep learning framework was used to extract PIO elements from PubMed abstracts. In the present paper, we build a dataset of PIO elements by improving the methodology found in (Jin and Szolovits, 2018) . Furthermore, we built a multi-label PIO classifier, along with a boosting framework, based on the state of the art text embedding, BERT. This embedding model has been proven to offer a better contextualization compared to a bidirectional LSTM model<cite> (Devlin et al., 2018)</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "2e636754342e9bb857068922519dbc_1",
  "x": "To our knowledge, that study was the first in which a deep learning framework was used to extract PIO elements from PubMed abstracts. In the present paper, we build a dataset of PIO elements by improving the methodology found in (Jin and Szolovits, 2018) . Furthermore, we built a multi-label PIO classifier, along with a boosting framework, based on the state of the art text embedding, BERT. This embedding model has been proven to offer a better contextualization compared to a bidirectional LSTM model<cite> (Devlin et al., 2018)</cite> . ---------------------------------- **DATASETS** In this study, we introduce PICONET, a multilabel dataset consisting of sequences with labels Population/Problem (P), Intervention (I), and Outcome (O).",
  "y": "differences"
 },
 {
  "id": "2e636754342e9bb857068922519dbc_2",
  "x": "Furthermore, we cleaned the remaining data with various approaches including, but not limited to, language identification, removal of missing values, cleaning unicode characters, and filtering for sequences between 5 and 200 words, inclusive. ---------------------------------- **BERT-BASED CLASSIFICATION MODEL** ---------------------------------- **BACKGROUND** BERT (Bidirectional Encoder Representations from Transformers) is a deep bidirectional attention text embedding model. The idea behind this model is to pre-train a bidirectional representation by jointly conditioning on both left and right contexts in all layers using a transformer (Vaswani et al., 2017;<cite> Devlin et al., 2018)</cite> .",
  "y": "motivation background"
 },
 {
  "id": "2e636754342e9bb857068922519dbc_3",
  "x": "Since its release, BERT has been pre-trained on a multitude of corpora. In the following, we describe different BERT embedding versions used for our classification problem. The first version is based on the original BERT release<cite> (Devlin et al., 2018)</cite> . This model is pre-trained on the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia, text passages were extracted while lists were ignored. The second version is BioBERT (Lee et al., 2019) , which was trained on biomedical corpora: PubMed (4.5B words) and PMC (13.5B words). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "2e636754342e9bb857068922519dbc_4",
  "x": "The model has 12 attention layers and all texts are converted to lowercase by the tokenizer<cite> (Devlin et al., 2018)</cite> . The architecture of the model is illustrated in Figure 1 . Using this framework, we trained the model using the two pretrained embedding models described in the previous section. It is worth to mention that the embedding is contextualized during the training phase. For both models, the pretrained embedding layer is frozen during the first epoch (the embedding vectors are not updated). After the first epoch, the embedding layer is unfrozen and the vectors are fine-tuned for the classification task during training. The advantage of this approach is that few parameters need to be learned from scratch (Howard and Ruder, 2018; Radford et al., 2018;<cite> Devlin et al., 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_0",
  "x": "We also show that a smaller, distilled version of our model produces the best results on two of the three tasks while limiting computational cost. We make both models available to the research community at large. 1 ---------------------------------- **INTRODUCTION** In the past year, the field of Natural Language Processing (NLP) has seen the rise of pretrained language models such as as ELMo (Peters et al., 2018) , ULMFiT (Howard and Ruder, 2018) and <cite>BERT</cite> (<cite>Devlin et al., 2019</cite>) . <cite>These approaches</cite> train a deep-learning language model on large volumes of unlabeled text, which is subsequently fine-tuned for particular NLP tasks.",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_1",
  "x": "**INTRODUCTION** In the past year, the field of Natural Language Processing (NLP) has seen the rise of pretrained language models such as as ELMo (Peters et al., 2018) , ULMFiT (Howard and Ruder, 2018) and <cite>BERT</cite> (<cite>Devlin et al., 2019</cite>) . <cite>These approaches</cite> train a deep-learning language model on large volumes of unlabeled text, which is subsequently fine-tuned for particular NLP tasks. Applying <cite>these models</cite> to the General Language Understanding Evaluation (GLUE) benchmark introduced by Wang et al. (2018) has achieved the best performance to date on tasks ranging from sentiment classification to question answering (<cite>Devlin et al., 2019</cite>) . The benefit of <cite>these models</cite> has also been demonstrated in specialized NLP domains. BioBERT (Lee et al., 2019) , a version of <cite>BERT</cite> trained exclusively on biomedical text, was able to significantly increase performance on biomedical named entity recognition. Further refining this model on clinical text produced an increase in performance in medical natural language inference (Alsentzer et al. 2019) .",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_2",
  "x": "In the past year, the field of Natural Language Processing (NLP) has seen the rise of pretrained language models such as as ELMo (Peters et al., 2018) , ULMFiT (Howard and Ruder, 2018) and <cite>BERT</cite> (<cite>Devlin et al., 2019</cite>) . <cite>These approaches</cite> train a deep-learning language model on large volumes of unlabeled text, which is subsequently fine-tuned for particular NLP tasks. Applying <cite>these models</cite> to the General Language Understanding Evaluation (GLUE) benchmark introduced by Wang et al. (2018) has achieved the best performance to date on tasks ranging from sentiment classification to question answering (<cite>Devlin et al., 2019</cite>) . The benefit of <cite>these models</cite> has also been demonstrated in specialized NLP domains. BioBERT (Lee et al., 2019) , a version of <cite>BERT</cite> trained exclusively on biomedical text, was able to significantly increase performance on biomedical named entity recognition. Further refining this model on clinical text produced an increase in performance in medical natural language inference (Alsentzer et al. 2019) . While large pretrained models offer significantly increased performance, they come with their own constraints, as the number of parameters in the <cite>classic BERT-base model</cite> exceeds 100 million.",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_3",
  "x": "**INTRODUCTION** In the past year, the field of Natural Language Processing (NLP) has seen the rise of pretrained language models such as as ELMo (Peters et al., 2018) , ULMFiT (Howard and Ruder, 2018) and <cite>BERT</cite> (<cite>Devlin et al., 2019</cite>) . <cite>These approaches</cite> train a deep-learning language model on large volumes of unlabeled text, which is subsequently fine-tuned for particular NLP tasks. Applying <cite>these models</cite> to the General Language Understanding Evaluation (GLUE) benchmark introduced by Wang et al. (2018) has achieved the best performance to date on tasks ranging from sentiment classification to question answering (<cite>Devlin et al., 2019</cite>) . The benefit of <cite>these models</cite> has also been demonstrated in specialized NLP domains. BioBERT (Lee et al., 2019) , a version of <cite>BERT</cite> trained exclusively on biomedical text, was able to significantly increase performance on biomedical named entity recognition. Further refining this model on clinical text produced an increase in performance in medical natural language inference (Alsentzer et al. 2019) .",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_5",
  "x": "<cite>These approaches</cite> train a deep-learning language model on large volumes of unlabeled text, which is subsequently fine-tuned for particular NLP tasks. Applying <cite>these models</cite> to the General Language Understanding Evaluation (GLUE) benchmark introduced by Wang et al. (2018) has achieved the best performance to date on tasks ranging from sentiment classification to question answering (<cite>Devlin et al., 2019</cite>) . The benefit of <cite>these models</cite> has also been demonstrated in specialized NLP domains. BioBERT (Lee et al., 2019) , a version of <cite>BERT</cite> trained exclusively on biomedical text, was able to significantly increase performance on biomedical named entity recognition. Further refining this model on clinical text produced an increase in performance in medical natural language inference (Alsentzer et al. 2019) . While large pretrained models offer significantly increased performance, they come with their own constraints, as the number of parameters in the <cite>classic BERT-base model</cite> exceeds 100 million. As such, <cite>their</cite> computational cost can thus be prohibitively high at both training and prediction time (<cite>Devlin et al., 2019</cite>) .",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_6",
  "x": "BioBERT (Lee et al., 2019) , a version of <cite>BERT</cite> trained exclusively on biomedical text, was able to significantly increase performance on biomedical named entity recognition. Further refining this model on clinical text produced an increase in performance in medical natural language inference (Alsentzer et al. 2019) . While large pretrained models offer significantly increased performance, they come with their own constraints, as the number of parameters in the <cite>classic BERT-base model</cite> exceeds 100 million. As such, <cite>their</cite> computational cost can thus be prohibitively high at both training and prediction time (<cite>Devlin et al., 2019</cite>) . More recent work has addressed this challenge by 'distilling' the models, training smaller versions of <cite>BERT</cite> which reduce the number of parameters to train by 40% while retaining more than 95% of the full model performance and even outperforming it on two out of eleven GLUE tasks . This paper shows that using pretrained models in learning analytics holds great potential for advancing the field. We apply the <cite>BERT</cite> approach to the following three previously explored LAK tasks on MOOC forum data (Wei et al., 2017) : Confusion detection, urgency of teacher intervention and sentimentality classification.",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_7",
  "x": "Further refining this model on clinical text produced an increase in performance in medical natural language inference (Alsentzer et al. 2019) . While large pretrained models offer significantly increased performance, they come with their own constraints, as the number of parameters in the <cite>classic BERT-base model</cite> exceeds 100 million. As such, <cite>their</cite> computational cost can thus be prohibitively high at both training and prediction time (<cite>Devlin et al., 2019</cite>) . More recent work has addressed this challenge by 'distilling' the models, training smaller versions of <cite>BERT</cite> which reduce the number of parameters to train by 40% while retaining more than 95% of the full model performance and even outperforming it on two out of eleven GLUE tasks . This paper shows that using pretrained models in learning analytics holds great potential for advancing the field. We apply the <cite>BERT</cite> approach to the following three previously explored LAK tasks on MOOC forum data (Wei et al., 2017) : Confusion detection, urgency of teacher intervention and sentimentality classification. In all three of these tasks, we are able to improve performance past the state of the art.",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_8",
  "x": "While large pretrained models offer significantly increased performance, they come with their own constraints, as the number of parameters in the <cite>classic BERT-base model</cite> exceeds 100 million. As such, <cite>their</cite> computational cost can thus be prohibitively high at both training and prediction time (<cite>Devlin et al., 2019</cite>) . More recent work has addressed this challenge by 'distilling' the models, training smaller versions of <cite>BERT</cite> which reduce the number of parameters to train by 40% while retaining more than 95% of the full model performance and even outperforming it on two out of eleven GLUE tasks . This paper shows that using pretrained models in learning analytics holds great potential for advancing the field. We apply the <cite>BERT</cite> approach to the following three previously explored LAK tasks on MOOC forum data (Wei et al., 2017) : Confusion detection, urgency of teacher intervention and sentimentality classification. In all three of these tasks, we are able to improve performance past the state of the art. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_9",
  "x": "In total, this dataset is comprised of more than 12 million tokens. The data used for the classification tasks was from the same Stanford MOOCPosts dataset. The posts are annotated by domain experts and given scores for sentiment (the degree of emotionality exhibited by the post), confusion expressed by the student and urgency for the post to receive a response from an instructor. Scores are given on a Likert scale from 1 (low) to 7 (high). Language Models: We constructed two models, EduBERT and EduDistilBERT, which respectively refine <cite>BERT-base</cite> and DistilBERT , <cite>both of which</cite> were trained on general domain text from books and Wikipedia (<cite>Devlin et al., 2019</cite>) . Both models are initialized from their <cite>base model</cite> and finetuned on educational data, using the Transformers library . The fine-tuning step allows the model to better capture how words are used in an educational context.",
  "y": "extends"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_10",
  "x": "In total, this dataset is comprised of more than 12 million tokens. The data used for the classification tasks was from the same Stanford MOOCPosts dataset. The posts are annotated by domain experts and given scores for sentiment (the degree of emotionality exhibited by the post), confusion expressed by the student and urgency for the post to receive a response from an instructor. Scores are given on a Likert scale from 1 (low) to 7 (high). Language Models: We constructed two models, EduBERT and EduDistilBERT, which respectively refine <cite>BERT-base</cite> and DistilBERT , <cite>both of which</cite> were trained on general domain text from books and Wikipedia (<cite>Devlin et al., 2019</cite>) . Both models are initialized from their <cite>base model</cite> and finetuned on educational data, using the Transformers library . The fine-tuning step allows the model to better capture how words are used in an educational context.",
  "y": "extends"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_11",
  "x": "Following previous work by Guo et al. (2019) , we split the data into a 2/3 training set and 1/3 test set and consider a post to express sentiment, urgency or confusion if and only if its respective score is \u2265 4. We compare between the four classifiers <cite>BERT-base</cite>, DistilBERT, EduBERT and EduDistilBERT. We evaluated multiple sets of parameters. Best results for these tasks were achieved with the following parameters: two learning epochs, maximal sequence length of 300 (<cite>BERT-base</cite>, EDUBERT) and 512 for the distilled models, all other parameter values were equal to the ones used for pre-training. Table 1 compares EduBERT, EduDistilBERT to <cite>their base versions</cite>, as well as the state-of-the-art (SoA) for urgency detection (Guo et al. 2019) . The table shows that all pretraining approaches outperformed the SoA for F1 and weighted F1 measures, with our distilled model EduDistilBERT achieving the best overall performance. Table 2 compares all of the <cite>models</cite> for all three tasks to the SoA using the same measures of accuracy as Wei et al. (2017) .",
  "y": "uses"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_12",
  "x": "We compare between the four classifiers <cite>BERT-base</cite>, DistilBERT, EduBERT and EduDistilBERT. We evaluated multiple sets of parameters. Best results for these tasks were achieved with the following parameters: two learning epochs, maximal sequence length of 300 (<cite>BERT-base</cite>, EDUBERT) and 512 for the distilled models, all other parameter values were equal to the ones used for pre-training. Table 1 compares EduBERT, EduDistilBERT to <cite>their base versions</cite>, as well as the state-of-the-art (SoA) for urgency detection (Guo et al. 2019) . The table shows that all pretraining approaches outperformed the SoA for F1 and weighted F1 measures, with our distilled model EduDistilBERT achieving the best overall performance. Table 2 compares all of the <cite>models</cite> for all three tasks to the SoA using the same measures of accuracy as Wei et al. (2017) . Here too, all the pretraining approaches outperform the SoA. EduDistilBERT obtains the best results on both urgency and confusion prediction while EduBERT performs the best for sentimentality classification.",
  "y": "uses"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_13",
  "x": "We evaluated multiple sets of parameters. Best results for these tasks were achieved with the following parameters: two learning epochs, maximal sequence length of 300 (<cite>BERT-base</cite>, EDUBERT) and 512 for the distilled models, all other parameter values were equal to the ones used for pre-training. Table 1 compares EduBERT, EduDistilBERT to <cite>their base versions</cite>, as well as the state-of-the-art (SoA) for urgency detection (Guo et al. 2019) . The table shows that all pretraining approaches outperformed the SoA for F1 and weighted F1 measures, with our distilled model EduDistilBERT achieving the best overall performance. Table 2 compares all of the <cite>models</cite> for all three tasks to the SoA using the same measures of accuracy as Wei et al. (2017) . Here too, all the pretraining approaches outperform the SoA. EduDistilBERT obtains the best results on both urgency and confusion prediction while EduBERT performs the best for sentimentality classification. However, EduDistilBERT has a lower memory footprint and is noticeably faster at inference time, allowing for a 30% speedup.",
  "y": "uses"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_14",
  "x": "We evaluated multiple sets of parameters. Best results for these tasks were achieved with the following parameters: two learning epochs, maximal sequence length of 300 (<cite>BERT-base</cite>, EDUBERT) and 512 for the distilled models, all other parameter values were equal to the ones used for pre-training. Table 1 compares EduBERT, EduDistilBERT to <cite>their base versions</cite>, as well as the state-of-the-art (SoA) for urgency detection (Guo et al. 2019) . The table shows that all pretraining approaches outperformed the SoA for F1 and weighted F1 measures, with our distilled model EduDistilBERT achieving the best overall performance. Table 2 compares all of the <cite>models</cite> for all three tasks to the SoA using the same measures of accuracy as Wei et al. (2017) . Here too, all the pretraining approaches outperform the SoA. EduDistilBERT obtains the best results on both urgency and confusion prediction while EduBERT performs the best for sentimentality classification. However, EduDistilBERT has a lower memory footprint and is noticeably faster at inference time, allowing for a 30% speedup.",
  "y": "uses"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_15",
  "x": "Table 2 compares all of the <cite>models</cite> for all three tasks to the SoA using the same measures of accuracy as Wei et al. (2017) . Here too, all the pretraining approaches outperform the SoA. EduDistilBERT obtains the best results on both urgency and confusion prediction while EduBERT performs the best for sentimentality classification. However, EduDistilBERT has a lower memory footprint and is noticeably faster at inference time, allowing for a 30% speedup. ---------------------------------- **RESULTS & DISCUSSIONS** . EduBERT and EduDistilBERT are fine-tuned on millions of tokens, in contrast to the billions of tokens required to make the most of the architecture potential (<cite>Devlin et al., 2019</cite>) . We are actively seeking more data to train models even more capable of producing contextualized word representations in the educational domain.",
  "y": "background"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_0",
  "x": "---------------------------------- **INTRODUCTION** With the availability and abundance of linguistic data that captures different avenues of human social interactions, there is an unprecedented opportunity to expand NLP to not only understand language, but also to understand the people who speak it and the social relations between them. Social power structures are ubiquitous in human interactions, and since power is often reflected through language, computational research at the intersection of language and power has gained interest recently. This research has been applied to a wide array of domains such as Wikipedia talk pages (Strzalkowski et al., 2010; Taylor et al., 2012; Danescu-Niculescu-Mizil et al., 2012; Swayamdipta and Rambow, 2012) , blogs (Rosenthal, 2014) as well as workplace interactions <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012; Prabhakaran, 2015) . The corporate environment is one social context in which power dynamics have a clearly defined structure and shape the interactions between individuals, making it an interesting case study on how language and power interact. Organizations stand to benefit greatly from being able to detect power dynamics within their internal interactions, in order to address disparities and ensure inclusive and productive workplaces.",
  "y": "motivation background"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_1",
  "x": "Prior work has investigated the use of NLP techniques to study manifestations of different types of power using the Enron email corpus (Diesner et al., 2005; Prabhakaran et al., 2012; Prabhakaran and Rambow, 2013; Prabhakaran and Rambow, 2014) . While early work <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) focused on surface level lexical features aggregated at corpus level, more recent work has looked into the thread structure of emails as well (Prabhakaran and Rambow, 2014) . However, both <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) and (Prabhakaran and Rambow, 2014 ) group all messages sent by an individual to another individual (at the corpus-level and at the thread-level, respectively) and rely on word-ngram * Authors (listed in alphabetical order) contributed equally. ---------------------------------- **ARXIV:1807.06557V1 [CS.CL] 17 JUL 2018** based features extracted from this concatenated text to infer power relations. They ignore the fact that the text comes from separate emails, and that there is a sequential order to them.",
  "y": "background"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_2",
  "x": "However, both <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) and (Prabhakaran and Rambow, 2014 ) group all messages sent by an individual to another individual (at the corpus-level and at the thread-level, respectively) and rely on word-ngram * Authors (listed in alphabetical order) contributed equally. ---------------------------------- **ARXIV:1807.06557V1 [CS.CL] 17 JUL 2018** based features extracted from this concatenated text to infer power relations. They ignore the fact that the text comes from separate emails, and that there is a sequential order to them. In this paper, we propose a hierarchical deep learning architecture for power prediction, using a combination of Convolutional Neural Networks (CNN) to capture linguistic manifestations of power in individual emails, and a Long Short-Term Memory (LSTM) that aggregates their outputs in an order-preserving fashion. We obtain significant improvements in accuracy on the corpus-level task (82.4% over 70.0%) and on the thread-level task (80.4% over 73.0%) over prior state-of-the-art techniques.",
  "y": "background"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_3",
  "x": "As in (Prabhakaran and Rambow, 2014) , we exclude pairs of employees who are peers, and we use the same train-dev-test splits so our results are comparable. Grouped: Here, we group all emails A sent to B across all threads in the corpus, and vice versa, and use these sets of emails to predict the power relation between A and B. This formulation is similar those in <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) , but our results are not directly comparable since, unlike them, we rely on the ground truth of power relations from (Agarwal et al., 2012) ; however, we created an SVM model that uses word-ngram features similar to theirs as a baseline to our proposed neural architectures. ---------------------------------- **METHODS** The inputs to our models take on two forms: Lexical features: We represent each email as a series of tokenized words, each of which is represented by a 100-dimensional GloVe vector pre-trained on Wikipedia and Gigaword (Pennington et al., 2014) . We cap the email length at a maximum of 200 words. Non-lexical features: We incorporate the structural non-lexical features identified as significant by Prabhakaran and Rambow (2014) for the Grouped problem formulation.",
  "y": "similarities extends differences"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_4",
  "x": "**EXPERIMENTS AND RESULTS** We use support vector machine (SVM) based approaches as our baseline, since they are the state-of-the art in this problem (Prabhakaran and Rambow, 2014;<cite> Bramsen et al., 2011</cite>; Gilbert, 2012) . We use the performance reported by (Prabhakaran and Rambow, 2014) using SVM as baseline for the Per-Thread formulation (using the same train-dev-test splits) and implemented an SVM baseline for the Grouped formulation (not directly comparable to performance reported by <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) ). For each of our neural net models, we trained for 30-70 epochs until the performance on the development set stopped improving, in order to avoid overfitting. We used Hyperas to tune hyperparameters on our development dataset for the same set of parameter options for each task formulation, varying activation functions, hidden layer size, batch size, dropout, number of filters, and number of words to include per email. Table 2 presents the accuracy obtained using different models. All of our models significantly outperformed the SVM baseline in both task formulations.",
  "y": "uses background"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_5",
  "x": "We use the performance reported by (Prabhakaran and Rambow, 2014) using SVM as baseline for the Per-Thread formulation (using the same train-dev-test splits) and implemented an SVM baseline for the Grouped formulation (not directly comparable to performance reported by <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) ). For each of our neural net models, we trained for 30-70 epochs until the performance on the development set stopped improving, in order to avoid overfitting. We used Hyperas to tune hyperparameters on our development dataset for the same set of parameter options for each task formulation, varying activation functions, hidden layer size, batch size, dropout, number of filters, and number of words to include per email. Table 2 presents the accuracy obtained using different models. All of our models significantly outperformed the SVM baseline in both task formulations. In the Per-Thread formulation, we obtained the best accuracy of 80.4% using the Sequential-CNN-LSTM approach, compared to the 73.0% reported by (Prabhakaran and Rambow, 2014) . This is also a marked improvement over the simpler Batched-CNN and Separated-CNN models.",
  "y": "differences extends"
 },
 {
  "id": "310272015a781b05c42015c0559b18_0",
  "x": "**ABSTRACT** Several computational simulations of how children solve the word segmentation problem have been proposed, but most have been applied only to a limited number of languages. One model with some experimental support uses distributional statistics of sound sequence predictability (Saffran et al. 1996) . However, the experimental design does not fully specify how predictability is best measured or modeled in a simulation. Saffran et al. (1996) assume transitional probability, but<cite> Brent (1999a)</cite> claims mutual information (MI) is more appropriate. Both assume predictability is measured locally, relative to neighboring segment-pairs. This paper replicates<cite> Brent's (1999a)</cite> mutualinformation model on a corpus of childdirected speech in Modern Greek, and introduces a variant model using a global threshold.",
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_1",
  "x": "This paper replicates<cite> Brent's (1999a)</cite> mutualinformation model on a corpus of childdirected speech in Modern Greek, and introduces a variant model using a global threshold. Brent's finding regarding the superiority of MI is confirmed; the relative performance of local comparisons and global thresholds depends on the evaluation metric. ---------------------------------- **INTRODUCTION** A substantial portion of research in child language acquisition focuses on the word segmentation problem-how children learn to extract words (or word candidates) from a continuous speech signal prior to having acquired a substantial vocabulary. While a number of robust strategies have been proposed and tested for infants learning English and a few other languages (discussed in Section 1.1), it is not clear whether or how these apply to all or most languages. In addition, experiments on infants often leave undetermined many details of how particular cues are actually used.",
  "y": "differences similarities background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_2",
  "x": "However, what is not in doubt is that infants are sensitive to the cues in question, and that this sensitivity begins well before the infant has acquired a large vocabulary. ---------------------------------- **IMPLEMENTATIONS AND AMBIGUITIES** While the infant studies discussed above focus primarily on the properties of particular cues, computational studies of word-segmentation must also choose between various implementations, which further complicates comparisons. Several models (e.g., Batchelder, 2002;<cite> Brent's (1999a)</cite> MBDP-1 model; Davis, 2000; de Marcken, 1996; Olivier, 1968) simultaneously address the question of vocabulary acquisition, using previously learned word-candidates to bootstrap later segmentations. (It is beyond the scope of this paper to discuss these in detail; see Brent 1999a,b for a review.) Other models do not accumulate a stored vocabulary, but instead rely on the degree of predictability of the next syllable (e.g., Saffran et al., 1996) or segment (e.g., Christiansen et al., 1998) .",
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_3",
  "x": [
   "Other models do not accumulate a stored vocabulary, but instead rely on the degree of predictability of the next syllable (e.g., Saffran et al., 1996) or segment (e.g., Christiansen et al., 1998) . The intuition here, first articulated by Harris (1954) , is that word boundaries are marked by a spike in unpredictability of the following phoneme. The results from Saffran et al. (1996) show that English-learning infants do respond to areas of unpredictability; however, it is not clear from the experiment how this unpredictability is best measured. Two specific ambiguities in measuring (un)predictability are examined here. Brent (1999a) points out one type of ambiguity, namely that Saffran and colleagues' (1996) results can be modeled as favoring word-breaks at points of either low transitional probability or low mutual information. Brent reports results for models relying on each of these measures. It should be noted that these models are not the main focus of his paper, but provided for illustrative purposes; nevertheless, these models provide the best comparison to Saffran and colleagues' experiment, and may be regarded as an implementation of the same."
  ],
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_4",
  "x": [
   "The results from Saffran et al. (1996) show that English-learning infants do respond to areas of unpredictability; however, it is not clear from the experiment how this unpredictability is best measured. Two specific ambiguities in measuring (un)predictability are examined here. Brent (1999a) points out one type of ambiguity, namely that Saffran and colleagues' (1996) results can be modeled as favoring word-breaks at points of either low transitional probability or low mutual information. Brent reports results for models relying on each of these measures. It should be noted that these models are not the main focus of his paper, but provided for illustrative purposes; nevertheless, these models provide the best comparison to Saffran and colleagues' experiment, and may be regarded as an implementation of the same. Brent (1999a) compares these two models in terms of word tokens correctly segmented (see Section 3 for exact criteria), reporting approximately 40% precision and 45% recall for transitional probability (TP) and 50% precision and 53% recall for mutual information (MI) on the first 1000 utterances of his corpus (with improvements given larger corpora). Indeed, their performance on word tokens is surpassed only by Brent's main model (MBDP-1), which seems to have about 73% precision and 67% recall for the same range."
  ],
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_5",
  "x": "Another question which Saffran et al. (1996) leave unanswered is whether the segmentation depends on local or global comparisons of predictability. Saffran et al. assume implicitly, and<cite> Brent (1999a)</cite> explicitly, that the proper comparison is local-in Brent, dependent solely on the adjacent pairs of segments. However, predictability measures for segmental bigrams (whether TP or MI) may be compared in any number of ways. One straightforward alternative to the local comparison is to compare the predictability measures compare to some global threshold. Indeed, and Christiansen et al. (1998) simply assumed the mean activation level as a global activation threshold within their neural network framework. ---------------------------------- **GLOBAL AND LOCAL COMPARISONS**",
  "y": "motivation background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_6",
  "x": "---------------------------------- **GLOBAL AND LOCAL COMPARISONS** The global comparison, taken on its own, seems a rather simplistic and inflexible heuristic: for any pair of phonemes xy, either a word boundary is always hypothesized between x and y, or it never is. Clearly, there are many cases where x and y sometimes straddle a word boundary and sometimes do not. The heuristic also takes no account of lengths of possible words. However, the local comparison may take length into account too much, disallowing words of certain lengths. In order to see that, we must examine<cite> Brent's (1999a)</cite> suggested implementation of Saffran et al. (1996) more closely.",
  "y": "motivation background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_7",
  "x": "In order to see that, we must examine<cite> Brent's (1999a)</cite> suggested implementation of Saffran et al. (1996) more closely. In the local comparison, given some string \u2026wxyz\u2026, in order for a word boundary to be inserted between x and y, the predictability measure for xy must be lower than both that of wx and of yz. It follows that neither wx nor yz can have word boundaries between them, since they cannot simultaneously have a lower predictability measure than xy. This means that, within an utterance, word boundaries must have at least two segments between them, so this heuristic will not correctly segment utterance-internal one-phoneme words. 3 Granted, only a few one-phoneme word types exist in either English or Greek (or other languages). However, these words are often function words and so are less likely to appear at edges of utterances (e.g., ends of utterances for articles and prepositions; beginnings for postposed elements). Neither<cite> Brent's (1999a)</cite> implementation of Saffran's et al. (1996) heuristic nor utterance-boundary heuristic can explain how these might be learned.",
  "y": "motivation background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_8",
  "x": "In the local comparison, given some string \u2026wxyz\u2026, in order for a word boundary to be inserted between x and y, the predictability measure for xy must be lower than both that of wx and of yz. It follows that neither wx nor yz can have word boundaries between them, since they cannot simultaneously have a lower predictability measure than xy. This means that, within an utterance, word boundaries must have at least two segments between them, so this heuristic will not correctly segment utterance-internal one-phoneme words. 3 Granted, only a few one-phoneme word types exist in either English or Greek (or other languages). However, these words are often function words and so are less likely to appear at edges of utterances (e.g., ends of utterances for articles and prepositions; beginnings for postposed elements). Neither<cite> Brent's (1999a)</cite> implementation of Saffran's et al. (1996) heuristic nor utterance-boundary heuristic can explain how these might be learned. Brent (1999a) himself points out another lengthrelated limitation-namely, the relative difficulty that the 'local comparison' heuristic has in segmenting learning longer words.",
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_9",
  "x": [
   "Longer words cannot be memorized in this representation (although common ends of words such as prefixes and suffixes might be). In order to test for this, Brent proposes that precision for word types (which he calls \"lexicon precision\") be measured as well as for word tokens. While the word-token metric emphasizes the correct segmentation of frequent words, the word-type metric does not share this bias. Brent defines this metric as follows: \"After each block [of 500 utterances], each word type that the algorithm produced was labeled a true positive if that word type had occurred anywhere in the portion of the corpus processed so far; otherwise it is labeled a false positive. \" Measured this way, MI yields a word type precision of only about 27%; transitional probability yields a precision of approximately 24% for the first 1000 utterances, compared to 42% for MBDP-1. He does not measure word type recall. This same limitation in finding longer, less frequent types may apply to comparisons against a global threshold as well. This is also in need of testing. It seems that both global and local comparisons, used on their own as sole or decisive heuristics, may have serious limitations. It is not clear a priori which limitation is most serious; hence both comparisons are tested here."
  ],
  "y": "motivation background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_10",
  "x": "---------------------------------- **CONSTRUCTING A FINITE-STATE MODEL** ---------------------------------- **OUTLINE OF CURRENT RESEARCH** While in its general approach the study reported here replicates the mutual-information and transitional-probability models in<cite> Brent (1999a)</cite> , it differs slightly in the details of their use. First, whereas Brent dynamically updated his measures over a single corpus, and thus blurred the line between training and testing data, our model precompiles statistics for each distinct bigram-type offline, over a separate training corpus. 4 Secondly, we compare the use of a global threshold (described in more detail in Section 2.3, below) to<cite> Brent's (1999a)</cite> use of the local context (as described in Section 1.3 above).",
  "y": "differences background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_11",
  "x": "First, whereas Brent dynamically updated his measures over a single corpus, and thus blurred the line between training and testing data, our model precompiles statistics for each distinct bigram-type offline, over a separate training corpus. 4 Secondly, we compare the use of a global threshold (described in more detail in Section 2.3, below) to<cite> Brent's (1999a)</cite> use of the local context (as described in Section 1.3 above). Like <cite>(Brent, 1999a)</cite> , but unlike Saffran et al. (1996) , our model focuses on pairs of segments, not on pairs of syllables. While Modern Greek syllabic structure is not as complicated as English's, it is still more complicated than the CV structure assumed in Saffran et al. (1996) ; hence, access to syllabification cannot be assumed. ---------------------------------- **CORPUS DATA** In addition to the technical differences discussed above, this replication breaks new ground in terms of the language from which the training and test corpora are drawn.",
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_12",
  "x": "**OUTLINE OF CURRENT RESEARCH** While in its general approach the study reported here replicates the mutual-information and transitional-probability models in<cite> Brent (1999a)</cite> , it differs slightly in the details of their use. First, whereas Brent dynamically updated his measures over a single corpus, and thus blurred the line between training and testing data, our model precompiles statistics for each distinct bigram-type offline, over a separate training corpus. 4 Secondly, we compare the use of a global threshold (described in more detail in Section 2.3, below) to<cite> Brent's (1999a)</cite> use of the local context (as described in Section 1.3 above). Like <cite>(Brent, 1999a)</cite> , but unlike Saffran et al. (1996) , our model focuses on pairs of segments, not on pairs of syllables. While Modern Greek syllabic structure is not as complicated as English's, it is still more complicated than the CV structure assumed in Saffran et al. (1996) ; hence, access to syllabification cannot be assumed. ----------------------------------",
  "y": "similarities background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_13",
  "x": "While such an implementation is not technically necessary here, one advantage of the finite-state framework is the compositionality of finite state machines, which allows for later composition of this approach with other heuristics depending on other cues, analogous to Christiansen et al. (1998) . Since the finite-state framework selects the best path over the whole utterance, it also allows for optimization over a sequence of decisions, rather than optimizing each local decision separately. 6 Unlike Belz (1998) , where the actual FSM structure (including classes of phonemes that could be group onto one arc) was learned, here the structure of each FSM is determined in advance. Only the weight on each arc is derived from data. No attempt is made to combine phonemes to produce more minimal FSMs; each phoneme (and phoneme-pair) is modeled separately. Like<cite> Brent (1999a)</cite> and indeed most models in the literature, this model assumes (for sake of convenience and simplicity) that the child hears each segment produced within an utterance without error. This assumption translates into the finitestate domain as a simple acceptor (or equivalently, an identity transducer) over the segment sequence for a given utterance.",
  "y": "background uses motivation"
 },
 {
  "id": "310272015a781b05c42015c0559b18_14",
  "x": "This boundary measure may be more conservative than that reported by other authors, but is easily convertible into other metrics. The second metric, the percentage of word tokens detected, is the same as<cite> Brent (1999a)</cite> . In order for a word to be counted as correctly found, three conditions must be met: (a) the word's beginning (left boundary) is correctly detected, (b) the word's ending (right boundary) is correctly detected, and (c) these two are consecutive (i.e., no false boundaries are posited within the word). The last metric (word type) is slightly more conservative than<cite> Brent's (1999a)</cite> in that the word type must have been actually spoken in the same utterance (not the same block of 500 utterances) in which it was detected to count as a match. This lessens the possibility that a mismatch that happens to be segmentally identical to an actual word (but whose semantic context may not be conducive to learning its correct meaning) is counted as a match. However, this situation is presumably rather rare. Tables 2 and 3 present the results over the test set for both the global and the local comparisons of the predictability statistics proposed by Saffran et al. (1996) and<cite> Brent (1999a)</cite> .",
  "y": "similarities"
 },
 {
  "id": "310272015a781b05c42015c0559b18_15",
  "x": "In order for a word to be counted as correctly found, three conditions must be met: (a) the word's beginning (left boundary) is correctly detected, (b) the word's ending (right boundary) is correctly detected, and (c) these two are consecutive (i.e., no false boundaries are posited within the word). The last metric (word type) is slightly more conservative than<cite> Brent's (1999a)</cite> in that the word type must have been actually spoken in the same utterance (not the same block of 500 utterances) in which it was detected to count as a match. This lessens the possibility that a mismatch that happens to be segmentally identical to an actual word (but whose semantic context may not be conducive to learning its correct meaning) is counted as a match. However, this situation is presumably rather rare. Tables 2 and 3 present the results over the test set for both the global and the local comparisons of the predictability statistics proposed by Saffran et al. (1996) and<cite> Brent (1999a)</cite> . ---------------------------------- **COMPARING THE FOUR VARIANTS**",
  "y": "extends motivation"
 },
 {
  "id": "310272015a781b05c42015c0559b18_16",
  "x": "However, this situation is presumably rather rare. Tables 2 and 3 present the results over the test set for both the global and the local comparisons of the predictability statistics proposed by Saffran et al. (1996) and<cite> Brent (1999a)</cite> . ---------------------------------- **COMPARING THE FOUR VARIANTS** The findings here confirm<cite> Brent's (1999a)</cite> contention that mutual information is a better measure of predictability than is transitional probability-at least for the task of identifying words, not just boundaries. This is particularly true in the global comparison. Transitional probability finds more word boundaries in the 'local comparison' model, but this does not carry over to the task of pulling out the word themselves, which is arguably the infant's main concern.",
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_17",
  "x": "The last metric (word type) is slightly more conservative than<cite> Brent's (1999a)</cite> in that the word type must have been actually spoken in the same utterance (not the same block of 500 utterances) in which it was detected to count as a match. This lessens the possibility that a mismatch that happens to be segmentally identical to an actual word (but whose semantic context may not be conducive to learning its correct meaning) is counted as a match. However, this situation is presumably rather rare. Tables 2 and 3 present the results over the test set for both the global and the local comparisons of the predictability statistics proposed by Saffran et al. (1996) and<cite> Brent (1999a)</cite> . ---------------------------------- **COMPARING THE FOUR VARIANTS** The findings here confirm<cite> Brent's (1999a)</cite> contention that mutual information is a better measure of predictability than is transitional probability-at least for the task of identifying words, not just boundaries.",
  "y": "similarities"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_0",
  "x": "The present work differs from Galley et al. (2003) in two respects, viz. we focus solely on textual information and we directly address the problem of tutorial dialogue. In this study we apply the methods of Foltz et al. (1998) ,<cite> Hearst (1994</cite> Hearst ( , 1997 , and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue. All three are vector space methods that measure lexical cohesion to determine topic shifts. Our results show that the new using an orthonormal basis significantly outperforms the other methods. Section 2 reviews previous work, and Section 3 reviews the vector space model. Section 4 introduces an extension of the vector space model which uses an orthonormal basis. Section 5 outlines the task domain of tutorial dialogue, and Section 6 presents the results of previous and the current method on this task domain.",
  "y": "uses"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_1",
  "x": "Section 8 concludes. ---------------------------------- **PREVIOUS WORK** Though the idea of using lexical cohesion to segment text has the advantages of simplicity and intuitive appeal, it lacks a unique implementation. An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment. Both<cite> Hearst (1994</cite> Hearst ( , 1997 and Foltz et al. (1998) use vector space methods discussed below to represent and compare units of text. The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.",
  "y": "background"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_2",
  "x": "An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment. Both<cite> Hearst (1994</cite> Hearst ( , 1997 and Foltz et al. (1998) use vector space methods discussed below to represent and compare units of text. The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text. However,<cite> Hearst (1994</cite> Hearst ( , 1997 and Foltz et al. (1998) differ on how text units are defined and on how to interpret the results of a comparison. The text unit's definition in<cite> Hearst (1994</cite> Hearst ( , 1997 and Foltz et al. (1998) is generally task dependent, depending on what size gives the best results. For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level. However, when using LSA to segment text, Foltz et al. (1998) use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion.",
  "y": "background"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_3",
  "x": "The text unit's definition in<cite> Hearst (1994</cite> Hearst ( , 1997 and Foltz et al. (1998) is generally task dependent, depending on what size gives the best results. For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level. However, when using LSA to segment text, Foltz et al. (1998) use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion. Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens <cite>(Hearst, 1994)</cite> , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size. Under a vector space model, comparisons are performed by calculating the cosine of vectors representing text. As stated previously, these comparisons reflect the cohesion between units of text. In order to use these comparisons to segment text, however, one must have a criterion in place.",
  "y": "background"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_4",
  "x": "The text unit's definition in<cite> Hearst (1994</cite> Hearst ( , 1997 and Foltz et al. (1998) is generally task dependent, depending on what size gives the best results. For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level. However, when using LSA to segment text, Foltz et al. (1998) use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion. Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens <cite>(Hearst, 1994)</cite> , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size. Under a vector space model, comparisons are performed by calculating the cosine of vectors representing text. As stated previously, these comparisons reflect the cohesion between units of text. In order to use these comparisons to segment text, however, one must have a criterion in place.",
  "y": "background"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_5",
  "x": "To replicate Foltz et al. (1998) , software was written in Java that created a moving window of varying sizes on the input text, and the software retrieved the LSA vector and calculated the cosine of each window. Hearst (1994 Hearst ( , 1997 was replicated using the JTextTile (Choi, 1999 ) Java software. A variant of<cite> Hearst (1994</cite> Hearst ( , 1997 was created by using LSA instead of the standard vector space method. The orthonormal basis method also used a moving window; however, in contrast to the previous methods, the window is not treated just as a large block of text. Instead, the window consists of two orthonormal bases, one on either side of an utterance. That is, a region of utterances above the test utterance is projected, utterance by utterance, into an orthonormal basis, and likewise a region of utterances below the test utterance is projected into another orthonormal basis. Then the test utterance is projected into each orthonormal basis, yielding measures of \"relevance\" and \"informativity\" with respect to each.",
  "y": "differences"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_6",
  "x": "The logistic equation of best fit is: cosine) (-13.345 1.887 odds) ln(shift \u22c5 + = F-measure of .49 is 48% higher than the Fmeasure reported by Foltz et al. (1998) for segmenting monologue. On the testing corpus the Fmeasure is .52, which demonstrates good generalization for the logistic equation given. Compared the F-measure of .33 reported by Foltz et al. (1998) , the current result is 58% higher. ---------------------------------- **HEARST (1994, 1997)** The JTextTile software was used to implement<cite> Hearst (1994)</cite> on dialogue. As with Foltz et al. (1998) , a text unit and window size had to be determined for dialogue.",
  "y": "uses"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_7",
  "x": "The JTextTile software was used to implement<cite> Hearst (1994)</cite> on dialogue. As with Foltz et al. (1998) , a text unit and window size had to be determined for dialogue. Hearst (1994) recommends using the average paragraph size as the window size. Using the development corpus's average topic length of 16 utterances as a reference point, F-measures were calculated for the combinations of window size and text unit size in Table 1 . The optimal combination of parameters (Fmeasure = .17) is a unit size of 16 words and a window size of 16 units. This combination matches<cite> Hearst (1994)</cite> 's heuristic of choosing the window size to be the average paragraph length. On the test set, this combination of parameters yielded an F-measure of .14 as opposed to the Fmeasure for monologue reported by Hearst (1997) , .70.",
  "y": "uses"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_9",
  "x": "While this result is not statistically significant, the direction of the result supports the use of an \"inferencing\" vector space method for segmenting dialogue. Unfortunately, the large difference in F-measure between the Foltz algorithm and the Hearst + LSA algorithm is more difficult to explain. These two methods differ by their segmentation criterion and by their training (Foltz is a regression model and Hearst is not). It may be that<cite> Hearst (1994</cite> Hearst ( , 1997 )'s segmentation criterion, i.e. depth scores, do not translate well to dialogue. Perhaps the assignment of segment boundaries based on the relative difference between a candidate score and its surrounding peaks is highly sensitive to cohesion gaps created by conversational implicatures. On the other hand the differences between these two methods may be entirely attributable to the amount of training they received. One way to separate the contributions of the segmentation criterion and training would be to create a logistic model using the Hearst + LSA method and to compare this to Foltz.",
  "y": "differences"
 },
 {
  "id": "318487ac270ca272ec11a3de6c0685_0",
  "x": "However, factoring the term-context matrix means throwing away a considerable amount of information, as the original matrix of size M \u00d7 N (number of instances by number of features) is factored into two smaller matrices of size M \u00d7 K and N \u00d7 K, with K M, N . If the factorization does not take into account labeled data about semantic similarity, important information can be lost. In this paper, we show how labeled data can considerably improve distributional methods for measuring semantic similarity. First, we develop a new discriminative term-weighting metric called TF-KLD, which is applied to the term-context matrix before factorization. On a standard paraphrase identification task (Dolan et al., 2004) , this method improves on both traditional TF-IDF and Weighted Textual Matrix Factorization (WTMF;<cite> Guo and Diab, 2012)</cite> . Next, we convert the latent representations of each sentence pair into a feature vector, which is used as input to a linear SVM classifier. This yields further improvements and substantially outperforms the current state-of-the-art on paraphrase classification.",
  "y": "differences"
 },
 {
  "id": "318487ac270ca272ec11a3de6c0685_2",
  "x": "The diagonal line running through the middle of the plot indicates zero KL-divergence, so features on this line will be ignored. 1 unigram recall 2 unigram precision 3 bigram recall 4 bigram precision 5 dependency relation recall 6 dependency relation precision 7 BLEU recall 8 BLEU precision 9 Difference of sentence length 10 Tree-editing distance Table 1 : Fine-grained features for paraphrase classification, selected from prior work (Wan et al., 2006) . ---------------------------------- **SUPERVISED CLASSIFICATION** While previous work has performed paraphrase classification using distance or similarity in the latent space<cite> (Guo and Diab, 2012</cite>; Socher et al., 2011) , more direct supervision can be applied. Specifically, we convert the latent representations of a pair of sentences v 1 and v 2 into a sample vector, concatenating the element-wise sum v 1 + v 2 and ab-",
  "y": "background"
 },
 {
  "id": "318487ac270ca272ec11a3de6c0685_4",
  "x": "**SIMILARITY-BASED CLASSIFICATION** In the first experiment, we predict whether a pair of sentences is a paraphrase by measuring their cosine similarity in latent space, using a threshold for the classification boundary. As in prior work<cite> (Guo and Diab, 2012)</cite> , the threshold is tuned on held-out training data. We consider two distributional feature sets: FEAT 1 , which includes unigrams; and FEAT 2 , which also includes bigrams and unlabeled dependency pairs obtained from MaltParser (Nivre et al., 2007) . To compare with<cite> Guo and Diab (2012)</cite> , we set the latent dimensionality to K = 100, which was the same in their paper. Both SVD and NMF factorization are evaluated; in both cases, we minimize the Frobenius norm of the reconstruction error. Table 2 compares the accuracy of a number of different configurations.",
  "y": "similarities uses"
 },
 {
  "id": "318487ac270ca272ec11a3de6c0685_5",
  "x": "To our knowledge, the current state-of-theart is a supervised system that combines several machine translation metrics (Madnani et al., 2012 ), but we also compare with state-of-the-art unsupervised matrix factorization work<cite> (Guo and Diab, 2012)</cite> . ---------------------------------- **SIMILARITY-BASED CLASSIFICATION** In the first experiment, we predict whether a pair of sentences is a paraphrase by measuring their cosine similarity in latent space, using a threshold for the classification boundary. As in prior work<cite> (Guo and Diab, 2012)</cite> , the threshold is tuned on held-out training data. We consider two distributional feature sets: FEAT 1 , which includes unigrams; and FEAT 2 , which also includes bigrams and unlabeled dependency pairs obtained from MaltParser (Nivre et al., 2007) . To compare with<cite> Guo and Diab (2012)</cite> , we set the latent dimensionality to K = 100, which was the same in their paper.",
  "y": "similarities uses"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_0",
  "x": "---------------------------------- **INTRODUCTION** Translation from speech utterances is a challenging problem that has been studied both under statistical, symbolic approaches (Ney, 1999; Casacuberta et al., 2004; Kumar et al., 2015) and more recently using neural models <cite>(Sperber et al., 2017)</cite> . Most previous work rely on pipeline approaches, using the output of a speech recognition system (ASR) as an input to a machine translation (MT) one. These inputs can be simply the 1-best sentence returned by the ASR system or a more structured representation such as a lattice. Some recent work on end-to-end systems bypass the need for intermediate representations, with impressive results (Weiss et al., 2017) . However, such a scenario has drawbacks.",
  "y": "background"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_1",
  "x": "From a theoretical perspective, intermediate representations such as lattices can be enriched through external, textual resources such as monolingual corpora or dictionaries. <cite>Sperber et al. (2017)</cite> proposes a lattice-tosequence model which, in theory, can address both problems above. However, <cite>their model</cite> suffers from training speed performance due to the lack of efficient batching procedures and they rely on transcriptions for pretraining. In this work, we address these two problems by applying lattice transformations and graph networks as encoders. More specifically, we enrich the lattices by applying subword segmentation using byte-pair encoding (Sennrich et al., 2016, BPE) and perform a minimisation step to remove redundant nodes arising from this procedure. Together with the standard batching strategies provided by graph networks, we are able to decrease training time by two orders of magnitude, enabling us to match their translation performance under the same training speed constraints without relying on gold transcriptions. ----------------------------------",
  "y": "background"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_2",
  "x": "From a practical perspective, it requires access to the original speech utterances and transcriptions, which can be unrealistic if a user needs to employ an out-ofthe-box ASR system. From a theoretical perspective, intermediate representations such as lattices can be enriched through external, textual resources such as monolingual corpora or dictionaries. <cite>Sperber et al. (2017)</cite> proposes a lattice-tosequence model which, in theory, can address both problems above. However, <cite>their model</cite> suffers from training speed performance due to the lack of efficient batching procedures and they rely on transcriptions for pretraining. In this work, we address these two problems by applying lattice transformations and graph networks as encoders. More specifically, we enrich the lattices by applying subword segmentation using byte-pair encoding (Sennrich et al., 2016, BPE) and perform a minimisation step to remove redundant nodes arising from this procedure. Together with the standard batching strategies provided by graph networks, we are able to decrease training time by two orders of magnitude, enabling us to match their translation performance under the same training speed constraints without relying on gold transcriptions.",
  "y": "motivation"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_3",
  "x": "1 This procedure is also done in <cite>Sperber et al. (2017)</cite> . The final step adds reverse and self-loop edges to the lattice, where these new edges have specific parameters in the encoder. This eases propagation of information and is standard practice when using graph networks as encoders Bastings et al., 2017; Beck et al., 2018) . We show an example of all the transformation steps on Figure 1 . In Figure 2 we show the architecture of our system, using the final lattice from Figure 1 as an example. Nodes are represented as embeddings that are updated according to the lattice structure, resulting in a set of hidden states as the output. Other components follow a standard seq2seq model, using a bilinear attention module (Luong et al., 2015) and a 2-layer LSTM (Hochreiter and Schmidhuber, 1997) as the decoder.",
  "y": "similarities"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_4",
  "x": "We use the original release by Post et al. (2013) , containing both 1-best and pruned lattice outputs from an ASR system for each Spanish utterance. 2 The Fisher corpus contain 150K instances and we use the original splits provided with the datasets. Following previous work (Post et al., 2013; <cite>Sperber et al., 2017)</cite> , we lowercase and remove punctuation from the English translations. To build the BPE models, we extract the vocabulary from the Spanish training lattices, using 8K split operations. Models and Evaluation All our models are trained on the Fisher training set. For the 1-best baseline we use a standard seq2seq architecture and for the GGNN models, we use the same setup as Beck et al. (2018) . Our implementation is based on the Sockeye toolkit (Hieber et al., 2017) and we use default values for most hyperparameters, except for batch size (16) and GGNN layers (8). 3 For regularisation, we apply 0.5 dropout on the input embeddings and perform early stopping on the corresponding Fisher dev set.",
  "y": "uses"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_5",
  "x": "Each model is trained using 5 different seeds and we report BLEU (Papineni et al., 2001) results using the median performance according to the dev set and an ensemble of the 5 models. For the word-based models, we remove any tokens with frequency lower than 2 (as in <cite>Sperber et al. (2017)</cite> ), while for subword models we do not perform any threshold pruning. We report all results on the Fisher \"dev2\" set. 4 ---------------------------------- **OUT-OF-THE-BOX ASR SCENARIO** In this scenario we assume only lattices and 1-best outputs are available, simulating a setting where we do not have access to the transcriptions.",
  "y": "uses"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_6",
  "x": "We also slightly outperform <cite>Sperber et al. (2017)</cite> in the setting where they ignore lattice scores, as in our approach. Most importantly, we are able to reach those results while being two orders of magnitude faster at training time: <cite>Sperber et al. (2017)</cite> report taking 1.5 days for each epoch while our architecture can process each epoch in 15min. The reason is because <cite>their model</cite> relies on the CPU while our GGNN-based model can be easily batched and computed in a GPU. Given those differences in training time, it is worth mentioning that the best model in <cite>Sperber et al. (2017)</cite> is surpassed by our best ensemble using lattices only. This means that we can obtain state-of-the-art performance even in an out-of-thebox scenario, under the same training speed constraints. While there are other constraints that may be considered (such as parameter budget), we nevertheless believe this is an encouraging result for real world scenarios. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_7",
  "x": "The results shown in Table 2 are consistent with previous work: adding transcriptions further enhance the system performance. We also slightly outperform <cite>Sperber et al. (2017)</cite> in the setting where they ignore lattice scores, as in our approach. Most importantly, we are able to reach those results while being two orders of magnitude faster at training time: <cite>Sperber et al. (2017)</cite> report taking 1.5 days for each epoch while our architecture can process each epoch in 15min. The reason is because <cite>their model</cite> relies on the CPU while our GGNN-based model can be easily batched and computed in a GPU. Given those differences in training time, it is worth mentioning that the best model in <cite>Sperber et al. (2017)</cite> is surpassed by our best ensemble using lattices only. This means that we can obtain state-of-the-art performance even in an out-of-thebox scenario, under the same training speed constraints. While there are other constraints that may be considered (such as parameter budget), we nevertheless believe this is an encouraging result for real world scenarios.",
  "y": "differences"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_8",
  "x": "The reason is because <cite>their model</cite> relies on the CPU while our GGNN-based model can be easily batched and computed in a GPU. Given those differences in training time, it is worth mentioning that the best model in <cite>Sperber et al. (2017)</cite> is surpassed by our best ensemble using lattices only. This means that we can obtain state-of-the-art performance even in an out-of-thebox scenario, under the same training speed constraints. While there are other constraints that may be considered (such as parameter budget), we nevertheless believe this is an encouraging result for real world scenarios. ---------------------------------- **ADDING LATTICE SCORES** Our approach is not without limitations.",
  "y": "differences"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_9",
  "x": "With this, we can simply take the union of transcriptions and lattices into a single training set. We keep the dev and test sets with lattices only, as this emulates test time conditions. The results shown in Table 2 are consistent with previous work: adding transcriptions further enhance the system performance. We also slightly outperform <cite>Sperber et al. (2017)</cite> in the setting where they ignore lattice scores, as in our approach. Most importantly, we are able to reach those results while being two orders of magnitude faster at training time: <cite>Sperber et al. (2017)</cite> report taking 1.5 days for each epoch while our architecture can process each epoch in 15min. The reason is because <cite>their model</cite> relies on the CPU while our GGNN-based model can be easily batched and computed in a GPU. Given those differences in training time, it is worth mentioning that the best model in <cite>Sperber et al. (2017)</cite> is surpassed by our best ensemble using lattices only.",
  "y": "differences"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_10",
  "x": "**ADDING LATTICE SCORES** Our approach is not without limitations. In particular, the GGNN encoder ignores lattice scores, which can help the model disambiguate between different paths in the lattice. As a simple first approach to incorporate scores, we embed them using a multilayer perceptron, using the score as the input. This however did not produce good results: performance dropped to 32.9 BLEU in the single model setting and 38.4 for the ensemble. It is worth noticing that <cite>Sperber et al. (2017)</cite> has a more principled approach to incorporate scores: by modifying the attention module. This is arguably a better choice, since the scores can directly inform the decoder about the ambiguity in the lattice.",
  "y": "differences"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_11",
  "x": "**CONCLUSIONS AND FUTURE WORK** In this work we proposed an architecture for lattice-to-string translation by treating lattices as general graphs and leveraging on recent advances in neural networks for graphs. 5 Compared to previous similar work, our model permits easy minibatching and allows one to freely enrich the lattices with additional information, which we exploit by incorporating BPE segmentation and lattice minimisation. We show promising results and outperform baselines in speech translation, particularly in out-of-the-box ASR scenarios, when one has no access to transcriptions. For future work, we plan to investigate better approaches to incorporate scores in the lattices. The approaches used by <cite>Sperber et al. (2017)</cite> can provide a starting point in this direction. The same minimisation procedures we employ can be adapted to weighted lattices (Eisner, 2003 ).",
  "y": "uses"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_12",
  "x": "In this work we proposed an architecture for lattice-to-string translation by treating lattices as general graphs and leveraging on recent advances in neural networks for graphs. 5 Compared to previous similar work, our model permits easy minibatching and allows one to freely enrich the lattices with additional information, which we exploit by incorporating BPE segmentation and lattice minimisation. We show promising results and outperform baselines in speech translation, particularly in out-of-the-box ASR scenarios, when one has no access to transcriptions. For future work, we plan to investigate better approaches to incorporate scores in the lattices. The approaches used by <cite>Sperber et al. (2017)</cite> can provide a starting point in this direction. The same minimisation procedures we employ can be adapted to weighted lattices (Eisner, 2003 ). Another important avenue is to explore this approach in low-resource scenarios such as ones involving endangered languages (Adams et al., 2017; Anastasopoulos and Chiang, 2018) .",
  "y": "future_work"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_0",
  "x": "We experiment with four languages from the Uto-Aztecan family. Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages. ---------------------------------- **INTRODUCTION** Computational morphology of polysynthetic languages is an emerging field of research. Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations (Homola, 2011; Mager et al., 2018d; Klavans, 2018a) . Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017) , hybrid models (Mager et al., 2018b; Moeller et al., 2018) , and supervised machine learning, particularly deep learning approaches (Micher, 2017; <cite>Kann et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_1",
  "x": "Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017) , hybrid models (Mager et al., 2018b; Moeller et al., 2018) , and supervised machine learning, particularly deep learning approaches (Micher, 2017; <cite>Kann et al., 2018)</cite> . While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009 ), or Arapaho (Littell, 2018 Moeller et al., 2018) ), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages. We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (Johnson et al., 2007) . We experiment with four UtoAztecan languages: Mexicanero (MX), Nahuatl (NH), Wixarika (WX) and Yorem Nokki (YN)<cite> (Kann et al., 2018)</cite> . Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016 Eskander et al., , 2018 . Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal. We use the datasets introduced by<cite> Kann et al. (2018)</cite> in an unsupervised fashion (unsegmented words).",
  "y": "background"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_2",
  "x": "We use the datasets introduced by<cite> Kann et al. (2018)</cite> in an unsupervised fashion (unsegmented words). We design several AG learning setups: 1) use the best-on-average AG setup from Eskander et al. (2016) ; 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from<cite> Kann et al. (2018)</cite> ; 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3). We show that the AG-based approaches outperform other unsupervised methods -M orf essor (Creutz and Lagus, 2007) and M orphoChain (Narasimhan et al., 2015) ) -, and that for two of the languages (NH and YN), the best AG-based approaches outperform the best supervised methods (Section 4). ---------------------------------- **LANGUAGES AND DATASETS** Typically, polysynthetic languages demonstrate holophrasis, i.e. the ability of an entire sentence to be expressed as what is considered by native speakers to be just one word. To illustrate, consider the following example from Inuktitut (Kla-vans, 2018b) , where the morpheme -tusaa-is the root and all the other morphemes are synthetically combined with it in one unit:",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_3",
  "x": "We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (Johnson et al., 2007) . We experiment with four UtoAztecan languages: Mexicanero (MX), Nahuatl (NH), Wixarika (WX) and Yorem Nokki (YN)<cite> (Kann et al., 2018)</cite> . Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016 Eskander et al., , 2018 . Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal. We use the datasets introduced by<cite> Kann et al. (2018)</cite> in an unsupervised fashion (unsegmented words). We design several AG learning setups: 1) use the best-on-average AG setup from Eskander et al. (2016) ; 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from<cite> Kann et al. (2018)</cite> ; 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3). We show that the AG-based approaches outperform other unsupervised methods -M orf essor (Creutz and Lagus, 2007) and M orphoChain (Narasimhan et al., 2015) ) -, and that for two of the languages (NH and YN), the best AG-based approaches outperform the best supervised methods (Section 4).",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_4",
  "x": "Thus, morphological analysis of polysynthetic languages is challenging due to the rootmorpheme complexity and to word class gradations. Linguists recognize a gradience in word classes, known as \"squishiness\", a term first discussed in Ross (1972) who argued that, instead of a fixed, distinct inventory of syntactic categories, a quasi-continuum from verb, adjective and noun best reflects most lexical distinctions. The rootmorpheme complexity and the word class \"squish\" makes developing segmented training data with reliability across annotators difficult to achieve. Kann et al. (2018) have made a first step by releasing a small set of morphologically segmented datasets although even in these carefully curated datasets, the distinction between affix and clitic is not always indicated. We use these datasets in an unsupervised fashion (i.e., we use the unsegmented words). These datasets were taken from detailed descriptions in the Archive of Indigenous Languages collection for MX (Canger, 2001 ), NH (de Su\u00e1rez, 1980) , WX (G\u00f3mez and L\u00f3pez, 1999) , and YN (Freeze, 1989) . They were constructed so they include both segmentable as well as non-<cite> Kann et al. (2018)</cite> , for training we do not use the segmented version of the data (our approach is unsupervised).",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_5",
  "x": "We use these datasets in an unsupervised fashion (i.e., we use the unsegmented words). These datasets were taken from detailed descriptions in the Archive of Indigenous Languages collection for MX (Canger, 2001 ), NH (de Su\u00e1rez, 1980) , WX (G\u00f3mez and L\u00f3pez, 1999) , and YN (Freeze, 1989) . They were constructed so they include both segmentable as well as non-<cite> Kann et al. (2018)</cite> , for training we do not use the segmented version of the data (our approach is unsupervised). In addition to the datasets, for NH and WX we also have available the Bible (Christodouloupoulos and Steedman, 2015; Mager et al., 2018a ), which we consider for one of our experimental setups as additional training data. In the dataset from<cite> (Kann et al., 2018)</cite> , the maximum number of morphemes per word for MX is seven with an average of 2.13; for NH, six with an average of 2.2; for WX, maximum of ten with an average of 3.3; and for YN, the maximum is ten, with an average of 2.13. ---------------------------------- **USING ADAPTOR GRAMMARS FOR POLYSYNTHETIC LANGUAGES**",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_6",
  "x": "They were constructed so they include both segmentable as well as non-<cite> Kann et al. (2018)</cite> , for training we do not use the segmented version of the data (our approach is unsupervised). In addition to the datasets, for NH and WX we also have available the Bible (Christodouloupoulos and Steedman, 2015; Mager et al., 2018a ), which we consider for one of our experimental setups as additional training data. In the dataset from<cite> (Kann et al., 2018)</cite> , the maximum number of morphemes per word for MX is seven with an average of 2.13; for NH, six with an average of 2.2; for WX, maximum of ten with an average of 3.3; and for YN, the maximum is ten, with an average of 2.13. ---------------------------------- **USING ADAPTOR GRAMMARS FOR POLYSYNTHETIC LANGUAGES** An Adaptor Grammar is typically composed of a PCFG and an adaptor that adapts the probabilities of individual subtrees. For morphological segmentation, a PCFG is a morphological grammar that specifies word structure, where AGs learn latent tree structures given a list of words.",
  "y": "background"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_7",
  "x": "Best AG Configuration per Language. In this experimental setup, we consider all nine grammars from Eskander et al. (2016) using both the Standard and the Cascaded approaches and choosing the one that is best for each polysynthetic language by training on the training set and evaluating on the development set. We denote this system as AG BestL . Using Seeded Knowledge. To approximate the effect of Scholar-seeded-Knowledge in Eskander et al. (2016), we used the training set to derive affixes and use them as scholar-seeded knowledge added to the grammars (before the learning happens). However, since affixes and stems are not distinguished in the training annotations from<cite> Kann et al. (2018)</cite> , we only consider the first and last morphemes that appear at least five times. We call this setup AG Scholar BestL .",
  "y": "motivation"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_8",
  "x": "Since the vocabulary in<cite> Kann et al. (2018)</cite> for each language is small, and the languages are from the same language family, one data augmentation approach is to train on all languages and test then on each language individually. We call this setup AG M ulti . Data Augmentation. In this setup, we examine the performance of the best AG configuration per language (AG BestL ) when more data is available. We merge the training corpus with unique words in the New Testament of the Bible (train Bible ). We run this only on NH and WX since the Bible text is only available for these two languages. We denote this setup as AG Aug .",
  "y": "motivation"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_9",
  "x": "---------------------------------- **EVALUATION AND DISCUSSION** We evaluate the different AG setups on the blind test set from<cite> Kann et al. (2018)</cite> and compare our AG approaches to state-of-the-art unsupervised systems as well as supervised models including the best supervised deep learning models from<cite> Kann et al. (2018)</cite> . As the metric, we use the segmentation-boundary F1-score, which is standard for this task (Virpioja et al., 2011) . Evaluating different AG setups. Table 3 shows the performance of our AG setups on the four languages. The best AG setup learned for each of the four polysynthetic languages (AG BestL ) is the PrStSu+SM grammar using the Cascaded learning setup.",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_13",
  "x": "Our AG approaches significantly outperform both M orf essor and M orphoChain on all four languages, as shown in Table 3 . Comparison with supervised baselines. To obtain an upper bound, we compare the best AG setup to the best supervised neural methods presented in<cite> Kann et al. (2018)</cite> for each language. We consider their best multi-task approach (BestMTT) and the best data-augmentation approach (BestDA), using F1 scores from their Table 4 for each language. In addition, we report the results on their other supervised baselines: a supervised seq-to-seq model (S2S) and a supervised CRF approach. As can be seen in Table  4 , our unsupervised AG-based approaches outperform the best supervised approaches for NH and YN with absolute F1-scores of 0.010 and 0.012, respectively. An interesting observation is that for YN we only used the words in the training set of<cite> Kann et al. (2018)</cite> (unsegmented) , without any data augmentation.",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_14",
  "x": "For MX and WX, the neural models from<cite> Kann et al. (2018)</cite> (BestMTT and BestDA), outperform our unsupervised AG-based approaches. Error Analysis. For the purpose of error analysis, we train our unsupervised segmentation on the training sets and perform the analysis of results on the output of the development sets based on our best unsupervised models AG BestL . Since there is no distinction between stems and affixes in the labeled data, we only consider the morphemes that appear at least three times in order to eliminate open-class morphemes in our statistics. We first define the degree of ambiguity of a morpheme to be the percentage of times its sequence of characters does not form a segmentable morpheme when they appear in the training set. We also define the degree of ambiguity of a language as the average degree of ambiguity of the morphemes in that language. Table 5 shows the number of morphemes, average length of a morpheme (in characters) and the degree of morpheme Table 6 : Examples of correct and incorrect segmentation ambiguity in each language.",
  "y": "differences"
 },
 {
  "id": "3452953ac579f1c05870442456a49c_0",
  "x": "Contrast) Previous studies show that the presence of connectives can greatly help with classification of the relation and can be disambiguated with 0.93 accuracy (4-ways) solely on the discourse relation connectives (Pitler et al., 2008) . In implicit relations, no such strong cue is available and the discourse relation instead needs to be inferred based on the two textual arguments. In recent studies, various classes of features are explored to capture lexical and semantic regularities for identifying the sense of implicit relations, including linguistically informed features like polarity tags, Levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features <cite>(Lin et al., 2009</cite>; Pitler et al., 2009; Zhou et al., 2010; Zhang et al., 2015; Chen et al., 2016) . For some of second-level relations (a level of granularity that should be much more meaningful to downstream tasks than the four-way distinction), there are only a dozen in-stances, so that it's important to make maximal use of both the data set for training and testing. The test set that is currently most often used for 11 way classification is section 23 <cite>(Lin et al., 2009</cite>; Ji and Eisenstein, 2015; Rutherford et al., 2017) , which contains only about 761 implicit relations. This small size implies that a gain of 1 percentage point in accuracy corresponds to just classifying an additional 7-8 instances correctly. This paper therefore aims to demonstrate the degree to which conclusions about the effectiveness of including certain features would depend on whether one evaluates on the standard test section only, or performs cross validation on the whole dataset for second-level discourse relation classification.",
  "y": "background"
 },
 {
  "id": "3452953ac579f1c05870442456a49c_1",
  "x": "For some of second-level relations (a level of granularity that should be much more meaningful to downstream tasks than the four-way distinction), there are only a dozen in-stances, so that it's important to make maximal use of both the data set for training and testing. The test set that is currently most often used for 11 way classification is section 23 <cite>(Lin et al., 2009</cite>; Ji and Eisenstein, 2015; Rutherford et al., 2017) , which contains only about 761 implicit relations. This small size implies that a gain of 1 percentage point in accuracy corresponds to just classifying an additional 7-8 instances correctly. This paper therefore aims to demonstrate the degree to which conclusions about the effectiveness of including certain features would depend on whether one evaluates on the standard test section only, or performs cross validation on the whole dataset for second-level discourse relation classification. The model that we use is a neural network that takes the words occurring in the relation arguments as input, as well as traditional features mentioned above, to make comparisons with most-used section splits. To our knowledge, this is the first paper that systematically evaluates the effect of the train/test split for the implicit discourse relation classification task on PDTB. We report the classification performances on random and conventional split sections.",
  "y": "background"
 },
 {
  "id": "3452953ac579f1c05870442456a49c_2",
  "x": "**CORPORA** The Penn Discourse Treebank (PDTB) We use the Penn Discourse Treebank (Prasad et al., 2008) , the largest available manually annotated corpora of discourse on top of one million word tokens from the Wall Street Journal (WSJ). The PDTB provides annotations for explicit and implicit discourse relations. By definition, an explicit relation contains an explicit discourse connective while the implicit one does not. The PDTB provides a three level hierarchy of relation tags for its annotation. Previous work in this task has been done over two schemes of evaluation: first-level 4-ways classification (Pitler et al., 2009; Rutherford and Xue, 2014; Chen et al., 2016) , second-level 11-way classification <cite>(Lin et al., 2009</cite>; Ji and Eisenstein, 2015) . The distribution of second-level relations in PDTB is illustrated in Table 1 .",
  "y": "background"
 },
 {
  "id": "3452953ac579f1c05870442456a49c_3",
  "x": "The PDTB provides annotations for explicit and implicit discourse relations. By definition, an explicit relation contains an explicit discourse connective while the implicit one does not. The PDTB provides a three level hierarchy of relation tags for its annotation. Previous work in this task has been done over two schemes of evaluation: first-level 4-ways classification (Pitler et al., 2009; Rutherford and Xue, 2014; Chen et al., 2016) , second-level 11-way classification <cite>(Lin et al., 2009</cite>; Ji and Eisenstein, 2015) . The distribution of second-level relations in PDTB is illustrated in Table 1 . We follow the preprocessing method in <cite>(Lin et al., 2009</cite>; Rutherford et al., 2017) . If the instance is annotated with two relations, we adopt the first one shown up, and remove those relations with too few instances.",
  "y": "uses"
 },
 {
  "id": "3452953ac579f1c05870442456a49c_4",
  "x": "The proportions of each class in the training and testing set are identical. With the same distribution of each class, we here avoid having an unbalanced number of instances per class among training and testing set. ---------------------------------- **MODEL** The task is to predict the discourse relation given the two arguments of an implicit instance. As a label set, we use 11-way distinction as proposed in<cite> Lin et al., (2009)</cite>; Ji and Eisenstein (2015) . Word Embeddings are trained with the Skip-gram architecture in Word2Vec (Mikolov et al., 2013) , which is able to capture semantic and syntactic patterns with an unsupervised method, on the training sections of WSJ data.",
  "y": "uses"
 },
 {
  "id": "3477c0225d6a0e55365242d95a3dc9_0",
  "x": "Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, <cite>9]</cite> . The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather The 2016 Conference on Computational Linguistics and Speech Processing ROCLING 2016, pp. 100-102 \uf0d3 The Association for Computational Linguistics and Chinese Language Processing 100 than window contexts in word2vec.",
  "y": "background"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_0",
  "x": "In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (Pavalanathan and Eisenstein, 2015) . Lexical dialectology is (in part) the converse of user geolocation (Eisenstein, 2015) : given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions. The complexity of the task is two-fold: (1) localised named entities (e.g. sporting team names) are not of interest; and (2) without semantic knowledge it is difficult to detect terms that are in general use but have a special meaning in a region. In this paper we propose a text-based geolocation method based on neural networks. Our contributions are as follows: (1) we achieve state-of-the-art results on benchmark Twitter geolocation datasets; (2) we show that the model is less sensitive to the specific location discretisation method; (3) we release the first broad-coverage dataset for evaluation of lexical dialectology models; (4) we incorporate our text-based model into a network-based model<cite> (Rahimi et al., 2015a)</cite> and improve the performance utilising both network and text; and (5) we use the model's embeddings for extraction of local terms and show that it outperforms two baselines. ---------------------------------- **RELATED WORK**",
  "y": "extends background"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_1",
  "x": "Three main text-based approaches are: (1) the use of gazetteers Quercini et al., 2010) ; (2) unsupervised text clustering based on topic models or similar (Eisenstein et al., 2010; Hong et al., 2012; Ahmed et al., 2013) ; and (3) supervised classification (Ding et al., 2000; Backstrom et al., 2008; Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Wing and Baldridge, 2011; Han et al., 2012; Rout et al., 2013) , which unlike gazetteers can be applied to informal text and compared to topic models, scales better. The classification models often rely on less than 1% of geotagged tweets for supervision and discretise real-valued coordinates into equalsized grids (Serdyukov et al., 2009 ), administrative regions (Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Han et al., 2012 , or flat (Wing and Baldridge, 2011) or hierarchical k-d tree clusters (Wing and Baldridge, 2014) . Network-based methods also use either real-valued coordinates (Jurgens et al., 2015) or discretised regions<cite> (Rahimi et al., 2015a)</cite> as labels, and use label propagation over the interaction graph (e.g. @-mentions). More recent methods have focused on representation learning by using sparse coding (Cha et al., 2015) or neural networks (Liu and Inkpen, 2015) , utilising both text and network information<cite> (Rahimi et al., 2015a)</cite> . Dialect is a variety of language shared by a group of speakers (Wolfram and Schilling, 2015) . Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas. The traditional approach to dialectology is to find the geographical distribution of known lexical alternatives (e.g. you, yall and yinz: (Labov et al., 2005; Nerbonne et al., 2008; Gon\u00e7alves and S\u00e1nchez, 2014; Doyle, 2014; Huang et al., 2015; Nguyen and Eisenstein, 2016) ), the shortcoming of which is that the alternative lexical variables must be known beforehand.",
  "y": "background"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_2",
  "x": "Network-based methods also use either real-valued coordinates (Jurgens et al., 2015) or discretised regions<cite> (Rahimi et al., 2015a)</cite> as labels, and use label propagation over the interaction graph (e.g. @-mentions). More recent methods have focused on representation learning by using sparse coding (Cha et al., 2015) or neural networks (Liu and Inkpen, 2015) , utilising both text and network information<cite> (Rahimi et al., 2015a)</cite> . Dialect is a variety of language shared by a group of speakers (Wolfram and Schilling, 2015) . Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas. The traditional approach to dialectology is to find the geographical distribution of known lexical alternatives (e.g. you, yall and yinz: (Labov et al., 2005; Nerbonne et al., 2008; Gon\u00e7alves and S\u00e1nchez, 2014; Doyle, 2014; Huang et al., 2015; Nguyen and Eisenstein, 2016) ), the shortcoming of which is that the alternative lexical variables must be known beforehand. There have also been attempts to automatically identify such words from geotagged documents (Eisenstein et al., 2010; Ahmed et al., 2013; Eisenstein, 2015) . The main idea is to find lexical variables that are disproportionately distributed in different locations either via model-based or statistical methods (Monroe et al., 2008) .",
  "y": "background"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_3",
  "x": "The parameters are optimised using Adamx (Kingma and Ba, 2014) using Lasagne/Theano (Theano Development Team, 2016) . Following Cheng (2010) and Eisenstein (2010) , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\"). Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161. 4 The results reported in Rahimi et al. (2015b;<cite> 2015a)</cite> for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset. While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user<cite> (Rahimi et al., 2015a)</cite> . Note that it would, of course, be possible to combine text and network information in a joint deep learning model (Yang et al., 2016; Kipf and Welling, 2016) , which we leave to future work (noting that scalability will potentially be a major issue for the larger datasets). To test the applicability of the model's embeddings in dialectology, we created DAREDS.",
  "y": "differences"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_4",
  "x": "The number of regions, regularisation strength, hidden layer and mini-batch size are tuned over development data and set to (32, 10 \u22125 , 896, 100), (256, 10 \u22126 , 2048, 10000) and (930, 10 \u22126 , 3720, 10000) for GEOTEXT, TWITTER-US and TWITTER-WORLD, respectively. The parameters are optimised using Adamx (Kingma and Ba, 2014) using Lasagne/Theano (Theano Development Team, 2016) . Following Cheng (2010) and Eisenstein (2010) , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\"). Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161. 4 The results reported in Rahimi et al. (2015b;<cite> 2015a)</cite> for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset. While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user<cite> (Rahimi et al., 2015a)</cite> . Note that it would, of course, be possible to combine text and network information in a joint deep learning model (Yang et al., 2016; Kipf and Welling, 2016) , which we leave to future work (noting that scalability will potentially be a major issue for the larger datasets).",
  "y": "extends differences"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_5",
  "x": "The results are also compared with state-of-the-art text-based methods based on a flat (Rahimi et al., 2015b; Cha et al., 2015) or hierarchical (Wing and Baldridge, 2014; Melo and Martins, 2015; Liu and Inkpen, 2015) geospatial representation. Our method outperforms both the flat and hierarchical text-based models by a large margin. Comparing the two discretisation strategies, k-means outperforms k-d tree by a reasonable margin. We also incorporated the MLP predictions into a network-based model based on the method of <cite>Rahimi et al. (2015a)</cite> , and improved upon their work. We analysed the Table 2 : Nearest neighbours of place names. in Figure 3 . The error is highest in states with lower training coverage (e.g. Maine, Montana, Wisconsin, Iowa and Kansas).",
  "y": "background extends"
 },
 {
  "id": "373795850c8f182051214a8ee09461_0",
  "x": "Our experiments show that (i) visual features contribute differently for verbs than for nouns, and (ii) images complement textual information, if (a) the textual modality by itself is poor and appropriate image subsets are used, or (b) the textual modality by itself is rich and large (potentially noisy) images are added. ---------------------------------- **INTRODUCTION** Distributional semantic models (DSMs) rely on the distributional hypothesis (Harris, 1954) , that words with similar distributions have related meanings. They represent a well-established tool for modelling semantic relatedness between words and phrases (Bullinaria and Levy, 2007; Turney and Pantel, 2010) . In the last decade, standard DSMs using bag-of-words or syntactic cooccurrence counts have been enhanced by integration into neural networks Levy et al., 2015; Nguyen et al., 2016) , or by integrating perceptual information (Silberer and Lapata, 2014; Bruni et al., 2014; <cite>Kiela et al., 2014</cite>; Lazaridou et al., 2015) . While standard DSMs have been applied to a variety of semantic relatedness tasks such as word sense discrimination, selectional preferences, relation distinction (among others), multi-modal models have predominantly been evaluated on their general ability to model semantic similarity as captured by SimLex (Hill et al., 2015) , WordSim (Finkelstein et al., 2002) , etc.",
  "y": "background"
 },
 {
  "id": "373795850c8f182051214a8ee09461_1",
  "x": "In this paper, we compare a neural network DSM relying on textual co-occurrences with a multi-modal model extension integrating visual information. We focus on the prediction of compositionality for two types of German multi-word expressions: noun-noun compounds and particle verbs. Differently to most previous multimodal approaches, we thus address a semantically specific task that was traditionally addressed by standard DSMs, mainly for English and German (Baldwin, 2005; Bannard, 2005; Reddy et al., 2011; Salehi and Cook, 2013; Schulte im Walde et al., 2013; Salehi et al., 2014; Bott and Schulte im Walde, 2014; Bott and Schulte im Walde, 2015; Schulte im Walde et al., 2016a) . Furthermore, we zoom into factors that might influence the quality of predictions, such as lexical and empirical target properties (e.g., ambiguity, frequency, compositionality); and filters to optimise the visual space, such as dispersion and imageability filters<cite> (Kiela et al., 2014)</cite> , and a novel clustering filter. Our experiments demonstrate that the contributions of the textual and the visual models differ for predictions across the nominal vs. verbal compositions. The visual modality adds complementary features in cases where (a) the textual modality performs poorly, and images of the most imaginable targets are added, or (b) the textual modality performs well, and all available -potentially noisy-images are added. In addition, we demonstrate that perceptual features of verbs, such as abstractness and imageability, have a different influence on multi-modality than for nouns, presumably because they are more difficult to grasp.",
  "y": "background"
 },
 {
  "id": "373795850c8f182051214a8ee09461_2",
  "x": "The experiments compare the predictions of compositionality across all targets in the gold standards. 2 Furthermore, we zoom into factors that might influence the quality of predictions: (A) the impact of lexical and empirical target properties, i.e., ambiguity (relying on the DUDEN dictionary 3 , frequency (as provided by the gold standards), abstractness and imageability (as taken from K\u00f6per and Schulte im Walde (2016)); (B) optimisation of the visual space: (i) In accordance with human concept processing (Paivio, 1990) , including image representations should be more useful for words which are visual. We therefore apply the dispersion-based filter suggested by<cite> Kiela et al. (2014)</cite> . The filter decides whether to include perceptual information for a specific word or not, relying on a pairwise similarity between all images of a concept. The underlying idea is that highly visual concepts are visualised by similar pictures and thus trigger a high average similarity between the word's images. Abstract concepts, on the other hand, are expected to provide a lower dispersion. For a given word, the filter decides about using only the textual representation, or both the textual and visual representations, depending on the dispersion value and a predefined threshold (set to the median of all the dispersion values).",
  "y": "uses"
 },
 {
  "id": "373795850c8f182051214a8ee09461_3",
  "x": "Zooming into target subsets, the predictions for monosemous targets are better than those for ambiguous targets (significant for GS-NN), see Figure 3 ; ditto for low-frequency vs. high-frequency targets. Taking frequency as an indicator of ambiguity, these differences are presumably due to the difficulty of distinguishing between multiple senses in vector spaces that subsume the features of all word senses within one vector, which applies to our textual and multi-modal models. The gold standard predictions strongly differ regarding the influence of target abstractness, imageability and compositionality. For GS-NN, the compositionality of concrete and imaginable targets is predicted better than for abstract and less imaginable targets, as one would expect and has been shown by<cite> Kiela et al. (2014)</cite> ; for GS-PV, the opposite is the case. Similarly, while for GS-NN highly compositional targets are predicted worse than low-and mid-compositional targets, for GS-PV mid-compositional targets are predicted much worse than low-and high-compositional targets. These differences in results point to questions that have still been unsolved across research fields: while humans can easily grasp intuitions about the abstractness, imageability and compositionality of nouns, the categorisations are difficult to define for verbs (Glenberg and Kaschak, 2002; Brysbaert et al., 2014) . Particle verbs add to this complexity, especially since compositionality (rating) is typically reduced to the semantic relatedness between the complex verb and the base verb, ignoring the particle that however contributes a considerable portion of meaning to the complex verb.",
  "y": "similarities"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_0",
  "x": "In recent years, an increasing number of studies have investigated character-level models with subwords in both unsupervised<cite> (Bojanowski et al., 2017</cite>; Pagliardini et al., 2018) and supervised learning (Zhang et al., 2015; Sennrich et al., 2016; Wieting et al., 2016; Lee et al., 2017) . In these models, the notion of vocabularies is extended to include sub-words. By enriching the information of the word, sub-words are useful for capturing morphological changes<cite> (Bojanowski et al., 2017)</cite> and the meaning of short phrases (Wieting et al., 2016) . In addition, OOV (or unseen) words can be composed from sub-words, which are present at training<cite> (Bojanowski et al., 2017)</cite> . In this paper, we propose a simple unsupervised method of character n-gram embedding for unsegmented languages, where the segmentation step is completely omitted thus words, phrases and sentences are treated seamlessly. Our model considers all possible character n-grams as embedding targets in a corpus. Each n-gram is explicitly modeled as a composition of its sub-n-grams just like each word is modeled as a composition of sub-words in the subword information skipgram model (SISG)<cite> (Bojanowski et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_1",
  "x": "In addition, newly given real-world datasets may include a lot of unseen words and phrases. Practically, OOV words in a corpus are replaced with a special token representing OOV. The larger OOV rate in a corpus affects the accuracies of downstream tasks (Sun et al., 2005) . In recent years, an increasing number of studies have investigated character-level models with subwords in both unsupervised<cite> (Bojanowski et al., 2017</cite>; Pagliardini et al., 2018) and supervised learning (Zhang et al., 2015; Sennrich et al., 2016; Wieting et al., 2016; Lee et al., 2017) . In these models, the notion of vocabularies is extended to include sub-words. By enriching the information of the word, sub-words are useful for capturing morphological changes<cite> (Bojanowski et al., 2017)</cite> and the meaning of short phrases (Wieting et al., 2016) . In addition, OOV (or unseen) words can be composed from sub-words, which are present at training<cite> (Bojanowski et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_2",
  "x": "In addition, newly given real-world datasets may include a lot of unseen words and phrases. Practically, OOV words in a corpus are replaced with a special token representing OOV. The larger OOV rate in a corpus affects the accuracies of downstream tasks (Sun et al., 2005) . In recent years, an increasing number of studies have investigated character-level models with subwords in both unsupervised<cite> (Bojanowski et al., 2017</cite>; Pagliardini et al., 2018) and supervised learning (Zhang et al., 2015; Sennrich et al., 2016; Wieting et al., 2016; Lee et al., 2017) . In these models, the notion of vocabularies is extended to include sub-words. By enriching the information of the word, sub-words are useful for capturing morphological changes<cite> (Bojanowski et al., 2017)</cite> and the meaning of short phrases (Wieting et al., 2016) . In addition, OOV (or unseen) words can be composed from sub-words, which are present at training<cite> (Bojanowski et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_3",
  "x": "In addition, OOV (or unseen) words can be composed from sub-words, which are present at training<cite> (Bojanowski et al., 2017)</cite> . In this paper, we propose a simple unsupervised method of character n-gram embedding for unsegmented languages, where the segmentation step is completely omitted thus words, phrases and sentences are treated seamlessly. Our model considers all possible character n-grams as embedding targets in a corpus. Each n-gram is explicitly modeled as a composition of its sub-n-grams just like each word is modeled as a composition of sub-words in the subword information skipgram model (SISG)<cite> (Bojanowski et al., 2017)</cite> . Our segmentation-free compositional n-gram embedding is referred to as SCNE in this paper. This kind of approach that does not consider any word boundaries for unsegmented languages may sound reckless since the embedding targets can include a lot of wrong boundaries. However, we found that we can compose vector representations for words and sentences with good quality by summing up the representations of their substrings.",
  "y": "similarities"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_5",
  "x": "Through the following two word-level tasks and one sentence-level task, we investigate the qualities of vector representations and their usefulness in practical applications. We will make the C++ implementation of our method and pre-trained models available open-source. ---------------------------------- **BASELINE SYSTEMS** As baseline systems, we use C-BOW, Skipgram (Mikolov et al., 2013) , Subword Information Skip-gram (SISG)<cite> (Bojanowski et al., 2017)</cite> and Segmentation-free word embedding for unsegmented languages (Sembei) (Oshikiri, 2017) for the word-level tasks. For the sentence-level task, baselines are PV-DBOW, PV-DM (Le and Mikolov, 2014) and Sent2vec (Pagliardini et al., 2018) . In addition, we test sentence embedding baselines obtained by simple averaging of word embeddings over the sentence, denoted as C-BOW * , Skip-gram * and SISG * .",
  "y": "uses"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_6",
  "x": "**EVALUATION TASKS** Word similarity task: Our model and the baselines are trained on portions of Wikipedia of increasing size to see the effect of the size of the training data. For pairs of words, the cosine similarity between embeddings is compared to human judgment, and the quality is measured by Spearman rank correlation. Most of the settings are the same as that of <cite>Bojanowski et al. (2017)</cite> . Two widely-used benchmark datasets are used: Chinese word similarity dataset (Jin and Wu, 2012) , which contains 297 pairs of words, and Japanese word similarity dataset (Sakaizawa and Komachi, 2017) , which contains 4429 pairs of words. Conventional word embedding methods, C-BOW, Skip-gram, and Sembei, cannot provide the embeddings of OOV words in the test data. In contrast, SISG and our model can compute representations for almost all words, since both methods learn compositional n-gram features.",
  "y": "similarities uses"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_7",
  "x": "Conventional word embedding methods, C-BOW, Skip-gram, and Sembei, cannot provide the embeddings of OOV words in the test data. In contrast, SISG and our model can compute representations for almost all words, since both methods learn compositional n-gram features. In order to show comparable results, we use the null vector for these OOV words following <cite>Bojanowski et al. (2017)</cite> . Noun category prediction task: We use 100MB of SNS data, Sina Weibo for Chinese and Twitter for Japanese and Korean, as training corpora. For evaluating the learned embeddings, noun words, including neologisms, and their categories are extracted from Wikidata with the predetermined semantic category set 2 . For each category, a logistic regression classifier is trained from the vector representations, where unseen words are skipped in training and treated as errors in testing. Sentiment analysis task: Movie review datasets are used for evaluating sentence embeddings.",
  "y": "similarities uses"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_0",
  "x": "We introduce an unsupervised method based on MT for GEC that does not almost use parallel learner data. In particular, we use methods proposed by Marie and Fujita (2018) , <cite>Artetxe et al. (2018b)</cite> , and Lample et al. (2018) . These methods are based on phrase-based statistical machine translation (SMT) and two phrase table refinements, i.e., forward and backward refinement. Forward refinement simply arguments a learner corpus with automatic corrections whereas backward refinement expends both source-side and target-side data to train GEC model using backtranslation (Sennrich et al., 2016a) . Unsupervised MT techniques do not require a parallel but a comparable corpus as training data. Therefore, we use comparable translated texts using Google Translation as the source-side data. Specifically, we use News Crawl written in English as target-side data and News Crawl written in another language translated into English as source-side data.",
  "y": "similarities uses"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_1",
  "x": "Therefore, we use comparable translated texts using Google Translation as the source-side data. Specifically, we use News Crawl written in English as target-side data and News Crawl written in another language translated into English as source-side data. We identified the difference between forward and backward refinement with CoNLL-2014 dataset and JFLEG dataset; the former generates fluent outputs. We also verified our GEC system through experiments for a low resource track of the shared task at Building Educational Applications 2019 (BEA2019). The experimental results show that our system achieved an F 0.5 score of 28.31 points in the low resource track of the shared task at BEA2019. 2 Unsupervised GEC Algorithm 1 shows the pseudocode for unsupervised GEC. This code is derived from <cite>Artetxe et al. (2018b)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_2",
  "x": "As in <cite>Artetxe et al. (2018b)</cite> , the term was set to 0.001. The backward lexical translation probability lex(e|f ) is calculated in a similar manner. Refinement of SMT system The phrase table created is considered to include noisy phrase pairs. Therefore, we update the phrase table using an SMT system. The SMT system trained on synthetic data eliminates the noisy phrase pairs using language models trained on the target-side corpus. This process corresponds to lines 4-23 in Algorithm 1. The phrase table can be refined in either of two ways: forward and backward refinement.",
  "y": "similarities"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_3",
  "x": "For forward refinement (Marie and Fujita, 2018) , target synthetic data were generated from the source monolingual data using the source-totarget phrase table P (0) s\u2192t and target language models LM t . A new phrase table P (1) s\u2192t was then created with this target synthetic corpus. This operation was executed N times. For backward refinement<cite> (Artetxe et al., 2018b)</cite> , source synthetic data were generated from the target monolingual data using the target to source phrase table P (0) t\u2192s and source language model LM s . A new source to target phrase table P (1) s\u2192t was created with this source synthetic parallel corpus. Next, target synthetic data were generated from the source monolingual data using P",
  "y": "background"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_4",
  "x": "Construction of a comparable corpus This unsupervised method is based on the assumption that the source and target corpora are comparable. In fact, Lample et al. (2018) , <cite>Artetxe et al. (2018b)</cite> and Marie and Fujita (2018) use the News Crawl of source and target language as training data. To make a comparable corpus for GEC, we use translated texts using Google Translation as 3 Experiment of low resource GEC 3.1 Experimental setting Table 1 shows the training and development data size. Unless mentioned otherwise, Finnish News Crawl 2014-2015 translated into English was used as source training data and English News Crawl 2017 was used as target training data. To train the extra language model of the target-side (LM t ), we used training data of One Billion Word Benchmark (Chelba et al., 2014) . We used googletrans v2.4.0 3 for Google Translation and obtained 2,122,714 translated sentences. We sampled the 3,000,000 sentences from English News Crawl 2017 and excluded the sentences with more than 150 words for either source-and targetside data.",
  "y": "background"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_5",
  "x": "The low resource track at BEA2019 permitted to use W&I+LOCNESS development set, so we split it in half; tune data and 3 https://github.com/ssut/py-googletrans dev data 4 . These data were tokenized by spaCy v1.9.0 5 and the en_core_web_sm-1.2.0 model for W&I+LOCNESS. For CoNLL-14 and JFLEG test set, NLTK (Bird, 2006) tokenizer was used. We used moses truecaser for the training data; this truecaser model was learned from processed English News Crawl. We used byte-pair-encoding (Sennrich et al., 2016b) learned from processed English News Crawl; the number of operations was 50K. The implementation made by <cite>Artetxe et al. (2018b)</cite> 6 was modified to conduct the experiments. Specifically, some features were added; word-level Levenshtein distance, word-, and character-level edit operation, operation sequence model (Durrani et al., 2013) 7 , and 9-gram word class language model 8 , similar to Grundkiewicz and Junczys-Dowmunt (2018) without sparse features.",
  "y": "extends differences"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_6",
  "x": "Difference between forward and backward refinements To examine how different the refinement methods are, we counted the number of corrections predicted by each method. The number of USMT forward in iter 1 and iter 2 is 3,437 and 3,257, respectively, whereas that of USMT backward in iter 1 and iter 2 is 4,092 and 2,789. As for USMT backward , the number of corrections from iter 1 to iter 2 decreases by 1,303. <cite>Artetxe et al. (2018b)</cite> and Lample et al. (2018) reported that the BLEU score (Papineni et al., 2002) of unsupervised MT with backward-refinement improves with increasing iterations. In GEC, increasing the iterations of USMT backward improves the GEC accuracy by predicting less corrections. The GLEU score for USMT forward is considered higher than that for USMT backward because the language model makes up for the synthetic target data. To compare the fluency, the outputs of each best iter on JFLEG were evaluated with the perplexity based on the Common Crawl language model 10 .",
  "y": "background"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_8",
  "x": "In this study, we apply the USMT method of <cite>Artetxe et al. (2018b)</cite> and Marie and Fujita (2018) to GEC. The UNMT method (Lample et al., 2018) was ineffective under the GEC setting in our preliminary experiments. GEC with NMT/SMT Several studies that introduce sequence-to-sequence models in GEC heavily rely on large amounts of training data. Ge et al. (2018) , who presented state-of-the-art results in GEC, proposed a supervised NMT method trained on corpora of a total 5.4 M sentence pairs. On the other hand, we mainly use the monolingual corpus and use small learner data as the tuning data. Despite the success of NMT, many studies on GEC traditionally use SMT Junczys-Dowmunt and Grundkiewicz, 2014) . These studies apply an offthe-shelf SMT toolkit, Moses, to GEC.",
  "y": "similarities uses"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_0",
  "x": "**INTRODUCTION** Japanese morphological analysis (MA) takes an unsegmented string of Japanese text as input, and outputs a string of morphemes annotated with parts of speech (POSs). As MA is the first step in Japanese NLP, its accuracy directly affects the accuracy of NLP systems as a whole. In addition, with the proliferation of text in various domains, there is increasing need for methods that are both robust and adaptable to out-of-domain data (Escudero et al., 2000) . Previous approaches have used structured predictors such as hidden Markov models (HMMs) or conditional random fields (CRFs), which consider the interactions between neighboring words and parts of speech (Nagata, 1994; Asahara and Matsumoto, 2000;<cite> Kudo et al., 2004)</cite> . However, while structure does provide valuable information, Liang et al. (2008) have shown that gains provided by structured prediction can be largely recovered by using a richer feature set. This approach has also been called \"pointwise\" prediction, as it makes a single independent decision at each point (Neubig and Mori, 2010) .",
  "y": "background"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_1",
  "x": "While Liang et al. (2008) focus on the speed benefits of pointwise prediction, we demonstrate that it also allows for more robust and adaptable MA. We find experimental evidence that pointwise MA can exceed the accuracy of a state-of-the-art structured approach<cite> (Kudo et al., 2004)</cite> on in-domain data, and is significantly more robust to out-of-domain data. We also show that pointwise MA can be adapted to new domains with minimal effort through the combination of active learning and partial annotation (Tsuboi et al., 2008) , where only informative parts of a particular sentence are annotated. In a realistic domain adaptation scenario, we find that a combination of pointwise prediction, partial annotation, and active learning allows for easy adaptation. ---------------------------------- **JAPANESE MORPHOLOGICAL ANALYSIS** Japanese MA takes an unsegmented string of characters x I 1 as input, segments it into morphemes w J 1 , and annotates each morpheme with a part of speech t J 1 .",
  "y": "differences"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_2",
  "x": "This can be formulated as a two-step process of first segmenting words, then estimating POSs (Ng and Low, 2004) , or as a single joint process of finding a morpheme/POS string from unsegmented text <cite>(Kudo et al., 2004</cite>; Nakagawa, 2004; Kruengkrai et al., 2009) . In this section we describe an existing joint sequence-based method for Japanese MA, as well as our proposed two-step pointwise method. ---------------------------------- **JOINT SEQUENCE-BASED MA** Japanese MA has traditionally used sequence based models, finding a maximal POS sequence for en- ---------------------------------- **TYPE**",
  "y": "background"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_3",
  "x": "Feature Strings tire sentences as in Figure 1 (a). The CRF-based method presented by<cite> Kudo et al. (2004)</cite> is generally accepted as the state-of-the-art in this paradigm. CRFs are trained over segmentation lattices, which allows for the handling of variable length sequences that occur due to multiple segmentations. The model is able to take into account arbitrary features, as well as the context between neighboring tags. We follow<cite> Kudo et al. (2004)</cite> in defining our feature set, as summarized in Table 1 1 . Lexical features were trained for the top 5000 most frequent words in the corpus. It should be noted that these are wordbased features, and information about transitions between POS tags is included.",
  "y": "background"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_4",
  "x": "---------------------------------- **TYPE** Feature Strings tire sentences as in Figure 1 (a). The CRF-based method presented by<cite> Kudo et al. (2004)</cite> is generally accepted as the state-of-the-art in this paradigm. CRFs are trained over segmentation lattices, which allows for the handling of variable length sequences that occur due to multiple segmentations. The model is able to take into account arbitrary features, as well as the context between neighboring tags. We follow<cite> Kudo et al. (2004)</cite> in defining our feature set, as summarized in Table 1 1 .",
  "y": "uses"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_5",
  "x": "When creating training data, the use of word-based features indicates that word boundaries must be annotated, while the use of POS transition information further indicates that all of these words must be annotated with POSs. 1 More fine-grained POS tags have provided small boosts in accuracy in previous research<cite> (Kudo et al., 2004)</cite> , but these increase the annotation burden, which is contrary to our goal. Feature Strings Character ---------------------------------- **2-STEP POINTWISE MA** In our research, we take a two-step approach, first segmenting character sequence x I 1 into the word sequence w J 1 with the highest probability, then tagging each word with parts of speech t J 1 . This approach is shown in Figure 1 (b) .",
  "y": "background"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_7",
  "x": "To create the dictionary, we added all of the words in the corpus, but left out a small portion of singletons to prevent overfitting on the training data 3 . As an evaluation measure, we follow Nagata (1994) and<cite> Kudo et al. (2004)</cite> and use Word/POS tag pair Fmeasure, so that both word boundaries and POS tags must be correct for a word to be considered correct. ---------------------------------- **ANALYSIS RESULTS** In our first experiment we compared the accuracy of the three methods on both the in-domain and outof-domain test sets (Table 4) . It can be seen that 2-LR outperforms JOINT, and achieves similar but slightly inferior results to 2-CRF. The reason for accuracy gains over JOINT lies largely in the fact that while JOINT is more reliant on the dictionary, and thus tends to mis-segment unknown words, the two-step methods are significantly more robust.",
  "y": "uses"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_1",
  "x": "For example, consider the sentence in Figure 1 . It is segmented into three EDUs, numbered from 1 to 3. EDUs 2 and 3 are related by the relation Enablement, forming a new span of text, which is related to 1 by the relation Attribution. In each relation, EDUs can be Nucleus (more essential) or Satellite to the writer's purpose. Many approaches have been used in DP, the majority of them using machine learning algorithms, such as probabilistic models<cite> (Soricut and Marcu, 2003)</cite> , SVMs (Reitter, 2003; duVerle and Prendinger, 2009; Hernault et al., 2010; Feng and Hirst, 2012) and dynamic conditional random field (Joty et al., 2012) . To obtain acceptable results, these approaches need plenty of labeled data. But even more than other levels of linguistic information, such as morphology or syntax, the annotation of discourse is an expensive task.",
  "y": "background"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_2",
  "x": "EDUs 2 and 3 are related by the relation Enablement, forming a new span of text, which is related to 1 by the relation Attribution. In each relation, EDUs can be Nucleus (more essential) or Satellite to the writer's purpose. Many approaches have been used in DP, the majority of them using machine learning algorithms, such as probabilistic models<cite> (Soricut and Marcu, 2003)</cite> , SVMs (Reitter, 2003; duVerle and Prendinger, 2009; Hernault et al., 2010; Feng and Hirst, 2012) and dynamic conditional random field (Joty et al., 2012) . To obtain acceptable results, these approaches need plenty of labeled data. But even more than other levels of linguistic information, such as morphology or syntax, the annotation of discourse is an expensive task. Given this fact, what can we do when there is not enough data to perform effective learning of DP, as in languages with little annotated data? This paper describes a methodology to overcome the problem of insufficient labeled data in the task of identifying rhetorical relations between Figure 2 : Lexicalized syntactic tree used by SPADE.",
  "y": "motivation"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_3",
  "x": "The results show that this approach improved the results to achieve nearhuman perfomance, even with the use of automatic tools (syntax parser and discourse segmenter). 2 Related Work 2.1 Supervised Discourse Parsing<cite> Soricut and Marcu (2003)</cite> use two probabilistic models to perform a sentence-level analysis, one for segmentation and other to identify the relations and build the rhetorical structure. The parser is named SPADE (Sentence-level Parsing of DiscoursE) and the authors base their model on lexical and syntactic information, extracting features from a lexicalized syntactic tree. They assume that the features extracted at the jointing point of two discursive segments are the most indicative information to identify the rhetorical structure of the sentence. For example, in Figure 2 , the circled nodes correspond to the most indicative cues to identify the structure and relation between each two adjacent segments. The authors report a F-measure of 0.49 in a set of 18 RST relations. The human performance in this same task is 0.77 (measured by interannotation agreement).",
  "y": "background"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_4",
  "x": "Chiarcos (2012) used SSL to develop a probabilistic model mapping the occurrence of discourse markers and verbs to rhetorical relations. For Italian, Soria and Ferrari (1998) conducted work in the same direction. Sporleder and Lascarides (2005) performed similar work to Marcu and Echihabi, with similar results for a different set of relations and a more sophisticated classifier. Building on this, there is an interesting idea, known as never-ending learning (NEL) by Carlson et al. (2010) , in which they apply SSL with infinite unlabeled data. The needed data is widely and freely available on the web. Their architecture runs 24 hours per day, forever, obtaining new information and performing a learning task. With the aim of surpassing the limitation of labeled RST in Portuguese to develop a good DP, we employ SSNEL in the task by adapting the work of<cite> Soricut and Marcu (2003)</cite> and Hernault et al. (2010) .",
  "y": "uses"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_5",
  "x": "This choice for SSLNEL was made considering the large and free availability of news texts on the web. ---------------------------------- **RST CORPORA** RST-DT (RST Discourse TreeBank) (Carlson et al., 2001 ) is the most widely used corpus annotated with RST in English. Table 1 compares it with available Portuguese corpora labeled according to RST (these corpora will be referred to as RST-DT-PT hereafter). The corpora CSTNews (Cardoso et al., 2011) , Summ-it (Collovini et al., 2007) and two-thirds of Rhetalho (Pardo and Seno, (45) and many more words (55,536) than RST-DT-PT. This work focuses on the identification of rhetorical relations at the sentence level, and as is common since the work of<cite> Soricut and Marcu (2003)</cite> , fine-grained relations were grouped: 29 sentence-level rhetorical relations were found and grouped into 16 groups.",
  "y": "similarities uses background"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_6",
  "x": "The imbalance of the relations is a natural characteristic in discourse and, to avoid overfitting of a learning model on the lessfrequent relations, no balancing was made. The relation Summary, for example, occurs only 2 times, and Elaboration occurs 1491 times, making very difficult the identification of the Summary relation. ---------------------------------- **ADAPTED MODELS** Syntactic information is crucial in SPADE<cite> (Soricut and Marcu, 2003)</cite> and for Portuguese the parser most similar to that used by Soricut and Marcu is the LX-parser (Stanford parser trained to Portuguese (Silva et al., 2010) ). After the parsing of the text by the syntactic parser, the same lexicalization procedure (Magerman, 1995) was applied and adapted according to the tagset used by LX-parser. In this adaptation, only pairs of adjacent segments at sentence-level were considered, and nuclearity was not considered, in order to avoid sparseness in the data.",
  "y": "uses"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_7",
  "x": "This work focuses on the identification of rhetorical relations at the sentence level, and as is common since the work of<cite> Soricut and Marcu (2003)</cite> , fine-grained relations were grouped: 29 sentence-level rhetorical relations were found and grouped into 16 groups. The imbalance of the relations is a natural characteristic in discourse and, to avoid overfitting of a learning model on the lessfrequent relations, no balancing was made. The relation Summary, for example, occurs only 2 times, and Elaboration occurs 1491 times, making very difficult the identification of the Summary relation. ---------------------------------- **ADAPTED MODELS** Syntactic information is crucial in SPADE<cite> (Soricut and Marcu, 2003)</cite> and for Portuguese the parser most similar to that used by Soricut and Marcu is the LX-parser (Stanford parser trained to Portuguese (Silva et al., 2010) ). After the parsing of the text by the syntactic parser, the same lexicalization procedure (Magerman, 1995) was applied and adapted according to the tagset used by LX-parser.",
  "y": "background"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_8",
  "x": "The experiment with SSNEL for English was realized in order to see the results that could be obtained when large annotated corpora are available. In the SSNEL for English, only decision-tree classifiers were used to classify new instances. For Portuguese, a symbolic model (lexical patterns) was also used together with the classifiers. The improved results presented in Table 2 and 3 are very different due to differing evaluation strategies. Using separated test data, we tried to avoid possible overfitting on training data, but the size of test data may not lead to a fair evaluation<cite> Soricut and Marcu (2003)</cite> or Joty et al. (2012) , since HILDA-PT used different corpora (RST-DT-PT instead of RST-DT), and some reported results are for the complete DP. However, our results show the potential of the SSNEL workflow when not enough labeled data is available for supervised learning, since the same approach for relation identification of Hernault et al. (2010) was used in HILDA-PT and 0.531 was initially obtained. These results constitute the state of art for rhetorical relation identification for Portuguese and it is believed that with more time (iterations in SSNEL), the results may increase.",
  "y": "background"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_0",
  "x": "Machine translation (MT) from Chinese to English has been a difficult problem: structural differences between Chinese and English, such as the different orderings of head nouns and relative clauses, cause BLEU scores to be consistently lower than for other difficult language pairs like Arabic-English. But use of hierarchical decoders has not solved the DE construction translation problem. An alternative way of dealing with structural differences is to reorder source language sentences to minimize structural divergence with the target language, (Xia and McCord, 2004; Collins et al., 2005;<cite> Wang et al., 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_1",
  "x": "For example <cite>Wang et al. (2007)</cite> introduced a set of rules to decide if a (DE) construction should be reordered or not before translating to English: \u2022 For DNPs (consisting of\"XP+DEG\"): -Reorder if XP is PP or LCP; -Reorder if XP is a non-pronominal NP \u2022 For CPs (typically formed by \"IP+DEC\"): -Reorder to align with the \"that+clause\" structure of English. Although this and previous reordering work has led to significant improvements, errors still remain. Indeed, <cite>Wang et al. (2007)</cite> found that the precision of their NP rules is only about 54.6% on a small human-judged set. One possible reason the (DE) construction remains unsolved is that previous work has paid insufficient attention to the many ways the (DE) construction can be translated and the rich structural cues to the translation.",
  "y": "motivation background"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_3",
  "x": "According to the Chinese Treebank tagging guidelines (Xia, 2000) , the character can be tagged as DEC, DEG, DEV, SP, DER, or AS. Similar to<cite> (Wang et al., 2007)</cite> , we only consider the majority case when the phrase with (DE) is a noun phrase modifier. The DEs in NPs have a part-of-speech tag of DEC (a complementizer or a nominalizer) or DEG (a genitive marker or an associative marker). ---------------------------------- **CLASS DEFINITION** The way we categorize the DEs is based on their behavior when translated into English. This is implicitly done in the work of <cite>Wang et al. (2007)</cite> where they use rules to decide if a certain DE and the words next to it will need to be reordered.",
  "y": "similarities background"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_4",
  "x": "**CLASS DEFINITION** The way we categorize the DEs is based on their behavior when translated into English. This is implicitly done in the work of <cite>Wang et al. (2007)</cite> where they use rules to decide if a certain DE and the words next to it will need to be reordered. Some NPs are translated into a hybrid of these categories, or just don't fit into one of the five categories, for instance, involving an adjectival premodifier and a relative clause. In those cases, they are put into an \"other\" category. 1 ----------------------------------",
  "y": "similarities background"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_6",
  "x": "To evaluate the classification performance and understand what features are useful, we compute the accuracy by averaging five 10-fold cross-validations. 2 As a baseline, we use the rules introduced in <cite>Wang et al. (2007)</cite> to decide if the DEs require reordering or not. However, since their rules only decide if there is reordering in an NP with DE, their classification result only has two classes. So, in order to compare our classifier's performance with the rules in <cite>Wang et al. (2007)</cite> , we have to map our five-class results into two classes. We mapped our five-class results into two classes. So we mapped B preposition A and relative clause into the class \"reordered\", and the other three classes into \"not-reordered\". ----------------------------------",
  "y": "extends"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_8",
  "x": "However, since we are using automatic parses instead of gold-standard ones, the DEPOS feature might have other values than just DEC and DEG. From Table 2 , we can see that with this simple feature, the 5-class accuracy is low but at least better than simply guessing the majority class (47.92%). The 2-class accuracy is still lower than using the heuristic rules in<cite> (Wang et al., 2007)</cite> , which is reasonable because their rules encode more information than just the POS tags of DEs. A-pattern: Chinese syntactic patterns appearing before Secondly, we want to incorporate the rules in<cite> (Wang et al., 2007)</cite> as features in the log-linear classifier. We added features for certain indicative patterns in the parse tree (listed in Table 3 ). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_9",
  "x": "true if A+DE is a DNP which is in the form of \"ADJP+DEG\". ---------------------------------- **A IS QP:** true if A+DE is a DNP which is in the form of \"QP+DEG\". 3. A is pronoun: true if A+DE is a DNP which is in the form of \"NP+DEG\", and the NP is a pronoun. 4. A ends with VA: true if A+DE is a CP which is in the form of \"IP+DEC\", and the IP ends with a VP that's either just a VA or a VP preceded by a ADVP. Features 1-3 are inspired by the rules in<cite> (Wang et al., 2007)</cite> , and the fourth rule is based on the observation that even though the predicative adjective VA acts as a verb, it actually corresponds to adjectives in English as described in (Xia, 2000) .",
  "y": "similarities uses"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_11",
  "x": "We have two different settings as baseline experiments. The first is without reordering or DE annotation on the Chinese side; we simply align the parallel texts, extract phrases and tune parameters. This experiment is referred to as BASELINE. Also, we reorder the training data, the tuning and the test sets with the NP rules in<cite> (Wang et al., 2007)</cite> and compare our results with this second baseline (WANG-NP). The NP reordering preprocessing (WANG-NP) showed consistent improvement in Table 5 on all test sets, with BLEU point gains ranging from 0.15 to 0.40. This confirms that having reordering around DEs in NP helps Chinese-English MT. ----------------------------------",
  "y": "uses background"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_12",
  "x": "Our approach DE-Annotated reorders the Chinese sentence, which is similar to the approach proposed by <cite>Wang et al. (2007)</cite> (WANG-NP). However, our focus is on the annotation on DEs and how this can improve translation quality. Table 7 shows an example that contains a DE construction that translates into a relative clause in English. 12 The automatic parse tree of the sentence is listed in Figure 3 . The reordered sentences of WANG-NP and DE-Annotated appear on the top and bottom in Figure 4 . For this example, both systems decide to reorder, but DE-Annotated had the extra information that this is a relc . In Figure 4 we can see that in WANG-NP, \" \" is being translated as \"for\", and the translation afterwards is not grammatically correct.",
  "y": "differences similarities"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_0",
  "x": "In a multilingual setting, an individual's preference to use a local language rather than the national one may reflect their political stance, as the local language can have strong ties to cultural and political identity (Moreno et al., 1998; Crameri, 2017) . The role of linguistic identity is enhanced in extreme situations such as referenda, where the voting decision may be driven by identification with a local culture or language (Schmid, 2001) . In October 2017, the semi-autonomous region of Catalonia held a referendum on independence from Spain, where 92% of respondents voted for independence (Fotheringham, 2017) . To determine the role of the local language Catalan in * Equal contributions. this setting, we apply the methodology used by <cite>Shoemark et al. (2017)</cite> in the context of the 2014 Scottish independence referendum to a dataset of tweets related to the Catalonian referendum. We use the phenomenon of code-switching between Catalan and Spanish to pursue the following research questions in order to understand the choice of language in the context of the referendum: 1. Is a speaker's stance on independence strongly associated with the rate at which they use Catalan?",
  "y": "similarities uses"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_1",
  "x": "A relatively unexplored area of code-switching behavior is politically-motivated code-switching, which we assume has a different set of constraints compared to everyday code-switching. With respect to political separatism, <cite>Shoemark et al. (2017)</cite> studied the use of Scots, a language local to Scotland, in the context of the 2014 Scotland independence referendum. They found that Twitter users who openly supported Scottish independence were more likely to incorporate words from Scots in their tweets. They also found that Twitter users who tweeted about the referendum were less likely to use Scots in referendum-related tweets than in non-referendum tweets. This study considers the similar scenario which took place in 2017 vis-\u00e0-vis the semi-autonomous region of Catalonia. Our main methodological divergence from <cite>Shoemark et al. (2017)</cite> relates to the linguistic phenomenon at hand: while Scots is mainly manifested as interleaving individual words within English text (code-mixing), Catalan is a distinct language which, when used, usually replaces Spanish altogether for the entire tweet (code-switching). ----------------------------------",
  "y": "background"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_2",
  "x": "They also found that Twitter users who tweeted about the referendum were less likely to use Scots in referendum-related tweets than in non-referendum tweets. This study considers the similar scenario which took place in 2017 vis-\u00e0-vis the semi-autonomous region of Catalonia. Our main methodological divergence from <cite>Shoemark et al. (2017)</cite> relates to the linguistic phenomenon at hand: while Scots is mainly manifested as interleaving individual words within English text (code-mixing), Catalan is a distinct language which, when used, usually replaces Spanish altogether for the entire tweet (code-switching). ---------------------------------- **DATA** The initial set of tweets for this study, T , was drawn from a 1% Twitter sample mined between January 1 and October 31, 2017, covering nearly a year of activity before the referendum, as well as its immediate aftermath. 2 The first step in building this dataset was to manually develop a seed set of hashtags related to the referendum.",
  "y": "similarities"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_3",
  "x": "Next, all tweets containing any referendum hashtag were extracted from T , yielding 190,061 tweets. After removing retweets and tweets from users whose tweets frequently contained URLs (i.e., likely bots), our final \"Catalonian Independence Tweets\" (CT) dataset is made up of 11,670 tweets from 10,498 users (cf. the Scottish referendum set IT with 59,664 tweets and 18,589 users in <cite>Shoemark et al. (2017)</cite> ). 36 referendum-related hashtags appear in the filtered dataset. They are shown with their frequencies (including variants) in Table 1 (cf. the 47 hashtags and similar frequency distribution in Table 1 of <cite>Shoemark et al. (2017)</cite> ). To address the control condition, all authors of tweets in the CT dataset were collected to form a set U , and all other tweets in T written by these users were extracted into a control dataset (XT) of 45,222 tweets (cf. the 693,815 control tweets in Table 6 of <cite>Shoemark et al. (2017)</cite> ). The CT dataset is very balanced with respect to the number of tweets per user: only four users contribute over ten tweets (max = 14) and only 16 have more than five.",
  "y": "similarities uses"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_4",
  "x": "36 referendum-related hashtags appear in the filtered dataset. They are shown with their frequencies (including variants) in Table 1 (cf. the 47 hashtags and similar frequency distribution in Table 1 of <cite>Shoemark et al. (2017)</cite> ). To address the control condition, all authors of tweets in the CT dataset were collected to form a set U , and all other tweets in T written by these users were extracted into a control dataset (XT) of 45,222 tweets (cf. the 693,815 control tweets in Table 6 of <cite>Shoemark et al. (2017)</cite> ). The CT dataset is very balanced with respect to the number of tweets per user: only four users contribute over ten tweets (max = 14) and only 16 have more than five. The XT dataset also has only a few \"power\" users, such that nine users have over 1,000 tweets (max = 3,581) and a total of 173 have over 100 tweets. Since the results are macro-averaged over all users, these few power users should not significantly distort the findings.",
  "y": "background"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_5",
  "x": "They are shown with their frequencies (including variants) in Table 1 (cf. the 47 hashtags and similar frequency distribution in Table 1 of <cite>Shoemark et al. (2017)</cite> ). To address the control condition, all authors of tweets in the CT dataset were collected to form a set U , and all other tweets in T written by these users were extracted into a control dataset (XT) of 45,222 tweets (cf. the 693,815 control tweets in Table 6 of <cite>Shoemark et al. (2017)</cite> ). The CT dataset is very balanced with respect to the number of tweets per user: only four users contribute over ten tweets (max = 14) and only 16 have more than five. The XT dataset also has only a few \"power\" users, such that nine users have over 1,000 tweets (max = 3,581) and a total of 173 have over 100 tweets. Since the results are macro-averaged over all users, these few power users should not significantly distort the findings. Language Identification.",
  "y": "background"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_6",
  "x": "The counts of users and tweets identified as either Spanish or Catalan are presented in Table 2 . To measure Catalan usage, let n To determine significance, the users are randomly shuffled between the two groups to recompute d over 100,000 iterations. The p-value is the proportion of permutations in which the randomized test statistic was greater than or equal to the original test statistic from the unpermuted data. Results. Catalan is used more often among the pro-independence users compared to the antiindependence users, across both the hashtagonly and all-tweet conditions. Table 3 shows that the proportion of tweets in Catalan for proindependence users (p pro ) is significantly higher than the proportion for anti-independence users (p anti ). This is consistent with <cite>Shoemark et al. (2017)</cite> , who found more Scots usage among proindependence users (d = 0.00555 for pro/anti tweets, d = 0.00709 for all tweets). The relative differences between the groups are large: in the all-tweet condition,p pro is five times greater than p anti , whereas Shoemark et al. found a twofold difference (p pro = 0.01443 versusp anti = 0.00734 for all-tweet condition).",
  "y": "similarities"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_7",
  "x": "The lack of a significant difference between referendum-related hashtags and other hashtags suggests that the topic being discussed is not as central in choosing one's language, compared with the audience being targeted. Our second result is the opposite of the prior finding that there were significantly fewer Scots words in referendum-related tweets than in control tweets (cf. Table 7 in <cite>Shoemark et al. (2017)</cite> ; d u = \u22120.0015 for all controls). This suggests that Catalan may serve a different function than Scots in terms of political identity expression. Rather than suppressing their use of Catalan in broadcast tweets, users increase their Catalan use, perhaps to signal their Catalonian identity to a broader audience. This is supported by literature highlighting the integral role Catalan plays in the Catalonian national narrative (Crameri, 2017) , as well as the relatively high proportion of Catalan speakers in Catalonia: 80.4% of the population has speaking knowledge of Catalan (Government of Catalonia, 2013) , versus 30% population of Scotland with speaking knowledge of Scots (Scots Language Centre, 2011) . There are also systemic differences between the political settings of the two cases: the Catalonian referendum had much larger support for separation among those who voted (92% in Catalonia vs. 45% in Scotland) (Fotheringham, 2017; Jeavens, 2014) .",
  "y": "differences"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_0",
  "x": "---------------------------------- **INTRODUCTION** Text chunking is a process to identify non-recursive cores of various phrase types without conducting deep parsing of text [3] . Abney first proposed it as an intermediate step toward full parsing [1] . Since Ramshaw and Marcus approached NP chunking using a machine learning method, many researchers have used various machine learning techniques [2, 4, 5, <cite>6,</cite> 10, 11, 13, 14] . The chunking task was extended to the CoNLL-2000 shared task with standard datasets and evaluation metrics, which is now a standard evaluation task for text chunking [3] . Most previous works with relatively high performance in English used machine learning methods for chunking [4, 13] .",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_1",
  "x": "Chunking in Korean texts with only simple heuristic rules obtained through observation on the text shows a good performance similar to other machine learning methods<cite> [6]</cite> . Park et al. proposed a hybrid of rule-based and machine learning method to handle exceptional cases of the rules, to improve the performance of chunking in Korean texts [5,<cite> 6]</cite> . In this paper, we present how CRFs, a recently introduced probabilistic model for labeling and segmenting sequence of data [12] , can be applied to the task of chunking in Korean texts. CRFs are undirected graphical models trained to maximize conditional probabilities of label sequence given input sequence. It takes advantage of generative and conditional models. CRFs can include many correlated, overlapping features, and they are trained discriminatively like conditional model. Since CRFs have single exponential model for the conditional probability of entire label sequence given input sequence, they also guarantee to obtain globally optimal label sequence.",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_2",
  "x": "The conditional approach views the chunking task as a sequence of classification problems, and defines a conditional probability p(Y|X) over label sequence given input sequence. A number of conditional models recently have been developed for use. They showed better performance than generative models as they can handle many arbitrary and overlapping features of input sequence [12] . A number of methods are applied to chunking in Korean texts. Unlike English, a rule-based chunking method [7, 8] is predominantly used in Korean because of its well-developed function words, which contain information such as grammatical relation, case, tense, modal, etc. Chunking in Korean texts with only simple heuristic rules obtained through observation on the text shows a good performance similar to other machine learning methods<cite> [6]</cite> . Park et al. proposed a hybrid of rule-based and machine learning method to handle exceptional cases of the rules, to improve the performance of chunking in Korean texts [5,<cite> 6]</cite> .",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_3",
  "x": "Chunking in Korean texts with only simple heuristic rules obtained through observation on the text shows a good performance similar to other machine learning methods<cite> [6]</cite> . Park et al. proposed a hybrid of rule-based and machine learning method to handle exceptional cases of the rules, to improve the performance of chunking in Korean texts [5,<cite> 6]</cite> . In this paper, we present how CRFs, a recently introduced probabilistic model for labeling and segmenting sequence of data [12] , can be applied to the task of chunking in Korean texts. CRFs are undirected graphical models trained to maximize conditional probabilities of label sequence given input sequence. It takes advantage of generative and conditional models. CRFs can include many correlated, overlapping features, and they are trained discriminatively like conditional model. Since CRFs have single exponential model for the conditional probability of entire label sequence given input sequence, they also guarantee to obtain globally optimal label sequence.",
  "y": "uses background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_4",
  "x": "For example, when the part-of-speech of current word is one of determiner, pronoun and noun, the following seven rules for NP chunking in Table 1 can find most NP chunks in text, with about 89% accuracy<cite> [6]</cite> . For this reason, boundaries of chunks are easily found in Korean, compared to other languages such as English or Chinese. This is why a rule-based chunking method is predominantly used. However, with sophisticated rules, the rule-based chunking method has limitations when handling exceptional cases. Park et al. proposed a hybrid of the rule-based and the machine learning method to resolve this problem [5,<cite> 6]</cite> . Many recent machine learning techniques can capture hidden characteristics for classification. Despite its simplicity and efficiency, the rule-based method has recently been outdone by the machine learning method in various classification problems.",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_5",
  "x": "Park et al. proposed a hybrid of the rule-based and the machine learning method to resolve this problem [5,<cite> 6]</cite> . Many recent machine learning techniques can capture hidden characteristics for classification. Despite its simplicity and efficiency, the rule-based method has recently been outdone by the machine learning method in various classification problems. ---------------------------------- **CHUNK TYPES OF KOREAN** Abney was the first to use the term 'chunk' to represent a non-recursive core of an intra-clausal constituent, extending from the beginning of constituent to its head. In Korean, there are four basic phrases: noun phrase (NP), verb phrase (VP), adverb phrase (ADVP), and independent phrase (IP)<cite> [6]</cite> .",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_6",
  "x": "Many recent machine learning techniques can capture hidden characteristics for classification. Despite its simplicity and efficiency, the rule-based method has recently been outdone by the machine learning method in various classification problems. ---------------------------------- **CHUNK TYPES OF KOREAN** Abney was the first to use the term 'chunk' to represent a non-recursive core of an intra-clausal constituent, extending from the beginning of constituent to its head. In Korean, there are four basic phrases: noun phrase (NP), verb phrase (VP), adverb phrase (ADVP), and independent phrase (IP)<cite> [6]</cite> . As function words such as postposition or ending are well-developed, the number of chunk types is small compared to other languages such as English or Chinese.",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_7",
  "x": "Experiments were performed with C++ implementation of CRFs (FlexCRFs) on Linux with 2.4 GHz Pentium IV dual processors and 2.0Gbyte of main memory [18] . We use L-BFGS to train the parameters and use a Gaussian prior regularization in order to avoid overfitting [20] . Total number of CRF features is 83,264. As shown in Table 5 , the performances of most chunk type are 96~100%, very high performance. However, the performance of NP chunk type is lowest, 94.27% because the border of NP chunk type is very ambiguous in case of consecutive nouns. Using more features such as previous chunk tag should be able to improve the performance of NP chunk type. Park et al. reported the performance of various chunking methods<cite> [6]</cite> .",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_8",
  "x": "Total number of CRF features is 83,264. As shown in Table 5 , the performances of most chunk type are 96~100%, very high performance. However, the performance of NP chunk type is lowest, 94.27% because the border of NP chunk type is very ambiguous in case of consecutive nouns. Using more features such as previous chunk tag should be able to improve the performance of NP chunk type. Park et al. reported the performance of various chunking methods<cite> [6]</cite> . We add the experimental results of the chunking methods using HMMs-bigram and CRFs. In Table 6 , F-score of chunking using CRFs in Korean texts is 97.19%, the highest performance of all.",
  "y": "similarities"
 },
 {
  "id": "3ec2dc9530699f55b8a4c234532daf_0",
  "x": "---------------------------------- **INTRODUCTION** Chatbots aim to engage users in open-domain human-computer conversations and are currently receiving increasing attention. The existing work on building chatbots includes generation-based methods and retrieval-based methods. The first type of methods synthesize a response with a natural language generation model (Shang et al., 2015; Serban et al., 2016; . In this paper, we focus on the second type and study the problem of multi-turn response selection. This task aims to select the best-matched response from a set of candidates, given the context of a conversation which is composed of multiple utterances<cite> (Lowe et al., 2015</cite>; Lowe et al., 2017; Wu et al., 2017 ).",
  "y": "background"
 },
 {
  "id": "3ec2dc9530699f55b8a4c234532daf_1",
  "x": "This approach enables the incorporation of rich context when mapping between consecutive dialogue turns (Shang et al., 2015; Serban et al., 2016; . Recently, some extended work has been made to incorporate external knowledge into generation with specific personas or emotions (Li et al., 2016; Zhou et al., 2018a) . Our work belongs to the retrieval-based methods, which learn a matching model for a pair of a conversational context and a response candidate. This approach has the advantage of providing informative and fluent responses because they select a proper response for the current conversation from a repository by means of response selection algorithms<cite> (Lowe et al., 2015</cite>; Lowe et al., 2017 EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EB  EB  EB  EB  EB   E0  E1  E2  E3  E4  E5  E18  E19  E6  E7  E8  E9  E10  E11  E17  E12  E13  E14  E15  E16  E20  E21  E22  E23  E24  E25 [ E0  E0  E0  E0  E0  E0  E0  E0  E1  E1  E1  E1  E1  E1  E0  E1  E1  E1  E1  E1  E0  E1  E1  E1  E1  E1 Speaker Embeddings + + + + + + + + + + + + + + + + + + + + + + + + + + Figure 1 : The input representation of SA-BERT. The final input embeddings are the sum of the token embeddings, the segmentation embeddings, the position embeddings and the speaker embeddings. Wu et al., 2017; Zhang et al., 2018b) . Previous work on retrieval-based chatbots focused on singleturn response selection (Wang et al., 2013; Ji et al., 2014) .",
  "y": "similarities background"
 },
 {
  "id": "3ec2dc9530699f55b8a4c234532daf_2",
  "x": "**OUTPUT REPRESENTATION** The first token of each concatenated sequence is the [CLS] token, with its embedding being used as the aggregated representation for a context-response pair classification. This embedding captures the matching information between a context-response pair, which is sent into a classifier with a sigmoid output layer. Parameters of this classifier need to be estimated during the fine-tuning process. Finally, the classifier returns a score to denote the matching degree of this context-response pair. We tested SA-BERT on five public multi-turn response selection datasets, Ubuntu Dialogue Corpus V1<cite> (Lowe et al., 2015)</cite> , Ubuntu Dialogue Corpus V2 (Lowe et al., 2017) , Douban Conversation Corpus (Wu et al., 2017) , E-commerce Dialogue Corpus (Zhang et al., 2018b) and DSTC 8-Track 2-Subtask 2 Corpus (Seokhwan Kim, 2019). The first four datasets have been disentangled in advance and our proposed speaker-aware disentanglement strategy has been applied to only the last DSTC 8-Track 2-Subtask 2 Corpus.",
  "y": "background uses"
 },
 {
  "id": "3ec2dc9530699f55b8a4c234532daf_3",
  "x": "---------------------------------- **EVALUATION METRICS** We used the same evaluation metrics as those used in previous work<cite> (Lowe et al., 2015</cite>; Lowe et al., 2017; Wu et al., 2017; Zhang et al., 2018b; Seokhwan Kim, 2019) . Each model was tasked with selecting the k best-matched responses from n available candidates for the given conversation context c, and we calculated the recall of the true positive replies among the k selected responses, denoted as R n @k, as the main evaluation metric. In addition to R n @k, we considered the mean average precision (MAP) (BaezaYates and Ribeiro-Neto, 1999), mean reciprocal rank (MRR) (Voorhees, 1999) and precision-at-one (P@1), especially for the Douban corpus, following the settings of previous work. ---------------------------------- **TRAINING DETAILS**",
  "y": "uses"
 },
 {
  "id": "400bd47879aaed0aa1195bafe54e76_0",
  "x": "We introduced KaWAT (Kata Word Analogy Task), a new word analogy task dataset for Indonesian. We evaluated on it several existing pretrained Indonesian word embeddings and embeddings trained on Indonesian online news corpus. We also tested them on two downstream tasks and found that pretrained word embeddings helped either by reducing the training epochs or yielding significant performance gains. ---------------------------------- **INTRODUCTION** Despite the existence of various Indonesian pretrained word embeddings, there are no publicly available Indonesian analogy task datasets on which to evaluate these embeddings. Consequently, it is unknown if Indonesian word embeddings introduced in, e.g., (Al-Rfou et al., 2013) and <cite>(Grave et al., 2018)</cite> , capture syntactic or semantic information as measured by analogy tasks.",
  "y": "background"
 },
 {
  "id": "400bd47879aaed0aa1195bafe54e76_1",
  "x": "KaWAT is available online. 1 One of the goals of this work is to evaluate and compare existing Indonesian pretrained word embeddings. We used fastText pretrained embeddings introduced in (Bojanowski et al., 2017 ) and <cite>(Grave et al., 2018)</cite> , which have been trained on Indonesian Wikipedia and Indonesian Wikipedia plus Common Crawl data respectively. We refer to them as Wiki/fastText and CC/fastText hereinafter. We also used another two pretrained embeddings: polyglot embedding trained on Indonesian Wikipedia (Al-Rfou et al., 2013) and NLPL embedding trained on the Indonesian portion of CoNLL 2017 corpus (Fares et al., 2017) . For training our word embeddings, we used online news corpus obtained from Tempo. 2 We used Tempo newspaper and magazine articles up to year 2014.",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_0",
  "x": "**INTRODUCTION** Distributed word representations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; . Traditionally, word representations have been obtained using countbased methods (Baroni et al., 2014) , where the cooccurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; . Techniques for generating lower-rank representations have also been employed, such as PPMI-SVD <cite>(Levy et al., 2015)</cite> and GloVe (Pennington et al., 2014) , both achieving state-of-the-art performance on a variety of tasks. * This is a preprint of the paper that will be presented at the 54th Annual Meeting of the Association for Computational Linguistics. Alternatively, vector-space models can be generated with predictive methods, which generally outperform the count-based methods (Baroni et al., 2014) , the most notable of which is Skip-gram with Negative Sampling (SGNS, Mikolov et al. (2013b) ), which uses a neural network to generate embeddings. It implicitly factorizes a shifted PMI matrix, and its performance has been linked to the weighting of positive and negative co-occurrences .",
  "y": "background"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_1",
  "x": "As this matrix is unbounded in the inferior limit, in most applications it is replaced by its positive definite version, PPMI, where negative values are set to zero. The performance of the PPMI matrix on word similarity tasks can be further improved by using context-distribution smoothing <cite>(Levy et al., 2015)</cite> and subsampling the corpus (Mikolov et al., 2013b) . As word embeddings with lower dimensionality may improve efficiency and generalization <cite>(Levy et al., 2015)</cite> , the improved PPMI * matrix can be factorized as a product of two lower rank matrices. where W w andW c are d-dimensional row vectors corresponding to vector embeddings for the target and context words. Using the truncated SVD of size d yields the factorization U \u03a3T \u22a4 with the lowest possible L 2 error (Eckert and Young, 1936) . Levy et al. (2015) recommend using W = U \u03a3 p as the word representations, as suggested by Bullinaria and Levy (2012) , who borrowed the idea of weighting singular values from the work of Caron (2001) on Latent Semantic Analysis. Although the optimal value of p is highly task-dependent (\u00d6sterlund et al., 2015) , we set p = 0.5 as it has been shown to perform well on the word similarity and analogy tasks we use in our experiments <cite>(Levy et al., 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_2",
  "x": "**PPMI-SVD** Given a word w and a symmetric window of win context words to the left and win to the right, the co-occurrence matrix of elements M wc is defined as the number of times a target word w and the context word c co-occurred in the corpus within the window. The PMI matrix is defined as where '*' represents the summation of the corresponding index. As this matrix is unbounded in the inferior limit, in most applications it is replaced by its positive definite version, PPMI, where negative values are set to zero. The performance of the PPMI matrix on word similarity tasks can be further improved by using context-distribution smoothing <cite>(Levy et al., 2015)</cite> and subsampling the corpus (Mikolov et al., 2013b) . As word embeddings with lower dimensionality may improve efficiency and generalization <cite>(Levy et al., 2015)</cite> , the improved PPMI * matrix can be factorized as a product of two lower rank matrices.",
  "y": "background"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_3",
  "x": "Using the truncated SVD of size d yields the factorization U \u03a3T \u22a4 with the lowest possible L 2 error (Eckert and Young, 1936) . Levy et al. (2015) recommend using W = U \u03a3 p as the word representations, as suggested by Bullinaria and Levy (2012) , who borrowed the idea of weighting singular values from the work of Caron (2001) on Latent Semantic Analysis. Although the optimal value of p is highly task-dependent (\u00d6sterlund et al., 2015) , we set p = 0.5 as it has been shown to perform well on the word similarity and analogy tasks we use in our experiments <cite>(Levy et al., 2015)</cite> . ---------------------------------- **GLOVE** GloVe (Pennington et al., 2014) factors the logarithm of the co-occurrence matrixM that considers the position of the context words in the window. The loss function for factorization is",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_4",
  "x": "**MATERIALS** All models were trained on a dump of Wikipedia from June 2015, split into sentences, with punctuation removed, numbers converted to words, and lower-cased. Words with less than 100 counts were removed, resulting in a vocabulary of 302,203 words. All models generate embeddings of 300 dimensions. The PPMI* matrix used by both PPMI-SVD and LexVec was constructed using smoothing of \u03b1 = 3/4 suggested in <cite>(Levy et al., 2015)</cite> and an unweighted window of size 2. A dirty subsampling of the corpus is adopted for PPMI* and SGNS with threshold of t = 10 \u22125 (Mikolov et al., 2013b) . 2 Additionally, SGNS uses 5 negative samples (Mikolov et al., 2013b) , a window of size 10 (<cite> Levy et al., 2015)</cite> , for 5 iterations with initial learning rate set to the default 0.025.",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_5",
  "x": "Words with less than 100 counts were removed, resulting in a vocabulary of 302,203 words. All models generate embeddings of 300 dimensions. The PPMI* matrix used by both PPMI-SVD and LexVec was constructed using smoothing of \u03b1 = 3/4 suggested in <cite>(Levy et al., 2015)</cite> and an unweighted window of size 2. A dirty subsampling of the corpus is adopted for PPMI* and SGNS with threshold of t = 10 \u22125 (Mikolov et al., 2013b) . 2 Additionally, SGNS uses 5 negative samples (Mikolov et al., 2013b) , a window of size 10 (<cite> Levy et al., 2015)</cite> , for 5 iterations with initial learning rate set to the default 0.025. GloVe is run with a window of size 10, x max = 100, \u03b2 = 3/4, for 50 iterations and initial learning rate of 0.05 (Pennington et al., 2014) . In LexVec two window sampling alternatives are compared: W S P P M I , which keeps the same fixed size win = 2 as used to create the P P M I * matrix; or W S SGN S , which adopts identical SGNS settings (win = 10 with size randomization).",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_6",
  "x": "All methods generate both word and context matrices (W andW ): W is used for SGNS, PPMI-SVD and W +W for GloVe (following<cite> Levy et al. (2015)</cite> , and W and W +W for LexVec. For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Pennington et al., 2014;<cite> Levy et al., 2015</cite> factorization of logM ) and Skip-gram (implicit factorization of the shifted PMI matrix), and compare the stochastic and mini-batch approaches. Word similarity tasks are: 3 WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001) , MEN (Bruni et al., 2012 ), MTurk (Radinsky et al., 2011 , RW (Luong et al., 2013) , SimLex-999 (Hill et al., 2015) , MC (Miller and Charles, 1991) , RG (Rubenstein and Goodenough, 1965) , and SCWS (Huang et al., 2012) , calculated using cosine. Word analogy tasks are: Google semantic (GSem) and syntactic (GSyn) (Mikolov et al., 2013a) and MSR syntactic analogy dataset (Mikolov et al., 2013c) , using 3CosAdd and 3CosM ul . ---------------------------------- **RESULTS** Results for word similarity and for the analogy tasks are in tables 1 and 2, respectively.",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_7",
  "x": "We run LexVec for 5 iterations over the training corpus. All methods generate both word and context matrices (W andW ): W is used for SGNS, PPMI-SVD and W +W for GloVe (following<cite> Levy et al. (2015)</cite> , and W and W +W for LexVec. For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Pennington et al., 2014;<cite> Levy et al., 2015</cite> factorization of logM ) and Skip-gram (implicit factorization of the shifted PMI matrix), and compare the stochastic and mini-batch approaches. Word similarity tasks are: 3 WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001) , MEN (Bruni et al., 2012 ), MTurk (Radinsky et al., 2011 , RW (Luong et al., 2013) , SimLex-999 (Hill et al., 2015) , MC (Miller and Charles, 1991) , RG (Rubenstein and Goodenough, 1965) , and SCWS (Huang et al., 2012) , calculated using cosine. Word analogy tasks are: Google semantic (GSem) and syntactic (GSyn) (Mikolov et al., 2013a) and MSR syntactic analogy dataset (Mikolov et al., 2013c) , using 3CosAdd and 3CosM ul . ---------------------------------- **RESULTS**",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_8",
  "x": "Levy et al. (2015) obtained similar results, and suggest that using positional contexts as done by might help in recovering syntactic analogies. In terms of configurations, WS SGN S performed marginally better than WS P P M I . We hypothesize it is simply because of the additional computation. While W and (W +W ) are roughly equivalent on word similarity tasks, W is better for analogies. This is inline with results for PPMI-SVD and SGNS models <cite>(Levy et al., 2015)</cite> . Both mini-batch and stochastic approaches result in similar scores for all tasks. For the same parameter k of negative samples, the mini-batch approach uses 2 * win W S P P M I times more negative samples than stochastic when using W S P P M I , and win W S SGNS times more samples when using W S SGN S .",
  "y": "similarities"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_9",
  "x": "In terms of configurations, WS SGN S performed marginally better than WS P P M I . We hypothesize it is simply because of the additional computation. While W and (W +W ) are roughly equivalent on word similarity tasks, W is better for analogies. This is inline with results for PPMI-SVD and SGNS models <cite>(Levy et al., 2015)</cite> . Both mini-batch and stochastic approaches result in similar scores for all tasks. For the same parameter k of negative samples, the mini-batch approach uses 2 * win W S P P M I times more negative samples than stochastic when using W S P P M I , and win W S SGNS times more samples when using W S SGN S . Therefore, the stochastic approach is more computationally efficient while delivering similar performance.",
  "y": "similarities"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_10",
  "x": "Levy et al. (2015) obtained similar results, and suggest that using positional contexts as done by might help in recovering syntactic analogies. In terms of configurations, WS SGN S performed marginally better than WS P P M I . We hypothesize it is simply because of the additional computation. While W and (W +W ) are roughly equivalent on word similarity tasks, W is better for analogies. This is inline with results for PPMI-SVD and SGNS models <cite>(Levy et al., 2015)</cite> . Both mini-batch and stochastic approaches result in similar scores for all tasks. For the same parameter k of negative samples, the mini-batch approach uses 2 * win W S P P M I times more negative samples than stochastic when using W S P P M I , and win W S SGNS times more samples when using W S SGN S .",
  "y": "similarities"
 },
 {
  "id": "40d370558d873499e493a83f106f17_0",
  "x": "---------------------------------- **INTRODUCTION** Large-scale distributional thesauri created automatically from corpora (Grefenstette, 1994;<cite> Lin, 1998</cite>; Weeds et al., 2004; Ferret, 2012) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998 ) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures<cite> (Lin, 1998</cite>; Curran and Moens, 2002; Ferret, 2010) , identifying and demoting bad neighbors (Ferret, 2013) , or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013) . For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is.",
  "y": "background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_1",
  "x": "Large-scale distributional thesauri created automatically from corpora (Grefenstette, 1994;<cite> Lin, 1998</cite>; Weeds et al., 2004; Ferret, 2012) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998 ) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures<cite> (Lin, 1998</cite>; Curran and Moens, 2002; Ferret, 2010) , identifying and demoting bad neighbors (Ferret, 2013) , or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013) . For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is. To remove uninformative and noisy contexts, filters have often been applied like pointwise mutual information (PMI), lexicographer's mutual information (LMI) (Biemann and Riedl, 2013) , t-score (Piasecki et al., 2007) and z-score (Broda et al., 2009 ). However, the selection of a measure and of a threshold value for these filters is generally empirically determined.",
  "y": "background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_2",
  "x": "In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears<cite> (Lin, 1998</cite>; McCarthy et al., 2003; Weeds et al., 2004) . The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like <cite>Lin's (1998)</cite> , cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area<cite> (Lin, 1998</cite>; Curran and Moens, 2002) . For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri<cite> (Lin, 1998)</cite> , and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004) . Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed. For instance, Biemann and Riedl (2013) found that filtering a subset of contexts based on LMI increased the similarity of a thesaurus with WordNet.",
  "y": "background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_3",
  "x": "The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears<cite> (Lin, 1998</cite>; McCarthy et al., 2003; Weeds et al., 2004) . The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like <cite>Lin's (1998)</cite> , cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area<cite> (Lin, 1998</cite>; Curran and Moens, 2002) . For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri<cite> (Lin, 1998)</cite> , and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004) . Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed. For instance, Biemann and Riedl (2013) found that filtering a subset of contexts based on LMI increased the similarity of a thesaurus with WordNet. In this work, we compare the impact of using different types of filters in terms of thesaurus agreement with WordNet, focusing on a distributional thesaurus of English verbs.",
  "y": "background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_4",
  "x": "Evaluation of the quality of distributional thesauri is a well know problem in the area<cite> (Lin, 1998</cite>; Curran and Moens, 2002) . For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri<cite> (Lin, 1998)</cite> , and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004) . Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed. For instance, Biemann and Riedl (2013) found that filtering a subset of contexts based on LMI increased the similarity of a thesaurus with WordNet. In this work, we compare the impact of using different types of filters in terms of thesaurus agreement with WordNet, focusing on a distributional thesaurus of English verbs. We also propose a frequency-based saliency measure to rank and filter contexts and compare it with PMI and LMI. Extrinsic evaluation of distributional thesauri has been carried out for tasks such as English lexical substitution (McCarthy and Navigli, 2009), phrasal verb compositionality detection (McCarthy et al., 2003) and the WordNet-based synonymy test (WBST) (Freitag et al., 2005) .",
  "y": "motivation background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_5",
  "x": "For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri<cite> (Lin, 1998)</cite> , and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004) . Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed. For instance, Biemann and Riedl (2013) found that filtering a subset of contexts based on LMI increased the similarity of a thesaurus with WordNet. In this work, we compare the impact of using different types of filters in terms of thesaurus agreement with WordNet, focusing on a distributional thesaurus of English verbs. We also propose a frequency-based saliency measure to rank and filter contexts and compare it with PMI and LMI. Extrinsic evaluation of distributional thesauri has been carried out for tasks such as English lexical substitution (McCarthy and Navigli, 2009), phrasal verb compositionality detection (McCarthy et al., 2003) and the WordNet-based synonymy test (WBST) (Freitag et al., 2005) . For comparative purposes in this work we adopt the latter.",
  "y": "background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_6",
  "x": "The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears<cite> (Lin, 1998</cite>; McCarthy et al., 2003; Weeds et al., 2004) . The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like <cite>Lin's (1998)</cite> , cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area<cite> (Lin, 1998</cite>; Curran and Moens, 2002) . For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri<cite> (Lin, 1998)</cite> , and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004) . Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed. For instance, Biemann and Riedl (2013) found that filtering a subset of contexts based on LMI increased the similarity of a thesaurus with WordNet. In this work, we compare the impact of using different types of filters in terms of thesaurus agreement with WordNet, focusing on a distributional thesaurus of English verbs.",
  "y": "motivation background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_7",
  "x": "The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears<cite> (Lin, 1998</cite>; McCarthy et al., 2003; Weeds et al., 2004) . The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like <cite>Lin's (1998)</cite> , cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area<cite> (Lin, 1998</cite>; Curran and Moens, 2002) . For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri<cite> (Lin, 1998)</cite> , and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004) . Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed. For instance, Biemann and Riedl (2013) found that filtering a subset of contexts based on LMI increased the similarity of a thesaurus with WordNet. In this work, we compare the impact of using different types of filters in terms of thesaurus agreement with WordNet, focusing on a distributional thesaurus of English verbs.",
  "y": "motivation"
 },
 {
  "id": "40d370558d873499e493a83f106f17_8",
  "x": "For comparative purposes in this work we adopt the latter. ---------------------------------- **METHODOLOGY** We focus on thesauri of English verbs constructed from the BNC (Burnard, 2007) 1 . Contexts are extracted from syntactic dependencies generated by RASP (Briscoe et al., 2006) , using nouns (heads of NPs) which have subject and direct object relations with the target verb. Thus, each target verb is represented by a set of triples containing (i) the verb itself, (ii) a context noun and (iii) a syntactic relation (object, subject). The thesauri were constructed using <cite>Lin's (1998)</cite> method.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_0",
  "x": "which is another transformer-based model trained on the WebText dataset, has also set state-of-theart benchmarks [27] . It is thus imperative to evaluate the extent to which they exhibit social and intersectional bias. May et al. <cite>[21]</cite> establish a preliminary study of social bias in BERT, but their analysis relies only on sentence level encodings. Our work extends beyond this to provide an analysis of gender and racial bias on a variety of state-of-the-art contextual word models. We also evaluate intersectional (gender + race) bias, since the lived experience of groups with multiple minority identities is cumulatively worse than that of each of the groups with a singular minority identity [9] . We adapt the Sentence Encoder Association Test (SEAT) <cite>[21]</cite> to evaluate how these techniques displays bias in contextual word representations. This effectively gives a new metric that considers a word embedding and its bias in context.",
  "y": "motivation background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_1",
  "x": "May et al. <cite>[21]</cite> establish a preliminary study of social bias in BERT, but their analysis relies only on sentence level encodings. Our work extends beyond this to provide an analysis of gender and racial bias on a variety of state-of-the-art contextual word models. We also evaluate intersectional (gender + race) bias, since the lived experience of groups with multiple minority identities is cumulatively worse than that of each of the groups with a singular minority identity [9] . We adapt the Sentence Encoder Association Test (SEAT) <cite>[21]</cite> to evaluate how these techniques displays bias in contextual word representations. This effectively gives a new metric that considers a word embedding and its bias in context. We find that the standard corpora for pre-training contextual word models exhibit significant gender bias imbalances. Furthermore, we show evidence of social and intersectional bias in state-of-the-art contextual word models.",
  "y": "extends"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_2",
  "x": "Bolukbasi et al. [2] and Caliskan et al. [5] demonstrate that word embeddings associate occupations with their stereotypical gender roles (eg. doctors are stereotypically male, nurses are stereotypically female) by evaluating occupation words with Word Embedding Association Tests (WEATs) to gender words. Inspired by implicit association tests, WEATs compute the differences in distances when word vectors are asked to pair two concepts that are similar (e.g., stereotypically female occupation words and female gender words) as opposed to two concepts that are different (e.g., stereotypically male occupation words and female gender words). Brunet et al. [4] trace the internalization of gender bias to representational differences at the corpus level. This line of work has very recently been extended to evaluate gender bias in contextual word representations in some specific settings. Zhao et al. [35] and Basta et al. [1] demonstrate gender bias in ELMo [25] word embeddings, whereas May et al. <cite>[21]</cite> evaluate various models of contextual word representations on a sentential generalization of WEAT. Our work extends such analyses in two ways: 1) we consider a wide variety of contextual word embedding tools including state-of-the-art approaches such as BERT and GPT-2, 2) we extend the evaluation to consider contextual word representations as opposed to prior work, which either used word embeddings without context or used sentence-level embeddings that can have additional confounds and obscure bias, and 3) we include additional embedding association tests targeting gender, race and intersectional identities. 1 and BooksCorpus (800M words) [36] .",
  "y": "background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_3",
  "x": "Furthermore, the use of such datasets is entrenched into NLP practice due to the high costs of constructing new datasets and importance of dataset size in training models, particularly contextual word models. Barring the precise construction of datasets that reach representational parity across social groups, other techniques like corpus-level constraints [32] , learning constraints [6] or post-processing [2] are likely to be more convenient solutions, though they ignore the origins of the problem [4] . ---------------------------------- **SOCIAL AND INTERSECTIONAL BIAS USING EMBEDDING ASSOCIATION TESTS** ---------------------------------- **EMBEDDING ASSOCIATION TESTS** We adopt the methodology of Caliskan et al. [5] and May et al. <cite>[21]</cite> to test social and intersectional bias using embedding association tests with contextual word representations.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_4",
  "x": "**SOCIAL AND INTERSECTIONAL BIAS USING EMBEDDING ASSOCIATION TESTS** ---------------------------------- **EMBEDDING ASSOCIATION TESTS** We adopt the methodology of Caliskan et al. [5] and May et al. <cite>[21]</cite> to test social and intersectional bias using embedding association tests with contextual word representations. Caliskan et al. [5] proposed Word Embedding Association Tests (WEATs), which follows human implicit association tests [14] in measuring the association between two target concepts and two attributes. We follow May et al. <cite>[21]</cite> in describing WEATs and SEATs. Let X and Y be equal-size sets of target concept embeddings, and A and B be sets of attribute embeddings.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_5",
  "x": "s(w, A, B) = mean a\u2208A cos(w, a) \u2212 mean b\u2208B cos(w, b). To compute the significance of the association between (A, B) and (X, Y ), a permutation test on s(X, Y, A, B) is used. where the probability is computed over the space of partitions (X i , Y i ) of X \u222a Y so that X i and Y i are of equal size. The effect size is defined to be A larger effect size corresponds to more severe pro-stereotypical representations, controlling for significance. May et al. <cite>[21]</cite> adopt the WEAT tests [5] into Sentence Encoder Association Tests (SEATs) to test biases using sentence encodings. The embeddings used in the association tests are encodings of a sentence, which are obtained by pooling per token contextual representations, or by using the representation of the first or last token.",
  "y": "background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_6",
  "x": "We refer to WEATs and SEATs as Caliskan tests [5] . ---------------------------------- **EXTENSION OF EMBEDDING ASSOCIATION TESTS TO CONTEXTUAL WORD REPRESENTATIONS** May et al. <cite>[21]</cite> suggest that although they find less bias in sentence encoders than context free word embeddings, the sentence templates may not be as semantically bleached as expected, and that a lack of evidence of bias should not be taken as a lack of bias. We propose to assess bias at the contextual word level. This allows an investigation into the bias of contextual word representation models (which allow for sentence encoding), and at the same time avoids confounding contextual effects at the sentence level, which can obscure bias. To determine underlying bias in contextual word representations, we adopt SEATs and make a simple modification.",
  "y": "background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_7",
  "x": "To investigate social and intersectional bias, we introduce new embedding association tests to more comprehensively target race, gender and intersectional identities. The new tests are prefixed by a \"+\" in Tables 3, 4 and 5. For race and gender, we are interested in attributes of pleasantness (P/U: Pleasant/Unpleasant), work (Career/Family), discipline (Science/Arts) [5] and the Heilman double bind [15,<cite> 21]</cite> . The Heilman double bind refers to how women, when clearly succeeding in a stereotypically male occupation, are perceived as less likable than similar men, and how women, when success is more ambiguous, are perceived as less competent than similar men. Although the Heilman double bind originated in the context of gender, we also extend the attribute lists 3 of competence and likability to the context of race. We preserve and report the original WEATs, SEATs and tests introduced by May et al. <cite>[21]</cite> where possible. We also prefer tests using names (e.g. Alice) as concept words over group terms (e.g. European American), since names were demonstrated to have a significant association more often than group terms <cite>[21]</cite> For intersectional identities, we are focused primarily on the identity which is the subject of discussion in the work of Crenshaw [9] : being both African American and female.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_8",
  "x": "The new tests are prefixed by a \"+\" in Tables 3, 4 and 5. For race and gender, we are interested in attributes of pleasantness (P/U: Pleasant/Unpleasant), work (Career/Family), discipline (Science/Arts) [5] and the Heilman double bind [15,<cite> 21]</cite> . The Heilman double bind refers to how women, when clearly succeeding in a stereotypically male occupation, are perceived as less likable than similar men, and how women, when success is more ambiguous, are perceived as less competent than similar men. Although the Heilman double bind originated in the context of gender, we also extend the attribute lists 3 of competence and likability to the context of race. We preserve and report the original WEATs, SEATs and tests introduced by May et al. <cite>[21]</cite> where possible. We also prefer tests using names (e.g. Alice) as concept words over group terms (e.g. European American), since names were demonstrated to have a significant association more often than group terms <cite>[21]</cite> For intersectional identities, we are focused primarily on the identity which is the subject of discussion in the work of Crenshaw [9] : being both African American and female. Specifically, <cite>[21]</cite> , which targets the stereotype of black women as loud, angry, and imposing [8] .",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_9",
  "x": "We also prefer tests using names (e.g. Alice) as concept words over group terms (e.g. European American), since names were demonstrated to have a significant association more often than group terms <cite>[21]</cite> For intersectional identities, we are focused primarily on the identity which is the subject of discussion in the work of Crenshaw [9] : being both African American and female. Specifically, <cite>[21]</cite> , which targets the stereotype of black women as loud, angry, and imposing [8] . 5 Empirical Analysis ---------------------------------- **EXPERIMENTS** We investigate biases in GPT-2 [27] , one of the state-of-the-art models for contextual word representations, in both its 117M and 345M versions. For comparison with previous work [1,<cite> 21,</cite> 35] , we also report on other word representation models: CBoW-GLoVe [24] , ELMo [25] , BERT bert-base-cased (bbc) and bert-large-cased (blc) versions [10] , and GPT [26] .",
  "y": "uses background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_10",
  "x": "The new tests are prefixed by a \"+\" in Tables 3, 4 and 5. For race and gender, we are interested in attributes of pleasantness (P/U: Pleasant/Unpleasant), work (Career/Family), discipline (Science/Arts) [5] and the Heilman double bind [15,<cite> 21]</cite> . The Heilman double bind refers to how women, when clearly succeeding in a stereotypically male occupation, are perceived as less likable than similar men, and how women, when success is more ambiguous, are perceived as less competent than similar men. Although the Heilman double bind originated in the context of gender, we also extend the attribute lists 3 of competence and likability to the context of race. We preserve and report the original WEATs, SEATs and tests introduced by May et al. <cite>[21]</cite> where possible. We also prefer tests using names (e.g. Alice) as concept words over group terms (e.g. European American), since names were demonstrated to have a significant association more often than group terms <cite>[21]</cite> For intersectional identities, we are focused primarily on the identity which is the subject of discussion in the work of Crenshaw [9] : being both African American and female. Specifically, <cite>[21]</cite> , which targets the stereotype of black women as loud, angry, and imposing [8] .",
  "y": "background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_11",
  "x": "For comparison with previous work [1,<cite> 21,</cite> 35] , we also report on other word representation models: CBoW-GLoVe [24] , ELMo [25] , BERT bert-base-cased (bbc) and bert-large-cased (blc) versions [10] , and GPT [26] . For all association tests, we use p = 0.01 for significance testing. We use PyTorch, as well as the framework and code from May et al. <cite>[21]</cite> , to conduct the experiments 6 . ---------------------------------- **OVERALL ANALYSIS** We report the proportion of tests with significant effects in Table 2 . Note that all instances of significant effects had positive effect sizes. We observe that the context-free GloVe model exhibits the highest overall proportion of significant positive effect sizes, indicating severe pro-stereotypical bias.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_12",
  "x": "We investigate biases in GPT-2 [27] , one of the state-of-the-art models for contextual word representations, in both its 117M and 345M versions. For comparison with previous work [1,<cite> 21,</cite> 35] , we also report on other word representation models: CBoW-GLoVe [24] , ELMo [25] , BERT bert-base-cased (bbc) and bert-large-cased (blc) versions [10] , and GPT [26] . For all association tests, we use p = 0.01 for significance testing. We use PyTorch, as well as the framework and code from May et al. <cite>[21]</cite> , to conduct the experiments 6 . ---------------------------------- **OVERALL ANALYSIS** We report the proportion of tests with significant effects in Table 2 . Note that all instances of significant effects had positive effect sizes.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_13",
  "x": "Furthermore, methods for de-biasing specifically across race, gender, and intersectional identities remains a challenging open question. Similar to May et al. <cite>[21]</cite> , in the continuous bag of words (CBoW) model we encode sentences as the average of word embeddings using 300-dimensional GloVe vectors 7 trained on the Common Crawl corpus [24] . There is no corresponding contextual word level equivalent since GloVe is context-free. ---------------------------------- **A DETAILS FOR MODELS** ---------------------------------- **A.2 ELMO**",
  "y": "similarities"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_14",
  "x": "---------------------------------- **A.2 ELMO** Following May et al. <cite>[21]</cite> , the sentence encoding of ELMo is a sequence of vectors, one for each token. We use mean-pooling over the tokens, followed by summation over aggregated layer outputs to obtain the final 1024-dimensional sentence encoding. At the contextual word level, we do not apply mean-pooling over tokens. Rather, we select the vector corresponding to the token of interest (i.e. for the sentence \"This is Shanice\", the vector of interest corresponds to \"Shanice\"), and sum over the layer outputs to obtain the 1024-dimensional contextual word encoding. We use the implementation of ELMo from AllenNLP 8 .",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_15",
  "x": "Since BERT uses subword tokenization, if the token of interest is subword tokenized we use the representation corresponding to the start of the token. We conduct experiments on both the 768-dimensional bert-base-cased (bbc) and 1024dimensional bert-large-cased (blc) versions of BERT. We use the implementations of BERT from Hugging Face 9 . ---------------------------------- **A.4 GPT** Similar to May et al. <cite>[21]</cite> and the original word [26] , we use the representation corresponding to the last word in the sequence as the sentence encoding. At the contextual word level, we use the representation corresponding to the token of interest.",
  "y": "similarities"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_16",
  "x": "GPT also uses subword tokenization, so the start of the token is used if the token of interest is subword tokenized. Both encoding types are 768-dimensional. Different from May et al. <cite>[21]</cite> , we use the implementation of GPT from Hugging Face, and not the jiant project 10 . prefix weat but sentence level tests have the prefix sent-weat. The Caliskan Tests are named weat1 to weat10and sent-weat1 to sent-weat10. Alternate versions of the Caliskan Tests using group terms instead of names and vice versa are denotetd by the suffix b. The additional tests are named weat+11 to weat+13 and sent-weat+11 to sent-weat+13, as well as weat+i1 to weat+i5 and sent-weat+i1 to sent-weat+i5, and lastly weat+occ and sent-weat+occ. The double bind tests targeting gender are denoted with the prefix weat_hdb or sent-weat_hdb.",
  "y": "differences"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_17",
  "x": "prefix weat but sentence level tests have the prefix sent-weat. The Caliskan Tests are named weat1 to weat10and sent-weat1 to sent-weat10. Alternate versions of the Caliskan Tests using group terms instead of names and vice versa are denotetd by the suffix b. The additional tests are named weat+11 to weat+13 and sent-weat+11 to sent-weat+13, as well as weat+i1 to weat+i5 and sent-weat+i1 to sent-weat+i5, and lastly weat+occ and sent-weat+occ. The double bind tests targeting gender are denoted with the prefix weat_hdb or sent-weat_hdb. The double bind tests targeting race are denoted with the prefix weat_r_hdb or sent-weat_r_hdb. The tests regarding the angry black woman stereotype are denoted with the prefix weat_angry or sent-weat_angry. The Caliskan Tests are detailed in Caliskan et al. [5] , and the double bind tests are detailed in May et al. <cite>[21]</cite> .",
  "y": "background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_18",
  "x": "The additional tests are detailed in the sections below. In the full results presented in Tables 7 and 8 , we also report results for neutral tests (C1-2) and tests regarding disability and age (C9-10). In both the race and gender double bind tests, sentence templates that are either bleached of semantic meaning or unbleached are used. Following May et al. <cite>[21]</cite> , examples of such templates (non-exhaustive) are as below. ---------------------------------- **B.3 INTERSECTIONAL** Tests targeting intersectional identities are broken down into the angry black woman (ABW) stereotype test and additional tests (+I1-I5).",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_0",
  "x": "Transformer<cite> [34]</cite> is based solely on the attention mechanism, and dispensing with recurrent and convolutions entirely. At present, this model has received extensive attentions and plays an key role in many neural language models, such as BERT [12] , GPT [27] and Universal Transformer [10] . However, in Transformer based model, a lot of model parameters may cause problems in training and deploying these parameters in a limited resource setting. Thus, the compression of large neural pre-training language model has been an essential problem in NLP research. In literature, there are some compression methods [18, 37, 14] proposed. When the vocabulary is large, the corresponding weight matrices can be enormous. Tensorized embedding (TE) [18] uses the way of tensor-train [25] to compress the embedding layers in Transformer-XL [7] .",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_1",
  "x": "On the other hand, some methods [14, 2] aim to develop a specific structure on its weight matrices and are successful in compressing the pre-trained models. However, the new structure after compressing can not be integrated into the model. In Transformer, the multi-head attention is a key part and it is constructed by a large number of parameters. Specifically, Ashish et.al<cite> [34]</cite> compute the attention function on a set of queries simultaneously, packed together into a matrix Q, while the keys and values are also packed together into matrices K and V , respectively. The attention function then adopts a no-linear function sof tmax over three matrices Q, K and V . There are two challenges to find a high-quality compression method to compress the multi-head attention in Transformer. First, the self-attention function in Transformer is a non-linear function, which makes it difficult to compress.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_2",
  "x": "Still, the model only considers the input layer compression by the idea of low-rank approximation. On the other hand, some methods [14, 2] aim to develop a specific structure on its weight matrices and are successful in compressing the pre-trained models. However, the new structure after compressing can not be integrated into the model. In Transformer, the multi-head attention is a key part and it is constructed by a large number of parameters. Specifically, Ashish et.al<cite> [34]</cite> compute the attention function on a set of queries simultaneously, packed together into a matrix Q, while the keys and values are also packed together into matrices K and V , respectively. The attention function then adopts a no-linear function sof tmax over three matrices Q, K and V . There are two challenges to find a high-quality compression method to compress the multi-head attention in Transformer.",
  "y": "motivation"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_3",
  "x": "The attention function then adopts a no-linear function sof tmax over three matrices Q, K and V . There are two challenges to find a high-quality compression method to compress the multi-head attention in Transformer. First, the self-attention function in Transformer is a non-linear function, which makes it difficult to compress. In order to address this challenge, we first prove that the output of the attention function of the self-attention model<cite> [34]</cite> can be linearly represented by a group of orthonormal base vectors. Q, K and V can be considered as factor matrices. Then, by initializing a low rank core tensor, we use Tucker-decomposition [32, 20] to reconstruct a new attention representation. In order to construct the multi-head mechanism and compress the model, we use the method of Block-Term Tensor Decomposition (BTD), which is a combination of CP decomposition [3] and Tucker decomposition [32] .",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_4",
  "x": "In order to address this challenge, we first prove that the output of the attention function of the self-attention model<cite> [34]</cite> can be linearly represented by a group of orthonormal base vectors. Q, K and V can be considered as factor matrices. Then, by initializing a low rank core tensor, we use Tucker-decomposition [32, 20] to reconstruct a new attention representation. In order to construct the multi-head mechanism and compress the model, we use the method of Block-Term Tensor Decomposition (BTD), which is a combination of CP decomposition [3] and Tucker decomposition [32] . The difference is that three factor matrices Q, K and V are shared in constructing each 3-order block tensor. This process can lead to reduce many parameters. The second challenge is that the attention model after compressing can not be directly integrated into the encoder and decoder framework of Transformer <cite>[34,</cite> 7] .",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_5",
  "x": "The second challenge is that the attention model after compressing can not be directly integrated into the encoder and decoder framework of Transformer <cite>[34,</cite> 7] . In order to address this challenge, there are three steps as follows. First, the average of each block tensor can be computed; Second, some matrices can be given by tensor split. Third, the concatenation of these matrices can serve as the input to the next layer network in Transformer. After that, it can be integrated into the encoder and decoder framework of Transformer <cite>[34,</cite> 7] and trained end-to-end. Moreover, we also prove that the 3-order tensor can reconstruct the scaled dot-product attention in Transformer by a sum on a particular dimension. Our method combines two ideas which are the low-rank approximation and parameters sharing at the same time.",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_6",
  "x": "The analysis of Multi-linear attention relies on these concepts and results from the field of tensor decomositon and multi-head attention. We cover below in Section 2.1 basic background on Block-Term tensor decomposition [9] . Then, we describe in Section 2.2 multi-head attention<cite> [34]</cite> . ---------------------------------- **TENSOR AND BLOCK-TERM TENSOR DECOMPOSITION** Tensor We use the Euler script letter A to denote a tensor which can be thought of as a multi-array. Thereby a vector and a matrix is a 1-order tensor and 2-order tensor, respectively.",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_7",
  "x": "where P is the CP rank, and d is the Core-order. In our work, we consider a tensor is 3-order tensor. Figure 1 demonstrates the example of how a 3-order tensor A can be decomposed into P block terms. ---------------------------------- **MULTI-HEAD ATTENTION** In Transformer, the attention function is named as \"Scaled Dot-Product Attention\". In practice, Transformer<cite> [34]</cite> processes query, keys and values as matrices Q, K, and V respectively.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_8",
  "x": "**MULTI-HEAD ATTENTION** In Transformer, the attention function is named as \"Scaled Dot-Product Attention\". In practice, Transformer<cite> [34]</cite> processes query, keys and values as matrices Q, K, and V respectively. The attention function can be written as follows: where d is the number of columns of Q and K. In these work <cite>[34,</cite> 12, 7] , they all use the multi-head attention, as introduced in<cite> [34]</cite> , where matrices W Q i and In this work<cite> [34]</cite> , multiple groups of parameters (W Q i , W K i and W V i ) are used, which results in a large number of redundant parameters.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_9",
  "x": "The Multi-linear attention can be incorporated into Transformer architecture. A diagram which is about the incorporating of Multi-linear attention in partial Transformer structure is given in Supplementary Materials E.1. ---------------------------------- **ANALYSIS OF COMPRESSION AND COMPLEXITY** Compression Our focus is on the compression of the multi-head mechanism in the multi-head attention of Transformer. Previous work<cite> [34]</cite> gets the multi-head attention by multiple groups of linear mappings. We use three linear ma for matrices Q, K and V , respectively.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_10",
  "x": "A diagram which is about the incorporating of Multi-linear attention in partial Transformer structure is given in Supplementary Materials E.1. ---------------------------------- **ANALYSIS OF COMPRESSION AND COMPLEXITY** Compression Our focus is on the compression of the multi-head mechanism in the multi-head attention of Transformer. Previous work<cite> [34]</cite> gets the multi-head attention by multiple groups of linear mappings. We use three linear ma for matrices Q, K and V , respectively. For the output of three mappings, we choose to share them which are considered as three factor matrices in reconstructing the Multi-linear attention.",
  "y": "similarities"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_11",
  "x": "h is the number of heads in<cite> [34]</cite> , and d is the dimension of factor matrices. The compression ratios can be computed by (3 \u00d7 h \u00d7 d)/(3 \u00d7 d + h). In practice, h is normally set to 8, d is set to 512. In this case, the compression raio can achive 8. In other words, we can reduce almost 8 times parameters in the attention layer. The details of the computing of compression ratios can be found in Supplementary Materials D. The Transformer also contains other network layers, such as Position-wise feed forward network and embedding layers et al. Therefore, for the compression ratios in whole Transformer, we can compare it by the analysis of experimental results for model parameters. Complexity Eq. 5 reduces the time complexity in the attention layer.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_12",
  "x": "However, we can reorder the computations to reduce the model complexity O(R 2 d), where R is the rank of the tensor which can be set in our experiments. In our experiments, R is set as the number between 10 and 18 which is smaller than N . The minimum number of sequential operations in Multi-linear attention for different layers is lower than that of the self-attention in Transformer<cite> [34]</cite> . ---------------------------------- **RELATED WORK** The field of language modeling has witnessed many significant advances. Different from the architectures of convolutional neural network (CNNs) and recurrent neural networks (RNNs) language modeling, the Transformer<cite> [34]</cite> and its variants [7, 12, 10] achieve excellent results in language modeling processing.",
  "y": "differences"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_13",
  "x": "Different from the architectures of convolutional neural network (CNNs) and recurrent neural networks (RNNs) language modeling, the Transformer<cite> [34]</cite> and its variants [7, 12, 10] achieve excellent results in language modeling processing. Transformer networks have a potential of learning long-term dependency, but are limited by a fixed-length context in the setting of language modeling. Vaswani et al.<cite> [34]</cite> uses a segment-level recurrence mechanism and a novel positional encoding scheme to resolve this question. BERT [12] is a kind of bidirectional encoder representations from transformers. It is designed to pre-train deep bidirectional representation and obtains new SoTA on some NLP tasks. Although these methods have achieved great results, a large number of parameters make it difficult for the model to be trained in limited resources. Transformer fail to generalize in many simple tasks, e.g. copying string and logical inference [10] .",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_14",
  "x": "The field of language modeling has witnessed many significant advances. Different from the architectures of convolutional neural network (CNNs) and recurrent neural networks (RNNs) language modeling, the Transformer<cite> [34]</cite> and its variants [7, 12, 10] achieve excellent results in language modeling processing. Transformer networks have a potential of learning long-term dependency, but are limited by a fixed-length context in the setting of language modeling. Vaswani et al.<cite> [34]</cite> uses a segment-level recurrence mechanism and a novel positional encoding scheme to resolve this question. BERT [12] is a kind of bidirectional encoder representations from transformers. It is designed to pre-train deep bidirectional representation and obtains new SoTA on some NLP tasks. Although these methods have achieved great results, a large number of parameters make it difficult for the model to be trained in limited resources.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_15",
  "x": "**NEURAL MACHINE TRANSLATION** The goal is to map an input sequence s = (x 1 , x 2 , . . . , x n ) representing a phrase in one language, to an output sequence y = (y 1 , y 2 , . . . , y m ) representing the same phrase in a different language. In this task, we have trained the Transformer model<cite> [34]</cite> on WMT 2016 English-German dataset [30] . Sentences were tokenized using the SentencePiece 3 . For our experiments, we have replaced each of the attention layers with Multi-linear attention. For evaluation we used beam search with a beam size of 5 and length penalty \u03b1=0.6. In this section, we only compared the results with Transformer<cite> [34]</cite> .",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_16",
  "x": "**NEURAL MACHINE TRANSLATION** The goal is to map an input sequence s = (x 1 , x 2 , . . . , x n ) representing a phrase in one language, to an output sequence y = (y 1 , y 2 , . . . , y m ) representing the same phrase in a different language. In this task, we have trained the Transformer model<cite> [34]</cite> on WMT 2016 English-German dataset [30] . Sentences were tokenized using the SentencePiece 3 . For our experiments, we have replaced each of the attention layers with Multi-linear attention. For evaluation we used beam search with a beam size of 5 and length penalty \u03b1=0.6. In this section, we only compared the results with Transformer<cite> [34]</cite> .",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_17",
  "x": "In Table 3 , we select two baseline models. The Base-line [30] is first model in WMT 2016 English-German dataset. For the other baseline, we use the basic Transformer architecture<cite> [34]</cite> . The BLEU score is 34.5 for the basic architecture. We carry out two tensorized Transformer structures, namely core-1 and core-2 respectively. When tensorized Transformer core-1 and core-2 are used, the BLEU scores are 34.10 and 34.91, which achieves better performance over Transformer. As for the reported model parameter size, our model uses less parameters.",
  "y": "uses"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_0",
  "x": "While there has been significant work on building end-to-end response generation systems (Vinyals and Le, 2015; Shang et al., 2015; , it has recently been shown that many of the automatic evaluation metrics used for such systems correlate poorly or not at all with human judgement of the generated responses (Liu et al., 2016) . Retrieval-based systems are of interest because they admit a natural evaluation metric, namely the recall and precision measures. First introduced for evaluating user simulations by Schatzmann et al. (2005) , such a framework has gained recent prominence for the evaluation of end-to-end dialogue systems<cite> (Lowe et al., 2015a</cite>; Kadlec et al., 2015; Dodge et al., 2015) . These models are trained on the task of selecting the correct response from a candidate list, which we call Next-Utterance-Classification (NUC, detailed in Section 2), and are evaluated using the metric of recall. NUC is useful for several reasons: 1) the performance (i.e. loss or error) is easy to com-pute automatically, 2) it is simple to adjust the difficulty of the task, 3) the task is interpretable and amenable to comparison with human performance, 4) it is an easier task compared to generative dialogue modeling, which is difficult for endto-end systems , and 5) models trained with NUC can be converted to dialogue systems by retrieving from the full corpus (Liu et al., 2016) . In this case, NUC additionally allows for making hard constraints on the allowable outputs of the system (to prevent offensive responses), and guarantees that the responses are fluent (because they were generated by humans). Thus, NUC can be thought of both as an intermediate task that can be used to evaluate the ability of systems to understand natural language conversations, similar to the bAbI tasks for language understanding , and as a useful framework for building chatbots.",
  "y": "background"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_1",
  "x": "NUC is useful for several reasons: 1) the performance (i.e. loss or error) is easy to com-pute automatically, 2) it is simple to adjust the difficulty of the task, 3) the task is interpretable and amenable to comparison with human performance, 4) it is an easier task compared to generative dialogue modeling, which is difficult for endto-end systems , and 5) models trained with NUC can be converted to dialogue systems by retrieving from the full corpus (Liu et al., 2016) . In this case, NUC additionally allows for making hard constraints on the allowable outputs of the system (to prevent offensive responses), and guarantees that the responses are fluent (because they were generated by humans). Thus, NUC can be thought of both as an intermediate task that can be used to evaluate the ability of systems to understand natural language conversations, similar to the bAbI tasks for language understanding , and as a useful framework for building chatbots. With the huge size of current dialogue datasets that contain millions of utterances<cite> (Lowe et al., 2015a</cite>; Banchs, 2012; Ritter et al., 2010) and the increasing amount of natural language data, it is conceivable that retrieval-based systems will be able to have engaging conversations with humans. However, despite the current work with NUC, there has been no verification of whether machine and human performance differ on this task. This cannot be assumed; it is possible that no significant gap exists between the two, as is the case with many current automatic response generation metrics (Liu et al., 2016) . Further, it is important to benchmark human performance on new tasks such as NUC to determine when research has outgrown their use.",
  "y": "background"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_2",
  "x": "However, despite the current work with NUC, there has been no verification of whether machine and human performance differ on this task. This cannot be assumed; it is possible that no significant gap exists between the two, as is the case with many current automatic response generation metrics (Liu et al., 2016) . Further, it is important to benchmark human performance on new tasks such as NUC to determine when research has outgrown their use. In this paper, we consider to what extent NUC is achievable by humans, whether human performance varies according to expertise, and whether there is room for machine performance to improve (or has reached human performance already) and we should move to more complex conversational tasks. We performed a user study on three different datasets: the SubTle Corpus of movie dialogues (Banchs, 2012) , the Twitter Corpus (Ritter et al., 2010) , and the Ubuntu Dialogue Corpus<cite> (Lowe et al., 2015a)</cite> . Since conversations in the Ubuntu Dialogue Corpus are highly technical, we recruit 'expert' humans who are adept with the Ubuntu terminology, whom we compare with a state-of-the-art machine learning agent on all datasets. We find that there is indeed a significant separation between machine and expert hu- man performance, suggesting that NUC is a useful intermediate task for measuring progress.",
  "y": "uses"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_3",
  "x": "Next-Utterance-Classification (NUC, see Figure 1 ) is a task, which is straightforward to evaluate, designed for training and validation of dialogue systems. They are evaluated using the metric of Recall@k, which we define in this section. In NUC, a model or user, when presented with the context of a conversation and a (usually small) pre-defined list of responses, must select the most appropriate response from this list. This list includes the actual next response of the conversation, which is the desired prediction of the model. The other entries, which act as false positives, are sampled from elsewhere in the corpus. Note that no assumptions are made regarding the number of utterances in the context: these can be fixed or sampled from arbitrary distributions. Performance on this task is easy to assess by measuring the success rate of picking the correct next response; more specifically, we measure Recall@k (R@k), which is the percentage of correct responses (i.e. the actual response of the conversation) that are found in the top k responses with the highest rankings according to the model. This task has gained some popularity recently for evaluating dialogue systems<cite> (Lowe et al., 2015a</cite>; Kadlec et al., 2015) .",
  "y": "background"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_4",
  "x": "Each participant was asked to answer either 30 or 40 questions (mean=31.9). To ensure a sufficient diversity of questions from each dataset, four versions of the survey with different questions were given to participants. For AMT respondents, the questions were approximately evenly distributed across the three datasets, while for the lab experts, half of the questions were related to Ubuntu and the remainder evenly split across Twitter and movies. Each question had 1 correct response, and 4 false responses drawn uniformly at random from elsewhere in the (same) corpus. Participants had a time limit of 40 minutes. Conversations were extracted to form NUC conversation-response pairs as described in Sec. 2. The number of utterances in the context were sampled according to the procedure in<cite> (Lowe et al., 2015a)</cite> , with a maximum context length of 6 turns -this was done for both the human trials and ANN model.",
  "y": "uses"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_5",
  "x": "As we can see from Table 1 , the AMT participants are mostly young adults, fluent in English with some undergraduate education. The split across genders is approximately equal, and the majority of respondents had never used Ubuntu before. Table 2 shows the NUC results on each corpus. The human results are separated into AMT nonexperts, consisting of paid respondents who have 'Beginner' or no knowledge of Ubuntu terminology; AMT experts, who claimed to have 'Intermediate' or 'Advanced' knowledge of Ubuntu; and Lab experts. We also presents results on the same task for a state-of-the-art artificial neural network (ANN) dialogue model (see<cite> (Lowe et al., 2015a)</cite> for implementation details). We first observe that subjects perform above chance level (20% for R@1) on all domains, thus the task is doable for humans. Second we observe difference in performances between the three domains.",
  "y": "uses"
 },
 {
  "id": "4498072885df2a126e2db553cf3aca_0",
  "x": "Intuitively, the DIH states that we should be able to replace any occurrence of \"cat\" with \"animal\" and still have a valid utterance. An important insight from work on distributional methods is that the definition of context is often critical to the success of a system<cite> (Shwartz et al., 2017)</cite> . Some distributional representations, like positional or dependency-based contexts, may even capture crude Hearst pattern-like features (Levy et al., 2015; Roller and Erk, 2016) . While both approaches for hypernym detection rely on co-occurrences within certain contexts, they differ in their context selection strategy: pattern-based methods use predefined manuallycurated patterns to generate high-precision extractions while DIH methods rely on unconstrained word co-occurrences in large corpora. Here, we revisit the idea of using pattern-based methods for hypernym detection. We evaluate several pattern-based models on modern, large corpora and compare them to methods based on the DIH. We find that simple pattern-based methods consistently outperform specialized DIH methods on several difficult hypernymy tasks, including detection, direction prediction, and graded entailment ranking.",
  "y": "background"
 },
 {
  "id": "4498072885df2a126e2db553cf3aca_1",
  "x": "We also consider factorizing a matrix that is constructed from occurrence probabilities as in Equation (1), denoted by sp(x, y). This approach is then closely related to the method of Cederberg and Widdows (2003) , which has been proposed to improve precision and recall for hypernymy detection from Hearst patterns. ---------------------------------- **DISTRIBUTIONAL HYPERNYM DETECTION** Most unsupervised distributional approaches for hypernymy detection are based on variants of the Distributional Inclusion Hypothesis (Weeds et al., 2004; Kotlerman et al., 2010; Santus et al., 2014; Lenci and Benotto, 2012;<cite> Shwartz et al., 2017)</cite> . Here, we compare to two methods with strong empirical results. As with most DIH measures, they are only defined for large, sparse, positively-valued distributional spaces.",
  "y": "background"
 },
 {
  "id": "4498072885df2a126e2db553cf3aca_2",
  "x": "To measure both the inclusion of x in y and the non-inclusion of y in x, invCL is then defined as Although most unsupervised distributional approaches are based on the DIH, we also consider the distributional SLQS model based on on an alternative informativeness hypothesis (Santus et al., 2014;<cite> Shwartz et al., 2017)</cite> . Intuitively, the SLQS model presupposes that general words appear mostly in uninformative contexts, as measured by entropy. Specifically, SLQS depends on the median entropy of a term's top N contexts, defined as , where H(c i ) is the Shannon entropy of context c i across all terms, and N is chosen in hyperparameter selection. Finally, SLQS is defined using the ratio between the two terms: Since the SLQS model only compares the relative generality of two terms, but does not make judgment about the terms' relatedness, we report SLQS-cos, which multiplies the SLQS measure by cosine similarity of x and y (Santus et al., 2014) .",
  "y": "uses"
 },
 {
  "id": "4498072885df2a126e2db553cf3aca_3",
  "x": "We limit ourselves to a 52,578 pair subset excluding multiword expressions. Finally, we evaluate on WBLESS (Weeds et al., 2014) , a 1,668 pair subset of BLESS, with negative pairs being selected from co-hyponymy, random, and hyponymy relations. Previous work has used different metrics for evaluating on BLESS (Lenci and Benotto, 2012; Levy et al., 2015; Roller and Erk, 2016) . We chose to evaluate the global ranking using Average Precision. This allowed us to use the same metric on all detection benchmarks, and is consistent with evaluations in <cite>Shwartz et al. (2017)</cite> . Direction: In direction prediction, the task is to identify which term is broader in a given pair of words. For this task, we evaluate all models on three datasets described by Kiela et al. (2015) : On BLESS, the task is to predict the direction for all 1337 positive pairs in the dataset.",
  "y": "uses"
 },
 {
  "id": "4498072885df2a126e2db553cf3aca_4",
  "x": "For SVD-based models, we select the rank from r \u2208 {5, 10, 15, 20, 25, 50, 100, 150, 200, 250 , 300, 500, 1000} on the validation set. The other pattern-based models do not have any hyperparameters. Distributional models: For the distributional baselines, we employ the large, sparse distributional space of <cite>Shwartz et al. (2017)</cite> , which is computed from UkWaC and Wikipedia, and is known to have strong performance on several of the detection tasks. The corpus was POS tagged and dependency parsed. Distributional contexts were constructed from adjacent words in dependency parses (Pad\u00f3 and Lapata, 2007; Levy and Goldberg, 2014) . Targets and contexts which appeared fewer than 100 times in the corpus were filtered, and the resulting co-occurrence matrix was PPMI transformed. 1 The resulting space contains representations for 218K words over 732K context dimensions.",
  "y": "uses"
 },
 {
  "id": "45723171ec398550e687c57d42e7cc_0",
  "x": "The increasing use of social media platforms world wide offers an interesting application of natural language processing tools for monitoring public health and health-related events on the social media. The social media mining for health applications (SMM4H) shared task<cite> (Weissenbacher et al., 2018)</cite> hosts four tasks aiming to identify mentions of different aspects medication use on Twitter. Briefly, the tasks and their descriptions are: Task 1: Automatic detection of posts mentioning drug names. Task 2: Automatic classification of posts describing medication intake. Task 3: Automatic classification of adverse drug reaction mentioning posts. Task 4: Automatic detection of posts mentioning vaccination behavior. All tasks, except Task 2 are binary classification tasks.",
  "y": "background"
 },
 {
  "id": "457f9916ed4d7eafacea57e208c760_0",
  "x": "The server interface is command-driven. A client may connect to the server and open up a dialogue (see Figure 1 in <cite>(Busemann et al., 1997)</cite> ). During the dialogue, the client may request texts to be analyzed or semantic descriptions to be verbalized. When given a text, the server returns the semantic representation, and vice versa. The client ensures that the server has available to it linguistically relevant information about the interlocutors, such as names, sexes etc. The user agents may access the dialogue server via Internet. They use the server as their NL front end to human participants.",
  "y": "uses"
 },
 {
  "id": "457f9916ed4d7eafacea57e208c760_1",
  "x": "The agent system used is a further development of the PASHA system (Schmeier and Schupeta, 1996) . NL analysis in the server is based on a shallow parsing strategy implemented in the SMES system (Neumann et al., 1997) . The use of SMES in COSMA, semantic analysis and inference, the dialogue model mapping between human and machine dialogue structures, utterance generation, the architectural framework of the server, and the PASHA agent system are described in <cite>(Busemann et al., 1997)</cite> . Both papers can be found in the ANLP '97 conference proceedings. We demonstrate extended versions of the systems described in <cite>(Busemann et al., 1997)</cite> . In particular, the systems to be demonstrated can process counterproposals, which form an important part of efficient and cooperative scheduling dialogues. ----------------------------------",
  "y": "background"
 },
 {
  "id": "457f9916ed4d7eafacea57e208c760_2",
  "x": "The agent system used is a further development of the PASHA system (Schmeier and Schupeta, 1996) . NL analysis in the server is based on a shallow parsing strategy implemented in the SMES system (Neumann et al., 1997) . The use of SMES in COSMA, semantic analysis and inference, the dialogue model mapping between human and machine dialogue structures, utterance generation, the architectural framework of the server, and the PASHA agent system are described in <cite>(Busemann et al., 1997)</cite> . Both papers can be found in the ANLP '97 conference proceedings. We demonstrate extended versions of the systems described in <cite>(Busemann et al., 1997)</cite> . In particular, the systems to be demonstrated can process counterproposals, which form an important part of efficient and cooperative scheduling dialogues. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_0",
  "x": "For example, in recurrent-neural-network-based models, pooling is often used to aggregate hidden states at different time steps (i.e., words in a sentence) to obtain sentence embedding. Convolutional neural networks (CNN) also often uses max or mean pooling to obtain a fixed-size sentence embedding. In this paper we explore generalized pooling methods to enhance sentence embedding. Specifically, by extending scalar self-attention models such as those proposed in<cite> Lin et al. (2017)</cite> , we propose vectorbased multi-head attention, which includes the widely used max pooling, mean pooling, and scalar selfattention itself as special cases. On one hand, the proposed method allows for extracting different aspects of the sentence into multiple vector representations through the multi-head mechanism. On the other, it allows the models to focus on one of many possible interpretations of the words encoded in the context vector through the vector-based attention mechanism. In the proposed model we design penalization terms to reduce redundancy in multi-head attention.",
  "y": "motivation similarities background"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_1",
  "x": [
   "They calculate scalar attention between the LSTM states and the mean pooling using multi-layer perceptron (MLP) to obtain the vector representation for a sentence. Lin et al. (2017) proposed a scalar structure/multi-head self-attention method for sentence embedding. The multi-head self-attention is calculated by a MLP with only LSTM states as input. There are two main differences from our proposed method; i.e., (1) they used scalar attention instead of vectorial attention, (2) we propose different penalization terms which is suitable for vector-based multi-head self-attention, while their penalization term on attention matrix is only designed for scalar multi-head self-attention. Choi et al. (2018) proposed a fine-grained attention mechanism for neural machine translation, which also extend scalar attention to vectorial attention. Shen et al. (2017) proposes multi-dimensional/vectorial self-attention pooling on the top of self-attention network instead of BiLSTM. However, both of them didn't consider multi-head self-attention."
  ],
  "y": "differences background"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_2",
  "x": "**PENALIZATION TERM ON ATTENTION MATRICES** The second penalization term is added on attention matrices. Instead of using AA T \u2212 I 2 F to encourage the diversity for scalar attention matrix as in<cite> Lin et al. (2017)</cite> , we propose the following formula to encourage the diversity for vectorial attention matrices. The penalization term on attention matrices is where \u03bb and \u00b5 are hyper-parameters which need to be tuned based on a development set. Intuitively, we try to encourage the diversity of any two different A i under the threshold \u03bb. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_3",
  "x": "We use the same data split as in Bowman et al. (2015) , i.e., 549.367 samples for training, 9,842 samples for development and 9,824 samples for testing. MultiNLI MultiNLI (Williams et al., 2017 ) is another natural language inference dataset. The data are collected from a broader range of genres such as fiction, letters, telephone speech, and 9/11 reports. Half of these 10 genres are used in training while the rest are not, resulting in-domain and cross-domain development and test sets used to test NLI systems. We use the same data split as in Williams et al. (2017) , i.e., 392,702 samples for training, 9,815/9,832 samples for in-domain/cross-domain development, and 9,796/9,847 samples for in-domain/cross-domain testing. Note that, we do not use SNLI as an additional training/development set in our experiments. Age Dataset To compare our models with that of<cite> Lin et al. (2017)</cite> , we use the same Age dataset in our experiment here, which is an Author Profiling dataset.",
  "y": "uses"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_4",
  "x": "The task is to predict the age range of authors of input tweets. The age range are split into 5 classes: 18-24, 25-34, 35-49, 50-64, 65+. We use the same data split as in<cite> Lin et al. (2017)</cite> , i.e., 68,485 samples for training, 4,000 for development, and 4,000 for testing. ---------------------------------- **YELP DATASET** The Yelp dataset 2 is a sentiment analysis task, which takes reviews as input and predicts the level of sentiment in terms of the number of stars, from 1 to 5 stars, where 5-star means the most positive. We use the same data split as in<cite> Lin et al. (2017)</cite> , i.e., 500,000 samples for training, 2,000 for development, and 2,000 for testing.",
  "y": "uses"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_5",
  "x": "The age range are split into 5 classes: 18-24, 25-34, 35-49, 50-64, 65+. We use the same data split as in<cite> Lin et al. (2017)</cite> , i.e., 68,485 samples for training, 4,000 for development, and 4,000 for testing. ---------------------------------- **YELP DATASET** The Yelp dataset 2 is a sentiment analysis task, which takes reviews as input and predicts the level of sentiment in terms of the number of stars, from 1 to 5 stars, where 5-star means the most positive. We use the same data split as in<cite> Lin et al. (2017)</cite> , i.e., 500,000 samples for training, 2,000 for development, and 2,000 for testing. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_7",
  "x": "Table 3 shows the results of different models for the Yelp and the Age dataset. The BiLSTM with self-attention proposed by<cite> Lin et al. (2017)</cite> achieves better result than CNN and BiLSTM with max pooling. One of our baseline models using max pooling on BiLSTM achieves accuracies of 65.00% and 82.30% on the Yelp and the Age dataset respectively, which is already better than the self-attention model proposed by<cite> Lin et al. (2017)</cite> . We also show that the results of baseline with mean pooling and last pooling, in which mean pooling achieves the best result on the Yelp dataset among three baseline models and max pooling achieves the best on the Age dataset among three baselines. Our proposed generalized pooling method obtains further improvement on these already strong baselines, achieving 66.55% on the Yelp dataset and 82.63% on the Age dataset (statistically significant p < 0.00001 against best baselines), ---------------------------------- **MODEL**",
  "y": "background"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_8",
  "x": "The first group is the results of previous sentence-encoding-based models. The proposed model with generalized pooling achieves an accuracy of 73.8% on the in-domain test set and 74.0% on the cross-domain test set; both improve over the baselines using max pooling, mean pooling and last pooling. In addition, the results on cross-domain test set yield a new state of the art at an accuracy of 74.0%, which is better than 73.6% of shortcut-stacked BiLSTM (Nie and Bansal, 2017) . Table 3 shows the results of different models for the Yelp and the Age dataset. The BiLSTM with self-attention proposed by<cite> Lin et al. (2017)</cite> achieves better result than CNN and BiLSTM with max pooling. One of our baseline models using max pooling on BiLSTM achieves accuracies of 65.00% and 82.30% on the Yelp and the Age dataset respectively, which is already better than the self-attention model proposed by<cite> Lin et al. (2017)</cite> . We also show that the results of baseline with mean pooling and last pooling, in which mean pooling achieves the best result on the Yelp dataset among three baseline models and max pooling achieves the best on the Age dataset among three baselines.",
  "y": "differences"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_10",
  "x": "**DETAILED ANALYSIS** Effect of Multiple Vectors/Scalars To compare the difference between vector-based attention and scalar attention, we draw the learning curves of different models using different heads on the SNLI development dataset without penalization terms as in Figure 1 . The green lines indicate scalar selfattention pooling added on top of the BiLSTMs, same as in<cite> Lin et al. (2017)</cite> , and the blue lines indicate vector-based attention used in our generalized pooling methods. It is obvious that the vector-based attention achieves improvement over scalar attention. Different line styles are used to indicate selfattention using different numbers of multi-head, ranging from 1, 3, 5, 7 to 9. For vector-based attention, the 9-head model achieves the best accuracy of 86.8% on the development set. For scalar attention, the 7-head model achieves the best accuracy of 86.4% on the development set.",
  "y": "background differences"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_11",
  "x": "**CONCLUSIONS** In this paper, we propose a generalized pooling method for sentence embedding through vector-based multi-head attention, which includes the widely used max pooling, mean pooling, and scalar selfattention as its special cases. Specifically the proposed model aims to use vectors to enrich the expressiveness of attention mechanism and leverage proper penalty terms to reduce redundancy in multi-head attention. We evaluate the proposed approach on three different tasks: natural language inference, author profiling, and sentiment classification. The experiments show that the proposed model achieves significant improvement over strong sentence-encoding-based methods, resulting in state-of-the-art performances on four datasets. The proposed approach can be easily implemented for more problems than we discuss in this paper. Our future work includes exploring more effective MLP to use the structures of multi-head vectors, inspired by the idea from<cite> Lin et al. (2017)</cite> .",
  "y": "future_work background"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_0",
  "x": "Adding pictures as input can provide information for guiding story construction by offering visual illustrations of the storyline. In the related task of image captioning, most methods try to generate descriptions only for individual images or for short videos depicting a single activity. Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (Rohrbach et al., 2016; Pan et al., 2016; Lu and Grauman, 2013; <cite>Huang et al., 2016)</cite> . The type of data we consider in this paper provides input illustrations for story generation in the form of photo albums, sampled over a few minutes to a few days of time. For this type of data, generating textual descriptions involves telling a temporally consistent story about the depicted visual information, where stories must be coherent and take into account the temporal context of the images. Applications of this include constructing visual and textual summaries of albums, or even enabling search through personal photo collections to find photos of life events. Previous visual storytelling works can be classified into two types, vision-based and languagebased, where image or language stories are constructed respectively.",
  "y": "background"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_1",
  "x": "Among the vision-based approaches, unsupervised learning is commonly applied: e.g., (Sigurdsson et al., 2016) learns the latent temporal dynamics given a large amount of albums, and (Kim and Xing, 2014) formulate the photo selection as a sparse time-varying directed graph. However, these visual summaries tend to be difficult to evaluate and selected photos may not agree with human selections. For languagebased approaches, a sequence of natural language sentences are generated to describe a set of photos. To drive this work (Park and collected a dataset mined from Blog Posts. However, this kind of data often contains contextual information or loosely related language. A more direct dataset was recently released<cite> (Huang et al., 2016)</cite> , where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk. In this paper, we make use of the Visual Storytelling Dataset<cite> (Huang et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_2",
  "x": "In this paper, we make use of the Visual Storytelling Dataset<cite> (Huang et al., 2016)</cite> . While the authors provide a seq2seq baseline, they only deal with the task of generating stories given 5-representative (summary) photos hand-selected by people from an album. Instead, we focus on the more challenging and realistic problem of end-toend generation of stories from entire albums. This requires us to either generate a story from all of the album's photos or to learn selection mechanisms to identify representative photos and then generate stories from those summary photos. We evaluate each type of approach. Ultimately, we propose a model of hierarchically-attentive recurrent neural nets, consisting of three RNN stages. The first RNN encodes the whole album context and each photo's content, the second RNN provides weights for photo selection, and the third RNN takes the weighted representation and decodes to the resulting sentences.",
  "y": "uses"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_3",
  "x": "For captioning video or movie content (Venugopalan et al., 2015; Pan et al., 2016) , sequence-to-sequence models are widely applied, where the first sequence encodes video frames and the second sequence decodes the description. Attention techniques (Xu et al., 2015; Yu et al., 2016; Yao et al., 2015) are commonly incorporated for both tasks to localize salient temporal or spatial information. Video Summarization: Similar to documentation summarization (Rush et al., 2015; Cheng and Lapata, 2016; Woodsend and Lapata, 2010) which extracts key sentences and words, video summarization selects key frames or shots. While some approaches use unsupervised learning (Lu and Grauman, 2013; Khosla et al., 2013) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (Gygli et al., 2015; Zhang et al., 2016b,a; Gong et al., 2014) . Recently, to better exploit semantics, (Choi et al., 2017) proposed textually customized summaries. Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set. Previous works include storyline graph modeling Xing, 2014), unsupervised mining (Sigurdsson et al., 2016) , blog-photo alignment , and language retelling<cite> (Huang et al., 2016</cite>; Park and Kim, 2015) .",
  "y": "background"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_4",
  "x": "While (Park and collects data by mining Blog Posts,<cite> (Huang et al., 2016)</cite> collects stories using Mechanical Turk, providing more directly relevant stories. ---------------------------------- **MODEL** Our model (Fig. 1) is composed of three modules: Album Encoder, Photo Selector, and Story Generator, jointly learned during training. ---------------------------------- **ALBUM ENCODER** Given an album A = {a 1 , a 2 , ..., a n }, composed of a set of photos, we use a bi-directional RNN to encode the local album context for each photo.",
  "y": "background"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_5",
  "x": "Our model (Fig. 1) is composed of three modules: Album Encoder, Photo Selector, and Story Generator, jointly learned during training. ---------------------------------- **ALBUM ENCODER** Given an album A = {a 1 , a 2 , ..., a n }, composed of a set of photos, we use a bi-directional RNN to encode the local album context for each photo. We first extract the 2048-dimensional visual representation f i \u2208 R k for each photo using ResNet101 , then a bi-directional RNN is applied to encode the full album. Following<cite> (Huang et al., 2016)</cite> , we choose a Gated Recurrent Unit (GRU) as the RNN unit to encode the photo sequence. The sequence output at each time step encodes the local album context for each photo (from both directions).",
  "y": "uses"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_6",
  "x": "We then apply a max-margin ranking loss to encourage correctly-ordered stories: L rank (S, S ) = max(0, m\u2212log p(S )+log p(S)). The final loss is then a combination of the generation and ranking losses: ---------------------------------- **EXPERIMENTS** We use the Visual Storytelling Dataset<cite> (Huang et al., 2016)</cite> , consisting of 10,000 albums with 200,000 photos. Each album contains 10-50 photos taken within a 48-hour span with two annotations: 1) 2 album summarizations, each with 5 selected representative photos, and 2) 5 stories describing the selected photos. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_0",
  "x": "Among the 30 participating teams, our system ranked the 4th (with 69.3% F-score). Post competition, we were able to score slightly higher than the 3rd ranking system (reaching 70.7%). Our system is trained on top of a pre-trained language model (LM), fine-tuned on the data provided by the task organizers. Our best results are acquired by an average of an ensemble of language models. We also offer an analysis of system performance and the impact of training data size on the task. For example, we show that training our best model for only one epoch with < 40% of the data enables better performance than the baseline reported by <cite>Klinger et al. (2018)</cite> for the task. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_1",
  "x": "In these works, data were labeled for the 6 basic emotions of Ekman (Ekman, 1972) . More recent works exploit distant supervision (Mintz et al., 2009 ) to automatically acquire emotion data for training systems. More specifically, a number of works use hashtags like #happy and #sad, especially occurring finally in Twitter data, as a proxy of emotion (Wang et al., 2012; Mohammad and Kiritchenko, 2015; Volkova and Bachrach, 2016) . Abdul-Mageed and Ungar (2017) report state-of-the-art results using a large dataset acquired with hashtags. Other works exploit emojis to capture emotion carrying data (Felbo et al., 2017) . Alhuzali et al. (2018) introduce a third effective approach that leverages firstperson seed phrases like \"I'm happy that\" to collect emotion data. <cite>Klinger et al. (2018)</cite> propose yet a fourth method for collecting emotion data that depends on the existence of the expression \"emotion-word + one of the following words (when, that or because)\" in a tweet, regardless of the position of the emotion word.",
  "y": "background"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_2",
  "x": "The full details of the dataset can be found in <cite>Klinger et al. (2018)</cite> . We now describe our methods in the nesxt section. ---------------------------------- **METHODS** ---------------------------------- **PRE-PROCESSING** We adopt a simple pre-processing scheme, similar to most of the pre-trained models we employ.",
  "y": "background"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_3",
  "x": "We develop a host of models based on deep neural networks, using some of these models as our baseline models. As an additional baseline, we compare to <cite>Klinger et al. (2018)</cite> who propose a model based on Logistic Regression with a bag of word unigrams (BOW). All our deep learning models are based on variations of recurrent neural networks (RNNs), which have proved useful for several NLP tasks. RNNs are able to capture sequential dependencies especially in time series data, of which language can be seen as an example. One weakness of RNNs, however, lies in gradient either vanishing or exploding during training. Longshort term memory (LSTM) networks were developed to target this problem, and hence we employ these in our work. We also use a bidirectional version (BiLSTM) where the vector of representation is built as a concatenation of two vectors, one that runs from left-to-right and another running from right-to-left.",
  "y": "uses"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_4",
  "x": "We list the network architectures and hyper-parameters for this set of experiments in Table 2 . Table 2 : Hyper-parameters for our submitted system exploiting fine-tuned language models from Howard and Ruder (2018) . Table 3 shows results of all our models in F-score. As the Table shows, all our models achieve sizable gains over the logistic regression model introduced by<cite> (Klinger et al., 2018)</cite> as a baseline for the competition (F-score = 60%). Even though our models trained based on fastText and ELMo each has a single hidden layer, which is not that deep, these at least 1.5% higher than the logistic regression model. We also observe that ELMo embeddings, which are acquired from language models rather than optimized from sequences of tokens, achieves higher performance than FastText embeddings. This is not surprising, and aligns with the results reported by Peters et al. (2018) .",
  "y": "differences"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_5",
  "x": "---------------------------------- **SIZE OF TRAINING DATA** We also investigate the impact of training data size on model accuracy. For this purpose, we train models with different data sizes with the best parameter settings shown in Table 2 6 . Figure 3 shows the impact of different percentages of training examples on model performance. We test model performance for this analysis on our validation data. Interestingly, as Figure 3 shows, the model exceeds the baseline model reported by the task organizers<cite> (Klinger et al., 2018)</cite> when trained on only 10% of the training data.",
  "y": "differences"
 },
 {
  "id": "47e109fd12ddbeebba894cead282d2_0",
  "x": "P4 CON I'm obviously ignorant. Look how many times i've been given the title. \"Gravity is a theory. Why aren't you giving it the same treatment you do to evolution?\" Because it doesn't carry the same weight. ;P bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011) ; (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003) ; and (3) online social and political public forums (Somasundaran and Wiebe, 2009;<cite> Somasundaran and Wiebe, 2010</cite>; Wang and Ros\u00e9, 2010; Biran and Rambow, 2011) . Debates in online public forums (e.g. Fig. 1 ) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are also personal, giving a strong sense of the indi-vidual making the argument, and whether s/he favors emotive or factual modes of expression, e.g. Let me answer.... NO! (P2 in Fig. 3 ). Other common features are sarcasm, e.g. I'm obviously ignorant.",
  "y": "background"
 },
 {
  "id": "47e109fd12ddbeebba894cead282d2_1",
  "x": "These properties may function to engage the audience and persuade them to form a particular opinion, but they make computational analysis of such debates challenging, with the best performance to date averaging 64% over several topics <cite>(Somasundaran and Wiebe, 2010</cite> Second, the affordances of different online debate sites provide differential support for dialogic relations between forum participants. For example, the research of<cite> Somasundaran and Wiebe (2010)</cite> , does not explicitly model dialogue or author relations. However debates in our corpus vary greatly by topic on two dialogic factors: (1) the percent of posts that are rebuttals to prior posts, and (2) the number of posts per author. The first 5 columns of Table 2 shows the variation in these dimensions by topic. In this paper we show that information about dialogic relations between authors (SOURCE factors) improves performance for STANCE classification, when compared to models that only have access to properties of the ARGUMENT. We model SOURCE relations with a graph, and add this information to classifiers operating on the text of a post. Sec. 2 describes the corpus and our approach.",
  "y": "background"
 },
 {
  "id": "47e109fd12ddbeebba894cead282d2_2",
  "x": "Look how many times i've been given the title (P4 in Fig. 1 ), questioning another's evidence or assumptions: Yes there is always room for human error, but is one accident that hasn't happened yet enough cause to get rid of a capital punishment? (P2 in Fig. 3 ), and insults: Or is it because you are ignorant? (P3 in Fig. 1 ). These properties may function to engage the audience and persuade them to form a particular opinion, but they make computational analysis of such debates challenging, with the best performance to date averaging 64% over several topics <cite>(Somasundaran and Wiebe, 2010</cite> Second, the affordances of different online debate sites provide differential support for dialogic relations between forum participants. For example, the research of<cite> Somasundaran and Wiebe (2010)</cite> , does not explicitly model dialogue or author relations. However debates in our corpus vary greatly by topic on two dialogic factors: (1) the percent of posts that are rebuttals to prior posts, and (2) the number of posts per author. The first 5 columns of Table 2 shows the variation in these dimensions by topic. In this paper we show that information about dialogic relations between authors (SOURCE factors) improves performance for STANCE classification, when compared to models that only have access to properties of the ARGUMENT. We model SOURCE relations with a graph, and add this information to classifiers operating on the text of a post.",
  "y": "background"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_0",
  "x": "Automatic essay scoring (AES) is the task of assigning grades to essays written in an educational setting, using a computer-based system with natural language processing capabilities. The aim of designing such systems is to reduce the involvement of human graders as far as possible. AES is a challenging task as it relies on grammar as well as semantics, pragmatics and discourse (Song et al., 2017) . Although traditional AES methods typically rely on handcrafted features (Larkey, 1998; Foltz et al., 1999; Attali and Burstein, 2006; Dikli, 2006; Wang and Brown, 2008; Chen and He, 2013; Somasundaran et al., 2014; Yannakoudakis et al., 2014; <cite>Phandi et al., 2015</cite>) , recent results indicate that state-of-the-art deep learning methods reach better performance (Alikaniotis et al., 2016; Dong and Zhang, 2016; Taghipour and Ng, 2016; Song et al., 2017; Tay et al., 2018) , perhaps because these methods are able to capture subtle and complex information that is relevant to the task (Dong and Zhang, 2016) . In this paper, we propose to combine string kernels (low-level character n-gram features) and word embeddings (high-level semantic features) to obtain state-of-the-art AES results. Since recent methods based on string kernels have demonstrated remarkable performance in various text classification tasks ranging from authorship identification (Popescu and Grozea, 2012) and sentiment analysis (Gim\u00e9nez-P\u00e9rez et al., 2017; to native language identification (Popescu and Ionescu, 2013; Ionescu et al., 2014; Ionescu, 2015; and dialect identification , we believe that string kernels can reach equally good results in AES. To the best of our knowledge, string kernels have never been used for this task.",
  "y": "background"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_1",
  "x": "We evaluate our approach on the Automated Student Assessment Prize data set, in both in-domain and cross-domain settings. The empirical results indicate that our approach yields a better performance than state-of-the-art approaches (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016; Tay et al., 2018) . ---------------------------------- **METHOD** String kernels. Kernel functions (Shawe-Taylor and Cristianini, 2004) capture the intuitive notion of similarity between objects in a specific domain. For example, in text mining, string kernels can be used to measure the pairwise similarity between text samples, simply based on character n-grams.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_2",
  "x": "To evaluate our approach, we use the Automated Student Assessment Prize (ASAP) 1 data set from Kaggle. The ASAP data set contains 8 prompts of different genres. The number of essays per prompt along with the score ranges are presented in Table 1 . Since the official test data of the ASAP competition is not released to the public, we, as well as others before us (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016;  1 https://www.kaggle.com/c/asap-aes/data Tay et al., 2018) , use only the training data in our experiments. Evaluation procedure. As Dong and Zhang (2016), we scaled the essay scores into the range 0-1. We closely followed the same settings for data preparation as (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) .",
  "y": "uses motivation similarities"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_3",
  "x": "To evaluate our approach, we use the Automated Student Assessment Prize (ASAP) 1 data set from Kaggle. The ASAP data set contains 8 prompts of different genres. The number of essays per prompt along with the score ranges are presented in Table 1 . Since the official test data of the ASAP competition is not released to the public, we, as well as others before us (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016;  1 https://www.kaggle.com/c/asap-aes/data Tay et al., 2018) , use only the training data in our experiments. Evaluation procedure. As Dong and Zhang (2016), we scaled the essay scores into the range 0-1. We closely followed the same settings for data preparation as (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) .",
  "y": "similarities uses"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_4",
  "x": "The 5-fold cross-validation procedure is repeated for 10 times and the results were averaged to reduce the accuracy variation introduced by randomly selecting the folds. We note that the standard deviation in all cases in below 0.2%. For the cross-domain experiments, we use the same source\u2192target domain pairs as (<cite>Phandi et al., 2015;</cite> Dong and Zhang, 2016) , namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928. All essays in the source domain are used as training data. Target domain samples are randomly divided into 5 folds, where one fold is used as test data, and the other 4 folds are collected together to sub-sample target domain train data. The sub-sample sizes are n t = {10, 25, 50, 100}. The sub-sampling is repeated for 5 times as in (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) to reduce bias. As our approach performs very well in the cross-domain setting, we also present experiments without subsampling data from the target domain, i.e. when the sub-sample size is n t = 0.",
  "y": "similarities uses"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_5",
  "x": "All essays in the source domain are used as training data. Target domain samples are randomly divided into 5 folds, where one fold is used as test data, and the other 4 folds are collected together to sub-sample target domain train data. The sub-sample sizes are n t = {10, 25, 50, 100}. The sub-sampling is repeated for 5 times as in (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) to reduce bias. As our approach performs very well in the cross-domain setting, we also present experiments without subsampling data from the target domain, i.e. when the sub-sample size is n t = 0. As evaluation metric, we use the quadratic weighted kappa (QWK). Baselines. We compare our approach with stateof-the-art methods based on handcrafted features (<cite>Phandi et al., 2015</cite>) , as well as deep features (Dong and Zhang, 2016; Tay et al., 2018) .",
  "y": "similarities"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_6",
  "x": "As evaluation metric, we use the quadratic weighted kappa (QWK). Baselines. We compare our approach with stateof-the-art methods based on handcrafted features (<cite>Phandi et al., 2015</cite>) , as well as deep features (Dong and Zhang, 2016; Tay et al., 2018) . We note that results for the cross-domain setting are reported only in some of these recent works (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . Implementation choices. For the string kernels approach, we used the histogram intersection string kernel (HISK) based on the blended range of character n-grams from 1 to 15. To compute the intersection string kernel, we used the open-source code provided by Ionescu et al. (2014) .",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_7",
  "x": "As our approach performs very well in the cross-domain setting, we also present experiments without subsampling data from the target domain, i.e. when the sub-sample size is n t = 0. As evaluation metric, we use the quadratic weighted kappa (QWK). Baselines. We compare our approach with stateof-the-art methods based on handcrafted features (<cite>Phandi et al., 2015</cite>) , as well as deep features (Dong and Zhang, 2016; Tay et al., 2018) . We note that results for the cross-domain setting are reported only in some of these recent works (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . Implementation choices. For the string kernels approach, we used the histogram intersection string kernel (HISK) based on the blended range of character n-grams from 1 to 15.",
  "y": "background"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_8",
  "x": "We note that results for the cross-domain setting are reported only in some of these recent works (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . Implementation choices. For the string kernels approach, we used the histogram intersection string kernel (HISK) based on the blended range of character n-grams from 1 to 15. To compute the intersection string kernel, we used the open-source code provided by Ionescu et al. (2014) . For the BOSWE approach, we used the pre-trained word embeddings computed by the word2vec toolkit (Mikolov et al., 2013) on the Google News data set using the Skip-gram model, which produces 300-dimensional vectors for 3 million words and phrases. We used functions from the VLFeat li- Table 2 : In-domain automatic essay scoring results of our approach versus several state-of-the-art methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016; Tay et al., 2018) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using 5-fold cross-validation.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_9",
  "x": "In-domain results. The results for the in-domain automatic essay scoring task are presented in Table 2. In our empirical study, we also include feature ablation results. We report the QWK measure on each prompt as well as the overall average. We first note that the histogram intersection string kernel alone reaches better overall performance (0.780) than all previous works (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016; Tay et al., 2018) . Remarkably, the overall performance of the HISK is also higher than the inter-human agreement (0.754). Although the BOSWE model can be regarded as a shallow approach, its overall results are comparable to those of deep learning approaches (Dong and Zhang, 2016; Tay et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_10",
  "x": "For each and every source\u2192target pair, we report better results than both state-of-theart methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . We observe that the difference between our best QWK scores and the <cite>other approaches</cite> are sometimes much higher in the cross-domain setting than in the in-domain setting. We particularly notice that the difference from (<cite>Phandi et al., 2015</cite>) when n t = 0 is always higher than 10%. Our highest improvement (more than 54%, from 0.187 to 0.728) over (<cite>Phandi et al., 2015</cite>) is recorded for the pair 5\u21926, when n t = 0. Our score in this case (0.728) is even higher than both scores of <cite>Phandi et al. (2015)</cite> and Dong and Zhang (2016) when they use n t = 50. Different from the in-domain setting, we note that the combination of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples (n t ) added into the training set is less or equal to 25. Discussion.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_11",
  "x": "The results for the crossdomain automatic essay scoring task are presented in Table 3 . For each and every source\u2192target pair, we report better results than both state-of-theart methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . We observe that the difference between our best QWK scores and the <cite>other approaches</cite> are sometimes much higher in the cross-domain setting than in the in-domain setting. We particularly notice that the difference from (<cite>Phandi et al., 2015</cite>) when n t = 0 is always higher than 10%. Our highest improvement (more than 54%, from 0.187 to 0.728) over (<cite>Phandi et al., 2015</cite>) is recorded for the pair 5\u21926, when n t = 0. Our score in this case (0.728) is even higher than both scores of <cite>Phandi et al. (2015)</cite> and Dong and Zhang (2016) when they use n t = 50. Different from the in-domain setting, we note that the combination of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples (n t ) added into the training set is less or equal to 25.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_12",
  "x": "The average QWK score of HISK and BOSWE (0.785) is more than 2% better the average scores of the best-performing state-of-the-art approaches Tay et al., 2018) . Cross-domain results. The results for the crossdomain automatic essay scoring task are presented in Table 3 . For each and every source\u2192target pair, we report better results than both state-of-theart methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . We observe that the difference between our best QWK scores and the <cite>other approaches</cite> are sometimes much higher in the cross-domain setting than in the in-domain setting. We particularly notice that the difference from (<cite>Phandi et al., 2015</cite>) when n t = 0 is always higher than 10%. Our highest improvement (more than 54%, from 0.187 to 0.728) over (<cite>Phandi et al., 2015</cite>) is recorded for the pair 5\u21926, when n t = 0.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_13",
  "x": "We observe that the difference between our best QWK scores and the <cite>other approaches</cite> are sometimes much higher in the cross-domain setting than in the in-domain setting. We particularly notice that the difference from (<cite>Phandi et al., 2015</cite>) when n t = 0 is always higher than 10%. Our highest improvement (more than 54%, from 0.187 to 0.728) over (<cite>Phandi et al., 2015</cite>) is recorded for the pair 5\u21926, when n t = 0. Our score in this case (0.728) is even higher than both scores of <cite>Phandi et al. (2015)</cite> and Dong and Zhang (2016) when they use n t = 50. Different from the in-domain setting, we note that the combination of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples (n t ) added into the training set is less or equal to 25. Discussion. It is worth noting that in a set of preliminary experiments (not included in the paper), we actually considered another approach based on word embeddings.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_14",
  "x": "The results for the crossdomain automatic essay scoring task are presented in Table 3 . For each and every source\u2192target pair, we report better results than both state-of-theart methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . We observe that the difference between our best QWK scores and the <cite>other approaches</cite> are sometimes much higher in the cross-domain setting than in the in-domain setting. We particularly notice that the difference from (<cite>Phandi et al., 2015</cite>) when n t = 0 is always higher than 10%. Our highest improvement (more than 54%, from 0.187 to 0.728) over (<cite>Phandi et al., 2015</cite>) is recorded for the pair 5\u21926, when n t = 0. Our score in this case (0.728) is even higher than both scores of <cite>Phandi et al. (2015)</cite> and Dong and Zhang (2016) when they use n t = 50. Different from the in-domain setting, we note that the combination of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples (n t ) added into the training set is less or equal to 25.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_15",
  "x": "We applied this method in the in-domain setting and we obtained a surprisingly low overall QWK score, around 0.251. We concluded that this simple apSource\u2192Target Method n t = 0 n t = 10 n t = 25 n t = 50 n t = 100 1\u21922 (<cite>Phandi et al., 2015</cite>) Table 3 : Corss-domain automatic essay scoring results of our approach versus two state-of-the-art methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using the same evaluation procedure as (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . The best QWK scores for each source\u2192target domain pair are highlighted in bold. proach is not useful, and decided to use BOSWE instead. It would have been interesting to present an error analysis based on the discriminant features weighted higher by the \u03bd-SVR method. Unfortunately, this is not possible because our approach works in the dual space and we cannot transform the dual weights into primal weights, as long as the histogram intersection kernel does not have an explicit embedding map associated to it.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_16",
  "x": "Results are reported in terms of the quadratic weighted kappa (QWK) measure, using the same evaluation procedure as (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . The best QWK scores for each source\u2192target domain pair are highlighted in bold. proach is not useful, and decided to use BOSWE instead. It would have been interesting to present an error analysis based on the discriminant features weighted higher by the \u03bd-SVR method. Unfortunately, this is not possible because our approach works in the dual space and we cannot transform the dual weights into primal weights, as long as the histogram intersection kernel does not have an explicit embedding map associated to it. In future work, however, we aim to replace the histogram intersection kernel with the presence bits kernel, which will enable us to perform an error analysis based on the overused or underused patterns, as described by . ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_17",
  "x": "Unfortunately, this is not possible because our approach works in the dual space and we cannot transform the dual weights into primal weights, as long as the histogram intersection kernel does not have an explicit embedding map associated to it. In future work, however, we aim to replace the histogram intersection kernel with the presence bits kernel, which will enable us to perform an error analysis based on the overused or underused patterns, as described by . ---------------------------------- **CONCLUSION** In this paper, we described an approach based on combining string kernels and word embeddings for automatic essay scoring. We compared our approach on the Automated Student Assessment Prize data set, in both in-domain and crossdomain settings, with several state-of-the-art approaches (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016; Tay et al., 2018) . Overall, the in-domain and the cross-domain comparative studies indicate that string kernels, both alone and in combination with word embeddings, attain the best performance on the automatic essay scoring task.",
  "y": "differences"
 },
 {
  "id": "484bc7c9c66bf4028eef4103beec7f_0",
  "x": "One of the poem's most distinctive voices is that of the woman who speaks at the end of its second section: I can't help it, she said, pulling a long face, It's them pills I took, to bring it off, she said [158] [159] Her chatty tone and colloquial grammar and lexis distinguish her voice from many others in the poem, such as the formal and traditionally poetic voice of a narrator that recurs many times in the poem: Above the antique mantel was displayed As though a window gave upon the sylvan scene The change of Philomel [97] [98] [99] Although the stylistic contrasts between these and other voices are clear to many readers, Eliot does not explicitly mark the transitions, nor is it obvious when a voice has reappeared. Our previous work focused on only the segmentation part of the voice identification task <cite>(Brooke et al., 2012)</cite> . Here, we instead assume an initial segmentation and then try to create clusters corresponding to segments of the The Waste Land which are spoken by the same voice. Of particular interest is the influence of the initial segmentation on the success of this downstream task. ----------------------------------",
  "y": "motivation background"
 },
 {
  "id": "484bc7c9c66bf4028eef4103beec7f_1",
  "x": "Clustering techniques have been applied to literature in general; for instance, Luyckx (2006) clustered novels according to style, and recent work in distinguishing two authors of sections of the Bible (Koppel et al., 2011) relies crucially on an initial clustering which is bootstrapped into a supervised classifier which is applied to segments. Beyond literature, the tasks of stylistic inconsistency detection (Graham et al., 2005; Guthrie, 2008) and intrinsic (unsupervised) plagiarism detection (Stein et al., 2011) are very closely related to our interests here, though in such tasks usually only two authors are posited; more general kinds of authorship identification (Stamatatos, 2009 ) may include many more authors, though some form of supervision (i.e. training data) is usually assumed. Our work here is built on our earlier work <cite>(Brooke et al., 2012)</cite> . Our segmentation model for The Waste Land was based on a stylistic change curve whose values are the distance between stylistic feature vectors derived from 50 token spans on either side of each point (spaces between tokens) in the text; the local maxima of this curve represent likely voice switches. Performance on The Waste Land was far from perfect, but evaluation using standard text segmentation metrics (Pevzner and Hearst, 2002) indicated that it was well above various baselines. ---------------------------------- **METHOD**",
  "y": "background"
 },
 {
  "id": "484bc7c9c66bf4028eef4103beec7f_2",
  "x": "**METHOD** Our approach to voice identification in The Waste Land consists first of identifying the boundaries of voice spans <cite>(Brooke et al., 2012)</cite> . Given a segmentation of the text, we consider each span as a data point in a clustering problem. The elements of the vector correspond to the best feature set from the segmentation task, with the rationale that features which were useful for detecting changes in style should also be useful for identifying stylistic similarities. Our features therefore include: a collection of readability metrics (including word length), frequency of punctuation, line breaks, and various parts-ofspeech, lexical density, average frequency in a large external corpus (Brants and Franz, 2006) , lexiconbased sentiment metrics using SentiWordNet (Baccianella et al., 2010) , formality score (Brooke et al., 2010) , and, perhaps most notably, the centroid of 20-dimensional distributional vectors built using latent semantic analysis (Landauer and Dumais, 1997), reflecting the use of words in a large web corpus (Burton et al., 2009) ; in previous work (Brooke et al., 2010) , we established that such vectors contain useful stylistic information about the English lexicon (including rare words that appear only occasionally in such a corpus), and indeed LSA vectors were the single most promising feature type for segmentation. For a more detailed discussion of the feature set, see<cite> Brooke et al. (2012)</cite> . All the features are normalized to a mean of zero and a standard deviation of 1.",
  "y": "background"
 },
 {
  "id": "484bc7c9c66bf4028eef4103beec7f_3",
  "x": "Given a segmentation of the text, we consider each span as a data point in a clustering problem. The elements of the vector correspond to the best feature set from the segmentation task, with the rationale that features which were useful for detecting changes in style should also be useful for identifying stylistic similarities. Our features therefore include: a collection of readability metrics (including word length), frequency of punctuation, line breaks, and various parts-ofspeech, lexical density, average frequency in a large external corpus (Brants and Franz, 2006) , lexiconbased sentiment metrics using SentiWordNet (Baccianella et al., 2010) , formality score (Brooke et al., 2010) , and, perhaps most notably, the centroid of 20-dimensional distributional vectors built using latent semantic analysis (Landauer and Dumais, 1997), reflecting the use of words in a large web corpus (Burton et al., 2009) ; in previous work (Brooke et al., 2010) , we established that such vectors contain useful stylistic information about the English lexicon (including rare words that appear only occasionally in such a corpus), and indeed LSA vectors were the single most promising feature type for segmentation. For a more detailed discussion of the feature set, see<cite> Brooke et al. (2012)</cite> . All the features are normalized to a mean of zero and a standard deviation of 1. For clustering, we use a slightly modified version of the popular k-means algorithm (MacQueen, 1967) . Briefly, k-means assigns points to a cluster based on their proximity to the k cluster centroids, which are initialized to randomly chosen points from the data and then iteratively refined until convergence, which in our case was defined as a change of less than 0.0001 in the position of each centroid during one iteration.",
  "y": "background"
 },
 {
  "id": "484bc7c9c66bf4028eef4103beec7f_4",
  "x": "Our gold standard thus represents just one potential interpretation of the poem, rather than a true, unique gold standard. The average size of the 69 segments in the gold standard is 50 tokens; the range, however, is fairly wide: the longest is 373 tokens, while the shortest consists of a single token. Our annotation has 13 voices altogether. We consider three segmentations: the segmentation of our gold standard (Gold), the segmentation predicted by our segmentation model (Automatic), and a segmentation which consists of equallength spans (Even), with the same number of spans as in the gold standard. The Even segmentation should be viewed as the baseline for segmentation, and the Gold segmentation an \"oracle\" representing an upper bound on segmentation performance. For the automatic segmentation model, we use the settings from<cite> Brooke et al. (2012)</cite> . We also compare three possible clusterings for each segmentation: no clustering at all (Initial), that is, we assume that each segment is a new voice; k-means clustering (k-means), as outlined above; and random clustering (Random), in which we randomly assign each voice to a cluster.",
  "y": "uses"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_0",
  "x": "An early approach for recipe retrieval was based on jointly learning to predict food category and its ingredients using deep CNN [4] . In a following step, the predicted ingredients are matched against a large corpus of recipes. More recent approach is proposed by [22] and is based on jointly learning recipe-text and image representations in a shared latent space. Recurrent Neural Networks (RNN) and CNN are mainly used to map text and image into the shared space. To align the text and image embedding vectors between matching recipe-image pairs, cosine similarity loss with margin was applied. Carvalho et al. <cite>[3]</cite> proposed a similar multi-modal embedding method for aligning text and image representations in a shared latent space. In contrast to Salvador et al. [22] , they formulated a joint objective function which incorporates the loss for the cross-modal retrieval task and a classification loss, instead of using the latent space for a multitask learning setup.",
  "y": "background"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_1",
  "x": "In order to encode the ingredients, a bidirectional RNN is used, since ingredients are an unordered list of words. All RNNs in the ingredients path were implemented with Long Short-Term Memory (LSTM) cells [11] . We fixed the ingredient representation to have a length of 600, independent of the amount of ingredients. Lastly, the outputs of the self-attention-instruction encoder with ingredient attention and the output of the bidirectional LSTM ingredient-encoder are concatenated and mapped to the joint embedding space. The image analysis path is composed of a ResNet-50 model [9] , pretrained on the ImageNet Dataset [7] , with a custom top layer for mapping the image features to the joint embedding space. All word embeddings are pretrained with the word2vec algorithm [19] and fine tuned during the joint embedding learning phase. We chose 512-dimensional word embedding for our model with self-attention, whereas [17] and <cite>[3]</cite> chose a vector length of 300.",
  "y": "differences"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_2",
  "x": "**LOSS FUNCTION** To align text and image embeddings of matching recipe-image pairs alongside each other, we maximize the cosine distance between positive pairs and minimize it between negative pairs. We have trained our model using cosine similarity loss with margin as in [17] and with the triplet loss proposed by <cite>[3]</cite> . Both objective functions and the semantic regularization by [17] aim at maximizing intra-class correlation and minimizing inter-class correlation. Let us define the text query embedding as \u03d5 q and the embedding of the image query as \u03d5 d , then the cosine embedding loss can be defined as follows: where cos (x,y) is the normalized cosine similarity and \u03b1 is a margin (\u22121 \u2a7d \u03b1 \u2a7d 1), that determines how similar negative pairs are allowed to be. Positive margins allow negative pairs to share at maximum \u03b1 similarity, where a maximum margin of zero or negative margins allow no correlation between non matching embedding vectors or force the model to learn antiparallel representations, respectively.",
  "y": "uses"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_3",
  "x": "We used Adam [14] optimizer with an initial learning rate of 10 \u22124 . At the beginning of the training session, we freeze the pretrained ResNet-50 weights and optimize only the text-processing branch until we do no longer make progress. Then, we alternate train image and text branch until we switched modality for 10 times. Lastly, we fine-tune the overall model by releasing all trainable parameters in the model. Our optimization strategy differs from [17] in that we use an aggressive learning rate decay, namely exponential decay, so that the learning rate is halved all 20 epochs. Since the timing of freezing layers proved not to be of importance unless the recipe path is trained first, we used the same strategy under the cosine distance objective [17] and for the triplet loss <cite>[3]</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_4",
  "x": "Similarly to [17] and <cite>[3]</cite> , we evaluated our model on 10 subsets of 1000 samples each. One sample of these subsets is composed of text embedding and image embedding in the shared latent space. Since our interest lies in the recipe retrieval task, we optimized and evaluated our model by using each image embedding in the subsets as query against all text embeddings. By ranking the query and the candidate embeddings according to their cosine distance, we estimate the median rank. The model's performance is best, if the matching text embedding is found at the first rank. Further, we estimate the recall percentage at the top K percent over all queries. The recall percentage describes the quantity of queries ranked amid the top K closest results.",
  "y": "similarities"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_6",
  "x": "MADiMa '19, October 21, 2019, Nice, France [17] and AdaMine <cite>[3]</cite> re-implementation. For all models we were using selected matching pairs generated by reducing noisy instruction sentences as described above. Recall rates are averaged over the evaluation batches. Image to Recipe MedR R@1 R@5 R@10 1k samples Random [17] 500.0 0.001 0.005 0.01 JNE [17] 5.0 \u00b1 0. Both [17] and <cite>[3]</cite> use time-consuming instruction text preprocessing over the skip-thought technique [15] . This process doubles the overall training time from three days to six days using two Nvidia Titan X GPU's.",
  "y": "background"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_7",
  "x": "Image to Recipe MedR R@1 R@5 R@10 1k samples Random [17] 500.0 0.001 0.005 0.01 JNE [17] 5.0 \u00b1 0. Both [17] and <cite>[3]</cite> use time-consuming instruction text preprocessing over the skip-thought technique [15] . This process doubles the overall training time from three days to six days using two Nvidia Titan X GPU's. By using online-instruction encoding with the self-attention encoder, we were able train the model for its main task in under 30 hours. Furthermore, the proposed approach offers more flexibility for dataset alterations. Figure 4 : Ingredient-Attention based focus on instruction sentences.",
  "y": "differences"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_9",
  "x": "Qualitative results such as recipe retrieval, quality of the cluster formation in the joint embedding space and heat maps of instruction words are more important than the previously mentioned benchmarking scores. Depending on meal type, all baseline implementations as well as our Ingredient Attention based model exhibit a broad range of retrieval accuracy. In Figure 5 we present a few typical results on the intended recipe retrieval task. AdaMine <cite>[3]</cite> creates more distinct class clusters than in [17] . In Figure 3 , we demonstrate the difference in cluster formation using the aforementioned Methods for our Ingredient Attention. We visualize the top ten most common recipe classes in Recipe1M using t-SNE [23] . Since chocolate chip, peanut butter, cream cheese and/or ice cream are used as ingredients in desserts, due to semantic regularization inside the triplet loss, clusters of sweet meals are close together (Figure 3b top right corner).",
  "y": "uses"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_10",
  "x": "---------------------------------- **CONCLUSIONS** In this paper, we have introduced self-attention for instruction encoding in the context of the recipe retrieval task and ingredient attention for disclosing ingredient dependent meal preparation steps. Our main contribution is the aforementioned ingredient attention, empowering our model to solve the recipe retrieval without any upstream skip instruction embedding, as well as the light-weight architecture provided by the transformer-like instruction encoder. On the recipe retrieval task, our method performs similarly to our baseline implementation of <cite>[3]</cite> . Regarding training time on the other hand, we increased the efficiency significantly for crossmodal based retrieval methods. There is no need for a maximum number of instructions for a recipe to be considered as valid for training or testing; only for total words, making more samples of the large Recipe1M corpus usable for training.",
  "y": "similarities"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_0",
  "x": "Before applying tools trained on specific languages, one must determine the language of the text. It has attracted considerable attention in recent years [1, 2,<cite> 3,</cite> 4, 5, 6, 7, 8] . Most of the existing approaches take words as features, and then adopt effective supervised classification algorithms to solve the problem. Generally speaking, language identification between different languages is a task that can be solved at a high accuracy. For example, Simoes et al. [9] achieved 97% accuracy for discriminating among 25 unrelated languages. However, it is generally difficult to distinguish between related languages or variations of a specific language (see [9] and [10] for example). To deal with this problem, Huang and Lee<cite> [3]</cite> proposed a contrastive approach based on documentlevel top-bag-of-word similarity to reflect distances among the three varieties of Mandarin in China, Taiwan and Singapore, which is a kind of word-level uni-gram feature.",
  "y": "background"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_1",
  "x": "However, it is generally difficult to distinguish between related languages or variations of a specific language (see [9] and [10] for example). To deal with this problem, Huang and Lee<cite> [3]</cite> proposed a contrastive approach based on documentlevel top-bag-of-word similarity to reflect distances among the three varieties of Mandarin in China, Taiwan and Singapore, which is a kind of word-level uni-gram feature. The word unigram feature is sufficient for document-level identification of language variants. More recent studies focus on sentence-level languages identification, such as the Discriminating between Similar Languages (DSL) shared task 2014 and 2015 [7, 8] . The best system of these shard tasks shows that the uni-gram is an effective feature. For the sentence-level language identification, you are given a single sentence, and you need to identify the language. Chinese is spoken in different regions, with noticeable differences between regions.",
  "y": "background"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_3",
  "x": "The above observation indicates that character form, PMI-based and word alignment-based information are useful information to discriminate dialects in the GCR. In order to investigate the detailed characteristics of different dialects of Mandarin Chinese, we extend <cite>3</cite> dialects in Huang and Lee<cite> [3]</cite> to 6 dialects. In fact, the more dialects there are, the more difficult the dialects discrimination becomes. It also has been verified through our experiments. Very often, texts written in a character set are converted to another character set, in particular on the Web. This makes the character form feature unusable. In order to detect dialects for those texts, we convert texts in traditional characters to simplified characters in order to investigate the effectiveness of linguistic and textual features alone.",
  "y": "extends"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_5",
  "x": "A number of studies on identification of similar languages and language varieties have been carride out. For example, Murthy and Kumar [1] focused on Indian languages identification. Meanwhile, Ranaivo-Malancon [2] proposed features based on frequencies of character n-grams to identify Malay and Indonesian. Huang and Lee<cite> [3]</cite> presented the top-bag-of-word similarity based contrastive approach to reflect distances among the three varieties of Mandarin in Mainland China, Taiwan and Singapore. Zampieri and Gebre [4] found that word uni-grams gave very similar performance to character n-gram features in the framework of the probabilistic language model for the Brazilian and European Portuguese language discrimination. Tiedemann and Ljubesic [5] ; Ljubesic and Kranjcic [6] showed that the Na\u00efve Bayes classifier with uni-grams achieved high accuracy for the South Slavic languages identification. Grefenstette [11] ; Lui and Cook [12] found that bag-of-words features outperformed the syntax or character sequencesbased features for the English varieties.",
  "y": "background"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_7",
  "x": "Besides these works, other recent studies include: Spanish varieties identification [1<cite>3</cite>] , Arabic varieties discrimination [14, 15, 16, 17] , and Persian and Dari identification [18] . Among the above related works, study<cite> [3]</cite> is the most related work to ours. The differences between study<cite> [3]</cite> and our work are two-fold: (1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore. In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in Huang and Lee<cite> [3]</cite> to 6 dialects. Also, the more dialects there are, the more difficult the dialects discrimination becomes. (2)The top-bag-of-word they proposed in Huang and Lee<cite> [3]</cite> is word uni-gram feature essentially.",
  "y": "similarities"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_8",
  "x": "Grefenstette [11] ; Lui and Cook [12] found that bag-of-words features outperformed the syntax or character sequencesbased features for the English varieties. Besides these works, other recent studies include: Spanish varieties identification [1<cite>3</cite>] , Arabic varieties discrimination [14, 15, 16, 17] , and Persian and Dari identification [18] . Among the above related works, study<cite> [3]</cite> is the most related work to ours. The differences between study<cite> [3]</cite> and our work are two-fold: (1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore. In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in Huang and Lee<cite> [3]</cite> to 6 dialects. Also, the more dialects there are, the more difficult the dialects discrimination becomes.",
  "y": "differences"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_9",
  "x": "Grefenstette [11] ; Lui and Cook [12] found that bag-of-words features outperformed the syntax or character sequencesbased features for the English varieties. Besides these works, other recent studies include: Spanish varieties identification [1<cite>3</cite>] , Arabic varieties discrimination [14, 15, 16, 17] , and Persian and Dari identification [18] . Among the above related works, study<cite> [3]</cite> is the most related work to ours. The differences between study<cite> [3]</cite> and our work are two-fold: (1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore. In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in Huang and Lee<cite> [3]</cite> to 6 dialects. Also, the more dialects there are, the more difficult the dialects discrimination becomes.",
  "y": "extends"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_10",
  "x": "Among the above related works, study<cite> [3]</cite> is the most related work to ours. The differences between study<cite> [3]</cite> and our work are two-fold: (1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore. In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in Huang and Lee<cite> [3]</cite> to 6 dialects. Also, the more dialects there are, the more difficult the dialects discrimination becomes. (2)The top-bag-of-word they proposed in Huang and Lee<cite> [3]</cite> is word uni-gram feature essentially. While in this paper, besides the traditional uni-gram feature, we propose some novel features, such as character form, PMI-based and word alignment-based features.",
  "y": "differences"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_11",
  "x": "**N-GRAM FEATURES** According to the related works [4, 5, 6] , word uni-grams are effective features for discriminating general languages. Compared with English, no space exists between words in Chinese sentence. Therefore, we use character uni-grams, bi-grams and tri-grams as features. However, Huang and Lee<cite> [3]</cite> did not use character-level n-grams. ---------------------------------- **CHARACTER FORM FEATURES**",
  "y": "differences"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_19",
  "x": "For evaluation, we adopt 5-cross validation for the two datasets. For the news dataset, we generate three scenarios: (1) 6-way detection: The dialects of Mainland China, Hong Kong, Taiwan, Macao, Malaysia and Singapore are all considered; (2) <cite>3</cite>-way detection: We detect dialects of Mainland China, Taiwan and Singapore as in Huang and Lee<cite> [3]</cite> ; (<cite>3</cite>) 2-way detection: We try to distinguish between two groups of dialects, the ones used in Mainland China, Malaysia and Singapore using simplified characters, and the ones used in Hong Kong, Taiwan and Macao using traditional characters. For the Wikipedia dataset, we also generate two similar scenarios: (1) <cite>3</cite>-way detection: We detect dialects of Mainland China, Hong Kong and Taiwan; (2) 2-way detection: We try to distinguish between two groups of dialects, the ones used in Mainland China using simplified characters, and the ones used in Hong Kong and Taiwan using traditional characters. Baseline system 1: As mentioned in Section 2, we take the Huang and Lee<cite> [3]</cite> 's top-bag-of-word similarity-based approach as one of our baseline system. We re-implement their method in this paper using the similar <cite>3</cite>-way news dataset.",
  "y": "uses"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_22",
  "x": "**(1) SINGLE FEATURES** If we use a single type of feature, we can see that the uni-gram feature (baseline system 2) is not the best one for Chinese dialect detection in the GCR, although it has been found effective for English detection in previous studies in the DSL shared task. Instead, bi-gram and word segmentation based features are better than uni-gram one. Both of the proposed bi-gram and word segmentation based features significantly outperforms the baseline systems with p<0.01 using paired t-test for significance. Also the bi-gram and word segmentation based features are better than the Huang and Lee<cite> [3]</cite> 's method (baseline system 1) for 6-way, <cite>3</cite>-way and 2-way dialect identification in the GCR. Obviously, the random method does not work for the GCR dialect identification. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_28",
  "x": "---------------------------------- **RESULTS ON WIKIPEDIA DATASET** As shown in Table <cite>3</cite> , character form based features are very effective (94.<cite>3</cite>6% for 2-way dialects classification). Similar to Huang and Lee<cite> [3]</cite> 's work, in order to eliminate the trivial issue of character encoding (simplified and traditional character), we convert Taiwan and Hong Kong texts to the same simplified character set using Zhconvertor 6 utility to focus on actual linguistic and textual features. Table 6 shows the experimental results for the dialect identification in the GCR. As shown, again, the bi-gram features work better than both uni-gram and tri-gram features on Wikipedia dataset. Also, the word alignment-based features can contribute about <cite>3</cite>.<cite>3</cite>2% performance improvement.",
  "y": "similarities"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_0",
  "x": "****A WORD EMBEDDING APPROACH TO IDENTIFYING VERB-NOUN IDIOMATIC COMBINATIONS**** **ABSTRACT** Verb-noun idiomatic combinations (VNICs) are idioms consisting of a verb with a noun in its direct object position. Usages of these expressions can be ambiguous between an idiomatic usage and a literal combination. In this paper we propose supervised and unsupervised approaches, based on word embeddings, to identifying token instances of VNICs. Our proposed supervised and unsupervised approaches perform better than the supervised and unsupervised approaches of <cite>Fazly et al. (2009)</cite> , respectively. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_1",
  "x": "The first example of hit the roof is an idiomatic usage, while the second is a literal combination. 1 MWE identification is the task of determining which token instances in running text are MWEs (Baldwin and Kim, 2010) . Although there has been relatively less work on MWE identification than other type-level MWE prediction tasks, it is nevertheless important for NLP applications such as machine translation that must be able to distinguish MWEs from literal combinations in context. Some recent work has focused on token-level identification of a wide range of types of MWEs and other multiword units (e.g., Newman et al., 2012; Schneider et al., 2014; Brooke et al., 2014) . Many studies, however, have taken a word sense disambiguation-inspired approach to MWE identification (e.g., Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010) , treating literal combinations and MWEs as different word senses, and have exploited linguistic knowledge of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005; Hashimoto and Kawahara, 2008; <cite>Fazly et al., 2009</cite>; Fothergill and Baldwin, 2012) . In this study we focus on English verb-noun idiomatic combinations (VNICs). VNICs are formed from a verb with a noun in its direct object position.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_2",
  "x": "Some recent work has focused on token-level identification of a wide range of types of MWEs and other multiword units (e.g., Newman et al., 2012; Schneider et al., 2014; Brooke et al., 2014) . Many studies, however, have taken a word sense disambiguation-inspired approach to MWE identification (e.g., Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010) , treating literal combinations and MWEs as different word senses, and have exploited linguistic knowledge of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005; Hashimoto and Kawahara, 2008; <cite>Fazly et al., 2009</cite>; Fothergill and Baldwin, 2012) . In this study we focus on English verb-noun idiomatic combinations (VNICs). VNICs are formed from a verb with a noun in its direct object position. They are a common and productive type of English idiom, and occur cross-lingually (<cite>Fazly et al., 2009</cite>) . VNICs tend to be relatively lexico-syntactically fixed, e.g., whereas hit the roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages. <cite>Fazly et al. (2009)</cite> exploit this property in their unsupervised approach, referred to as CFORM.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_3",
  "x": "VNICs tend to be relatively lexico-syntactically fixed, e.g., whereas hit the roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages. <cite>Fazly et al. (2009)</cite> exploit this property in their unsupervised approach, referred to as CFORM. <cite>They</cite> define lexico-syntactic patterns for VNIC token instances based on the noun's determiner (e.g., a, the, or possibly no determiner), the number of the noun (singular or plural), and the verb's voice (active or passive). <cite>They</cite> propose a statistical method for automatically determining a given VNIC type's canonical idiomatic form, based on the frequency of its usage in these patterns in a corpus. 2 <cite>They</cite> then classify a given token instance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise. <cite>Fazly et al</cite>. also consider a supervised approach that classifies a given VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training. Distributed representations of word meaning in the form of word embeddings (Mikolov et al., 2013) have recently been demonstrated to benefit a wide range of NLP tasks including POS tagging (e.g., Ling et al., 2015) , question answering (e.g., Dong et al., 2015) , and machine translation (e.g., Zou et al., 2013) .",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_4",
  "x": "VNICs are formed from a verb with a noun in its direct object position. They are a common and productive type of English idiom, and occur cross-lingually (<cite>Fazly et al., 2009</cite>) . VNICs tend to be relatively lexico-syntactically fixed, e.g., whereas hit the roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages. <cite>Fazly et al. (2009)</cite> exploit this property in their unsupervised approach, referred to as CFORM. <cite>They</cite> define lexico-syntactic patterns for VNIC token instances based on the noun's determiner (e.g., a, the, or possibly no determiner), the number of the noun (singular or plural), and the verb's voice (active or passive). <cite>They</cite> propose a statistical method for automatically determining a given VNIC type's canonical idiomatic form, based on the frequency of its usage in these patterns in a corpus. 2 <cite>They</cite> then classify a given token instance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_5",
  "x": "They are a common and productive type of English idiom, and occur cross-lingually (<cite>Fazly et al., 2009</cite>) . VNICs tend to be relatively lexico-syntactically fixed, e.g., whereas hit the roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages. <cite>Fazly et al. (2009)</cite> exploit this property in their unsupervised approach, referred to as CFORM. <cite>They</cite> define lexico-syntactic patterns for VNIC token instances based on the noun's determiner (e.g., a, the, or possibly no determiner), the number of the noun (singular or plural), and the verb's voice (active or passive). <cite>They</cite> propose a statistical method for automatically determining a given VNIC type's canonical idiomatic form, based on the frequency of its usage in these patterns in a corpus. 2 <cite>They</cite> then classify a given token instance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise. <cite>Fazly et al</cite>. also consider a supervised approach that classifies a given VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_6",
  "x": "<cite>They</cite> propose a statistical method for automatically determining a given VNIC type's canonical idiomatic form, based on the frequency of its usage in these patterns in a corpus. 2 <cite>They</cite> then classify a given token instance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise. <cite>Fazly et al</cite>. also consider a supervised approach that classifies a given VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training. Distributed representations of word meaning in the form of word embeddings (Mikolov et al., 2013) have recently been demonstrated to benefit a wide range of NLP tasks including POS tagging (e.g., Ling et al., 2015) , question answering (e.g., Dong et al., 2015) , and machine translation (e.g., Zou et al., 2013) . Moreover, word embeddings have been shown to improve over count-based models of distributional similarity for predicting MWE compositionality (Salehi et al., 2015) . In this work we first propose a supervised approach to identifying VNIC token instances based on word embeddings that outperforms the supervised method of <cite>Fazly et al. (2009)</cite> . We then propose an unsupervised approach to this task, that combines word embeddings with <cite>Fazly et al</cite>.'s unsupervised CFORM approach, that improves over <cite>CFORM</cite>.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_7",
  "x": "2 <cite>They</cite> then classify a given token instance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise. <cite>Fazly et al</cite>. also consider a supervised approach that classifies a given VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training. Distributed representations of word meaning in the form of word embeddings (Mikolov et al., 2013) have recently been demonstrated to benefit a wide range of NLP tasks including POS tagging (e.g., Ling et al., 2015) , question answering (e.g., Dong et al., 2015) , and machine translation (e.g., Zou et al., 2013) . Moreover, word embeddings have been shown to improve over count-based models of distributional similarity for predicting MWE compositionality (Salehi et al., 2015) . In this work we first propose a supervised approach to identifying VNIC token instances based on word embeddings that outperforms the supervised method of <cite>Fazly et al. (2009)</cite> . We then propose an unsupervised approach to this task, that combines word embeddings with <cite>Fazly et al</cite>.'s unsupervised CFORM approach, that improves over <cite>CFORM</cite>. ----------------------------------",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_8",
  "x": "Distributed representations of word meaning in the form of word embeddings (Mikolov et al., 2013) have recently been demonstrated to benefit a wide range of NLP tasks including POS tagging (e.g., Ling et al., 2015) , question answering (e.g., Dong et al., 2015) , and machine translation (e.g., Zou et al., 2013) . Moreover, word embeddings have been shown to improve over count-based models of distributional similarity for predicting MWE compositionality (Salehi et al., 2015) . In this work we first propose a supervised approach to identifying VNIC token instances based on word embeddings that outperforms the supervised method of <cite>Fazly et al. (2009)</cite> . We then propose an unsupervised approach to this task, that combines word embeddings with <cite>Fazly et al</cite>.'s unsupervised CFORM approach, that improves over <cite>CFORM</cite>. ---------------------------------- **MODELS FOR VNIC IDENTIFICATION BASED ON WORD EMBEDDINGS** The following subsections propose supervised and unsupervised approaches to VNIC identification based on word embeddings.",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_9",
  "x": "<cite>They</cite> propose a statistical method for automatically determining a given VNIC type's canonical idiomatic form, based on the frequency of its usage in these patterns in a corpus. 2 <cite>They</cite> then classify a given token instance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise. <cite>Fazly et al</cite>. also consider a supervised approach that classifies a given VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training. Distributed representations of word meaning in the form of word embeddings (Mikolov et al., 2013) have recently been demonstrated to benefit a wide range of NLP tasks including POS tagging (e.g., Ling et al., 2015) , question answering (e.g., Dong et al., 2015) , and machine translation (e.g., Zou et al., 2013) . Moreover, word embeddings have been shown to improve over count-based models of distributional similarity for predicting MWE compositionality (Salehi et al., 2015) . In this work we first propose a supervised approach to identifying VNIC token instances based on word embeddings that outperforms the supervised method of <cite>Fazly et al. (2009)</cite> . We then propose an unsupervised approach to this task, that combines word embeddings with <cite>Fazly et al</cite>.'s unsupervised CFORM approach, that improves over <cite>CFORM</cite>.",
  "y": "differences uses"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_10",
  "x": "where k is the window size that the word2vec model was trained on, and w j t is the embedding of the word in position t of the input sentence relative to the jth component of the MWE (i.e., either the verb or noun). In forming c verb and c noun the other component token of the VNIC is not considered part of the context. The summation is done over the same window size that the word2vec model was trained on so that c j captures the same information that the word2vec model has learned to capture. After computing c verb and c noun these vectors are averaged to form c. Figure 1 shows the process for forming c for an example sentence. Finally, to form the feature vector representing a VNIC instance, we subtract e from c, and append to this vector a single binary feature representing whether the VNIC instance occurs in its canonical form, as determined by <cite>Fazly et al. (2009)</cite> . The feature vectors are then used to train a supervised classifier; in our experiments we use the linear SVM implementation from Pedregosa et al. (2011) . The motivation for the subtraction is to capture the difference between the context in which a VNIC instance occurs ( c) and a type-level representation of that expression ( e), to potentially represent VNIC instances such that the classifier is able to generalize across expressions (i.e., to generalize to MWE types that are unseen during training).",
  "y": "uses"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_11",
  "x": "The canonical form feature is included because it is known to be highly informative as to whether an instance is idiomatic or literal. ---------------------------------- **UNSUPERVISED VNIC IDENTIFICATION** Our unsupervised approach combines the word embedding-based representation used in the supervised approach (without relying on training a supervised classifier, of course) with the unsupervised CFORM method of <cite>Fazly et al. (2009)</cite> . In this approach, we first represent each token instance of a given VNIC type as a feature vector, using the same representation as in Section 2.1. 3 We then apply k-means clustering to form k clusters of the token instances. 4 All instances in each cluster are then assigned a single class, idiomatic or literal, depending on whether the majority of token instances in a cluster are in that VNIC's canonical form or not, respectively.",
  "y": "uses"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_12",
  "x": "Following <cite>Fazly et al. (2009)</cite> , the supervised approach was evaluated using a leave-one-token-out strategy. That is, for each MWE, a single token instance is held out, and the classifier is trained on the remaining instances. The trained model is then used to classify the held out instance. This is Table 1 for a variety of settings of window size and number of dimensions for the word embeddings. The results reveal the general trend that smaller window sizes, and more dimensions, tend to give higher accuracy, although the overall amount of variation is relatively small. The accuracy on DEV and TEST ranges from 85.5%-88.2% and 83.4%-88.3%, respectively. All of these accuracies are higher than those reported by <cite>Fazly et al. (2009)</cite> for their supervised approach.",
  "y": "uses"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_13",
  "x": "That is, for each MWE, a single token instance is held out, and the classifier is trained on the remaining instances. The trained model is then used to classify the held out instance. This is Table 1 for a variety of settings of window size and number of dimensions for the word embeddings. The results reveal the general trend that smaller window sizes, and more dimensions, tend to give higher accuracy, although the overall amount of variation is relatively small. The accuracy on DEV and TEST ranges from 85.5%-88.2% and 83.4%-88.3%, respectively. All of these accuracies are higher than those reported by <cite>Fazly et al. (2009)</cite> for their supervised approach. They are also substantially higher than the most-frequent class baseline, and the unsupervised CFORM method of <cite>Fazly et al</cite>.",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_14",
  "x": "All of these accuracies are higher than those reported by <cite>Fazly et al. (2009)</cite> for their supervised approach. They are also substantially higher than the most-frequent class baseline, and the unsupervised CFORM method of <cite>Fazly et al</cite>. That a window size of just 1 performs well is interesting. A word2vec model with a smaller window size gives more syntactically-oriented word embeddings, whereas a larger window size gives more semantically-oriented embeddings (Trask et al., 2015) . The CFORM method of <cite>Fazly et al. (2009 )</cite> is a strong unsupervised benchmark for this task, and relies on the lexico-syntactic pattern in which an MWE token instance occurs. A smaller window size for the word embedding features might be better able to capture similar information to <cite>CFORM</cite>, which could explain the good performance of the model using a window size of 1. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_15",
  "x": "The accuracy on DEV and TEST ranges from 85.5%-88.2% and 83.4%-88.3%, respectively. All of these accuracies are higher than those reported by <cite>Fazly et al. (2009)</cite> for their supervised approach. They are also substantially higher than the most-frequent class baseline, and the unsupervised CFORM method of <cite>Fazly et al</cite>. That a window size of just 1 performs well is interesting. A word2vec model with a smaller window size gives more syntactically-oriented word embeddings, whereas a larger window size gives more semantically-oriented embeddings (Trask et al., 2015) . The CFORM method of <cite>Fazly et al. (2009 )</cite> is a strong unsupervised benchmark for this task, and relies on the lexico-syntactic pattern in which an MWE token instance occurs. A smaller window size for the word embedding features might be better able to capture similar information to <cite>CFORM</cite>, which could explain the good performance of the model using a window size of 1.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_16",
  "x": "They are also substantially higher than the most-frequent class baseline, and the unsupervised CFORM method of <cite>Fazly et al</cite>. That a window size of just 1 performs well is interesting. A word2vec model with a smaller window size gives more syntactically-oriented word embeddings, whereas a larger window size gives more semantically-oriented embeddings (Trask et al., 2015) . The CFORM method of <cite>Fazly et al. (2009 )</cite> is a strong unsupervised benchmark for this task, and relies on the lexico-syntactic pattern in which an MWE token instance occurs. A smaller window size for the word embedding features might be better able to capture similar information to <cite>CFORM</cite>, which could explain the good performance of the model using a window size of 1. ---------------------------------- **GENERALIZATION TO UNSEEN VNICS**",
  "y": "similarities"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_17",
  "x": "For these experiments we consider the setup that performed best on average over DEV and TEST in the previous experiments (i.e., a window size of 1 and 300 dimensional vectors). The macroaveraged accuracy on DEV and TEST is 68.9% and 69.4%, respectively. Although this is a substantial improvement over the most-frequent class baseline, it is well-below the accuracy for the previously-considered leave-one-token-out setup. Moreover, the unsupervised CFORM method of <cite>Fazly et al. (2009)</cite> gives substantially higher accuracies than this supervised approach. The limited ability of this model to generalize to unseen MWE types further motivates exploring unsupervised approaches to this task. ---------------------------------- **UNSUPERVISED RESULTS**",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_18",
  "x": "For k = 4 and 5 on TEST, this approach surpasses the unsupervised CFORM method of <cite>Fazly et al. (2009)</cite> ; however, on DEV this approach does not outperform <cite>Fazly et al</cite>.'s CFORM approach for any of the val-ues of k considered. Analyzing the results on individual expressions indicates that the unsupervised approach gives especially low accuracy for hit roof -which is in DEV-as compared to the CFORM method of <cite>Fazly et al.</cite>, which could contribute to the overall lower accuracy of the unsupervised approach on this dataset. We now consider the upperbound of an unsupervised approach that selects a single label for each cluster of usages. In the right panel of Table 2 we show results for an oracle approach that always selects the best label for each cluster. In this case, as the number of clusters increases, so too will the accuracy. 9 Nevertheless, these results show that, even for relatively small values of k, there is scope for improving the proposed unsupervised method through improved methods for selecting the label for each cluster, and that the performance of such a method could potentially come close to that of the supervised approach. A word's predominant sense is known to be a powerful baseline in word-sense disambiguation, and prior work has addressed automatically identifying predominant word senses (McCarthy et al., 2007; Lau et al., 2014) .",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_19",
  "x": "---------------------------------- **UNSUPERVISED RESULTS** The k-means clustering for the unsupervised approach is repeated 100 times with randomlyselected initial centroids, for several values of k. The average accuracy and standard deviation of the unsupervised approach over these 100 runs are shown in the left panel of Table 2 . For k = 4 and 5 on TEST, this approach surpasses the unsupervised CFORM method of <cite>Fazly et al. (2009)</cite> ; however, on DEV this approach does not outperform <cite>Fazly et al</cite>.'s CFORM approach for any of the val-ues of k considered. Analyzing the results on individual expressions indicates that the unsupervised approach gives especially low accuracy for hit roof -which is in DEV-as compared to the CFORM method of <cite>Fazly et al.</cite>, which could contribute to the overall lower accuracy of the unsupervised approach on this dataset. We now consider the upperbound of an unsupervised approach that selects a single label for each cluster of usages. In the right panel of Table 2 we show results for an oracle approach that always selects the best label for each cluster.",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_20",
  "x": "In this case, as the number of clusters increases, so too will the accuracy. 9 Nevertheless, these results show that, even for relatively small values of k, there is scope for improving the proposed unsupervised method through improved methods for selecting the label for each cluster, and that the performance of such a method could potentially come close to that of the supervised approach. A word's predominant sense is known to be a powerful baseline in word-sense disambiguation, and prior work has addressed automatically identifying predominant word senses (McCarthy et al., 2007; Lau et al., 2014) . The findings here suggest that methods for determining whether a set of usages of a VNIC are predominantly literal or idiomatic could be leveraged to give further improvements in unsupervised VNIC identification. ---------------------------------- **CONCLUSIONS** In this paper we proposed supervised and unsupervised approaches, based on word embeddings, to identifying token instances of VNICs that performed better than the supervised approach, and unsupervised CFORM approach, of <cite>Fazly et al. (2009)</cite> , respectively.",
  "y": "differences"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_0",
  "x": "Researchers have found a variety of ways in which dangerous unintended bias can show up in NLP applications (Blodgett and O'Connor, 2017; Hovy and Spruit, 2016; Tatman, 2017) . Mitigating such biases is a difficult problem, and researchers have created many ways to make fairer NLP applications. Much of the focus for mitigating unintended bias in NLP is either targeted at reducing gender stereotypes in text (Bolukbasi et al., 2016b,a; Zhao et al., 2017; Zhang et al., 2018) , or inequality of sentiment or toxicity for various protected groups <cite>(Caliskan-Islam et al., 2016</cite>; Bakarov, 2018; Dixon et al.; Garg et al., 2018; Kiritchenko and Mohammad, 2018) . More specifically, word embeddings has been an area of focus for evaluating unintended bias. (Bolukbasi et al., 2016b ) defines a useful metric for identifying gender bias and<cite> (Caliskan-Islam et al., 2016)</cite> defines a metric called the WEAT score for evaluating unfair correlations with sentiment for various demographics in text. Unfortunately metrics like these leverage vector space arguments between only two identities at a time like man vs woman (Bolukbasi et al., 2016a) , or European American names vs. African American names<cite> (Caliskan-Islam et al., 2016)</cite> . Though geometrically intuitive, these tests do not have a direct relation to discrimination in general.",
  "y": "background"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_1",
  "x": "Mitigating such biases is a difficult problem, and researchers have created many ways to make fairer NLP applications. Much of the focus for mitigating unintended bias in NLP is either targeted at reducing gender stereotypes in text (Bolukbasi et al., 2016b,a; Zhao et al., 2017; Zhang et al., 2018) , or inequality of sentiment or toxicity for various protected groups <cite>(Caliskan-Islam et al., 2016</cite>; Bakarov, 2018; Dixon et al.; Garg et al., 2018; Kiritchenko and Mohammad, 2018) . More specifically, word embeddings has been an area of focus for evaluating unintended bias. (Bolukbasi et al., 2016b ) defines a useful metric for identifying gender bias and<cite> (Caliskan-Islam et al., 2016)</cite> defines a metric called the WEAT score for evaluating unfair correlations with sentiment for various demographics in text. Unfortunately metrics like these leverage vector space arguments between only two identities at a time like man vs woman (Bolukbasi et al., 2016a) , or European American names vs. African American names<cite> (Caliskan-Islam et al., 2016)</cite> . Though geometrically intuitive, these tests do not have a direct relation to discrimination in general. Our framework and RNSB metric enable a clear evaluation of discrimination with respect to word embedding bias for a whole class of demographics.",
  "y": "background"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_2",
  "x": "(Bolukbasi et al., 2016b ) defines a useful metric for identifying gender bias and<cite> (Caliskan-Islam et al., 2016)</cite> defines a metric called the WEAT score for evaluating unfair correlations with sentiment for various demographics in text. Unfortunately metrics like these leverage vector space arguments between only two identities at a time like man vs woman (Bolukbasi et al., 2016a) , or European American names vs. African American names<cite> (Caliskan-Islam et al., 2016)</cite> . Though geometrically intuitive, these tests do not have a direct relation to discrimination in general. Our framework and RNSB metric enable a clear evaluation of discrimination with respect to word embedding bias for a whole class of demographics. ---------------------------------- **METHODS** We present our framework for understanding and evaluating unintentional demographic bias in word embeddings.",
  "y": "background"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_3",
  "x": "(Bolukbasi et al., 2016b ) defines a useful metric for identifying gender bias and<cite> (Caliskan-Islam et al., 2016)</cite> defines a metric called the WEAT score for evaluating unfair correlations with sentiment for various demographics in text. Unfortunately metrics like these leverage vector space arguments between only two identities at a time like man vs woman (Bolukbasi et al., 2016a) , or European American names vs. African American names<cite> (Caliskan-Islam et al., 2016)</cite> . Though geometrically intuitive, these tests do not have a direct relation to discrimination in general. Our framework and RNSB metric enable a clear evaluation of discrimination with respect to word embedding bias for a whole class of demographics. ---------------------------------- **METHODS** We present our framework for understanding and evaluating unintentional demographic bias in word embeddings.",
  "y": "differences"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_4",
  "x": "After training, we take a set of neutral identity terms from a protected group (i.e. national origin) and predict the probability of negative sentiment for each word in the set. Neutral identity terms that are unfairly entangled with negative sentiment in the word embeddings will be classified like their neighboring sentiment words from the sentiment dataset. We leverage this set of negative sentiment probabilities to summarize unintended demographic bias using RNSB. ---------------------------------- **MODELS AND DATA** We evaluate three pretrained embedding models: GloVe (Pennington et al., 2014) , Word2vec (Mikolov et al., 2013 ) (trained on the large Google News corpus), and ConceptNet . GloVe and Word2vec embeddings have been shown to contain unintended bias in (Bolukbasi et al., 2016a;<cite> Caliskan-Islam et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_5",
  "x": "So the more fair a word embedding model with respect to sentiment bias, the lower the RNSB metric. ---------------------------------- **RESULTS AND DISCUSSION** We evaluate our framework and metric on two cases studies: National Origin Discrimination and Religious Discrimination. For each case study, we create a set of the most frequent identity terms from the protected groups in the Wikipedia word corpus and analyze bias with respect to these terms via our framework. First, we compare the RNSB metric for 3 pretrained word embeddings, showing that our metric is consistent with other word embedding analysis like WEAT<cite> (Caliskan-Islam et al., 2016)</cite> . We then show that our framework enables an insightful view into word embedding bias.",
  "y": "similarities"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_6",
  "x": "We vary the word embeddings used in our framework and calculate the RNSB metric for each embedding. The results are displayed in Table 1 . For both case studies, the bias is largest in GloVe, as shown by the largest RNSB metric. As mentioned earlier, ConceptNet is a state of the art model that mixes models like GloVe and Word2vec, creating fairer word embeddings. Through the RNSB metric, one can see that the unintended demographic bias of these word embeddings is an order of magnitude lower than GloVe or Word2vec. Although the RNSB metric is not directly comparable to WEAT scores, these results are still consistent with some of the bias predicted by<cite> (Caliskan-Islam et al., 2016)</cite> . The WEAT score shows that word embeddings like Word2vec and GloVe are biased with respect to national origin because European-American names are more correlated with positive sentiment than AfricanAmerican names.",
  "y": "similarities"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_0",
  "x": "Phrase-based statistical machine translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004; Koehn, 2004; Koehn et al., 2007) have achieved significant improvements in translation accuracy over the original IBM word-based model. However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk et al., 2005;<cite> Liu et al., 2007</cite>; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea, 2003; Galley et al., 2004; Bod, 2007) . The basic motivation behind syntax-based model is that the syntax information has the potential to model the structure reordering and discontiguous corresponding by the intrinsic structural generalization ability. Although remarkable progresses have been reported, the strict syntactic constraint (the both sides of the rules should strictly be a subtree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents. To alleviate this constraint, a few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks.",
  "y": "motivation background"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_1",
  "x": "To alleviate this constraint, a few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks. For example, forest-to-string transformation rules have been integrated into the tree-to-string translation framework by (Liu et al., 2006;<cite> Liu et al., 2007)</cite> . Zhang et al. (2008a) made it possible to utilize the non-syntactic rules and even the phrases which are used in phrase based model by advancing a general tree sequence to tree sequence framework based on the tree-to-tree model presented in (Zhang et al., 2007) . In these models, various kinds of rules can be employed. For example, as shown in Figure 1 and Figure 2 , Figure 1 shows a Chinese-to-English sentence pair with syntax parses on both sides and the word alignments (dotted lines). Figure 2 lists some of the rules which can be extracted from the sentence pair in Figure 1 by the system used in (Zhang et al., 2008a) . These rules includes not only conventional syntax rules but also the tree sequence rules (the multi-headed syntax rules ).",
  "y": "background"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_2",
  "x": "However, one of the precondition for the investigations of these issues is what are the \"rule categories\"? In other words, some comprehensive rule classifications are necessary to make the rule analyses feasible. The motivation of this paper is to present such a rule classification. ---------------------------------- **RELATED WORKS** A few researches have made some exploratory investigations towards the effects of different rules by classifying the translation rules into different subcategories<cite> (Liu et al., 2007</cite>; Zhang et al., 2008a; DeNeefe et al., 2007) . <cite>Liu et al. (2007)</cite> differentiated the rules in their tree-to-string model which integrated with forest 1 -to-string into fully lexicalized rules, non-lexicalized rules and partial lexicalized rules according to the lexicalization levels.",
  "y": "background"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_3",
  "x": "A few researches have made some exploratory investigations towards the effects of different rules by classifying the translation rules into different subcategories<cite> (Liu et al., 2007</cite>; Zhang et al., 2008a; DeNeefe et al., 2007) . <cite>Liu et al. (2007)</cite> differentiated the rules in their tree-to-string model which integrated with forest 1 -to-string into fully lexicalized rules, non-lexicalized rules and partial lexicalized rules according to the lexicalization levels. As an extension, Zhang et al. (2008a) proposed two more categories: Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR). The SRR stands for the rules which have at least two non-terminal leaf nodes with inverted order in the source and target side. And DPR refers to the rules having at least one non-terminal leaf node between two terminal leaf nodes. (DeNeefe et al., 2007) made an illuminating breakdown of the different kinds of rules. Firstly, they classify all the GHKM 2 rules (Galley et al., 2004; Galley et al., 2006) into two categories: lexical rules and non-lexical rules.",
  "y": "motivation background"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_4",
  "x": "---------------------------------- **RULES CLASSIFICATIONS** Currently, there have been several classifications in SMT research community. Generally, the rules can be classified into two main groups according to whether syntax information is involved: bilingual phrases (Phrase) and syntax rules (Syntax). Further, the syntax rules can be divided into three categories according to the lexicalization levels<cite> (Liu et al., 2007</cite>; Zhang et al., 2008a source and target sides are non-lexicons (nonterminals) 3) Partially lexicalized (PLex): otherwise. In Figure 2 , R 1 -R 3 are FLex rules, and R 5 -R 8 are PLex rules.",
  "y": "background"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_5",
  "x": "Based on this classification system, the rules are classified according to different standards, such as lexicalization level and generalization. Especially, we refresh the concepts of the structure reordering rules and the discontiguous phrase rules. This novel classification system may supports the SMT research community with some helpful references. In the future works, aiming to analyze the rule contributions and the redundances issues using the presented rule classification based on some real translation systems, we plan to implement some synchronous grammar based syntax translation models such as the one presented in<cite> (Liu et al., 2007)</cite> or in (Zhang et al., 2008a) . Taking such a system as the experimental platform, we can perform comprehensive statistics about distributions of different rule categories. What is more important, the contribution of each rule category can be evaluated seriatim. Furthermore, which kinds of rules are preferentially applied in the 1-best decoding can be studied.",
  "y": "future_work"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_0",
  "x": "This paper considers the reading comprehension task in which some discrete-reasoning abilities are needed to correctly answer questions. Specifically, we focus on a new reading comprehension dataset called DROP <cite>(Dua et al., 2019)</cite> , which requires Discrete Reasoning Over the content of Paragraphs to obtain the final answer. Unlike previous benchmarks such as CNN/DM (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016) that have been well solved Devlin et al., 2019) , DROP is substantially more challenging in three ways. First, the answers to 1 https://github.com/huminghao16/MTMSN the questions involve a wide range of types such as numbers, dates, or text strings. Therefore, various kinds of prediction strategies are required to successfully find the answers. Second, rather than restricting the answer to be a span of text, DROP loosens the constraint so that answers may be a set of multiple text strings. Third, for questions that require discrete reasoning, a system must have a more comprehensive understanding of the context and be able to perform numerical operations such as addition, counting, or sorting.",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_1",
  "x": "Existing approaches, when applied to this more realistic scenario, have three problems. First, to produce various answer types,<cite> Dua et al. (2019)</cite> extend previous one-type answer prediction (Seo et al., 2017) to multi-type prediction that supports span extraction, counting, and addition/subtraction. However, they have not fully considered all potential types. Take the question \"What percent are not non-families?\" and the passage snippet \"39.9% were non-families\" as an example, a negation operation is required to infer the answer. Second, previous reading comprehension models (Wang et al., 2017; Yu et al., 2018; Hu et al., 2018) are designed to produce one single span as the answer. But for some questions such as \"Which ancestral groups are smaller than 11%?\", there may exist several spans as correct answers (e.g., \"Italian\", \"English\", and \"Polish\"), which can not be well handled by these works. Third, to support numerical reasoning, prior work <cite>(Dua et al., 2019)</cite> learns to predict signed numbers for obtaining an arithmetic expression that can be executed by a symbolic system. Nevertheless, the prediction of each signed number is isolated, and the expression's context information has not been considered.",
  "y": "background"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_2",
  "x": "Therefore, various kinds of prediction strategies are required to successfully find the answers. Second, rather than restricting the answer to be a span of text, DROP loosens the constraint so that answers may be a set of multiple text strings. Third, for questions that require discrete reasoning, a system must have a more comprehensive understanding of the context and be able to perform numerical operations such as addition, counting, or sorting. Existing approaches, when applied to this more realistic scenario, have three problems. First, to produce various answer types,<cite> Dua et al. (2019)</cite> extend previous one-type answer prediction (Seo et al., 2017) to multi-type prediction that supports span extraction, counting, and addition/subtraction. However, they have not fully considered all potential types. Take the question \"What percent are not non-families?\" and the passage snippet \"39.9% were non-families\" as an example, a negation operation is required to infer the answer.",
  "y": "motivation"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_3",
  "x": "Take the question \"What percent are not non-families?\" and the passage snippet \"39.9% were non-families\" as an example, a negation operation is required to infer the answer. Second, previous reading comprehension models (Wang et al., 2017; Yu et al., 2018; Hu et al., 2018) are designed to produce one single span as the answer. But for some questions such as \"Which ancestral groups are smaller than 11%?\", there may exist several spans as correct answers (e.g., \"Italian\", \"English\", and \"Polish\"), which can not be well handled by these works. Third, to support numerical reasoning, prior work <cite>(Dua et al., 2019)</cite> learns to predict signed numbers for obtaining an arithmetic expression that can be executed by a symbolic system. Nevertheless, the prediction of each signed number is isolated, and the expression's context information has not been considered. As a result, obviously-wrong expressions, such as all predicted signs are either minus or zero, are likely produced. To address the above issues, we introduce the Multi-Type Multi-Span Network (MTMSN), a neural reading comprehension model for predicting various types of answers as well as dynamically extracting one or multiple spans. MTMSN utilizes a series of pre-trained Transformer blocks (Devlin et al., 2019) to obtain a deep bidirectional context representation.",
  "y": "background"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_4",
  "x": "Existing approaches, when applied to this more realistic scenario, have three problems. Third, to support numerical reasoning, prior work <cite>(Dua et al., 2019)</cite> learns to predict signed numbers for obtaining an arithmetic expression that can be executed by a symbolic system. Nevertheless, the prediction of each signed number is isolated, and the expression's context information has not been considered.",
  "y": "motivation"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_5",
  "x": "Based on the representations, we employ a multi-type answer predictor that is able to produce four answer types: (1) span from the text; (2) arithmetic expression; (3) count number; (4) negation on numbers ( \u00a73.2). Following<cite> Dua et al. (2019)</cite> , we first predict the answer type of a given passage-question pair, and then adopt individual prediction strategies. To support multispan extraction ( \u00a73.3), the model explicitly predicts the number of answer spans. It then outputs non-overlapped spans until the specific amount is reached. Moreover, we do not directly use the arithmetic expression that possesses the maximum probability, but instead re-rank several expression candidates that are decoded by beam search to further confirm the prediction ( \u00a73.4). Finally, the model is trained under weakly-supervised signals to maximize the marginal likelihood over all possible annotations ( \u00a73.5). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_6",
  "x": "**MULTI-TYPE ANSWER PREDICTOR** Rather than restricting the answer to always be a span of text, the discrete-reasoning reading comprehension task involves different answer types (e.g., number, date, span of text). Following<cite> Dua et al. (2019)</cite> , we design a multi-type answer predictor to selectively produce different kinds of answers such as span, count number, and arithmetic expression. To further increase answer coverage, we propose adding a new answer type to support logical negation. Moreover, unlike prior work that separately predicts passage spans and question spans, our approach directly extracts spans from the input sequence. Answer type prediction Inspired by the Augmented QANet model <cite>(Dua et al., 2019)</cite> , we use the contextualized token representations from the last four blocks (H L\u22123 , ..., H L ) as the inputs to our answer predictor, which are denoted as M 0 , M 1 , M 2 , M 3 , respectively. To predict the answer type, we first split the representation M 2 into a question representation Q 2 and a passage representation P 2 according to the index of intermediate [SEP] token.",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_7",
  "x": "Moreover, unlike prior work that separately predicts passage spans and question spans, our approach directly extracts spans from the input sequence. Answer type prediction Inspired by the Augmented QANet model <cite>(Dua et al., 2019)</cite> , we use the contextualized token representations from the last four blocks (H L\u22123 , ..., H L ) as the inputs to our answer predictor, which are denoted as M 0 , M 1 , M 2 , M 3 , respectively. To predict the answer type, we first split the representation M 2 into a question representation Q 2 and a passage representation P 2 according to the index of intermediate [SEP] token. Then the model computes two vectors h Q 2 and h P 2 that summarize the question and passage information respectively: where h P 2 is computed in a similar way over P 2 . Next, we calculate a probability distribution to represent the choices of different answer types as: Here, h CLS is the first vector in the final contextualized representation M 3 , and FFN denotes a feed-forward network consisting of two linear projections with a GeLU activation (Hendrycks and Gimpel, 2016) followed by a layer normalization (Lei Ba et al., 2016) in between.",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_8",
  "x": "Span To extract the answer either from the passage or from the question, we combine the gating mechanism of Wang et al. (2017) with the standard decoding strategy of Seo et al. (2017) to predict the starting and ending positions across the entire sequence. Specifically, we first compute three vectors, namely g Q 0 , g Q 1 , g Q 2 , that summarize the question information among different levels of question representations: where g Q 0 and g Q 1 are computed over Q 0 and Q 1 respectively, in a similar way as described above. Then we compute the probabilities of the starting and ending indices of the answer span from the input sequence as: where \u2297 denotes the outer product between the vector g and each token representation in M. Arithmetic expression In order to model the process of performing addition or subtraction among multiple numbers mentioned in the passage, we assign a three-way categorical variable (plus, minus, or zero) for each number to indicate its sign, similar to<cite> Dua et al. (2019)</cite> . As a result, an arithmetic expression that has a number as the final answer can be obtained and easily evaluated.",
  "y": "similarities"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_9",
  "x": "Since each expression consists of several signed numbers, we construct an expression representation by taking both the numbers and the signs into account. For each number in the expression, we gather its corresponding vector from the representation U. As for the signs, we initialize an embedding matrix E \u2208 R 3\u00d72 * D , and find the sign embeddings for each signed number. In this way, given the i-th expression that contains M signed numbers at most, we can obtain number vectors V i \u2208 R M \u00d72 * D as well as sign embeddings C i \u2208 R M \u00d72 * D . Then the expression representation along with the reranking probability can be calculated as: ---------------------------------- **TRAINING AND INFERENCE** Since DROP does not indicate the answer type but only provides the answer string, we therefore adopt the weakly supervised annotation scheme, as suggested in Berant et al. (2013);<cite> Dua et al. (2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_10",
  "x": "Dataset We consider the reading comprehension benchmark that requires Discrete Reasoning Over Paragraphs (DROP) <cite>(Dua et al., 2019)</cite> prehensive understanding of the context as well as the ability of numerical reasoning are required. Model settings We build our model upon two publicly available uncased versions of BERT: BERT BASE and BERT LARGE 2 , and refer readers to Devlin et al. (2019) for details on model sizes. We use Adam optimizer with a learning rate of 3e-5 and warmup over the first 5% steps to train. The maximum number of epochs is set to 10 for base models and 5 for large models, while the batch size is 12 or 24 respectively. A dropout probability of 0.1 is used unless stated otherwise. The number of counting class is set to 10, and the maximum number of spans is 8. The beam size is 3 by default, while the maximum amount of signed numbers M is set to 4.",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_11",
  "x": "Baselines Following the implementation of Augmented QANet (NAQANet) <cite>(Dua et al., 2019)</cite> , we introduce a similar baseline called Augmented BERT (NABERT). The main difference is that we replace the encoder of QANet (Yu et al., 2018) with the pre-trained Transformer blocks (Devlin et al., 2019) . Moreover, it also supports the prediction of various answer types such as span, arithmetic expression, and count number. ---------------------------------- **MAIN RESULTS** Two metrics, namely Exact Match (EM) and F1 score, are utilized to evaluate models. We use the official script to compute these scores.",
  "y": "similarities"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_12",
  "x": "See Appendix for some examples of the above error cases. ---------------------------------- **RELATED WORK** Reading comprehension benchmarks Promising advancements have been made for reading comprehension due to the creation of many large datasets. While early research used cloze-style tests (Hermann et al., 2015; Hill et al., 2016) , most of recent works (Rajpurkar et al., 2016; Joshi et al., 2017) are designed to extract answers from the passage. Despite their success, these datasets only require shallow pattern matching and simple logical reasoning, thus being well solved Devlin et al., 2019) . Recently,<cite> Dua et al. (2019)</cite> released a new benchmark named DROP that demands discrete reasoning as well as deeper paragraph understanding to find the answers.",
  "y": "background"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_13",
  "x": "Reading comprehension benchmarks Promising advancements have been made for reading comprehension due to the creation of many large datasets. While early research used cloze-style tests (Hermann et al., 2015; Hill et al., 2016) , most of recent works (Rajpurkar et al., 2016; Joshi et al., 2017) are designed to extract answers from the passage. Despite their success, these datasets only require shallow pattern matching and simple logical reasoning, thus being well solved Devlin et al., 2019) . Recently,<cite> Dua et al. (2019)</cite> released a new benchmark named DROP that demands discrete reasoning as well as deeper paragraph understanding to find the answers. Saxton et al. (2019) introduced a dataset consisting of different types of mathematics problems to focuses on mathematical computation. We choose to work on DROP to test both the numerical reasoning and linguistic comprehension abilities. Neural reading models Previous neural reading models, such as BiDAF (Seo et al., 2017) , R-Net (Wang et al., 2017) , QANet (Yu et al., 2018) , Reinforced Mreader (Hu et al., 2018) , are usually designed to extract a continuous span of text as the answer.",
  "y": "uses"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_0",
  "x": "Regular expressions are an efficient tool to represent structured data with specific rules in a variety of fields such as natural language processing or text classification. However, it is not always an easy task to write an exact regular expression for those who do not have a deep knowledge of regular expressions or when the expression is very complicated, and incorrect or sloppy regular expressions may cause unexpected consequences in practice (Bispo et al., 2006; Zhang et al., 2018) . Indeed, even a single character difference between regular expressions can cause them * Now at Google. to represent completely different sets of strings. As such, researchers have begun working on a system than generates a regular expression from a natural language description provided by a human while reducing possible errors caused by incorrect regular expressions (Liu et al., 2019) . Recently, <cite>Locascio et al. (2016)</cite> designed the Deep-Regex model based on the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014) using minimal domain knowledge during the learning phase while still accurately predicting regular expressions from NLs. Later, Zhong et al. (2018a) improved the performance by training on not only syntactic content of the expressions (i.e. the exact textual representation of the expression that was used), but also the semantic content (the regular language described by the expression). However, the reward function in the SemRegex model (Zhong et al., 2018a) that determines if the predicted regular expression is semantically equivalent to the ground truth expression is known to be PSPACE-complete and is a bottleneck in practice (Stockmeyer and Meyer, 1973) .",
  "y": "background"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_1",
  "x": "---------------------------------- **RELATED WORK** Generating Regular Expressions: Ranta (1998) studied rule-based techniques for the conversion between multi-languages and regular expressions. Kushman and Barzilay (2013) built a parsing model that translates a natural language sentence to a regular expression, and provided a dataset which is now a popular benchmark dataset for related research. <cite>Locascio et al. (2016)</cite> proposed the Deep-Regex model based on Seq2Seq for generating regular expressions from natural language descriptions together with a dataset of 10,000 NL-RX pairs. However, due to the limitations of the standard Seq2Seq model, the Deep-Regex model can only generate regular expressions similar in shape to the training data. The SemRegex model improved the Deep-Regex model by using reinforcement learning based on the determinisitic finite automata (DFA) equivalence oracle (which determines if two regular expressions describe the same language) as a reward function.",
  "y": "background"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_2",
  "x": "**EXPERIMENTAL SETUP** We run our experiments on a computer with the following specifications: AMD Ryzen 7 1700 8core with 64GB RAM and a GTX 1080Ti GPU on Ubuntu 16.04.1. Our source code is written using PyTorch 0.4.0 (Ketkar, 2017) . ---------------------------------- **EQ REG MODEL** Datasets: <cite>Locascio et al. (2016)</cite> created a set of NL-RX pair data by arbitrarily creating and combining data in a tree form. We define the depth of a regular expression in this dataset as the depth of the tree that generated the NL-RX pair (see Appendix A).",
  "y": "background"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_3",
  "x": "We define the depth of a regular expression in this dataset as the depth of the tree that generated the NL-RX pair (see Appendix A). Similar to <cite>Locascio et al. (2016)</cite> , we randomly generate regular expression pairs up to depth three and label the equivalence between each pair. We sample approximately 200,000 pairs using this method with a ratio of equivalent and non-equivalent pairs of about 2:1. We prepare three sets of data having different depths for test data. One set is made up of only regular expressions with depth at most 3, which is the same as the training data (10,000 pairs). The other two have depths 4 and 5, respectively, and each contain 1,000 pairs of regular expressions of which half are equivalent. Model Settings: We set the two LSTMs in our model to not have shared parameters as we found this gives better performance.",
  "y": "similarities"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_4",
  "x": "We set the batch size to 256 and the learning rate to 0.1 and use a stochastic gradient descent optimizer (Bottou, 2010) . ---------------------------------- **SOFTREGEX MODEL** Datasets: We use three public datasets to compare SoftRegex with the-state-of-the-art model, Sem-Regex. The KB13 (Kushman and Barzilay, 2013) dataset was constructed by regex experts and is relatively small. On the other hand, NL-RX-Synth is data generated automatically and NL-RX-Turk is made from ordinary people by paraphrasing NL descriptions in NL-RX-Synth using Mechanical Turk<cite> (Locascio et al., 2016)</cite> . Both datasets have 10,000 pairs of NL-RX data.",
  "y": "background"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_5",
  "x": "Figure 3 shows the average training time per epoch over 30 epochs for each of the three models (SoftRegex and both SemRegex variants) and datasets. The training time with EQ Reg vastly outperforms the training time for SemRegex. In the worst case (the NL-RX-Synth dataset), we see that our new method is still 3.6 times faster than that of SemRegex. Though the speedup described in experimental result may appear constant, our softened equivalence approximately decides a PSPACE-complete problem in linear time to the length of regular expressions, which would otherwise take exponential time. Zhong et al. (2018b) pointed out some problems in the NL-RX datasets. Specifically, there are some ambiguities since <cite>Locascio et al. (2016)</cite> tried to obtain data from machine-generated sentences. Thus, there are situations that even expert humans cannot accurately classify.",
  "y": "background"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_6",
  "x": "Our SoftRegex model with EQ Reg as a reward function substantially speeds up the training phase (at least 3.6 times faster than SemRegex) while having similar or better performance on a series of standard benchmarks. ---------------------------------- **A SUPPLEMENTAL MATERIAL** We now describe how <cite>Locascio et al. (2016)</cite> generated their synthetic regular expression data. To begin, they manually mapped primitive regular expression operations, such as union and concatenation, to natural language. They then defined a small alphabet on which the operations could be performed. In the end, their system has 15 operations and 6 types of characters in the vocabulary.",
  "y": "background"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_0",
  "x": "achieves state-of-the-art on RACE (Lai et al., 2017) with a Transformer-XL based model . The key to success in the performance of many of these models is their ability to train on extremely large datasets. BERT <cite>(Devlin et al., 2018)</cite> , for example, trains on the BooksCorpus (Zhu et al., 2015) and English Wikipedia, for a combined 3,200M words. Other iterations increased the amount of knowledge used during pre-training, such as RoBERTa (Liu et al., 2019) . Training large-scale models on these massive datasets has drawbacks, such as significantly increased carbon pollution and harm to the environment (Schwartz et al., 2019; Strubell et al., 2019) . We present a methodology of combining queries from commonsense knowledge bases with contextual embeddings, BIG MOOD -BERT Infused Graphs: Matching Over Other embeDdings, and abbreviated for its relationship to human knowledge awareness. Our methodology achieves a increase without significant additional fine-tuning or pre-training.",
  "y": "background"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_1",
  "x": "We present our model for this shared task. Our model has three major components: language model adaptation, knowledge graph embeddings, and attention for classification. ---------------------------------- **DATA PREPROCESSING** Before model usage, we preprocess the data in two ways to make it easier for the model to un-derstand. For language modeling, we create training data similar to those in BERT <cite>(Devlin et al., 2018)</cite> . For knowledge graph use, we preprocess language to create commonsense object and relationship vocabulary and to match as many related commonsense objects as possible.",
  "y": "similarities"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_2",
  "x": "For knowledge graph use, we preprocess language to create commonsense object and relationship vocabulary and to match as many related commonsense objects as possible. ---------------------------------- **LANGUAGE MODEL PREPROCESSING** We prepossess each passage for training. We use this process for each training epoch, since it allows for the most dense pretraining framework. Commonly known as a cloze task,<cite> Devlin et al. (2018)</cite> introduced a framework that pretrained transformers (Vaswani et al., 2017) based on masked token prediction. First, we preprocess the tokens with WordPiece embeddings (Wu et al., 2016) .",
  "y": "background"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_3",
  "x": "Then, we append special [CLS] and [SEP ] to each datum. We append [CLS] to the beginning of each datum, and [SEP ] to separate the question with the answer, as such: Then, we randomly mask 15% of all WordPiece embeddings. Unlike<cite> Devlin et al. (2018)</cite> , we run the randomization script once per each training epoch. Otherwise, we follow the procedure in<cite> Devlin et al. (2018)</cite> . 80% of the time, we replace the word with the [M ASK] prediction, to be replaced through cloze task prediction. 10% of the time, we replace the word with a random word.",
  "y": "differences"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_4",
  "x": "Commonly known as a cloze task,<cite> Devlin et al. (2018)</cite> introduced a framework that pretrained transformers (Vaswani et al., 2017) based on masked token prediction. First, we preprocess the tokens with WordPiece embeddings (Wu et al., 2016) . Then, we append special [CLS] and [SEP ] to each datum. We append [CLS] to the beginning of each datum, and [SEP ] to separate the question with the answer, as such: Then, we randomly mask 15% of all WordPiece embeddings. Unlike<cite> Devlin et al. (2018)</cite> , we run the randomization script once per each training epoch. Otherwise, we follow the procedure in<cite> Devlin et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_5",
  "x": "80% of the time, we replace the word with the [M ASK] prediction, to be replaced through cloze task prediction. 10% of the time, we replace the word with a random word. 10% of the time, we keep the word unchanged. Combined with the above cloze task, we process the data for next sentence prediction. We do this process after the cloze task masking, similar to<cite> Devlin et al. (2018)</cite> . For each datum, we randomly pick either a sentence labeled correctly as the next sentence 50% of the time, or a random sentence 50% of the time. We ensure that the random sentence is not the next sentence.",
  "y": "similarities"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_6",
  "x": "---------------------------------- **LANGUAGE MODEL FINE-TUNING** Contrary to<cite> Devlin et al. (2018)</cite> , we do language model fine-tuning in addition to classification finetuning. We find that this generally provides better results, and allows for more stable accuracy since the shared task involves a small dataset. For each prompt, we use the previous preprocessed data to create tasks for our model to predict. We do this before token realignment, so this happens before any extra knowledge graph embeddings are added to the model architecture. For masked tokens, we predict that token through bidirectional context, the same as<cite> Devlin et al. (2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_7",
  "x": "**LANGUAGE MODEL FINE-TUNING** Contrary to<cite> Devlin et al. (2018)</cite> , we do language model fine-tuning in addition to classification finetuning. We find that this generally provides better results, and allows for more stable accuracy since the shared task involves a small dataset. For each prompt, we use the previous preprocessed data to create tasks for our model to predict. We do this before token realignment, so this happens before any extra knowledge graph embeddings are added to the model architecture. For masked tokens, we predict that token through bidirectional context, the same as<cite> Devlin et al. (2018)</cite> . For next sentence prediction, we use the unbiased method previously introduced as well as in<cite> Devlin et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_8",
  "x": "For each prompt, we use the previous preprocessed data to create tasks for our model to predict. We do this before token realignment, so this happens before any extra knowledge graph embeddings are added to the model architecture. For masked tokens, we predict that token through bidirectional context, the same as<cite> Devlin et al. (2018)</cite> . For next sentence prediction, we use the unbiased method previously introduced as well as in<cite> Devlin et al. (2018)</cite> . ---------------------------------- **TOKEN REALIGNMENT** We do a word-level fusion to incorporate knowledge embeddings into the BERT model.",
  "y": "uses"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_9",
  "x": "We also allow the knowledge embeddings to be modified through this back-propagation. Hyperparameters are noted in Section 4.1. We also ablate our use of this extra attention layer, showing that it is important to learn comparisons between knowledge embeddings. For BERT baselines, we use the process in<cite> Devlin et al. (2018)</cite> , and use the [CLS] token, without attention, for classification. ---------------------------------- **ANALYSIS** ----------------------------------",
  "y": "uses"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_10",
  "x": "Commonly known as a cloze task,<cite> Devlin et al. (2018)</cite> introduced a framework that pretrained transformers (Vaswani et al., 2017) based on masked token prediction. Unlike<cite> Devlin et al. (2018)</cite> , we run the randomization script once per each training epoch. Otherwise, we follow the procedure in<cite> Devlin et al. (2018)</cite> .",
  "y": "extends"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_0",
  "x": "An entity is an object or a set of objects in the real world, while a mention is a textual reference to an entity 1 . Most of the previous coreference resolution methods have similar classification phases, implemented either as decision trees (Soon et al., 2001) or as maximum entropy classifiers<cite> (Luo et al., 2004)</cite> . Moreover, these methods employ similar feature sets. The clusterization phase is different across current approaches. For example, there are several linking decisions for clusterization. (Soon et al., 2001 ) advocate the link-first decision, which links a mention to its closest candidate referent, while (Ng and Cardie, 2002) consider instead the link-best decision, which links a mention to its most confident candidate referent. Both these clustering decisions are locally optimized.",
  "y": "background"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_1",
  "x": "(Soon et al., 2001 ) advocate the link-first decision, which links a mention to its closest candidate referent, while (Ng and Cardie, 2002) consider instead the link-best decision, which links a mention to its most confident candidate referent. Both these clustering decisions are locally optimized. In contrast, globally optimized clustering decisions were reported in<cite> (Luo et al., 2004)</cite> and (DaumeIII and Marcu, 2005a) , where all clustering possibilities are considered by searching on a Bell tree representation or by using the Learning as Search Optimization (LaSO) framework (DaumeIII and Marcu, 2005b) respectively, but the first search is partial and driven by heuristics and the second one only looks back in text. We argue that a more adequate clusterization phase for coreference resolution can be obtained by using a graph representation. In this paper we describe a novel representation of the coreference space as an undirected edge-weighted graph in which the nodes represent all the mentions from a text, whereas the edges between nodes constitute the confidence values derived from the coreference classification phase. In order to detect the entities referred in the text, we need to partition the graph such that all nodes in each subgraph refer to the same entity. We have devised a graph partitioning method for coreference resolution, called BESTCUT, which is inspired from the well-known graph-partitioning algorithm Min-Cut (Stoer and Wagner, 1994) .",
  "y": "motivation background"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_2",
  "x": "We have devised a graph partitioning method for coreference resolution, called BESTCUT, which is inspired from the well-known graph-partitioning algorithm Min-Cut (Stoer and Wagner, 1994) . BESTCUT has a different way of computing the cut weight than Min-Cut and a different way of stopping the cut 2 . Moreover, we have slightly modified the Min-Cut procedures. BESTCUT replaces the bottom-up search in a tree representation (as it was performed in<cite> (Luo et al., 2004)</cite> ) with the top-down problem of obtaining the best partitioning of a graph. We start by assuming that all mentions refer to a single entity; the graph cut splits the mentions into subgraphs and the split-ting continues until each subgraph corresponds to one of the entities. The cut stopping decision has been implemented as an SVM-based classification (Cortes and Vapnik, 1995) . The classification and clusterization phases assume that all mentions are detected.",
  "y": "background extends differences"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_4",
  "x": "Based on the data seen, a maximum entropy model (Berger et al., 1996) offers an expression (1) for the probability that there exists coreference C between a mention m i and a mention m j . where g k (m i , m j , C) is a feature and \u03bb k is its weight; Z(m i , m j ) is a normalizing factor. We created the training examples in the same way as<cite> (Luo et al., 2004)</cite> , by pairing all mentions of the same type, obtaining their feature vectors and taking the outcome (coreferent/noncoreferent) from the key files. ---------------------------------- **FEATURE REPRESENTATION** We duplicated the statistical model used by<cite> (Luo et al., 2004)</cite> , with three differences. First, no feature combination was used, to prevent long running times on the large amount of ACE data.",
  "y": "uses"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_5",
  "x": "We duplicated the statistical model used by<cite> (Luo et al., 2004)</cite> , with three differences. First, no feature combination was used, to prevent long running times on the large amount of ACE data. Second, through an analysis of the validation data, we implemented seven new features, presented in Table 1. Third, as opposed to<cite> (Luo et al., 2004)</cite> , who represented all numerical features quantized, we translated each numerical feature into a set of binary features that express whether the value is in certain intervals. This transformation was necessary because our maximum entropy tool performs better on binary features. (Luo et al., 2004) 's features were not reproduced here from lack of space; please refer to the relevant paper for details. Each of these initial graphs will be cut repeatedly until the resulting partition is satisfactory.",
  "y": "extends"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_7",
  "x": "Only the mentions that are a part of the five considered classes are treated as anaphoric and clustered, while the UNK mentions are ignored, even if an outside anaphoricity classifier might categorize some of them as anaphoric. ---------------------------------- **EXPERIMENTAL RESULTS** The clusterization algorithms that we implemented to evaluate in comparison with our method are<cite> (Luo et al., 2004)</cite> 's Belltree and Link-Best (best-first clusterization) from (Ng and Cardie, 2002) . The features used were described in section 2.2. We experimented on the ACE Phase 2 (NIST, 2003) and MUC6 (MUC-6, 1995) corpora. Since we aimed to measure the performance of coreference, the metrics used for evaluation are the ECM-F<cite> (Luo et al., 2004)</cite> and the MUC P, R and F scores (Vilain et al., 1995) .",
  "y": "uses"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_8",
  "x": "Only the mentions that are a part of the five considered classes are treated as anaphoric and clustered, while the UNK mentions are ignored, even if an outside anaphoricity classifier might categorize some of them as anaphoric. ---------------------------------- **EXPERIMENTAL RESULTS** The clusterization algorithms that we implemented to evaluate in comparison with our method are<cite> (Luo et al., 2004)</cite> 's Belltree and Link-Best (best-first clusterization) from (Ng and Cardie, 2002) . The features used were described in section 2.2. We experimented on the ACE Phase 2 (NIST, 2003) and MUC6 (MUC-6, 1995) corpora. Since we aimed to measure the performance of coreference, the metrics used for evaluation are the ECM-F<cite> (Luo et al., 2004)</cite> and the MUC P, R and F scores (Vilain et al., 1995) .",
  "y": "uses"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_9",
  "x": "Despite not being as dramatically affected as BESTCUT, the other two algorithms also decrease in performance with the decrease of the mention information available, which empirically proves that mention detection is a very important module for coreference resolution. Even with an F-score of 77.2% for detecting entity types, our mention detection system boosts the scores of all three algorithms when compared to the case where no information is available. It is apparent that the MUC score does not vary significantly between systems. This only shows that none of them is particularly poor, but it is not a relevant way of comparing methods-the MUC metric has been found too indulgent by researchers (<cite> (Luo et al., 2004)</cite> , (Baldwin et al., 1998) Table 4 : Comparison of results between three clusterization algorithms on ACE Phase 2. The learning algorithms are maxent for coreference and SVM for stopping the cut in BESTCUT. In turn, we obtain the mentions from the key files, detect them with our mention detection algorithm or do not use any information about them. annotation keys and the system output, while the ECM-F metric aligns the detected entities with the key entities so that the number of common mentions is maximized.",
  "y": "background"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_10",
  "x": "Baseline system has the<cite> (Luo et al., 2004)</cite> features. The system was tested on key mentions. ---------------------------------- **MUC SCORE MODEL** From Table 5 we can observe that the lexical features (head-match, type-pair, name-alias) have the most influence on the ECM-F and MUC scores, succeeded by the syntactic features (samegoverning-category, path, coll-comm) . Despite what intuition suggests, the improvement the grammatical feature gn-agree brings to the system is very small. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_12",
  "x": "Despite what intuition suggests, the improvement the grammatical feature gn-agree brings to the system is very small. ---------------------------------- **RELATED WORK** It is of interest to discuss why our implementation of the Belltree system<cite> (Luo et al., 2004</cite> ) is comparable in performance to Link-Best (Ng and Cardie, 2002) . (Luo et al., 2004) do the clusterization through a beam-search in the Bell tree using either a mention-pair or an entity-mention model, the first one performing better in their experiments. Despite the fact that the Bell tree is a complete representation of the search space, the search in it is optimized for size and time, while potentially losing optimal solutions-similarly to a Greedy search. Moreover, the fact that the two implementations are comparable is not inconceivable once we consider that<cite> (Luo et al., 2004</cite> ) never compared their system to another coreference resolver and reported their competitive results on true mentions only.",
  "y": "motivation background"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_0",
  "x": "Another strategy is to employ fast sampling techniques (Ahmed et al., 2012; Ravi, 2013) or incorporate deep learning models with graph learning like (Bui et al., 2017 (Bui et al., , 2018 , which result in large models but have proven to be extremely powerful for complex language understanding tasks like response completion (Pang and Ravi, 2012) and Smart Reply (Kannan et al., 2016) . In this paper, we propose Self-Governing Neural Networks (SGNNs) inspired by projection networks (Ravi, 2017) . SGNNs are on-device deep learning models learned via embedding-free projection operations. We employ a modified version of the locality sensitive hashing (LSH) to reduce input dimension from millions of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art <cite>(Lee and Dernoncourt, 2016</cite>; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017) . The main contributions of the paper are:",
  "y": "similarities"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_1",
  "x": "The main contributions of the paper are: \u2022 Novel Self-Governing Neural Networks (SGNNs) for on-device deep learning for short text classification. \u2022 Compression technique that effectively captures low-dimensional semantic text representation and produces compact models that save on storage and computational cost. \u2022 On the fly computation of projection vectors that eliminate the need for large pre-trained word embeddings or vocabulary pruning. \u2022 Exhaustive experimental evaluation on dialog act datasets, outperforming state-of-theart deep CNN<cite> (Lee and Dernoncourt, 2016)</cite> and RNN variants (Khanpour et al., 2016; Ortega and Vu, 2017 ). ---------------------------------- **SELF-GOVERNING NEURAL NETWORKS**",
  "y": "differences"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_2",
  "x": "We use the train, validation and test splits as defined in <cite>(Lee and Dernoncourt, 2016</cite>; Ortega and Vu, 2017) . ---------------------------------- **EXPERIMENTAL SETUP** We setup our experimental evaluation, as follows: given a classification task and a dataset, we generate an on-device model. The size of the model can be configured (by adjusting the projection matrix P) to fit in the memory footprint of the device, i.e. a phone has more memory compared to a smart watch. For each classification task, we report Accuracy on the test set. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_3",
  "x": "**HYPERPARAMETER AND TRAINING** For both datasets we used the following: 2-layer SGNN (P T =80,d=14 \u00d7 FullyConnected 256 \u00d7 FullyConnected 256 ), mini-batch size of 100, dropout rate of 0.25, learning rate was initialized to 0.025 with cosine annealing decay (Loshchilov and Hutter, 2016) . Unlike prior approaches <cite>(Lee and Dernoncourt, 2016</cite>; Ortega and Vu, 2017 ) that rely on pre-trained word embeddings, we learn the projection weights on the fly during training, i.e word embeddings (or vocabularies) do not need to be stored. Instead, features are computed on the fly and are dynamically compressed via the projection matrices into projection vectors. These values were chosen via a grid search on development sets, we do not perform any other dataset-specific tuning. Training is performed through stochastic gradient descent over shuffled mini-batches with Nesterov momentum optimizer (Sutskever et al., 2013) , run for 1M steps. Tables 2 and 3 show results on the SwDA and MRDA dialog act datasets.",
  "y": "differences"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_4",
  "x": "**RESULTS** ---------------------------------- **BASELINES** We compare our model against a majority class baseline and Naive Bayes classifier<cite> (Lee and Dernoncourt, 2016)</cite> . Our model significantly outperforms both baselines by 12 to 35% absolute. ---------------------------------- **COMPARISON AGAINST STATE-OF-ART METHODS**",
  "y": "uses"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_5",
  "x": "Overall, our SGNN model consistently outperforms the baselines and prior state-of-the-art deep learning models. ---------------------------------- **RESULTS** ---------------------------------- **BASELINES** We compare our model against a majority class baseline and Naive Bayes classifier<cite> (Lee and Dernoncourt, 2016)</cite> . Our model significantly outperforms both baselines by 12 to 35% absolute.",
  "y": "differences"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_6",
  "x": "We compare our model against a majority class baseline and Naive Bayes classifier<cite> (Lee and Dernoncourt, 2016)</cite> . Our model significantly outperforms both baselines by 12 to 35% absolute. ---------------------------------- **COMPARISON AGAINST STATE-OF-ART METHODS** We also compare our performance against prior work using HMMs (Stolcke et al., 2000) and recent deep learning methods like CNN<cite> (Lee and Dernoncourt, 2016)</cite> , RNN (Khanpour et al., 2016) and RNN with gated attention (Tran et al., 2017) . To the best of our knowledge, <cite>(Lee and Dernoncourt, 2016</cite>; Ortega and Vu, 2017; Tran et al., 2017) are the latest approaches in dialog act classification, which also reported on the same data splits. Therefore, we compare our research against these works.",
  "y": "uses"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_7",
  "x": "Our model significantly outperforms both baselines by 12 to 35% absolute. ---------------------------------- **COMPARISON AGAINST STATE-OF-ART METHODS** We also compare our performance against prior work using HMMs (Stolcke et al., 2000) and recent deep learning methods like CNN<cite> (Lee and Dernoncourt, 2016)</cite> , RNN (Khanpour et al., 2016) and RNN with gated attention (Tran et al., 2017) . To the best of our knowledge, <cite>(Lee and Dernoncourt, 2016</cite>; Ortega and Vu, 2017; Tran et al., 2017) are the latest approaches in dialog act classification, which also reported on the same data splits. Therefore, we compare our research against these works. According to (Ortega and Vu, 2017) , prior work by (Ji and Bilmes, 2006) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly.",
  "y": "background"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_8",
  "x": "Our study also shows that the proposed method is very effective for such natural language tasks compared to more complex neural network architectures such as deep CNN<cite> (Lee and Dernoncourt, 2016)</cite> and RNN variants (Khanpour et al., 2016; Ortega and Vu, 2017) . We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing lowdimensional semantic text representations that are useful for text classification applications. ---------------------------------- **DISCUSSION ON MODEL SIZE AND INFERENCE** LSTMs have millions of parameters, while our on-device architecture has just 300K parameters (order of magnitude lower). Most deep learning methods also use large vocabulary size of 10K or higher. Each word embedding is represented as 100-dimensional vector leading to a storage requirement of 10, 000 \u00d7 100 parameter weights just in the first layer of the deep network.",
  "y": "differences"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_10",
  "x": "Acc. Majority Class (baseline) (Ortega and Vu, 2017) 33.7 Naive Bayes (baseline) (Khanpour et al., 2016) 47.3 HMM (Stolcke et al., 2000) 71.0 DRLM-conditional training (Ji and Bilmes, 2006) 77.0 DRLM-joint training (Ji and Bilmes, 2006) 74.0 LSTM<cite> (Lee and Dernoncourt, 2016)</cite> 69.9 CNN<cite> (Lee and Dernoncourt, 2016)</cite> 73.1 Gated-Attention&HMM (Tran et al., 2017) 74.2 RNN+Attention (Ortega and Vu, 2017) 73.8 RNN (Khanpour et al., 2016) 80.1 SGNN: Self-Governing Neural Network (ours) 83.1 (Ortega and Vu, 2017) 59.1 Naive Bayes (baseline) (Khanpour et al., 2016) 74.6 Graphical Model (Ji and Bilmes, 2006) 81.3 CNN<cite> (Lee and Dernoncourt, 2016)</cite> 84.6 RNN+Attention (Ortega and Vu, 2017) 84.3 RNN (Khanpour et al., 2016) 86.8 SGNN: Self-Governing Neural Network (ours) 86.7 Table 3 : MRDA Dataset Results in further speed up for high-dimensional feature spaces. This amounts to a huge savings in storage and computation cost wrt FLOPs (floating point operations per second). ---------------------------------- **CONCLUSION** We proposed Self-Governing Neural Networks for on-device short text classification. Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods <cite>(Lee and Dernoncourt, 2016</cite>; Khanpour et al., 2016; Ortega and Vu, 2017) .",
  "y": "differences"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_1",
  "x": "The DPMM used in<cite> Kawahara et al. (2014)</cite> for clustering verb senses. M is the number of verb senses, and N is the sum total of slot counts for that verb sense. ---------------------------------- **PRIOR WORK** Parisien and Stevenson (2011) and<cite> Kawahara et al. (2014)</cite> showed distinct ways of applying the Hierarchical Dirichlet Process (Teh et al., 2006) to uncover the latent clusters from cluster examples. The latter used significantly larger corpora, and explicitly separated verb sense induction from the syntactic/semantic clustering, which allowed more fine-grained control of each step. In<cite> Kawahara et al. (2014)</cite> , two identical DPMM's were used.",
  "y": "background"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_2",
  "x": "The latter used significantly larger corpora, and explicitly separated verb sense induction from the syntactic/semantic clustering, which allowed more fine-grained control of each step. In<cite> Kawahara et al. (2014)</cite> , two identical DPMM's were used. The first clustered verb instances into senses, and one such model was trained for each verb. These verb-sense clusters are available publicly, and are used unmodified in this paper. The second DPMM clusters verb senses into VerbNet-like clusters of verbs. The result is a resource that, like Verbnet, inherently captures the inherent polysemy of verbs. We focus our improvements on this second step, and try to derive verb clusters that more closely align to VerbNet.",
  "y": "background"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_3",
  "x": "We focus our improvements on this second step, and try to derive verb clusters that more closely align to VerbNet. ---------------------------------- **DIRICHLET PROCESS MIXTURE MODELS** The DPMM used in<cite> Kawahara et al. (2014)</cite> is shown in Figure 1 . The clusters are drawn from a Dirichlet Process with hyperparameter \u03b1 and base distribution G. The Dirichlet process prior creates a clustering effect described by the Chinese Restaurant Process. Each cluster is chosen proportionally to the number of elements it already contains, i.e. where C k ( * ) is the count of clustered items already in cluster k. Each cluster k has an associated multinomial distribution over vocabulary items (e.g. slot:token pairs), \u03c6 k , which is drawn from G, a Dirichlet distribution of the same size as the vocabulary, parameterized by a constant \u03b2.",
  "y": "uses"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_4",
  "x": "By separating the verb sense induction and the clustering of verb senses, the features can be optimized for the distinct tasks. According to <cite>(Kawahara et al., 2014)</cite> , the best features for inducing verb classes are joint slot:token pairs. For the verb clustering task, slot features which ignore the lexical items were the most effective. This aligns with Levin's hypothesis of diathesis alternations -the syntactic contexts are sufficient for the clustering. In this paper, we re-create the second stage clustering with the same features, but add supervision. Supervised Topic Modeling (Mimno and McCallum, 2008; Ramage et al., 2009 ) builds on the Bayesian framework by adding, for each item, a prediction about a variable of interest, which is observed at least some of the time. This encourages the topics to be useful at predicting a supervised signal, as well as coherent as topics.",
  "y": "background"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_5",
  "x": "Instead, we estimate, for each verb in VerbNet, a distribution \u03b8 describing the likelihood a verb will participate in a particular class, using counts from SemLink. When sampling a cluster for a verb sense with a verb in VerbNet, we sample y from a product of experts. We cannot incorporate \u03b8 as a prior when sampling y, because we have multiple verbs, with distinct distributions \u03b8 v 1 , \u03b8 v 2 , . . .. Because the product-of-experts is a discrete probability distribution, it is easy to marginalize out this variable when sampling k, using Either way, once a cluster is selected, we should update the \u03c1 and \u03b8. So, once a cluster is selected, we still sample a discrete y. We compare performance for sampling k with assigned y and with marginalized y. When incorporating supervision, we flatten VerbNet, using only the top-level categories, simplifying the selection process for y. In<cite> Kawahara et al. (2014)</cite> , slot features were most effective features at producing a VerbNet-like structure; we follow suit.",
  "y": "uses"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_6",
  "x": "---------------------------------- **RESULTS** For evaluation, we compare using the same dataset and metrics as<cite> Kawahara et al. (2014)</cite> . There, the authors use the polysemous verb classes of Korhonen et al. (2003) , a subset of frequent polysemous verbs. This makes the test set a sort of miniVerbNet, suitable for evaluation. They also define a normalized modified purity and normalized inverse purity for evaluation, explained below. The standard purity of a hard clustering averages, for each cluster's majority gold standard class, the percentage of clustered items of that class.",
  "y": "uses"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_7",
  "x": "We also define a recall analogue, the normalized inverse purity (niPU), as, This measures how well each gold standard cluster is recovered. We report each metric, and the F1 score combining them, to compare the clustering accuracy with respect to the gold standard G. We use the clustering from<cite> Kawahara et al. (2014)</cite> as a baseline for comparison. However, for evaluation, the authors only clustered senses of verbs in the evaluation set. Since we would like to test the effectiveness of adding supervision, we treat all verbs in the evaluation set as unsupervised, with no initialization of \u03b8. Therefore, to compare apples-to-apples, we calculate the nPU, niPU, and F1 of the<cite> Kawahara et al. (2014)</cite> full clustering against the evaluation set. Our model also computes the full clustering, but with supervision for known verbs (other than the evaluation set).",
  "y": "uses"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_8",
  "x": "Therefore, to compare apples-to-apples, we calculate the nPU, niPU, and F1 of the<cite> Kawahara et al. (2014)</cite> full clustering against the evaluation set. Our model also computes the full clustering, but with supervision for known verbs (other than the evaluation set). Parameters were selected using a grid search, and cross-validation. The results are summarized in Table 1 , comparing the unsupervised DPMM baseline (DPMM) to the supervised DPMM (SDPMM), and the supervised DPMM sampling k with y marginalized out (mSDPMM). ---------------------------------- **COMPARISON OF PRODUCED CLUSTERS** The supervised sampling scheme produces fewer clusters than the unsupervised baseline.",
  "y": "uses"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_9",
  "x": "The supervision tends to encourage a smaller number of clusters, so the precision-like metric, nmPU, is lower, but the recall-like metric, niPU, is much higher. Marginalizing out the variable y when sampling k does not make an appreciable difference to the F1 score. Swapping out the Dirichlet process for a Pitman-Yor process may bring finer control over the number of clusters. We have expanded the work in<cite> Kawahara et al. (2014)</cite> by explicitly modeling a VerbNet class for each verb sense, drawn from a product of experts based on the cluster and verb. This allowed us to leverage data from SemLink with VerbNet annotation, to produce a higher-quality clustering. It also allows us to describe each cluster in terms of alignment to VerbNet classes. Both of these improvements bring us closer to extending VerbNet's usefulness, using only automated dependency parses of corpora.",
  "y": "extends"
 },
 {
  "id": "4d8ae52583d41b4124800c419963df_0",
  "x": "Using each forward and backward LSTM language model, we assign a probability to the underlying fluent parts of each candidate analysis. ---------------------------------- **RERANKER** In order to rank the the 25-best candidate disfluency analyses of the NCM and select the most suitable one, we apply the MaxEnt reranker proposed by . We use the feature set introduced by<cite> Zwarts and Johnson (2011)</cite> , but instead of n-gram scores, we apply the LSTM language model probabilities. The features are so good that the reranker without any external language model is already a state-of-the-art system, providing a very strong baseline for our work. The reranker uses both model-based scores (including NCM scores and LM probabilities) and surface pattern features (which are boolean indicators) as described in Table 1 .",
  "y": "extends"
 },
 {
  "id": "4d8ae52583d41b4124800c419963df_1",
  "x": "The reason is the polynomial-time dynamic programming parsing algorithms of TAG can be used to search for likely repairs if they are used with simple language models such as a bigram LM . The bigram LM within the NCM is too simple to capture more complicated language structure. In order to alleviate this problem, we follow<cite> Zwarts and Johnson (2011)</cite> by training LMs on different corpora, but we apply state-ofthe-art recurrent neural network (RNN) language models. ---------------------------------- **LSTM** We use a long short-term memory (LSTM) neural network for training language models. LSTM is a particular type of recurrent neural networks which has achieved state-of-the-art performance in many tasks including language modelling (Mikolov et al., 2010; Jozefowicz et al., 2016) .",
  "y": "extends"
 },
 {
  "id": "4d8ae52583d41b4124800c419963df_2",
  "x": "---------------------------------- **RERANKER** In order to rank the the 25-best candidate disfluency analyses of the NCM and select the most suitable one, we apply the MaxEnt reranker proposed by . We use the feature set introduced by<cite> Zwarts and Johnson (2011)</cite> , but instead of n-gram scores, we apply the LSTM language model probabilities. The features are so good that the reranker without any external language model is already a state-of-the-art system, providing a very strong baseline for our work. The reranker uses both model-based scores (including NCM scores and LM probabilities) and surface pattern features (which are boolean indicators) as described in Table 1 . Our reranker optimizes the expected f-score approximation described in<cite> Zwarts and Johnson (2011)</cite> with L2 regularisation.",
  "y": "uses"
 },
 {
  "id": "4d8ae52583d41b4124800c419963df_3",
  "x": "Since the bigram language model of the NCM is trained on this corpus, we cannot directly use Switchboard to build LSTM LMs. The reason is that if the training data of Switchboard is used both for predicting language fluency and optimizing the loss function, the reranker will overestimate the model-based features 1-2. forward & backward LSTM LM scores 3-7. This is because the fluent sentence itself is part of the language model <cite>(Zwarts and Johnson, 2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_0",
  "x": "**ABSTRACT** We describe a machine learning based method to identify incorrect entries in translation memories. It extends previous work by<cite> Barbu (2015)</cite> through incorporating recall-based machine translation and part-of-speech-tagging features. Our system ranked first in the Binary Classification (II) task for two out of three language pairs: English-Italian and English-Spanish. ---------------------------------- **INTRODUCTION** Autodesk has accumulated more than 40 million professionally translated segments over the past 17 years.",
  "y": "extends differences"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_1",
  "x": "Going forward, we strive to make human translation more efficient (by showing translators less erroneous fuzzy matches) and machine translation more accurate (by reducing noise in training data). In this paper, we describe our submitted system for distinguishing correct from incorrect TUs. Rather than tailoring it to individual languages, we aimed at a languageindependent solution to cover all of the language pairs in this shared task or, looking to the future, Autodesk's production environments. The system is based on previous work by<cite> Barbu (2015)</cite> and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification. Specifics about previous work are given in the next section. In Section 3, we describe our method and, in Section 4, show how it compares to <cite>Barbu's (2015)</cite> approach as well as other submissions to this shared task. Lastly, we offer preliminary conclusions and outline future work in Section 5.",
  "y": "extends differences"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_2",
  "x": "Rather than tailoring it to individual languages, we aimed at a languageindependent solution to cover all of the language pairs in this shared task or, looking to the future, Autodesk's production environments. The system is based on previous work by<cite> Barbu (2015)</cite> and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification. Specifics about previous work are given in the next section. In Section 3, we describe our method and, in Section 4, show how it compares to <cite>Barbu's (2015)</cite> approach as well as other submissions to this shared task. Lastly, we offer preliminary conclusions and outline future work in Section 5. ---------------------------------- **BACKGROUND**",
  "y": "similarities"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_3",
  "x": "Barbu (2015) has proposed to cast the identification of such incorrect translations as a supervised classification problem. In his work, 1,243 labelled TUs were used to train binary classifiers based on 17 features. The \"most important\" of them, according to the author, were bisegment_similarity and lang_diff : the former is defined as the cosine similarity between a target segment and its machine translated source segment, while the latter denotes whether the language codes declared in a translation unit correspond with the codes detected by a language detector. The best classifier, a support vector machine with linear kernel, achieved 82% precision and 81% recall on a held-out test set of 309 TUs. To the best of our knowledge, Barbu provided the first and so far only research contribution on automatic TM cleaning, which the author himself described as \"a neglected research area\"<cite> (Barbu, 2015)</cite> . With our participation to this shared task, we seek to extend his work by examining new features based on statistical MT and POS tagging. As outlined above, comparing machine translated source segments to their actual target segments has proven effective in <cite>Barbu's (2015)</cite> experiments.",
  "y": "similarities"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_4",
  "x": "In his work, 1,243 labelled TUs were used to train binary classifiers based on 17 features. The \"most important\" of them, according to the author, were bisegment_similarity and lang_diff : the former is defined as the cosine similarity between a target segment and its machine translated source segment, while the latter denotes whether the language codes declared in a translation unit correspond with the codes detected by a language detector. The best classifier, a support vector machine with linear kernel, achieved 82% precision and 81% recall on a held-out test set of 309 TUs. To the best of our knowledge, Barbu provided the first and so far only research contribution on automatic TM cleaning, which the author himself described as \"a neglected research area\"<cite> (Barbu, 2015)</cite> . With our participation to this shared task, we seek to extend his work by examining new features based on statistical MT and POS tagging. As outlined above, comparing machine translated source segments to their actual target segments has proven effective in <cite>Barbu's (2015)</cite> experiments. We propose to complement or replace the similarity function used for this comparison (cosine similarity) by two automatic MT evaluation metrics, Bleu (Papineni et al., 2002) and characterbased Levenshtein distance, in order to reward higher-order n-gram (n > 1) and partial word overlaps, respectively.",
  "y": "background"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_5",
  "x": "---------------------------------- **CLASSIFICATION** Our feature extraction pipeline, including <cite>Barbu's (2015)</cite> as well as our own features (see Section 3.1), is implemented in Scala. This pipeline is used to transform translation units into feature vectors and train classifiers using the scikitlearn framework (Pedregosa et al., 2011) . From the various classification algorithms we tested, Random Forests performed best with our selection of features (see below). ---------------------------------- **FEATURE SELECTION**",
  "y": "similarities"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_6",
  "x": "For the reasons mentioned in Section 1, we aimed at finding a combination of features that would perform well with all language pairs rather than tailoring solutions to individual languages. We focused on gearing our classifiers to distinguish correct or almost correct (classes 1, 2) from incorrect TUs (class 3) -i.e., the Binary Classification (II) task -by optimising the weighted F 1 -score (F 1 ) on training data (see Tables 2a and 2b) . From the various feature combinations we tested, we found the following to be most successful: ratio_words, pos_sim_all, language_detection, mt_cfs, mt_bleu, ratio_chars (as described in Section 3.1), alongside cg_score, only_capletters_dif, and punctuation_similarity (from<cite> Barbu, 2015)</cite> . Evaluation results are given in the next section. ---------------------------------- **RESULTS** We tested our final submission -a Random Forests classifier based on the nine features described in Section 3.4 -on three language pairs (en-de, en-es, en-it) and two tasks: Binary II and Fine-Grained Classification (see Sections 4.1 and 4.2, respectively).",
  "y": "similarities"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_7",
  "x": "Our rationale for focusing on telling apart correct or almost correct from incorrect TUs was that a first application of our method, if successful, would most likely be the filtering of TM data for MT training. While eliminating almost correct TUs might decrease rather than increase MT quality, filtering out incorrect segments can have a positive impact (Vogel, 2003) . Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by<cite> Barbu (2015)</cite> (Baseline 2). More importantly, however, we compared our system to <cite>Barbu's (2015)</cite> approach, using the classification algorithms which reportedly worked best with the 17 features in his work. Our system performed well in this comparison, surpassing Barbu's approach in all language pairs except en-de, where both systems were en par. Details are shown in Table 2a , where we report weighted precision (P), recall (R), and F 1 -scores averaged over 5-fold cross-validation with 2 /3-1 /3 splits of the training data. The final evaluation and ranking produced by the organizers, shown in Table 3a , confirms our findings from experimenting with training data: our system performs well on the en-es and en-it test sets (best in class), while performance is substantially lower on the en-de test set.",
  "y": "similarities"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_8",
  "x": "While eliminating almost correct TUs might decrease rather than increase MT quality, filtering out incorrect segments can have a positive impact (Vogel, 2003) . Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by<cite> Barbu (2015)</cite> (Baseline 2). More importantly, however, we compared our system to <cite>Barbu's (2015)</cite> approach, using the classification algorithms which reportedly worked best with the 17 features in his work. Our system performed well in this comparison, surpassing Barbu's approach in all language pairs except en-de, where both systems were en par. Details are shown in Table 2a , where we report weighted precision (P), recall (R), and F 1 -scores averaged over 5-fold cross-validation with 2 /3-1 /3 splits of the training data. The final evaluation and ranking produced by the organizers, shown in Table 3a , confirms our findings from experimenting with training data: our system performs well on the en-es and en-it test sets (best in class), while performance is substantially lower on the en-de test set. The reasons for this are yet to be ascertained (see also Section 5).",
  "y": "similarities uses"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_9",
  "x": "---------------------------------- **FINE-GRAINED CLASSIFICATION** Although geared to the Binary Classification (II) task (see above), we also assessed our system on the Fine-Grained Classification task. Here, the goal was to distinguish between all of the three classes, i.e., determine whether a TU is correct, almost correct, or incorrect. Again, we compared our system's performance to <cite>Barbu's (2015)</cite> method, using 2 /3-1 /3 splits of the training data (5-fold cross-validation). The results, shown in Table 2b , implied that the nine features we selected would not suffice for a more fine-grained classification of TUs. This was confirmed in the official evaluation and ranking: our system scored low on en-de and mediocre on en-es and en-it.",
  "y": "similarities"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_0",
  "x": "Aletras and Stevenson (2013) showed that coherence can be measured by a classical distributional similarity approach. More recently, <cite>Lau et al. (2014)</cite> proposed a methodology to automate the word intrusion task directly. Their results also reveal the differences between these methodologies in their assessment of topic coherence. A hyper-parameter in all these methodologies is the number of topic words, or its cardinality. These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for Chang et al. (2009) , N = 5, whereas for Newman et al. (2010) , Aletras and Stevenson (2013) and <cite>Lau et al. (2014)</cite> , N = 10. The germ of this paper came when using the automatic word intrusion methodology<cite> (Lau et al., 2014)</cite> , and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction. This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence.",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_1",
  "x": "Their results also reveal the differences between these methodologies in their assessment of topic coherence. A hyper-parameter in all these methodologies is the number of topic words, or its cardinality. These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for Chang et al. (2009) , N = 5, whereas for Newman et al. (2010) , Aletras and Stevenson (2013) and <cite>Lau et al. (2014)</cite> , N = 10. The germ of this paper came when using the automatic word intrusion methodology<cite> (Lau et al., 2014)</cite> , and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction. This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence. To investigate this, we develop a new dataset with human-annotated coherence judgements for a range of cardinality settings (N = {5, 10, 15, 20}). We experiment with the automatic word intrusion<cite> (Lau et al., 2014)</cite> and discover that correlation with human ratings decreases systematically as cardinality increases.",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_2",
  "x": "A hyper-parameter in all these methodologies is the number of topic words, or its cardinality. These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for Chang et al. (2009) , N = 5, whereas for Newman et al. (2010) , Aletras and Stevenson (2013) and <cite>Lau et al. (2014)</cite> , N = 10. The germ of this paper came when using the automatic word intrusion methodology<cite> (Lau et al., 2014)</cite> , and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction. This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence. To investigate this, we develop a new dataset with human-annotated coherence judgements for a range of cardinality settings (N = {5, 10, 15, 20}). We experiment with the automatic word intrusion<cite> (Lau et al., 2014)</cite> and discover that correlation with human ratings decreases systematically as cardinality increases. We also test the PMI methodology (Newman et al., 2010) and make the same observation.",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_3",
  "x": "Their results also reveal the differences between these methodologies in their assessment of topic coherence. A hyper-parameter in all these methodologies is the number of topic words, or its cardinality. These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for Chang et al. (2009) , N = 5, whereas for Newman et al. (2010) , Aletras and Stevenson (2013) and <cite>Lau et al. (2014)</cite> , N = 10. The germ of this paper came when using the automatic word intrusion methodology<cite> (Lau et al., 2014)</cite> , and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction. This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence. To investigate this, we develop a new dataset with human-annotated coherence judgements for a range of cardinality settings (N = {5, 10, 15, 20}). We experiment with the automatic word intrusion<cite> (Lau et al., 2014)</cite> and discover that correlation with human ratings decreases systematically as cardinality increases.",
  "y": "motivation background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_4",
  "x": "We experiment with the automatic word intrusion<cite> (Lau et al., 2014)</cite> and discover that correlation with human ratings decreases systematically as cardinality increases. We also test the PMI methodology (Newman et al., 2010) and make the same observation. To remedy this, we show that performance can be substantially improved if system scores and human ratings are aggregated over different cardinality settings before computing the correlation. This has broad implications for topic model evaluation. ---------------------------------- **DATASET AND GOLD STANDARD** To examine the relationship between topic cardinality and topic coherence, we require a dataset that has topics for a range of cardinality settings.",
  "y": "uses"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_5",
  "x": "This has broad implications for topic model evaluation. ---------------------------------- **DATASET AND GOLD STANDARD** To examine the relationship between topic cardinality and topic coherence, we require a dataset that has topics for a range of cardinality settings. Although there are existing datasets with human-annotated coherence scores (Newman et al., 2010; Aletras and Stevenson, 2013;<cite> Lau et al., 2014</cite>; Chang et al., 2009) , these topics were annotated using a fixed cardinality setting (e.g. 5 or 10). We thus develop a new dataset for this experiment. Following <cite>Lau et al. (2014)</cite> , we use two domains: (1) WIKI, a collection of 3.3 million English Wikipedia articles (retrieved November 28th 2009); and (2) NEWS, a collection of 1.2 million New York Times articles from 1994 to 2004 (English Gigaword).",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_6",
  "x": "This has broad implications for topic model evaluation. ---------------------------------- **DATASET AND GOLD STANDARD** To examine the relationship between topic cardinality and topic coherence, we require a dataset that has topics for a range of cardinality settings. Although there are existing datasets with human-annotated coherence scores (Newman et al., 2010; Aletras and Stevenson, 2013;<cite> Lau et al., 2014</cite>; Chang et al., 2009) , these topics were annotated using a fixed cardinality setting (e.g. 5 or 10). We thus develop a new dataset for this experiment. Following <cite>Lau et al. (2014)</cite> , we use two domains: (1) WIKI, a collection of 3.3 million English Wikipedia articles (retrieved November 28th 2009); and (2) NEWS, a collection of 1.2 million New York Times articles from 1994 to 2004 (English Gigaword).",
  "y": "uses"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_7",
  "x": "We then generate 300 LDA topics for each of the sub-sampled collection. 2 There are two primary approaches to assessing topic coherence: (1) via word intrusion (Chang et (2) by directly measuring observed coherence (Newman et al., 2010;<cite> Lau et al., 2014)</cite> . With the first method, Chang et al. (2009) injects an intruder word into the top-5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words. In preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e.g. when N = 20. As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities. 3 To collect the coherence judgements, we used Amazon Mechanical Turk and asked Turkers to rate topics in terms of coherence using a 3-point ordinal scale, where 1 indicates incoherent and 3 very coherent (Newman et al., 2010) . For each topic (600 topics in total) we experiment with 4 cardinality settings: N = {5, 10, 15, 20}. For example, for N = 5, we display the top-5 topic words for coherence judgement.",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_8",
  "x": "We then generate 300 LDA topics for each of the sub-sampled collection. 2 There are two primary approaches to assessing topic coherence: (1) via word intrusion (Chang et (2) by directly measuring observed coherence (Newman et al., 2010;<cite> Lau et al., 2014)</cite> . With the first method, Chang et al. (2009) injects an intruder word into the top-5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words. In preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e.g. when N = 20. As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities. 3 To collect the coherence judgements, we used Amazon Mechanical Turk and asked Turkers to rate topics in terms of coherence using a 3-point ordinal scale, where 1 indicates incoherent and 3 very coherent (Newman et al., 2010) . For each topic (600 topics in total) we experiment with 4 cardinality settings: N = {5, 10, 15, 20}. For example, for N = 5, we display the top-5 topic words for coherence judgement.",
  "y": "uses"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_9",
  "x": "<cite>Lau et al. (2014)</cite> proposed an automated approach to the word intrusion task. The methodology computes pairwise word association features for the top-N words, and trains a support vector regression model to rank the words. The top-ranked word is then selected as the predicted intruder word. Note that even though it is supervised, no manual annotation is required as the identity of the true intruder word is known. Following the original paper, we use as features normalised PMI (NPMI) and two conditional probabilities (CP1 and CP2), computed over the full collection of WIKI (3.3 million articles) and NEWS (1.2 million articles), respectively. We use 10-fold cross validation to predict the intruder words for all topics. To generate an intruder for a topic, we select a random word that has a low probability (P < 0.0005) in the topic but high probability (P > 0.01) in another topic.",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_10",
  "x": "This is due to idiosyncratic words that are closely related in the collection, e.g. remnant Wikipedia markup tags. The topic model discovers them as topics and the word statistics derived from the same collection supports the association, but these topics are generally not coherent, as revealed by out-of-domain statistics. This result is consistent with previous studies<cite> (Lau et al., 2014)</cite> . We see that correlation decreases systematically as N increases, implying that N has high impact on topic coherence evaluation and that if a single value of N is to be used, a lower value is preferable. To test whether we can leverage the additional information from the different values of N , we aggregate the model precision values and human ratings per-topic before computing the correlation (Table 3 : Cardinality = \"Avg\"). We also test the significance of difference for each N with the aggregate correlation using the Steiger Test (Steiger, 1980) The correlation improves substantially. In fact, for NEWS using in-domain features, the correlation is higher than that of any individual cardinality setting.",
  "y": "similarities"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_11",
  "x": "Instead, we should repeat it several times, varying N . ---------------------------------- **AUTOMATED METHOD -NPMI** The other mainstream approach to evaluating topic coherence is to directly measure the average pairwise association between the top-N words. Newman et al. (2010) found PMI to be the best association measure, and later studies (Aletras and Stevenson, 2013;<cite> Lau et al., 2014)</cite> found that normalised PMI (NPMI: Bouma (2009)) improves PMI further. To see if the benefit of aggregating coherence measures over several cardinalities transfers across to other methodologies, we test the NPMI methodology. We compute the topic coherence using the full collection of WIKI and NEWS, respectively, for varying N .",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_12",
  "x": "Instead, we should repeat it several times, varying N . ---------------------------------- **AUTOMATED METHOD -NPMI** The other mainstream approach to evaluating topic coherence is to directly measure the average pairwise association between the top-N words. Newman et al. (2010) found PMI to be the best association measure, and later studies (Aletras and Stevenson, 2013;<cite> Lau et al., 2014)</cite> found that normalised PMI (NPMI: Bouma (2009)) improves PMI further. To see if the benefit of aggregating coherence measures over several cardinalities transfers across to other methodologies, we test the NPMI methodology. We compute the topic coherence using the full collection of WIKI and NEWS, respectively, for varying N .",
  "y": "uses"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_0",
  "x": "Knowledge Bases (KB's) are structured knowledge sources widely used in applications like question answering (Kwiatkowski et al., 2013; Berant et al., 2013; Bordes et al., 2014) and search engines like Google Search and Microsoft Bing. This has led to the creation of large KB's like Freebase (Bollacker et al., 2008) , YAGO (Suchanek et al., 2007) and NELL (Carlson et al., 2010 ). KB's contains millions of facts usually in the form of triples (entity1, relation, entity2). However, KB's are woefully incomplete (Min et al., 2013) , missing important facts, and hence limiting their usefulness in downstream tasks. Figure 1: The two paths above consist of the same relations (locatedIn \u2192 locatedIn) and, hence, the model of <cite>Neelakantan (2015)</cite> will assign them the same score for the relation AirportServesPlace without considering the fact that Yankee Stadium is not an airport. To overcome this difficulty, Knowledge Base Completion (KBC) methods aim to complete the KB using existing facts. For example, we can infer nationality of a person from their place of birth.",
  "y": "background"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_1",
  "x": "Figure 1: The two paths above consist of the same relations (locatedIn \u2192 locatedIn) and, hence, the model of <cite>Neelakantan (2015)</cite> will assign them the same score for the relation AirportServesPlace without considering the fact that Yankee Stadium is not an airport. To overcome this difficulty, Knowledge Base Completion (KBC) methods aim to complete the KB using existing facts. For example, we can infer nationality of a person from their place of birth. A common approach in many KBC methods for relation extraction is reasoning on individual relations (single-hop reasoning) to predict new relations (Mintz et al., 2009; Bordes et al., 2013; Socher et al., 2013) . For example, predicting Nationality(X, Y) from BornIn(X, Y). The performance of relation extraction methods have been greatly improved by incorporating selectional preferences, i.e., relations enforce constraints on the allowed entity types for the candidate entities, both in sentence level (Roth and Yih, 2007; Singh et al., 2013) and KB relation extraction (Chang et al., 2014) , and in learning entailment rules (Berant et al., 2011) . Another line of work in relation extraction performs reasoning on the paths (multi-hop reasoning on paths of length \u2265 1) connecting an entity pair (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014;<cite> Neelakantan et al., 2015</cite>; Guu et al., 2015) .",
  "y": "background"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_2",
  "x": "All these methods utilize only the relations in the path and do not include any information about the entities. In this work, we extend the method of <cite>Neelakantan (2015)</cite> by incorporating entity type information. Their method can generalize to paths unseen in training by composing embeddings of relations in the path non-linearly using a Recurrent Neural Network (RNN) (Werbos, 1990) . While entity type information has been successfully incorporated into relation extraction methods that perform single hop reasoning, here, we include them for multi-hop relation extraction. For example, Figure 1 illustrates an example where reasoning without type information would score both the paths equally although the latter path should receive a lesser score since there is an entity type mismatch for the first entity. Our approach constructs vector representation of paths in the KB graph from representations of relations and entity types occurring in the path. We achieve a 17.67% improvement in Mean Average Precision (MAP) scores in a relation extraction task when compared to a method that does not use entity type information.",
  "y": "extends"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_3",
  "x": "Lastly, the SHERLOCK system (Schoenmackers et al., 2010 ) also discovers multihop clauses using typed predicates from web text, but, unlike our RNN approach it employs a Inductive Logic Programming method. ---------------------------------- **MODEL** This paper extends the Recurrent Neural Network model of <cite>Neelakantan (2015)</cite> by jointly reasoning over the relations and entity types occurring in the paths between an entity pair. Paths are represented Figure 2: The encoder network for a path between an entity pair. The inputs to the network are embeddings of entities, entity types and relations. This architecture corresponds to equation 4 below.",
  "y": "extends"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_4",
  "x": "In our work, we consider the top l types (sorted by corpus frequency) for an entity and we obtain a combined representation by summing the embeddings of types. Let v r (\u03b4) \u2208 R d denote the vector representation of relation type \u03b4. Let v e (e) \u2208 R m denote the vector representation of an entity e and v et (e) \u2208 R n denote the combined representation of the types of e obtained by taking the sum of the representation of its top l types. Let \u03c0 be a path between the entity pair (e1, e2) containing the relation types \u03b4 1 , \u03b4 2 , . . . , \u03b4 N . In the following section, we first briefly describe the model proposed by <cite>Neelakantan (2015)</cite> (RNN model henceforth) followed by our extensions to it. ---------------------------------- **RNN MODEL**",
  "y": "extends background"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_5",
  "x": "However, we are continuously augmenting the dataset and the latest version of the dataset can be downloaded from http://iesl. cs.umass.edu/downloads/akbc16/. Table  1 displays some statistics of the dataset gathered till now and also the subset that was used for running the current experiments. ---------------------------------- **LINK PREDICTION** We compare our models with the baseline model on predicting whether an entity pair participates in a target relation. We rank the entity pairs in the test set based on their scores and calculate the Mean Average Precision (MAP) score for the ranking following previous work<cite> Neelakantan et al., 2015)</cite> . Table 2 lists the MAP scores of both the models averaged over 12 freebase relation types.",
  "y": "uses"
 },
 {
  "id": "4f0dec0ce2d7639c250be00d5efee4_0",
  "x": "If two word images can match with each other, they should have same symbolic identity. Visual inter-word relations provide a way to link word images in the text and to interpret them systematically. By integrating visual inter-word constraints with word collocation data, the performance of the relaxation algorithm is improved. ---------------------------------- **INTRODUCTION** Word collocation is one source of information that has been proposed as a useful tool to post-process word recognition results( <cite>[1,</cite> 4] ). It can be considered as a constraint on candidate selection so that the word candidate selection problem can be formalized as an instance of constraint satisfaction.",
  "y": "background"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_1",
  "x": "ceived helpfulness of a review depends not only on its review content, but also on social effects such as product qualities, and individual bias in the presence of mixed opinion distribution (Danescu-NiculescuMizil et al., 2009 ). Nonetheless, several properties distinguish our corpus of peer reviews from other types of reviews: 1) The helpfulness of our peer reviews is directly rated using a discrete scale from one to five instead of being defined as a function of binary votes (e.g. the percentage of \"helpful\" votes<cite> (Kim et al., 2006)</cite> ); 2) Peer reviews frequently refer to the related students' papers, thus review analysis needs to take into account paper topics; 3) Within the context of education, peer-review helpfulness often has a writing specific semantics, e.g. improving revision likelihood; 4) In general, peer-review corpora collected from classrooms are of a much smaller size compared to online product reviews. To tailor existing techniques to peer reviews, we will thus propose new specialized features to address these issues. ---------------------------------- **DATA AND FEATURES** In this study, we use a previously annotated peerreview corpus (Nelson and Schunn, 2009; Patchan et al., 2009 ), collected using a freely available webbased peer-review system (Cho and Schunn, 2007) in an introductory college history class. The corpus consists of 16 papers (about six pages each) and 267 reviews (varying from twenty words to about two hundred words).",
  "y": "differences"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_2",
  "x": "Your paper and its main points are easy to find and to follow. As shown in Table 1 , we first mine generic linguistic features from reviews and papers based on the results of syntactic analysis of the texts, aiming to replicate the feature sets used by<cite> Kim et al. (2006)</cite> . While structural, lexical and syntactic features are created in the same way as suggested in their paper, we adapt the semantic and meta-data features to peer reviews by converting the mentions of product properties to mentions of the history topics and by using paper ratings assigned by peers instead of product scores. 1 In addition, the following specialized features are motivated by an empirical study in cognitive science (Nelson and Schunn, 2009 ), which suggests that students' revision likelihood is significantly correlated with certain feedback features, and by our prior work for detecting these cognitive science constructs automatically: Cognitive-science features (cogS): For a given review, cognitive-science constructs that are significantly correlated with review implementation likelihood are manually coded for each idea unit (Nelson and Schunn, 2009 ) within the review. Note, however, that peer-review helpfulness is rated for the whole review, which can include multiple idea units.",
  "y": "uses"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_3",
  "x": "We first manually created a list of words that were specified as signal words for annotating feedbackType and problem localization in the coding manual; then we supplemented the list with words selected by a decision tree model learned using a Bag-of-Words representation of the peer reviews. These categories will also be helpful for reducing the feature space size as discussed below. Localization features (LOC): Five features developed in our prior work for automatically identifying the manually coded problem localization tags, such as the percentage of problems in reviews that could be matched with a localization pattern (e.g. \"on page 5\", \"the section about\"), the percentage of sentences in which topic words exist between the subject and object, etc. ---------------------------------- **EXPERIMENT AND RESULTS** Following<cite> Kim et al. (2006)</cite> , we train our helpfulness model using SVM regression with a radial basis function kernel provided by SVM light (Joachims, 1999) . We first evaluate each feature type in isolation to investigate its predictive power of peerreview helpfulness; we then examine them together in various combinations to find the most useful feature set for modeling peer-review helpfulness.",
  "y": "uses"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_4",
  "x": "---------------------------------- **PERFORMANCE OF GENERIC FEATURES** Evaluation of the generic features is presented in Table 2 , showing that all classes except syntactic (SYN) and meta-data (MET) features are sig-nificantly correlated with both helpfulness rating (r) and helpfulness ranking (r s ). Structural features (bolded) achieve the highest Pearson (0.60) and Spearman correlation coefficients (0.59) (although within the significant correlations, the difference among coefficients are insignificant). Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews<cite> (Kim et al., 2006)</cite> where product scores are significantly correlated with product-review helpfulness. However, when combined with other features, MET does appear to add value (last row). When comparing the performance between predicting helpfulness ratings versus ranking, we observe r \u2248 r s consistently for our peer reviews, while<cite> Kim et al. (2006)</cite> reported r < r s for product reviews.",
  "y": "differences"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_5",
  "x": "Structural features (bolded) achieve the highest Pearson (0.60) and Spearman correlation coefficients (0.59) (although within the significant correlations, the difference among coefficients are insignificant). Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews<cite> (Kim et al., 2006)</cite> where product scores are significantly correlated with product-review helpfulness. However, when combined with other features, MET does appear to add value (last row). When comparing the performance between predicting helpfulness ratings versus ranking, we observe r \u2248 r s consistently for our peer reviews, while<cite> Kim et al. (2006)</cite> reported r < r s for product reviews. 4 Finally, we observed a similar feature redundancy effect as<cite> Kim et al. (2006)</cite> did, in that simply combining all features does not improve the model's performance. Interestingly, our best feature combination (last row) is the same as theirs. In sum our results verify our hypothesis that the effectiveness of generic features can be transferred to our peerreview domain for predicting review helpfulness.",
  "y": "differences"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_6",
  "x": "When comparing the performance between predicting helpfulness ratings versus ranking, we observe r \u2248 r s consistently for our peer reviews, while<cite> Kim et al. (2006)</cite> reported r < r s for product reviews. 4 Finally, we observed a similar feature redundancy effect as<cite> Kim et al. (2006)</cite> did, in that simply combining all features does not improve the model's performance. Interestingly, our best feature combination (last row) is the same as theirs. In sum our results verify our hypothesis that the effectiveness of generic features can be transferred to our peerreview domain for predicting review helpfulness. ---------------------------------- **ANALYSIS OF THE SPECIALIZED FEATURES** Evaluation of the specialized features is shown in Table 3 , where all features examined are signifi- 4 The best performing single feature type reported<cite> (Kim et al., 2006)</cite> was review unigrams: r = 0.398 and rs = 0.593.",
  "y": "similarities"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_7",
  "x": "Interestingly, our best feature combination (last row) is the same as theirs. In sum our results verify our hypothesis that the effectiveness of generic features can be transferred to our peerreview domain for predicting review helpfulness. ---------------------------------- **ANALYSIS OF THE SPECIALIZED FEATURES** Evaluation of the specialized features is shown in Table 3 , where all features examined are signifi- 4 The best performing single feature type reported<cite> (Kim et al., 2006)</cite> was review unigrams: r = 0.398 and rs = 0.593. cantly correlated with both helpfulness rating and ranking. When evaluated in isolation, although specialized features have weaker correlation coefficients ([0.43, 0.51] ) than the best generic features, these differences are not significant, and the specialized features have the potential advantage of being theory-based.",
  "y": "background"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_8",
  "x": "3) When cognitive-science and localization features are introduced, the prediction becomes even more accurate, which reaches a Pearson correlation of 0.67 and a Spearman correlation of 0.67 (Table 3 , last row). ---------------------------------- **DISCUSSION** Despite the difference between peer reviews and other types of reviews as discussed in Section 2, our work demonstrates that many generic linguistic features are also effective in predicting peer-review helpfulness. The model's performance can be alter- natively achieved and further improved by adding auxiliary features tailored to peer reviews. These specialized features not only introduce domain expertise, but also capture linguistic information at an abstracted level, which can help avoid the risk of over-fitting. Given only 267 peer reviews in our case compared to more than ten thousand product reviews<cite> (Kim et al., 2006)</cite> , this is an important consideration.",
  "y": "differences motivation"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_9",
  "x": "Though our absolute quantitative results are not directly comparable to the results of<cite> Kim et al. (2006)</cite> , we indirectly compared them by analyzing the utility of features in isolation and combined. While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews). More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews<cite> (Kim et al., 2006</cite>; Danescu-Niculescu-Mizil et al., 2009) , have no predictive power for peer reviews. Perhaps because the paper grades and other helpfulness ratings are not visible to the reviewers, we have less of a social dimension for predicting the helpfulness of peer reviews. We also found that SVM regression does not favor ranking over predicting helpfulness as in<cite> (Kim et al., 2006)</cite> . ---------------------------------- **CONCLUSIONS AND FUTURE WORK**",
  "y": "uses"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_10",
  "x": "Though our absolute quantitative results are not directly comparable to the results of<cite> Kim et al. (2006)</cite> , we indirectly compared them by analyzing the utility of features in isolation and combined. While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews). More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews<cite> (Kim et al., 2006</cite>; Danescu-Niculescu-Mizil et al., 2009) , have no predictive power for peer reviews. Perhaps because the paper grades and other helpfulness ratings are not visible to the reviewers, we have less of a social dimension for predicting the helpfulness of peer reviews. We also found that SVM regression does not favor ranking over predicting helpfulness as in<cite> (Kim et al., 2006)</cite> . ---------------------------------- **CONCLUSIONS AND FUTURE WORK**",
  "y": "differences"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_11",
  "x": "More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews<cite> (Kim et al., 2006</cite>; Danescu-Niculescu-Mizil et al., 2009) , have no predictive power for peer reviews. Perhaps because the paper grades and other helpfulness ratings are not visible to the reviewers, we have less of a social dimension for predicting the helpfulness of peer reviews. We also found that SVM regression does not favor ranking over predicting helpfulness as in<cite> (Kim et al., 2006)</cite> . ---------------------------------- **CONCLUSIONS AND FUTURE WORK** The contribution of our work is three-fold: 1) Our work successfully demonstrates that techniques used in predicting product review helpfulness ranking can be effectively adapted to the domain of peer reviews, with minor modifications to the semantic and metadata features. 2) Our qualitative comparison shows that the utility of generic features (e.g. meta-data features) in predicting review helpfulness varies between different review types.",
  "y": "differences"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_0",
  "x": "---------------------------------- **INTRODUCTION** Springing forth from the pages of science fiction and capturing the daydreams of weary chore-doers everywhere, the promise and potential of general-purpose robotic assistants that follow natural language instructions has been long understood. Taking a small step towards this goal, recent work has begun developing artificial agents that follow natural language navigation instructions in perceptually-rich, simulated environments<cite> [4,</cite> 6] . An example instruction might be \"Go down the hall and turn left at the wooden desk. Continue until you reach the kitchen and then stop by the kettle.\" and agents are evaluated by their ability to follow the described path in (potentially novel) simulated environments. Many of these tasks have been developed from datasets of panoramic images captured in real scenes -e.g.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_1",
  "x": "Many of these tasks have been developed from datasets of panoramic images captured in real scenes -e.g. Google StreetView images in Touchdown [6] or Matterport3D panoramas captured in homes in Vision-and-Language Navigation (VLN)<cite> [4]</cite> . This paradigm enables efficient data collection and high visual fidelity compared to 3D scanning or creating synthetic environments; however, scenes are only observed from a sparse set of points relative to the full 3D environment (\u223c117 viewpoints per environment in VLN). As a consequence, environments in these tasks are defined in terms of a navigation graph (or nav-graph for short) (a) Vision-and-Language Navigation (VLN) (b) VLN in Continuous Environments (VLN-CE) Fig. 1 . The VLN setting (a) operates on a fixed topology of panoramic images (shown in blue) -assuming perfect navigation between nodes (often meters apart) and precise localization. Our VLN-CE setting (b) lifts these assumptions by instantiating the task in continuous environments with low-level actions -providing a more realistic testbed for robot instruction following. -a static topological representation of 3D space.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_2",
  "x": "Likewise, algorithms for path planning and control can navigate short distances in the presence of obstacles [11, 25, 31] . Further, it is reasonable to suggest that issuing commands at the level of relative waypoints (in analogy to nav-graph nodes) is the proper interface between language-guided AI navigators and lowerlevel agent control. However, these techniques are each independently far from perfect and such an agent would need to learn the limitations of these lowerlevel control systems -facing consequences when proposed waypoints cannot be reached effectively. Integrative studies such as these that combine and evaluate techniques for control and mapping with learned AI agents are not possible in current nav-graph based problem settings. In this work, we develop a continuous setting that enables these types of studies and take a first step towards integrating VLN agents with control via low-level actions. Vision-and-Language Navigation in Continuous Environments. In this work, we focus in on the Vision-and-Language Navigation (VLN)<cite> [4]</cite> task and lift these implicit assumptions by instantiating it in continuous 3D environments rendered in a high-throughput simulator [19] .",
  "y": "uses"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_5",
  "x": "-Investigate how a number of popular techniques in VLN transfer to this more challenging long-horizon setting -identifying significant gaps in performance. ---------------------------------- **RELATED WORK** Language-guided Visual Navigation Tasks. Language-guided visual navigation tasks require agents to follow navigation directions in simulated environments. There have been a number of recent tasks proposed in this space<cite> [4,</cite> 6, 13, 20] . Chen et al. [6] introduce the Touchdown task which studies outdoor language-guided navigation in Google Street View panoramas.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_6",
  "x": "Misra et al. [20] introduce a simulated environment with unconstrained navigation and a dataset of crowdsourced instructions; however, the environments are unrealistic, synthetic scenes. Most related to our work is the Vision-and-Language Navigation (VLN) task of Anderson et al.<cite> [4]</cite> . VLN provides nav-graph trajectories and crowdsourced instructions in Matterport3D [5] environments as the Room-to-Room (R2R) dataset. We build VLN-CE directly on these annotations -converting R2R panorama-based trajectories to fine-grained paths in continuous Matterport3D environments ( Fig. 1 (a) to Fig. 1(b) ). As outlined in the introduction, this shift to continuous environments with unconstrained agent navigation lifts a number of unrealistic assumptions. The variation in these tasks is primarily in the source of navigation instructions (crowdsourced from human annotators vs. generated via template), environment realism (hand-designed synthetic worlds vs. captures from real locations), and constraints on agent navigation (nav-graph based navigation vs. unconstrained agent motion). Tab.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_7",
  "x": "In overview, we develop this setting by transferring nav-graph-based Room-to-Room (R2R)<cite> [4]</cite> trajectories to reconstructed continuous Matterport3D environments in the Habitat simulator [19] . We discuss the task specification and the details of this transfer process in this section. Continuous Matterport3D Environments in Habitat. We set our problem in the Matterport3D (MP3D) [5] dataset, a collection of 90 environments captured through over 10,800 high-definition RGB-D panoramas. In addition to the panoramic images, MP3D also provides corresponding mesh-based 3D environment reconstructions. To enable agent interaction with these meshes, we develop the VLN-CE task on top of the Habitat Simulator [19] , a high-throughput simulator that supports basic movement and collision checking for 3D environments including MP3D. In contrast to the simulator used in VLN<cite> [4]</cite> , Habitat allows agents to navigate freely in the continuous environments.",
  "y": "uses"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_8",
  "x": "We discuss the task specification and the details of this transfer process in this section. Continuous Matterport3D Environments in Habitat. We set our problem in the Matterport3D (MP3D) [5] dataset, a collection of 90 environments captured through over 10,800 high-definition RGB-D panoramas. In addition to the panoramic images, MP3D also provides corresponding mesh-based 3D environment reconstructions. To enable agent interaction with these meshes, we develop the VLN-CE task on top of the Habitat Simulator [19] , a high-throughput simulator that supports basic movement and collision checking for 3D environments including MP3D. In contrast to the simulator used in VLN<cite> [4]</cite> , Habitat allows agents to navigate freely in the continuous environments. Observations and Actions.",
  "y": "differences"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_9",
  "x": "Agents perceive the world through egocentric RGBD images from the simulator with a resolution of 256\u00d7256 and a horizontal field-of-view of 90 degrees. Note that this is similar to the egocentric RGB perception in the original VLN task<cite> [4]</cite> but differs from the panoramic observation space adopted by nearly all follow-up work [9, 17, 26, 29] . While the simulator is quite flexible in terms of agent actions, we consider four simple, low-level actions for agents in VLN-CE -move forward 0.25m, turn-left or turn-right 15 degrees, or stop to declare that the goal position has been reached. These actions can easily be implemented on robotic agents with standard motion controllers. In contrast, actions to move between panoramas in<cite> [4]</cite> traverse 2.25m on average and can include avoiding obstacles. ---------------------------------- **TRANSFERRING NAV-GRAPH TRAJECTORIES**",
  "y": "similarities"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_10",
  "x": "In contrast, actions to move between panoramas in<cite> [4]</cite> traverse 2.25m on average and can include avoiding obstacles. ---------------------------------- **TRANSFERRING NAV-GRAPH TRAJECTORIES** Rather than collecting a new dataset of trajectories and instructions, we instead transfer those from the nav-graph-based Room-to-Room dataset to our continuous setting. Doing so enables us to compare existing nav-graph-based techniques with our methods that operate in continuous environments on the same instructions. Matterport3D Simulator and the Room-to-Room Dataset. The original VLN task is based on panoramas from Matterport3D (MP3D) [5] .",
  "y": "differences"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_11",
  "x": "The original VLN task is based on panoramas from Matterport3D (MP3D) [5] . To enable agent interaction with these panoramas, Anderson et al.<cite> [4]</cite> developed the Matterport3D Simulator. Environments in this simulator are defined as nav-graphs E = {V, E}. Each node v \u2208 V corresponds to a panoramic image I captured by a Matterport camera at location x, y, z -i.e. v = {I, x, y, z}. Edges in the graph correspond to navigability between nodes. Navigability was defined by ray-tracing between node locations at varying heights to check for obstacles in the reconstructed MP3D scene and then manually inspected. Edges were manually added or removed based on judgement whether an agent could navigate between nodes -including by avoiding minor obstacles 4 . Agents act by teleporting between adjacent nodes in this graph.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_12",
  "x": "v = {I, x, y, z}. Edges in the graph correspond to navigability between nodes. Navigability was defined by ray-tracing between node locations at varying heights to check for obstacles in the reconstructed MP3D scene and then manually inspected. Edges were manually added or removed based on judgement whether an agent could navigate between nodes -including by avoiding minor obstacles 4 . Agents act by teleporting between adjacent nodes in this graph. Based on this simulator, Anderson et al.<cite> [4]</cite> collect the Roomto-Room (R2R) dataset containing 7189 trajectories each with three humangenerated instructions on average. These trajectories consist of a sequence of nodes \u03c4 = [v 1 , . . . , v T ] with length T averaging between 4 and 6 nodes. Converting Room-to-Room Trajectories to Habitat.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_13",
  "x": "As shown in Fig. 3(c) , the low-level action space of VLN-CE makes our trajectories significantly longer horizon tasks -with an average of 55.88 steps compared to the 4-6 in R2R. ---------------------------------- **INSTRUCTION-GUIDED NAVIGATION MODELS IN VLN-CE** We develop two models for VLN-CE. A simple sequence-to-sequence baseline and a more powerful cross-modal attentional model. While there are many differences in the details, these models are conceptually similar to early<cite> [4]</cite> and more recent [29] work in the nav-graph based VLN task. Exploring these gives insight into the difficulty of this setting in isolation and by comparison relative to VLN.",
  "y": "similarities"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_14",
  "x": "---------------------------------- **AUXILIARY LOSSES AND TRAINING REGIMES** Aside from modeling details, much of the remaining progress in VLN has come from adjusting the training regime -adding auxiliary losses / rewards [17, 29] , mitigating exposure bias during training<cite> [4,</cite> 29] , or reducing data sparsity by incorporating synthetically generated data augmentation [9, 26] . We explore some of these directions for VLN-CE, but note that this is not an exhaustive accounting of impactful techniques. Particularly, we suspect that methods addressing exposure bias and data sparsity in VLN will help in the VLN-CE setting where these problems may be amplified by lengthy action sequences. We report ablations with and without these techniques in Sec. 5. Imitation Learning.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_15",
  "x": "Aside from modeling details, much of the remaining progress in VLN has come from adjusting the training regime -adding auxiliary losses / rewards [17, 29] , mitigating exposure bias during training<cite> [4,</cite> 29] , or reducing data sparsity by incorporating synthetically generated data augmentation [9, 26] . We explore some of these directions for VLN-CE, but note that this is not an exhaustive accounting of impactful techniques. Particularly, we suspect that methods addressing exposure bias and data sparsity in VLN will help in the VLN-CE setting where these problems may be amplified by lengthy action sequences. We report ablations with and without these techniques in Sec. 5. Imitation Learning. A natural starting point for training is simply to maximize the likelihood of the ground truth trajectories. To do so, we perform teacherforcing training with inflection weighting.",
  "y": "motivation"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_16",
  "x": "This was found to be helpful for problems like navigation with long sequences of repeated actions (e.g. going forward down a hall). We observe a similar effect in early experiments and apply inflection weighting in all our experiments. Coping with Exposure Bias. Imitation learning in auto-regressive settings suffers from a disconnect between training and test -agents are not exposed to the consequences of their actions during training. Prior work has shown significant gains by addressing this issue for VLN through scheduled sampling<cite> [4]</cite> or reinforcement learning fine-tuning [26, 29] . In this work, we apply Dataset Aggregation (DAgger) [24] towards the same end. While DAgger and scheduled sampling share many similarities, DAgger trains on the aggregated set of trajectories from all iterations 1 to n. Thus, the resulting policy after iteration n is optimized over all past experiences and not just those collected from iteration n. Synthetic Data Augmentation.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_17",
  "x": "As described in [30] , inflection weighting places emphasis on time-steps where actions change (i.e. a t\u22121 = a t ), adjusting loss weight proportionally to the rarity of such events. This was found to be helpful for problems like navigation with long sequences of repeated actions (e.g. going forward down a hall). We observe a similar effect in early experiments and apply inflection weighting in all our experiments. Coping with Exposure Bias. Imitation learning in auto-regressive settings suffers from a disconnect between training and test -agents are not exposed to the consequences of their actions during training. Prior work has shown significant gains by addressing this issue for VLN through scheduled sampling<cite> [4]</cite> or reinforcement learning fine-tuning [26, 29] . In this work, we apply Dataset Aggregation (DAgger) [24] towards the same end.",
  "y": "similarities"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_18",
  "x": "---------------------------------- **EXPERIMENTS** Setting and Metrics. We train and evaluate our models in VLN-CE. As is common practice, we perform early stopping based on val-unseen performance. We report standard metrics for visual navigation tasks defined in [2, <cite>4,</cite> 18] -trajectory length in meters (TL), navigation error in meters from goal at termination (NE), oracle success rate (OS), success rate (SR), success weighted by inverse path length (SPL), and normalized dynamic-time warping (nDTW). For our discussion, we will examine success rate and SPL as the primary metrics for performance and use NDTW to describe how paths differ in shape from ground truth trajectories.",
  "y": "uses"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_19",
  "x": "We train and evaluate our models in VLN-CE. As is common practice, we perform early stopping based on val-unseen performance. We report standard metrics for visual navigation tasks defined in [2, <cite>4,</cite> 18] -trajectory length in meters (TL), navigation error in meters from goal at termination (NE), oracle success rate (OS), success rate (SR), success weighted by inverse path length (SPL), and normalized dynamic-time warping (nDTW). For our discussion, we will examine success rate and SPL as the primary metrics for performance and use NDTW to describe how paths differ in shape from ground truth trajectories. For full details on these metrics, see [2, <cite>4,</cite> 18] . Implementation Details. We utilize the Adam optimizer [15] with a learning rate of 2.5 \u00d7 10 \u22124 and a batch size of 5 full trajectories.",
  "y": "uses"
 },
 {
  "id": "51575bb1ffb066d9570551f3347622_0",
  "x": "Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks <cite>[7,</cite> 8, 9 ]. The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather ---------------------------------- ****",
  "y": "background"
 },
 {
  "id": "51575bb1ffb066d9570551f3347622_1",
  "x": "Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather ---------------------------------- **** knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, 5, 6] . How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks <cite>[7,</cite> 8, 9] . The word2vec [10] is among the most widely used word embedding models today.",
  "y": "background"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_0",
  "x": "This framework holds that meaning can be inferred from the linguistic context of the word, usually seen as co-occurrence data. The context of usage is even more crucial for characterizing meanings of ambiguous or polysemous words: a definition that does not take disambiguating context into account will be of limited use <cite>(Gadetsky et al., 2018)</cite> . We argue that definition modeling should preserve the link between the definiendum and its context of occurrence. The most natural approach to this task is to treat it as a sequence-to-sequence task, rather than a word-to-sequence task: given an input sequence with a highlighted word, generate a contextually appropriate definition for it (cf. sections 3 & 4) . We implement this approach in a Transformer-based sequence-to-sequence model that achieves state-of-the-art performances (sections 5 & 6). arXiv:1911.05715v1 [cs.CL] 13 Nov 2019 2 Related Work",
  "y": "motivation"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_1",
  "x": "3 Definition modeling as a sequence-to-sequence task<cite> Gadetsky et al. (2018)</cite> remarked that words are often ambiguous or polysemous, and thus generating a correct definition requires that we either use sense-level representations, or that we disambiguate the word embedding of the definiendum. The disambiguation that<cite> Gadetsky et al. (2018)</cite> proposed was based on a contextual cue-ie. a short text fragment. As notes, the cues in Gadetsky et al.'s (2018) dataset do not necessarily contain the definiendum or even an inflected variant thereof. For instance, one training example disambiguated the word \"fool\" using the cue \"enough horsing around-let's get back to work!\". Though the remark that definienda must be disambiguated is pertinent, the more natural formulation of such a setup would be to disambiguate the definiendum using its actual context of occurrence. In that respect, the definiendum and the contextual cue would form a linguistically coherent sequence, and thus it would make sense to encode the context together with the definiendum, rather than to merely rectify the definiendum embedding using a contextual cue.",
  "y": "background"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_2",
  "x": "On a more abstract level, definition modeling is related to research on the analysis and evaluation of word embeddings (Levy and Goldberg, 2014a,b; Arora et al., 2018; Batchkarov et al., 2016; Swinger et al., 2018, e.g.) . It also relates to other works associating definitions and embeddings, like the \"reverse dictionary task\" (Hill et al., 2016 )-retrieving the definiendum knowing its definition, which can be argued to be the opposite of definition modeling-or works that derive embeddings from definitions (Wang et al., 2015; Tissier et al., 2017; Bosc and Vincent, 2018) . 3 Definition modeling as a sequence-to-sequence task<cite> Gadetsky et al. (2018)</cite> remarked that words are often ambiguous or polysemous, and thus generating a correct definition requires that we either use sense-level representations, or that we disambiguate the word embedding of the definiendum. The disambiguation that<cite> Gadetsky et al. (2018)</cite> proposed was based on a contextual cue-ie. a short text fragment. As notes, the cues in Gadetsky et al.'s (2018) dataset do not necessarily contain the definiendum or even an inflected variant thereof. For instance, one training example disambiguated the word \"fool\" using the cue \"enough horsing around-let's get back to work!\".",
  "y": "background"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_3",
  "x": "They argued for an approach related to, though distinct from sequence-to-sequence architectures. Concretely, a specific encoding procedure was applied to the definiendum, so that it could be used as a feature vector during generation. In the simplest case, vector encoding of the definiendum consists in looking up its vector in a vocabulary embedding matrix. We argue that the whole context of a word's usage should be accessible to the generation algorithm rather than a single vector. To take a more specific case of verb definitions, we observe that context explicitly represents argument structure, which is obviously useful when defining the verb. There is no guarantee that a single embedding, even if it be contextualized, would preserve this wealth of information-that is to say, that you can cram all the information pertaining to the syntactic context into a single vector. Despite some key differences, all of the previously proposed architectures we are aware of (Noraset et al., 2017;<cite> Gadetsky et al., 2018</cite>; followed a pattern similar to sequence-to-sequence models.",
  "y": "background"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_4",
  "x": "This SELECT approach may seem intuitive and naturally interpretable, as it directly controls what information is passed to the decoder-we carefully select only the contextualized definiendum, thus the only remaining zone of uncertainty would be how exactly contextualization is performed. It also seems to provide a strong and reasonable bias for training the definition generation system. Such an approach, however, is not guaranteed to excel: forcibly omitted context could contain important information that might not be easily incorporated in the definiendum embedding. Being simple and natural, the SELECT approach resembles architectures like that of<cite> Gadetsky et al. (2018)</cite> and : the full encoder is dedicated to altering the embedding of the definiendum on the basis of its context; in that, the encoder may be seen as a dedicated contextualization sub-module. ---------------------------------- **ADDITIVE MARKING: ADD** We also study an additive mechanism shown in Figure 1b (henceforth ADD).",
  "y": "similarities"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_5",
  "x": "---------------------------------- **DATASETS** We train our models on three distinct datasets, which are all borrowed or adapted from previous works on definition modeling. As a consequence, our experiments focus on the English language. The dataset of Noraset et al. (2017) (henceforth D Nor ) maps definienda to their respective definientia, as well as additional information not used here. In the dataset of<cite> Gadetsky et al. (2018)</cite> (henceforth D Gad ), each example consists of a definiendum, the definientia for one of its meanings and a contextual cue sentence. D Nor contains on average shorter definitions than D Gad .",
  "y": "background"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_6",
  "x": "We use perplexity, a standard metric in definition modeling, to evaluate and compare our models. Informally, perplexity assesses the model's confidence in producing the ground-truth output when presented the source input. It is formally defined as the exponentiation of cross-entropy. We do not report BLEU or ROUGE scores due to the fact that an important number of ground-truth definitions are comprised of a single word, in particular in D Nor (\u2248 25%). Single word outputs can either be assessed as entirely correct or entirely wrong using BLEU or ROUGE. However consider for instance the word \"elation\": that it be defined either as \"mirth\" or \"joy\" should only influence our metric slightly, and not be discounted as a completely wrong prediction. , as they did not report the perplexity of their system and focused on a different dataset; likewise, consider only the Chinese variant of the task. Perplexity measures for Noraset et al. (2017) and<cite> Gadetsky et al. (2018)</cite> are taken from the authors' respective publications.",
  "y": "uses"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_7",
  "x": "All our models perform better than previous proposals, by a margin of 4 to 10 points, for a relative improvement of 11-23%. Part of this improvement may be due to our use of Transformerbased architectures (Vaswani et al., 2017) , which is known to perform well on semantic tasks (Radford, 2018; Cer et al., 2018; Devlin et al., 2018; Radford et al., 2019, eg.) . Like<cite> Gadetsky et al. (2018)</cite> , we conclude that disambiguating the definiendum, when done correctly, improves performances: our best performing contex-tual model outranks the non-contextual variant by 5 to 6 points. The marking of the definiendum out of its context (ADD vs. SELECT) also impacts results. Note also that we do not rely on taskspecific external resources (unlike Noraset et al., 2017; or on pre-training (unlike<cite> Gadetsky et al., 2018)</cite> . Our contextual systems trained on the D Gad dataset used the concatenation of the definiendum and the contextual cue as inputs. The definiendum was always at the start of the training example.",
  "y": "similarities"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_8",
  "x": "All our models perform better than previous proposals, by a margin of 4 to 10 points, for a relative improvement of 11-23%. Part of this improvement may be due to our use of Transformerbased architectures (Vaswani et al., 2017) , which is known to perform well on semantic tasks (Radford, 2018; Cer et al., 2018; Devlin et al., 2018; Radford et al., 2019, eg.) . Like<cite> Gadetsky et al. (2018)</cite> , we conclude that disambiguating the definiendum, when done correctly, improves performances: our best performing contex-tual model outranks the non-contextual variant by 5 to 6 points. The marking of the definiendum out of its context (ADD vs. SELECT) also impacts results. Note also that we do not rely on taskspecific external resources (unlike Noraset et al., 2017; or on pre-training (unlike<cite> Gadetsky et al., 2018)</cite> . Our contextual systems trained on the D Gad dataset used the concatenation of the definiendum and the contextual cue as inputs. The definiendum was always at the start of the training example.",
  "y": "differences"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_9",
  "x": "Adding such a sub-module to our proposed architecture might diminish the number of mistagged definienda. Another possibility would be to pre-train the model, as was done by<cite> Gadetsky et al. (2018)</cite> : in our case in particular, the encoder could be trained for POS-tagging or lemmatization. Lastly, one important kind of mistakes we observed is hallucinations. Consider for instance this production by the ADD model trained on D Ctx , for the word \"beta\": \"the twentieth letter of the Greek alphabet (\u03ba), transliterated as 'o'.\". Nearly everything it contains is factually wrong, though the general semantics are close enough to deceive an unaware reader. 8 We conjecture that filtering out hallucinatory productions will be a main challenge for future definition modeling architectures, for two main reasons: firstly, the tools and metrics necessary to assess and handle such hallucinations have yet to be developed; secondly, the input given to the system being word embeddings, research will be faced with the problem of grounding these distributional representations-how can we ensure that \"beta\" is correctly defined as \"the second letter of the Greek alphabet, transliterated as 'b'\", if we only have access to a representation derived from its contexts of usage? Integration of word embeddings with structured knowledge bases might be needed for accurate treatment of such cases.",
  "y": "future_work"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_10",
  "x": "---------------------------------- **CONCLUSION** We introduced an approach to generating word definitions that allows the model to access rich contextual information about the word token to be defined. Building on the distributional hypothesis, we naturally treat definition generation as a sequence-to-sequence task of mapping the word's context of usage (input sequence) into the contextappropriate definition (output sequence). We showed that our approach is competitive against a more naive 'contextualize and select' pipeline. This was demonstrated by comparison both to the previous contextualized model by<cite> Gadetsky et al. (2018)</cite> and to the Transformerbased SELECT variation of our model, which differs from the proposed architecture only in the context encoding pipeline. While our results are encouraging, given the existing benchmarks we were limited to perplexity measurements in our quantitative evaluation.",
  "y": "similarities"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_0",
  "x": "While Levin's classification can be extended manually (Kipper-Schuler, 2005) , a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002) . There has also been some success incorporating selectional preferences<cite> (Sun and Korhonen, 2009)</cite> . Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004) . Exceptions to this include Merlo and Stevenson (2001) , Joanis et al. (2008) and Stevenson (2010, 2011) . Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale.",
  "y": "background"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_1",
  "x": "F3: The frame pair features (DAs) built from F1 with the frame dependency assumption (equation 4). This is the DA feature which considers the correlation of the two frames which are generated from the alternation. F3 implicitly includes F1, as a frame can pair with itself. 2 In the example in Table 2 , the frame pair \"PP(on) PP(on)\" will always have the same value as the \"PP(on)\" frame in F1. We extracted the SCFs using the system of Preiss et al. (2007) which classifies each corpus occurrence of a verb as a member of one of the 168 SCFs on the basis of grammatical relations identified by the RASP (Briscoe et al., 2006) parser. We experimented with two datasets that have been used in prior work on verb clustering: the test sets 7-11 (3-14 classes) in Joanis et al. (2008) , and the 17 classes set in Sun et al. (2008) . We used the spectral clustering (SPEC) method and settings as in<cite> Sun and Korhonen (2009)</cite> but adopted the Bhattacharyya kernel (Jebara and Kondor, 2003) to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space.",
  "y": "uses"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_2",
  "x": "We used the spectral clustering (SPEC) method and settings as in<cite> Sun and Korhonen (2009)</cite> but adopted the Bhattacharyya kernel (Jebara and Kondor, 2003) to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space. The mean-filed bound of the Bhattacharyya kernel is very similar to the KL divergence kernel (Jebara et al., 2004) which is frequently used in verb clustering experiments (Korhonen et al., 2003;<cite> Sun and Korhonen, 2009)</cite> . To further reduce computational complexity, we restricted our scope to the more frequent features. In the experiment described in this section we used the 50 most frequent features for the 3-6 way classifications (Joanis et al.'s test set 7-9) and 100 features for the 7-17 way classifications. In the next section, we will demonstrate that F3 outperforms F1 regardless of the feature number setting. The features are normalized to sum 1. The clustering results are evaluated using FMeasure as in<cite> Sun and Korhonen (2009)</cite> which provides the harmonic mean of precision (P ) and recall (R) P is calculated using modified purity -a global measure which evaluates the mean precision of clusters.",
  "y": "background"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_3",
  "x": "The mean-filed bound of the Bhattacharyya kernel is very similar to the KL divergence kernel (Jebara et al., 2004) which is frequently used in verb clustering experiments (Korhonen et al., 2003;<cite> Sun and Korhonen, 2009)</cite> . To further reduce computational complexity, we restricted our scope to the more frequent features. In the experiment described in this section we used the 50 most frequent features for the 3-6 way classifications (Joanis et al.'s test set 7-9) and 100 features for the 7-17 way classifications. In the next section, we will demonstrate that F3 outperforms F1 regardless of the feature number setting. The features are normalized to sum 1. The clustering results are evaluated using FMeasure as in<cite> Sun and Korhonen (2009)</cite> which provides the harmonic mean of precision (P ) and recall (R) P is calculated using modified purity -a global measure which evaluates the mean precision of clusters. Each cluster (k i \u2208 K) is associated with the gold-standard class to which the majority of its members belong.",
  "y": "uses"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_4",
  "x": "The result of F3 is 6.4% higher than the result (F=63.28) reported in<cite> Sun and Korhonen (2009)</cite> using the F1 feature. This experiment shows, on two datasets, that DA features are clearly more effective than the frame features for verb clustering, even when relaxations are used. ---------------------------------- **ANALYSIS OF FEATURE FREQUENCY** A further experiment was carried out using F1 and F3 on Joanis et al. (2008) 's test sets 10 and 11. The frequency ranked features were added to the clustering one at a time, starting from the most frequent one. The results are shown in figure 2.",
  "y": "differences"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_5",
  "x": "We have also demonstrated that the performance of frame features is dominated by the high frequency frames. In contrast, the DA features enable the mid-range frequency frames to further improve the performance. In the future, we plan to evaluate the performance of DA features in a larger scale experiment. Due to the high dimensionality of the transformed feature space (quadratic of the original feature space), we will need to improve the computational efficiency further, e.g. via use of an unsupervised dimensionality reduction technique Zhao and Liu (2007) . Moreover, we plan to use Bayesian inference as in Vlachos et al. (2009); Stevenson (2010, 2011) to infer the actual parameter values and avoid the relaxation. Finally, we plan to supplement the DA feature with evidence from the slot fillers of the alternating slots, in the spirit of earlier work (McCarthy, 2000; Merlo and Stevenson, 2001; Joanis et al., 2008) . Unlike these previous works, we will use selectional preferences to generalize the argument heads but will do so using preferences from distributional data <cite>(Sun and Korhonen, 2009</cite> ) rather than WordNet, and use all argument head data in all frames.",
  "y": "uses"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_0",
  "x": "One is a phrase-based translation in which a phrasal unit is employed for translation <cite>(Koehn et al., 2003)</cite> . The other is a hierarchical phrase-based translation in which translation is realized as a set of paired production rules (Chiang, 2005) . Section 2 discusses those two models and details extraction algorithms, decoding algorithms and feature functions. We also explored three types of corpus preprocessing in Section 3. As expected, different tokenization would lead to different word alignments which, in turn, resulted in the divergence of the extracted phrase/rule size. In our method, phrase/rule translation pairs extracted from three distinctly word-aligned corpora are aggregated into one large phrase/rule translation table. The experiments and the final translation results are presented in Section 4.",
  "y": "background"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_1",
  "x": "Feature function scaling factors \u03bb m are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003) . This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. In a phrase-based statistical translation <cite>(Koehn et al., 2003)</cite> , a bilingual text is decomposed as K phrase translation pairs (\u0113 1 ,f\u0101 1 ), (\u0113 2 ,f\u0101 2 ), ...: The input foreign sentence is segmented into phrasesf K 1 , mapped into corresponding English\u0113 K 1 , then, reordered to form the output English sentence according to a phrase alignment index mapping\u0101. In a hierarchical phrase-based translation (Chiang, 2005) , translation is modeled after a weighted synchronous-CFG consisting of production rules whose right-hand side is paired (Aho and Ullman, 1969) : X \u2192 \u03b3, \u03b1, \u223c where X is a non-terminal, \u03b3 and \u03b1 are strings of terminals and non-terminals. \u223c is a one-to-one correspondence for the non-terminals appeared in \u03b3 and \u03b1. Starting from an initial non-terminal, each rule rewrites non-terminals in \u03b3 and \u03b1 that are associated with \u223c.",
  "y": "background"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_2",
  "x": "\u223c is a one-to-one correspondence for the non-terminals appeared in \u03b3 and \u03b1. Starting from an initial non-terminal, each rule rewrites non-terminals in \u03b3 and \u03b1 that are associated with \u223c. ---------------------------------- **PHRASE/RULE EXTRACTION** The phrase extraction algorithm is based on those presented by<cite> Koehn et al. (2003)</cite> . First, manyto-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003) , in both directions and by combining the results based on a heuristic (Och and Ney, 2004) . Second, phrase translation pairs are extracted from the word aligned corpus <cite>(Koehn et al., 2003)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_3",
  "x": "Second, phrase translation pairs are extracted from the word aligned corpus <cite>(Koehn et al., 2003)</cite> . The method exhaustively extracts phrase pairs ( f j+m j , e i+n i ) from a sentence pair ( f J 1 , e I 1 ) that do not violate the word alignment constraints a. In the hierarchical phrase-based model, production rules are accumulated by computing \"holes\" for extracted contiguous phrases (Chiang, 2005) : 2. A rule X \u2192 \u03b3, \u03b1 and a phrase pair (f ,\u0113) s.t. \u03b3 = \u03b3 \u2032f \u03b3 \u2032\u2032 and \u03b1 = \u03b1 \u2032\u0113 \u03b1 \u2032\u2032 constitutes a rule: ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_4",
  "x": "\u03b3 = \u03b3 \u2032f \u03b3 \u2032\u2032 and \u03b1 = \u03b1 \u2032\u0113 \u03b1 \u2032\u2032 constitutes a rule: ---------------------------------- **DECODING** The decoder for the phrase-based model is a left-toright generation decoder with a beam search strategy synchronized with the cardinality of already translated foreign words. The decoding process is very similar to those described in <cite>(Koehn et al., 2003)</cite> : It starts from an initial empty hypothesis. From an existing hypothesis, new hypothesis is generated by consuming a phrase translation pair that covers untranslated foreign word positions. The score for the newly generated hypothesis is updated by combining the scores of feature functions described in Section 2.3.",
  "y": "similarities uses"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_5",
  "x": "In the hierarchical phrase-based model, decoding is realized as an Earley-style top-down parser on the foreign language side with a beam search strategy synchronized with the cardinality of already translated foreign words (Watanabe et al., 2006) . The major difference to the phrase-based model's decoder is the handling of non-terminals, or holes, in each rule. ---------------------------------- **FEATURE FUNCTIONS** Our phrase-based model uses a standard pharaoh feature functions listed as follows <cite>(Koehn et al., 2003)</cite> : \u2022 Relative-count based phrase translation probabilities in both directions. \u2022 Lexically weighted feature functions in both directions.",
  "y": "similarities uses"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_6",
  "x": "Our phrase-based model uses a standard pharaoh feature functions listed as follows <cite>(Koehn et al., 2003)</cite> : \u2022 Relative-count based phrase translation probabilities in both directions. \u2022 Lexically weighted feature functions in both directions. \u2022 The supplied trigram language model. \u2022 Distortion model that counts the number of words skipped. \u2022 The number of words in English-side and the number of phrases that constitute translation. For details, please refer to<cite> Koehn et al. (2003)</cite> .",
  "y": "background"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_7",
  "x": "Second, corpora were transformed by a Porter's algorithm based multilingual stemmer (stem) 1 . Third, mixed-cased corpora were truncated to the prefix of four letters of each word (prefix4). For each differently tokenized corpus, we computed word alignments by a HMM translation model (Och and Ney, 2003) and by a word alignment refinement heuristic of \"grow-diagfinal\" <cite>(Koehn et al., 2003)</cite> . Different preprocessing yields quite divergent alignment points as illustrated in Table 1 . The table also shows the numbers for the intersection and union of three alignment annotations. The (hierarchical) phrase translation pairs are extracted from three distinctly word aligned corpora. In this process, each word is recovered into its lowercased form.",
  "y": "similarities uses"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_0",
  "x": "---------------------------------- **INTRODUCTION** There has been a steadily increasing interest in syntactic parsing based on dependency analysis in recent years. One important reason seems to be that dependency parsing offers a good compromise between the conflicting demands of analysis depth, on the one hand, and robustness and efficiency, on the other. Thus, whereas a complete dependency structure provides a fully disambiguated analysis of a sentence, this analysis is typically less complex than in frameworks based on constituent analysis and can therefore often be computed deterministically with reasonable accuracy. Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000) , English<cite> (Yamada and Matsumoto, 2003)</cite> , Turkish (Oflazer, 2003) , and Swedish (Nivre et al., 2004) . For English, the interest in dependency parsing has been weaker than for other languages.",
  "y": "background"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_1",
  "x": "Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000) , English<cite> (Yamada and Matsumoto, 2003)</cite> , Turkish (Oflazer, 2003) , and Swedish (Nivre et al., 2004) . For English, the interest in dependency parsing has been weaker than for other languages. To some extent, this can probably be explained by the strong tradition of constituent analysis in Anglo-American linguistics, but this trend has been reinforced by the fact that the major treebank of American English, the Penn Treebank (Marcus et al., 1993) , is annotated primarily with constituent analysis. On the other hand, the best available parsers trained on the Penn Treebank, those of Collins (1997) and Charniak (2000) , use statistical models for disambiguation that make crucial use of dependency relations. Moreover, the deterministic dependency parser of<cite> Yamada and Matsumoto (2003)</cite> , when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of Collins (1997) and Charniak (2000) . The parser described in this paper is similar to that of<cite> Yamada and Matsumoto (2003)</cite> in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank. However, there are also important differences between the two approaches.",
  "y": "background"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_2",
  "x": "Moreover, the deterministic dependency parser of<cite> Yamada and Matsumoto (2003)</cite> , when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of Collins (1997) and Charniak (2000) . The parser described in this paper is similar to that of<cite> Yamada and Matsumoto (2003)</cite> in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank. However, there are also important differences between the two approaches. First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (essentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithm proposed in Nivre (2003) , which combines bottomup and top-down processing in a single pass in order to achieve incrementality. This also means that the time complexity of the algorithm used here is linear in the size of the input, while the algorithm of Yamada and Matsumoto is quadratic in the worst case. Another difference is that Yamada and Matsumoto use support vector machines (Vapnik, 1995) , while we instead rely on memory-based learning (Daelemans, 1999) . Most importantly, however, the parser presented in this paper constructs labeled dependency graphs, i.e. dependency graphs where arcs are labeled with dependency types.",
  "y": "similarities uses"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_3",
  "x": "For English, the interest in dependency parsing has been weaker than for other languages. To some extent, this can probably be explained by the strong tradition of constituent analysis in Anglo-American linguistics, but this trend has been reinforced by the fact that the major treebank of American English, the Penn Treebank (Marcus et al., 1993) , is annotated primarily with constituent analysis. On the other hand, the best available parsers trained on the Penn Treebank, those of Collins (1997) and Charniak (2000) , use statistical models for disambiguation that make crucial use of dependency relations. Moreover, the deterministic dependency parser of<cite> Yamada and Matsumoto (2003)</cite> , when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of Collins (1997) and Charniak (2000) . The parser described in this paper is similar to that of<cite> Yamada and Matsumoto (2003)</cite> in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank. However, there are also important differences between the two approaches. First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (essentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithm proposed in Nivre (2003) , which combines bottomup and top-down processing in a single pass in order to achieve incrementality.",
  "y": "extends differences"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_4",
  "x": "First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (essentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithm proposed in Nivre (2003) , which combines bottomup and top-down processing in a single pass in order to achieve incrementality. This also means that the time complexity of the algorithm used here is linear in the size of the input, while the algorithm of Yamada and Matsumoto is quadratic in the worst case. Another difference is that Yamada and Matsumoto use support vector machines (Vapnik, 1995) , while we instead rely on memory-based learning (Daelemans, 1999) . Most importantly, however, the parser presented in this paper constructs labeled dependency graphs, i.e. dependency graphs where arcs are labeled with dependency types. As far as we know, this makes it different from all previous systems for dependency parsing applied to the Penn Treebank (Eisner, 1996; <cite>Yamada and Matsumoto, 2003)</cite> , although there are systems that extract labeled grammatical relations based on shallow parsing, e.g. Buchholz (2002) . The fact that we are working with labeled dependency graphs is also one of the motivations for choosing memory-based learning over support vector machines, since we require a multi-class classifier. Even though it is possible to use SVM for multi-class classification, this can get cumbersome when the number of classes is large.",
  "y": "differences"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_5",
  "x": "The data has been converted to dependency trees using head rules (Magerman, 1995; Collins, 1996) . We are grateful to Yamada and Matsumoto for letting us use their rule set, which is a slight modification of the rules used by Collins (1999) . This permits us to make exact comparisons with the parser of<cite> Yamada and Matsumoto (2003)</cite> , but also the parsers of Collins (1997) and Charniak (2000) , which are evaluated on the same data set in<cite> Yamada and Matsumoto (2003)</cite> . One problem that we had to face is that the standard conversion of phrase structure trees to dependency trees gives unlabeled dependency trees, whereas our parser requires labeled trees. Since the annotation scheme of the Penn Treebank does not include dependency types, there is no straightforward way to derive such labels. We have therefore experimented with two different sets of labels, none of which corresponds to dependency types in a strict sense. The first set consists of the function tags for grammatical roles according to the Penn II annotation guidelines (Bies et al., 1995) ; we call this set G. The second set consists of the ordinary bracket labels (S, NP, VP, etc.), combined with function tags for grammatical roles, giving composite labels such as NP-SBJ; we call this set B. We assign labels to arcs by letting each (non-root) word that heads a phrase P in the original phrase structure have its incoming edge labeled with the label of P (modulo the set of labels used).",
  "y": "uses"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_6",
  "x": "We use the following metrics for evaluation: The proportion of non-root words that are assigned the correct head<cite> (Yamada and Matsumoto, 2003)</cite> . The proportion of root words that are analyzed as such<cite> (Yamada and Matsumoto, 2003)</cite> . The proportion of sentences whose unlabeled dependency structure is completely correct<cite> (Yamada and Matsumoto, 2003)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_7",
  "x": "First of all, we see that Model 1 gives better accuracy than Model 2 with the smaller label set G, which confirms our expectations that the added part-of-speech features are helpful when the dependency labels are less informative. Conversely, we see that Model 2 outperforms Model 1 with the larger label set B, which is consistent with the hypothesis that part-of-speech features become redundant as dependency labels get more informative. It is interesting to note that this effect holds even in the case where the dependency labels are mostly derived from phrase structure categories. We can also see that the unlabeled attachment score improves, for both models, when the set of dependency labels is extended. On the other hand, the labeled attachment score drops, but it must be remembered that these scores are not really comparable, since the number of classes in the classification problem increases from 7 to 50 as we move from the G set to the B set. Therefore, we have also included the labeled attachment score restricted to the G set for the parser using the B set (BG), and we see then that the attachment score improves, especially for Model 2. (All differences are significant beyond the .01 level; McNemar's test.) Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with Collins (1997) (Model 3) , Charniak (2000) , and<cite> Yamada and Matsumoto (2003)</cite> . 5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of<cite> Yamada and Matsumoto (2003)</cite> .",
  "y": "uses"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_8",
  "x": "We can also see that the unlabeled attachment score improves, for both models, when the set of dependency labels is extended. On the other hand, the labeled attachment score drops, but it must be remembered that these scores are not really comparable, since the number of classes in the classification problem increases from 7 to 50 as we move from the G set to the B set. Therefore, we have also included the labeled attachment score restricted to the G set for the parser using the B set (BG), and we see then that the attachment score improves, especially for Model 2. (All differences are significant beyond the .01 level; McNemar's test.) Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with Collins (1997) (Model 3) , Charniak (2000) , and<cite> Yamada and Matsumoto (2003)</cite> . 5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of<cite> Yamada and Matsumoto (2003)</cite> . We believe that there are mainly three reasons for this. First of all, the part-of-speech tagger used for preprocessing in our experiments has a lower accuracy than the one used by<cite> Yamada and Matsumoto (2003)</cite> (96.1% vs. 97.1%) . Although this is not a very interesting explanation, it undoubtedly accounts for part of the difference.",
  "y": "differences"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_9",
  "x": "We believe that there are mainly three reasons for this. First of all, the part-of-speech tagger used for preprocessing in our experiments has a lower accuracy than the one used by<cite> Yamada and Matsumoto (2003)</cite> (96.1% vs. 97.1%) . Although this is not a very interesting explanation, it undoubtedly accounts for part of the difference. Secondly, since 5 The information in the first three rows is taken directly from<cite> Yamada and Matsumoto (2003)</cite> . our parser makes crucial use of dependency type information in predicting the next action of the parser, it is very likely that it suffers from the lack of real dependency labels in the converted treebank. Indirect support for this assumption can be gained from previous experiments with Swedish data, where almost the same accuracy (85% unlabeled attachment score) has been achieved with a treebank which is much smaller but which contains proper dependency annotation (Nivre et al., 2004) . A third important factor is the relatively low root accuracy of our parser, which may reflect a weakness in the one-pass parsing strategy with respect to the global structure of complex sentences.",
  "y": "differences"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_10",
  "x": "Although this is not a very interesting explanation, it undoubtedly accounts for part of the difference. Secondly, since 5 The information in the first three rows is taken directly from<cite> Yamada and Matsumoto (2003)</cite> . our parser makes crucial use of dependency type information in predicting the next action of the parser, it is very likely that it suffers from the lack of real dependency labels in the converted treebank. Indirect support for this assumption can be gained from previous experiments with Swedish data, where almost the same accuracy (85% unlabeled attachment score) has been achieved with a treebank which is much smaller but which contains proper dependency annotation (Nivre et al., 2004) . A third important factor is the relatively low root accuracy of our parser, which may reflect a weakness in the one-pass parsing strategy with respect to the global structure of complex sentences. It is noteworthy that our parser has lower root accuracy than dependency accuracy, whereas the inverse holds for all the other parsers. The problem becomes even more visible when we consider the dependency and root accuracy for sentences of different lengths, as shown in Table 3 .",
  "y": "uses"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_11",
  "x": "The corresponding F-measures for our best parser (Model 2, BG) are 99.0% and 94.7%. For the larger B set, our best parser achieves an F-measure of 96.9% (DEP labels included), which can be compared with 97.0% for a similar (but larger) set of labels in Collins (1999) . 6 Although none of the previous results on labeling accuracy is strictly comparable to ours, it nevertheless seems fair to conclude that the<cite> (Yamada and Matsumoto, 2003)</cite> labeling accuracy of the present parser is close to the state of the art, even if its capacity to derive correct structures is not. ---------------------------------- **CONCLUSION** This paper has explored the application of a datadriven dependency parser to English text, using data from the Penn Treebank. The parser is deterministic and uses a linear-time parsing algorithm, guided by memory-based classifiers, to construct labeled dependency structures incrementally in one pass over the input.",
  "y": "differences"
 },
 {
  "id": "5596207b89d917db38c04af49c08aa_0",
  "x": "**INTRODUCTION** Attention-based neural networks have demonstrated success in a wide range of NLP tasks ranging from neural machine translation , image captioning (Xu et al., 2015) , and speech recognition (Chorowski et al., 2015) . Benefiting from the availability of large-scale benchmark datasets such as SQuAD (Rajpurkar et al., 2016) , the attention-based neural networks has spread to machine comprehension and question answering tasks to allow the model to attend over past output vectors (Wang & Jiang, 2017; Seo et al., 2017; Xiong et al., 2017;<cite> Hu et al., 2017</cite>; Pan et al., 2017) . Wang & Jiang (2017) uses attention mechanism in Pointer Network to detect an answer boundary by predicting the start and the end indices in the passage. Seo et al. (2017) introduces a bi-directional attention flow network that attention models are decoupled from the recurrent neural networks. Xiong et al. (2017) employs a coattention mechanism that attends to the question and document together. uses a gated attention network that includes both question and passage match and self-matching attentions. Both Pan et al. (2017) and Hu et al. (2017) employs the structure of multi-hops or iterative aligner to repeatedly fuse the passage representation with the question representation as well as the passage representation itself.",
  "y": "background"
 },
 {
  "id": "5596207b89d917db38c04af49c08aa_2",
  "x": "Finally, the fused vectors are sent to the output layer to predict the boundary of the answer span described in Section 2.4. ---------------------------------- **ENCODER LAYERS** The concatenation of raw features as inputs are processed in fusion layers followed by encoder layers to form more abstract representations. Here we choose a bi-directional Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997 ) to obtain more abstract representations for words in passages and questions. Different from the commonly used approaches that every single model has exactly one question and passage encoder (Seo et al., 2017;<cite> Hu et al., 2017)</cite> , our encoder layers simultaneously calculate multiple question and passage representations, for the purpose of serving different parts of attention functions of different phases. We use two types of encoders, independent encoder and shared encoder.",
  "y": "differences"
 },
 {
  "id": "5596207b89d917db38c04af49c08aa_4",
  "x": "All the experiment settings are the same for PhaseCond and Iterative Aligner including the number of attention layers, input features, optimizer and learning rate, number of training steps and etc. As shown in Table 2 which summarizes the performance of single models, we achieve steady improvements when 1) additional question encoders are used to extend the passage-question attention function, denoted as QPAtt+, as detailed in Section 2.1 and Section 2.2, and 2) on top of that, using PhaseCond making our model better than using Iterative Aligner. Specifically, PhaseCond's computational path for two question-aware passage attention layers On the other hand, Iterative Aligner builds path in turn through different kinds of attention layers: The performance of our models and published results of competing attention-based architectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo et al., 2017) and RNET from their recently published papers instead of using the up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline is one implementation of MReader, re-named as Iterative Aligner which has very similar results as those of MReader <cite>(Hu et al., 2017)</cite> 71.1 / 79.5 71.3 / 79.7 75.6 / 82.8 75.9 / 82.9 MReader <cite>(Hu et al., 2017)</cite> N As shown in Table 3 , in the single model setting, our model PhaseCond is clearly more effective than all the single-layered models (BiDAF and RNET) and multi-layered models (MReader and Iterative Aligner). We draw the same conclusion for the ensemble model setting, despite that the RNET works better on the Dev EM measure.",
  "y": "similarities"
 },
 {
  "id": "55e429045af4434f9cb27ae8c6db66_0",
  "x": "AES is challenging since it relies not only on grammars, but also on semantics, discourse and pragmatics. Traditional approaches treat AES as a classification (Larkey, 1998; Rudner and Liang, 2002) , regression (Attali and Burstein, 2004;<cite> Phandi et al., 2015)</cite> , or ranking classification problem (Yannakoudakis et al., 2011; Chen and He, 2013) , addressing AES by supervised learning. Features are typically bag-of-words, spelling errors and lengths, such word length, sentence length and essay length, etc. Some grammatical features are considered to assess the quality of essays (Yannakoudakis et al., 2011) . A drawback is feature engineering, which can be time-consuming, since features need to be carefully handcrafted and selected to fit the approriate model. A further drawback of manual feature templates is that they are sparse, instantiated by discrete pattern-matching. As a result, parsers and semantic analyzers are necessary as a preprocessing step to offer syntactic and semantic patterns for feature extraction.",
  "y": "background"
 },
 {
  "id": "55e429045af4434f9cb27ae8c6db66_1",
  "x": "CNN has been used in many NLP applications, such as sequence labeling (Collobert et al., 2011) , sentences modeling (Kalchbrenner et al., 2014) , sentences classification (Kim, 2014 ), text categorization (Johnson and Zhang, 2014; Zhang et al., 2015) and sentimental analysis (dos Santos and Gatti, 2014), etc. In this paper, we explore CNN representation ability for AES tasks on both in-domain and domain-adaptation settings. ---------------------------------- **BASELINE** Bayesian Linear Ridge Regression (BLRR) and Support Vector Regression (SVR) (Smola and Vapnik, 1997 ) are chosen as state-of-art baselines. Feature templates follow <cite>(Phandi et al., 2015)</cite> , extracted by EASE 1 , which are briefly listed in Table 1 . \"Useful n-grams\" are determined using the Fisher test to separate the good scoring essays and bad scoring essays. Good essays are essays with a score greater than or equal to the average score, and the remainder are considered as bad scoring essays.",
  "y": "uses"
 },
 {
  "id": "55e429045af4434f9cb27ae8c6db66_2",
  "x": "The top 201 ngrams with the highest Fisher values are chosen as the bag of features and these top 201 n-grams constitute useful n-grams. Correct POS tags are generated using grammatically correct texts, which is done by EASE. The POS tags that are not included in the correct POS tags are treated as bad POS tags, and these bad POS tags make up the \"bad POS n-grams\" features. The features tend to be highly useful for the in-domain task since the discrete features of same prompt data share the similar statistics. However, for different prompts, features statistics vary significantly. This raises challenges for discrete feature patterns. ML-\u03c1 <cite>(Phandi et al., 2015)</cite> was proposed to address this issue.",
  "y": "uses"
 },
 {
  "id": "55e429045af4434f9cb27ae8c6db66_3",
  "x": "The settings of data preparation follow <cite>(Phandi et al., 2015)</cite> . We use quadratic weighted kappa (QWK) as the metric. For domainadaptation (cross-domain) experiments, we follow <cite>(Phandi et al., 2015)</cite> , picking four pairs of essay prompts, namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928, where 1\u21922 denotes prompt 1 as source domain and prompt Hyper-parameters We use Adagrad for optimization. Word embeddings are randomly initialized and the hyper-parameter settings are listed in Table 3 . ---------------------------------- **RESULTS** In-domain The in-domain results are shown in Figure 2 .",
  "y": "uses"
 },
 {
  "id": "55e429045af4434f9cb27ae8c6db66_4",
  "x": "**SETUP** Data We use the Automated Student Assessment Prize (ASAP) 2 dataset as evaluation data for our task, which contains 8 prompts of different genres as listed in Table 2 . The essay scores are scaled into the range from 0 to 1. The settings of data preparation follow <cite>(Phandi et al., 2015)</cite> . We use quadratic weighted kappa (QWK) as the metric. For domainadaptation (cross-domain) experiments, we follow <cite>(Phandi et al., 2015)</cite> , picking four pairs of essay prompts, namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928, where 1\u21922 denotes prompt 1 as source domain and prompt Hyper-parameters We use Adagrad for optimization. Word embeddings are randomly initialized and the hyper-parameter settings are listed in Table 3 .",
  "y": "uses"
 },
 {
  "id": "57af9690eb41ff3f9217da6138425f_0",
  "x": "Part 6 covers discourse structure and centering, commenting on open questions such as how local utterance processing relates to the global discourse context and how centering interacts with the global context in constraining the surface form of referring expressions other than pronouns (Passonneau). Part 6 also looks at the place of centering in a general theory of anaphora resolution and proposes a new hybrid theory that integrates Grosz and Sidner's discourse structure theory with a dynamic theory of semantic interpretation (Roberts). Finally in the last chapter of Part 6, it is argued that the restriction of centering to operate within a discourse segment should be abandoned in favor of a new model integrating centering and the global discourse structure and to this end it is proposed that a model of attentional state, the cache model, be integrated with the centering algorithm (Walker). Centering has proved to be a powerful tool for accounting for discourse coherence and has been used successfully in anaphora resolution; however, as with every theory in linguistics, it has its limitations. Some chapters suggest extensions of or amendments to the centering theory with a view to achieving a more comprehensive and successful model (e.g., the chapters by Kameyama, Roberts, and Walker). Ideally, in addition to papers such as Kameyama's and Walker's, this collection could perhaps also have featured extended versions of papers, such as those of Kehler (1997) and Hahn and Strube (1997) , that highlight certain weaknesses of the original centering model or suggest extensions or alternative solutions <cite>(Strube 1998)</cite> . It must be acknowledged here that the production schedule of this volume may have been a factor in not including some of this recent work, and also that space limits might not have allowed all possible areas of centering to be covered.",
  "y": "background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_0",
  "x": "**INTRODUCTION** Recent work has shown evidence of substantial bias in machine learning systems, which is typically a result of bias in the training data. This includes both supervised (Blodgett and O'Connor, 2017; Tatman, 2017; Kiritchenko and Mohammad, 2018; De-Arteaga et al., 2019) and unsupervised natural language processing systems (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018) . Machine learning models are currently being deployed in the field to detect hate speech and abusive language on social media platforms including Facebook, Instagram, and Youtube. The aim of these models is to identify abusive language that directly targets certain individuals or groups, particularly people belonging to protected categories (Waseem et al., 2017) . Bias may reduce the accuracy of these models, and at worst, will mean that the models actively discriminate against the same groups they are designed to protect. Our study focuses on racial bias in hate speech and abusive language detection datasets<cite> (Waseem, 2016</cite>; Waseem and Hovy, 2016; Golbeck et al., 2017; Founta et al., 2018) , all of which use data collected from Twitter.",
  "y": "background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_1",
  "x": "While hate speech and abusive language detection has become an important area for natural language processing research (Schmidt and Wiegand, 2017; Waseem et al., 2017; Fortuna and Nunes, 2018) , there has been little work addressing the potential for these systems to be biased. The danger posed by bias in such systems is, however, particularly acute, since it could result in negative impacts on the same populations the systems are designed to protect. For example, if we mistakenly consider speech by a targeted minority group as abusive we might unfairly penalize the victim, but if we fail to identify abuse against them we will be unable to take action against the perpetrator. Although no model can perfectly avoid such problems, we should be particularly concerned about the potential for such models to be systematically biased against certain social groups, particularly protected classes. A number of studies have shown that false positive cases of hate speech are associated with the presence of terms related to race, gender, and sexuality (Kwok and Wang, 2013; Burnap and Williams, 2015; . While not directly measuring bias, prior work has explored how annotation schemes and the identity of the annotators<cite> (Waseem, 2016</cite> ) might be manipulated to help to avoid bias. Dixon et al. (2018) directly measured biases in the Google Perspective API classifier, 1 trained on data from Wikipedia talk comments, finding that it tended to give high toxicity scores to innocuous statements like \"I am a gay man\".",
  "y": "background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_2",
  "x": "These annotators were then reviewed by \"a 25 year old woman studying gender studies and a nonactivist feminist\" to check for bias. This dataset consists of 16,849 tweets labeled as either racism, sexism, or neither. Most of the tweets categorized as sexist relate to debates over an Australian TV show and most of those considered as racist are anti-Muslim. To account for potential bias in the previous dataset,<cite> Waseem (2016)</cite> relabeled 2876 tweets in the dataset, along with a new sample from the tweets originally collected. The tweets were annotated by \"feminist and anti-racism activists\", based upon the assumption that they are domain-experts. A fourth category, racism and sexism was also added to account for the presence of tweets which exhibit both types of abuse. The dataset contains 6,909 tweets.",
  "y": "background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_3",
  "x": "In all but one of the comparisons, there are statistically significant (p < 0.001) differences and in all but one of these we see that tweets in the black-aligned corpus are assigned negative labels more frequently than those by whites. The only case where blackaligned tweets are classified into a negative class less frequently than white-aligned tweets is the racism class in the Waseem and Hovy (2016) classifier. Note, however, the extremely low rate at which tweets are predicted to belong to this class for both groups. On the other hand, this classifier is 1.7 times more likely to classify tweets in the black-aligned corpus as sexist. For<cite> Waseem (2016)</cite> we see that there is no significant difference in the estimated rates at which tweets are classified as racist across groups, although the rates remain low. Tweets in the black-aligned corpus are classified as containing sexism almost twice as frequently and 1.1 times as frequently classified as containing racism and sexism compared to those in the white-aligned corpus. Moving onto , we find large disparities, with around 5% of tweets in the black-aligned corpus classified as hate speech compared to 2% of those in the white-aligned set.",
  "y": "similarities"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_4",
  "x": "Table 3 shows that for tweets containing the word \"n*gga\", classifiers trained on Waseem and Hovy (2016) and<cite> Waseem (2016)</cite> are both predict black-aligned tweets to be instances of sexism approximately 1.5 times as often as white-aligned tweets. The classifier trained on the data is significantly less likely to classify black-aligned tweets as hate speech, although it is more likely to classify them as offensive. Golbeck et al. (2017) classifies black-aligned tweets as harassment at a higher rate for both groups than in the previous experiment, although the disparity is narrower. For the Founta et al. (2018) classifier we see that black-aligned tweets are slightly less frequently considered to be hate speech but are much more frequently classified as abusive. The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 . We see similar results for Waseem and Hovy (2016) and<cite> Waseem (2016)</cite> . In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism.",
  "y": "background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_5",
  "x": "For the Founta et al. (2018) classifier we see that black-aligned tweets are slightly less frequently considered to be hate speech but are much more frequently classified as abusive. The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 . We see similar results for Waseem and Hovy (2016) and<cite> Waseem (2016)</cite> . In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism. The Waseem and Hovy (2016) classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class. For almost all of these tweets are classified as offensive, however those in the blackaligned corpus are 1.15 times as frequently classified as hate speech. We see a very similar result for Golbeck et al. (2017) compared to the previous experiment, with black-aligned tweets flagged as harassment at 1.1 times the rate of those in the white-aligned corpus.",
  "y": "similarities"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_6",
  "x": "While some of the remaining disparities are likely due to differences in the distributions of other keywords we did not condition on, we expect that other more innocuous aspects of black-aligned language may be associated with negative labels in the training data, leading classifiers to disproportionately predict that tweets by African-Americans belong to negative classes. We now discuss the results as they pertain to each of the datasets used. Classifiers trained on data from Waseem and Hovy (2016) and<cite> Waseem (2016)</cite> only predicted a small fraction of the tweets to be racism. We suspect that this is due to the composition of their dataset, since the majority of the racist training examples consist of anti-Muslim rather than anti- Table 4 : Experiment 2, t = \"b*tch\" black language. Across both datasets the words \"n*gger\" and \"n*gga\" appear in 4 and 10 tweets respectively. Looking at the sexism class on the other hand, we see that both models were consistently classifying tweets in the black-aligned corpus as sexism at a substantially higher rate than those in the white-aligned corpus. Given this result, and the gender biases identified in these data by Park et al. (2018), it not apparent that the purportedly expert annotators were any less biased than amateur annotators<cite> (Waseem, 2016)</cite> .",
  "y": "similarities background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_7",
  "x": "While these datasets are still valuable for academic research, we caution against using them in the field to detect and particularly to take enforcement action against different types of abusive language. If they are used in this way we expect that they will systematically penalize African-Americans more than whites, resulting in racial discrimination. We have not evaluated these datasets for bias related to other ethnic and racial groups, nor other protected categories like gender and sexuality, but expect that such bias is also likely to exist. We recommend that efforts to measure and mitigate bias should start by focusing on how bias enters into datasets as they are collected and labeled. In particular, future work should focus on the following three areas. First, we expect that some biases emerge at the point of data collection. Some studies sampled tweets using small, ad hoc sets of keywords created by the authors (Waseem and Hovy, 2016;<cite> Waseem, 2016</cite>; Golbeck et al., 2017) , an approach demonstrated to produce poor results (King et al., 2017) .",
  "y": "uses background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_8",
  "x": "In particular, we need to consider whether the linguistic markers we use to identify potentially abusive language may be associated with language used by members of protected categories. For example, although started with thousands of terms from the Hatebase lexicon, AAE is over-represented in the dataset (Waseem et al., 2018) because some keywords associated with this speech community were used more frequently on Twitter than other keywords in the lexicon and were consequentially over-sampled. Second, we expect that the people who annotate data have their own biases. Since individual biases in reflect societal prejudices, they aggregate into systematic biases in training data. The datasets considered here relied upon a range of different annotators, from the authors (Golbeck et al., 2017; Waseem and Hovy, 2016) and crowdworkers Founta et al., 2018) to activists<cite> (Waseem, 2016)</cite> . Even the classifier trained on expert-labeled data<cite> (Waseem, 2016)</cite> flags black-aligned tweets as sexist at almost twice the rate of white-aligned tweets. While we agree that there is value in working with domain-experts to annotate data, these results suggest that activists may be prone to similar biases as academics and crowdworkers.",
  "y": "extends differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_0",
  "x": "---------------------------------- **INTRODUCTION** Extracting entities (Florian et al., 2006 (Florian et al., , 2010 and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013 ) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004) . Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011) , predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; <cite>Miwa and Bansal, 2016</cite>; . End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task dependencies to be modeled explicitly for entity recognition. As a result, it gives better relation extraction accuracies compared to pipelines.",
  "y": "motivation"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_1",
  "x": "Extracting entities (Florian et al., 2006 (Florian et al., , 2010 and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013 ) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004) . Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011) , predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; <cite>Miwa and Bansal, 2016</cite>; . End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task dependencies to be modeled explicitly for entity recognition. As a result, it gives better relation extraction accuracies compared to pipelines. <cite>Miwa and Bansal (2016)</cite> were among the first to use neural networks for end-to-end relation extraction, showing highly promising results. In particular, <cite>they</cite> used bidirectional LSTM (Graves et al., 2013) to learn hidden word representations under a sentential context, and further leveraged treestructured LSTM (Tai et al., 2015) to encode syntactic information, given the output of a parser.",
  "y": "background"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_2",
  "x": "As a result, it gives better relation extraction accuracies compared to pipelines. <cite>Miwa and Bansal (2016)</cite> were among the first to use neural networks for end-to-end relation extraction, showing highly promising results. In particular, <cite>they</cite> used bidirectional LSTM (Graves et al., 2013) to learn hidden word representations under a sentential context, and further leveraged treestructured LSTM (Tai et al., 2015) to encode syntactic information, given the output of a parser. The resulting representations are then used for making local decisions for entity and relation extraction incrementally, leading to much improved results compared with the best statistical model (Li and Ji, 2014) . This demonstrates the strength of neural representation learning for end-to-end relation extraction. On the other hand, <cite>Miwa and Bansal (2016)</cite> 's model is trained locally, without considering structural correspondences between incremental decisions. This is unlike existing statistical methods, which utilize well-studied structured prediction methods to address the problem (Li and Ji, 2014; Miwa and Sasaki, 2014) .",
  "y": "motivation background"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_3",
  "x": "We address this potential issue by building a structural neural model for end-to-end relation extraction, following a recent line of efforts on globally optimized models for neural structured prediction (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016; Wiseman and Rush, 2016) . In particular, we follow Miwa and Sasaki (2014) , casting the task as an end-to-end tablefilling problem. This is different from the actionbased method of Li and Ji (2014) , yet has shown to be more flexible and accurate (Miwa and Sasaki, 2014) . We take a different approach to representation learning, addressing two potential limitations of <cite>Miwa and Bansal (2016)</cite> . First, <cite>Miwa and Bansal (2016)</cite> rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008) . However, parsing errors can lead to encoding inaccuracies of tree-LSTMs, thereby hurting relation extraction potentially. We take an alternative approach to integrating syntactic information, by taking the hidden LSTM layers of a bi-affine attention parser (Dozat and Manning, 2016) to augment input representations.",
  "y": "motivation"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_4",
  "x": "We address this potential issue by building a structural neural model for end-to-end relation extraction, following a recent line of efforts on globally optimized models for neural structured prediction (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016; Wiseman and Rush, 2016) . In particular, we follow Miwa and Sasaki (2014) , casting the task as an end-to-end tablefilling problem. This is different from the actionbased method of Li and Ji (2014) , yet has shown to be more flexible and accurate (Miwa and Sasaki, 2014) . We take a different approach to representation learning, addressing two potential limitations of <cite>Miwa and Bansal (2016)</cite> . First, <cite>Miwa and Bansal (2016)</cite> rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008) . However, parsing errors can lead to encoding inaccuracies of tree-LSTMs, thereby hurting relation extraction potentially. We take an alternative approach to integrating syntactic information, by taking the hidden LSTM layers of a bi-affine attention parser (Dozat and Manning, 2016) to augment input representations.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_5",
  "x": "However, parsing errors can lead to encoding inaccuracies of tree-LSTMs, thereby hurting relation extraction potentially. We take an alternative approach to integrating syntactic information, by taking the hidden LSTM layers of a bi-affine attention parser (Dozat and Manning, 2016) to augment input representations. Pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly represent parsing decisions, thereby avoiding potential issues caused by incorrect parses. Our method is also free from a particular syntactic formalism, such as dependency grammar, constituent grammar or combinatory categorial grammar, requiring only hidden representations on word that contain syntactic information. In contrast, the method of <cite>Miwa and Bansal (2016)</cite> must consider tree LSTM formulations that are specific to grammar formalisms, which can be structurally different (Tai et al., 2015) . Second, <cite>Miwa and Bansal (2016)</cite> did not explicitly learn the representation of segments when predicting entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in several studies (Wang and Chang, 2016; . We take the LSTM-Minus method of Wang and Chang (2016) , modelling a segment as the difference between its last and first LSTM hidden vectors.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_6",
  "x": "Second, <cite>Miwa and Bansal (2016)</cite> did not explicitly learn the representation of segments when predicting entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in several studies (Wang and Chang, 2016; . We take the LSTM-Minus method of Wang and Chang (2016) , modelling a segment as the difference between its last and first LSTM hidden vectors. This method is highly efficient, yet gives as accurate results as compared to more complex neural network structures to model a span of words (Cross and Huang, 2016) . Evaluation on two benchmark datasets shows that our method outperforms previous methods of <cite>Miwa and Bansal (2016)</cite> , Li and Ji (2014) and Miwa and Sasaki (2014) , giving the best reported results on both benchmarks. Detailed analysis shows that our integration of syntactic features is as effective as traditional approaches based on discrete parser outputs. We make our code publicly As shown in Figure 1 , the goal of relation extraction is to mine relations from raw texts. It consists of two sub-tasks, namely entity detection, which recognizes valid entities, and relation classification, which determines the relation categories over entity pairs.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_7",
  "x": "Evaluation on two benchmark datasets shows that our method outperforms previous methods of <cite>Miwa and Bansal (2016)</cite> , Li and Ji (2014) and Miwa and Sasaki (2014) , giving the best reported results on both benchmarks. Detailed analysis shows that our integration of syntactic features is as effective as traditional approaches based on discrete parser outputs. We make our code publicly As shown in Figure 1 , the goal of relation extraction is to mine relations from raw texts. It consists of two sub-tasks, namely entity detection, which recognizes valid entities, and relation classification, which determines the relation categories over entity pairs. We follow recent studies and recognize entities and relations as one single task. ---------------------------------- **METHOD**",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_8",
  "x": "We follow Miwa and Sasaki (2014) and , treating relation extraction as a tablefilling problem, performing entity detection and relation classification using a single incremental model, which is similar in spirit to <cite>Miwa and Bansal (2016)</cite> by performing the task end-to-end. Formally, given a sentence w 1 w 2 \u00b7 \u00b7 \u00b7 w n , we maintain a table T n\u00d7n , where T (i, j) denotes the relation between w i and w j . When i = j, T (i, j) denotes an entity boundary label. We map entity words into labels under the BILOU (Begin, Inside, Last, Outside, Unit) scheme, assuming that there are no overlapping entities in one sentence (Li and Ji, 2014; Miwa and Sasaki, 2014; <cite>Miwa and Bansal, 2016</cite>) . Only the upper triangular table is necessary for indicating the relations. We adopt the close-first left-to-right order (Miwa and Sasaki, 2014) to map the twodimensional table into a sequence, in order to fill the table incrementally. As shown in Figure 2 , first {T (i, i)} are filled by growing i, and then the sequence {T (i, i + 1)} is filled, and then {T (i, i + 2)}, \u00b7 \u00b7 \u00b7 , {T (i, i + n)} are filled incrementally, until the table is fully annotated.",
  "y": "similarities"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_9",
  "x": "Formally, given a sentence w 1 w 2 \u00b7 \u00b7 \u00b7 w n , we maintain a table T n\u00d7n , where T (i, j) denotes the relation between w i and w j . When i = j, T (i, j) denotes an entity boundary label. We map entity words into labels under the BILOU (Begin, Inside, Last, Outside, Unit) scheme, assuming that there are no overlapping entities in one sentence (Li and Ji, 2014; Miwa and Sasaki, 2014; <cite>Miwa and Bansal, 2016</cite>) . Only the upper triangular table is necessary for indicating the relations. We adopt the close-first left-to-right order (Miwa and Sasaki, 2014) to map the twodimensional table into a sequence, in order to fill the table incrementally. As shown in Figure 2 , first {T (i, i)} are filled by growing i, and then the sequence {T (i, i + 1)} is filled, and then {T (i, i + 2)}, \u00b7 \u00b7 \u00b7 , {T (i, i + n)} are filled incrementally, until the table is fully annotated. During the table-filling process, we take two label sets for entity detection (i = j) and relation Table- filling example, where numbers indicate the filling order.",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_10",
  "x": "At the ith step, we determine the label l i of the next table slot based on the current hypothesis T i\u22121 . Following <cite>Miwa and Bansal (2016)</cite> , we use a neural network to learn the vector representation of T i\u22121 , and then use Equation 1 to rank candidate next labels. There are two types of input features, including the word sequence w 1 w 2 \u00b7 \u00b7 \u00b7 w n , and the readily filled label sequence l 1 l 2 \u00b7 \u00b7 \u00b7 l i\u22121 . We build a neural network to represent T i\u22121 . ---------------------------------- **WORD REPRESENTATION** Shown in Figure 3 , we represent each word w i by a vector h w i using its word form, POS tag and characters.",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_11",
  "x": "**WORD REPRESENTATION** Shown in Figure 3 , we represent each word w i by a vector h w i using its word form, POS tag and characters. Two different forms of embeddings are used based on the word form, one being obtained by using a randomly initialized look-up table E w , 2 We remove the illegal table-filling labels during decoding for training and testing. For example, tuned during training and represented by e w , and the other being a pre-trained external word embedding from E w , which is fixed and represented by e w . 3 For a POS tag t, its embedding e t is obtained from a look-up table E t similar to E w . The above two components have also been used by <cite>Miwa and Bansal (2016)</cite> . We further enhance the word representation by using its character sequence Lample et al., 2016) , taking a convolution neural network (CNN) to derive a character-based word representation h char , which has been demonstrated effective for several NLP tasks (dos Santos and Gatti, 2014) .",
  "y": "extends"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_12",
  "x": "A word's entity boundary label embedding e l is obtained by Figure 4: Segment representation. using a randomly initialized looking-up table E l . ---------------------------------- **LSTM FEATURES** We follow <cite>Miwa and Bansal (2016)</cite> , learning global context representations using LSTMs. Three basic LSTM structures are used: a leftto-right word LSTM (",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_13",
  "x": "Three basic LSTM structures are used: a leftto-right word LSTM ( and a left-to-right entity boundary label LSTM ( \u2212 \u2212\u2212\u2212 \u2192 LSTM e ). Each LSTM derives a sequence of hidden vectors for inputs. For example, for Different from <cite>Miwa and Bansal (2016)</cite> , who use the output hidden vectors {h i } of LSTMs to represent words, we exploit segment representations as well. In particular, for a segment of text [i, j] , the representation is computed by using LSTM-Minus (Wang and Chang, 2016) , shown by Figure 4 , where h j \u2212 h i\u22121 in a left-to-right LSTM and h i \u2212 h j+1 in a right-to-left LSTM are used to represent the segment [i, j]. The segment representations can reflect entities in a sentence, and thus can be potentially useful for both entity detection and relation extraction.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_14",
  "x": "First, we extract six feature vectors from the three basic LSTMs, three of which are word features, namely h , K&G (2016) refers to the best parser of Kiperwasser and Goldberg (2016) , D&M (2016) refers to Dozat and Manning (2016) , and LAS (labeled attachment score) is the major evaluation metric. tity label LSTM, we only use the segment features of entity i and entity j . ---------------------------------- **SYNTACTIC FEATURES** Previous work has shown that syntactic features are useful for relation extraction (Zhou et al., 2005) . For example, the shortest dependency path has been used by several relation extraction models (Bunescu and Mooney, 2005; <cite>Miwa and Bansal, 2016</cite>) . Here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures.",
  "y": "background"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_15",
  "x": "tity label LSTM, we only use the segment features of entity i and entity j . ---------------------------------- **SYNTACTIC FEATURES** Previous work has shown that syntactic features are useful for relation extraction (Zhou et al., 2005) . For example, the shortest dependency path has been used by several relation extraction models (Bunescu and Mooney, 2005; <cite>Miwa and Bansal, 2016</cite>) . Here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures. In particular, we take state-of-the-art syntactic parsers that use encoder-decoder neural models (Buys and Blunsom, 2015; Kiperwasser and Goldberg, 2016) , where the encoder represents the syntactic features of the input sentences.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_16",
  "x": "Table 1 shows the encoder structures of three state-of-the-art dependency parsers. Our method is to leverage trained syntactic parsers, dumping the encoder feature representations given our inputs, using them directly as part of input embeddings in our proposed model. Denoting the dumped syntactic features on each word as h In this paper, we exploit the parser of Dozat and Manning (2016) , since it achieves the current best performance for dependency parsing. Our method can be easily generalized to other parsers, which are potentially useful for our task as well. For example, we can use a constituent parser in the same way by dumping the implicit encoder features. Our exploration of syntactic features has two main advantages over the method of <cite>Miwa and Bansal (2016)</cite> , where dependency path LSTMs are used for relation classification. On the one hand, incorrect dependency paths between entity pairs can propagate to relation classification in <cite>Miwa and Bansal (2016)</cite> , because these paths rely on explicit discrete outputs from a syntactic parser.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_17",
  "x": "Denoting the dumped syntactic features on each word as h In this paper, we exploit the parser of Dozat and Manning (2016) , since it achieves the current best performance for dependency parsing. Our method can be easily generalized to other parsers, which are potentially useful for our task as well. For example, we can use a constituent parser in the same way by dumping the implicit encoder features. Our exploration of syntactic features has two main advantages over the method of <cite>Miwa and Bansal (2016)</cite> , where dependency path LSTMs are used for relation classification. On the one hand, incorrect dependency paths between entity pairs can propagate to relation classification in <cite>Miwa and Bansal (2016)</cite> , because these paths rely on explicit discrete outputs from a syntactic parser. Our method can avoid the problem since we do not compute parser outputs. On the other hand, the computation complexity is largely reduced by using our method since sequential LSTMs are based on inputs only, while the dependency path LSTMs should be computed based on the dynamic entity detection outputs.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_18",
  "x": "**LOCAL OPTIMIZATION** Previous work (<cite>Miwa and Bansal, 2016</cite>; trains model parameters by modeling each step for labeling one input sentence separately. Given a partial table T , its neural representation h T is first obtained, and then compute the next label scores {l 1 , l 2 , \u00b7 \u00b7 \u00b7 , l s } using Equation 1. The output scores are regularized into a probability distribution {p l 1 , p l 2 , \u00b7 \u00b7 \u00b7 , p ls } by using a softmax layer. The training objective is to minimize the cross-entropy loss between this output distribution with the gold-standard distribution: where l g i is the gold-standard next label for T , and \u0398 is the set of all model parameters. We refer this training method as local optimization, because it maximizes the score of the gold-standard label at each step locally.",
  "y": "background"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_19",
  "x": "**DATA AND EVALUATION** We evaluate the proposed model on two datasets, namely the ACE05 data and the corpus of Roth and Yih (2004) (CONLL04) , respectively. The ACE05 dataset defines seven coarse-grained entity types and six coarse-grained relation categories, while the CONLL04 dataset defines four entity types and five relation categories. For the ACE05 dataset, we follow Li and Ji (2014) and <cite>Miwa and Bansal (2016)</cite> , splitting and preprocessing the dataset into training, development and test sets. 5 For the CONLL04 dataset, we follow Miwa and Sasaki (2014) to split the data into training and test corpora, and then divide 10% of the training corpus for development. We use the micro F1-measure as the major metric to evaluate model performances, treating an entity as correct when its head region and type are both correct, 6 and regard a relation as correct when the argument entities and the relation category are all correct. We exploit pairwise t-test for measuring significance values.",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_20",
  "x": "We select the best iteration number according to the development results. In particular, we exploit pre-training techniques (Wiseman and Rush, 2016 ) to learn better model parameters. For the local model, we follow <cite>Miwa and Bansal (2016)</cite> , training parameters only for entity detection during the first 20 iterations. For the global model, we pretrain our model using local optimization for 40 iterations, before conducting beam global optimization. ---------------------------------- **DEVELOPMENT EXPERIMENTS** We conduct several development experiments on the ACE05 development dataset.",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_21",
  "x": "---------------------------------- **FEATURE ABLATION TESTS** We consider the baseline system with no syntactic features using local training. Compared with <cite>Miwa and Bansal (2016)</cite> features for entity detection. Feature ablation experiments are conducted for the two types of features. Table 3 shows the experimental results, which demonstrate that the character-level features and the segment features we use are both useful for relation extraction. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_22",
  "x": "Table 3 shows the experimental results, which demonstrate that the character-level features and the segment features we use are both useful for relation extraction. ---------------------------------- **LOCAL V.S. GLOBAL TRAINING** We study the influence of training strategies for relation extraction without using syntactic features. For the local model, we apply scheduled sampling (Bengio et al., 2015) , which has been shown to improve the performance of relation extraction by <cite>Miwa and Bansal (2016)</cite> . Table 4 shows the results. Scheduled sampling achieves improved F-measure scores for the local model.",
  "y": "background"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_23",
  "x": "Thus we exploit a beam size of 5 for global training considering both performance and efficiency. ---------------------------------- **SYNTACTIC FEATURES** We examine the effectiveness of the proposed implicit syntactic features. Table 5 shows the development results using both local and global optimization. The proposed features improve the relation performances significantly under both settings (p < 10 \u22124 ), demonstrating that our use of syntactic features is highly effective. We also compare our feature integration method with the traditional methods based on syntactic outputs which <cite>Miwa and Bansal (2016)</cite> and all previous methods use.",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_24",
  "x": "We use the same parser of Dozat and Manning (2016) , building features on its dependency outputs. We exploit the bidirectional tree LSTM of to extract neural features, and then exploit a nonlinear feed-forward neural network to combine the two features. Similarly, we extract segment features but by using max pooling instead over the sequential outputs of the feed-forward layer, since the vector minus is nonsense here. The final relation results are 53.1% and 53.9% for the local and global models, respectively, which have no significantly differences compared with our models. On the other hand, our method is relatively more efficient, and flexible to the grammar formalism. <cite>Miwa and Bansal (2016)</cite> , who exploit end-to-end LSTM neural networks with local optimization, and L&J (2014) and M&S (2014) refer to Li and Ji (2014) and Miwa and Sasaki (2014) , respectively, which are both globally optimized models using discrete features, giving the top F-scores among statistical models. 7 Overall, neural models give better performances than statistical models, and global optimization can give improved performances as well.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_25",
  "x": "We verify this by examining the sentence-level accuracies, where one sentence is regarded as correct when all the labels in the resulted table are correct. Figure 6 shows the result, which is consistent with our intuition. The sentence-level accuracies of the globally normalized model are consistently better than the local model. In addition, the accuracy decreases sharply as the sentence length increases, with the local model suffering more severely from larger sentences. To understand the effectiveness of the proposed syntactic features, we examine the relation Fscores with respect to entity distances. <cite>Miwa and Bansal (2016)</cite> exploit the shortest dependency path, which can make the distance between two entities closer compared with their sequential dis-tance, thus facilitating relation extraction. We verify whether the proposed syntactic features can benefit our model similarly.",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_26",
  "x": "**RELATED WORK** Entity recognition (Florian et al., 2004 (Florian et al., , 2006 Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016) . Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Yih, 2004, 2007) . Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; <cite>Miwa and Bansal, 2016</cite>; , and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging Lample et al., 2016) , parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) , relation classification Vu et al., 2016; <cite>Miwa and Bansal, 2016</cite>) and sentiment analysis .",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_27",
  "x": "The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016) . Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Yih, 2004, 2007) . Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; <cite>Miwa and Bansal, 2016</cite>; , and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging Lample et al., 2016) , parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) , relation classification Vu et al., 2016; <cite>Miwa and Bansal, 2016</cite>) and sentiment analysis . Based on the output of LSTM structures, Wang and Chang (2016) introduce segment features, and apply it to dependency parsing. The same method is applied to constituent parsing by Cross and Huang (2016) .",
  "y": "background"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_0",
  "x": "ED is a crucial component in the overall task of event extraction, which also involves event argument discovery. Recent systems for event extraction have employed either a pipeline architecture with separate classifiers for trigger and argument labeling (Ji and Grishman, 2008; Gupta and Ji, 2009 ; Patwardhan 1 https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ english-events-guidelines-v5. 4.3.pdf and Rilof, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a) or a joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b;<cite> Li et al., 2013b</cite>; Venugopal et al., 2014) . Both approaches have coped with the ED task by elaborately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers etc) to extract these features to be fed into statistical classifiers. Although this approach has achieved the top performance (Hong et al., 2011;<cite> Li et al., 2013b)</cite> , it suffers from at least two issues: (i) The choice of features is a manual process and requires linguistic intuition as well as domain expertise, implying additional studies for new application domains and limiting the capacity to quickly adapt to these new domains. (ii) The supervised NLP toolkits and resources for feature extraction might involve errors (either due to the imperfect nature or the performance loss of the toolkits on new domains (Blitzer et al., 2006; Daum\u00e9 III, 2007; McClosky et al., 2010) ), probably propagated to the final event detector.",
  "y": "background"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_1",
  "x": "4.3.pdf and Rilof, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a) or a joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b;<cite> Li et al., 2013b</cite>; Venugopal et al., 2014) . Both approaches have coped with the ED task by elaborately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers etc) to extract these features to be fed into statistical classifiers. Although this approach has achieved the top performance (Hong et al., 2011;<cite> Li et al., 2013b)</cite> , it suffers from at least two issues: (i) The choice of features is a manual process and requires linguistic intuition as well as domain expertise, implying additional studies for new application domains and limiting the capacity to quickly adapt to these new domains. (ii) The supervised NLP toolkits and resources for feature extraction might involve errors (either due to the imperfect nature or the performance loss of the toolkits on new domains (Blitzer et al., 2006; Daum\u00e9 III, 2007; McClosky et al., 2010) ), probably propagated to the final event detector. This paper presents a convolutional neural network (LeCun et al., 1988; Kalchbrenner et al., 2014) for the ED task that automatically learns features from sentences, and minimizes the dependence on supervised toolkits and resources for features, thus alleviating the error propagation and improving the performance for this task. Due to the emerging interest of the NLP community in deep learning recently, CNNs have been studied extensively and applied effectively in various tasks: semantic parsing (Yih et al., 2014) , search query retrieval (Shen et al., 2014) , semantic matching (Hu et al., 2014) , sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 2014), name tagging and semantic role labeling (Collobert et al., 2011) , relation classification and extraction (Zeng et al., 2014; Nguyen and Grishman, 2015) .",
  "y": "motivation"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_2",
  "x": "We formalize the event detection problem as a multi-class classification problem. Given a sentence, for every token in that sentence, we want to predict if the current token is an event trigger: i.e, does it express some event in the pre-defined event set or not<cite> (Li et al., 2013b)</cite> ? The current token along with its context in the sentence constitute an event trigger candidate or an example in multiclass classification terms. In order to prepare for the CNNs, we limit the context to a fixed window size by trimming longer sentences and padding shorter sentences with a special token when necessary. Let 2w + 1 be the fixed window size, and x = [x \u2212w , x \u2212w+1 , . . . , x 0 , . . . , x w\u22121 , x w ] be some trigger candidate where the current token is positioned in the middle of the window (token x 0 ). Before entering the CNNs, each token x i is transformed into a real-valued vector by looking up the following embedding tables to capture different characteristics of the token: -Word Embedding Table ( initialized by some pre-trained word embeddings): to capture the hidden semantic and syntactic properties of the tokens (Collobert and Weston, 2008; Turian et al., 2010) .",
  "y": "uses background"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_3",
  "x": "The window size for triggers is set to 31 while the dimensionality of the position embeddings and entity type embeddings is 50 3 .We inherit the values for the other parameters from Kim (2014) , i.e, the dropout rate \u03c1 = 0.5, the mini-batch size = 50, the hyperparameter for the l 2 norms = 3. Finally, we employ the pre-trained word embeddings word2vec with 300 dimensions from Mikolov et al. (2013) for initialization. We evaluate the presented CNN over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaning 529 documents (14,849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010;<cite> Li et al., 2013b)</cite> . The ACE 2005 corpus has 33 event subtypes that, along with one class \"None\" for the non-trigger tokens, constitutes a 34-class classification problem. In order to evaluate the effectiveness of the position embeddings and the entity type embeddings, Table 1 reports the performance of the proposed CNN on the development set when these embeddings are either included or excluded from the systems. With the large margins of performance, it is very clear from the table that the position embeddings are crucial while the entity embeddings are also very useful for CNNs on ED.",
  "y": "uses"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_4",
  "x": "The state-of-the-art systems for event detection on the ACE 2005 dataset have followed the traditional feature-based approach with rich hand-designed feature sets, and statistical classifiers such as MaxEnt and perceptron for structured prediction in a joint architecture (Hong et al., 2011;<cite> Li et al., 2013b)</cite> . In this section, we compare the proposed CNNs with these state-of-the-art systems on the blind test set. Table 2 presents the overall performance of the systems with gold-standard entity mention and type information 4 . As we can see from the table, considering the systems that only use sentence level information, CNN1 significantly outperforms the MaxEnt classifier as well as the joint beam search with local features from <cite>Li et al. (2013b)</cite> (an improvement of 1.6% in F1 score), and performs comparably with the joint beam search approach using both local and global features<cite> (Li et al., 2013b)</cite> . This is remarkable since CNN1 does not require any external features 5 , in contrast to the other featurebased systems that extensively rely on such external features to perform well. More interestingly, when the entity type information is incorporated into CNN1, we obtain CNN2 that still only needs sentence level information but achieves the stateof-the-art performance for this task (an improvement of 1.5% over the best system with only sentence level information<cite> (Li et al., 2013b)</cite> ). Except for CNN1, all the systems reported in Table 2 employ the gold-standard (perfect) entities mentions and types from manual annotation which might not be available in reality.",
  "y": "background"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_5",
  "x": "In this section, we compare the proposed CNNs with these state-of-the-art systems on the blind test set. Table 2 presents the overall performance of the systems with gold-standard entity mention and type information 4 . As we can see from the table, considering the systems that only use sentence level information, CNN1 significantly outperforms the MaxEnt classifier as well as the joint beam search with local features from <cite>Li et al. (2013b)</cite> (an improvement of 1.6% in F1 score), and performs comparably with the joint beam search approach using both local and global features<cite> (Li et al., 2013b)</cite> . This is remarkable since CNN1 does not require any external features 5 , in contrast to the other featurebased systems that extensively rely on such external features to perform well. More interestingly, when the entity type information is incorporated into CNN1, we obtain CNN2 that still only needs sentence level information but achieves the stateof-the-art performance for this task (an improvement of 1.5% over the best system with only sentence level information<cite> (Li et al., 2013b)</cite> ). Except for CNN1, all the systems reported in Table 2 employ the gold-standard (perfect) entities mentions and types from manual annotation which might not be available in reality. Table 3 compares the performance of CNN1 and the feature-based systems in a more realistic setting, where entity mentions and types are acquired from an automatic high-performing name tagger and information extraction system<cite> (Li et al., 2013b)</cite> .",
  "y": "differences"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_6",
  "x": "Table 2 presents the overall performance of the systems with gold-standard entity mention and type information 4 . As we can see from the table, considering the systems that only use sentence level information, CNN1 significantly outperforms the MaxEnt classifier as well as the joint beam search with local features from <cite>Li et al. (2013b)</cite> (an improvement of 1.6% in F1 score), and performs comparably with the joint beam search approach using both local and global features<cite> (Li et al., 2013b)</cite> . This is remarkable since CNN1 does not require any external features 5 , in contrast to the other featurebased systems that extensively rely on such external features to perform well. More interestingly, when the entity type information is incorporated into CNN1, we obtain CNN2 that still only needs sentence level information but achieves the stateof-the-art performance for this task (an improvement of 1.5% over the best system with only sentence level information<cite> (Li et al., 2013b)</cite> ). Except for CNN1, all the systems reported in Table 2 employ the gold-standard (perfect) entities mentions and types from manual annotation which might not be available in reality. Table 3 compares the performance of CNN1 and the feature-based systems in a more realistic setting, where entity mentions and types are acquired from an automatic high-performing name tagger and information extraction system<cite> (Li et al., 2013b)</cite> . Note that CNN1 is eligible for this comparison as it does not utilize any external features, thus avoiding usage of the name tagger and the information extraction system to identify entity mentions and types.",
  "y": "differences"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_7",
  "x": "Table 3 compares the performance of CNN1 and the feature-based systems in a more realistic setting, where entity mentions and types are acquired from an automatic high-performing name tagger and information extraction system<cite> (Li et al., 2013b)</cite> . Note that CNN1 is eligible for this comparison as it does not utilize any external features, thus avoiding usage of the name tagger and the information extraction system to identify entity mentions and types. ---------------------------------- **DOMAIN ADAPTATION EXPERIMENT** In this section, we aim to further compare the proposed CNNs with the feature-based systems under the domain adaptation setting for event detection. The ultimate goal of domain adaptation research is to develop techniques taking training 5 External features are the features generated from the supervised NLP modules and manual resources such as parsers, name tagger, entity mention extractors (either automatic or manual), gazetteers etc. Methods F Sentence level in Ji and Grishman (2008) 59.7 MaxEnt with local features in <cite>Li et al. (2013b)</cite> 64.7 Joint beam search with local features in <cite>Li et al. (2013b)</cite> 63.7",
  "y": "uses"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_9",
  "x": "First, rather than relying on the symbolic and concrete forms (i.e words, types etc) to construct features as the traditional feature-based systems (Ji and Grishman, 2008;<cite> Li et al., 2013b)</cite> do, CNNs automatically induce their features from word embeddings, the general distributed representation of words that is shared across domains. This helps CNNs mitigate the lexical sparsity, learn more general and effective feature representation for trigger candidates, and thus bridge the gap between domains. Second, as CNNs minimize the reliance on the supervised pre-processing toolkits for features, they can alleviate the error Table 4 : In-domain (first column) and Out-of-domain Performance (columns two to four). Cells marked with \u2020designate CNN models that significantly outperform (p < 0.05) all the reported feature-based methods on the specified domain. propagation and be more robust to domain shifts. ---------------------------------- **DATASET**",
  "y": "differences"
 },
 {
  "id": "59a7c1fffdd45f8e152d060a4b9f50_0",
  "x": "We tokenize and truecase all of the corpora using code released with Moses (Koehn et al., 2007) 1 . Table 1 lists the number of sentences in the training and test sets for each task. 1 mosesdecoder/scripts/ ---------------------------------- **RTM PREDICTION MODELS AND OPTIMIZATION** We present results using support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Sch\u00f6lkopf, 2004) for sentence and document translation prediction tasks and Global Linear Models (GLM) (Collins, 2002) with dynamic learning (GLMd) <cite>(Bi\u00e7ici, 2013</cite>; Bi\u00e7ici and Way, 2014) for word-level translation performance prediction. We also use these learning models after a feature subset selection (FS) with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009 ), or PLS after FS (FS+PLS).",
  "y": "uses"
 },
 {
  "id": "59a7c1fffdd45f8e152d060a4b9f50_1",
  "x": "where \u03a6 returns a global representation for instance i and the weights are updated by \u03b1, which dynamically decays the amount of the change during weight updates at later stages and prevents large fluctuations with updates. The learning rate updates the weight values with weights in the range [a, b] using the following function taking error rate as the input: Learning rate curve for a = 0.5 and b = 1.0 is provided in Figure 2: ---------------------------------- **TRAINING RESULTS** We use mean absolute error (MAE), relative absolute error (RAE), root mean squared error (RMSE), and correlation (r) as well as relative MAE (MAER) and relative RAE (MRAER) to evaluate (Bi\u00e7ici, 2015;<cite> Bi\u00e7ici, 2013)</cite> . MAER is mean absolute error relative to the magnitude of the target and MRAER is mean absolute error relative to the absolute error of a predictor always predicting the target mean assuming that target mean is known (Bi\u00e7ici, 2015 2012) calculates the average quality difference between the top n\u22121 quartiles and the overall quality for the test set.",
  "y": "uses"
 },
 {
  "id": "59a7c1fffdd45f8e152d060a4b9f50_2",
  "x": "We obtain the rankings by sorting according to the predicted scores and randomly assigning ranks in case of ties. RTMs with FS followed by PLS and learning with SVR is able to achieve the top rank in this task. Task 2: Prediction of Word-level Translation Quality Task 2 is about binary classification of word-level quality. We develop individual RTM models for each subtask and use GLMd model <cite>(Bi\u00e7ici, 2013</cite>; Bi\u00e7ici and Way, 2014) , for predicting the quality at the word-level. The results on the test set are in Table 5 where the ranks are out of about 17 submissions. RTMs with GLMd becomes the second best system this task. Task 3: Predicting METEOR of Document Translations Task 3 is about predicting ME-TEOR (Lavie and Agarwal, 2007) and their ranking.",
  "y": "uses"
 },
 {
  "id": "59a7c1fffdd45f8e152d060a4b9f50_3",
  "x": "**RTMS ACROSS TASKS AND YEARS** We compare the difficulty of tasks according to MRAER levels achieved. In Table 6 , we list the RTM test results for tasks and subtasks that predict HTER or METEOR from QET15, QET14 (Bi\u00e7ici and Way, 2014) , and QET13<cite> (Bi\u00e7ici, 2013)</cite> . The best results when predicting HTER are obtained this year. ---------------------------------- **CONCLUSION** Referential translation machines achieve top performance in automatic, accurate, and language independent prediction of document-, sentence-, and word-level statistical machine translation (SMT) performance.",
  "y": "uses"
 },
 {
  "id": "59a7c1fffdd45f8e152d060a4b9f50_4",
  "x": "In Table 6 , we list the RTM test results for tasks and subtasks that predict HTER or METEOR from QET15, QET14 (Bi\u00e7ici and Way, 2014) , and QET13<cite> (Bi\u00e7ici, 2013)</cite> . The best results when predicting HTER are obtained this year. ---------------------------------- **CONCLUSION** Referential translation machines achieve top performance in automatic, accurate, and language independent prediction of document-, sentence-, and word-level statistical machine translation (SMT) performance. RTMs remove the need to access any SMT system specific information or prior knowledge of the training data or models used when generating the translations. RTMs achieve top performance when predicting translation performance.",
  "y": "uses"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_0",
  "x": "However, in a long and winding meeting, participants might not have these earlier decisions at the forefront of their minds, and so an accurate and succinct reminder, as provided by a real-time decision detector, could potentially be very useful. A record of earlier decisions could also help users to identify outstanding issues for discussion, and to therefore make better use of the remainder of the meeting. Our approach to decision detection uses an annotation scheme which distinguishes between different types of utterance based on the roles which they play in the decision-making process. Such a scheme facilitates the detection of decision discussions<cite> (Fern\u00e1ndez et al., 2008)</cite> , and by indicating which utterances contain particular types of information, it also aids their summarization. To automatically detect decision discussions, we use what we refer to as hierarchical classification. Here, independent binary sub-classifiers detect the different decision dialogue acts, and then based on the sub-classifier hypotheses, a super-classifier determines which dialogue regions are decision discussions. In this paper then, we address the challenges for applying this approach in real-time, and produce a system which is able to detect decisions soon after they are made, (for example within a minute).",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_1",
  "x": "Only very recent research has specifically investigated the automatic detection of decisions, namely (Hsueh and Moore, 2007) and<cite> (Fern\u00e1ndez et al., 2008)</cite> . Hsueh and Moore (2007) used the AMI Meeting Corpus, and attempted to automatically identify dialogue acts (DAs) in meeting transcripts which are \"decision-related\". Within any meeting, the authors decided which DAs were decision-related based on two different kinds of manually created summary: the first was an extractive summary of the whole meeting, and the second, an abstractive summary of the decisions which were made. Those DAs in the extractive summary which support any of the decisions in the abstractive summary were manually tagged as decision-related. Hsueh and Moore (2007) then trained a Maximum Entropy classifier to recognize this single DA class, using a variety of lexical, prosodic, dialogue act and conversational topic features. They achieved an F-score of 0.35, which gives an indication of the difficulty of this task. Unlike Hsueh and Moore (2007),<cite> Fern\u00e1ndez et al. (2008)</cite> made an attempt at modelling the structure of decision-making dialogue.",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_2",
  "x": "They achieved an F-score of 0.35, which gives an indication of the difficulty of this task. Unlike Hsueh and Moore (2007),<cite> Fern\u00e1ndez et al. (2008)</cite> made an attempt at modelling the structure of decision-making dialogue. They designed an annotation scheme that takes account of the different roles which different utterances play in the decision-making process -for example, their scheme distinguishes between decision DAs (DDAs) which initiate a discussion by raising a topic/issue, those which propose a resolution, and those which express agreement for a proposed resolution and cause it to be accepted as a decision. The authors applied the annotation scheme to a portion of the AMI corpus, and then took what they refer to as a hierarchical classification approach in order to automatically identify decision discussions and their component DAs. Here, one binary Support Vector Machine (SVM) per DDA class hypothesized occurrences of that DDA class, and then based on the hypotheses of these socalled sub-classifiers, a super-classifier, (a further SVM), determined which regions of dialogue represented decision discussions. This approach produced better results than the kind of \"flat classification\" approach pursued by Hsueh and Moore (2007) where a single classifier looks for examples of a single decision-related DA class. Using manual transcripts, and a variety of lexical, utterance, speaker, DA and prosodic features for the sub-classifiers, the super-classifier's F1-score was 0.58 according to a lenient match metric.",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_3",
  "x": "Using manual transcripts, and a variety of lexical, utterance, speaker, DA and prosodic features for the sub-classifiers, the super-classifier's F1-score was 0.58 according to a lenient match metric. Note that (Purver et al., 2007) had previously pursued the same basic approach as<cite> Fern\u00e1ndez et al. (2008)</cite> in order to detect action items. While both Hsueh and Moore (2007), and<cite> Fern\u00e1ndez et al. (2008)</cite> attempted off-line decision detection, in this paper, we attempt real-time decision detection. We take the same basic approach as<cite> Fern\u00e1ndez et al. (2008)</cite> , and make changes to its implementation so that it can work effectively in real-time. ---------------------------------- **DATA** The AMI corpus , is a freely available corpus of multi-party meetings containing both audio and video recordings, as well as a wide range of annotated information including dialogue acts and topic segmentation.",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_4",
  "x": "Note that (Purver et al., 2007) had previously pursued the same basic approach as<cite> Fern\u00e1ndez et al. (2008)</cite> in order to detect action items. While both Hsueh and Moore (2007), and<cite> Fern\u00e1ndez et al. (2008)</cite> attempted off-line decision detection, in this paper, we attempt real-time decision detection. We take the same basic approach as<cite> Fern\u00e1ndez et al. (2008)</cite> , and make changes to its implementation so that it can work effectively in real-time. ---------------------------------- **DATA** The AMI corpus , is a freely available corpus of multi-party meetings containing both audio and video recordings, as well as a wide range of annotated information including dialogue acts and topic segmentation. Conversations are all in English, but participants can include non-native English speakers.",
  "y": "differences"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_5",
  "x": "The authors applied the annotation scheme to a portion of the AMI corpus, and then took what they refer to as a hierarchical classification approach in order to automatically identify decision discussions and their component DAs. Here, one binary Support Vector Machine (SVM) per DDA class hypothesized occurrences of that DDA class, and then based on the hypotheses of these socalled sub-classifiers, a super-classifier, (a further SVM), determined which regions of dialogue represented decision discussions. This approach produced better results than the kind of \"flat classification\" approach pursued by Hsueh and Moore (2007) where a single classifier looks for examples of a single decision-related DA class. Using manual transcripts, and a variety of lexical, utterance, speaker, DA and prosodic features for the sub-classifiers, the super-classifier's F1-score was 0.58 according to a lenient match metric. Note that (Purver et al., 2007) had previously pursued the same basic approach as<cite> Fern\u00e1ndez et al. (2008)</cite> in order to detect action items. While both Hsueh and Moore (2007), and<cite> Fern\u00e1ndez et al. (2008)</cite> attempted off-line decision detection, in this paper, we attempt real-time decision detection. We take the same basic approach as<cite> Fern\u00e1ndez et al. (2008)</cite> , and make changes to its implementation so that it can work effectively in real-time.",
  "y": "differences"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_6",
  "x": "The real-time ASR transcripts for the 17 meetings contain a total of 8440 utterances/dialogue acts, (around 496 per meeting), and the off-line ASR transcripts, 7495 utterances/dialogue acts, (around 441 per meeting). ---------------------------------- **MODELLING DECISION DISCUSSIONS** We use the same annotation scheme as<cite> (Fern\u00e1ndez et al., 2008</cite> ) in order to model decision-making dialogue. As stated in Section 2, this scheme distinguishes between a small number of dialogue act types based on the role which they perform in the formulation of a decision. Recall that using this scheme in conjunction with hierarchical classification produced better decision detection than a \"flat classification\" approach with a single \"decision-related\" DA class. Since it indicates which utterances contain particular types of information, such a scheme also aids the summarization of decision discussions.",
  "y": "uses"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_7",
  "x": "---------------------------------- **EXPERIMENTAL DATA FOR REAL-TIME DECISION DETECTION** Originally, two individuals used the annotation scheme described above in order to annotate the manual transcripts of 9 and 10 meetings respectively. The annotators overlapped on two meetings, and their kappa inter-annotator agreement ranged from 0.63 to 0.73 for the four DDA classes. (2007) are part of the AMI corpus, and are for the manual transcriptions. The reader can find a comparison between these annotations and our own manual transcript annotations in<cite> (Fern\u00e1ndez et al., 2008)</cite> . After obtaining the new off-line and real-time ASR transcripts, we transferred the DDA annotations from the manual transcripts.",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_9",
  "x": "Evaluation can be made at three levels: 1. The sub-classifiers' detection of each of the DDA classes. 2. The sub-classifiers' detection of each of the DDA classes after correction by the superclassifier. 3. The super-classifier's detection of decision discussion regions. For 1 and 2, we use the same lenient-match metric as<cite> (Fern\u00e1ndez et al., 2008</cite>; Hsueh and Moore, 2007) , which allows a margin of 20 seconds preceding and following a hypothesized DDA. Note that here we only give credit for hypotheses based on a 1-1 mapping with the gold-standard labels. For 3, we follow<cite> (Fern\u00e1ndez et al., 2008</cite>; Purver et al., 2007) and use a windowed metric that divides the dialogue into 30-second windows and evaluates on a per window basis.",
  "y": "uses"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_10",
  "x": "1. The sub-classifiers' detection of each of the DDA classes. 2. The sub-classifiers' detection of each of the DDA classes after correction by the superclassifier. 3. The super-classifier's detection of decision discussion regions. For 1 and 2, we use the same lenient-match metric as<cite> (Fern\u00e1ndez et al., 2008</cite>; Hsueh and Moore, 2007) , which allows a margin of 20 seconds preceding and following a hypothesized DDA. Note that here we only give credit for hypotheses based on a 1-1 mapping with the gold-standard labels. For 3, we follow<cite> (Fern\u00e1ndez et al., 2008</cite>; Purver et al., 2007) and use a windowed metric that divides the dialogue into 30-second windows and evaluates on a per window basis. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_11",
  "x": "The F1-scores for the real-time and off-line decision super-classifiers are .54 and .55 respectively, and the difference is not statistically significant. This may indicate that the hierarchical classification approach is fairly robust to increasing ASR Word Error Rates (WERs). Combining the output from each of the independent sub-classifiers might compensate somewhat for any decreases in their individual accuracy, as there was here for the I and RP sub-classifiers. The hierarchical real-time detector's F1-score is also 10 points higher than a flat classifier (.54 vs. .44). Hence, while<cite> Fern\u00e1ndez et al. (2008)</cite> demonstrated that the hierarchical classification approach could improve off-line decision detection, we have demonstrated here that it can also improve realtime decision detection. Table 5 shows the results when an off-line detector is applied to real-time ASR transcripts. Here, the super-classifier obtains an F1-score of .55, one point higher than the real-time detector, but again, the difference is not statistically significant.",
  "y": "differences"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_12",
  "x": "Predictive words for Resolutions also include semantically-related words which are key in defining the decision (\"kinetic\",\"green\"). Additional predictive words for RPs are the personal pronouns \"I\" and \"we\", and the verbs, \"think\" and \"like\", and for RRs, words which we would associate with summing up (\"consensus\", \"definitely\", and \"okay\"). Unsurprisingly, for Agreements, \"yeah\" and \"okay\" both score very highly. 7 Conclusion<cite> (Fern\u00e1ndez et al., 2008)</cite> described an approach to decision detection in multi-party meetings and demonstrated how it could work relatively well in an off-line system. The approach has two defining characteristics. The first is its use of an annotation scheme which distinguishes between different utterance types based on the roles which they play in the decision-making process. The second is its use of hierarchical classification, whereby binary sub-classifiers detect instances of each of the decision DAs (DDAs), and then based on the sub-classifier hypotheses, a super-classifier determines which regions of dialogue are decision discussions.",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_13",
  "x": "The approach has two defining characteristics. The first is its use of an annotation scheme which distinguishes between different utterance types based on the roles which they play in the decision-making process. The second is its use of hierarchical classification, whereby binary sub-classifiers detect instances of each of the decision DAs (DDAs), and then based on the sub-classifier hypotheses, a super-classifier determines which regions of dialogue are decision discussions. In this paper then, we have taken the same basic approach to decision detection as<cite> Fern\u00e1ndez et al. (2008)</cite> , but changed the way in which it is implemented so that it can work effectively in realtime. Our implementation changes include running the detector at regular and frequent intervals during the meeting, and reprocessing recent utterances in case a decision discussion straddles these and brand new utterances. The fact that the detector reprocesses utterances means that on consecutive runs, overlapping and duplicate hypothesized decision discussions are possible. We have therefore added facilities to merge overlapping hypotheses and to remove duplicates.",
  "y": "differences background"
 },
 {
  "id": "5c63296c36cbd95e07f05f2563a2a1_0",
  "x": "---------------------------------- **INTRODUCTION** Named Entity Recognition (NER) is designed to extract entities such as location and product from texts. The results are used in sophisticated tasks including summarizations and recommendations. In the past several years, sequential neural models such as long-short term memory (LSTM) have been applied to NER. They have outperformed the conventional models (Huang et al., 2015) . Recently, Convolutional Neural Network (CNN) was introduced into many models for extracting sub-word information from a word (Santos and Guimaraes, 2015;<cite> Ma and Hovy, 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "5c63296c36cbd95e07f05f2563a2a1_1",
  "x": "Above all, BLSTM-CNNs-CRF<cite> (Ma and Hovy, 2016</cite> ) achieved state-of-theart performance on the standard English corpus: CoNLL2003 (Tjong Kim Sang and De Meulder, 2003) . Character-based Neural Models: Kuru et al. (2016) proposed a character-based neural model. This model, which inputs only characters, exhibits good performance on the condition that no external knowledge is used. This model predicts a tag for each character and forces that predicted tags in a word are the same. Therefore, it is unsuitable for languages in which boundary conflicts occur. Japanese NER: For Japanese NER, many models using conventional algorithms have been proposed (Iwakura, 2011; Sasano and Kurohashi, 2008) . Most such models are character-based models to deal with boundary conflicts.",
  "y": "background"
 },
 {
  "id": "5c63296c36cbd95e07f05f2563a2a1_2",
  "x": "Word lengths of Japanese language tend to be shorter than those of English. The average word length in entities in CoNLL 2003 (Reuters news service) is 6.43 characters. That in the Mainichi newspaper corpus is 1.95. Therefore, it is difficult to extract sub-word information in Japanese in a manner that is suitable for English. ---------------------------------- **NER MODELS 4.1 WORD-BASED NEURAL MODEL** In this study, we specifically examine BLSTMCNNs-CRF<cite> (Ma and Hovy, 2016)</cite> because it achieves state-of-the-art performance in the CoNLL 2003 corpus.",
  "y": "motivation"
 },
 {
  "id": "5c63296c36cbd95e07f05f2563a2a1_3",
  "x": "In this study, we specifically examine BLSTMCNNs-CRF<cite> (Ma and Hovy, 2016)</cite> because it achieves state-of-the-art performance in the CoNLL 2003 corpus. Figure 1 presents the architecture of this model. This word-based model combines CNN, BLSTM, and CRF layers. We describe each layer of this model as the following. CNN Layer: This layer is aimed at extracting subword information. The inputs are character embeddings of a word. This layer consists of convolution and pooling layers. The convolution layer produces a matrix for a word with consideration of the sub-word. The pooling layer compresses the matrix for each dimension of character embedding. BLSTM Layer: BLSTM (Graves and Schmidhuber, 2005 ) is an approach to treat sequential data. The output of CNN and word embedding are concatenated as an input of BLSTM. CRF Layer: This layer was designed to select the best tag sequence from all possible tag sequences with consideration of outputs from BLSTM and correlations between adjacent tags. This layer introduces a transition score for each transition pattern between adjacent tags. The objective function is calculated using the sum of the outputs from BLSTM and the transition scores for a sequence.",
  "y": "background"
 },
 {
  "id": "5c63296c36cbd95e07f05f2563a2a1_4",
  "x": "Seven years (1995-1996 and 1998-2002) of Mainichi newspaper articles which include almost 500 million words are used for pre-training. We conduct parameter tuning using the development dataset. We choose the unit number of LSTM as 300, the size of word embedding as 500, that of character embedding as 50, the maximum epoch as 20, and the batch size as 60. We use Adam (Kingma and Ba, 2014) , with the learning rate of 0.001 for optimization. We use MeCab (Kudo, 2005) for word segmentation. Other conditions are the same as those reported for an earlier study<cite> (Ma and Hovy, 2016)</cite> . Table 2 : F1 score of each models.",
  "y": "uses"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_0",
  "x": "We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one. ---------------------------------- **INTRODUCTION** Fine-grained opinion mining aims to detect structured user opinions in text, which has drawn much attention in the natural language processing (NLP) community <cite>(Kim and Hovy, 2006</cite>; Breck et al., 2007; Ruppenhofer et al., 2008; Wilson et al., 2009; Qiu et al., 2011; Cardie, 2013, 2014; Liu et al., 2015; Wiegand et al., 2016) . A structured opinion includes the key arguments of one opinion, such as expressions, holders and targets (Breck et al., 2007; Cardie, 2012, 2013; Katiyar and Cardie, 2016 ).",
  "y": "background"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_1",
  "x": "Fine-grained opinion mining aims to detect structured user opinions in text, which has drawn much attention in the natural language processing (NLP) community <cite>(Kim and Hovy, 2006</cite>; Breck et al., 2007; Ruppenhofer et al., 2008; Wilson et al., 2009; Qiu et al., 2011; Cardie, 2013, 2014; Liu et al., 2015; Wiegand et al., 2016) . A structured opinion includes the key arguments of one opinion, such as expressions, holders and targets (Breck et al., 2007; Cardie, 2012, 2013; Katiyar and Cardie, 2016 ). Here we focus on opinion role labeling (ORL) (Marasovi\u0107 and Frank, 2018) , which identifies opinion holders and * Corresponding author. We want to resolve all issues peacefully targets assuming that the opinion expressions are given. Figure 1 shows an example of the task. The focused task behaves very similar with semantic role labeling (SRL) which identifies the core semantic roles for given predicates. Earlier work attempts to exploit a well-trained SRL model to recognize possible semantic roles for a given opinion expression, and then map the semantic roles into opinion roles <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008) .",
  "y": "background"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_2",
  "x": "The focused task behaves very similar with semantic role labeling (SRL) which identifies the core semantic roles for given predicates. Earlier work attempts to exploit a well-trained SRL model to recognize possible semantic roles for a given opinion expression, and then map the semantic roles into opinion roles <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008) . The heuristic approach is unable to obtain high performance for ORL because there are large mismatches between SRL and ORL. For example, opinion expressions are different from verb/noun predicates in SRL, and meanwhile, opinion holders and targets may not always correspond to semantic agents (ARG0) and patients (ARG1), respectively. We can exploit machine learning based method to solve the mismatching problem between ORL and SRL. With a small number of annotated ORL corpus, we can feed the SRL outputs as inputs to build a statistical model for ORL. By this way, the model can learn the consistencies and inconsistencies between SRL and ORL, arriving at a full exploration of SRL.",
  "y": "motivation"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_3",
  "x": "In addition, we compare this method with two other representative methods of SRL integration as well: one uses discrete SRL outputs as features directly for ORL and the other one exploits a multi-tasklearning (MTL) framework to benefit ORL by SRL information. Experiments are conducted on the MPQA 2.0 dataset, which is a standard benchmark for opinion mining. Results show that SRL is highly effective for ORL, which is consistent with previous findings <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008; Marasovi\u0107 and Frank, 2018) . Meanwhile, our implicit SRL-SAWR method can achieve the best ORL performance, 2.23% higher F-scores than the second best method. All the codes and datasets are released publicly available for research purpose under Apache Licence 2.0 at https://github.com/zhangmeishan/SRL4ORL. ---------------------------------- **METHOD**",
  "y": "differences"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_4",
  "x": "According to the preliminary experiments, the SRL model can reach an F-measure of 81.8%, which is comparable to the reported result (81.7%) in He et al. (2017) . Table 1 shows the final results on the test dataset. We report the overall as well as the fine-grained performance in term of opinion arguments (i.e., holder and target). Compared with the baseline system, our final SRL-SAWR model can bring significantly better results (p < 10 \u22125 under pairwise t-test). For fine-grained evaluations, the final model outperforms the baseline model consistently on opinion holders and targets. The tendencies are similar by exploiting the binary and proportional matching methods. The results show that SRL information is very helpful for ORL, which is consistent with previous studies <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008; Marasovi\u0107 and Frank, 2018) .",
  "y": "similarities"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_5",
  "x": "Results show that SRL is highly effective for ORL, which is consistent with previous findings <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008; Marasovi\u0107 and Frank, 2018) . Meanwhile, our implicit SRL-SAWR method can achieve the best ORL performance, 2.23% higher F-scores than the second best method. All the codes and datasets are released publicly available for research purpose under Apache Licence 2.0 at https://github.com/zhangmeishan/SRL4ORL. ---------------------------------- **METHOD** ---------------------------------- **BASELINE**",
  "y": "similarities uses"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_6",
  "x": "**SRL INTEGRATION** SRL aims to find the core semantic arguments for a given predicate, which is highly correlative with the ORL task. The semantic roles agent (ARG0) and patient (ARG1) are often corresponding to the opinion holder and target, respectively. Several works even directly transfer semantic roles into opinion roles for ORL <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008) , treating opinion expressions as the major predicates. These systems can achieve good performances, indicating that SRL information can be greatly useful for ORL. Here we propose a novel method to encode the SRL information implicitly, enhancing ORL model with semantic-aware word representations from a neural SRL model (SRL-SAWR). Figure 3 shows the overall architectures of our SRL integration method.",
  "y": "background"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_7",
  "x": "SRL aims to find the core semantic arguments for a given predicate, which is highly correlative with the ORL task. The semantic roles agent (ARG0) and patient (ARG1) are often corresponding to the opinion holder and target, respectively. Several works even directly transfer semantic roles into opinion roles for ORL <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008) , treating opinion expressions as the major predicates. These systems can achieve good performances, indicating that SRL information can be greatly useful for ORL. Here we propose a novel method to encode the SRL information implicitly, enhancing ORL model with semantic-aware word representations from a neural SRL model (SRL-SAWR). Figure 3 shows the overall architectures of our SRL integration method. Instead of using the discrete outputs from the SRL model, the SRL-SAWR method exploits the intermediate encoder outputs as inputs for ORL, which can alleviate the problems in the above two methods.",
  "y": "extends"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_8",
  "x": "Thus SRL and ORL are highly correlative. Considering the much larger scale of annotated SRL corpora, SRL can benefit ORL potentially. According to the above findings, we design a simple system by mapping SRL outputs into ORL directly <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008) . We simply convert the semantic role ARG0 into holder, and ARG1 into target. Table 2 shows the performance. The results of the baseline system are shown for comparison. We can see that the simple mapping method is also one feasible alternative as a whole.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_0",
  "x": "---------------------------------- **INTRODUCTION** Dependency parsing is one of the mainstream research areas in natural language processing. Dependency representations are useful for a number of NLP applications, for example, machine translation (Ding and Palmer, 2005) , information extraction (Yakushiji et al., 2006) , analysis of typologically diverse languages (Bunt et al., 2010) and parser stacking (\u00d8vrelid et al., 2009 ). There were several shared tasks organized on dependency parsing (CoNLL 2006 (CoNLL -2007 and labeled dependencies (CoNLL 2008 (CoNLL -2009 ) and there were a number of attempts to compare various dependencies intrinsically, e.g. (Miyao et al., 2007) , and extrinsically, e.g. (Wu et al., 2012) . In this paper we focus on practical issues of data representation for dependency parsing. The central aspects of our discussion are (a) three dependency formats: two 'classic' representations for dependency parsing, namely, Stanford Basic (SB) and CoNLL Syntactic Dependencies (CD), and bilexical dependencies from the HPSG English Resource Grammar (ERG), so-called DELPH-IN Syntactic Derivation Tree (DT), proposed recently by Ivanova et al. (2012) ; (b) three state-of-the art statistical parsers: Malt (Nivre et al., 2007) , MST (McDonald et al., 2005) and the parser of <cite>Bohnet and Nivre (2012)</cite> ; (c) two approaches to wordcategory disambiguation, e.g. exploiting common PTB tags and using supertags (i.e. specialized ERG lexical types).",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_2",
  "x": "In the experiments described in Section 4 we used parsers that adopt different approaches and implement various algorithms. Malt (Nivre et al., 2007) : transition-based dependency parser with local learning and greedy search. MST (McDonald et al., 2005) : graph-based dependency parser with global near-exhaustive search. <cite>Bohnet and Nivre (2012)</cite> parser: transitionbased dependency parser with joint tagger that implements global learning and beam search. ---------------------------------- **DEPENDENCY SCHEMES** In this work we extract DeepBank data in the form of bilexical syntactic dependencies, DELPH-IN Syntactic Derivation Tree (DT) format.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_3",
  "x": "Supertagging for the ERG grammar is an ongoing research effort and an off-the-shelf supertagger for the ERG is not currently available. ---------------------------------- **EXPERIMENTS** In this section we give a detailed analysis of parsing into SB, CD and DT dependencies with Malt, MST and the <cite>Bohnet and Nivre (2012)</cite> parser. ---------------------------------- **SETUP** For Malt and MST we perform the experiments on gold PoS tags, whereas the <cite>Bohnet and Nivre (2012)</cite> parser predicts PoS tags during testing.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_4",
  "x": "Supertagging for the ERG grammar is an ongoing research effort and an off-the-shelf supertagger for the ERG is not currently available. ---------------------------------- **EXPERIMENTS** In this section we give a detailed analysis of parsing into SB, CD and DT dependencies with Malt, MST and the <cite>Bohnet and Nivre (2012)</cite> parser. ---------------------------------- **SETUP** For Malt and MST we perform the experiments on gold PoS tags, whereas the <cite>Bohnet and Nivre (2012)</cite> parser predicts PoS tags during testing.",
  "y": "differences"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_5",
  "x": "Prior to each experiment with Malt, we used MaltOptimizer to obtain settings and a feature model; for MST we exploited default configuration; for the <cite>Bohnet and Nivre (2012)</cite> parser we set the beam parameter to 80 and otherwise employed the default setup. With regards to evaluation metrics we use labelled attachment score (LAS), unlabeled attachment score (UAS) and label accuracy (LACC) excluding punctuation. Our results cannot be directly compared to the state-of-the-art scores on the Penn Treebank because we train on sections 0-13 and test on section 15 of WSJ. Also our results are not strictly inter-comparable because the setups we are using are different. ---------------------------------- **DISCUSSION** The results that we are going to analyze are presented in Tables 1 and 2 .",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_6",
  "x": "The results that we are going to analyze are presented in Tables 1 and 2 . Statistical significance was assessed using Dan Bikel's parsing evaluation comparator 1 at the 0.001 significance level. We inspect three different aspects in the interpretation of these results: parser, dependency format and tagset. Below we will look at these three angles in detail. From the parser perspective Malt and MST are not very different in the traditional setup with gold PTB tags (Table 1, Gold PTB tags). The <cite>Bohnet and Nivre (2012)</cite> parser outperforms Malt on CD and DT and MST on SB, CD and DT with PTB tags even though it does not receive gold PTB tags during test phase but predicts them (Table 2 , Predicted PTB tags). This is explained by the fact that the <cite>Bohnet and Nivre (2012)</cite> parser implements a novel approach to parsing: beam-search algorithm with global structure learning.",
  "y": "background"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_7",
  "x": "**DISCUSSION** The results that we are going to analyze are presented in Tables 1 and 2 . Statistical significance was assessed using Dan Bikel's parsing evaluation comparator 1 at the 0.001 significance level. We inspect three different aspects in the interpretation of these results: parser, dependency format and tagset. Below we will look at these three angles in detail. From the parser perspective Malt and MST are not very different in the traditional setup with gold PTB tags (Table 1, Gold PTB tags). The <cite>Bohnet and Nivre (2012)</cite> parser outperforms Malt on CD and DT and MST on SB, CD and DT with PTB tags even though it does not receive gold PTB tags during test phase but predicts them (Table 2 , Predicted PTB tags).",
  "y": "differences"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_8",
  "x": "This leads to the lower scores of parsing accuracy for MST. For the <cite>Bohnet and Nivre (2012)</cite> parser the complexity of supertag prediction has significant negative influence on the attachment and labeling accuracies ( Table 2 , Predicted supertags). The addition of gold PTB tags as a feature lifts the performance of the <cite>Bohnet and Nivre (2012)</cite> parser to the level of performance of Malt and MST on CD with gold supertags and Malt on SB with gold supertags (compare Table 2 , Predicted supertags + gold PTB, and Table 1 , Gold supertags). Both Malt and MST benefit slightly from the combination of gold PTB tags and gold supertags (Table 1 , Gold PTB tags + gold supertags). For the <cite>Bohnet and Nivre (2012)</cite> parser we also observe small rise of accuracy when gold supertags are provided as a feature for prediction of PTB tags (compare Predicted PTB tags and Predicted PTB tags + gold supertags sections of Table 2 ). The parsers have different running times: it takes minutes to run an experiment with Malt, about 2 hours with MST and up to a day with the <cite>Bohnet and Nivre (2012)</cite> parser. From the point of view of the dependency format, SB has the highest LACC and CD is first-rate on UAS for all three parsers in most of the configurations (Tables 1 and 2 ).",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_9",
  "x": "For the <cite>Bohnet and Nivre (2012)</cite> parser the complexity of supertag prediction has significant negative influence on the attachment and labeling accuracies ( Table 2 , Predicted supertags). The addition of gold PTB tags as a feature lifts the performance of the <cite>Bohnet and Nivre (2012)</cite> parser to the level of performance of Malt and MST on CD with gold supertags and Malt on SB with gold supertags (compare Table 2 , Predicted supertags + gold PTB, and Table 1 , Gold supertags). Both Malt and MST benefit slightly from the combination of gold PTB tags and gold supertags (Table 1 , Gold PTB tags + gold supertags). For the <cite>Bohnet and Nivre (2012)</cite> parser we also observe small rise of accuracy when gold supertags are provided as a feature for prediction of PTB tags (compare Predicted PTB tags and Predicted PTB tags + gold supertags sections of Table 2 ). The parsers have different running times: it takes minutes to run an experiment with Malt, about 2 hours with MST and up to a day with the <cite>Bohnet and Nivre (2012)</cite> parser. From the point of view of the dependency format, SB has the highest LACC and CD is first-rate on UAS for all three parsers in most of the configurations (Tables 1 and 2 ). This means that SB is easier to label and CD is easier to parse structurally.",
  "y": "extends"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_10",
  "x": "This parser exploits context features \"POS tag of each intervening word between head and dependent\" (McDonald et al., 2006) . Due to the far larger size of the supertag set compared to the PTB tagset, such features are sparse and have low frequencies. This leads to the lower scores of parsing accuracy for MST. For the <cite>Bohnet and Nivre (2012)</cite> parser the complexity of supertag prediction has significant negative influence on the attachment and labeling accuracies ( Table 2 , Predicted supertags). The addition of gold PTB tags as a feature lifts the performance of the <cite>Bohnet and Nivre (2012)</cite> parser to the level of performance of Malt and MST on CD with gold supertags and Malt on SB with gold supertags (compare Table 2 , Predicted supertags + gold PTB, and Table 1 , Gold supertags). Both Malt and MST benefit slightly from the combination of gold PTB tags and gold supertags (Table 1 , Gold PTB tags + gold supertags). For the <cite>Bohnet and Nivre (2012)</cite> parser we also observe small rise of accuracy when gold supertags are provided as a feature for prediction of PTB tags (compare Predicted PTB tags and Predicted PTB tags + gold supertags sections of Table 2 ).",
  "y": "extends"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_11",
  "x": "This leads to the lower scores of parsing accuracy for MST. For the <cite>Bohnet and Nivre (2012)</cite> parser the complexity of supertag prediction has significant negative influence on the attachment and labeling accuracies ( Table 2 , Predicted supertags). The addition of gold PTB tags as a feature lifts the performance of the <cite>Bohnet and Nivre (2012)</cite> parser to the level of performance of Malt and MST on CD with gold supertags and Malt on SB with gold supertags (compare Table 2 , Predicted supertags + gold PTB, and Table 1 , Gold supertags). Both Malt and MST benefit slightly from the combination of gold PTB tags and gold supertags (Table 1 , Gold PTB tags + gold supertags). For the <cite>Bohnet and Nivre (2012)</cite> parser we also observe small rise of accuracy when gold supertags are provided as a feature for prediction of PTB tags (compare Predicted PTB tags and Predicted PTB tags + gold supertags sections of Table 2 ). The parsers have different running times: it takes minutes to run an experiment with Malt, about 2 hours with MST and up to a day with the <cite>Bohnet and Nivre (2012)</cite> parser. From the point of view of the dependency format, SB has the highest LACC and CD is first-rate on UAS for all three parsers in most of the configurations (Tables 1 and 2 ).",
  "y": "differences"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_12",
  "x": "Unique one-tomany correspondence holds only for possessive wh-pronoun and punctuation. Thus, supertags do not provide extra level of detalization for PTB tags, but PTB tags and supertags are complementary. As discussed in section 3.4, they contain bits of information that are different. For this reason their combination results in slight increase of accuracy for all three parsers on all dependency formats (Table 1 , Gold PTB tags + gold supertags, and Table 2 , Predicted PTB + gold supertags and Predicted supertags + gold PTB). The <cite>Bohnet and Nivre (2012)</cite> parser predicts supertags with an average accuracy of 89.73% which is significantly lower than state-ofthe-art 95% (Ytrest\u00f8l, 2011) . When we consider punctuation in the evaluation, all scores raise significantly for DT and at the same time decrease for SB and CD for all three parsers. This is explained by the fact that punctuation in DT is always attached to the nearest token which is easy to learn for a statistical parser.",
  "y": "differences"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_13",
  "x": "This is explained by the fact that punctuation in DT is always attached to the nearest token which is easy to learn for a statistical parser. ---------------------------------- **ERROR ANALYSIS** Using the CoNLL-07 evaluation script 2 on our test set, for each parser we obtained the error rate distribution over CPOSTAG on SB, CD and DT. VBP, VBZ and VBG. VBP (verb, non-3rd person singular present), VBZ (verb, 3rd person singular present) and VBG (verb, gerund or present participle) are the PTB tags that have error rates in 10 highest error rates list for each parser (Malt, MST and the <cite>Bohnet and Nivre (2012)</cite> parser) with each dependency format (SB, CD and DT) and with each PoS tag set (PTB PoS and supertags) when PTB tags are included as CPOSTAG feature. We automatically collected all sentences that contain 1) attachment errors, 2) label errors, 3) attachment and label errors for VBP, VBZ and VBG made by Malt parser on DT format with PTB PoS. For each of these three lexical categories we manually analyzed a random sample of sentences with errors and their corresponding gold-standard versions.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_14",
  "x": "Sentences with coordination are particularly difficult for the correct attachment and labeling of the VBP (see Figure 2 for an example). Coordination. The error rate of Malt, MST and the <cite>Bohnet and Nivre (2012)</cite> parser for the coordination is not so high for SB and CD ( 1% and 2% correspondingly with MaltParser, PTB tags) whereas for DT the error rate on the CPOSTAGS is especially high (26% with MaltParser, PTB tags). It means that there are many errors on incoming dependency arcs for coordinating conjunctions when parsing DT. On outgoing arcs parsers also make more mistakes on DT than on SB and CD. This is related to the difference in choice of annotation principle (see Figure 1) . As it was shown in (Schwartz et al., 2012) , it is harder to parse coordination headed by coordinating conjunction.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_15",
  "x": "On outgoing arcs parsers also make more mistakes on DT than on SB and CD. This is related to the difference in choice of annotation principle (see Figure 1) . As it was shown in (Schwartz et al., 2012) , it is harder to parse coordination headed by coordinating conjunction. Although the approach used in DT is harder for parser to learn, it has some advantages: using SB and CD annotations, we cannot distinguish the two cases illustrated with the sentences (a) and (b): a) The fight is putting a tight squeeze on profits of many, threatening to drive the smallest ones out of business and straining relations between the national fast-food chains and their franchisees. b) Proceeds from the sale will be used for remodelling and reforbishing projects, as well as for the planned MGM Grand hotel/casino and theme park. In the sentence a) \"the national fast-food\" refers only to the conjunct \"chains\", while in the sentence b) \"the planned\" refers to both conjuncts and \"MGM Grand\" refers only to the first conjunct. The <cite>Bohnet and Nivre (2012)</cite> parser succeeds in finding the correct conjucts (shown in bold font) on DT and makes mistakes on SB and CD in some difficult cases like the following ones: a) <. . .",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_16",
  "x": "> investors hoard gold and help underpin its price <. . . > b) Then take the expected return and subtract one standard deviation. CD and SB wrongly suggest \"gold\" and \"help\" to be conjoined in the first sentence and \"return\" and \"deviation\" in the second. ---------------------------------- **CONCLUSIONS AND FUTURE WORK** In this survey we gave a comparative experimental overview of (i) parsing three dependency schemes, viz., Stanford Basic (SB), CoNLL Syntactic Dependencies (CD) and DELPH-IN Syntactic Derivation Tree (DT), (ii) with three leading dependency parsers, viz., Malt, MST and the <cite>Bohnet and Nivre (2012)</cite> parser (iii) exploiting two different tagsets, viz., PTB tags and supertags. From the parser perspective, the <cite>Bohnet and Nivre (2012)</cite> parser performs better than Malt and MST not only on conventional formats but also on the new representation, although this parser solves a harder task than Malt and MST.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_17",
  "x": "CD and SB wrongly suggest \"gold\" and \"help\" to be conjoined in the first sentence and \"return\" and \"deviation\" in the second. ---------------------------------- **CONCLUSIONS AND FUTURE WORK** In this survey we gave a comparative experimental overview of (i) parsing three dependency schemes, viz., Stanford Basic (SB), CoNLL Syntactic Dependencies (CD) and DELPH-IN Syntactic Derivation Tree (DT), (ii) with three leading dependency parsers, viz., Malt, MST and the <cite>Bohnet and Nivre (2012)</cite> parser (iii) exploiting two different tagsets, viz., PTB tags and supertags. From the parser perspective, the <cite>Bohnet and Nivre (2012)</cite> parser performs better than Malt and MST not only on conventional formats but also on the new representation, although this parser solves a harder task than Malt and MST. From the dependency format perspective, DT appeares to be a more difficult target dependency representation than SB and CD. This suggests that the expressivity that we gain from the grammar theory (e.g. for coordination) is harder to learn with state-of-the-art dependency parsers.",
  "y": "differences"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_0",
  "x": "To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; <cite>Hoffmann et al., 2011</cite>; Surdeanu et al., 2012) exploited multi-instance learning models. Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it (Sun et al., 2011; Wang et al., 2011; Takamatsu et al., 2012) . However, their methods are handicapped by the built-in assumption that a sentence does not express a relation unless it mentions two entities which participate in the relation in the knowledge base, leading to false negatives. In reality, knowledge bases are often incomplete, giving rise to numerous false negatives in the training data. We sampled 1834 sentences that contain two entities in the New York Times 2006 corpus and manually evaluated whether they express any of a set of 50 common Freebase 1 relations. As shown in Figure 1 , of the 133 (7.3%) sentences that truly express one of these relations, only 32 (1.7%) are covered by Freebase, leaving 101 (5.5%) false negatives. Even for one of the most complete relations in Freebase, Employee-of (with more than 100,000 entity pairs), 6 out of 27 sentences with the pattern 'PERSON executive of ORGANIZATION' contain a fact that is not included in Freebase and are thus mislabeled as negative.",
  "y": "background"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_1",
  "x": "**INTRODUCTION** A recent approach for training information extraction systems is distant supervision, which exploits existing knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011) . To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; <cite>Hoffmann et al., 2011</cite>; Surdeanu et al., 2012) exploited multi-instance learning models. Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it (Sun et al., 2011; Wang et al., 2011; Takamatsu et al., 2012) . However, their methods are handicapped by the built-in assumption that a sentence does not express a relation unless it mentions two entities which participate in the relation in the knowledge base, leading to false negatives. In reality, knowledge bases are often incomplete, giving rise to numerous false negatives in the training data. We sampled 1834 sentences that contain two entities in the New York Times 2006 corpus and manually evaluated whether they express any of a set of 50 common Freebase 1 relations.",
  "y": "motivation"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_2",
  "x": "In fact, our system corrects the relation labels of the above 6 sentences before training the relation extractor. (1) matches relation instances to sentences and (2) learns a passage retrieval model to (3) provide relevance feedback on sentences; Relevant sentences (4) yield new relation instances which are added to the knowledge base; Finally, instances are again (5) matched to sentences to (6) create training data for relation extraction. Encouraged by the recent success of simple methods for coreference resolution (Raghunathan et al., 2010) and inspired by pseudo-relevance feedback (Xu and Croft, 1996; Lavrenko and Croft, 2001; Matveeva et al., 2006; Cao et al., 2008) in the field of information retrieval, which expands or reformulates query terms based on the highest ranked documents of an initial query, we propose to increase the quality and quantity of training data generated by distant supervision for information extraction task using pseudo feedback. As shown in Figure 2 , we expand an original knowledge base with possibly missing relation instances with information from the highest ranked sentences returned by a passage retrieval model trained on the same data. We use coarse features for our passage retrieval model to aggressively expand the knowledge base for maximum recall; at the same time, we exploit a multi-instance learning model with fine features for relation extraction to handle the newly introduced false positives and maintain high precision. Similar to iterative bootstrapping techniques (Yangarber, 2001) , this mechanism uses the outputs of the first trained model to expand training data for the second model, but unlike bootstrapping it does not require iteration and avoids the problem of semantic drift. We further note that iterative bootstrapping over a single distant supervision system is difficult, because state-of-the-art systems (Surdeanu et al., 2012; <cite>Hoffmann et al., 2011</cite>; Riedel et al., 2010; Mintz et al., 2009) , detect only few false negatives in the training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009) .",
  "y": "motivation background"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_3",
  "x": "(1) matches relation instances to sentences and (2) learns a passage retrieval model to (3) provide relevance feedback on sentences; Relevant sentences (4) yield new relation instances which are added to the knowledge base; Finally, instances are again (5) matched to sentences to (6) create training data for relation extraction. Encouraged by the recent success of simple methods for coreference resolution (Raghunathan et al., 2010) and inspired by pseudo-relevance feedback (Xu and Croft, 1996; Lavrenko and Croft, 2001; Matveeva et al., 2006; Cao et al., 2008) in the field of information retrieval, which expands or reformulates query terms based on the highest ranked documents of an initial query, we propose to increase the quality and quantity of training data generated by distant supervision for information extraction task using pseudo feedback. As shown in Figure 2 , we expand an original knowledge base with possibly missing relation instances with information from the highest ranked sentences returned by a passage retrieval model trained on the same data. We use coarse features for our passage retrieval model to aggressively expand the knowledge base for maximum recall; at the same time, we exploit a multi-instance learning model with fine features for relation extraction to handle the newly introduced false positives and maintain high precision. Similar to iterative bootstrapping techniques (Yangarber, 2001) , this mechanism uses the outputs of the first trained model to expand training data for the second model, but unlike bootstrapping it does not require iteration and avoids the problem of semantic drift. We further note that iterative bootstrapping over a single distant supervision system is difficult, because state-of-the-art systems (Surdeanu et al., 2012; <cite>Hoffmann et al., 2011</cite>; Riedel et al., 2010; Mintz et al., 2009) , detect only few false negatives in the training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009) . We present a reliable and novel way to address these issues and achieve significant improvement over the <cite>MULTIR</cite> system (<cite>Hoffmann et al., 2011</cite>) , increasing recall from 47.7% to 61.2% at comparable precision.",
  "y": "extends"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_4",
  "x": "We extend the learning-to-rank techniques (Liu, 2011) to distant supervision setting to create a robust passage retrieval system. While relation extraction systems exploit rich and complex features that are necessary to extract the exact relation (Mintz et al., 2009; Riedel et al., 2010; <cite>Hoffmann et al., 2011</cite>) , passage retrieval components use coarse features in order to provide different and complementary feedback to information extraction models. We exploit two types of lexical features: BagOf-Words and Word-Position. The two types of simple binary features are shown in the following example: Sentence: Apple founder Steve Jobs died. For each relation r, we assume each sentence has a binary relevance label to form distantly supervised training data: sentences in P OS(r) are relevant and sentences in N EG(r) are irrelevant. As a pointwise learning-to-rank approach (Nallapati, 2004) , the probabilities of relevance estimated by SVMs (Platt and others, 1999) are used for ranking all the sentences in the original training corpus for each relation respectively.",
  "y": "background"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_5",
  "x": "We define the positive data set P OS(r) to be the set of sentences in which any related pair of entities of relation r (according to the knowledge base) is mentioned. The negative data set RAW (r) is the rest of the training data, which contain two entities of the required types in the knowledge base, e.g. one person and one organization for the CEO-of relation in Freebase. Another negative data set with more conservative sense N EG(r) is defined as the set of sentences which contain the primary entity e1 (e.g. person in any CEO-of relation in the knowledge base) and any secondary entity e2 of required type (e.g. organization for the CEO-of relation) but the relation does not hold for this pair of entities in the knowledge base. ---------------------------------- **DISTANTLY SUPERVISED PASSAGE RETRIEVAL** We extend the learning-to-rank techniques (Liu, 2011) to distant supervision setting to create a robust passage retrieval system. While relation extraction systems exploit rich and complex features that are necessary to extract the exact relation (Mintz et al., 2009; Riedel et al., 2010; <cite>Hoffmann et al., 2011</cite>) , passage retrieval components use coarse features in order to provide different and complementary feedback to information extraction models.",
  "y": "extends"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_6",
  "x": "We identify the most likely relevant entity pairs as follows: initialize \u2206 \u2190\u2212 \u2206 for each relation type r \u2208 R do learn a passage (sentence) retrieval model L(r) using coarse features and P OS(r)\u222aN EG(r) as training data score the sentences in the RAW (r) by L(r) score the entity pairs according to the scores of sentences they are involved in select the top ranked pairs of entities, then add the relation r to their label in \u2206 end for We select the entity pairs whose average score of the sentences they are involved in is greater than p, where p is a parameter tuned on development data. 3 The relation extraction model is then trained using (\u03a3, E, R, \u2206 ) with a more complete database than the original knowledge base \u2206. ---------------------------------- **DISTANTLY SUPERVISED RELATION EXTRACTION** We use a state-of-the-art open-source system, <cite>MULTIR</cite> (<cite>Hoffmann et al., 2011</cite>) , as the relation extraction component.",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_7",
  "x": "We use a state-of-the-art open-source system, <cite>MULTIR</cite> (<cite>Hoffmann et al., 2011</cite>) , as the relation extraction component. <cite>MULTIR</cite> is based on multi-instance learning, which assumes that at least one sentence of those matching a given entity-pair contains the relation of interest (Riedel et al., 2010) in the given knowledge base to tolerate false positive noise in the training data and superior than previous models (Riedel et al., 2010; Mintz et al., 2009 ) by allowing overlapping relations. <cite>MULTIR</cite> uses features which are based on Mintz et al. (2009) and consist of conjunctions of named entity tags, syntactic dependency paths between arguments, and lexical information. ---------------------------------- **EXPERIMENTS** For evaluating extraction accuracy, we follow the experimental setup of <cite>Hoffmann et al. (2011)</cite> , and use <cite>their</cite> implementation of <cite>MULTIR 4</cite> with 50 training iterations as our baseline. Our complete system, which we call IRMIE, combines our passage retrieval component with <cite>MULTIR</cite>.",
  "y": "background"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_8",
  "x": "**EXPERIMENTS** For evaluating extraction accuracy, we follow the experimental setup of <cite>Hoffmann et al. (2011)</cite> , and use <cite>their</cite> implementation of <cite>MULTIR 4</cite> with 50 training iterations as our baseline. Our complete system, which we call IRMIE, combines our passage retrieval component with <cite>MULTIR</cite>. We use the same datasets as in <cite>Hoffmann et al. (2011)</cite> and Riedel et al. (2010) , which include 3-years of New York Times articles aligned with Freebase. The sentential extraction evaluation is performed on a small amount of manually annotated sentences, sampled from the union of matched sentences and Table 1 : Overall sentential extraction performance evaluated on the original test set of <cite>Hoffmann et al. (2011)</cite> and our corrected test set: Our proposed relevance feedback technique yields a substantial increase in recall. system predictions. We define S e as the sentences where some system extracted a relation and S F as the sentences that match the arguments of a fact in \u2206. The sentential precision and recall is computed on a randomly sampled set of sentences from S e \u222a S F , in which each sentence is manually labeled whether it expresses any relation in R. Figure 3 shows the precision/recall curves for <cite>MULTIR</cite> with and without pseudo-relevance feedback computed on the test dataset of 1000 sentence used by <cite>Hoffmann et al. (2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_9",
  "x": "For evaluating extraction accuracy, we follow the experimental setup of <cite>Hoffmann et al. (2011)</cite> , and use <cite>their</cite> implementation of <cite>MULTIR 4</cite> with 50 training iterations as our baseline. Our complete system, which we call IRMIE, combines our passage retrieval component with <cite>MULTIR</cite>. We use the same datasets as in <cite>Hoffmann et al. (2011)</cite> and Riedel et al. (2010) , which include 3-years of New York Times articles aligned with Freebase. The sentential extraction evaluation is performed on a small amount of manually annotated sentences, sampled from the union of matched sentences and Table 1 : Overall sentential extraction performance evaluated on the original test set of <cite>Hoffmann et al. (2011)</cite> and our corrected test set: Our proposed relevance feedback technique yields a substantial increase in recall. system predictions. We define S e as the sentences where some system extracted a relation and S F as the sentences that match the arguments of a fact in \u2206. The sentential precision and recall is computed on a randomly sampled set of sentences from S e \u222a S F , in which each sentence is manually labeled whether it expresses any relation in R. Figure 3 shows the precision/recall curves for <cite>MULTIR</cite> with and without pseudo-relevance feedback computed on the test dataset of 1000 sentence used by <cite>Hoffmann et al. (2011)</cite> . With the pseudo-relevance feedback from passage retrieval, IRMIE achieves significantly higher recall at a consistently high level of precision.",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_10",
  "x": "<cite>MULTIR</cite> uses features which are based on Mintz et al. (2009) and consist of conjunctions of named entity tags, syntactic dependency paths between arguments, and lexical information. ---------------------------------- **EXPERIMENTS** For evaluating extraction accuracy, we follow the experimental setup of <cite>Hoffmann et al. (2011)</cite> , and use <cite>their</cite> implementation of <cite>MULTIR 4</cite> with 50 training iterations as our baseline. Our complete system, which we call IRMIE, combines our passage retrieval component with <cite>MULTIR</cite>. We use the same datasets as in <cite>Hoffmann et al. (2011)</cite> and Riedel et al. (2010) , which include 3-years of New York Times articles aligned with Freebase. The sentential extraction evaluation is performed on a small amount of manually annotated sentences, sampled from the union of matched sentences and Table 1 : Overall sentential extraction performance evaluated on the original test set of <cite>Hoffmann et al. (2011)</cite> and our corrected test set: Our proposed relevance feedback technique yields a substantial increase in recall.",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_11",
  "x": "Our complete system, which we call IRMIE, combines our passage retrieval component with <cite>MULTIR</cite>. We use the same datasets as in <cite>Hoffmann et al. (2011)</cite> and Riedel et al. (2010) , which include 3-years of New York Times articles aligned with Freebase. The sentential extraction evaluation is performed on a small amount of manually annotated sentences, sampled from the union of matched sentences and Table 1 : Overall sentential extraction performance evaluated on the original test set of <cite>Hoffmann et al. (2011)</cite> and our corrected test set: Our proposed relevance feedback technique yields a substantial increase in recall. system predictions. We define S e as the sentences where some system extracted a relation and S F as the sentences that match the arguments of a fact in \u2206. The sentential precision and recall is computed on a randomly sampled set of sentences from S e \u222a S F , in which each sentence is manually labeled whether it expresses any relation in R. Figure 3 shows the precision/recall curves for <cite>MULTIR</cite> with and without pseudo-relevance feedback computed on the test dataset of 1000 sentence used by <cite>Hoffmann et al. (2011)</cite> . With the pseudo-relevance feedback from passage retrieval, IRMIE achieves significantly higher recall at a consistently high level of precision. At the highest recall point, IRMIE reaches 78.5% precision and 59.2% recall, for an F1 score of 68.9%.",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_12",
  "x": "**EXPERIMENTS** For evaluating extraction accuracy, we follow the experimental setup of <cite>Hoffmann et al. (2011)</cite> , and use <cite>their</cite> implementation of <cite>MULTIR 4</cite> with 50 training iterations as our baseline. Our complete system, which we call IRMIE, combines our passage retrieval component with <cite>MULTIR</cite>. We use the same datasets as in <cite>Hoffmann et al. (2011)</cite> and Riedel et al. (2010) , which include 3-years of New York Times articles aligned with Freebase. The sentential extraction evaluation is performed on a small amount of manually annotated sentences, sampled from the union of matched sentences and Table 1 : Overall sentential extraction performance evaluated on the original test set of <cite>Hoffmann et al. (2011)</cite> and our corrected test set: Our proposed relevance feedback technique yields a substantial increase in recall. system predictions. We define S e as the sentences where some system extracted a relation and S F as the sentences that match the arguments of a fact in \u2206. The sentential precision and recall is computed on a randomly sampled set of sentences from S e \u222a S F , in which each sentence is manually labeled whether it expresses any relation in R. Figure 3 shows the precision/recall curves for <cite>MULTIR</cite> with and without pseudo-relevance feedback computed on the test dataset of 1000 sentence used by <cite>Hoffmann et al. (2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_13",
  "x": "At the highest recall point, IRMIE reaches 78.5% precision and 59.2% recall, for an F1 score of 68.9%. Because the two types of lexical features used in our passage retrieval models are not used in <cite>MUL-TIR</cite>, we created another baseline MULTIRLEX by adding these features into <cite>MULTIR</cite> in order to rule out the improvement from additional information. Note that the sentences are sampled from the union of Freebase matches and sentences from which some systems in <cite>Hoffmann et al. (2011)</cite> extracted a relation. It underestimates the improvements of the newly developed systems in this paper. We therefore also created a new test set of 1000 sentences by sampling from the union of Freebase matches and sentences where MULTIR-LEX or IRMIELEX extracted a relation. Table 1 shows the overall precision and recall computed against these two test datasets, with and without adding lexical features into multi-instance learning models. The performance improvement by using pseudo-feedback is significant (p < 0.05) in McNemar's test for both datasets.",
  "y": "extends"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_14",
  "x": "The sentential extraction evaluation is performed on a small amount of manually annotated sentences, sampled from the union of matched sentences and Table 1 : Overall sentential extraction performance evaluated on the original test set of <cite>Hoffmann et al. (2011)</cite> and our corrected test set: Our proposed relevance feedback technique yields a substantial increase in recall. system predictions. We define S e as the sentences where some system extracted a relation and S F as the sentences that match the arguments of a fact in \u2206. The sentential precision and recall is computed on a randomly sampled set of sentences from S e \u222a S F , in which each sentence is manually labeled whether it expresses any relation in R. Figure 3 shows the precision/recall curves for <cite>MULTIR</cite> with and without pseudo-relevance feedback computed on the test dataset of 1000 sentence used by <cite>Hoffmann et al. (2011)</cite> . With the pseudo-relevance feedback from passage retrieval, IRMIE achieves significantly higher recall at a consistently high level of precision. At the highest recall point, IRMIE reaches 78.5% precision and 59.2% recall, for an F1 score of 68.9%. Because the two types of lexical features used in our passage retrieval models are not used in <cite>MUL-TIR</cite>, we created another baseline MULTIRLEX by adding these features into <cite>MULTIR</cite> in order to rule out the improvement from additional information. Note that the sentences are sampled from the union of Freebase matches and sentences from which some systems in <cite>Hoffmann et al. (2011)</cite> extracted a relation.",
  "y": "uses"
 },
 {
  "id": "5e6d5bb4fb5be2b18ce3256302bf28_0",
  "x": "The existing algorithms in GRE lie in two extremities. Incremental algorithm is simple and speedy but less expressive in nature whereas others are complex and exhaustive but more expressive in nature. Our prefix tree based model not only incorporates all relevant features of GRE (like describing set, generating Boolean and context sensitive description etc.) but also try to attain simplicity and speed properties of Incremental algorithm. Thus this model provides a simple and linguistically rich approach to GRE. ---------------------------------- **INTRODUCTION** Generation of referring expression (GRE) is an important task in the field of Natural Language Generation (NLG) systems (Reiter and <cite>Dale, 1995)</cite> .",
  "y": "background"
 },
 {
  "id": "5e6d5bb4fb5be2b18ce3256302bf28_1",
  "x": "When we generate any natural language text in a particular domain, it has been observed that the text is centered on certain objects for that domain. When we give introductory description of any object, we generally give full length description (e.g. \"The large black hairy dog\"). But the later references to that object tend to be shorter and only support referential communication goal of distinguishing the target from other objects. For example the expression \"The black dog\" suffices if the other dogs in the environment are all non black. Grice, an eminent philosopher of language, has stressed on brevity of referential communication to avoid conversational implicature. Dale (1992) developed Full Brevity algorithm based on this observation. It always generates shortest possible referring description to identify an object. But Reiter and<cite> Dale (1995)</cite> later proved that Full Brevity requirement is an NP-Hard task, thus computationally intractable and offered an alternative polynomial time Incremental Algorithm. This algorithm adds properties in a predetermined order, based on the observation that human speakers and audiences prefer certain kinds of properties when describing an object in a domain (Krahmer et al. 2003) .",
  "y": "background"
 },
 {
  "id": "5e6d5bb4fb5be2b18ce3256302bf28_2",
  "x": "The algorithm is analyzed for time com-plexity in section 6 and conclusion is drawn in section 7. ---------------------------------- **MODELING GRE USING TRIE STRUCTURE** In this section, it is shown how a scene can be represented using a trie data structure. The scheme is based on Incremental algorithm<cite> (Reiter and Dale 1995)</cite> and incorporates the attractive properties (e.g. speed, simplicity etc) of that algorithm. Later it is extended to take care of different refinements (like relational, boolean description etc) that could not be handled by Incremental algorithm. Reiter and<cite> Dale (1995)</cite> pointed out the notion of 'PreferredAttributes' (e.g. Type, Size, Color etc) which is a sequence of attributes of an object that human speakers generally use to identify that object from the contrast set.",
  "y": "uses"
 },
 {
  "id": "5e6d5bb4fb5be2b18ce3256302bf28_3",
  "x": "In this section, it is shown how a scene can be represented using a trie data structure. The scheme is based on Incremental algorithm<cite> (Reiter and Dale 1995)</cite> and incorporates the attractive properties (e.g. speed, simplicity etc) of that algorithm. Later it is extended to take care of different refinements (like relational, boolean description etc) that could not be handled by Incremental algorithm. Reiter and<cite> Dale (1995)</cite> pointed out the notion of 'PreferredAttributes' (e.g. Type, Size, Color etc) which is a sequence of attributes of an object that human speakers generally use to identify that object from the contrast set. We assume that the initial description of an entity is following this sequence (e.g. \"The large black dog\") then the later references will be some subset of initial description (like \"The dog\" or \"The large dog\") which is defined as the prefix of the initial description. So, we have to search for a prefix of the initial full length description so that it is adequate to distinguish the target object. Following the Incremental version we will add properties one by one from the 'PreferredAttributes' list.",
  "y": "uses"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_0",
  "x": "---------------------------------- **INTRODUCTION** Fine-grained entity typing aims to assign types (e.g., \"person\", \"politician\", etc.) to entity mentions in the local context (a single sentence), and the type set constitutes a treestructured hierarchy (i.e., type hierarchy). Recent years witness the boost of neural models in this task, e.g.,<cite> (Shimaoka et al. 2016)</cite> employs an attention based LSTM to attain sentence representations and achieves state-of-the-art performance. However, it still suffers from noise in training data, which is a main challenge in this task. The training data is generated by distant supervision, which assumes that if an entity has a type in knowledge bases (KBs), then all sentences containing this entity will express this type. This method inevitably introduces irrelevant types to the context.",
  "y": "background"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_1",
  "x": "Recent years witness the boost of neural models in this task, e.g.,<cite> (Shimaoka et al. 2016)</cite> employs an attention based LSTM to attain sentence representations and achieves state-of-the-art performance. However, it still suffers from noise in training data, which is a main challenge in this task. The training data is generated by distant supervision, which assumes that if an entity has a type in knowledge bases (KBs), then all sentences containing this entity will express this type. This method inevitably introduces irrelevant types to the context. For example, the entity \"Donald Trump\" has types \"person\", \"businessman\" and \"politician\" in KBs, thus all three types are annotated for its mentions in the training corpora. But in sentence \"Donald Trump announced his candidacy for President of US.\", only \"person\" and \"politician\" are correct types, while \"businessman\" can not be deduced from the sentence, thus serves as noise. To alleviate this issue, a few systems try to denoise training data by filtering irrelevant types ahead of training.",
  "y": "motivation background"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_2",
  "x": "Experiments on two data sets validate effectiveness of PAN. ---------------------------------- **PATH-BASED ATTENTION NEURAL MODEL** The architecture of PAN is illustrated in Figure1. Supposing that there are n sentences containing entity e, i.e., S e = {s 1 , s 2 , ..., s n }, and T e is the automatically labeled types based on KBs. Firstly PAN employs LSTM to generate representations of sentences s i following<cite> (Shimaoka et al. 2016)</cite> , where s i \u2208 R d is the semantic representation of s i , i \u2208 {1, 2, ..., n}. Afterwards, we build path-based attention \u03b1 i,t over sentences s i for each type t \u2208 T e , which is expected to focus on relevant sentences to type t. Then, the representation of sentence set S e for type t, denoted by s e,t \u2208 R d , is calculated through weighted sum of vectors of sentences. Finally, we obtain predicted types through a classification layer.",
  "y": "uses"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_3",
  "x": "Since one mention can have multiple types, we employ a classification layer consisting of N logistic classifiers, where N is the total number of types. Each classifier outputs the probability of respective type, i.e., P (t|s e,t ) = exp(w T t s e,t + b t ) 1 + exp(w T t s e,t + b t ) , where w t , b t \u2208 R d are the logistic regression parameters. To optimize the model, a multi-type loss is defined according to the cross entropy as follows, J = \u2212 e t [I t ln P (t|s e,t ) + (1 \u2212 I t ) ln(1 \u2212 P (t|s e,t ))], where I t is indicator function to indicate whether t is the annotated type of entity e, i.e., t \u2208 T e . ---------------------------------- **EXPERIMENTS AND CONCLUSION** Experiments are carried on two widely used datasets OntoNotes and FIGER(GOLD), and the training dataset of OntoNotes is noisy compared to FIGER(GOLD)<cite> (Shimaoka et al. 2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_4",
  "x": ", where w t , b t \u2208 R d are the logistic regression parameters. To optimize the model, a multi-type loss is defined according to the cross entropy as follows, J = \u2212 e t [I t ln P (t|s e,t ) + (1 \u2212 I t ) ln(1 \u2212 P (t|s e,t ))], where I t is indicator function to indicate whether t is the annotated type of entity e, i.e., t \u2208 T e . ---------------------------------- **EXPERIMENTS AND CONCLUSION** Experiments are carried on two widely used datasets OntoNotes and FIGER(GOLD), and the training dataset of OntoNotes is noisy compared to FIGER(GOLD)<cite> (Shimaoka et al. 2016)</cite> . The statistics of the datasets are listed in Table1. We employ Strict Accuracy (Acc), Loose Macro F1 (Ma-F1), and Loose Micro F1 (Mi-F1) as evaluation measures following <cite>(Shimaoka et al. 2016</cite> ).",
  "y": "uses"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_5",
  "x": "The baselines are chosen from two aspects: (1) Predicting types in a unified process using raw noisy data, i.e., TLSTM<cite> (Shimaoka et al. 2016)</cite> , and other methods shown in Table2. (2) Predicting types using clean data by denoising ahead, i.e., H PLE and F PLE (Ren et al. 2016) . To prove the superiority of path-based attention, we also directly apply the attention neural model in relation extraction (Lin et al. 2016) without using type hierarchy (AN). The results of baselines are the best results reported in their papers. We can observed that: (1) When using the same raw noisy data, PAN outperforms all methods on both data sets, which proves the anti-noise ability of PAN. (2) PAN performs better than AN, since the attention learned in PAN utilizes the hierarchical structure to enable parameter sharing. (3) The improvements on OntoNotes are higher than FIGER(GOLD), because OntoNotes is more noisy, and the hierarchical structure in OntoNotes is more complex with more layers, which further demonstrates that path-based attention does well with type hierarchy, and proves the superiority of PAN in reducing noise.",
  "y": "uses"
 },
 {
  "id": "5f25b6a3bcaca2e4beb59ce0f3eb5f_0",
  "x": "****SUDOKU: TREATING WORD SENSE DISAMBIGUATION & ENTITIY LINKING AS A DETERMINISTIC PROBLEM - VIA AN UNSUPERVISED & ITERATIVE APPROACH**** **ABSTRACT** SUDOKU's submissions to SemEval Task 13 treats Word Sense Disambiguation and Entity Linking as a deterministic problem that exploits two key attributes of open-class words as constraints -their degree of polysemy and their part of speech. This is an extension and further validation of the results achieved by <cite>Manion and Sainudiin (2014)</cite>. SUDOKU's three submissions are incremental in the use of the two aforementioned constraints. Run1 has no constraints and disambiguates all lemmas in one pass. Run2 disambiguates lemmas at increasing degrees of polysemy, leaving the most polysemous until last.",
  "y": "extends"
 },
 {
  "id": "5f25b6a3bcaca2e4beb59ce0f3eb5f_2",
  "x": "For this task the author proposes an iterative approach which makes several passes based on a set of constraints. For a more formal distinction between the conventional and iterative approach to WSD, please refer to this paper<cite> (Manion and Sainudiin, 2014</cite> Table 1 : Parts of Speech disambiguated (as percentages) for each SemEval Task (denoted by its year). In-Degree Centrality as implemented in<cite> (Manion and Sainudiin, 2014)</cite> observes F-Score improvement (F + \u2206F) by applying the iterative approach. The author found in the investigations of his thesis (Manion, 2014) that the iterative approach performed best on the SemEval 2013 Multilingual WSD Task (Navigli et al., 2013) , as opposed to earlier tasks such as SensEval 2004 English All Words WSD Task (Snyder and Palmer, 2004) and the SemEval 2010 All Words WSD task on a Specific Domain (Agirre et al., 2010) . While these earlier tasks also experienced improvement, F-Scores remained lower overall. Table 1 Depicted above are distributions for each domain and language, detailing the probability (y-axis) of specific parts of speech at increasing degrees of polysemy (x-axis). These distributions were produced from the gold keys (or synsets) of the test documents by querying BabelNet for the polysemy of each word.",
  "y": "background"
 },
 {
  "id": "5f25b6a3bcaca2e4beb59ce0f3eb5f_3",
  "x": "Table 1 Depicted above are distributions for each domain and language, detailing the probability (y-axis) of specific parts of speech at increasing degrees of polysemy (x-axis). These distributions were produced from the gold keys (or synsets) of the test documents by querying BabelNet for the polysemy of each word. Each distribution was normalised with one sense per discourse assumed, therefore duplicate synsets were ignored. Lastly the difference in F-Score between the conventional Run1 and the iterative Run2 and Run3 is listed beside each distribution. Firstly WSD tasks before 2013 generally relied on only a lexicon, such as WordNet (Fellbaum, 1998) or an alternative equivalent, whereas SemEval 2013 Task 12 WSD and this task (Moro and Navigli, 2015) included Entity Linking (EL) using the encyclopaedia Wikipedia via BabelNet (Navigli and Ponzetto, 2012) . Secondly, as shown by <cite>Manion and Sainudiin (2014)</cite> with a simple linear regression, the iterative approach increases WSD performance for documents that have a higher degree of document monosemy -the percentage of unique monosemous lemmas in a document. As seen in Figures 1(a) to (i) on the previous page, named entities (or unique rather than common nouns) are more monosemous compared to other parts of speech, especially for more technical domains.",
  "y": "background"
 },
 {
  "id": "5f25b6a3bcaca2e4beb59ce0f3eb5f_4",
  "x": "**SYSTEM DESCRIPTION & IMPLEMENTATION** Run1 (SUDOKU-1) is the conventional approachno constraints are applied. Formalised in<cite> (Manion and Sainudiin, 2014)</cite> , this run can act as a baseline to gauge any improvement for Run2 and Run3 that apply the iterative approach. Run2 (SUDOKU-2) has the constraint of words being disambiguated in order of increasing polysemy, leaving the most polysemous to last. Run3 (SUDOKU-3) is an untested and unpublished version of the iterative approach. It includes Run2's constraint plus a second constraint -that all nouns and named entities must be disambiguated before other parts of speech. For each run, a semantic subgraph is constructed from BabelNet (version 2.5.1).",
  "y": "background"
 },
 {
  "id": "5f2f4087b80aa8dc3a5ccdb686983d_0",
  "x": "This section briefly summarizes our prior work and data set. ---------------------------------- **DATA SET** For our experiments we use the dataset described in<cite> (DeVault et al., 2011b)</cite> . It contains 19 Wizard of Oz dialogues with a virtual human called Amani . The user plays the role of an Army commander whose unit has been attacked by a sniper. The user interviews Amani, who was a witness to the incident and has some information about the sniper.",
  "y": "uses"
 },
 {
  "id": "5f2f4087b80aa8dc3a5ccdb686983d_2",
  "x": "In each fold, we hold out one dialogue and use the remaining 18 as training data. To measure policy performance, we count an automatically produced system SA as correct if that SA was chosen by the original wizard or at least one external referee for that dialogue turn. We then count the proportion of the correct SAs among all the SAs produced across all 19 dialogues, and use this measure of weak accuracy to score dialogue policies. We can use the weak accuracy of one referee, measured against all the others, to establish a performance ceiling for this metric. This score is .79; see <cite>DeVault et al. (2011b)</cite> . ---------------------------------- **BASELINE SYSTEMS**",
  "y": "uses"
 },
 {
  "id": "5f5a59f8fbf999b9eecfe7c1897b2c_0",
  "x": "**INTRODUCTION** Several ensemble models have been proposed for the parsing of syntactic dependencies. These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008; Attardi and Dell'Orletta, 2009) , and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; <cite>Hall et al., 2007</cite>; Attardi and Dell'Orletta, 2009 ). In the latter case, the correctness of the final dependency tree is ensured by: (a) selecting entire trees proposed by the base parsers (Henderson and Brill, 1999) ; or (b) re-parsing the pool of dependencies proposed by the base models (Sagae and Lavie, 2006) . The latter approach was shown to perform better for constituent parsing (Henderson and Brill, 1999) . While all these models achieved good performance, the previous work has left several questions Table 2 : Scores of unsupervised combination models using different voting strategies. The combined trees are assembled using a word-by-word voting scheme.",
  "y": "background"
 },
 {
  "id": "5f5a59f8fbf999b9eecfe7c1897b2c_1",
  "x": "In the latter case, the correctness of the final dependency tree is ensured by: (a) selecting entire trees proposed by the base parsers (Henderson and Brill, 1999) ; or (b) re-parsing the pool of dependencies proposed by the base models (Sagae and Lavie, 2006) . The latter approach was shown to perform better for constituent parsing (Henderson and Brill, 1999) . While all these models achieved good performance, the previous work has left several questions Table 2 : Scores of unsupervised combination models using different voting strategies. The combined trees are assembled using a word-by-word voting scheme. parser variants are built by varying the parsing algorithm (we used three parsing models: Nivre's arceager (AE), Nivre's arc-standard (AS), and Covington's non-projective model (CN)), and the parsing direction (left to right (\u2192) or right to left (\u2190)), similar to<cite> (Hall et al., 2007)</cite> . The parameters of the Malt models were set to the values reported in<cite> (Hall et al., 2007)</cite> . The MST parser was used with the default configuration.",
  "y": "similarities"
 },
 {
  "id": "5f5a59f8fbf999b9eecfe7c1897b2c_2",
  "x": "The parameters of the Malt models were set to the values reported in<cite> (Hall et al., 2007)</cite> . The MST parser was used with the default configuration. Table 1 shows the performance of these models in the development and test partitions. ---------------------------------- **EXPERIMENTS** ---------------------------------- **ON SCORING MODELS FOR PARSER COMBINATION**",
  "y": "uses"
 },
 {
  "id": "5f5a59f8fbf999b9eecfe7c1897b2c_3",
  "x": "Obviously, a group of minority dependencies provides beneficial signal only if its precision is larger than 50%. Table 3 lists the total number of minority dependencies in groups with precision larger than 50% for all our base parsers and the most representative features. The table shows that the number of minority dependencies with useful signal is extremely low. All in all, it accounts for less than 0.7% of all dependencies in the development corpus. ---------------------------------- **ON RE-PARSING ALGORITHMS** To guarantee that the resulting dependency tree is well-formed, most previous work used the dynamic programming algorithm of Eisner (1996) for reparsing (Sagae and Lavie, 2006;<cite> Hall et al., 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_0",
  "x": "**ABSTRACT** We propose a novel tensor embedding method that can effectively extract lexical features for humor recognition. Specifically, we use wordword co-occurrence to encode the contextual content of documents, and then decompose the tensor to get corresponding vector representations. We show that this simple method can capture features of lexical humor effectively for continuous humor recognition. In particular, we achieve a distance of 0.887 on a global humor ranking task, comparable to the top performing systems from SemEval 2017 Task 6B (Potash et al., 2017) but without the need for any external training corpus. In addition, we further show that this approach is also beneficial for small sample humor recognition tasks through a semi-supervised label propagation procedure, which achieves about 0.7 accuracy on the 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun of the Day<cite> (Yang et al., 2015)</cite> humour classification datasets using only 10% of known labels. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_1",
  "x": "Recognizing humor automatically is an important step for natural human-computer interaction (Shahaf et al., 2015) . While early works tend to frame humor recognition as a binary classification task (Mihalcea and Strapparava, 2005; <cite>Yang et al., 2015)</cite> , the last few years have seen the emergence of humor recognition as a pairwise relative ranking task (Cattle and Ma, 2016; Shahaf et al., 2015) . In addition to pairwise ranking, SemEval 2017 Task 6 also includes a global ranking subtask. However, the majority of submissions build * Zhenjie Zhao and Andrew Cattle contributed equally to this work. \u2020 E. Papalexakis was supported by a UCR-China collaboration grant by the Bourns College of Engineering at UCR, and by the National Science Foundation CDSE Grant no. OAC-1808591 global rankings using a series of pairwise comparisons (Potash et al., 2017) . Only Yan and Pedersen (2017) attempt to predict global rankings directly, ranking documents inversely to their probability according to an n-gram language model. State-of-the-art humor recognition algorithms usually require a considerable amount of training data with labels to learn effective features<cite> (Yang et al., 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_2",
  "x": "While early works tend to frame humor recognition as a binary classification task (Mihalcea and Strapparava, 2005; <cite>Yang et al., 2015)</cite> , the last few years have seen the emergence of humor recognition as a pairwise relative ranking task (Cattle and Ma, 2016; Shahaf et al., 2015) . In addition to pairwise ranking, SemEval 2017 Task 6 also includes a global ranking subtask. However, the majority of submissions build * Zhenjie Zhao and Andrew Cattle contributed equally to this work. \u2020 E. Papalexakis was supported by a UCR-China collaboration grant by the Bourns College of Engineering at UCR, and by the National Science Foundation CDSE Grant no. OAC-1808591 global rankings using a series of pairwise comparisons (Potash et al., 2017) . Only Yan and Pedersen (2017) attempt to predict global rankings directly, ranking documents inversely to their probability according to an n-gram language model. State-of-the-art humor recognition algorithms usually require a considerable amount of training data with labels to learn effective features<cite> (Yang et al., 2015)</cite> . However, such data are difficult to obtain -especially fine-grained humor annotations.",
  "y": "background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_3",
  "x": "In this way, we can rank the degree of humor effectively via lexical centrality (Radev et al., 2015) , namely, regarding the distance to the lexical center as an indicator of the degree of humor. Experimental results on the SemEval 2017 Task 6 dataset (Potash et al., 2017) show that without external training data, the tensor embedding method can achieve performance equivalent to the second place on SemEval 2017 Task 6B without the need for any external training corpus. In addition, by applying a semi-supervised label propagation procedure (Zhou et al., 2003) , we can also use the tensor embedding method for small sample humor recognition, achieving about 0.7 accuracy with only 10% of known labels on the 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun of the Day<cite> (Yang et al., 2015)</cite> datasets. The contributions of this paper are: 1) we propose a tensor embedding method to model the lexical features of documents, which can capture lexical similarity effectively regardless of the size of the corpus, 2) we show that the lexical features can be used effectively for finegrained humor ranking and small sample humor recognition. Our implementation is open-sourced, and can be found at https://github.com/ zhaozj89/TensorEmbeddingNLP. ---------------------------------- **RELATED WORK**",
  "y": "motivation background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_4",
  "x": "Stylistic features include alliteration, rhyming, negative sentiment, and adult slang (Mihalcea and Strapparava, 2005) as well as emotional scenarios (Reyes et al., 2012) . Semantic features range from attempts to measure incongruity (Cattle and Ma, 2018; Shahaf et al., 2015; <cite>Yang et al., 2015)</cite> to the use of word embeddings as inputs to neural models (Bertero and Fung, 2016; Donahue et al., 2017) . Content-based approaches include word frequency (Mihalcea and Strapparava, 2005) , n-gram probability (Yan and Pedersen, 2017) , and lexical centrality (Radev et al., 2015) . Centrality is based on the observation that humorous responses to common stimuli tend to cluster around a small number of core jokes (Radev et al., 2015; Shahaf et al., 2015) , with more central documents benefiting from \"wisdom of the crowd\". While most humor features involve making population-level inferences based on document-level features, centrality is instead population-level feature directly. Radev et al. (2015) calculate their centrality feature using LexRank, a graph-based text summarization method (Erkan and Radev, 2004) . Compared with more traditional lexical similarity measures like tfidf, this method is better suited to short humor texts due to their short lengths leading to sparse vector representations (Erkan and Radev, 2004) .",
  "y": "background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_5",
  "x": "**SMALL SAMPLE HUMOR RECOGNITION** Once the humor features have been extracted, the next step is training a machine learning model to make predictions. Although learning-based methods have shown significant performance improvement recently<cite> (Yang et al., 2015)</cite> , one of their main bottlenecks is the lack of appropriate training corpora. While previous works have employed data crawled from websites (Mihalcea and Strapparava, 2005; <cite>Yang et al., 2015)</cite> , Twitter (Cattle and Ma, 2016; Reyes et al., 2012) , sitcom subtitles (Bertero and Fung, 2016; Purandare and Litman, 2006) , or the New Yorker Cartoon Caption Contest (Radev et al., 2015; Shahaf et al., 2015) , these datasets are generally not released publicly. Owing to the difficulty in obtaining fine-grained labeled humor data, it is critical to study how to recognize humor by a small training sample or even without labeled data. ---------------------------------- **METHOD**",
  "y": "differences"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_6",
  "x": "The objective of tensor decomposition is to find an approximation\u0174 of W so<cite> Yang et al. (2015)</cite> that:\u0174 where v r \u2208 R V , d r \u2208 R D , R is the predefined rank parameter, and \u2297 is the outer product, namely, v r \u2297 v r \u2297 d r being a three-dimensional tensor, and With the tensor decomposition, we can find low-rank embeddings of sentences that capture the similarity of contextual patterns (Hosseinimotlagh and Papalexakis, 2018). In particular, C = [d 1 , d 2 , . . . , d R ] \u2208 R D\u00d7R , where the s-row of C is the embedding vector of sentence s. The Euclidean distance of embeddings is used to measure the similarity of two sentences. ---------------------------------- **LEXICAL CENTRALITY** We use lexical centrality to rank the degree of humor (Radev et al., 2015) .",
  "y": "background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_7",
  "x": "Our system outperforms all but the Duluth (Yan and Pedersen, 2017) system in the official results for Se-mEval 2017 Task 6B (Potash et al., 2017) , making our performance equivalent to second place. It is notable that our system can perform well on the Broadway prompt, where other methods usually fail. Moreover, because our system does not have a learning procedure, the performance is more stable than others. ---------------------------------- **BINARY HUMOR CLASSIFICATION** To show the effectiveness of label propagation of our tensor embedding method for small sample humor recognition, we conduct an experiment on two humor classification datasets 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun of the Day<cite> (Yang et al., 2015)</cite> . Similarly, we run a grid search procedure to find optimal parameters, and set the rank as 10, window size as 5, neighbor number as 50, \u03b1 as 0.2.",
  "y": "similarities uses"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_8",
  "x": "We run a 10-fold procedure, and report the average accuracy, precision, recall, and F1 score values. The results of humor classification are shown in 1 www.tensortoolbox.org Table 2 . Our own implementation of<cite> Yang et al. (2015)</cite> is included as a baseline. While<cite> Yang et al. (2015)</cite> uses a large portion of data for training and combine different features, we find that at similar portion of training data (90%), the results of our method are comparable to it. In addition, with only a small portion of training data, our method still achieves good results. ---------------------------------- **DISCUSSION**",
  "y": "similarities uses"
 },
 {
  "id": "604807137ee5d9a6775821496c6af5_0",
  "x": "**SODA FEATURES** ---------------------------------- **SIMILARITY** In social media analytics, especially for sentiment categorization, there exist numerous collections about different products or services where labeled data is available and thus can be used to adapt to a new unlabeled collection. Given a target collection, the key question is to identify the best possible source collection to adapt from. The similarity module in SODA identifies the best adaptable source collection based on the similarity between the source and target collections. This is based on the observations from existing literature (Bhatt et al., 2015;<cite> Blitzer et al., 2007)</cite> which suggest that if the source and target collections are similar, the adaptation performance tends to be better than if the two collections are dissimilar.",
  "y": "uses"
 },
 {
  "id": "604807137ee5d9a6775821496c6af5_1",
  "x": "During generalization, it learns shared common representation<cite> (Blitzer et al., 2007</cite>; Ji et al., 2011; Pan et al., 2010) which minimizes the divergence between two collections. We leverage one of the widely used structural correspondence learning (SCL) approach<cite> (Blitzer et al., 2007)</cite> to compute shared representations. The idea adhered here is that a model learned on the shared feature representation using labeled data from the source collection will also generalize well on the target collection. Towards this, we learn a model (C S ) on the shared feature representation from the source collection, referred to as \"source classifier\". C S is then used to predict labels for the pool of unlabeled instances from the target collection, referred to as P u , using the shared representations. All instances in P u which are predicted with a confidence (\u03b1 1 ) higher than a predefined threshold (\u03b8 1 ) are moved to the pool of pseudo-labeled target instances, referred to as P s . We now learn a target domain model C T on P s using the target specific representation, referred to as \"target classifier\".",
  "y": "uses"
 },
 {
  "id": "604807137ee5d9a6775821496c6af5_2",
  "x": "We leverage one of the widely used structural correspondence learning (SCL) approach<cite> (Blitzer et al., 2007)</cite> to compute shared representations. The idea adhered here is that a model learned on the shared feature representation using labeled data from the source collection will also generalize well on the target collection. Towards this, we learn a model (C S ) on the shared feature representation from the source collection, referred to as \"source classifier\". C S is then used to predict labels for the pool of unlabeled instances from the target collection, referred to as P u , using the shared representations. All instances in P u which are predicted with a confidence (\u03b1 1 ) higher than a predefined threshold (\u03b8 1 ) are moved to the pool of pseudo-labeled target instances, referred to as P s . We now learn a target domain model C T on P s using the target specific representation, referred to as \"target classifier\". C T captures a separate view of the target instances than the shared representation and hence brings in discriminating target specific information which is useful for categorization in target collection.",
  "y": "uses"
 },
 {
  "id": "604807137ee5d9a6775821496c6af5_3",
  "x": "When there is no labeled data in the target collection, in-domain classifier can not be applied while SODA still yields good classification accuracy. Moreover, SODA consistently performs better than the in-domain classifier with same amount of labeled data. We also evaluated the performance of domain adaptation (DA) module of SODA on the Amazon review dataset<cite> (Blitzer et al., 2007)</cite> which is a benchmark dataset for sentiment categorization. It has 4 domains, namely, books(B), dvds(D), electronics(E), and kitchen(K) each with 2000 reviews divided equally into positive and negative reviews. Table 2 shows that DA module of SODA outperforms 1) a widely used domain adaptation technique , namely, structural correspondence learning (SCL)<cite> (Blitzer et al., 2007</cite>; Blitzer et al., 2006) , 2) the baseline (BL) where a classifier trained on one domain is applied on another domain, and 3) the in-domain classifier. Note that in Table 2 , the performance of DA module of SODA is reported when it does not use any labeled instances from the target domain. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "604807137ee5d9a6775821496c6af5_4",
  "x": "Moreover, SODA consistently performs better than the in-domain classifier with same amount of labeled data. We also evaluated the performance of domain adaptation (DA) module of SODA on the Amazon review dataset<cite> (Blitzer et al., 2007)</cite> which is a benchmark dataset for sentiment categorization. It has 4 domains, namely, books(B), dvds(D), electronics(E), and kitchen(K) each with 2000 reviews divided equally into positive and negative reviews. Table 2 shows that DA module of SODA outperforms 1) a widely used domain adaptation technique , namely, structural correspondence learning (SCL)<cite> (Blitzer et al., 2007</cite>; Blitzer et al., 2006) , 2) the baseline (BL) where a classifier trained on one domain is applied on another domain, and 3) the in-domain classifier. Note that in Table 2 , the performance of DA module of SODA is reported when it does not use any labeled instances from the target domain. ---------------------------------- **CONCLUSION**",
  "y": "differences"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_0",
  "x": "****MULTILINGUAL CONSTITUENCY PARSING WITH SELF-ATTENTION AND PRE-TRAINING**** **ABSTRACT** We extend <cite>our</cite> previous work on constituency parsing <cite>(Kitaev and Klein, 2018)</cite> by incorporating pre-training for ten additional languages, and compare the benefits of no pre-training, ELMo , and BERT (Devlin et al., 2018). Pre-training is effective across all languages evaluated, and BERT outperforms ELMo in large part due to the benefits of increased model capacity. Our parser obtains new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1). ---------------------------------- **INTRODUCTION**",
  "y": "extends"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_1",
  "x": "There has recently been rapid progress in developing contextual word representations that improve accuracy across a range of natural language tasks Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018) . In <cite>our</cite> earlier work <cite>(Kitaev and Klein, 2018)</cite> , <cite>we</cite> showed that such representations are helpful for constituency parsing. However, <cite>these</cite> results only considered the LSTM-based ELMo representations , and only for the English language. We now extend <cite>this</cite> work to show that using only self-attention also works by substituting BERT (Devlin et al., 2018) . We further demonstrate that pre-training and self-attention are effective across languages by applying our parsing architecture to ten additional languages. Our parser code and trained models for 11 languages are publicly available. 1",
  "y": "background"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_2",
  "x": "**INTRODUCTION** There has recently been rapid progress in developing contextual word representations that improve accuracy across a range of natural language tasks Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018) . In <cite>our</cite> earlier work <cite>(Kitaev and Klein, 2018)</cite> , <cite>we</cite> showed that such representations are helpful for constituency parsing. However, <cite>these</cite> results only considered the LSTM-based ELMo representations , and only for the English language. We now extend <cite>this</cite> work to show that using only self-attention also works by substituting BERT (Devlin et al., 2018) . We further demonstrate that pre-training and self-attention are effective across languages by applying our parsing architecture to ten additional languages. Our parser code and trained models for 11 languages are publicly available.",
  "y": "motivation"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_3",
  "x": "Our parser obtains new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1). ---------------------------------- **INTRODUCTION** There has recently been rapid progress in developing contextual word representations that improve accuracy across a range of natural language tasks Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018) . In <cite>our</cite> earlier work <cite>(Kitaev and Klein, 2018)</cite> , <cite>we</cite> showed that such representations are helpful for constituency parsing. However, <cite>these</cite> results only considered the LSTM-based ELMo representations , and only for the English language. We now extend <cite>this</cite> work to show that using only self-attention also works by substituting BERT (Devlin et al., 2018) .",
  "y": "extends"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_4",
  "x": "We now extend <cite>this</cite> work to show that using only self-attention also works by substituting BERT (Devlin et al., 2018) . We further demonstrate that pre-training and self-attention are effective across languages by applying our parsing architecture to ten additional languages. Our parser code and trained models for 11 languages are publicly available. 1 ---------------------------------- **MODEL** Our parser as described in <cite>Kitaev and Klein (2018)</cite> accepts as input a sequence of vectors corresponding to words in a sentence, transforms these repre-1 https://github.com/nikitakit/self-attentive-parser sentations using one or more self-attention layers, and finally uses these representations to output a parse tree.",
  "y": "background"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_5",
  "x": "Our parser as described in <cite>Kitaev and Klein (2018)</cite> accepts as input a sequence of vectors corresponding to words in a sentence, transforms these repre-1 https://github.com/nikitakit/self-attentive-parser sentations using one or more self-attention layers, and finally uses these representations to output a parse tree. We incorporate BERT by taking the token representations from the last layer of a BERT model and projecting them to 512 dimensions (the default size used by our parser) using a learned projection matrix. While our parser operates on vectors aligned to words in a sentence, BERT associates vectors to sub-word units based on Word-Piece tokenization (Wu et al., 2016) . We bridge this difference by only retaining the BERT vectors corresponding to the last sub-word unit for each word in the sentence. We briefly experimented with other alternatives, such as using only the first sub-word instead, but did not find that this choice had a substantial effect on English parsing accuracy. The fact that additional layers are applied to the output of BERT -which itself uses a selfattentive architecture -may at first seem redundant, but there are important differences between these two portions of the architecture. The extra layers on top of BERT use word-based tokenization instead of sub-words, apply the factored version of self-attention proposed in <cite>Kitaev and Klein (2018)</cite> , and are randomly-initialized instead of being pre-trained.",
  "y": "uses"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_6",
  "x": "We found that omitting these additional layers and using the BERT vectors directly hurt parsing accuracies. We also extend the parser to predict part-ofspeech tags in addition to constituent labels, a feature we include based on feedback from users of our previous parser. Tags are predicted using a small feed-forward network (with only one ReLU nonlinearity) after the final layer of self-attention. This differs slightly from Joshi et al. (2018) , where tags are predicted based on span representations instead. The tagging head is trained jointly with the parser by adding an auxiliary softmax crossentropy loss, averaged over all words present in a given batch. (2018) We train our parser with a learning rate of 5 \u00d7 10 \u22125 and batch size 32, where BERT parameters are fine-tuned as part of training. All other hyperparameters are unchanged from <cite>Kitaev and Klein (2018)</cite> and Devlin et al. (2018) .",
  "y": "similarities"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_0",
  "x": "**INTRODUCTION** Incorporating natural language processing into systems that provide writing assistance beyond grammar is an area of increasing research and commercial interest (e.g., (Writelab, 2015; Roscoe et al., 2015) ). As one example, the automatic recognition of the purpose of each of an author's revisions allows writing assistance systems to provide better rewriting suggestions. In this paper, we propose contextbased methods to improve the automatic identification of revision purposes in student argumentative writing. Argumentation plays an important role in analyzing many types of writing such as persuasive essays , scientific papers (Teufel, 2000) and law documents (Palau and Moens, 2009 ). In student papers, identifying revision purposes with respect to argument structure has been used to predict the grade improvement in the paper after revision<cite> (Zhang and Litman, 2015)</cite> . Existing works on the analysis of writing revisions (Adler et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013;<cite> Zhang and Litman, 2015)</cite> typically compare two versions of a text to extract revisions, then classify the purpose of each revision in isolation.",
  "y": "background"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_1",
  "x": "Existing works on the analysis of writing revisions (Adler et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013;<cite> Zhang and Litman, 2015)</cite> typically compare two versions of a text to extract revisions, then classify the purpose of each revision in isolation. That is, while limited contextual features such as revision location have been utilized in prior work, such features are computed from the revision being classified but typically not its neighbors. In addition, ordinary classifiers rather than structured prediction models are typically used. To increase the role of context during prediction, in this paper we 1) introduce new contextual features (e.g., the impact of a revision on local text cohesion), and 2) transform revision purpose classification to a sequential labeling task to capture dependencies among revisions (as in Table 1 ). An experimental evaluation demonstrates the utility of our approach. ---------------------------------- **RELATED WORK**",
  "y": "background"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_2",
  "x": "That is, while limited contextual features such as revision location have been utilized in prior work, such features are computed from the revision being classified but typically not its neighbors. In addition, ordinary classifiers rather than structured prediction models are typically used. To increase the role of context during prediction, in this paper we 1) introduce new contextual features (e.g., the impact of a revision on local text cohesion), and 2) transform revision purpose classification to a sequential labeling task to capture dependencies among revisions (as in Table 1 ). An experimental evaluation demonstrates the utility of our approach. ---------------------------------- **RELATED WORK** There are multiple works on the classification of revisions (Adler et al., 2011; Javanmardi et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013;<cite> Zhang and Litman, 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_3",
  "x": "Thus, the types of previous revisions cannot always be used as the contextual information. Moreover, the type of the revision is not necessarily the argument type of its revised sentence. For example, a revision on the evidence argument can be just a correction of spelling mistakes. ---------------------------------- **DATA DESCRIPTION** Revision purposes. To label our data, we adapt the schema defined in<cite> (Zhang and Litman, 2015)</cite> as it can be reliably annotated and is argument- (Faigley and Witte, 1981) .",
  "y": "extends"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_4",
  "x": "As we focus on argumentative changes, we merge all the Surface subcategories into one Surface category. As <cite>Zhang and Litman (2015)</cite> reported that both Rebuttals and multiple labels for a single revision were rare, we merge Rebuttal and Warrant into one Warrant category 1 and allow only a single (primary) label per revision. Corpora. Our experiments use two corpora consisting of Drafts 1 and 2 of papers written by high school students taking AP-English courses; papers were revised after receiving and generating peer feedback. Corpus A was collected in our earlier pa-per<cite> (Zhang and Litman, 2015)</cite> , although the original annotations were modified as described above. It contains 47 paper draft pairs about placing contemporaries in Dante's Inferno. Corpus B was collected in the same manor as A with agreement Kappa 0.69.",
  "y": "motivation"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_5",
  "x": "As we focus on argumentative changes, we merge all the Surface subcategories into one Surface category. As <cite>Zhang and Litman (2015)</cite> reported that both Rebuttals and multiple labels for a single revision were rare, we merge Rebuttal and Warrant into one Warrant category 1 and allow only a single (primary) label per revision. Corpora. Our experiments use two corpora consisting of Drafts 1 and 2 of papers written by high school students taking AP-English courses; papers were revised after receiving and generating peer feedback. Corpus A was collected in our earlier pa-per<cite> (Zhang and Litman, 2015)</cite> , although the original annotations were modified as described above. It contains 47 paper draft pairs about placing contemporaries in Dante's Inferno. Corpus B was collected in the same manor as A with agreement Kappa 0.69.",
  "y": "uses"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_6",
  "x": "4 Utilizing Context ---------------------------------- **ADDING CONTEXTUAL FEATURES** Our previous work<cite> (Zhang and Litman, 2015)</cite> used three types of features primarily from prior work (Adler et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013) for argumentative revision classification. Location features encode the location of the sentence in the paragraph and the location of the sentence's paragraph in the essay. Textual features encode revision operation, sentence length, edit distance between aligned sentences and the difference in sentence length and punctuation numbers. Language features encode part of speech (POS) unigrams and difference in POS tag counts.",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_0",
  "x": "Recently, the mechanism of self-attention<cite> [22,</cite> 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time. Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer<cite> [22]</cite> ), sentiment analysis [25] , and other tasks. These successes have motivated preliminary work in self-attention for ASR. Time-restricted self-attention was used as a drop-in replacement for individual layers in the state-of-theart lattice-free MMI model [26] , an HMM-NN system. Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) [27] , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems. In this work, we propose and evaluate fully self-attentional networks for CTC (SAN-CTC). We are motivated by practicality: selfattention could be used as a drop-in replacement in existing CTClike systems, where only attention has been evaluated in the past [30, 31] ; unlike encoder-decoder systems, SAN-CTC is able to predict tokens in parallel at inference time; an analysis of SAN-CTC is useful for future state-of-the-art ASR systems, which may equip self-attentive encoders with auxiliary CTC losses [17, 20] .",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_1",
  "x": "Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer<cite> [22]</cite> ), sentiment analysis [25] , and other tasks. These successes have motivated preliminary work in self-attention for ASR. Time-restricted self-attention was used as a drop-in replacement for individual layers in the state-of-theart lattice-free MMI model [26] , an HMM-NN system. Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) [27] , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems. In this work, we propose and evaluate fully self-attentional networks for CTC (SAN-CTC). We are motivated by practicality: selfattention could be used as a drop-in replacement in existing CTClike systems, where only attention has been evaluated in the past [30, 31] ; unlike encoder-decoder systems, SAN-CTC is able to predict tokens in parallel at inference time; an analysis of SAN-CTC is useful for future state-of-the-art ASR systems, which may equip self-attentive encoders with auxiliary CTC losses [17, 20] . Unlike past works, we do not require convolutional frontends [19] or interleaved recurrences [27] to train self-attention for ASR.",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_2",
  "x": "**MOTIVATING THE SELF-ATTENTION LAYER** We now replace recurrent and convolutional layers for CTC with self-attention [24] . Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder<cite> [22]</cite> , previous explorations of self-attention in ASR [19, 27] , and defined in Section 2.3. The other stages are downsampling, which reduces input length T via methods like those in Section 2.4; embedding, which learns a dh-dim. embedding that also describes token position (Section 2.5); and projection, where each final representation is mapped framewise to logits over the intermediate alphabet L . The first implements self-attention, where the success of attention in CTC and encoder-decoder models [14, 31] is parallelized by using each position's representation to attend to all others, giving a contextualized representation for that position. Hence, the full receptive field is immediately available at the cost of O(T 2 ) inner products (Table 1) , enabling richer representations in fewer layers. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_3",
  "x": "**SEQUENTIAL OPERATIONS** Maximum path length Table 1 : Operation complexity of each layer type, based on<cite> [22]</cite> . T is input length, d is no. of hidden units, and k is filter/context width. We also see inspiration from convolutional blocks: residual connections, layer normalization, and tied dense layers with ReLU for representation learning. In particular, multi-head attention is akin to having a number of infinitely-wide filters whose weights adapt to the content (allowing fewer \"filters\" to suffice). One can also assign interpretations; for example, [27] argue their LAS self-attention heads are differentiated phoneme detectors. Further inductive biases like filter widths and causality could be expressed through time-restricted self-attention [26] and directed self-attention [25] , respectively.",
  "y": "similarities uses"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_4",
  "x": "The first sublayer performs multi-head, scaled dot-product, self-attention<cite> [22]</cite> . For each head i of nhds, we learn linear maps W , and values V (i) of the i-th head, which combine to give where \u03c3 is row-wise softmax. Heads are concatenated along the dh/nhds axis to give MltHdAtt = [HdAtt (1) , . . . , HdAtt (n hds ) ]. The second sublayer is a position-wise feed-forward network<cite> [22]</cite> FFN(H) = ReLU(HW1 + b1)W2 + b2 where parameters with the biases b1, b2 broadcasted over all T positions.",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_5",
  "x": "The second sublayer is a position-wise feed-forward network<cite> [22]</cite> FFN(H) = ReLU(HW1 + b1)W2 + b2 where parameters with the biases b1, b2 broadcasted over all T positions. This sublayer aggregates the multiple heads at time t into the attention layer's final output at t. All together, the layer is given by: ---------------------------------- **DOWNSAMPLING** In speech, the input length T of frames can be many times larger than the output length U , in contrast to the roughly word-to-word setting of machine translation. This is especially prohibitive for self-attention in terms of memory: recall that an attention matrix of dimension",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_6",
  "x": "\u2208 R T \u00d7T is created, giving the T 2 factor in Table 1 . A convolutional frontend is a typical downsampling strategy [8, 19] ; however, we leave integrating other layer types into SAN-CTC as future work. Instead, we consider three fixed approaches, from least-to most-preserving of the input data: subsampling, which only takes every k-th frame; pooling, which aggregates every k consecutive frames via a statistic (average, maximum); reshaping, where one concatenates k consecutive frames into one [27] . Note that CTC will still require U \u2264 T /k, however. ---------------------------------- **POSITION** Self-attention is inherently content-based<cite> [22]</cite> , and so one often encodes position into the post-embedding vectors.",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_7",
  "x": "We take a window of 25ms, a hop of 10ms, and concatenate cepstral mean-variance normalized features with temporal first-and second-order differences. 1 We downsample by a factor of k = 3 (this also gave an ideal T /k \u2248 dh for our data; see Table 1 ). We perform Nesterov-accelerated gradient descent on batches of 20 utterances. As self-attention architectures can be unstable in early training, we clip gradients to a global norm of 1 and use the standard linear warmup period before inverse square decay associated with these architectures [19, <cite>22]</cite> . Let n denote the global step number of the batch (across epochs); the learning rate is given by 1 Rescaling so that these differences also have var. \u2248 1 helped WSJ training.",
  "y": "background"
 },
 {
  "id": "653327ecbc925624d509c679fbe0ba_0",
  "x": "How to leverage the commonsense knowledge for better reading comprehension remains largely unexplored. Recently, some preliminary studies have begun to incorporate certain side information (e.g., triplets from external knowledge base) into the model design of various NLP tasks, such as question answering [1] and conversation generation [18] . Generally, there are two lines of this work. The first line focuses on designing task-specific model structures [1, 15] , which exploit the retrieved concepts from external knowledge base for enhancing the representation. Recently, the other line has studied to pre-train a language model over large corpus to learn the inherent word-level knowledge in an unsupervised way<cite> [4,</cite> 8] , which achieves very promising performance. The first line of work is usually carefully designed for the target task, which is not widely applicable. The second line can only learn the co-occurrence of words or entities in the context, while it may not be that robust for some complex scenarios such as reasoning task.",
  "y": "background"
 },
 {
  "id": "653327ecbc925624d509c679fbe0ba_1",
  "x": "There exists increasing interest in incorporating commonsense knowledge into commonsense reading comprehension tasks. Most of previous studies focused on developing special model structures to introduce external knowledge into neural network models [1, 15] , which have achieved promising results. For example, Yang and Mitchell [15] use concepts from WordNet and weighted average vectors of the retrieved concepts to calculate a new LSTM state. These methods relied on task-specific model structures which are difficult to adapt to other tasks. Pre-trained language model such as BERT and GPT<cite> [4,</cite> 8] is also used as a kind of commonsense knowledge source. However, the LM method mainly captures the co-occurrence of words and phrases and cannot address some more complex problems which may require the reasoning ability. Unlike previous work, we incorporate external knowledge by jointly training MRC model with two auxiliary tasks which are relevant to commonsense knowledge.",
  "y": "background"
 },
 {
  "id": "653327ecbc925624d509c679fbe0ba_2",
  "x": "Here we utilize BERT <cite>[4]</cite> as the pretrained encoder for its superior performance in a range of natural language understanding tasks. Specially, we concatenate the given document, question (as sentence A) and each option (as sentence B) Next, on top of BERT encoder, we add a task-specific output layer and view the multi-choice MRC as a multi-class classification task. Specifically, we apply a linear head layer plus a softmax layer on the final contextualized word representation of [CLS] token h L 0 . We minimize the Negative Log Likelihood (NLL) loss with respect to the correct class label, as: where\u0125 0 is the final hidden state of the correct option\u00d4, N is the number of options and v T \u2208 R H is a learnable vector. Finally, to make the model be aware of certain implicit commonsense relations between concepts, we further introduce two auxiliary relation-aware prediction tasks for joint learning. Since it may be difficult to directly predict the actual relation type between two concepts without adequate training data, we split the relation prediction problem into two related tasks: i.e., relation-existence task and relation-type task.",
  "y": "uses"
 },
 {
  "id": "653327ecbc925624d509c679fbe0ba_3",
  "x": "**INCORPORATING RELATION KNOWLEDGE** Task 1 is the relation-existence task. Following <cite>[4]</cite> , we first convert the concept to a set of BPE tokens tokens A and tokens B, with beginning index i and j in the input sequence respectively. The probability of whether there is a relation in each pair (tokens A, where W 1 \u2208 R H \u00d7H is a trainable matrix. We define the pair (tokens A, tokens B) that has relation in ConceptNet as a positive example and others as negative examples. We down-sample the negative examples and keep ratio of positive vs negative is 1 : \u03b3 .",
  "y": "uses"
 },
 {
  "id": "653327ecbc925624d509c679fbe0ba_4",
  "x": "**EXPERIMENTS 4.1 DATASET** We conduct experiments on two commonsense reading comprehension tasks: SemEval-2018 shared task11 [7] and Story Cloze Test [6] . The statistics of the datasets are shown in Table 1 3 . ---------------------------------- **IMPLEMENTATION DETAILS** We use the uncased BERT(base) <cite>[4]</cite> as pre-trained language model. We set the batch size to 24, learning rate to 2e-5.",
  "y": "uses"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_0",
  "x": "****QUADRATIC-TIME DEPENDENCY PARSING FOR MACHINE TRANSLATION**** **ABSTRACT** Efficiency is a prime concern in syntactic MT decoding, yet significant developments in statistical parsing with respect to asymptotic efficiency haven't yet been explored in MT. Recently,<cite> McDonald et al. (2005b)</cite> formalized dependency parsing as a maximum spanning tree (MST) problem, which can be solved in quadratic time relative to the length of the sentence. They show that MST parsing is almost as accurate as cubic-time dependency parsing in the case of English, and that it is more accurate with free word order languages. This paper applies MST parsing to MT, and describes how it can be integrated into a phrase-based decoder to compute dependency language model scores. Our results show that augmenting a state-ofthe-art phrase-based system with this dependency language model leads to significant improvements in TER (0.92%) and BLEU (0.45%) scores on five NIST Chinese-English evaluation test sets.",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_1",
  "x": "For languages with relatively rigid word order such as English, there may be some concern that searching the space of non-projective dependency trees, which is considerably larger than the space of projective dependency trees, would yield poor performance. That is not the case: dependency accuracy for nonprojective parsing is 90.2% for English<cite> (McDonald et al., 2005b)</cite> , only 0.7% lower than a projective parser (McDonald et al., 2005a ) that uses the same set of features and learning algorithm. In the case of dependency parsing for Czech,<cite> (McDonald et al., 2005b)</cite> even outperforms projective parsing, and was one of the top systems in the CoNLL-06 shared task in multilingual dependency parsing. ---------------------------------- **O(N 2 )-TIME DEPENDENCY PARSING FOR MT** We now formalize weighted non-projective dependency parsing similarly to<cite> (McDonald et al., 2005b)</cite> and then describe a modified and more efficient version that can be integrated into a phrasebased decoder. Given the single-head constraint, parsing an input sentence x = (x 0 , x 1 , \u00b7 \u00b7 \u00b7 , x n ) is reduced to labeling each word x j with an index i identifying its head word x i .",
  "y": "uses"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_2",
  "x": "We write h-word, h-pos, m-word, m-pos to refer to head and modifier words and POS tags, and append a numerical value to shift the word offset either to the left or to the right (e.g., h-pos+1 is the POS to the right of the head word). We use the symbol \u2227 to represent feature conjunctions. Each feature in the table has a distinct identifier, so that, e.g., the POS features In-between POS features: h-pos are all distinct from m-pos features. 3 The primary difference between our feature sets and the ones of McDonald et al. is that their set of \"in between POS features\" includes the set of all tags appearing between each pair of words. Extracting all these tags takes time O(n) for any arbitrary pair (i, j). Since i and j are both free variables, feature computation in<cite> (McDonald et al., 2005b)</cite> takes time O(n 3 ), even though parsing itself takes O(n 2 ) time. To make our parser genuinely O(n 2 ), we modified the set of in-between POS features in two ways.",
  "y": "motivation"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_3",
  "x": "While deterministic parsers are often deemed inadequate for dealing with ambiguities of natural language, highly accurate O(n 2 ) algorithms exist in the case of dependency parsing. Building upon the theoretical work of (Chu and Liu, 1965; Edmonds, 1967) ,<cite> McDonald et al. (2005b)</cite> present a quadratic-time dependency parsing algorithm that is just 0.7% less accurate than \"full-fledged\" chart parsing (which, in the case of dependency parsing, runs in time O(n 3 ) (Eisner, 1996) ). In this paper, we show how to exploit syntactic dependency structure for better machine translation, under the constraint that the depen-dency structure is built as a by-product of phrasebased decoding, without reliance on a dynamicprogramming or chart parsing algorithm such as CKY or Earley. Adapting the approach of<cite> McDonald et al. (2005b)</cite> for machine translation, we incrementally build dependency structure left-toright in time O(n 2 ) during decoding. Most interestingly, the time complexity of non-projective dependency parsing remains quadratic as the order of the language model increases. This provides a compelling advantage over previous dependency language models for MT (Shen et al., 2008) , which use a 5-gram LM only during reranking. In our experiments, we build a competitive baseline incorporating a 5-gram LM trained on a large part of Gigaword and show that our dependency language model provides improvements on five different test sets, with an overall gain of 0.92 in TER and 0.45 in BLEU scores.",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_4",
  "x": "Adapting the approach of<cite> McDonald et al. (2005b)</cite> for machine translation, we incrementally build dependency structure left-toright in time O(n 2 ) during decoding. Most interestingly, the time complexity of non-projective dependency parsing remains quadratic as the order of the language model increases. This provides a compelling advantage over previous dependency language models for MT (Shen et al., 2008) , which use a 5-gram LM only during reranking. In our experiments, we build a competitive baseline incorporating a 5-gram LM trained on a large part of Gigaword and show that our dependency language model provides improvements on five different test sets, with an overall gain of 0.92 in TER and 0.45 in BLEU scores. These results are found to be statistically very significant (p \u2264 .01). ---------------------------------- **DEPENDENCY PARSING FOR MACHINE TRANSLATION**",
  "y": "extends"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_5",
  "x": "In this section, we review dependency parsing formulated as a maximum spanning tree problem<cite> (McDonald et al., 2005b)</cite> , which can be solved in quadratic time, and then present its adaptation and novel application to phrase-based decoding. Dependency models have recently gained considerable interest in many NLP applications, including machine translation (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008) . Dependency structure provides several compelling advantages compared to other syntactic representations. First, dependency links are close to the semantic relationships, which are more likely to be consistent across languages. Indeed, Fox (2002) found inter-lingual phrasal cohesion to be greater than for a CFG when using a dependency representation, for which she found only 12.6% of head crossings and 9.2% modifier crossings. Second, dependency trees contain exactly one node per word, which contributes to cutting down the search space during parsing: indeed, the task of the parser is merely to connect existing nodes rather than hypothesizing new ones. Finally, dependency models are more flexible and account for (non-projective) head-modifier relations that CFG models fail to represent adequately, which is problematic with certain types of grammatical constructions and with free word order languages, Figure 1: A dependency tree with directed edges going from heads to modifiers.",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_6",
  "x": "Such a head-modifier relationship is difficult to represent with a CFG, since all words directly or indirectly headed by hired (i.e., who, think, they, and hired) do not constitute a contiguous sequence of words. as we will see later in this section. The most standardly used algorithm for parsing with dependency grammars is presented in (Eisner, 1996; Eisner and Satta, 1999) . It runs in time O(n 3 ), where n is the length of the sentence. Their algorithm exploits the special properties of dependency trees to reduce the worst-case complexity of bilexical parsing, which otherwise requires O(n 4 ) for bilexical constituency-based parsing. While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999) ,<cite> McDonald et al. (2005b)</cite> show O(n 2 )-time parsing is possible if trees are not required to be projective. This relaxation entails that dependencies may cross each other rather than being required to be nested, as shown in Fig. 1 .",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_7",
  "x": "Non-projective dependency structures are sometimes even needed for languages like English, e.g., in the case of the wh-movement shown in Fig. 1 . For languages with relatively rigid word order such as English, there may be some concern that searching the space of non-projective dependency trees, which is considerably larger than the space of projective dependency trees, would yield poor performance. That is not the case: dependency accuracy for nonprojective parsing is 90.2% for English<cite> (McDonald et al., 2005b)</cite> , only 0.7% lower than a projective parser (McDonald et al., 2005a ) that uses the same set of features and learning algorithm. In the case of dependency parsing for Czech,<cite> (McDonald et al., 2005b)</cite> even outperforms projective parsing, and was one of the top systems in the CoNLL-06 shared task in multilingual dependency parsing. ---------------------------------- **O(N 2 )-TIME DEPENDENCY PARSING FOR MT** We now formalize weighted non-projective dependency parsing similarly to<cite> (McDonald et al., 2005b)</cite> and then describe a modified and more efficient version that can be integrated into a phrasebased decoder.",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_8",
  "x": "This relaxation is key to computational efficiency, since the parser does not need to keep track of whether dependencies assemble into contiguous spans. It is also linguistically desirable in the case of free word order languages such as Czech, Dutch, and German. Non-projective dependency structures are sometimes even needed for languages like English, e.g., in the case of the wh-movement shown in Fig. 1 . For languages with relatively rigid word order such as English, there may be some concern that searching the space of non-projective dependency trees, which is considerably larger than the space of projective dependency trees, would yield poor performance. That is not the case: dependency accuracy for nonprojective parsing is 90.2% for English<cite> (McDonald et al., 2005b)</cite> , only 0.7% lower than a projective parser (McDonald et al., 2005a ) that uses the same set of features and learning algorithm. In the case of dependency parsing for Czech,<cite> (McDonald et al., 2005b)</cite> even outperforms projective parsing, and was one of the top systems in the CoNLL-06 shared task in multilingual dependency parsing. ----------------------------------",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_9",
  "x": "The O(n 3 ) non-projective parser of <cite>(McDonald et al., 2005b</cite> ) is slightly more accurate than our version, though ours runs in O(n 2 ) time. \"Local classifier\" refers to non-projective dependency parsing without removing loops as a post-processing step. The result marked with (*) identifies the parser used for our MT experiments, which is only about 1% less accurate than a state-of-the-art dependency parser (**). on our test was down 0.9%). To make our genuinely O(n 2 ) parser almost as accurate as the nonprojective parser of McDonald et al., we conjoin each in-between POS with its position relative to (i, j). This relatively simple change reduces the drop in accuracy to only 0.34%. 4 ----------------------------------",
  "y": "differences"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_10",
  "x": "Additional experiments reveal two main contributing factors to this drop on WSJ: tagging uncased texts reduces tagging accuracy by about 1%, and using only wordbased features further reduces it by 0.6%. Table 4 shows that the accuracy of our truly O(n 2 ) parser is only .25% to .34% worse than the O(n 3 ) implementation of<cite> (McDonald et al., 2005b)</cite> . 5 Compared to the state-of-the-art projective parser as implemented in (McDonald et al., 2005a) , performance is 1.28% lower on WSJ, but only 0.95% when training on all our available data and using the MT setting. Overall, we believe that the drop of performance is a reasonable price to pay considering the computational constraints imposed by integrating the dependency parser into an MT decoder. The table also shows a gain of more than 1% in dependency accuracy by adding ATB, OntoNotes, and WSJ to the English CTB training set. The four sources were assigned non-uniform weights: we set the weight of the CTB data to be 10 times larger than the other corpora, which seems to work best in our parsing experiments. While this improvement of 1% may seem relatively small considering that the amount of training data is more than 20 times larger in the latter case, it is quite consistent with previous findings in domain adaptation, which is known to be a difficult task.",
  "y": "differences"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_11",
  "x": "We use the standard features implemented almost exactly as in Moses: four translation features (phrase-based translation probabilities and lexically-weighted probabilities), word penalty, phrase penalty, linear distortion, and language model score. We also incorporated the lexicalized reordering features of Moses, in order to experiment with a baseline that is stronger than the default Moses configuration. The language pair for our experiments is Chinese-to-English. The training data consists of about 28 million English words and 23.3 million 5 Note that our results on WSJ are not exactly the same as those reported in <cite>(McDonald et al., 2005b</cite> ), since we used slightly different head finding rules. To extract dependencies from treebanks, we used the LTH Penn Converter (http:// nlp.cs.lth.se/pennconverter/), which extracts dependencies that are almost identical to those used for the CoNLL-2008 Shared Task. We constrain the converter not to use functional tags found in the treebanks, in order to make it possible to use automatically parsed texts (i.e., perform selftraining) in future work. Chinese words drawn from various news parallel corpora distributed by the Linguistic Data Consortium (LDC).",
  "y": "differences"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_12",
  "x": "---------------------------------- **CONCLUSION AND FUTURE WORK** In this paper, we presented a non-projective dependency parser whose time-complexity of O(n 2 ) improves upon the cubic time implementation of<cite> (McDonald et al., 2005b)</cite> , and does so with little loss in dependency accuracy (.25% to .34%). Since this parser does not need to enforce projectivity constraints, it can easily be integrated into a phrase-based decoder during search (rather than during rescoring). We use dependency scores as an extra feature in our MT experiments, and found that our dependency model provides significant gains over a competitive baseline that incorporates a large 5-gram language model (0.92% TER and 0.45% BLEU absolute improvements). We plan to pursue other research directions using dependency models discussed in this paper. While we use a dependency language model to exemplify the use of hierarchical structure within phrase based decoders, we could extend this work to incorporate dependency features of both sourceand target side.",
  "y": "uses"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_0",
  "x": "****EXTENDING SENSE COLLOCATIONS IN INTERPRETING NOUN COMPOUNDS**** **ABSTRACT** This paper investigates the task of noun compound interpretation, building on the sense collocation approach proposed by <cite>Moldovan et al. (2004)</cite> . Our primary task is to evaluate the impact of similar words on the sense collocation method, and decrease the sensitivity of the classifiers by expanding the range of sense collocations via different semantic relations. Our method combines hypernyms, hyponyms and sister words of the component nouns, based on WordNet. The data used in our experiments was taken from the nominal pair interpretation task of SEMEVAL-2007 (4th International Workshop on Semantic Evaluation 2007. In the evaluation, we test 7-way and 2-way class data, and show that the inclusion of hypernyms improves the performance of the sense collocation method, while the inclusion of hyponym and sister word information leads to a deterioration in performance.",
  "y": "extends"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_1",
  "x": "Processing NCs exhibits many challenges due to the following issues associated with the task (Lapata, 2002) : (1) the compounding process is extremely productive; (2) the semantic relationship between the head noun and its modifier(s) is implicit; and (3) the interpretation of an NC can vary due to contextual and pragmatic factors. Due to these challenges, current NC interpretation methods are too error-prone to employ directly in NLP applications without any human intervention or preprocessing. In this paper, we investigate the task of NC interpretation based on sense collocation. It has been shown that NCs with semantically similar compo-nents share the same SR ; this is encapsulated by the phrase coined as sense collocation in <cite>Moldovan et al. (2004)</cite> . For example, NCs such as apple pie have the same interpretation as banana cake, where the modifiers of both NCs are semantically similar (they are both classified as fruit), and the head nouns of both NCs are a type of baked edible concoction. Given that the modifier is a fruit and the head noun is a baked concoction, then we can interpret NCs with this sense collocation as having the PRODUCT-PRODUCER SR. However, unlike the method of , where new data was induced by substituting the components of the NCs with semantically similar terms, our approach adds related terms as features for the classifier.",
  "y": "background"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_2",
  "x": "In Section 8, we briefly outline some of the issues associated with NC research. Finally, we conclude our work in Section 9. ---------------------------------- **RELATED WORK** A majority of research undertaken in interpreting NCs have been based on two statistical methods: SEMANTIC SIMILARITY (Barker and Szpakowicz, 1998; Rosario, 2001;<cite> Moldovan et al., 2004</cite>; Kim and Baldwin, 2005; Nastase, 2006; Girju, 2007; and SEMANTIC INTER-PRETABILITY (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006) . Our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity category. A significant contribution to this area is by <cite>Moldovan et al. (2004)</cite> , who used the sense collocation (i.e. pair-of-word-senses) as their primary feature in disambiguating NCs.",
  "y": "background"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_3",
  "x": "---------------------------------- **RELATED WORK** A majority of research undertaken in interpreting NCs have been based on two statistical methods: SEMANTIC SIMILARITY (Barker and Szpakowicz, 1998; Rosario, 2001;<cite> Moldovan et al., 2004</cite>; Kim and Baldwin, 2005; Nastase, 2006; Girju, 2007; and SEMANTIC INTER-PRETABILITY (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006) . Our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity category. A significant contribution to this area is by <cite>Moldovan et al. (2004)</cite> , who used the sense collocation (i.e. pair-of-word-senses) as their primary feature in disambiguating NCs. Many subsequent studies have been based on this sense collocation method, with the addition of other performance-improving features. For example, Girju (2007) added contextual information (e.g. the grammatical role and POS) and cross-lingual information from 5 European languages as features to her model.",
  "y": "uses"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_4",
  "x": "A significant contribution to this area is by <cite>Moldovan et al. (2004)</cite> , who used the sense collocation (i.e. pair-of-word-senses) as their primary feature in disambiguating NCs. Many subsequent studies have been based on this sense collocation method, with the addition of other performance-improving features. For example, Girju (2007) added contextual information (e.g. the grammatical role and POS) and cross-lingual information from 5 European languages as features to her model. In contrast, utilise sense collocations in a different way: instead of adding additional features in their model, they increase the size of their training data by substituting components of existing training instances to generate additional training instances (which is assumed to have the same SR as the original). For an SR to be preserved the newly-generated NC must be semantically similar and hence maintain the same sense collocation as the original NC on which it was based. Rosario (2001; Kim and Baldwin (2005; Nastase (2006) attempted to interpret NCs by applying implicit sense collocations. In particular, they used various ways to retrieve sense collocations instead of manually assigning word senses to the NCs.",
  "y": "background"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_5",
  "x": "Nastase (2006) listed the hypernyms of components as sense features. ---------------------------------- **MOTIVATION** As mentioned above, <cite>Moldovan et al. (2004)</cite> showed that the sense collocation of NCs is a key feature when interpreting NCs. Further research in this area has shown that not only synonymous NCs share the same SR, but NCs whose components are replaced with similar words also have the same SR as the original NCs . For example, car factory, automobile factory and truck factory substituted with a synonym, hypernym and sister word, respectively, share the same SR of PRODUCT-PRODUCER. Figure 3 shows an example of semantic neighbours for the two NCs car key and apple pie.",
  "y": "background"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_6",
  "x": "Car key can be interpreted as PRODUCT-PRODUCER by referring to the training NC automobile key, since they have the same sense collocation. With apple juice, the sense collocation method tries to locate matching sense collocations in the training data, and finds that fruit juice matches closely, with the mod- ifier being a hypernym of apple. From this, we can hope to correctly interpret apple juice as having the SR PRODUCT-PRODUCER. In order to achieve this, we require some means of comparing nouns taxonomically, both vertically to capute hypernyms and hyponyms, and horizontally to capture sister words. As intimated above, our motivation in conducting this research is to be able to include hypernym, hyponym and sister word information without using direct substitution over the training instances, but still preserving the essence of the sense collocation approach. The disadvantage of the method employed by is that noise will inevitably infect the training data, skewing the classifier performance. The original method described in <cite>Moldovan et al. (2004)</cite> only relies on observed sense collocations.",
  "y": "motivation"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_7",
  "x": "As synonyms have the identical sense collocation (i.e. pairing of synsets) within WordNet, we ignore it in this research. Instead, we add hyponyms as a means of broadening the range of sense collocation. ---------------------------------- **METHOD** At first, we describe the principal idea of sense collocation method on NC interpretation and the probability model proposed in<cite> (Moldovan et al., 2004)</cite> . Then we present our method using hypernyms, hyponyms and sister words in order to extend sense collocation method. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_8",
  "x": "**SENSE COLLOCATION** The basic idea behind sense collocation method in <cite>Moldovan et al. (2004)</cite> was based on the \"pair-ofword-senses\" from the component nouns in noun compounds as features of the classifier. They also introduced a probability model called semantic scattering, as detailed in Equations 1 and 2 below as a supervised classification technique. In essence, the probability P (r|f i f j ) (simplified to P (r|f ij )) of a modifier and head noun with word sense f i and f j , respectively, occurring with SR r is calculated based on simple maximum likelihood estimation: The preferred SR r * for the given sense combination is that which maximizes the probability: (2) ----------------------------------",
  "y": "background"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_9",
  "x": "---------------------------------- **ADDING SIMILAR WORDS** We extend the approach of <cite>Moldovan et al. (2004)</cite> by adding similar words as features focusing on hypernyms, hyponyms and sister words of the modifier and head noun. We accumulate the features for semantic relations based on different taxonomic relation types, from which we construct a feature vector to build a classifier over. The features of each taxonomic relation types are listed below. The first is features used in the original sense collocation method. The second, third and fourth are our experimental features added hypernyms, hyponyms and sister words respectively.",
  "y": "extends"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_10",
  "x": "The baseline was computed using a Zero-R classifier (i.e. majority vote). 2 The performance of the original method proposed in <cite>Moldovan et al. (2004)</cite> is considered as a benchmark. .217 .496 .544 .552 .573 .562 .588 .568 .557 .197 .142 .547 .547 533 .573 .600 .606 .586 .607 .630 .467 .453 IA .507 .581 .595 .608 .649 .671 .653 .629 .645 .500 .500 PP .655 .667 .679 .691 .679 .737 .700 .690 .687 .655 .655 OE .558 .636 .623 .610 .662 .645 .662 .625 .712 .558 .558 TT .636 .697 .727 .712 .742 .766 .732 .717 .650 .515 .394 PW .634 .620 .690 .690 .629 .657 .585 .731 .630 .633 .634 CC .514 .676 .703 .689 .689 .676 .667 .647 .698 .446 .514 All .579 .632 .649 .653 .662 .679 .654 .661 .667 .541 .534 Table 4 : Results for each of the 2-way classification tasks: B = baseline, M+ = <cite>Moldovan et al. (2004)</cite> method, H i = ith-order Hypernym, O = Hyponym and S = Sister word; the best performing system is indicated in boldface and that of extended sense collocation as proposed in this paper. Table 3 shows that our method, combined with hypernyms outperforms the original sense collocation method, with the highest accuracy of .5880 achieved with 5th-degree ancestors of the head noun and modifier. This confirms that hypernyms are valuable in extending the range of sense collocation for NC interpretation. In stark contrast to the results for hypernyms, the results for hyponyms and sister words significantly reduced the accuracy. The reason for this anomaly is that hypernyms are able to generalize the sense collocation without losing key discriminative features (i.e. the hypernyms always, by definition, subsume the original semantic information), while hyponyms and sister words add many sense collocations for which we have no direct evidence (i.e. we indiscriminately specialise the semantics without any motivation).",
  "y": "uses"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_12",
  "x": "We ran our first experiment over the 7-class dataset. The baseline was computed using a Zero-R classifier (i.e. majority vote). 2 The performance of the original method proposed in <cite>Moldovan et al. (2004)</cite> is considered as a benchmark. .217 .496 .544 .552 .573 .562 .588 .568 .557 .197 .142 .547 .547 533 .573 .600 .606 .586 .607 .630 .467 .453 IA .507 .581 .595 .608 .649 .671 .653 .629 .645 .500 .500 PP .655 .667 .679 .691 .679 .737 .700 .690 .687 .655 .655 OE .558 .636 .623 .610 .662 .645 .662 .625 .712 .558 .558 TT .636 .697 .727 .712 .742 .766 .732 .717 .650 .515 .394 PW .634 .620 .690 .690 .629 .657 .585 .731 .630 .633 .634 CC .514 .676 .703 .689 .689 .676 .667 .647 .698 .446 .514 All .579 .632 .649 .653 .662 .679 .654 .661 .667 .541 .534 Table 4 : Results for each of the 2-way classification tasks: B = baseline, M+ = <cite>Moldovan et al. (2004)</cite> method, H i = ith-order Hypernym, O = Hyponym and S = Sister word; the best performing system is indicated in boldface and that of extended sense collocation as proposed in this paper. Table 3 shows that our method, combined with hypernyms outperforms the original sense collocation method, with the highest accuracy of .5880 achieved with 5th-degree ancestors of the head noun and modifier. This confirms that hypernyms are valuable in extending the range of sense collocation for NC interpretation. In stark contrast to the results for hypernyms, the results for hyponyms and sister words significantly reduced the accuracy.",
  "y": "uses"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_13",
  "x": "In the study conducted by Levi (1979) , it was claimed that there were 9 distinct SRs, which could be discretely defined and interpreted within NCs, while Finin (1980) claimed an unlimited number of SRs. The problems surrounding this task involve the issue of granularity versus coverage, which to date remains widely debated. Syntactic disambiguation (called bracketing) is required when NCs are composed of more than 2 components, such as in the case of computer science department, introducing the need for phrasal disambiguation (Lauer, 1995; Nakov, 2005) . Lauer (1995) proposed probabilistic models (based on dependency and adjacency analyses of the data). Later Nakov (2005) built upon this by adding linguistic features into these probabilistic models. Methods employed in word sense disambiguation (WSD) have also been used to enhance NC interpretation; the noun components that comprise the NCs are disambiguated using these WSD techniques (SparckJones, 1983; . carried out experiments on automatically modeling WSD and attested the usefulness of conducting analysis of the word senses in the NC in determining its SR. In the automatic interpretation of NCs, many claims have been made for the increase in performance, but these works make their own assumptions for interpretation (Barker and Szpakowicz, 1998;<cite> Moldovan et al., 2004</cite>; Kim and Baldwin, 2005; Girju, 2007; Seaghdha, 2007) .",
  "y": "background"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_0",
  "x": "**INTRODUCTION** Financial risk is the chance that a chosen investment instruments (e.g., stock) will lead to a loss. In finance, volatility is an empirical measure of risk and will vary based on a number of factors. This paper attempts to use text information in financial reports as factors to rank the risk of stock returns. Considering such a problem is a text ranking problem, we attempt to use learning-to-rank techniques to deal with the problem. Unlike the previous study<cite> (Kogan et al., 2009)</cite> , in which a regression model is employed to predict stock return volatilities via text information, our work utilizes learning-to-rank methods to model the ranking of relative risk levels directly. The reason of this practice is that, via text information only, predicting ranks among real-world quantities should be more reasonable than predicting their real values.",
  "y": "differences"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_1",
  "x": "The difficulty of predicting the values is partially because of the huge amount of noise within texts<cite> (Kogan et al., 2009</cite> ) and partially because of the weak connection between texts and the quantities. Regarding these issues, we turn to rank the relative risk levels of the companies (their stock returns). By means of learning-to-ranking techniques, we attempt to identify some key factors behind the text ranking problem. Our experimental results show that in terms of two different ranking correlation metrics, our ranking approach significantly outperforms the regression-based method with a confidence level over 95%. In addition to the improvements, through the learned ranking models, we also discover meaningful words that are financially risk-related, some of which were not identified in<cite> (Kogan et al., 2009</cite> ). These words enable us to get more insight and understanding into financial reports. Finally, in this paper, a visualization interface is provided to demonstrate the learned relations between financial risk and text information in the reports.",
  "y": "background"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_2",
  "x": "Our experimental results show that in terms of two different ranking correlation metrics, our ranking approach significantly outperforms the regression-based method with a confidence level over 95%. In addition to the improvements, through the learned ranking models, we also discover meaningful words that are financially risk-related, some of which were not identified in<cite> (Kogan et al., 2009</cite> ). These words enable us to get more insight and understanding into financial reports. Finally, in this paper, a visualization interface is provided to demonstrate the learned relations between financial risk and text information in the reports. This demonstration not only enables users to easily obtain useful information from a number of financial reports but offer a novel way to understand these reports. The remainder of this paper is organized as follows. In Section 2, we briefly review some previous work.",
  "y": "differences"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_3",
  "x": "Considering the prevalence of learning-to-rank techniques, this paper attempts to use such techniques to deal with the ranking problem of financial risk. In recent year, there have been some studies conducted on mining financial reports, such as (Lin et al., 2008; <cite>Kogan et al., 2009</cite>; Leidner and Schilder, 2010) . (Lin et al., 2008 ) use a weighting scheme to combine both qualitative and quantitative features of financial reports together, and propose a method to predict short-term stock price movements. In the work, a Hierarchical Agglomerative Clustering (HAC) method with K-means updating is employed to improve the purity of the prototypes of financial reports, and then the generated prototypes are used to predict stock price movements. (Leidner and Schilder, 2010 ) use text mining techniques to detect whether there is a risk within a company, and classify the detected risk into several types. The above two studies both use a classification manner to mine financial reports. (Kogan et al., 2009 ) apply a regression approach to predict stock return volatilities of companies via their financial reports; in specific, the Support Vector Regression (SVR) model is applied to conduct mining on text information.",
  "y": "background"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_4",
  "x": "**OUR RANKING APPROACH** In finance, volatility is a common risk metric, which is measured by the standard deviation of a stock's returns over a period of time. Let S t be the price of a stock at time t. Holding the stock for one period from time t \u2212 1 to time t would result in a simple net return: R t = S t /S t\u22121 (Tsay, 2005) . The volatility of returns for a stock from time t \u2212 n to t can be defined as . We now proceed to classify the volatilities of n stocks into 2\u2113 + 1 risk levels, where n, \u2113 \u2208 {1, 2, 3, \u00b7 \u00b7 \u00b7 }. Let m be the sample mean and s be the sample standard deviation of the logarithm of volatilities of n stocks (denoted as ln(v)). The distribution over ln(v) across companies tends to have a bell shape<cite> (Kogan et al., 2009)</cite> .",
  "y": "background"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_5",
  "x": "Bold faced numbers denote improvements over the baseline, and * indicates that the entry is statistically significant from the baseline at 95% confidence level. where w is a learned weight vector, C is the trade-off parameter, \u03be i, j,k is a slack variable, and Y k is a set of pairs of financial reports within a year. ---------------------------------- **EXPERIMENTS AND ANALYSIS** In this paper, the 10-K Corpus<cite> (Kogan et al., 2009</cite> ) is used to conduct the experiments; only Section 7 \"management's discussion and analysis of financial conditions and results of operations\" (MD&A) is included in the experiments since typically Section 7 contains the most important forward-looking statements. In the experiments, all documents were stemmed by the Porter stemmer, and the documents in each year are indexed separately. In addition to the reports, the twelve months after the report volatility for each company can be calculated by Equation (1), where the price return series can be obtained from the Center for Research in Security Prices (CRSP) US Stocks Database.",
  "y": "similarities"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_6",
  "x": "For regression, linear kernel is adopted with \u03b5 = 0.1 and the trade-off C is set to the default choice of SVM light , which are the similar settings of<cite> (Kogan et al., 2009</cite> ). For ranking, linear kernel is adopted with C = 1, all other parameters are left for the default values of SVM Rank . Table 1 tabulates the experimental results, in which all reports from the five-year period preceding the test year are used as the training data (we denote the training data from the n-year period preceding the test year as T n hereafter). For example, the reports from year 1996 to 2000 constitute a training data T 5 , and the resulting model is tested on the reports of year 2001. As shown in the table, with the feature of TF-IDF, our results are significantly better than those of the baseline in terms of both two measures. In addition to using T 5 as the training data, we also conduct other 4 sets of experiments with T 1 , T 2 , T 3 , T 4 to test the reports from Words with Negative Weights 1996-2000 1997-2001 1998-2002 1999-2003 2000-2004 2001-2005 Figure 1 Figure 1 illustrates the top positive and negative weighted terms appearing more than twice in the six T 5 models trained on TF-IDF; these terms (8 positive and 8 negative) constitute the radar chart in Figure 1 . Almost all the terms found by our ranking approach are financially meaningful; in addition, some of highly risk-correlated terms are not even reported in<cite> (Kogan et al., 2009)</cite> .",
  "y": "similarities"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_7",
  "x": "For example, the reports from year 1996 to 2000 constitute a training data T 5 , and the resulting model is tested on the reports of year 2001. As shown in the table, with the feature of TF-IDF, our results are significantly better than those of the baseline in terms of both two measures. In addition to using T 5 as the training data, we also conduct other 4 sets of experiments with T 1 , T 2 , T 3 , T 4 to test the reports from Words with Negative Weights 1996-2000 1997-2001 1998-2002 1999-2003 2000-2004 2001-2005 Figure 1 Figure 1 illustrates the top positive and negative weighted terms appearing more than twice in the six T 5 models trained on TF-IDF; these terms (8 positive and 8 negative) constitute the radar chart in Figure 1 . Almost all the terms found by our ranking approach are financially meaningful; in addition, some of highly risk-correlated terms are not even reported in<cite> (Kogan et al., 2009)</cite> . We now take the term defaut (only identified by our ranking approach) as an example. In finance, a company \"defaults\" when it cannot meet its legal obligations according to the debt contract; as a result, the term \"default\" is intuitively associated with a relative high risk level. One piece of the paragraph quoted from the original report (from AFC Enterprises, Inc.) is listed as follows:",
  "y": "differences"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_0",
  "x": "Translating Japanese to English is difficult because they belong to different language families. Na\u00efve phrase-based statistical machine translation (SMT) often fails to address syntactic difference between Japanese and English. Preordering methods are one of the simple but effective approaches that can model reordering in a long distance, which is crucial in translating Japanese and English. Thus, we apply a predicate-argument structure-based preordering method to the Japanese-English statistical machine translation task of scientific papers. Our method is based on the method described in (<cite>Hoshino et al., 2013</cite>) , and extends <cite>their</cite> rules to handle abbreviation and passivization frequently found in scientific papers. Experimental results show that our proposed method improves performance of both (<cite>Hoshino et al., 2013</cite>) 's system and our phrase-based SMT baseline without preordering. ----------------------------------",
  "y": "extends uses"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_1",
  "x": "Preordering methods are one of the simple but effective approaches that can model reordering in a long distance, which is crucial in translating Japanese and English. Thus, we apply a predicate-argument structure-based preordering method to the Japanese-English statistical machine translation task of scientific papers. Our method is based on the method described in (<cite>Hoshino et al., 2013</cite>) , and extends <cite>their</cite> rules to handle abbreviation and passivization frequently found in scientific papers. Experimental results show that our proposed method improves performance of both (<cite>Hoshino et al., 2013</cite>) 's system and our phrase-based SMT baseline without preordering. ---------------------------------- **INTRODUCTION** Preordering method is one of the popular techniques in statistical machine translation.",
  "y": "differences"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_2",
  "x": "Specifically, previous work in the literature uses morphological analysis (Katz-Brown and Collins, 2008) , dependency structure (Katz-Brown and Collins, 2008) and predicate-argument structure (Komachi et al., 2006; <cite>Hoshino et al., 2013</cite>) for preordering in Japanese-English statistical machine translation. However, these preordering methods are tested on limited domains: travel (Komachi et al., 2006) and patent (Katz-Brown and Collins, 2008; <cite>Hoshino et al., 2013</cite>) corpora. Translating Japanese to English in a different domain such as scientific papers is still a big challenge for preordering-based approach. For example, academic writing in English traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the Japanese translation of passive construction on the English side. It is not clear whether existing preordering rules are applicable to scientific domain due to such stylistic difference. Predicate-argument structure-based preordering is one of the promising approaches that can solve syntactic and stylistic difference between a language pair. Predicate-argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction.",
  "y": "background"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_3",
  "x": "Preordering methods employ various kinds of linguistic information to achieve better alignment between source and target languages. Specifically, previous work in the literature uses morphological analysis (Katz-Brown and Collins, 2008) , dependency structure (Katz-Brown and Collins, 2008) and predicate-argument structure (Komachi et al., 2006; <cite>Hoshino et al., 2013</cite>) for preordering in Japanese-English statistical machine translation. However, these preordering methods are tested on limited domains: travel (Komachi et al., 2006) and patent (Katz-Brown and Collins, 2008; <cite>Hoshino et al., 2013</cite>) corpora. Translating Japanese to English in a different domain such as scientific papers is still a big challenge for preordering-based approach. For example, academic writing in English traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the Japanese translation of passive construction on the English side. It is not clear whether existing preordering rules are applicable to scientific domain due to such stylistic difference. Predicate-argument structure-based preordering is one of the promising approaches that can solve syntactic and stylistic difference between a language pair.",
  "y": "motivation"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_4",
  "x": "Predicate-argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction. Following (<cite>Hoshino et al., 2013</cite>) , we perform predicate-argument structure analysis on the Japanese side to preorder Japanese sentences to form an SVO-like word order. We propose three modifications to the preordering rules to extend their model to better handle translation of scientific papers. The main contribution of this work is as follows: \u2022 We propose an extension to (<cite>Hoshino et al., 2013</cite>) in order to deal with abbreviation and passivization frequently found in scientific papers. ---------------------------------- **PREVIOUS WORK**",
  "y": "uses"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_5",
  "x": "We propose three modifications to the preordering rules to extend their model to better handle translation of scientific papers. The main contribution of this work is as follows: \u2022 We propose an extension to (<cite>Hoshino et al., 2013</cite>) in order to deal with abbreviation and passivization frequently found in scientific papers. ---------------------------------- **PREVIOUS WORK** There are several related work that take preordering approaches to Japanese-English statistical machine translation. First, Komachi et al. (2006) suggested a preordering approach for Japanese-English speech translation in travel domain based on predicateargument structure.",
  "y": "extends motivation"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_6",
  "x": "Second, Katz-Brown and Collins (2008) presented two preordering methods for JapaneseEnglish patent translation based on morphological analysis and dependency structure, respectively. Morphological analysis-based method splits sentences into segments by punctuation and a topic marker (\" \"), and then reverses the segments. Dependency analysis-based method reorders segments into a head-initial sentence, and moves verbs to make an SVO-like structure. Unlike (Komachi et al., 2006) , they also reverse all words in each phrase. Third, <cite>Hoshino et al. (2013)</cite> proposed predicate-argument structure-based preordering rules in two-level for the Japanese-English patent translation task. The first is sentence-level and the second is phrase-level. Furthermore, sentence-level preordering rules are divided into three parts.",
  "y": "background"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_7",
  "x": "Dependency analysis-based method reorders segments into a head-initial sentence, and moves verbs to make an SVO-like structure. Unlike (Komachi et al., 2006) , they also reverse all words in each phrase. Third, <cite>Hoshino et al. (2013)</cite> proposed predicate-argument structure-based preordering rules in two-level for the Japanese-English patent translation task. The first is sentence-level and the second is phrase-level. Furthermore, sentence-level preordering rules are divided into three parts. In total, sentences are reordered sequentially by four rules. Since <cite>this method</cite> is the one we re-implemented in this paper, we will describe <cite>their method</cite> in detail below.",
  "y": "uses"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_8",
  "x": "Intra-chunk preordering We apply the phraselevel rule, which swaps function words and content words in a phrase. It will improve alignments because function words in Japanese (e.g. postposition) appear after content words while those in English (e.g. preposition) appear before content words. 3 Extension to (<cite>Hoshino et al., 2013</cite>) Our proposed preordering model is based on (<cite>Hoshino et al., 2013</cite>) with three extensions to better handle academic writing in scientific papers. ---------------------------------- **PARENTHESIS PREORDERING** Scientific papers often include parenthetical expressions. The training data (1,000,000 parallel sentences, hereafter referred to as 1M training corpus) contains 168,336 (16.8%) parentheses on Japanese side, and 187,400 parentheses on English side.",
  "y": "extends uses"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_9",
  "x": "Figure 1c describes how this rule transforms a Japanese sentence with a zero-pronoun. Even though the Japanese side is in active voice, English translation is expressed in passive voice. Note that a Japanese sentence in active voice may be translated into different expressions even in the same passive construction (e.g. \". . . (explained . . .)\" can be either \". . . was explained\" or \"It was explained that . . .\".). <cite>Hoshino et al. (2013)</cite> proposed to move a predicate after the subject (inter-chunk preordering). However, if a subject is modified by other phrases, this rule moves the predicate to the middle of a subjective phrase composed of multiple phrases.",
  "y": "background"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_10",
  "x": "---------------------------------- **SUBJECT PREORDERING** ---------------------------------- **EXPERIMENTS** We compared translation performance using a standard phrase-based statistical machine translation technique with three kinds of data: \u2022 original data (baseline), \u2022 preordered data by our re-implementation of (<cite>Hoshino et al., 2013</cite>) , and",
  "y": "uses"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_11",
  "x": "We analyzed predicate-argument structure of only the last predicate for each sentence, regardless of the number of predicates in a sentence. Also, following (<cite>Hoshino et al., 2013</cite>) , we did not consider event nouns as predicates. ---------------------------------- **EXPERIMENTAL SETTINGS** We used 1M Japanese-English parallel sentences extracted from scientific papers (train-1.txt) from the Asian Scientific Paper Excerpt Corpus (ASPEC) 3 . We varied the size of the training corpus and used the best size determined by preliminary experiments. We identified predicate-argument structure in Japanese by SynCha 4 0.3.",
  "y": "uses"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_12",
  "x": "We performed minimum error rate training (Och, 2003) optimized for BLEU using the development set (dev.txt) of the ASPEC corpus. We conducted all the experiments using the scripts distributed at KFTT Moses Baseline v1.4 11 . Table 1 shows the experimental results. In terms of BLEU, our re-implementation of (<cite>Hoshino et al., 2013</cite>) is below the baseline method while our proposed methods better than the baseline. In terms of RIBES, all preordering methods outperform the baseline, and our proposed method archieve the highest score. ---------------------------------- **EXPERIMENTAL RESULTS**",
  "y": "differences"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_13",
  "x": [
   "---------------------------------- **EXPERIMENTAL RESULTS** All methods including parenthesis preordering outperform the baseline method, and when we subtract three modifications one by one from proposed method, the parenthesis rule has the largest impact on the translation quality. ---------------------------------- **DISCUSSION** Some of the errors found in a translation result are due to the errors in predicate-argument structure analysis. We found that it is hard for predicateargument structure analyzer trained on a newswire Table 1 : Comparison of the preordering methods."
  ],
  "y": "uses"
 },
 {
  "id": "66b6283cf1f20977286f99ef21b3c7_0",
  "x": "I expect many further exciting developments in our understanding of meaning and interpretation as we enrich the social intelligence of NLG. Modeling efforts will remain crucial to the exploration of these new capabilities. When we build and assemble models of actions and interpretations, we get systems that can plan their own behavior simply by exploiting what they know about communication. These systems give new evidence about the information and problem-solving that's involved. The challenge is that these models must describe semantics and pragmatics, as well as syntax and behavior. My own slow progress (Cassell et al., 2000; <cite>Koller and Stone, 2007)</cite> shows that there's still lots of hard work needed to develop suitable techniques. I keep going because of the methodological payoffs I see on the horizon.",
  "y": "future_work motivation"
 },
 {
  "id": "672d4299e60752e866293d72f97905_0",
  "x": "**INTRODUCTION** Besides frequency, form, and meaning, words also have several other less known words properties, such as imageability, concreteness, familiarity, subjective frequency, and age of acquisition (AoA), which are subjective psycholinguistic properties, as they depend on the personal experiences that individuals had using those words. According to [15] , word imageability is the ease and speed with which a word evokes a mental image; concreteness is the degree to which words refer to objects, people, places, or things that can be experienced by the senses; experiential familiarity is the degree to which individuals know and use words in their everyday life; subjective frequency is the estimation of the number of times a word is encountered by individuals in its written or spoken form, and AoA is the estimation of the age at which a word was learned. Psycholinguistic properties have been used in various approaches, such as for Lexical Simplification<cite> [12]</cite> , for Text Simplification at the sentence level, with the aim of reducing the difficulty of informative text for language learners [18] , to predict the reading times (RTs) of each word in a sentence to assess sentence complexity [14] and also to create robust text level readability models [17] , which is also one of the purposes of this paper. Because of its inherent costs, the measurement of subjective psycholinguistic properties is usually used in the creation of datasets of limited size [2, 7, 8, 15] . For the English language, the most well known database of this kind is the MRC Psycholinguistic Database 4 , which contains 27 subjective psycholinguistic properties for 150,837 words. For BP for example, there is a psycholinguistic database 5 containing 21 columns of information for 215,175 words, but no subjective psycholinguistic properties.",
  "y": "background"
 },
 {
  "id": "672d4299e60752e866293d72f97905_1",
  "x": "According to [15] , word imageability is the ease and speed with which a word evokes a mental image; concreteness is the degree to which words refer to objects, people, places, or things that can be experienced by the senses; experiential familiarity is the degree to which individuals know and use words in their everyday life; subjective frequency is the estimation of the number of times a word is encountered by individuals in its written or spoken form, and AoA is the estimation of the age at which a word was learned. Psycholinguistic properties have been used in various approaches, such as for Lexical Simplification<cite> [12]</cite> , for Text Simplification at the sentence level, with the aim of reducing the difficulty of informative text for language learners [18] , to predict the reading times (RTs) of each word in a sentence to assess sentence complexity [14] and also to create robust text level readability models [17] , which is also one of the purposes of this paper. Because of its inherent costs, the measurement of subjective psycholinguistic properties is usually used in the creation of datasets of limited size [2, 7, 8, 15] . For the English language, the most well known database of this kind is the MRC Psycholinguistic Database 4 , which contains 27 subjective psycholinguistic properties for 150,837 words. For BP for example, there is a psycholinguistic database 5 containing 21 columns of information for 215,175 words, but no subjective psycholinguistic properties. In this work we aim to overcome this gap by automatically inferring the psycholinguistic properties of imageability, concreteness, AoA and subjective frequency (similar to familiarity) for a large database of 26,874 BP words using a resource-light regression approach. As for the automatic inference, this work is strongly based on the results of<cite> [12]</cite> which proposed an automatic bootstrapping method for regression to populate the MRC Database.",
  "y": "similarities"
 },
 {
  "id": "672d4299e60752e866293d72f97905_2",
  "x": "We explore here 3 research questions: (1) is it possible to achieve high Pearson and Spearman correlations values and low MSE values with a regression method using only word embedding features to infer the psycholinguistic properties for BP? (2) which size a database with psycholinguistic properties should have to be used in regression models? Does merging databases from different sources yield better correlation and lower MSE scores? (3) can the inferred values help in creating features that result in more reliable readability prediction models of BP texts for early school years (from 3rd to 6th grades)? Moreover, we assessed interrater reliability (Cronbach's alpha) between ratings generated by our method and the imageability and concreteness produced for 237 nouns by [9] . Besides that, we analyzed the relations between the inferred ratings and other psycholinguistic variables. ---------------------------------- **RELATED WORKS** To the best of our knowledge there are only two studies that propose regression methods to automatically estimate missing psycholinguistic properties in the MRC Database [4, <cite>12]</cite> .",
  "y": "background"
 },
 {
  "id": "672d4299e60752e866293d72f97905_3",
  "x": "The Pearson correlation between the estimated concreteness score and the concreteness score in the test set was 0.82. [<cite>12]</cite> automatically estimate missing psycholinguistic properties in the MRC Database through a bootstrapping algorithm for regression. Their method exploits word embedding models and 15 lexical features, including the number of senses, synonyms, hyper-nyms and hyponyms for word in WordNet and also minimum, maximum and average distance between the word's senses in WordNet and the thesaurus' root sense. The Pearson correlation between the estimated score and the inferred score for familiarity was 0.846; 0.862 for AoA; 0.823 for imagenery and 0.869 for concretness, which is better than the results of [4] . ---------------------------------- **A LIGHTWEIGHT REGRESSION METHOD TO INFER PSYCHOLINGUISTIC PROPERTIES OF WORDS** The fact that the methods developed by [4] and<cite> [12]</cite> are based on a large, scarce lexical resources as WordNet, led us to raise the question \"Could we have a similar performance with a simpler set of features which are easily obtainable for most languages?\".",
  "y": "background"
 },
 {
  "id": "672d4299e60752e866293d72f97905_4",
  "x": "[<cite>12]</cite> automatically estimate missing psycholinguistic properties in the MRC Database through a bootstrapping algorithm for regression. Their method exploits word embedding models and 15 lexical features, including the number of senses, synonyms, hyper-nyms and hyponyms for word in WordNet and also minimum, maximum and average distance between the word's senses in WordNet and the thesaurus' root sense. The Pearson correlation between the estimated score and the inferred score for familiarity was 0.846; 0.862 for AoA; 0.823 for imagenery and 0.869 for concretness, which is better than the results of [4] . ---------------------------------- **A LIGHTWEIGHT REGRESSION METHOD TO INFER PSYCHOLINGUISTIC PROPERTIES OF WORDS** The fact that the methods developed by [4] and<cite> [12]</cite> are based on a large, scarce lexical resources as WordNet, led us to raise the question \"Could we have a similar performance with a simpler set of features which are easily obtainable for most languages?\". Therefore we decided to build our regressors using only word length, frequency lists, lexical databases composed of school dictionaries and word embeddings models.",
  "y": "motivation background"
 },
 {
  "id": "672d4299e60752e866293d72f97905_5",
  "x": "---------------------------------- **A LIGHTWEIGHT REGRESSION METHOD TO INFER PSYCHOLINGUISTIC PROPERTIES OF WORDS** The fact that the methods developed by [4] and<cite> [12]</cite> are based on a large, scarce lexical resources as WordNet, led us to raise the question \"Could we have a similar performance with a simpler set of features which are easily obtainable for most languages?\". Therefore we decided to build our regressors using only word length, frequency lists, lexical databases composed of school dictionaries and word embeddings models. One critical difference between the strategy of<cite> [12]</cite> and ours is that they concatenate all features to train a regressor, while we take a different approach. Although simply combining all features is straightforward, it can lead to noise insertion, given that the features used greatly contrast among them (e.g. word embeddings and word length). Instead, we adopted a more elegant solution, called Multi-View Learning [19] .",
  "y": "differences motivation"
 },
 {
  "id": "672d4299e60752e866293d72f97905_6",
  "x": "**USING REGRESSION IN A MULTI-VIEW LEARNING APPROACH** We used a linear least squares regressor with L2 regularization, which is also known as Ridge Regression or Tikhonov regularization [6] . We choose this regression method due to the promising results reported by<cite> [12]</cite> . We trained three regressors in different feature spaces: lexical features, Skip-Gram embeddings, and GloVe embeddings. ---------------------------------- **EVALUATION** We experimented with several dimensions of word embeddings, but for space reasons, here we include only the best results: Skip-Gram and GloVe embeddings with 300 word vector dimensions.",
  "y": "uses"
 },
 {
  "id": "6869f08e826aa434471c51c010ef28_0",
  "x": "However, it is well known that speech signals have multilevel structures including at least phonemes and words, and such structures are very helpful in analysing or decoding speech [12] . In a previous work, we proposed to discover the hierarchical structure of two-level acoustic patterns, including subword-like and word-like patterns. A similar two-level framework was also developed recently [18] . In a more recent attempt <cite>[19]</cite> , we further proposed a framework of discovering multi-level acoustic patterns with varying model granularity. The different pattern HMM configurations (number of states per model, number of distinct models) form a two-dimensional model granularity space. Different sets of acoustic patterns with HMM model configurations represented by different points properly distributed over this two-dimensional space are complementary to one another, thus jointly capture the characteristics of the corpora considered. Such a multi-level framework was shown to be very helpful in the task of unsupervised spoken term detection (STD) with spoken queries, because token matching can be performed with pattern indices on different levels of signal characteristics, and the information integration across multiple model granularities offered the improved performance.",
  "y": "background"
 },
 {
  "id": "6869f08e826aa434471c51c010ef28_1",
  "x": "The above process can be performed with many different HMM configurations, each characterized by two hyperparameters: the number of states m in each acoustic pattern HMM, and the total number of distinct acoustic patterns n during initialization, \u03c8 = (m, n). The transcription of a signal decoded with these patterns can be considered as a temporal segmentation of the signal, so the HMM length (or number of states in each HMM) m represents the temporal granularity. The set of all distinct acoustic patterns can be considered as a segmentation of the phonetic space, so the total number n of distinct acoustic patterns represents the phonetic granularity. This gives a two-dimensional representation of the acoustic pattern configurations in terms of temporal and phonetic granularities as in Fig. 1 . Any point in this two-dimensional space in Fig. 1 corresponds to an acoustic pattern configuration. Note that in our previous work <cite>[19]</cite> , the effect of the third dimension, the acoustic granularity which is the number of Gaussians in each state, was shown to be negligible, thus here we simply set the number of Gaussians in each state to be 4 in all cases. Although the selection of the hyperparameters can be arbitrary in this two-dimensional space, here we only select M temporal granularities and N phonetic granularities, forming a two-dimensional array of M \u00d7 N hyperparameter sets in the granularity space.",
  "y": "background"
 },
 {
  "id": "6869f08e826aa434471c51c010ef28_2",
  "x": "There can be various applications for the acoustic patterns presented here. In this section we summarize the way to perform spoken term detection <cite>[19]</cite> . Let {pr, r = 1, 2, 3, .., n} denote the n acoustic patterns in the set of \u03c8=(m, n). We first construct a similarity matrix S of size n \u00d7 n off-line for every pattern set \u03c8=(m, n), for which the element S(i, j) is the similarity between any two pattern HMMs pi and pj in the set. The KL-divergence KL(i, j) between two pattern HMMs in (7) is defined as the symmetric KL-divergence between the states based on the variational approximation [22] summed over the states. To transform the KL divergence into a similarity measure between 0 and 1, a negative exponential was applied [23] with a scaling factor \u03b2. When \u03b2 is small, similarity between distinct patterns in (7) approaches zero, so (7) approaches the delta function \u03b4(i, j).",
  "y": "background"
 },
 {
  "id": "6869f08e826aa434471c51c010ef28_4",
  "x": "It is also possible to consider dynamic time warping (DTW) on the matrix W as shown in Fig. 4(b) . However, previous experiments showed that the extra improvements brought in this way is almost negligible, probably because here we have jointly considered the M \u00d7 N different pattern sequences based on the M \u00d7 N different pattern sets (e.g. including longer /shorter patterns), so the different time-warped matching and insertion/deletion between d and q is already automatically included <cite>[19]</cite> . The M \u00d7N relevance scores R(d, q) in (9) obtained with M \u00d7N pattern sets \u03c8=(m, n) are then averaged and the average scores are used in ranking all the documents for spoken term detection. It is also possible to learn the weights for different pattern sets to produce better results using a development set. But here we simply assume the detection is completely unsupervised without any annotation, and all pattern sets are equally weighted <cite>[19]</cite> . ---------------------------------- **EXPERIMENTS**",
  "y": "background"
 },
 {
  "id": "6a693f9cbc6dbb3676d765eee97db7_0",
  "x": "Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000;<cite> Wang et al., 2005</cite>; McDonald et al., 2005) . Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997) . Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004) . Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood (i.e. \"maximum entropy\") to be applied (Ratnaparkhi, 1999; Charniak, 2000) . Currently, the work on conditional parsing models appears to have culminated in large margin training approaches (Taskar et al., 2004; McDonald et al., 2005) , which demonstrates the state of the art performance in English dependency parsing. Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al., 2005) , a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches. For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997;<cite> Wang et al., 2005)</cite> , and yet they are not being used in current large margin training algorithms.",
  "y": "background"
 },
 {
  "id": "6a693f9cbc6dbb3676d765eee97db7_1",
  "x": "Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood (i.e. \"maximum entropy\") to be applied (Ratnaparkhi, 1999; Charniak, 2000) . Currently, the work on conditional parsing models appears to have culminated in large margin training approaches (Taskar et al., 2004; McDonald et al., 2005) , which demonstrates the state of the art performance in English dependency parsing. Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al., 2005) , a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches. For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997;<cite> Wang et al., 2005)</cite> , and yet they are not being used in current large margin training algorithms. Another unexploited connection is that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach-the \"structured margin loss\" (McDonald et al., 2005) -is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component. I have addressed both of these issues, as well as others in my work. ----------------------------------",
  "y": "background"
 },
 {
  "id": "6a693f9cbc6dbb3676d765eee97db7_2",
  "x": "Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood (i.e. \"maximum entropy\") to be applied (Ratnaparkhi, 1999; Charniak, 2000) . Currently, the work on conditional parsing models appears to have culminated in large margin training approaches (Taskar et al., 2004; McDonald et al., 2005) , which demonstrates the state of the art performance in English dependency parsing. Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al., 2005) , a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches. For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997;<cite> Wang et al., 2005)</cite> , and yet they are not being used in current large margin training algorithms. Another unexploited connection is that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach-the \"structured margin loss\" (McDonald et al., 2005) -is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component. I have addressed both of these issues, as well as others in my work. ----------------------------------",
  "y": "motivation background"
 },
 {
  "id": "6a693f9cbc6dbb3676d765eee97db7_3",
  "x": "In particular, I follow Eisner (1996) and McDonald et al. (2005) and assume that the score of a complete spanning tree for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair). In which case, the parsing problem reduces to 7 6 \u00a1 9 8 where the score s can depend on any measurable property of \u00a4 and \u00a4 % $ within the tree . This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models<cite> (Wang et al., 2005</cite>; Eisner, 1996) as well as non-probabilistic models (McDonald et al., 2005; Wang et al., 2006) . For the purpose of learning, the score of each link can be expressed as a weighted linear combination of features where i are the weight parameters to be estimated during training.",
  "y": "uses"
 },
 {
  "id": "6a693f9cbc6dbb3676d765eee97db7_4",
  "x": "This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models<cite> (Wang et al., 2005</cite>; Eisner, 1996) as well as non-probabilistic models (McDonald et al., 2005; Wang et al., 2006) . For the purpose of learning, the score of each link can be expressed as a weighted linear combination of features where i are the weight parameters to be estimated during training. ---------------------------------- **LEXICALISED DEPENDENCY PARSING** To learn an accurate dependency parser from data, the first approach I investigated is based on a strictly lexical parsing model where all the parameters are based on words<cite> (Wang et al., 2005)</cite> . The advantage of this approach is that it does not rely on part-ofspeech tags nor grammatical categories.",
  "y": "uses"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_0",
  "x": "Compared with original multilingual models and other cross-lingual classification models, we observe significant gains in effectiveness on document and sentiment classification for a range of diverse languages. ---------------------------------- **INTRODUCTION** Owing to notable advances in deep learning and representation learning, important progress has been achieved on text classification, reading comprehension, and other NLP tasks. Recently, pretrained language representations with self-supervised objectives (Peters et al., 2018; <cite>Devlin et al., 2018</cite>; Radford et al., 2018) have further pushed forward the state-of-the-art on many English tasks. While these sorts of deep models can be trained on different languages, deep models typically require substantial amounts of labeled data for the specific domain of data. Unfortunately, the cost of acquiring new custom-built resources for each combination of language and domain is very high, as it typically requires human annotation.",
  "y": "background"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_1",
  "x": "The scarcity of non-English annotated corpora may preclude our ability to train language-specific machine learning models. In contrast, English-language annotations are often readily available to train deep models. Although translation can be an option, human translation is very costly and for many language pairs, any available domain-specific parallel corpora are too small to train high-quality machine translation systems. Cross-lingual systems rely on training data from one language to train a model that can be applied to other languages (de Melo and Siersdorfer, 2007) , alleviating the training bottleneck issues for low-resource languages. This is facilitated by recent advances in learning joint multilingual representations (Lample and Conneau, 2019; Artetxe and Schwenk, 2018;<cite> Devlin et al., 2018)</cite> . In our work, we propose a self-learning framework to incorporate the predictions of the multilingual BERT model<cite> (Devlin et al., 2018)</cite> on non-English data into an English training procedure. The initial multilingual BERT model was simultaneously pretrained on 104 languages, and has shown to perform well for cross-lingual transfer of natural language tasks (Wu and Dredze, 2019) .",
  "y": "background"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_2",
  "x": "In our work, we propose a self-learning framework to incorporate the predictions of the multilingual BERT model<cite> (Devlin et al., 2018)</cite> on non-English data into an English training procedure. The initial multilingual BERT model was simultaneously pretrained on 104 languages, and has shown to perform well for cross-lingual transfer of natural language tasks (Wu and Dredze, 2019) . Our model begins by learning just from available English samples, but then makes predictions on unlabeled non-English samples and a part of those samples with high confidence prediction scores are repurposed to serve as labeled examples for a next iteration of fine-tuning until the model converges. Based on this multilingual self-learning technique, we demonstrate the superiority of our framework on Multilingual Document Classification (MLDoc) (Schwenk and Li, 2018) in comparison with several strong baselines. Our study then proceeds to show that our method is better on Chinese sentiment classification than other cross-lingual methods that also consider unla-beled non-English data. This shows that our method is more effective at cross-lingual transfer for domain-specific tasks, using a mix of labeled and unlabeled data via a multilingual BERT sentence model. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_3",
  "x": "Model Details. We tune the hyper-parameters for our neural network architecture based on each non-English validation set. For the encoder, we invoke the multilingual BERT model<cite> (Devlin et al., 2018)</cite> , which supports 104 languages 1 . It relies on a shared 110k WordPiece vocabulary across all languages and yields sentence representations in a common multilingual space. Most model hyperparameters are the same as in pretraining, with the exception of the batch size, max. sequence length, and number of training epochs. The batch size, max.",
  "y": "uses"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_4",
  "x": "For cross-lingual sentiment analysis, Wan (2009) uses machine translation to directly convert English training data to Chinese, which provides two views for co-training. Xu and Yang (2017) propose to use soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, while there is no need to use parallel corpora in our work. Chen et al. (2018) propose an Adversarial Deep Averaging Network to learn invariance across languages, which is another baseline considered in our experiments. Cross-lingual Representation Learning. With models such as ELMo (Peters et al., 2018) , GPT-2 (Radford et al., 2018) , and BERT<cite> (Devlin et al., 2018)</cite> , important progress has been made in learning improved sentence representations with context-specific encodings of words via a language modeling objective. The latter two approaches both rely on Transformer encoders, but BERT is trained using masked language modeling instead of right-to-left or left-to-right language modeling. Additionally, BERT also optimizes a next sentence classification objective.",
  "y": "background"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_5",
  "x": [
   "Recent work has also investigated cross-lingual extensions. Devlin et al. (2018) themselves published a multilingual version of BERT, following the same model architecture and training procedure, except that the union of 104 different language editions of Wikipedia serves as the training input. Lample and Conneau (2019) incorporate parallel text into BERT's architecture by training on a new supervised learning objective. Artetxe and Schwenk (2018) also show that the encoder from a pretrained sequence-to-sequence model can be used to produce cross-lingual sentence embeddings. All these methods are compatible with our self-learning framework, since they provide a shared sentence meaning representation across languages as needed by our approach. ---------------------------------- **CONCLUSION**"
  ],
  "y": "background"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_0",
  "x": "LID refers to the process of automatically identifying the language class for given speech segment or text document, while DID classifies between dialects within the same language class, making it a more challenging task than LID. A good DID system used as a front-end to an automatic speech recognition system, can help improve the recognition performance by providing dialectal data for acoustic and language model adaptation to the specific dialect being spoken [1] . In this work, we focus on Arabic DID which can can be posed as a five class classification problem, given that the Arabic language can be divided into five major dialects; Egyptian (EGY), Gulf (GLF), Lavantine (LAV), Modern Standard Arabic (MSA) and North African (NOR) <cite>[2]</cite> . Over the past decade, great advances have been made in the field of automatic language identification (LID). Research effort has focused on coming up with mathematical representations of speech utterances, that encodes the information about the language being spoken. These approaches are also known as Vector Space Modeling approaches [3] , where speech utterances are represented by a continuous vector of high dimensions. Two predominant Vector Space Modeling approaches are Phonotactic and Acoustic.",
  "y": "similarities"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_1",
  "x": "Phonotactic approaches attempt to model the n-gram phone statistics of speech. Phone sequences for each utterance are extracted using one or multiple phone recognisers. A Vector Space Model (VSM) is then constructed using a term-document matrix [4] , followed by an unsupervised dimensionality reduction technique, such as Principal Component Analysis (PCA) [5] to map the high dimensional feature space to a low dimensional Vector Subspace (Section 2.1), giving a Phonotactic VSM. In other cases, a phone n-gram language model is used to model the phone statistics instead of a VSM [6, 7, 8] . On the other hand, Acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, prosody, shifted delta ceptral coefficients, bottleneck features [9, 10] . One of the most successful acoustic approaches is, the use of i-Vector framework for LID, where i-Vectors are extracted for each speech utterance, using an i-Vector extractor that consists of a GMM-UBM trained on top of BNF, followed by a Total Variability Subspace Model<cite> [2,</cite> 11] . The extracted i-Vectors give an Acoustic VSM (Section 2.2).",
  "y": "extends differences"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_2",
  "x": "At prediction time, output scores from the two DID systems are combined to give a final score, on the basis of which classification decision is made. This model combination approach has been shown to give performace improvements on the DID task <cite>[2]</cite> . This also shows that the two systems are complementary to each other, which leads us to investigate a feature space combination approach i.e. to construct a single VSM by combining Phonotactic and Acoustic VSMs, in an attempt to encode useful discriminative information in that single VSM. In this work, we present a feature space combination approach. We form a combined VSM that incorporates useful information, necessary for DID, from both the Phonotactic and Acoustic VSMs. To achieve this goal, we make use of the well known multi-view dimensionality reduction technique known as Canonical Correlation Analysis (CCA), devloped by H. Hotelling [12] (Section 2.3). We show the performance of the combined VSM on Arabic DID task and compare it against the performance of Phonotactic and Acoustic VSMs used alone (Section 5).",
  "y": "extends differences"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_3",
  "x": "**VECTOR SPACE MODELS** This section gives details about the construction of combined VSM, also referred to as CCA VSM, Z C . We start by presenting the Phonotactic VSM, X P and Acoustic VSM, X A , used in this work, followed by the section on CCA VSM, Z C . ---------------------------------- **PHONOTACTIC VSM; X P** Phonotactic VSM is constructed by modeling the n-gram phone statistics of the phone sequences that are extracted using an Arabic phone recognizer. Details about the phone recognizer can be found in <cite>[2]</cite> .",
  "y": "background"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_4",
  "x": "In our case, the n-gram dictionary consisted of phone 2-grams and 3-grams with a total of 8K features i.e. d = 8K. The dimensionality of the VSM, X P , was chosen to be 1200, i.e. k = 1200. 1200 was the optimal value chosen experimentally. ---------------------------------- **ACOUSTIC VSM; X A** Acoustic VSM is constructed in two steps; 1) Extracting the bottleneck features (BNF) from speech and 2) Modeling BNF using the i-Vector extraction framework. We use the same Deep Neural Network (DNN) based ASR system to extract the BNF as in our previous works<cite> [2,</cite> 13] .",
  "y": "similarities uses"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_5",
  "x": "This update information is encoded in a low dimensional latent vector known as an i-Vector. The latent variable model used to extract i-Vector is called Total Variability Subspace Model and is given by the equation: where u is GMM-UBM mean supervector. v is the latent vector, known as the i \u2212 V ector and T is the lower dimensional Vector Subspace. The parameters of the model are estimated using Maximum Likelihood training criterion. For a detailed explanation of i-Vector modeling framework, reader is directed to excellent work in [15, 11] . In this work, GMM-UBM model has 2048 gaussian components, MFCC features are extracted using a 25 ms window and the i-Vectors are 400 dimensional <cite>[2]</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_6",
  "x": "Finally, we construct the acoustic VSM, X A \u2208 R N \u00d7400 , where the i th row is the 400 dimensional i-Vector representation corresponding to the speech utterance, a i . We also perform Linear Discriminant Analysis (LDA) and Within Class Co-variance Normalization (WCCN) on the Acoustic Vector Space, to increase the discriminative strength of the VSM. This method has been shown to improve DID (LID) performance<cite> [2,</cite> 11] . Here, we give a brief overview of the mathematical foundations of the CCA . Fig 2 gives a probabilistic graphical model of CCA. Nodes of the graph represent Random Variables (RVs) and the structure encodes conditional independence assumptions. X P and X A are the Random Variables corresponding to the Phonotactic and Acoustic views of the data.",
  "y": "background"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_7",
  "x": "Training and test data used in this work is the same as used in <cite>[2]</cite> . Table 1 gives the number of hours of data available for each dialect for training and testing. Train 13  10  11  9  10  Test  2  2  2  2  2   Table 1 . Number of hours of training and testing data for each dialect Table 2 shows the number of speech utterances that are available for training and testing the DID system. Train 1720 1907 1059 1934 1820  Test  315  348  238  355  265   Table 2 . Number of training and test utterances for DID system development ---------------------------------- **DATA EGY GLF LAV NOR MSA** Training data consist of recording from the Arabic Broadcast domain and contains utterances spoken in all the five dialects; EGY, GLF, LAV, MSA and NOR.",
  "y": "similarities uses"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_8",
  "x": "More details about the train and test data can be found in<cite> [2,</cite> 18] . Fig 3 gives an overview of our DID system, which can be seen as a combination of two broad components; 1) Vector Space Modeling Component and 2) Back-end classifier. ---------------------------------- **SYSTEM DESCRIPTION** The most important pieces of the DID system are the four latent Vector Subspaces; 1) \u03a0: which is learned by performing SVD on the n-gram phootactic term-document matrix (Section 2.1), 2) T: is the total variability subspace that is learned in an unsupervised manner in the i-Vector framework (Section 2.2), 3) \u03c6 p : is the latent vector subspace corresponding to the Phonotactic VSM learned in the CCA Vector Space modeling framework and 4) \u03c6 a : is the latent vector subspace corresponding to Acoustic VSM learned in the CCA vector space modeling framework (Section 2.3). The shared VSM is then fed to a back-end discriminative classifier. In our case, we use multi-class logistic regression, also known as softmax classification for DID [19, 20] .",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_0",
  "x": "Labels on the two ends of an edge, initially indicating fields of tables in a database, are considered as semantic roles of the corresponding words. Abstract denotation is proposed to capture the meaning of this abstract version of DCS tree, and a textual inference system based on abstract denotation is built (<cite>Tian et al., 2014</cite>) . It is quite natural to apply DCS trees, a simple and expressive semantic representation, to textual inference; however the use of abstract denotations to convey logical inference is somehow unusual. There are two seemingly obvious way to equip DCS with logical inference: (i) at the tree level, by defining a set of logically sound transformations of DCS trees; or (ii) at the logic level, by converting DCS trees to first order predicate logic (FOL) formulas and then utilizing a theorem prover. For (i), it may not be easy to enumerate all types of logically sound transformations, but tree transformations can be seen as an approximation of logical inference. For (ii), abstract denotation is more efficient than FOL formula, because abstract denotation eliminates quantifiers and meanings of natural language texts can be represented by atomic sentences. To elaborate the above discussion and to provide more topics to the literature, in this paper we discuss the following four questions: ( \u00a72) How well can tree transformation approximate logical inference? ( \u00a73) With rigorous inference on DCS trees, where does logic contribute in the system of <cite>Tian et al. (2014)</cite> In the tree transformation based approach to RTE, it has been realized that some gaps between T and H cannot be filled even by a large number of tree transformation rules extracted from corpus (BarHaim et al., 2007a) .",
  "y": "uses"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_1",
  "x": "It is quite natural to apply DCS trees, a simple and expressive semantic representation, to textual inference; however the use of abstract denotations to convey logical inference is somehow unusual. There are two seemingly obvious way to equip DCS with logical inference: (i) at the tree level, by defining a set of logically sound transformations of DCS trees; or (ii) at the logic level, by converting DCS trees to first order predicate logic (FOL) formulas and then utilizing a theorem prover. For (i), it may not be easy to enumerate all types of logically sound transformations, but tree transformations can be seen as an approximation of logical inference. For (ii), abstract denotation is more efficient than FOL formula, because abstract denotation eliminates quantifiers and meanings of natural language texts can be represented by atomic sentences. To elaborate the above discussion and to provide more topics to the literature, in this paper we discuss the following four questions: ( \u00a72) How well can tree transformation approximate logical inference? ( \u00a73) With rigorous inference on DCS trees, where does logic contribute in the system of <cite>Tian et al. (2014)</cite> In the tree transformation based approach to RTE, it has been realized that some gaps between T and H cannot be filled even by a large number of tree transformation rules extracted from corpus (BarHaim et al., 2007a) . For example in Figure 1 , it is possible to extract the rule blamed for death \u2192 cause loss of life, but not easy to extract tropical storm Debby \u2192 storm, because \"Debby\" could be an arbitrary name which may not even appear in the corpus. This kind of gaps was typically addressed by approximate matching methods, for example by counting common sub-graphs of T and H, or by computing a cost of tree edits that convert T to H. In the example of Figure 1 , we would expect that T is \"similar enough\" (i.e. has many common sub-graphs) with H, or the cost to convert T into H (e.g. by deleting the node Debby and then add the node storm) is low.",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_2",
  "x": "The following is a modified example from RTE2-dev: T: Hurricane Isabel, which caused significant damage, was a tropical storm when she entered Virginia. Note the coreference between Hurricane Isabel and she, suggesting us to copy the subtree of Hurricane Isabel to she, in a tree edit approach. This is not enough yet, because the head storm in T is not placed at the subject of cause. The issue is indeed very logical: from \"Hurricane Isabel = she\", \"Hurricane Isabel = storm\", \"she = subject of enter\" and \"Hurricane Isabel = subject of cause\", we can imply that \"storm = subject of enter = subject of cause\". 3 Alignment with logical clues <cite>Tian et al. (2014)</cite> proposed a way to generate onthe-fly knowledge to fill knowledge gaps: if H is not proven, compare DCS trees of T and H to generate path alignments (e.g. blamed for death \u223c cause loss of life, as underscored in Figure 1) ; evaluate the path alignments by a similarity score function; and path alignments with a score greater than a threshold (0.4) are accepted and converted to inference rules. The word vectors <cite>Tian et al. (2014)</cite> use to calculate similarities are reported able to capture semantic compositions by simple additions and subtractions (Mikolov et al., 2013) .",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_3",
  "x": "3 Alignment with logical clues <cite>Tian et al. (2014)</cite> proposed a way to generate onthe-fly knowledge to fill knowledge gaps: if H is not proven, compare DCS trees of T and H to generate path alignments (e.g. blamed for death \u223c cause loss of life, as underscored in Figure 1) ; evaluate the path alignments by a similarity score function; and path alignments with a score greater than a threshold (0.4) are accepted and converted to inference rules. The word vectors <cite>Tian et al. (2014)</cite> use to calculate similarities are reported able to capture semantic compositions by simple additions and subtractions (Mikolov et al., 2013) . This is also the case when used as knowledge resource for RTE, for example the similarities between blamed+death and cause+loss+life, or between found+shot+dead and killed, are computed > 0.4. However, generally such kind of similarity is very noisy. <cite>Tian et al. (2014)</cite> used some logical clues to filter out irrelevant path alignments, which helps to keep a high precision. To evaluate the effect of such logical filters, we compare it with some other alignment strategies, the performance of which on RTE5-test data is shown in Table 1 . Each strategy is described in the following.",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_4",
  "x": "This is not enough yet, because the head storm in T is not placed at the subject of cause. The issue is indeed very logical: from \"Hurricane Isabel = she\", \"Hurricane Isabel = storm\", \"she = subject of enter\" and \"Hurricane Isabel = subject of cause\", we can imply that \"storm = subject of enter = subject of cause\". 3 Alignment with logical clues <cite>Tian et al. (2014)</cite> proposed a way to generate onthe-fly knowledge to fill knowledge gaps: if H is not proven, compare DCS trees of T and H to generate path alignments (e.g. blamed for death \u223c cause loss of life, as underscored in Figure 1) ; evaluate the path alignments by a similarity score function; and path alignments with a score greater than a threshold (0.4) are accepted and converted to inference rules. The word vectors <cite>Tian et al. (2014)</cite> use to calculate similarities are reported able to capture semantic compositions by simple additions and subtractions (Mikolov et al., 2013) . This is also the case when used as knowledge resource for RTE, for example the similarities between blamed+death and cause+loss+life, or between found+shot+dead and killed, are computed > 0.4. However, generally such kind of similarity is very noisy. <cite>Tian et al. (2014)</cite> used some logical clues to filter out irrelevant path alignments, which helps to keep a high precision.",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_5",
  "x": "Logical inference is shown to be useful for RTE, as <cite>Tian et al. (2014)</cite> demonstrates a system with competitive results. However, despite the expectation that all entailment matters can be explained logically, our observation is that currently logical inference only fills very limited short gaps from T to H. The logical phenomena easily addressed by <cite>Tian et al. (2014)</cite> Table 2 : Proportion (%) of exit status of Prover9 The system of <cite>Tian et al. (2014)</cite> generated onthe-fly knowledge to join several fragments in T and wrongly proved H. In examples of such complexity, distributional similarity is no longer reliable. However, it may be possible to build a priori logical models at the meta level, such as on epistemic, intentional and reportive attitudes. The models then can provide signals for semantic parsing to connect the logic to natural language, such as the words \"grant\", \"decertify\", and \"accuse\" in the above example. We hope this approach can bring new progress to RTE and other semantic processing tasks. ----------------------------------",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_6",
  "x": "This is very similar to the system described in Bar-Haim et al. . NoFilter + Coverage Same as above, but all paths alignments with similarity score > 0.4 are accepted. ---------------------------------- **HOW CAN LOGICAL INFERENCE HELP RTE?** Logical inference is shown to be useful for RTE, as <cite>Tian et al. (2014)</cite> demonstrates a system with competitive results. However, despite the expectation that all entailment matters can be explained logically, our observation is that currently logical inference only fills very limited short gaps from T to H. The logical phenomena easily addressed by <cite>Tian et al. (2014)</cite> Table 2 : Proportion (%) of exit status of Prover9",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_7",
  "x": "---------------------------------- **HOW CAN LOGICAL INFERENCE HELP RTE?** Logical inference is shown to be useful for RTE, as <cite>Tian et al. (2014)</cite> demonstrates a system with competitive results. However, despite the expectation that all entailment matters can be explained logically, our observation is that currently logical inference only fills very limited short gaps from T to H. The logical phenomena easily addressed by <cite>Tian et al. (2014)</cite> Table 2 : Proportion (%) of exit status of Prover9 The system of <cite>Tian et al. (2014)</cite> generated onthe-fly knowledge to join several fragments in T and wrongly proved H. In examples of such complexity, distributional similarity is no longer reliable. However, it may be possible to build a priori logical models at the meta level, such as on epistemic, intentional and reportive attitudes. The models then can provide signals for semantic parsing to connect the logic to natural language, such as the words \"grant\", \"decertify\", and \"accuse\" in the above example.",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_8",
  "x": "However, it may be possible to build a priori logical models at the meta level, such as on epistemic, intentional and reportive attitudes. The models then can provide signals for semantic parsing to connect the logic to natural language, such as the words \"grant\", \"decertify\", and \"accuse\" in the above example. We hope this approach can bring new progress to RTE and other semantic processing tasks. ---------------------------------- **EFFICIENCY OF ABSTRACT DENOTATIONS** To evaluate the efficiency of logical inference on abstract denotations, we took 110 true entailment pairs from RTE5 development set, which are also pairs that can be proven with on-the-fly knowledge. We plot the running time of <cite>Tian et al. (2014)</cite> 's inference engine (single-threaded) on a 2.27GHz Xeon CPU, with respect to the weighted sum of all statements 2 , as shown in Figure 3 .",
  "y": "uses"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_9",
  "x": "3 Sec.\" column), and only 16% pairs can be proven in 5 minutes (the \"Orig. 5 Min.\" column), showing severe difficulties for an FOL prover to handle textual inferences with many (usually hundreds of) on-the-fly rules. As such, we use <cite>Tian et al. (2014)</cite> 's inference engine to pin down statements that are actually needed for proving H (usually just 2 or 3 statements), and try to prove H by Prover9 again, using only necessary statements. Proven pairs in 5 minutes then jump to 82% (the \"Red. 5 Min.\" column), showing that a large number of on-the-fly rules may drastically increase computation cost. Still, nearly 20% pairs cannot be proven even in this setting, suggesting that traditional FOL prover is not suited for textual inference. ---------------------------------- **CONCLUSION AND FUTURE WORK**",
  "y": "uses"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_10",
  "x": "Improvement of similarity score To calculate phrase similarities, <cite>Tian et al. (2014)</cite> use the cosine similarity of sums of word vectors, which ignores syntactic information. We plan to add syntactic information to words by some supertags, and learn a vector space embedding for this structure. Integration of FreeBase to RTE It would be exciting if we can utilize the huge amount of FreeBase data in RTE task. Using the framework of abstract denotation, meanings of sentences can be explained as relational database queries; to convert it to FreeBase data queries is like relational to ontology schema matching. In order to make effective use of FreeBase data, we also need to recognize entities and relations in natural language sentences. Previous research on semantic parsing will be very helpful for learning such mapping. Winograd Schema Challenge (WSC) As the RTE task, WSC (Levesque et al., 2012 ) also provides a test bed for textual inference systems.",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_11",
  "x": "It would be exciting if we can combine different types of structured data with natural language in semantic processing tasks. Directions of our future work are described below. Improvement of similarity score To calculate phrase similarities, <cite>Tian et al. (2014)</cite> use the cosine similarity of sums of word vectors, which ignores syntactic information. We plan to add syntactic information to words by some supertags, and learn a vector space embedding for this structure. Integration of FreeBase to RTE It would be exciting if we can utilize the huge amount of FreeBase data in RTE task. Using the framework of abstract denotation, meanings of sentences can be explained as relational database queries; to convert it to FreeBase data queries is like relational to ontology schema matching. In order to make effective use of FreeBase data, we also need to recognize entities and relations in natural language sentences.",
  "y": "extends"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_12",
  "x": "The models then can provide signals for semantic parsing to connect the logic to natural language, such as the words \"grant\", \"decertify\", and \"accuse\" in the above example. We hope this approach can bring new progress to RTE and other semantic processing tasks. ---------------------------------- **EFFICIENCY OF ABSTRACT DENOTATIONS** To evaluate the efficiency of logical inference on abstract denotations, we took 110 true entailment pairs from RTE5 development set, which are also pairs that can be proven with on-the-fly knowledge. We plot the running time of <cite>Tian et al. (2014)</cite> 's inference engine (single-threaded) on a 2.27GHz Xeon CPU, with respect to the weighted sum of all statements 2 , as shown in Figure 3 . The graph shows all pairs can be proven in 6 seconds, and proof time scales logarithmically on weight of statements.",
  "y": "similarities"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_0",
  "x": "**INTRODUCTION** Until recently, the dominant paradigm in approaching natural language processing (NLP) tasks has been to concentrate on neural architecture design, using only task-specific data and shallow pre-trained word embeddings, such as GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) . Numerous literature surveys detail this historical neural progress: Young et al. (2018) describe a series of increasingly intricate neural NLP approaches, all of which follow the classical recipe of training on word embeddings of task-specific data. In their targeted review of sentence-pair modeling, Lan and Xu (2018) likewise examine neural networks that abide by this paradigm. * Equal contribution. The NLP community is, however, witnessing a dramatic paradigm shift toward the pretrained deep language representation model, which achieves state of the art in question answering, sentiment classification, and similarity modeling, to name a few. Bidirectional Encoder Representations from Transformers (BERT;<cite> Devlin et al., 2018)</cite> represents one of the latest developments in this line of work.",
  "y": "background"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_1",
  "x": "These task-specific neural architectures have dominated the NLP literature-until recently. Enabled by more computational resources and data, the deep language representation model has greatly improved state of the art on a variety of tasks. Under this paradigm, a neural network is first pre-trained on vast amounts of text under an unsupervised objective (e.g., masked language modeling and next-sentence prediction), and then fine-tuned on task-specific data. The resulting models achieve state of the art in question answering, named-entity recognition, and natural language inference, to name a few. Bidirectional Encoder Representations from Transformers (BERT;<cite> Devlin et al., 2018)</cite> currently represents state of the art, vastly outperforming previous models, such as the Generative Pretrained Transformer (GPT; Radford et al.) and Embeddings from Language Models (ELMo; Peters et al., 2018) . ---------------------------------- **MODEL**",
  "y": "background"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_2",
  "x": "**MODEL** We begin with the pre-trained BERT base and BERT large models, which respectively represent the normal and large model variants <cite>(Devlin et al., 2018)</cite> . To adapt BERT for document classifica- Figure 1 . During fine-tuning, we optimize the entire model end-to-end, with the additional softmax classifier parameters W \u2208 IR K\u00d7H , where H is the dimension of the hidden state vectors and K is the number of classes. We minimize the crossentropy and binary cross-entropy loss for singlelabel and multi-label tasks, respectively. ---------------------------------- **EXPERIMENTAL SETUP**",
  "y": "uses"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_3",
  "x": "The resulting models achieve state of the art in question answering, named-entity recognition, and natural language inference, to name a few. Bidirectional Encoder Representations from Transformers (BERT;<cite> Devlin et al., 2018)</cite> currently represents state of the art, vastly outperforming previous models, such as the Generative Pretrained Transformer (GPT; Radford et al.) and Embeddings from Language Models (ELMo; Peters et al., 2018) . ---------------------------------- **MODEL** We begin with the pre-trained BERT base and BERT large models, which respectively represent the normal and large model variants <cite>(Devlin et al., 2018)</cite> . To adapt BERT for document classifica- Figure 1 . During fine-tuning, we optimize the entire model end-to-end, with the additional softmax classifier parameters W \u2208 IR K\u00d7H , where H is the dimension of the hidden state vectors and K is the number of classes.",
  "y": "extends"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_4",
  "x": "As is the case with<cite> Devlin et al. (2018)</cite> , we find that choosing a batch size of 16, learning rate of 2\u00d710 \u22125 , and MSL of 512 tokens yields optimal performance on the validation sets of all datasets. Hyperparameter study. To gauge the improvement over the default hyperparameters, as well as to highlight the differences in fine-tuning BERT for document classification, we explore varying several key hyperparameters: namely, the number of epochs and the MSL. Originally,<cite> Devlin et al. (2018)</cite> find that fine-tuning for three or four epochs works well for both small and large datasets alike. They also apply a generous MSL of 512, which may be unnecessary for document classification, where fewer tokens may suffice in determining the topic. Furthermore, while conducting our experiments, we find that even fine-tuning BERT is a computationally intensive task. We argue that it is important to study these two hyperparameters, as they are major determinants of the computational resources required to fine-tune BERT.",
  "y": "similarities"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_5",
  "x": "As is the case with<cite> Devlin et al. (2018)</cite> , we find that choosing a batch size of 16, learning rate of 2\u00d710 \u22125 , and MSL of 512 tokens yields optimal performance on the validation sets of all datasets. Hyperparameter study. To gauge the improvement over the default hyperparameters, as well as to highlight the differences in fine-tuning BERT for document classification, we explore varying several key hyperparameters: namely, the number of epochs and the MSL. Originally,<cite> Devlin et al. (2018)</cite> find that fine-tuning for three or four epochs works well for both small and large datasets alike. They also apply a generous MSL of 512, which may be unnecessary for document classification, where fewer tokens may suffice in determining the topic. Furthermore, while conducting our experiments, we find that even fine-tuning BERT is a computationally intensive task. We argue that it is important to study these two hyperparameters, as they are major determinants of the computational resources required to fine-tune BERT.",
  "y": "background"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_6",
  "x": "To gauge the improvement over the default hyperparameters, as well as to highlight the differences in fine-tuning BERT for document classification, we explore varying several key hyperparameters: namely, the number of epochs and the MSL. Originally,<cite> Devlin et al. (2018)</cite> find that fine-tuning for three or four epochs works well for both small and large datasets alike. They also apply a generous MSL of 512, which may be unnecessary for document classification, where fewer tokens may suffice in determining the topic. Furthermore, while conducting our experiments, we find that even fine-tuning BERT is a computationally intensive task. We argue that it is important to study these two hyperparameters, as they are major determinants of the computational resources required to fine-tune BERT. BERT large , for example, requires eight V100s to fine-tune on our datasets, which is of course prohibitive. The number of epochs determines the duration of finetuning, while maximum sequence length dictates the models' memory and computational footprint during both fine-tuning and inference.",
  "y": "motivation"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_7",
  "x": "---------------------------------- **MODEL QUALITY** Trending with<cite> Devlin et al. (2018)</cite> , BERT large achieves state-of-the-art results on all four datasets, followed by BERT base (see Table 2 , rows 11 and 12). The considerably simpler LSTM reg model (row 10) achieves a high F 1 and accuracy of 87.0 and 52.8, respectively, coming close to the quality of BERT base . Surprisingly, the LR and SVM baselines yield competitive results for the multi-label datasets. For instance, the SVM approaches BERT base results on Reuters, with an F 1 score of 86.1, astonishingly exceeding most of our neural baselines (rows 2-11). This can also be observed on AAPD, where the SVM surpasses most of the neural models, except SGM, LSTM reg , and BERT.",
  "y": "similarities background"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_8",
  "x": "Alternatively, one can argue that, since IMDB contains longer documents, truncating tokens may hurt less. Figure 2 shows that this is not the case, since truncating to even 256 tokens causes accuracy to fall lower than that of the much smaller LSTM reg (see Table 2 ). From these results, we conclude that any amount of truncation is detrimental in document classification, but the level of degradation may differ. Epoch analysis. The rightmost two subplots in Figure 2 illustrate the F 1 score of BERT fine-tuned using a various number of epochs for AAPD and Reuters. Contrary to<cite> Devlin et al. (2018)</cite> , who achieve state of the art on small datasets with only a few epochs of fine-tuning, we find that smaller datasets require many more epochs to converge. On both the datasets (see Figure 2) , we see a significant drop in model quality when the BERT models are fine-tuned on only four epochs, as suggested in the original paper.",
  "y": "differences"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_0",
  "x": "The dataset for the shared task was introduced by <cite>Thorne et al. (2018)</cite> and consists of 185,445 claims. Table 1 shows three instances from the data set with the claim, the evidence and the verdict. Table 1 : Examples of claims, the extracted evidence from Wikipedia and the verdicts from the shared task dataset<cite> (Thorne et al., 2018)</cite> The baseline system described by <cite>Thorne et al. (2018)</cite> uses 3 major components: \u2022 Document Retrieval: Given a claim, identify relevant documents from Wikipedia which contain the evidence to verify the claim. <cite>Thorne et al. (2018)</cite> used the document retrieval component from the DrQA system (Chen et al., 2017) , which returns the k nearest documents for a query using cosine similarity between binned unigram and bigram TF-IDF vectors. \u2022 Sentence Selection: Given the set of retrieved document, identify the candidate evidence sentences. <cite>Thorne et al. (2018)</cite> used a modified document retrieval component of DrQA (Chen et al., 2017) to select the top most similar sentences w.r.t the claim, using bigram TF-IDF with binning.",
  "y": "background"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_1",
  "x": "Table 1 shows three instances from the data set with the claim, the evidence and the verdict. Table 1 : Examples of claims, the extracted evidence from Wikipedia and the verdicts from the shared task dataset<cite> (Thorne et al., 2018)</cite> The baseline system described by <cite>Thorne et al. (2018)</cite> uses 3 major components: \u2022 Document Retrieval: Given a claim, identify relevant documents from Wikipedia which contain the evidence to verify the claim. <cite>Thorne et al. (2018)</cite> used the document retrieval component from the DrQA system (Chen et al., 2017) , which returns the k nearest documents for a query using cosine similarity between binned unigram and bigram TF-IDF vectors. \u2022 Sentence Selection: Given the set of retrieved document, identify the candidate evidence sentences. <cite>Thorne et al. (2018)</cite> used a modified document retrieval component of DrQA (Chen et al., 2017) to select the top most similar sentences w.r.t the claim, using bigram TF-IDF with binning. \u2022 Textual Entailment: For the entailment task, training is done using labeled claims paired with evidence (labels are SUPPORTS, REFUTES, NOT ENOUGH INFO).",
  "y": "background"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_2",
  "x": "Table 1 : Examples of claims, the extracted evidence from Wikipedia and the verdicts from the shared task dataset<cite> (Thorne et al., 2018)</cite> The baseline system described by <cite>Thorne et al. (2018)</cite> uses 3 major components: \u2022 Document Retrieval: Given a claim, identify relevant documents from Wikipedia which contain the evidence to verify the claim. <cite>Thorne et al. (2018)</cite> used the document retrieval component from the DrQA system (Chen et al., 2017) , which returns the k nearest documents for a query using cosine similarity between binned unigram and bigram TF-IDF vectors. \u2022 Sentence Selection: Given the set of retrieved document, identify the candidate evidence sentences. <cite>Thorne et al. (2018)</cite> used a modified document retrieval component of DrQA (Chen et al., 2017) to select the top most similar sentences w.r.t the claim, using bigram TF-IDF with binning. \u2022 Textual Entailment: For the entailment task, training is done using labeled claims paired with evidence (labels are SUPPORTS, REFUTES, NOT ENOUGH INFO). <cite>Thorne et al. (2018)</cite> used the decomposable attention model (Parikh et al., 2016) for this task.",
  "y": "background"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_3",
  "x": "<cite>Thorne et al. (2018)</cite> used the document retrieval component from the DrQA system (Chen et al., 2017) , which returns the k nearest documents for a query using cosine similarity between binned unigram and bigram TF-IDF vectors. \u2022 Sentence Selection: Given the set of retrieved document, identify the candidate evidence sentences. <cite>Thorne et al. (2018)</cite> used a modified document retrieval component of DrQA (Chen et al., 2017) to select the top most similar sentences w.r.t the claim, using bigram TF-IDF with binning. \u2022 Textual Entailment: For the entailment task, training is done using labeled claims paired with evidence (labels are SUPPORTS, REFUTES, NOT ENOUGH INFO). <cite>Thorne et al. (2018)</cite> used the decomposable attention model (Parikh et al., 2016) for this task. For the case where multiple sentences are required as evidence, the strings were concatenated. Our system implements changes in all three modules (Section 2), which leads to significant improvements both on the development and test sets.",
  "y": "background"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_4",
  "x": "\u2022 Textual Entailment: For the entailment task, training is done using labeled claims paired with evidence (labels are SUPPORTS, REFUTES, NOT ENOUGH INFO). <cite>Thorne et al. (2018)</cite> used the decomposable attention model (Parikh et al., 2016) for this task. For the case where multiple sentences are required as evidence, the strings were concatenated. Our system implements changes in all three modules (Section 2), which leads to significant improvements both on the development and test sets. On the shared task development set our document retrieval approach covers 94.4% of the claims requiring evidence, compared to 55.30% in the baseline. Further, on the dev set our evidence recall is improved by 33 points over the baseline. For entailment, our model improves the baseline by 7.5 points on dev set.",
  "y": "background"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_5",
  "x": "Our point of interest here is Marnie (film). We only experimented with film to capture the performance gains. One of our future goals is to build better computational models to handle entity ambiguity or entity linking. \u2022 Combined: We use the union of the documents returned by the three approaches as the final set of relevant documents to be used by the Sentence Selection module. Table 2 shows the percentage of claims that can be fully supported or refuted by the retrieved docu-ments before sentence selection on the dev set. We see that our best approach (combined) achieved a high coverage 94.4% compared to the baseline<cite> (Thorne et al., 2018)</cite> of 55.3%. Because we do not have the gold evidences for the blind test set we cannot report the claim coverage using our pipeline .",
  "y": "differences"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_6",
  "x": "For sentence selection, we used the modified document retrieval component of DrQA (Chen et al., 2017) to select sentences using bigram TF-IDF with binning as proposed by<cite> (Thorne et al., 2018)</cite> . We extract the top 5 most similar sentences from the k-most relevant documents using the TF-IDF vector similarity. Our evidence recall is 78.4 as compared to 45.05 in the development set of FEVER<cite> (Thorne et al., 2018)</cite> , which demonstrates the importance of document retrieval in fact extraction and verification. On the blind test set our sentence selection approach achieves an evidence recall of 75.89. However, even though TF-IDF proves to be a strong baseline for sentence selection we noticed on the dev set that using all 5 evidences together introduced additional noise to the entailment model. To solve this, we further filtered the top 3 evidences from the selected 5 evidences using distributed semantic representations. Peters et al. (2018) show how deep contextualized word representations model both complex characteristics of word use (e.g., syntax and semantics), and usage across various linguistic contexts.",
  "y": "similarities uses"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_7",
  "x": "We see that our best approach (combined) achieved a high coverage 94.4% compared to the baseline<cite> (Thorne et al., 2018)</cite> of 55.3%. Because we do not have the gold evidences for the blind test set we cannot report the claim coverage using our pipeline . ---------------------------------- **SENTENCE SELECTION** For sentence selection, we used the modified document retrieval component of DrQA (Chen et al., 2017) to select sentences using bigram TF-IDF with binning as proposed by<cite> (Thorne et al., 2018)</cite> . We extract the top 5 most similar sentences from the k-most relevant documents using the TF-IDF vector similarity. Our evidence recall is 78.4 as compared to 45.05 in the development set of FEVER<cite> (Thorne et al., 2018)</cite> , which demonstrates the importance of document retrieval in fact extraction and verification.",
  "y": "differences"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_8",
  "x": "We then calculated cosine similarity between claim and evidence vectors and extracted the top 3 sentences based on the score. Because there was no penalty involved for poor evidence precision, we returned all five selected sentences as our predicted evidence but used only the top three sentences for the entailment model. ---------------------------------- **TEXTUAL ENTAILMENT** The final stage of our pipeline is recognizing textual entailment. Unlike <cite>Thorne et al. (2018)</cite> , we did not concatenate evidences, but trained our model for each claim-evidence pair. For recognizing textual entailment we used the model introduced by Conneau et al. (2017) in their work on supervised learning of universal sentence representations.",
  "y": "differences"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_0",
  "x": "TITLE AND ABSTRACT IN CHINESE ---------------------------------- **INTRODUCTION** Beam-search has been applied to transition-based dependency parsing in recent studies (Zhang and Clark, 2008; Huang and Sagae, 2010; Hatori et al., 2011) . In addition to reducing search errors compared to greedy search, it also enables the use of global models that accommodate richer non-local features without overfitting, leading to recent state-of-the-art accuracies of transition-based dependency parsing<cite> (Zhang and Nivre, 2011</cite>; Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012) that are competitive with the best graph-based dependency parsers. It has been known that a transition-based parser using global learning, beam-search and rich features gives significantly higher accuracies than one with local learning and greedy search. However, the effects of global learning, beam-search and rich features have not been separately studied.",
  "y": "background"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_1",
  "x": "However, the effects of global learning, beam-search and rich features have not been separately studied. Apart from the natural conclusion that beam-search reduces error propagation compared to greedy search, exactly how these techniques help to improve parsing has not been discussed, and many interesting questions remain unanswered. For example, the contribution of global learning in improving the accuracies has not been separately studied. It has not been shown how global learning affects the accuracies, or whether it is important at all. For another example, it would be interesting to know whether a local, greedy, transition-based parser can be equipped with the rich features of<cite> Zhang and Nivre (2011)</cite> to improve its accuracy, and in particular whether MaltParser (Nivre et al., 2006) can achieve the same level of accuracies as ZPar<cite> (Zhang and Nivre, 2011)</cite> by using the same range of rich feature definitions. In this paper, we answer the above questions empirically. First, we separate out global learning and beam-search, and study the effect of each technique by comparison with a local greedy baseline.",
  "y": "motivation"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_2",
  "x": "For example, the contribution of global learning in improving the accuracies has not been separately studied. It has not been shown how global learning affects the accuracies, or whether it is important at all. For another example, it would be interesting to know whether a local, greedy, transition-based parser can be equipped with the rich features of<cite> Zhang and Nivre (2011)</cite> to improve its accuracy, and in particular whether MaltParser (Nivre et al., 2006) can achieve the same level of accuracies as ZPar<cite> (Zhang and Nivre, 2011)</cite> by using the same range of rich feature definitions. In this paper, we answer the above questions empirically. First, we separate out global learning and beam-search, and study the effect of each technique by comparison with a local greedy baseline. Our results show that significant improvements are achieved only when the two are jointly applied. Second, we show that the accuracies of a local, greedy transition-based parser cannot be improved by adding the rich features of<cite> Zhang and Nivre (2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_3",
  "x": "An interesting question is how such differences in models and algorithms affect empirical errors. McDonald and Nivre (2007) make a comparative analysis of local greedy transition-based MaltParser and global near-exhaustive graph-based MSTParser (McDonald and Pereira, 2006) using the CoNLL-X Shared Task data (Buchholz and Marsi, 2006) , showing that the parsers give near identical overall accuracies, but have very different error distributions according to various metrics. While MaltParser is more accurate on frequently occurring short sentences and dependencies, it performs worse on long sentences and dependencies due to search errors. We present empirical studies of the error distribution of global, beam-search transition-based dependency parsing, using ZPar<cite> (Zhang and Nivre, 2011)</cite> as a representative system. We follow McDonald and Nivre (2007) and perform a comparative error analysis of ZPar, MSTParser and MaltParser using the CoNLL-X shared task data. Our results show that beam-search im-proves the precision on long sentences and dependencies compared to greedy search, while the advantage of transition-based parsing on short dependencies is preserved. Under particular measures, such as precision for arcs at different levels of the trees, ZPar shows characteristics surprisingly similar to MSTParser.",
  "y": "uses"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_4",
  "x": "To make comparisons with local learning under different settings, we make configurations and modifications to ZPar where necessary. Global learning is implemented in the same way as<cite> Zhang and Nivre (2011)</cite> , using the averaged perceptron algorithm (Collins, 2002) and early update (Collins and Roark, 2004) . This is a global learning method in the sense that it tries to maximize accuracy over the entire sentence and not on isolated local transitions. Unless explicitly specified, the same beam size is applied for training and testing when beam-search is applied. Local learning is implemented as a multi-class classifier that predicts the next transition action given a parser configuration (i.e. a stack and an incoming queue), trained using the averaged perceptron algorithm. In local learning, each transition is considered in isolation and there is no global view of the transition sequence needed to parse an entire sentence. Figure 1 shows the UAS of ZPar under different settings, where 'global' refers to a global model trained using the same method as<cite> Zhang and Nivre (2011)</cite> , 'local' refers to a local classifier trained using the averaged perceptron, 'base features' refers to the set of base feature templates in<cite> Zhang and Nivre (2011)</cite> , and 'all features' refers to the set of base and all extended feature templates in<cite> Zhang and Nivre (2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_5",
  "x": "Figure 1 shows the UAS of ZPar under different settings, where 'global' refers to a global model trained using the same method as<cite> Zhang and Nivre (2011)</cite> , 'local' refers to a local classifier trained using the averaged perceptron, 'base features' refers to the set of base feature templates in<cite> Zhang and Nivre (2011)</cite> , and 'all features' refers to the set of base and all extended feature templates in<cite> Zhang and Nivre (2011)</cite> . When the size of the beam is 1, the decoding algorithm is greedy local search. Using base features, a locally trained model gives a UAS of 89.15%, higher than that of a globally trained model (89.04%). Here a global model does not give better accuracies compared to a local model under greedy search. As the size of the beam increses, the UAS of the global model increases, but the UAS of the local model decreases. Global learning gives significantly better accuracies than local learning under beam-search. There are two ways to explain the reason that beam-search hurts the UAS of a locally trained model.",
  "y": "uses"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_6",
  "x": "With greedy local search, the UAS of a local model improves from 89.15% with base features to 89.28% with all features. Beam-search does not bring additional improvements. For further evidence, we add rich non-local features in the same increments as<cite> Zhang and Nivre (2011)</cite> to both ZPar and MaltParser, and evaluate UAS on the same development data set. Original settings are applied to both parsers, with ZPar using global learning and beam-search, and MaltParser using local learning and greedy search. Table 2 shows that while ZPar's accuracy consistently improves with the addition of each new set of features, there is very little impact on MaltParser's accuracy and in some cases the effect is in fact negative, indicating that the locally trained greedy parser cannot benefit from the rich non-local features. Yet another evidence for the support of more complex models by global learning and beamsearch is the work of Bohnet and Nivre (2012) , where non-projective parsing using online reordering (Nivre, 2009 ) and rich features led to significant improvements over greedy search (Nivre, 2009) , achieving state-of-the-art on a range of typologically diverse languages. 3 Characterizing the errors",
  "y": "uses"
 },
 {
  "id": "7404a4e4e23eea9663b580f9959689_0",
  "x": "This paper introduces a system that utilizes parameters learned for a pair Hidden Markov Model (pair HMM) in a shared transliteration generation task 1 . The pair HMM has been used before (Mackay and Kondrak, 2005; Wieling et al., 2007) for string similarity estimation, and is based on the notion of string Edit Distance (ED). String ED is defined here as the total edit cost incurred in transforming a source language string (S) to a target language string (T) through a sequence of edit operations. The edit operations include: (M)atching an element in S with an element in T; (I)nserting an element into T, and (D)eleting an element in S. 1 The generation task is part of the NEWS 2009 machine transliteration shared task<cite> (Li et al., 2009)</cite> Based on all representative symbols used for each of the two languages, emission costs for each of the edit operations and transition parameters can be estimated and used in measuring the similarity between two strings. To generate transliterations using pair HMM parameters, WFST (Graehl, 1997) techniques are adopted. Transliteration training is based mainly on the initial orthographic representation and no explicit phonetic scheme is used. Instead, transliteration quality is tested for different bigram combinations including all English vowel bigram combinations and n-gram combinations specified for Cyrillic Romanization by the US Board on Geographic Names and British Permanent Committee on Geographic Names (BGN/PCGN).",
  "y": "uses background"
 },
 {
  "id": "7404a4e4e23eea9663b580f9959689_1",
  "x": "**EXPERIMENTS** ---------------------------------- **DATA SETUP** The data used is divided according to the experimental runs that were specified for the NEWS 2009 shared transliteration task<cite> (Li et al., 2009</cite> ): a standard run and non-standard runs. The standard run involved using the transliteration system described above that uses pair HMM parameters combined with transformation rules. The English-Russian datasets used here were provided for the NEWS 2009 shared transliteration task (Kumaran and Kellner, 2009): 5977 pairs of names for training, 943 pairs for development, and 1000 for testing. For the non-standard runs, an additional English-Russian dataset extracted from the Geonames data dump was merged with the shared transliteration task data above to form 10481 pairs for training and development.",
  "y": "uses"
 },
 {
  "id": "7404a4e4e23eea9663b580f9959689_3",
  "x": "---------------------------------- **RESULTS** Six measures were used for evaluating system transliteration quality. These include<cite> (Li et al., 2009)</cite> : Accuracy (ACC), Fuzziness in Top-1 (Mean F Score), Mean Reciprocal Rank (MRR), Mean Average Precision for reference transliterations (MAP_R), Mean Average Precision in 10 best candidate transliterations (MAP_10), Mean Average Precision for the system (MAP_sys). Table 1 shows the results obtained using only the data sets provided for the shared transliteration task. The system used for the standard run is \"phmm_rules\" described in section 2 to sub section 2.3. \"phmm_basic\" is the system in which pair HMM parameters are used for transliteration generation but there is no representation for bigrams as described for the system used in the standard run. Table 2 shows the results obtained when additional data from Geonames data dump was used for training and development.",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_0",
  "x": "These representations combine setlike constructs (Hosseini et al., 2014) with hierarchical representations like equation trees (Koncel-Kedziorski et al., 2015; Roy and Roth, 2015; Wang et al., 2018) . Such methods have the benefit of being interpretable, but no semantic representation general enough to solve all varieties of math word problems, including proportion problems and those that map to systems of equations, has been found. Another popular line of research is on purely data-driven solvers. Given enough training data, data-driven models can learn to map word problem texts to arbitrarily complex equations or systems of equations. These models have the additional advantage of being more language-independent than semantic methods, which often rely on parsers and other NLP tools. To train these fully data driven models, large-scale datasets for both English and Chinese were recently introduced<cite> (Wang et al., 2017</cite>; Koncel-Kedziorski et al., 2016) . In response to the success of representation learning elsewhere in NLP, sequence to sequence (seq2seq) models have been applied to algebra problem solving<cite> (Wang et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_1",
  "x": "In response to the success of representation learning elsewhere in NLP, sequence to sequence (seq2seq) models have been applied to algebra problem solving<cite> (Wang et al., 2017)</cite> . These powerful models have been shown to outperform other data-driven approaches in a variety of tasks. However, it is not obvious that solving word problems is best modeled as a sequence prediction task rather than a classification or retrieval task. Downstream applications such as question answering or automated tutoring systems may never have to deal with arbitrarily complex or even unseen equation types, obviating the need for a sequence prediction model. These considerations beg the questions: how do data-driven approaches to math word problem solving compare to each other? How can datadriven approaches benefit from recent advances in neural representation learning? What are the limits of data-driven solvers? In this paper, we thoroughly examine datadriven techniques on three larger algebra word problem datasets (Huang et al., 2016; Koncel-Kedziorski et al., 2016;<cite> Wang et al., 2017)</cite> . We study classification, generation, and information retrieval models, and examine popular extensions to these models such as structured self-attention (Lin et al., 2017) and the use of pretrained word embeddings (Pennington et al., 2014; Peters et al., 2018) .",
  "y": "background"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_2",
  "x": "In this paper, we thoroughly examine datadriven techniques on three larger algebra word problem datasets (Huang et al., 2016; Koncel-Kedziorski et al., 2016;<cite> Wang et al., 2017)</cite> . We study classification, generation, and information retrieval models, and examine popular extensions to these models such as structured self-attention (Lin et al., 2017) and the use of pretrained word embeddings (Pennington et al., 2014; Peters et al., 2018) . Our experiments show that a well-tuned neural equation classifier consistently performs better than more sophisticated solvers. We provide evidence that pretrained word embeddings, useful in other tasks, are not helpful for word problem solving. Advanced modeling such as structured self-attention is not shown to improve performance versus a well-tuned BiLSTM Classifier. Our error analysis supports the idea that, while data-driven techniques are powerful and robust, many word problems require semantic or world knowledge that cannot be easily incorporated into an end-to-end learning framework. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_3",
  "x": "**MODELS** ---------------------------------- **RETRIEVAL** Retrieval methods map test word problem texts at inference time to the nearest training problem according to some similarity metric. The nearest neighbor's equation template is then filled in with numbers from the test problem and solved. Following <cite>Wang et al. (2017)</cite> , we use Jaccard distance in this model. For test problem S and training problem T , the Jaccard similarity is computed as: jacc(S, T ) = S\u2229T S\u222aT .",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_4",
  "x": "We generate equation templates with seq2seq models (Sutskever et al., 2014) with attention mechanisms (Luong et al., 2015) . These models condition the token-by-token generation of the equation template on encodings of the word problem text. Following <cite>Wang et al. (2017)</cite> we evaluate a seq2seq with LSTMs as the encoder and decoder. We also evaluate the use of Convolutional Neural Networks (CNNs) in the encoder and decoder. ---------------------------------- **EXPERIMENTS** ----------------------------------",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_5",
  "x": "Following <cite>Wang et al. (2017)</cite> we evaluate a seq2seq with LSTMs as the encoder and decoder. We also evaluate the use of Convolutional Neural Networks (CNNs) in the encoder and decoder. ---------------------------------- **EXPERIMENTS** ---------------------------------- **EXPERIMENTAL SETUP** Datasets For comparison, we report solution accuracy on the Chinese language Math23K dataset<cite> (Wang et al., 2017)</cite> , and the English language DRAW (Upadhyay and Chang, 2015) and MAWPS (Koncel-Kedziorski et al., 2016) datasets.",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_6",
  "x": "Datasets For comparison, we report solution accuracy on the Chinese language Math23K dataset<cite> (Wang et al., 2017)</cite> , and the English language DRAW (Upadhyay and Chang, 2015) and MAWPS (Koncel-Kedziorski et al., 2016) datasets. Math23K and MAWPS consist of single equation problems, and DRAW contains both single and simultaneous equation problems. Details on the datasets are shown in Table 1 . The Math23K dataset contains problems with possibly irrelevant quantities. To prune these quantities, we implement a significant number identifier (SNI) as discussed in <cite>Wang et al. (2017)</cite> . Our best accuracy for SNI is 97%, slightly weaker than previous results. each dataset.",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_7",
  "x": "Neither of these methods help over the English language data. It appears that the ELMo technique may require more training examples before it can improve solution accuracy. The previous state of the art model for the DRAW dataset is described in Upadhyay and Chang (2015) . The state of the art for Math23K, described in <cite>Wang et al. (2017)</cite> , uses a hybrid Jaccard retrieval and seq2seq model. All models shown here fall well short of the highest possible classification/retrieval accuracy, shown in Table 2 as \"Oracle\". This gap invites a more detailed error analysis regarding the possible limitations of data-driven solvers. ----------------------------------",
  "y": "background"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_8",
  "x": "Wang et al. (2018) advance this line of work even further by modeling the search using deep Q-learning. Still, these semantic approaches are limited by their inability to model systems of equations as well as use of hand-engineered features. Data-driven math word problem solvers include , who learn to predict equation templates and subsequently align numbers and unknowns from the text. Zhou et al. (2015) only assign numbers to the predicted template, reducing the search space significantly. More recently, <cite>Wang et al. (2017)</cite> provide a large dataset of Chinese algebra word problems and learn a hybrid model consisting of both retrieval and seq2seq components. The current work extends these approaches by exploring advanced techniques in data-driven solving. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_0",
  "x": "Automatic disentanglement could be used to provide more interpretable results when searching over chat logs, and to help users understand what is happening when they join a channel. Over a decade of research has considered conversation disentanglement (Shen et al., 2006) , but using datasets that are either small (2,500 messages,<cite> Elsner and Charniak, 2008)</cite> or not released (Adams and Martell, 2008) . * jkummerf@umich.edu We introduce a conversation disentanglement dataset of 77,563 messages of IRC manually annotated with reply-to relations between messages. 1 Our data is sampled from a technical support channel at 173 points in time between 2004 and 2018, providing a diverse set of speakers and topics, while remaining in a single domain. Our data is the first to include context, which differentiates messages that start a conversation from messages that are responding to an earlier point in time. We are also the first to adjudicate disagreements in disentanglement annotations, producing higher quality development and test sets.",
  "y": "motivation background"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_1",
  "x": "These all start with ===, but not all messages starting with === are system messages, as shown by the second message in Figure 1 . 3 Related Work IRC Disentanglement Data: The most significant work on conversation disentanglement is a line of papers developing data and models for the #Linux IRC channel<cite> (Elsner and Charniak, 2008</cite>; Elsner and Schudy, 2009; Charniak, 2010, 2011) . Until now, their dataset was the only publicly available set of messages with annotated conversations (partially re-annotated by Mehri and Carenini (2017) with reply-structure graphs), and has been used for training and evaluation in subsequent work (Wang and Oard, 2009; Mehri and Carenini, 2017; Jiang et al., 2018) . We are aware of three other IRC disentanglement datasets. First, Adams and Martell (2008) studied disentanglement and topic identification, but did not release their data. Second, Riou et al. (2015) annotated conversations and discourse relations in the #Ubuntu-fr channel (French Ubuntu support). Third, Lowe et al. (2015 Lowe et al. ( , 2017 heuristically extracted conversations from the #Ubuntu channel.",
  "y": "background"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_2",
  "x": "3 Related Work IRC Disentanglement Data: The most significant work on conversation disentanglement is a line of papers developing data and models for the #Linux IRC channel<cite> (Elsner and Charniak, 2008</cite>; Elsner and Schudy, 2009; Charniak, 2010, 2011) . Using our data we provide the first empirical evaluation of their method.",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_3",
  "x": "Other Disentanglement Data: IRC is not the only form of synchronous group conversation online. Other platforms with similar communication formats have been studied in settings such as classes (Wang et al., 2008; Dulceanu, 2016) , support communities (Mayfield et al., 2012) , and customer service (Du et al., 2017) . Unfortunately, only one of these resources (Dulceanu, 2016) is available, possibly due to privacy concerns. Another stream of research has used userprovided structure to get conversation labels (Shen et al., 2006; Domeniconi et al., 2016) and replyto relations (Wang and Ros\u00e9, 2010; Wang et al., 2011a; Aumayr et al., 2011; Balali et al., 2013 Balali et al., , 2014 Chen et al., 2017a) . By removing these labels and mixing conversations they create a disentanglement problem. While convenient, this risks introducing a bias, as people write differently when explicit structure is defined, and only a few papers have released data (Abbott et al., 2016; Zhang et al., 2017; Louis and Cohen, 2015) . Models: <cite>Elsner and Charniak (2008)</cite> explored various message-pair feature sets and linear classifiers, combined with local and global inference methods.",
  "y": "background"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_4",
  "x": "While convenient, this risks introducing a bias, as people write differently when explicit structure is defined, and only a few papers have released data (Abbott et al., 2016; Zhang et al., 2017; Louis and Cohen, 2015) . Models: <cite>Elsner and Charniak (2008)</cite> explored various message-pair feature sets and linear classifiers, combined with local and global inference methods. Their system is the only publicly released statistical model for disentanglement of chat conversation, but most of the other work cited above applied similar models. We evaluate their model on both our data and our re-annotated version of their data. Recent work has applied neural networks (Mehri and Carenini, 2017; Guo et al. (2017) 1,500 1 48 hr 5 n/a 2 Table 1 : Annotated disentanglement dataset comparison. Our data is much larger than prior work, one of the only released sets, and the only one with context and adjudication. '+a' indicates there was an adjudication step to resolve disagreements. '?' indicates the value is not in the paper and the authors no longer have access to the data. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_5",
  "x": "We introduce a manually annotated dataset of 77,563 messages: 74,963 from the #Ubuntu IRC channel, 3 and 2,600 messages from the #Linux IRC channel. 4 Annotating the #Linux data enables comparison with <cite>Elsner and Charniak (2008)</cite> , while the #Ubuntu channel has over 34 million messages, making it an interesting largescale resource for dialogue research. It also allows us to evaluate Lowe et al. (2015 Lowe et al. ( , 2017 's widely used heuristically disentangled conversations. When choosing samples we had to strike a balance between the number of samples and the size of each one. We sampled the training set in three ways: (1) 95 uniform length samples, (2) 10 smaller samples to check annotator agreement, and (3) 48 time spans of one hour that are diverse in terms of the number of messages, the number of participants, and what percentage of messages are directed. For additional details of the data selection process, see the supplementary material. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_6",
  "x": "In Channel Two, we also see mistakes and ambiguous cases, including a particularly long discussion about a user's financial difficulties that could be divided in multiple ways (also noted by <cite>Elsner and Charniak (2008)</cite> ). Graphs: We measure agreement on the graph structure annotation using Cohen (1960) 's \u03ba. This measure of inter-rater reliability corrects for chance agreement, accounting for the class imbalance between linked and not-linked pairs. Values are in the good agreement range proposed by Altman (1990) , and slightly higher than for Mehri and Carenini (2017)'s annotations. Results are not shown for <cite>Elsner and Charniak (2008)</cite> because they did not annotate graphs. ---------------------------------- **CONVERSATIONS:**",
  "y": "similarities"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_9",
  "x": "**CONVERSATIONS:** We consider three metrics: 6 (1) Variation of Information (VI, Meila, 2007) . A measure of information gained or lost when going from one clustering to another. It is the sum of conditional entropies H(Y |X) + H(X|Y ), where X and Y are clusterings of the same set of items. We consider a scaled version, using the bound for n items that VI(X; Y ) \u2264 log(n), and present 1\u2212VI so that larger values are better. (2) One-to-One Overlap (1-1,<cite> Elsner and Charniak, 2008)</cite> . Percentage overlap when conversations from two annotations are optimally paired up using the max-flow algorithm.",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_10",
  "x": "Second, <cite>Elsner and Charniak (2008)</cite> and Lowe et al. (2017) per-form similarly, with one doing better on VI and the other on 1-1, though <cite>Elsner and Charniak (2008)</cite> do consistently better across the exact conversation extraction metrics. Third, our methods do best, with x10 vote best in all cases except precision, where the intersect approach is much better. Dataset Variations: Table 5 shows results for the feedforward model with several modifications to the training set, designed to test corpus design decisions. Removing context does not substantially impact results. Decreasing the data size to match <cite>Elsner and Charniak (2008)</cite> 's training set leads to worse results, both if the sentences are from diverse contexts (3rd row), and if they are from just two contexts (bottom row). We also see a substantial increase in the standard deviation when only two samples are used, indicating that performance is not robust when the data is not widely sampled. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_11",
  "x": "First, the baseline has consistently low scores since it forms a single conversation containing all messages. Second, <cite>Elsner and Charniak (2008)</cite> and Lowe et al. (2017) per-form similarly, with one doing better on VI and the other on 1-1, though <cite>Elsner and Charniak (2008)</cite> do consistently better across the exact conversation extraction metrics. Third, our methods do best, with x10 vote best in all cases except precision, where the intersect approach is much better. Dataset Variations: Table 5 shows results for the feedforward model with several modifications to the training set, designed to test corpus design decisions. Removing context does not substantially impact results. Decreasing the data size to match <cite>Elsner and Charniak (2008)</cite> 's training set leads to worse results, both if the sentences are from diverse contexts (3rd row), and if they are from just two contexts (bottom row). We also see a substantial increase in the standard deviation when only two samples are used, indicating that performance is not robust when the data is not widely sampled.",
  "y": "differences uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_12",
  "x": "We also see a substantial increase in the standard deviation when only two samples are used, indicating that performance is not robust when the data is not widely sampled. ---------------------------------- **CHANNEL TWO RESULTS** For channel Two, we consider two annotations of the same underlying text: ours and <cite>Elsner and Charniak (2008)</cite>'s. To compare with prior work, we use the metrics defined by Shen et al. (2006, Shen) and Elsner and Charniak (2008, Loc) . 8 We do not use these for our data as they have been superseded by more rigorously studied metrics (VI for Shen) or make strong assumptions about the data (Loc). We do not evaluate on graphs because <cite>Elsner and Charniak (2008)</cite> 's annotations do not include them.",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_13",
  "x": "8 We do not use these for our data as they have been superseded by more rigorously studied metrics (VI for Shen) or make strong assumptions about the data (Loc). We do not evaluate on graphs because <cite>Elsner and Charniak (2008)</cite> 's annotations do not include them. This also prevents us from training our method on their data. Model Comparison: For Elsner's annotations (top section of Table 6 ), their approach remains the most effective with just Channel Two data. However, training on our Ubuntu data, treating Channel Two as an out-of-domain sample, yields substantially higher performance on two metrics and comparable performance on the third. On our annotations (bottom section), we see the same trend. In both cases, the heuristic from Lowe et al. (2015 Lowe et al. ( , 2017 performs poorly.",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_14",
  "x": "Number of samples: Table 1 shows that all prior work with available data has considered a small number of samples. In Table 5 , we saw that training on less diverse data samples led to models that performed worse and with higher variance. We can also investigate this by looking at performance on the different samples in our test set. The difficulty of samples varies considerably, with the F-score of our model varying from 11 to 40 and annotator agreement scores before adjudication varying from 0.65 to 0.78. The model performance and agreement levels are also strongly correlated, with a Spearman's rank correlation of 0.77. This demonstrates the importance of evaluating on data from more than one point in time to get a robust estimate of performance. How far apart consecutive messages in a conversation are: <cite>Elsner and Charniak (2008)</cite> and Mehri and Carenini (2017) use a limit of 129 seconds, Jiang et al. (2018) limit to within 1 hour, Guo et al. (2017) limit to within 8 messages, and we limit to within 100 messages.",
  "y": "differences"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_0",
  "x": "This has facilitated developement of various syntactic and semantic task specific datasets and neural architectures, but has been limited by the expensive efforts towards annotation. As a result, while these efforts have enabled processing of codemixed texts, they still suffer from data scarcity and poor representation learning, and the small individual dataset sizes usually limiting the model performance. Curriculum Learning, as introduced by [Bengio et al., 2009 ] is \"to start small, learn easier aspects of the task or easier subtasks, and then gradually increase the difficulty level\". They also draw parallels with human learning curriculum and education system, where different concepts are introduced in an order at different times, and has led to advancement in research towards animal training [Krueger and Dayan, 2009] . Previous experiments with tasks like language modelling<cite> [Bengio et al., 2009]</cite> , Dependency Parsing, and entailment <cite>[Hashimoto et al., 2016]</cite> have shown faster convergence and performance gains by following a curriculum training regimen in the order of increasingly complicated syntactic and semantic tasks. [Weinshall and Cohen, 2018 ] also find theoretical and experimental evidence for curriculum learning by pretraining on another task leading to faster convergence. With this purview, we propose a syntactico-semantic curriculum training strategy for Hi-En codemixed twitter sentiment analysis.",
  "y": "background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_1",
  "x": "[Singh et al., 2018b] developed a dataset for Hindi English Codemixed Language Identification and NER, and propose a CRF based approach with handcrafted features for Named Entity Recognition. Bengio et. al. [2009] introduced curriculum learning approaches towards both vision and language related task, and show significant convergence and performance gains for language modelling task. <cite>[Hashimoto et al., 2016]</cite> propose a hierarchical multitask neural architecture with the lower layers performing syntactic tasks, and the higher layers performing the more involved semantic tasks while using the lower layer predictions. [Swayamdipta et al., 2018] also propose a syntactico semantic curriculum with chunking, semantic role labelling and coreference resolution, and show performance gains over strong baselines. Like <cite>[Hashimoto et al., 2016]</cite> , they hypothesize the incorporation of simpler syntactic information into semantic tasks, and provide empirical evidence for the same. 3 Datasets [Prabhu et al., 2016] released a Hi-En codemixed dataset for Sentiment Analysis, comprising 3879 Facebook comments on public pages of Salman Khan and Narendra Modi.",
  "y": "background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_2",
  "x": "Bengio et. al. [2009] introduced curriculum learning approaches towards both vision and language related task, and show significant convergence and performance gains for language modelling task. <cite>[Hashimoto et al., 2016]</cite> propose a hierarchical multitask neural architecture with the lower layers performing syntactic tasks, and the higher layers performing the more involved semantic tasks while using the lower layer predictions. [Swayamdipta et al., 2018] also propose a syntactico semantic curriculum with chunking, semantic role labelling and coreference resolution, and show performance gains over strong baselines. Like <cite>[Hashimoto et al., 2016]</cite> , they hypothesize the incorporation of simpler syntactic information into semantic tasks, and provide empirical evidence for the same. 3 Datasets [Prabhu et al., 2016] released a Hi-En codemixed dataset for Sentiment Analysis, comprising 3879 Facebook comments on public pages of Salman Khan and Narendra Modi. Comments are annotated as positive, negative and neutral based on their sentiment polarity, and are distributed across the 3 classes as 15% negative, 50% neutral and 35% positive comments.",
  "y": "background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_3",
  "x": "**CURRICULUM TRAINING** While our proposed model enables efficient transfer learning by progressive abstraction of representations for more complicated tasks, the highlight of the approach lies in the training regimen followed. Curriculum learning can be seen as a sequence of training criteria<cite> [Bengio et al., 2009]</cite> , with increasing task or sample difficulty as the training progresses. It is also closely related with transfer learning by pretraining, especially in the case when the tasks form a logical hierarchy and contribute to the downstream tasks. With this purview, we propose a linguistic hierarchy of training tasks for codemixed languages, with further layers abstracting over the previous ones to achieve increasingly complicated tasks. Considering the codemixed nature of texts and linguistic hierarchy of information, we propose the tasks in the order of : Language Identification, Part of Speech Tagging, Language Modelling and further semantic tasks like sentiment analysis. Since tokens in codemixed texts have distinct semantic spaces based on their source language, Language Identification can incorporate this disparity among the learnt trigram representations.",
  "y": "motivation background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_4",
  "x": "With this purview, we propose a linguistic hierarchy of training tasks for codemixed languages, with further layers abstracting over the previous ones to achieve increasingly complicated tasks. Considering the codemixed nature of texts and linguistic hierarchy of information, we propose the tasks in the order of : Language Identification, Part of Speech Tagging, Language Modelling and further semantic tasks like sentiment analysis. Since tokens in codemixed texts have distinct semantic spaces based on their source language, Language Identification can incorporate this disparity among the learnt trigram representations. Following this, the Part of Speech Tagging groups the words based on their logical semantic categories, and encodes simpler word category information in a sequence. Also, as in [Singh et al., 2018a; Sharma et al., 2016] , Language Tag and Part of Speech Tag have previously been provided as manual handcrafted features for a range of downstream syntactic and semantic tasks. In addition to the above tasks, Language Model pretraining has shown significant performance gains as reported by<cite> [Howard and Ruder, 2018]</cite> . It captures various aspects of language such as long range dependencies [Linzen et al., 2016] , word categories, and sentiment [Radford et al., 2017] .",
  "y": "background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_5",
  "x": "Subsequently, the model is trained on Language Modelling task, in process training the LSTM Layer 2 to build over the LSTM Layer 1 inputs to learn meaningful sequence representation. Lastly, the model is trained to predict the sentiment of the input text based on the LSTM Layer 2 representation. ---------------------------------- **TRANSFER LEARNING** As noted in earlier efforts<cite> [Howard and Ruder, 2018</cite> ] towards finetuning pretrained models for NLP tasks, aggressive finetuning can cause catastrophic forgetting, thus causing the model to simply fit over the target task and forget any capabilities gained during the pretraining stage. On the other hand, too cautious finetuning can cause slow convergence and overfitting. To this end, we experiment with different strategies which can be broadly categorized as:",
  "y": "motivation background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_6",
  "x": "We thus split the parameters as {\u03b8 1 , ..., \u03b8 l } , where \u03b8 i corresponds to the parameters of layer i, and optimize them with separate learning rates {\u03b7 1 , ...., \u03b7 l } . Also, when finetuning a pretrained layer for a downstream task, we keep \u03b7 i < \u03b7 j ; \u2200i < j. Thus, while finetuning the POS + Lang Id pretrained model for Language Modeling, we propose to keep the learning rates for Embedding Layer and LSTM Layer 1 lower than the LSTM Layer 2 weights. Similarly, when finetuning the Language Model for Sentiment Analysis, we keep the learning rates of the deeper layers lower than that of the shallower ones. Gradual Unfreezing: Similar to<cite> [Howard and Ruder, 2018]</cite> , rather than updating all the layers together for finetuning, we explore gradual ordered unfreezing of layers. Thus, initially we freeze all the layers. Then starting from the last layer, we train the model for a certain number of epochs before unfreezing the layer below it. Thus for Sentiment Analysis finetuning, for the first epoch, only \u03b8 sentiment receives the gradient updates, after which we unfreeze the \u03b8 lstm2 , and subsequently unfreeze the lower layers in a similar manner.",
  "y": "similarities uses"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_7",
  "x": "With this purview, similar to<cite> [Howard and Ruder, 2018]</cite> , we propose optimizing different layers in our model to different extents, and keep lower step sizes for the deeper pretrained layers while finetuning on a downstream task. We thus split the parameters as {\u03b8 1 , ..., \u03b8 l } , where \u03b8 i corresponds to the parameters of layer i, and optimize them with separate learning rates {\u03b7 1 , ...., \u03b7 l } . Also, when finetuning a pretrained layer for a downstream task, we keep \u03b7 i < \u03b7 j ; \u2200i < j. Thus, while finetuning the POS + Lang Id pretrained model for Language Modeling, we propose to keep the learning rates for Embedding Layer and LSTM Layer 1 lower than the LSTM Layer 2 weights. Similarly, when finetuning the Language Model for Sentiment Analysis, we keep the learning rates of the deeper layers lower than that of the shallower ones. Gradual Unfreezing: Similar to<cite> [Howard and Ruder, 2018]</cite> , rather than updating all the layers together for finetuning, we explore gradual ordered unfreezing of layers. Thus, initially we freeze all the layers. Then starting from the last layer, we train the model for a certain number of epochs before unfreezing the layer below it.",
  "y": "similarities uses"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_8",
  "x": "The input to the LSTM stack is the sequence of character trigram dense representations, which we keep as 64 dimensional vectors. We also explore other token representations such as sequence of unigrams, convolution over unigrams [Prabhu et al., 2016] , and Byte Pair Encoding (BPE) [Sennrich et al., 2015] . BPE is an unsupervised approach towards subword decomposition, and has shown improvements in MT systems and summarization. We train our model from scratch for Sentiment Analysis using the above mentioned character encodings, and report the results in Table 3 . Our LSTM stack consists of two layers of bidirectional LSTMs, with 64 hidden state dimensions. We add a dropout layer with the dropout rate set to 0.2 between the LSTM layers to prevent overfitting. We experiment with average pooling and max pooling concatenation over hidden states for semantic prediction, similar to<cite> [Howard and Ruder, 2018]</cite> , and observe increase in model accuracy by 2.2% on sentiment analysis.",
  "y": "similarities background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_9",
  "x": "We approach the evaluation of our curriculum by training the model sequentially for four subtasks -Language Identification, POS Tagging, Language Modelling and Sentiment Analysis. We evaluate the strategy of pretraining with only POS Tagging and Language Identification, and observe similar performance as no curriculum training. We hypothesize the potential reasons for this drop and find a significant divergence in character trigram occurance between the Source Tasks (POS + Lang Id) and Target Task(Sentiment Analysis). This experiement highlights the importance of inclusion of language model pretraining for better token level representation learning, and provides a better model prior for sequence representation (LSTM Layer 2 output). We experiment with only Language Modelling as pretraining task, and observe significant gains over no curriculum strategy. We note the convergence of our model with and without curriculum training, and observe that the curriculum training regimen causes faster convergence, as has been observed in previous works [Bengio et al., 2009;<cite> Howard and Ruder, 2018]</cite> . This is expected as the model is pretrained on prior tasks already have a general purpose representation learning, and only needs to adapt to the idiosyncrasies of the target task, i.e. Sentiment Analysis in this case.",
  "y": "similarities background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_10",
  "x": "This experiement highlights the importance of inclusion of language model pretraining for better token level representation learning, and provides a better model prior for sequence representation (LSTM Layer 2 output). We experiment with only Language Modelling as pretraining task, and observe significant gains over no curriculum strategy. We note the convergence of our model with and without curriculum training, and observe that the curriculum training regimen causes faster convergence, as has been observed in previous works [Bengio et al., 2009;<cite> Howard and Ruder, 2018]</cite> . This is expected as the model is pretrained on prior tasks already have a general purpose representation learning, and only needs to adapt to the idiosyncrasies of the target task, i.e. Sentiment Analysis in this case. As discussed in Section 4.3, for our transfer learning optimization experiments, we segment the optimization of different parameters of our model with different learning rates, in order to limit catastrophic forgetting and interference among the tasks, as proposed by<cite> [Howard and Ruder, 2018]</cite> . We segregate our model parameters in the following 4 groups: \u2022 Emb Layer",
  "y": "similarities background"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_0",
  "x": "As a specific and concrete implementation of THM, Crossed Co-Attention Networks (CCNs) are designed based on the Transformer model. We demonstrate CCNs on WMT 2014 EN-DE and WMT 2016 EN-FI translation tasks and our model outperforms the strong Transformer baseline by 0.51 (big) and 0.74 (base) BLEU points on EN-DE and by 0.17 (big) and 0.47 (base) BLEU points on EN-FI. ---------------------------------- **INTRODUCTION** Attention has emerged as a prominent neural module extensively adopted in a wide range of deep learning research problems (Das et al., 2017; Rockt\u00e4schel et al., 2015; Santos et al., 2016; Xu and Saenko, 2016; Yang et al., 2016; Yin et al., 2016; Zhu et al., 2016; Xu et al., 2015; Chorowski et al., 2015) such as VQA, reading comprehension, textual entailment, image captioning, speech recognition and so forth. It's remarkable success is also embodied in machine translation tasks (Bahdanau et al., 2014;<cite> Vaswani et al., 2017)</cite> . This work proposes an end-to-end co-attentional neural structure, named Crossed Co-Attention Networks (CCNs) to address machine translation, a typical sequence-to-sequence NLP task.",
  "y": "background"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_1",
  "x": "**INTRODUCTION** Attention has emerged as a prominent neural module extensively adopted in a wide range of deep learning research problems (Das et al., 2017; Rockt\u00e4schel et al., 2015; Santos et al., 2016; Xu and Saenko, 2016; Yang et al., 2016; Yin et al., 2016; Zhu et al., 2016; Xu et al., 2015; Chorowski et al., 2015) such as VQA, reading comprehension, textual entailment, image captioning, speech recognition and so forth. It's remarkable success is also embodied in machine translation tasks (Bahdanau et al., 2014;<cite> Vaswani et al., 2017)</cite> . This work proposes an end-to-end co-attentional neural structure, named Crossed Co-Attention Networks (CCNs) to address machine translation, a typical sequence-to-sequence NLP task. We customize the transformer<cite> (Vaswani et al., 2017)</cite> featured by non-local operations (Wang et al., 2018) with two * The work was done when Yaoyiran was working at Living Analytics Research Centre, Singapore Management University who is now a PhD student at University of Cambridge. input branches and tailor the transformer's multihead attention mechanism to the needs of information exchange between these two parallel branches. A higher-level and more abstract paradigm generalized from CCNs is denoted as \"Two-Headed Monster\" (THM), representing a broader class of neural structure benefiting from two parallel neural channels that would be intertwined with each other through, for example, co-attention mechanism as illustrated in Fig. 1 .",
  "y": "extends"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_2",
  "x": "A non-local operation is defined as a building block in deep neural networks which captures long-range dependencies where every response is computed as a linear combination of all features in the input feature map (Wang et al., 2018) . Suppose the input feature maps are V = [v 1 , v 2 , ..., v n ] T \u2208 R n\u00d7d , K = [k 1 , k 2 , ..., k n ] T \u2208 R n\u00d7d and Q = [q 1 , q 2 , ..., q n ] T \u2208 R n\u00d7d and the output feature map Y = [y 1 , y 2 , ..., y n ] T \u2208 R n\u00d7d is of the same size as the input. Then a generic non-local operation is formulated as follows: We basically follow the definition of no-local operation in (Wang et al., 2018) where f : is a unary function and C : R d \u00d7 R n\u00d7d \u2192 R calculates a normalizer, but dispense with the assumption that then the non-local operation degrades to the multihead self-attention as is described in<cite> (Vaswani et al., 2017)</cite> (formula 2 describes only one attention head): Considering two input channels, denoted as 'left' and 'right', we present the following non-local operation as a definition of co-attention where \u03b1(\u00b7), \u03b2(\u00b7) \u2208 { lef t , right }.",
  "y": "uses"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_3",
  "x": "Based on the transformer model<cite> (Vaswani et al., 2017)</cite> , we design a novel co-attention mechanism. Our proposed mechanism consists of two symmetrical branches that work in parallel to assimilate information from two input channels respectively. Different from previously known coattention mechanisms such as (Xiong et al., 2017; Lu et al., 2016a) , our co-attention is built through connecting two multiplicative attention modules<cite> (Vaswani et al., 2017</cite> ) each containing three gates, i.e., Value, Key and Query. The information flows from two input channels then interact with and benefit from each other via crossed connections. Suppose the input fed into the left branch is X Lef t , and the right branch X right . In our encoder, the left branch takes input from X Lef t as Value (V) and Key (K) and takes the input X right as Query (Q). The right branch, however, takes the input X Lef t as Query (Q) and X right as Value (V) and Key (K).",
  "y": "uses"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_4",
  "x": "**CROSSED CO-ATTENTION NETWORKS** Based on the transformer model<cite> (Vaswani et al., 2017)</cite> , we design a novel co-attention mechanism. Our proposed mechanism consists of two symmetrical branches that work in parallel to assimilate information from two input channels respectively. Different from previously known coattention mechanisms such as (Xiong et al., 2017; Lu et al., 2016a) , our co-attention is built through connecting two multiplicative attention modules<cite> (Vaswani et al., 2017</cite> ) each containing three gates, i.e., Value, Key and Query. The information flows from two input channels then interact with and benefit from each other via crossed connections. Suppose the input fed into the left branch is X Lef t , and the right branch X right . In our encoder, the left branch takes input from X Lef t as Value (V) and Key (K) and takes the input X right as Query (Q).",
  "y": "extends uses"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_5",
  "x": "**CROSSED CO-ATTENTION NETWORKS** Based on the transformer model<cite> (Vaswani et al., 2017)</cite> , we design a novel co-attention mechanism. Our proposed mechanism consists of two symmetrical branches that work in parallel to assimilate information from two input channels respectively. Different from previously known coattention mechanisms such as (Xiong et al., 2017; Lu et al., 2016a) , our co-attention is built through connecting two multiplicative attention modules<cite> (Vaswani et al., 2017</cite> ) each containing three gates, i.e., Value, Key and Query. The information flows from two input channels then interact with and benefit from each other via crossed connections. Suppose the input fed into the left branch is X Lef t , and the right branch X right . In our encoder, the left branch takes input from X Lef t as Value (V) and Key (K) and takes the input X right as Query (Q).",
  "y": "extends"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_6",
  "x": "**CROSSED CO-ATTENTION NETWORKS** Based on the transformer model<cite> (Vaswani et al., 2017)</cite> , we design a novel co-attention mechanism. Our proposed mechanism consists of two symmetrical branches that work in parallel to assimilate information from two input channels respectively. Different from previously known coattention mechanisms such as (Xiong et al., 2017; Lu et al., 2016a) , our co-attention is built through connecting two multiplicative attention modules<cite> (Vaswani et al., 2017</cite> ) each containing three gates, i.e., Value, Key and Query. The information flows from two input channels then interact with and benefit from each other via crossed connections. Suppose the input fed into the left branch is X Lef t , and the right branch X right . In our encoder, the left branch takes input from X Lef t as Value (V) and Key (K) and takes the input X right as Query (Q).",
  "y": "extends"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_7",
  "x": "**PERFORMANCE ACROSS LANGUAGES:** We test our proposed method on two language pairs, EN-DE and EN-FI and the improved BLEU scores and the capability of model selection on both base and big models demonstrate the universality of our proposed method. ---------------------------------- **RELATED WORK** Attention: Multi-head self-attention has demonstrated its capacity in neural transduction models<cite> (Vaswani et al., 2017)</cite> , language model pre-training (Devlin et al., 2018; Radford et al., 2018) and speech synthesis (Yang et al., 2019c) . While the novel attention mechanism, eschewing recurrence, is famous for modeling global dependencies and considered faster than recurrent layers<cite> (Vaswani et al., 2017)</cite> , recent work points out that it may tend to overlook neighboring information (Yang et al., 2019a; Xu et al., 2019) . It is found that applying an adaptive attention span could be conducive to character level language modeling tasks (Sukhbaatar et al., 2019) .",
  "y": "background"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_8",
  "x": "We test our proposed method on two language pairs, EN-DE and EN-FI and the improved BLEU scores and the capability of model selection on both base and big models demonstrate the universality of our proposed method. ---------------------------------- **RELATED WORK** Attention: Multi-head self-attention has demonstrated its capacity in neural transduction models<cite> (Vaswani et al., 2017)</cite> , language model pre-training (Devlin et al., 2018; Radford et al., 2018) and speech synthesis (Yang et al., 2019c) . While the novel attention mechanism, eschewing recurrence, is famous for modeling global dependencies and considered faster than recurrent layers<cite> (Vaswani et al., 2017)</cite> , recent work points out that it may tend to overlook neighboring information (Yang et al., 2019a; Xu et al., 2019) . It is found that applying an adaptive attention span could be conducive to character level language modeling tasks (Sukhbaatar et al., 2019) . Yang et al. propose to model localness for self-attention which would be conducive to capturing local information by learning a Gaussian bias predicting the region of local attention (Yang et al., 2018a) .",
  "y": "background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_0",
  "x": "Finally, we increase the probability of actions that cause the retrieved n-gram action subtree to be in the predicted code. We show that our approach improves the performance on two code generation tasks by up to +2.6 BLEU. ---------------------------------- **1** ---------------------------------- **INTRODUCTION** Natural language to code generation, a subtask of semantic parsing, is the problem of converting natural language (NL) descriptions to code (Ling et al., 2016;<cite> Yin and Neubig, 2017</cite>; Rabinovich et al., 2017) .",
  "y": "background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_1",
  "x": "---------------------------------- **INTRODUCTION** Natural language to code generation, a subtask of semantic parsing, is the problem of converting natural language (NL) descriptions to code (Ling et al., 2016;<cite> Yin and Neubig, 2017</cite>; Rabinovich et al., 2017) . This task is challenging because it has a well-defined structured output and the input structure and output structure are in different forms. A number of neural network approaches have been proposed to solve this task. Sequential approaches (Ling et al., 2016; Jia and Liang, 2016; Locascio et al., 2016) convert the target code into a sequence of symbols and apply a sequence-tosequence model, but this approach does not ensure that the output will be syntactically correct. 1 Code available at https://github.com/ sweetpeach/ReCode Tree-based approaches<cite> (Yin and Neubig, 2017</cite>; Rabinovich et al., 2017) represent code as Abstract Syntax Trees (ASTs), which has proven effective in improving accuracy as it enforces the well-formedness of the output code.",
  "y": "background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_2",
  "x": "A number of neural network approaches have been proposed to solve this task. Sequential approaches (Ling et al., 2016; Jia and Liang, 2016; Locascio et al., 2016) convert the target code into a sequence of symbols and apply a sequence-tosequence model, but this approach does not ensure that the output will be syntactically correct. 1 Code available at https://github.com/ sweetpeach/ReCode Tree-based approaches<cite> (Yin and Neubig, 2017</cite>; Rabinovich et al., 2017) represent code as Abstract Syntax Trees (ASTs), which has proven effective in improving accuracy as it enforces the well-formedness of the output code. However, representing code as a tree is not a trivial task, as the number of nodes in the tree often greatly exceeds the length of the NL description. As a result, tree-based approaches are often incapable of generating correct code for phrases in the corresponding NL description that have low frequency in the training data. In machine translation (MT) problems Gu et al., 2018; Amin Farajian et al., 2017; Li et al., 2018) , hybrid methods combining retrieval of salient examples and neural models have proven successful in dealing with rare words. Following the intuition of these models, we hypothesize that our model can benefit from querying pairs of NL descriptions and AST structures from training data.",
  "y": "motivation background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_3",
  "x": "---------------------------------- **SYNTACTIC CODE GENERATION** Given an NL description q, our purpose is to generate code (e.g. Python) represented as an AST a. In this work, we start with the syntactic code gen-eration model by<cite> Yin and Neubig (2017)</cite> , which uses sequences of actions to generate the AST before converting it to surface code. Formally, we want to find the best generated AST\u00e2 given by: where y t is the action taken at time step t and y <t = y 1 ...y t\u22121 and T is the number of total time steps of the whole action sequence resulting in AST a. We have two types of actions to build an AST: APPLYRULE and GENTOKEN. APPLYRULE(r) expands the current node in the tree by applying production rule r from the abstract syntax grammar 2 to the current node.",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_4",
  "x": "We have two types of actions to build an AST: APPLYRULE and GENTOKEN. APPLYRULE(r) expands the current node in the tree by applying production rule r from the abstract syntax grammar 2 to the current node. GENTOKEN(v) populates terminal nodes with the variable v which can be generated from vocabulary or by COPYing variable names or values from the NL description. The generation process follows a preorder traversal starting with the root node. Figure 1 shows an action tree for the example code: the nodes correspond to actions per time step in the construction of the AST. Interested readers can reference<cite> Yin and Neubig (2017)</cite> for more detail of the neural model, which consists of a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) encoder-decoder with action embeddings, context vectors, parent feeding, and a copy mechanism using pointer networks. ----------------------------------",
  "y": "background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_5",
  "x": "Thus, in the example in Figure 1 , the GENTOKEN(LST) action in t 5 will not be executed. ---------------------------------- **RETRIEVAL-GUIDED CODE GENERATION** N -gram subtrees from all retrieved sentences are assigned a score, based on the best similarity score<cite> Yin and Neubig (2017)</cite> of all instances where they appeared. We normalize the scores for each input sentence by subtracting the average over the training dataset. At decoding time, incorporate these retrievalderived scores into beam search: for a given time step, all actions that would result in one of the retrieved n-grams u to be in the prediction tree has its log probability log(p(y t | y t\u22121 1 )) increased by \u03bb * score(u) where \u03bb is a hyperparameter, and score(u) is the maximal sim(q, q m ) from which u is extracted. The probability distribution is then renormalized.",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_6",
  "x": "At decoding time, incorporate these retrievalderived scores into beam search: for a given time step, all actions that would result in one of the retrieved n-grams u to be in the prediction tree has its log probability log(p(y t | y t\u22121 1 )) increased by \u03bb * score(u) where \u03bb is a hyperparameter, and score(u) is the maximal sim(q, q m ) from which u is extracted. The probability distribution is then renormalized. ---------------------------------- **DATASETS AND EVALUATION METRICS** We evaluate RECODE with the Hearthstone (HS) (Ling et al., 2016) and Django (Oda et al., 2015) datasets, as preprocessed by<cite> Yin and Neubig (2017)</cite> . HS consists of Python classes that implement Hearthstone card descriptions while Django contains pairs of Python source code and English pseudo-code from Django web framework. Table  1 summarizes dataset statistics.",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_7",
  "x": "---------------------------------- **DATASETS AND EVALUATION METRICS** We evaluate RECODE with the Hearthstone (HS) (Ling et al., 2016) and Django (Oda et al., 2015) datasets, as preprocessed by<cite> Yin and Neubig (2017)</cite> . HS consists of Python classes that implement Hearthstone card descriptions while Django contains pairs of Python source code and English pseudo-code from Django web framework. Table  1 summarizes dataset statistics. For evaluation metrics, we use accuracy of exact match and the BLEU score following<cite> Yin and Neubig (2017)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_8",
  "x": "For the neural code generation model, we use the settings explained in<cite> Yin and Neubig (2017)</cite> . For the retrieval method, we tuned hyperparameters and achieved best result when we set n max = 4 and \u03bb = 3 for both datasets 3 . For HS, we set M = 3 and M = 10 for Django. We compare our model with<cite> Yin and Neubig (2017)</cite>'s model that we call YN17 for brevity, and a sequence-to-sequence (SEQ2SEQ) model that we implemented. SEQ2SEQ is an attentionenabled encoder-decoder model (Bahdanau et al., 2015) . The encoder is a bidirectional LSTM and the decoder is an LSTM. We ran statistical significance tests for RECODE and YN17, using bootstrap resampling with N = 10,000.",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_9",
  "x": "For HS, we set M = 3 and M = 10 for Django. We compare our model with<cite> Yin and Neubig (2017)</cite>'s model that we call YN17 for brevity, and a sequence-to-sequence (SEQ2SEQ) model that we implemented. SEQ2SEQ is an attentionenabled encoder-decoder model (Bahdanau et al., 2015) . The encoder is a bidirectional LSTM and the decoder is an LSTM. We ran statistical significance tests for RECODE and YN17, using bootstrap resampling with N = 10,000. For the BLEU scores of both datasets, p < 0.001. For the exact match accuracy, p < 0.001 for Django dataset, but for Hearthstone, p > 0.3, showing that the retrieval-based model is on par with YN17.",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_10",
  "x": "---------------------------------- **DISCUSSION AND ANALYSIS** From our observation and as mentioned in Rabinovich et al. (2017) , HS contains classes with similar structure, so the code generation task could be simply matching the tree structure and filling the terminal tokens with correct variables and values. However, when the code consists of complex logic, partial implementation errors occur, leading to low exact match accuracy<cite> (Yin and Neubig, 2017)</cite> . Analyzing our result, we find this intuition to be true not only for HS but also for Django. Examining the generated output for the Django dataset in Table 3 , we can see that in the first example, our retrieval model can successfully generate the correct code when YN17 fails. This difference suggests that our retrieval model benefits from the action subtrees from the retrieved sentences.",
  "y": "background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_11",
  "x": "From our observation and as mentioned in Rabinovich et al. (2017) , HS contains classes with similar structure, so the code generation task could be simply matching the tree structure and filling the terminal tokens with correct variables and values. However, when the code consists of complex logic, partial implementation errors occur, leading to low exact match accuracy<cite> (Yin and Neubig, 2017)</cite> . Analyzing our result, we find this intuition to be true not only for HS but also for Django. Examining the generated output for the Django dataset in Table 3 , we can see that in the first example, our retrieval model can successfully generate the correct code when YN17 fails. This difference suggests that our retrieval model benefits from the action subtrees from the retrieved sentences. In the second example, although our generated code does not perfectly match the reference code, it has a higher BLEU score compared Example 1 \"if offset is lesser than integer 0, sign is set to '-', otherwise sign is '+' \" Input sign = offset < 0 or '-' YN17 sign = '-' if offset < 0 else '+' RECODE sign = '-' if offset < 0 else '+' to the output of YN17 because our model can predict part of the code (timesince(d, now, reversed)) correctly. The third example shows where our method fails to apply the correct action as it cannot cast s to str type while YN17 can at least cast s into a type (bool).",
  "y": "similarities"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_12",
  "x": "However, when the code consists of complex logic, partial implementation errors occur, leading to low exact match accuracy<cite> (Yin and Neubig, 2017)</cite> . Analyzing our result, we find this intuition to be true not only for HS but also for Django. Examining the generated output for the Django dataset in Table 3 , we can see that in the first example, our retrieval model can successfully generate the correct code when YN17 fails. This difference suggests that our retrieval model benefits from the action subtrees from the retrieved sentences. In the second example, although our generated code does not perfectly match the reference code, it has a higher BLEU score compared Example 1 \"if offset is lesser than integer 0, sign is set to '-', otherwise sign is '+' \" Input sign = offset < 0 or '-' YN17 sign = '-' if offset < 0 else '+' RECODE sign = '-' if offset < 0 else '+' to the output of YN17 because our model can predict part of the code (timesince(d, now, reversed)) correctly. The third example shows where our method fails to apply the correct action as it cannot cast s to str type while YN17 can at least cast s into a type (bool). Another common type of error that we found RECODE's generated outputs is incorrect variable copying, similarly to what is discussed in<cite> Yin and Neubig (2017)</cite> and Rabinovich et al. (2017) .",
  "y": "similarities"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_13",
  "x": "For general purpose code generation, some data-driven work has been done for predicting input parsers (Lei et al., 2013) or a set of relevant methods (Raghothaman et al., 2016) . Some attempts using neural networks have used sequence-to-sequence models (Ling et al., 2016) or tree-based architectures (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017) . Ling et al. (2016) ; Jia and Liang (2016) ; Locascio et al. (2016) treat semantic parsing as a sequence generation task by linearizing trees. The closest work to ours are<cite> Yin and Neubig (2017)</cite> and Rabinovich et al. (2017) which represent code as an AST. Another close work is Dong and Lapata (2018) , which uses a two-staged structure-aware neural architecture. They initially generate a lowlevel sketch and then fill in the missing information using the NL and the sketch. Recent works on retrieval-guided neural machine translation have been presented by Gu et al. (2018); Amin Farajian et al. (2017) ; Li et al. (2018) ; .",
  "y": "similarities"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_0",
  "x": "**INTRODUCTION** Human knowledge is naturally organized as semantic hierarchies. For example, in WordNet (Miller, 1995) , specific concepts are categorized and assigned to more general ones, leading to a semantic hierarchical structure (a.k.a taxonomy). A variety of NLP tasks, such as question answering (Harabagiu et al., 2003) , document clustering (Hotho et al., 2002) and text generation (Biran and McKeown, 2013) can benefit from the conceptual relationship present in these hierarchies. Traditional methods of manually constructing taxonomies by experts (e.g. WordNet) and interest communities (e.g. Wikipedia) are either knowledge or time intensive, and the results have limited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014;<cite> Bansal et al., 2014</cite>; Tuan et al., 2015) .",
  "y": "background"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_1",
  "x": "Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014;<cite> Bansal et al., 2014</cite>; Tuan et al., 2015) . These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; . On the other hand, researchers have built visual hierarchies by utilizing only visual features (Griffin and Perona, 2008; Yan et al., 2015; Sivic et al., 2008) . The resulting hierarchies are limited in interpretability and usability for knowledge transfer. Hence, we propose to combine both visual and textual knowledge to automatically build taxonomies. We induce is-a taxonomies by supervised learning from existing entity ontologies where each concept category (entity) is associated with images, either from existing dataset (e.g. ImageNet (Deng et al., 2009)) or retrieved from the web using search engines, as illustrated in Fig 1.",
  "y": "motivation extends"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_2",
  "x": "We induce is-a taxonomies by supervised learning from existing entity ontologies where each concept category (entity) is associated with images, either from existing dataset (e.g. ImageNet (Deng et al., 2009)) or retrieved from the web using search engines, as illustrated in Fig 1. Such a scenario is realistic and can be extended to a variety of tasks; for example, in knowledge base construction ), text and image collections are readily available but label relations among categories are to be uncovered. In largescale object recognition, automatically learning relations between labels can be quite useful (Deng et al., 2014; Zhao et al., 2011) . Both textual and visual information provide important cues for taxonomy induction. Fig 1 il lustrates this via an example. The parent category seafish and its two child categories shark and ray are closely related as: (1) there is a hypernym-hyponym (is-a) relation between the words \"seafish\" and \"shark\"/\"ray\" through text descriptions like \"...seafish, such as shark and ray...\", \"...shark and ray are a group of seafish...\"; (2) images of the close neighbors, e.g., shark and ray are usually visually similar and images of the child, e.g. shark/ray are similar to a subset of images of seafish. To effectively capture these patterns, in contrast to previous works that rely on various hand-crafted features<cite> Bansal et al., 2014)</cite> , we extract features by leveraging the distributed representations that embed images (Simonyan and Zisserman, 2014) and words as compact vectors, based on which the semantic closeness is directly measured in vector space.",
  "y": "differences"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_3",
  "x": "Many approaches have been recently developed that build hierarchies purely by identifying either lexical patterns or statistical features in text corpora (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Zhu et al., 2013; Fu et al., 2014;<cite> Bansal et al., 2014</cite>; Tuan et al., 2014; Tuan et al., 2015; Kiela et al., 2015) . The approaches in Yang and Callan (2009) and Snow et al. (2006) assume a starting incomplete hierarchy and try to extend it by inserting new terms. Kozareva and Hovy (2010) and Navigli et al. (2011) first find leaf nodes and then use lexical patterns to find intermediate terms and all the attested hypernymy links between them. In (Tuan et al., 2014) , syntactic contextual similarity is exploited to construct the taxonomy, while Tuan et al. (2015) go one step further to consider trustiness and collective synonym/contrastive evidence. Different from them, our model is discriminatively trained with multi-modal data. The works of Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> use similar language-based features as ours. Specifically, in (Fu et al., 2014) , linguistic regularities between pretrained word vectors are modeled as projection mappings.",
  "y": "background"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_4",
  "x": "The approaches in Yang and Callan (2009) and Snow et al. (2006) assume a starting incomplete hierarchy and try to extend it by inserting new terms. Kozareva and Hovy (2010) and Navigli et al. (2011) first find leaf nodes and then use lexical patterns to find intermediate terms and all the attested hypernymy links between them. In (Tuan et al., 2014) , syntactic contextual similarity is exploited to construct the taxonomy, while Tuan et al. (2015) go one step further to consider trustiness and collective synonym/contrastive evidence. Different from them, our model is discriminatively trained with multi-modal data. The works of Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> use similar language-based features as ours. Specifically, in (Fu et al., 2014) , linguistic regularities between pretrained word vectors are modeled as projection mappings. The trained projection matrix is then used to induce pairwise hypernym-hyponym relations between words.",
  "y": "similarities"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_5",
  "x": "The works of Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> use similar language-based features as ours. Specifically, in (Fu et al., 2014) , linguistic regularities between pretrained word vectors are modeled as projection mappings. The trained projection matrix is then used to induce pairwise hypernym-hyponym relations between words. Our features are partially motivated by Fu et al. (2014) , but we jointly leverage both textual and visual information. In Kiela et al. (2015) , both textual and visual evidences are exploited to detect pairwise lexical entailments. Our work is significantly different as our model is optimized over the whole taxonomy space rather than considering only word pairs separately. In<cite> (Bansal et al., 2014)</cite> , a structural learning model is developed to induce a globally optimal hierarchy.",
  "y": "background"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_6",
  "x": "Our work is significantly different as our model is optimized over the whole taxonomy space rather than considering only word pairs separately. In<cite> (Bansal et al., 2014)</cite> , a structural learning model is developed to induce a globally optimal hierarchy. Compared with this work, we exploit much richer features from both text and images, and leverage distributed representations instead of hand-crafted features. Several approaches (Griffin and Perona, 2008; Bart et al., 2008; Marsza\u0142ek and Schmid, 2008) have also been proposed to construct visual hierarchies from image collections. In (Bart et al., 2008) , a nonparametric Bayesian model is developed to group images based on low-level features. In (Griffin and Perona, 2008) and (Marsza\u0142ek and Schmid, 2008) , a visual taxonomy is built to accelerate image categorization. In , only binary object-object relations are extracted using co-detection matrices.",
  "y": "differences"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_7",
  "x": "To apply the model to discover the underlying taxonomy from a given set of categories, we first obtain the marginals of z by averaging over the samples generated through eq 3, then output the optimal taxonomy z * by finding the maximum spanning tree (MST) using the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965;<cite> Bansal et al., 2014)</cite> . Training. We need to learn the model parameters w l of each layer l, which capture the relative importance of different features. The model is trained using the EM algorithm. Let (x n ) be the depth (layer) of category x n ; andz (siblings c n ) denote the gold structure in training data. Our training algorithm updates w through maximum likelihood estimation, wherein the gradient of w l is (see the supplementary materials for details): which is the net difference between gold feature vectors and expected feature vectors as per the model.",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_8",
  "x": "Word surface features. In addition to the embedding-based features, we further leverage lexical features based on the surface forms of child/parent category names. Specifically, we employ the Capitalization, Ends with, Contains, Suffix match, LCS and Length different features, which are commonly used in previous works in taxonomy induction (Yang and Callan, 2009;<cite> Bansal et al., 2014)</cite> . ---------------------------------- **EXPERIMENTS** We first disclose our implementation details in section 5.1 and the supplementary material for better reproducibility. We then compare our model with previous state-of-the-art methods (Fu et al., 2014;<cite> Bansal et al., 2014)</cite> with two taxonomy induction tasks.",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_9",
  "x": "In addition to the embedding-based features, we further leverage lexical features based on the surface forms of child/parent category names. Specifically, we employ the Capitalization, Ends with, Contains, Suffix match, LCS and Length different features, which are commonly used in previous works in taxonomy induction (Yang and Callan, 2009;<cite> Bansal et al., 2014)</cite> . ---------------------------------- **EXPERIMENTS** We first disclose our implementation details in section 5.1 and the supplementary material for better reproducibility. We then compare our model with previous state-of-the-art methods (Fu et al., 2014;<cite> Bansal et al., 2014)</cite> with two taxonomy induction tasks. Finally, we provide analysis on the weights and taxonomies induced.",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_10",
  "x": "In the test phase, we infer the parent of each removed node and compare it with groundtruth. This task is designed to figure out whether our model can successfully induce hierarchical relations after learning from within-domain parent-child pairs. (2) Different from the previous one, the hierarchy construction task is designed to test the generalization ability of our model, i.e. whether our model can learn statistical patterns from one hierarchy and transfer the knowledge to build a taxonomy for another collection of out-of-domain labels. Specifically, we select two trees as the training set to learn w. In the test phase, the model is required to build the full taxonomy from scratch for the third tree. We use Ancestor F 1 as our evaluation metric (Kozareva and Hovy, 2010; Navigli et al., 2011;<cite> Bansal et al., 2014)</cite> . Specifically, we measure F 1 = 2P R/(P + R) values of predicted \"is-a\" relations where the precision (P) and recall (R) are: We compare our method to two previously state-of-the-art models by Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> , which are closest to ours.",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_11",
  "x": "Specifically, we select two trees as the training set to learn w. In the test phase, the model is required to build the full taxonomy from scratch for the third tree. We use Ancestor F 1 as our evaluation metric (Kozareva and Hovy, 2010; Navigli et al., 2011;<cite> Bansal et al., 2014)</cite> . Specifically, we measure F 1 = 2P R/(P + R) values of predicted \"is-a\" relations where the precision (P) and recall (R) are: We compare our method to two previously state-of-the-art models by Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> , which are closest to ours. Table 2 : Comparisons among different variants of our model, Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> on two tasks. The ancestor-F 1 scores are reported. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_12",
  "x": "We compare our method to two previously state-of-the-art models by Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> , which are closest to ours. Table 2 : Comparisons among different variants of our model, Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> on two tasks. The ancestor-F 1 scores are reported. ---------------------------------- **RESULTS** Hierarchy completion. In the hierarchy completion task, we split each tree into 70% nodes for training and 30% for test, and experiment with different h. We compare the following three systems: (1) Fu2014 4 (Fu et al., 2014) ; (2) Ours (L):",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_13",
  "x": "We observe that the performance gradually drops when h increases, as more nodes are inserted when the tree grows higher, leading to a more complex and difficult taxonomy to be accurately constructed. Overall, our model outperforms Fu2014 in terms of the F 1 score, even without visual features. In the most difficult case with h = 7, our model still holds an F 1 score of 0.42 (2\u00d7 of Fu2014), demonstrating the superiority of our model. Hierarchy construction. The hierarchy construction task is much more difficult than hierarchy completion task because we need to build a taxonomy from scratch given only a hyper-root. For this task, we use a leave-one-out strategy, i.e. we train our model on every two trees and test on the third, and report the average performance in Table 2 . We compare the following methods: (1) Fu2014, (2) Ours (L), and (3) Ours (LV), as described above; (4) Bansal2014: The model by <cite>Bansal et al. (2014)</cite> retrained using our dataset; (5) Ours (LB): By excluding visual features, but including other language features from <cite>Bansal et al. (2014)</cite> ; (6) Ours (LVB): Our full model further enhanced with all semantic features from <cite>Bansal et al. (2014)</cite> ; (7) Ours (LVB -E): By excluding word embeddingbased language features from Ours (LVB).",
  "y": "differences uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_14",
  "x": "Second, their method is designed to induce only pairwise relations. To build the full taxonomy, they first identify all possible pairwise relations using a simple thresholding strategy and then eliminate conflicted relations to obtain a legitimate tree hierarchy. In contrast, our model is optimized over the full space of all legitimate taxonomies by taking the structure operation in account during Gibbs sampling. When comparing to Bansal2014, our model with only word embedding-based features underperforms theirs. However, when introducing visual features, our performance is comparable (pvalue = 0.058).Furthermore, if we discard visual features but add semantic features from <cite>Bansal et al. (2014)</cite> , we achieve a slight improvement of 0.02 over Bansal2014 (p-value = 0.016), which is largely attributed to the incorporation of word embedding-based features that encode high-level linguistic regularity. Finally, if we enhance our full model with all semantic features from <cite>Bansal et al. (2014)</cite> , our model outperforms theirs by a gap of 0.04 (p-value < 0.01), which justifies our intuition that perceptual semantics underneath visual contents are quite helpful. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_15",
  "x": "Visual representations. To investigate how the image representations affect the final performance, we compare the ancestor-F1 score when different pre-trained CNNs are used for visual feature extraction. Specifically, we employ both the CNN-128 model (128 dimensional feature with 15.6% top-5 error on ILSVRC12) and the VGG-16 model (4096 dimensional feature with 7.5% top-5 error) by Simonyan and Zisserman (2014) , but only observe a slight improvement of 0.01 on the ancestor-F1 score for the later one. Relevance of textual and visual features v.s. depth of tree. Compared to <cite>Bansal et al. (2014)</cite> , a major difference of our model is that different layers of the taxonomy correspond to different weights w l , while in<cite> (Bansal et al., 2014)</cite> all layers share the same weights. Intuitively, introducing layer-wise w not only extends the model capacity, but also differentiates the importance of each feature at different layers. For example, the images of two specific categories, such as shark and ray, are very likely to be visually similar.",
  "y": "differences"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_0",
  "x": "In most of the previous studies (Mihalcea and Strapparava, 2005; Purandare and Litman, 2006; <cite>Yang et al., 2015</cite>) , humor recognition was modeled as a binary classification task. In the seminal work (Mihalcea and Strapparava, 2005) , a corpus of 16,000 \"one-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work (<cite>Yang et al., 2015</cite>) , a new corpus was constructed from the Pun of the Day website. <cite>Yang et al. (2015)</cite> explained and computed stylistic features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style. In addition, Word2Vec (Mikolov et al., 2013) distributed representations were utilized in the model building. Beyond lexical cues from text inputs, other research has also utilized speakers' acoustic cues (Purandare and Litman, 2006; Bertero and Fung, 2016b) .",
  "y": "background"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_1",
  "x": "In most of the previous studies (Mihalcea and Strapparava, 2005; Purandare and Litman, 2006; <cite>Yang et al., 2015</cite>) , humor recognition was modeled as a binary classification task. In the seminal work (Mihalcea and Strapparava, 2005) , a corpus of 16,000 \"one-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work (<cite>Yang et al., 2015</cite>) , a new corpus was constructed from the Pun of the Day website. <cite>Yang et al. (2015)</cite> explained and computed stylistic features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style. In addition, Word2Vec (Mikolov et al., 2013) distributed representations were utilized in the model building. Beyond lexical cues from text inputs, other research has also utilized speakers' acoustic cues (Purandare and Litman, 2006; Bertero and Fung, 2016b) .",
  "y": "background"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_2",
  "x": "1 CNNbased text categorization methods have been applied to humor recognition (e.g., in (Bertero and Fung, 2016b) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in <cite>Yang et al. (2015)</cite> is missing; (b) CNN's performance in the previous research is not quite clear; and (c) some important techniques that can improve CNN performance (e.g., using varied-sized filters and dropout regularization (Hinton et al., 2012)) were not applied. Therefore, the present study is meant to address these limitations. ---------------------------------- **TED TALK DATA** TED Talks 2 are recordings from TED conferences and other special TED programs. Many effects in a presentation can cause audience laugh, such as speaking content, presenters' nonverbal behaviors, and so on. In the present study, we focused on the transcripts of the talks.",
  "y": "background"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_3",
  "x": "Beyond treating humor detection as a binary classification task, Bertero and Fung (2016a) formulated the recognition to be a sequential labeling task and utilized Recurrent Neural Networks (RNNs) (Hochreiter and Schmidhuber, 1997) on top of CNN models (serving as feature extractors) to utilize context information among utterances. From the brief review, it is clear that there is a great need for an open corpus that can support investigating humor in presentations. 1 CNNbased text categorization methods have been applied to humor recognition (e.g., in (Bertero and Fung, 2016b) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in <cite>Yang et al. (2015)</cite> is missing; (b) CNN's performance in the previous research is not quite clear; and (c) some important techniques that can improve CNN performance (e.g., using varied-sized filters and dropout regularization (Hinton et al., 2012)) were not applied. Therefore, the present study is meant to address these limitations. ---------------------------------- **TED TALK DATA** TED Talks 2 are recordings from TED conferences and other special TED programs.",
  "y": "motivation"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_4",
  "x": "This special markup was used to determine utterance labels. We collected 1,192 TED Talk transcripts 3 . An example transcription is given in Figure 1 . The collected transcripts were split into sentences using the Stanford CoreNLP tool (Manning et al., 2014) . In this study, sentences containing or immediately followed by '(Laughter)' were used as 'Laughter' sentences, as shown in Figure 1 ; all other sentences were defined as 'No-Laughter' sentences. Following Mihalcea and Strapparava (2005) and <cite>Yang et al. (2015)</cite> , we selected the same numbers (n = 4726) of 'Laughter' and 'NoLaughter' sentences. To minimize possible topic shifts between positive and negative instances, for each positive instance, we randomly picked one negative instance nearby (the context window was 7 sentences in this study).",
  "y": "uses"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_5",
  "x": "**CONVENTIONAL MODEL** Following <cite>Yang et al. (2015)</cite> , we applied Random Forest (Breiman, 2001 ) to perform humor recognition by using the following two groups of features. The first group are humor-specific stylistic features covering the following 4 categories 4 : Incongruity (2), Ambiguity (6), Interpersonal Effect (4), and Phonetic Pattern (4). The second group are semantic distance features, including the humor label classes from 5 sentences in the training set that are closest to the sentence being evaluated (found by using a k-Nearest Neighbors (kNN) ---------------------------------- **CNN MODEL** Our CNN-based text classification's setup follows Kim (2014) .",
  "y": "uses"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_6",
  "x": "We used two corpora: the TED Talk corpus (denoted as TED) and the Pun of the Day corpus 5 (denoted as Pun). Note that we normalized words in the Pun data to lowercase to avoid a possibly elevated result caused by a special pattern: in the original format, all negative instances started with capital letters. The Pun data allows us to verify that our implementation of the conventional model is consistent with the work reported in <cite>Yang et al. (2015)</cite> . In our experiment, we firstly divided each corpus into two parts. The smaller part (the Dev set) was used for setting various hyper-parameters used in text classifiers. The larger portion (the CV set) was then formulated as a 10-fold crossvalidation setup for obtaining a stable and comprehensive model evaluation result. For the PUN data, the Dev contains 482 sentences, while the CV set contains 4344 sentences.",
  "y": "similarities"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_7",
  "x": "After running 200 iterations of tweaking, we ended up with the following selection: f w is 6 (entailing that the various filter sizes are (5, 6, 7)), n f is 100, dropout 1 is 0.7 and dropout 2 is 0.35, optimization uses Adam (Kingma and Ba, 2014) . When training the CNN model, we randomly selected 10% of the training data as the validation set for using early stopping to avoid over-fitting. On the Pun data, the CNN model shows consistent improved performance over the conventional model, as suggested in <cite>Yang et al. (2015)</cite> . In particular, precision has been greatly increased from 0.762 to 0.864. On the TED data, we also observed that the CNN model helps to increase precision (from 0.515 to 0.582) and accuracy (from 52.0% to 58.9%). The empirical evaluation results suggest that the CNN-based model has an advantage on the humor recognition task. In addition, focusing on the system development time, gener-7 Our code implementation was based on https://github.com/shagunsodhani/ CNN-Sentence-Classifier ating and implementing those features in the conventional model would take days or even weeks.",
  "y": "similarities"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_1",
  "x": "<cite>As Bonial et al. (2014)</cite> stated 'PB has previously treated language as if it were purely compositional, and has therefore lumped the majority of MWEs in with lexical verb usages'. For example the predicates in the CPs take a hard line, take time and many others are all annotated with a sense of take, meaning acquire, come to have, chose, bring with you from somewhere. This results in a loss of semantic information in the PB annotations. This is especially critical because CPs are a frequent phenomenon. The Wiki50 corpus (Vincze et al., 2011) , which provides a full coverage MWE annotation, counts 814 occurrences of LVCs and VPCs in 4350 sentences. This makes for one CP in every fifth sentence. Recently, <cite>Bonial et al. (2014)</cite> have introduced an approach to improve the handling of MWEs in PB while keeping annotation costs low.",
  "y": "background"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_2",
  "x": "This is especially critical because CPs are a frequent phenomenon. The Wiki50 corpus (Vincze et al., 2011) , which provides a full coverage MWE annotation, counts 814 occurrences of LVCs and VPCs in 4350 sentences. This makes for one CP in every fifth sentence. Recently, <cite>Bonial et al. (2014)</cite> have introduced an approach to improve the handling of MWEs in PB while keeping annotation costs low. The process is called <cite>aliasing</cite>. Instead of creating new frames for CPs, human annotators map them to existing PB rolesets which encompass the same semantic and argument structure. For example, the CP give (a) talk could be mapped to the alias lecture.01.",
  "y": "background"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_4",
  "x": "In addition, we set up an annotation effort to gather a frequency-balanced, data-driven evaluation set that is larger and more diverse than the annotated set provided by <cite>Bonial et al. (2014)</cite> . ---------------------------------- **REPRESENTING CPS FOR SRL** Previous work on representing CPs for SRL has mostly focused on PB. The currently available version of the PB corpus represents most CPs as if they were lexical usages of the verb involved in the predicate. Figure 1 shows an example for the annotation of the LVC take care in PB. 1 The CP is split up into its two components that are each assigned their own roleset.",
  "y": "differences"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_5",
  "x": "In contrast to the proposals for PB by Hwang et al. (2010) and Duran et al. (2011) , they suggest to annotate the light verb and its counterpart separately. The <cite>aliasing</cite> process introduced by <cite>Bonial et al. (2014)</cite> tries to extend the coverage of PB for CPs while keeping the number of rolesets that should be newly created to a minimum. <cite>Bonial et al. (2014)</cite> conducted a pilot study re-annotating 138 CPs involving the verb take. As a first step, the annotators determined the meaning(s) of the CP by looking at their usage in corpora. If they found that the CP is already adequately represented by the existing rolesets for take, no further action was needed (18/138). Otherwise, they were instructed to propose as alias an existing PB entry that encompasses the same semantics and argument structure as the CP (100/138). If unable to find an alias, they could suggest to create a new roleset for this CP (20/138).",
  "y": "background"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_6",
  "x": "Duran et al. (2011) discuss the analysis of Brazilian Portuguese CPs. Similarly to Hwang et al. (2010) they argue that CPs should be treated as single predicates, not only for LVCs but for all CPs. They automatically extract CP candidates from a corpus and represent, if possible, the meaning of the CPs with one or more single-verb paraphrases. Atkins et al. (2003) describe a way in which LVCs can be annotated in FrameNet (Baker et al., 1998) , a framework that describes the semantic argument structure of predicates with semantic roles specific to the meaning of the predicate. In contrast to the proposals for PB by Hwang et al. (2010) and Duran et al. (2011) , they suggest to annotate the light verb and its counterpart separately. The <cite>aliasing</cite> process introduced by <cite>Bonial et al. (2014)</cite> tries to extend the coverage of PB for CPs while keeping the number of rolesets that should be newly created to a minimum. <cite>Bonial et al. (2014)</cite> conducted a pilot study re-annotating 138 CPs involving the verb take.",
  "y": "background"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_7",
  "x": "If unable to find an alias, they could suggest to create a new roleset for this CP (20/138). Expressions for which the annotators were unable to determine the meaning were marked as idiomatic expressions that need further treatment (4/138). 2 According to this process, take care could be aliased to the existing PB roleset care.01 whose entry is shown in Figure 3 . This alias replaces (take+care).01 shown in Figure 2 and thus avoids the creation of a new roleset. Roleset id: care.01, to be concerned Arg0: carer, agent Arg1: thing cared for/about Encouraged by the high proportion of CPs that could successfully be aliased in the pilot study by <cite>Bonial et al. (2014)</cite> , we created a method to automatically find aliases for CPs in order to decrease the amount of human intervention, thereby scaling up the coverage of CPs in PB. ---------------------------------- **METHOD**",
  "y": "motivation"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_8",
  "x": "We processed the English section of the Europarl corpus (Koehn, 2005) (about 2 million sentences) with the MATE tools (Bj\u00f6rkelund et al., 2010 ) to obtain lemmas, part-of-speech (POS) tags, dependency structures and semantic role labels. These annotations are used to find occurrences of the CPs and words assigned with PB rolesets in the English part. The word alignments produced with the grow-diagfinal-and-heuristics (Koehn et al., 2003) provided by the OPUS project (Tiedemann, 2012) are then used to find their alignments to all other 20 languages in the corpus and exploited as features in the distributional model. ---------------------------------- **EVALUATION FRAMEWORK** Human Annotation. In order to evaluate our system, we set up an annotation effort loosely following the guidelines provided by <cite>Bonial et al. (2014)</cite> .",
  "y": "extends"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_9",
  "x": "For the remaining CPs, they were asked to suggest PB rolesets (aliases) that share the same semantics and argument structure as the CP. The simple inter-annotator agreement 5 was 67% for annotator A%B, 51% for A&C and 44% for A&D. These agreement figures are higher than the figures in <cite>Bonial et al. (2014)</cite> , and actual agreement is probably even higher, because synonymous rolesets are regarded as disagreements. Annotator A discussed the annotations with the other annotators and they were able to reach a consensus that resulted in a final agreed-upon test set. Table 1 shows the final decisions with respect to the complete set of 197 expressions. In line with the results from <cite>Bonial et al. (2014)</cite> who aliased 100 out of 138 uncompositional take MWEs, we were also able to alias most of the CPs in our annotation set. The final Wiki50 set consists of 154 7 instances of CPs from the categories 'aliased' and 'multi-word PB predicate' (low-frequency: 34, high-frequency: 120). The latter were included because the predicted roleset of the SRL only coincides with the gold standard for 23 out of 60 instances.",
  "y": "differences"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_10",
  "x": "First, they were asked to decide if there is already an appropriate PB roleset for the CP and then provide it. The annotators were requested to divide these cases into semantically compositional CPs (e.g. obtain permission with the roleset obtain.01) and uncompositional CPs for which PB already provides a multi-word predicate (e.g. open.03 for open up). For the remaining CPs, they were asked to suggest PB rolesets (aliases) that share the same semantics and argument structure as the CP. The simple inter-annotator agreement 5 was 67% for annotator A%B, 51% for A&C and 44% for A&D. These agreement figures are higher than the figures in <cite>Bonial et al. (2014)</cite> , and actual agreement is probably even higher, because synonymous rolesets are regarded as disagreements. Annotator A discussed the annotations with the other annotators and they were able to reach a consensus that resulted in a final agreed-upon test set. Table 1 shows the final decisions with respect to the complete set of 197 expressions. In line with the results from <cite>Bonial et al. (2014)</cite> who aliased 100 out of 138 uncompositional take MWEs, we were also able to alias most of the CPs in our annotation set.",
  "y": "similarities"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_11",
  "x": "All CPs were labeled with one to four appropriate PB alias rolesets. In addition, we evaluated our system on the dataset from <cite>Bonial et al. (2014)</cite> , restricted to the type of CP our system handles (LVCs and VPCs) and verb aliases (as opposed to aliases being a noun or adjective roleset). We used 70 of the 100 MWEs from their annotations. Evaluation Measures and Baseline. We report the accuracy of our system's predictions as compared to the gold standard. For the STRICT AC-CURACY, an alias is counted as correct if it corresponds exactly to one of the gold aliases. This evaluation is very rigid and regards synonymous rolesets as incorrect.",
  "y": "uses"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_12",
  "x": "For VPCs, it checks whether there exists a PB multiword predicate for the expression and selects the first roleset of that predicate (e.g. there exists a predicate called open up (open.03) for the VPC 'open up'). For LVCs, it checks whether the noun has a corresponding verb predicate in PB and selects the first roleset of this predicate (e.g. walk.01 for take a walk). Note that this is an informed baseline that is very hard to beat and only fails in case of lack in coverage. ---------------------------------- **RESULTS AND DISCUSSION** We evaluated our approach on the 160 CPs annotated in the course of this work (Wiki50 set), as well as on the 70 take CPs from <cite>Bonial et al. (2014)</cite> (take set) and compare our results to the baseline. Table 2 shows percentage coverage, accuracy and the harmonic mean of coverage and accuracy for our system and the baseline.",
  "y": "uses"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_13",
  "x": "**CONCLUSIONS** We have presented an approach to handle CPs in SRL that extends on work from <cite>Bonial et al. (2014)</cite> . We automatically link VPCs and LVCs to the PB roleset that best describes their meaning, by relying on word alignments in parallel corpora and distributional methods. We set up an annotation effort to gather a frequency-balanced, contextualized evaluation set that is more natural, varied and larger than the pilot annotations provided by <cite>Bonial et al. (2014)</cite> . Our method can be used to alleviate the manual annotation effort by providing a correct alias in 44% of the cases (up to 72% for the more frequent test items when taking synonymous rolesets into account). These results are not too far from the upper bounds we calculate from human annotations. In future work, we would like to improve our method by incorporating the methods discussed in the error analysis section.",
  "y": "extends"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_14",
  "x": "Lastly, our system adheres to the most frequent sense baseline due to lack of word sense disambiguation of the CPs and assigns the alias that fits the most dominant sense of the CP in the corpus. ---------------------------------- **CONCLUSIONS** We have presented an approach to handle CPs in SRL that extends on work from <cite>Bonial et al. (2014)</cite> . We automatically link VPCs and LVCs to the PB roleset that best describes their meaning, by relying on word alignments in parallel corpora and distributional methods. We set up an annotation effort to gather a frequency-balanced, contextualized evaluation set that is more natural, varied and larger than the pilot annotations provided by <cite>Bonial et al. (2014)</cite> . Our method can be used to alleviate the manual annotation effort by providing a correct alias in 44% of the cases (up to 72% for the more frequent test items when taking synonymous rolesets into account).",
  "y": "extends"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_0",
  "x": "**INTRODUCTION** Deep neural network models have demonstrated strong performance on a number of challenging tasks, such as image classification (He et al., 2016) , speech recognition (Graves et al., 2013) , and various natural language processing tasks Kim, 2014; Xiong et al., 2016) . Recently, the augmentation of neural networks with external memory components has been shown to be a powerful means of capturing context of different types (Graves et al., 2014 Rae et al., 2016) . Of particular interest to this work is the work by<cite> Sukhbaatar et al. (2015)</cite> , on end-toend memory networks (N2Ns), which exhibit remarkable reasoning capabilities, e.g. for reasoning and goal-oriented dialogue tasks . Typically, such tasks consist of three key components: a sequence of supporting facts (the story), a question, and its answer. An example task is given in Figure 1 . Given the first two as input, it is the model's job to reason over the supporting facts and predict the answer to the question.",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_1",
  "x": "---------------------------------- **INTRODUCTION** Deep neural network models have demonstrated strong performance on a number of challenging tasks, such as image classification (He et al., 2016) , speech recognition (Graves et al., 2013) , and various natural language processing tasks Kim, 2014; Xiong et al., 2016) . Recently, the augmentation of neural networks with external memory components has been shown to be a powerful means of capturing context of different types (Graves et al., 2014 Rae et al., 2016) . Of particular interest to this work is the work by<cite> Sukhbaatar et al. (2015)</cite> , on end-toend memory networks (N2Ns), which exhibit remarkable reasoning capabilities, e.g. for reasoning and goal-oriented dialogue tasks . Typically, such tasks consist of three key components: a sequence of supporting facts (the story), a question, and its answer. An example task is given in Figure 1 .",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_2",
  "x": "Of particular interest to this work is the work by<cite> Sukhbaatar et al. (2015)</cite> , on end-toend memory networks (N2Ns), which exhibit remarkable reasoning capabilities, e.g. for reasoning and goal-oriented dialogue tasks . Typically, such tasks consist of three key components: a sequence of supporting facts (the story), a question, and its answer. An example task is given in Figure 1 . Given the first two as input, it is the model's job to reason over the supporting facts and predict the answer to the question. One drawback of N2Ns is the problem of choosing between two types of weight tying (adjacent and layer-wise; see Section 2 for a technical description). While N2Ns generally work well with either weight tying approach, as reported in<cite> Sukhbaatar et al. (2015)</cite> , the performance is uneven on some difficult tasks. That is, for some tasks, one weight tying approach attains nearperfect accuracy and the other performs poorly, but for other tasks, this trend is reversed.",
  "y": "motivation"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_3",
  "x": "End-to-End Memory Networks: Building on top of memory networks ,<cite> Sukhbaatar et al. (2015)</cite> ing the memory position supervision and making the model trainable in an end-to-end fashion, through the advent of supporting memories and a memory access controller. Representations of the context sentences x 1 , . . . , x n in the story are encoded using two sets of embedding matrices A and C (both of size d \u21e5 |V | where d is the embedding size and |V | the vocabulary size), and stored in the input and output memory cells m 1 , . . . , m n and c 1 , . . . , c n , each of which is obtained via is a function that maps the input into a bag of dimension |V |. The input question q is encoded with another embedding matrix B 2 R d\u21e5|V | such that u = B (q). N2N utilises the question embedding u and the input memory representations m i to measure the relevance between the question and each supporting context sentence, resulting in a vector of attention weights: where softmax(a i ) = e a i P j e a j . Once the attention weights have been computed, the memory access controller receives the response o in the form of a weighted sum over the output memory representations:",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_4",
  "x": "The input question q is encoded with another embedding matrix B 2 R d\u21e5|V | such that u = B (q). N2N utilises the question embedding u and the input memory representations m i to measure the relevance between the question and each supporting context sentence, resulting in a vector of attention weights: where softmax(a i ) = e a i P j e a j . Once the attention weights have been computed, the memory access controller receives the response o in the form of a weighted sum over the output memory representations: To enhance the model's ability to cope with more challenging tasks requiring multiple supporting facts from the memory,<cite> Sukhbaatar et al. (2015)</cite> further extended the model by stacking multiple memory layers (also known as \"hops\"), in which case the output of the k th hop is taken as input to the (k + 1) th hop: Lastly, N2N predicts the answer to question q using a softmax function: where\u0177 is the predicted answer distribution, W 2 R |V |\u21e5d is a parameter matrix for the model to learn (note that in the context of bAbI tasks, answers are single words), and K is the total number of hops. ----------------------------------",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_5",
  "x": "Once the attention weights have been computed, the memory access controller receives the response o in the form of a weighted sum over the output memory representations: To enhance the model's ability to cope with more challenging tasks requiring multiple supporting facts from the memory,<cite> Sukhbaatar et al. (2015)</cite> further extended the model by stacking multiple memory layers (also known as \"hops\"), in which case the output of the k th hop is taken as input to the (k + 1) th hop: Lastly, N2N predicts the answer to question q using a softmax function: where\u0177 is the predicted answer distribution, W 2 R |V |\u21e5d is a parameter matrix for the model to learn (note that in the context of bAbI tasks, answers are single words), and K is the total number of hops. ---------------------------------- **CURRENT ISSUES AND MOTIVATION:** In<cite> Sukhbaatar et al. (2015)</cite> , two types of weight tying were explored for N2N, namely adjacent (\"ADJ\") and layer-wise (\"LW\"). With LW, the input and output embedding matrices are shared across different hops (i.e., A 1 = A 2 = . . . = A K and C 1 = C 2 = . . . = C K ), resembling RNNs.",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_6",
  "x": "With LW, the input and output embedding matrices are shared across different hops (i.e., A 1 = A 2 = . . . = A K and C 1 = C 2 = . . . = C K ), resembling RNNs. With ADJ, on the other hand, not only is the output embedding for a given layer shared with the corresponding input embedding (i.e., A k+1 = C k ), the answer prediction matrix W and question embedding matrix B are also constrained such that W > = C K and B = A 1 . While both ADJ and LW work well, achieving comparable overall performance in terms of mean error over the 20 bAbI tasks, their performance on a subset of the tasks (i.e., tasks 3, 16, 17 and 19, as shown in Table 1 ) is inconsistent, with one performing very well, and the other performing poorly. Based on this observation, we propose a unified weight tying mechanism exploiting the benefits of both ADJ and LW, and capable of dynamically determining the best weight tying approach for a given task. Table 1 : Accuracy (%) reported in<cite> (Sukhbaatar et al., 2015)</cite> on a selected subset of the 20 bAbI 10k tasks. Note that performance in the LW column is obtained with a larger embedding size d = 100 and ReLU non-linearity applied to the internal state after each hop. Related reasoning models: Gated End-to-End Memory Networks (GN2Ns) (Liu and Perez, 2017) are a variant of N2N with a simple yet effective gating mechanism on the connections between hops, allowing the model to dynamically regulate the information flow between the controller and the memory.",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_7",
  "x": "2 Next, z is defined to be a vector of dimension d: where W z is a weight matrix, b z a bias term , the sigmoid function and Dim. of u 0 and h T . Essentially, the gating vector z is now dependent on not only the question u 0 , but also the context sentences in the memory encoded in h T . Note that the gating vector z can be replaced by a gating scalar z, but we choose to use a vector for more fine-grained control as in LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs . To simplify the model, we constrain B and W > to share the same parameters as A 1 and C K . Moreover, following<cite> Sukhbaatar et al. (2015)</cite> , we add a linear mapping H 2 R d\u21e5d to the update connection between memory hops, but in our case, down-weight it by 1 z, resulting in:",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_8",
  "x": "3 Training Details: Following<cite> Sukhbaatar et al. (2015)</cite>, we hold out 10% of the bAbI training set to form a development set. Position encoding and temporal encoding (with 10% random noise) are also incorporated into the model. Training is performed over 100 epochs with a batch size of 32 using the Adam optimiser (Kingma and Ba, 2015) with a learning rate of 0.005. Following<cite> Sukhbaatar et al. (2015)</cite> , linear start is employed in all our experiments for the first 20 epochs. All weight parameters are initialised based on a Gaussian distribution with zero mean and = 0.1. Gradients with an`2 norm of 40 are divided by a scalar to have norm 40.",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_9",
  "x": "Position encoding and temporal encoding (with 10% random noise) are also incorporated into the model. Training is performed over 100 epochs with a batch size of 32 using the Adam optimiser (Kingma and Ba, 2015) with a learning rate of 0.005. Following<cite> Sukhbaatar et al. (2015)</cite> , linear start is employed in all our experiments for the first 20 epochs. All weight parameters are initialised based on a Gaussian distribution with zero mean and = 0.1. Gradients with an`2 norm of 40 are divided by a scalar to have norm 40. Also following<cite> Sukhbaatar et al. (2015)</cite> , we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and to ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_10",
  "x": "Position encoding and temporal encoding (with 10% random noise) are also incorporated into the model. Training is performed over 100 epochs with a batch size of 32 using the Adam optimiser (Kingma and Ba, 2015) with a learning rate of 0.005. Following<cite> Sukhbaatar et al. (2015)</cite> , linear start is employed in all our experiments for the first 20 epochs. All weight parameters are initialised based on a Gaussian distribution with zero mean and = 0.1. Gradients with an`2 norm of 40 are divided by a scalar to have norm 40. Also following<cite> Sukhbaatar et al. (2015)</cite> , we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and to ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_11",
  "x": "All weight parameters are initialised based on a Gaussian distribution with zero mean and = 0.1. Gradients with an`2 norm of 40 are divided by a scalar to have norm 40. Also following<cite> Sukhbaatar et al. (2015)</cite> , we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and to ---------------------------------- **0.001.** Consistent with other published results over bAbI<cite> (Sukhbaatar et al., 2015</cite>; Seo et al., 2017) , we repeat training 30 times for each task, and select the model which performs best on the development set. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_12",
  "x": "The results on the 20 bAbI QA tasks are presented in Table 3 . We benchmark against other memory network based models: (1) N2N with ADJ and LW<cite> (Sukhbaatar et al., 2015)</cite> ; (2) DMN (Kumar et al., 2016) and its improved version DMN+ (Xiong et al., 2016) ; and (3) GN2N (Liu and Perez, 2017) . Major improvements on the difficult tasks. The most noticeable performance gains are over tasks 16, 17 and 19 where, compared with the vanilla N2N, UN2N achieves much better results than the worst of ADJ and LW, surpassing both ADJ and LW in the case of tasks 17 and 18. This confirms the validity of the model. UN2N maintains equally competitive performance on the other tasks. Our unified weight tying scheme does not degrade performance on the less challenging tasks.",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_13",
  "x": "First, we mark the identity of the speaker of a given utterance (either user or bot). Second, we extend by 7 additional features, one for each of the 7 properties associated with a restaurant. Each of these 7 features indicates whether there are any exact matches between words in the candidate and those in the question or memory. We refer to these 7 features as the match features. In terms of the training procedure, experiments are carried out with the same configuration as described in Section 4.1. As a large variance can be observed due to how sensitive memory-based models are to parameter initialisation, following<cite> (Sukhbaatar et al., 2015)</cite> and (Liu and Perez, 2017) , we repeat each training 10 times using the Table 4 : Per-response accuracy on the Dialog bAbI tasks. N2N: .",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_14",
  "x": "same hyper-parameters and choose the best system based on validation performance. ---------------------------------- **RESULTS** The results on the Dialog bAbI tasks are shown in Table 4 . In terms of baselines, we benchmark against other memory network-based models: 4 (1) N2N<cite> (Sukhbaatar et al., 2015)</cite> ; and (2) GN2N<cite> (Sukhbaatar et al., 2015)</cite> . While the results of GN2N is achieved with ADJ, the type of weight tying for N2N is not reported in . Improvements on task 5.",
  "y": "uses"
 },
 {
  "id": "7b4a976ba6a43b5ba42cc350b4d132_0",
  "x": "Recent work has been inspired by efforts in improving model's interpretability in image processing tasks, in particular by the Layerwise Relevance Propagation (LRP) [3] . In LRP, the classification decision of a deep neural network is decomposed backward across the network layers and evidence about the contribution to the final decision brought by individual input fragments (i.e., pixels of the input image) is gathered. We propose here to extend the LRP application to a linguistically motivated network architecture, known as <cite>Kernel-Based Deep Architecture</cite> (<cite>KDA</cite>) <cite>[5]</cite> , which frames semantic information captured by linguistic Tree Kernel [2] methods within the neural-based learning paradigm. The result is a mechanism that, for each system's prediction such as in question classification, generates an argument-by-analogy explanation based on real training examples, not necessarily similar to the input. We also propose here a novel approach to evaluate numerically the interpretability of any explanation-enriched model applied in semantic inference tasks. By defining a specific audit process, we derive a synthetic metric, i.e. Auditing Accuracy, that takes into account the properties of transparency, informativeness and effectiveness. The evaluation of the proposed methodology shows the meaningful impact of LRP-based explanation models: users faced with explanations are systematically oriented to accept (or reject) the system decisions, so that post-hoc judgments may even help in improving the overall application accuracy.",
  "y": "uses"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_0",
  "x": "We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006;<cite> Galley et al., 2006</cite>; May and Knight, 2007) , as opposed to formally syntactic systems such as Hiero (Chiang, 2007) . The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given. Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs. In this paper, we explore methods for restricting the space of possible TTS templates under consideration, while still allowing good templates to emerge directly from the data as much as possible. We find an improvement in translation accuracy through, first, using constraints to limit the number of new templates, second, using Bayesian methods to limit which of these new templates are favored when re-analyzing the training data with EM, and, third, experimenting with different renormalization techniques for the EM re-analysis. We introduce two constraints to limit the number of TTS templates that we extract directly from tree/string pairs without using word alignments. The first constraint is to limit direct TTS template extraction to the part of the corpus where word alignment tools such as GIZA++ do poorly.",
  "y": "uses"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_1",
  "x": "The RHS of a TTS template shows one possible translation and reordering of its LHS. The variables in a TTS template are further transformed using other TTS templates, and the recursive process continues until there are no variables left. There are two ways that TTS templates are commonly used in machine translation. The first is synchronous parsing <cite>(Galley et al., 2006</cite>; May and Knight, 2007) , where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up. The other way is the TTS transducer (Liu et al., 2006; Huang et al., 2006) , where TTS templates are used just as their name indicates: to transform a source parse tree (or forest) into the proper target string. Since synchronous parsing considers all possible synchronous parse trees of the source sentence, it is less constrained than TTS transducers and hence requires more computational power. In this paper, we use a TTS transducer to test the performance of different TTS templates, but our techniques could also be applied to SSMT systems based on synchronous parsing.",
  "y": "background"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_2",
  "x": "There are two ways that TTS templates are commonly used in machine translation. The first is synchronous parsing <cite>(Galley et al., 2006</cite>; May and Knight, 2007) , where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up. The other way is the TTS transducer (Liu et al., 2006; Huang et al., 2006) , where TTS templates are used just as their name indicates: to transform a source parse tree (or forest) into the proper target string. Since synchronous parsing considers all possible synchronous parse trees of the source sentence, it is less constrained than TTS transducers and hence requires more computational power. In this paper, we use a TTS transducer to test the performance of different TTS templates, but our techniques could also be applied to SSMT systems based on synchronous parsing. ---------------------------------- **BASELINE APPROACH: TTS TEMPLATES OBEYING WORD ALIGNMENT**",
  "y": "similarities background"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_3",
  "x": "---------------------------------- **BASELINE APPROACH: TTS TEMPLATES OBEYING WORD ALIGNMENT** TTS templates are commonly generated by decomposing a pair of aligned source syntax tree and target string into smaller pairs of tree fragments and target string (i.e., the TTS templates). To keep the number of TTS templates to a manageable scale, only the non-decomposable TTS templates are generated. This algorithm is referred to as GHKM (Galley et al., 2004) and is widely used in SSMT systems <cite>(Galley et al., 2006</cite>; Liu et al., 2006; Huang et al., 2006) . The word alignment used in GHKM is usually computed independent of the syntactic structure, and as DeNero and Klein (2007) and May and Knight (2007) have noted, Ch-En En-Ch Union Heuristic 28.6% 33.0% 45.9% 20.1% Table 2 : In the selected big templates, the distribution of words in the templates of different sizes, which are measured based on the number of symbols in their RHSs is not the best for SSMT systems.",
  "y": "similarities"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_4",
  "x": "The other one is normalization based on the root of the LHS (ROOTN)<cite> (Galley et al., 2006)</cite> , corresponding to the generative process where, given the root of the syntax subtree, the LHS syntax subtree and the RHS string are generated simultaneously. By omitting the decomposition probability in the LHS-based generative model, the two generative models share the same formula for computing the probability of a training instance: where T and S denote the source syntax tree and target string respectively, R denotes the decomposition of (T, S), and t denotes the TTS template. The expected counts of the TTS templates can then be efficiently computed using an inside-outsidelike dynamic programming algorithm (May and Knight, 2007) . LHSN, as shown by<cite> Galley et al. (2006)</cite> , cannot accurately restore the true conditional probabilities of the target sentences given the source sentences in the training corpus. This indicates that LHSN is not good at predicting unseen sentences or at translating new sentences. But this deficiency does not affect its ability to estimate the expected counts of the TTS templates, because the posteriors of the TTS templates only depend on the comparative probabilities of the different derivations of a training instance (a pair of tree and string).",
  "y": "background"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_5",
  "x": "There are two commonly used generative models for syntaxbased MT systems, each of which corresponds to a normalization method for the TTS templates. The LHS-based normalization (LHSN) (Liu et al., 2006; Huang et al., 2006) , corresponds to the generative process where the source syntax subtree is first generated, and then the target string is generated given the source syntax subtree. The other one is normalization based on the root of the LHS (ROOTN)<cite> (Galley et al., 2006)</cite> , corresponding to the generative process where, given the root of the syntax subtree, the LHS syntax subtree and the RHS string are generated simultaneously. By omitting the decomposition probability in the LHS-based generative model, the two generative models share the same formula for computing the probability of a training instance: where T and S denote the source syntax tree and target string respectively, R denotes the decomposition of (T, S), and t denotes the TTS template. The expected counts of the TTS templates can then be efficiently computed using an inside-outsidelike dynamic programming algorithm (May and Knight, 2007) . LHSN, as shown by<cite> Galley et al. (2006)</cite> , cannot accurately restore the true conditional probabilities of the target sentences given the source sentences in the training corpus.",
  "y": "background"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_6",
  "x": "where T and S denote the source syntax tree and target string respectively, R denotes the decomposition of (T, S), and t denotes the TTS template. The expected counts of the TTS templates can then be efficiently computed using an inside-outsidelike dynamic programming algorithm (May and Knight, 2007) . LHSN, as shown by<cite> Galley et al. (2006)</cite> , cannot accurately restore the true conditional probabilities of the target sentences given the source sentences in the training corpus. This indicates that LHSN is not good at predicting unseen sentences or at translating new sentences. But this deficiency does not affect its ability to estimate the expected counts of the TTS templates, because the posteriors of the TTS templates only depend on the comparative probabilities of the different derivations of a training instance (a pair of tree and string). In fact, as we show in Section 4, LHSN is better than ROOTN in liberating smaller TTS templates out of the big templates, since it is less biased to the big templates in the EM training. 4 Because the two normalization methods have their 4 Based on LHSN, the difference between the probability of a big Template and the product of the probabilities of E-step: for all pair of syntax tree T and target string S do for all TTS Template t do EC(t)+ = (Rose et al., 1992 ) is is used in our system to speed up the training process, similar to Goldwater et al. (2006) .",
  "y": "uses"
 },
 {
  "id": "7c3f94a231c83c94b5d93c33ab8bfa_0",
  "x": "**INTRODUCTION** This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b) , which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation <cite>(Zhang and Clark, 2007)</cite> , dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012) , context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013) , combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013) , achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010) , joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012) , joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013) , and joint segmentation, POS-tagging and parsing . In addition to the aforementioned tasks, the framework can be applied to all structural prediction tasks for which the output can be constructed using an incremental process. The advantage of this framework is two-fold. First, beamsearch enables highly efficient decoding, which typically has linear time complexity, depending on the incremental process. Second, free from DPstyle constraints and Markov-style independence assumptions, the framework allows arbitrary features to be defined to capture structural patterns.",
  "y": "background"
 },
 {
  "id": "7c3f94a231c83c94b5d93c33ab8bfa_1",
  "x": "Second, free from DPstyle constraints and Markov-style independence assumptions, the framework allows arbitrary features to be defined to capture structural patterns. In addition to feature advantages, the high accuracies of this framework are also enabled by direct interactions between learning and search (Daum\u00e9 III and Marcu, 2005; Huang et al., 2012; Zhang and Nivre, 2012) . ---------------------------------- **TUTORIAL OVERVIEW** In this tutorial, we make an introduction to the framework, illustrating how it can be applied to a range of NLP problems, giving theoretical discussions and demonstrating a software implementation. We start with a detailed introduction of the framework, describing the averaged perceptron algorithm (Collins, 2002) and its efficient implementation issues <cite>(Zhang and Clark, 2007)</cite> , as well as beam-search and the early-update strategy (Collins and Roark, 2004) . We then illustrate how the framework can be applied to NLP tasks, including word segmentation, joint segmentation & POS-tagging, labeled and unlabeled dependency parsing, joint POS-tagging and dependency parsing, CFG parsing, CCG parsing, and joint segmentation, POS-tagging and parsing.",
  "y": "motivation"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_0",
  "x": "Recently there has been a growing interest in models for sentence-level representations using neural networks. Sentence embeddings are distributed representations of natural language sentences with the intention to encode the meaning of the sentences in a neural network representation. Sentence embeddings have been generated using unsupervised learning approaches (e.g. Hill et al., 2016) , and supervised learning (e.g. Bowman et al., 2016;<cite> Conneau et al., 2017)</cite> . Sentence-level representations have shown promise in multiple different NLP tasks. One prominent example is natural language inference. Natural language inference (NLI) is the task of determining the inferential relationship between two or more sentences. That is, given two sentences, the premise p and the hypothesis h, the task is to determine whether h is entailed by p, whether the sentences are in contradiction with each other or whether they are neutral.",
  "y": "background"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_1",
  "x": "There are two main approaches to NLI utilizing neural networks. Some approaches focus on building sentence embeddings for the premises and the hypothesis separately and then combine those using a classifier (e.g. Bowman et al., 2015 Bowman et al., , 2016<cite> Conneau et al., 2017)</cite> . Other approaches do not treat the two sentences separately but utilize e.g. crosssentence attention (Tay et al., 2018; Chen et al., 2017a) . In this paper we focus on the sentence embedding approach. Motivated by the success of the architecture of InferSent<cite> (Conneau et al., 2017)</cite> , we build a hierarchical architecture utilizing bidirectional LSTM (BiLSTM) layers and max pooling. All in all, our model in on par with the state of the art for Stanford Natural Language Inference corpus (SNLI) (Bowman et al., 2015) sentence encoding-based models and improves the previous state of the art for SciTail (Khot et al., 2018) . We also achieve strong results for the Multi-Genre Natural Language Inference corpus (MultiNLI) (Williams et al., 2018) .",
  "y": "background"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_2",
  "x": "Motivated by the success of the architecture of InferSent<cite> (Conneau et al., 2017)</cite> , we build a hierarchical architecture utilizing bidirectional LSTM (BiLSTM) layers and max pooling. All in all, our model in on par with the state of the art for Stanford Natural Language Inference corpus (SNLI) (Bowman et al., 2015) sentence encoding-based models and improves the previous state of the art for SciTail (Khot et al., 2018) . We also achieve strong results for the Multi-Genre Natural Language Inference corpus (MultiNLI) (Williams et al., 2018) . We also test our model on a number of transfer learning tasks using the SentEval testing library<cite> (Conneau et al., 2017)</cite> , and show that our model outperforms the InferSent model on 7 out of 10 and SkipThought on 8 out of 9 tasks, comparing to the scores reported by <cite>Conneau et al. (2017)</cite> . Moreover, our model outperforms the InferSent model in 8 out of 10 recently published SentEval probing tasks designed to evaluate sentence embeddings' ability to capture some of the important linguistic properties of sentences (Conneau et al., 2018) . This highlights the generalization capability of our proposed model, confirming that the proposed architecture is able to produce sentence embeddings with strong performance across a wide variety of different NLP tasks. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_3",
  "x": "Some approaches focus on building sentence embeddings for the premises and the hypothesis separately and then combine those using a classifier (e.g. Bowman et al., 2015 Bowman et al., , 2016<cite> Conneau et al., 2017)</cite> . Other approaches do not treat the two sentences separately but utilize e.g. crosssentence attention (Tay et al., 2018; Chen et al., 2017a) . In this paper we focus on the sentence embedding approach. Motivated by the success of the architecture of InferSent<cite> (Conneau et al., 2017)</cite> , we build a hierarchical architecture utilizing bidirectional LSTM (BiLSTM) layers and max pooling. All in all, our model in on par with the state of the art for Stanford Natural Language Inference corpus (SNLI) (Bowman et al., 2015) sentence encoding-based models and improves the previous state of the art for SciTail (Khot et al., 2018) . We also achieve strong results for the Multi-Genre Natural Language Inference corpus (MultiNLI) (Williams et al., 2018) . We also test our model on a number of transfer learning tasks using the SentEval testing library<cite> (Conneau et al., 2017)</cite> , and show that our model outperforms the InferSent model on 7 out of 10 and SkipThought on 8 out of 9 tasks, comparing to the scores reported by <cite>Conneau et al. (2017)</cite> .",
  "y": "differences uses"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_4",
  "x": "<cite>Conneau et al. (2017)</cite> explore multiple different sentence embedding architectures ranging from LSTM, BiLSTM and intra-attention to convolution neural networks and the performance of these architectures on NLI tasks. They show that out of these models BiLSTM with max pooling achieves the strongest results in NLI. They also show that their model trained on NLI data achieves strong performance on various transfer learning tasks. Although sentence embedding approaches have shown their effectiveness in NLI, there are multiple studies showing that treating the hypothesis and premise sentences together and focusing on the relationship between those sentences yields better results (e.g. Tay et al., 2018; Chen et al., 2017a) . However, as these methods are focused on the inference relations rather than the internal semantics of the sentences, they cannot as straightforwardly be used outside of the NLI context and do not offer similar insights about the sentence level semantics, as sentence embeddings do. By choosing a sentence embedding-based architecture we can more easily use the models in a wide variety of NLP tasks requiring sentence-level semantic information. ----------------------------------",
  "y": "background"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_5",
  "x": "of a forward and backward LSTMs The max pooling layer produces a vector of the same dimensionality as h t , returning, for each dimension, its maximum value over the hidden units (h 1 , . . . , h T ). Motivated by the strong results of the BiLSTM max pooling network by <cite>Conneau et al. (2017)</cite> , we experimented with combining BiLSTM max pooling networks as a hierarchical structure. 1 To improve the BiLSTM layers' ability to remember the input words, we let each layer of the stack re-read the input sentence. In our baseline model we stack three BiLSTM max pooling networks as a hierarchical structure, where each BiLSTM reads the input sentence as the input. At each BiLSTM layer except the first one, we initialize the initial hidden state and the cell state with the final state of the previous layer. We take the max value over each dimension of the hidden units for each BiLSTM layer.",
  "y": "motivation extends"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_6",
  "x": "To understand better what kind of inferential relationships our model is able to identify, we conducted an error analysis for the three datasets. We report the results below. 3 We also conducted a linguistic error analysis and compared our results to the results obtained with the InferSent BiLSTM max pooling model of <cite>Conneau et al. (2017)</cite> (our implementation). 4 3 For more detailed error statistics, see the appendix. 4 The scores for our implementation of InferSent are on par or slightly higher than the scores reported by <cite>Conneau et al. (2017)</cite> using their training setup. ---------------------------------- **SNLI**",
  "y": "uses"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_7",
  "x": "We report the results below. 3 We also conducted a linguistic error analysis and compared our results to the results obtained with the InferSent BiLSTM max pooling model of <cite>Conneau et al. (2017)</cite> (our implementation). 4 3 For more detailed error statistics, see the appendix. 4 The scores for our implementation of InferSent are on par or slightly higher than the scores reported by <cite>Conneau et al. (2017)</cite> using their training setup. ---------------------------------- **SNLI** On the SNLI dataset our model makes the least errors on sentence pairs labeled with entailment, having an accuracy of: 90.5%.",
  "y": "similarities"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_8",
  "x": "Table 6 summarizes the prediction accuracies per each gold label and compares the results to InferSent results. We also conducted additional linguistic error analysis using the annotation test set provided for MultiNLI. The results are reported in the appendix and they are mostly inconclusive. ---------------------------------- **TRANSFER LEARNING** To better understand how well our model generalizes to different tasks, we conducted additional transfer learning tests using the SentEval sentence embedding evaluation library 5<cite> (Conneau et al., 2017)</cite> and compared our results to the results published for InferSent and SkipThought . For the transfer learning tasks, we trained our model on NLI data consisting of the concatenation of the SNLI and MultiNLI training sets consisting of 942,854 sentence pairs in total.",
  "y": "uses"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_9",
  "x": "---------------------------------- **TRANSFER LEARNING** To better understand how well our model generalizes to different tasks, we conducted additional transfer learning tests using the SentEval sentence embedding evaluation library 5<cite> (Conneau et al., 2017)</cite> and compared our results to the results published for InferSent and SkipThought . For the transfer learning tasks, we trained our model on NLI data consisting of the concatenation of the SNLI and MultiNLI training sets consisting of 942,854 sentence pairs in total. This allows us to compare our results to the InferSent results which were obtained using a model trained on the same data<cite> (Conneau et al., 2017)</cite> . <cite>Conneau et al. (2017)</cite> have shown that including all the training data from SNLI and MultiNLI improves significantly the model performance on transfer learning tasks, compared to training the model only on SNLI data. For training the model we used the same setup as described above in Section 4.",
  "y": "similarities"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_10",
  "x": "**TRANSFER LEARNING** To better understand how well our model generalizes to different tasks, we conducted additional transfer learning tests using the SentEval sentence embedding evaluation library 5<cite> (Conneau et al., 2017)</cite> and compared our results to the results published for InferSent and SkipThought . For the transfer learning tasks, we trained our model on NLI data consisting of the concatenation of the SNLI and MultiNLI training sets consisting of 942,854 sentence pairs in total. This allows us to compare our results to the InferSent results which were obtained using a model trained on the same data<cite> (Conneau et al., 2017)</cite> . <cite>Conneau et al. (2017)</cite> have shown that including all the training data from SNLI and MultiNLI improves significantly the model performance on transfer learning tasks, compared to training the model only on SNLI data. For training the model we used the same setup as described above in Section 4. We used the SentEval sentence embedding evaluation library using the default settings 6 recommended on the SentEval website, with a logistic regression classifier, Adam optimizer with learning rate of 0.001, batch size of 64 and epoch size of 4.",
  "y": "background"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_13",
  "x": "We compare our results with the results published by Glockner et al. (2018) and to results obtained with InferSent sentence encoder (our implementation). Our HBMP model outperforms the InferSent model in 7 out of 14 categories, receiving an overall score of 65.1% (InferSent: 65.6%). Our model also compares well against the other models, outperforming Decomposable Attention model (51.90%) (Parikh et al., 2016) and Residual Encoders (62.20%) (Nie and Bansal, 2017b) in the overall score. As these models are not based purely on sentence embeddings, the obtained result highlights that sentence embedding approaches can be competitive when handling inferences requiring lexical information. Our model is still outperformed by and ESIM (Chen et al., 2017a) and KIM, an ESIM model incorporating external knowledge (Chen et al., 2018) . The results of the comparison are summarized in Glockner et al. (2018) . InferSent results obtained with our implementation using the architecture and training set-up described in<cite> (Conneau et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_14",
  "x": "We evaluated the sentence embedding architecture with three different natural language inference datasets, including the Stanford Natural Language Inference (SNLI) corpus, the Multi-Genre Natural Language Inference (MultiNLI) corpus and the SciTail dataset. In all our experiments with the three datasets we used only the training data provided in the respective corpus. For the transfer learning tasks, described in Section 7, we used training data from both the SNLI and the MultiNLI datasets in order to compare to the results by <cite>Conneau et al. (2017)</cite> . ---------------------------------- **SNLI** The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) is a dataset of 570k human-written sentence pairs manually labeled with entailment, contradiction, and neutral. The dataset is divided into training (550,152 pairs), development (10,000 pairs) and test sets (10,000 pairs).",
  "y": "motivation"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_0",
  "x": "In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation -namely WordSim-353 (Finkelstein et al., 2001) , MEN (Bruni et al., 2014) and <cite>SimLex-999</cite> (<cite>Hill et al., 2015</cite>) . For comparison, Vector Cosine is also calculated on several countbased DSMs. We implement a total of twenty-eight models with different parameters settings, each of which differs according to corpus size, context window width, weighting scheme and SVD application. The new metric is shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (Bullinaria and Levy, 2012) , against which APSyn still obtains competitive performances. The results are also discussed in relation to the state-of-the-art DSMs, as reported in <cite>Hill et al. (2015)</cite> . In such comparison, the best settings of our models outperform the word embeddings in almost all datasets. A pilot study was also carried out to investigate whether APSyn is scalable.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_1",
  "x": "We implement a total of twenty-eight models with different parameters settings, each of which differs according to corpus size, context window width, weighting scheme and SVD application. The new metric is shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (Bullinaria and Levy, 2012) , against which APSyn still obtains competitive performances. The results are also discussed in relation to the state-of-the-art DSMs, as reported in <cite>Hill et al. (2015)</cite> . In such comparison, the best settings of our models outperform the word embeddings in almost all datasets. A pilot study was also carried out to investigate whether APSyn is scalable. Results prove its high performance also when calculated on large corpora, such as those used by . On top of the performance, APSyn seems not to be subject to some of the biases that affect Vector Cosine.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_2",
  "x": "The results are also discussed in relation to the state-of-the-art DSMs, as reported in <cite>Hill et al. (2015)</cite> . In such comparison, the best settings of our models outperform the word embeddings in almost all datasets. A pilot study was also carried out to investigate whether APSyn is scalable. Results prove its high performance also when calculated on large corpora, such as those used by . On top of the performance, APSyn seems not to be subject to some of the biases that affect Vector Cosine. Finally, considering the debate about the ability of DSMs to calculate genuine similarity as opposed to word relatedness (Turney, 2001; Agirre et al., 2009; <cite>Hill et al., 2015</cite>) , we test the ability of the models to quantify genuine semantic similarity. ----------------------------------",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_3",
  "x": "The second higher score would be assigned to a and c, for the same reason as above. The lower score would be instead assigned to a and b, as they only share one non-salient feature. In section 3.4, we briefly discuss the hubness problem. ---------------------------------- **DATASETS** For our evaluation, we used three widely popular datasets: WordSim-353 (Finkelstein et al., 2001) , MEN (Bruni et al., 2014) , <cite>SimLex-999</cite> (<cite>Hill et al., 2015</cite>) . These datasets have a different history, but all of them consist in word pairs with an associated score, that should either represent word association or word similarity.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_4",
  "x": "In section 3.4, we briefly discuss the hubness problem. ---------------------------------- **DATASETS** For our evaluation, we used three widely popular datasets: WordSim-353 (Finkelstein et al., 2001) , MEN (Bruni et al., 2014) , <cite>SimLex-999</cite> (<cite>Hill et al., 2015</cite>) . These datasets have a different history, but all of them consist in word pairs with an associated score, that should either represent word association or word similarity. WordSim-353 (Finkelstein et al., 2001 ) was proposed as a word similarity dataset containing 353 pairs annotated with scores between 0 and 10. However, <cite>Hill et al. (2015)</cite> claimed that the instructions to the annotators were ambiguous with respect to similarity and association, so that the subjects assigned high similarity scores to entities that are only related by virtue of frequent association (e.g. coffee and cup; movie and theater).",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_5",
  "x": "Even though such a classification made a clear distinction between the two types of relations (i.e. similarity and association), <cite>Hill et al. (2015)</cite> argue that these gold standards still carry the scores they had in WordSim-353, which are known to be ambiguous in this regard. The MEN Test Collection (Bruni et al., 2014) includes 3,000 word pairs divided in two sets (one for training and one for testing) together with human judgments, obtained through Amazon Mechanical Turk. The construction was performed by asking subjects to rate which pair -among two of themwas the more related one (i.e. the most associated). Every pairs-couple was proposed only once, and a final score out of 50 was attributed to each pair, according to how many times it was rated as the most related. According to <cite>Hill et al. (2015)</cite> , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association. <cite>SimLex-999</cite> is the dataset introduced by <cite>Hill et al. (2015)</cite> to address the above mentioned criticisms of confusion between similarity and association. The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness.",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_6",
  "x": "The MEN Test Collection (Bruni et al., 2014) includes 3,000 word pairs divided in two sets (one for training and one for testing) together with human judgments, obtained through Amazon Mechanical Turk. The construction was performed by asking subjects to rate which pair -among two of themwas the more related one (i.e. the most associated). Every pairs-couple was proposed only once, and a final score out of 50 was attributed to each pair, according to how many times it was rated as the most related. According to <cite>Hill et al. (2015)</cite> , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association. <cite>SimLex-999</cite> is the dataset introduced by <cite>Hill et al. (2015)</cite> to address the above mentioned criticisms of confusion between similarity and association. The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness. The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association.",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_7",
  "x": "Even though such a classification made a clear distinction between the two types of relations (i.e. similarity and association), <cite>Hill et al. (2015)</cite> argue that these gold standards still carry the scores they had in WordSim-353, which are known to be ambiguous in this regard. The MEN Test Collection (Bruni et al., 2014) includes 3,000 word pairs divided in two sets (one for training and one for testing) together with human judgments, obtained through Amazon Mechanical Turk. The construction was performed by asking subjects to rate which pair -among two of themwas the more related one (i.e. the most associated). Every pairs-couple was proposed only once, and a final score out of 50 was attributed to each pair, according to how many times it was rated as the most related. According to <cite>Hill et al. (2015)</cite> , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association. <cite>SimLex-999</cite> is the dataset introduced by <cite>Hill et al. (2015)</cite> to address the above mentioned criticisms of confusion between similarity and association. The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness.",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_8",
  "x": "The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness. The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association. <cite>Hill et al. (2015)</cite> claim that differently from other datasets, <cite>SimLex-999</cite> interannotator agreement has not been surpassed by any automatic approach. ---------------------------------- **STATE OF THE ART VECTOR SPACE MODELS** In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by <cite>Hill et al. (2015)</cite> , who used the code (or directly the embeddings) shared by the original authors. As we trained our models on almost the same corpora used by <cite>Hill and colleagues</cite>, the results are perfectly comparable.",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_9",
  "x": "<cite>SimLex-999</cite> is the dataset introduced by <cite>Hill et al. (2015)</cite> to address the above mentioned criticisms of confusion between similarity and association. The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness. The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association. <cite>Hill et al. (2015)</cite> claim that differently from other datasets, <cite>SimLex-999</cite> interannotator agreement has not been surpassed by any automatic approach. ---------------------------------- **STATE OF THE ART VECTOR SPACE MODELS** In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by <cite>Hill et al. (2015)</cite> , who used the code (or directly the embeddings) shared by the original authors.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_10",
  "x": "<cite>Hill et al. (2015)</cite> claim that differently from other datasets, <cite>SimLex-999</cite> interannotator agreement has not been surpassed by any automatic approach. ---------------------------------- **STATE OF THE ART VECTOR SPACE MODELS** In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by <cite>Hill et al. (2015)</cite> , who used the code (or directly the embeddings) shared by the original authors. As we trained our models on almost the same corpora used by <cite>Hill and colleagues</cite>, the results are perfectly comparable. The three models we compare our results to are: i) the convolutional neural network of Collobert and Weston (2008) , which was trained on 852 million words of Wikipedia; ii) the neural network of Huang et al. (2012) , which was trained on 990 million words of Wikipedia; and iii) the word2vec of Mikolov et al. (2013) , which was trained on 1000 million words of Wikipedia and on the RCV Vol. 1 Corpus (Lewis et al., 2004 Mikolov et al. (2013) , as reported in <cite>Hill et al. (2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_12",
  "x": "The RCV Vol. 1 and Wikipedia were automatically tagged, respectively, with the POS tagger described in Dell'Orletta (2009) and with the TreeTagger (Schmid, 1994) . ---------------------------------- **DSMS** For our experiments, we implemented twenty-eight DSMs, but for reasons of space only sixteen of them are reported in the tables. All of them include the pos-tagged target words used in the three datasets (i.e. MEN, WordSim-353 and <cite>SimLex-999</cite>) and the pos-tagged contexts having frequency above 100 in the two corpora. We considered as contexts the content words (i.e. nouns, verbs and adjectives) within a window of 2, 3 and 5, even though the latter was given up for its poor performances.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_13",
  "x": "Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets <cite>Simlex-999</cite>, WordSim-353 and MEN. In the bottom the performance of the state-of-the-art models of Collobert and Weston (2008) , Huang et al. (2012), Mikolov et al. (2013) , as reported in <cite>Hill et al. (2015)</cite> . The Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2 . In particular, Table 1 describes the performances on <cite>SimLex-999</cite>, WordSim-353 and MEN for the measures applied on RCV Vol. 1 models. Table 2 , instead, describes the performances of the measures on the three datasets for the Wikipedia models. Concurrently, Table 3 and Table 4 describe the performances of the measures respectively on the RCV Vol.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_14",
  "x": "---------------------------------- **MEASURING WORD SIMILARITY AND RELATEDNESS** Given the twenty-eight DSMs, for each dataset we have measured the Vector Cosine and APSyn between the words in the test pairs. Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets <cite>Simlex-999</cite>, WordSim-353 and MEN. In the bottom the performance of the state-of-the-art models of Collobert and Weston (2008) , Huang et al. (2012), Mikolov et al. (2013) , as reported in <cite>Hill et al. (2015)</cite> . The Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2 . In particular, Table 1 describes the performances on <cite>SimLex-999</cite>, WordSim-353 and MEN for the measures applied on RCV Vol.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_15",
  "x": "In particular, Table 1 describes the performances on <cite>SimLex-999</cite>, WordSim-353 and MEN for the measures applied on RCV Vol. 1 models. Table 2 , instead, describes the performances of the measures on the three datasets for the Wikipedia models. Concurrently, Table 3 and Table 4 describe the performances of the measures respectively on the RCV Vol. 1 and Wikipedia models, tested on the subsets of WordSim-353 extracted by Agirre et al. (2009) . Table 1 shows the Spearman correlation scores for Vector Cosine and APSyn on the three datasets for the eight most representative DSMs built using RCV Vol. 1. Table 2 does the same for the DSMs built using Wikipedia. For the sake of comparison, we also report the results of the state-of-the-art DSMs mentioned in <cite>Hill et al. (2015)</cite> (see Section 2.5).",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_16",
  "x": "1 models. Table 2 , instead, describes the performances of the measures on the three datasets for the Wikipedia models. Concurrently, Table 3 and Table 4 describe the performances of the measures respectively on the RCV Vol. 1 and Wikipedia models, tested on the subsets of WordSim-353 extracted by Agirre et al. (2009) . Table 1 shows the Spearman correlation scores for Vector Cosine and APSyn on the three datasets for the eight most representative DSMs built using RCV Vol. 1. Table 2 does the same for the DSMs built using Wikipedia. For the sake of comparison, we also report the results of the state-of-the-art DSMs mentioned in <cite>Hill et al. (2015)</cite> (see Section 2.5). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_17",
  "x": "As a note about iii), the results of using SVD jointly with LMI spaces are less predictable than when combining it with PPMI. Also, we can notice that the smaller window (i.e. 2) does not always perform better than the larger one (i.e. 3). The former appears to perform better on <cite>SimLex-999</cite>, while the latter seems to have some advantages on the other datasets. This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in <cite>SimLex-999</cite> (i.e. genuine similarity). On top of it, despite <cite>Hill et al. (2015)</cite> 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (Agirre et al., 2009; Kiela and Clark, 2014) , we need to mention that window 5 was abandoned because of its low performance. With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by Schnabel et al. (2015) , using the words of the <cite>SimLex-999</cite> dataset as query words and collecting for each of them the top 1000 nearest neighbors. Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora.",
  "y": "differences"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_18",
  "x": "These two models perform consistently and in a comparable way across the datasets, generally outperforming the state-of-the-art DSMs, with an exception for the Wikipedia-trained models in WordSim-353. Some further observations are: i) corpus size strongly affects the results; ii) PPMI strongly outperforms LMI for both Vector Cosine and APSyn; iii) SVD boosts the Vector Cosine, especially when it is combined with PPMI; iv) N has some impact on the performance of APSyn, which generally achieves the best results for N=500. As a note about iii), the results of using SVD jointly with LMI spaces are less predictable than when combining it with PPMI. Also, we can notice that the smaller window (i.e. 2) does not always perform better than the larger one (i.e. 3). The former appears to perform better on <cite>SimLex-999</cite>, while the latter seems to have some advantages on the other datasets. This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in <cite>SimLex-999</cite> (i.e. genuine similarity). On top of it, despite <cite>Hill et al. (2015)</cite> 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (Agirre et al., 2009; Kiela and Clark, 2014) , we need to mention that window 5 was abandoned because of its low performance.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_19",
  "x": "These two models perform consistently and in a comparable way across the datasets, generally outperforming the state-of-the-art DSMs, with an exception for the Wikipedia-trained models in WordSim-353. Some further observations are: i) corpus size strongly affects the results; ii) PPMI strongly outperforms LMI for both Vector Cosine and APSyn; iii) SVD boosts the Vector Cosine, especially when it is combined with PPMI; iv) N has some impact on the performance of APSyn, which generally achieves the best results for N=500. As a note about iii), the results of using SVD jointly with LMI spaces are less predictable than when combining it with PPMI. Also, we can notice that the smaller window (i.e. 2) does not always perform better than the larger one (i.e. 3). The former appears to perform better on <cite>SimLex-999</cite>, while the latter seems to have some advantages on the other datasets. This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in <cite>SimLex-999</cite> (i.e. genuine similarity). On top of it, despite <cite>Hill et al. (2015)</cite> 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (Agirre et al., 2009; Kiela and Clark, 2014) , we need to mention that window 5 was abandoned because of its low performance.",
  "y": "differences"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_20",
  "x": "With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by Schnabel et al. (2015) , using the words of the <cite>SimLex-999</cite> dataset as query words and collecting for each of them the top 1000 nearest neighbors. Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora. Figure 1 shows the relation between the rank in the nearest neighbor list and the rank in the frequency list. It can be easily noticed that the highest ranked nearest neighbors tend to have higher rank also in the frequency list, supporting the idea that frequent words are more likely to be nearest neighbors. APSyn does not seem to be able to overcome such bias, which seems to be in fact an inherent property of the DSMs (Radovanovic et al., 2010) . Further investigation is needed to see whether variations of APSyn can tackle this problem. Finally, few words need to be spent with regard to the ability of calculating genuine similarity, as distinguished from word relatedness (Turney, 2001; Agirre et al., 2009; <cite>Hill et al., 2015</cite>) .",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_21",
  "x": "Finally, few words need to be spent with regard to the ability of calculating genuine similarity, as distinguished from word relatedness (Turney, 2001; Agirre et al., 2009; <cite>Hill et al., 2015</cite>) . Table 3 and  Table 4 show the Spearman correlation scores for the two measures calculated on the models respectively trained on RCV1 and Wikipedia, tested on the subsets of WordSim-353 extracted by Agirre et al. (2009) . It can be easily noticed that our best models work better on the similarity subset. In particular, APSynPPMI performs about 20-30% better for the similarity subset than for the relatedness one (see Table 3 ), as well as both APSynPPMI and CosSVDPPMI do in Wikipedia (see Table 4 ). Table 4 : Spearman correlation results for our eight models trained on Wikipedia, in the subsets of WordSim-353. ---------------------------------- **SCALABILITY**",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_22",
  "x": "We developed twenty-eight count-based DSMs, each of which implementing different hyperparameters. PPMI emerged as the most efficient association measure: it works particularly well with Vector Cosine, when combined with SVD, and it boosts APSyn. APSyn showed extremely promising results, despite its conceptual simplicity. It outperforms the Vector Cosine in almost all settings, except when the latter is used on a PPMI-weighed SVD-reduced DSM. Even in this case, anyway, its performance is very competitive. Interestingly, our best models achieve results that are comparable to -or even better than -those reported by <cite>Hill et al. (2015)</cite> for the stateof-the-art word embeddings models. In Section 3.5 we show that APSyn is scalable, outperforming the state-of-the-art count-based models reported in .",
  "y": "similarities"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_23",
  "x": "Even in this case, anyway, its performance is very competitive. Interestingly, our best models achieve results that are comparable to -or even better than -those reported by <cite>Hill et al. (2015)</cite> for the stateof-the-art word embeddings models. In Section 3.5 we show that APSyn is scalable, outperforming the state-of-the-art count-based models reported in . On top of it, APSyn does not suffer from some of the problems reported for the Vector Cosine, such as the inability of identifying the number of shared features. It still however seems to be affected by the hubness issue, and more research should be carried out to tackle it. Concerning the discrimination between similarity and association, the good performance of APSyn on <cite>SimLex-999</cite> (which was built with a specific attention to genuine similarity) and the large difference in performance between the two subsets of WordSim-353 described in Table  3 and Table 4 make us conclude that APSyn is indeed efficient in quantifying genuine similarity. To conclude, being a linguistically and cognitively grounded metric, APSyn offers the possibility for further improvements, by simply combining it to other properties that were not yet considered in its definition.",
  "y": "uses"
 },
 {
  "id": "7e52a90a9a0a703250d5c3c1890058_0",
  "x": "As for language, we are interested in Runyankore, a Bantu language indigenous to south western Uganda. The highly agglutinative structure and complex verbal morphology of Runyankore make existing NLG systems based on templates inapplicable (Keet and Khumalo, 2017) . There have been efforts undertaken to apply the grammar engine technique instead (Byamugisha et al., 2016a;<cite> Byamugisha et al., 2016b</cite>; Byamugisha et al., 2016c) , which resulted in theoretical advances in verbalization rules for ontologies, pluralization of nouns, and verb conjugation that address the text generation needs for Runyankore. We present our implementation of these algorithms and required linguistic annotations as a Prot\u00e9g\u00e9 5.x plugin. also ensures no typographical errors are made in the XML file. These annotation fields are mandatory, and we allowed for the use of 0 as the NC for the POS which is not a noun. These restrictions to input were achieved using document filters.",
  "y": "background"
 },
 {
  "id": "7e52a90a9a0a703250d5c3c1890058_1",
  "x": "The highly agglutinative structure and complex verbal morphology of Runyankore make existing NLG systems based on templates inapplicable (Keet and Khumalo, 2017) . There have been efforts undertaken to apply the grammar engine technique instead (Byamugisha et al., 2016a;<cite> Byamugisha et al., 2016b</cite>; Byamugisha et al., 2016c) , which resulted in theoretical advances in verbalization rules for ontologies, pluralization of nouns, and verb conjugation that address the text generation needs for Runyankore. We present our implementation of these algorithms and required linguistic annotations as a Prot\u00e9g\u00e9 5.x plugin. also ensures no typographical errors are made in the XML file. These annotation fields are mandatory, and we allowed for the use of 0 as the NC for the POS which is not a noun. These restrictions to input were achieved using document filters. The XML file is queried during the verbalization process so as to obtain the required annotations that are needed for the algorithms.",
  "y": "uses"
 },
 {
  "id": "7e52a90a9a0a703250d5c3c1890058_2",
  "x": "The CFG specified in <cite>(Byamugisha et al., 2016b)</cite> was implemented using the CFG Java tool (Xu et al., 2011) . We used this tool for three main reasons: our grammar engine implementation was done in Java, so we wanted a Java tool as well; we wanted a small CFG implementation for reasonable performance; and their tool extended Purdom's algorithm to fulfill Context-Dependent Rule Coverage (CDRC), which generates more and simpler sentences. A sample of the generated text is presented below: \u2022 Buri rupapura rwamakuru n'ekihandiiko ekishohoziibwe, (generated from: Newspaper Publication) \u2022 Buri ntaama nerya ebinyaansi byoona, (gener- The generated text is saved in a text file, which ensures that the text can be linked to other application scenarios. We are working on a better design to present the sentences within the tool, for interaction during multi-modal ontology development.",
  "y": "background"
 },
 {
  "id": "7e52a90a9a0a703250d5c3c1890058_3",
  "x": "These restrictions to input were achieved using document filters. The XML file is queried during the verbalization process so as to obtain the required annotations that are needed for the algorithms. ---------------------------------- **IMPLEMENTATION OF THE GRAMMAR ENGINE** We implemented the algorithms for verbalization and pluralization presented in (Byamugisha et al., 2016a; Byamugisha et al., 2016c ) as a Java application. The CFG specified in <cite>(Byamugisha et al., 2016b)</cite> was implemented using the CFG Java tool (Xu et al., 2011) . We used this tool for three main reasons: our grammar engine implementation was done in Java, so we wanted a Java tool as well; we wanted a small CFG implementation for reasonable performance; and their tool extended Purdom's algorithm to fulfill Context-Dependent Rule Coverage (CDRC), which generates more and simpler sentences.",
  "y": "uses"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_0",
  "x": "Work in factoid question-answering (Q/A) has long leveraged answer type detection (ATD) systems in order to identify the semantic class of the entities, words, or phrases which are most likely to correspond to the exact answer to a natural language question. For example, given a question like Who was responsible for coordinating disaster relief for victims of Hurricane Katrina?, ATD components enable Q/A systems to retrieve the sets of candidate answers which include the INDIVIDUALs and/or ORGANIZATIONs who provided aid to the victims of the hurricane. Early work in ATD (Harabagiu et al., 2000; Harabagiu et al., 2001 ) leveraged sets of heuristics in order to identify the expected answer types (EATs) of questions submitted to a Q/A system. Most modern Q/A systems, however, follow work done by<cite> (Li and Roth, 2002)</cite> in using machine learning classifiers in order to select the one (or more) EATs from a fixed hierarchy of answer types which are most appropriate for a particular question. While learning-based approaches have dramatically increased the precision of open-domain ATD systems, most current ATD components have only been tasked with distinguishing amongst a limited set of EATs. For example, the most commonly-used answer type hierarchy (ATH), the University of Illinois (UIUC) answer type hierarchy created by<cite> (Li and Roth, 2002)</cite> , includes only a total of 50 unique expected answer types (generally referred to as \"fine\" answer types), organized into 6 different categories (referred to as \"coarse\" answer types). (The entire UIUC ATH is presented in Table 1 .) While ATD systems based on the the UIUC hierarchy has been employed by a number of participants in recent TREC 1 Question-Answering evaluations, we believe that the size (and coverage) of current answer type hierarchies represents a factor which significantly limits both the performance of open-domain factoid question-answering systems and the number of questions that a Q/A system can be used to answer.",
  "y": "background"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_1",
  "x": "While learning-based approaches have dramatically increased the precision of open-domain ATD systems, most current ATD components have only been tasked with distinguishing amongst a limited set of EATs. For example, the most commonly-used answer type hierarchy (ATH), the University of Illinois (UIUC) answer type hierarchy created by<cite> (Li and Roth, 2002)</cite> , includes only a total of 50 unique expected answer types (generally referred to as \"fine\" answer types), organized into 6 different categories (referred to as \"coarse\" answer types). (The entire UIUC ATH is presented in Table 1 .) While ATD systems based on the the UIUC hierarchy has been employed by a number of participants in recent TREC 1 Question-Answering evaluations, we believe that the size (and coverage) of current answer type hierarchies represents a factor which significantly limits both the performance of open-domain factoid question-answering systems and the number of questions that a Q/A system can be used to answer. Without coverage for a specific answer type within its ATH, a Q/A system must retrieve - and extract -answers using other answer types from its ATH which are conceptually \"nearest\" to the expected answer type of the question. For example, an ATH without a unique EAT for ORGANIZATIONs may be forced to search for answers using similar types such as PERSON or NA-TIONALITY. In a similar fashion, limited ATHs can also increase the challenge of extracting exact answers from retrieved text passages. For example, while most ATD systems can identify when a question is seeking a INDIVID-UAL as its EAT, a more articulated ATD system capable of distinguish between classes of individuals (e.g. CEO, BASEBALL-PLAYER, POLITICIAN, RELIGIOUS LEADER) could greatly reduce the total number of (spurious) candidate answers retrieved for the question.",
  "y": "background"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_2",
  "x": "Answer type hierarchies were first employed by (Harabagiu et al., 2000) using a heuristic classifier based on the WORDNET (Miller, 1995) ontology. (Li and Roth, 2002) explored the use of machine learning techniques to answer type detection and (Krishnan et al., 2005) improved accuracy through the use of their informer span. Alternatively, (Pinchak and Lin, 2006 ) use a probabilistic model with no pre-defined hierarchy in order to identify the type of information sought by a factoid question. We know of no previous work which combines the ability to scale to large ATHs and provide the benefits of a machinelearning based system. While the ATH in (Harabagiu et al., 2000) could easily be scaled to include a potentially very large number of types (e.g. see (Harabagiu et al., 2005) for an example of how this could be accomplished for a top-performing TREC Q/A system), it is constrained in its aapproach to WORDNET's hand-built hypernym relations, which does not coincide with common answer types in questions. in contrast, the<cite> (Li and Roth, 2002)</cite> UIUC ATH is designed especially for questions, but lacks the ability to extend the depth of the hierarchy when Q/A systems are capable of handling more detailed answer types. (See Section 3 for a more detailed discussion of the UIUC hierarchy.) In order to create a new ATH capable of addressing the needs of today's open-domain factoid Q/A systems, we wanted to develop an ATH which was capable of scaling to any given set of named entity types.",
  "y": "differences"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_3",
  "x": "**CREATING A LARGE ANSWER TYPE HIERARCHY FOR FACTOID Q/A** In this section, we describe how we created a large, multitiered hierarchy of over 200 expected answer types from the set of entity types recognized by LCC's CICEROLITE named entity recognition system. We assume that the taxonomy of answer types included in an answer type hierarchy (ATH) can be represented as a tree structure of varying depth where branchings mark decision points between different semantic classes that all share a hypernym-like relationship with their parent. (A graphical representation of a portion of the LCC ATH is presented in Figure 1 .) The UIUC answer type hierarchy<cite> (Li and Roth, 2002)</cite> We feel that the time is right for work in ATD to move beyond the UIUC ATH and to begin to tackle problems of organizing and learning answer type hierarchies that encompass several hundreds of diverse expected answer types. We believe that this effort would be in line with recent work looking at a similar type of semantic categorization problem -named entity recognition -in which researchers have moved from using simple heuristics and classifiers to unsupervised or semi-supervised methods capable of inducing hundreds (if not thousands) of entity types from large collections of texts. In our work, we used output from LCC's own, widecoverage named entity recognition system, CICEROLITE in order to construct a novel ATH which included more than 200 different EATs. (A table listing Furthermore, answers need not necessarily be drawn from entities.",
  "y": "background"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_4",
  "x": "---------------------------------- **ANNOTATING THE QUESTION CORPUS** In this section, we describe how we used the large ATH introduced in Section 3 in order to annotate a corpus drawn from more than 10,000 questions compiled from (1) existing annotated question corpora<cite> (Li and Roth, 2002)</cite> , (2) collections of questions mined from the web, and (3) questions submitted to LCC's FERRET question-answering system (Hickl et al., 2006a) . (A breakdown of the number of questions obtained from each of these three strategies is provided in Table 3 Table 3 : Distribution of 10,000 annotated questions by originating data set. ---------------------------------- **METHODOLOGY** We used a custom graphical user interface (GUI) in order to facilitate annotation of questions with their EATs.",
  "y": "similarities uses"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_5",
  "x": "3 classifiers are machine-learning based while the remainder are heuristic classifiers. In a departure from previous machine-learning based approaches<cite> (Li and Roth, 2002</cite>; Krishnan et al., 2005) , we used a maximum entropy classifier to learn our ATH. Our classification process currently uses three machinelearned classifiers. The first resolves all questions into one of 11 \"coarse\" types that are similar to the UIUC coarse types in Table 1 . If the first classifier's outcome is HU-MAN, then a machine classifier resolves between INDIVID-UAL, HUMAN-GROUP, ORGANIZATION, and HUMAN (not enough information). If the first classifier's outcome is LO-CATION, then a machine classifier is used to resolve between PHYSICAL-LOCATION, GPE, FACILITY, and LO-CATION (again, when not enough information is present to make a decision). At every other decision point in the hierarchy, a heuristic classifier uses features extracted from the question to make a decision. Both experiments were trained on 8,000 and tested on 2,000 of the questions from the annotated set of questions from Section 4.",
  "y": "similarities uses"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_6",
  "x": "Furthermore, both ATHs discussed in this paper are designed for TREC questions. The set of TREC questions and their corresponding expected answer types by no means represents a true open-domain set. Indeed, the ability to adapt the hierarchy described in Section 3 to include alternate domains such as biological and aeronautical questions means there is no functional limit on the number of answer types. ---------------------------------- **CONCLUSIONS** This paper described the creation of a new answer type detection system capable of recognizing more than 200 different expected answer types with greater 85% precision. In a departure from previous work in answer type detection (Krishnan et al., 2005;<cite> Li and Roth, 2002)</cite> , we have demonstrated how a large, multi-tiered answer type hierarchy can be created which incorporates many of the entity types included in LCC's wide coverage named entity recognition system, CICEROLITE; this hierarchy was then used in order to create a new corpus of more than 10,000 questions which could be used to train an ATD system.",
  "y": "similarities"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_0",
  "x": "Unsupervised speech representation learning [2, 3, 4, 5, 6,<cite> 7,</cite> 8, 9, 10] is effective in extracting high-level properties from speech. SLP downstream tasks can be improved through speech representations because surface features such as log Mel-spectrograms or waveform can poorly reveal the abundant information within speech. Contrastive Predictive Coding (CPC) [5] and wav2vec <cite>[7]</cite> use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task. Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss. Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6,<cite> 7]</cite> . However, this constraint on model architectures limits the potential of speech representation learning. The recently proposed vq-wav2vec [8] approach attempts to apply the well-performing Natural Language Processing (NLP) algorithm BERT [12] on continuous speech.",
  "y": "background"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_1",
  "x": "The proposed framework is illustrated in Figure 1 . ---------------------------------- **RELATED WORK** Unsupervised speech representation learning [2, 3, 4, 5, 6,<cite> 7,</cite> 8, 9, 10] is effective in extracting high-level properties from speech. SLP downstream tasks can be improved through speech representations because surface features such as log Mel-spectrograms or waveform can poorly reveal the abundant information within speech. Contrastive Predictive Coding (CPC) [5] and wav2vec <cite>[7]</cite> use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task. Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss.",
  "y": "background"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_2",
  "x": "Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6,<cite> 7]</cite> . However, this constraint on model architectures limits the potential of speech representation learning. The recently proposed vq-wav2vec [8] approach attempts to apply the well-performing Natural Language Processing (NLP) algorithm BERT [12] on continuous speech. Input speech is discretized to a K-way quantized embedding space, so continuous speech could act like discrete units similar to word tokens in NLP tasks. In vq-wav2vec [8] , an exhaustive two-stage training pipeline with massive computing resources are required to adapt speech to NLP algorithm, as the quantization process is against the continuous nature of speech. Unlike [8] that adapts speech to BERT [12] through quantization, the proposed approach can be seen as a modified version of BERT [12] for direct application on continuous speech. ----------------------------------",
  "y": "background"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_3",
  "x": "Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6,<cite> 7]</cite> . However, this constraint on model architectures limits the potential of speech representation learning. The recently proposed vq-wav2vec [8] approach attempts to apply the well-performing Natural Language Processing (NLP) algorithm BERT [12] on continuous speech. Input speech is discretized to a K-way quantized embedding space, so continuous speech could act like discrete units similar to word tokens in NLP tasks. In vq-wav2vec [8] , an exhaustive two-stage training pipeline with massive computing resources are required to adapt speech to NLP algorithm, as the quantization process is against the continuous nature of speech. Unlike [8] that adapts speech to BERT [12] through quantization, the proposed approach can be seen as a modified version of BERT [12] for direct application on continuous speech. ----------------------------------",
  "y": "motivation background"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_4",
  "x": "In vq-wav2vec [8] , an exhaustive two-stage training pipeline with massive computing resources are required to adapt speech to NLP algorithm, as the quantization process is against the continuous nature of speech. Unlike [8] that adapts speech to BERT [12] through quantization, the proposed approach can be seen as a modified version of BERT [12] for direct application on continuous speech. ---------------------------------- **PROPOSED METHOD** Unlike previous left-to-right unidirectional approaches that only consider past sequences to predict information about future frames, the proposed method allows us to train a bidirectional speech representation model, alleviating the unidirectionality constraint of previous methods. As a result, the Mockingjay model obtains substantial improvements in several SLP tasks. Moreover, as previous approaches restrict the power of the pre-trained models to representation extraction only [5, 6,<cite> 7,</cite> 8] , the proposed method is robust and can be fine-tuned easily on downstream tasks.",
  "y": "differences"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_5",
  "x": "We train with a batch size of 6 using a single 1080Ti GPU. We provide pre-trained models with our implementation, they are publicly available for reproducibility 2 . ---------------------------------- **EXPERIMENT** Following previous works [2, 3, 4, 5, 6,<cite> 7,</cite> 8] , we evaluate different features and representations on downstream tasks, including: phoneme classification, speaker recognition, and sentiment classification on spoken content. For a fair comparison, each downstream task uses an identical model architecture and hyperparameters despite different input features. We report results from 5 of our models: 1) BASE and 2) LARGE where Mockingjay representations are extracted from the last encoder layer, 3) the BASE-FT2 where we finetune BASE with random initialized downstream models for 2 epochs, and 4) the BASE-FT500 where we fine-tune for 500k steps, and finally 5) the LARGE-WS where we incorporate hidden states from all encoder layers of the LARGE model through a learnable weighted sum.",
  "y": "uses"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_6",
  "x": "Empirically we found that even with supervised training, a random initialized Mockingjay model followed by any downstream model is hard to be trained from scratch. This shows that the proposed pre-training is essentially indispensable. ---------------------------------- **COMPARING WITH OTHER REPRESENTATIONS** The proposed approaches are mainly compared with APC [6] representations, as they also experiment on phone classification and speaker verification. As reported in [6] , the APC approach outperformed CPC representations [5,<cite> 7,</cite> 9] in both two tasks, which makes APC suitable as a strong baseline. APC uses an unidirectional autoregressive model.",
  "y": "differences"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_0",
  "x": "As much as 23 % of the time more than one speaker is active at the same time [12] . The challenge's baseline system poor performance (about 80 % WER) is an indication that ASR training did not work well. Recently, Guided Source Separation (GSS) enhancement on the test data was shown to significantly improve the performance of an acoustic model, which had been trained with a large amount of unprocessed and simulated noisy data <cite>[13]</cite> . GSS is a spatial mixture model based blind source separation approach which exploits the annotation given in the CHiME-5 database for initialization and, in this way, avoids the frequency permutation problem [14] . We conjectured that cleaning up the training data would enable a more effective acoustic model training for the CHiME-5 scenario. We have therefore experimented with enhancement algorithms of various strengths, from relatively simple beamforming over singlearray GSS to a quite sophisticated multi-array GSS approach, and tested all combinations of training and test data enhancement methods. Furthermore, compared to the initial GSS approach in [14] , we describe here some modifications, which led to improved performance.",
  "y": "background"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_1",
  "x": "We also propose an improved neural acoustic modeling structure compared to the CHiME-5 baseline system described in [10] . It consists of initial Convolutional Neural Network (CNN) layers followed by factorized TDNN (TDNN-F) layers, instead of a homogeneous TDNN-F architecture. Using a single acoustic model trained with 308 hrs of training data, which resulted after applying multi-array GSS data cleaning and a three-fold speed perturbation, we achieved a WER of 41.6 % on the development (DEV) and 43.2 % on the evaluation (EVAL) test set of CHiME-5, if the test data is also enhanced with multi-array GSS. This compares very favorably with the recently published topline in <cite>[13]</cite> , where the single-system best result, i.e., the WER without system combination, was 45.1 % and 47.3 % on DEV and EVAL, respectively, using an augmented training data set of 4500 hrs total. The rest of this paper is structured as follows. Section 2 describes the CHiME-5 corpus, Section 3 briefly presents the guided source separation enhancement method, Section 4 shows the ASR experiments and the results, followed by a discussion in Section 5. Finally, the paper is concluded in Section 6.",
  "y": "differences"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_2",
  "x": "Given a mixture of reverberated overlapped speech, GSS aims to separate the sources using a pure signal processing approach. An Expectation Maximization (EM) algorithm estimates the parameters of a spatial mixture model and the posterior probabilities of each speaker being active are used for mask based beamforming. An overview block diagram of this enhancement by source separation is depicted in Fig. 1 . It follows the approach presented in <cite>[13]</cite> , which was shown to outperform the baseline version. The system operates in the STFT domain and consists of two stages: (1) a dereverberation stage, and (2) a guided source separation stage. For the sake of simplicity, the overall system is referred to as GSS for the rest of the paper. Regarding the first stage, the multiple input multiple output version of the Weighted Prediction Error (WPE) method was used for dereverberation (M inputs and M outputs) [15, 16] five mixture components, one representing each speaker, and an additional component representing the noise class.",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_3",
  "x": "However, having such a large temporal context may become problematic when the speakers are moving, because the estimated spatial covariance matrix can become outdated due to the movement <cite>[13]</cite> . Alternatively, one can run the EM first with a larger temporal context until convergence, then drop the context and re-run it for some more iterations. As shown later in the paper, this approach did not improve ASR performance. Therefore, the temporal context was only used for dereverberation and the mixture model parameter estimation, while for the estimation of covariance matrices for beamforming the context was dropped and only the original segment length was considered <cite>[13]</cite> . Another avenue we have explored for further source separation improvement was to refine the baseline CHiME-5 annotations using ASR output (see Fig. 1 ). A first-pass decoding using an ASR system is used to predict silence intervals. Then this information is used to adjust the time annotations, which are used in the EM algorithm as described above.",
  "y": "motivation"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_4",
  "x": "Therefore, the temporal context was only used for dereverberation and the mixture model parameter estimation, while for the estimation of covariance matrices for beamforming the context was dropped and only the original segment length was considered <cite>[13]</cite> . Another avenue we have explored for further source separation improvement was to refine the baseline CHiME-5 annotations using ASR output (see Fig. 1 ). A first-pass decoding using an ASR system is used to predict silence intervals. Then this information is used to adjust the time annotations, which are used in the EM algorithm as described above. When the ASR decoder indicates silence for a speaker, the corresponding class posterior in the MM is forced to zero. Depending on the number of available arrays for CHiME-5, two flavours of GSS enhancement were used in this work. In the single array track, all 4 channels of the array are used as input (M = 4), and the system is referred to as GSS1.",
  "y": "extends uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_5",
  "x": "In Table 3 , in each row the recognition accuracy improves monotonically from left to right, i.e., as the enhancement strategy on the test data becomes stronger. Reading the table in each column from top to bottom, one observes that accuracy improves with increasing power of the enhancement on the training data, however, only as long as the enhancement on the training data is not stronger than on the test data. Compared with unprocessed training and test data (None-None), GSS6-GSS6 yields roughly 35 % (24 %) relative WER reduction on the DEV (EVAL) set, and 12 % (11 %) relative WER reduction when compared with the None-GSS6 scenario. Comparing the amount of training data used to train the acoustic models, we observe that it decreases drastically from no enhancement to the GSS6 enhancement. ---------------------------------- **STATE-OF-THE-ART SINGLE-SYSTEM FOR CHIME-5** To facilitate comparison with the recently published top-line in <cite>[13]</cite> (H/UPB), we have conducted a more focused set of experiments whose results are depicted in Table 4 .",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_6",
  "x": "To facilitate comparison with the recently published top-line in <cite>[13]</cite> (H/UPB), we have conducted a more focused set of experiments whose results are depicted in Table 4 . As explained in Section 5.1, we opted for <cite>[13]</cite> instead of [14] as baseline because the former system is stronger. The experiments include refining the GSS enhancement using time annotations from ASR output (GSS w/ ASR), performing discriminative training on top of the AMs trained with LF-MMI and performing RNN LM rescoring. All the above helped further improve ASR performance. We report performance of our system on both single and multiple array tracks. To have a fair comparison, the results are compared with the single-system performance reported in <cite>[13]</cite> . For the single array track, the proposed system without RNN LM rescoring achieves 16 % (11 %) relative WER reduction on the DEV (EVAL) set when compared with System8 in <cite>[13]</cite> (row one in Table 4 ).",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_7",
  "x": "**STATE-OF-THE-ART SINGLE-SYSTEM FOR CHIME-5** To facilitate comparison with the recently published top-line in <cite>[13]</cite> (H/UPB), we have conducted a more focused set of experiments whose results are depicted in Table 4 . As explained in Section 5.1, we opted for <cite>[13]</cite> instead of [14] as baseline because the former system is stronger. The experiments include refining the GSS enhancement using time annotations from ASR output (GSS w/ ASR), performing discriminative training on top of the AMs trained with LF-MMI and performing RNN LM rescoring. All the above helped further improve ASR performance. We report performance of our system on both single and multiple array tracks. To have a fair comparison, the results are compared with the single-system performance reported in <cite>[13]</cite> .",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_8",
  "x": "For the single array track, the proposed system without RNN LM rescoring achieves 16 % (11 %) relative WER reduction on the DEV (EVAL) set when compared with System8 in <cite>[13]</cite> (row one in Table 4 ). RNN LM rescoring further helps improve the proposed system performance. For the multi array track, the proposed system without RNN LM rescoring achieved 6 % (7 %) relative WER reduction on the DEV (EVAL) set when compared with System16 in <cite>[13]</cite> (row six in Table 4 ). We also performed a test using GSS with the oracle alignments (GSS w/ oracle) to assess the potential of time annotation refinement (gray shade lines in Table 4 ). It can be seen that there is some, however not much room for improvement. Finally, cleaning up the training set not only boosted the recognition performance, but managed to do so using a fraction of the training data in <cite>[13]</cite> , as shown in Table 5 . This translates to significantly faster and cheaper training of acoustic models, which is a major advantage in practice.",
  "y": "differences"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_9",
  "x": "The experiments include refining the GSS enhancement using time annotations from ASR output (GSS w/ ASR), performing discriminative training on top of the AMs trained with LF-MMI and performing RNN LM rescoring. All the above helped further improve ASR performance. We report performance of our system on both single and multiple array tracks. To have a fair comparison, the results are compared with the single-system performance reported in <cite>[13]</cite> . For the single array track, the proposed system without RNN LM rescoring achieves 16 % (11 %) relative WER reduction on the DEV (EVAL) set when compared with System8 in <cite>[13]</cite> (row one in Table 4 ). RNN LM rescoring further helps improve the proposed system performance. For the multi array track, the proposed system without RNN LM rescoring achieved 6 % (7 %) relative WER reduction on the DEV (EVAL) set when compared with System16 in <cite>[13]</cite> (row six in Table 4 ).",
  "y": "differences"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_10",
  "x": "We also performed a test using GSS with the oracle alignments (GSS w/ oracle) to assess the potential of time annotation refinement (gray shade lines in Table 4 ). It can be seen that there is some, however not much room for improvement. Finally, cleaning up the training set not only boosted the recognition performance, but managed to do so using a fraction of the training data in <cite>[13]</cite> , as shown in Table 5 . This translates to significantly faster and cheaper training of acoustic models, which is a major advantage in practice. ---------------------------------- **DISCUSSION** ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_11",
  "x": "Two cases are investigated: (i) partially dropping the temporal context for the EM stage, and (ii) dropping the temporal context for beamforming. The evaluation was conducted with an acoustic model trained on unprocessed speech and the enhancement was applied during test only. Results are depicted in Table 6 . The first row corresponds to the GSS configuration in [14] while the second one corresponds to the GSS configuration in <cite>[13]</cite> . First two rows show that dropping the temporal context for estimating statistics for beamforming improves ASR accuracy. For the last row, the EM algorithm was run 20 iterations with temporal context, followed by another 10 without context. Since the performance de- [14] w/ context 54.7 (52.3) 20 w/ context <cite>[13]</cite> w/o context 51.8 (51.6) 20 w/ + 10 w/o context w/o context 52.",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_13",
  "x": "Results are depicted in Table 6 . The first row corresponds to the GSS configuration in [14] while the second one corresponds to the GSS configuration in <cite>[13]</cite> . First two rows show that dropping the temporal context for estimating statistics for beamforming improves ASR accuracy. For the last row, the EM algorithm was run 20 iterations with temporal context, followed by another 10 without context. Since the performance de- [14] w/ context 54.7 (52.3) 20 w/ context <cite>[13]</cite> w/o context 51.8 (51.6) 20 w/ + 10 w/o context w/o context 52. 2 (52.5) creased, we concluded that the best configuration for the GSS enhancement in CHiME-5 scenario is using full temporal context for the EM stage and dropping it for the beamforming stage. Consequently, we have chosen system <cite>[13]</cite> as baseline in this study since is using the stronger GSS configuration.",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_0",
  "x": "Multiword expressions (MWEs) are combinations of multiple words that exhibit some degree of idiomaticity (Baldwin and Kim, 2010) . Verb-noun combinations (VNCs), consisting of a verb with a noun in its direct object position, are a common type of semantically-idiomatic MWE in English and cross-lingually (Fazly et al., 2009 ). Many VNCs are ambiguous between MWEs and literal combinations, as in the following examples of see stars, in which 1 is an idiomatic usage (i.e., an MWE), while 2 is a literal combination. 1 1. Hereford United were seeing stars at Gillingham after letting in 2 early goals 2. Look into the night sky to see the stars MWE identification is the task of automatically determining which word combinations at the token-level form MWEs (Baldwin and Kim, 2010) , and must be able to make such distinctions. This is particularly important for applications such as machine translation (Sag et al., 2002) , where the appropriate meaning of word combinations in context must be preserved for accurate translation. In this paper, following prior work (e.g., <cite>Salton et al., 2016</cite> ), we frame token-level identification of VNCs as a supervised binary classification problem, i.e., idiomatic vs. literal. We consider a range of approaches to forming distributed representations of the context in which a VNC occurs, including word embeddings (Mikolov et al., 2013) , word embeddings tailored to representing sentences (Kenter et al., 2016) , and skip-thoughts sentence embeddings (Kiros et al., 2015) .",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_1",
  "x": "We consider a range of approaches to forming distributed representations of the context in which a VNC occurs, including word embeddings (Mikolov et al., 2013) , word embeddings tailored to representing sentences (Kenter et al., 2016) , and skip-thoughts sentence embeddings (Kiros et al., 2015) . We then train a support vector machine (SVM) on these representations to classify unseen VNC instances. Surprisingly, we find that an approach based on representing sentences as the average of their word embeddings performs comparably to, or better than, the skip-thoughts based approach previously proposed by <cite>Salton et al. (2016)</cite> . VNCs exhibit lexico-syntactic fixedness. For example, the idiomatic interpretation in example 1 above is typically only accessible when the verb see has active voice, the determiner is null, and the noun star is in plural form, as in see stars or seeing stars. Usages with a determiner (as in example 2), a singular noun (e.g., see a star), or passive voice (e.g., stars were seen) typically only have the literal interpretation. In this paper we further incorporate knowledge of the lexico-syntactic fixedness of VNCs -automatically acquired from corpora using the method of Fazly et al. (2009) -into our various embedding-based approaches.",
  "y": "differences"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_2",
  "x": "Our experimental results show that this leads to substantial improve-ments, indicating that this rich linguistic knowledge is complementary to that available in distributed representations. ---------------------------------- **RELATED WORK** Much research on MWE identification has focused on specific kinds of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005) , including English VNCs (e.g., Fazly et al., 2009; <cite>Salton et al., 2016</cite>) , although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017) . Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., Fazly et al., 2009; Fothergill and Baldwin, 2012) , treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008) , incorporated topic models (e.g., Li et al., 2010) , and made use of distributed representations of words (Gharbieh et al., 2016) . In the most closely related work to ours, <cite>Salton et al. (2016)</cite> represent token instances of VNCs by embedding the sentence that they occur in using skip-thoughts (Kiros et al., 2015) -an encoderdecoder model that can be viewed as a sentencelevel counterpart to the word2vec (Mikolov et al., 2013 ) skip-gram model. During training the target sentence is encoded using a recurrent neural network, and is used to predict the previous and next sentences.",
  "y": "background"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_3",
  "x": "Our experimental results show that this leads to substantial improve-ments, indicating that this rich linguistic knowledge is complementary to that available in distributed representations. ---------------------------------- **RELATED WORK** Much research on MWE identification has focused on specific kinds of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005) , including English VNCs (e.g., Fazly et al., 2009; <cite>Salton et al., 2016</cite>) , although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017) . Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., Fazly et al., 2009; Fothergill and Baldwin, 2012) , treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008) , incorporated topic models (e.g., Li et al., 2010) , and made use of distributed representations of words (Gharbieh et al., 2016) . In the most closely related work to ours, <cite>Salton et al. (2016)</cite> represent token instances of VNCs by embedding the sentence that they occur in using skip-thoughts (Kiros et al., 2015) -an encoderdecoder model that can be viewed as a sentencelevel counterpart to the word2vec (Mikolov et al., 2013 ) skip-gram model. During training the target sentence is encoded using a recurrent neural network, and is used to predict the previous and next sentences.",
  "y": "background"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_4",
  "x": "<cite>Salton et al.</cite> then use these sentence embeddings, representing VNC token instances, as features in a supervised classifier. We treat this skip-thoughts based approach as a strong baseline to compare against. Fazly et al. (2009) formed a set of eleven lexicosyntactic patterns for VNC instances capturing the voice of the verb (active or passive), determiner (e.g., a, the), and number of the noun (singular or plural). They then determine the canonical form, C(v, n), for a given VNC as follows: 2 where P is the set of patterns, T z is a predetermined threshold, which is set to 1, and z(v, n, pt k ) is calculated as follows: where f (\u00b7) is the frequency of a VNC occurring in a given pattern in a corpus, 3 and f and s are the mean and standard deviations for all patterns for the given VNC, respectively. Fazly et al. (2009) showed that idiomatic usages of a VNC tend to occur in that expression's canonical form, while literal usages do not.",
  "y": "background"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_5",
  "x": "<cite>Salton et al.</cite> then use these sentence embeddings, representing VNC token instances, as features in a supervised classifier. We treat this skip-thoughts based approach as a strong baseline to compare against. Fazly et al. (2009) formed a set of eleven lexicosyntactic patterns for VNC instances capturing the voice of the verb (active or passive), determiner (e.g., a, the), and number of the noun (singular or plural). They then determine the canonical form, C(v, n), for a given VNC as follows: 2 where P is the set of patterns, T z is a predetermined threshold, which is set to 1, and z(v, n, pt k ) is calculated as follows: where f (\u00b7) is the frequency of a VNC occurring in a given pattern in a corpus, 3 and f and s are the mean and standard deviations for all patterns for the given VNC, respectively. Fazly et al. (2009) showed that idiomatic usages of a VNC tend to occur in that expression's canonical form, while literal usages do not.",
  "y": "motivation"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_6",
  "x": "We use a Siamese CBOW model that was pretrained on a snapshot of Wikipedia from November 2012 using randomly initialized word embeddings. 5 Similarly to the word2vec model, to embed a given sentence containing a VNC instance, we average the word embeddings for each word in the sentence. ---------------------------------- **SKIP-THOUGHTS** We use a publicly-available skip-thoughts model, that was pre-trained on a corpus of books. 6 We represent a given sentence containing a VNC instance using the skip-thoughts encoder. Note that this approach is our re-implementation of the skipthoughts based method of <cite>Salton et al. (2016)</cite> , and we use it as a strong baseline for comparison.",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_7",
  "x": "Note that this approach is our re-implementation of the skipthoughts based method of <cite>Salton et al. (2016)</cite> , and we use it as a strong baseline for comparison. ---------------------------------- **DATA AND EVALUATION** In this section, we discuss the dataset used in our experiments, and the evaluation of our models. ---------------------------------- **DATASET** We use the VNC-Tokens dataset (Cook et al., 2008) -the same dataset used by Fazly et al. (2009) and <cite>Salton et al. (2016)</cite> -to train and evaluate our models.",
  "y": "similarities uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_8",
  "x": "Following <cite>Salton et al. (2016)</cite> , we use DEV and TEST, and ignore all token instances annotated as \"unknown\". Fazly et al. (2009) and <cite>Salton et al. (2016)</cite> structured their experiments differently. Fazly et al. report results over DEV and TEST separately. In this setup TEST consists of expressions that were not seen during model development (done on DEV). <cite>Salton et al.</cite>, on the other hand, merge DEV and TEST, and create new training and testing sets, such that each expression is present in the training and testing data, and the ratio of idiomatic to literal usages of each expression in the training data is roughly equal to that in the testing data. We borrowed ideas from both of these approaches in structuring our experiments. We retain We then divide each of these into training and testing sets, using the same ratios of idiomatic to literal usages for each expression as <cite>Salton et al. (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_9",
  "x": "Fazly et al. (2009) and <cite>Salton et al. (2016)</cite> structured their experiments differently. Fazly et al. report results over DEV and TEST separately. In this setup TEST consists of expressions that were not seen during model development (done on DEV). <cite>Salton et al.</cite>, on the other hand, merge DEV and TEST, and create new training and testing sets, such that each expression is present in the training and testing data, and the ratio of idiomatic to literal usages of each expression in the training data is roughly equal to that in the testing data. We borrowed ideas from both of these approaches in structuring our experiments. We retain We then divide each of these into training and testing sets, using the same ratios of idiomatic to literal usages for each expression as <cite>Salton et al. (2016)</cite> . This allows us to develop and tune a model on DEV, and then determine whether, when retrained on instances of unseen VNCs in (the training portion of) TEST, that model is able to generalize to new VNCs without further tuning to the specific expressions in TEST.",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_10",
  "x": "Following <cite>Salton et al. (2016)</cite> , we use DEV and TEST, and ignore all token instances annotated as \"unknown\". Fazly et al. (2009) and <cite>Salton et al. (2016)</cite> structured their experiments differently. Fazly et al. report results over DEV and TEST separately. In this setup TEST consists of expressions that were not seen during model development (done on DEV). <cite>Salton et al.</cite>, on the other hand, merge DEV and TEST, and create new training and testing sets, such that each expression is present in the training and testing data, and the ratio of idiomatic to literal usages of each expression in the training data is roughly equal to that in the testing data. We borrowed ideas from both of these approaches in structuring our experiments. We retain We then divide each of these into training and testing sets, using the same ratios of idiomatic to literal usages for each expression as <cite>Salton et al. (2016)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_11",
  "x": "We therefore use accuracy to evaluate our models following Fazly et al. (2009) because the classes are roughly balanced. We randomly divide both DEV and TEST into training and testing portions ten times, following <cite>Salton et al. (2016)</cite> . For each of the ten runs, we compute the accuracy for each expression, and then compute the average accuracy over the expressions. We then report the average accuracy over the ten runs. ---------------------------------- **EXPERIMENTAL RESULTS** In this section we first consider the effect of tuning the cost parameter of the SVM for each model on DEV, and then report results on DEV and TEST using the tuned models.",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_12",
  "x": "\u2212CF corresponds to the models as described in Section 3. +CF further incorporates lexico-syntactic knowledge of canonical forms into each model by concatenating the embedding representing each VNC token instance with a one-dimensional vector which is one if the VNC occurs in its canonical form, and zero otherwise. We first consider results for the \u2212CF setup. On both DEV and TEST, the accuracy achieved by each supervised model is higher than that of the unsupervised CForm approach, except for Siamese CBOW on TEST. The word2vec model achieves the highest accuracy on DEV and TEST of 0.830 and 0.804, respectively. The difference between the word2vec model and the next-best model, skip-thoughts, is significant using a bootstrap test (Berg-Kirkpatrick et al., 2012) with 10k repetitions for DEV (p = 0.006), but not for TEST (p = 0.051). Nevertheless, it is remarkable that the relatively simple approach to averaging word embeddings used by word2vec performs as well as, or better than, the much more complex skipthoughts model used by <cite>Salton et al. (2016)</cite> . 8 8 The word2vec and skip-thoughts models were trained on different corpora, which could contribute to the differences in results for these models.",
  "y": "differences"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_13",
  "x": "canonical form feature itself performs relatively poorly on literal usages, it provides information that enables the word2vec model to better identify literal usages. ---------------------------------- **CONCLUSIONS** Determining whether a usage of a VNC is idiomatic or literal is important for applications such as machine translation, where it is vital to preserve the meanings of word combinations. In this paper we proposed two approaches to the task of classifying VNC token instances as idiomatic or literal based on word2vec embeddings and Siamese CBOW. We compared these approaches against a linguistically-informed unsupervised baseline, and a model based on skip-thoughts previously applied to this task (<cite>Salton et al., 2016</cite>) . Our experimental results show that a comparatively simple approach based on averaging word embeddings performs at least as well as, or better than, the approach based on skip-thoughts.",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_0",
  "x": "While the problem of cue phrase classi cation has often been noted (Grosz & Sidner 1986; Halliday & Hassan 1976; Reichman 1985; Schi rin 1987; Zuckerman & Pearl 1986) , it has generally not received careful study. Recently, however, <cite>Hirschberg and Litman (1993)</cite> have presented rules for classifying cue phrases in both text and speech. <cite>Hirschberg and Litman</cite> pre-classi ed a set of naturally occurring cue phrases, described each cue phrase in terms of prosodic and textual features, then manually examined the data to construct rules that best predicted the classi cations from the features. This paper examines the utility of machine learning for automating the construction of rules for classifying cue phrases. A set of experiments are conducted that use two machine learning programs, cgrendel (Cohen 1992; 1993) and C4.5 (Quinlan 1986; , to induce classi cation rules from sets of preclassi ed cue phrases and their features. To support a quantitative and comparative evaluation of the automated and manual approaches, both the error rates and the content of the manually derived and learned rulesets are compared. The experimental results show that machine learning is indeed an e ective technique for automating the generation of classi cation rules.",
  "y": "motivation background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_1",
  "x": "The accuracy of the learned rulesets is often higher than the accuracy of the rules in <cite>(Hirschberg & Litman 1993)</cite> , while the linguistic implications are more precise. ---------------------------------- **CUE PHRASE CLASSI CATION** This section summarizes <cite>Hirschberg and Litman's study</cite> of the classi cation of multiple cue phrases in text and speech <cite>(Hirschberg & Litman 1993)</cite> . The data from <cite>this study</cite> is used to create the input for the machine learning experiments, while the results are used as a benchmark for evaluating performance. The corpus examined was a technical address by a single speaker, lasting 75 minutes and consisting of approximately 12,500 words. The corpus yielded 953 instances of 34 di erent single word cue phrases.",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_3",
  "x": "---------------------------------- **CUE PHRASE CLASSI CATION** This section summarizes <cite>Hirschberg and Litman's study</cite> of the classi cation of multiple cue phrases in text and speech <cite>(Hirschberg & Litman 1993)</cite> . The data from <cite>this study</cite> is used to create the input for the machine learning experiments, while the results are used as a benchmark for evaluating performance. The corpus examined was a technical address by a single speaker, lasting 75 minutes and consisting of approximately 12,500 words. The corpus yielded 953 instances of 34 di erent single word cue phrases. <cite>Hirschberg and Litman</cite> each classi ed the 953 tokens (as discourse, sentential or ambiguous) while listening to a recording and reading a transcription.",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_4",
  "x": "The data from <cite>this study</cite> is used to create the input for the machine learning experiments, while the results are used as a benchmark for evaluating performance. The corpus examined was a technical address by a single speaker, lasting 75 minutes and consisting of approximately 12,500 words. The corpus yielded 953 instances of 34 di erent single word cue phrases. <cite>Hirschberg and Litman</cite> each classi ed the 953 tokens (as discourse, sentential or ambiguous) while listening to a recording and reading a transcription. Each token was also described as a set of prosodic and textual features. Previous observations in the literature correlating discourse structure with prosodic information, and discourse usages of cue phrases with initial position in a clause, contributed to the choice of features. The prosody of the corpus was described using Pierrehumbert's theory of English intonation (Pierrehumbert 1980) .",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_5",
  "x": "To produce the F0 contour, the recording of the corpus was digitized and pitchtracked using speech analysis software. This resulted in a display of the F0 where the x-axis represented time and the y-axis represented frequency in Hz. Various phrase nal characteristics (e.g., phrase accents, boundary tones, as well as pauses and syllable lengthening) helped to identify intermediate and intonational phrases, while peaks or valleys in the display of the F0 contour helped to identify pitch accents. In <cite>(Hirschberg & Litman 1993)</cite> , every cue phrase was described using the following prosodic features. Accent corresponded to the pitch accent (if any) that was associated with the token. For both the intonational and intermediate phrases containing each token, the feature composition of phrase represented whether or not the token was alone in the phrase (the phrase contained only the token, or only cue phrases). Position in phrase represented whether the token was rst (the rst lexical item in the phrase { possibly preceded by other cue phrases), the last item in the phrase, or other.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_6",
  "x": "The set of classi ed and described tokens was used to evaluate the accuracy of the classi cation models shown in Figure 1 , developed in earlier studies. The prosodic model resulted from a study of 48 \\now\"s produced by multiple speakers in a radio callin show (Hirschberg & Litman 1987) . In a procedure similar to that described above, <cite>Hirschberg and Litman</cite> rst classi ed and described each of the 48 tokens. <cite>They</cite> then examined their data manually to develop the prosodic model, which correctly classi ed all of the 48 tokens. (When later tested on 52 new examples of \\now\" from the radio corpus, the model also performed nearly perfectly). The model uniquely classi es any cue phrase using the features composition of Figure 1 : Decision tree representation of the classi cation models of (<cite>Hirschberg and Litman 1993</cite> Figure 1 ), or in a larger intermediate phrase with an initial position (possibly preceded by other cue phrases) and a L* accent or deaccented, it is classi ed as discourse. When part of a larger intermediate phrase and either in initial position with a H* or complex accent, or in a non-initial position, it is sentential.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_7",
  "x": "Every cue phrase was also described in terms of the following textual features, derived directly from the transcript using fully automated methods. The part of speech of each token was obtained by running a program for tagging words with one of approximately 80 parts of speech on the transcript (Church 1988) . Several characteristics of the token's immediate context were also noted, in particular, whether the token was immediately preceded or succeeded by orthography (punctuation or a paragraph boundary), and whether the token was immediately preceded or succeeded by a lexical item corresponding to a cue phrase. The set of classi ed and described tokens was used to evaluate the accuracy of the classi cation models shown in Figure 1 , developed in earlier studies. The prosodic model resulted from a study of 48 \\now\"s produced by multiple speakers in a radio callin show (Hirschberg & Litman 1987) . In a procedure similar to that described above, <cite>Hirschberg and Litman</cite> rst classi ed and described each of the 48 tokens. <cite>They</cite> then examined their data manually to develop the prosodic model, which correctly classi ed all of the 48 tokens.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_8",
  "x": "In a procedure similar to that described above, <cite>Hirschberg and Litman</cite> rst classi ed and described each of the 48 tokens. <cite>They</cite> then examined their data manually to develop the prosodic model, which correctly classi ed all of the 48 tokens. (When later tested on 52 new examples of \\now\" from the radio corpus, the model also performed nearly perfectly). The model uniquely classi es any cue phrase using the features composition of Figure 1 : Decision tree representation of the classi cation models of (<cite>Hirschberg and Litman 1993</cite> Figure 1 ), or in a larger intermediate phrase with an initial position (possibly preceded by other cue phrases) and a L* accent or deaccented, it is classi ed as discourse. When part of a larger intermediate phrase and either in initial position with a H* or complex accent, or in a non-initial position, it is sentential. The textual model was also manually developed, and was based on an examination of the rst 17 minutes of the single speaker technical address (Litman & Hirschberg 1990) ; the model correctly classi ed 89.4% of these 133 tokens. When a cue phrase is preceded by any type of orthography it is classi ed as discourse, otherwise as sentential.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_9",
  "x": "The model uniquely classi es any cue phrase using the features composition of Figure 1 : Decision tree representation of the classi cation models of (<cite>Hirschberg and Litman 1993</cite> Figure 1 ), or in a larger intermediate phrase with an initial position (possibly preceded by other cue phrases) and a L* accent or deaccented, it is classi ed as discourse. When part of a larger intermediate phrase and either in initial position with a H* or complex accent, or in a non-initial position, it is sentential. The textual model was also manually developed, and was based on an examination of the rst 17 minutes of the single speaker technical address (Litman & Hirschberg 1990) ; the model correctly classi ed 89.4% of these 133 tokens. When a cue phrase is preceded by any type of orthography it is classi ed as discourse, otherwise as sentential. The models were evaluated by quantifying their performance in correctly classifying two subsets of the 953 tokens from the corpus. The rst subset (878 examples) consisted of only the classi able tokens, i.e., the tokens that both <cite>Hirschberg and Litman</cite> classi ed as discourse or that both classi ed as sentential. The second subset, the classi able non-conjuncts (495 examples), was created from the classi able tokens by removing all examples of \\and\", \\or\" and \\but\".",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_10",
  "x": "This subset was considered particularly reliable since 97.2% of non-conjuncts were classi able compared to 92.1% of all tokens. The error rate of the prosodic model was 24.6% for the classi able tokens and 14.7% for the classi able non-conjuncts. The error rate of the textual model was 19.1% for the classi able tokens and 16.1% for the classi able non-conjuncts. In contrast, a model which just predicts the most frequent class in the corpus (sentential) has an error rate of 39% and 41% for the classi able tokens and the classi able nonconjuncts, respectively. ---------------------------------- **EXPERIMENTS USING MACHINE INDUCTION** This section describes experiments that use the machine learning programs C4.5 (Quinlan 1986; and cgrendel (Cohen 1992; 1993) to automatically induce cue phrase classi cation rules from both the data of <cite>(Hirschberg & Litman 1993)</cite> and an extension of this data.",
  "y": "extends uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_11",
  "x": "The output of each program is a set of classi cation rules, expressed in C4.5 as a decision tree and in cgrendel as an ordered set of if-then rules. Both cgrendel and C4.5 learn the classi cation rules using greedy search guided by an \\information gain\" metric. The rst set of experiments does not distinguish among the 34 cue phrases. In each experiment, a different subset of the features coded in <cite>(Hirschberg & Litman 1993 )</cite> is examined. The experiments consider every feature in isolation (to comparatively evaluate the utility of each individual knowledge source for classi cation), as well as linguistically motivated sets of features (to gain insight into the interactions between the knowledge sources). The second set of experiments treats cue phrases individually. This is done by adding a lexical feature representing the cue phrase to each feature set from the rst set of experiments.",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_12",
  "x": "Both cgrendel and C4.5 learn the classi cation rules using greedy search guided by an \\information gain\" metric. The rst set of experiments does not distinguish among the 34 cue phrases. In each experiment, a different subset of the features coded in <cite>(Hirschberg & Litman 1993 )</cite> is examined. The experiments consider every feature in isolation (to comparatively evaluate the utility of each individual knowledge source for classi cation), as well as linguistically motivated sets of features (to gain insight into the interactions between the knowledge sources). The second set of experiments treats cue phrases individually. This is done by adding a lexical feature representing the cue phrase to each feature set from the rst set of experiments. The potential use of such a lexical feature was noted but not used in <cite>(Hirschberg & Litman 1993)</cite> .",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_13",
  "x": "The potential use of such a lexical feature was noted but not used in <cite>(Hirschberg & Litman 1993)</cite> . These experiments evaluate the utility of developing classi cation models specialized for particular cue phrases, and also provide qualitatively new linguistic insights into the data. The rst input to each learning program de nes the classes and features. The classi cations produced <cite>by Hirschberg and by Litman</cite> (discourse, sentential, and ambiguous) are combined into a single classi cation for each cue phrase. A cue phrase is classi ed as discourse (or as sentential) if both <cite>Hirschberg and Litman</cite> agreed upon the classi cation discourse (or upon sentential). A cue phrase is non-classi able if at least one of <cite>Hirschberg and/or Litman</cite> classi ed the token as ambiguous, or one classi ed it as discourse while the other classi ed it as sentential. The features considered in the learning experiments are shown in Figure 2 .",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_14",
  "x": "This is done by adding a lexical feature representing the cue phrase to each feature set from the rst set of experiments. The potential use of such a lexical feature was noted but not used in <cite>(Hirschberg & Litman 1993)</cite> . These experiments evaluate the utility of developing classi cation models specialized for particular cue phrases, and also provide qualitatively new linguistic insights into the data. The rst input to each learning program de nes the classes and features. The classi cations produced <cite>by Hirschberg and by Litman</cite> (discourse, sentential, and ambiguous) are combined into a single classi cation for each cue phrase. A cue phrase is classi ed as discourse (or as sentential) if both <cite>Hirschberg and Litman</cite> agreed upon the classi cation discourse (or upon sentential). A cue phrase is non-classi able if at least one of <cite>Hirschberg and/or Litman</cite> classi ed the token as ambiguous, or one classi ed it as discourse while the other classi ed it as sentential.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_15",
  "x": "This is done by adding a lexical feature representing the cue phrase to each feature set from the rst set of experiments. The potential use of such a lexical feature was noted but not used in <cite>(Hirschberg & Litman 1993)</cite> . These experiments evaluate the utility of developing classi cation models specialized for particular cue phrases, and also provide qualitatively new linguistic insights into the data. The rst input to each learning program de nes the classes and features. The classi cations produced <cite>by Hirschberg and by Litman</cite> (discourse, sentential, and ambiguous) are combined into a single classi cation for each cue phrase. A cue phrase is classi ed as discourse (or as sentential) if both <cite>Hirschberg and Litman</cite> agreed upon the classi cation discourse (or upon sentential). A cue phrase is non-classi able if at least one of <cite>Hirschberg and/or Litman</cite> classi ed the token as ambiguous, or one classi ed it as discourse while the other classi ed it as sentential.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_16",
  "x": "Feature values can either be a numeric value or one of a xed set of user-de ned symbolic values. The feature representation shown here follows the representation of <cite>(Hirschberg & Litman 1993 )</cite> except as noted. Length of phrase (P-L and I-L) represents the number of words in the phrase. This feature was not coded in the data from which the prosodic model was developed, but was coded (although not used) in the later data of <cite>(Hirschberg & Litman 1993)</cite> . Position in phrase (P-P and I-P) uses numeric rather than symbolic values. The conjunction of the rst two values for I-C is equivalent to alone in Figure 1 . Ambiguous, the last value of A, is assigned when the prosodic anal- ysis of <cite>(Hirschberg & Litman 1993 )</cite> is a disjunction (e.g., \\H*+L or H*\").",
  "y": "similarities"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_17",
  "x": "This feature was not coded in the data from which the prosodic model was developed, but was coded (although not used) in the later data of <cite>(Hirschberg & Litman 1993)</cite> . Position in phrase (P-P and I-P) uses numeric rather than symbolic values. The conjunction of the rst two values for I-C is equivalent to alone in Figure 1 . Ambiguous, the last value of A, is assigned when the prosodic anal- ysis of <cite>(Hirschberg & Litman 1993 )</cite> is a disjunction (e.g., \\H*+L or H*\"). NA (not applicable) in the textual features re ects the fact that 39 recorded examples were not included in the transcription, which was done independently of <cite>(Hirschberg & Litman 1993)</cite> . While the original representation noted the actual token (e.g., \\and\") when there was a preceding or succeeding cue phrase, here the value true encodes all such cases. Similarly, A*, O-P*, and O-S* re-represent the symbolic values of three features using a more abstract level of description (e.g., L*+H, L+H*, H*+L, and H+L* are represented as separate values in A but as a single value { the superclass complex { in A*).",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_18",
  "x": "The conjunction of the rst two values for I-C is equivalent to alone in Figure 1 . Ambiguous, the last value of A, is assigned when the prosodic anal- ysis of <cite>(Hirschberg & Litman 1993 )</cite> is a disjunction (e.g., \\H*+L or H*\"). NA (not applicable) in the textual features re ects the fact that 39 recorded examples were not included in the transcription, which was done independently of <cite>(Hirschberg & Litman 1993)</cite> . While the original representation noted the actual token (e.g., \\and\") when there was a preceding or succeeding cue phrase, here the value true encodes all such cases. Similarly, A*, O-P*, and O-S* re-represent the symbolic values of three features using a more abstract level of description (e.g., L*+H, L+H*, H*+L, and H+L* are represented as separate values in A but as a single value { the superclass complex { in A*). Finally, the lexical feature token is new to this study, and represents the actual cue phrase being described. The second input to each learning program is training data, i.e., a set of examples for which the class and feature values are speci ed. Consider the following utterance, taken from the corpus of <cite>(Hirschberg & Litman 1993)</cite>: Example 1 (Now) (now that we have all been welcomed here)] it's time to get on with the business of the conference.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_19",
  "x": "Length of phrase (P-L and I-L) represents the number of words in the phrase. This feature was not coded in the data from which the prosodic model was developed, but was coded (although not used) in the later data of <cite>(Hirschberg & Litman 1993)</cite> . Position in phrase (P-P and I-P) uses numeric rather than symbolic values. The conjunction of the rst two values for I-C is equivalent to alone in Figure 1 . Ambiguous, the last value of A, is assigned when the prosodic anal- ysis of <cite>(Hirschberg & Litman 1993 )</cite> is a disjunction (e.g., \\H*+L or H*\"). NA (not applicable) in the textual features re ects the fact that 39 recorded examples were not included in the transcription, which was done independently of <cite>(Hirschberg & Litman 1993)</cite> . While the original representation noted the actual token (e.g., \\and\") when there was a preceding or succeeding cue phrase, here the value true encodes all such cases.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_20",
  "x": "Ambiguous, the last value of A, is assigned when the prosodic anal- ysis of <cite>(Hirschberg & Litman 1993 )</cite> is a disjunction (e.g., \\H*+L or H*\"). NA (not applicable) in the textual features re ects the fact that 39 recorded examples were not included in the transcription, which was done independently of <cite>(Hirschberg & Litman 1993)</cite> . While the original representation noted the actual token (e.g., \\and\") when there was a preceding or succeeding cue phrase, here the value true encodes all such cases. Similarly, A*, O-P*, and O-S* re-represent the symbolic values of three features using a more abstract level of description (e.g., L*+H, L+H*, H*+L, and H+L* are represented as separate values in A but as a single value { the superclass complex { in A*). Finally, the lexical feature token is new to this study, and represents the actual cue phrase being described. The second input to each learning program is training data, i.e., a set of examples for which the class and feature values are speci ed. Consider the following utterance, taken from the corpus of <cite>(Hirschberg & Litman 1993)</cite>: Example 1 (Now) (now that we have all been welcomed here)] it's time to get on with the business of the conference. This utterance contains two cue phrases, corresponding to the two instances of \\now\".",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_21",
  "x": "These sets correspond to the two subsets of the corpus examined in <cite>(Hirschberg & Litman 1993 )</cite> { the classi able tokens, and the classi able non-conjuncts. ---------------------------------- **RESULTS** This section examines the results of running the learning programs C4.5 and cgrendel on 112 sets of examples (56 feature sets x 2 sets of examples). The results are qualitatively examined by comparing the linguistic content of the learned rulesets with the rules of Figure 1 . The results are quantitatively evaluated by comparing the error rate of the learned rulesets in classifying new examples to the error rate of the rules of Figure 1 . The error rate of a set of rules is computed by using the rules to predict the classi cation of a set of (pre-classi ed) examples, then comparing the predicted and known classi cations.",
  "y": "similarities"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_22",
  "x": "The error rate of a set of rules is computed by using the rules to predict the classi cation of a set of (pre-classi ed) examples, then comparing the predicted and known classi cations. In the cue phrase domain, the error rate is computed by summing the number of discourse examples misclassi ed as sentential with the number of sentential examples misclassi ed as discourse, then dividing by the total number of examples. Cross-validation (Weiss & Kulikowski 1991 ) is used to estimate the error rates of the learned rulesets. Instead of running each learning program once on each of the 112 sets of examples, 10 runs are performed, each using a random 90% of the examples for training (i.e., for learning the ruleset) and the remaining 10% for testing. An estimated error rate is obtained by averaging the error rate on the testing portion of the data from each of the 10 runs. Note that for each run, the training and testing examples are disjoint subsets of the same set of examples, and the training set is much larger than the test set. In contrast (as discussed above), the \\training\" and test sets for the intonational model of <cite>(Hirschberg & Litman 1993)</cite> were taken from di erent corpora, while for the textual model of <cite>(Hirschberg & Litman 1993 )</cite> the test set was a superset of the training set.",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_23",
  "x": "The top of the table considers prosodic features, the bottom textual features, and the bottom right prosodic/textual combinations. The error rates in italics indicate that the performance of the learned ruleset exceeds the performance reported in <cite>(Hirschberg & Litman 1993)</cite> , where the rules of Figure 1 were tested using 100% of the 878 classi able tokens. These error rates were 24.6% and 19.1% for the intonational and textual models, respectively. When considering only a single intonational feature (the rst 3 columns of the rst 7 rows), the results of the learning programs suggest that position in intonational phrase (P-P) is the most useful feature for cue phrase classi cation. In addition, this feature classi es cue phrases signi cantly better than the 3 feature prosodic model of Figure 1 . The majority of the learned multiple feature rulesets (columns 7-9) also perform better than the model of Figure 1 , although none signi cantly improve upon the single feature ruleset. Note that the performance of the manually derived model is better than the performance of hl93features (which uses the same set of features but in di erent rules).",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_24",
  "x": "(Recall that length was coded by <cite>Hirschberg and Litman</cite> only in <cite>their</cite> test data. Length was thus never used to generate or revise their prosodic model.) Both of the learned rulesets perform similarly to each other, and outperform the prosodic model of Figure 1 . Examination of the learned textual rulesets yields similar ndings. Consider the rulesets learned from O-P* (preceding orthography, where the particular type of orthography is not noted), shown towards the bottom of Figure 4 . These rules outperform the rules using the other single features, and perform comparably to the model in Figure 1 and to the multiple feature textual rulesets incorporating preceding orthography. Again, note the similarity to lines (9) and (10) Performance of a feature set is often improved when the additional feature token is taken into account (columns 4-6 and 10-12) . This phenomenon will be discussed below.",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_26",
  "x": "Table 3 presents the results using a smaller portion of the corpus, the 495 classi able non-conjuncts. The error rates of the intonational and textual models of Figure 1 on this subcorpus decrease to 14.7% and 16.1%, respectively <cite>(Hirschberg & Litman 1993)</cite> . Without the feature token, the single feature sets based on position and preceding orthography are again the best performers, and along with many multiple feature non-token sets, perform nearly as well as the models in Figure 1 . When the feature token is taken into account, however, the learned rulesets outperform the models of <cite>(Hirschberg & Litman 1993</cite> sider this feature), and also provide new insights into cue phrase classi cation. Figure 5 shows the cgrendel ruleset learned from A+, which reduces the 30% error rate of A to 12%. The rst rule corresponds to line (5) of Figure 1 . In contrast to line (4), however, cgrendel uses deaccenting to predict discourse for only the tokens \\say\" and \\so.\" If the token is \\now\", \\ nally\", \\however\", or \\ok\", discourse is assigned (for all accents).",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_27",
  "x": "Rulesets such as these suggest that even though features such as accent may not characterize all cue phrases, they may nonetheless be used successfully if the feature is used di erently for di erent cue phrases or subsets of cue phrases. Note that in the subcorpus of non-conjuncts (in contrast to the classi able subcorpus), machine learning only improves on human performance by considering more features, either the extra feature token or textual and prosodic features in combination. This might re ect the fact that the manually derived theories already achieve optimal performance with respect to the examined features in this less noisy subcorpus, and/or that the automatically derived theory for this subcorpus was based on a smaller training set than used in the previous subcorpus. Related Work in Discourse Analysis Grosz and Hirschberg (1992) used the system cart (Brieman et al. 1984) to construct decision trees for classifying aspects of discourse structure from intonational feature values. Siegel (in press) was the rst to apply machine learning to cue phrases. He developed a genetic learning algorithm to induce decision trees using the non-ambiguous examples of <cite>(Hirschberg & Litman 1993 )</cite> (using the classi cations of only one judge) as well as additional examples. Each example was described using a feature corresponding to token, as well as textual features containing the lexical or orthographic item immediately to the left of and in the 4 positions to the right of the example.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_28",
  "x": "Thus, new textual features were examined. Prosodic features were not investigated. Siegel reported a 21% estimated error rate, with half of the corpus used for training and half for testing. An examination of Table 2 shows that the error of the best C4.5 and cgrendel rulesets was often lower than 21% (even for theories which did not consider the token), as was the 19.1% error of the textual model of <cite>(Hirschberg & Litman 1993)</cite> . Siegel and McKeown (1994) have also proposed a method for developing linguistically viable rulesets, based on the partitioning of the training data produced during induction. ---------------------------------- **CONCLUSION**",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_29",
  "x": "Siegel reported a 21% estimated error rate, with half of the corpus used for training and half for testing. An examination of Table 2 shows that the error of the best C4.5 and cgrendel rulesets was often lower than 21% (even for theories which did not consider the token), as was the 19.1% error of the textual model of <cite>(Hirschberg & Litman 1993)</cite> . Siegel and McKeown (1994) have also proposed a method for developing linguistically viable rulesets, based on the partitioning of the training data produced during induction. ---------------------------------- **CONCLUSION** This paper has demonstrated the utility of machine learning techniques for cue phrase classi cation. A rst set of experiments were presented that used the programs cgrendel (Cohen 1992; 1993) and C4.5 (Quinlan 1986; to induce classi cation rules from the preclassi ed cue phrases and their features that were used as test data in <cite>(Hirschberg & Litman 1993)</cite> .",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_30",
  "x": "**CONCLUSION** This paper has demonstrated the utility of machine learning techniques for cue phrase classi cation. A rst set of experiments were presented that used the programs cgrendel (Cohen 1992; 1993) and C4.5 (Quinlan 1986; to induce classi cation rules from the preclassi ed cue phrases and their features that were used as test data in <cite>(Hirschberg & Litman 1993)</cite> . The results of these experiments suggest that machine learning is an e ective technique for not only automating the generation of linguistically plausible classi cation rules, but also for improving accuracy. In particular, a large number of learned rulesets (including P-P, an extremely simple one feature model) had signicantly lower error rates than the rulesets of <cite>(Hirschberg & Litman 1993)</cite> . One possible explanation is that the hand-built classi cation models were derived using very small \\training\" sets; as new data became available, this data was used for testing but not for updating the original models. In contrast, machine learning supported the building of rulesets using a much larger amount of the data for training.",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_31",
  "x": "The results of these experiments suggest that machine learning is an e ective technique for not only automating the generation of linguistically plausible classi cation rules, but also for improving accuracy. In particular, a large number of learned rulesets (including P-P, an extremely simple one feature model) had signicantly lower error rates than the rulesets of <cite>(Hirschberg & Litman 1993)</cite> . One possible explanation is that the hand-built classi cation models were derived using very small \\training\" sets; as new data became available, this data was used for testing but not for updating the original models. In contrast, machine learning supported the building of rulesets using a much larger amount of the data for training. Furthermore, if new data becomes available, it is trivial to regenerate the rulesets. For example, in a second set of experiments, new classi cation rules were induced using the feature token, which was not considered in <cite>(Hirschberg & Litman 1993)</cite> . Allowing the learning programs to treat cue phrases individually further improved the accuracy of the resulting rulesets, and added to the body of linguistic knowledge regarding cue phrases.",
  "y": "extends"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_0",
  "x": "We also extend previous work on surface n-gram features from Web1T to the Google Books corpus and from first-order to second-order, comparing and analysing performance over newswire and web treebanks. Surface and syntactic n-grams both produce substantial and complementary gains in parsing accuracy across domains. Our best system combines the two feature sets, achieving up to 0.8% absolute UAS improvements on newswire and 1.4% on web text. ---------------------------------- **INTRODUCTION** Current state-of-the-art parsers score over 90% on the standard newswire evaluation, but the remaining errors are difficult to overcome using only the training corpus. Features from n-gram counts over resources like Web1T (Brants and Franz, 2006 ) have proven to be useful proxies for syntax<cite> (Bansal and Klein, 2011</cite>; Pitler, 2012) , but they enforce linear word order, and are unable to distinguish between syntactic and non-syntactic co-occurrences.",
  "y": "motivation background"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_1",
  "x": "Current state-of-the-art parsers score over 90% on the standard newswire evaluation, but the remaining errors are difficult to overcome using only the training corpus. Features from n-gram counts over resources like Web1T (Brants and Franz, 2006 ) have proven to be useful proxies for syntax<cite> (Bansal and Klein, 2011</cite>; Pitler, 2012) , but they enforce linear word order, and are unable to distinguish between syntactic and non-syntactic co-occurrences. Longer n-grams are also noisier and sparser, limiting the range of potential features. In this paper we develop new features for the graph-based MSTParser (McDonald and Pereira, 2006) from the Google Syntactic Ngrams corpus (Goldberg and Orwant, 2013) , a collection of Stanford dependency subtree counts. These features capture information collated across millions of subtrees produced by a shift-reduce parser, trading off potential systemic parser errors for data that is better aligned with the parsing task. We compare the performance of our syntactic n-gram features against the surface n-gram features of<cite> Bansal and Klein (2011)</cite> in-domain on newswire and out-of-domain on the English Web Treebank (Petrov and McDonald, 2012) across CoNLL-style (LTH) dependencies. We also extend the first-order surface n-gram features to second-order, and compare the utility of Web1T and the Google Books Ngram corpus (Lin et al., 2012) as surface n-gram sources.",
  "y": "uses"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_2",
  "x": "Given a head-argument ambiguity, we extract different combinations of word, POS tag, and directionality, and search the Syntactic Ngrams corpus for matching subtrees. To reduce the impact of this search during run time, we extract all possible combinations in the training and test corpora ahead of time and total the frequencies of each configuration, storing these in a lookup table that is used by the parser at run-time to compute feature values. We did not use any features based on the dependency label as these are assigned in a separate pass in MSTParser. Table 1 summarizes the first-order features extracted from the dependency hold \u2192 hearing depicted in Figure 1 . The final feature encodes the POS tags of the head and argument, directionality, the binned distance between the head and argument, and a bucketed frequency of the syntactic n-gram calculated as per Equation 1, creating bucket labels from 0 in increments of 5 (0, 5, 10, etc.) . Additional features for each bucket value up to the maximum are also encoded. We also develop paraphrase-style features like those of<cite> Bansal and Klein (2011)</cite> based on the most frequently occurring words and POS tags before, in between, and after each head-argument ambiguity (see Section 3.2).",
  "y": "similarities uses"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_3",
  "x": "We extract all triple and sibling word and POS structures considered by the parser in the training and test corpora (following the factorization depicted in Figure 2 ), and counted their frequency in the Syntactic Ngrams corpus. Importantly, we require that matching subtrees in the Syntactic Ngrams corpus maintain the position of the parent relative to its children. We generate separate features encoding the word and POS tag variants of each triple and sibling structure. Similar to the surface n-gram features (Section 3), counts for our syntactic n-gram features are precomputed to improve the run-time efficiency of the parser. Experiments on the development set led to a minimum cutoff frequency of 10,000 for each feature to avoid noise from parser and OCR errors. 3 Surface n-gram Features<cite> Bansal and Klein (2011)</cite> demonstrate that features generated from bucketing simple surface n-gram counts and collecting the top paraphrase-based contextual words over Web1T are useful for almost all attachment decisions, boosting dependency parsing accuracy by up to 0.6%. However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts.",
  "y": "background"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_4",
  "x": "Experiments on the development set led to a minimum cutoff frequency of 10,000 for each feature to avoid noise from parser and OCR errors. 3 Surface n-gram Features<cite> Bansal and Klein (2011)</cite> demonstrate that features generated from bucketing simple surface n-gram counts and collecting the top paraphrase-based contextual words over Web1T are useful for almost all attachment decisions, boosting dependency parsing accuracy by up to 0.6%. However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts. Bansal and Klein (2011) also tested only on in-domain text, though these external count features should be useful out of domain. We extract<cite> Bansal and Klein (2011)</cite> 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts. Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts. We also extend<cite> Bansal and</cite> Klein's affinity and paraphrase features to second-order.",
  "y": "motivation"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_5",
  "x": "Bansal and Klein (2011) also tested only on in-domain text, though these external count features should be useful out of domain. We extract<cite> Bansal and Klein (2011)</cite> 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts. Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts. We also extend<cite> Bansal and</cite> Klein's affinity and paraphrase features to second-order. ---------------------------------- **SURFACE N-GRAM CORPORA** The Web1T corpus contains counts of 1 to 5-grams over 1 trillion words of web text (Brants and Franz, 2006) .",
  "y": "uses"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_6",
  "x": "This corpus is affected by the accuracy of OCR and digitization tools; the changing typography of books across time is one issue that may create spurious cooccurrences and counts (Lin et al., 2012) . ---------------------------------- **FIRST-ORDER SURFACE N-GRAM FEATURES** Affinity features rely on the intuition that frequently co-occurring words in large unlabeled text collections are likely to be in a syntactic relationship (Nakov and Hearst, 2005;<cite> Bansal and Klein, 2011)</cite> . N-gram resources such as Web1T and Google Books provide large offline collections from which these co-occurrence statistics can be harvested; given each head and argument ambiguity in a training and test corpus, the corpora can be linearly scanned ahead of parsing time to reduce the impact of querying in the parser. When scanning, the head and argument word may appear immediately adjacent to one another in linear order (CONTIG), or with up to three intervening words (GAP1, GAP2, and GAP3) as the maximum n-gram length is five. The total count is then discretized as per Equation 1 previously.",
  "y": "background"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_7",
  "x": "In<cite> Bansal and Klein (2011)</cite> , paraphrase features are generated for all full-parse attachment ambiguities from the surface n-gram corpus. For each attachment ambiguity, 3-grams of the form ( q 1 q 2 ), (q 1 q 2 ), and (q 1 q 2 ) are extracted, where q 1 and q 2 are the head and argument in their linear order of appearance in the original sentence, and is any single context word appearing before, in between, or after the query words. Then the most frequent words appearing in each of these configurations for each head-argument ambiguity is encoded as a feature with the POS tags of the head and argument 2 . Given the arc hold \u2192 hearing in Figure 2 , public is the most frequent word appearing in the n-gram (hold hearing) in Web1T. Thus, the final encoded feature is POS (hold) \u2227 POS (hearing) \u2227 public \u2227 mid. Further generalization is achieved by using a unigram POS tagger trained on the WSJ data to tag each context word, and encoding features using each unique tag of the most frequent context words. ----------------------------------",
  "y": "background"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_8",
  "x": "We also extract paraphrase-style features for siblings in the same way as for first-order n-grams, and cumulative variants up to the maximum bucket size. ---------------------------------- **EXPERIMENTAL SETUP** As with<cite> Bansal and Klein (2011) and</cite> Pitler (2012) , we convert the Penn Treebank to dependencies using pennconverter 3 (Johansson and Nugues, 2007) (henceforth LTH) and generate POS tags with MX-POST (Ratnaparkhi, 1996) . We used sections 02-21 of the WSJ for training, 22 for development, and 23 for final testing. The test sections of the answers, newsgroups, and reviews sections of the English Web Treebank as per the SANCL 2012 Shared Task (Petrov and McDonald, 2012) were converted to LTH and used for out-of-domain evaluation. We used MSTParser (McDonald and Pereira, 2006) , trained with the parameters order:2, training-k:5, iters:10, and loss-type:nopunc.",
  "y": "uses"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_9",
  "x": "Resampling Web1T to reduce it to a comparable corpus that is the same size as Google Books would also provide better insights on how many n-grams are noise. ---------------------------------- **RELATED WORK** Surface n-gram counts from large web corpora have been used to address NP and PP attachment errors (Volk, 2001; Nakov and Hearst, 2005) Aside from<cite> Bansal and Klein (2011)</cite> , other feature-based approaches to improving dependency parsing include Pitler (2012) , who exploits Brown clusters and point-wise mutual information of surface n-gram counts to specifically address PP and coordination errors. Chen et al. (2013) describe a novel way of generating meta-features that work to emphasise important feature types used by the parser. Chen et al. (2009) generate subtree-based features that are similar to ours. However, they use the in-domain BLLIP newswire corpus to generate their subtree counts, whereas the Syntactic Ngrams corpus is out-of-domain and an order of magnitude larger.",
  "y": "background"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_10",
  "x": "We extract<cite> Bansal and Klein (2011)</cite> 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts. Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts. We also extend<cite> Bansal and</cite> Klein's affinity and paraphrase features to second-order. ---------------------------------- **SURFACE N-GRAM CORPORA** The Web1T corpus contains counts of 1 to 5-grams over 1 trillion words of web text (Brants and Franz, 2006) . Unigrams must appear at least 200 times in the source text before being included in the corpus, while longer n-grams have a cutoff of 40.",
  "y": "extends"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_11",
  "x": "We extract<cite> Bansal and Klein (2011)</cite> 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts. Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts. We also extend<cite> Bansal and</cite> Klein's affinity and paraphrase features to second-order. ---------------------------------- **SURFACE N-GRAM CORPORA** The Web1T corpus contains counts of 1 to 5-grams over 1 trillion words of web text (Brants and Franz, 2006) . Unigrams must appear at least 200 times in the source text before being included in the corpus, while longer n-grams have a cutoff of 40.",
  "y": "extends"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_0",
  "x": "Similarly, recent work on neural TTS has focused on adapting to new voices with limited reference data [13] [14] [15] [16] . Initial approaches to end-to-end speech-to-text translation (ST) [17, 18] performed worse than a cascade of an ASR model and an MT model. [19, 20] achieved better end-to-end performance by leveraging weakly supervised data with multitask learning. <cite>[21]</cite> further showed that use of synthetic training data can work better than multitask training. In this work we take advantage of both synthetic training targets and multitask training. The proposed model resembles recent sequence-to-sequence models for voice conversion, the task of recreating an utterance in another person's voice [22] [23] [24] . For example, [23] proposes an attention-based model to generate spectrograms in the target voice based on input features (spectrogram concatenated with ASR bottleneck features) from the source voice.",
  "y": "background"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_1",
  "x": "Following [15, 26] , it is composed of several separately trained components: 1) an attention-based sequence- to-sequence network (blue) which generates target spectrograms, 2) a vocoder (red) which converts target spectrograms to timedomain waveforms, and, 3) optionally, a pretrained speaker encoder (green) which can be used to condition the decoder on the identity of the source speaker, enabling cross-language voice conversion [27] simultaneously with translation. The sequence-to-sequence encoder stack maps 80-channel log-mel spectrogram input features into hidden states which are passed through an attention-based alignment mechanism to condition an autoregressive decoder, which predicts 1025-dim log spectrogram frames corresponding to the translated speech. Two optional auxiliary decoders, each with their own attention components, predict source and target phoneme sequences. Following recent speech translation <cite>[21]</cite> and recognition [28] models, the encoder is composed of a stack of 8 bidirectional LSTM layers. As shown in Fig. 1 , the final layer output is passed to the primary decoder, whereas intermediate activations are passed to auxiliary decoders predicting phoneme sequences. We hypothesize that early layers of the encoder are more likely to represent the source content well, while deeper layers might learn to encode more information about the target content. The spectrogram decoder uses an architecture similar to Tacotron 2 TTS model [26] , including pre-net, autoregressive LSTM stack, and post-net components.",
  "y": "similarities uses"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_2",
  "x": "We study two Spanish-to-English translation datasets: the large scale \"conversational\" corpus of parallel text and read speech pairs from <cite>[21]</cite> , and the Spanish Fisher corpus of telephone conversations and corresponding English translations [38] , which is smaller and more challenging due to the spontaneous and informal speaking style. In Sections 3.1 and 3.2, we synthesize target speech from the target transcript using a single (female) speaker English TTS system; In Section 3.4, we use real human target speech for voice transfer experiments on the conversational dataset. Models were implemented using the Lingvo framework [39] . See Table 1 for dataset-specific hyperparameters. To evaluate speech-to-speech translation performance we compute BLEU scores [40] as an objective measure of speech intelligibility and translation quality, by using a pretrained ASR system to recognize the generated speech, and comparing the resulting transcripts to ground truth reference translations. Due to potential recognition errors (see Figure 2) , this can be thought of as a lower bound on the underlying translation quality. We use the 16k Word-Piece attention-based ASR model from [41] trained on the 960 hour LibriSpeech corpus [42] , which obtained word error rates of 4.7% and 13.4% on the test-clean and testother sets, respectively.",
  "y": "similarities uses"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_3",
  "x": "We use the 16k Word-Piece attention-based ASR model from [41] trained on the 960 hour LibriSpeech corpus [42] , which obtained word error rates of 4.7% and 13.4% on the test-clean and testother sets, respectively. In addition, we conduct listening tests to measure subjective speech naturalness mean opinion score (MOS), as well as speaker similarity MOS for voice transfer. ---------------------------------- **CONVERSATIONAL SPANISH-TO-ENGLISH** This proprietary dataset described in <cite>[21]</cite> was obtained by crowdsourcing humans to read the both sides of a conversational Spanish-English MT dataset. In this section, instead of using the human target speech, we use a TTS model to synthesize target In addition, we augment the input source speech by adding background noise and reverberation in the same manner as <cite>[21]</cite> . The resulting dataset contains 979k parallel utterance pairs, containing 1.4k hours of source speech and 619 hours of synthesized target speech.",
  "y": "similarities uses"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_4",
  "x": "In addition, we conduct listening tests to measure subjective speech naturalness mean opinion score (MOS), as well as speaker similarity MOS for voice transfer. ---------------------------------- **CONVERSATIONAL SPANISH-TO-ENGLISH** This proprietary dataset described in <cite>[21]</cite> was obtained by crowdsourcing humans to read the both sides of a conversational Spanish-English MT dataset. In this section, instead of using the human target speech, we use a TTS model to synthesize target In addition, we augment the input source speech by adding background noise and reverberation in the same manner as <cite>[21]</cite> . The resulting dataset contains 979k parallel utterance pairs, containing 1.4k hours of source speech and 619 hours of synthesized target speech. The total target speech duration is much smaller because the TTS output is better endpointed, and contains fewer pauses.",
  "y": "extends differences"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_5",
  "x": "Input feature frames are created by stacking 3 adjacent frames of an 80-channel log-mel spectrogram as in <cite>[21]</cite> . The speaker encoder was not used in these experiments since the target speech always came from the same speaker. Table 2 shows performance of the model trained using different combinations of auxiliary losses, compared to a baseline ST \u2192 TTS cascade model using a speech-to-text translation model <cite>[21]</cite> trained on the same data, and the same Tacotron 2 TTS model used to synthesize training targets. Note that the ground truth BLEU score is below 100 due to ASR errors during evaluation, or TTS failure when synthesizing the ground truth. Training without auxiliary losses leads to extremely poor performance. The model correctly synthesizes common words and simple phrases, e.g. translating \"hola\" to \"hello\". However, it does not consistently translate full utterances.",
  "y": "similarities"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_6",
  "x": "In this section, instead of using the human target speech, we use a TTS model to synthesize target In addition, we augment the input source speech by adding background noise and reverberation in the same manner as <cite>[21]</cite> . The resulting dataset contains 979k parallel utterance pairs, containing 1.4k hours of source speech and 619 hours of synthesized target speech. The total target speech duration is much smaller because the TTS output is better endpointed, and contains fewer pauses. 9.6k pairs are held out for testing. Input feature frames are created by stacking 3 adjacent frames of an 80-channel log-mel spectrogram as in <cite>[21]</cite> . The speaker encoder was not used in these experiments since the target speech always came from the same speaker. Table 2 shows performance of the model trained using different combinations of auxiliary losses, compared to a baseline ST \u2192 TTS cascade model using a speech-to-text translation model <cite>[21]</cite> trained on the same data, and the same Tacotron 2 TTS model used to synthesize training targets.",
  "y": "similarities uses"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_7",
  "x": "Finally, as in <cite>[21]</cite> , we find that pretraining the bottom 6 encoder layers on an ST task improves BLEU scores by over 5 points. This is the best performing direct S2ST model, obtaining 76% of the baseline performance. ---------------------------------- **SUBJECTIVE EVALUATION OF SPEECH NATURALNESS** To evaluate synthesis quality of the best performing models from Tables 2 and 3 we use the framework from [15] to crowdsource 5-point MOS evaluations based on subjective listening tests. 1k examples were rated for each dataset, each one by a single rater. Although this evaluation is expected to be independent of the correctness of the translation, translation errors can result in low scores for examples raters describe as \"not understandable\".",
  "y": "similarities"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_8",
  "x": "We find that it is important to use speech transcripts during training, but no intermediate speech transcription is necessary for inference. Exploring alternate training strategies which alleviate this requirement is an interesting direction for future work. The model achieves high translation quality on two Spanish-to-English datasets, although performance is not as good as a baseline cascade of ST and TTS models. In addition, we demonstrate a variant which simultaneously transfers the source speaker's voice to the translated speech. The voice transfer does not work as well as in a similar TTS context [15] , reflecting the difficulty of the cross-language voice transfer task, as well as evaluation [44] . Potential strategies to improve voice transfer performance include improving the speaker encoder by adding a language adversarial loss, or by incorporating a cycle-consistency term [13] into the S2ST loss. Other future work includes utilizing weakly supervision to scale up training with synthetic data <cite>[21]</cite> or multitask learning [19, 20] , and transferring prosody and other acoustic factors from the source speech to the translated speech following [45] [46] [47] .",
  "y": "future_work"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_0",
  "x": "An alternative to DTW that has begun to be explored is the use of acoustic word embeddings (AWEs), or vector representations of spoken word segments. AWEs are representations that can be learned from data, ideally such that the embeddings of two segments corresponding to the same word are close, while embeddings of segments corresponding to different words are far apart. Once word segments are represented via fixed-dimensional embeddings, computing distances is as simple as measuring a cosine or Euclidean distance between two vectors. There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13,<cite> 14,</cite> 15, 16, 17] . In this paper we explore new embedding models based on recurrent neural networks (RNNs), applied to a word discrimination task related to query-by-example search. RNNs are a natural model class for acoustic word embeddings, since they can handle arbitrary-length sequences. We compare several types of RNN-based embeddings and analyze their properties.",
  "y": "background"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_1",
  "x": "This representation was subsequently used in several applications: a segmental approach for query-by-example search [13] , lexical clustering [19] , and unsupervised speech recognition [20] . Voinea et al. [16] developed a representation also based on templates, in their case phone templates, designed to be invariant to specific transformations, and showed their robustness on digit classification. Kamper et al.<cite> [14]</cite> compared several types of acoustic word embeddings for a word discrimination task related to query-by-example search, finding that embeddings based on convolutional neural networks (CNNs) trained with a contrastive loss outperformed the reference vector approach of Levin et al. [12] as well as several other CNN and DNN embeddings and DTW using several feature types. There have now been a number of approaches compared on this same task and data [12, 21, 22, 23] . For a direct comparison with this prior work, in this paper we use the same task and some of the same training losses as Kamper et al., but develop new embedding models based on RNNs. The only prior work of which we are aware using RNNs for acoustic word embeddings is that of Chen et al. [17] and Chung et al. [15] . Chen et al. learned a long short-term memory (LSTM) RNN for word classification and used the resulting hidden state vectors as a word embedding in a queryby-example task.",
  "y": "background"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_2",
  "x": "We train the RNN-based embedding models using a set of pre-segmented spoken words. We use two main training approaches, inspired by prior work but with some differences in the details. As in <cite>[14,</cite> 11] , our first approach is to use the word labels of the training segments and train the networks to classify the word. In this case, the final layer of g(X) is a log-softmax layer. Here we are limited to the subset of the training set that has a sufficient number of segments per word to train a good classifier, and the output dimensionality is equal to the number of words (but see<cite> [14]</cite> for a study of varying the dimensionality in such a classifier-based embedding model by introducing a bottleneck layer). This model is trained end-to-end and is optimized with a cross entropy loss. Although labeled data is necessarily limited, the hope is that the learned models will be useful even when applied to spoken examples of words not previously seen in the training data.",
  "y": "similarities uses"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_4",
  "x": "In this case, the final layer of g(X) is a log-softmax layer. Here we are limited to the subset of the training set that has a sufficient number of segments per word to train a good classifier, and the output dimensionality is equal to the number of words (but see<cite> [14]</cite> for a study of varying the dimensionality in such a classifier-based embedding model by introducing a bottleneck layer). This model is trained end-to-end and is optimized with a cross entropy loss. Although labeled data is necessarily limited, the hope is that the learned models will be useful even when applied to spoken examples of words not previously seen in the training data. For words not seen in training, the embeddings should correspond to some measure of similarity of the word to the training words, measured via the posterior probabilities of the previously seen words. In the experiments below, we examine this assumption by analyzing performance on words that appear in the training data compared to those that do not. The second training approach, based on earlier work of Kamper et al.<cite> [14]</cite> , is to train \"Siamese\" networks [31] .",
  "y": "uses"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_5",
  "x": "The acoustic features in each frame (the input to the word embedding models x t ) are 39-dimensional MFCCs+\u2206+\u2206\u2206. We use the same train, development, and test partitions as in prior work <cite>[14,</cite> 12] , and the same acoustic features as in<cite> [14]</cite> , for as direct a comparison as possible. The train set contains approximately 10k example segments, while dev and test each contain approximately 11k segments (corresponding to about 60M pairs for computing the dev/test AP). As in<cite> [14]</cite> , when training the classificationbased embeddings, we use a subset of the training set containing all word types with a minimum of 3 occurrences, reducing the training set size to approximately 9k segments. 1 When training the Siamese networks, the training data consists of all of the same-word pairs in the full training set (approximately 100k pairs). For each such training pair, we randomly sample a third example belonging to a different word type, as required for the l cos hinge loss. ---------------------------------- **CLASSIFICATION NETWORK DETAILS**",
  "y": "uses"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_6",
  "x": "We do this by measuring the cosine similarity between their acoustic word embeddings and declaring them to be the same if the distance is below a threshold. By sweeping the threshold, we obtain a precision-recall curve from which we compute the AP. The data used for this task is drawn from the Switchboard conversational English corpus [32] . The word segments range from 50 to 200 frames in length. The acoustic features in each frame (the input to the word embedding models x t ) are 39-dimensional MFCCs+\u2206+\u2206\u2206. We use the same train, development, and test partitions as in prior work <cite>[14,</cite> 12] , and the same acoustic features as in<cite> [14]</cite> , for as direct a comparison as possible. The train set contains approximately 10k example segments, while dev and test each contain approximately 11k segments (corresponding to about 60M pairs for computing the dev/test AP). As in<cite> [14]</cite> , when training the classificationbased embeddings, we use a subset of the training set containing all word types with a minimum of 3 occurrences, reducing the training set size to approximately 9k segments.",
  "y": "similarities uses"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_7",
  "x": "Then, for each of these B triplets (x a , x s , x d ) , an additional triplet (x s , x a , x d ) is added to the mini-batch to allow all segments to serve as anchors. This is a slight departure from earlier work<cite> [14]</cite> , which we found to improve stability in training and performance on the development set. In preliminary experiments, we compared two methods for choosing the negative examples x d during training, a uniform sampling approach and a non-uniform one. In the case of uniform sampling, we sample x d uniformly at random from the full set of training examples with labels different from x a . This sampling method requires only word-pair supervision. In the case of non-uniform sampling, x d is sampled in two steps. First, we construct a distribution P y|label(xa) over word labels y and sample a different label from it.",
  "y": "differences"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_10",
  "x": "However, performance still seems to be improving with additional layers, suggesting that we may be able to further improve performance by adding even more layers of either type. However, we fixed the model to S = F = 3 in order to allow for more experimentation and analysis within a reasonable time. Table 2 reveals an interesting trend. When only one fully connected layer is used, the GRU networks outperform the LSTMs given a sufficient number of stacked layers. On the other hand, once we add more fully connected layers, the LSTMs outperform the GRUs. In the first few lines of Table 2, we use 2, 3, and 4 layer stacks of LSTMs and GRUs while holding fixed the number of fully-connected layers at F = 1. There is clear utility in stacking additional layers; however, even with 4 stacked layers the RNNs still underperform the CNN-based embeddings of<cite> [14]</cite> until we begin adding fully connected layers.",
  "y": "differences"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_12",
  "x": "---------------------------------- **EFFECT OF EMBEDDING DIMENSIONALITY** For the Siamese networks, we varied the output embedding dimensionality, as shown in Fig. 2 . This analysis shows that the embeddings learned by the Siamese RNN network are quite robust to reduced dimensionality, outperforming the classifier model for all dimensionalities 32 or higher and outperforming previously reported dev set performance with CNN-based embeddings<cite> [14]</cite> for all dimensionalities \u2265 16. ---------------------------------- **EFFECT OF TRAINING VOCABULARY** We might expect the learned embeddings to be more accurate for words that are seen in training than for ones that are not.",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_0",
  "x": "****MEAN BOX POOLING: A RICH IMAGE REPRESENTATION AND OUTPUT EMBEDDING FOR THE <cite>VISUAL MADLIBS TASK</cite>**** **ABSTRACT** We present Mean Box Pooling, a novel visual representation that pools over CNN representations of a large number, highly overlapping object proposals. We show that such representation together with nCCA, a successful multimodal embedding technique, achieves state-of-the-art performance on the <cite>Visual Madlibs task</cite>. Moreover, inspired by the nCCA's objective function, we extend classical CNN+LSTM approach to train the network by directly maximizing the similarity between the internal representation of the deep learning architecture and candidate answers. Again, such approach achieves a significant improvement over the prior work that also uses CNN+LSTM approach on <cite>Visual Madlibs</cite>. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_1",
  "x": "**ABSTRACT** We present Mean Box Pooling, a novel visual representation that pools over CNN representations of a large number, highly overlapping object proposals. We show that such representation together with nCCA, a successful multimodal embedding technique, achieves state-of-the-art performance on the <cite>Visual Madlibs task</cite>. Moreover, inspired by the nCCA's objective function, we extend classical CNN+LSTM approach to train the network by directly maximizing the similarity between the internal representation of the deep learning architecture and candidate answers. Again, such approach achieves a significant improvement over the prior work that also uses CNN+LSTM approach on <cite>Visual Madlibs</cite>. ---------------------------------- **INTRODUCTION**",
  "y": "differences motivation"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_2",
  "x": "Arguably, it is also less prone to over-interpretations compared with the classical Turing Test [16, 25] . To foster progress on this task, a few metrics and datasets have been proposed [2, 4, 14, 20] . The recently introduced <cite>Visual Madlibs task</cite> <cite>[32]</cite> removes ambiguities in question or scene interpretations by introducing a multiple choice \"filling the blank\" task, where a c 2016. The copyright of <cite>this document</cite> resides with its authors. <cite>It</cite> may be distributed unchanged freely in print or electronic forms. ---------------------------------- **ARXIV:1608.02717V1 [CS.CV] 9 AUG 2016**",
  "y": "background"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_4",
  "x": "Thanks to such a problem formulation, a traditional accuracy measure can be used to monitor the progress on this task. Due to its unambiguous evaluation, this work focuses on this task. Contributions. We present two main contributions. Mean Box Pooling: We argue for a rich image representation in the form of pooled representations of the objects. Although related ideas have been explored for visual question answering [22] , and even have been used in <cite>Visual Madlibs</cite> <cite>[32]</cite> , we are first to show a significant improvement of such representation by using object proposals. More precisely, we argue for an approach that pools over a large number, highly overlapping object proposals.",
  "y": "differences motivation"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_5",
  "x": "Our approach in the combination with the Normalized Correlation Analysis embedding technique improves on the state-of-the-art of the <cite>Visual Madlibs task</cite>. Text-Embedding Loss: Motivated by the popularity of deep architectures for visual question answering, that combine a global CNN image representation with an LSTM [7] question representation [4, 13, 17, 20, 29, 30, 31] , as well as the leading performance of nCCA on the multi-choice <cite>Visual Madlibs task</cite> <cite>[32]</cite> , we propose a novel extension of the CNN+LSTM architecture that chooses a prompt completion out of four candidates (see Figure 4 ) by measuring similarities directly in the embedding space. This contrasts with the prior approach of <cite>[32]</cite> that uses a post-hoc comparison between the discrete output of the CNN+LSTM method and all four candidates. To achieve this, we directly train an LSTM with a cosine similarity loss between the output embedding of the network and language representation of the ground truth completion. Such an approach integrates more tightly with the multi-choice filling the blanks task, and significantly outperforms the prior CNN+LSTM method <cite>[32]</cite> . ---------------------------------- **RELATED WORK**",
  "y": "extends"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_6",
  "x": "Text-Embedding Loss: Motivated by the popularity of deep architectures for visual question answering, that combine a global CNN image representation with an LSTM [7] question representation [4, 13, 17, 20, 29, 30, 31] , as well as the leading performance of nCCA on the multi-choice <cite>Visual Madlibs task</cite> <cite>[32]</cite> , we propose a novel extension of the CNN+LSTM architecture that chooses a prompt completion out of four candidates (see Figure 4 ) by measuring similarities directly in the embedding space. This contrasts with the prior approach of <cite>[32]</cite> that uses a post-hoc comparison between the discrete output of the CNN+LSTM method and all four candidates. To achieve this, we directly train an LSTM with a cosine similarity loss between the output embedding of the network and language representation of the ground truth completion. Such an approach integrates more tightly with the multi-choice filling the blanks task, and significantly outperforms the prior CNN+LSTM method <cite>[32]</cite> . ---------------------------------- **RELATED WORK** Question answering about images is a relatively new task that switches focus from recognizing objects in the scene to a holistic \"image understanding\".",
  "y": "extends"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_7",
  "x": "Text-Embedding Loss: Motivated by the popularity of deep architectures for visual question answering, that combine a global CNN image representation with an LSTM [7] question representation [4, 13, 17, 20, 29, 30, 31] , as well as the leading performance of nCCA on the multi-choice <cite>Visual Madlibs task</cite> <cite>[32]</cite> , we propose a novel extension of the CNN+LSTM architecture that chooses a prompt completion out of four candidates (see Figure 4 ) by measuring similarities directly in the embedding space. This contrasts with the prior approach of <cite>[32]</cite> that uses a post-hoc comparison between the discrete output of the CNN+LSTM method and all four candidates. To achieve this, we directly train an LSTM with a cosine similarity loss between the output embedding of the network and language representation of the ground truth completion. Such an approach integrates more tightly with the multi-choice filling the blanks task, and significantly outperforms the prior CNN+LSTM method <cite>[32]</cite> . ---------------------------------- **RELATED WORK** Question answering about images is a relatively new task that switches focus from recognizing objects in the scene to a holistic \"image understanding\".",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_8",
  "x": "Our approach in the combination with the Normalized Correlation Analysis embedding technique improves on the state-of-the-art of the <cite>Visual Madlibs task</cite>. Text-Embedding Loss: Motivated by the popularity of deep architectures for visual question answering, that combine a global CNN image representation with an LSTM [7] question representation [4, 13, 17, 20, 29, 30, 31] , as well as the leading performance of nCCA on the multi-choice <cite>Visual Madlibs task</cite> <cite>[32]</cite> , we propose a novel extension of the CNN+LSTM architecture that chooses a prompt completion out of four candidates (see Figure 4 ) by measuring similarities directly in the embedding space. This contrasts with the prior approach of <cite>[32]</cite> that uses a post-hoc comparison between the discrete output of the CNN+LSTM method and all four candidates. To achieve this, we directly train an LSTM with a cosine similarity loss between the output embedding of the network and language representation of the ground truth completion. Such an approach integrates more tightly with the multi-choice filling the blanks task, and significantly outperforms the prior CNN+LSTM method <cite>[32]</cite> . ---------------------------------- **RELATED WORK**",
  "y": "extends"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_9",
  "x": "Since then different variants and larger datasets have been proposed: FM-IQA [4] , COCO-QA [20] , and VQA [2] . Although answering questions on images is, arguably, more susceptible to automatic evaluation than the image description task [3, 8, 27] , ambiguities in the output space still remain. While such ambiguities can be handled using appropriate metrics [14, 15, 17, 26] , <cite>Visual Madlibs</cite> <cite>[32]</cite> has taken another direction, and handles them directly within the task. <cite>It</cite> asks machines to fill the blank prompted with a natural language description with a phrase chosen from four candidate completions (Figure 4 ). In general, the phrase together with the prompted sentence should serve as the accurate description of the image. With such problem formulation the standard accuracy measure is sufficient to automatically evaluate the architectures. The first proposed architecture [14] to deal with the question answering about images task uses image analysis methods and a set of hand-defined schemas to create a database of visual facts.",
  "y": "background"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_10",
  "x": "Most recently, attention based architectures, which put weights on a fixed grid Figure 2 : Overview of our full model, i.e. our proposed image representation using Mean Box Pooling, text encoding using average of Word2Vec representations, and normalized CCA for learning the joint space. over the image, yield state of the art results [29, 30, 31] . Another, more focused \"hard\" attention, has also been studied in the image-to-text retrieval scenario [9] as well as fine-grained categorization [33] , person recognition [19] and zero-shot learning [1] . Here representations are computed on objects, visual fragments or parts, that are further aggregated to form a visual representation. Closer to our work, [22] use Edge Boxes [34] to form memories [28] consisting of different image fragments that are either pooled or \"softmax\" weighted in order to provide the final score. However, in contrast to [22] , our experiments indicate a strong improvement by using object proposals. While a majority of the most recent work on visual question answering combine LSTM [7] with CNN [11, 23, 24] by concatenation or summation or piece-wise multiplication, Canonical Correlation Analysis (CCA and nCCA) [6] have also been shown to be a very effective multimodal embedding technique <cite>[32]</cite> .",
  "y": "background"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_11",
  "x": "Furthermore, we also extend popular in the VQA community CNN+LSTM approach by learning to compare in the answer space. In Section 3.1, we propose a richer representation of the entire image obtained by pooling of CNN representations extracted from object proposals. Figure 1 illustrates the proposed Mean Box Pooling image representation and Figure 2 illustrates our whole method. In Section 3.3, we describe nCCA approach to encode two modalities into a joint space in greater details. In Section 3.4, we also investigate a CNN+LSTM architecture. Instead of generating a prompt completion that is next compared against candidate completions in a post-hoc process, we propose to choose a candidate completion by directly comparing candidates in the embedding space. This puts CNN+LSTM approach closer to nCCA with a tighter integration with the multi-choice <cite>Visual Madlibs task</cite>.",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_13",
  "x": "The embedded words are next mean pooled to form a vector representation of the answer. Note that, we do not encode prompts as they follow the same pattern for each <cite>Visual Madlibs</cite> category. ---------------------------------- **MULTIMODAL EMBEDDING** We use the Normalized Canonical Correlation Analysis (nCCA) to learn a mapping from two modalities: image and textual answers, into a joint embedding space. This embedding method has shown outstanding performance on the <cite>Visual Madlibs task</cite> <cite>[32]</cite> . At the test time, given the encoded image, we choose an answer (encoded by the mean pooling over word2vec words representations) from the set of four candidate answers that is the most similar to the encoded image in the multimodal embedding space.",
  "y": "uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_14",
  "x": "At the test time, given the encoded image, we choose an answer (encoded by the mean pooling over word2vec words representations) from the set of four candidate answers that is the most similar to the encoded image in the multimodal embedding space. Formally, the Canonical Correlation Analysis (CCA) maximizes the cosine similarity between two modalities (also called views) in the embedding space, that is: where tr is the matrix trace,X := XW 1 ,\u0176 := YW 2 , and X,Y are two views (encoded images, and textual answers in our case). Normalized Canonical Correlation Analysis (nCCA) [6] has been reported to work significantly better than the plain CCA. Here, columns of the projection matrices W 1 and W 2 are scaled by the p-th power (p=4) of the corresponding eigen values. The improvement is consistent with the findings of <cite>[32]</cite> , where nCCA performs better than CCA by about five percentage points in average on the hard task. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_15",
  "x": "Figure 3 depicts our architecture. Similarly to prior work, we encode an image with a CNN encoder that is next concatenated with (learnable) word embeddings of the prompt sentence, and fed to a recurrent neural network. We use a special '<BLANK>' token to denote the empty blank space in the image description. On the other side, for each completion candidate s we compute its representation by averaging over word2vec [18] representations of the words contributing to s. However, in contrast to the prior work <cite>[32]</cite> , instead of comparing the discrete output of the network with the representation of s, we directly optimize an objective in the embedding space. During training we maximize the similarity measure between the output embedding and the representation of \u03c3 by optimizing the following objective: which is a cosine similarity between the representation of the available during the training correct completion\u015d i , and an output embedding vector of the i-th image-prompt training Table 4 : BLEU-1 and BLEU-2 computed on <cite>Madlibs testing dataset</cite> for different approaches. ImageNet using R-CNN [13] , covering 42 MS COCO categories.",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_17",
  "x": "Although the CNN+LSTM models we trained on <cite>Madlibs</cite> were not quite as accurate as nCCA for selecting the correct multiple-choice answer, they did result in better, sometimes much better, accuracy (as measured by BLEU scores) for targeted generation. ---------------------------------- **CONCLUSIONS** We have introduced a new fill-in-the blank strategy for targeted natural language descriptions and used this to collect a <cite>Visual Madlibs dataset</cite>. Our analyses show that these descriptions are usually more detailed than generic whole image descriptions. We also introduce a targeted natural language description generation task, and a multiplechoice question answering task, then train and evaluate joint-embedding and generation models. Data produced by this paper will be publicly released upon acceptance.",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_19",
  "x": "---------------------------------- **EXPERIMENTAL RESULTS** We evaluate our method on the multiple choice task of the <cite>Visual Madlibs dataset</cite>. <cite>The dataset</cite> consists of about 360k descriptions, spanning 12 different categories specified by different types of templates, of about 10k images. The selected images from the MS COCO dataset comes with rich annotations. In the multi-choice scenario a textual prompt is given (every category follows the same, fixed template) with a blank to be filled, together with 4 candidate completions (see Figure 4 ). Every category represents a different type of question including scenes, affordances, emotions, or activities (the full list is shown in the first column of Table 1 ).",
  "y": "background uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_20",
  "x": "The selected images from the MS COCO dataset comes with rich annotations. In the multi-choice scenario a textual prompt is given (every category follows the same, fixed template) with a blank to be filled, together with 4 candidate completions (see Figure 4 ). Every category represents a different type of question including scenes, affordances, emotions, or activities (the full list is shown in the first column of Table 1 ). Since each category has fixed prompt, there is no need to include the prompt in the modeling given the training is done per each category. Finally, <cite>Visual Madlibs</cite> considers an easy and difficult tasks that differ in how the negative 3 candidate completions (distractors) are chosen. In the easy task, the distractors are randomly chosen from three descriptions of the same question type from other images. In the hard task, 3 distractors are chosen only from these images that contain the same objects as the given question image, and hence it requires a more careful and detailed image understanding.",
  "y": "background"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_22",
  "x": "Comparison to the state-of-the-art. Guided by the results of the previous experiments, we compare nCCA that uses Edge Boxes object proposals (nCCA (ours)) with the state-ofthe-arts on <cite>Visual Madlibs</cite> (nCCA <cite>[32]</cite> ). Both models use the same VGG Convolutional Neural Network [23] to encode images (or theirs crops), and word2vec to encode words. The models are trained per category (a model trained over all the categories performs inferior on the hard task <cite>[32]</cite> ). As Table 3 shows using a large number of object proposals Table 4 : Accuracies computed for different approaches on the easy and hard task. nCCA (ours) uses the representation with object proposals (NMS 0.75, and 100 proposals with mean-pooling). nCCA(bbox) mean-pools over the representations computed on the available ground-truth bounding boxes both at train and test time.",
  "y": "uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_23",
  "x": "Guided by the results of the previous experiments, we compare nCCA that uses Edge Boxes object proposals (nCCA (ours)) with the state-ofthe-arts on <cite>Visual Madlibs</cite> (nCCA <cite>[32]</cite> ). Both models use the same VGG Convolutional Neural Network [23] to encode images (or theirs crops), and word2vec to encode words. The models are trained per category (a model trained over all the categories performs inferior on the hard task <cite>[32]</cite> ). As Table 3 shows using a large number of object proposals Table 4 : Accuracies computed for different approaches on the easy and hard task. nCCA (ours) uses the representation with object proposals (NMS 0.75, and 100 proposals with mean-pooling). nCCA(bbox) mean-pools over the representations computed on the available ground-truth bounding boxes both at train and test time. The averages are computed only over 7 categories.",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_24",
  "x": "To see the limits, we compare nCCA (ours) against nCCA (bbox) <cite>[32]</cite> that crops over ground truth bounding boxes from MS COCO segmentations and next averages over theirs representations (Table 3 in <cite>[32]</cite> shows that ground truth bounding boxes outperforms automatically detected bounding boxes, and hence they can be seen as an upper bound for a detection method trained to detect objects on MS COCO). Surprisingly, nCCA (ours) outperforms nCCA (bbox) by a large margin as Table 4 shows. Arguably, object proposals have better recall and captures multi-scale, multi-parts phenomena. CNN+LSTM with comparison in the output embedding space. On one hand nCCA tops the leaderboard on the <cite>Visual Madlibs task</cite> <cite>[32]</cite> . Table 5 : Comparison between our Embedded CNN+LSTM approach that computes the similarity between input and candidate answers in the embedding space, and the plain CNN+LSTM original approach from <cite>[32]</cite> . Since the accuracies of CNN+LSTM <cite>[32]</cite> are unavailable for two categories, we report average over 10 categories in this case.",
  "y": "differences uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_25",
  "x": "To see the limits, we compare nCCA (ours) against nCCA (bbox) <cite>[32]</cite> that crops over ground truth bounding boxes from MS COCO segmentations and next averages over theirs representations (Table 3 in <cite>[32]</cite> shows that ground truth bounding boxes outperforms automatically detected bounding boxes, and hence they can be seen as an upper bound for a detection method trained to detect objects on MS COCO). Surprisingly, nCCA (ours) outperforms nCCA (bbox) by a large margin as Table 4 shows. Arguably, object proposals have better recall and captures multi-scale, multi-parts phenomena. CNN+LSTM with comparison in the output embedding space. On one hand nCCA tops the leaderboard on the <cite>Visual Madlibs task</cite> <cite>[32]</cite> . Table 5 : Comparison between our Embedded CNN+LSTM approach that computes the similarity between input and candidate answers in the embedding space, and the plain CNN+LSTM original approach from <cite>[32]</cite> . Since the accuracies of CNN+LSTM <cite>[32]</cite> are unavailable for two categories, we report average over 10 categories in this case.",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_26",
  "x": "On one hand nCCA tops the leaderboard on the <cite>Visual Madlibs task</cite> <cite>[32]</cite> . Table 5 : Comparison between our Embedded CNN+LSTM approach that computes the similarity between input and candidate answers in the embedding space, and the plain CNN+LSTM original approach from <cite>[32]</cite> . Since the accuracies of CNN+LSTM <cite>[32]</cite> are unavailable for two categories, we report average over 10 categories in this case. Results in %. completion of the prompt sentence out of four candidates, the comparison between the candidate completions should be directly done in the output embedding space. This contrasts to a post-hoc process used in <cite>[32]</cite> where an image description architecture (CNN+LSTM) first generates a completion that is next compared against the candidates in the word2vec space (see section 3 for more details). Moreover, since the \"Ask Your Neurons\" architecture [17] is more suitable for the question answering task, we extend that method to do comparisons directly in the embedding space (\"Embedded CNN+LSTM\" in Table 5 ).",
  "y": "uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_27",
  "x": "CNN+LSTM with comparison in the output embedding space. On one hand nCCA tops the leaderboard on the <cite>Visual Madlibs task</cite> <cite>[32]</cite> . Table 5 : Comparison between our Embedded CNN+LSTM approach that computes the similarity between input and candidate answers in the embedding space, and the plain CNN+LSTM original approach from <cite>[32]</cite> . Since the accuracies of CNN+LSTM <cite>[32]</cite> are unavailable for two categories, we report average over 10 categories in this case. Results in %. completion of the prompt sentence out of four candidates, the comparison between the candidate completions should be directly done in the output embedding space. This contrasts to a post-hoc process used in <cite>[32]</cite> where an image description architecture (CNN+LSTM) first generates a completion that is next compared against the candidates in the word2vec space (see section 3 for more details).",
  "y": "uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_28",
  "x": "CNN+LSTM with comparison in the output embedding space. On one hand nCCA tops the leaderboard on the <cite>Visual Madlibs task</cite> <cite>[32]</cite> . Table 5 : Comparison between our Embedded CNN+LSTM approach that computes the similarity between input and candidate answers in the embedding space, and the plain CNN+LSTM original approach from <cite>[32]</cite> . Since the accuracies of CNN+LSTM <cite>[32]</cite> are unavailable for two categories, we report average over 10 categories in this case. Results in %. completion of the prompt sentence out of four candidates, the comparison between the candidate completions should be directly done in the output embedding space. This contrasts to a post-hoc process used in <cite>[32]</cite> where an image description architecture (CNN+LSTM) first generates a completion that is next compared against the candidates in the word2vec space (see section 3 for more details).",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_29",
  "x": "completion of the prompt sentence out of four candidates, the comparison between the candidate completions should be directly done in the output embedding space. This contrasts to a post-hoc process used in <cite>[32]</cite> where an image description architecture (CNN+LSTM) first generates a completion that is next compared against the candidates in the word2vec space (see section 3 for more details). Moreover, since the \"Ask Your Neurons\" architecture [17] is more suitable for the question answering task, we extend that method to do comparisons directly in the embedding space (\"Embedded CNN+LSTM\" in Table 5 ). Note that, here we feed the sentence prompt to LSTM even though it is fixed per category. Table 5 shows the performance of different methods. Our \"Embedded CNN+LSTM\" outperforms other methods on both tasks confirming our hypothesis. \"Ask Your Neurons\" [17] is also slightly better than the original CNN+LSTM <cite>[32]</cite> (on the 10 categories that the results for CNN+LSTM are available it achieves 49.8% accuracy on the easy task, which is 2.1 percentage points higher than CNN+LSTM). ----------------------------------",
  "y": "extends"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_30",
  "x": "completion of the prompt sentence out of four candidates, the comparison between the candidate completions should be directly done in the output embedding space. This contrasts to a post-hoc process used in <cite>[32]</cite> where an image description architecture (CNN+LSTM) first generates a completion that is next compared against the candidates in the word2vec space (see section 3 for more details). Moreover, since the \"Ask Your Neurons\" architecture [17] is more suitable for the question answering task, we extend that method to do comparisons directly in the embedding space (\"Embedded CNN+LSTM\" in Table 5 ). Note that, here we feed the sentence prompt to LSTM even though it is fixed per category. Table 5 shows the performance of different methods. Our \"Embedded CNN+LSTM\" outperforms other methods on both tasks confirming our hypothesis. \"Ask Your Neurons\" [17] is also slightly better than the original CNN+LSTM <cite>[32]</cite> (on the 10 categories that the results for CNN+LSTM are available it achieves 49.8% accuracy on the easy task, which is 2.1 percentage points higher than CNN+LSTM). ----------------------------------",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_31",
  "x": "---------------------------------- **CONCLUSION** We study an image representation formed by averaging over representations of object proposals, and show its effectiveness through experimental evaluation on the <cite>Visual Madlibs dataset</cite> <cite>[32]</cite> . We achieve state of the art performance on the multi-choice \"filling the blank\" task. We have also shown and discussed effects of different parameters that affect how the proposals are obtained. Surprisingly, the larger number of proposals the better overall performance. Moreover, the model benefits even from highly overlapping proposals.",
  "y": "uses"
 },
 {
  "id": "87a190b1df5a7a941ba7b9a98064a3_0",
  "x": "This paper describes an expanded convolution parse tree kernel to incorporate entity information into syntactic structure of relation examples. Similar to<cite> Zhang et al. (2006)</cite> , we employ a convolution parse tree kernel in order to model syntactic structures. Different from their method, we use the convolution parse tree kernel expanded with entity information other than a composite kernel. One of our motivations is to capture syntactic and semantic information in a single parse tree for further graceful refinement, the other is that we can avoid the difficulty with tuning parameters in composite kernels. Evaluation on the ACE2004 corpus shows that our method slightly outperforms the previous feature-base and kernel-based methods. The rest of the paper is organized as follows. First, we present our expanded convolution tree kernel in Section 2.",
  "y": "similarities uses"
 },
 {
  "id": "87a190b1df5a7a941ba7b9a98064a3_1",
  "x": "Finally, we conclude our work with some general observations and indicate future work in Section 4. ---------------------------------- **EXPANDED TREE KERNEL** In this section, we describe the expanded convolution parse tree kernel and demonstrate how entity information can be incorporated into the parse tree. Figure 1: Different representations of a relation instance in the example sentence \"in many cities, angry crowds roam the streets.\", which is excerpted from the ACE2004 corpus, where a relation \"PHSY.Located\" holds between the first entity \"crowds\"(PER) and the second entity \"streets\" (FAC). We employ the same convolution tree kernel used by Collins and Duffy (2001) , Moschitti (2004) and<cite> Zhang et al. (2006)</cite> . This convolution tree kernel counts the number of subtrees that have similar productions on every node between two parse trees.",
  "y": "similarities uses"
 },
 {
  "id": "87a190b1df5a7a941ba7b9a98064a3_2",
  "x": "For example, PER-SOC relations describe the relationship between entities of type PER. Zhang et al. (2006) described five cases to extract the portion of parse tree for relation extraction. Their experiments show that PT (Path-enclosed Tree) achieves best performance among those cases, so we begin with PT and then incorporate entity features at different locations as depicted in Figure 1 . The four cases is listed as follows: (1) Compressed Path-enclosed Tree (CPT, T1 in Fig.1 ): Originated from PT in<cite> Zhang et al. (2006)</cite> , we further make two kinds of compression. One is to prune out the children nodes right before the second entity under the same parent node of NP. The other is to compress the sub-structure like \"X-->Y-->Z\" into \"X-->Z\" in the parse trees.",
  "y": "uses"
 },
 {
  "id": "87a190b1df5a7a941ba7b9a98064a3_3",
  "x": "(3) Entity-attached CPT (E-CPT, T3 in Fig.1 ): the entity type name is combined with entity order name, e.g. \"E1-PER\" denotes the first entity whose type is \"PER\". This case is also explored by<cite> Zhang et al. (2006)</cite> , and we include it here just for the purpose of comparison. (4) Top-attached CPT (T-CPT, T4 in Fig.1 ): the entity type information is attached to the top node of the parse tree. In order to distinguish between two entities, we use tags \"TP1\" and \"TP2\" to represent the first entity type and the second entity type respectively. From the above four cases, we want to evaluate whether and how the entity information will be useful for relation extraction and in what way we can embed the entity information (especially the location where we attach) in the parse tree in order to achieve the best performance. ---------------------------------- **EXPERIMENTS**",
  "y": "similarities"
 },
 {
  "id": "87a190b1df5a7a941ba7b9a98064a3_4",
  "x": "The reason may be that there are many syntactic errors in the parse trees produced by Charniak's parser although this parser represents the-start-of-art in parsing. Finally our system achieves surprisingly lowest performance in the condition \"Verbal\" although they occur frequently in the testing data. This may be that the syntactic structure in this condition is diverse and it contains too much noise in this kind of parse tree. It also suggests that much more noise needs to be pruned out from the parse tree while the key relation structure should remain in this condition. (4) Comparison with recent work Table 4 compares our system with recent work on the ACE2004 corpus. It shows that our system slightly outperforms recently best-reported systems. Compared with the composite kernel<cite> (Zhang et al, 2006)</cite> , our system further prunes the parse tree and incorporates entity features into the convolution parse tree kernel.",
  "y": "differences"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_0",
  "x": "**INTRODUCTION** Most spoken language translation (SLT) systems integrate (loosely or closely) two main modules: source language speech recognition (ASR) and source-to-target text translation (MT). In these approaches, a symbolic sequence of words (or characters) in the source language is used as an intermediary representation during the speech translation process. However, recent works have attempted to build end-to-end speech-to-text translation without using source language transcription during learning or decoding. One attempt to translate directly a source speech signal into target language text is that of [1] . However, the authors focus on the alignment between source speech utterances and their text translation without proposing a complete end-to-end translation system. The first attempt to build an end-to-end speech-to-text translation system (which does not use source language) is <cite>our own work</cite> <cite>[2]</cite> but it was applied to a synthetic (TTS) speech corpus.",
  "y": "background"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_1",
  "x": "A similar approach was then proposed and evaluated on a real speech corpus by [3] . This paper is a follow-up of <cite>our previous work</cite> <cite>[2]</cite> . We now investigate end-to-end speech-to-text translation on a corpus of audiobooks -LibriSpeech [4] -specifically augmented to perform end-to-end speech translation [5] . While previous works <cite>[2,</cite> 3] investigated the extreme case where source language transcription is not available during learning nor decoding (unwritten language scenario defined in [6, 7] ), we also investigate, in this paper, a midway case where a certain amount of source language transcription is available during training. In this intermediate scenario, a unique (endto-end) model is trained to decode source speech into target text through a single pass (which can be interesting if compact speech translation models are needed). This paper is organized as follows: after presenting our corpus in section 2, we present our end-to-end models in section 3. Section 4 describes our evaluation on two datasets: the synthetic dataset used in <cite>[2]</cite> and the audiobook dataset described in section 2.",
  "y": "extends"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_2",
  "x": "While previous works <cite>[2,</cite> 3] investigated the extreme case where source language transcription is not available during learning nor decoding (unwritten language scenario defined in [6, 7] ), we also investigate, in this paper, a midway case where a certain amount of source language transcription is available during training. In this intermediate scenario, a unique (endto-end) model is trained to decode source speech into target text through a single pass (which can be interesting if compact speech translation models are needed). This paper is organized as follows: after presenting our corpus in section 2, we present our end-to-end models in section 3. Section 4 describes our evaluation on two datasets: the synthetic dataset used in <cite>[2]</cite> and the audiobook dataset described in section 2. Finally, section 5 concludes this work. ---------------------------------- **AUDIOBOOK CORPUS FOR END-TO-END SPEECH TRANSLATION**",
  "y": "extends"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_3",
  "x": "While previous works <cite>[2,</cite> 3] investigated the extreme case where source language transcription is not available during learning nor decoding (unwritten language scenario defined in [6, 7] ), we also investigate, in this paper, a midway case where a certain amount of source language transcription is available during training. In this intermediate scenario, a unique (endto-end) model is trained to decode source speech into target text through a single pass (which can be interesting if compact speech translation models are needed). This paper is organized as follows: after presenting our corpus in section 2, we present our end-to-end models in section 3. Section 4 describes our evaluation on two datasets: the synthetic dataset used in <cite>[2]</cite> and the audiobook dataset described in section 2. Finally, section 5 concludes this work. ---------------------------------- **AUDIOBOOK CORPUS FOR END-TO-END SPEECH TRANSLATION**",
  "y": "uses"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_4",
  "x": "The test, dev and train sets correspond to the highest rated alignments. The remaining data (extended train) is more noisy, as it contains more incorrect alignments. The test set was manually checked, and incorrect alignments were removed. We perform all our experiments using train only (without extended train). Furthermore, we double the training size by concatenating the aligned references with the Google Translate references. We also mirror our experiments on the BTEC synthetic speech corpus, as a follow-up to <cite>[2]</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_5",
  "x": "Furthermore, we double the training size by concatenating the aligned references with the Google Translate references. We also mirror our experiments on the BTEC synthetic speech corpus, as a follow-up to <cite>[2]</cite> . ---------------------------------- **END-TO-END MODELS** For the three tasks, we use encoder-decoder models with attention [9, 10, 11, <cite>2,</cite> 3] . Because we want to share some parts of the model between tasks (multi-task training), the ASR and AST models use the same encoder architecture, and the AST and MT models use the same decoder architecture. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_6",
  "x": "For the three tasks, we use encoder-decoder models with attention [9, 10, 11, <cite>2,</cite> 3] . Because we want to share some parts of the model between tasks (multi-task training), the ASR and AST models use the same encoder architecture, and the AST and MT models use the same decoder architecture. ---------------------------------- **SPEECH ENCODER** The speech encoder is a mix between the convolutional encoder presented in [3] and our previously proposed encoder <cite>[2]</cite> . It takes as input a sequence of audio features: x = (x 1 , . . . , x Tx ) \u2208 R Tx\u00d7n . Like <cite>[2]</cite> , these features are given as input to two non-linear (tanh) layers, which output new features of size n .",
  "y": "uses"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_7",
  "x": "The speech encoder is a mix between the convolutional encoder presented in [3] and our previously proposed encoder <cite>[2]</cite> . It takes as input a sequence of audio features: x = (x 1 , . . . , x Tx ) \u2208 R Tx\u00d7n . Like <cite>[2]</cite> , these features are given as input to two non-linear (tanh) layers, which output new features of size n . Like [3] , this new set of features is then passed to a stack of two convolutional layers. Each layer applies 16 convolution filters of shape (3, 3, depth) with a stride of (2, 2) w.r.t. time and feature dimensions; depth is 1 for the first layer, and 16 for the second layer. We get features of shape (T x /2, n /2, 16) after the 1 st layer, and (T x /4, n /4, 16) after the 2 nd layer.",
  "y": "similarities"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_8",
  "x": "---------------------------------- **EXPERIMENTS** ---------------------------------- **MODEL SETTINGS** Speech files were preprocessed using Yaafe [13] , to extract 40 MFCC features and frame energy for each frame with a step size of 10 ms and window size of 40 ms, following [14, <cite>2]</cite> . We tokenize and lowercase all the text, and normalize the punctuation, with the Moses scripts 3 for LibriSpeech are of size 46 for English (transcriptions) and 167 for French (translation). The decoder outputs are always at the character-level (for AST, MT and ASR).",
  "y": "uses"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_9",
  "x": "The pre-trained model is identical to end-to-end, but its encoder and decoder are initialized with our ASR and MT Table 3 : Results of the AST task on BTEC test. \u2020 was obtained with an ensemble of 5 models, while we use ensembles of 2 models. The non-cascaded ensemble combines the pre-trained and multi-task models. Contrary to <cite>[2]</cite> , we only present mono-reference results. ASR mono ASR multi MT mono MT multi Fig. 1 : Augmented LibriSpeech Dev BLEU scores for the MT task, and WER scores for the ASR task, with the initial (mono-task) models, and when multi-task training picks up. ---------------------------------- **RESULTS**",
  "y": "differences"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_10",
  "x": "On both corpora, we show that: (1) it is possible to train compact end-to-end AST models with a performance close to cascaded models; (2) pretraining and multi-task learning 6 improve AST performance; (3) contrary to [3] , in both BTEC and LibriSpeech settings, best AST performance is observed when a symbolic sequence of symbols in the source language is used as an intermediary representation during the speech translation process (cascaded system); (4) finally, the AST results presented on Lib-riSpeech demonstrate that our augmented corpus is useful, although challenging, to benchmark end-to-end AST systems on real speech at a large scale. We hope that our baseline on Augmented LibriSpeech will be challenged in the future. The large improvements on MT and AST on the BTEC corpus, compared to <cite>[2]</cite> are mostly due to our use of a better decoder, which outputs characters instead of words. 6 BLEU End-to-End Pre-train Multi-task Fig. 2 : Dev BLEU scores on 3 models for end-to-end AST of audiobooks. Best scores on the dev set for the end-to-end (mono-task), pre-train and multi-task models were achieved at steps 369k, 129k and 95k. Figure 1 shows the evolution of BLEU and WER scores for MT and ASR tasks with single models, and when we continue training them as part of a multi-task model.",
  "y": "differences"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_11",
  "x": "Eventually, the End-to-End system reaches a similarly good solution, but after three times as many updates. Multi-Task training does not seem to be helpful when combined with pre-training. ---------------------------------- **ANALYSIS** ---------------------------------- **CONCLUSION** We present baseline results on End-to-End Automatic Speech Translation on a new speech translation corpus of audiobooks, and on a synthetic corpus extracted from BTEC (follow-up to <cite>[2]</cite> ).",
  "y": "extends"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_0",
  "x": "Recent NLP studies have thrived on distributional hypothesis. More recently, there have been efforts in applying the intuition to larger semantic units, such as sentences, or documents. However, approaches based on distributional semantics are limited by the grounding problem [7] , which calls for techniques to ground certain conceptual knowledge in perceptual information. Both NLP and vision communities have proposed various multi-modal learning methods to bridge the gap between language and vision. However, how general sentence representations can be benefited from visual grounding has not been fully explored yet. Very recently, <cite>[8]</cite> proposed a multi-modal <cite>encoder-decoder framework</cite> that, given an image caption, jointly predicts another caption and the features of associated image. <cite>The work</cite> showed promising results for further improving general sentence representations by grounding them visually.",
  "y": "background"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_1",
  "x": "Both NLP and vision communities have proposed various multi-modal learning methods to bridge the gap between language and vision. However, how general sentence representations can be benefited from visual grounding has not been fully explored yet. Very recently, <cite>[8]</cite> proposed a multi-modal <cite>encoder-decoder framework</cite> that, given an image caption, jointly predicts another caption and the features of associated image. <cite>The work</cite> showed promising results for further improving general sentence representations by grounding them visually. However, according to <cite>the model</cite>, visual association only occurs at the final hidden state of the encoder, potentially limiting the effect of visual grounding. Attention mechanism helps neural networks to focus on specific input features relevant to output. In the case of visually grounded multi-modal framework, applying such attention mechanism could help the encoder to identify visually significant words or phrases.",
  "y": "motivation"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_2",
  "x": "Both NLP and vision communities have proposed various multi-modal learning methods to bridge the gap between language and vision. However, how general sentence representations can be benefited from visual grounding has not been fully explored yet. Very recently, <cite>[8]</cite> proposed a multi-modal <cite>encoder-decoder framework</cite> that, given an image caption, jointly predicts another caption and the features of associated image. <cite>The work</cite> showed promising results for further improving general sentence representations by grounding them visually. However, according to <cite>the model</cite>, visual association only occurs at the final hidden state of the encoder, potentially limiting the effect of visual grounding. Attention mechanism helps neural networks to focus on specific input features relevant to output. In the case of visually grounded multi-modal framework, applying such attention mechanism could help the encoder to identify visually significant words or phrases.",
  "y": "background"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_3",
  "x": "However, approaches based on distributional semantics are limited by the grounding problem [7] , which calls for techniques to ground certain conceptual knowledge in perceptual information. Both NLP and vision communities have proposed various multi-modal learning methods to bridge the gap between language and vision. However, how general sentence representations can be benefited from visual grounding has not been fully explored yet. Very recently, <cite>[8]</cite> proposed a multi-modal <cite>encoder-decoder framework</cite> that, given an image caption, jointly predicts another caption and the features of associated image. <cite>The work</cite> showed promising results for further improving general sentence representations by grounding them visually. However, according to <cite>the model</cite>, visual association only occurs at the final hidden state of the encoder, potentially limiting the effect of visual grounding. Attention mechanism helps neural networks to focus on specific input features relevant to output.",
  "y": "motivation"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_4",
  "x": "Convergence between computer vision and NLP researches have increasingly become common. Image captioning [11] [12] [13] [14] and image synthesis [15] are two common tasks. There have been significant studies focusing on improving word embeddings [16, 17] , phrase embeddings [18] , sentence embeddings <cite>[8</cite>, 19] , language models [20] through multi-modal learning of vision and language. Among all studies, <cite>[8]</cite> is the first to apply skip-gram-like intuition (predicting multiple modalities from langauge) to joint learning of language and vision in the perspective of general sentence representations. Attention Mechanism in Multi-Modal Semantics. Attention mechanism was first introduced in [21] for neural machine translation. Similar intuitions have been applied to various NLP [5, 22, 23] and vision tasks [11] .",
  "y": "background"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_5",
  "x": "Image captioning [11] [12] [13] [14] and image synthesis [15] are two common tasks. There have been significant studies focusing on improving word embeddings [16, 17] , phrase embeddings [18] , sentence embeddings <cite>[8</cite>, 19] , language models [20] through multi-modal learning of vision and language. Among all studies, <cite>[8]</cite> is the first to apply skip-gram-like intuition (predicting multiple modalities from langauge) to joint learning of language and vision in the perspective of general sentence representations. Attention Mechanism in Multi-Modal Semantics. Attention mechanism was first introduced in [21] for neural machine translation. Similar intuitions have been applied to various NLP [5, 22, 23] and vision tasks [11] . [11] applied attention mechanism to images to bind specific visual features to language.",
  "y": "background"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_6",
  "x": "Joint Learning of Language and Vision. Convergence between computer vision and NLP researches have increasingly become common. Image captioning [11] [12] [13] [14] and image synthesis [15] are two common tasks. There have been significant studies focusing on improving word embeddings [16, 17] , phrase embeddings [18] , sentence embeddings <cite>[8</cite>, 19] , language models [20] through multi-modal learning of vision and language. Among all studies, <cite>[8]</cite> is the first to apply skip-gram-like intuition (predicting multiple modalities from langauge) to joint learning of language and vision in the perspective of general sentence representations. Attention Mechanism in Multi-Modal Semantics. Attention mechanism was first introduced in [21] for neural machine translation.",
  "y": "background"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_7",
  "x": "To the best of our knowledge, such attempt is the first among studies on joint learning of language and vision. ---------------------------------- **PROPOSED METHOD** Given a data sample (X, Y, h I ) \u2208 D, where X is the source caption, Y is the target caption, and h I is the hidden representation of the image, our goal is to predict Y and h I with X, and the hidden representation in the middle serves as the general sentence representation. ---------------------------------- **VISUALLY GROUNDED ENCODER-DECODER FRAMEWORK** We base our model on the <cite>encoder-decoder framework</cite> introduced in <cite>[8]</cite> .",
  "y": "uses"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_8",
  "x": "We train the model to rank the similarity between predicted image featuresh I and the target image features h I higher than other pairs, which is achieved by ranking loss functions. Although margin ranking loss has been the dominant choice for training cross-modal feature matching <cite>[8</cite>, 20, 25] , we find that log-exp-sum pairwise ranking [26] yields better results in terms of evaluation performance and efficiency. Thus, the objective for ranking where N is the set of negative examples and sim is cosine similarity. ---------------------------------- **VISUAL GROUNDING WITH SELF-ATTENTION** Let h t \u2208 R d h be the encoder hidden state at timestep t concatenated from two opposite directional LSTMs (d h is the dimensionality of sentence representations).",
  "y": "differences"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_9",
  "x": "**LEARNING OBJECTIVES** Following the experimental design of <cite>[8]</cite> , we conduct experiments on three different learning objectives: CAP2ALL, CAP2CAP, CAP2IMG. Under CAP2ALL, the model is trained to predict both the target caption and the associated image: L = L C + L V G . Under CAP2CAP, the model is trained to predict only the target caption (L = L C ) and, under CAP2IMG, only the associated image (L = L V G ). ---------------------------------- **EXPERIMENTS** ----------------------------------",
  "y": "uses"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_10",
  "x": "We employ orthogonal initialization [30] for recurrent weights and xavier initialization [31] for all others. For the datasets, we use Karpathy and Fei-Fei's split for MS-COCO dataset [13] . Image features are prepared by extracting hidden representations at the final layer of ResNet-101 [32] . We evaluate sentence representation quality using SentEval 2 <cite>[8</cite>, 10] scripts. Mini-batch size is 128 and negative samples are prepared from remaining data samples in the same mini-batch. ---------------------------------- **EVALUATION**",
  "y": "uses"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_11",
  "x": "We evaluate sentence representation quality using SentEval 2 <cite>[8</cite>, 10] scripts. Mini-batch size is 128 and negative samples are prepared from remaining data samples in the same mini-batch. ---------------------------------- **EVALUATION** Adhering to the experimental settings of <cite>[8]</cite> , we concatenate sentence representations produced from our model with those obtained from the state-of-the-art unsupervised learning model (Layer Normalized Skip-Thoughts, ST-LN) [33] . We evaluate the quality of sentence representations produced from different variants of our encoders on well-known transfer tasks: movie review sentiment (MR) [34] , customer reviews (CR) [35] , subjectivity (SUBJ) [36] , opinion polarity (MPQA) [37] , paraphrase identification (MSRP) [38] , binary sentiment classification (SST) [39] , SICK entailment and SICK relatedness [40] . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_12",
  "x": "---------------------------------- **ATTENTION MECHANISM AT WORK** In order to study the effects of incorporating self-attention mechanism in joint prediction of image and language features, we examine attention vectors for selected samples from MS-COCO dataset and compare them to associated images ( Figure 1 ). For example, given the sentence \"man in black shirt is playing guitar\", our model identifies words that have association with strong visual imagery, such as \"man\", \"black\" and \"guitar\". Given the second sentence, our model learned to attend to visually significant words such as \"cat\" and \"bowl\". These findings show that visually grounding self-attended sentence representations helps to expose word-level visual features onto sentence representations <cite>[8]</cite> . Figure 1: Activated attention weights on two samples from MS-COCO dataset.",
  "y": "extends differences"
 },
 {
  "id": "88900d3533701056f6a26bf7c68670_0",
  "x": "Given a training corpus of NL sentences annotated with their correct MRs, the goal of a learning system for semantic parsing is to induce an efficient and accurate semantic parser that can map novel sentences into their correct MRs. Several learning systems have been developed for semantic parsing, many of them recently (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005;<cite> Kate and Mooney, 2006)</cite> . These systems use supervised learning methods which only utilize annotated NL sentences. However, it requires considerable human effort to annotate sentences. In contrast, unannotated NL sentences are usually easily available. Semi-supervised learning methods utilize cheaply available unannotated data during training along with annotated data and often perform better than purely supervised learning methods trained on the same amount of annotated data (Chapelle et al., 2006) . In this paper we present, to our knowledge, the first semi-supervised learning system for semantic parsing.",
  "y": "background"
 },
 {
  "id": "88900d3533701056f6a26bf7c68670_1",
  "x": "Several learning systems have been developed for semantic parsing, many of them recently (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005;<cite> Kate and Mooney, 2006)</cite> . These systems use supervised learning methods which only utilize annotated NL sentences. However, it requires considerable human effort to annotate sentences. In contrast, unannotated NL sentences are usually easily available. Semi-supervised learning methods utilize cheaply available unannotated data during training along with annotated data and often perform better than purely supervised learning methods trained on the same amount of annotated data (Chapelle et al., 2006) . In this paper we present, to our knowledge, the first semi-supervised learning system for semantic parsing. We modify KRISP, a supervised learning system for semantic parsing presented in<cite> (Kate and Mooney, 2006)</cite> , to make a semi-supervised system we call SEMISUP-KRISP.",
  "y": "differences background"
 },
 {
  "id": "88900d3533701056f6a26bf7c68670_2",
  "x": "However, it requires considerable human effort to annotate sentences. In contrast, unannotated NL sentences are usually easily available. Semi-supervised learning methods utilize cheaply available unannotated data during training along with annotated data and often perform better than purely supervised learning methods trained on the same amount of annotated data (Chapelle et al., 2006) . In this paper we present, to our knowledge, the first semi-supervised learning system for semantic parsing. We modify KRISP, a supervised learning system for semantic parsing presented in<cite> (Kate and Mooney, 2006)</cite> , to make a semi-supervised system we call SEMISUP-KRISP. Experiments on a realworld dataset show the improvements SEMISUP-KRISP obtains over KRISP by utilizing unannotated sentences. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "88900d3533701056f6a26bf7c68670_3",
  "x": "**BACKGROUND** This section briefly provides background needed for describing our approach to semi-supervised semantic parsing. ---------------------------------- **KRISP: THE SUPERVISED SEMANTIC PARSING** Learning System KRISP (Kernel-based Robust Interpretation for Semantic Parsing) <cite>(Kate and Mooney, 2006</cite> ) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data. The productions of the formal MRL grammar are treated like semantic concepts. For each of these productions, a Support-Vector Machine (SVM) (Cristianini and Shawe-Taylor, 2000) classifier is trained using string similarity as the kernel (Lodhi et al., 2002) .",
  "y": "background"
 },
 {
  "id": "88900d3533701056f6a26bf7c68670_4",
  "x": "In the first iteration, the set of positive examples for production contains all sentences whose corresponding MRs use the production in their parse trees. The set of negative examples includes all of the other training sentences. Using these positive and negative examples, an SVM classifier is trained for each production using a string kernel. In subsequent iterations, the parser learned from the previous iteration is applied to the training examples and more refined positive and negative examples, which are more specific substrings within the sentences, are collected for training. Iterations are continued until the classifiers converge, analogous to iterations in EM (Dempster et al., 1977) . Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data<cite> (Kate and Mooney, 2006)</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "891d0e17bf2fb79a378c2d77dda768_0",
  "x": "Language models can be optimized to recognize syntax and semantics with great accuracy [1] . However, the output generated can be repetitive and generic leading to monotonous or uninteresting responses (e.g \"I don't know\") regardless of the input [2] . While application of attention [3, 4] and advanced decoding mechanisms like beam search and variation sampling <cite>[5]</cite> have shown improvements, it does not solve the underlying problem. In creative text generation, the objective is not strongly bound to the ground truth-instead the objective is to generate diverse, unique or original samples. We attempt to do this through a discriminator which can give feedback to the generative model through a cost function that encourages sampling of creative tokens. The contributions of this paper are in the usage of a GAN framework to generate creative pieces of writing. Our experiments suggest that generative text models, while very good at encapsulating semantic, syntactic and domain information, perform better with external feedback from a discriminator for fine-tuning objectiveless decoding tasks like that of creative text.",
  "y": "motivation"
 },
 {
  "id": "891d0e17bf2fb79a378c2d77dda768_1",
  "x": "Once we have our fine-tuned encoders for each target dataset, we train in an adversarial manner. The discriminator objective here is to score the quality of the creative text. The discriminator is trained for 3 iterations for every iteration of the generator, a practice seen in previous work [26] . Creative-GAN relies on using the reward from the discriminator [13, 16] for backpropagation. We follow a similar training procedure for GumbelGAN. Outputs are generated through sampling over a multinomial distribution for all methods, instead of argmax on the log-likelihood probabilities, as sampling has shown to produce better output quality <cite>[5]</cite> . Please refer to Supplementary Section Table 3 for training parameters of each dataset and Table 2 for hyperparameters of each encoder. We pick these values after experimentation with our validation set.",
  "y": "motivation"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_0",
  "x": "Because the reactions of other users to an abuse case are completely beyond the abuser's control, some authors consider the content of messages occurring around the targeted message, instead of focusing only on the targeted message itself. For instance, (Yin et al., 2009 ) use features derived from the sentences neighboring a given message to detect harassment on the Web. (Balci and Salah, 2015) take advantage of user features such as the gender, the number of in-game friends or the number of daily logins to detect abuse in the community of an online game. In our previous work<cite> (Papegnies et al., 2019)</cite> , we proposed a radically different method that completely ignores the textual content of the messages, and relies only on a graph-based modeling of the conversation. This is the only graph-based approach ignoring the linguistic content proposed in the context of abusive messages detection. Our conversational network extraction process is inspired from other works leveraging such graphs for other purposes: chat logs (Mutton, 2004) or online forums (Forestier et al., 2011) interaction modeling, user group detection (Camtepe et al., 2004) . Additional references on abusive message detection and conversational network modeling can be found in<cite> (Papegnies et al., 2019)</cite> . In this paper, based on the assumption that the interactions between users and the content of the exchanged messages convey different information, we propose a new method to perform abuse detection while leveraging both sources.",
  "y": "background"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_1",
  "x": "For instance, (Yin et al., 2009 ) use features derived from the sentences neighboring a given message to detect harassment on the Web. (Balci and Salah, 2015) take advantage of user features such as the gender, the number of in-game friends or the number of daily logins to detect abuse in the community of an online game. In our previous work<cite> (Papegnies et al., 2019)</cite> , we proposed a radically different method that completely ignores the textual content of the messages, and relies only on a graph-based modeling of the conversation. This is the only graph-based approach ignoring the linguistic content proposed in the context of abusive messages detection. Our conversational network extraction process is inspired from other works leveraging such graphs for other purposes: chat logs (Mutton, 2004) or online forums (Forestier et al., 2011) interaction modeling, user group detection (Camtepe et al., 2004) . Additional references on abusive message detection and conversational network modeling can be found in<cite> (Papegnies et al., 2019)</cite> . In this paper, based on the assumption that the interactions between users and the content of the exchanged messages convey different information, we propose a new method to perform abuse detection while leveraging both sources. For this purpose, we take advantage of the content- (Papegnies et al., 2017b) and graph-based<cite> (Papegnies et al., 2019</cite> ) methods that we previously developed.",
  "y": "background"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_2",
  "x": "For this purpose, we take advantage of the content- (Papegnies et al., 2017b) and graph-based<cite> (Papegnies et al., 2019</cite> ) methods that we previously developed. We propose three different ways to combine them, and compare their performance on a corpus of chat logs originating from the community of a French multiplayer online game. We then perform a feature study, finding the most informative ones and discussing their role. Our contribution is twofold: the exploration of fusion methods, and more importantly the identification of discriminative features for this problem. The rest of this article is organized as follows. In Section 2, we describe the methods and strategies used in this work. In Section 3 we present our dataset, the experimental setup we use for this classification task, and the performances we obtained.",
  "y": "extends"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_3",
  "x": "For this purpose, we take advantage of the content- (Papegnies et al., 2017b) and graph-based<cite> (Papegnies et al., 2019</cite> ) methods that we previously developed. We propose three different ways to combine them, and compare their performance on a corpus of chat logs originating from the community of a French multiplayer online game. We then perform a feature study, finding the most informative ones and discussing their role. Our contribution is twofold: the exploration of fusion methods, and more importantly the identification of discriminative features for this problem. The rest of this article is organized as follows. In Section 2, we describe the methods and strategies used in this work. In Section 3 we present our dataset, the experimental setup we use for this classification task, and the performances we obtained.",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_4",
  "x": "In Section 2, we describe the methods and strategies used in this work. In Section 3 we present our dataset, the experimental setup we use for this classification task, and the performances we obtained. Finally, we summarize our contributions in Section 4 and present some perspectives for this work. ---------------------------------- **METHODS** In this section, we summarize the content-based method from (Papegnies et al., 2017b ) (Section 2.1) and the graph-based method from<cite> (Papegnies et al., 2019</cite> ) (Section 2.2). We then present the fusion method proposed in this paper, aiming at taking advantage of both sources of information (Section 2.3).",
  "y": "background"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_5",
  "x": "Figure 1 shows the whole process, and is discussed through this section. Figure 1 . Representation of our processing pipeline. Existing methods refers to our previous work described in (Papegnies et al., 2017b ) (content-based method) and<cite> (Papegnies et al., 2019)</cite> (graph-based method), whereas the contribution presented in this article appears on the right side (fusion strategies). ---------------------------------- **CONTENT-BASED METHOD** This method corresponds to the bottom-left part of Figure 1 (in green).",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_6",
  "x": "---------------------------------- **GRAPH-BASED METHOD** This method corresponds to the top-left part of Figure 1 (in red). It completely ignores the content of the messages, and only focuses on the dynamics of the conversation, based on the interactions between its participants<cite> (Papegnies et al., 2019)</cite> . It is three-stepped: 1) extracting a conversational graph based on the considered message as well as the messages preceding and/or following it; 2) computing the topological measures of this graph to characterize its structure; and 3) using these values as features to train an SVM to distinguish between abusive and non-abusive messages. The vertices of the graph model the participants of the conversation, whereas its weighted edges represent how intensely they communicate. The graph extraction is based on a number of concepts illustrated in Figure 2 , in which each rectangle represents a message.",
  "y": "motivation"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_7",
  "x": "The scope can be either micro-, meso-or macroscopic: it corresponds to the amount of information considered by the measure. For instance, the graph density is microscopic, the modularity is mesoscopic, and the diameter is macroscopic. All these measures are computed for each graph, and allow describing the conversation surrounding the message of interest. The SVM is then trained using these values as features. In this work, we use exactly the same measures as in<cite> (Papegnies et al., 2019)</cite> . ---------------------------------- **FUSION**",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_8",
  "x": "---------------------------------- **EXPERIMENTS** In this section, we first describe our dataset and the experimental protocol followed in our experiments (Section 3.1). We then present and discuss our results, in terms of classification performance (Sections 3.2) and feature selection (Section 3.3). ---------------------------------- **EXPERIMENTAL PROTOCOL** The dataset is the same as in our previous publications (Papegnies et al., 2017b <cite>(Papegnies et al., , 2019</cite> .",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_9",
  "x": "Some inconsistencies in the database prevent us from retrieving the context of certain messages, which we remove from the set. After this cleaning, the Abuse class contains 655 messages. In order to keep a balanced dataset, we further extract the same number of messages at random from the ones that have not been flagged as abusive. This constitutes our Non-abuse class. Each message, whatever its class, is associated to its surrounding context (i.e. messages posted in the same thread). The graph extraction method used to produce the graph-based features requires to set certain parameters. We use the values matching the best performance, obtained during the greedy search of the parameter space performed in<cite> (Papegnies et al., 2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_10",
  "x": "In particular, regarding the two most important parameters (see Section 2.2), we fix the context period size to 1,350 messages and the sliding window length to 10 messages. Implementation-wise, we use the iGraph library (Csardi and Nepusz, 2006) to extract the conversational networks and process the corresponding features. We use the Sklearn toolkit (Pedregosa et al., 2011) to get the text-based features. We use the SVM classifier implemented in Sklearn under the name SVC (C-Support Vector Classification). Because of the relatively small dataset, we set-up our experiments using a 10-fold cross-validation. Each fold is balanced between the Abuse and Non-abuse classes, 70% of the dataset being used for training and 30% for testing. Table 1 presents the Precision, Recall and F -measure scores obtained on the Abuse class, for both baselines (Content-based (Papegnies et al., 2017b) and Graph-based<cite> (Papegnies et al., 2019)</cite> ) and all three proposed fusion strategies (Early Fusion, Late Fusion and Hybrid Fusion).",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_11",
  "x": "The last 4 columns precise which variants of the graph-based features are concerned. Indeed, as explained in Section 2.2, most of these topological measures can handle/ignore edge weights and/or edge directions, can be vertex-or graph-focused, and can be computed for each of the three types of networks (Before, After and Full). There are three Content-Based TF. The first is the Naive Bayes prediction, which is not surprising as it comes from a fully fledged classifier processing BoWs. The second is the tf -idf score computed over the Abuse class, which shows that considering term frequencies indeed improve the classification performance. The third is the Capital Ratio (proportion of capital letters in the comment), which is likely to be caused by abusive message tending to be shouted, and therefore written in capitals. The Graph-Based TF are discussed in depth in our previous article<cite> (Papegnies et al., 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_12",
  "x": "---------------------------------- **CONCLUSION AND PERSPECTIVES** In this article, we tackle the problem of automatic abuse detection in online communities. We take advantage of the methods that we previously developed to leverage message content (Papegnies et al., 2017a) and interactions between users<cite> (Papegnies et al., 2019)</cite> , and create a new method using both types of information simultaneously. We show that the features extracted from our content-and graph-based approaches are complementary, and that combining them allows to sensibly improve the results up to 93.26 (F -measure). One limitation of our method is the computational time required to extract certain features. However, we show that using only a small subset of relevant features allows to dramatically reduce the processing time (down to 3%) while keeping more than 97% of the original performance.",
  "y": "extends"
 },
 {
  "id": "8abffa3f807bad5ae2073aa7db215d_0",
  "x": "Learning knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, 5, 6] . How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, 9 ]. The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding <cite>[11,</cite> 12, 13] , which considers syntactic contexts rather",
  "y": "background"
 },
 {
  "id": "8abffa3f807bad5ae2073aa7db215d_1",
  "x": "The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding <cite>[11,</cite> 12, 13] , which considers syntactic contexts rather The 2016 Conference on Computational Linguistics and Speech Processing ROCLING 2016, pp. 100-102 \uf0d3 The Association for Computational Linguistics and Chinese Language Processing 100 than window contexts in word2vec. Bansal et al. [8] and <cite>Melamud et al. [11]</cite> show the benefits of such modified-context embeddings in dependency parsing task. The dependency-based word embedding can relieve the problem of data sparseness, since even without occurrence of dependency word pairs in a corpus, dependency scores can be still calculated by word embeddings [12] . In this paper, we proposed a rescoring approach for parsing, based on a combination of original parsing scores and dependency word embedding scores to assist the determination of the best parse tree among the n-best parse trees.",
  "y": "background"
 },
 {
  "id": "8abffa3f807bad5ae2073aa7db215d_2",
  "x": "Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding <cite>[11,</cite> 12, 13] , which considers syntactic contexts rather The 2016 Conference on Computational Linguistics and Speech Processing ROCLING 2016, pp. 100-102 \uf0d3 The Association for Computational Linguistics and Chinese Language Processing 100 than window contexts in word2vec. Bansal et al. [8] and <cite>Melamud et al. [11]</cite> show the benefits of such modified-context embeddings in dependency parsing task. The dependency-based word embedding can relieve the problem of data sparseness, since even without occurrence of dependency word pairs in a corpus, dependency scores can be still calculated by word embeddings [12] .",
  "y": "background"
 },
 {
  "id": "8b223f35a4685d6627d29c907e4742_0",
  "x": "However, such an approach of manual analysis cannot scale to millions of songs. Caliskan et al. proposed Word Embedding Association Test (WEAT) to computationally measure biases in any text repository <cite>[5]</cite> . Their test quantifies biases by computing similarity scores between various sets of words. To compute similarity, the WEAT test represents words using a distributed word representation method such as fastText or word2vec [4, 16] . We apply the WEAT test on song lyrics and discuss its implications. ---------------------------------- **STYLE ANALYSIS**",
  "y": "background"
 },
 {
  "id": "8b223f35a4685d6627d29c907e4742_1",
  "x": "However, such an approach of manual analysis cannot scale to millions of songs. Caliskan et al. proposed Word Embedding Association Test (WEAT) to computationally measure biases in any text repository <cite>[5]</cite> . Their test quantifies biases by computing similarity scores between various sets of words. To compute similarity, the WEAT test represents words using a distributed word representation method such as fastText or word2vec [4, 16] . We apply the WEAT test on song lyrics and discuss its implications. ---------------------------------- **STYLE ANALYSIS**",
  "y": "uses background"
 },
 {
  "id": "8b223f35a4685d6627d29c907e4742_2",
  "x": "A positive value of the IAT test indicates that people are biased to associate first attribute word set to the first target word set (Bias: flowers are pleasant) and second attribute word set with second target word set (Bias: insects are unpleasant). A negative value of the effect size indicates the bias in the other direction, that is flowers are unpleasant, and insects are pleasant. Larger magnitude of effect size indicates a stronger bias. If the value of effect size is closer to zero, then it indicates slight or no bias. Caliskan et al. designed the Word Embedding Association Test (WEAT) by tweaking the IAT test <cite>[5]</cite> . Similar to the IAT test, this test can measure bias given the sets of attribute and target words. However, the IAT test requires human subjects to compute the bias value.",
  "y": "background"
 },
 {
  "id": "8b223f35a4685d6627d29c907e4742_3",
  "x": "The WEAT test represents attribute and target words as vectors using distributed representation methods such as word2vec and fastText [4, 16] . The WEAT test computes the similarity between words using the cosine similarity. Caliskan et al. have performed the bias measurement on a large internet crawl text corpus using the WEAT test. They have shown that their results correlate with the IAT tests conducted with human subjects. We applied the WEAT test on our song lyrics dataset. Due to the small size of popular songs dataset, we cannot apply the WEAT test separately on popular songs lyrics. Please refer to Table 1 . Corresponding to eight rows of the table, we have measured eight biases. We borrowed these attribute and target word sets from Caliskan et al. <cite>[5]</cite> .",
  "y": "uses"
 },
 {
  "id": "8b5e14bdf3f415725333de672be114_0",
  "x": "Most of the traditional readability formulas were also based on countable features of text, such as syllable counts (Flesch, 1948) . More advanced machine learning techniques such as classification and regression have been applied to the task of reading level prediction (Collins- Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009; <cite>Feng et al., 2010)</cite> ; such works are described in further detail in the next Section 2. In recent work (Ma et al., 2012) , we approached the problem of fine-grained leveling of books, demonstrating that a ranking approach to predicting reading level outperforms both classification and regression approaches in that domain. A further finding was that visually-oriented features that consider the visual layout of the page (e.g. number of text lines per annotated text region, text region area compared to the whole page area and font size etc.) play an important role in predicting the reading levels of children's books in which pictures and textual layout dominate the book content over text. However, the data preparation process in our previous study involves human intervention-we ask human annotators to draw rectangle markups around text region over pages. Moreover, we only use a very shallow surface level text-based feature set to compare with the visually-oriented features. Hence in this paper, we assess the effect of using completely automated annotation processing within the same framework.",
  "y": "background"
 },
 {
  "id": "8b5e14bdf3f415725333de672be114_1",
  "x": "At the same time, we have also extended our previous feature set by introducing a richer set of automatically derived textbased features, proposed by<cite> Feng et al. (2010)</cite> , which capture deeper syntactic complexities of the text. Unlike our previous work, the major goal of this paper is not trying to compare different machine learning techniques used in readability assessment task, but rather to compare the performance differences between with and without human labor involved within our previous proposed system framework. We begin the paper with the description of related work in Section 2, followed by detailed explanation regarding data preparation and automatic annotations in Section 3. The extended features will be covered in Section 4, followed by experimental analysis in Section 5, in which we will compare the results between human annotations and automatic annotations. We will also report the system performance after incorporating the rich text features (structural features). Conclusions follow in Section 6. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "8b5e14bdf3f415725333de672be114_2",
  "x": "Aluisio et al. (2010) have developed a tool for text simplification for the authoring process which addresses lexical and syntactic phenomena to make text readable but their assessment takes place at more coarse levels of literacy instead of finer-grained levels used for children's books. A detailed analysis of various features for automatic readability assessment has been done by<cite> Feng et al. (2010)</cite> . Most of the previous work has used web page documents, short passages or articles from educational newspapers as their datasets; typically the task is to assess reading level at a whole-grade level. In contrast, early primary children's literature is typically leveled in a more fine-grained manner, and the research question we pursued in our previous study was to investigate appropriate methods of predicting what we suspected was a non-linear reading level scale. Automating the process of readability assessment is crucial for eventual widespread acceptance. Previous studies have looked at documents that were already found in electronic form, such as web texts. While e-books are certainly on the rise (and would help automated processing) it is unlikely that paper books will be completely eliminated from the primary school classroom soon.",
  "y": "background"
 },
 {
  "id": "8b5e14bdf3f415725333de672be114_3",
  "x": "Since our previous work only uses surface level of text features, we are interested in investigating the contribution of high-level structural features to the current system. Feng et al. (2010) found several parsing-based features and part-of-speech based features to be useful. We utilize the Stanford Parser (Klein and Manning, 2003) to extract the following features from the XML files based on those used in<cite> (Feng et al., 2010)</cite> : ---------------------------------- **EXPERIMENTS** In the experiments, we look at how much the performance dropped by switching to zero human inputs. We also investigate the impact of using a richer set of text-based features.",
  "y": "uses"
 },
 {
  "id": "8b5e14bdf3f415725333de672be114_4",
  "x": "While the process does still require manual scanning of each book (which can be time consuming depending on the kind of scanner), the automatic processing can reduce the labor per book from approximately twenty minutes per book to just a few seconds. ---------------------------------- **INCORPORATING STRUCTURAL FEATURES** Our previous study demonstrated that combining surface features with visual features produces promising results. As mentioned above, the second aim of this study is to see how much benefit we can get from incorporating high-level structural features, such as those used in<cite> (Feng et al., 2010)</cite> (described in Section 4.2), with the features in our previous study. annotation under the \u00b11 accuracy metric, the visual features and the structural features have the same performance, whose accuracy are both slightly lower than that of surface level features. By combining the surface level features with the visual features, the system obtains the best performance.",
  "y": "uses"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_0",
  "x": "However, informal arguments in online argumentative discourses can exhibit different styles. Recent work has begun to model different aspects of these naturally occurring lay arguments, with tasks including stance classification (Somasundaran and Wiebe, 2009; Walker et al., 2012) , argument summarization (Misra et al., 2015) , sarcasm detection (Justo et al., 2014) and classification of propositions and arguments <cite>(Park and Cardie, 2014</cite>; Park et al., 2015; Oraby et al., 2015) . Of particular interest is the fact that arguments in online user comments, unlike those written by professionals, often have inappropriate or missing justifications. Recognizing such propositions and determining the appropriate types of support can be useful for assessing the strength of the supporting information and, in turn, the strength of the whole argument. To this end, two previous studies have produced data sets and methods for classifying propositions in online argumentative discourse. The first of these studies<cite> (Park and Cardie, 2014)</cite> compiled online user comments from a discussion website and developed a framework for automatically classifying each proposition as either \"unverifiable\", \"verifiable non-experiential\", or \"verifiable experiential\", where the appropriate types of support are reason, evidence, and optional evidence, respectively. The second study, Oraby et al. (2015) , uses a different online corpus (Walker et al., 2012 ) of short argumentative responses to quotes, and classifies each response as either \"factual\" or \"feeling\" according to whether the support invoked appeals to facts or to emotions.",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_1",
  "x": "To this end, two previous studies have produced data sets and methods for classifying propositions in online argumentative discourse. The first of these studies<cite> (Park and Cardie, 2014)</cite> compiled online user comments from a discussion website and developed a framework for automatically classifying each proposition as either \"unverifiable\", \"verifiable non-experiential\", or \"verifiable experiential\", where the appropriate types of support are reason, evidence, and optional evidence, respectively. The second study, Oraby et al. (2015) , uses a different online corpus (Walker et al., 2012 ) of short argumentative responses to quotes, and classifies each response as either \"factual\" or \"feeling\" according to whether the support invoked appeals to facts or to emotions. In this paper, we use the term \"claim\" loosely to refer to an individual proposition (a sentence or independent clause) in an argument, or to a short argumentative text containing one or more propositions. In classifying propositions,<cite> Park and Cardie (2014)</cite> followed previous work such as Reed et al. (2008) and Palau and Moens (2009) , employing supervised learning methods. Despite using a rich set of linguistic features, these approaches suffer from low accuracy. Moreover, generating these features can be a tedious and complex process.",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_2",
  "x": "To this end, two previous studies have produced data sets and methods for classifying propositions in online argumentative discourse. The first of these studies<cite> (Park and Cardie, 2014)</cite> compiled online user comments from a discussion website and developed a framework for automatically classifying each proposition as either \"unverifiable\", \"verifiable non-experiential\", or \"verifiable experiential\", where the appropriate types of support are reason, evidence, and optional evidence, respectively. The second study, Oraby et al. (2015) , uses a different online corpus (Walker et al., 2012 ) of short argumentative responses to quotes, and classifies each response as either \"factual\" or \"feeling\" according to whether the support invoked appeals to facts or to emotions. In this paper, we use the term \"claim\" loosely to refer to an individual proposition (a sentence or independent clause) in an argument, or to a short argumentative text containing one or more propositions. In classifying propositions,<cite> Park and Cardie (2014)</cite> followed previous work such as Reed et al. (2008) and Palau and Moens (2009) , employing supervised learning methods. Despite using a rich set of linguistic features, these approaches suffer from low accuracy. Moreover, generating these features can be a tedious and complex process.",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_3",
  "x": "In classifying propositions,<cite> Park and Cardie (2014)</cite> followed previous work such as Reed et al. (2008) and Palau and Moens (2009) , employing supervised learning methods. Despite using a rich set of linguistic features, these approaches suffer from low accuracy. Moreover, generating these features can be a tedious and complex process. In this paper, we show that state-of-the-art performance in claim classification for online user comments can be achieved without the need for expensive features. Our work, which employs CNN-and LSTM-based deep neural networks, is inspired by advances in sentence classification (Kim, 2014) and sequence classification (Hochreiter and Schmidhuber, 1997 ) using distributional word representations and deep learning. In particular, our approach leverages word2vec 1 distributional embeddings, dependency context-based embeddings (Levy and Goldberg, 2014) , and factuality/certainty-indicating embeddings for improving claim classification. (We refer to these embeddings as linguistic embeddings, as these are compiled from linguistic annotations such as dependency relations, verb modalities, and actuality information.) In this paper, we separately evaluate the usefulness of word and linguistic embeddings in the claim classification task on both the aforementioned data sets.",
  "y": "differences motivation"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_4",
  "x": "Dependency-based Embeddings. Claims containing multiple clauses or propositions might be better distinguished with the help of dependency embeddings inferred from the respective proposition contexts. Consider the following claim from one of our data sets: \"The Governor said that he enjoyed it.\" In this claim, the main clause, \"The Governor said\", is the core proposition, which excludes consideration of the remainder. The reason is that \"said\" is a reporting predicate, so it is unnecessary to verify whether or not the governor really has enjoyed the object mentioned in the subordinate clause. In some other claims, it is the subordinate rather than the main clause predicate that decides the claim type. Park and Cardie (2014) extracted clause-specific features using the Stanford syntactic parser and the Penn Treebank. (Merely using clause tags without capturing dependencies for important clauses may not help much in distinguishing objective verifiable claims from unverifiable subjective ones.)<cite> Park and Cardie (2014)</cite> also used tense and person counts for distinguishing verifiable claims from unverifiable claims.",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_5",
  "x": "Claims containing multiple clauses or propositions might be better distinguished with the help of dependency embeddings inferred from the respective proposition contexts. Consider the following claim from one of our data sets: \"The Governor said that he enjoyed it.\" In this claim, the main clause, \"The Governor said\", is the core proposition, which excludes consideration of the remainder. The reason is that \"said\" is a reporting predicate, so it is unnecessary to verify whether or not the governor really has enjoyed the object mentioned in the subordinate clause. In some other claims, it is the subordinate rather than the main clause predicate that decides the claim type. Park and Cardie (2014) extracted clause-specific features using the Stanford syntactic parser and the Penn Treebank. (Merely using clause tags without capturing dependencies for important clauses may not help much in distinguishing objective verifiable claims from unverifiable subjective ones.)<cite> Park and Cardie (2014)</cite> also used tense and person counts for distinguishing verifiable claims from unverifiable claims. We hypothesize that word2vec and dependency context-based embeddings can inherently capture these linguistic characteristics and can replace these features.",
  "y": "extends"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_6",
  "x": "We investigate the use of certainty-and factualityrelated distributed signals for distinguishing claims. In online argumentative discourse, claims often serve as implicit arguments with inappropriate or missing justification<cite> (Park and Cardie, 2014)</cite> . The certainty and factuality signals present in such claims may be appropriate for determining its factuality or verifiability. As the claims in our data set are objective, subjective and factual types, predicates, adverbs and other modals (related to certainty and factuality) present in FactBank 1.0 may help in better distinguishing various types of claims. As an example, consider the sentence in Figure 2 , a complex claim of type \"verifiable non-experiential\". The predicate \"seems\" and the modal verb \"must\" can be viewed as certainty and factuality information related to the speaker's commitment to their utterance. Factual embeddings of these co-occurrence indicators can help in better identifying the type of the claim.",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_7",
  "x": "We also use a development set to tune the hyper-parameters of the model. (Park and Cardie, 2014) . This corpus consists of 9476 manually annotated sentences and independent clauses from 1047 user comments extracted from the Regulation Room website. 2<cite> Park and Cardie (2014)</cite> and Park et al. (2015) used this corpus for examining each proposition with respect to its verifiability to determine the desirable types of support for the analysis of arguments. The propositions are manually annotated with three classes-\"verifiable experiential\", \"verifiable non-experiential\", and \"unverifiable\"-where the support types are evidence, optional evidence, and reason, respectively. The annotation distribution and our train/test splits are shown in Table 2 . ----------------------------------",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_8",
  "x": "It consists of quote-response pairs that are manually annotated according to whether the response is primarily a \"factual\"-or \"feeling\"-based argument. In our experiments, we use the training and test splits from Oraby et al. (2015) ; these consist of claims that can span multiple sentences. The annotation distribution for these splits is shown in Table 1 . We also use a development set to tune the hyper-parameters of the model. (Park and Cardie, 2014) . This corpus consists of 9476 manually annotated sentences and independent clauses from 1047 user comments extracted from the Regulation Room website. 2<cite> Park and Cardie (2014)</cite> and Park et al. (2015) used this corpus for examining each proposition with respect to its verifiability to determine the desirable types of support for the analysis of arguments.",
  "y": "similarities"
 },
 {
  "id": "8c530e0c9f7256ac44b1a2adfaf6a9_0",
  "x": "However, identification of emotions from textual conversations is a challenging problem due to absence of above factors. Emotions in text are not only identified by its cue words such as happy, good, bore, hurt, hate and fun, but also the presence of interjections (e.g. \"whoops\"), emoticons (e.g. \":)\"), idiomatic expressions (e.g. \"am in cloud nine\"), metaphors (e.g. \"sending clouds\") and other descriptors mark the existence of emotions in the conversational text. Recently, the growth of text messaging applications for communications require emotion detection from conversation transcripts. This helps conversational agents, chat bots and messengers to avoid emotional cues and miscommunications by detecting the emotions during conversation. EmoContext@SemEval2019 shared task (Chatterjee et al., 2019) goal is to encourage more research in the field of contextual emotion detection in textual conversations. The shared task focuses on identifying emotions namely Angry, Happy, Sad and Others from conversation with three turns. Since, emotion detection is a classification problem, research works have been carried out by using machine learning with lexical features (Sharma et al., 2017) and deep learning with deep neural network (Phan et al., 2016) and convolutional neural network<cite> (Zahiri and Choi, 2018)</cite> to detect the emotions from text.",
  "y": "background"
 },
 {
  "id": "8c530e0c9f7256ac44b1a2adfaf6a9_1",
  "x": "The shared task focuses on identifying emotions namely Angry, Happy, Sad and Others from conversation with three turns. Since, emotion detection is a classification problem, research works have been carried out by using machine learning with lexical features (Sharma et al., 2017) and deep learning with deep neural network (Phan et al., 2016) and convolutional neural network<cite> (Zahiri and Choi, 2018)</cite> to detect the emotions from text. However, we have adopted Seq2Seq deep neural network for detecting the emotions from textual conversations which include sequence of phrases. This paper elaborates our Seq2Seq approach for identifying emotions from text sequences. ---------------------------------- **RELATED WORK** This section reviews the research work reported for emotion detection from text / tweets (Perikos and Hatzilygeroudis, 2013; Rao, 2016; AbdulMageed and Ungar, 2017; Samy et al., 2018; AlBalooshi et al., 2018; Gaind et al., 2019 ) and text conversations (Phan et al., 2016; Sharma et al., 2017;<cite> Zahiri and Choi, 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "8c530e0c9f7256ac44b1a2adfaf6a9_2",
  "x": "All the models discussed above considered the fine-grained emotion categories and used the twitter data to create a manually annotated corpus. These models used the rule-based or machine learning based algorithms to classify the emotion category. A new C-GRU (Context-aware Gated Recurrent Units) a variant of LSTM was proposed by Samy et al. (2018) which extracts the contextual information (topics) from tweets and uses them as an extra layer to determine sentiments conveyed by the tweet. The topic vectors resembling an image are fed to CNN to learn the contextual information. Abdul-Mageed and Ungar (2017) built a very large dataset with 24 fine-grained types of emotions and classified the emotions using gated RNN. Instead of using basic CNN, a new recurrent sequential CNN is used by <cite>Zahiri and Choi (2018)</cite> . They proposed several sequence-based convolution neural network (SCNN) models with attention to facilitate sequential dependencies among utterances.",
  "y": "background"
 },
 {
  "id": "8c530e0c9f7256ac44b1a2adfaf6a9_3",
  "x": "Instead of using basic CNN, a new recurrent sequential CNN is used by <cite>Zahiri and Choi (2018)</cite> . They proposed several sequence-based convolution neural network (SCNN) models with attention to facilitate sequential dependencies among utterances. All the models discussed above show that the emotion prediction can be handled using variants of deep neural network such as C-GRU, G-RNN and Sequential-CNN. The commonality between the above models are the variations of RNN or LSTM. This motivated us to use the Sequenceto-Sequence (Seq2Seq) model which consists of stacked LSTMs to predic the emotion labels conditioned on the given utterance sequences. ---------------------------------- **DATA AND PREPROCESSING**",
  "y": "motivation background"
 },
 {
  "id": "8dbc779d455ad72def6654564f9e13_0",
  "x": "This transcription bottleneck problem can be handled by translating into a widely spoken language to ensure subsequent interpretability of the collected recordings, and such parallel corpora have been recently created by aligning the collected audio with translations in a well-resourced language (Adda et al., 2016; Godard et al., 2017; Boito et al., 2018) . Moreover, some linguists suggested that more than one translation should be collected to capture deeper layers of meaning (Evans and Sasse, 2004) . This work is a contribution to the Computational Language Documentation (CLD) research field, that aims to replace part of the manual steps performed by linguists during language documentation initiatives by automatic approaches. Here we investigate the unsupervised word discovery and segmentation task, using the bilingual-rooted approach from <cite>Godard et al. (2018)</cite> . There, words in the well-resourced language are aligned to unsegmented phonemes in the endangered language in order to identify group of phonemes, and to cluster them into word-like units. We experiment with the Mboshi-French parallel corpus, translating the French text into four other well-resourced languages in order to investigate language impact in this CLD approach. Our results hint that this language impact exists, and that models based on different languages will output different word-like units.",
  "y": "uses"
 },
 {
  "id": "8dbc779d455ad72def6654564f9e13_1",
  "x": "The Multilingual Mboshi Parallel Corpus: In this work we extend the bilingual Mboshi-French parallel corpus (Godard et al., 2017) , fruit of the documentation process of Mboshi (Bantu C25), an endangered language spoken in Congo-Brazzaville. The corpus contains 5,130 utterances, for which it provides audio, transcriptions and translations in French. We translate the French into four other well-resourced languages through the use of the DeepL translator. 1 The languages added to the dataset are: English, German, Portuguese and Spanish. Table 1 shows some statistics for the produced Multilingual Mboshi parallel corpus. 2 Bilingual Unsupervised Word Segmentation/Discovery Approach: We use the bilingual neuralbased Unsupervised Word Segmentation (UWS) approach from <cite>Godard et al. (2018)</cite> to discover words in Mboshi. In this approach, Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target, the language to document (unsegmented phonemic sequence).",
  "y": "uses"
 },
 {
  "id": "8dbc779d455ad72def6654564f9e13_2",
  "x": "These matrices give us sentence-level source-to-target alignment information, and by using it for clustering neighbor phonemes aligned to the same translation word, we are able to create segmentation in the target side. The product of this approach is a set of (discovered-units, translation words) pairs. Multilingual Leveraging: In this work we apply two simple methods for including multilingual information into the bilingual models from <cite>Godard et al. (2018)</cite> . The first one, Multilingual Voting, consists of merging the information learned by models trained with different language pairs by performing a voting over the final discovered boundaries. The voting is performed by applying an agreement threshold T over the output boundaries. This threshold balances between accepting all boundaries from all the bilingual models (zero agreement) and accepting only input boundaries discovered by all these models (total agreement). The second method is ANE Selection.",
  "y": "uses"
 },
 {
  "id": "8e0dcaec15a3b9c4947946a4e885c8_0",
  "x": "Although, to address the loss of information when using AV, Singular Value Decomposition has been used as a dimensionality reduction technique to factorize the embeddings into a lower-rank approximation of the concatenated meta-embedding set. Linear methods include the use of a projection layer for meta-embedding (known as 1TON) Yin and Sch\u00fctze (2015) , which is simply trained using an 2 -based loss. Similarly, Bollegala et al. (Bollegala et al., 2017) has focused on finding a linear transformation between count-based and prediction-based embeddings, showing that linearly transformed count-based embeddings can be used for predictions in the localized neighborhoods in the target space. Most recent work<cite> (Bao and Bollegala, 2018</cite> ) has focused on the use of an autoencoder (AE) to encode a set of N pretrained embeddings using 3 different variants: (1) Decoupled Autoencoded Meta Embeddings (DAEME) that keep activations separated for each respective embedding input during encoding and uses a reconstruction loss for both predicted embeddings while minimizing the loss for each respective decoded output, (2) Coupled Autoencoded Meta Embeddings (CAEME) which instead learn to predict from a shared encoding and (3) Averaged Autoencoded Meta-Embedding (AAME) is simply an averaging of the embedding set as input instead of using a concatenation. This is the most relevant work to our paper, hence, we include these 3 autoencoding schemes along with aforementioned methods for experiments, described in Section 3. We also include two subtle variations of the aforementioned arXiv:1808.04334v1 [cs.CL] 13 Aug 2018 AEs.",
  "y": "background"
 },
 {
  "id": "8e0dcaec15a3b9c4947946a4e885c8_1",
  "x": "We demonstrated this across multiple benchmark datasets. ---------------------------------- **METHODOLOGY** Before describing the loss functions used, we explain the aforementioned variation on the autoencoding method and how it slightly differs from 1TON/1TON + (Yin and Sch\u00fctze, 2015) and standard AEs<cite> (Bao and Bollegala, 2018)</cite> presented in previous work. Target Autoencoders (TAE) are defined as learning an ensemble of nonlinear transformations between sets of bases X s in sets of vector spaces X S = {X 1 , .., X s , .., X N } s.t X s \u2208 R |vs|\u00d7ds to a target space X t \u2208 R |vt|\u00d7dt , where f \u2192 X t \u2200i is the nonlinear transformation function used to make the mapping. Once a set of M number of parameteric models f w = {f",
  "y": "differences uses"
 },
 {
  "id": "8e0dcaec15a3b9c4947946a4e885c8_2",
  "x": "w } are trained with various objective functions to learn the mappings between the word vectors we obtain a set of lowerdimensional target latent representation that represents different combinations of mappings from one vector space to another. After training, all H set of latent variables Z s = {z 1 , .., z H } are concatenated with an autoencoded target vector. This means thats all vector spaces have been mapped to a target space and there hidden meta-word representations have been averaged, as illustrated in Figure 1 . Figure 2 shows a comparison of the previous autoencoder approaches<cite> (Bao and Bollegala, 2018)</cite> (left) and the alternative AE (right), where dashed lines indicate connections during training and bold lines indicate prediction. The ConcatAutoEncoder (CAEME) simply concatenates the embedding set into a single vector and trains the autoencoder so to produce a lower-dimensional representation (shown in red), while the decoupled autoencoder (DAEME) keeps the embedding vectors separate in the encoding. In contrast the target encoder (TAE) is similar to that of CAEME only the label is a single embedding from the embedding set and the input are remaining embeddings from the set. After training, TAE then concatenates the hidden layer encoding with the original target vector.",
  "y": "background uses"
 },
 {
  "id": "8e0dcaec15a3b9c4947946a4e885c8_3",
  "x": "The Mean Target AutoEncoder (MTE) instead performs an averaging over separate autoencoded representation. Weights are initialized with a normal distribution, mean \u00b5 = 0 and standard deviation \u03c3 = 1. Dropout is used with a dropout rate p = 0.2 for all datasets. The model takes all unique vocabulary terms pertaining to all tested word association and word similarity datasets (n = 4819) and performs Stochastic Gradient Descent (SGD) with batch sizex = 32 trained between 50 epochs for each dataset \u2200d \u2208 D. This results in a set of vectors X j \u2208 R |v j |\u00d7200 \u2200j that are then used for finding the similarity between word pairs. The above parameters were chosen (h d ,x and number of epochs) over a small grid search. As stated, we compare against previous methods (Yin and Sch\u00fctze, 2015;<cite> Bao and Bollegala, 2018</cite> ) that use 2 distance, as shown in Equation 1). Similarly, the Mean Absolute Error ( 1 norm of difference) loss 1 N N i=1 |y \u2212\u0177| is tested.",
  "y": "uses"
 },
 {
  "id": "8e0dcaec15a3b9c4947946a4e885c8_4",
  "x": "The following word association and word similarity datasets are used throughout experimentation: Simlex (Hill et al., 2015) , WordSim-353 (Finkelstein et al., 2001) , RG (Rubenstein and Goodenough, 1965) , MTurk (MechanicalTurk-771) (Halawi et al., 2012) , RareWord (RW) (Luong et al., 2014) and MEN (Bruni et al., 2012) . The word vectors considered in the embeddings set are skipgram and cbow (Mikolov et al., 2013) , FastText (Bojanowski et al., 2016) , LexVec (Salle et al., 2016) , Hellinger PCA (HPCA) (Lebret and Collobert, 2013) and Hierarchical Document Context (HDC) (Sun et al., 2015) . We now report results on the performance of meta-embedding autoencodings with various loss functions, while also presenting target autoencoders for combinations of word embeddings and compare against existing current SoTA meta-embeddings. Table 1 shows the scaled Spearman correlation test scores, where (1) shows the original single embeddings, (2) results for standard metaembedding approaches that either apply a single mathematical operation or employ a linear projection as an encoding, (3) presents the results using autoencoder schemes by<cite> (Bao and Bollegala, 2018</cite> ) that we have used to test the various losses, (4) introduces TAE without concatenating the target Y embedding post-training with MSE loss and (5) shows the results of concatenating Y with the lower-dimensional (200-dimensions) vector that encodes all embeddings apart from the target vector. Therefore reported results from (4) are of a 200d vector, while (5) concatenates the vector leading to a vector between 300-500 dimensions dependent on the target vector. All trained encodings from sections 3-5 are 200-dimensional vectors. Results in red shading indicate the best performing meta-embedding for all presented approaches, while black shading indicates the best performing meta-embedding for the respective section.",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_0",
  "x": "**INTRODUCTION** Spoken conversation summarization is an important task, since speech is the primary medium of human-human communication. Vast amounts of spoken conversation data are produced daily in call-centers. Due to this overwhelming number of conversations, call-centers can only evaluate a small percentage of the incoming calls . Automatic methods of conversation summarization have a potential to increase the capacity of the call-centers to analyze and assess their work. Earlier works on conversation summarization have mainly focused on extractive techniques. However, as pointed out in (Murray et al., 2010) and<cite> (Oya et al., 2014)</cite> , abstractive summaries are preferred to extractive ones by human judges.",
  "y": "motivation"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_1",
  "x": "Due to this overwhelming number of conversations, call-centers can only evaluate a small percentage of the incoming calls . Automatic methods of conversation summarization have a potential to increase the capacity of the call-centers to analyze and assess their work. Earlier works on conversation summarization have mainly focused on extractive techniques. However, as pointed out in (Murray et al., 2010) and<cite> (Oya et al., 2014)</cite> , abstractive summaries are preferred to extractive ones by human judges. The possible reason for this is that extractive techniques are not well suited for the conversation summarization, since there are style differences between spoken conversations and humanauthored summaries. Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (Mehdad et al., 2013; <cite>Oya et al., 2014)</cite> . The authors cluster conversation sentences/utterances into communities to identify most relevant ones and aggregate them using word-graph models.",
  "y": "background"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_2",
  "x": "In this paper we evaluate a set of heuristics for automatic linking of summary and conversations sentences, i.e. 'community' creation. The heuristics rely on the similarity between the two, and we experiment with the cosine similarity computation on different levels of representation -raw text, text after replacing the verbs with their WordNet SynSet IDs, and the similarity computed using distributed word embeddings. The heuristics are evaluated within the template-based abstractive summarization system of<cite> Oya et al. (2014)</cite> . We extend this system to Italian using required NLP tools. However, the approach transparently extends to other languages with available WordNet, minimal supervised summarization corpus and running text. Heuristics are evaluated and compared on AMI meeting corpus and Italian LUNA Human-Human conversation corpus. The overall description of the system with the more detailed description of the heuristics is provided in Section 2.",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_3",
  "x": "In this section we describe the conversation summarization pipeline that is partitioned into community creation, template generation, ranker training, and summary generation components. The whole pipeline is depicted in Figure 1 . ---------------------------------- **TEMPLATE GENERATION** Template Generation follows the approach of<cite> (Oya et al., 2014)</cite> and, starting from human-authored summaries, produces abstract templates applying slot labeling, summary clustering and template fusion steps. The information required for the template generation are part-of-speech (POS) tags, noun and verb phrase chunks, and root verbs from dependency parsing. For English, we use Illinois Chunker (Punyakanok and Roth, 2001) to identify noun phrases and extract part-of-speech tags; and the the tool of (De Marneffe et al., 2006) for generating dependency parses.",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_4",
  "x": "The clustered templates are further generalized using a word graph algorithm extended to templates in<cite> (Oya et al., 2014)</cite> . The paths in the word graph are ranked using language models trained on the abstract templates and the top 10 are selected as a template for the cluster. ---------------------------------- **COMMUNITY CREATION** In the AMI Corpus, sentences in human-authored summaries are manually linked to a set of the sentences/utterances in the meeting transcripts, referred to as communities. It is hypothesized that a community sentence covers a single topic and conveys vital information about the conversation segment. For the automatic community creation we explore four heuristics.",
  "y": "extends"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_5",
  "x": "**SENTENCE RANKING** Since the system produces many sentences that might repeat the same information, the final set of automatic sentences is selected from these filled templates with respect to the ranking using the token and part-of-speech tag 3-gram language models. In this paper, different from<cite> (Oya et al., 2014)</cite> , the sentence ranking is based solely on the n-gram language models trained on the tokens and part-ofspeech tags from the human-authored summaries. ---------------------------------- **EXPERIMENTS AND RESULTS** We evaluate the automatic community creation heuristics on the AMI meeting corpus (Carletta et al., 2006) and Italian and English LUNA Human-Human corpora (Dinarelli et al., 2009 ). ----------------------------------",
  "y": "differences"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_6",
  "x": "**EXPERIMENTS AND RESULTS** We evaluate the automatic community creation heuristics on the AMI meeting corpus (Carletta et al., 2006) and Italian and English LUNA Human-Human corpora (Dinarelli et al., 2009 ). ---------------------------------- **DATA SETS** The two corpora used for the evaluation of the heuristics are AMI and LUNA. The AMI meeting corpus (Carletta et al., 2006 ) is a collection of 139 meeting records where groups of people are engaged in a 'roleplay' as a team and each speaker assumes a certain role in a team (e.g. project manager (PM)). Following<cite> (Oya et al., 2014)</cite> , we removed 20 dialogs used by the authors for development, and use the remaining dialogs for the threefold cross-validation.",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_7",
  "x": "**EVALUATION** ROUGE-2 metric (Lin, 2004 ) is used for the evaluation. The metric considers bigram-level precision, recall and F-measure between a set of reference and hypothesis summaries. For AMI corpus, following<cite> (Oya et al., 2014)</cite> , we report ROUGE-2 F-measures on 3-fold cross-validation. For LUNA Corpus, on the other hand, we have used the modified version of ROUGE 1.5.5 toolkit from the CCCS Shared Task , which was adapted to deal with a conversation-dependent length limit of 7%. Unlike the AMI Corpus, the official reported results for the CCCS Shared Task were recall; thus, for LUNA Corpus the reported values are ROUGE-2 recall. For statistical significance testing, we use a paired bootstrap resampling method proposed in (Koehn, 2004) .",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_8",
  "x": "**RESULTS** In this section we report on the results of the abstractive summarization system using the community creation heuristics described in Section 2. Following the Call-Center Conversation Summarization Shared Task at MultiLing 2015 , for LUNA Corpus (Dinarelli et al., 2009) we compare performances to three extractive baselines: (1) the longest turn in the conversation up to the length limit (7% of a conversation) (Baseline-L), (2) the longest turn in the first 25% of the conversation up to the length limit (Baseline-LB) (Trione, 2014) , and (3) Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998 ) with \u03bb = 0.7. For AMI corpus, on the other hand, we compare performances to the abstractive systems reported in<cite> (Oya et al., 2014)</cite> . The performances of the heuristics on AMI corpus are given in Table 1 . In the table we also report the performances of the previously published summarization systems that make use of the manual communities -<cite> (Oya et al., 2014)</cite> and (Mehdad et al., 2013) ; and our run of the system of<cite> (Oya et al., 2014)</cite> . With manual communities we have Table 2 : ROUGE-2 recall with 7% summary length limit for the extractive baselines and abstractive summarization systems with the community creation heuristics on LUNA corpus.",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_9",
  "x": "Following the Call-Center Conversation Summarization Shared Task at MultiLing 2015 , for LUNA Corpus (Dinarelli et al., 2009) we compare performances to three extractive baselines: (1) the longest turn in the conversation up to the length limit (7% of a conversation) (Baseline-L), (2) the longest turn in the first 25% of the conversation up to the length limit (Baseline-LB) (Trione, 2014) , and (3) Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998 ) with \u03bb = 0.7. For AMI corpus, on the other hand, we compare performances to the abstractive systems reported in<cite> (Oya et al., 2014)</cite> . The performances of the heuristics on AMI corpus are given in Table 1 . In the table we also report the performances of the previously published summarization systems that make use of the manual communities -<cite> (Oya et al., 2014)</cite> and (Mehdad et al., 2013) ; and our run of the system of<cite> (Oya et al., 2014)</cite> . With manual communities we have Table 2 : ROUGE-2 recall with 7% summary length limit for the extractive baselines and abstractive summarization systems with the community creation heuristics on LUNA corpus. obtained average F-measure of 0.072. From the table, we can observe that all the systems with automatic community creation heuristics and the simplified sentence ranking described in Section 2 outperform the systems with manual communities.",
  "y": "uses"
 },
 {
  "id": "8f0aab7fd30ffc56cc477b25e6bb16_0",
  "x": "We will cover the basics of Frame Semantics, explain how the database was created, introduce the Python API and the state of the art in automatic frame semantic role labeling systems; and we will discuss FrameNet collaboration with commercial partners. Time permitting, we will present new research on frames and annotation of locative relations, as well as corresponding metaphorical uses, along with information about how frame semantic roles can aid the interpretation of metaphors. \u2022 Introduction ---------------------------------- **** \u2022 Introduction Michael Ellsworth (International Computer Science Institute, infinity@icsi.berkeley.edu) has been involved with FrameNet for well over a decade. His chief focus is on semantic relations in FrameNet (Ruppenhofer et al. 2006) , how they can be used for paraphrase <cite>(Ellsworth & Janin 2007)</cite> , and mapping to other resources (Sche\u21b5czyk & Ellsworth 2006; Ferr\u00e1ndez et al. 2010b) .",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_0",
  "x": "Top row: baseline approach by Zellers et al. <cite>[18]</cite> . Bottom row: our approach. Q:Question, Ac:Correct Answer, Ap: Predicted Answer, R: Predicted Rationale, Q\u2212 > AR: Both answer and rationale prediction given question. well beyond such trivial recognition tasks. By just looking at an image, we are able to deduce many things -contexts, situations, mental states of actors and many more things. Such a higher order of intellect is termed as cognition. Cognition is extremely important and relevant. For example, a higher cognitive ability will help social robots to interact seamlessly with humans.",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_1",
  "x": "Ability to judge and comprehend mental states of humans will be invaluable to healthcare robots. Additionally, being able to solve this challenging task will help the vision community as a whole to move to the next generation of vision systems which goes beyond normal recognition. The main goal of Visual Commonsense Reasoning (VCR) is to solve this cognition task. Precisely, the task is introduced and formulated in <cite>[18]</cite> as follows: given an image and a question related to the image, the model has to predict the correct answer from four possible choices and at the same time, it has to pick the right rationale, again from four options. As the task is new, Zellers et al. <cite>[18]</cite> provided a new baseline for it which seeks to tackle the task of predicting answers and predicting rationales separately. At first, Figure 2 . The Visual Commonsense Reasoning task, Zellers et al. <cite>[18]</cite> answers are predicted given the question and image and then, rationales are predicted given the image and question with the correct answer (see figure 1 and 2).",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_2",
  "x": "Ability to judge and comprehend mental states of humans will be invaluable to healthcare robots. Additionally, being able to solve this challenging task will help the vision community as a whole to move to the next generation of vision systems which goes beyond normal recognition. The main goal of Visual Commonsense Reasoning (VCR) is to solve this cognition task. Precisely, the task is introduced and formulated in <cite>[18]</cite> as follows: given an image and a question related to the image, the model has to predict the correct answer from four possible choices and at the same time, it has to pick the right rationale, again from four options. As the task is new, Zellers et al. <cite>[18]</cite> provided a new baseline for it which seeks to tackle the task of predicting answers and predicting rationales separately. At first, Figure 2 . The Visual Commonsense Reasoning task, Zellers et al. <cite>[18]</cite> answers are predicted given the question and image and then, rationales are predicted given the image and question with the correct answer (see figure 1 and 2).",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_3",
  "x": "For example, a higher cognitive ability will help social robots to interact seamlessly with humans. Ability to judge and comprehend mental states of humans will be invaluable to healthcare robots. Additionally, being able to solve this challenging task will help the vision community as a whole to move to the next generation of vision systems which goes beyond normal recognition. The main goal of Visual Commonsense Reasoning (VCR) is to solve this cognition task. Precisely, the task is introduced and formulated in <cite>[18]</cite> as follows: given an image and a question related to the image, the model has to predict the correct answer from four possible choices and at the same time, it has to pick the right rationale, again from four options. As the task is new, Zellers et al. <cite>[18]</cite> provided a new baseline for it which seeks to tackle the task of predicting answers and predicting rationales separately. At first, Figure 2 .",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_4",
  "x": "By adopting these methods, we can get rid of the assumption that answer prediction network has to be 100% accurate. Concurrently, it makes our approaches incomparable to the baselines provided by Zellers et al. <cite>[18]</cite> . It is so because they always condition their rationale prediction module on correct answer and as such it is bound to be better than a model which conditions on predicted answer. With this in mind, we propose new baselines in which we train the rationale prediction network by conditioning on correct answer 75% of the time and random answers for the rest 25%. Correct answers are provided 75% of the time to keep a safe estimate of state-of-the-art VQA model. We demonstrate through experiments that our proposed approaches are able to learn the correct answers for the correct reasons. Even though our models are not provided with correct answer for rationale prediction, they still perform competitively to the state-of-the-art VCR model.",
  "y": "differences"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_5",
  "x": "The task in Zellers et al. <cite>[18]</cite> is essentially posed as a question-answering task. Although they have enforced reasoning for the network, the reasoning is still in the questionanswer format. As such, it makes sense to explore current work in visual question answer domain. A common approach in VQA (Visual Question Answering) is to encode the question and the images into representative vectors, combine the meaning of both vectors using [2, 9, 6, 10, 3] and use a MLP (Multi-layer Perceptron) with softmax for answer prediction. Agrawal et al. [2] use a two layer LSTM [8] to encode the question and the last layer of VGGNet [17] to encode the image. To normalize image features, l2 norm is used. The image and question features are then fused via element wise multiplication.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_6",
  "x": "Anderson et al. [1] propose an orthogonal work to <cite>[18]</cite> , in which Faster-RCNN [15] is used to predict the image regions the model should attend to. We note the difference from our current proposed work -annotations are provided in the VCR 1.0 dataset <cite>[18]</cite> in form of bounding box and segmentation maps. Akira et al. [6] propose a more sophisticated method to combine the feature vectors from questions and images. After extracting features from questions and images using LSTM and CNN respectively, they use bi-linear pooling (outer vector product) to encode the interplay between image and question representations. This method is much more expressive, but it is also very computationally expensive as outer vector product increases the parameters exponentially. To tackle this, they reduce the full outer vector product to tractable operations using FFT and convolutions. While the method was more sophisticated than naive multiplication [2] or concatenation [9] , it still makes use of some critical assumptions which limits its ability to fully capture the expressive power of outer vector product.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_7",
  "x": "We note the difference from our current proposed work -annotations are provided in the VCR 1.0 dataset <cite>[18]</cite> in form of bounding box and segmentation maps. Akira et al. [6] propose a more sophisticated method to combine the feature vectors from questions and images. After extracting features from questions and images using LSTM and CNN respectively, they use bi-linear pooling (outer vector product) to encode the interplay between image and question representations. This method is much more expressive, but it is also very computationally expensive as outer vector product increases the parameters exponentially. To tackle this, they reduce the full outer vector product to tractable operations using FFT and convolutions. While the method was more sophisticated than naive multiplication [2] or concatenation [9] , it still makes use of some critical assumptions which limits its ability to fully capture the expressive power of outer vector product. Kim et al. [10] improves upon [6] by improving the outer vector product computation using low-rank bi-linear pooling utilizing Hadamard product (elementwise computation).",
  "y": "differences"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_8",
  "x": "To tackle this, they reduce the full outer vector product to tractable operations using FFT and convolutions. While the method was more sophisticated than naive multiplication [2] or concatenation [9] , it still makes use of some critical assumptions which limits its ability to fully capture the expressive power of outer vector product. Kim et al. [10] improves upon [6] by improving the outer vector product computation using low-rank bi-linear pooling utilizing Hadamard product (elementwise computation). Benyounes et al. [3] further improve upon [6] by using Tucker Decomposition of image/question correlation tensor which is able to represent full bi-linear interactions while maintaining the size of the model tractable. Zellers et al. <cite>[18]</cite> propose to jointly learn language and image representation using Bi-LSTM by feeding in image features from CNN for all annotated words. They call this step grounding. Further, query and responses is contextualized using attention mechanism.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_9",
  "x": "They call this step grounding. Further, query and responses is contextualized using attention mechanism. Finally, the attended query, attended image and response is passed through a Bi-LSTM to make final predictions. One major drawback of the work is that separate networks are trained to predict answers and to reason. We seek to build on <cite>[18]</cite> by proposing a method to jointly train prediction and reasoning networks. We first choose the correct answer based on the predicted probability distribution using image and question. Then, a combined representation of image, question and chosen answer is fed to reasoning network to select the correct reason.",
  "y": "extends"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_10",
  "x": "One, using just a softmax weighted representation of all answers. Two, sampling an answer based on the softmax probability distribution. The network is made differentiable using an expected loss as defined in [16] . We use the same backbone architecture to predict answers and rationales as in <cite>[18]</cite> . The questions and response (answers and rationales) are provided as a combination of natural language words and tags for annotated objects in the image. Word embedding for question q and response r are calculated using BERT [5] . Image features for annotated objects are calculated using ResNet50 [7] .",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_11",
  "x": "Word embedding for question q and response r are calculated using BERT [5] . Image features for annotated objects are calculated using ResNet50 [7] . The word embedding and image features for tagged objects in the question and response are fed to a bi-directional LSTM [8] . This learns a joint language-visual representation vector. <cite>[18]</cite> call this step Grounding. Next, the response vector is contextualized against the question vector using attention mechanism. In this step an attended question representation is found for every token in the response.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_12",
  "x": "---------------------------------- **EXPERIMENTAL DETAILS** ---------------------------------- **DATASETS** The dataset used in all experiments in this work is VCR 1.0 <cite>[18]</cite> . VCR dataset contains 290k multiple choice questions which has been collected from 110k movie scenes. The dataset provides object annotations, labels and classes for all objects in the image.",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_13",
  "x": "We train the model using the whole VCR dataset for 20 epochs as was done in <cite>[18]</cite> to align with the baselines. We also use gradient clipping while training. We use two losses for all our methods -answer prediction loss and rationale prediction loss, corresponding to each module. All our results have been reported on the validation set of the dataset as test set labels are not available since it's an ongoing challenge. We report test set results only for our best model which was submitted to the leaderboard. We compare only this model with state-of-the-art. We define certain terms which we are going to use henceforth: Q->A -answer prediction network, given question and image, QA->R -rationale prediction network, given question, image and answer, Q->AR -rnswer and rationale both prediction network, given question and image.",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_14",
  "x": "---------------------------------- **BASELINES** As mentioned earlier, our approach is not directly comparable to the baseline provided by Zellers et al. <cite>[18]</cite> since they feed the correct answer to the rationale module while we feed in the predicted answer. As such, we generate new baseline model. State-of-the-art VQA model have at max 75% accuracy, Kim et al. [11] . Consequently, it's a reasonable assumption that the answer prediction module can predict the correct answer 75% time. Keeping this in mind we make our baseline wherein we feed to the rationale module original question appended by the correct answer 75% time and random answer 25% time.",
  "y": "differences"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_15",
  "x": "We leave the answer prediction module as is. Finally, we train the two networks separately and combine the results of answer prediction module and rationale prediction module using \"AND\" operation as was done by Zellers et al. <cite>[18]</cite> . For completeness, we also mention the results of four other baselines from <cite>[18]</cite> . These baseline methods use the ResNet-50 (same as <cite>[18]</cite> ) visual architecture and Glove as text representations. These baselines are as follows: \u2022 RevisitedVQA [9] : This is a version of VQA model which is mainly optimized for response like 'yes' and 'no'. Basically, it takes a query, response, and image features as inputs and trains by passing the result through MLP layer.",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_16",
  "x": "State-of-the-art VQA model have at max 75% accuracy, Kim et al. [11] . Consequently, it's a reasonable assumption that the answer prediction module can predict the correct answer 75% time. Keeping this in mind we make our baseline wherein we feed to the rationale module original question appended by the correct answer 75% time and random answer 25% time. We leave the answer prediction module as is. Finally, we train the two networks separately and combine the results of answer prediction module and rationale prediction module using \"AND\" operation as was done by Zellers et al. <cite>[18]</cite> . For completeness, we also mention the results of four other baselines from <cite>[18]</cite> . These baseline methods use the ResNet-50 (same as <cite>[18]</cite> ) visual architecture and Glove as text representations.",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_17",
  "x": "Consequently, it's a reasonable assumption that the answer prediction module can predict the correct answer 75% time. Keeping this in mind we make our baseline wherein we feed to the rationale module original question appended by the correct answer 75% time and random answer 25% time. We leave the answer prediction module as is. Finally, we train the two networks separately and combine the results of answer prediction module and rationale prediction module using \"AND\" operation as was done by Zellers et al. <cite>[18]</cite> . For completeness, we also mention the results of four other baselines from <cite>[18]</cite> . These baseline methods use the ResNet-50 (same as <cite>[18]</cite> ) visual architecture and Glove as text representations. These baselines are as follows:",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_18",
  "x": "For completeness, we also mention the results of four other baselines from <cite>[18]</cite> . These baseline methods use the ResNet-50 (same as <cite>[18]</cite> ) visual architecture and Glove as text representations. These baselines are as follows: \u2022 RevisitedVQA [9] : This is a version of VQA model which is mainly optimized for response like 'yes' and 'no'. Basically, it takes a query, response, and image features as inputs and trains by passing the result through MLP layer. \u2022 Bottom-up and Top-down attention (BottomUpTop-Down) [1] : <cite>[18]</cite> adopted this model as another baseline by passing object regions referenced by the query and response. The main model attends over region proposals given by an object detector.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_19",
  "x": "Softmax: Our Softmax approach performs well on Q->A task and acheives best result on QA->R task among the four approaches we tried. It is to be noted that for the QA->R task, we don't provide the correct answer as input to the model. Rather, a weighted average of answers (according to probabilities predicted by Q->A module) is provided to the rationale prediction module, unlike the baseline <cite>[18]</cite> , which gives the correct answer as input. Gumbel-Softmax: For gumbel-softmax, we anneal the temperature \u03c4 from 5 to 1 for 10 epochs and then keep it constant at 1. As can be seen from 1, this model gives the best result among all the approaches we used. Again, we provided the gumbel-softmax weighted average of answer representation to the rationale prediction module rather than the correct answer as was used in the baseline. Reinforcement Learning Sampling: Surprisingly, the RL sampling based method performed poorly as compared to softmax and gumbel-softmax based method.",
  "y": "differences"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_20",
  "x": "Comparison with State-of-the-art: We provide comparison of our best model against state-of-the-art for visual common sense reasoning task. We also summarize results of other baselines reported in <cite>[18]</cite> . As can be seen from table 3, our Gumbel-softmax method performs better than the baseline <cite>[18]</cite> in Q->A task. For the QA->R task, it is to be expected that our method should perform worse than the baseline as we are providing predicted answers, rather than the correct answer to the QA->R module. Still, our approach performs comparably against the baseline on the task. We conclude that our method learns to predict correct answers for the correct rationales. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_21",
  "x": "We conclude from this that gradient flow between the two Q->A and QA->R modules enabled by our end-to-end joint learning scheme, helps the network learn better answers for better/correct reasons. Comparison with State-of-the-art: We provide comparison of our best model against state-of-the-art for visual common sense reasoning task. We also summarize results of other baselines reported in <cite>[18]</cite> . As can be seen from table 3, our Gumbel-softmax method performs better than the baseline <cite>[18]</cite> in Q->A task. For the QA->R task, it is to be expected that our method should perform worse than the baseline as we are providing predicted answers, rather than the correct answer to the QA->R module. Still, our approach performs comparably against the baseline on the task. We conclude that our method learns to predict correct answers for the correct rationales.",
  "y": "differences"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_0",
  "x": "**ABSTRACT** We develop a grammatical error correction (GEC) system for German using a small gold GEC corpus augmented with edits extracted from Wikipedia revision history. We extend the automatic error annotation tool ERRANT (Bryant et al., 2017) for German and use it to analyze both gold GEC corrections and Wikipedia edits (Grundkiewicz and JunczysDowmunt, 2014) in order to select as additional training data Wikipedia edits containing grammatical corrections similar to those in the gold corpus. Using a multilayer convolutional encoder-decoder neural network GEC approach<cite> (Chollampatt and Ng, 2018)</cite>, we evaluate the contribution of Wikipedia edits and find that carefully selected Wikipedia edits increase performance by over 5%. ---------------------------------- **INTRODUCTION AND PREVIOUS WORK** In the past decade, there has been a great deal of research on grammatical error correction for English including a series of shared tasks, Helping Our Own in 2011 and 2012 (Dale and Kilgarriff, 2011; Dale et al., 2012) and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013 (Ng et al., , 2014 , which have contributed to the development of larger English GEC corpora.",
  "y": "uses"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_1",
  "x": "We extend the automatic error annotation tool ERRANT (Bryant et al., 2017) for German and use it to analyze both gold GEC corrections and Wikipedia edits (Grundkiewicz and JunczysDowmunt, 2014) in order to select as additional training data Wikipedia edits containing grammatical corrections similar to those in the gold corpus. Using a multilayer convolutional encoder-decoder neural network GEC approach<cite> (Chollampatt and Ng, 2018)</cite>, we evaluate the contribution of Wikipedia edits and find that carefully selected Wikipedia edits increase performance by over 5%. ---------------------------------- **INTRODUCTION AND PREVIOUS WORK** In the past decade, there has been a great deal of research on grammatical error correction for English including a series of shared tasks, Helping Our Own in 2011 and 2012 (Dale and Kilgarriff, 2011; Dale et al., 2012) and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013 (Ng et al., , 2014 , which have contributed to the development of larger English GEC corpora. On the basis of these resources along with advances in machine translation, the current state-of-the-art English GEC systems use ensembles of neural MT models<cite> (Chollampatt and Ng, 2018)</cite> and hybrid systems with both statistical and neural MT models (Grundkiewicz and Junczys-Dowmunt, 2018) . In addition to using gold GEC corpora, which are typically fairly small in the context of MTbased approaches, research in GEC has taken a number of alternate data sources into consideration such as artificially generated errors (e.g., Wagner et al., 2007; Foster and Andersen, 2009; Yuan and Felice, 2013) , crowd-sourced corrections (e.g., Mizumoto et al., 2012) , or errors from native language resources (e.g., Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014) .",
  "y": "background"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_2",
  "x": "Using a similar approach that extends existing gold GEC data with Wikipedia edits, we develop a neural machine translation grammatical error correction system for a new language, in this instance German, for which there are only small gold GEC corpora but plentiful native language resources. ---------------------------------- **DATA AND RESOURCES** The following sections describe the data and resources used in our experiments on GEC for German. We create a new GEC corpus for German along with the models needed for the neural GEC approach presented in <cite>Chollampatt and Ng (2018)</cite> . Throughout this paper we will refer to the source sentence as the original and the target sentence as the correction. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_3",
  "x": "As suggested by <cite>Chollampatt and Ng (2018)</cite> , we encode the Wikipedia article text using the BPE model and learn fastText embeddings (Bojanowski et al., 2017) with 500 dimensions. ---------------------------------- **LANGUAGE MODEL** For reranking, we train a language model on the first one billion lines (~12 billion tokens) of the deduplicated German Common Crawl corpus (Buck et al., 2014) . ---------------------------------- **METHOD** We extend the Falko-MERLIN GEC training data with sentence-level Wikipedia edits that include similar types of corrections.",
  "y": "uses"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_4",
  "x": "---------------------------------- **RESULTS AND DISCUSSION** We evaluate the effect of extending the Falko-MERLIN GEC Corpus with Wikipedia edits for a German GEC system using the multilayer convolutional encoder-decoder neural network approach from <cite>Chollampatt and Ng (2018)</cite> , using the same parameters as for English. 8 We train a single model for each condition and evaluate on the Falko-MERLIN test set using M 2 scorer (Dahlmeier and Ng, 2012 The results, presented in Table 3 , show that the addition of both unfiltered and filtered Wikipedia edits to the Falko-MERLIN GEC training data lead to improvements in performance, however larger numbers of unfiltered edits (>250K) do not consistently lead to improvements, similar to the results for English in Grundkiewicz and JunczysDowmunt (2014) . However for filtered edits, increasing the number of additional edits from 100K to 1M continues to lead to improvements, with an overall improvement of 5.2 F 0.5 for 1M edits over the baseline without additional reranking. In contrast to the results for English in <cite>Chollampatt and Ng (2018)</cite> In order to explore the possibility of developing GEC systems for languages with fewer resources, we trained models solely on Wikipedia edits, which leads to a huge drop in performance (45.22 vs. 24.37 F 0.5 ). However, the genre differences may be too large to draw solid conclusions and this approach may benefit from further work on Wikipedia edit selection, such as using a language model to exclude some Wikipedia edits that introduce (rather than correct) grammatical errors.",
  "y": "uses"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_6",
  "x": "Similarly, the process of filtering Wikipedia edits could use alternate methods in place of a gold reference corpus, such as a list of targeted token or error types, to generate GEC training data for any language with resources similar to a Wikipedia revision history. For the current German GEC system, a detailed error analysis for the output could identify the types of errors where Wikipedia edits make a significant contribution and other areas where additional data could be incorporated, potentially through artificial error generation or crowd-sourcing. ---------------------------------- **CONCLUSION** We provide initial results for grammatical error correction for German using data from the Falko and MERLIN corpora augmented with Wikipedia edits that have been filtered using a new German extension of the automatic error annotation tool ERRANT (Bryant et al., 2017) . Wikipedia edits are extracted using Wiki Edits (Grundkiewicz and Junczys-Dowmunt, 2014) , profiled with ER-RANT, and filtered with reference to the gold GEC data. We evaluate our method using the multilayer convolutional encoder-decoder neural network GEC approach from <cite>Chollampatt and Ng (2018)</cite> and find that augmenting a small gold German GEC corpus with one million filtered Wikipedia edits improves the performance from 39.22 to 44.47 F 0.5 and additional language model reranking increases performance to 45.22.",
  "y": "uses"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_0",
  "x": "In the experiment, questions are represented by 13 features, 9 of which are semantic features based on WordNet. <cite>Li and Roth (2002)</cite> use a Sparse Network of Winnows (SNoW) to classify questions with respect to their expected answer type. The taxonomy consists of 6 coarse and 50 fine semantic classes. The training corpus used consists of 5,500 questions. Some of these are manually constructed, while other stems from the TREC-8 and TREC-9 conferences. The test corpus comprise 500 questions from the TREC-10 conference. The input to the classifiers is a list of features.",
  "y": "background"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_1",
  "x": "Some of these are manually constructed, while other stems from the TREC-8 and TREC-9 conferences. The test corpus comprise 500 questions from the TREC-10 conference. The input to the classifiers is a list of features. The features used were words, part-ofspeech tags, chunks, named entities, head chunks (e.g. the first noun chunk in a sentence), and semantically related words (words that often occur with a specific question class). Apart from these primitive features, a set of operators were used to compose more complex features. Zhang and Lee (2003) used the same taxonomy as <cite>Li and Roth (2002)</cite> , as well as the same training and testing data. In an initial experiment they compared different machine learning approaches with regards to the question classification problem: Nearest Neighbors (NN), Na\u00efve Bayes (NB), Decision Trees (DT), SNoW, and Support Vector Machines.",
  "y": "background"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_2",
  "x": "Hacioglu and Ward (2003) used a SVM with error correcting codes to convert the multi-class classification problem into a number of binary ones. In essence each class is assigned a codeword of 1's and -1's of length m, where m equals or is greater than the number of classes. This splits the multi-class data into m binary class data. Therefore, m SVM classifiers can be designed and their output combined. The SVM:s also used linear kernels. The same taxonomy, training and testing data was used as in <cite>Li and Roth (2002)</cite> ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_3",
  "x": "Thses were originally developed for the text categorization task, but as question classification bears many resemblances and can be seen as a special case of text categorization. The taxonomy used is the taxonomy proposed by <cite>Li and Roth (2002)</cite> . This taxonomy has been chosen since it is the most frequently used one in earlier work in the field<cite> (Li and Roth, 2002</cite>; Zhang and Lee, 2003; Hacioglu and Ward, 2003) . The corpora used is both the corpus constructed and tagged by <cite>Li and Roth (2002)</cite> , as well as a newly tagged corpus extracted from the AnswerBus logs. AnswerBus is a question answering system that has been online and logged real users questions. The AnswerBus corpus consists of 25,000 questions. For present purposes 2,000 questions have been selected and tagged according to the aforementioned taxonomy.",
  "y": "uses"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_4",
  "x": "This taxonomy has been chosen since it is the most frequently used one in earlier work in the field<cite> (Li and Roth, 2002</cite>; Zhang and Lee, 2003; Hacioglu and Ward, 2003) . The corpora used is both the corpus constructed and tagged by <cite>Li and Roth (2002)</cite> , as well as a newly tagged corpus extracted from the AnswerBus logs. AnswerBus is a question answering system that has been online and logged real users questions. The AnswerBus corpus consists of 25,000 questions. For present purposes 2,000 questions have been selected and tagged according to the aforementioned taxonomy. Questions are in all experiments treated as a bag-of-words and represented as binary feature vectors. The results will be reported in terms of microand macro-averaged precision, recall and F-score.",
  "y": "background"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_5",
  "x": "The taxonomy used is the taxonomy proposed by <cite>Li and Roth (2002)</cite> . This taxonomy has been chosen since it is the most frequently used one in earlier work in the field<cite> (Li and Roth, 2002</cite>; Zhang and Lee, 2003; Hacioglu and Ward, 2003) . The corpora used is both the corpus constructed and tagged by <cite>Li and Roth (2002)</cite> , as well as a newly tagged corpus extracted from the AnswerBus logs. AnswerBus is a question answering system that has been online and logged real users questions. The AnswerBus corpus consists of 25,000 questions. For present purposes 2,000 questions have been selected and tagged according to the aforementioned taxonomy. Questions are in all experiments treated as a bag-of-words and represented as binary feature vectors.",
  "y": "similarities uses"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_6",
  "x": "---------------------------------- **EXPERIMENT 1** The first experiment is intended to be a straightforward re-examination of previous work to establish what differences in performance there really are between machine learners. This experiment has been done under two different settings. First, we have used the corpus originally developed by<cite> (Li and Roth, 2002)</cite> , but since the test corpus used consists of questions solely from TREC-10 and the TREC conferences have a specific agenda the test corpus might be slightly different from the training data. Therefore, a second setting was used where the questions from the training and test corpora were pooled together and a randomized test corpus was extracted. This will be refered to as the repartitioned corpus.",
  "y": "extends differences"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_7",
  "x": "The results in this paper indicate that some of the results found in previous work<cite> (Li and Roth, 2002</cite>; Zhang and Lee, 2003; Hacioglu and Ward, 2003) on question classification might be incorrect due to an unbiased training and test corpus. This bias stems from the fact that the training corpus is derived exclusively from TREC-10 data, while the training data stems from other sources. Since the TREC conferences have an explicit agenda that shifts from year to year this is perhaps no surprise. In relation to this, TREC material is maybe not the best source of information if one is interested in how different machine learners might perform on actual user data. ---------------------------------- **FUTURE WORK** The results from experiment 2 in this paper stems from a corpus of 2,000 questions.",
  "y": "background"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_0",
  "x": "In more detail, a seed question set is first generated by applying a small number of hand-crafted templates on the input KB, then more questions are retrieved by iteratively forming already obtained questions as search queries into a standard search engine, before finally questions are selected by estimating their fluency and domain relevance. Evaluated by human graders on 500 random-selected triples from Freebase, questions generated by our system are judged to be more fluent than those of <cite>Serban et al. (2016)</cite> by human graders. ---------------------------------- **INTRODUCTION** Question generation is important as questions are useful for student assessment or coaching purposes in educational or professional contexts, and a large-scale corpus of question and answer pairs is also critical to many NLP tasks including question answering, dialogue interaction and intelligent tutoring systems. There has been much literature so far (Chen et al., 2009; Ali et al., 2010; Heilman and Smith, 2010; Curto et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014; Labutov et al., 2015) studying question generation from text. Recently people are becoming interested in question generation from KB, since large-scale KBs, such as Freebase (Bollacker et al., 2008) and DBPedia (Auer et al., 2007) , are freely available, and entities and their relations are already present in KBs but not for texts.",
  "y": "differences"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_1",
  "x": "**INTRODUCTION** Question generation is important as questions are useful for student assessment or coaching purposes in educational or professional contexts, and a large-scale corpus of question and answer pairs is also critical to many NLP tasks including question answering, dialogue interaction and intelligent tutoring systems. There has been much literature so far (Chen et al., 2009; Ali et al., 2010; Heilman and Smith, 2010; Curto et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014; Labutov et al., 2015) studying question generation from text. Recently people are becoming interested in question generation from KB, since large-scale KBs, such as Freebase (Bollacker et al., 2008) and DBPedia (Auer et al., 2007) , are freely available, and entities and their relations are already present in KBs but not for texts. Question generation from KB is challenging as function words and morphological forms for entities are abstracted away when a KB is created. To tackle this challenge, previous work (Seyler et al., 2015; <cite>Serban et al., 2016</cite> ) relies on massive human-labeled data. Treating question generation as a machine translation problem, <cite>Serban et al. (2016)</cite> train a neural machine translation (NMT) system with 10,000 triple 1 , question pairs.",
  "y": "background"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_2",
  "x": "Question generation is important as questions are useful for student assessment or coaching purposes in educational or professional contexts, and a large-scale corpus of question and answer pairs is also critical to many NLP tasks including question answering, dialogue interaction and intelligent tutoring systems. There has been much literature so far (Chen et al., 2009; Ali et al., 2010; Heilman and Smith, 2010; Curto et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014; Labutov et al., 2015) studying question generation from text. Recently people are becoming interested in question generation from KB, since large-scale KBs, such as Freebase (Bollacker et al., 2008) and DBPedia (Auer et al., 2007) , are freely available, and entities and their relations are already present in KBs but not for texts. Question generation from KB is challenging as function words and morphological forms for entities are abstracted away when a KB is created. To tackle this challenge, previous work (Seyler et al., 2015; <cite>Serban et al., 2016</cite> ) relies on massive human-labeled data. Treating question generation as a machine translation problem, <cite>Serban et al. (2016)</cite> train a neural machine translation (NMT) system with 10,000 triple 1 , question pairs. At test time, input triples are \"translated\" into questions with the NMT system.",
  "y": "background"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_3",
  "x": "The seed question set is further expanded through a search engine (e.g., Google, Bing), by iteratively forming each generated question as a search query to retrieve more related question candidates. Finally a selection step is applied by estimating the fluency and domain relevance of each question candidate. The only human labor in this work is the question template construction. Our system does not require a large number of templates because: (1) the iterative question expansion can produce a large number of questions even with a relatively small number of seed questions, as we see in the experiments, (2) multiple entities in the KB share the same predicates. Another advantage is that our system can easily generate updated questions as web is self-updating consistently. In our experiment, we compare with <cite>Serban et al. (2016)</cite> on 500 random selected triples from Freebase (Bollacker et al., 2008) . Evaluated by 3 human graders, questions generated by our system are significantly better then <cite>Serban et al. (2016)</cite> on grammaticality and naturalness.",
  "y": "uses"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_4",
  "x": "Our system does not require a large number of templates because: (1) the iterative question expansion can produce a large number of questions even with a relatively small number of seed questions, as we see in the experiments, (2) multiple entities in the KB share the same predicates. Another advantage is that our system can easily generate updated questions as web is self-updating consistently. In our experiment, we compare with <cite>Serban et al. (2016)</cite> on 500 random selected triples from Freebase (Bollacker et al., 2008) . Evaluated by 3 human graders, questions generated by our system are significantly better then <cite>Serban et al. (2016)</cite> on grammaticality and naturalness. ---------------------------------- **KNOWLEDGE BASE** A knowledge base (KB) can be viewed as a directed graph, in which nodes are entities (such as \"jigsaw\" and \"CurveCut\") and edges are relations of entities (such as \"performsActivity\").",
  "y": "differences"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_5",
  "x": "The questions collected from the web search engine may not be fluent or domain relevant; especially the domain relevance drops significantly as the iteration goes on. Here we adopt a skip-gram model (Mikolov and Dean, 2013 ) and a language model for evaluating the domain relevance and fluency of the expanded questions, respectively. For System grammatical naturalness <cite>Serban et al. (2016)</cite> 3.36 3.14 Ours 3.53 3.31 Table 1 : Comparing generated questions domain relevance, we take the seed question set as the in-domain data D in , the domain relevance of expanded question q is defined as: where v(\u00b7) is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as: where LM(\u00b7) is the general-domain language model score (log probability), and LEN(\u00b7) is the word count. We apply thresholds t rel and t f lu for domain relevance and fluency respectively, and filter out questions whose scores are below these thresholds.",
  "y": "differences"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_6",
  "x": "For fluency, we define the averaged language model score as: where LM(\u00b7) is the general-domain language model score (log probability), and LEN(\u00b7) is the word count. We apply thresholds t rel and t f lu for domain relevance and fluency respectively, and filter out questions whose scores are below these thresholds. ---------------------------------- **EXPERIMENTS** We perform three experiments to evaluate our system qualitatively and quantitatively. In the first experiment, we compare our end-to-end system with the previous state-of-the-art method <cite>(Serban et al., 2016)</cite> on Freebase (Bollacker et al., 2008) , a domain-general KB.",
  "y": "uses"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_7",
  "x": "---------------------------------- **EVALUATION ON FREEBASE** We first compare our system with <cite>Serban et al. (2016)</cite> on 500 randomly selected triples from Freebase (Bollacker et al., 2008) 2 . For the 500 triples, we hand-crafted 106 templates, as these triples share only 53 distinct predicates (we made 2 templates for each predicate on average). 991 seed questions are generated by applying the templates on the triples, and 1529 more questions are retrieved from Google. To evaluate the fluency of the candidate questions, we train a 4-gram language model (LM) on gigaword (LDC2011T07) with Kneser Ney smoothing. Using the averaged language model score as index, the top 500 questions are selected to compare with the results from <cite>Serban et al. (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_8",
  "x": "To evaluate the fluency of the candidate questions, we train a 4-gram language model (LM) on gigaword (LDC2011T07) with Kneser Ney smoothing. Using the averaged language model score as index, the top 500 questions are selected to compare with the results from <cite>Serban et al. (2016)</cite> . We ask three native English speakers to evaluate the fluency and the naturalness 3 of both results based on a 4-point scheme where 4 is the best. We show the averaged human rate in Table 2 , where we can see that our questions are more grammatical and natural than <cite>Serban et al. (2016)</cite> . The naturalness score is less than the grammatical score for both methods. It is because naturalness is a more strict metric since a natural question should also be grammatical. Shown in Table 1 , we compare our questions with <cite>Serban et al. (2016)</cite> where questions in the same line describe the same entity.",
  "y": "uses"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_9",
  "x": "For the 500 triples, we hand-crafted 106 templates, as these triples share only 53 distinct predicates (we made 2 templates for each predicate on average). 991 seed questions are generated by applying the templates on the triples, and 1529 more questions are retrieved from Google. To evaluate the fluency of the candidate questions, we train a 4-gram language model (LM) on gigaword (LDC2011T07) with Kneser Ney smoothing. Using the averaged language model score as index, the top 500 questions are selected to compare with the results from <cite>Serban et al. (2016)</cite> . We ask three native English speakers to evaluate the fluency and the naturalness 3 of both results based on a 4-point scheme where 4 is the best. We show the averaged human rate in Table 2 , where we can see that our questions are more grammatical and natural than <cite>Serban et al. (2016)</cite> . The naturalness score is less than the grammatical score for both methods.",
  "y": "differences"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_10",
  "x": "Using the averaged language model score as index, the top 500 questions are selected to compare with the results from <cite>Serban et al. (2016)</cite> . We ask three native English speakers to evaluate the fluency and the naturalness 3 of both results based on a 4-point scheme where 4 is the best. We show the averaged human rate in Table 2 , where we can see that our questions are more grammatical and natural than <cite>Serban et al. (2016)</cite> . The naturalness score is less than the grammatical score for both methods. It is because naturalness is a more strict metric since a natural question should also be grammatical. Shown in Table 1 , we compare our questions with <cite>Serban et al. (2016)</cite> where questions in the same line describe the same entity. We can see that our questions are grammatical and natural as these questions are what people usually ask on the web.",
  "y": "uses"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_11",
  "x": "Shown in Table 1 , we compare our questions with <cite>Serban et al. (2016)</cite> where questions in the same line describe the same entity. We can see that our questions are grammatical and natural as these questions are what people usually ask on the web. On the other hand, questions from <cite>Serban et al. (2016)</cite> Ma et al. (2015) 85.48 Ours 85.65 Table 3 : Precision on the web snippet dataset was someone who was involved in the leukemia ?\" and \"whats the title of a book of the subject of the bible ?\"), unnatural (\"what 's one of the mountain where can you found in argentina in netflix ?\") or confusing (\"who was someone who was involved in the leukemia ?\"). ---------------------------------- **DOMAIN RELEVANCE** We test our domain-relevance evaluating method on the web snippet dataset, which is a commonlyused for domain classification of short documents. It contains 10,060 training and 2,280 test snippets (short documents) in 8 classes (domains), and each snippet has 18 words on average.",
  "y": "differences"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_12",
  "x": "Finally, in addition to the simple factoid questions, our system generates many complex questions such as \"how to cut a groove in wood without a router\". ---------------------------------- **CONCLUSION** We presented a system to generate natural language questions from a knowledge base. By leveraging rich web information, our system is able to generate domain-relevant questions in wide scope, while human effort is significantly reduced. Evaluated by human graders, questions generated by our system are significantly better than these from <cite>Serban et al. (2016)</cite> on 500 random-selected triples from Freebase. We also demonstrated generated questions from our in-house KB of power tool domain, which are fluent and domain-relevant in general.",
  "y": "differences"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_0",
  "x": "Results presented by <cite>Tseng (2001)</cite> show that speakers of Mandarin adopt some 30 words for building core structures of utterances in conversation, independently of individual speakers. All subjects used these words more than three times. The occurrences of these 30 core words make up about 80% of the overall tokens in conversation. Interestingly but also expected in conversational dialogues, the distribution of token frequency across all subjects is highly symmetric<cite> (Tseng 2001)</cite> . For instance, verbs \"is located\", \"is\", \"that is\", \"say\", \"want\" and \"have\" were frequently used, so were pronouns \"s/he\", \"you\" and \"I\". The negation \"don't have\" was a high-frequency word, so were words \"right\", \"this/these\" and \"that/those\". Grammatical particles as well as discourse particles were also among the core words.",
  "y": "background"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_1",
  "x": "It includes lexical distribution, discourse markers, turn-taking and prosodic characterization. ---------------------------------- **LEXICAL DISTRIBUTION IN SPOKEN MANDARIN** Results presented by <cite>Tseng (2001)</cite> show that speakers of Mandarin adopt some 30 words for building core structures of utterances in conversation, independently of individual speakers. All subjects used these words more than three times. The occurrences of these 30 core words make up about 80% of the overall tokens in conversation. Interestingly but also expected in conversational dialogues, the distribution of token frequency across all subjects is highly symmetric<cite> (Tseng 2001)</cite> .",
  "y": "background"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_2",
  "x": "Among the core words, eleven words were discourse particles, or they were used as discourse markers. In the literature, there is still no consistent definition for discourse markers (Hirschberg and Litman 1993) . Discourse markers can be defined as follows: elements whose original semantic meaning tends to decrease and their use in spoken discourse becomes more pragmatic and indicative of discourse structuring are discourse markers. In addition to several adverbs and determiners, discourse particles can also be categorized as discourse markers. They are very often observed in Mandarin spoken conversations as mentioned in <cite>Tseng (2001)</cite> and Clancy et al. (1996) . In <cite>Tseng (2001)</cite> , each subject used on average 1.6 discourse particles per turn. This result leads to the consideration, if there is a need to add special categories for discourse particles or particle-like words for spoken Mandarin.",
  "y": "background"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_3",
  "x": "Among the core words, eleven words were discourse particles, or they were used as discourse markers. In the literature, there is still no consistent definition for discourse markers (Hirschberg and Litman 1993) . Discourse markers can be defined as follows: elements whose original semantic meaning tends to decrease and their use in spoken discourse becomes more pragmatic and indicative of discourse structuring are discourse markers. In addition to several adverbs and determiners, discourse particles can also be categorized as discourse markers. They are very often observed in Mandarin spoken conversations as mentioned in <cite>Tseng (2001)</cite> and Clancy et al. (1996) . In <cite>Tseng (2001)</cite> , each subject used on average 1.6 discourse particles per turn. This result leads to the consideration, if there is a need to add special categories for discourse particles or particle-like words for spoken Mandarin.",
  "y": "background"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_4",
  "x": "They are very often observed in Mandarin spoken conversations as mentioned in <cite>Tseng (2001)</cite> and Clancy et al. (1996) . In <cite>Tseng (2001)</cite> , each subject used on average 1.6 discourse particles per turn. This result leads to the consideration, if there is a need to add special categories for discourse particles or particle-like words for spoken Mandarin. Discourse particles were found to have different and specific discourse use in conversation. Namely, there exist discourse particles appearing preferably in turn-beginning position and some other discourse particles may exclusively mark the location of repairs. Regarding the small size of data used in <cite>Tseng (2001)</cite> , it is one of the reasons why the ongoing project is necessary for research of Mandarin spontaneous conversations. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_5",
  "x": "In addition to several adverbs and determiners, discourse particles can also be categorized as discourse markers. They are very often observed in Mandarin spoken conversations as mentioned in <cite>Tseng (2001)</cite> and Clancy et al. (1996) . In <cite>Tseng (2001)</cite> , each subject used on average 1.6 discourse particles per turn. This result leads to the consideration, if there is a need to add special categories for discourse particles or particle-like words for spoken Mandarin. Discourse particles were found to have different and specific discourse use in conversation. Namely, there exist discourse particles appearing preferably in turn-beginning position and some other discourse particles may exclusively mark the location of repairs. Regarding the small size of data used in <cite>Tseng (2001)</cite> , it is one of the reasons why the ongoing project is necessary for research of Mandarin spontaneous conversations.",
  "y": "motivation"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_6",
  "x": "---------------------------------- **1.3** Taking Turns in Dialogues In spontaneous conversation, turn-taking usually takes place arbitrarily to the extent that every individual interacts differently with the others under different circumstances. Thus, how to annotate overlapping sequences is one of the essential tasks in developing annotation systems. In Mandarin conversation, there are words preferably used in turn-initial position<cite> (Tseng 2001</cite> , Chui 2000 . They normally have their own discourse-related pragmatic function associated with their positioning in utterances. Similarly, how to mark up turn-initial positions is also directly connected with the annotation convention.",
  "y": "background"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_7",
  "x": "---------------------------------- **1.4** Prosody in Spoken Mandarin Lexical tones are typically characteristic of spoken Mandarin. The interaction of lexical tones and the other prosodic means such as stress and intonation are related to a number of research issues, particularly in conversation. Falling tones may not show falling tendency anymore, when the associated words are used for specific discourse functions such as for indicating hesitation or the beginning of a turn<cite> (Tseng 2001)</cite> . ---------------------------------- **MANDARIN CONVERSATIONAL DIALOGUE CORPUS**",
  "y": "background"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_0",
  "x": "With the growth of the Internet and rapid production of a vast amount of information, question answering systems, designed to find a relevant proper answer by searching throughout a data source, are of great importance. The production of knowledge bases and the need to answer questions over such resources received researchers attentions to propose different models to find the answer of questions from the knowledge bases, known as KBQA 1 . Answering factoid questions with one relation, also known as simple question answering, has been widely studied in recent years (Dai et al., 2016; Yin et al., 2016; He and Golub, 2016;<cite> Yu et al., 2017)</cite> . A common approach that has been used in most of the researches is utilizing a two-component system, including an entity linker and a relation extractor. In this paper, we focus on the relation extraction component, which is also treated as a classification problem. This topic demands certain tools to capture relation that is mentioned in the questions, as a part of the QA systems. In this paper, we aim to view this kind of relation prediction. The term \"relation extraction\" is originally referred to capturing relation between two entities, if there is any.",
  "y": "background"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_1",
  "x": "In this context, extracting relation from single-relation questions obtains higher accuracy compared to multi-relational ones. Having a large number of relations in a knowledge base, however, this simple question relation extraction is not a solved problem yet. Classifying questions to predefined set of relations is one of the main approaches for this task (Mohammed et al., 2018) . Moreover, matching question content with relations has also been proposed and shown promising results (Yin et al., 2016;<cite> Yu et al., 2017)</cite> . In this paper, the relation extraction is viewed from a new perspective such that relation extraction is done within a question-question matching model, instead of only matching questions and relations. Indeed, while many of the previous works use a matching between question mention and relations, we use an instance-based method for classifying relations. The proposed model benefits from a text matching model, namely 1 Knowledge Base Question Answering 2 MatchPyramid , and enhances it with a two-channel model for considering lexical match and semantic match between questions.",
  "y": "similarities"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_2",
  "x": "**RELATED WORKS** ---------------------------------- **QUESTION ANSWERING OVER KNOWLEDGE BASE** One paradigm in proposed approaches for relation extraction in KBQA is based on semantic parsing in which questions were parsed and turned into logical forms in order to query the knowledge base (Berant et al., 2013; Berant and Liang, 2014) . However, most of the recent approaches (Mohammed et al., 2018; Bordes et al., 2015; Dai et al., 2016; He and Golub, 2016;<cite> Yu et al., 2017)</cite> are based on automatically extracted features of terms; thanks to the prominent performance of neural network on representation learning (Mikolov et al., 2013a,b) . From another point of view, two mainstreams for extracting relations in KBQA are studied: (a) using a classifier which chooses the most probable relation among all (Mohammed et al., 2018) ; (b) matching questions and relations through learning of an embedding space for representing all relations and question words (Bordes et al., 2015; Dai et al., 2016; Yin et al., 2016; He and Golub, 2016;<cite> Yu et al., 2017)</cite> , in which each relation is considered either as a meaningful sequence of words or as a unique entity. Dai et al. (2016) considered the relation prediction, as well as the whole KBQA problem, as a conditional probability task in which the goal is finding the most probable relation given the question mention.",
  "y": "background"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_3",
  "x": "---------------------------------- **QUESTION ANSWERING OVER KNOWLEDGE BASE** One paradigm in proposed approaches for relation extraction in KBQA is based on semantic parsing in which questions were parsed and turned into logical forms in order to query the knowledge base (Berant et al., 2013; Berant and Liang, 2014) . However, most of the recent approaches (Mohammed et al., 2018; Bordes et al., 2015; Dai et al., 2016; He and Golub, 2016;<cite> Yu et al., 2017)</cite> are based on automatically extracted features of terms; thanks to the prominent performance of neural network on representation learning (Mikolov et al., 2013a,b) . From another point of view, two mainstreams for extracting relations in KBQA are studied: (a) using a classifier which chooses the most probable relation among all (Mohammed et al., 2018) ; (b) matching questions and relations through learning of an embedding space for representing all relations and question words (Bordes et al., 2015; Dai et al., 2016; Yin et al., 2016; He and Golub, 2016;<cite> Yu et al., 2017)</cite> , in which each relation is considered either as a meaningful sequence of words or as a unique entity. Dai et al. (2016) considered the relation prediction, as well as the whole KBQA problem, as a conditional probability task in which the goal is finding the most probable relation given the question mention. To this aim, they used",
  "y": "background"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_4",
  "x": "Additionally, following <cite>Yu et al. (2017)</cite> , we add another neural network (Q'-R), the right part of the architecture, to compute the matching score of Q' with the relation of Q (R). By doing so, we are enhancing the matching signals between Q' and Q to estimate the overall score. In the first step, our proposed model projects the input question as well as the available questions and relations of training data into an embedding space. To do so, each sequence of words (Q', Q and R) are fed to an embedding layer and all of their corresponding vectors are fetched. In this work, we use pre-trained vectors due to the fact that current neural embeddings which are learned by using large scale text corpora provide rich enough representation. In the next step, input vectors are fed to the two neural networks. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_5",
  "x": "**EVALUATION** ---------------------------------- **DATASET** Following the previous works by Yin et al. (2016) and <cite>Yu et al. (2017)</cite> , we use the common benchmark dataset of the simple question answering, namely SimpleQuestions, which was originally introduced by Bordes et al. (2015) . This dataset contains 108442 questions gathered with the help of English-speaking annotators. Yin et al. (2016) proposed a new benchmark for evaluating relation extraction task on SimpleQuestion. In this benchmark, every question, whose entity is replaced by a unique token, is labeled with its ground truth relation as its positive label, and all other relation of the gold entity that is mentioned in the question are considered as negative labels.",
  "y": "similarities uses"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_7",
  "x": "Proposed Q'-Q + Q'-R model 93.41 are not originally evaluated on relation prediction of simple questions. In fact, the authors of AMPCNN (Yin et al., 2016) , conducted the corresponding experiments on a one-way-attention adaptation of these two models to compare them with the available methods in this task. Hier-Res-BiLSTM<cite> (Yu et al., 2017)</cite> uses hierarchical residual connections to ease the training procedure of BiL-STM. BiCNN (Yih et al., 2015) uses convolutional neural networks for matching a question with relations. The model is reimplemented for SimpleQuestions by <cite>Yu et al. (2017)</cite> . As it is shown, our proposed model outperforms the state-of-the-art models in relation extraction for SimpleQuestions dataset by a margin of 0.11 percentage. We believe that this improvement is an effect of the two contributions that we had in this paper, namely proposing a combined Q'-Q + Q'-R network and the two-channel text matching model in the Q'-Q network.",
  "y": "background"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_8",
  "x": "Proposed Q'-Q + Q'-R model 93.41 are not originally evaluated on relation prediction of simple questions. In fact, the authors of AMPCNN (Yin et al., 2016) , conducted the corresponding experiments on a one-way-attention adaptation of these two models to compare them with the available methods in this task. Hier-Res-BiLSTM<cite> (Yu et al., 2017)</cite> uses hierarchical residual connections to ease the training procedure of BiL-STM. BiCNN (Yih et al., 2015) uses convolutional neural networks for matching a question with relations. The model is reimplemented for SimpleQuestions by <cite>Yu et al. (2017)</cite> . As it is shown, our proposed model outperforms the state-of-the-art models in relation extraction for SimpleQuestions dataset by a margin of 0.11 percentage. We believe that this improvement is an effect of the two contributions that we had in this paper, namely proposing a combined Q'-Q + Q'-R network and the two-channel text matching model in the Q'-Q network.",
  "y": "similarities uses"
 },
 {
  "id": "9340338e7cf8ff8de4db84b462dfe5_0",
  "x": "Existing fact checking systems are capable of detecting fact-check-worthy claims in text (Hassan et al., 2015b) , returning semantically similar textual claims (Walenz et al., 2014) ; and scoring the truth of triples on a knowledge graph through semantic distance (Ciampaglia et al., 2015) . However, neither of these are suitable for fact checking a claim made in natural language against a database. Previous works appropriate for this task operate on a limited domain and are not able to incorporate temporal information when checking time-dependent claims<cite> (Vlachos and Riedel, 2015)</cite> . In this paper we introduce our fact checking tool, describe its architecture and design decisions, evaluate its accuracy and discuss future work. We highlight the ease of incorporating new information sources to fact check, which may be unavailable during training. To validate the extensibility of the system, we complete an additional evaluation of the system using claims taken from <cite>Vlachos and Riedel (2015)</cite> . We make the source code publicly available to the community.",
  "y": "background"
 },
 {
  "id": "9340338e7cf8ff8de4db84b462dfe5_1",
  "x": "We highlight the ease of incorporating new information sources to fact check, which may be unavailable during training. To validate the extensibility of the system, we complete an additional evaluation of the system using claims taken from <cite>Vlachos and Riedel (2015)</cite> . We make the source code publicly available to the community. ---------------------------------- **DESIGN CONSIDERATIONS** We developed our fact-checking approach in the context of the HeroX challenge 2 -a competition organised by the fact checking organization FullFact 3 . The types of claims the system presented can fact check was restricted to those which require looking up a value in a KB, similar to the one in Figure 1 .",
  "y": "uses"
 },
 {
  "id": "9340338e7cf8ff8de4db84b462dfe5_2",
  "x": "To learn a model to perform the KB look up (essentially a semantic parsing task), we extend the work of <cite>Vlachos and Riedel (2015)</cite> who used distant supervision (Mintz et al., 2009 ) to generate training data, obviating the need for manual labeling. In particular, we extend it to handle simple temporal expressions in order to fact check time-dependent claims appropriately, i. e. population in 2015. While the recently proposed semantic parser of Pasupat and Liang (2015) is also able to handle temporal expressions, it makes the assumption that the table against which the claim needs to be interpreted is known, which is unrealistic in the context of fact checking. Furthermore, the system we propose can predict relations from the KB on which the semantic parser has not been trained, a paradigm referred to as zero-shot learning (Larochelle et al., 2008) . We achieve this by learning a binary classifier that assesses how well the claim \"matches\" each relation in the KB. Finally, another consideration in our design is algorithmic accountability (Diakopoulos, 2016) so that the predictions and the decision process used by the system are interpretable by a human. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "9340338e7cf8ff8de4db84b462dfe5_3",
  "x": "The only requirement is to provide the system with an appropriate KB. Thus we ensured that the system can readily incorporate new tables taken from encyclopedic sources such as Wikipedia and the World Bank. In our system, this step is achieved by simply importing a CSV file and running a script to generate the new instances to train the relation matching classifier. Analysis of our entry to this competition showed that two errors were caused by incorrect initial source data and one partial error caused by recalling a correct property but making an incorrect deduction. Of numerical claims that we did not attempt, we observed that many required looking up multiple entries and performing a more complex deduction step which was beyond the scope of this project. We further validate the system by evaluating the ability of this fact checking system to make veracity assessments on simple numerical claims from the data set collected by<cite> (Vlachos and Riedel, 2015)</cite> . Of the 4,255 claims about numerical properties about countries and geographical areas in this data set, our KB contained information to fact check 3,418.",
  "y": "uses"
 },
 {
  "id": "9340338e7cf8ff8de4db84b462dfe5_4",
  "x": "This was because the absolute percentage error between the incorrectly retrieved properties and the claimed value was outside of the threshold we defined; thus these incorrectly retrieved properties never resulted in a true verdict, allowing the correct one to determine the verdict. ---------------------------------- **CONCLUSIONS AND FUTURE WORK** The core capability of the system demonstration we presented is to fact check natural language claims against relations stored in a KB. Although the range of claims is limited, the system is a fieldtested prototype and has been evaluated on a published data set<cite> (Vlachos and Riedel, 2015)</cite> and on real-world claims presented as part of the HeroX fact checking challenge. In future work, we will extend the semantic parsing technique used and apply our system to more complex claim types. Additionally, further work is required to reduce the number of candidate relations recalled from the KB.",
  "y": "uses"
 },
 {
  "id": "93a1f611592ce6aa5cde7538486f97_0",
  "x": "---------------------------------- **INTRODUCTION** The lexicon can be considered the most dynamic part of all linguistic knowledge sources over time. There are two innovative change strategies typical for lexical systems: the creation of entirely new lexical items, commonly reflecting the emergence of novel ideas, technologies or artifacts, on the one hand, and, on the other hand, shifts in the meaning of already existing lexical items, a process which usually takes place over larger periods of time. Tracing semantic changes of the latter type is the main focus of our research. Meaning shift has recently been investigated with emphasis on neural language models<cite> (Kim et al., 2014</cite>; Kulkarni et al., 2015) . This work is based on the assumption that the measurement of semantic change patterns can be reduced to the measurement of lexical similarity between lexical items.",
  "y": "background"
 },
 {
  "id": "93a1f611592ce6aa5cde7538486f97_1",
  "x": "Neural language models, originating from the word2vec algorithm (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c) , are currently considered as state-of-the-art solutions for implementing this assumption (Schnabel et al., 2015) . Within this approach, changes in similarity relations between lexical items at two different points of time are interpreted as a signal for meaning shift. Accordingly, lexical items which are very similar to the lexical item under scrutiny can be considered as approximating its meaning at a given point in time. Both techniques were already combined in prior work to show, e.g., the increasing association of the lexical item \"gay\" with the meaning dimension of \"homosexuality\"<cite> (Kim et al., 2014</cite>; Kulkarni et al., 2015) . We here investigate the accuracy and reliability of such similarity judgments derived from different training protocols dependent on word frequency, word ambiguity and the number of training epochs (i.e., iterations over all training material). Accuracy renders a judgment of the overall model quality, whereas reliability between repeated experiments ensures that qualitative judgments can indeed be transferred between experiments. Based on the identification of critical conditions in the experimental set-up of previously employed protocols, we recommend improved training strategies for more adequate neural language models dealing with diachronic lexical change patterns.",
  "y": "motivation background"
 },
 {
  "id": "93a1f611592ce6aa5cde7538486f97_2",
  "x": "Based on the identification of critical conditions in the experimental set-up of previously employed protocols, we recommend improved training strategies for more adequate neural language models dealing with diachronic lexical change patterns. Our results concerning reliability also cast doubt on the reproducibility of experiments where semantic similarity between lexical items is taken as a computationally valid indicator for properly capturing lexical meaning (and, consequently, meaning shifts) under a diachronic perspective. ---------------------------------- **RELATED WORK** Neural language models for tracking semantic changes over time typically distinguish between two different training protocols-continuous training of models<cite> (Kim et al., 2014)</cite> where the model for each time span is initialized with the embeddings of its predecessor, and, alternatively, independent training with a mapping between models for different points in time (Kulkarni et al., 2015) . A comparison between these two protocols, such as the one proposed in this paper, has not been carried out before. Also, the application of such protocols to non-English corpora is lacking, with the exception of our own work relating to German data (Hellrich and Hahn, 2016b; Hellrich and Hahn, 2016a) .",
  "y": "motivation background"
 },
 {
  "id": "93a1f611592ce6aa5cde7538486f97_3",
  "x": "While these commonly lead to very similar performance (LeCun et al., 2015) , they cause different representations in the course of repeated experiments. Approaches to modelling changes of lexical semantics not using neural language models, e.g., Wijaya and Yeniterzi (2011), Gulordava and Baroni (2011), Mihalcea and Nastase (2012) , Riedl et al. (2014) or Jatowt and Duh (2014) are, intentionally, out of the scope of this paper. In the same way, we here refrain from comparison with computational studies dealing with literary discussions related to the Romantic period (e.g., Aggarwal et al. (2014) ). ---------------------------------- **EXPERIMENTAL SET-UP** For comparability with earlier studies<cite> (Kim et al., 2014</cite>; Kulkarni et al., 2015) , we use the fiction part of the GOOGLE BOOKS NGRAM corpus (Michel et al., 2011; Lin et al., 2012) . This part of the corpus is also less affected by sampling irregularities than other parts (Pechenick et al., 2015) .",
  "y": "uses"
 },
 {
  "id": "93a1f611592ce6aa5cde7538486f97_4",
  "x": "Due to the opaque nature of GOOGLE's corpus acquisition strategy, the influence of OCR errors on our results cannot be reasonably estimated, yet we assume that they will affect all experiments in an equal manner. The wide range of experimental parameters described in Section 2 makes it virtually impossible to test all their possible combinations, especially as repeated experiments are necessary to probe a method's reliability. We thus concentrate on two experimental protocols-the one described by <cite>Kim et al. (2014)</cite> (referred to as Kim protocol) and the one from Kulkarni et al. (2015) (referred to as Kulkarni protocol), including close variations thereof. Kulkarni's protocol operates on all 5-grams occurring during five consecutive years (e.g., [1900] [1901] [1902] [1903] [1904] and trains models independently of each other. Kim's protocol operates on uniformly sized samples of 10M 5-grams for each year from 1850 onwards in a continuous fashion (years before 1900 are used for initialization only). Its constant sampling sizes result in both oversampling and undersampling as is evident from Figure 1 . We use the PYTHON-based GENSIM 1 implementation of word2vec for our experiments; the relevant code is made available via GITHUB.",
  "y": "uses background"
 },
 {
  "id": "966106c9e00f0333bda45d977a9f35_0",
  "x": "We compared convolutional neural network (ConvS2S)<cite> (Gehring et al., 2017)</cite> and recurrent neural network (RNNS2S) (Bahdanau et al., 2014) based sequence to sequence learning architectures. While RNN based architectures have proved to be successful and produce state-of-the-art results for machine translation, they take a long time to train. The temporal dependencies between the elements in the sequence due to the RNN state vector requires sequential processing. On the other hand, different parts of the sequence can be processed in parallel using a ConvS2S. Hence, it is appealing to explore ConvS2S as the basis of an architecture to speed up training and decoding. Recent work<cite> (Gehring et al., 2017)</cite> has shown that a purely CNN based encoder-decoder network is competitive with a RNN based network. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "966106c9e00f0333bda45d977a9f35_1",
  "x": "Hence, it is appealing to explore ConvS2S as the basis of an architecture to speed up training and decoding. Recent work<cite> (Gehring et al., 2017)</cite> has shown that a purely CNN based encoder-decoder network is competitive with a RNN based network. ---------------------------------- **RECURRENT SEQUENCE TO SEQUENCE MODEL (RNNS2S)** Recurrent sequence to sequence model (Bahdanau et al., 2014) is currently the most popular method for neural machine translation. It is been shown to be useful for other sequence to sequence tasks like image captioning (Vinyals et al., 2015) , language modeling, question answering (Wang and Nyberg, 2015) etc. The typical architecture encodes the sequence of source word embeddings to generate annotations for the source words.",
  "y": "motivation background"
 },
 {
  "id": "966106c9e00f0333bda45d977a9f35_2",
  "x": "---------------------------------- **CONVOLUTIONAL SEQUENCE TO SEQUENCE MODEL (CONVS2S)** In convolutional sequence to sequence model<cite> (Gehring et al., 2017)</cite> , the input sequence is encoded into distributional vector space using a CNN and decoded back to output sequence again using CNN instead of RNN (Sutskever et al., 2014) . Each input element embedding is combined with its positional embedding (signifies the position of the input element). Positional embeddings help the network to realize what part of input it is dealing with, currently. Encoder-Decoder. Both the encoder and decoder are CNN blocks along with a multistep attention mechanism with multiple 'hops' (Sukhbaatar et al., 2015) .",
  "y": "background"
 },
 {
  "id": "966106c9e00f0333bda45d977a9f35_3",
  "x": "For handling the rare words, the source side and target side corpora were segmented using byte pair encoding (BPE) (Shibata et al., 1999) . The baseline model with 4 encoder layers and 3 decoder layers was trained using nag optimizer<cite> (Gehring et al., 2017</cite> ) with a learning rate of 0.25 with 0.2 as its dropout value and gradient clipping was also applied. Table 6 : Hindi to English Translation Systems at WAT2017 The inferencing was done using beam search with a beam size of 10 for both Hindi-English and English-Hindi translation task. The model was also trained with more number of layers in the encoder and the decoder. The resulting BLEU scores for different number of encoder and decoder layers are shown in Table 4 . The best results were obtained when the number of encoder layers were set to 13 and decoder layers to 7, with learning rate of 0.1 and no dropout regularization.",
  "y": "uses"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_0",
  "x": "A supervised machine learning model is designed and trained on a single dataset to predict the relation type of pairs of entities. Traditional methods rely on linguistic or semantic features (Zhou et al., 2005; Jing and Zhai, 2007) , or kernels based on syntax or sequences (Bunescu and Mooney, 2005a,b; Plank and Moschitti, 2013) to represent sentences of relations. More recently, deep neural nets start to show promising results. Most rely on convolutional neural nets (Zeng et al., 2014 (Zeng et al., , 2015 Grishman, 2015, 2016; Fu et al., 2017) or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016) . Our initial experiments did not use syntactic features <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017 ) that require additional parsers. In order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation.",
  "y": "differences"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_1",
  "x": "Their method is a kind of soft-parameter sharing, which does not involve sharing any part of the model directly. Fu et al. (2017) applied domain adversarial networks (Ganin and Lempitsky, 2015) to relation extraction and obtained improvement on out-of-domain evaluation. Inspired by the adversarial training, we attempt to use it as a regularization tool in our multi-task model and find some improvement. ---------------------------------- **SUPERVISED NEURAL RELATION EXTRACTION MODEL** The supervised neural model on a single dataset was introduced by Zeng et al. (2014) and followed by many others (Nguyen and Grishman, 2015; Zhou et al., 2016; Miwa and Bansal, 2016;<cite> Nguyen and Grishman, 2016</cite>; Fu et al., 2017) . We use a similar model as our base model.",
  "y": "background"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_2",
  "x": "---------------------------------- **SUPERVISED NEURAL RELATION EXTRACTION MODEL** The supervised neural model on a single dataset was introduced by Zeng et al. (2014) and followed by many others (Nguyen and Grishman, 2015; Zhou et al., 2016; Miwa and Bansal, 2016;<cite> Nguyen and Grishman, 2016</cite>; Fu et al., 2017) . We use a similar model as our base model. It takes word tokens, position of arguments and their entity types as input. Some work <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017) used extra syntax features as input. However, the parsers that produce syntax features could have errors and vary depending on the domain of text.",
  "y": "background"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_3",
  "x": "We use a similar model as our base model. It takes word tokens, position of arguments and their entity types as input. Some work <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017) used extra syntax features as input. However, the parsers that produce syntax features could have errors and vary depending on the domain of text. The syntax features learned could also be too specific for a single dataset. Thus, we focus on learning representation from scratch, but also compare the models with extra features later in the experiments. The encoder is a bidirectional RNN with attention and the decoder is one hidden fully connected layer followed by a softmax output layer.",
  "y": "motivation"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_4",
  "x": "We also convert the entity types of the arguments to entity embeddings. The setup of word embedding and position embedding was introduced by Zeng et al. (2014) . The entity embedding <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017 ) is included for arguments that are entities rather than common nouns. At the end, each token is converted to an embedding w i as the concatenation of these three types of embeddings, where i \u2208 [0, T ), T is the length of the sentence. A wide range of encoders have been proposed for relation extraction. Most of them fall into categories of CNN (Zeng et al., 2014) , RNN (Zhou et al., 2016) and TreeRNN (Miwa and Bansal, 2016) . In this work, we follow Zhou et al. (2016) to use Bidirectional RNN with attention (BiRNN), which works well on both of the datasets we are going to evaluate on.",
  "y": "uses"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_5",
  "x": "The decoder uses this high level representation as features for relation classification. It usually contains one hidden layer (Zeng et al., 2014;<cite> Nguyen and Grishman, 2016</cite>; Fu et al., 2017 ) and a softmax output layer. We use the same structure which can be formalized as the following: where W h and b h are the weights for the hidden ---------------------------------- **LEARNING UNIFIED REPRESENTATION** While the data for one relation task may be small, noisy and biased, we can learn a better representation combining multiple relation tasks.",
  "y": "background"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_6",
  "x": "The importance weight is then normalized through a softmax function. Then we obtain the high level summarization \u03c6(x) for the relation example. The decoder uses this high level representation as features for relation classification. It usually contains one hidden layer (Zeng et al., 2014;<cite> Nguyen and Grishman, 2016</cite>; Fu et al., 2017 ) and a softmax output layer. We use the same structure which can be formalized as the following: where W h and b h are the weights for the hidden ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_8",
  "x": "To apply the multi-task learning, we need at least two datasets. We pick ACE05 and ERE for our case study. The ACE05 dataset provides a cross-domain evaluation setting . It contains 6 domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and weblogs (wl). Previous work (Gormley et al., 2015;<cite> Nguyen and Grishman, 2016</cite>; Fu et al., 2017) set, and the other half of bc, cts and wl as the test sets. We followed their split of documents and their split of the relation types for asymmetric relations. The ERE dataset has a similar relation schema to ACE05, but is different in some annotation guidelines (Aguilar et al., 2014) .",
  "y": "uses"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_9",
  "x": "**MODEL CONFIGURATIONS** We use word embedding pre-trained on newswire with 300 dimensions from word2vec (Mikolov et al., 2013) . We fix the word embeddings during the training. We follow<cite> Nguyen and Grishman (2016)</cite> to set the position and entity type embedding size to be 50. We use 150 dimensions for the GRU state, 100 dimensions for the word context vector and use 300 dimensions for the hidden layer in the decoders. We train the model using Adam (Kingma and Ba, 2014) optimizer with learning rate 0.001. We tune \u03bb linearly from 0 to 1, and \u03b2 logarithmically from 5 \u00b7 10 \u22121 to 10 \u22124 For all scores, we run experiments 10 times and take the average.",
  "y": "uses"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_10",
  "x": "We tune \u03bb linearly from 0 to 1, and \u03b2 logarithmically from 5 \u00b7 10 \u22121 to 10 \u22124 For all scores, we run experiments 10 times and take the average. ---------------------------------- **AUGMENTATION BETWEEN ACE05 AND ERE** Training separately on the two corpora (row \"Supervised\" in Table 1 ), we obtain results on ACE05 comparable to previous work (Gormley et al., 2015) with substantially fewer features. With syntactic features as <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017) did, it could be further improved. In this paper, however, we want to focus on representation learning from scratch first. Our experiments focus on whether we can improve the representation with more sources of data.",
  "y": "future_work"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_11",
  "x": "**AUGMENTATION BETWEEN ACE05 AND ERE** Training separately on the two corpora (row \"Supervised\" in Table 1 ), we obtain results on ACE05 comparable to previous work (Gormley et al., 2015) with substantially fewer features. With syntactic features as <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017) did, it could be further improved. In this paper, however, we want to focus on representation learning from scratch first. Our experiments focus on whether we can improve the representation with more sources of data. A common way to do so is pre-training. As a baseline, we pre-train the encoder of the supervised model on ERE and then fine-tune on ACE05, and vice versa (row \"Pretraining\" in Table 1 ).",
  "y": "motivation"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_12",
  "x": "Although representation learning from scratch could be more general across multiple datasets, we compare the effect of multi-task learning with extra features on this specific dataset. We add chunk embedding and on dep path embedding<cite> (Nguyen and Grishman, 2016)</cite> . Similar to entity type embedding, chunk embedding is created according to each token's chunk type, we set the embedding size to 50. On dep path embedding is a vector indicating whether the token is on the dependency path between the two entities. In the multi-task model, the shared encoder is a bidirectional RNN (BiRNN) without attention (Equation (1-3) ). These two embeddings will be concatenated to the output of the BiRNN to obtain the new h i and then passed to Equation (4). As the results (Table 2) , our supervised baseline is slightly worse than the previous state-of-the-art neural model with extra features, but the multitask learning can consistently help.",
  "y": "uses"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_0",
  "x": "As Gruner (1985) said, humor in public speakings will \"produce a more favorable reaction toward a speaker\" and \"enhance speaker image.\" However, there is no guarantee that the expected reactions would occur in an actual talk. If an automatic system can provide audience reactions which are likely to occur in actual talks, it will be helpful in the process of preparing a talk. In this study, we investigated the feasibility of current NLP technologies in building a system which provides expected audience reactions to public speaking. Studies on automatic humor recognition (Mihalcea and Strapparava, 2005;<cite> Yang et al., 2015</cite>; Zhang and Liu, 2014; Purandare and Litman, 2006 ) have defined the recognition task as a binary classification task. So, their classification models categorized a given sentence as a humorous or non-humorous sentence. Among the studies on humor classification, Mihalcea and Strapparava (2005) and <cite>Yang et al. (2015)</cite> reported high performance on the task. Considering the performance of their systems, it is reasonable to test the applicability of their models to a real application.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_1",
  "x": "If an automatic system can provide audience reactions which are likely to occur in actual talks, it will be helpful in the process of preparing a talk. In this study, we investigated the feasibility of current NLP technologies in building a system which provides expected audience reactions to public speaking. Studies on automatic humor recognition (Mihalcea and Strapparava, 2005;<cite> Yang et al., 2015</cite>; Zhang and Liu, 2014; Purandare and Litman, 2006 ) have defined the recognition task as a binary classification task. So, their classification models categorized a given sentence as a humorous or non-humorous sentence. Among the studies on humor classification, Mihalcea and Strapparava (2005) and <cite>Yang et al. (2015)</cite> reported high performance on the task. Considering the performance of their systems, it is reasonable to test the applicability of their models to a real application. In this study, we specifically applied a state-of-the-art automatic humor recognition model to talks and investigated if the model could be used to provide simulated laughters.",
  "y": "motivation background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_2",
  "x": "In our application of the state-of-art system to talks, we could not achieve a comparable performance to the reported performance of the system. We investigated the potential reasons for the performance difference through further analysis. Some humor classification studies (Mihalcea and Strapparava, 2005;<cite> Yang et al., 2015</cite>; Barbieri and Saggion, 2014 ) have used negative instances from different domains or topics, because non-humorous sentences could not be found or are very challenging to collect in target domains or topics. Their studies showed that it was possible to achieve promising performance using data from heterogeneous domains. However, our study showed that humorous sentences which were semantically close to non-humorous sentences were very challenging to distinguish. We first describe previous studies related to our study. Then, the data we used is described.",
  "y": "motivation differences background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_3",
  "x": "---------------------------------- **BACKGROUND** Previous studies (Mihalcea and Strapparava, 2005;<cite> Yang et al., 2015</cite>; Zhang and Liu, 2014; Purandare and Litman, 2006; Bertero and Fung, 2016) dealt with the humor recognition task as a binary classification task, which was to categorize a given text as humorous or non-humorous. These studies collected textual data which consisted of humorous texts and non-humorous texts and built a classification model using textual features. Humorous and non-humorous texts were from different domains across the studies. Pun websites, daily joke websites, or tweets were used as sources of humorous texts. Resources such as news websites, proverb websites, etc. were used as sources of non-humorous texts.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_4",
  "x": "**BACKGROUND** Previous studies (Mihalcea and Strapparava, 2005;<cite> Yang et al., 2015</cite>; Zhang and Liu, 2014; Purandare and Litman, 2006; Bertero and Fung, 2016) dealt with the humor recognition task as a binary classification task, which was to categorize a given text as humorous or non-humorous. These studies collected textual data which consisted of humorous texts and non-humorous texts and built a classification model using textual features. Humorous and non-humorous texts were from different domains across the studies. Pun websites, daily joke websites, or tweets were used as sources of humorous texts. Resources such as news websites, proverb websites, etc. were used as sources of non-humorous texts. <cite>Yang et al. (2015)</cite> tried to minimize genre differences between humorous and non-humorous texts in order to avoid a chance that a trained model was optimized to distinguish genre differences.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_5",
  "x": "We only examined textual features. Compared to previous studies, one innovation of this study was that a trained model was evaluated using humorous and non-humorous sentences from the same genre and same topic. Mihalcea and Strapparava (2005) and <cite>Yang et al. (2015)</cite> borrowed negative instances from different genres such as news websites or proverbs. Barbieri and Saggion (2014) borrowed negative instances from different topics among tweets, though both their positive and negative instances came from the same genre, tweets. Our talk data, on the other hand, was distinct from Barbieri and Saggion (2014) in that negative instances were selected from the same talks as positive instances. As a result, negative instances were inherently from the same topic as corresponding positive instances. In addition, we used real audience reactions (audience laughters) in building our data set.",
  "y": "differences"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_6",
  "x": "**PUN OF DAY DATA** <cite>Yang et al. (2015)</cite> collected a corpus of Pun of Day data 1 . The data consisted of 2,423 humorous (positive) texts and 2,403 non-humorous (negative) texts. The humorous texts were from the Pun of the Day website, and the negative texts from AP News2, New York Times, Yahoo! Answers and Proverb websites. Examples of humorous and non-humorous sentences are given below. Humorous The one who invented the door knocker got a No-bell prize. Non-Humorous The one who discovered/invented it had the last name of fahrenheit.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_7",
  "x": "Non-Humorous The one who discovered/invented it had the last name of fahrenheit. In order to reduce the differences between positive and negative instances in the data, <cite>Yang et al. (2015)</cite> used two constraints when collecting negative instances. Non-humorous texts were required to have lengths between the minimum and maximum lengths of positive instances, in order to be selected as negative instances. In addition, only non-humorous texts which consisted of words found in positive instances were collected. ---------------------------------- **TED TALK DATA** TED Talks 2 are recordings from TED conferences, and other special TED programs.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_8",
  "x": "The average length of 'Pun of the Day' data was 14 words, with a standard deviation of 5. The number of humorous sentences left after removing sentences with fewer than seven words was 4,726. Utilizing the same experimental setup as Mihalcea and Strapparava (2005) and <cite>Yang et al. (2015)</cite> (50% positive and 50% negative instances), we selected 4,726 sentences from among all collected nonhumorous sentences as negative instances. During selection, we minimized differences between positive and negative instances. A negative instance was selected from among sentences located close to a positive instance in a talk. We made a candidate set of non-humorous sentences using sentences within a window size of seven (e.g. from sent-7 to sent-1 and from sent+1 to sent+7 in the following): sent-7 . . . . . . sent-1 And she reminded me that my favorite color was blue.",
  "y": "uses"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_9",
  "x": "Among the candidates, sentences which consisted of less than seven words were removed and a negative instance was randomly selected among the remaining ones. ---------------------------------- **IMPLEMENTATION OF FEATURES** Features from <cite>Yang et al. (2015)</cite> , which we implemented, consisted of (1) two incongruity features, (2) six ambiguity features, (3) four interpersonal effect features, (4) four phonetic features, (5) five k-Nearest Neighbor features, and (6) 300 Word2Vec features. The total number of features used in this study was 321. We describe our implementation of the features in this section. The justifications for the features can be found in the original paper.",
  "y": "uses"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_10",
  "x": "---------------------------------- **EXPERIMENTS** ---------------------------------- **APPLICATION OF STATE-OF-ART TECHNOLOGY TO TALK DATA** In this section, we present expeirments that we ran to determine 1) how effective a model trained using 'Pun of Day' data (Pun) is when applied to TED Talk data (Talk), and 2) whether the performance of a model trained using Talk data would be similar to the performance reported in <cite>Yang et al. (2015)</cite> . We reimplemented features developed by <cite>Yang et al. (2015)</cite> and evaluated those features on Talk data. Considering the different characteristics of Talk data versus Pun data, we sought to investigate whether Yang's model could achieve the reported performance (over 85% accuracy) on our Talk data.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_11",
  "x": "We reimplemented features developed by <cite>Yang et al. (2015)</cite> and evaluated those features on Talk data. Considering the different characteristics of Talk data versus Pun data, we sought to investigate whether Yang's model could achieve the reported performance (over 85% accuracy) on our Talk data. The differences were 1) humorous sentences in Talk data were sentences which induced audience laughters, compared to Pun data which used canned textual humor, 2) all non-humorous sentences in Talk data were also from TED talks, and 3) each pair of humorous and non-humorous sentences were semantically close because they were closely placed. These differences made the humor classification task more challenging. We first validated the performance of the reimplemented features. We followed the experimental setup of <cite>Yang et al. (2015)</cite> in order to see if the performance of our duplicated features was comparable to their reported performance. Their best performance was 85.4% accuracy (Yang in Table 1 ) when they used Random Forest as a classifier and 10-fold cross validation (CV) as an evaluation method.",
  "y": "extends differences"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_12",
  "x": "We first validated the performance of the reimplemented features. We followed the experimental setup of <cite>Yang et al. (2015)</cite> in order to see if the performance of our duplicated features was comparable to their reported performance. Their best performance was 85.4% accuracy (Yang in Table 1 ) when they used Random Forest as a classifier and 10-fold cross validation (CV) as an evaluation method. Replicating this experiment setup, we were able to achieve 86.0% accuracy (Pun-to-Pun in Table 1 ), which is slightly better than the performance reported in their paper. The performance difference could be due to the difference in partitions in CV. After verifying the feature implementation, we built a humor recognition model using the entirety of the Pun data. The model was evaluated on Talk data in order to see how effective a state-of-art model was in spite of differences between the two data sets.",
  "y": "differences uses"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_13",
  "x": "**CROSS DOMAIN DATA COMBINATIONS** In the experiments described in the preceding section, we weren't able to get results comparable to <cite>Yang et al. (2015)</cite> when Talk data was used in both train and evaluation data. The results of our experiments raised questions about why two different results were observed for two different data sets. A major difference in the two data sets was the source of negative instances. <cite>Yang et al. (2015)</cite> borrowed negative instances from different genres such as news websites and proverbs. But, in Talk-to-Talk, both positive and negative instances were from the same genre. Furthermore, each humorous instance had a corresponding non-humorous instance from the same talk. In this section, we investigate the impact of genre differences in the humor classification task, using Pun and Talk data.",
  "y": "differences"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_0",
  "x": "In this work, we focus specifically on the evaluation of visual conversational agents and develop a human computation game to benchmark their performance as members of human-AI teams. Visual conversational agents (Das et al. 2017a;<cite> Das et al. 2017b</cite>; ) are AI agents Figure 1 : A human and an AI (a visual conversation agent called ALICE) play the proposed GuessWhich game. At the start of the game (top), ALICE is provided an image (shown above ALICE) which is unknown to the human. Both ALICE and the human are then provided a brief description of the image. The human then attempts to identify the secret image. In each subsequent round of dialog, the human asks a question about the unknown image, receives an answer from ALICE, and makes a best guess of the secret image from a fixed pool of images. After 9 rounds of dialog, the human makes consecutive guesses until the secret image is identified.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_1",
  "x": "After 9 rounds of dialog, the human makes consecutive guesses until the secret image is identified. The fewer guesses the human needs to identify the secret image, the better the human-AI team performance. trained to understand and communicate about the contents of a scene in natural language. For example, in Fig. 1 , the visual conversational agent (shown on the right) replies to answers questions about a scene while inferring context from the dialog history -Human: \"What is he doing?\" Agent: \"Playing frisbee\". These agents are typically trained to mimic large corpora of human-human dialogs and are evaluated automatically on how well they retrieve actual human responses (ground truth) in novel dialogs. Recent work has evaluated these models more pragmatically by evaluating how well pairs of visual conversational agents perform on goal-based conversational tasks rather than response retrieval from fixed dialogs. Specifically, <cite>(Das et al. 2017b</cite> ) train two visual conversational agents -a questioning bot QBOT, and an answering bot ABOT -for an image-guessing task.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_2",
  "x": "Specifically, <cite>(Das et al. 2017b</cite> ) train two visual conversational agents -a questioning bot QBOT, and an answering bot ABOT -for an image-guessing task. Starting from a description of the scene, QBOT and ABOT converse over multiple rounds of questions (QBOT) and answers (ABOT) in order to improve QBOT's understanding of a secret image known only to ABOT. After a fixed number of rounds, QBOT must guess the secret image from a large pool and both QBOT and ABOT are evaluated based on this guess. <cite>(Das et al. 2017b</cite> ) compare supervised baseline models with QBOT-ABOT teams trained through reinforcement learning based self-talk on this image-guessing task. They find that the AI-AI teams improve significantly at guessing the correct image after self-talk updates compared to the supervised pretraining. While these results indicate that the self-talk fine-tuned agents are better visual conversational agents, crucially, it remains unclear if these agents are indeed better at this task when interacting with humans. GuessWhich.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_3",
  "x": "While these results indicate that the self-talk fine-tuned agents are better visual conversational agents, crucially, it remains unclear if these agents are indeed better at this task when interacting with humans. GuessWhich. In this work, we propose to evaluate if and how this progress in AI-AI evaluation translates to the performance of human-AI teams. Inspired by the popular GuessWhat or 20-Questions game, we design a human computation game -GuessWhich -which requires collaboration between human and visual conversational AI agents. Mirroring the setting of<cite> (Das et al. 2017b)</cite> , GuessWhich is an image-guessing game that consists of 2 participants -questioner and answerer. At the start of the game, the answerer is provided an image that is unknown to the questioner and both questioner and answerer are given a brief description of the image content. The questioner interacts with the answerer for a fixed number of rounds of question-answer (dialog) to identify the secret image from a fixed pool of images (see Fig. 1 ).",
  "y": "similarities uses"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_4",
  "x": "At the start of the game, the answerer is provided an image that is unknown to the questioner and both questioner and answerer are given a brief description of the image content. The questioner interacts with the answerer for a fixed number of rounds of question-answer (dialog) to identify the secret image from a fixed pool of images (see Fig. 1 ). We evaluate human-AI team performance in GuessWhich, for the setting where the questioner is a human and the answerer is an AI (that we denote ALICE). Specifically, we evaluate two versions of ALICE for GuessWhich: 1. ALICE SL which is trained in a supervised manner on the Visual Dialog dataset (Das et al. 2017a ) to mimic the answers given by humans when engaged in a conversation with other humans about an image, and 2. ALICE RL which is pre-trained with supervised learning and fine-tuned via reinforcement learning for an imageguessing task as in<cite> (Das et al. 2017b)</cite> . It is important to appreciate the difficulty and sensitivity of the GuessWhich game as an evaluation tool -agents have to understand human questions and respond with accurate, consistent, fluent and informative answers for the human-AI team to do well.",
  "y": "similarities uses"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_5",
  "x": "\u2022 We design an interactive image-guessing game (GuessWhich) for evaluating human-AI team performance in the specific context of the AIs being visual conversational agents. GuessWhich pairs humans with ALICE, an AI capable of answering a sequence of questions about images. ALICE is assigned a secret image and answers questions asked about that image from a human for 9 rounds to help them identify the secret image (Sec. 4). \u2022 We evaluate human-AI team performance on this game for both supervised learning (SL) and reinforcement learning (RL) versions of ALICE. Our main experimental finding is that despite significant differences between SL and RL agents reported in previous work<cite> (Das et al. 2017b)</cite> , we find no significant difference in performance between ALICE SL or ALICE RL when paired with human partners (Sec. 6.1). This suggests that while self-talk and RL are interesting directions to pursue for building better visual conversational agents, there appears to be a disconnect between AI-AI and human-AI evaluations -progress on former does not seem predictive of progress on latter. This is an important finding to guide future research.",
  "y": "differences"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_6",
  "x": "This suggests that while self-talk and RL are interesting directions to pursue for building better visual conversational agents, there appears to be a disconnect between AI-AI and human-AI evaluations -progress on former does not seem predictive of progress on latter. This is an important finding to guide future research. ---------------------------------- **RELATED WORK** Given that our goal is to evaluate visual conversational agents through a human computation game, we draw connections to relevant work on visual conversational agents, human computation games, and dialog evaluation below. Visual Conversational Agents. Our AI agents are visual conversational models, which have recently emerged as a popular research area in visually-grounded language modeling (Das et al. 2017a;<cite> Das et al. 2017b</cite>; . (Das et al. 2017a ) introduced the task of Visual Dialog and collected the VisDial dataset by pairing subjects on Amazon Mechanical Turk (AMT) to chat about an image (with assigned roles of questioner and answerer).",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_7",
  "x": "---------------------------------- **RELATED WORK** Given that our goal is to evaluate visual conversational agents through a human computation game, we draw connections to relevant work on visual conversational agents, human computation games, and dialog evaluation below. Visual Conversational Agents. Our AI agents are visual conversational models, which have recently emerged as a popular research area in visually-grounded language modeling (Das et al. 2017a;<cite> Das et al. 2017b</cite>; . (Das et al. 2017a ) introduced the task of Visual Dialog and collected the VisDial dataset by pairing subjects on Amazon Mechanical Turk (AMT) to chat about an image (with assigned roles of questioner and answerer). <cite>(Das et al. 2017b</cite> ) pre-trained questioner and answerer agents on this VisDial dataset via supervised learning and fine-tuned them via self-talk (reinforcement learning), observing that RL-fine-tuned QBOT-ABOT are better at image-guessing after interacting with each other. However, (Aras et al. 2010; Chamberlain, Poesio, and Kruschwitz 2008) , movies (Michelucci 2013) etc.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_8",
  "x": "However, (Aras et al. 2010; Chamberlain, Poesio, and Kruschwitz 2008) , movies (Michelucci 2013) etc. While such games have traditionally focused on human-human collaboration, we extend these ideas to human-AI teams. Rather than collecting labeled data, our game is designed to measure the effectiveness of the AI in the context of human-AI teams. Evaluating Conversational Agents. Goal-driven (nonvisual) conversational models have typically been evaluated on task-completion rate or time-to-task-completion (Paek 2001) , so shorter conversations are better. At the other end of the spectrum, free-form conversation models are often evaluated by metrics that rely on n-gram overlaps, such as BLEU, METEOR, ROUGE, but these have been shown to correlate poorly with human judgment (Liu et al. 2016) . Human evaluation of conversations is typically in the format where humans rate the quality of machine utterances given context, without actually taking part in the conversation, as in <cite>(Das et al. 2017b</cite> ) and (Li et al. 2016) .",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_10",
  "x": "Next, we formally define the AI agent ALICE (Sec. 3), describe the GuessWhich game setup (Sec. 4 and 5), and present results and analysis from human studies (Sec. 6). ---------------------------------- **THE AI: ALICE** Recall from Section 1 that our goal is to evaluate how progress in AI measured through automatic evaluation translates to performance of human-AI teams in the context of visual conversational agents. Specifically, we are considering the question-answering agent ABOT from<cite> (Das et al. 2017b)</cite> as ABOT is the agent more likely to be deployed with a human partner in real applications (e.g. to answer questions about visual content to aid a visually impaired user). For completeness, we will review this work in this section. <cite>(Das et al. 2017b</cite> ) formulate a self-supervised imageguessing task between a questioner bot (QBOT) and an answerer bot (ABOT) which plays out over multiple rounds of dialog.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_12",
  "x": "**RESULTS** ---------------------------------- **ALICE SL VS. ALICE RL** We compare the performance of the two agents ALICE SL and ALICE RL in the GuessWhich game. These bots are state-ofthe-art visual dialog agents with respect to emulating human responses and generating visually discriminative responses in AI-AI dialog. <cite>(Das et al. 2017b</cite> ) evaluate these agents against strong baselines and report AI-AI team results that are significantly better than chance on a pool of \u223c10k images (rank \u223c1000 for SL, rank \u223c500 for RL). In addition to evaluating them in the context of human-AI teams we also report QBOT-ALICE team performances for reference.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_13",
  "x": "With ALICE RL as their teammate, the average number of guesses required was 7.19. We also observe that ALICE RL outperforms ALICE SL on the MRR metric. On both metrics, however, the differences are within the standard error margins (reported in the table) and not statisti- Table 1 : Performance of Human-ALICE teams with AL-ICE SL and ALICE RL measured by MR (lower is better) and MRR (higher is better). Error bars are 95% CIs from 1000 bootstrap samples. Unlike<cite> (Das et al., 2017b)</cite> , we find no significant difference between ALICE SL and ALICE RL . cally significant. As we collected additional data, the error margins became smaller but the means also became closer.",
  "y": "differences"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_14",
  "x": "As we collected additional data, the error margins became smaller but the means also became closer. This interesting finding stands in stark contrast to the results reported by<cite> (Das et al. 2017b)</cite> , where ALICE RL was found to be significantly more accurate than ALICE SL when evaluated in an AI-AI team. Our results suggest that the improvements of RL over SL (in AI-AI teams) do not seem to translate to when the agents are paired with a human in a similar setting. MR with varying number of games. In Fig. 4a , we plot the mean rank (MR) of the secret image across different games. We see that the human-ALICE team performs about the same for both ALICE SL and ALICE RL except Game 5, where ALICE SL seems to marginally outperform ALICE RL . We compare the performance of these teams against a baseline model that makes a string of random guesses at the end of the game.",
  "y": "differences"
 },
 {
  "id": "97fd0f1ce3d4f510c1566d642e9d2c_0",
  "x": "Our system is based on encoder-decoder framework with attention mechanism. We experimented with Multilingual Neural MT models. Our experiments show that Multilingual Neural Machine Translation leveraging parallel data from related language pairs helps in significant BLEU improvements upto 11.5, for low resource language pairs like Gujarati-English. ---------------------------------- **INTRODUCTION** Neural Machine Translation <cite>(Luong et al., 2015</cite>; Bahdanau et al., 2014; Johnson et al., 2017; Vaswani et al., 2017) has been receiving considerable attention in the recent years, given its superior performance without the demand of heavily hand crafted engineering efforts. NMT often outperforms Statistical Machine Translation (SMT) techniques but it still struggles if the parallel data is insufficient like in the case of Indian languages.",
  "y": "background"
 },
 {
  "id": "97fd0f1ce3d4f510c1566d642e9d2c_1",
  "x": "**NEURAL MT ARCHITECTURE** Our NMT model consists of an encoder and a decoder, each of which is a Recurrent Neural Network (RNN) as described in<cite> (Luong et al., 2015)</cite> . The model directly estimates the posterior distribution P \u03b8 (y|x) of translating a source sentence x = (x 1 , .., x n ) to a target sentence y = (y 1 , .., y m ) as: Each of the local posterior distribution P (y t |y 1 , 2 , .., y t\u22121 , x) is modeled as a multinomial distribution over the target language vocabulary which is represented as a linear transformation followed by a softmax function on the decoder's output vectorh dec t : where c t is the context vector, h enc and h dec are the hidden vectors generated by the encoder and decoder respectively, AttentionFunction(. , .) is the attention mechanism as shown in<cite> (Luong et al., 2015)</cite> and [. ; .] is the concatenation of two vectors. An RNN encoder first encodes x to a continuous vector, which serves as the initial hidden vector for the decoder and then the decoder performs recursive updates to produce a sequence of hidden vectors by applying the transition function f as: where e(.) is the word embedding operation.",
  "y": "uses"
 },
 {
  "id": "97fd0f1ce3d4f510c1566d642e9d2c_2",
  "x": "---------------------------------- **NEURAL MT ARCHITECTURE** Our NMT model consists of an encoder and a decoder, each of which is a Recurrent Neural Network (RNN) as described in<cite> (Luong et al., 2015)</cite> . The model directly estimates the posterior distribution P \u03b8 (y|x) of translating a source sentence x = (x 1 , .., x n ) to a target sentence y = (y 1 , .., y m ) as: Each of the local posterior distribution P (y t |y 1 , 2 , .., y t\u22121 , x) is modeled as a multinomial distribution over the target language vocabulary which is represented as a linear transformation followed by a softmax function on the decoder's output vectorh dec t : where c t is the context vector, h enc and h dec are the hidden vectors generated by the encoder and decoder respectively, AttentionFunction(. , .) is the attention mechanism as shown in<cite> (Luong et al., 2015)</cite> and [. ; .] is the concatenation of two vectors. An RNN encoder first encodes x to a continuous vector, which serves as the initial hidden vector for the decoder and then the decoder performs recursive updates to produce a sequence of hidden vectors by applying the transition function f as:",
  "y": "uses"
 },
 {
  "id": "97fd0f1ce3d4f510c1566d642e9d2c_3",
  "x": "where c t is the context vector, h enc and h dec are the hidden vectors generated by the encoder and decoder respectively, AttentionFunction(. , .) is the attention mechanism as shown in<cite> (Luong et al., 2015)</cite> and [. ; .] is the concatenation of two vectors. An RNN encoder first encodes x to a continuous vector, which serves as the initial hidden vector for the decoder and then the decoder performs recursive updates to produce a sequence of hidden vectors by applying the transition function f as: where e(.) is the word embedding operation. Popular choices for mapping f are Long-Short-Term Memory (LSTM) units and Gated Recurrent Units (GRU), the former of which we use in our models. An NMT model is typically trained under the maximum log-likelihood objective: where D is the training set. Our NMT model uses a bi-directional RNN as an encoder and a unidirectional RNN as a decoder with global attention<cite> (Luong et al., 2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "97fd0f1ce3d4f510c1566d642e9d2c_4",
  "x": "---------------------------------- **TRAINING DETAILS** The structure of our NMT model is same as in<cite> Luong et al. (2015)</cite> , an RNN based encoder-decoder model with Global Attention mechanism. We used an LSTM based Bi-directional encoder and a unidirectional decoder. We kept 4 layers in both the encoder & decoder with embedding size set to 512. The batch size was set to 64 and a dropout rate of 0.3. We used Adam optimizer (Kingma and Ba, 2014) for our experiments.",
  "y": "uses"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_0",
  "x": "System messages, which indicate actions like users entering the channel. These all start with ===, but not all messages starting with === are system messages, as shown by the second message in Figure 1 . 3 Related Work IRC Disentanglement Data: The most significant work on conversation disentanglement is a line of papers developing data and models for the #Linux IRC channel (Elsner and Charniak, 2008; Elsner and Schudy, 2009; Charniak, 2010, 2011) . Until now, their dataset was the only publicly available set of messages with annotated conversations (partially re-annotated by<cite> Mehri and Carenini (2017)</cite> with reply-structure graphs), and has been used for training and evaluation in subsequent work (Wang and Oard, 2009;<cite> Mehri and Carenini, 2017</cite>; Jiang et al., 2018) . We are aware of three other IRC disentanglement datasets. First, Adams and Martell (2008) studied disentanglement and topic identification, but did not release their data. Second, Riou et al. (2015) annotated conversations and discourse relations in the #Ubuntu-fr channel (French Ubuntu support).",
  "y": "background"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_1",
  "x": "Their system is the only publicly released statistical model for disentanglement of chat conversation, but most of the other work cited above applied similar models. We evaluate their model on both our data and our re-annotated version of their data. Recent work has applied neural networks <cite>(Mehri and Carenini, 2017</cite>; Guo et al. (2017) 1,500 1 48 hr 5 n/a 2 Table 1 : Annotated disentanglement dataset comparison. Our data is much larger than prior work, one of the only released sets, and the only one with context and adjudication. '+a' indicates there was an adjudication step to resolve disagreements. '?' indicates the value is not in the paper and the authors no longer have access to the data. ---------------------------------- **2018), WITH SLIGHT GAINS IN PERFORMANCE.** Graph Structure: Within a conversation, we define a graph of reply-to relations.",
  "y": "background"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_2",
  "x": "Studies that do consider graphs for disentanglement have used small datasets (Dulceanu, 2016;<cite> Mehri and Carenini, 2017)</cite> that are not always released (Wang et al., 2008; Guo et al., 2017) . ---------------------------------- **DATA** We introduce a manually annotated dataset of 77,563 messages: 74,963 from the #Ubuntu IRC channel, 3 and 2,600 messages from the #Linux IRC channel. 4 Annotating the #Linux data enables comparison with Elsner and Charniak (2008) , while the #Ubuntu channel has over 34 million messages, making it an interesting largescale resource for dialogue research. It also allows us to evaluate Lowe et al. (2015 Lowe et al. ( , 2017 's widely used heuristically disentangled conversations. When choosing samples we had to strike a balance between the number of samples and the size of each one.",
  "y": "background"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_3",
  "x": "This measure of inter-rater reliability corrects for chance agreement, accounting for the class imbalance between linked and not-linked pairs. Values are in the good agreement range proposed by Altman (1990) , and slightly higher than for<cite> Mehri and Carenini (2017)</cite>'s annotations. Results are not shown for Elsner and Charniak (2008) because they did not annotate graphs. ---------------------------------- **CONVERSATIONS:** We consider three metrics: 6 (1) Variation of Information (VI, Meila, 2007) . A measure of information gained or lost when going from one clustering to another.",
  "y": "differences"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_4",
  "x": "We consider a scaled version, using the bound for n items that VI(X; Y ) \u2264 log(n), and present 1\u2212VI so that larger values are better. (2) One-to-One Overlap (1-1, Elsner and Charniak, 2008) . Percentage overlap when conversations from two annotations are optimally paired up using the max-flow algorithm. We follow<cite> Mehri and Carenini (2017)</cite> and keep system messages. (3) Exact Match F 1 . Calculated using the number of perfectly matching conversations, excluding conversations with only one message (mostly system messages). This is an extremely challenging metric.",
  "y": "similarities uses"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_5",
  "x": "(3) Exact Match F 1 . Calculated using the number of perfectly matching conversations, excluding conversations with only one message (mostly system messages). This is an extremely challenging metric. We include it because it is easy to understand and it directly measures a desired value (perfectly extracted conversations). Our scores are higher in 4 cases and lower in 5. Interestingly, while \u03ba was higher for us than<cite> Mehri and Carenini (2017)</cite> , our scores for conversations are lower. This is possible because a single link can merge two conversations, meaning a single disagreement in links can cause a major difference in conversations.",
  "y": "differences"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_6",
  "x": "Union: Run 10 FF models trained with different random seeds and combine their output by keeping all edges predicted. Vote: Run 10 FF models and combine output by keeping the edges they all agree on. Link messages with no agreed antecedent to themselves. Intersect: Conversations that 10 FF models agree on, and other messages as singleton conversations. For Channel Two we also compare to Wang and Oard (2009) and<cite> Mehri and Carenini (2017)</cite> , but their code was unavailable, preventing evaluation on our data. We exclude Jiang et al. (2018) as they substantially modified the dataset. For details of models, including hyperparameters tuned on the development set, see the supplementary material.",
  "y": "similarities"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_7",
  "x": "The model performance and agreement levels are also strongly correlated, with a Spearman's rank correlation of 0.77. This demonstrates the importance of evaluating on data from more than one point in time to get a robust estimate of performance. How far apart consecutive messages in a conversation are: Elsner and Charniak (2008) and<cite> Mehri and Carenini (2017)</cite> use a limit of 129 seconds, Jiang et al. (2018) limit to within 1 hour, Guo et al. (2017) limit to within 8 messages, and we limit to within 100 messages. Figure 4 shows the distribution of time differences in our conversations. 94.9% are within 2 minutes, and almost all are within an hour. 88.3% are 8 messages or less apart, and 99.4% are 100 or less apart. This suggests that the lower limits in prior work are too low.",
  "y": "differences"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_0",
  "x": "Social media platforms like Twitter provide support for users to declare their location manually in their text profile or automatically with GPS-based geotagging. However, the text-based profile locations are noisy and only 1-3% of tweets are geotagged (Cheng et al., 2010; Morstatter et al., 2013b) , meaning that geolocation needs to be inferred from other information sources such as the tweet text and network relationships. User geolocation is the task of inferring the primary (or \"home\") location of a user from available sources of information, such as text posted by that individual, or network relationships with other individuals (Han et al., 2014) . Geolocation models are usually trained on the small set of users whose location is known (e.g. through GPS-based geotagging), and other users are geolocated using the resulting model. These models broadly fall into two categories: text-based and network-based methods. Orthogonally, the geolocation task can be viewed as a regression task over real-valued geographical coordinates, or a classification task over discretised region-based locations. Most previous research on user geolocation has focused either on text-based classification approaches (Eisenstein et al., 2010; Wing and Baldridge, 2011;<cite> Roller et al., 2012</cite>; Han et al., 2014) or, to a lesser extent, network-based regression approaches (Jurgens, 2013; Compton et al., 2014;<cite> Rahimi et al., 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_1",
  "x": "Orthogonally, the geolocation task can be viewed as a regression task over real-valued geographical coordinates, or a classification task over discretised region-based locations. Most previous research on user geolocation has focused either on text-based classification approaches (Eisenstein et al., 2010; Wing and Baldridge, 2011;<cite> Roller et al., 2012</cite>; Han et al., 2014) or, to a lesser extent, network-based regression approaches (Jurgens, 2013; Compton et al., 2014;<cite> Rahimi et al., 2015)</cite> . Methods which combine the two, however, are rare. In this paper, we present work on Twitter user geolocation using both text and network information. Our contributions are as follows: (1) we propose the use of Modified Adsorption (Talukdar and Crammer, 2009) as a baseline networkbased geolocation model, and show that it outperforms previous network-based approaches (Jurgens, 2013;<cite> Rahimi et al., 2015)</cite> ; (2) we demonstrate that removing \"celebrity\" nodes (nodes with high in-degrees) from the network increases geolocation accuracy and dramatically decreases network edge size; and (3) we integrate textbased geolocation priors into Modified Adsorption, and show that our unified geolocation model outperforms both text-only and network-only approaches, and achieves state-of-the-art results over three standard datasets. ---------------------------------- **RELATED WORK**",
  "y": "motivation"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_2",
  "x": "Most previous research on user geolocation has focused either on text-based classification approaches (Eisenstein et al., 2010; Wing and Baldridge, 2011;<cite> Roller et al., 2012</cite>; Han et al., 2014) or, to a lesser extent, network-based regression approaches (Jurgens, 2013; Compton et al., 2014;<cite> Rahimi et al., 2015)</cite> . Methods which combine the two, however, are rare. In this paper, we present work on Twitter user geolocation using both text and network information. Our contributions are as follows: (1) we propose the use of Modified Adsorption (Talukdar and Crammer, 2009) as a baseline networkbased geolocation model, and show that it outperforms previous network-based approaches (Jurgens, 2013;<cite> Rahimi et al., 2015)</cite> ; (2) we demonstrate that removing \"celebrity\" nodes (nodes with high in-degrees) from the network increases geolocation accuracy and dramatically decreases network edge size; and (3) we integrate textbased geolocation priors into Modified Adsorption, and show that our unified geolocation model outperforms both text-only and network-only approaches, and achieves state-of-the-art results over three standard datasets. ---------------------------------- **RELATED WORK** A recent spike in interest on user geolocation over social media data has resulted in the development of a range of approaches to automatic geolocation prediction, based on information sources such as the text of messages, social networks, user profile data, and temporal data.",
  "y": "uses differences"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_3",
  "x": "Jurgens (2013) and Compton et al. (2014) used a Twitter reciprocal mention network, and geolocated users based on the geographical coordinates of their friends, by minimising the weighted distance of a given user to their friends. For a reciprocal mention network to be effective, however, a huge amount of Twitter data is required. Rahimi et al. (2015) showed that this assumption could be relaxed to use an undirected mention network for smaller datasets, and still attain state-of-theart results. The greatest shortcoming of networkbased models is that they completely fail to geolocate users who are not connected to geolocated components of the graph. As shown by<cite> Rahimi et al. (2015)</cite> , geolocation predictions from text can be used as a backoff for disconnected users, but there has been little work that has investigated a more integrated text-and network-based approach to user geolocation. ---------------------------------- **DATA**",
  "y": "background"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_4",
  "x": "For a reciprocal mention network to be effective, however, a huge amount of Twitter data is required. Rahimi et al. (2015) showed that this assumption could be relaxed to use an undirected mention network for smaller datasets, and still attain state-of-theart results. The greatest shortcoming of networkbased models is that they completely fail to geolocate users who are not connected to geolocated components of the graph. As shown by<cite> Rahimi et al. (2015)</cite> , geolocation predictions from text can be used as a backoff for disconnected users, but there has been little work that has investigated a more integrated text-and network-based approach to user geolocation. ---------------------------------- **DATA** We evaluate our models over three pre-existing geotagged Twitter datasets: (1) GEOTEXT (Eisen-stein et al., 2010), (2) TWITTER-US <cite>(Roller et al., 2012)</cite> , and (3) TWITTER-WORLD (Han et al., 2012) .",
  "y": "uses"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_5",
  "x": "In the case of the collapsed network of TWITTER-WORLD, k is decreased by a factor of 4000 after setting the celebrity threshold T to 5. We apply celebrity removal over both binary (\"MADCEL-B\") and weighted (\"MADCEL-W\") networks (using the respective T for each dataset). The effect of celebrity removal over the development set of TWITTER-US is shown in Figure 2 where it dramatically reduces the graph edge size and simultaneously leads to an improvement in the mean error. A Unified Geolocation Model To address the issue of disconnected test users, we incorporate text information into the model by attaching a labelled dongle node to every test node (Zhu and Ghahramani, 2002; Goldberg and Zhu, 2006) . The label for the dongle node is based on a textbased l 1 regularised logistic regression model, using the method of<cite> Rahimi et al. (2015)</cite> . The dongle nodes with their corresponding label confidences are added to the seed set, and are treated in the same way as other labelled nodes (i.e. the training nodes). Once again, we experiment with text-based labelled dongle nodes over both binary (\"MADCEL-B-LR\") and weighted (\"MADCEL-W-LR\") networks.",
  "y": "uses"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_6",
  "x": "Note that higher numbers are better for Acc@161, but lower numbers are better for mean and median error, with a lower bound of 0 and no (theoretical) upper bound. To generate a continuous-valued latitude/longitude coordinate for a given user from the k-d tree cell, we use the median coordinates of all training points in the predicted region. Table 1 shows the performance of MAD-B, MADCEL-B, MADCEL-W, MADCEL-B-LR and MADCEL-W-LR over the GEOTEXT, TWITTER-US and TWITTER-WORLD datasets. The results are also compared with prior work on network-based geolocation using label propagation (LP)<cite> (Rahimi et al., 2015)</cite> , text-based classification models (Han et al., 2012; Wing and Baldridge, 2011;<cite> Rahimi et al., 2015</cite>; Cha et al., 2015) , textbased graphical models (Ahmed et al., 2013) , and network-text hybrid models (LP-LR)<cite> (Rahimi et al., 2015)</cite> . ---------------------------------- **RESULTS** Our baseline network-based model of MAD-B outperforms the text-based models and also previous network-based models (Jurgens, 2013; Compton et al., 2014;<cite> Rahimi et al., 2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_7",
  "x": "**RESULTS** Our baseline network-based model of MAD-B outperforms the text-based models and also previous network-based models (Jurgens, 2013; Compton et al., 2014;<cite> Rahimi et al., 2015)</cite> . The inference, however, is intractable for TWITTER-US and TWITTER-WORLD due to the size of the network. Celebrity removal in MADCEL-B and MADCEL-W has a positive effect on geolocation accuracy, and results in a 47% reduction in Median over GEOTEXT. It also makes graph inference over TWITTER-US and TWITTER-WORLD tractable, and results in superior Acc@161 and Median, but slightly inferior Mean, compared to the state-of-the-art results of LR, based on text-based classification<cite> (Rahimi et al., 2015)</cite> . MADCEL-W (weighted graph) outperforms MADCEL-B (binary graph) over the smaller GEOTEXT dataset where it compensates for the sparsity of network information, but doesn't improve the results for the two larger datasets where network information is denser. Adding text to the network-based geolocation models in the form of MADCEL-B-LR (binary edges) and MADCEL-W-LR (weighted edges), we achieve state-of-the-art results over all three datasets.",
  "y": "differences"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_8",
  "x": "The inference, however, is intractable for TWITTER-US and TWITTER-WORLD due to the size of the network. Celebrity removal in MADCEL-B and MADCEL-W has a positive effect on geolocation accuracy, and results in a 47% reduction in Median over GEOTEXT. It also makes graph inference over TWITTER-US and TWITTER-WORLD tractable, and results in superior Acc@161 and Median, but slightly inferior Mean, compared to the state-of-the-art results of LR, based on text-based classification<cite> (Rahimi et al., 2015)</cite> . MADCEL-W (weighted graph) outperforms MADCEL-B (binary graph) over the smaller GEOTEXT dataset where it compensates for the sparsity of network information, but doesn't improve the results for the two larger datasets where network information is denser. Adding text to the network-based geolocation models in the form of MADCEL-B-LR (binary edges) and MADCEL-W-LR (weighted edges), we achieve state-of-the-art results over all three datasets. The inclusion of text-based priors has the greatest impact on Mean, resulting in an additional 26% and 23% error reduction over TWITTER-US and TWITTER-WORLD, respectively. The reason for this is that it provides a user-specific geolocation prior for (relatively) disconnected users.",
  "y": "differences"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_0",
  "x": "Finally, for several quarters, the system has been deployed for the purpose of matching queries to ads in Gemini, the sponsored search advertising platform at Yahoo, resulting in significant improvement of business metrics. ---------------------------------- **INTRODUCTION** Embedding words in a common vector space can enable machine learning algorithms to achieve better performance in natural language processing (NLP) tasks. Word2vec <cite>[23]</cite> is a recently proposed family of algorithms for training such vector representations from unstructured text data via shal- * Work done while with Yahoo, Inc. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.",
  "y": "background"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_1",
  "x": "Request permissions from permissions@acm.org. low neural networks. The geometry of the resulting vectors was shown in <cite>[23]</cite> to capture word semantic similarity through the cosine similarity of the corresponding vectors as well as more complex semantic relationships through vector differences, such as vec(\"Madrid\") -vec(\"Spain\") + vec(\"France\") \u2248 vec(\"Paris\"). More recently, novel applications of word2vec involving unconventional generalized \"words\" and training corpuses have been proposed. These powerful ideas from the NLP community have been adapted by researchers from other domains to tasks beyond representation of words, including relational entities [10, 32] , general text-based attributes [17] , descriptive text of images [18] , nodes in graph structure of networks [27] , and queries [15] , to name a few. While most NLP applications of word2vec do not require training of large vocabularies, many of the above mentioned real-world applications do. For example, the number of unique nodes in a social network [27] or the number of unique queries in a search engine [15] can easily reach few hundred million, a scale that is not achievable using existing word2vec implementations.",
  "y": "background"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_2",
  "x": "The geometry of the resulting vectors was shown in <cite>[23]</cite> to capture word semantic similarity through the cosine similarity of the corresponding vectors as well as more complex semantic relationships through vector differences, such as vec(\"Madrid\") -vec(\"Spain\") + vec(\"France\") \u2248 vec(\"Paris\"). More recently, novel applications of word2vec involving unconventional generalized \"words\" and training corpuses have been proposed. These powerful ideas from the NLP community have been adapted by researchers from other domains to tasks beyond representation of words, including relational entities [10, 32] , general text-based attributes [17] , descriptive text of images [18] , nodes in graph structure of networks [27] , and queries [15] , to name a few. While most NLP applications of word2vec do not require training of large vocabularies, many of the above mentioned real-world applications do. For example, the number of unique nodes in a social network [27] or the number of unique queries in a search engine [15] can easily reach few hundred million, a scale that is not achievable using existing word2vec implementations. The training of vectors for such large vocabularies presents several challenges. In word2vec, each vocabulary word has two associated d-dimensional vectors which must be trained, respectively referred to as input and output vectors, each of which is represented as an array of d single precision floating point numbers <cite>[23]</cite> .",
  "y": "background"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_3",
  "x": "Moreover, we found that there are around 800 million generalized words that occur 5 or more times in our largest data sets, indicating that additional scaling far beyond 200 million is well worth pursuing. The results of [15] were based on training the largest vocabulary that could fit into the large memory of a special purpose server, which resulted in learned vector representations for about 45 million words. The proposed training system herein enables increasing this by several fold, resulting in far greater coverage of queries and a potentially significant boost in query monetization, as indicated by Figure 2 ---------------------------------- **THE WORD2VEC TRAINING PROBLEM** In this paper we focus on the skipgram approach with random negative examples proposed in <cite>[23]</cite> . This has been found to yield the best results among the proposed variants on a variety of semantic tests of the resulting vectors [19,<cite> 23]</cite> .",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_4",
  "x": "---------------------------------- **THE WORD2VEC TRAINING PROBLEM** In this paper we focus on the skipgram approach with random negative examples proposed in <cite>[23]</cite> . This has been found to yield the best results among the proposed variants on a variety of semantic tests of the resulting vectors [19,<cite> 23]</cite> . Given a corpus consisting of a sequence of sentences s1, s2, . . . , sn each comprising a sequence of words si = wi,1, wi,2, . . . , wi,m i , the objective is to maximize the log likelihood: over input and output word row vectors u(w) and v(w) with w ranging over the words in the vocabulary V, where:",
  "y": "background"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_5",
  "x": "This has been found to yield the best results among the proposed variants on a variety of semantic tests of the resulting vectors [19,<cite> 23]</cite> . Given a corpus consisting of a sequence of sentences s1, s2, . . . , sn each comprising a sequence of words si = wi,1, wi,2, . . . , wi,m i , the objective is to maximize the log likelihood: over input and output word row vectors u(w) and v(w) with w ranging over the words in the vocabulary V, where: \u2022 \u03c3(\u00b7) denotes the sigmoid function \u03c3(x) = 1/(1 + e \u2212x ); \u2022 window sizes bi,j are randomly selected so that each inner sum includes between 1 and a maximum B terms, as in <cite>[23]</cite> and its open-source implementation; 2 \u2022 negative examples N i,j,k associated with positive output word w i,k are selected randomly according to a probability distribution suggested in <cite>[23]</cite> ;",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_6",
  "x": "\u2022 window sizes bi,j are randomly selected so that each inner sum includes between 1 and a maximum B terms, as in <cite>[23]</cite> and its open-source implementation; 2 \u2022 negative examples N i,j,k associated with positive output word w i,k are selected randomly according to a probability distribution suggested in <cite>[23]</cite> ; \u2022 and the vocabulary V consists of the set of words for which vectors are to be trained. We follow <cite>[23]</cite> for setting V and select words occurring in the corpus a sufficient number of times (e.g., at least 5 times), or, if this results in too many words, as the most frequently occurring N words, where N is the largest number words that can be handled by available computational resources. We further also assume a randomized version of (1) according to the subsampling technique of <cite>[23]</cite> , which removes some occurrences of frequent words. The algorithm for maximizing (1) advocated in <cite>[23]</cite> , and implemented in its open-source counterpart, is a minibatch stochastic gradient descent (SGD). Our training system is also based on minibatch SGD optimization of (1), however, as described in Section 5, it is carried out in a distributed fashion in a manner quite different from the implementation of <cite>[23]</cite> .",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_7",
  "x": ". , wi,m i , the objective is to maximize the log likelihood: over input and output word row vectors u(w) and v(w) with w ranging over the words in the vocabulary V, where: \u2022 \u03c3(\u00b7) denotes the sigmoid function \u03c3(x) = 1/(1 + e \u2212x ); \u2022 window sizes bi,j are randomly selected so that each inner sum includes between 1 and a maximum B terms, as in <cite>[23]</cite> and its open-source implementation; 2 \u2022 negative examples N i,j,k associated with positive output word w i,k are selected randomly according to a probability distribution suggested in <cite>[23]</cite> ; \u2022 and the vocabulary V consists of the set of words for which vectors are to be trained. We follow <cite>[23]</cite> for setting V and select words occurring in the corpus a sufficient number of times (e.g., at least 5 times), or, if this results in too many words, as the most frequently occurring N words, where N is the largest number words that can be handled by available computational resources.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_8",
  "x": "over input and output word row vectors u(w) and v(w) with w ranging over the words in the vocabulary V, where: \u2022 \u03c3(\u00b7) denotes the sigmoid function \u03c3(x) = 1/(1 + e \u2212x ); \u2022 window sizes bi,j are randomly selected so that each inner sum includes between 1 and a maximum B terms, as in <cite>[23]</cite> and its open-source implementation; 2 \u2022 negative examples N i,j,k associated with positive output word w i,k are selected randomly according to a probability distribution suggested in <cite>[23]</cite> ; \u2022 and the vocabulary V consists of the set of words for which vectors are to be trained. We follow <cite>[23]</cite> for setting V and select words occurring in the corpus a sufficient number of times (e.g., at least 5 times), or, if this results in too many words, as the most frequently occurring N words, where N is the largest number words that can be handled by available computational resources. We further also assume a randomized version of (1) according to the subsampling technique of <cite>[23]</cite> , which removes some occurrences of frequent words.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_9",
  "x": "\u2022 window sizes bi,j are randomly selected so that each inner sum includes between 1 and a maximum B terms, as in <cite>[23]</cite> and its open-source implementation; 2 \u2022 negative examples N i,j,k associated with positive output word w i,k are selected randomly according to a probability distribution suggested in <cite>[23]</cite> ; \u2022 and the vocabulary V consists of the set of words for which vectors are to be trained. We follow <cite>[23]</cite> for setting V and select words occurring in the corpus a sufficient number of times (e.g., at least 5 times), or, if this results in too many words, as the most frequently occurring N words, where N is the largest number words that can be handled by available computational resources. We further also assume a randomized version of (1) according to the subsampling technique of <cite>[23]</cite> , which removes some occurrences of frequent words. The algorithm for maximizing (1) advocated in <cite>[23]</cite> , and implemented in its open-source counterpart, is a minibatch stochastic gradient descent (SGD). Our training system is also based on minibatch SGD optimization of (1), however, as described in Section 5, it is carried out in a distributed fashion in a manner quite different from the implementation of <cite>[23]</cite> .",
  "y": "background uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_10",
  "x": "Our training system is also based on minibatch SGD optimization of (1), however, as described in Section 5, it is carried out in a distributed fashion in a manner quite different from the implementation of <cite>[23]</cite> . Any form of minibatch SGD optimization of (1) involves the computation of dot products and linear combinations between input and output word vectors for all pairs of words occurring within the same window (with indices in {k = j : |k \u2212 j| \u2264 bi,j}). This is a massive computational task when carried out for multiple iterations over data sets with tens of billions of words, as encountered in applications described in the previous section. ---------------------------------- **EXISTING WORD2VEC SYSTEMS** ---------------------------------- **SINGLE MACHINE**",
  "y": "differences"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_11",
  "x": "---------------------------------- **EXISTING WORD2VEC SYSTEMS** ---------------------------------- **SINGLE MACHINE** Several existing word2vec training systems are limited to running on a single machine, though with multiple parallel threads of execution operating on different segments of training data. These include the original open source implementation of word2vec <cite>[23]</cite> , as well as those of Medallia [22] , and Rehurek [28] . As mentioned in the introduction, these systems would require far larger memory configurations than available on typical commodity-scale servers.",
  "y": "background"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_12",
  "x": "There are on average w \u00b7 n of these per minibatch word. For w = 10, n = 10, d = 500, values within the ranges recommended in <cite>[23]</cite> , this works out to r(10, 10, 500) \u2248 200, 000 bytes transferred per word with each get and put. For 10 iterations of training on a data set of roughly 50 billion words, which is in the middle of the relevant range for the sponsored search application described in Section 2, attaining a total training latency of one week using the above system would require an aggregate bandwidth of at least 1300Gbits/sec to and from the parameter servers 4 . This is impractically large for a single application on a commodity-hardware shared compute cluster. Moreover, one week training latency is already at the boundary of usefulness for our applications. In the next section, we present a different distributed sys- 3 This expression tends to lower bound the total bandwidth, as it accounts only for the word vector and gradient bytes. The indices into the vocabulary sent with each get and put request require bandwidth as well, although this is small relative to the vector data.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_13",
  "x": "where w1, . . . , w |V| are the words in the vocabulary according to a fixed ordering O (e.g., by decreasing frequency of occurrence in the corpus). In the sequel, we shall equate each word w with , its index in this ordering, so that u(w ) \u2261 u( ), and so on. For S shards, the vocabulary size can thus be scaled up by as much as a factor of S relative to a single machine. The vectors are initialized in the parameter server shards as in <cite>[23]</cite> . Multiple clients running on cluster nodes then read in different portions of the corpus and interact with the parameter server shards to carry out minibatch stochastic gradient descent (SGD) optimization of (1) over the word vectors, following the algorithm in Figure 7 (in the appendix). Specifically, the corpus is partitioned into disjoint minibatches with index sets B1, B2, . . . , BN wherein each B h is a subset of (sentence index, word index) pairs. For each B h the word vectors are adjusted based on the gradient of the summation (1) restricted to the input words belonging to B h , as given by",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_14",
  "x": "For example, consider a pair of indices (io, jo) belonging to B h . The gradient components corresponding to the word vector u(wi o ,jo ) can be expressed as We see that evaluation of \u2207\u039b(B h ) where \u03b1 is a (suitably small) learning rate. Once a client has assembled the indices (indexing according to the order O above) of positive output examples and input words corresponding to a minibatch B h , it interacts with the parameter server shards to compute (4) and (5) using two remote procedure calls (RPCs), dotprod and adjust, which are broadcasted to all PS shards, along with an intervening computation to aggregate results from the dotprod RPC returned by each shard. The RPC calls are detailed in Figures 5 and 6 (in the Appendix), and, at a higher level, entail the following server/shard side operations: \u2022 dotprod: Select negative examplesw in (4) according to a probability distribution derived from the vocabulary histogram proposed in <cite>[23]</cite> , but with the client thread supplied seed initializing the random number generation, and then return all partial dot products required to evaluate the gradient (4) for all positive output, negative output, and input word vectors associated with the minibatch, wherein the partial dot products involve those vector components stored on the designated shard: usv T s .",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_15",
  "x": "These weights are then passed to the adjust RPC, along with the seeds for regenerating the identical random negative example indicesw that were generated during the dotprod RPC. The retransmission simplifies the server in that state need not be maintained between corresponding dotprod and adjust calls. Note that the same seeds are sent to all shards in both calls so that each shard generates the same set of negative example indices. The shards are multithreaded and each thread handles the stream of RPC's coming from all client threads running on a single node. In a typical at scale run of the algorithm, the above process is carried out by multiple client threads running on each of a few hundred nodes, all interacting with the PS shards in parallel. The data set is iterated over multiple times and after each iteration, the learning rate \u03b1 is reduced in a manner similar to the open source implementation of <cite>[23]</cite> . Note that there is no locking or synchronization of the word vector state within or across shards or across client threads during any part of the computation.",
  "y": "similarities"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_16",
  "x": "**BENCHMARK DATA SET** To compare the proposed distributed system we trained vectors on a publicly available data set collected and processed by the script 'demo-train-big-model-v1-compute-only.sh' from the open-source package of <cite>[23]</cite> . This script collects a variety of publicly available text corpuses and processes them using the algorithm described in <cite>[23]</cite> to coalesce sufficiently co-occurring words into phrases. We then randomly shuffled the order of sentences (delimited by new line) in the data set, retaining order of words within each sentence. The resulting data set has about 8 billion words and yields a vocabulary of about 7 million words and phrases (based on a cut off of 5 occurrences in the data set). We evaluated accuracy on the phrase analogies in the 'question-phrases.txt' file and also evaluated Spearman's rank correlation with respect to the editorial evaluation of semantic relatedness of pairs of words in the well known wordsim-353 collection [14] . The results are shown in Table 1 .",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_17",
  "x": "This script collects a variety of publicly available text corpuses and processes them using the algorithm described in <cite>[23]</cite> to coalesce sufficiently co-occurring words into phrases. We then randomly shuffled the order of sentences (delimited by new line) in the data set, retaining order of words within each sentence. The resulting data set has about 8 billion words and yields a vocabulary of about 7 million words and phrases (based on a cut off of 5 occurrences in the data set). We evaluated accuracy on the phrase analogies in the 'question-phrases.txt' file and also evaluated Spearman's rank correlation with respect to the editorial evaluation of semantic relatedness of pairs of words in the well known wordsim-353 collection [14] . The results are shown in Table 1 . The first column shows results for the single machine implementation of <cite>[23]</cite> , the second for a 'low parallelism' configuration of our system using 50 Spark executors, minibatch size of 1, and 1 thread per executor, and the third column for a 'high parallelism' configuration again with 50 executors, but with minibatch size increased to 50 and 8 threads per executor. The various systems were run using the skipgram variant with 500 dimensional vectors, maximum window size of 20 (10 in each direction), 5 negative examples, subsample ratio of 1e-6 (see <cite>[23]</cite> ), initial learning rate of 0.01875, and 3 iterations over the data set.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_18",
  "x": "The results are shown in Table 1 . The first column shows results for the single machine implementation of <cite>[23]</cite> , the second for a 'low parallelism' configuration of our system using 50 Spark executors, minibatch size of 1, and 1 thread per executor, and the third column for a 'high parallelism' configuration again with 50 executors, but with minibatch size increased to 50 and 8 threads per executor. The various systems were run using the skipgram variant with 500 dimensional vectors, maximum window size of 20 (10 in each direction), 5 negative examples, subsample ratio of 1e-6 (see <cite>[23]</cite> ), initial learning rate of 0.01875, and 3 iterations over the data set. It can be seen that the vectors trained by the 'high parallelism' configuration of the proposed system, which is the closest to the configurations required for acceptable training latency in the large-scale sponsored search application, suffers only a modest loss in quality as measured by these tests. Note that this data set is more challenging for our system than the sponsored search data set, as it is less sparse and there is on average more overlap between words in different minibatches. In fact, if we attempt to increase the parallelism to 200 executors as was used for the training of the vectors described in the next subsection, training fails to converge altogether. We are unsure why our system yields better results than the implementation of <cite>[23]</cite> on the wordsim test, yet worse scores on the analogies test.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_19",
  "x": "The resulting data set has about 8 billion words and yields a vocabulary of about 7 million words and phrases (based on a cut off of 5 occurrences in the data set). We evaluated accuracy on the phrase analogies in the 'question-phrases.txt' file and also evaluated Spearman's rank correlation with respect to the editorial evaluation of semantic relatedness of pairs of words in the well known wordsim-353 collection [14] . The results are shown in Table 1 . The first column shows results for the single machine implementation of <cite>[23]</cite> , the second for a 'low parallelism' configuration of our system using 50 Spark executors, minibatch size of 1, and 1 thread per executor, and the third column for a 'high parallelism' configuration again with 50 executors, but with minibatch size increased to 50 and 8 threads per executor. The various systems were run using the skipgram variant with 500 dimensional vectors, maximum window size of 20 (10 in each direction), 5 negative examples, subsample ratio of 1e-6 (see <cite>[23]</cite> ), initial learning rate of 0.01875, and 3 iterations over the data set. It can be seen that the vectors trained by the 'high parallelism' configuration of the proposed system, which is the closest to the configurations required for acceptable training latency in the large-scale sponsored search application, suffers only a modest loss in quality as measured by these tests. Note that this data set is more challenging for our system than the sponsored search data set, as it is less sparse and there is on average more overlap between words in different minibatches.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_20",
  "x": "It can be seen that the vectors trained by the 'high parallelism' configuration of the proposed system, which is the closest to the configurations required for acceptable training latency in the large-scale sponsored search application, suffers only a modest loss in quality as measured by these tests. Note that this data set is more challenging for our system than the sponsored search data set, as it is less sparse and there is on average more overlap between words in different minibatches. In fact, if we attempt to increase the parallelism to 200 executors as was used for the training of the vectors described in the next subsection, training fails to converge altogether. We are unsure why our system yields better results than the implementation of <cite>[23]</cite> on the wordsim test, yet worse scores on the analogies test. We also note that the analogies test scores reported here involve computing the closest vector for each analogy \"question\" over the entire vocabulary and not just over the 1M most frequent words, as in the script 'demo-train-big-model-v1-computeonly.sh' of <cite>[23]</cite> . ---------------------------------- **SPONSORED SEARCH DATA SET**",
  "y": "differences"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_21",
  "x": "The first column shows results for the single machine implementation of <cite>[23]</cite> , the second for a 'low parallelism' configuration of our system using 50 Spark executors, minibatch size of 1, and 1 thread per executor, and the third column for a 'high parallelism' configuration again with 50 executors, but with minibatch size increased to 50 and 8 threads per executor. The various systems were run using the skipgram variant with 500 dimensional vectors, maximum window size of 20 (10 in each direction), 5 negative examples, subsample ratio of 1e-6 (see <cite>[23]</cite> ), initial learning rate of 0.01875, and 3 iterations over the data set. It can be seen that the vectors trained by the 'high parallelism' configuration of the proposed system, which is the closest to the configurations required for acceptable training latency in the large-scale sponsored search application, suffers only a modest loss in quality as measured by these tests. Note that this data set is more challenging for our system than the sponsored search data set, as it is less sparse and there is on average more overlap between words in different minibatches. In fact, if we attempt to increase the parallelism to 200 executors as was used for the training of the vectors described in the next subsection, training fails to converge altogether. We are unsure why our system yields better results than the implementation of <cite>[23]</cite> on the wordsim test, yet worse scores on the analogies test. We also note that the analogies test scores reported here involve computing the closest vector for each analogy \"question\" over the entire vocabulary and not just over the 1M most frequent words, as in the script 'demo-train-big-model-v1-computeonly.sh' of <cite>[23]</cite> .",
  "y": "differences"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_22",
  "x": "sional vector, 15 PS shard settings described in Section 6.2. We found the vector quality demonstrated in Figure 4 to be the norm based on inspections of similar matchings of query vectors to a number of ad vectors. We also compared the cosine similarities for pairs of vectors trained using the proposed distributed system and for corresponding vector pairs trained using the open-source implementation of <cite>[23]</cite> , again on a large search session data set. The former was trained using a vocabulary of 200 million generalized words while the latter was trained using about 90 million words which is the most that could fit onto a specialized large memory machine. For a set of 7,560 generalized word pairs with words common to the vocabularies trained by the respective systems we found very good agreement in cosine similarities between the corresponding vectors from the two systems, with over 50% of word pairs having cosine similarity differences less than 0.06, and 91% of word pairs having differences less than 0.1. ---------------------------------- **ONLINE A/B TESTS**",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_23",
  "x": "The former was trained using a vocabulary of 200 million generalized words while the latter was trained using about 90 million words which is the most that could fit onto a specialized large memory machine. For a set of 7,560 generalized word pairs with words common to the vocabularies trained by the respective systems we found very good agreement in cosine similarities between the corresponding vectors from the two systems, with over 50% of word pairs having cosine similarity differences less than 0.06, and 91% of word pairs having differences less than 0.1. ---------------------------------- **ONLINE A/B TESTS** Following successful offline evaluation of the proposed distributed system, in the following set of experiments we conducted tests on live web search traffic. We ran two bucket tests, each on 5% of search traffic, where we compared queryad matches produced by training query and ad vectors using search session data set spanning 9 months of search data. One model was trained using implementation from <cite>[23]</cite> and the other was trained using the proposed distributed system.",
  "y": "uses"
 },
 {
  "id": "99aebc86f34ace0133c9f0922373fe_0",
  "x": "The presentation of this tutorial is arranged into five parts. First of all, we share the current status of researches on natural language understanding, statistical modeling and deep neural network and explain the key issues in deep Bayesian learning for discrete-valued observation data and latent semantics. A new paradigm called the symbolic neural learning is introduced to extend how data analysis is performed from language processing to semantic learning and memory networking. Secondly, we address a number of Bayesian models ranging from latent variable model to VB inference (Chien and Chang, 2014; Chien and Chueh, 2011; Chien, 2015b) , MCMC sampling (Watanabe and Chien, 2015) and BNP learning (Chien, 2016; Chien, 2015a; Chien, 2018) for hierarchical, thematic and sparse topics from natural language. In the third part, a series of deep models including deep unfolding (Chien and Lee, 2018) , Bayesian RNN (Gal and Ghahramani, 2016; Chien and Ku, 2016) , sequence-to-sequence learning (Graves et al., 2006; <cite>Gehring et al., 2017)</cite> , CNN (Kalchbrenner et al., 2014; Xingjian et al., 2015; , GAN (Tsai and Chien, 2017) and VAE are introduced. The coffee break is arranged within this part. Next, the fourth part focuses on a variety of advanced studies which illustrate how deep Bayesian learning is developed to infer the sophisticated recurrent models for natural language understanding.",
  "y": "background"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_0",
  "x": "****SPEECH2VEC: A SEQUENCE-TO-SEQUENCE FRAMEWORK FOR LEARNING WORD EMBEDDINGS FROM SPEECH**** **ABSTRACT** In this paper, we propose a novel deep neural network architecture, Speech2Vec, for learning fixed-length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the underlying spoken words, and are close to other vectors in the embedding space if their corresponding underlying spoken words are semantically similar. The proposed model can be viewed as a speech version of Word2Vec <cite>[1]</cite>. Its design is based on a RNN Encoder-Decoder framework, and borrows the methodology of skipgrams or continuous bag-of-words for training. Learning word embeddings directly from speech enables Speech2Vec to make use of the semantic information carried by speech that does not exist in plain text. The learned word embeddings are evaluated and analyzed on<cite> 1</cite>3 widely used word similarity benchmarks, and outperform word embeddings learned by Word2Vec from the transcriptions.",
  "y": "extends"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_1",
  "x": "Its design is based on a RNN Encoder-Decoder framework, and borrows the methodology of skipgrams or continuous bag-of-words for training. Learning word embeddings directly from speech enables Speech2Vec to make use of the semantic information carried by speech that does not exist in plain text. The learned word embeddings are evaluated and analyzed on<cite> 1</cite>3 widely used word similarity benchmarks, and outperform word embeddings learned by Word2Vec from the transcriptions. ---------------------------------- **INTRODUCTION** Natural language processing (NLP) techniques such as Word2Vec<cite> [1,</cite> 2] and GloVe [3] transform words into fixed dimensional vectors, or word embeddings. The embeddings are obtained via unsupervised learning from co-occurrence information in text, and contain semantic information about the word which are useful for many NLP tasks [4, 5, 6, 7, 8] .",
  "y": "background"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_2",
  "x": "**INTRODUCTION** Natural language processing (NLP) techniques such as Word2Vec<cite> [1,</cite> 2] and GloVe [3] transform words into fixed dimensional vectors, or word embeddings. The embeddings are obtained via unsupervised learning from co-occurrence information in text, and contain semantic information about the word which are useful for many NLP tasks [4, 5, 6, 7, 8] . Researchers have also explored the concept of learning vector representations from speech [9,<cite> 1</cite>0,<cite> 1</cite>1,<cite> 1</cite>2,<cite> 1</cite>3,<cite> 1</cite>4] . These approaches are based on notions of acoustic-phonetic (rather than semantic) similarity, so that different instances of the same underlying word would map to the same point in a latent embedding space. Our work, highly inspired by Word2Vec <cite>[1]</cite> , uses a skipgrams or continuous bag-of-words formulation to focus on neighboring acoustic regions, rather than the acoustic segment associated with the word itself. We show that the resulting acoustic embedding space is more semantic in nature.",
  "y": "motivation"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_9",
  "x": "We conducted experiments using the specified architecture since it produced the most stable and satisfactory results. ---------------------------------- **EVALUATION** Existing schemes for evaluating methods for word embeddings fall into two major categories: extrinsic and intrinsic [29] . With the extrinsic method, the learned word embeddings are used as input features to a downstream task [4, 5, 6, 7, 8] , and the performance metric varies from task to task. The intrinsic method directly tests for semantic or syntactic relationships between words, and includes the tasks of word similarity and word analogy <cite>[1]</cite> . In this paper, we focus on the intrinsic method, especially the word similarity task, for evaluating and analyzing the Speech2Vec word embeddings.",
  "y": "uses"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_21",
  "x": "This result aligns with the empirical fact that skipgrams Word2Vec is likely to work better than cbow Word2Vec with small training corpus size <cite>[1]</cite> . Training size <cite> 1</cite>0%  40%  70% <cite> 1</cite>00% <cite> 1</cite>0%  40%  70% <cite> 1</cite>00% <cite> 1</cite>0%  40%  70% <cite> 1</cite>00% <cite> 1</cite>0%  40%  70% Impact of training corpus size. From Table 2 we observe that when<cite> 1</cite>0% of the corpus was used for training, the resulting word embeddings perform poorly. Unsurprisingly, the performance continues to improve as training size increases. ---------------------------------- **VARIANCE STUDY** In Section 2.3 we mention that in Speech2Vec, every instance of a spoken word will produce a different embedding vector.",
  "y": "similarities"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_26",
  "x": "---------------------------------- **CONCLUSIONS AND FUTURE WORK** Speech2Vec, which integrates a RNN Encoder-Decoder framework with skipgrams or cbow for training, extends the textbased Word2Vec <cite>[1]</cite> model to learn word embeddings directly from speech. Speech2Vec has access to richer information in the speech signal that does not exist in plain text. In our experiments, the learned word embeddings outperform those produced by Word2Vec from the transcriptions. In the future, we plan to evaluate the word embeddings on speech-related extrinsic tasks such as machine listening comprehension [44, 45] and speech-based visual question answering [46] by initializing the embedding layers of the neural network models. Finally, in this work, some supervision was incorporated into the learning by using forced alignment segmentations as the basis for audio segments.",
  "y": "extends"
 },
 {
  "id": "9a52e0ea1f12e3455fca48ac8f8936_1",
  "x": "Recurrent neural networks (RNNs) being well suited to dealing with sequences of vectors, have found much success in NLP by leveraging the sequential structure of language, A variant of RNNs known as Long Short-Term Memory Networks (LSTMs) [5] have particularly been widely used and stands as the state of the art technique for language modeling on benchmark datasets such as Penn Treebank (PTB) [1] and One billion words [6] among others. Language models (LMs) by themselves are valuable because well trained LMs improve the underlying metrics of downstream tasks such as word error rate for speech recognition, BLEU score for translation. In addition, LMs compactly extract knowledge encoded in training data [7] . The current state of the art on modeling both PTB and WikiText 2 [8] datasets as reported in <cite>[4]</cite> shows little sensitivity to hyper parameters; sharing almost all hyper parameters values between both datasets. In [9] , its is also shown that deep learning model can jointly learn a number of large-scale tasks from multiple domains by designing a multi-modal architecture in which as many parameters as possible are shared. Training and evaluating a neural network involves mapping the hyper parameter configuration (set of values for each hyper parameter) used in training the network to the validation error obtained at the end. Strategies for searching and obtaining an optimal configuration that have been applied and found considerable success include grid search, random search, Bayesian optimization, Sequential Model-based Bayesian Optimization (SMBO) [10] , deterministic hyperparameter optimization methods that employs radial basis functions as error surrogates proposed by [11] , Gaussian Process Batch Upper Confidence Bound (GP-BUCB) [12] ; an upper confidence bound-based algorithm, which models the reward function as a sample from a Gaussian process.",
  "y": "background"
 },
 {
  "id": "9a52e0ea1f12e3455fca48ac8f8936_2",
  "x": "In [9] , its is also shown that deep learning model can jointly learn a number of large-scale tasks from multiple domains by designing a multi-modal architecture in which as many parameters as possible are shared. Training and evaluating a neural network involves mapping the hyper parameter configuration (set of values for each hyper parameter) used in training the network to the validation error obtained at the end. Strategies for searching and obtaining an optimal configuration that have been applied and found considerable success include grid search, random search, Bayesian optimization, Sequential Model-based Bayesian Optimization (SMBO) [10] , deterministic hyperparameter optimization methods that employs radial basis functions as error surrogates proposed by [11] , Gaussian Process Batch Upper Confidence Bound (GP-BUCB) [12] ; an upper confidence bound-based algorithm, which models the reward function as a sample from a Gaussian process. In [13] , the authors propose initializing Bayesian hyper parameters using meta-learning. The idea being initializing the configuration space for a novel dataset based on configurations that are known to perform well on similar, previously evaluated, datasets. Following a meta-learning approach, we apply a genetic algorithm and a sequential search algorithm, described in the next section, initialized using the best configuration reported in <cite>[4]</cite> to search the space around optimal hyper parameters for the AWD-LSTM model. Twitter tweets collected using a geolocation filter for Nigeria and Kenya with the goal of acquiring a code-mixed text corpus serve as our evaluation datasets.",
  "y": "uses"
 },
 {
  "id": "9a52e0ea1f12e3455fca48ac8f8936_3",
  "x": "We begin our work by establishing what the baseline and current state of the art model is for a language modeling task <cite>[4]</cite> . Applying the AWD-LSTM model, based on the open sourced code and trained on code-mixed Twitter data, we sample 84 different hyper parameter configurations for each dataset, and evaluate the resulting test perplexity distributions while varying individual hyperparameter values to understand the effect of the set of hyper parameter values selected on the model perplexity. ---------------------------------- **A. DATASETS** Two sources of data are collected using the Twitter streaming API with a geolocation filter set to geo-cordinates for Kenya and Nigeria. The resulting data is code-mixed with the Kenya corpus (Dataset 1) containing several mixes of English and Swahili both official languages in Kenya. The Nigeria data (Dataset 2) on the other hand, does not predominantly contain mixes of English with another language in the same sentence.",
  "y": "background"
 },
 {
  "id": "9a52e0ea1f12e3455fca48ac8f8936_4",
  "x": "Each hyper parameter configuration in the subsequent generation is derived from two parent configurations selected via this approach. Mimicking biological crossover chromosomes [15] , the two configurations selected are mixed, and one of the resulting configurations are selected at random. Finally, a random subset of the components of each derived configuration is perturbed by adding noise. This sequence of processes define how configurations from a current generation are used to derive the next generation. ---------------------------------- **E. META-LEARNING INITIALIZATION** Both the population based and sequential search space were manually initialized with four (4) values of each hyperparameter in the neighbourhood of the best values reported in <cite>[4]</cite> as shown in Table I .",
  "y": "similarities"
 },
 {
  "id": "9a52e0ea1f12e3455fca48ac8f8936_6",
  "x": "In this work we assess the performance of the AWD-LSTM model <cite>[4]</cite> for language modeling to better understand how relevant the published hyper parameters may be for a codemixed corpus and to isolate which hyper parameters could be further tuned to improve performance. Our results show that as a whole, the set of hyperparameters considered the best <cite>[4]</cite> are reasonably good, however ther are better sets hyperparamers for the code-mixed corpora. Moreover, even with the best set of hyper parameters, the perplexity observed for our data are significantly higher (i.e. performance is worse at the task of language modeling) than the performance demonstrated in the literature. Finally, our implemented approach is one that not only enables confirmation of the goodness of the hyper parameters values, but we can also develop inferences about which hyper parameter values would yield better results. ---------------------------------- **II. BACKGROUND** Deeplearning has found sucess in various applications including natural language processing tasks such as language modeling, parts of speech tagging, summarization and many others.",
  "y": "uses"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_0",
  "x": "---------------------------------- **RELATED WORK** Automating word sense disambiguation tasks based on annotated corpora have been proposed. Examples of supervised learning methods for WSD appear in [2] [3] [4] , [7] [8] . The learning algorithms applied including: decision tree, decisionlist [15] , neural networks [7] , na\u00efve Bayesian learning ( [5] , [11] ) and maximum entropy <cite>[10]</cite> . Among these leaning methods, the most important issue is what features will be used to construct the classifier. It is common in WSD to use contextual information that can be found in the neighborhood of the ambiguous word in training data ( [6] , [16] [17] [18] ).",
  "y": "background"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_1",
  "x": "Most of the recent works for English corpus including [7] and [8] , which combine both local and topical information in order to improve their performance. An interesting study on feature selection for Chinese <cite>[10]</cite> has considered topical features as well as local collocational, syntactic, and semantic features using the maximum entropy model. In Dang's <cite>[10]</cite> work, collocational features refer to the local PoS information and bi-gram co-occurrences of words within 2 positions of the ambiguous word. A useful result from this work based on (about one million words) the tagged People's Daily News shows that adding more features from richer levels of linguistic information such as PoS tagging yielded no significant improvement (less than 1%) over using only the bi-gram co-occurrences information. Another similar study for Chinese [11] is based on the Naive Bayes classifier model which has taken into consideration PoS with position information and bi-gram templates in the local context. The system has a reported 60.40% in both precision and recall based on the SENSEVAL-3 Chinese training data. Even though in both approaches, statistically significant bi-gram co-occurrence information is used, they are not necessarily true collocations.",
  "y": "background"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_2",
  "x": "It is also generally true that the nearby context words of an ambiguous word give more effective patterns and features values than those far from it [12] . The existing methods consider features selection for context representation including both local and topic features where local features refer to the information pertained only to the given context and topical features are statistically obtained from a training corpus. Most of the recent works for English corpus including [7] and [8] , which combine both local and topical information in order to improve their performance. An interesting study on feature selection for Chinese <cite>[10]</cite> has considered topical features as well as local collocational, syntactic, and semantic features using the maximum entropy model. In Dang's <cite>[10]</cite> work, collocational features refer to the local PoS information and bi-gram co-occurrences of words within 2 positions of the ambiguous word. A useful result from this work based on (about one million words) the tagged People's Daily News shows that adding more features from richer levels of linguistic information such as PoS tagging yielded no significant improvement (less than 1%) over using only the bi-gram co-occurrences information. Another similar study for Chinese [11] is based on the Naive Bayes classifier model which has taken into consideration PoS with position information and bi-gram templates in the local context.",
  "y": "background"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_3",
  "x": "Early researches have proven that using lexical statistical information, such as bi-gram co-occurrences was sufficient to produce close to the best results <cite>[10]</cite> for Chinese WSD. Instead of including bi-gram features as part of discrimination features, in our system, we consider both topical contextual features as well as local collocation features. These features are extracted form the 60MB human sense-tagged People's Daily News with segmentation information. ---------------------------------- **TOPICAL CONTEXTUAL FEATURES** Niu [11] proved in his experiments that Na\u00efve Bayes classifier achieved best disambiguation accuracy with small topical context window size (< 10 words). We follow their method and set the contextual window size as 10 in our system.",
  "y": "background"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_4",
  "x": "Domminic [19] used three different methods called bilingual method, collocation method and UMLS (Unified Medical Language System) relation based method to disambiguate unsupervised English and German medical documents. As expected, the collocation method achieved a good precision around 79% in English and 82% in German but a very low recall which is 3% in English and 1% in German. The low recall is due to the nature of UMLS where many collocations would almost never occur in natural text. To avoid this problem, we combine the contextual features in the target context with the pre-prepared collocations list to build our classifier. As stated early, an important issue is what features will be used to construct the classifier in WSD. Early researches have proven that using lexical statistical information, such as bi-gram co-occurrences was sufficient to produce close to the best results <cite>[10]</cite> for Chinese WSD. Instead of including bi-gram features as part of discrimination features, in our system, we consider both topical contextual features as well as local collocation features.",
  "y": "differences"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_5",
  "x": "Collocations can be classified as fully fixed collocations, fixed collocations, strong collocations and loose collocations. Fixed collocations means the appearance of one word implies the co-occurrence of another one such as \" \" (\"burden of history\"), while strong collocations allows very limited substitution of the components, for example, \" \" (\"local college\"), or \" \" (\"local university\"). The sense of ambiguous words can be uniquely determined in these two types of collocations, therefore are the collocations applied in our system. The sources of the collocations will be explained in Section 4.1. In both Niu [11] and Dang's <cite>[10]</cite> work, topical features as well as the so called collocational features were used. However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features.",
  "y": "background"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_6",
  "x": "In both Niu [11] and Dang's <cite>[10]</cite> work, topical features as well as the so called collocational features were used. However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features. However, bi-gram co-occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations. Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD. The local features in our system make use of the collocations using the template (w i , w) within a window size of ten (where i = \u00b1 5). For example, \" \" (\"Government departments and local government commanded that\") fits the bi-gram collocation template (w, w 1 ) with the value of ( ). During the training and the testing processes, the counting of frequency value of the collocation feature will be increased by 1 if a collocation containing the ambiguous word occurs in a sentence.",
  "y": "differences"
 },
 {
  "id": "9ea99bf57e9113b2f03f2285741397_0",
  "x": "The attention mechanism helps the decoder to generate meaningful and grammatical sentences without needing any sequence alignment. This idea is also in line with code-mixing by borrowing words from the embedded language [9] and intuitively, the copying mechanism can be seen as an end-to-end approach to translate, align, and reorder the given words into a grammatical code-switching sentence. This approach is the unification of all components in the work of [6] into a single computational model. A code-switching language model learned in this way is able to capture the patterns and constraints of the switches and mitigate the out-of-vocabulary (OOV) issue during sequence generation. By adding the generated sentences and incorporating syntactic information to the training data, we achieve better performance by 10% compared to an LSTM baseline <cite>[10]</cite> and 5% to the equivalent constraint. ---------------------------------- **RELATED WORK**",
  "y": "differences"
 },
 {
  "id": "9ea99bf57e9113b2f03f2285741397_1",
  "x": "A multi-task learning approach was introduced to train the syntax representation of languages by constraining the language generator <cite>[10]</cite> . A copy mechanism was proposed to copy words directly from the input to the output using an attention mechanism [16] . This mechanism has proven to be effective in several NLP tasks including text summarization [8] , and dialog systems [17] . The common characteristic of these tasks is parts of the output are exactly the same as the input source. For example, in dialog systems the responses most of the time have appeared in the previous dialog steps. ---------------------------------- **METHODOLOGY**",
  "y": "background"
 },
 {
  "id": "9ea99bf57e9113b2f03f2285741397_2",
  "x": "The quality of the generated code-switching sentences is evaluated using a language modeling task. Indeed, if the perplexity in this task drops consistently we can assume that the generated sentences are well-formed. Hence, we use an LSTM language model with weight tying [20] that can capture an unbounded number of context words to approximate the probability of the next word. Syntactic information such as Partof-speech (POS) [p 1 , ..., p T ] is added to further improve the performance. The POS tags are generated phrase-wise using pretrained English and Chinese Stanford POS Tagger [21] by adding a word at a time in a unidirectional way to avoid any intervention from future information. The word and syntax unit are represented as a vector x w and x p respectively. Next, we concatenate both vectors and use it as an input [x w |x p ] to an LSTM layer similar to <cite>[10]</cite> . 4.",
  "y": "similarities"
 },
 {
  "id": "9ea99bf57e9113b2f03f2285741397_3",
  "x": "EXPERIMENT ---------------------------------- **CORPUS** In our experiment, we use a conversational Mandarin-English code-switching speech corpus called SEAME Phase II (South East Asia Mandarin-English). The data are collected from spontaneously spoken interviews and conversations in Singapore and Malaysia by bilinguals [22] . As the data preprocessing, words are tokenized using Stanford NLP toolkit [23] and all hesitations and punctuations were removed except apostrophe. The split of the dataset is identical to <cite>[10]</cite> and it is showed in Table 1 .",
  "y": "similarities"
 },
 {
  "id": "9f1d2be80dbfd726a24fb2a05e130b_0",
  "x": "Transliteration of named entities is the essential part of many multilingual applications, such as machine translation (Koehn, 2010) and cross-language information retrieval (Jadidinejad and Mahmoudi, 2010) . Recent studies pay a great attention to the task of Neural Machine Translation (Cho et al., 2014a; <cite>Sutskever et al., 2014</cite>) . In neural machine translation, a single neural network is responsible for reading a source sentence and generates its translation. From a probabilistic perspective, translation is equivalent to finding a target sentence y that maximizes the conditional probability of y given a source sentence x, i.e., arg max y p(y | x). The whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence, using the bilingual corpus. Transforming a name from spelling to phonetic and then use the constructed phonetic to generate the spelling on the target language is a very complex task (Oh et al., 2006; Finch et al., 2015) . Based on successful studies on Neural Machine Translation (Cho et al., 2014a; <cite>Sutskever et al., 2014</cite>; Hirschberg and Manning, 2015) , in this paper, we proposed a character-based encoderdecoder model which learn to transliterate endto-end.",
  "y": "background"
 },
 {
  "id": "9f1d2be80dbfd726a24fb2a05e130b_1",
  "x": "where c t is a context vector computed by a soft attention mechanism: The soft attention mechanism f a weights each vector in the context set C according to its relevance given what has been transliterated. Finally, the hidden state h t , together with the previous target symbol y t \u22121 and the context vector c t , is fed into a feedforward neural network to result in the conditional distribution described in Equation 2. The whole model, consisting of the encoder, decoder and soft attention mechanism, is trained end-to-end to minimize the negative loglikelihood using stochastic gradient descent. ---------------------------------- **EXPERIMENTS** We conducted a set of experiments to show the effectiveness of <cite>RNN Encoder-Decoder model</cite> (Cho et al., 2014b; <cite>Sutskever et al., 2014</cite>) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task .",
  "y": "uses"
 },
 {
  "id": "9f1d2be80dbfd726a24fb2a05e130b_2",
  "x": "Since we leverage a same model for different datasets without tuning the model for each dataset, differences in the learning curves are expectable. For some datasets (such as 'En-Ch'), it takes more time to fit the model to the training data while for some others (such as 'En-He'), the model fit to the training data after a few iterations. ---------------------------------- **CONCLUSION** In this paper we proposed Neural Machine Transliteration based on successful studies in sequence to sequence learning (<cite>Sutskever et al., 2014</cite>) and Neural Machine Translation (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Bahdanau et al., 2015; Cho et al., 2014a) . Neural Machine Transliteration typically consists of two components, the first of which encodes a source name sequence x and the second decodes to a target name sequence y. Different parts of the proposed model jointly trained using stochastic gradient descent to minimize the log-likelihood. Experiments on different datasets using benchmark measures revealed that the proposed model is able to achieve significantly higher transliteration quality over traditional statistical models (Koehn, 2010) .",
  "y": "uses"
 },
 {
  "id": "9f1d2be80dbfd726a24fb2a05e130b_3",
  "x": "The whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence, using the bilingual corpus. Transforming a name from spelling to phonetic and then use the constructed phonetic to generate the spelling on the target language is a very complex task (Oh et al., 2006; Finch et al., 2015) . Based on successful studies on Neural Machine Translation (Cho et al., 2014a; <cite>Sutskever et al., 2014</cite>; Hirschberg and Manning, 2015) , in this paper, we proposed a character-based encoderdecoder model which learn to transliterate endto-end. In the opposite side of classical models which contains different components, the proposed model is trained end-to-end, so it able to apply to any language pairs without tuning for a spacific one. ---------------------------------- **PROPOSED MODEL** Here, we describe briefly the underlying framework, called <cite>RNN Encoder-Decoder</cite>, proposed by (Cho et al., 2014b) and <cite>(Sutskever et al., 2014)</cite> upon which we build a machine transliteration model that learns to transliterate end-to-end.",
  "y": "uses background"
 },
 {
  "id": "9f1d2be80dbfd726a24fb2a05e130b_4",
  "x": "In the opposite side of classical models which contains different components, the proposed model is trained end-to-end, so it able to apply to any language pairs without tuning for a spacific one. ---------------------------------- **PROPOSED MODEL** Here, we describe briefly the underlying framework, called <cite>RNN Encoder-Decoder</cite>, proposed by (Cho et al., 2014b) and <cite>(Sutskever et al., 2014)</cite> upon which we build a machine transliteration model that learns to transliterate end-to-end. The enoder is a character-based recurrent neural network that learns a highly nonlinear mapping from a spelling to the phonetic of the input sequence. This network reads the source name x = (x 1 , . . . , x T ) and encodes it into a sequence of hidden states h = (h 1 , \u00b7 \u00b7 \u00b7 , h T ): Each hidden state h i is a bidirectional recurrent representation with forward and backward sequence information around the ith character.",
  "y": "uses background"
 },
 {
  "id": "9fdeb20207af1e8ee0c6e5374e3731_0",
  "x": "**RESEARCH** The SINNET system is the result of several years of research<cite> Agarwal et al., 2012</cite>; Agarwal et al., 2013) . In , we introduced the notion of social events. A social event is a happening between two people, at least one of whom is cognizant of the other and of the event taking place. At a broad level, there are two types of social events: interaction (INR) and observation (OBS). INR is a bi-directional event in which both parties are mutually aware of each other. Examples of INR are a meeting or a dinner.",
  "y": "background"
 },
 {
  "id": "9fdeb20207af1e8ee0c6e5374e3731_1",
  "x": "At a broad level, there are two types of social events: interaction (INR) and observation (OBS). INR is a bi-directional event in which both parties are mutually aware of each other. Examples of INR are a meeting or a dinner. OBS is a one-directional event in which only one party is aware of the other. Examples of OBS are thinking about someone, or missing someone. In , we presented a preliminary system that uses tree kernels and Support Vector Machines (SVMs) to extract social events from news articles. In<cite> Agarwal et al. (2012)</cite> , we presented a case study on a manually extracted network from Alice in Wonderland, showing that analyzing networks based on these social events gives us insight into the roles of characters in the story.",
  "y": "background"
 },
 {
  "id": "9fdeb20207af1e8ee0c6e5374e3731_2",
  "x": "There is an OBS event between Alice and Rabbit triggered by the word in bold -saw. The direction of the event is from the observer to the one being observed. In the second figure there are two entity mentions: Alice and Mouse. There is a bidirectional interaction link between the Alice and Mouse triggered by the word asked. Figure 2 shows the network extracted from an abridged version of Alice in Wonderland <cite>(Agarwal et al., 2012)</cite> . Figure 3 shows the output of running the Hubs and Authority algorithm (Kleinberg, 1998 ) on the network. In information retrieval, an authority is a webpage that many hubs point to and a hub is a webpage that points to many authorities.",
  "y": "background"
 },
 {
  "id": "9fdeb20207af1e8ee0c6e5374e3731_3",
  "x": "We see that the main character of the story, Alice, is the main authority but not the main hub. This network may be used for other In<cite> Agarwal et al. (2012)</cite> , we argued that a static network does not bring out the true nature of a network. For example, even though the centrality of the Mouse in a static network is high, a dynamic network analysis shows that the mouse is central only in one chapter of the novel (Chapter 3 -The drying ceremony). Figure 4 shows the the network at the end of chapter 1 and chapter 3. ---------------------------------- **SYSTEM DETAILS AND WEB DEMO** SINNET is fully implemented in Java.",
  "y": "background"
 },
 {
  "id": "a0614f13b4ed0c6370deb26032f62b_0",
  "x": "The details of the approach are interesting, particularly the insights about how to build linguistically rich grammars that can be effectively compiled into high-utility context-free grammars for speech recognition. The primary shortcoming of this presentation lies in perpetuating the false dichotomy between \"grammar-based\" and \"data-driven\" approaches to language modeling for speech recognition, which motivates the final chapter of the book. In fact, the authors' approach is both grammar-based and data-driven, given the corpus-based grammar specialization and PCFG estimation, which the authors themselves demonstrate to be indispensable. Robust grammar-based language modeling is a topic that has received a fair bit of attention over the past decade (Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang, Stolcke, and Harper 2004, among others) , and while this line of research has not focused on the use of manually built, narrow-domain feature grammars, there is enough similarity between the approach described in this book and the cited papers that the papers would seem to be better comparison points than the class-based language models that are chosen to represent robust approaches in the comparison. Beyond language modeling, methods for PCFG induction from treebanks have been a popular topic in the field over the past decade, and some understanding of the impact of flattening trees can be had in <cite>Johnson (1998)</cite> , where the beneficial impact of various tree transformations for probabilistic grammars is presented. None of this work is discussed or cited, and the naive reader might be left with the impression that data-driven approaches have been demonstrated to underperform relative to knowledge-based approaches, when in fact the authors simply demonstrate that their hybrid grammar-based/data-driven approach outperforms class-based language models. Perhaps this is worth demonstrating, but the chapter couches the results within the context of a clash between paradigms, which simply does not ring true.",
  "y": "background"
 },
 {
  "id": "a0614f13b4ed0c6370deb26032f62b_1",
  "x": "The primary shortcoming of this presentation lies in perpetuating the false dichotomy between \"grammar-based\" and \"data-driven\" approaches to language modeling for speech recognition, which motivates the final chapter of the book. In fact, the authors' approach is both grammar-based and data-driven, given the corpus-based grammar specialization and PCFG estimation, which the authors themselves demonstrate to be indispensable. Robust grammar-based language modeling is a topic that has received a fair bit of attention over the past decade (Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang, Stolcke, and Harper 2004, among others) , and while this line of research has not focused on the use of manually built, narrow-domain feature grammars, there is enough similarity between the approach described in this book and the cited papers that the papers would seem to be better comparison points than the class-based language models that are chosen to represent robust approaches in the comparison. Beyond language modeling, methods for PCFG induction from treebanks have been a popular topic in the field over the past decade, and some understanding of the impact of flattening trees can be had in <cite>Johnson (1998)</cite> , where the beneficial impact of various tree transformations for probabilistic grammars is presented. None of this work is discussed or cited, and the naive reader might be left with the impression that data-driven approaches have been demonstrated to underperform relative to knowledge-based approaches, when in fact the authors simply demonstrate that their hybrid grammar-based/data-driven approach outperforms class-based language models. Perhaps this is worth demonstrating, but the chapter couches the results within the context of a clash between paradigms, which simply does not ring true. This one misstep, however, does not detract from the quality of the authors' system, nor from the interesting presentation of too-often-ignored aspects of spoken language systems engineering.",
  "y": "motivation"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_0",
  "x": "It is also not robust to diversified datasets. In the following, we will introduce the tree-based solutions, which are widely adopted and become the mainstreaming solutions to arithmetic word problems. ---------------------------------- **TREE-BASED METHODS** The arithmetic expression can be naturally represented as a binary tree structure such that the operators with higher priority are placed in the lower level and the root of the tree contains the operator with the lowest priority. The idea of tree-based approaches <cite>[26]</cite> , [27] , [28] , [15] is to transform the derivation of the arithmetic expression to constructing an equivalent tree structure step by step in a bottom-up manner. One of the advantages is that there is no need for additional annotations such as equation template, tags or logic forms.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_1",
  "x": "Figure 3 shows two tree examples derived from the math word problem in Figure 2 . One is called expression tree that is used in <cite>[26]</cite> , [28] , [15] and the other is called equation tree in [27] . These two types of trees are essentially equivalent and result in the same solution, except that equation tree contains a node for the unknown variable x. The overall algorithmic framework among the tree-based approaches consists of two processing stages. In the first stage, the quantities are extracted from the text and form the bottom level of the tree. The candidate trees that are syntactically valid, but with different structures and internal nodes, are enumerated. In the second stage, a scoring function is defined to pick the best matching candidate tree, which will be used to derive the final solution. A common strategy among these algorithms is to build a local classifier to determine the likelihood of an operator being selected as the internal node.",
  "y": "uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_2",
  "x": "These two types of trees are essentially equivalent and result in the same solution, except that equation tree contains a node for the unknown variable x. The overall algorithmic framework among the tree-based approaches consists of two processing stages. In the first stage, the quantities are extracted from the text and form the bottom level of the tree. The candidate trees that are syntactically valid, but with different structures and internal nodes, are enumerated. In the second stage, a scoring function is defined to pick the best matching candidate tree, which will be used to derive the final solution. A common strategy among these algorithms is to build a local classifier to determine the likelihood of an operator being selected as the internal node. Such local likelihood is taken into account in the global scoring function to determine the likelihood of the entire tree. Roy et al. <cite>[26]</cite> proposed the first algorithmic approach that leverages the concept of expression tree to solve arithmetic word problems.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_3",
  "x": "To further reduce the tree enumeration space, beam search is applied in <cite>[26]</cite> . To generate the next state T \u2032 from the current partial tree, the algorithm avoids choosing all the possible pairs of terms and determining their operator. Instead, only top-k candidates with the highest partial scores are retained. Experimental results with k = 200 show that the strategy achieves a good balance between accuracy and running time. In [29] , the authors publish the service as a web tool and it can respond promptly to a math word problem. The solution in [27] , named ALGES, differs from <cite>[26]</cite> in two major ways. First, it adopts a more brutal-force manner to exploit all the possible equation trees.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_4",
  "x": "To further reduce the tree enumeration space, beam search is applied in <cite>[26]</cite> . To generate the next state T \u2032 from the current partial tree, the algorithm avoids choosing all the possible pairs of terms and determining their operator. Instead, only top-k candidates with the highest partial scores are retained. Experimental results with k = 200 show that the strategy achieves a good balance between accuracy and running time. In [29] , the authors publish the service as a web tool and it can respond promptly to a math word problem. The solution in [27] , named ALGES, differs from <cite>[26]</cite> in two major ways. First, it adopts a more brutal-force manner to exploit all the possible equation trees.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_5",
  "x": "The tree is built in a bottom-up manner. It is worth noting that to reduce the search space and simplify the tree construction, only adjacent nodes are combined to generate their parent node. UnitDep [28] can be viewed as an extension work of <cite>[26]</cite> by the same authors. An important concept, named Unit Dependency Graph (UDG), is proposed to enhance the scoring function. The vertices in UDG consist of the extracted quantities. If the quantity correspond to a rate (e.g., 8 dollars per hour), the vertex is marked as RATE. There are six types of edge relations to be considered, such as whether two quantities are associated with the same unit.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_7",
  "x": "We present their descriptions in the following and summarize the statistics of the datasets in Table 1. 1) AI2 [21] . There are 395 single-step or multi-step arithmetic word problems for the third, fourth, and fifth graders. It involves problems that can be solved with only addition and subtraction. The dataset is harvested from two websites: math-aids.com and ixl.com and comprises three subsets: MA1 (from math-aids.com), IXL (from ixl.com) and MA2 (from math-aids.com). Among them, IXL and MA2 are more challenging than MA1 because IXL contains more information gaps and MA2 includes more irrelevant information in its math problems. 2) IL <cite>[26]</cite> . The problems are collected from websites k5learning.com and dadsworksheets.com.",
  "y": "background uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_9",
  "x": "We present their descriptions in the following and summarize the statistics of the datasets in Table 1. 1) AI2 [21] . 3) CC <cite>[26]</cite> .",
  "y": "uses background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_10",
  "x": "For instance, all quantities and the main-goal are first identified by rules in LogicForm [24] , [25] and explicitly associated with their roletags. Thus, with sufficient human intervention, the accuracy of statistic-baesd methods in AI2 can boost to 88.64%, much higher than that of tree-based methods. Nevertheless, these statistic-based methods are considered as brittle and rigid [12] and not scalable to handle large and diversified datasets, primarily due to the heavy annotation cost to train an accurate mapping between the text and the logic representation. Second, the results of tree-based methods in AI2, IL and CC are collected from [15] where the same experimental setting of 3fold cross validation is applied. It is interesting to observe that ALGES [27] , ExpressionTree <cite>[26]</cite> and UNITDEP [28] cannot perform equally well on the three datasets. ALGES works poorly in AI2 because irrelevant quantities exist in its math problems and ALGES is not trained with a classifier to get rid of them. However, it outperforms ExpressionTree and UNITDEP by a wide As to the datasets of SingleEQ and AllArith, UNITDEP is a winner in both datasets, owning to the effectiveness of the proposed unit dependency graph (UDG).",
  "y": "uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_11",
  "x": "For example, if the i th character in the output sequence is an operator in {+, \u2212, \u00d7, \u00f7}, then the model cannot result in c \u2208 {+, \u2212, \u00d7, \u00f7, ), =} for the (i + 1) th character. To further improve the accuracy, DNS enhances the model in two ways. First, it builds a LSTM-based binary classification model to determine whether a number is relevant. This is similar to the relevance model trained in ExpressionTree <cite>[26]</cite> and UNITDEP [28] . The difference is that DNS uses LSTM as the classifier with unsupervised word-embedding features whereas ExpressionTree and UNITDEP use SVM with handcrafted features. Second, the seq2seq model is integrated with a similarity-based method [12] introduced in Section 3.2. Given a pre-defined threshold, the similarity-based retrieval strategy is selected as the solver if the maximal similarity score is higher than the threshold.",
  "y": "similarities"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_12",
  "x": "For example, [69] presents an easy-fist parsing algorithm that iteratively selects the best pair of neighbors in the tree structure to connect at each parsing step. Those parsers account in WMP solvers. For instance, the neural-network parser [66] is adopted in [70] for coreference resolution, which is another pre-processing step for MWP solvers. UnitDep [28] automatically generates features from a given math problem by analyzing its derived parser tree using the Compositional Vector Grammar parser [67] . Additionally, the Stanford Dependencies representation [68] has been applied in multitple solvers. We observed its occurrence in Formula [23] and ARIS [21] to extract attributes of entities (the subject, verb, object, preposition and temporal information), in KAZB [39] to generate part-of-speech tags, lematizations, and dependency parses to compute features, and in ALGES [27] to obtain syntactic information used for grounding and feature computation. ExpressionTree <cite>[26]</cite> is an exceptional case without using Stanford Parser.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_13",
  "x": "**QUANTITY-RELATED FEATURES** The basic units in an arithmetic expression or an equation set consist of quantities, unknown variables and operators. Hence, a natural idea is to extract quantity-related features to help identify the relevant operands and their associated operators. As shown in Table 5 , a binary indicator to determine whether a quantity refers to a rate is adopted in many solvers <cite>[26]</cite> [28] [15] [40] [45] . It signals a strong connection between the quantity and operators of {\u00d7, \u00f7}. The value of the quantity is also useful for operator classifier or quantity relevance classifier. For instance, a quantity whose value is a real number between [0, 1] is likely to be associated with multiplication or division operators [40] , [45] . It is also observed that quantities in the text format of \"one\" or \"two\" are unlikely to be relevant with the solution [39] [40], [45] , [13] .",
  "y": "uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_14",
  "x": "In this manner, quantities associated with the same operators would to likely to share similar context information. A trivial trick used in <cite>[26]</cite> [28] [15] is to examine whether there exists comparative adverbs. For example, terms \"more\", \"less\" and \"than\" indicate operators of {+, \u2212}. ---------------------------------- **QUANTITY-PAIR FEATURES** The relationship between two quantities is helpful to determine their associated operator. A straightforward example is that if two quantities are associated with the same unit, they can be applied with addition and subtraction <cite>[26]</cite> [28] [15] [40] .",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_15",
  "x": "The relationship between two quantities is helpful to determine their associated operator. A straightforward example is that if two quantities are associated with the same unit, they can be applied with addition and subtraction <cite>[26]</cite> [28] [15] [40] . If one quantity is related to a rate and the other is associated with a unit that is part of the rate, their operator is likely to be multiplication or division <cite>[26]</cite> [27] [28] [15] . Numeric relation and context similarity are two types of quantity-pair features proposed in [40] [45] . The former obtains two sets of nouns located within the same sentence as the two quantities and sorts them by the distance in the dependency tree. Then, a scoring function is defined to measure the similarity between these two sorted noun lists. Higher similarity implies that the two quantities are more likely to be connected by addition or subtraction operators.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_16",
  "x": "A trivial trick used in <cite>[26]</cite> [28] [15] is to examine whether there exists comparative adverbs. For example, terms \"more\", \"less\" and \"than\" indicate operators of {+, \u2212}. ---------------------------------- **QUANTITY-PAIR FEATURES** The relationship between two quantities is helpful to determine their associated operator. A straightforward example is that if two quantities are associated with the same unit, they can be applied with addition and subtraction <cite>[26]</cite> [28] [15] [40] . If one quantity is related to a rate and the other is associated with a unit that is part of the rate, their operator is likely to be multiplication or division <cite>[26]</cite> [27] [28] [15] .",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_17",
  "x": "The first type is the dependency path between a pair of quantities. Their similarity may be helpful to determine the corresponding positions (or number slots) in the equation template. For example, given a sentence \"2 footballs and 3 soccer balls cost 220 dollars\", the dependency paths between two quantity pairs (2, 220) and (3, 220) are identical, implying that 2 and 3 refer to similar types of number slots in the template. The other feature is whether two quantities appear in the same sentence. If so, they are likely to appear in the same equation of the template. Finally, a popular quantity-pair feature used in <cite>[26]</cite> [28] [15] [39] [40] [45] examines whether the value of one quantity is greater than the other, which is helpful to determine the correct operands for subtraction operator. ----------------------------------",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_18",
  "x": "Two types of quantity-pair features were both adopted in the template-based solutions to equation set problems [39] [40] . Finally, a popular quantity-pair feature used in <cite>[26]</cite> [28] [15] [39] [40] [45] examines whether the value of one quantity is greater than the other, which is helpful to determine the correct operands for subtraction operator.",
  "y": "background uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_19",
  "x": "It is straightforward to figure out that the unknown variable can be inferred from the question and if a quantity whose unit appears in the question, this quantity is likely to be relevant. The remain question-related features presented in Table 5 were proposed by Roy et al. <cite>[26]</cite> , [28] and followed by MathDQN [15] . Their feature design leverages the number of matching tokens between the related noun phrase of a quantity and the question text. The quantities with the highest number of matching tokens are considered as useful clues. They also check whether the question contains rate indicators such as \"each\" and \"per\", or comparison indicators such as \"more\" or \"less\". The former is related to {\u00d7, \u00f7} and the latter is related to {+, \u2212}. Moreover, if the question text contains \"how many\", it implies that the solution is a positive number. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_20",
  "x": "Verbs are important indicators for correct operator determination. For example, \"lose\" is a verb indicating quantity loss for an entity and related to the subtraction operator. Given a quantity, we call the verb closest to it in the dependency tree as its dependent verb. <cite>[26]</cite> [27] [28] [15] directly use dependent verb as one of the features. Another widely-adopted verb-related feature is a vector capturing the distance between the dependent verb and a small pre-defined collection of verbs that are found to be useful in categorizing arithmetic operations. Again, the remaining features come from the works <cite>[26]</cite> , [28] , [15] . The features indicate whether two quantities have the same dependent verbs or whether their dependent verbs refer to the same verb mention.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_21",
  "x": "Given a quantity, we call the verb closest to it in the dependency tree as its dependent verb. <cite>[26]</cite> [27] [28] [15] directly use dependent verb as one of the features. Another widely-adopted verb-related feature is a vector capturing the distance between the dependent verb and a small pre-defined collection of verbs that are found to be useful in categorizing arithmetic operations. Again, the remaining features come from the works <cite>[26]</cite> , [28] , [15] . The features indicate whether two quantities have the same dependent verbs or whether their dependent verbs refer to the same verb mention. As we can see from the examples in Table 5 , the difference between these two types of features is the occurrence number of the dependent verb in the sentence. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_22",
  "x": "Another widely-adopted verb-related feature is a vector capturing the distance between the dependent verb and a small pre-defined collection of verbs that are found to be useful in categorizing arithmetic operations. Again, the remaining features come from the works <cite>[26]</cite> , [28] , [15] . The features indicate whether two quantities have the same dependent verbs or whether their dependent verbs refer to the same verb mention. As we can see from the examples in Table 5 , the difference between these two types of features is the occurrence number of the dependent verb in the sentence. ---------------------------------- **GLOBAL FEATURES** There are certain types of global features in the document-level proposed by existing solvers.",
  "y": "uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_23",
  "x": "---------------------------------- **GLOBAL FEATURES** There are certain types of global features in the document-level proposed by existing solvers. <cite>[26]</cite> , [28] , [15] use the number of quantities in the problem text as part of feature space. Unigrams and bigrams are also applied in [20] [39] . They may play certain effect in determining the quantities and their order. Note that the unigrams and bigrams are defined in the word level rather than the character level.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_24",
  "x": "Word lemma [39] [40] [45] For \"Connie has 41.0 red markers. \", the word lemmas around the quantity \"41.0\" are {Connie, have, red, marker}. POS tags [28] [39] [40] [45] [20] For \"A chef needs to cook 16.0 potatoes.\", the POS tags within a window of size 2 centered at the quantity \"16.0\" are {TO, VB, NNS}. Dependence type [39] [40] [45] For \"Ned bought 14.0 boxes of chocolates candy.\", we can detect multiple dependencies within the window of size 2 around the \"14.0\": (boxes, 14.0) \u2192 (num), (boxes, of)\u2192 (prep), (bought, Ned) \u2192 (nsubj). The dependence root is \"bought\". Comparative adverbs <cite>[26]</cite> [28] [15] For \"If she drank 25 of them and then bought 30 more.\", \"more\" is a comparative term in the window of quantity \"30\". ---------------------------------- **QUANTITY-PAIR FEATURES** Whether both quantities have the same unit <cite>[26]</cite> [28] [15] [40] For \"Student tickets cost 4 dollars and general admission tickets cost 6 dollars\", quantities \"4\" and \"6\" have the same unit.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_25",
  "x": "**QUANTITY-PAIR FEATURES** Whether both quantities have the same unit <cite>[26]</cite> [28] [15] [40] For \"Student tickets cost 4 dollars and general admission tickets cost 6 dollars\", quantities \"4\" and \"6\" have the same unit. If one quantity is related to a rate and the other is associated with a unit that is part of the rate <cite>[26]</cite> [27] [28] [15] For \"each box has 9 pieces\" and \"Paul bought 6 boxes of chocolate candy\", \"9\" is related to a rate ( i.e., pieces/box) and \"6\" is associated to the unit \"box\". Numeric relation of two quantities [40] [45] For each quantity, the nouns around it are extracted and sorted by the distance in the dependency tree. Then, a scoring function is defined on the two sorted lists to measure the numeric relation. Context similarity between two quantities [40] [45] The context is represented by the set of words around the quantity. Dependency path between two quantities.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_26",
  "x": "\", the word lemmas around the quantity \"41.0\" are {Connie, have, red, marker}. POS tags [28] [39] [40] [45] [20] For \"A chef needs to cook 16.0 potatoes.\", the POS tags within a window of size 2 centered at the quantity \"16.0\" are {TO, VB, NNS}. Dependence type [39] [40] [45] For \"Ned bought 14.0 boxes of chocolates candy.\", we can detect multiple dependencies within the window of size 2 around the \"14.0\": (boxes, 14.0) \u2192 (num), (boxes, of)\u2192 (prep), (bought, Ned) \u2192 (nsubj). The dependence root is \"bought\". Comparative adverbs <cite>[26]</cite> [28] [15] For \"If she drank 25 of them and then bought 30 more.\", \"more\" is a comparative term in the window of quantity \"30\". ---------------------------------- **QUANTITY-PAIR FEATURES** Whether both quantities have the same unit <cite>[26]</cite> [28] [15] [40] For \"Student tickets cost 4 dollars and general admission tickets cost 6 dollars\", quantities \"4\" and \"6\" have the same unit. If one quantity is related to a rate and the other is associated with a unit that is part of the rate <cite>[26]</cite> [27] [28] [15] For \"each box has 9 pieces\" and \"Paul bought 6 boxes of chocolate candy\", \"9\" is related to a rate ( i.e., pieces/box) and \"6\" is associated to the unit \"box\".",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_27",
  "x": "Then, a scoring function is defined on the two sorted lists to measure the numeric relation. Context similarity between two quantities [40] [45] The context is represented by the set of words around the quantity. Dependency path between two quantities. [39] [40] For \"2 footballs and 3 soccer balls cost 220 dollars\", the dependency path for the quantity pair (2, 3) is num(footballs,2) conj(footballs, balls)num(balls, 3). Whether both quantities appear in the same sentence [39] [40] Whether the value of the first quantity is greater than the other [ [15] For the question \"How many apples are left in the box?\" and a quantity 77 that appears in \"77 apples in a box\", there are two matching tokens (\"apples\" and \"box\"). Number of quantities which happen to have the maximum number of matching tokens with the question <cite>[26]</cite> [28] [15] For \"Rose have 9 apples and 12 erasers. ... 3 friends. How many apples dose each friend get?\", the number of matching tokens for quantities 9, 12 and 3 is 1, 0 and 1.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_28",
  "x": "Whether both quantities appear in the same sentence [39] [40] Whether the value of the first quantity is greater than the other [ [15] For the question \"How many apples are left in the box?\" and a quantity 77 that appears in \"77 apples in a box\", there are two matching tokens (\"apples\" and \"box\"). Number of quantities which happen to have the maximum number of matching tokens with the question <cite>[26]</cite> [28] [15] For \"Rose have 9 apples and 12 erasers. ... 3 friends. How many apples dose each friend get?\", the number of matching tokens for quantities 9, 12 and 3 is 1, 0 and 1. Hence, there are two quantities with the maximum matching token number. Whether any component of the rate is present in the question <cite>[26]</cite> [28] [15] Given a question \"How many blocks does George have?\" and a quantity 6 associated with rate \"blocks/box\", the feature indicator is set to 1 since block appears in the question. Whether the question contains terms like \"each\" or \"per\" <cite>[26]</cite> [28] [15] Whether the question contains comparison-related terms like \"more\" or \"less\" <cite>[26]</cite> [28] [15] Whether the question contains terms like \"how many\" [39] [40] [45] [13] It implies that the solution is positive. ----------------------------------",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_29",
  "x": "Number of quantities which happen to have the maximum number of matching tokens with the question <cite>[26]</cite> [28] [15] For \"Rose have 9 apples and 12 erasers. ... 3 friends. How many apples dose each friend get?\", the number of matching tokens for quantities 9, 12 and 3 is 1, 0 and 1. Hence, there are two quantities with the maximum matching token number. Whether any component of the rate is present in the question <cite>[26]</cite> [28] [15] Given a question \"How many blocks does George have?\" and a quantity 6 associated with rate \"blocks/box\", the feature indicator is set to 1 since block appears in the question. Whether the question contains terms like \"each\" or \"per\" <cite>[26]</cite> [28] [15] Whether the question contains comparison-related terms like \"more\" or \"less\" <cite>[26]</cite> [28] [15] Whether the question contains terms like \"how many\" [39] [40] [45] [13] It implies that the solution is positive. ---------------------------------- **VERB-RELATED FEATURES**",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_30",
  "x": "Hence, there are two quantities with the maximum matching token number. Whether any component of the rate is present in the question <cite>[26]</cite> [28] [15] Given a question \"How many blocks does George have?\" and a quantity 6 associated with rate \"blocks/box\", the feature indicator is set to 1 since block appears in the question. Whether the question contains terms like \"each\" or \"per\" <cite>[26]</cite> [28] [15] Whether the question contains comparison-related terms like \"more\" or \"less\" <cite>[26]</cite> [28] [15] Whether the question contains terms like \"how many\" [39] [40] [45] [13] It implies that the solution is positive. ---------------------------------- **VERB-RELATED FEATURES** Dependent verb of a quantity <cite>[26]</cite> [27] [28] [15] the verb closest to the quantity in the dependency tree Distance vector between the dependent verb and a small collection of predefined verbs that are useful for arithmetic operator classification [21] [24] [27] Whether two quantities have the same dependent verbs <cite>[26]</cite> [28] [15] For \"In the first round she scored 40 points and in the second round she scored 50 points\", the quantities \"40\" and \"50\" both have the same verb \"scored\". Note that \"scored\" appeared twice in the sentence. Whether both dependent verbs refer to the same verb mention <cite>[26]</cite> [28] [15] For \"She baked 4 cupcakes and 29 cookies.\", the quantities \"4\" and \"29\" both shared the verb \"baked\". Note that \"baked\" appeared only once in the sentence.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_31",
  "x": "**VERB-RELATED FEATURES** Dependent verb of a quantity <cite>[26]</cite> [27] [28] [15] the verb closest to the quantity in the dependency tree Distance vector between the dependent verb and a small collection of predefined verbs that are useful for arithmetic operator classification [21] [24] [27] Whether two quantities have the same dependent verbs <cite>[26]</cite> [28] [15] For \"In the first round she scored 40 points and in the second round she scored 50 points\", the quantities \"40\" and \"50\" both have the same verb \"scored\". Note that \"scored\" appeared twice in the sentence. Whether both dependent verbs refer to the same verb mention <cite>[26]</cite> [28] [15] For \"She baked 4 cupcakes and 29 cookies.\", the quantities \"4\" and \"29\" both shared the verb \"baked\". Note that \"baked\" appeared only once in the sentence. ---------------------------------- **GLOBAL FEATURES** Number of quantities mentioned in text <cite>[26]</cite> [28] [15] Unigrams and bigrams of sentences in the problem text [20] [39] presented an efficient characteristic pattern detection method by scanning the distribution of black pixels and generating feature points graph. In [78] , a structure mapping engine named GeoRep was proposed to generate qualitative spatial descriptions from line diagrams.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_32",
  "x": "---------------------------------- **VERB-RELATED FEATURES** Dependent verb of a quantity <cite>[26]</cite> [27] [28] [15] the verb closest to the quantity in the dependency tree Distance vector between the dependent verb and a small collection of predefined verbs that are useful for arithmetic operator classification [21] [24] [27] Whether two quantities have the same dependent verbs <cite>[26]</cite> [28] [15] For \"In the first round she scored 40 points and in the second round she scored 50 points\", the quantities \"40\" and \"50\" both have the same verb \"scored\". Note that \"scored\" appeared twice in the sentence. Whether both dependent verbs refer to the same verb mention <cite>[26]</cite> [28] [15] For \"She baked 4 cupcakes and 29 cookies.\", the quantities \"4\" and \"29\" both shared the verb \"baked\". Note that \"baked\" appeared only once in the sentence. ---------------------------------- **GLOBAL FEATURES** Number of quantities mentioned in text <cite>[26]</cite> [28] [15] Unigrams and bigrams of sentences in the problem text [20] [39] presented an efficient characteristic pattern detection method by scanning the distribution of black pixels and generating feature points graph.",
  "y": "background"
 },
 {
  "id": "a127218cca5653f1700c0de6c8318a_0",
  "x": "However, the information that conventional concordancers can provide for analyses of each usage is limited to the frequency of surrounding context patterns, parts of speech, and so on. The words that second language learners can search to learn their usages tend to be frequent. Therefore, a more sophisticated method to summarize many word usages in a large corpus for concordancers is desirable. Recently, contextualized word embeddings such as (<cite>Devlin et al., 2019</cite>) were proposed in NLP to capture the context of each word usage in vectors and to model the semantic distances between the usages using contexts as a clue. Unlike previous studies (Liu et al., 2017; Smilkov et al., 2016) that visualized different words using word embeddings, in this paper, we introduce a novel system intuitively helpful for concordancer users to visualize different usages of a word of interest. 2 System Overview and Use Cases Fig. 1 shows our system layout. Once a user provides a word to the system, it automatically searches the word in the corpus in a similar way to typical concordancers.",
  "y": "background"
 },
 {
  "id": "a127218cca5653f1700c0de6c8318a_1",
  "x": "Unlike concordancers, our system has a database that stores contextualized word embeddings for each usage or occurrence of each word in the corpus. We used half a million sentences from the British National Corpus (BNC Consortium, 2007) as the raw corpus. We built the database by applying the bert-base-uncased model of the PyTorch Pretrained the BERT project 1 (<cite>Devlin et al., 2019</cite>) to the corpus. We used the last layer, which was more distant from the surface input, as the embeddings. The size of the database is roughly 200MB per thousand sentences. Our system visualizes these searched contextualized word embedding vectors. We visualize the contextualized word embedding vectors for the provided word by projecting these vectors into a twodimensional space.",
  "y": "uses"
 },
 {
  "id": "a127218cca5653f1700c0de6c8318a_2",
  "x": "The red lightly-colored point is the probe point: the usages are listed in the nearest order of the probe point. No usage is linked to the probe point. Users can 1 <cite>https://github.com/huggingface/ pytorch-pretrained-BERT</cite> 2 Fig. 2 and Fig. 3 shows use cases on a 10, 000-sentence experpt of the BNC corpus to avoid having too many hits hinder the reading of the paper. freely and interactively drag and move the probe point to change the list of usages below the visualization. Each line of the list shows the surrounding words of the usage, followed by the distance between the vectors of the usage and probe point in the two-dimensional visualization. In Fig. 2 , the probe point is on the left part of the visualized figure. In the first several lines of the list, the system successfully shows the usages of the word book about reading.",
  "y": "uses"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_0",
  "x": "Whilst gender bias in the form of concepts of masculinity and femininity has been found inscribed in implicit ways in AI systems more broadly (Adam, 2006) , this paper focuses on gender bias on word embeddings. Word embeddings are one of the most common techniques for giving semantic meaning to words in text and are used as input in virtually every neural NLP system (Goldberg, 2017) . It has been shown that word embeddings capture human biases (such as gender bias) present in these corpora in how they relate words to each other (Bolukbasi et al., 2016;<cite> Caliskan et al., 2017</cite>; Garg et al., 2018) . For the purposes of this paper, gender bias is understood as the inclination towards or prejudice against one gender. Several methods have been proposed to test for the presence of gender bias in word embeddings; an example being the Word Embedding Association Test (WEAT) <cite>(Caliskan et al., 2017)</cite> . WEAT is a statistical test that detects bias in word embeddings using cosine similarity and averaging methods, paired with hypothesis testing. WEAT's authors applied these tests to the publicly-available GloVe embeddings trained on the English-language \"Common Crawl\" corpus (Pennington et al., 2014) as well as the Skip-Gram (word2vec) embeddings trained on the Google News corpus (Mikolov et al., 2013) .",
  "y": "background"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_1",
  "x": "For example, in Natural Language Processing (NLP), r\u00e9sum\u00e9 search engines can produce rankings that disadvantage some candidates, when these ranking algorithms take demographic features into account (directly or indirectly) (Chen et al., 2018) , while abusive online language detection systems have been observed to produce false positives on terms associated with minorities and women (Dixon et al., 2018; Park et al., 2018) . Another example where bias (specifically gender bias) can be harmful is in personal pronoun coreference resolution, where systems carry the risk of relying on societal stereotypes present in the training data (Webster et al., 2018) . Whilst gender bias in the form of concepts of masculinity and femininity has been found inscribed in implicit ways in AI systems more broadly (Adam, 2006) , this paper focuses on gender bias on word embeddings. Word embeddings are one of the most common techniques for giving semantic meaning to words in text and are used as input in virtually every neural NLP system (Goldberg, 2017) . It has been shown that word embeddings capture human biases (such as gender bias) present in these corpora in how they relate words to each other (Bolukbasi et al., 2016;<cite> Caliskan et al., 2017</cite>; Garg et al., 2018) . For the purposes of this paper, gender bias is understood as the inclination towards or prejudice against one gender. Several methods have been proposed to test for the presence of gender bias in word embeddings; an example being the Word Embedding Association Test (WEAT) <cite>(Caliskan et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_2",
  "x": "For the purposes of this paper, gender bias is understood as the inclination towards or prejudice against one gender. Several methods have been proposed to test for the presence of gender bias in word embeddings; an example being the Word Embedding Association Test (WEAT) <cite>(Caliskan et al., 2017)</cite> . WEAT is a statistical test that detects bias in word embeddings using cosine similarity and averaging methods, paired with hypothesis testing. WEAT's authors applied these tests to the publicly-available GloVe embeddings trained on the English-language \"Common Crawl\" corpus (Pennington et al., 2014) as well as the Skip-Gram (word2vec) embeddings trained on the Google News corpus (Mikolov et al., 2013) . However, there is a diverse range of publicly-available word embeddings trained on corpora of different domains. To address this, we applied the WEAT test on four sets of word embeddings trained on corpora from four domains: social media (Twit-ter), a Wikipedia-based gender-balanced corpus (GAP) and a biomedical corpus (PubMed) and news (Google News, in order to reproduce and validate our results against those of<cite> Caliskan et al. (2017)</cite> ) (see Section 3). Caliskan et al. (2017) confirmed the presence of gender bias using three categories of words wellknown to be prone to exhibit gender bias: (B1) career vs. family activities, (B2) Maths vs. Arts and (B3) Science vs. Arts.",
  "y": "similarities uses"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_3",
  "x": "English-language word embeddings were selected with the intention of giving an insight into gender bias over a range of domains and with the expectation that some word embeddings would demonstrate much more bias than others. The word embeddings selected were: (a) Skip-Gram embeddings trained on the Google News corpus 2 , with a vocabulary of 3M word types (Mikolov et al., 2013) ; (b) Skip-Gram embeddings trained on 400 million Twitter micro-posts 3 , with a vocabulary of slightly more than 3M word types (Godin et al., 2015) ; (c) Skip-Gram embeddings trained on the PubMed Central Open Access subset (PMC) and PubMed 4 , with a vocabulary of about 2.2M word types (Chiu et al., 2016) and trained using two different sliding window sizes: 2 and 30 words; (d) FastText embeddings trained on the GAP corpus (Webster et al., 2018) by us 5 , with a vocabulary of 7,400 word types. ---------------------------------- **WEAT HYPOTHESIS TESTING 4.1 EXPERIMENTAL PROTOCOL** We largely follow the WEAT Hypothesis testing protocol introduced by<cite> Caliskan et al. (2017)</cite> . The input is a suspected gender bias word category represented by two lists, X and Y , of target words, i.e. words which are suspected to be biased to one or another gender. E.g. X = {programmer, engineer, scientist}, Y = {nurse, teacher, librarian}. We wish to test whether X or Y is more biased to one gender or the other, or whether there is not difference in bias between the two lists.",
  "y": "similarities uses"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_4",
  "x": "(1) where s(w, M, F ) is the measure of association between target word w and the attribute words in M and F : In<cite> Caliskan et al. (2017)</cite> H o is tested through a permutation test, in which X \u222a Y is partitioned into alternative target listsX and\u0176 exhaustively and computing the one-sided p-value p[s(X,\u0176 , M, F ) > s(X, Y, M, F )], i.e. the proportion of partition permutationsX,\u0176 in which the test statistic s(X,\u0176 , M, F ) is greater than the observed test statistic s(X, Y, M, F ). This p-value is the probability that H o is true. In other words, it is the probability that there is no difference between X and Y (in relation to M and F ) and therefore that the word category is not biased. The ---------------------------------- **ATTRIBUTE WORDS**",
  "y": "background"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_5",
  "x": "**WEAT RESULTS** Before experimentation we expected to find a great deal of gender bias across the Google News and Twitter embedding sets and far less in the PubMed and GAP sets. However, results in Table 2 are somewhat different to our expectations: Google News We detect statistically significant (p-values in bold) gender bias in all 5 categories (B1-B5) on this corpus. Although one would hope to find little gender bias in a news corpus, given that its authors are professional journalists, bias had already been detected by<cite> Caliskan et al. (2017)</cite> and Garg et al. (2018) using methods similar to ours. This is not surprising given that women represent only a third (33.3%) of the full-time journalism workforce (Byerly, 2011) . In addition, it has been found that news coverage of female personalities more frequently mentions family situations and is more likely to invoke matters of superficial nature, such as personality, appearance and fashion decisions, whereas the focus on men in news coverage tends to be be given to their experience and accomplishments (Armstrong et al., 2006) .",
  "y": "similarities"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_0",
  "x": "To build a robust acoustic model, previous work primarily focused on collecting labeled in-domain data for fully supervised training [2, 3, 4] . However, in practice, it is expensive and laborious to collect labeled data for all possible testing conditions. In contrast, collecting large amount of unlabeled indomain data and labeled out-of-domain data can be fast and economical. Hence, an important question arises for this scenario: how can we do unsupervised adaptation for acoustic models by utilizing labeled out-of-domain data and unlabeled in-domain data, in order to achieve good performance on in-domain data? Research on unsupervised adaptation for acoustic models can be roughly divided into three categories: (1) constrained model adaptation [5, 6, 7] , (2) domain-invariant feature extraction [8, 9, 10] , and (3) labeled in-domain data augmentation by synthesis [11, 12, <cite>13]</cite> . Among these approaches, data augmentation-based adaptation is favorable, because it does not require extra hyperparameter tuning for acoustic model training, and can utilize full model capacity by training a model with as much and as diverse a dataset as possible. Another benefit of this approach is that data in their original domain are more intuitive to humans.",
  "y": "background"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_1",
  "x": "Hence, an important question arises for this scenario: how can we do unsupervised adaptation for acoustic models by utilizing labeled out-of-domain data and unlabeled in-domain data, in order to achieve good performance on in-domain data? Research on unsupervised adaptation for acoustic models can be roughly divided into three categories: (1) constrained model adaptation [5, 6, 7] , (2) domain-invariant feature extraction [8, 9, 10] , and (3) labeled in-domain data augmentation by synthesis [11, 12, <cite>13]</cite> . Among these approaches, data augmentation-based adaptation is favorable, because it does not require extra hyperparameter tuning for acoustic model training, and can utilize full model capacity by training a model with as much and as diverse a dataset as possible. Another benefit of this approach is that data in their original domain are more intuitive to humans. In other words, it is easier for us to inspect and manipulate the data. Furthermore, with the recent progress on domain translation<cite> [13,</cite> 14, 15] , conditional synthesis of indomain data without parallel data has become achievable, which makes data augmentation-based adaptation a more promising direction to investigate. Variational autoencoder-based data augmentation (VAE-DA) is a domain adaptation method proposed in<cite> [13]</cite> , which pools in-domain and out-domain to train a VAE that learns factorized latent representations of speech segments.",
  "y": "background"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_2",
  "x": "Research on unsupervised adaptation for acoustic models can be roughly divided into three categories: (1) constrained model adaptation [5, 6, 7] , (2) domain-invariant feature extraction [8, 9, 10] , and (3) labeled in-domain data augmentation by synthesis [11, 12, <cite>13]</cite> . Among these approaches, data augmentation-based adaptation is favorable, because it does not require extra hyperparameter tuning for acoustic model training, and can utilize full model capacity by training a model with as much and as diverse a dataset as possible. Another benefit of this approach is that data in their original domain are more intuitive to humans. In other words, it is easier for us to inspect and manipulate the data. Furthermore, with the recent progress on domain translation<cite> [13,</cite> 14, 15] , conditional synthesis of indomain data without parallel data has become achievable, which makes data augmentation-based adaptation a more promising direction to investigate. Variational autoencoder-based data augmentation (VAE-DA) is a domain adaptation method proposed in<cite> [13]</cite> , which pools in-domain and out-domain to train a VAE that learns factorized latent representations of speech segments. To disentangle linguistic factors from nuisance ones in the latent space, statistics of the latent representations for each utterance are computed.",
  "y": "background"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_3",
  "x": "Variational autoencoder-based data augmentation (VAE-DA) is a domain adaptation method proposed in<cite> [13]</cite> , which pools in-domain and out-domain to train a VAE that learns factorized latent representations of speech segments. To disentangle linguistic factors from nuisance ones in the latent space, statistics of the latent representations for each utterance are computed. By altering the latent representations of the segments from a labeled out-of-domain utterance properly according to the computed statistics, one can synthesize an in-domain utterance without changing the linguistic content using the trained VAE decoder. This approach shows promising results on synthesizing noisy read speech from clean speech. However, it is non-trivial to apply this approach to conversational speech, because utterances tend to be shorter, which makes estimating the statistics of a disentangled representation difficult. In this paper, we extend VAE-DA and address the issue by learning interpretable and disentangled representations using a variant of VAEs that is designed for sequential data, named factorized hierarchical variational autoencoders (FHVAEs) [15] . Instead of estimating the latent representation statistics on short utterances, we use a loss that considers the statistics across utterances in the entire corpus.",
  "y": "extends"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_4",
  "x": "In [19] , a VAE is proposed to model a generative process of speech segments. A latent vector in the latent space is assumed to be a linear combination of orthogonal vectors corresponding to the independent factors, such as phonetic content and speaker identity. In other words, we assume that z = z + zn where z encodes the linguistic/phonetic content and zn encodes the nuisance factors, and z \u22a5 zn. To augment the data set while reusing the labels, for any pair of utterance and its corresponding label sequence (X, y) in the data set, we generate (X, y) by altering the nuisance part of X in the latent space. ---------------------------------- **ESTIMATING LATENT NUISANCE VECTORS** A key observation made in<cite> [13]</cite> is that nuisance factors, such as speaker identity and room acoustics, are generally constant over segments within an utterance, while linguistic content changes from segment to segment.",
  "y": "background"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_5",
  "x": "To transform nuisance factors of an utterance X without changing the corresponding transcript, one only needs to perturb Z2. Furthermore, since each z2 within an utterance is generated conditioned on a Gaussian whose mean is \u00b52, we can regard \u00b52 as the representation of nuisance factors of an utterance. We now derive two data augmentation methods similar to those proposed in<cite> [13]</cite> , named nuisance factor replacement and nuisance factor perturbation. ---------------------------------- **NUISANCE FACTOR REPLACEMENT** Given a labeled out-of-domain utterance (Xout, yout) and an unlabeled in-domain utterance Xin, we want to transform Xout toXout such that it exhibits the same nuisance factors as Xin, while maintaining the original linguistic content. We can then add the synthesized labeled in-domain data (Xout, yout) to the ASR training set.",
  "y": "similarities"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_6",
  "x": "However, in practice, VAE-type of models suffer from an over-pruning issue [21] in that some latent variables become inactive, which we do not want to perturb. Instead, we only want to perturb the linear subspace which models the variation of nuisance factors between utterances. Therefore, we adopt a similar soft perturbation scheme as in<cite> [13]</cite> . First, {\u00b52} M i=1 for all M utterances are estimated with the approximated MAP. Principle component analysis is performed to obtain D pairs of eigenvalue \u03c3 d and eigenvectors e d , where D is the dimension of \u00b52. Lastly, one random perturbation vector p is drawn for each utterance to perturb as follows: where \u03b3 is used to control the perturbation scale.",
  "y": "similarities"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_8",
  "x": "The third row reports the results of training with domain invariant feature, z1, extracted with a FHVAE as is done in [10] . It improves over the baseline by 6% absolute. VAE-DA<cite> [13]</cite> results with nuisance factor replacement (repl) and latent nuisance perturbation (p) are shown in the last three rows. We then examine the effectiveness of our proposed method and show the results in the second, third, and fourth rows in Table 2 . We observe about 12% WER reduction on the in-domain development set for both nuisance factor perturbation (p) and nuisance factor replacement (repl), with little degradation on the out-of-domain development set. Both augmentation methods outperform their VAE counterparts and the domain invariant feature baseline using the same FHVAE model. We attribute the improvement to the better quality of the transformed IHM data, which covers the nuisance factors of the SDM data, without altering the original linguistic content.",
  "y": "uses"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_9",
  "x": "VAE-DA<cite> [13]</cite> results with nuisance factor replacement (repl) and latent nuisance perturbation (p) are shown in the last three rows. We then examine the effectiveness of our proposed method and show the results in the second, third, and fourth rows in Table 2 . We observe about 12% WER reduction on the in-domain development set for both nuisance factor perturbation (p) and nuisance factor replacement (repl), with little degradation on the out-of-domain development set. Both augmentation methods outperform their VAE counterparts and the domain invariant feature baseline using the same FHVAE model. We attribute the improvement to the better quality of the transformed IHM data, which covers the nuisance factors of the SDM data, without altering the original linguistic content. To verify the superiority of the proposed method of drawing random perturbation vectors, we compare two alternative sampling methods: rev-p and uni-p, similar to<cite> [13]</cite> , with the same expected squared Euclidean norm as the proposed method. Table 2 confirm that the proposed sampling method is more effective under the same perturbation scale \u03b3 = 1.0 compared to the alternative methods as expected.",
  "y": "similarities"
 },
 {
  "id": "a62f376adefad10c5fb8b6c08ebb63_0",
  "x": "---------------------------------- **INTRODUCTION** The first stage in training a machine translation system is typically that of aligning bilingual text. The quality of alignments is in that case of vital importance to the quality of the induced translation rules used by the system in subsequent stages. In string-based statistical machine translation, the alignment space is typically restricted by the n-grams considered in the underlying language model, but in syntax-based machine translation the alignment space is restricted by very different and less transparent structural contraints. While it is easy to estimate the consequences of restrictions to n-grams of limited size, it is less trivial to estimate the consequences of the structural constraints imposed by syntax-based machine translation formalisms. Consequently, much work has been devoted to this task (Wu, 1997; Zens and Ney, 2003;<cite> Wellington et al., 2006</cite>; Macken, 2007; S\u00f8gaard and Kuhn, 2009) .",
  "y": "background"
 },
 {
  "id": "a62f376adefad10c5fb8b6c08ebb63_1",
  "x": "The first stage in training a machine translation system is typically that of aligning bilingual text. The quality of alignments is in that case of vital importance to the quality of the induced translation rules used by the system in subsequent stages. In string-based statistical machine translation, the alignment space is typically restricted by the n-grams considered in the underlying language model, but in syntax-based machine translation the alignment space is restricted by very different and less transparent structural contraints. While it is easy to estimate the consequences of restrictions to n-grams of limited size, it is less trivial to estimate the consequences of the structural constraints imposed by syntax-based machine translation formalisms. Consequently, much work has been devoted to this task (Wu, 1997; Zens and Ney, 2003;<cite> Wellington et al., 2006</cite>; Macken, 2007; S\u00f8gaard and Kuhn, 2009) . The task of estimating the consequences of the structural constraints imposed by a particular syntax-based formalism consists in finding what is often called \"empirical lower bounds\" on the coverage of the formalism <cite>(Wellington et al., 2006</cite>; S\u00f8gaard and Kuhn, 2009 ). Gold standard alignments are constructed and queried in some way as to identify complex alignment configurations, or they are parsed by an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism.",
  "y": "background"
 },
 {
  "id": "a62f376adefad10c5fb8b6c08ebb63_2",
  "x": "The assumption in this and related work that enables us to introduce a meaningful notion of alignment capacity is that simultaneously recognized words are aligned (Wu, 1997; Zhang and Gildea, 2004;<cite> Wellington et al., 2006</cite>; S\u00f8gaard and Kuhn, 2009) . As noted by S\u00f8gaard (2009) , this definition of alignment has the advantageous consequence that candidate alignments can be singled out by mere inspection of the grammar rules. It also has the consequence that alignments are transitive (Goutte et al., 2004) , since simultaneity is transitive. While previous work (S\u00f8gaard and Kuhn, 2009 ) has estimated empirical lower bounds for normal form ITGs at the level of translation units (TUER), or cepts (Goutte et al., 2004) , defined as maximally connected subgraphs in alignments, nobody has done this for the full class of ITGs. What is important to understand is that while normal form ITGs can induce the same class of translations as the full class of ITGs, they do not induce the same class of alignments. They do not, for ex-ample, induce discontinuous translation units (see Sect. 3).",
  "y": "motivation background"
 },
 {
  "id": "a62f376adefad10c5fb8b6c08ebb63_3",
  "x": "**NON-INDUCED CONFIGURATIONS** Inside-out alignments were first described by Wu (1997) , and their frequency has been a matter of some debate (Lepage and Denoual, 2005;<cite> Wellington et al., 2006</cite>; S\u00f8gaard and Kuhn, 2009) . Cross-serial DTUs are made of two DTUs noncontiguous to the same side such that both have material in the gap of each other. Bonbons are similar, except the DTUs are non-contiguous to different sides, i.e. D has a gap in the source side that contains at least one token in E, and E has a gap in the target side that contains at least one token in D. Here's an example of a bonbon configuration from Simard et al. (2005) Multigap DTUs with mixed transfer are, as already mentioned multigap DTUs with crossing alignments from material in two distinct gaps. ---------------------------------- **RESULTS** The lower bounds on TUER for the full class of ITGs are obtained by summing the ratios of insideout alignments, cross-serial DTUs, bonbons and mixed order multigap DTUs, subtracting any overlap between these classes of configurations.",
  "y": "background"
 },
 {
  "id": "a7559a8775941622d269433937633a_0",
  "x": "This half of the book will be more enjoyable for readers of this journal, who are presumably interested in more general questions of computation and language than the step-by-step tutorial format of the first half of the book. The details of the approach are interesting, particularly the insights about how to build linguistically rich grammars that can be effectively compiled into high-utility context-free grammars for speech recognition. The primary shortcoming of this presentation lies in perpetuating the false dichotomy between \"grammar-based\" and \"data-driven\" approaches to language modeling for speech recognition, which motivates the final chapter of the book. In fact, the authors' approach is both grammar-based and data-driven, given the corpus-based grammar specialization and PCFG estimation, which the authors themselves demonstrate to be indispensable. Robust grammar-based language modeling is a topic that has received a fair bit of attention over the past decade (Chelba and Jelinek 2000; <cite>Charniak 2001</cite>; Roark 2001; Wang, Stolcke, and Harper 2004, among others) , and while this line of research has not focused on the use of manually built, narrow-domain feature grammars, there is enough similarity between the approach described in this book and the cited papers that the papers would seem to be better comparison points than the class-based language models that are chosen to represent robust approaches in the comparison. Beyond language modeling, methods for PCFG induction from treebanks have been a popular topic in the field over the past decade, and some understanding of the impact of flattening trees can be had in Johnson (1998) , where the beneficial impact of various tree transformations for probabilistic grammars is presented. None of this work is discussed or cited, and the naive reader might be left with the impression that data-driven approaches have been demonstrated to underperform relative to knowledge-based approaches, when in fact the authors simply demonstrate that their hybrid grammar-based/data-driven approach outperforms class-based language models.",
  "y": "similarities background"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_0",
  "x": "This temporal tagger then contributes towards high performance at matching event mentions with the month and year in which they occurred based on the complete posting history of users. It does so with high accuracy on informal event mentions in social media by learning to integrate the likelihood of multiple candidate dates extracted from event mentions in timerich sentences with temporal constraints extracted from event-related sentences. Despite considerable prior work in temporal information extraction, to date state-of-the-art resources are designed for extracting temporally scoped facts about public figures/organizations from newswire or Wikipedia articles<cite> McClosky and Manning, 2012</cite>; Garrido et [11/15/2008] I have noticed some pulling recently and I won't start rads until March. [11/20/2008] It is sloowwwly healing, so slowly, in fact, that she said she HOPES it will be healed by March, when I am supposed to start rads. al., 2012). When people are instead communicating informally about their lives, they refer to time more informally and frequently from their personal frame of reference rather than from an impersonal third person frame of reference. For example, they may use their own birthday as a time reference.",
  "y": "background"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_1",
  "x": "Besides a standard event-time classifier for within-sentence event-time anchoring, we leverage a new source of temporal information to train a constraint-based event-time classifier. Previous work only retrieves time-rich sentences that include both the query and some TEs<cite> McClosky and Manning, 2012</cite>; Garrido et al., 2012) . However, sentences that contain only the event mention but no explicit TE can also be informative. For example, the post time (usually referred to as document creation time or DCT) of the sentence \"metastasis was found in my bone\" might be labeled as being after the \"metastasis\" event date. These DCTs impose constraints on the possible event dates, which can be integrated with the event-time classifier, as a variant on related work (Chambers, 2012) . ---------------------------------- **RELATED WORK**",
  "y": "motivation background"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_2",
  "x": "Previous work on TE extraction has focused mainly on newswire text (Strotgen and Gertz, 2010; Chang and Manning, 2012) . This paper presents a rule-based TE extractor that identifies and resolves a higher percentage of nonstandard TEs than earlier state-of-art temporal taggers. Our task is closest to the temporal slot filling track in the TAC-KBP 2011 shared task and timelining task<cite> (McClosky and Manning, 2012)</cite> . Their goal was to extract the temporal bounds of event relations. Our task has two key differences. First, they used newswire, Wikipedia and blogs as data sources from which they extract temporal bounds of facts found in Wikipedia infoboxes. Second, in the KBP task, the set of gold event relations are provided as input, so that the task is only to identify a date for an event that is guaranteed to have been mentioned.",
  "y": "similarities"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_3",
  "x": "Features for the classifier include many of those in <cite>(McClosky and Manning, 2012</cite>; Yoshikawa et al., 2009 ): namely, event keyword and its dominant verb, verb and preposition that dominate TE, dependency path between TE and keyword and its length, unigram and bigram word and POS features. New features include the Event-Subject, Negative and Modality features. In online support groups, users not only tell stories about themselves, they also share other patients' stories (as shown in Figure 1 ). So we add subject features to remove this kind of noise, which includes the governing subject of the event keyword and its POS tag. Modality features include the appearance of modals before the event keyword (e.g., may, might). Negative features include the presence/absence of negative words (e.g., no, never). These two features indicate a hypothetical or counter-factual expression of the event.",
  "y": "similarities"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_4",
  "x": "Previous work only retrieves time-rich sentences (i.e., date sentences) (Ling and Weld, 2010;<cite> McClosky and Manning, 2012</cite>; Garrido et al., 2012) . However, keyword sentences can inform temporal constraints for events and therefore should not be ignored. For example, \"Well, I'm officially a Radiation grad!\" indicates the user has done radiation by the time of the post (DCT). \"Radiation is not a choice for me.\" indicates the user probably never had radiation. The topic of the sentence can also indicate the temporal relation. For example, before chemotherapy, the users tend to talk about choices of drug combinations. After chemotherapy, they talk about side-effects.",
  "y": "motivation background"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_5",
  "x": "**.1 EVALUATION METRIC** The extracted date is only considered correct if it completely matches the gold date. For less than 4% of users, we have multiple dates for the same event (e.g., a user had a mastectomy twice). Similar to the evaluation metric in a previous study , in these cases, we give the system the benefit of the doubt and the extracted date is considered correct if it matches one of the gold dates. In previous work <cite>(McClosky and Manning, 2012</cite>; , the evaluation metric score is defined as 1/((1 + |d|)) where d is the difference between the values in years. We choose a much stricter evaluation metric because we need a precise event date to study user behavior changes. ----------------------------------",
  "y": "background"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_6",
  "x": "Similar to the evaluation metric in a previous study , in these cases, we give the system the benefit of the doubt and the extracted date is considered correct if it matches one of the gold dates. In previous work <cite>(McClosky and Manning, 2012</cite>; , the evaluation metric score is defined as 1/((1 + |d|)) where d is the difference between the values in years. We choose a much stricter evaluation metric because we need a precise event date to study user behavior changes. ---------------------------------- **BASELINES AND ORACLE** Based on our temporal tagger, we provide two baselines to describe heuristic methods of aggregating the hard decisions from the classifier To set an upper bound on performance given our TE retrieval system, we calculate the oracle score by considering an extraction as correct if the gold date is one of the retrieved candidate dates. The oracle score can differ from a perfect score since we can only use candidate temporal expressions if (a)the relation is known and (b)mentions of the event are retrievable, (c)the TE and event keyword appear in the same sentence, and (d)our temporal tagger is able to recognize and resolve it correctly.",
  "y": "background differences"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_0",
  "x": "We present results for identifing rebuttals with 63% accuracy, and for identifying stance on a per topic basis that range from 54% to 69%, as compared to unigram baselines that vary between 49% and 60%. Our results suggest that methods that take into account the dialogic context of such posts might be fruitful. ---------------------------------- **INTRODUCTION** Recent work has highlighted the challenges of identifying the STANCE that a speaker holds towards a particular political, social or technical topic. Classifying stance involves identifying a holistic subjective disposition, beyond the word or sentence (Lin et al., 2006; Malouf and Mullen, 2008; Greene and Resnik, 2009; Somasundaran and Wiebe, 2009;<cite> Somasundaran and Wiebe, 2010)</cite> . Our work is inspired by the large variety of such conversations now freely available online, and our observation that the contextual affordances of different debate and discussion websites vary a great deal.",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_1",
  "x": "Actually most hardened criminals are more afraid to live-then die. I'd like to see life sentences without parole in lieu of capital punishment with hard labor and no amenities for hard core repeat offenders, the hell with PC and prisoner's rights-they lose priveledges for their behaviour. Figure 2: Posts on the topic Capital punishment without explicit link structure. The discussion topic was \"Death Penalty\", and the argument was framed as yes we should keep it vs. no we should not. Our long term goal is to understand the discourse and dialogic structure of such conversations. This could be useful for: (1) creating automatic summaries of each position on an issue (SparckJones, 1999); (2) gaining a deeper understanding of what makes an argument persuasive (Marwell and Schmitt, 1967) ; and (3) identifying the linguistic reflexes of perlocutionary acts such as persuasion and disagreement (Walker, 1996; Greene and Resnik, 2009;<cite> Somasundaran and Wiebe, 2010</cite>; Marcu, 2000) . As a first step, in this paper we aim to automatically identify rebuttals, and identify the speaker's stance towards a particular topic.",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_2",
  "x": "Cat owners don't seem to share this infatuation. Not if they're dog owners who live in the country. If your dog sh*ts in a field you aren't going to walk out and pick it up. Cat owners HAVE to handle sh*t, they MUST clean out a litter box...so suck on that! The most similar work to our own is that of Somasundaran & Wiebe (2009 , 2010 who also focus on automatically determining the stance of a debate participant with respect to a particular issue. Their data does not provide explicit indicators of dialogue structure such as are provided by the rebuttal links in Convinceme. Thus, this work treats each post as a monologic text to be classified in terms of stance, for a particular topic. They show that discourse relations such as concessions and the identification of argumentation triggers improves performance over sentiment features alone (Somasundaran and Wiebe, 2009;<cite> Somasundaran and Wiebe, 2010)</cite> . This work, along with others, indicates that for such tasks it is difficult to beat a unigram baseline (Pang and Lee, 2008) .",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_3",
  "x": "Other factors we examined were words per sen- tence (WPS), the length of words used (6LTR) which typically indicates scientific or low frequency words, the use of pronominal forms (Pro), and the use of positive and negative emotion words (PosE,NegE) (Pennebaker et al., 2001 ). For example, Table 1 shows that discussions about Cats vs. Dogs consist of short simple words in short sentences with relatively high usage of positive emotion words and pronouns, whereas 2nd amendment debates use relatively longer sentences, and death penalty debates (unsurprisingly) use a lot of negative emotion words. Human Topline. The best performance for siding ideological debates in previous work is approximately 64% accuracy over all topics, for a collection of 2nd Amendment, Abortion, Evolution, and Gay Rights debate posts<cite> (Somasundaran and Wiebe, 2010)</cite> . Their best performance is 70% for the 2nd amendment topic. The website that these posts were collected from apparently did not support dialogic threading, and thus there are no explicitly linked rebuttals in this data set. Given the dialogic nature of our data, as indicated by the high percentage of rebuttals in the ideological debates, we first aim to determine how difficult it is for humans to side an individual post from a debate without context.",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_4",
  "x": "The best performance for siding ideological debates in previous work is approximately 64% accuracy over all topics, for a collection of 2nd Amendment, Abortion, Evolution, and Gay Rights debate posts<cite> (Somasundaran and Wiebe, 2010)</cite> . Their best performance is 70% for the 2nd amendment topic. The website that these posts were collected from apparently did not support dialogic threading, and thus there are no explicitly linked rebuttals in this data set. Given the dialogic nature of our data, as indicated by the high percentage of rebuttals in the ideological debates, we first aim to determine how difficult it is for humans to side an individual post from a debate without context. To our knowledge, none of the previous work on debate side classification has attempted to establish a human topline. We set up a Mechanical Turk task by randomly selected a subset of our data excluding the first post on each side of a debate and debates with fewer than 6 posts on either side. Each of our 12 topics consists of more than one debate: each debate was mapped by hand to the topic and topic-siding (as in<cite> (Somasundaran and Wiebe, 2010)</cite>).",
  "y": "similarities"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_5",
  "x": "Table 3 provides a summary of the features we extract for each post. We describe and motivate these feature sets below. Counts, Unigrams, Bigrams. Previous work suggests that the unigram baseline can be difficult to beat for certain types of debates <cite>(Somasundaran and Wiebe, 2010</cite> ). Thus we derived both unigrams and bigrams as features. We also include basic counts such as post length. Cue Words.",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_6",
  "x": "Syntactic Dependency. Previous research in this area suggests the utility of dependency structure to determine the TARGET of an opinion word (Joshi and Penstein-Ros\u00e9, 2009; Somasundaran and Wiebe, 2009;<cite> Somasundaran and Wiebe, 2010)</cite> . The dependency parse for a given sentence is a set of triples, composed of a grammatical relation and the pair of words for which the grammatical relation holds (rel i , w j , w k ), where rel i is the dependency relation among words w j and w k . The word w j is the HEAD of the dependency relation. We use the Stanford parser to parse the utterances in the posts and extract dependency features (De Marneffe et al., 2006; Klein and Manning, 2003) . Generalized Dependency. To create generalized dependencies, we \"back off\" the head word in each of the above features to its part-of-speech tag (Joshi and Penstein-Ros\u00e9, 2009 ).",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_7",
  "x": "It is interesting to note that in general the unigram accuracies are significantly below what Somasundaran and Wiebe achieve (who report overall unigram of 62.5%). This suggests a difference between the debate posts in their corpus and the Convinceme data we used which may be related to the proportion of rebuttals. The overal lack of impact for either the POS generalized dependency features (GDepP) or the Opinion generalized dependency features (GDep0) is surprising given that they improve accuracy for other similar tasks (Joshi and Penstein-Ros\u00e9, 2009;<cite> Somasundaran and Wiebe, 2010)</cite> . While our method of extracting the GDepP features is identical to (Joshi and Penstein-Ros\u00e9, 2009 ), our method for extracting GDepO is an approximation of the method of<cite> (Somasundaran and Wiebe, 2010)</cite> , that does not rely on selecting particular patterns indicating the topics of arguing by using a development set. The LIWC feature set, which is based on a lexical hierarchy that includes social features, negative and positive emotion, and psychological processes, is the only feature set that appears to have the potential to systematically show improvement over a good range of topics. We believe that further analysis is needed; we do not want to handpick topics for which particular feature sets perform well. Our results also showed that context did not seem to help uniformly over all topics.",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_8",
  "x": "While our method of extracting the GDepP features is identical to (Joshi and Penstein-Ros\u00e9, 2009 ), our method for extracting GDepO is an approximation of the method of<cite> (Somasundaran and Wiebe, 2010)</cite> , that does not rely on selecting particular patterns indicating the topics of arguing by using a development set. The LIWC feature set, which is based on a lexical hierarchy that includes social features, negative and positive emotion, and psychological processes, is the only feature set that appears to have the potential to systematically show improvement over a good range of topics. We believe that further analysis is needed; we do not want to handpick topics for which particular feature sets perform well. Our results also showed that context did not seem to help uniformly over all topics. The mean performance over all topics for contextual features using the combination of all features and the Naive Bayes learner was 53.0% for context and 54.62% for no context (p = .15%, not significant). Interesting, the use of contextual features provided surprisingly greater performance for particular topics. For example for 2nd Amendment, unigrams with context yield a performance of 69.23% as opposed to the best performing without context features using LIWC of 64.10%.",
  "y": "extends differences"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_9",
  "x": "The mean performance over all topics for contextual features using the combination of all features and the Naive Bayes learner was 53.0% for context and 54.62% for no context (p = .15%, not significant). Interesting, the use of contextual features provided surprisingly greater performance for particular topics. For example for 2nd Amendment, unigrams with context yield a performance of 69.23% as opposed to the best performing without context features using LIWC of 64.10%. The best performance of<cite> (Somasundaran and Wiebe, 2010)</cite> below the majority class baseline for all of the features without context. Should we conclude anything from the fact that 6 of the topics are idealogical, out of the 7 topics where contextual features provide the best performance? We believe that the significantly greater percentage of rebuttals for these topics should give a greater weight to contextual features, so it would be useful to examine stance classification performance on the subset of the posts that are rebuttals. We believe that context is important; our conclusion is that our current contextual features are naive -they are not capturing the relationship between a post and a parent post.",
  "y": "background"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_0",
  "x": "3 Experiments and Results ---------------------------------- **DATASETS AND CORPUS** We experimented with four datasets widely used in literature: BLESS (Baroni and Lenci, 2011) , EVALution (Santus et al., 2015) , Lenci/Benotto (Benotto, 2015) , and Weeds (Weeds et al., 2014) taken from the repository provided by <cite>(Shwartz et al., 2017)</cite> . The corpus of articles we use is a complete xml dump of the English Wikipedia dated 3 Nov 2017. ---------------------------------- **TESTING DIRECTIONALITY**",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_1",
  "x": "However, here too, it is 91.8%. For BLESS, as seen in (Santus et al., 2014) , the performance of SLQS is 87%. Similar to SLQS, our depth measure is motivated by distributional informativeness hypothesis <cite>(Shwartz et al., 2017)</cite> . However, without using the extensive computation of context vectors and entropy, we are able to demonstrate good performance. As can be seen by physically examining Wikipedia articles, many of them tend to have a Star topology. This is also indicative that the topology used plays a major role in this feature. More sophisticated techniques will be needed to identify the topology of individual articles.",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_2",
  "x": "This is also indicative that the topology used plays a major role in this feature. More sophisticated techniques will be needed to identify the topology of individual articles. ---------------------------------- **TESTING DETECTION** For this experiment, we aim to discriminate pairs of words connected by the hypernym relation, from words connected by other relations (meronym, coord, attribute, event, antonym, synonym). For each pair, we evaluate our scoring function given in expression (2). We compared our numbers with those given in <cite>(Shwartz et al., 2017)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_3",
  "x": "For this experiment, we aim to discriminate pairs of words connected by the hypernym relation, from words connected by other relations (meronym, coord, attribute, event, antonym, synonym). For each pair, we evaluate our scoring function given in expression (2). We compared our numbers with those given in <cite>(Shwartz et al., 2017)</cite> . In that paper, multiple measures are used, and the best performing measure for every row of the table is presented. We conducted the experiments for both, Star as well as the Linear topology. However, the results for Star topology were slightly better, hence we present these in Table ( 2). For the case of hypernym vs all other relations, except for EVALution, in all other data sets, our average precision (AP ) using both Jaccard and word2vec ( <cite>(Shwartz et al., 2017)</cite> call this as AP @all) is better than the best unsu- Table 2 : AP = average precision.",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_4",
  "x": "We conducted the experiments for both, Star as well as the Linear topology. However, the results for Star topology were slightly better, hence we present these in Table ( 2). For the case of hypernym vs all other relations, except for EVALution, in all other data sets, our average precision (AP ) using both Jaccard and word2vec ( <cite>(Shwartz et al., 2017)</cite> call this as AP @all) is better than the best unsu- Table 2 : AP = average precision. The Best AP and Best Measure is taken from <cite>(Shwartz et al., 2017)</cite> . pervised measure as reported in <cite>(Shwartz et al., 2017)</cite> . For comparing hypernyms against individual relations, we find that with Jaccard similarity, it performs better than the best measures on meronyms in EVALution, and coordinates in Weeds. However, it performs worse for both the relations in BLESS.",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_5",
  "x": "For the case of hypernym vs all other relations, except for EVALution, in all other data sets, our average precision (AP ) using both Jaccard and word2vec ( <cite>(Shwartz et al., 2017)</cite> call this as AP @all) is better than the best unsu- Table 2 : AP = average precision. The Best AP and Best Measure is taken from <cite>(Shwartz et al., 2017)</cite> . pervised measure as reported in <cite>(Shwartz et al., 2017)</cite> . For comparing hypernyms against individual relations, we find that with Jaccard similarity, it performs better than the best measures on meronyms in EVALution, and coordinates in Weeds. However, it performs worse for both the relations in BLESS. Our systems performs worse than the best measure whenever an Informativeness Measure <cite>(Shwartz et al., 2017)</cite> , like SLQS and its variants perform well. It performs better, or at least competitive, when the best performing measure is an Inclusion Measure or Similarity Measure (except for hypernym-vs-event in BLESS).",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_6",
  "x": "pervised measure as reported in <cite>(Shwartz et al., 2017)</cite> . For comparing hypernyms against individual relations, we find that with Jaccard similarity, it performs better than the best measures on meronyms in EVALution, and coordinates in Weeds. However, it performs worse for both the relations in BLESS. Our systems performs worse than the best measure whenever an Informativeness Measure <cite>(Shwartz et al., 2017)</cite> , like SLQS and its variants perform well. It performs better, or at least competitive, when the best performing measure is an Inclusion Measure or Similarity Measure (except for hypernym-vs-event in BLESS). A possible explanation of this is that the heading features that we use do not capture how informative a phrase is. However, having common headings is an indication of shared features, implying similarity, which is also indicated by inclusion measures.",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_7",
  "x": "Our systems performs worse than the best measure whenever an Informativeness Measure <cite>(Shwartz et al., 2017)</cite> , like SLQS and its variants perform well. It performs better, or at least competitive, when the best performing measure is an Inclusion Measure or Similarity Measure (except for hypernym-vs-event in BLESS). A possible explanation of this is that the heading features that we use do not capture how informative a phrase is. However, having common headings is an indication of shared features, implying similarity, which is also indicated by inclusion measures. However, it should be noted, that we are comparing our single system against the best performing one in each case. For finding the best measure, <cite>(Shwartz et al., 2017)</cite> finds the best by varying the measures as well as the features, whereas we have a fixed system. Our system took a day to set up (including coding effort), and a few mins to run.",
  "y": "extends differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_0",
  "x": "**RELATED WORKS** There are several works for WSD that do not depend on a sense tagged corpus, and they can be classified into three approaches according to main resources used: raw corpus based approach [2] , dictionary based approach [3, 4] and hierarchical lexical database approach. The hierarchical lexical database approach can be reclassified into three groups according to usages of the database: gloss based method [5] , conceptual density based method [6, 7] and relative based method [8,<cite> 9,</cite> 10] . Since our method is a kind of the relative based method, this section describes the related works of the relative based method. [8] introduced the relative based method using International Roget's Thesaurus as a hierarchical lexical database. His method is conducted as follows: 1) Get relatives of each sense of a target word from the Roget's Thesaurus. 2) Collect example sentences of the relatives, which are representative of each sense.",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_1",
  "x": "For example, an ambiguous word rail is a relative of a meaning bird of a target word crane at WordNet, but the word rail means railway for the most part, not the meaning related to bird. Therefore, most of the example sentences of rail are not helpful for WSD of crane. His method has another problem in disambiguating senses of a large number of target words because it requires a great amount of time and storage space to collect example sentences of relatives of the target words. <cite>[9]</cite> followed the method of [8] , but tried to resolve the ambiguous relative problem by using just unambiguous relatives. That is, the ambiguous relative rail is not utilized to build a training data of the word crane because the word rail is ambiguous. Another difference from [8] is on a lexical database: they utilized WordNet as a lexical database for acquiring relatives of target words instead of International Roget's Thesaurus. Since WordNet is freely available for research, various kinds of WSD studies based on WordNet can be compared with the method of <cite>[9]</cite> .",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_2",
  "x": "Therefore, most of the example sentences of rail are not helpful for WSD of crane. His method has another problem in disambiguating senses of a large number of target words because it requires a great amount of time and storage space to collect example sentences of relatives of the target words. <cite>[9]</cite> followed the method of [8] , but tried to resolve the ambiguous relative problem by using just unambiguous relatives. That is, the ambiguous relative rail is not utilized to build a training data of the word crane because the word rail is ambiguous. Another difference from [8] is on a lexical database: they utilized WordNet as a lexical database for acquiring relatives of target words instead of International Roget's Thesaurus. Since WordNet is freely available for research, various kinds of WSD studies based on WordNet can be compared with the method of <cite>[9]</cite> . They evaluated their method on 14 ambiguous nouns and achieved a good performance comparable to the methods based on the sense tagged corpus.",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_3",
  "x": "The problem becomes more serious when verbs, which most of the relatives are ambiguous, are disambiguated. Like [8] , the method also has a difficulty in disambiguating senses of many words because the method collects the example sentences of relatives of many words. [10] reimplemented the method of <cite>[9]</cite> using a web, which may be a very large corpus, in order to collect example sentences. They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as [8] and <cite>[9]</cite> . ---------------------------------- **WORD SENSE DISAMBIGUATION BY RELATIVE SELECTION** Our method disambiguates senses of a target word in a sentence by selecting only a relative among the relatives of the target word that most probably occurs in the sentence.",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_4",
  "x": "[10] reimplemented the method of <cite>[9]</cite> using a web, which may be a very large corpus, in order to collect example sentences. They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as [8] and <cite>[9]</cite> . ---------------------------------- **WORD SENSE DISAMBIGUATION BY RELATIVE SELECTION** Our method disambiguates senses of a target word in a sentence by selecting only a relative among the relatives of the target word that most probably occurs in the sentence. A flowchart of our method is presented in Figure 1 with an example 3 : 1) Given a new sentence including a target word, a set of relatives of the target word is created by looking up in WordNet. 2) Next, the relative that most probably occurs in the sentence is chosen from the set.",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_5",
  "x": "3) Finally, a sense of the target word is determined as the sense that is related to the selected relative. In this example, the relative stork is selected with the highest probability and the proper sense is determined as crane#1, which is related to the selected relative stork. Our method makes use of ambiguous relatives as well as unambiguous relatives unlike <cite>[9]</cite> and hence overcomes the shortage problem of relatives and also reduces the problem of ambiguous relatives in [8] by handling relatives separately instead of putting example sentences of the relatives together into a pool. ---------------------------------- **RELATIVE SELECTION** The selected relative of the i-th target word tw i in a sentence C is defined to be the relative of tw i that has the largest co-occurrence probability with the words in the sentence: where SR is the selected relative, r ij is the j-th relative of tw i , S rij is a sense of tw i that is related to the relative r ij , and W is a weight of r ij .",
  "y": "differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_6",
  "x": "We tried to compare our proposed method with the previous relative based methods. However, both of [8] and <cite>[9]</cite> did not evaluate their methods on a publicly available data. We implemented their methods and compared our method with them on the same evaluation data. When both of the methods are implemented, it is practically difficult to collect example sentences of all target words in the evaluation data. Instead, we implemented the previous methods to work with our CFM. WordNet was utilized as a lexical database to acquire relatives of target words and the sense disambiguation modules were implemented by using on Na\u00efve Bayesian classifier, which <cite>[9]</cite> adopted though [8] utilized International Roget's Thesaurus and other classifier similar to decision lists. Also the bias of word senses, which is presented at WordNet, is reflected on the implementation in order to be in a same condition with our method.",
  "y": "differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_7",
  "x": "In the table, hyper3 means 1 to 3 hypernyms (i.e. parents, grandparents and great-grandparent) and hypo3 is 1 to 3 hyponyms (i.e. children, grandchildren and great-grandchildren). ---------------------------------- **EXPERIMENTAL RESULTS** Comparison with Other Relative Based Methods. We tried to compare our proposed method with the previous relative based methods. However, both of [8] and <cite>[9]</cite> did not evaluate their methods on a publicly available data. We implemented their methods and compared our method with them on the same evaluation data.",
  "y": "uses"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_8",
  "x": "We tried to compare our proposed method with the previous relative based methods. However, both of [8] and <cite>[9]</cite> did not evaluate their methods on a publicly available data. We implemented their methods and compared our method with them on the same evaluation data. When both of the methods are implemented, it is practically difficult to collect example sentences of all target words in the evaluation data. Instead, we implemented the previous methods to work with our CFM. WordNet was utilized as a lexical database to acquire relatives of target words and the sense disambiguation modules were implemented by using on Na\u00efve Bayesian classifier, which <cite>[9]</cite> adopted though [8] utilized International Roget's Thesaurus and other classifier similar to decision lists. Also the bias of word senses, which is presented at WordNet, is reflected on the implementation in order to be in a same condition with our method.",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_9",
  "x": "f req(r l , w k ) and f req(r l ) are the co-occurrence frequency between r l and w k and the frequency of r l , respectively, and both frequencies can be obtained by looking up the matrix since the matrix contains the frequencies of words and word pairs. The main difference between [8] and <cite>[9]</cite> is whether ambiguous relatives are utilized or not. Considering the difference, we implemented the method of [8] to include the ambiguous relatives into relatives, but the method of <cite>[9]</cite> to exclude the ambiguous relatives. Table 3 . Comparison results with top 3 systems at SENSEVAL S2 LS S2 ALL S3 ALL [15] 40.2% 56.9% . [16] 29.3% 45.1% . [5] 24.4% 32.8% .",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_10",
  "x": "Considering the difference, we implemented the method of [8] to include the ambiguous relatives into relatives, but the method of <cite>[9]</cite> to exclude the ambiguous relatives. Table 3 . Comparison results with top 3 systems at SENSEVAL S2 LS S2 ALL S3 ALL [15] 40.2% 56.9% . [16] 29.3% 45.1% . [5] 24.4% 32.8% . [17] . . 58.3% [18] . . 54.8% [19] . . 48.1% Our method 40.94% 45.12% 51.35% Table 2 shows the comparison results. 7 In the table, All Relatives and Unambiguous Relatives represent the results of the reimplemented methods of [8] and <cite>[9]</cite> , respectively.",
  "y": "uses"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_11",
  "x": "[17] . . 58.3% [18] . . 54.8% [19] . . 48.1% Our method 40.94% 45.12% 51.35% Table 2 shows the comparison results. 7 In the table, All Relatives and Unambiguous Relatives represent the results of the reimplemented methods of [8] and <cite>[9]</cite> , respectively. It is observed in the table that our proposed method achieves better performance on all evaluation data than the previous methods though the improvement is not large. Hence, we may have an idea that our method handles relatives and in particular ambiguous relatives more effectively than [8] and <cite>[9]</cite> . Compared with <cite>[9]</cite> , [8] obtains a better performance, and the difference between the performance of them are totally more than 15 % on all of the evaluation data. From the comparison results, it is desirable to utilize ambiguous relatives as well as unambiguous relatives. [10] evaluated their method on nouns of lexical sample task of SENSEVAL-2.",
  "y": "uses"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_12",
  "x": "[16] 29.3% 45.1% . [5] 24.4% 32.8% . [17] . . 58.3% [18] . . 54.8% [19] . . 48.1% Our method 40.94% 45.12% 51.35% Table 2 shows the comparison results. 7 In the table, All Relatives and Unambiguous Relatives represent the results of the reimplemented methods of [8] and <cite>[9]</cite> , respectively. It is observed in the table that our proposed method achieves better performance on all evaluation data than the previous methods though the improvement is not large. Hence, we may have an idea that our method handles relatives and in particular ambiguous relatives more effectively than [8] and <cite>[9]</cite> . Compared with <cite>[9]</cite> , [8] obtains a better performance, and the difference between the performance of them are totally more than 15 % on all of the evaluation data.",
  "y": "differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_13",
  "x": "7 In the table, All Relatives and Unambiguous Relatives represent the results of the reimplemented methods of [8] and <cite>[9]</cite> , respectively. It is observed in the table that our proposed method achieves better performance on all evaluation data than the previous methods though the improvement is not large. Hence, we may have an idea that our method handles relatives and in particular ambiguous relatives more effectively than [8] and <cite>[9]</cite> . Compared with <cite>[9]</cite> , [8] obtains a better performance, and the difference between the performance of them are totally more than 15 % on all of the evaluation data. From the comparison results, it is desirable to utilize ambiguous relatives as well as unambiguous relatives. [10] evaluated their method on nouns of lexical sample task of SENSEVAL-2. Their method achieved 49.8% recall.",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_14",
  "x": "[10] evaluated their method on nouns of lexical sample task of SENSEVAL-2. Their method achieved 49.8% recall. When evaluated on the same nouns of the lexical sample task, our proposed method achieved 47.26%, and the method of [8] 45.61%, and the method of <cite>[9]</cite> 38.03%. Compared with our implementations, [10] utilized a web as a raw corpus that is much larger than our raw corpus, and employed various kinds of features such as bigram, trigram, part-of-speeches, etc. 8 Therefore, it can be conjectured that a size of a raw corpus and features play an important role in the performance. We can observe that in our implementation of the method of <cite>[9]</cite> , the data sparseness problem is very serious since unambiguous relatives are usually not frequent in the raw corpus. In the web, the problem seems to be alleviated.",
  "y": "differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_15",
  "x": "We can observe that in our implementation of the method of <cite>[9]</cite> , the data sparseness problem is very serious since unambiguous relatives are usually not frequent in the raw corpus. In the web, the problem seems to be alleviated. Further studies are required for the effects of various features. Comparison with Systems Participated in SENSEVAL. We also compared our method with the top systems at SENSEVAL that did not use sense tagged corpora. 9 Table 3 shows the official results of the top 3 participating systems at SENSEVAL-2 & 3 and experimental performance of our method. In the table, it is observed that our method is ranked in top 3 systems.",
  "y": "uses"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_16",
  "x": "The experimental results show that the proposed method effectively disambiguates many ambiguous words in SemCor and in test data for SENSEVAL all words task, as well as a small number of ambiguous words in test data for SENSEVAL lexical sample task. Also our method more correctly disambiguates senses than [8] and <cite>[9]</cite> . Furthermore, the proposed method achieved comparable performance with the top 3 ranked systems at SENSEVAL-2 & 3. In consequence, our method has two advantages over the previous methods ( [8] and <cite>[9]</cite> ): our method 1) handles the ambiguous relatives and unambiguous relatives more effectively, and 2) utilizes only one co-occurrence matrix for disambiguating all contents words instead of collecting training data of the content words. However, our method did not achieve good performances. One reason of the low performance is on the relatives irrelevant to the target words. That is, investigation of several instances which assign to incorrect senses shows that relatives irrelevant to the target words are often selected as the most probable relatives.",
  "y": "differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_17",
  "x": "The relative is selected by using a co-occurrence frequency between the relative and the words surrounding the target word in a given sentence. The cooccurrence frequencies are obtained from a raw corpus, not from a sense tagged corpus that is often required by other approaches. We tested the proposed method on SemCor data and SENSEVAL data, which are publicly available. The experimental results show that the proposed method effectively disambiguates many ambiguous words in SemCor and in test data for SENSEVAL all words task, as well as a small number of ambiguous words in test data for SENSEVAL lexical sample task. Also our method more correctly disambiguates senses than [8] and <cite>[9]</cite> . Furthermore, the proposed method achieved comparable performance with the top 3 ranked systems at SENSEVAL-2 & 3. In consequence, our method has two advantages over the previous methods ( [8] and <cite>[9]</cite> ): our method 1) handles the ambiguous relatives and unambiguous relatives more effectively, and 2) utilizes only one co-occurrence matrix for disambiguating all contents words instead of collecting training data of the content words.",
  "y": "differences"
 },
 {
  "id": "ad62ec914bb7b002952f22afdca15f_0",
  "x": "Learning knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, 5, 6] . How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, 9 ]. The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, <cite>13]</cite> , which considers syntactic contexts rather",
  "y": "background"
 },
 {
  "id": "ad62ec914bb7b002952f22afdca15f_1",
  "x": "How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, 9] . The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, <cite>13]</cite> , which considers syntactic contexts rather The 2016 Conference on Computational Linguistics and Speech Processing ROCLING 2016, pp.",
  "y": "background"
 },
 {
  "id": "ad62ec914bb7b002952f22afdca15f_2",
  "x": "There are three main steps in our rescoring approach. The first step is to have the parser to produce n-best parse trees with their structural scores. For each parsed tree including words, part-of-speech (PoS) and semantic role labels. Second, we extract word-to-word associations (or called word dependency, a dependency implies its close association with other words in either syntactic or semantic perspective) from large amounts of auto-parsed data and adopt word2vecf <cite>[13]</cite> to train dependency-based word embeddings. The last step is to build a structural rescoring method to find the best tree structure from the n-best candidates. We conduct experiments on the standard data sets of the Chinese Treebank. We also study how different types of embeddings influence on rescoring, including word, word with semantic role labels, and word senses (concepts).",
  "y": "uses"
 },
 {
  "id": "aeb6a815732b36d7602a9c43c47cfa_0",
  "x": "We propose a simple yet effective textbased user geolocation model based on a neural network with one hidden layer, which achieves state of the art performance over three Twitter benchmark geolocation datasets, in addition to producing word and phrase embeddings in the hidden layer that we show to be useful for detecting dialectal terms. As part of our analysis of dialectal terms, we release DAREDS, a dataset for evaluating dialect term detection methods. ---------------------------------- **INTRODUCTION** Many services such as web search (Leung et al., 2010) , recommender systems (Ho et al., 2012) , targeted advertising (Lim and Datta, 2013) , and rapid disaster response (Ashktorab et al., 2014) rely on the location of users to personalise information and extract actionable knowledge. Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (Jurgens et al., 2015) or some combination of these<cite> (Rahimi et al., 2015b</cite>,a) . The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (Eisenstein et al., 2010; Roller et al., 2012; Rout et al., 2013; Wing and Baldridge, 2014) or dialectology Eisenstein, 2015) .",
  "y": "background"
 },
 {
  "id": "aeb6a815732b36d7602a9c43c47cfa_1",
  "x": "The parameters are optimised using Adamx (Kingma and Ba, 2014) using Lasagne/Theano (Theano Development Team, 2016) . Following Cheng (2010) and Eisenstein (2010) , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\"). Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161. 4 The results reported in <cite>Rahimi et al. (2015b</cite>; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset. While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user (Rahimi et al., 2015a) . Note that it would, of course, be possible to combine text and network information in a joint deep learning model (Yang et al., 2016; Kipf and Welling, 2016) , which we leave to future work (noting that scalability will potentially be a major issue for the larger datasets). To test the applicability of the model's embeddings in dialectology, we created DAREDS.",
  "y": "differences"
 },
 {
  "id": "aeb6a815732b36d7602a9c43c47cfa_2",
  "x": [
   "The error is highest in states with lower training coverage (e.g. Maine, Montana, Wisconsin, Iowa and Kansas). We also randomly sampled 50 development samples from the 1000 samples with highest prediction errors to check the biases of the model. Most of the errors are the result of geolocating users from Eastern U.S. in Western U.S. particularly in Los Angeles and San Francisco. ---------------------------------- **DIALECTOLOGY** We quantitatively tested the quality of the geographical embeddings by calculating the micro-average recall of the k-nearest dialect terms (in terms of the proportion of retrieved dialect terms) given a dialect region, as shown in Figure 4 . Recall at 0.5% is about 3.6%, meaning that we were able to retrieve 3.6% of the dialect terms given the dialect region name in the geographical embedding space."
  ],
  "y": "differences"
 },
 {
  "id": "aeb6a815732b36d7602a9c43c47cfa_3",
  "x": "---------------------------------- **RESULTS** ---------------------------------- **GEOLOCATION** The performance of the text-based MLP model with k-d tree and k-means discretisation over the three datasets is shown in Table 1 . The results are also compared with state-of-the-art text-based methods based on a flat<cite> (Rahimi et al., 2015b</cite>; Cha et al., 2015) or hierarchical (Wing and Baldridge, 2014; Melo and Martins, 2015; Liu and Inkpen, 2015) geospatial representation. Our method outperforms both the flat and hierarchical text-based models by a large margin.",
  "y": "differences"
 },
 {
  "id": "af39041414dec545df878404328aab_0",
  "x": "****DISCRIMINATIVE TRAINING OF 150 MILLION TRANSLATION PARAMETERS AND ITS APPLICATION TO PRUNING**** **ABSTRACT** Until recently, the application of discriminative training to log linear-based statistical machine translation has been limited to tuning the weights of a limited number of features or training features with a limited number of parameters. In this paper, we propose to scale up discriminative training of<cite> (He and Deng, 2012)</cite> to train features with 150 million parameters, which is one order of magnitude higher than previously published effort, and to apply discriminative training to redistribute probability mass that is lost due to model pruning. The experimental results confirm the effectiveness of our proposals on NIST MT06 set over a strong baseline. ---------------------------------- **INTRODUCTION**",
  "y": "extends"
 },
 {
  "id": "af39041414dec545df878404328aab_1",
  "x": "It is therefore desirable to train the dense features for each rule in a discriminative fashion to maximize some translation criterion. The maximum expected BLEU training of<cite> (He and Deng, 2012</cite> ) is a recent effort towards this direction, and in this paper, we extend their work to a scaled-up task of discriminative training of the features of a strong hierarchical phrase-based model and confirm its effectiveness empirically. In this work, we further consider the application of discriminative training to pruned model. Various pruning techniques (Johnson et al., 2007; Zens et al., 2012; Eck et al., 2007; Lee et al., 2012; Tomeh et al., 2011) have been proposed recently to filter translation rules. One common consequence of pruning is that the probability distribution of many surviving rules become deficient, i.e. f p(f |e) < 1. In practice, others have chosen either to leave the pruned rules as it-is, or simply to re-normalize the probability mass by distributing the pruned mass to surviving rules proportionally. We argue that both approaches are suboptimal, and propose a more principled method to re-distribute the probability mass, i.e. using discriminative training with some translation criterion.",
  "y": "extends background"
 },
 {
  "id": "af39041414dec545df878404328aab_2",
  "x": "We argue that both approaches are suboptimal, and propose a more principled method to re-distribute the probability mass, i.e. using discriminative training with some translation criterion. Our experimental results demonstrate that at various pruning levels, our approach improves performance consistently. Particularly at the level of 50% of rules being pruned, the discriminatively trained models performs better than the unpruned baseline grammar. This shows that discriminative training makes it possible to achieve smaller models that perform comparably or even better than the baseline model. Our contributions in this paper are two-folded: First of all, we scale up the maximum expected BLEU training proposed in<cite> (He and Deng, 2012)</cite> in a number of ways including using 1) a hierarchical phrase-based model, 2) a richer feature set, and 3) a larger training set with a much larger parameter set, resulting in more than 150 million parameters in the model being updated, which is one order magnitude higher than the phrase-based model reported in<cite> (He and Deng, 2012)</cite> . We are able to show a reasonable improvement over this strong baseline. Secondly, we combine discriminative training with pruning techniques to reestimate parameters of pruned grammar.",
  "y": "differences extends"
 },
 {
  "id": "af39041414dec545df878404328aab_3",
  "x": "Our goal is to update \u03a6 towards \u03a6 that maximizes the expected BLEU scores of the entire training data given the current \u03bb: where B(\u00ca 1 ...\u00ca N ) is the BLEU score of the concatenated hypothesis of the entire training data, following<cite> (He and Deng, 2012)</cite> . Eq. 1 summarizes over all possible combinations of\u00ca 1 ...\u00ca N , which is intractable. Hence we make two simplifying approximations as follows. First, let the k-best hypotheses of the n-th sentence,\u00ca n = \u00ca 1 n , ...,\u00ca K n , approximate all its possible translation. In other words, we assume that K k=1P (\u00ca k n |F n ) = 1, \u2200n. Second, let the sum of sentence-level BLEU approximate the corpus BLEU.",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_4",
  "x": "where We detail the process in the Appendix. To further simplify the problem and relate it with model pruning, we consider to update a subset of \u03b8 \u2282 \u03a6 while keeping other parameterization of \u03a6 unchanged, where \u03b8 = {\u03b8 ij = p(e j |f i )} denotes our parameter set that satisfies j \u03b8 ij = 1 and \u03b8 ij \u2265 0. In experiments, we also consider {\u03b8 ji = p(f i |e j )}. To alleviate overfitting, we introduce KL-distance based reguralization as in<cite> (He and Deng, 2012)</cite> . We thus arrive at the following objective function: where \u03c4 controls the regularization term's contribution, and \u03b8 0 represents a prior parameter set, e.g., from the conventional maximum likelihood training.",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_5",
  "x": "We thus arrive at the following objective function: where \u03c4 controls the regularization term's contribution, and \u03b8 0 represents a prior parameter set, e.g., from the conventional maximum likelihood training. The optimization algorithm is based on the Extended Baum Welch (EBW) (Gopalakrishnan et al., 1991) as derived by<cite> (He and Deng, 2012)</cite> . The final update rule is as follow: where \u03b8 ij is the updated parameter, and \u03bb is the current feature's weight. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_6",
  "x": "For each sentence of this subset, we generate 500-best of unique hypotheses using the baseline model. The 1-best and the oracle BLEU scores for this subset are 40.19 and 47.06 respectively. Following<cite> (He and Deng, 2012)</cite> , we focus on discriminative training of p(f |e) and p(e|f ), which in practice affects around 150 million of parameters; hence the title. For the tuning and development sets, we set aside 1275 and 1239 sentences respectively from LDC2010E30 corpus. The tune set is used by PRO for tuning \u03bb while the dev set is used to decide the best DT model. As for the blind test set, we report the performance on the NIST MT06 evaluation set, which consists of 1644 sentences from news and web-blog domains. Our baseline system's performance on MT06 is 39.91 which is among the best number ever published so far in the community.",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_7",
  "x": "Following<cite> (He and Deng, 2012)</cite> , we focus on discriminative training of p(f |e) and p(e|f ), which in practice affects around 150 million of parameters; hence the title. For the tuning and development sets, we set aside 1275 and 1239 sentences respectively from LDC2010E30 corpus. The tune set is used by PRO for tuning \u03bb while the dev set is used to decide the best DT model. As for the blind test set, we report the performance on the NIST MT06 evaluation set, which consists of 1644 sentences from news and web-blog domains. Our baseline system's performance on MT06 is 39.91 which is among the best number ever published so far in the community. Table 1 compares the key components of our baseline system with that of<cite> (He and Deng, 2012)</cite> . As shown, we are working with a stronger system than<cite> (He and Deng, 2012)</cite> , especially in terms of the number of parameters under consideration |\u03b8|.",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_8",
  "x": "As for the blind test set, we report the performance on the NIST MT06 evaluation set, which consists of 1644 sentences from news and web-blog domains. Our baseline system's performance on MT06 is 39.91 which is among the best number ever published so far in the community. Table 1 compares the key components of our baseline system with that of<cite> (He and Deng, 2012)</cite> . As shown, we are working with a stronger system than<cite> (He and Deng, 2012)</cite> , especially in terms of the number of parameters under consideration |\u03b8|. He&Deng (2012) ---------------------------------- **DT OF 150 MILLION PARAMETERS**",
  "y": "differences"
 },
 {
  "id": "af39041414dec545df878404328aab_9",
  "x": "We then explore the optimal setting for \u03c4 which controls the contribution of the regularization term. Specifically, we perform grid search, exploring values of \u03c4 from 0.1 to 0.75. For each \u03c4 , we run several iterations of discriminative training where each iteration involves one simultaneous update of p(f |e) and p(e|f ) according to Eq. 4, followed by one update of \u03bb via PRO (as in<cite> (He and Deng, 2012)</cite> ). In total, we run 10 such iterations for each \u03c4 . Across different \u03c4 , we find that the first iteration provides most of the gain while the subsequent iterations provide additional, smaller gain with occassional performance degradation; thus the translation performance is not always monotonically increasing over iteration. We report the best score of each \u03c4 in Fig. 1 and at which iteration that score is produced. As shown in Fig. 1 , all settings of \u03c4 improve over the baseline and \u03c4 = 0.10 gives the highest gain of 0.45 BLEU score.",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_10",
  "x": "Specifically, we perform grid search, exploring values of \u03c4 from 0.1 to 0.75. For each \u03c4 , we run several iterations of discriminative training where each iteration involves one simultaneous update of p(f |e) and p(e|f ) according to Eq. 4, followed by one update of \u03bb via PRO (as in<cite> (He and Deng, 2012)</cite> ). In total, we run 10 such iterations for each \u03c4 . Across different \u03c4 , we find that the first iteration provides most of the gain while the subsequent iterations provide additional, smaller gain with occassional performance degradation; thus the translation performance is not always monotonically increasing over iteration. We report the best score of each \u03c4 in Fig. 1 and at which iteration that score is produced. As shown in Fig. 1 , all settings of \u03c4 improve over the baseline and \u03c4 = 0.10 gives the highest gain of 0.45 BLEU score. This improvement is in the same ballpark as in<cite> (He and Deng, 2012</cite> ) though on a scaledup task.",
  "y": "similarities"
 },
 {
  "id": "af39041414dec545df878404328aab_11",
  "x": "In total, we run 10 such iterations for each \u03c4 . Across different \u03c4 , we find that the first iteration provides most of the gain while the subsequent iterations provide additional, smaller gain with occassional performance degradation; thus the translation performance is not always monotonically increasing over iteration. We report the best score of each \u03c4 in Fig. 1 and at which iteration that score is produced. As shown in Fig. 1 , all settings of \u03c4 improve over the baseline and \u03c4 = 0.10 gives the highest gain of 0.45 BLEU score. This improvement is in the same ballpark as in<cite> (He and Deng, 2012</cite> ) though on a scaledup task. We next decode the MT06 using the best model (i.e. \u03c4 = 0.10 at 6-th iteration) observed on the dev set, and obtained 40.33 BLEU with an improvement of around 0.4 BLEU point. We see this result as confirming the effectiveness of discriminative training but on a larger-scale task, adding to what was reported by<cite> (He and Deng, 2012)</cite> .",
  "y": "extends"
 },
 {
  "id": "af39041414dec545df878404328aab_12",
  "x": "With the DT on pruned model, all pruning losses are reclaimed and the new pruned model is even better than the unpruned original model. This empirical result shows that leaving probability mass unassigned after pruning is suboptimal and that discriminative training provides a principled way to redistribute the mass. ---------------------------------- **CONCLUSION** In this paper, we first extend the maximum expected BLEU training of<cite> (He and Deng, 2012)</cite> to train two features of a state-of-the-art hierarchical phrasebased system, namely: p(f |e) and p(e|f ). Compared to<cite> (He and Deng, 2012)</cite> , we apply the algorithm to a strong baseline that is trained on a bigger parallel corpora and comes with a richer feature set. The number of parameters under consideration amounts to 150 million.",
  "y": "extends"
 },
 {
  "id": "af39041414dec545df878404328aab_13",
  "x": "BLEU point after pruning. With the DT on pruned model, all pruning losses are reclaimed and the new pruned model is even better than the unpruned original model. This empirical result shows that leaving probability mass unassigned after pruning is suboptimal and that discriminative training provides a principled way to redistribute the mass. ---------------------------------- **CONCLUSION** In this paper, we first extend the maximum expected BLEU training of<cite> (He and Deng, 2012)</cite> to train two features of a state-of-the-art hierarchical phrasebased system, namely: p(f |e) and p(e|f ). Compared to<cite> (He and Deng, 2012)</cite> , we apply the algorithm to a strong baseline that is trained on a bigger parallel corpora and comes with a richer feature set.",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_14",
  "x": "**CONCLUSION** In this paper, we first extend the maximum expected BLEU training of<cite> (He and Deng, 2012)</cite> to train two features of a state-of-the-art hierarchical phrasebased system, namely: p(f |e) and p(e|f ). Compared to<cite> (He and Deng, 2012)</cite> , we apply the algorithm to a strong baseline that is trained on a bigger parallel corpora and comes with a richer feature set. The number of parameters under consideration amounts to 150 million. Our experiments show that discriminative training these two features (out of 50) gives around 0.40 BLEU point improvement, which is consistent with the conclusion of<cite> (He and Deng, 2012)</cite> but in a much larger-scale system. Furthermore, we apply the algorithm to redistribute the probability mass of p(f |e) and p(e|f ) that is commonly lost due to conventional model pruning. Previous techniques either leave the probability mass as it is or distribute it proportionally among the surviving rules.",
  "y": "similarities"
 },
 {
  "id": "af39041414dec545df878404328aab_15",
  "x": "**APPENDIX** We describe the process to simplify Eq. 1 to Eq. 2, which is omitted in<cite> (He and Deng, 2012)</cite> . For conciseness, we drop the conditions and write P (\u00ca i |F i ) as P (\u00ca i ). We write Eq. 1 again below as Eq. 5 . We first focus on the first sentence E 1 /F 1 and expand the related terms from the equation as follow: Expanding the inner summation, we arrive at: Due to the that K k=1P (\u00ca K n |F n ) = 1, we can equate \u2200\u00ca 2 ...\u00ca N N i=2 P (\u00ca i ) and \u2200\u00ca 1 P (\u00ca 1 ) to 1.",
  "y": "differences"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_0",
  "x": "---------------------------------- **INTRODUCTION** To be able to answer the questions \"What causes ebola?\", \"What are the duties of a medical doctor?\", \"What are the differences between a terrorist and a victim?\", \"Which are the animals that have wings but cannot fly?\" one requires knowledge about verb-based relations. Over the years, researchers have developed various relation learning algorithms. Some (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) targeted specific relations like BornInYear, CorporationAcquired, others (Wu and Weld, 2010; <cite>Fader et al., 2011</cite> ) extracted any phrase denoting a relation in an English sentence. (Banko, 2009) used labeled data to learn relations, (Suchanek et al., 2007) used information encoded in the structured Wikipedia documents, (Riloff and Jones, 1999) bootstrapped patterns. As a result various knowledge bases have been produced like TopicSignatures (Agirre and Lacalle, 2004) , ConceptNet (Liu and Singh, 2004) , Yago (Suchanek et al., 2007) , NELL (Carlson et al., 2009) and ReVerb<cite> (Fader et al., 2011)</cite> .",
  "y": "background"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_1",
  "x": "Some (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) targeted specific relations like BornInYear, CorporationAcquired, others (Wu and Weld, 2010; <cite>Fader et al., 2011</cite> ) extracted any phrase denoting a relation in an English sentence. (Banko, 2009) used labeled data to learn relations, (Suchanek et al., 2007) used information encoded in the structured Wikipedia documents, (Riloff and Jones, 1999) bootstrapped patterns. As a result various knowledge bases have been produced like TopicSignatures (Agirre and Lacalle, 2004) , ConceptNet (Liu and Singh, 2004) , Yago (Suchanek et al., 2007) , NELL (Carlson et al., 2009) and ReVerb<cite> (Fader et al., 2011)</cite> . Despite the many efforts to date, yet there is no universal repository (or even a system), which for a given term it can immediately return all verb relations related to the term. However, one would still like to dispose of an automated procedure, which on the fly can accurately and quickly produce such information for any term. If available, such resource can aid different natural language processing tasks such as preposition sense disambiguation (Litkowski and Hargraves, 2007) , selectional preferences (Resnik, 1996; Ritter et al., 2010) , question answering (Ferrucci et al., 2010) and textual entailment (Szpektor et al., 2004) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_2",
  "x": "\u2022 We conduct a comparative study with the verb-based relation extraction system ReVerb<cite> (Fader et al., 2011)</cite> and show that our approach accurately extracts more verb-based relations. \u2022 We also compare the verb relations produced by our system with those available in existing knowledge bases, and observe that despite their completeness these repositories lack many verb-based relations. The rest of the paper is organized as follows. Next, we present related work. Section 3 outlines the verb-based relation learner. Section 4 describes the data collection process. Section 5 reports on the experimental results.",
  "y": "differences"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_3",
  "x": "Lots of attention has been payed on learning is-a and part-of relations (Hearst, 1992; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Pantel and Pennacchiotti, 2006; Carlson et al., 2009; Talukdar et al., 2008) . Others (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) have focused on learning specific relations like BornInYear, EmployedBy and CorporationAcquired. However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (Kim and Moldovan, 1993; Soderland et al., 1999) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance<cite> (Fader et al., 2011)</cite> . However, recently developed OpenIE systems like TextRunner (Banko et al., 2007; Banko, 2009) and ReVerb<cite> (Fader et al., 2011)</cite> surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in English sentences. (Banko et al., 2007; Banko, 2009 ) define relation to be any verb-prep, adj-noun construction. While such systems are great at learning general relations, they are not guided but simply gather in an undifferentiated way whatever happens to be contained in their input. In order to be able to extract all verb relations associated with a given term, such systems need to part-of-speech tag and parse a large document collection, then they have to extract all verb constructions and all arguments matching specific sets of patterns which were written by humans (or experts).",
  "y": "background"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_4",
  "x": "However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (Kim and Moldovan, 1993; Soderland et al., 1999) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance<cite> (Fader et al., 2011)</cite> . However, recently developed OpenIE systems like TextRunner (Banko et al., 2007; Banko, 2009) and ReVerb<cite> (Fader et al., 2011)</cite> surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in English sentences. (Banko et al., 2007; Banko, 2009 ) define relation to be any verb-prep, adj-noun construction. While such systems are great at learning general relations, they are not guided but simply gather in an undifferentiated way whatever happens to be contained in their input. In order to be able to extract all verb relations associated with a given term, such systems need to part-of-speech tag and parse a large document collection, then they have to extract all verb constructions and all arguments matching specific sets of patterns which were written by humans (or experts). Finally, they must filter out the information and retrieve only those verb relations that are associated with the specific term. Once compiled the repository is straightforward to query and use, however if a term is not present in the compiled repository, repeating the whole process on a new document collection becomes time consuming and unpractical.",
  "y": "background"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_5",
  "x": "However, there were only 11 verb relations for bomb and 2 verb relations for virus. This analysis shows that despite their completeness and richness, existing knowledge repositories can be further enriched with verb-based relations produced by our learning procedure. , shoot, beat, fought, fell, destroyed, fired, attacked, are trained, died, took, said, laughed, kicked, die, were humiliating, cheered, mocked, raised, drummed, captured, looted, ran, arrested, buried, defended ---------------------------------- **COMPARISON WITH EXISTING RELATION LEARNER** For our comparative study with existing systems, we used ReVerb 4<cite> (Fader et al., 2011)</cite> , which similarly to our approach was specifically designed to learn verb-based relations from unstructured texts. Currently, ReVerb has extracted relations from ClueWeb09 5 and Wikipedia, which have been freely distributed to the public. ReVerb learns relations by taking as input any document and applies POS-tagging, NP-chunking and a set of rules over all sentences in the document to generate triples containing the verbs and the arguments associated with them.",
  "y": "similarities"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_6",
  "x": "---------------------------------- **COMPARISON WITH EXISTING RELATION LEARNER** For our comparative study with existing systems, we used ReVerb 4<cite> (Fader et al., 2011)</cite> , which similarly to our approach was specifically designed to learn verb-based relations from unstructured texts. Currently, ReVerb has extracted relations from ClueWeb09 5 and Wikipedia, which have been freely distributed to the public. ReVerb learns relations by taking as input any document and applies POS-tagging, NP-chunking and a set of rules over all sentences in the document to generate triples containing the verbs and the arguments associated with them. According to<cite> (Fader et al., 2011)</cite> ReVerb outperforms TextRunner (Banko et al., 2007) and the open Wikipedia extractor WOE (Wu and Weld, 2010) in terms of the quantity and quality of the learned relations. For comparison, we took five terms from our experiment: ant, bomb, president, terrorists, virus and collected all verbs found by ReVerb in the ClueWeb09 and Wikipedia triples.",
  "y": "background"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_7",
  "x": "For comparison, we took five terms from our experiment: ant, bomb, president, terrorists, virus and collected all verbs found by ReVerb in the ClueWeb09 and Wikipedia triples. ---------------------------------- **CONCLUSION** Our key contribution is the development of a semi-supervised procedure, which starts with a term and a verb to learn from Web documents a large and diverse set of verb relations. We have conducted an experimental evaluation with 36 terms and have collected 26, 678 unique candidate verbs and 1, 040, 651 candidate argument fillers. We have evaluated the accuracy of our approach using human based evaluation and have compared results against the ReVerb<cite> (Fader et al., 2011)</cite> system and existing knowledge bases like NELL (Carlson et al., 2009) , Yago (Suchanek et al., 2007) and ConceptNet (Liu and Singh, 2004) . Our study showed that despite their completeness these resources lack verb-based information and there is plenty of room for improvement since they can be further enriched with verbs using our harvesting procedure.",
  "y": "similarities"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_0",
  "x": "Zero-shot NMT was first demonstrated by<cite> Johnson et al. (2017)</cite> . However, this zero-shot translation method is inferior to pivoting. They showed that the context vectors (from attention) for unseen language pairs differ from the seen language pairs, possibly explaining the degradation in translation quality. Lakew et al. (2017) tried to overcome this limitation by augmenting the training data with the pseudo-parallel unseen pairs generated by iterative application of the same zeroshot translation. Arivazhagan et al. (2018) included explicit language invariance losses in the optimization function to encourage parallel sentences to have the same representation. Reinforcement learning for zero-shot learning was explored by Sestorain et al. (2018) where the dual learning framework was combined with rewards from language models. Corpus Size.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_1",
  "x": "While initial research on NMT started with building translation systems between two languages, researchers discovered that the NMT framework can naturally incorporate multiple languages. Hence, there has been a massive increase in work on MT systems that involve more than two languages (Dong et al., 2015; Firat et al., 2016a; Cheng et al., 2017; <cite>Johnson et al., 2017</cite>;<cite> Chen et al., 2017</cite> Neubig and Hu, 2018) etc. We refer to NMT systems handling translation between more than one language pair as multilingual NMT (MNMT) systems. The ultimate goal MNMT research is to develop one model for translation between all possible languages by effective use of available linguistic resources. MNMT systems are desirable because training models with data from many language pairs might help acquire knowledge from multiple sources . Moreover, MNMT systems tend to generalize better due to exposure to diverse languages, leading to improved translation quality. This particular phenomenon is known as knowledge transfer (Pan and Yang, 2010) .",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_2",
  "x": "The ultimate goal MNMT research is to develop one model for translation between all possible languages by effective use of available linguistic resources. MNMT systems are desirable because training models with data from many language pairs might help acquire knowledge from multiple sources . Moreover, MNMT systems tend to generalize better due to exposure to diverse languages, leading to improved translation quality. This particular phenomenon is known as knowledge transfer (Pan and Yang, 2010) . Knowledge transfer has been strongly observed for translation between low-resource languages, which have scarce parallel corpora or other linguistic resources but have benefited from data in other languages . In addition, MNMT systems will be compact, because a single model handles translations for multiple languages<cite> (Johnson et al., 2017)</cite> . This can reduce the deployment footprint, which is crucial for con- There are multiple MNMT scenarios based on available resources and studies have been conducted for the following scenarios ( Figure 1 1 ) : Multiway Translation.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_3",
  "x": "For most of the language pairs in the world, there are small or no parallel corpora, and three main directions have been studied for this scenario. Transfer learning: Transferring translation knowledge from a high-resource language pair to improve the translation of a low-resource language pair . Pivot translation: Using a high-resource language (usually English) as a pivot to translate between a language pair (Firat et al., 2016a) . Zeroshot translation: Translating between language pairs without parallel corpora<cite> (Johnson et al., 2017)</cite> . Multi-Source Translation. Documents that have been translated into more than one language might, in the future, be required to be translated 1 Please see the supplementary material for papers related to each category. into another language.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_4",
  "x": "Zaremoodi et al. (2018) further proposed a routing network to dynamically control parameter sharing learned from the data. Designing the right sharing strategy is important to maintaining a balance between model compactness and translation accuracy. Dynamic Parameter or Representation Generation. Instead of defining the parameter sharing protocol a priori, Platanios et al. (2018) learned the degree of parameter sharing from the data. This is achieved by defining the language specific model parameters as a function of global parameters and language embeddings. This approach also reduces the number of language specific parameters (only language embeddings), while still allowing each language to have its own unique parameters for different network layers. In fact, the number of parameters is only a small multiple of the compact model (the multiplication factor accounts for the language embedding size)<cite> (Johnson et al., 2017)</cite> , but the language embeddings can directly impact the model parameters instead of the weak influence that language tags have.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_5",
  "x": "explored multiple methods for supporting target languages: (a) target language tag at beginning of the decoder, (b) target language dependent positional embeddings, and (c) divide hidden units of each decoder layer into shared and language-dependent ones. Each of these methods provide gains over<cite> Johnson et al. (2017)</cite> , and combining all gave the best results. ---------------------------------- **TRAINING PROTOCOLS** Joint Training. All the available languages pairs are trained jointly to minimize the mean negative log-likelihood for each language pair. As some language pairs would have more data than other languages, the model may be biased.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_6",
  "x": "All the available languages pairs are trained jointly to minimize the mean negative log-likelihood for each language pair. As some language pairs would have more data than other languages, the model may be biased. To avoid this, sentence pairs from different language pairs are sampled to maintain a healthy balance. Mini-batches can be comprised of a mix of samples from different language pairs<cite> (Johnson et al., 2017)</cite> or the training schedule can cycle through mini-batches consisting of a language pair only (Firat et al., 2016a) . For architectures with language specific layers, the latter approach is convenient to implement. Knowledge Distillation. In this approach suggested by Tan et al. (2019) , bilingual models are first trained for all language pairs involved.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_7",
  "x": "Leveraging out-of-domain parallel corpora and in-domain monolingual corpora for in-domain translation is known as domain adaptation for MT (Chu and Wang, 2018) . As we can treat each domain as a language, there are many similarities and common approaches between MNMT and domain adaptation for NMT. Therefore, similar to MNMT, when using out-of-domain parallel corpora for domain adaptation, multi-domain NMT and transfer learning based approaches (Chu et al., 2017) have been proposed for domain adaptation. When using in-domain monolingual corpora, a typical way of doing domain adaptation is generating a pseduo-parallel corpus by back-translating target in-domain monolingual corpora (Sennrich et al., 2016a) , which is similar to the pseduo-parallel corpus generation in MNMT (Firat et al., 2016b) . There are also many differences between MNMT and domain adaptation for NMT. While pivoting is a popular approach for MNMT (Cheng et al., 2017) , it is unsuitable for domain adaptation. As there are always vocabulary overlaps between different domains, there are no zero-shot translation<cite> (Johnson et al., 2017)</cite> settings in domain adaptation.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_8",
  "x": "Recent advances in cross-lingual word (Klementiev et al., 2012; Mikolov et al., 2013; Chandar et al., 2014; Artetxe et al., 2016; Conneau et al., 2018a; Jawanpuria et al., 2019) and sentence embeddings (Conneau et al., 2018b; Chen et al., 2018a; Artetxe and Schwenk, 2018) could provide directions for this line of investigation. Related Languages, Language Registers and Dialects. Translation involving related languages, language registers and dialects can be further explored given the importance of this use case. Code-Mixed Language. Addressing intrasentence multilingualism i.e. code mixed input and output, creoles and pidgins is an interesting research direction. The compact MNMT models can handle code-mixed input, but code-mixed output remains an open problem<cite> (Johnson et al., 2017)</cite> . Multilingual and Multi-Domain NMT.",
  "y": "future_work"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_0",
  "x": "Most image captioning research has focused on single-sentence captions, but the descriptive capacity of this form is limited; a single sentence can only describe in detail a small aspect of an image. Recent work has argued instead for image paragraph captioning with the aim of generating a (usually 5-8 sentence) paragraph describing an image. Compared with single-sentence captioning, paragraph captioning is a relatively new task. The main paragraph captioning dataset is the Visual Genome corpus, introduced by <cite>Krause et al. (2016)</cite> . When strong single-sentence captioning models are trained on this dataset, they produce repetitive paragraphs that are unable to describe diverse aspects of images. The generated paragraphs repeat a slight variant of the same sentence multiple times, even when beam search is used. Prior work, discussed in the following section, tried to address this repetition with architectural changes, such as hierarchical LSTMs, which separate the generation of sentence topics and words.",
  "y": "background"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_1",
  "x": "As introduced by Vinyals et al. (2014) , the encoder is a CNN pre-trained for classification and the decoder is a LSTM or GRU. Following work in machine translation, Xu et al. (2015) added an attention mechanism over the encoder features. Recently, Anderson et al. (2017) further improved single-sentence captioning performance by incorporating object detection in the encoder (bottomup attention) and adding an LSTM layer before attending to spatial features in the decoder (top-down attention). Single-sentence and paragraph captioning models are evaluated with a number of metrics, including some designed specifically for captioning (CIDEr) and some adopted from machine translation (BLEU, METEOR). CIDEr and BLEU measure accuracy with n-gram overlaps, with CIDEr weighting n-grams by TF-IDF (termfrequency inverse-document-frequency), and ME-TEOR uses unigram overlap, incorporating synonym and paraphrase matches. We discuss these metrics in greater detail when analyzing our experiments. <cite>Krause et al. (2016)</cite> introduced the first large-scale paragraph captioning dataset, a subset of the Visual Genome dataset, along with a number of models for paragraph captioning.",
  "y": "background"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_2",
  "x": "---------------------------------- **RELATED MODELS** The paragraph captioning models proposed by <cite>Krause et al. (2016)</cite> included template-based (nonneural) approaches and two encoder-decoder models. In both neural models, the encoder is an object detector pre-trained for dense captioning. In the first model, called the flat model, the decoder is a single LSTM which outputs an entire paragraph word-by-word. In the second model, called the hierarchical model, the decoder is composed of two LSTMs, where the output of one sentencelevel LSTM is used as input to the other word-level LSTM. Recently, Liang et al. (2017) extended this model with a third (paragraph-level) LSTM and added adversarial training.",
  "y": "background"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_3",
  "x": "In the first model, called the flat model, the decoder is a single LSTM which outputs an entire paragraph word-by-word. In the second model, called the hierarchical model, the decoder is composed of two LSTMs, where the output of one sentencelevel LSTM is used as input to the other word-level LSTM. Recently, Liang et al. (2017) extended this model with a third (paragraph-level) LSTM and added adversarial training. In total, their model (RTT-GAN) incorporates three LSTMs, two attention mechanisms, a phrase copy mechanism, and two adversarial discriminators. To the best of our knowledge, this model achieves state-ofthe-art performance of 16.9 CIDEr on the Visual Genome dataset (without external data). For our experiments, we use the top-down single-sentence captioning model in Anderson et al. (2017) . This model is similar to the \"flat\" model in <cite>Krause et al. (2016)</cite> , except that it incorporates attention with a top-down mechanism.",
  "y": "similarities background"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_4",
  "x": "Our encoder is a convolutional network pretrained for object detection (as opposed to dense captioning, as in <cite>Krause et al. (2016)</cite> and Liang et al. (2017) ). METEOR CIDEr BLEU-1 BLEU-2 BLEU-3 BLEU-4 Krause et al. (Template) The encoder extracts between 10 and 100 objects per image and applies spatial max-pooling to yield a single feature vector of dimension 2048 per object. The decoder is a 1-layer LSTM with hidden dimension 512 and top-down attention. Evaluation is done on the Visual Genome dataset with the splits provided by <cite>Krause et al. (2016)</cite> . We first train for 25 epochs with crossentropy (XE) loss, using Adam with learning rate 5 \u00b7 10 \u22124 . We then train an additional 25 epochs with repetition-penalized SCST targeting a CIDEr-based reward, using Adam with learning rate 5 \u00b7 10 \u22125 . Our PyTorch-based implementation is available at https://github.com/lukemelas/ image-paragraph-captioning.",
  "y": "differences"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_5",
  "x": "---------------------------------- **METHODS AND RESULTS** For our paragraph captioning model we use the top-down model from Anderson et al. (2017) . Our encoder is a convolutional network pretrained for object detection (as opposed to dense captioning, as in <cite>Krause et al. (2016)</cite> and Liang et al. (2017) ). METEOR CIDEr BLEU-1 BLEU-2 BLEU-3 BLEU-4 Krause et al. (Template) The encoder extracts between 10 and 100 objects per image and applies spatial max-pooling to yield a single feature vector of dimension 2048 per object. The decoder is a 1-layer LSTM with hidden dimension 512 and top-down attention. Evaluation is done on the Visual Genome dataset with the splits provided by <cite>Krause et al. (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "b2cb08afadadeddc0f8e7267163c0e_0",
  "x": "While most of the researchers in the field are familiar with the methods applied on English, few of them have closely looked at the original research carried out in other languages. For example, in languages such as Chinese, researchers have been looking at the ability of characters to carry sentiment information (Ku et al., 2005; Xiang, 2011) . In Romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve (<cite>Banea et al., 2008</cite>) . These additional sources of information may not be available across all languages, yet, various articles have pointed out that by investigating a synergistic approach for detecting subjectivity and sentiment in multiple languages at the same time, improvements can be achieved not only in other languages, but in English as well. The development and interest in these methods is also highly motivated by the fact that only 27% of Internet users speak English (www.internetworldstats.com/stats.htm, Oct 11, 2011), and that number diminishes further every year, as more people across the globe gain Internet access. The aim of this tutorial is to familiarize the attendees with the subjectivity and sentiment research carried out on languages other than English in order to enable and promote crossfertilization. Specifically, we will review work along three main directions.",
  "y": "background"
 },
 {
  "id": "b2cb08afadadeddc0f8e7267163c0e_1",
  "x": "**** While much of the research work in this area has been applied to English, research on other languages is growing, including Japanese, Chinese, German, Spanish, Romanian. While most of the researchers in the field are familiar with the methods applied on English, few of them have closely looked at the original research carried out in other languages. For example, in languages such as Chinese, researchers have been looking at the ability of characters to carry sentiment information (Ku et al., 2005; Xiang, 2011) . In Romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve (<cite>Banea et al., 2008</cite>) . These additional sources of information may not be available across all languages, yet, various articles have pointed out that by investigating a synergistic approach for detecting subjectivity and sentiment in multiple languages at the same time, improvements can be achieved not only in other languages, but in English as well. The development and interest in these methods is also highly motivated by the fact that only 27% of Internet users speak English (www.internetworldstats.com/stats.htm, Oct 11, 2011), and that number diminishes further every year, as more people across the globe gain Internet access.",
  "y": "motivation"
 },
 {
  "id": "b2cb08afadadeddc0f8e7267163c0e_2",
  "x": "While subjectivity classification labels text as either subjective or objective, sentiment classification adds an additional level of granularity, by further classifying subjective text as either positive, negative or neutral. ---------------------------------- **** While much of the research work in this area has been applied to English, research on other languages is growing, including Japanese, Chinese, German, Spanish, Romanian. While most of the researchers in the field are familiar with the methods applied on English, few of them have closely looked at the original research carried out in other languages. For example, in languages such as Chinese, researchers have been looking at the ability of characters to carry sentiment information (Ku et al., 2005; Xiang, 2011) . In Romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve (<cite>Banea et al., 2008</cite>) .",
  "y": "motivation"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_0",
  "x": "**INTRODUCTION** Word embeddings are nowadays pervasive on a wide spectrum of Natural Language Processing (NLP) and Natural Language Understanding (NLU) applications. These word representations improved downstream tasks in many domains such as machine translation, syntactic parsing, text classification, and machine comprehension, among others [6] . Ranging from count-based to predictive or task-based methods, in the past years, many approaches were developed to produce word embeddings, such as Neural Probabilistic Language Model [3] , Word2Vec [28] , GloVe [32] , and more recently ELMo <cite>[33]</cite> , to name a few. Although most of the recent word embedding techniques rely on the distributional linguistic hypothesis, they differ on the assumptions of how meaning or context are modeled to produce the word embeddings. These differences between word embedding techniques can have unsuspected implications regarding their performance in downstream tasks as well as in their capacity to capture linguistic properties. Nowadays, the choice of word embeddings for particular downstream tasks is still a matter of experimentation and evaluation.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_1",
  "x": "By using weighted averages and modifying them using singular-value decomposition (SVD), the method known as smooth inverse frequency (SIF) [1] , proved to be a strong baseline over traditional averaging. Recently, p-mean [35] also demonstrated improvements over SIF and traditional averaging by concatenating power means of the embeddings, closing the gap with other complex sentence embedding techniques such as InferSent [10] . Other sentence embedding techniques were also developed based on encoder/decoder architectures, such as the Skip-Thought [23] , where the skip-gram model from Word2Vec [28] was abstracted to form a sentence level encoder that is trained on a self-supervised fashion. Recently, bi-directional LSTM models were also employed by InferSent [10] on a supervised training scheme using the Stanford Natural Language Inference (SNLI) dataset [5] to predict entailment/contradiction. InferSent [10] proved to yield much better results on a variety of downstream tasks when compared to many strong baselines or self-supervised methods such as Skip-Thought [23] , by leveraging strong supervision. Lately, the Universal Sentence Encoder (USE) [8] mixed an unsupervised task using a large corpus together with the supervised SNLI task and showed a significant improvement by leveraging the Transformer architecture [37] , which is solely based on attention mechanisms, although without providing an evaluation with other baselines and previous works such as InferSent [10] . Neural Language Models can be tracked back to [3] , and more recently deep bi-directional language models (biLM) <cite>[33]</cite> have successfully been applied to word embeddings in order to incorporate contextual information.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_2",
  "x": "There are several implementations of word embeddings in the literature. Following the pioneering work by [3] on the neural language model for distributed word representations, the seminal Word2Vec [28] is one of the first popular approaches of word embeddings based on neural networks. This type of representation is able to preserve semantic relationships between words and their context, where context is modeled by nearby words. In [28] , they presented two different methods to compute Two main challenges exist when learning high-quality representations: they should capture semantic and syntax and the different meanings the word can represent in different contexts (polysemy). To solve these two issues, Embedding from Language Models (ELMo) <cite>[33]</cite> was recently introduced. It uses representations from a bi-directional LSTM that is trained with a language model (LM) objective on a large text dataset. ELMo <cite>[33]</cite> representations are a function of the internal layers of the bi-directional Language Model (biLM), which provides a very rich representation about the tokens.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_3",
  "x": "Following the pioneering work by [3] on the neural language model for distributed word representations, the seminal Word2Vec [28] is one of the first popular approaches of word embeddings based on neural networks. This type of representation is able to preserve semantic relationships between words and their context, where context is modeled by nearby words. In [28] , they presented two different methods to compute Two main challenges exist when learning high-quality representations: they should capture semantic and syntax and the different meanings the word can represent in different contexts (polysemy). To solve these two issues, Embedding from Language Models (ELMo) <cite>[33]</cite> was recently introduced. It uses representations from a bi-directional LSTM that is trained with a language model (LM) objective on a large text dataset. ELMo <cite>[33]</cite> representations are a function of the internal layers of the bi-directional Language Model (biLM), which provides a very rich representation about the tokens. Like in fastText [4] , ELMo <cite>[33]</cite> breaks the tradition of word embeddings by incorporating sub-word units, but ELMo <cite>[33]</cite> has also some fundamental differences with previous shallow representations such as fastText or Word2Vec.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_4",
  "x": "It uses representations from a bi-directional LSTM that is trained with a language model (LM) objective on a large text dataset. ELMo <cite>[33]</cite> representations are a function of the internal layers of the bi-directional Language Model (biLM), which provides a very rich representation about the tokens. Like in fastText [4] , ELMo <cite>[33]</cite> breaks the tradition of word embeddings by incorporating sub-word units, but ELMo <cite>[33]</cite> has also some fundamental differences with previous shallow representations such as fastText or Word2Vec. In ELMo <cite>[33]</cite> , they use a deep representation by incorporating internal representations of the LSTM network, therefore capturing the meaning and syntactical aspects of words. Since ELMo <cite>[33]</cite> is based on a language model, each token representation is a function of the entire input sentence, which can overcome the limitations of previous word embeddings where each word is usually modeled as an average of their multiple contexts. Through the lens of the Ludwig Wittgenstein philosophy of language [40] , it is clear that the ELMo <cite>[33]</cite> embeddings are a better approximation to the idea of \"meaning is use\" [40] , where a word can contain a wide spectrum of different meanings depending on context, as opposed to traditional word embeddings that are not only context-independent but have a very limited definition of context. Although bag-of-words of word embeddings showed good performance for some tasks, it is still unclear how to properly represent the full sentence meaning.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_5",
  "x": "ELMo <cite>[33]</cite> representations are a function of the internal layers of the bi-directional Language Model (biLM), which provides a very rich representation about the tokens. Like in fastText [4] , ELMo <cite>[33]</cite> breaks the tradition of word embeddings by incorporating sub-word units, but ELMo <cite>[33]</cite> has also some fundamental differences with previous shallow representations such as fastText or Word2Vec. In ELMo <cite>[33]</cite> , they use a deep representation by incorporating internal representations of the LSTM network, therefore capturing the meaning and syntactical aspects of words. Since ELMo <cite>[33]</cite> is based on a language model, each token representation is a function of the entire input sentence, which can overcome the limitations of previous word embeddings where each word is usually modeled as an average of their multiple contexts. Through the lens of the Ludwig Wittgenstein philosophy of language [40] , it is clear that the ELMo <cite>[33]</cite> embeddings are a better approximation to the idea of \"meaning is use\" [40] , where a word can contain a wide spectrum of different meanings depending on context, as opposed to traditional word embeddings that are not only context-independent but have a very limited definition of context. Although bag-of-words of word embeddings showed good performance for some tasks, it is still unclear how to properly represent the full sentence meaning. Nowadays, there is still no consensus on how to represent sentences and many studies were proposed towards that research direction.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_6",
  "x": "ELMo <cite>[33]</cite> representations are a function of the internal layers of the bi-directional Language Model (biLM), which provides a very rich representation about the tokens. Like in fastText [4] , ELMo <cite>[33]</cite> breaks the tradition of word embeddings by incorporating sub-word units, but ELMo <cite>[33]</cite> has also some fundamental differences with previous shallow representations such as fastText or Word2Vec. In ELMo <cite>[33]</cite> , they use a deep representation by incorporating internal representations of the LSTM network, therefore capturing the meaning and syntactical aspects of words. Since ELMo <cite>[33]</cite> is based on a language model, each token representation is a function of the entire input sentence, which can overcome the limitations of previous word embeddings where each word is usually modeled as an average of their multiple contexts. Through the lens of the Ludwig Wittgenstein philosophy of language [40] , it is clear that the ELMo <cite>[33]</cite> embeddings are a better approximation to the idea of \"meaning is use\" [40] , where a word can contain a wide spectrum of different meanings depending on context, as opposed to traditional word embeddings that are not only context-independent but have a very limited definition of context. Although bag-of-words of word embeddings showed good performance for some tasks, it is still unclear how to properly represent the full sentence meaning. Nowadays, there is still no consensus on how to represent sentences and many studies were proposed towards that research direction.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_7",
  "x": "Like in fastText [4] , ELMo <cite>[33]</cite> breaks the tradition of word embeddings by incorporating sub-word units, but ELMo <cite>[33]</cite> has also some fundamental differences with previous shallow representations such as fastText or Word2Vec. In ELMo <cite>[33]</cite> , they use a deep representation by incorporating internal representations of the LSTM network, therefore capturing the meaning and syntactical aspects of words. Since ELMo <cite>[33]</cite> is based on a language model, each token representation is a function of the entire input sentence, which can overcome the limitations of previous word embeddings where each word is usually modeled as an average of their multiple contexts. Through the lens of the Ludwig Wittgenstein philosophy of language [40] , it is clear that the ELMo <cite>[33]</cite> embeddings are a better approximation to the idea of \"meaning is use\" [40] , where a word can contain a wide spectrum of different meanings depending on context, as opposed to traditional word embeddings that are not only context-independent but have a very limited definition of context. Although bag-of-words of word embeddings showed good performance for some tasks, it is still unclear how to properly represent the full sentence meaning. Nowadays, there is still no consensus on how to represent sentences and many studies were proposed towards that research direction. Skip-Thought Vectors [23] are based on a sentence encoder that, instead of predicting the context of a word as Word2Vec, it predicts the surrounding sentences of a given sentence.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_8",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** In this section, we describe where the pre-trained models were obtained as well as the procedures employed to evaluate each method. ELMo (BoW, all layers, 5.5B) <cite>[33]</cite> : this model was obtained from the authors' website at https: //allennlp.org/elmo. According to the authors, the model was trained on a dataset with 5.5B tokens consisting of Wikipedia (1.9B) and all of the monolingual news crawl data from WMT 2008-2012 (3.6B). To evaluate this model, we used the AllenNLP framework [13] . An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_9",
  "x": "An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . ELMo (BoW, all layers, original) <cite>[33]</cite> : this model was obtained from the authors website at https://allennlp.org/elmo. According to the authors, the model was trained on the 1 Billion Word Benchmark, approximately 800M tokens of news crawl data from WMT 2011. To evaluate this model, we used the AllenNLP framework [13] . An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model and averaging along the word dimension. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> .",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_10",
  "x": "In this section, we describe where the pre-trained models were obtained as well as the procedures employed to evaluate each method. ELMo (BoW, all layers, 5.5B) <cite>[33]</cite> : this model was obtained from the authors' website at https: //allennlp.org/elmo. According to the authors, the model was trained on a dataset with 5.5B tokens consisting of Wikipedia (1.9B) and all of the monolingual news crawl data from WMT 2008-2012 (3.6B). To evaluate this model, we used the AllenNLP framework [13] . An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . ELMo (BoW, all layers, original) <cite>[33]</cite> : this model was obtained from the authors website at https://allennlp.org/elmo.",
  "y": "differences"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_11",
  "x": "ELMo (BoW, all layers, original) <cite>[33]</cite> : this model was obtained from the authors website at https://allennlp.org/elmo. According to the authors, the model was trained on the 1 Billion Word Benchmark, approximately 800M tokens of news crawl data from WMT 2011. To evaluate this model, we used the AllenNLP framework [13] . An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model and averaging along the word dimension. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . ELMo (BoW, top layer, original) <cite>[33]</cite> : the same model and procedure as in ELMo (BoW, all layers, original) was employed, except that in this experiment, we used only the top layer representation from the ELMo <cite>[33]</cite> model. As shown in <cite>[33]</cite> , the higher-level LSTM representations capture context-dependent aspects of meaning, while the lower level representations capture aspects of syntax.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_12",
  "x": "According to the authors, the model was trained on the 1 Billion Word Benchmark, approximately 800M tokens of news crawl data from WMT 2011. To evaluate this model, we used the AllenNLP framework [13] . An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model and averaging along the word dimension. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . ELMo (BoW, top layer, original) <cite>[33]</cite> : the same model and procedure as in ELMo (BoW, all layers, original) was employed, except that in this experiment, we used only the top layer representation from the ELMo <cite>[33]</cite> model. As shown in <cite>[33]</cite> , the higher-level LSTM representations capture context-dependent aspects of meaning, while the lower level representations capture aspects of syntax. Therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_13",
  "x": "ELMo (BoW, all layers, original) <cite>[33]</cite> : this model was obtained from the authors website at https://allennlp.org/elmo. According to the authors, the model was trained on the 1 Billion Word Benchmark, approximately 800M tokens of news crawl data from WMT 2011. To evaluate this model, we used the AllenNLP framework [13] . An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model and averaging along the word dimension. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . ELMo (BoW, top layer, original) <cite>[33]</cite> : the same model and procedure as in ELMo (BoW, all layers, original) was employed, except that in this experiment, we used only the top layer representation from the ELMo <cite>[33]</cite> model. As shown in <cite>[33]</cite> , the higher-level LSTM representations capture context-dependent aspects of meaning, while the lower level representations capture aspects of syntax.",
  "y": "differences"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_14",
  "x": "ELMo (BoW, top layer, original) <cite>[33]</cite> : the same model and procedure as in ELMo (BoW, all layers, original) was employed, except that in this experiment, we used only the top layer representation from the ELMo <cite>[33]</cite> model. As shown in <cite>[33]</cite> , the higher-level LSTM representations capture context-dependent aspects of meaning, while the lower level representations capture aspects of syntax. Therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . [7] To measure the semantic similarity between two sentences from 0 (not similar Knowledge Semantic Relatedness (SICK-R) [27] To measure the degree of semantic relatedness between sentences from 0 (not related) to 5 (related) A man is singing a song and playing the guitar",
  "y": "extends"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_15",
  "x": "To evaluate this model, we used the AllenNLP framework [13] . An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model and averaging along the word dimension. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . ELMo (BoW, top layer, original) <cite>[33]</cite> : the same model and procedure as in ELMo (BoW, all layers, original) was employed, except that in this experiment, we used only the top layer representation from the ELMo <cite>[33]</cite> model. As shown in <cite>[33]</cite> , the higher-level LSTM representations capture context-dependent aspects of meaning, while the lower level representations capture aspects of syntax. Therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> .",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_16",
  "x": "Therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . [7] To measure the semantic similarity between two sentences from 0 (not similar Knowledge Semantic Relatedness (SICK-R) [27] To measure the degree of semantic relatedness between sentences from 0 (not related) to 5 (related) A man is singing a song and playing the guitar A man is opening a package that contains headphones ----------------------------------",
  "y": "differences"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_17",
  "x": "**DOWNSTREAM CLASSIFICATION TASKS** In Table 6 we show the tabular results for the downstream classification tasks, and in Figure 1 we show a graphical comparison between the different methods. As seen in Table 6 , although no method had a consistent performance among all tasks, ELMo <cite>[33]</cite> achieved best results in 5 out of 9 tasks. Even though ELMo <cite>[33]</cite> was trained on a language model objective, it is important to note that in this experiment a bag-of-words approach was employed. Therefore, these results are quite impressive, which lead us to believe that excellent results can be obtained by integrating ELMo <cite>[33]</cite> and the trainable task-specific weighting scheme described in <cite>[33]</cite> into InferSent [10] . InferSent [10] achieved very good results in the paraphrase detection as well as in the SICK-E (entailment). We hypothesize that these results were due to the similarity of these tasks to the tasks were InferSent [10] was trained on (SNLI and MultiNLI).",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_18",
  "x": "Even though ELMo <cite>[33]</cite> was trained on a language model objective, it is important to note that in this experiment a bag-of-words approach was employed. Therefore, these results are quite impressive, which lead us to believe that excellent results can be obtained by integrating ELMo <cite>[33]</cite> and the trainable task-specific weighting scheme described in <cite>[33]</cite> into InferSent [10] . InferSent [10] achieved very good results in the paraphrase detection as well as in the SICK-E (entailment). We hypothesize that these results were due to the similarity of these tasks to the tasks were InferSent [10] was trained on (SNLI and MultiNLI). As described in [10] , the SICK-E can be seen as an out-domain version of the SNLI dataset. The Universal Sentence Encoder (USE) [8] model, with the Transformer encoder, also achieved good results on the product review (CR) and on the question-type (TREC) tasks. Given that the USE model was trained on SNLI as well as on web question-answer pages, it is possible that these results were also due to the similarity of these tasks to the training data employed by the USE model.",
  "y": "differences"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_19",
  "x": "As seen in Table 6 , although no method had a consistent performance among all tasks, ELMo <cite>[33]</cite> achieved best results in 5 out of 9 tasks. Even though ELMo <cite>[33]</cite> was trained on a language model objective, it is important to note that in this experiment a bag-of-words approach was employed. Therefore, these results are quite impressive, which lead us to believe that excellent results can be obtained by integrating ELMo <cite>[33]</cite> and the trainable task-specific weighting scheme described in <cite>[33]</cite> into InferSent [10] . InferSent [10] achieved very good results in the paraphrase detection as well as in the SICK-E (entailment). We hypothesize that these results were due to the similarity of these tasks to the tasks were InferSent [10] was trained on (SNLI and MultiNLI). As described in [10] , the SICK-E can be seen as an out-domain version of the SNLI dataset. The Universal Sentence Encoder (USE) [8] model, with the Transformer encoder, also achieved good results on the product review (CR) and on the question-type (TREC) tasks.",
  "y": "future_work"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_20",
  "x": "As we can see, sentence embedding methods are still far away from the idea of a universal sentence encoder that can have a broad transfer quality. Given that ELMo <cite>[33]</cite> demonstrated excellent results on a broad set of tasks, it is clear that a proper integration of deep representation from language models can potentially improve sentence embedding methods by a significant margin and it is a promising research line. For completeness, we also provide an evaluation using Logistic Regression instead of a MLP in Table  11 of the appendix. ---------------------------------- **SEMANTIC RELATEDNESS AND TEXTUAL SIMILARITY TASKS** As can be seen in Table 7 , where we report the results for the semantic relatedness and textual similarity tasks, the Universal Sentence Encoder (USE) [8] using Transformer model achieved excellent results on almost all tasks, except for the SICK-R (semantic relatedness) where InferSent [10] achieved better results. In Figure 2 we show a graphical comparison.",
  "y": "future_work"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_21",
  "x": "In Table 8 we report the results for the linguistic probing tasks and in Figure 3 we show a graphical comparison as well. As we can see in Table 8 , ELMo <cite>[33]</cite> was one of the methods that were able to achieve high performance on a broad set of different tasks. Interestingly, in the BShift (bi-gram shift) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, ELMo <cite>[33]</cite> achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as \"This is my Eve Christmas\", a sample from the BShift dataset. In [11] , they found that the binned sentence length task (SentLen) was negatively correlated with the performance in downstream tasks. This hypothesis was also supported by the model learning dynamics, since it seems that as model starts to capture deeper linguistic properties, it will tend to forget about this superficial feature [11] . However, the <cite>[33]</cite> bag-of-words not only achieved the best result in the SentLent task but also in many downstream tasks. Our hypothesis is that this is due to the fact that ELMo <cite>[33]</cite> is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging SOMO task.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_22",
  "x": "As can be seen in Table 7 , where we report the results for the semantic relatedness and textual similarity tasks, the Universal Sentence Encoder (USE) [8] using Transformer model achieved excellent results on almost all tasks, except for the SICK-R (semantic relatedness) where InferSent [10] achieved better results. In Figure 2 we show a graphical comparison. ---------------------------------- **LINGUISTIC PROBING TASKS** In Table 8 we report the results for the linguistic probing tasks and in Figure 3 we show a graphical comparison as well. As we can see in Table 8 , ELMo <cite>[33]</cite> was one of the methods that were able to achieve high performance on a broad set of different tasks. Interestingly, in the BShift (bi-gram shift) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, ELMo <cite>[33]</cite> achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as \"This is my Eve Christmas\", a sample from the BShift dataset.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_23",
  "x": "In [11] , they found that the binned sentence length task (SentLen) was negatively correlated with the performance in downstream tasks. This hypothesis was also supported by the model learning dynamics, since it seems that as model starts to capture deeper linguistic properties, it will tend to forget about this superficial feature [11] . However, the <cite>[33]</cite> bag-of-words not only achieved the best result in the SentLent task but also in many downstream tasks. Our hypothesis is that this is due to the fact that ELMo <cite>[33]</cite> is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging SOMO task. ELMo <cite>[33]</cite> word embeddings can be seen as analogous to the hypercolumns [15] approach in Computer Vision, where multiple feature levels are aggregated to form a single pixelwise representation. We leave the exploration of probing tasks for each ELMo <cite>[33]</cite> layer representation to future research, given that it could provide a framework to expose the linguistic properties capture by each representation level of the LSTM. In [11] , they also found that the WC (Word Content) task was positively correlated with the performance in a wide variety of downstream tasks.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_24",
  "x": "Our hypothesis is that this is due to the fact that ELMo <cite>[33]</cite> is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging SOMO task. ELMo <cite>[33]</cite> word embeddings can be seen as analogous to the hypercolumns [15] approach in Computer Vision, where multiple feature levels are aggregated to form a single pixelwise representation. We leave the exploration of probing tasks for each ELMo <cite>[33]</cite> layer representation to future research, given that it could provide a framework to expose the linguistic properties capture by each representation level of the LSTM. In [11] , they also found that the WC (Word Content) task was positively correlated with the performance in a wide variety of downstream tasks. However, in our evaluation, the p-mean [35] approach, which has achieved better results in the WC task did not exceed other techniques such as ELMo <cite>[33]</cite> bag-of-words or InferSent [10] and USE in the downstream classification tasks. We believe that the high performance of the p-mean [35] in the WC task is due to the concatenative approach employed to aggregate the different power means. For completeness, we also provide an evaluation using Logistic Regression instead of a MLP in Table  10 of the appendix.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_25",
  "x": "As we can see in Table 8 , ELMo <cite>[33]</cite> was one of the methods that were able to achieve high performance on a broad set of different tasks. Interestingly, in the BShift (bi-gram shift) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, ELMo <cite>[33]</cite> achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as \"This is my Eve Christmas\", a sample from the BShift dataset. In [11] , they found that the binned sentence length task (SentLen) was negatively correlated with the performance in downstream tasks. This hypothesis was also supported by the model learning dynamics, since it seems that as model starts to capture deeper linguistic properties, it will tend to forget about this superficial feature [11] . However, the <cite>[33]</cite> bag-of-words not only achieved the best result in the SentLent task but also in many downstream tasks. Our hypothesis is that this is due to the fact that ELMo <cite>[33]</cite> is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging SOMO task. ELMo <cite>[33]</cite> word embeddings can be seen as analogous to the hypercolumns [15] approach in Computer Vision, where multiple feature levels are aggregated to form a single pixelwise representation.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_26",
  "x": "Interestingly, in the BShift (bi-gram shift) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, ELMo <cite>[33]</cite> achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as \"This is my Eve Christmas\", a sample from the BShift dataset. In [11] , they found that the binned sentence length task (SentLen) was negatively correlated with the performance in downstream tasks. This hypothesis was also supported by the model learning dynamics, since it seems that as model starts to capture deeper linguistic properties, it will tend to forget about this superficial feature [11] . However, the <cite>[33]</cite> bag-of-words not only achieved the best result in the SentLent task but also in many downstream tasks. Our hypothesis is that this is due to the fact that ELMo <cite>[33]</cite> is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging SOMO task. ELMo <cite>[33]</cite> word embeddings can be seen as analogous to the hypercolumns [15] approach in Computer Vision, where multiple feature levels are aggregated to form a single pixelwise representation. We leave the exploration of probing tasks for each ELMo <cite>[33]</cite> layer representation to future research, given that it could provide a framework to expose the linguistic properties capture by each representation level of the LSTM.",
  "y": "future_work"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_27",
  "x": "However, the <cite>[33]</cite> bag-of-words not only achieved the best result in the SentLent task but also in many downstream tasks. Our hypothesis is that this is due to the fact that ELMo <cite>[33]</cite> is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging SOMO task. ELMo <cite>[33]</cite> word embeddings can be seen as analogous to the hypercolumns [15] approach in Computer Vision, where multiple feature levels are aggregated to form a single pixelwise representation. We leave the exploration of probing tasks for each ELMo <cite>[33]</cite> layer representation to future research, given that it could provide a framework to expose the linguistic properties capture by each representation level of the LSTM. In [11] , they also found that the WC (Word Content) task was positively correlated with the performance in a wide variety of downstream tasks. However, in our evaluation, the p-mean [35] approach, which has achieved better results in the WC task did not exceed other techniques such as ELMo <cite>[33]</cite> bag-of-words or InferSent [10] and USE in the downstream classification tasks. We believe that the high performance of the p-mean [35] in the WC task is due to the concatenative approach employed to aggregate the different power means.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_28",
  "x": "Best viewed in color. ---------------------------------- **DISCUSSION** We provided a comprehensive evaluation of the inductive transfer as well as an exploration of the linguistic properties of multiple sentence embedding techniques that included bag-of-word baselines, as well as encoder architectures trained with supervised or self-supervised approaches. We showed that a bag-of-words approach using a recently introduced context-dependent word embedding technique was able to achieve excellent performance on many downstream tasks as well as capturing important linguistic properties. We demonstrated the importance of the linguistic probing tasks as a means for exploration of sentence embeddings. Especially for evaluating different levels of word representations, where it can be a very useful tool to provide insights on what kind of relationships and linguistic properties each representation level (in the case of deep representations such as ELMo <cite>[33]</cite> ) is capturing.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_29",
  "x": "We also showed that no method had a consistent performance across all tasks, with performance being linked mostly with the downstream task similarity to the trained task of these techniques. Given that we are still far from a universal sentence encoder, we believe that this evaluation can provide an important basis for choosing which technique can potentially perform well in particular tasks. Finally, we believe that new embedding training techniques that include language models as a way to capture context and meaning, such as ELMo <cite>[33]</cite> , combined with clever techniques of encoding sentences such as in InferSent [10] , can improve the performance of these encoders by a significant margin. However, as we saw in the experiments, the performance of these encoders trained on particular datasets such as entailment did not perform well on a broad set of downstream tasks. Therefore, one hypothesis is that these encoders are too narrow at modeling what these embeddings can carry. We believe that the research direction of incorporating language models and multiple levels of representations can help to provide a wide set of rich features that can capture context-dependent semantics as well as linguistic features, such as seen on ELMo <cite>[33]</cite> downstream and linguistic probing task experiments, but for sentence embeddings. ----------------------------------",
  "y": "future_work"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_30",
  "x": "Given that we are still far from a universal sentence encoder, we believe that this evaluation can provide an important basis for choosing which technique can potentially perform well in particular tasks. Finally, we believe that new embedding training techniques that include language models as a way to capture context and meaning, such as ELMo <cite>[33]</cite> , combined with clever techniques of encoding sentences such as in InferSent [10] , can improve the performance of these encoders by a significant margin. However, as we saw in the experiments, the performance of these encoders trained on particular datasets such as entailment did not perform well on a broad set of downstream tasks. Therefore, one hypothesis is that these encoders are too narrow at modeling what these embeddings can carry. We believe that the research direction of incorporating language models and multiple levels of representations can help to provide a wide set of rich features that can capture context-dependent semantics as well as linguistic features, such as seen on ELMo <cite>[33]</cite> downstream and linguistic probing task experiments, but for sentence embeddings. ---------------------------------- **A.1 SUPPLEMENTAL RESULTS**",
  "y": "future_work differences"
 },
 {
  "id": "b4093db328fd6839777a6d34507b34_0",
  "x": "**INTRODUCTION** Readers fixate more and longer on open syntactic categories (verbs, nouns, adjectives) than on closed class items like prepositions and conjunctions (Rayner and Duffy, 1988; Nilsson and Nivre, 2009) . Recently,<cite> Barrett and S\u00f8gaard (2015)</cite> presented evidence that gaze features can be used to discriminate between most pairs of parts of speech (POS) . Their study uses all the coarse-grained POS labels proposed by Petrov et al. (2011) . This paper investigates to what extent gaze data can also be used to predict grammatical functions such as subjects and objects. We first show that a simple logistic regression classifier trained on a very small seed of data using gaze features discriminates between some pairs of grammatical functions. We show that the same kind of classifier distinguishes well between the four main grammatical functions of nouns, POBJ, DOBJ, NN and NSUBJ.",
  "y": "background"
 },
 {
  "id": "b4093db328fd6839777a6d34507b34_1",
  "x": "We show a practical use for data reflecting human cognitive processing. Finally, we use gaze features to improve a transition-based dependency parser, comparing also to dependency parsers augmented with word embeddings. ---------------------------------- **EYE TRACKING DATA** The data comes from <cite>(Barrett and S\u00f8gaard, 2015)</cite> and is publicly available 1 . In this experiment 10 native English speakers read 250 syntactically annotated sentences in English (min. 3 tokens, max.",
  "y": "uses"
 },
 {
  "id": "b4093db328fd6839777a6d34507b34_2",
  "x": "Table 1 shows the 15 most used features in ranked order with their proportion of all votes. The features predictive of grammatical functions are similar to the features that were found to be predictive of POS <cite>(Barrett and S\u00f8gaard, 2015)</cite> , however, the probability that a word gets first and second fixation were not important features for POS classification, whereas they are contributing to dependency classification. This could suggest that words with certain grammatical functions are consistently more likely or less likely to get first and second fixation, but could also be due to a frequent syntactic order in the sample. Binary discrimination Error reduction over the baseline can be seen in Figure 2 . The mean accuracy using logistic regression on all binary classification problems between grammatical functions is 0.722. The frequency-position-word length baseline is 0.706. In other words, using gaze features leads to a 5.6% error reduction over the baseline.",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_0",
  "x": "We also compare our model with an end-toend tree-based LSTM model (SPTree) by <cite>Miwa and Bansal (2016)</cite> and show that our model performs within 1% on entity mentions and 2% on relations. Our finegrained analysis also shows that our model performs significantly better on AGENT-ARTIFACT relations, while SPTree performs better on PHYSICAL and PART-WHOLE relations. ---------------------------------- **INTRODUCTION** Extraction of entities and their relations from text belongs to a very well-studied family of structured prediction tasks in NLP. There are several NLP tasks such as fine-grained opinion mining (Choi et al., 2006) , semantic role labeling (Gildea and Jurafsky, 2002) , etc., which have a similar structure; thus making it an important and a challenging task. Several methods have been proposed for entity mention and relation extraction at the sentencelevel.",
  "y": "similarities uses"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_1",
  "x": "These can be broadly categorized into -1) pipeline models that treat the identification of entity mentions (Nadeau and Sekine, 2007) and relation classification (Zhou et al., 2005) as two separate tasks; and 2) joint models, also the more recent, which simultaneously identify the entity mention and relations (Li and Ji, 2014; Miwa and Sasaki, 2014) . Joint models have been argued to perform better than the pipeline models as knowledge of the typed relation can increase the confidence of the model on entity extraction and vice versa. Recurrent networks (RNNs) (Elman, 1990 ) have recently become very popular for sequence tagging tasks such as entity extraction that involves a set of contiguous tokens. However, their ability to identify relations between non-adjacent tokens in a sequence, e.g., the head nouns of two entities, is less explored. For these tasks, RNNs that make use of tree structures have been deemed more suitable. <cite>Miwa and Bansal (2016)</cite> , for example, propose an RNN comprised of a sequencebased long short term memory (LSTM) for entity identification and a separate tree-based dependency LSTM layer for relation classification using shared parameters between the two components. As a result, their model depends critically on access to dependency trees, restricting it to sentencelevel extraction and to languages for which (good) dependency parsers exist.",
  "y": "background"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_2",
  "x": "Unlike other models, our model does not depend on any dependency tree information. Our RNN-based model is a multi-layer bidirectional LSTM over a sequence. We encode the output sequence from left-to-right. At each time step, we use an attention-like model on the previously decoded time steps, to identify the tokens in a specified relation with the current token. We also add an additional layer to our network to encode the output sequence from right-to-left and find significant improvement on the performance of relation identification using bi-directional encoding. Our model significantly outperforms the feature-based structured perceptron model of Li and Ji (2014) , showing improvements on both entity and relation extraction on the ACE05 dataset. In comparison to the dependency treebased LSTM model of <cite>Miwa and Bansal (2016)</cite> , our model performs within 1% on entities and 2% on relations on ACE05 dataset.",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_3",
  "x": "These models assume conditional independence at the output layer whereas the model we propose in this paper does not assume any conditional independence at the output layer, allowing it to model an arbitrary distribution over output sequences. Relation classification has been widely studied as a stand-alone task, assuming that the arguments of the relations are known in advance. There have been several models proposed including featurebased models (Bunescu and Mooney, 2005; Zelenko et al., 2003) and neural network based models (Socher et al., 2012; dos Santos et al., 2015; Hashimoto et al., 2015; Xu et al., 2015a,b) . For joint-extraction of entities and relations, feature-based structured prediction models (Li and Ji, 2014; Miwa and Sasaki, 2014) , joint inference integer linear programming models (Yih and Roth, 2007; Yang and Cardie, 2013) , card-pyramid parsing (Kate and Mooney, 2010) and probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013) have been proposed. In contrast, we propose a neural network model which does not depend on the availability of any features such as part of speech (POS) tags, dependency trees, etc. Recently, <cite>Miwa and Bansal (2016)</cite> proposed an end-to-end LSTM based sequence and treestructured model. They extract entities via a sequence layer and relations between the entities via the shortest path dependency tree network.",
  "y": "background"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_4",
  "x": "There have been several models proposed including featurebased models (Bunescu and Mooney, 2005; Zelenko et al., 2003) and neural network based models (Socher et al., 2012; dos Santos et al., 2015; Hashimoto et al., 2015; Xu et al., 2015a,b) . For joint-extraction of entities and relations, feature-based structured prediction models (Li and Ji, 2014; Miwa and Sasaki, 2014) , joint inference integer linear programming models (Yih and Roth, 2007; Yang and Cardie, 2013) , card-pyramid parsing (Kate and Mooney, 2010) and probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013) have been proposed. In contrast, we propose a neural network model which does not depend on the availability of any features such as part of speech (POS) tags, dependency trees, etc. Recently, <cite>Miwa and Bansal (2016)</cite> proposed an end-to-end LSTM based sequence and treestructured model. They extract entities via a sequence layer and relations between the entities via the shortest path dependency tree network. In this paper, we try to investigate recurrent neural networks with attention for extracting semantic relations between entity mentions without using any dependency parse tree features. We also present the first neural network based joint model that can extract entity mentions and relations along with the relation type.",
  "y": "motivation"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_5",
  "x": "We formulate entity detection as a sequence labeling task using BILOU scheme similar to Li and Ji (2014) and <cite>Miwa and Bansal (2016)</cite> . We assign each token in the entity with the tag B appended with the entity type if it is the beginning of the entity, I for inside of an entity, L for the end of the entity or U if there is only one token in the entity. Figure 1 shows an example of the entity tag sequence assigned to the sentence. For each token in the sequence, we perform a softmax over all candidate tags to output the most likely tag: Our network structure as shown in Figure 2 also contains connections from the output y t\u22121 of the previous time step to the current top hidden layer. Thus our outputs are not conditionally independent from each other. In order to add connections from y t\u22121 , we transform this output k into a label embedding b k t\u22121 1 .",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_6",
  "x": "At inference time, we can greedily decode the sequence to find the most likely entity E and relation R tag sequences: Since, we add another top layer to encode tag sequences in the reverse order as explained in Section 3.5, there may be conflicts in the output. We select the positive and more confident label similar to <cite>Miwa and Bansal (2016)</cite> . Multiple Relations Our approach to relation extraction is different from <cite>Miwa and Bansal (2016)</cite> . <cite>Miwa and Bansal (2016)</cite> present each pair of entities to their model for relation classification. In our approach, we use pointer networks to identify the related entities. Thus, for our approach described so far if we only compute the argmax on our objective then we limit our model to output only one relation label per token.",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_7",
  "x": "However, at test time when the gold label is not available we use the predicted label at previous time step as input to the current step. At inference time, we can greedily decode the sequence to find the most likely entity E and relation R tag sequences: Since, we add another top layer to encode tag sequences in the reverse order as explained in Section 3.5, there may be conflicts in the output. We select the positive and more confident label similar to <cite>Miwa and Bansal (2016)</cite> . Multiple Relations Our approach to relation extraction is different from <cite>Miwa and Bansal (2016)</cite> . <cite>Miwa and Bansal (2016)</cite> present each pair of entities to their model for relation classification. In our approach, we use pointer networks to identify the related entities.",
  "y": "differences"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_8",
  "x": "For the scope of this paper, we only use the entity head phrase similar to Li and Ji (2014) and <cite>Miwa and Bansal (2016)</cite> . Also, there are relation types namely Physical (PHYS), Person-Social (PER-SOC), Organization-Affiliation (ORG-AFF), Agent-Artifact (ART), GPE-Affiliation (GPE-AFF). ACE05 has a total of 6 relation types including PART-WHOLE. We use the same data splits as Li and Ji (2014) and <cite>Miwa and Bansal (2016)</cite> such that there are 351 documents for training, 80 for development and the remaining 80 documents for the test set. ACE04 has 7 relation types with an additional Discourse (DISC) type and split ORG-AFF relation type into ORG-AFF and OTHER-AFF. We perform 5-fold cross validation similar to Chan and Roth (2011) for fair comparison with the state-of-theart. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_9",
  "x": "Also, there are relation types namely Physical (PHYS), Person-Social (PER-SOC), Organization-Affiliation (ORG-AFF), Agent-Artifact (ART), GPE-Affiliation (GPE-AFF). ACE05 has a total of 6 relation types including PART-WHOLE. We use the same data splits as Li and Ji (2014) and <cite>Miwa and Bansal (2016)</cite> such that there are 351 documents for training, 80 for development and the remaining 80 documents for the test set. ACE04 has 7 relation types with an additional Discourse (DISC) type and split ORG-AFF relation type into ORG-AFF and OTHER-AFF. We perform 5-fold cross validation similar to Chan and Roth (2011) for fair comparison with the state-of-theart. ---------------------------------- **EVALUATION METRICS**",
  "y": "uses"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_10",
  "x": "ACE04 has 7 relation types with an additional Discourse (DISC) type and split ORG-AFF relation type into ORG-AFF and OTHER-AFF. We perform 5-fold cross validation similar to Chan and Roth (2011) for fair comparison with the state-of-theart. ---------------------------------- **EVALUATION METRICS** In order to compare our system with the previous systems, we report micro F1-scores, Precision and Recall on both entities and relations similar to Li and Ji (2014) and <cite>Miwa and Bansal (2016)</cite> . An entity is considered correct if we can identify its head and the entity type correctly. A relation is considered correct if we can identify the head of the argument entities and also the relation type.",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_12",
  "x": "We train our model using Adadelta (Zeiler, 2012) with gradient clipping. We regularize our network using dropout (Srivastava et al., 2014) with the drop-out rate tuned using development set. We initialized our word embeddings Table 1 : Performance on ACE05 test dataset. The dashed (\"-\") performance numbers were missing in the original paper<cite> (Miwa and Bansal, 2016)</cite> . 1 We ran the system made publicly available by <cite>Miwa and Bansal (2016)</cite> , on ACE05 dataset for filling in the missing values and comparing our system with theirs at fine-grained level. Table 2 : Performance of different encoding methods on ACE05 dataset. with 300-dimensional word2vec (Mikolov et al., 2013) word embeddings trained on Google News dataset.",
  "y": "uses"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_13",
  "x": "**ERROR ANALYSIS** In this section, we perform a fine-grained comparison of our model with respect to the SPTree<cite> (Miwa and Bansal, 2016)</cite> model. We compare the performance of the two models with respect to entities, relation types and the distance between the relation arguments and provide examples from the test set in Table 6 . Table 3 : Performance on ACE04 test dataset. The dashed (\"-\") performance numbers were missing in the original paper<cite> (Miwa and Bansal, 2016)</cite> . ---------------------------------- **ENTITIES**",
  "y": "uses"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_15",
  "x": "**ENTITIES** We find that our model has lower recall on entity extraction than SPTree as shown in Table 1 . <cite>Miwa and Bansal (2016)</cite> , in one of the ablation tests on ACE05 development set, show that their model can gain upto 2% improvement in recall by entity pretraining. Since we propose a jointmodel, we cannot directly apply their pretraining trick on entities separately. We leave it for future work. Li and Ji (2014) mentioned in their analysis of the dataset that there were many \"UNK\" tokens in the test set which were never seen during training. We verified the same and we hypothesize that for this reason the performance on the entities depends largely on the pretrained word embeddings being used.",
  "y": "background"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_16",
  "x": "The dashed (\"-\") performance numbers were missing in the original paper<cite> (Miwa and Bansal, 2016)</cite> . ---------------------------------- **ENTITIES** We find that our model has lower recall on entity extraction than SPTree as shown in Table 1 . <cite>Miwa and Bansal (2016)</cite> , in one of the ablation tests on ACE05 development set, show that their model can gain upto 2% improvement in recall by entity pretraining. Since we propose a jointmodel, we cannot directly apply their pretraining trick on entities separately. We leave it for future work.",
  "y": "future_work"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_17",
  "x": "Since we propose a jointmodel, we cannot directly apply their pretraining trick on entities separately. We leave it for future work. Li and Ji (2014) mentioned in their analysis of the dataset that there were many \"UNK\" tokens in the test set which were never seen during training. We verified the same and we hypothesize that for this reason the performance on the entities depends largely on the pretrained word embeddings being used. We found considerable improvements on entity recall when using pretrained word embeddings, if available, for these \"UNK\" tokens. <cite>Miwa and Bansal (2016)</cite> also use additional features such as POS tags in addition to pretrained word embeddings at the input layer. Table 5 : Performance based on the distance between entity arguments in relations for ACE05 test dataset.",
  "y": "background"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_18",
  "x": "Experimentally, we found that our model significantly outperforms feature-rich structured perceptron joint model by Li and Ji (2014) . We also compare our model to an endto-end LSTM model by <cite>Miwa and Bansal (2016)</cite> which comprises of a sequence layer for entity extraction and a tree-based dependency layer for relation classification. We find that our model, without access to dependency trees, POS tags, etc performs within 1% on entities and 2% on relations on ACE05 dataset. We also find that our model performs significantly better than their treebased model on the ART relation, while their treebased model performs better on PHYS and PART-WHOLE relations; the two models perform comparably on all other relation types. In future, we plan to explore pretraining methods for our model which were shown to improve recall on entity and relation performance by <cite>Miwa and Bansal (2016)</cite> . We introduce bi-directional output encoding as well as an objective to learn multiple relations in this paper. However, this presents the challenge of combining predictions from the two directions.",
  "y": "uses"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_19",
  "x": "Experimentally, we found that our model significantly outperforms feature-rich structured perceptron joint model by Li and Ji (2014) . We also compare our model to an endto-end LSTM model by <cite>Miwa and Bansal (2016)</cite> which comprises of a sequence layer for entity extraction and a tree-based dependency layer for relation classification. We find that our model, without access to dependency trees, POS tags, etc performs within 1% on entities and 2% on relations on ACE05 dataset. We also find that our model performs significantly better than their treebased model on the ART relation, while their treebased model performs better on PHYS and PART-WHOLE relations; the two models perform comparably on all other relation types. In future, we plan to explore pretraining methods for our model which were shown to improve recall on entity and relation performance by <cite>Miwa and Bansal (2016)</cite> . We introduce bi-directional output encoding as well as an objective to learn multiple relations in this paper. However, this presents the challenge of combining predictions from the two directions.",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_20",
  "x": "In future, we plan to explore pretraining methods for our model which were shown to improve recall on entity and relation performance by <cite>Miwa and Bansal (2016)</cite> . We introduce bi-directional output encoding as well as an objective to learn multiple relations in this paper. However, this presents the challenge of combining predictions from the two directions. We use heuristics in this paper to combine the predictions. We think that using probabilistic methods to combine model predictions from both directions may further improve the performance. We also plan to use Sparsemax (Martins and Astudillo, 2016) instead of Softmax for multiple relations, as the former is more suitable for multi-label classification for sparse labels. It would also be interesting to see the effect of reranking (Collins and Koo, 2005 ) on our joint model.",
  "y": "future_work"
 },
 {
  "id": "b722b98f50669bf3b22208a25f6854_0",
  "x": "Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) <cite>[5]</cite> . In our semantic tests, on average 85% of the quality can be obtained by training on a random \u223c 4% subset of the original corpus (e.g. as in Fig. 1 , 5 random million lines yield 64.14% instead of 75.14%). Our claims are that i) such evaluation posteriors are Normally distributed (Tab. I), and that ii) the variance is inversely proportional to the subset size (Tab. II). It is therefore possible to select the best random subset for a given size, if an information criterion is known. Such metric is currently under investigation. Within the robotics domain, in order to reduce computational complexity of the training phase, cardinality reduction of human-written instructions is particularly important for non-recursive online training algorithms, such as current symbol-based probabilistic reasoning systems [1] , [3] , [6] . ----------------------------------",
  "y": "background"
 },
 {
  "id": "b722b98f50669bf3b22208a25f6854_1",
  "x": "We prove the concept on human-written texts, and conjecture this work will reduce training data size of sequential instructions, while preserving semantic relations, when gathering information from large remote sources [3] . We computed multiple random subsets of sentences from the UMBC WEBBASE CORPUS (\u223c 17.13GB) via a custom implementation using the SPARK distributed framework. We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC [4]), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) <cite>[5]</cite> . In our semantic tests, on average 85% of the quality can be obtained by training on a random \u223c 4% subset of the original corpus (e.g. as in Fig. 1 , 5 random million lines yield 64.14% instead of 75.14%). Our claims are that i) such evaluation posteriors are Normally distributed (Tab. I), and that ii) the variance is inversely proportional to the subset size (Tab. II). It is therefore possible to select the best random subset for a given size, if an information criterion is known.",
  "y": "extends"
 },
 {
  "id": "b722b98f50669bf3b22208a25f6854_2",
  "x": "**POSTERIOR EVALUATION DISTRIBUTION OF SUBSETS** We computed multiple random subsets of sentences from the UMBC WEBBASE CORPUS (\u223c 17.13GB) via a custom implementation using the SPARK distributed framework. We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC [4] ), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) <cite>[5]</cite> . In our semantic tests, on average 85% of the quality can be obtained by training on a random \u223c 4% subset of the original corpus (e.g. as in Fig. 1 , 5 random million lines yield 64.14% instead of 75.14%). Our claims are that i) such evaluation posteriors are Normally distributed (Tab. I), and that ii) the variance is inversely proportional to the subset size (Tab. II). It is therefore possible to select the best random subset for a given size, if an information criterion is known.",
  "y": "background"
 },
 {
  "id": "b722b98f50669bf3b22208a25f6854_3",
  "x": "We computed multiple random subsets of sentences from the UMBC WEBBASE CORPUS (\u223c 17.13GB) via a custom implementation using the SPARK distributed framework. We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC [4] ), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) <cite>[5]</cite> . In our semantic tests, on average 85% of the quality can be obtained by training on a random \u223c 4% subset of the original corpus (e.g. as in Fig. 1 , 5 random million lines yield 64.14% instead of 75.14%). Our claims are that i) such evaluation posteriors are Normally distributed (Tab. I), and that ii) the variance is inversely proportional to the subset size (Tab. II). It is therefore possible to select the best random subset for a given size, if an information criterion is known. Such metric is currently under investigation.",
  "y": "extends"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_0",
  "x": "(2) EAGER also extracts categories from DBPE-DIA abstracts using dependency analysis. Finally, EAGER extracts plural forms and synonyms from redirect information. (3) For entity recognition, we integrate the gazetteer with a simple, but effective machine learning classifier, and experimentally show that the extended gazetteers improve the F 1 score between 7% and 12% over our baseline approach and outperform <cite>(Zhang and Iria, 2009 )</cite> on all learned concepts (subject, location, temporal). ---------------------------------- **RELATED WORK** We divide the related work in automatic gazetteer population into three groups: (1) Machine learning approaches (2) Pattern driven approaches Finally, like our own work, (3) knowledge driven approaches Knowledge Driven. In any case, machine learning and pattern driven approaches extract their terms from unstructured sources -despite the fact that large, general knowledge bases became available in the last years.",
  "y": "differences"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_1",
  "x": "The approach presented in (Kazama and Torisawa, 2007; Kazama and Torisawa, 2008 ) relies solely on WIKIPEDIA, producing gazetteers without explicitly named concepts, arguing that consistent but anonymous labels are still useful. Most closely related to our own work, the authors of <cite>(Zhang and Iria, 2009 )</cite> build an approach solely on WIKIPEDIA which does not only exploit the article text but also analyzes the structural elements of WIKIPEDIA: 3 Automatically Extending Gazetteer Lists 3.1 Extraction Algorithm: Overview Algorithm 1 shows an outline of the gazetteer expansion algorithm used in EAGER. To extend an initial seed set S EAGER proceeds, roughly, in three steps: First, it identifies DBPEDIA articles for seed entities and extracts implicit category and synonym information from abstracts and redirect information (Lines 1-11). Second, it finds additional categories from the DBPEDIA category hierarchy . Finally, it uses the categories from the first two steps to extract additional entities (Lines 21-24). In the following, we consider the three steps separately.",
  "y": "similarities"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_2",
  "x": "Here, we observe the first advantage of DBPEDIA's more structured information: DB-PEDIA already contains plain text labels such as \"Barack Obama\" and we can directly query (using the SPARQL endpoint) all articles with a label equal (or starting with) an entity in our seed set. This allows for more precise article matching and avoids complex URL encodings as necessary in previous, add all labels of a to G ; WIKIPEDIA-based approaches such as (Kazama and Torisawa, 2007) . As <cite>(Zhang and Iria, 2009 )</cite>, we reject redirection entries in this step as ambiguous. With the articles identified, we can proceed to extract category information from the abstracts and new entities from the redirect information. In the dependency analysis of article abstracts (Lines 6-9), we aim to extract category (or, more generally, hypernym) information from the abstracts of articles on the ssed list. We perform a standard dependency analysis on the sentences of the abstract and return all nouns that stand in nsubj relation to a seed entity or (directly or indirectly) in conj (correlative conjunction) relation to a noun that stands in nsubj relation to a seed entity. This allows us to extract, e.g., both \"general\" and \"statesman\" as categories from a sentence such as \"Julius Caesar was a Roman general and statesman\".",
  "y": "similarities"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_3",
  "x": "We perform a standard dependency analysis on the sentences of the abstract and return all nouns that stand in nsubj relation to a seed entity or (directly or indirectly) in conj (correlative conjunction) relation to a noun that stands in nsubj relation to a seed entity. This allows us to extract, e.g., both \"general\" and \"statesman\" as categories from a sentence such as \"Julius Caesar was a Roman general and statesman\". This analysis is inspired by <cite>(Zhang and Iria, 2009 )</cite>, but performed on the entire abstract which is clearly dis- <cite>(Zhang and Iria, 2009)</cite> , where this is applied only to the first sentence (as WIKIPEDIA does not directly provide a concept of \"abstract\"). All categories thus obtained are added to P and will be used in Section 3.4 to generate additional entities. Finally, we are interested in redirection information (Lines 10-11) about an article for a seed entity as that provides such with synonyms, plural forms, different spellings, etc. Fortunately, DB-PEDIA provides this information directly by means of the dbpedia-owl:wikiPageRedirects property. The labels of all redirect articles with this property pointing to a seed entity articles are directly added to the Gazetteer.",
  "y": "differences motivation"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_4",
  "x": "The labels of all redirect articles with this property pointing to a seed entity articles are directly added to the Gazetteer. ---------------------------------- **EXPLICIT: CATEGORY GRAPH** In addition to categories from the abstract analysis, we also use the category graph of DBPEDIA. It has been previously observed, <cite>(Zhang and Iria, 2009 )</cite> and (Strube and Ponzetto, 2006) , that the category graph of poor quality. DBPEDIA improves little on that fact. However, EAGER uses a sophisticated analysis of categories related to seed entities that allows us to prune most of the noise in the category graph.",
  "y": "background"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_5",
  "x": "In addition to categories from the abstract analysis, we also use the category graph of DBPEDIA. It has been previously observed, <cite>(Zhang and Iria, 2009 )</cite> and (Strube and Ponzetto, 2006) , that the category graph of poor quality. DBPEDIA improves little on that fact. However, EAGER uses a sophisticated analysis of categories related to seed entities that allows us to prune most of the noise in the category graph. Biased towards precision over recall, Section 4 shows that combined with the category extraction from abstracts it provides a significantly extended Gazetteer without introducing substantial noise. The fundamental contribution of EAGER is a category pruning based on finding a connected component in the graph of related categories that is supported by as many different entities from the seed list as possible. Figure 1 illustrates this further: From the articles for the seed entities, we compute (Line 12) the direct categories (via subject edges) and associate them to their seed entities e via Cats(e).",
  "y": "similarities"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_6",
  "x": "An article is called sufficiently unambiguous, if it is categorised only with categories from P up to a threshold \u03b8 (here, set to 5) of non-P categories. This avoids adding very general entities that tend to have large number of categories in WIKIPEDIA and thus DBPEDIA. The output of Algorithm 3 is the extended gazetteers G . ---------------------------------- **EVALUATION** To evaluate the impact of EAGER on entity recognition, we performed a large set of experiments (on the archeology domain). The experiment domains and <cite>(Zhang and Iria, 2009 )</cite>, which we outperform for all entity types, in some cases up to 5% in F 1 score.",
  "y": "differences"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_7",
  "x": "In this experiment, we consider entity recognition in the domain of archaeology. As part of this effort, (Jeffrey et al., 2009) identified three types of entities that are most useful for archaeological research; Subject(SUB), Temporal Terms(TEM), Location (LOC). In this evaluation, we use the same setup as in <cite>(Zhang and Iria, 2009 )</cite>: A corpus of 30 full length UK archaeological reports archived by the Arts and Humanities Data Service (AHDS). The length of the documents varies from 4 to 120 pages. The corpus is inter-annotated by three archaeologists. ---------------------------------- **RESULT**",
  "y": "uses"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_8",
  "x": "For the latter, we consider full EAGER as described in Section 3 as well as only the entities derived from dependency analysis of abstracts, from the category graph, and from redirection information. Finally, we also include the performance numbers report in <cite>(Zhang and Iria, 2009 )</cite> for comparison (since we share <cite>their</cite> evaluation settings). Table 1 show the results of the comparison: EA-GER significantly improves precision and recall over the baseline system and outperforms <cite>(Zhang and Iria, 2009 )</cite> in all cases. Furthermore, the impact of all three types of information (dependencies from abstract, category, redirection) of EAGER individually is quite notable with a slight disadvantage for category information. However, in all cases the combination of all three types as proposed in EAGER shows a significant further increase in performance. ---------------------------------- **CONCLUSION**",
  "y": "uses"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_9",
  "x": "For the latter, we consider full EAGER as described in Section 3 as well as only the entities derived from dependency analysis of abstracts, from the category graph, and from redirection information. Finally, we also include the performance numbers report in <cite>(Zhang and Iria, 2009 )</cite> for comparison (since we share <cite>their</cite> evaluation settings). Table 1 show the results of the comparison: EA-GER significantly improves precision and recall over the baseline system and outperforms <cite>(Zhang and Iria, 2009 )</cite> in all cases. Furthermore, the impact of all three types of information (dependencies from abstract, category, redirection) of EAGER individually is quite notable with a slight disadvantage for category information. However, in all cases the combination of all three types as proposed in EAGER shows a significant further increase in performance. ---------------------------------- **CONCLUSION**",
  "y": "differences"
 },
 {
  "id": "b7e0879c4cac85054870146e61aa6f_0",
  "x": "Concretely, an agent is spawned at a random location in an environment (a house or building) and asked a question (e.g. 'What color is the car?'). The agent perceives its environment through first-person egocentric vision and can perform a few atomic actions (move-forward, turn, strafe, etc.). The goal of the agent is to intelligently navigate the environment and gather visual information necessary for answering the question. Subsequent to the introduction of the task, several methods have been introduced to solve the EmbodiedQA task [5, <cite>6]</cite> , using some combination of reinforcement learning, behavior cloning and hierarchical control. Apart from using the question and images from the environment, these methods also rely on varying degrees of expert supervision such as shortest path demonstrations and subgoal policy sketches. In this work, we evaluate simple question-only baselines that never see the environment and receive no form of expert supervision. We examine whether existing methods outperform baselines designed to solely capture dataset bias, in order to better understand the performance of these existing methods.",
  "y": "background"
 },
 {
  "id": "b7e0879c4cac85054870146e61aa6f_1",
  "x": "In a later work, Das et al.<cite> [6]</cite> introduce Neural Modular Control (NMC) which is a hierarchical policy network that operates over expert sub-policy sketches. The master and sub-policies are initialized with Behavior Cloning (BC), and later fine-tuned with Asynchronous Advantage Actor-Critic (A3C) [19] . Dataset Biases and Trivial Baselines: Many recent studies in language and vision show how biases in a dataset allow models to perform well on a task without leveraging the meaning of the text or image in the underlying dataset. A simple CNN-BoW model was shown to achieve state-of-the-art results [12] on the Visual7W [2<cite>6]</cite> task while also performing surprisingly well compared to the most complex systems proposed for the VQA dataset [1] and other joint vision and language tasks [2, 10] . Simple nearest neighbor approaches have been shown to perform well on image captioning datasets [7] . This phenomenon has also been observed in language processing tasks. On the Story-cloze task which was presented to evaluate common-sense reasoning, Schwartz et al. [23] achieved state-ofthe-art performance by ignoring the narrative and training a linear classifier with features related to the writing style of the two potential endings, rather than their content.",
  "y": "background"
 },
 {
  "id": "b7e0879c4cac85054870146e61aa6f_2",
  "x": "Oracles: We now examine whether the EQAv1 dataset and the proposed oracle navigation can improve over pure text baselines, to leverage visual information in the most ideal case. We reproduce the settings for training the VQA model 2 . Specifically we train the VQA model described in<cite> [6]</cite> on the last 5 frames of oracle navigation for 50 epochs with ADAM and a learning rate of 3e \u2212 4 using batch size 20. We observe the accuracy is improved over text baselines in this unrealistic setting, but the use of this model with navigation in PACMAN reduces performance to below the text baselines. For completeness we benchmark an oracle with our BoW embedding model in place of the LSTM with all other settings kept constant. As noted in [5] , we re-iterate that these oracles are far from perfect, as they may not contain the best vantage or context to answer the question. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "b7e0879c4cac85054870146e61aa6f_3",
  "x": "**T 10** T 20 T 50 T any Navigation + VQA PACMAN (BC) [5] 48 BOW-CNN VQA-Only 56.5 Table 1 : We compare to the published results from<cite> [6]</cite> for agent spawned at various steps away from the target: 10, 30, 50, and anywhere in the environment. Question-only baselines outperform Navigation+VQA methods except when spawned 10 steps from the target object. A VQA-only system with oracle navigation can improve on a pure text baseline but isn't effective when combined with navigation. (*) indicates our reproduction of the model described in [5] Error Analysis: To better understand the shortcomings and limitations, we perform an error analysis of the one of the runs of the BoW model on different question types: Here, the color category Preposition Location Color 9.09 51.72 53.31 Table 2 : Accuracy of the BoW model on different question types subsumes color and color_room both. The particularly low accuracy on preposition questions is due to the fact that there exist very few questions of this type in the training set (2.44%), and the entropy of answer distribution in this class is much higher compared to color and location question types. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "b7e0879c4cac85054870146e61aa6f_4",
  "x": "We use the Adam optimizer (batch-size of 64) with a learning rate of 5e \u22123 which is annealed via a scheduling mechanism based on plateaus in the validation loss. The training procedure is run for 200 epochs and we use the checkpoint with minimum validation loss to compute accuracy on the test set. The NN-AnswerDist and the Majority baselines are self-descriptive and there are no specific training details that we apply. We also train the [5] text embedding model (an LSTM) with the optimization settings described in [5] for 200 epochs. Results Detailed results are reported in Table 4 . Following Das et al.<cite> [6]</cite> , we report the agent's top-1 accuracy on the test set when spawned 10, 20 and 50 steps away from the goal, denoted as T 10 , T 20 and T 50 respectively. Since the performance of blindfold baselines are not affected based on where the agent is spawned, their accuracy is same across T 10 , T 20 and T 50 .",
  "y": "uses"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_0",
  "x": "Finally, we discuss our future plans using this corpus. ---------------------------------- **INTRODUCTION** The Ubuntu Dialogue Corpus is the largest freely available multi-turn based dialog corpus <cite>[1]</cite> 1 . It was constructed from the Ubuntu chat logs 2 -a collection of logs from Ubuntu-related chat rooms on the Freenode IRC network. Although multiple users can talk at the same time in the chat room, the logs were preprocessed using heuristics to create two-person conversations. The resulting corpus consists of almost one million two-person conversations, where a user seeks help with his/her Ubuntu-related problems (the average length of a dialog is 8 turns, with a minimum of 3 turns).",
  "y": "background"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_1",
  "x": "The rest of the paper continues as follows: 1. we introduce the setup -the data as well as the evaluation of the task; 2. we briefly describe the previously evaluated models; 3. we introduce three different models (one of them being the same as in the previous work); 4. we evaluate these models and experiment with different amount of training data; 5. we conclude and discuss our plans for future works ---------------------------------- **DATA** In this section we briefly describe the data and evaluation metrics used in <cite>[1]</cite> . First, all the collected data was preprocessed by replacing named entities with corresponding tags (name, location, organization, url, path). This is analogical to the prepossessing of [2] (note that the IT helpdesk dataset used there is not publicly available). Second, these data are further processed to create tuples of (context, response, f lag).",
  "y": "background"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_2",
  "x": "The baselines were reported with (n, k) of (2, 1), (10, 1), (10, 2) and (10, 5). ---------------------------------- **APPROACHES** This task can naturally be formulated as a ranking problem which is often tackled by three techniques [3] : (i) pointwise; (ii) pairwise and (iii) listwise ranking. While pairwise and listwise ranking approaches are empirically superior to the pointwise ranking approach, our preliminary experiments use pointwise ranking approach for its simplicity. Note that pointwise method was also used in the original baselines <cite>[1]</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_3",
  "x": "**PREVIOUS WORK** The pointwise architectures reported in <cite>[1]</cite> included (i) TF-IDF, (ii) RNN and (iii) LSTM. In this section, we briefly describe these models. A neural network is used to compute the embedding for the context and the response, denoted as c and r. These are fed through a sigmoid function to compute the pairwise probability. ---------------------------------- **TF-IDF** The motivation here is that the correct response tends to share more words with the context than the incorrect ones.",
  "y": "background"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_4",
  "x": "To match the original setup of <cite>[1]</cite> we use the same training data 3 . We use one million training examples and we use the same word vectors pre-trained by GloVe [9] . All our models were implemented using Theano [10] and Blocks [11] . For training we use ADAM learning rule [12] and binary negative log-likelihood as training objective. We stop the training once Recall@1 starts increasing on a validation set. The experiments were executed on Nvidia K40 GPUs. The best meta-parameters were found by simple grid search.",
  "y": "similarities uses"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_5",
  "x": "---------------------------------- **RESULTS** Baselines from <cite>[1]</cite> Our Table 1 : Results of our experiments compared to the results reported in <cite>[1]</cite> . Meta-parameters of our architectures are the following: our CNN had 400 filters of length 1, 100 filters of length 2 and 100 filters of length 3; our LSTM had 200 hidden units and our bidirectional LSTM had 250 hidden units in each network. For CNNs and LSTMs, the best results were achieved with batch size 256. For Bi-LSTM, the best batch size was 128. Turn User Text 1 A: anyone know why \" aptitude update \" returns a non-successful status (255) ?",
  "y": "uses background"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_6",
  "x": "Therefore, the prediction could be better interpreted by a human. Additional accuracy improvements might be also achieved by different text pre-processing pipelines. For instance, in the current dataset all named entities were replaced with generic tags, which could possibly harm the performance. ---------------------------------- **CONCLUSION** In this work we achieved a new state-of-the-art results on the next utterance ranking problem recently introduced in <cite>[1]</cite> . The best performing system is an ensemble of multiple diverse neural networks.",
  "y": "extends differences"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_7",
  "x": "We believe that this is mostly due to the max operation performed on top of the feature maps. Thanks to the simplicity of this operation, the model does not over-fit the data and generalizes better when learned on small training datasets. On the other hand, the simplicity of the operation does not allow the model to properly handle more complicated dependencies (such as the order in which the n-grams occur in the text), thus recurrent models perform better given enough data; (ii) the recurrent models have not made its peak yet, suggesting that adding more training data would improve the model's accuracy. This agrees with Figure 3 of the previous evaluation <cite>[1]</cite> . Figure 3 : Training data size ranging from 100, 000 to the full 1, 000, 000 examples (X axis) and the resulting Recall@1 (Y axis). The CNN has 500, 100 and 100 filters of length 1, 2 and 3. The LSTM and Bi-LSTM has both 300 hidden units in each recurrent layer.",
  "y": "background"
 },
 {
  "id": "bb45a61408a0ade8ce0aba2b8f9ce7_0",
  "x": "This tutorial will provide an overview of the growing number of multimodal tasks and datasets that combine textual and visual understanding. We will comprehensively review existing stateof-the-art approaches to selected tasks such as image captioning (Chen et al., 2015) , visual question answering (VQA) (Antol et al., 2015; Goyal et al., 2017) and visual dialog (Das et al., 2017a,b) , presenting the key architectural building blocks (such as co-attention (Lu et al., 2016) ) and novel algorithms (such as cooperative/adversarial games (Das et al., 2017b) ) used to train models for these tasks. We will then discuss some of the current and upcoming challenges of combining language, vision and actions, and introduce some recently-released interactive 3D simulation environments designed for this purpose (Anderson et al., 2018b; <cite>Das et al., 2018)</cite> . The goal of this tutorial is to provide a comprehensive yet accessible overview of existing work and to reduce the entry barrier for new researchers. In detail, we will first review the building blocks of the neural network architectures used for these tasks, starting from variants of recurrent sequenceto-sequence language models (Ilya Sutskever, 2014), applied to image captioning (Vinyals et al., 2015) , optionally with visual attentional mechanisms (Bahdanau et al., 2015; Xu et al., 2015; You et al., 2016; Anderson et al., 2018a ). We will then look at evaluation metrics for image captioning Anderson et al., 2016) , before reviewing how these metrics can be optimized directly using reinforcement learning (RL) (Williams, 1992; Rennie et al., 2017) . Next, on the topic of visual question answering, we will look at more sophisticated multimodal attention mechanisms, wherein the network simultaneously attends to visual and textual features (Fukui et al., 2016; Lu et al., 2016) .",
  "y": "background"
 },
 {
  "id": "bb45a61408a0ade8ce0aba2b8f9ce7_1",
  "x": "Next, on the topic of visual question answering, we will look at more sophisticated multimodal attention mechanisms, wherein the network simultaneously attends to visual and textual features (Fukui et al., 2016; Lu et al., 2016) . We will see how to combine factual and commonsense reasoning from learnt memory representations (Sukhbaatar et al., 2015) and external knowledge bases , and approaches that use the question to dynamically compose the answering neural network from specialized modules (Andreas et al., 2016a,b; Johnson et al., 2017a,b; Hu et al., 2017) . Following the success of adversarial learning in visual recognition (Goodfellow et al., 2014) , it has recently been gaining momentum in language modeling (Yu et al., 2016) and in multimodal tasks such as captioning (Dai et al., 2017) and dialog (Wu et al., 2018a) . Within visual dia-log, we will look at recent work that uses cooperative multi-agent tasks as a proxy for training effective visual conversational models via RL (Kottur et al., 2017; Das et al., 2017b) . Finally, as a move away from static datasets, we will cover recent work on building active RL environments for language-vision tasks. Although models that link vision, language and actions have a rich history (Tellex et al., 2011; Paul et al., 2016; Misra et al., 2017) , we will focus primarily on embodied 3D environments (Anderson et al., 2018b; , considering tasks such as visual navigation from natural language instructions (Anderson et al., 2018b) , and question answering<cite> (Das et al., 2018</cite>; Gordon et al., 2018) . We will position this work in context of related simulators that also offer significant potential for grounded language learning (Beattie et al., 2016; Zhu et al., 2017) .",
  "y": "background"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_0",
  "x": "Recurrent neural networks (RNNs) yield high-quality results in many applications but often are memory-and time-consuming due to a large number of parameters. A popular approach for RNN compression is sparsification (setting a lot of weights to zero), it may compress RNN orders of times with only a slight quality drop or even with quality improvement due to the regularization effect [11] . Sparsification of the RNN is usually performed either at the level of individual weights (unstructured sparsification) [13, 11,<cite> 1]</cite> or at the level of neurons [14] (structured sparsification -removing weights by groups corresponding to neurons). The latter additionally accelerates the testing stage. However, most of the modern recurrent architectures (e. g. LSTM [3] or GRU [2] ) have a gated structure. We propose to add an intermediate level of sparsification between individual weights <cite>[1]</cite> and neurons [14] -gates (see fig. 1 , left). Precisely, we remove weights by groups corresponding to gates, which makes some gates constant, independent of the inputs, and equal to the activation function of the bias.",
  "y": "background"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_1",
  "x": "However, most of the modern recurrent architectures (e. g. LSTM [3] or GRU [2] ) have a gated structure. We propose to add an intermediate level of sparsification between individual weights <cite>[1]</cite> and neurons [14] -gates (see fig. 1 , left). Precisely, we remove weights by groups corresponding to gates, which makes some gates constant, independent of the inputs, and equal to the activation function of the bias. As a result, the LSTM/GRU structure is simplified. With this intermediate level introduced, we obtain a three-level sparsification hierarchy: sparsification of individual weights helps to sparsify gates (make them constant), and sparsification of gates helps to sparsify neurons (remove them from the model). The described idea can be implemented for any gated architecture in any sparsification framework. We implement the idea for LSTM in two frameworks: pruning [14] and Bayesian sparsification <cite>[1]</cite> and observe that resulting gate structures (which gates are constant and which are not) vary for different NLP tasks.",
  "y": "extends uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_2",
  "x": "Precisely, we remove weights by groups corresponding to gates, which makes some gates constant, independent of the inputs, and equal to the activation function of the bias. As a result, the LSTM/GRU structure is simplified. With this intermediate level introduced, we obtain a three-level sparsification hierarchy: sparsification of individual weights helps to sparsify gates (make them constant), and sparsification of gates helps to sparsify neurons (remove them from the model). The described idea can be implemented for any gated architecture in any sparsification framework. We implement the idea for LSTM in two frameworks: pruning [14] and Bayesian sparsification <cite>[1]</cite> and observe that resulting gate structures (which gates are constant and which are not) vary for different NLP tasks. We analyze these gate structures and connect them to the specifics of the particular tasks. The proposed method also improves neuron-wise compression of the RNN in most cases.",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_3",
  "x": "In contrast to our approach, they do not sparsify gates, and remove a neuron if all its ingoing and outgoing connections are set to zero. Bayesian sparsification. We rely on Sparse Variational Dropout [10,<cite> 1]</cite> to sparsify individual weights. Following [5] , for each neuron, we introduce a group weight which is multiplied by the output of this neuron in the computational graph (setting to zero this group weight entails removing the neuron). To sparsify gates, for each gate we introduce a separate group weight which is multiplied by the preactivation of the gate before adding a bias (setting to zero this group weight makes the gate constant). <cite>[1]</cite> with additional group weights for neurons [5] . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_5",
  "x": "<cite>[1]</cite> with additional group weights for neurons [5] . ---------------------------------- **EXPERIMENTS** In the pruning framework, we perform experiments on word-level language modeling (LM) on a PTB dataset [7] following ISS [14] . We use a standard model of Zaremba et al. [16] of two sizes (small and large) with an embedding layer, two LSTM layers, and a fully-connected output layer (Emb + 2 LSTM + FC). Here regularization is applied only to LSTM layers following [14] , and its strength is selected using grid search so that qualities of ISS and our model are approximately equal. In the Bayesian framework, we perform an evaluation on the text classification (datasets IMDb [6] and AGNews [17]) and language modeling (dataset PTB, character and word level tasks) following <cite>[1]</cite> .",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_6",
  "x": "Here we regularize and sparsify all layers following <cite>[1]</cite> . Sizes of LSTM layers may be found in tab. 1. Embedding layers have 300/200/1500 neurons for classification tasks/small/large word level LM. More experimental details are given in Appendix C. ---------------------------------- **QUANTITATIVE RESULTS** We compare our three-level sparsification approach (W+G+N) with the original dense model and a two-level sparsification (weights and neurons, W+N) in tab.",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_7",
  "x": "Our approach relies on Sparse variational dropout [10] (SparseVD). This model treats the weights of the neural network as random variables and comprises a log-uniform prior over the weights: p(|w ij |) \u221d 1 |wij | and a fully factorized normal approximate posterior over the weights: q(w ij ) = N (w ij |m ij , \u03c3 2 ij ). Biases are treated as deterministic parameters. To find the parameters of the approximate posterior distribution and biases, the evidence lower bound (ELBO) is optimized: Because of the log-uniform prior, for the majority of weights, the signal-to-noise ratio m 2 ij /\u03c3 2 ij \u2192 0 and these weights do not affect the network's output. In <cite>[1]</cite> , SparseVD is adapted to the RNNs. Training our model.",
  "y": "background"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_8",
  "x": "Training our model. We work with the group weights z in the same way as with the weights W : we approximate the posterior with the fully factorized normal distribution given the fully factorized log-uniform prior distribution. To estimate the expectation in (1), we sample weights from the approximate posterior distribution in the same way as in <cite>[1]</cite> . With the integral estimated with one Monte-Carlo sample, the first term in (1) becomes the usual loss function (for example, cross-entropy in language modeling). The second term is a regularizer depending on the parameters \u00b5 and \u03c3 (for the exact formula, see [10] ). After learning, we zero out all the weights and the group weights with the signal-to-noise ratio less than 0.05. At the testing stage, we use the mean values of all the weights and the group weights.",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_9",
  "x": "We use the same threshold 1e \u2212 4 as in the small models. Bayesian sparsification. In all the Bayesian models, we sparsify the weight matrices of all layers. Since in text classification tasks, usually only a small number of input words are important, we use additional multiplicative weights to sparsify the input vocabulary following Chirkova et al. <cite>[1]</cite> . For the networks with the embedding layer, in configurations W+N and W+G+N, we also sparsify the embedding components (by introducing group weights z x multiplied by x t .) We train our networks using Adam [4] . Baseline networks overfit for all our tasks, therefore, we present results for them with early stopping.",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_10",
  "x": "Bayesian sparsification. In all the Bayesian models, we sparsify the weight matrices of all layers. Since in text classification tasks, usually only a small number of input words are important, we use additional multiplicative weights to sparsify the input vocabulary following Chirkova et al. <cite>[1]</cite> . For the networks with the embedding layer, in configurations W+N and W+G+N, we also sparsify the embedding components (by introducing group weights z x multiplied by x t .) We train our networks using Adam [4] . Baseline networks overfit for all our tasks, therefore, we present results for them with early stopping. Models for the text classification and the character-level LM are trained in the same setting as in <cite>[1]</cite> (we used the code provided by the authors).",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_12",
  "x": "In the last columns, the numbers of the remaining hidden neurons and non-constant gates in the LSTM layers are reported. ---------------------------------- **D EXPERIMENTS WITH UNSTRUCTURED BAYESIAN SPARSIFICATION** In this section, we present experimental results for the unstructured Bayesian sparsification (configuration Bayes W). This configuration corresponds to a model of Chirkova et al. <cite>[1]</cite> . Table 2 shows quantitative results, and figure 5 shows the resulting gate structures for the IMDB classification task and the second LSTM layer of the word-level language modeling task. Since Bayes W model does not comprise any group weights, the overall compression of the RNN is lower than for Bayes W+G+N (tab. 1), so there are more non-constant gates.",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_13",
  "x": "In the last columns, the numbers of the remaining hidden neurons and non-constant gates in the LSTM layers are reported. ---------------------------------- **D EXPERIMENTS WITH UNSTRUCTURED BAYESIAN SPARSIFICATION** In this section, we present experimental results for the unstructured Bayesian sparsification (configuration Bayes W). This configuration corresponds to a model of Chirkova et al. <cite>[1]</cite> . Table 2 shows quantitative results, and figure 5 shows the resulting gate structures for the IMDB classification task and the second LSTM layer of the word-level language modeling task. Since Bayes W model does not comprise any group weights, the overall compression of the RNN is lower than for Bayes W+G+N (tab. 1), so there are more non-constant gates.",
  "y": "background uses"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_0",
  "x": "To achieve the goal of \"strong AI\", we need to change our learning goal to really understand the sentiment of words. Which means that the algorithm should know how each word influences the sentiment of a document in different tasks. If we can achieve this learning goal, the algorithms are able to solve new tasks without teaching. Zhiyuan Chen and etc. <cite>[2]</cite> ever proposed a approach to close the goal. They made a big progress but the supervised learning still is needed. Guangyi Lv and etc.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_1",
  "x": "They made a big progress but the supervised learning still is needed. Guangyi Lv and etc. [4] extend the work of <cite>[2]</cite> with a neural network based approach. However, the supervised learning still is necessary under their setting and huge volume of labeled data are required. Hence, this paper aims to decrease the usage of labeled data while maintain the performance. ---------------------------------- **LIFELONG MACHINE LEARNING**",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_2",
  "x": "Comparing with the multi-task learning [1] , ELLA is much more efficient. Zhiyuan and etc. <cite>[2]</cite> improved the sentiment classification by involving knowledge. The object function was modified with two penalty terms which corresponding with previous tasks. The knowledge system contains the following components: ---------------------------------- **COMPONENTS OF LML**",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_3",
  "x": "---------------------------------- **COMPONENTS OF LML** \u2022 Knowledge Base (KB): The knowledge Base <cite>[2]</cite> mainly used to maintain the previous knowledge. Based on the type of knowledge, it could be divided as Past Information Store (PIS), Meta-Knowledge Miner (MKM) and Meta-Knowledge Store (MKS). \u2022 Knowledge Reasoner (KR): The knowledge reasoner is designed to generate new knowledge upon the archived knowledge by logic inference. A strict logic design is required so the most of the LML algorithms lack of the component. \u2022 Knowledge-Base Learner (KBL): The Knowledge-Based Learner <cite>[2]</cite> aims to retrieve and transfer previous knowledge to the current task.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_4",
  "x": "Based on the type of knowledge, it could be divided as Past Information Store (PIS), Meta-Knowledge Miner (MKM) and Meta-Knowledge Store (MKS). \u2022 Knowledge Reasoner (KR): The knowledge reasoner is designed to generate new knowledge upon the archived knowledge by logic inference. A strict logic design is required so the most of the LML algorithms lack of the component. \u2022 Knowledge-Base Learner (KBL): The Knowledge-Based Learner <cite>[2]</cite> aims to retrieve and transfer previous knowledge to the current task. Hence, it contains two parts: task knowledge miner and leaner. The miner seeks and determines which knowledge could be reused, and the learner transfers such knowledge to the current task. ----------------------------------",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_5",
  "x": "Hence, it contains two parts: task knowledge miner and leaner. The miner seeks and determines which knowledge could be reused, and the learner transfers such knowledge to the current task. ---------------------------------- **SENTIMENT CLASSIFICATION** Hong and etc. [3] had discussed that the NLP field is most suitable for the lifelong machine learning researches due to its knowledge is easy to extract and to be understood by human. Previous classical paper <cite>[2]</cite> chose the sentiment classification as the learning target because it could be regarded as a large task as well as a group of related sub-tasks in the different domains. Although these sub-tasks are related to each other but a model only trained on a single subtasks is unable to perform well in the rest sub-tasks.",
  "y": "motivation"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_6",
  "x": "Hong and etc. [3] had discussed that the NLP field is most suitable for the lifelong machine learning researches due to its knowledge is easy to extract and to be understood by human. Previous classical paper <cite>[2]</cite> chose the sentiment classification as the learning target because it could be regarded as a large task as well as a group of related sub-tasks in the different domains. Although these sub-tasks are related to each other but a model only trained on a single subtasks is unable to perform well in the rest sub-tasks. This requires the algorithms could know when the knowledge can be used and when can not due to the distribution of each sub-tasks is different. Known these, an algorithm can be called as \"lifelong\" because it is able to transfer previous knowledge to new tasks to improve performance. Although deep learning already is applied in sentiment classification, it still could not leverage past knowledge well. This because the complexity of neural network limits the researches to define and extract knowledge from the data.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_7",
  "x": "As the previous work <cite>[2]</cite> , this paper also uses Na\u00efve Bayes as the knowledge can be presented by the probability. In this way, we need to know the probability of each word that shows in the positive or negative content. We also need to know well that some words may only have sentiment polarity in some specific domains(equal to tasks in this paper). \"Lifelong Sentiment Classification\" (\"LSC\" for simple below) <cite>[2]</cite> records that which domain does a word have the sentiment orientation. If a word always has sentiment polarity or has significant polarity in current domain, a higher weight will sign to it more than other words. This approach contains a knowledge transfer operation and a knowledge validation operation. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_8",
  "x": "Although deep learning already is applied in sentiment classification, it still could not leverage past knowledge well. This because the complexity of neural network limits the researches to define and extract knowledge from the data. As the previous work <cite>[2]</cite> , this paper also uses Na\u00efve Bayes as the knowledge can be presented by the probability. In this way, we need to know the probability of each word that shows in the positive or negative content. We also need to know well that some words may only have sentiment polarity in some specific domains(equal to tasks in this paper). \"Lifelong Sentiment Classification\" (\"LSC\" for simple below) <cite>[2]</cite> records that which domain does a word have the sentiment orientation. If a word always has sentiment polarity or has significant polarity in current domain, a higher weight will sign to it more than other words.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_9",
  "x": "This approach contains a knowledge transfer operation and a knowledge validation operation. ---------------------------------- **CONTRIBUTION OF THIS PAPER** Although LSC <cite>[2]</cite> already raised a lifelong approach, it only aims to improve the classification accuracy. It still is under the setting of the supervised learning and also is unable to deliver an explicit knowledge to guild further learning. Based on the LSC, this paper advances the lifelong learning in sentiment classification and have two main contributions: \u2022 A improved lifelong learning paradigm is proposed to solve the sentiment classification problem under unsupervised learning setting with previous knowledge.",
  "y": "motivation"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_10",
  "x": "---------------------------------- **CONTRIBUTION OF THIS PAPER** Although LSC <cite>[2]</cite> already raised a lifelong approach, it only aims to improve the classification accuracy. It still is under the setting of the supervised learning and also is unable to deliver an explicit knowledge to guild further learning. Based on the LSC, this paper advances the lifelong learning in sentiment classification and have two main contributions: \u2022 A improved lifelong learning paradigm is proposed to solve the sentiment classification problem under unsupervised learning setting with previous knowledge. \u2022 We introduce a novel approach to discover and store the words with sentiment polarity for reuse.",
  "y": "extends"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_11",
  "x": "**SENTIMENT POLARITY WORDS 4.1 NA\u00cfVE BAYESIAN TEXT CLASSIFICATION** In this paper, we define a word has sentiment polarity by calculating the probability that it appears in a positive or negative content (sentence or document). If a word has a high probability with sentiment polarity, it also will leads to the document have higher probability of sentiment probability based on the Na\u00efve Bayesian (NB) formula. Hence, to determine the words with polarity is the key to predict the sentiment. Na\u00efve Bayesian (NB) classifier [5] calculates the probability of each word w in a document d and then to predict the sentiment polarity (positive or negative). We use the same formula below as in the LSC <cite>[2]</cite> . P(w |c j ) is the probability of a word appears in a class:",
  "y": "uses"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_12",
  "x": [
   "As we known, not all words have sentimental polarity like \"a\", \"one\" and etc. while some words always have polarity like \"good\", \"hate\", \"excellent\" and so on. In addition, some words only have sentiment polarity in specific domains. For example, \"tough\" in reviews of the diamond indicates that the diamond have a good quality while it means hard to chew in the domain of food. Hence, in order to achieve the goal of the lifelong learning. We need to find the words always have sentiment polarity and be careful for those words only shows polarity in specific domains. ----------------------------------"
  ],
  "y": "motivation"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_13",
  "x": "We need to find the words always have sentiment polarity and be careful for those words only shows polarity in specific domains. ---------------------------------- **LIFELONG SEMI-SUPERVISED LEARNING FOR SENTIMENT CLASSIFICATION** Although LSC <cite>[2]</cite> considered the difference among domains, it still is a typical supervised learning approach. In this paper, we proposed to learn as two stages: (1) Initial Learning Stage: to explore a basic set of sentiment words. After that, the model should be able to basically classify a new domain with a good performance.",
  "y": "motivation"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_14",
  "x": "**EXPERIMENT 6.1 DATASETS** In the experiment, we use the same datasets as LSC <cite>[2]</cite> used. It contains the reviews from 20 domains crawled from the Amazon.com and each domain has 1,000 reviews (the distribution of positive and negative reviews is imbalanced). ---------------------------------- **WORD POLARITY ANALYSIS** To answer the first question for the initial learning stage, we need to know which words exactly influence the sentiment classification. Firstly, we calculate P(w |+) and P(w |\u2212) for each words.",
  "y": "uses"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_0",
  "x": "**INTRODUCTION** Lexical simplification is the task of automatically rewriting a text by substituting words or phrases with simpler variants, while retaining its meaning and grammaticality. The goal is to make the text easier to understand for children, language learners, people with cognitive disabilities and even machines. Approaches to lexical simplification generally follow a standard pipeline consisting of two main steps: generation and ranking. In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (Devlin and Tait, 1998) , learning substitution rules from sentence-aligned parallel corpora of complex-simple texts<cite> (Horn et al., 2014</cite>; Paetzold and Specia, 2017) , and learning word embeddings from a large corpora to obtain similar words of the complex word (Glava\u0161 and\u0160tajner, 2015; Kim et al., 2016; Specia, 2016a, 2017) . In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness. * This research was conducted while the first author was a Post Doctoral Fellow at the City University of Hong Kong.",
  "y": "background"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_1",
  "x": "In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (Devlin and Tait, 1998) , learning substitution rules from sentence-aligned parallel corpora of complex-simple texts<cite> (Horn et al., 2014</cite>; Paetzold and Specia, 2017) , and learning word embeddings from a large corpora to obtain similar words of the complex word (Glava\u0161 and\u0160tajner, 2015; Kim et al., 2016; Specia, 2016a, 2017) . In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness. * This research was conducted while the first author was a Post Doctoral Fellow at the City University of Hong Kong. The ranking step is challenging because the substitution candidates usually have similar meaning to the target word, and thus share similar context features. State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model<cite> (Horn et al., 2014</cite>; Bingel and S\u00f8gaard, 2016; Specia, 2016a, 2017) . Moreover, deep architectures are not explored in these models. Surface features and shallow architectures might not be able to explore the features at different levels of abstractions that capture nuances that discriminate the candidates.",
  "y": "background"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_2",
  "x": "* This research was conducted while the first author was a Post Doctoral Fellow at the City University of Hong Kong. The ranking step is challenging because the substitution candidates usually have similar meaning to the target word, and thus share similar context features. State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model<cite> (Horn et al., 2014</cite>; Bingel and S\u00f8gaard, 2016; Specia, 2016a, 2017) . Moreover, deep architectures are not explored in these models. Surface features and shallow architectures might not be able to explore the features at different levels of abstractions that capture nuances that discriminate the candidates. In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to rank substitution candidates. The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sentences.",
  "y": "motivation"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_3",
  "x": "For each target word and a substitution candidate word we also compute: cosine similarity, word length, and alignment probability in the sentence-aligned Normal-Simple Wikipedia corpus (Kauchak, 2013) . The cosine similarity feature is computed using the SubIMDB corpus. ---------------------------------- **IMPLEMENTATION DETAILS AND TRAINING PROCEDURE OF THE DSSM** Following previous works that used supervised machine learning for ranking in lexical simplification<cite> (Horn et al., 2014</cite>; Paetzold and Specia, 2017) , we train the DSSM using the LexMTurk dataset<cite> (Horn et al., 2014)</cite> , which contains 500 instances composed of a sentence, a target word and substitution candidates ranked by simplicity (Paetzold and Specia, 2017) . In order to learn the parameters W t and W s (Figure 1 ) of the DSSM, we use the standard backpropagation algorithm (Rumelhart et al., 1988) . The objective used in this paper follows the pair-wise learning-to-rank paradigm outlined in (Burges et al., 2005) .",
  "y": "uses"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_4",
  "x": "---------------------------------- **DATASETS AND EVALUATION METRICS** To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (Paetzold and Specia, 2016b) , which contains 929 instances, and NNSEval (Paetzold and Specia, 2016a) , which contains 239 instances. Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity (Paetzold and Specia, 2017) . Since both datasets contain instances from the LexMturk dataset<cite> (Horn et al., 2014)</cite> , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 . We finally obtain 429 remaining instances in the BenchLS dataset, and 78 instances in the NNEval dataset, which are used in our evaluation. We adopt the same evaluation metrics featured in Glava\u0161 and\u0160tajner (2015) and<cite> Horn et al. (2014)</cite> : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system.",
  "y": "uses"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_5",
  "x": "To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (Paetzold and Specia, 2016b) , which contains 929 instances, and NNSEval (Paetzold and Specia, 2016a) , which contains 239 instances. Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity (Paetzold and Specia, 2017) . Since both datasets contain instances from the LexMturk dataset<cite> (Horn et al., 2014)</cite> , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 . We finally obtain 429 remaining instances in the BenchLS dataset, and 78 instances in the NNEval dataset, which are used in our evaluation. We adopt the same evaluation metrics featured in Glava\u0161 and\u0160tajner (2015) and<cite> Horn et al. (2014)</cite> : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system. ---------------------------------- **BASELINES**",
  "y": "uses"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_6",
  "x": "All values marked in bold are significantly higher compared to the best baseline, SVM rank , measured by t-test at p-value of 0.05. with default parameters) for ranking substitution candidates, similar to the method described in<cite> (Horn et al., 2014)</cite> . All the three models employ the n-gram probability features extracted from the SubIMDB corpus (Paetzold and Specia, 2015) , as described in (Paetzold and Specia, 2017) , and are trained using the LexMTurk dataset. ---------------------------------- **RESULTS** The top part of table 1 (Substitution Candidates Ranking) summarizes the results of all three systems. Overall, both SVM rank and DSSM Ranking outperform the NSR Baseline.",
  "y": "similarities"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_0",
  "x": "**ABSTRACT** While paragraph embedding models are remarkably effective for downstream classification tasks, what they learn and encode into a single vector remains opaque. In this paper, we investigate a state-of-the-art paragraph embedding method proposed by<cite> Zhang et al. (2017)</cite> and discover that it cannot reliably tell whether a given sentence occurs in the input paragraph or not. We formulate a sentence content task to probe for this basic linguistic property and find that even a much simpler bag-of-words method has no trouble solving it. This result motivates us to replace the reconstructionbased objective of<cite> Zhang et al. (2017)</cite> with our sentence content probe objective in a semisupervised setting. Despite its simplicity, our objective improves over paragraph reconstruction in terms of (1) downstream classification accuracies on benchmark datasets, (2) faster training, and (3) better generalization ability. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_1",
  "x": "---------------------------------- **1** ---------------------------------- **INTRODUCTION** Methods that embed a paragraph into a single vector have been successfully integrated into many NLP applications, including text classification<cite> (Zhang et al., 2017)</cite> , document retrieval (Le and Mikolov, 2014) , and semantic similarity and relatedness (Dai et al., 2015; Chen, 2017) . However, downstream performance provides little insight into the kinds of linguistic properties that are encoded by these embeddings. Inspired by the growing body of work on sentence-level linguistic probe tasks (Adi et al., 2017; Conneau et al., 2018) , we set out to evaluate a state-of-the-art paragraph embedding method using a probe task to measure how well it encodes the identity of the sentences within a paragraph.",
  "y": "background"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_2",
  "x": "Methods that embed a paragraph into a single vector have been successfully integrated into many NLP applications, including text classification<cite> (Zhang et al., 2017)</cite> , document retrieval (Le and Mikolov, 2014) , and semantic similarity and relatedness (Dai et al., 2015; Chen, 2017) . However, downstream performance provides little insight into the kinds of linguistic properties that are encoded by these embeddings. Inspired by the growing body of work on sentence-level linguistic probe tasks (Adi et al., 2017; Conneau et al., 2018) , we set out to evaluate a state-of-the-art paragraph embedding method using a probe task to measure how well it encodes the identity of the sentences within a paragraph. We discover that the method falls short of capturing this basic property, and that implementing a simple objective to fix this issue improves classification performance, training speed, and generalization ability. We specifically investigate the paragraph embedding method of<cite> Zhang et al. (2017)</cite> , which consists of a CNN-based encoder-decoder model paired with a reconstruction objective to learn powerful paragraph embeddings that are capable of accurately reconstructing long paragraphs. This model significantly improves downstream classification accuracies, outperforming LSTM-based alternatives (Li et al., 2015) . How well do these embeddings encode whether or not a given sentence appears in the paragraph?",
  "y": "uses"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_4",
  "x": "Sentence content significantly improves over reconstruction on standard benchmark datasets both with and without fine-tuning; additionally, this objective is four times faster to train than the reconstruction-based variant. Furthermore, pre-training with sentence content substantially boosts generalization ability: fine-tuning a pre-trained model on just 500 labeled reviews from the Yelp sentiment dataset surpasses the accuracy of a purely supervised model trained on 100,000 labeled reviews. Our results indicate that incorporating probe objectives into downstream models might help improve both accuracy and efficiency, which we hope will spur more linguistically-informed research into paragraph embedding methods. ---------------------------------- **PROBING PARAGRAPH EMBEDDINGS FOR SENTENCE CONTENT** In this section, we first fully specify our probe task before comparing the model of<cite> Zhang et al. (2017)</cite> to a simple bag-of-words model. Somewhat surprisingly, the latter substantially outperforms the former despite its relative simplicity.",
  "y": "differences"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_5",
  "x": "4 Bag-of-words (BoW): This model is simply an average of the word vectors learned by a trained CNN-R model. BoW models have been shown to be surprisingly good at sentence-level probe tasks (Adi et al., 2017; Conneau et al., 2018) . ---------------------------------- **PROBE EXPERIMENTAL DETAILS** Paragraphs to train our classifiers are extracted from the Hotel Reviews corpus (Li et al., 2015) , which has previously been used for evaluating the quality of paragraph embeddings (Li et al., 2015; <cite>Zhang et al., 2017)</cite> . We only consider paragraphs that have at least two sentences.",
  "y": "background"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_6",
  "x": "To examine the dataset effect, we repeat our experiments using paragraph embeddings pre-trained using these objectives on a subset of Wikipedia (560K paragraphs). The second row of Table 3 shows that both approaches suffer a drop in downstream accuracy when pre-trained on out-of-domain data. Interestingly, CNN-SC still performs best, indicating that sentence content is more suitable for downstream classification. Another advantage of our sentence content objective over reconstruction is that it better correlates to downstream accuracy (see Appendix A.2). For reconstruction, there is no apparent correlation between BLEU and downstream accuracy; while BLEU increases with the number of epochs, the downstream performance quickly begins to decrease. This result indicates that early stopping based on BLEU is not feasible with reconstruction-based pre-training objectives. With fine-tuning, CNN-SC substantially boosts accuracy and generalization We switch gears Table 4 : CNN-SC outperforms other baseline models that do not use external data, including CNN-R. All baseline models are taken from<cite> Zhang et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_8",
  "x": "Text embeddings and probe tasks A variety of methods exist for obtaining fixed-length dense vector representations of words (e.g., Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018) , sentences (e.g., Kiros et al., 2015; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018) , and larger bodies of text (e.g., Le and Mikolov, 2014; Dai et al., 2015; Iyyer et al., 2015; Li et al., 2015; Chen, 2017; <cite>Zhang et al., 2017)</cite> To analyze word and sentence embeddings, recent work has studied classification tasks that probe them for various linguistic properties (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017a,b; Conneau et al., 2018; Tenney et al., 2019) . In this paper, we extend the notion of probe tasks to the paragraph level. Transfer learning Another line of related work is transfer learning, which has been the driver of recent successes in NLP. Recently-proposed objectives for transfer learning include surrounding sentence prediction (Kiros et al., 2015) , paraphrasing (Wieting and Gimpel, 2017) , entailment (Conneau et al., 2017) , machine translation (McCann et al., 2017) , discourse (Jernite et al., 2017; Nie et al., 2017) , and language modeling (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018) . ---------------------------------- **CONCLUSIONS AND FUTURE WORK** In this paper, we evaluate a state-of-the-art paragraph embedding model, based on how well it captures the sentence identity within a paragraph.",
  "y": "background"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_9",
  "x": "****ENCOURAGING PARAGRAPH EMBEDDINGS TO REMEMBER SENTENCE IDENTITY IMPROVES CLASSIFICATION**** **ABSTRACT** While paragraph embedding models are remarkably effective for downstream classification tasks, what they learn and encode into a single vector remains opaque. In this paper, we investigate a state-of-the-art paragraph embedding method proposed by<cite> Zhang et al. (2017)</cite> and discover that it cannot reliably tell whether a given sentence occurs in the input paragraph or not. We formulate a sentence content task to probe for this basic linguistic property and find that even a much simpler bag-of-words method has no trouble solving it. This result motivates us to replace the reconstructionbased objective of<cite> Zhang et al. (2017)</cite> with our sentence content probe objective in a semisupervised setting. Despite its simplicity, our objective improves over paragraph reconstruction in terms of (1) downstream classification accuracies on benchmark datasets, (2) faster training, and (3) better generalization ability.",
  "y": "motivation"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_0",
  "x": "3. complicated: they design sophisticated features, including those from parse-trees. We argue for the first time that discourse parsing should be viewed as an extension of, and be performed in conjunction with, constituency parsing. We propose the first joint syntacto-discourse treebank, by unifying constituency and discourse tree representations. Based on this, we propose the first end-to-end incremental parser that jointly parses at both constituency and discourse levels. Our algorithm builds up on the span-based parser <cite>(Cross and Huang, 2016)</cite> ; it employs the strong generalization power of bi-directional LSTMs, and parses efficiently and robustly with an extremely simple span-based feature set that does not use any tree structure information. We make the following contributions: 1. We develop a combined representation of constituency and discourse trees to facilitate parsing at both levels without explicit conversion mechanism.",
  "y": "extends differences"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_1",
  "x": "Using this representation, we build and release a joint treebank based on the Penn Treebank (Marcus et al., 1993) and RST Treebank (Marcu, 2000a,b) (Section 2). 2. We propose a novel joint parser that parses at both constituency and discourse levels. Our parser performs discourse parsing in an endto-end manner, which greatly reduces the efforts required in preprocessing the text for segmentation and feature extraction, and, to our best knowledge, is the first end-to-end discourse parser in literature (Section 3). 3. Even though it simultaneously performs constituency parsing, our parser does not use any explicit syntactic feature, nor does it need any binarization of discourse trees, thanks to the powerful span-based framework of<cite> Cross and Huang (2016)</cite> (Section 3). 4. Empirically, our end-to-end parser outperforms the existing pipelined discourse parsing efforts. When the gold EDUs are provided, our parser is also competitive to other existing approaches with sophisticated features (Section 4). ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_2",
  "x": "**JOINT SYNTACTO-DISCOURSE PARSING** Given the combined syntacto-discourse treebank, we now propose a joint parser that can perform end-to-end discourse segmentation and parsing. ---------------------------------- **EXTENDING SPAN-BASED PARSING** As mentioned above, the input sequences are substantially longer than PTB parsing, so we choose linear-time parsing, by adapting a popular greedy constituency parser, the span-based constituency parser of<cite> Cross and Huang (2016)</cite> . As in span-based parsing, at each step, we maintain a a stack of spans. Notice that in conventional incremental parsing, the stack stores the subtrees Similar to span-based constituency parsing, we alternate between structural (either shift or combine) and label (label X or nolabel) actions in an odd-even fashion.",
  "y": "similarities uses"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_3",
  "x": "Given the combined syntacto-discourse treebank, we now propose a joint parser that can perform end-to-end discourse segmentation and parsing. ---------------------------------- **EXTENDING SPAN-BASED PARSING** As mentioned above, the input sequences are substantially longer than PTB parsing, so we choose linear-time parsing, by adapting a popular greedy constituency parser, the span-based constituency parser of<cite> Cross and Huang (2016)</cite> . As in span-based parsing, at each step, we maintain a a stack of spans. Notice that in conventional incremental parsing, the stack stores the subtrees Similar to span-based constituency parsing, we alternate between structural (either shift or combine) and label (label X or nolabel) actions in an odd-even fashion. But different from<cite> Cross and Huang (2016)</cite> , after a structural action, we choose to keep the last branching point k, i.e., i Some text and the symbol or scaled k j (mostly for combine, but also trivially for shift).",
  "y": "extends differences"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_4",
  "x": "As in span-based parsing, at each step, we maintain a a stack of spans. Notice that in conventional incremental parsing, the stack stores the subtrees Similar to span-based constituency parsing, we alternate between structural (either shift or combine) and label (label X or nolabel) actions in an odd-even fashion. But different from<cite> Cross and Huang (2016)</cite> , after a structural action, we choose to keep the last branching point k, i.e., i Some text and the symbol or scaled k j (mostly for combine, but also trivially for shift). This is because in our parsing mechanism, the discourse relation between two EDUs is actually determined after the previous combine action. We need to keep the splitting point to clearly find the spans of the two EDUs to determine their relations. This midpoint k disappears after a label action; therefore we can use the shape of the last span on the stack (whether it contains the split point, i.e., i xt and the symbol or scaled k j or i Some text and the symbol or scaled j ) to determine the parity of the step and thus no longer need to carry the step z in the state as in<cite> Cross and Huang (2016)</cite> . The nolabel action makes the binarization of the discourse/constituency tree unnecessary, because nolabel actually combines the top two spans on the stack \u03c3 into one span, but without annotating the new span a label.",
  "y": "differences"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_5",
  "x": "The scoring functions in the deductive system (Figure 4 ) are calculated by an underlying neural model, which is similar to the bi-directional LSTM model in<cite> Cross and Huang (2016)</cite> that evaluates based on span boundary features. Again, it is important to note that no discourse or syntactic tree structures are represented in the features. During the decoding time, a document is firstl passed into a two-layer bi-directional LSTM model, then the outputs at each text position of the two layers of the bi-directional LSTMs are concatenated as the positional features. The spans at each parsing step can be represented as the feature vectors at the boundaries. The span features are then passed into fully connected networks with softmax to calculate the likelihood of performing the corresponding action or marking the corresponding label. We use the \"training with exploration\" strategy (Goldberg and Nivre, 2013) and the dynamic oracle mechanism described in<cite> Cross and Huang (2016)</cite> to make sure the model can handle unseen parsing configurations properly. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_6",
  "x": "The span features are then passed into fully connected networks with softmax to calculate the likelihood of performing the corresponding action or marking the corresponding label. We use the \"training with exploration\" strategy (Goldberg and Nivre, 2013) and the dynamic oracle mechanism described in<cite> Cross and Huang (2016)</cite> to make sure the model can handle unseen parsing configurations properly. ---------------------------------- **EMPIRICAL RESULTS** We use the treebank described in Section 2 for empirical evaluation. We randomly choose 30 documents from the training set as the development set. We tune the hyperparameters of the neural model on the development set.",
  "y": "similarities uses"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_7",
  "x": "We randomly choose 30 documents from the training set as the development set. We tune the hyperparameters of the neural model on the development set. For most of the hyperparameters we settle with the same values suggested by<cite> Cross and Huang (2016)</cite> . To alleviate the overfitting problem for training on the relative small RST Treebank, we use a dropout of 0.5. One particular hyperparameter is that we use a value \u03b2 to balance the chances between training following the exploration (i.e., the best action chosen by the neural model) and following the correct path provided by the dynamic oracle. We find that \u03b2 = 0.8, i.e., following the dynamic oracle with asyntactic feats. segmentation structure +nuclearity +relation Bach et al. (2012) segmentation only Table 2 : F1 scores of end-to-end systems. \"+nuclearity\" indicates scoring of tree structures with nuclearity included. \"+relation\" has both nuclearity and relation included (e.g., \u2190Elaboration).",
  "y": "similarities uses"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_8",
  "x": "icantly larger than the constituency trees in Penn Treebank, lower \u03b2 makes the parsing easily divert into wrong trails that are difficult to learn from. Since our parser essentially performs both constituency parsing task and discourse parsing task. We also evaluate the performances on sentence constituency level and discourse level separately. The result is shown in Table 1 . Note that in constituency level, the accuracy is not directly comparable with the accuracy reported in<cite> Cross and Huang (2016)</cite> , since: a) our parser is trained on a much smaller dataset (RST Treebank is about 1/6 of Penn Treebank); b) the parser is trained to optimize the discourse-level accuracy. Table 2 shows that, in the perspective of endto-end discourse parsing, our parser first outperforms the state-of-the-art segmentator of Bach et al. (2012) , and furthermore, in end-to-end parsing, the superiority of our parser is more pronounced comparing to the previously best parser of Hernault et al. (2010) . On the other hand, the majority of the conventional discourse parsers are not end-to-end: they rely on gold EDU segmentations and pre-trained tools like Stanford parsers to generate features. We perform an experiment to compare the performance of our parser with them given the gold EDU segments ( Table 3 ).",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_1",
  "x": "**ABSTRACT** Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes. However, methods for measuring and removing such biases remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection <cite>(Bolukbasi et al., 2016)</cite> is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA). Experiments with RIPA reveal that, on average, skipgram with negative sampling (SGNS) does not make most words any more gendered than they are in the training corpus.",
  "y": "uses"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_2",
  "x": "**INTRODUCTION** A common criticism of word embeddings is that they capture undesirable associations in vector space. In addition to gender-appropriate analogies such as king:queen::man:woman, stereotypical analogies such as doctor:nurse::man:woman also hold in SGNS embedding spaces <cite>(Bolukbasi et al., 2016)</cite> . Caliskan et al. (2017) created an association test for word vectors called WEAT, which uses cosine similarity to measure how associated words are with respect to two sets of attribute words (e.g., 'male' vs. 'female'). For example, they claimed that science-related words were significantly more associated with male attributes and art-related words with female ones. Since these associations are socially undesirable, they were described as gender bias. Despite these remarkable findings, such undesirable word associations remain poorly understood.",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_3",
  "x": "Since these associations are socially undesirable, they were described as gender bias. Despite these remarkable findings, such undesirable word associations remain poorly understood. For one, what causes them -is it biased training data, the embedding model itself, or just noise? Why should WEAT be the test of choice for measuring associations in word embeddings? Bolukbasi et al. (2016) found that word vectors could be debiased by defining a \"bias subspace\" in the embedding space and then subtracting from each vector its projection on this subspace. But what theoretical guarantee is there that this method actually debiases vectors? In this paper, we answer several of these open questions. We begin by proving that for any embedding model that implicitly does matrix factorization (e.g., GloVe, SGNS), debiasing vectors post hoc via subspace projection is, under certain conditions, equivalent to training on an unbiased corpus without reconstruction error. We find that contrary to what<cite> Bolukbasi et al. (2016)</cite> suggested, word embeddings should not be normalized before debiasing, as vector length can contain important information (Ethayarajh et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_4",
  "x": "This allows us to directly compare the actual word association in embedding space with what we would expect the word association to be, given the training corpus. Making such comparisons yields several novel insights: 1. SGNS does not, on average, make the vast majority of words any more gendered in the vector space than they are in the training corpus; individual words may be slightly more or less gendered due to reconstruction error. However, for words that are genderstereotyped (e.g., 'nurse') or gender-specific by definition (e.g., 'queen'), SGNS amplifies the gender association in the training corpus. 2. To use the subspace projection method, one must have prior knowledge of which words are gender-specific by definition, so that they are not also debiased. Debiasing all vectors can preclude gender-appropriate analogies such as king:queen::man:woman from holding in the embedding space. In contrast to the supervised method proposed by<cite> Bolukbasi et al. (2016)</cite> for identifying these gender-specific words, we introduce an unsupervised method.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_5",
  "x": "This subspace was defined by the first m principal components for ten gender relation vectors (e.g., man \u2212 woman). Each debiased word vector was thus orthogonal to the gender bias subspace and its projection on the subspace was zero. While this subspace projection method precluded gender-biased analogies from holding in the embedding space,<cite> Bolukbasi et al. (2016)</cite> did not provide any theoretical guarantee that the vectors were unbiased (i.e., equivalent to vectors that would be obtained from training on a gender-agnostic corpus with no reconstruction error). Other work has tried to learn gender-neutral embeddings from scratch (Zhao et al., 2018) , despite this approach requiring custom changes to the objective of each embedding model. ---------------------------------- **MEASURING ASSOCIATIONS** ----------------------------------",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_6",
  "x": "**DEBIASING EMBEDDINGS** ---------------------------------- **PROVABLY DEBIASING EMBEDDINGS** Experiments by<cite> Bolukbasi et al. (2016)</cite> found that debiasing word embeddings using the subspace projection method precludes gender-biased analogies from holding. However, as we noted earlier, despite this method being intuitive, there is no theoretical guarantee that the debiased vectors are perfectly unbiased or that the debiasing method works for embedding models other than SGNS. In this section, we prove that for any embedding model that does implicit matrix factorization (e.g., GloVe, SGNS), debiasing embeddings post hoc using the subspace projection method is, under certain conditions, equivalent to training on a perfectly unbiased corpus without reconstruction error. Definition 1 Let M denote the symmetric wordcontext matrix for a given training corpus that is implicitly or explicitly factorized by the embedding model.",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_7",
  "x": "**PROVABLY DEBIASING EMBEDDINGS** Experiments by<cite> Bolukbasi et al. (2016)</cite> found that debiasing word embeddings using the subspace projection method precludes gender-biased analogies from holding. However, as we noted earlier, despite this method being intuitive, there is no theoretical guarantee that the debiased vectors are perfectly unbiased or that the debiasing method works for embedding models other than SGNS. In this section, we prove that for any embedding model that does implicit matrix factorization (e.g., GloVe, SGNS), debiasing embeddings post hoc using the subspace projection method is, under certain conditions, equivalent to training on a perfectly unbiased corpus without reconstruction error. Definition 1 Let M denote the symmetric wordcontext matrix for a given training corpus that is implicitly or explicitly factorized by the embedding model. Let S denote a set of word pairs. A word w is unbiased with respect to S iff \u2200 (x, y) \u2208 S, M w,x = M w,y .",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_8",
  "x": "Each debiased word vector w d is orthogonal to the bias subspace in the word embedding space, so \u2200 (x, y) \u2208 S, w d , x \u2212 y = 0. In conjunction with (2), this implies that This means that if a debiased word w is represented with vector w d instead of w, it is unbiased with respect to S by Definition 1. This implies that the co-occurrence matrix M d that is reconstructed using the debiased word matrix W d is also unbiased with respect to S. The subspace projection method is therefore far more powerful than initially stated in<cite> Bolukbasi et al. (2016)</cite> : not only can it be applied to any embedding model that implicitly does matrix factorization (e.g., GloVe, SGNS), but debiasing word vectors in this way is equivalent to training on a perfectly unbiased corpus when there is no reconstruction error. However, word vectors should not be normalized prior to debiasing, since the matrix that is factorized by the embedding model cannot necessarily be reconstructed with normalized embeddings. Unbiasedness with respect to word pairs S is also only guaranteed when the bias subspace Because we define unbiasedness with respect to a set of word pairs, we cannot make any claims about word pairs outside that set.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_9",
  "x": "Because of this, WEAT seems to be an intuitive measure. However, as shown in Propositions 1 and 2, there are two key theoretical flaws to WEAT that cause it to overestimate the degree of association and ultimately make it an inappropriate metric for word embeddings. The only other metric of note quantifies association as |cos( w, b)| c , where b is the bias subspace and c \u2208 R the \"strictness\" of the measurement <cite>(Bolukbasi et al., 2016)</cite> . For the same reason discussed in Proposition 1, this measure can also overestimate the degree of association. ---------------------------------- **RELATIONAL INNER PRODUCT ASSOCIATION** Given the theoretical flaws of WEAT, we derive a new measure of word embedding association using the subspace projection method, which can provably debias embeddings (section 3).",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_10",
  "x": "**DEFINITION 2** The relational inner product association \u03b2 ( w; b) of a word vector w \u2208 V with respect to a relation vector b \u2208 V is w, b . Where S is a non-empty set of ordered word pairs (x, y) that define the association, b is the first principal component of { x \u2212 y | (x, y) \u2208 S}. Our metric, the relational inner product association (RIPA), is simply the inner product of a relation vector describing the association and a given word vector in the same embedding space. To use the terminology in<cite> Bolukbasi et al. (2016)</cite> , RIPA is the scalar projection of a word vector onto a onedimensional bias subspace defined by the unit vector b. In their experiments,<cite> Bolukbasi et al. (2016)</cite> defined b as the first principal component for a set of gender difference vectors (e.g., man \u2212 woman). This would be the means of deriving b for RIPA as well. For the sake of interpretability, we do not define b as the span of difference vectors, as would be required if one were using b to provably debias words with respect to S (see section 3).",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_11",
  "x": "We create lists of biased and appropriate words using the<cite> Bolukbasi et al. (2016)</cite> lists of gender-biased and gender-appropriate analogies. For example, doctor:nurse::man:woman is biased, so we classify the first two words as biased. The last category, neutral, contains uniformly randomly sampled words that appear at least 10K times in the corpus and that are not in either of the other categories, and which we therefore expect to be gender-agnostic. ---------------------------------- **BREAKING DOWN GENDER ASSOCIATION** For any given word, the gender association in the training corpus is what the gender association in the embedding space would be if there were no reconstruction error. By comparing these two quantities, we can infer the change induced by the embedding model.",
  "y": "uses"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_12",
  "x": "---------------------------------- **BREAKING DOWN GENDER ASSOCIATION** For any given word, the gender association in the training corpus is what the gender association in the embedding space would be if there were no reconstruction error. By comparing these two quantities, we can infer the change induced by the embedding model. Let g(w; x, y) denote the RIPA of a word w with respect to the gender relation vector defined by word pair (x, y), let\u011d(w; x, y) denote what g(w; x, y) would be under perfect reconstruction for an SGNS embedding model, and let \u2206 g denote the change in absolute gender association from corpus to embedding space. Where S is a set of gender-defining word pairs 1 from<cite> Bolukbasi et al. (2016)</cite> and \u03bb , \u03b1 are the model-specific constants defined in section 5.1, We take the absolute value of each term because the embedding model may make a word more gendered, but in the direction opposite of what is implied in the corpus.",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_13",
  "x": "**BREAKING DOWN GENDER ASSOCIATION** For any given word, the gender association in the training corpus is what the gender association in the embedding space would be if there were no reconstruction error. By comparing these two quantities, we can infer the change induced by the embedding model. Let g(w; x, y) denote the RIPA of a word w with respect to the gender relation vector defined by word pair (x, y), let\u011d(w; x, y) denote what g(w; x, y) would be under perfect reconstruction for an SGNS embedding model, and let \u2206 g denote the change in absolute gender association from corpus to embedding space. Where S is a set of gender-defining word pairs 1 from<cite> Bolukbasi et al. (2016)</cite> and \u03bb , \u03b1 are the model-specific constants defined in section 5.1, We take the absolute value of each term because the embedding model may make a word more gendered, but in the direction opposite of what is implied in the corpus. \u03bb \u2190 1 because we expect Figure 1 : Before debiasing words using subspace projection, one needs to identify which words are genderappropriate -to avoid debiasing them.",
  "y": "uses"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_14",
  "x": "\u03bb \u2190 1 because we expect Figure 1 : Before debiasing words using subspace projection, one needs to identify which words are genderappropriate -to avoid debiasing them. The<cite> Bolukbasi et al. (2016)</cite> method of identifying these words is ineffective: it ends up precluding most gender-appropriate analogies (dotted line, left) while preserving most gender-biased analogies (dotted line, right). Our unsupervised method (dashed line) does much better in both respects. \u03bb \u2248 1 in practice (Ethayarajh et al., 2018; Mimno and Thompson, 2017) . Similarly, \u03b1 \u2190 \u22121 because it minimizes the difference between x \u2212 y and its information theoretic interpretation over the gender-defining word pairs in S, though this is an estimate and may differ from the true value of \u03b1. In Table 2 , we list the gender association in the training corpus (g(w)), the gender association in embedding space (\u011d(w)), and the absolute change (\u2206 g (w)) for each group of words. On average, the SGNS embedding model does not make gender-neutral words any more gendered than they are in the training corpus.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_15",
  "x": "As a result, \u03bb king, man > (PMI('king', 'man') \u2212 log k) for SGNS, for example. What is often treated as a useful property of word embeddings can have, with respect to gender bias, a pernicious effect. ---------------------------------- **DEBIASING WITHOUT SUPERVISION** To use the subspace projection method <cite>(Bolukbasi et al., 2016)</cite> , one must have prior knowledge of which words are gender-appropriate, so that they are not debiased. Debiasing all vectors can preclude gender-appropriate analogies such as king:queen::man:woman from holding in the embedding space. To create an exhaustive list of gender-appropriate words,<cite> Bolukbasi et al. (2016)</cite> started with a small, human-labelled set of words and then trained an SVM to predict more genderappropriate terms in the vocabulary. This bootstrapped list of gender-appropriate words was then left out during debiasing.",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_16",
  "x": "In an environment with some reconstruction error, such as low-dimensional embedding spaces, secondorder similarity permits words to be closer in embedding space than would be the case if only first-order similarity had an effect. As a result, \u03bb king, man > (PMI('king', 'man') \u2212 log k) for SGNS, for example. What is often treated as a useful property of word embeddings can have, with respect to gender bias, a pernicious effect. ---------------------------------- **DEBIASING WITHOUT SUPERVISION** To use the subspace projection method <cite>(Bolukbasi et al., 2016)</cite> , one must have prior knowledge of which words are gender-appropriate, so that they are not debiased. Debiasing all vectors can preclude gender-appropriate analogies such as king:queen::man:woman from holding in the embedding space. To create an exhaustive list of gender-appropriate words,<cite> Bolukbasi et al. (2016)</cite> started with a small, human-labelled set of words and then trained an SVM to predict more genderappropriate terms in the vocabulary.",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_17",
  "x": "**DEBIASING WITHOUT SUPERVISION** To use the subspace projection method <cite>(Bolukbasi et al., 2016)</cite> , one must have prior knowledge of which words are gender-appropriate, so that they are not debiased. Debiasing all vectors can preclude gender-appropriate analogies such as king:queen::man:woman from holding in the embedding space. To create an exhaustive list of gender-appropriate words,<cite> Bolukbasi et al. (2016)</cite> started with a small, human-labelled set of words and then trained an SVM to predict more genderappropriate terms in the vocabulary. This bootstrapped list of gender-appropriate words was then left out during debiasing. The way in which<cite> Bolukbasi et al. (2016)</cite> evaluated their method is unorthodox: they tested the ability of their debiased embedding space to generate new analogies. However, this does not capture whether gender-appropriate analogies are successfully preserved and gender-biased analogies successfully precluded.",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_18",
  "x": "Using difference vectors from biased analogies, such as doctor \u2212 midwife, we then create a bias-defining relation vector b the same way. We then debias a word w using the subspace projection method iff it satisfies |\u03b2 ( w; b * )|< |\u03b2 ( w; b )|. As seen in Figure 1 , this simple condition is sufficient to preserve almost all gender-appropriate analogies while precluding most gender-biased ones. In our debiased embedding space, 94.9% of gender-appropriate analogies with a strength of at least 0.5 are preserved in the embedding space while only 36.7% of gender-biased analogies are. In contrast, the<cite> Bolukbasi et al. (2016)</cite> approach 2 Available at https://github.com/tolga-b/debiaswe preserves only 16.5% of appropriate analogies with a strength of at least 0.5 while preserving 80.0% of biased ones. Recall that we use the same debiasing method as<cite> Bolukbasi et al. (2016)</cite> ; the difference in performance can only be ascribed to how we choose the gender-appropriate words. Combining our heuristic with other methods may yield even better results, which we leave as future work.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_19",
  "x": "Recall that we use the same debiasing method as<cite> Bolukbasi et al. (2016)</cite> ; the difference in performance can only be ascribed to how we choose the gender-appropriate words. Combining our heuristic with other methods may yield even better results, which we leave as future work. ---------------------------------- **CONCLUSION** In this paper, we answered several open questions about undesirable word associations in embedding spaces. We found that for any embedding model that implicitly does matrix factorization (e.g., SGNS, GloVe), debiasing with the subspace projection method is, under certain conditions, equivalent to training on a corpus that is unbiased with respect to the words defining the bias subspace. We proved that WEAT, the most common test of word embedding association, has theoretical flaws that cause it to systematically overestimate bias.",
  "y": "differences uses"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_0",
  "x": "****ASSESSING BERT'S SYNTACTIC ABILITIES**** **ABSTRACT** I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) <cite>\"coloreless green ideas\"</cite> subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases. ---------------------------------- **INTRODUCTION** The recently introduced BERT model (Devlin et al., 2018) exhibits strong performance on several language understanding benchmarks.",
  "y": "motivation"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_1",
  "x": "**INTRODUCTION** The recently introduced BERT model (Devlin et al., 2018) exhibits strong performance on several language understanding benchmarks. To what extent does it capture syntax-sensitive structures? Recent work examines the extent to which RNN-based models capture syntax-sensitive phenomena that are traditionally taken as evidence for the existence in hierarchical structure. In particular, in (Linzen et al., 2016) we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences. <cite>(Gulordava et al., 2018 )</cite> also consider subject-verb agreement, but in a <cite>\"colorless green ideas\"</cite> setting in which content words in naturally occurring sentences are replaced with random words with the same partof-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues. Marvin and Linzen (2018) consider a wider range of syntactic phenomena (subjectverb agreement, reflexive anaphora, negative polarity items) using manually constructed stimuli, allowing for greater coverage and control than in the naturally occurring setting.",
  "y": "differences"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_2",
  "x": "<cite>(Gulordava et al., 2018 )</cite> also consider subject-verb agreement, but in a <cite>\"colorless green ideas\"</cite> setting in which content words in naturally occurring sentences are replaced with random words with the same partof-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues. Marvin and Linzen (2018) consider a wider range of syntactic phenomena (subjectverb agreement, reflexive anaphora, negative polarity items) using manually constructed stimuli, allowing for greater coverage and control than in the naturally occurring setting. The BERT model is based on the \"Transformer\" architecture (Vaswani et al., 2017) , which-in contrast to RNNs-relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute-position embedding. This reliance on attention may lead one 1 to expect decreased performance on syntax-sensitive tasks compared to RNN (LSTM) models that do model word order directly, and explicitly track states across the sentence. Indeed, Tran et al. (2018) finds that transformerbased models perform worse than LSTM models on the Linzen et al. (2016) agreement prediction dataset. In contrast, (Tang et al., 2018) find that self-attention performs on par with LSTM for syntax sensitive dependencies in the context of machine-translation, and performance on syntactic tasks is correlated with the number of attention heads in multi-head attention. I adapt the evaluation protocol and stimuli of Linzen et al. (2016) , <cite>Gulordava et al. (2018)</cite> and Marvin and Linzen (2018) to the bidirectional setting required by BERT, and evaluate the pretrained BERT models (both the LARGE and the BASE models).",
  "y": "background"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_3",
  "x": "In contrast, (Tang et al., 2018) find that self-attention performs on par with LSTM for syntax sensitive dependencies in the context of machine-translation, and performance on syntactic tasks is correlated with the number of attention heads in multi-head attention. I adapt the evaluation protocol and stimuli of Linzen et al. (2016) , <cite>Gulordava et al. (2018)</cite> and Marvin and Linzen (2018) to the bidirectional setting required by BERT, and evaluate the pretrained BERT models (both the LARGE and the BASE models). Surprisingly (at least to me), the out-of-the-box models (without any task-specific fine-tuning) perform very well on all the syntactic tasks. ---------------------------------- **METHODOLOGY** I use the stimuli provided by (Linzen et al., 2016; <cite>Gulordava et al., 2018</cite>; Marvin and Linzen, 2018) , but change the experimental protocol to adapt it to the bidirectional nature of the BERT model. This requires discarding some of the stimuli, as described below.",
  "y": "extends"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_4",
  "x": "Indeed, Tran et al. (2018) finds that transformerbased models perform worse than LSTM models on the Linzen et al. (2016) agreement prediction dataset. In contrast, (Tang et al., 2018) find that self-attention performs on par with LSTM for syntax sensitive dependencies in the context of machine-translation, and performance on syntactic tasks is correlated with the number of attention heads in multi-head attention. I adapt the evaluation protocol and stimuli of Linzen et al. (2016) , <cite>Gulordava et al. (2018)</cite> and Marvin and Linzen (2018) to the bidirectional setting required by BERT, and evaluate the pretrained BERT models (both the LARGE and the BASE models). Surprisingly (at least to me), the out-of-the-box models (without any task-specific fine-tuning) perform very well on all the syntactic tasks. ---------------------------------- **METHODOLOGY** I use the stimuli provided by (Linzen et al., 2016; <cite>Gulordava et al., 2018</cite>; Marvin and Linzen, 2018) , but change the experimental protocol to adapt it to the bidirectional nature of the BERT model.",
  "y": "extends"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_5",
  "x": "All three previous work use uni-directional language-model-like models. Linzen et al. (2016) start with existing sentences from wikipedia that contain a present-tense verb. They feed each sentence word by word into an LSTM, stop right before the focus verb, and ask the model to predict a binary plural/singular decision (supervised setup) or compare the probability assigned by a pre-trained language model (LM) to the plural vs singular forms of the verb (LM setup). 2 The evaluation is then performed on sentences with \"agreement attractors\" in which at there is at least one noun between the verb and its subject, and all of the nouns between the verb and subject are of the opposite number from the subject. <cite>Gulordava et al. (2018)</cite> also start with existing sentences. However, in order to control for the possibillity of the model learning to rely on \"semantic\" selectional-preferences cues rather than syntactic ones, <cite>they</cite> replace each content word with random words from the same part-ofspeech and inflection. This results in \"<cite>coloreless green ideas</cite>\" nonce sentences.",
  "y": "background"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_6",
  "x": "Similarly, for the pair the game that the guard hates is bad . the game that the guard hates are bad . (from (Marvin and Linzen, 2018) ), I feed into BERT: [CLS] the game that the guard hates [MASK] bad . and compare the scores predicted for is and are. This differs from Linzen et al. (2016) and <cite>Gulordava et al. (2018)</cite> by considering the entire sentence (excluding the verb) and not just its prefix leading to the verb, and differs from Marvin and Linzen (2018) by conditioning the focus verb on bidirectional context. I use the PyTorch implementation of BERT, with the pre-trained models supplied by Google.",
  "y": "motivation extends"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_7",
  "x": "This differs from Linzen et al. (2016) and <cite>Gulordava et al. (2018)</cite> by considering the entire sentence (excluding the verb) and not just its prefix leading to the verb, and differs from Marvin and Linzen (2018) by conditioning the focus verb on bidirectional context. I use the PyTorch implementation of BERT, with the pre-trained models supplied by Google. 4 I experiment with the bert-large-uncased and bert-base-uncased models. Discarded Material The bi-directional setup precludes using using the NPI stimuli of Marvin and Linzen (2018) , in which the minimal pair differs in two words position, which I discard from the evaluation. I also discard the agreement cases involving the verbs is or are in Linzen et al. (2016) and in <cite>Gulordava et al. (2018)</cite> , because some of them are copular construction, in which strong agreement hints can be found also on the object following the verb. 5 This is not an issue in the manually constructed (Marvin and Linzen, 2018 ) stimuli due to the patterns they chose. Finally, I discard stimuli in which the focus verb or its plural/singular inflection does not appear as a single word in the BERT wordpiece-based vocabulary (and hence cannot be predicted by the model).",
  "y": "differences extends"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_8",
  "x": "This include discarding Marvin and Linzen (2018) stimuli involving the words swims or admires, resulting in 23,368 discarded pairs (out of 152,300). I similarly discard 680 sentences from (Linzen et al., 2016) where the focus verb or its inflection were one of 108 out-ofvocabulary tokens, 6 and 28 sentence-pairs (8 tokens 7 ) from <cite>(Gulordava et al., 2018)</cite> . Limitations The BERT results are not directly comparable to the numbers reported in previous work. Beyond the differences due to bidirectionality and the discarded stimuli, the BERT models are also trained on a different and larger corpus (covering both wikipedia and books). 4 https://github.com/huggingface/pytorch-pretrained-BERT 5 Results are generally a bit higher when not discarding the is/are cases. 6 blames, dislike, inhabit, exclude, revolves, governs, delete, composes, overlap, edits, embrace, compose, undertakes, disagrees, redirect, persist, recognise, rotates, accompanies, attach, undertake, earn, communicates, imagine, contradicts, specialize, accuses, obtain, caters, welcomes, interprets, await, communicate, templates, qualify, reverts, achieve, achieves, govern, restricts, violate, behave, emit, contend, adopt, overlaps, reproduces, rotate, defends, submit, revolve, lend, pertain, disagree, concentrate, detects, endorses, detect, predate, persists, consume, locates, earns, predict, interact, merge, consumes, behaves, locate, predates, enhances, predicts, integrates, inhabits, satisfy, contradict, swear, activate, restrict, satisfies, redirects, excludes, violates, interacts, admires, speculate, blame, drag, qualifies, activates, criticize, assures, welcome, depart, characterizes, defend, obtains, lends, strives, accuse, recognises, characterize, contends, perceive, complain, awaits 7 toss, spills, tosses, affirms, spill, melt, approves, affirm Table 2 : Results on the EN NONCE <cite>(Gulordava et al., 2018)</cite> stimuli. While not strictly comparable, the numbers reported by <cite>Gulordava et al. (2018)</cite> for the LSTM in this condition (on All) is 74.1 \u00b1 1.6.",
  "y": "extends"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_10",
  "x": "Beyond the differences due to bidirectionality and the discarded stimuli, the BERT models are also trained on a different and larger corpus (covering both wikipedia and books). 4 https://github.com/huggingface/pytorch-pretrained-BERT 5 Results are generally a bit higher when not discarding the is/are cases. 6 blames, dislike, inhabit, exclude, revolves, governs, delete, composes, overlap, edits, embrace, compose, undertakes, disagrees, redirect, persist, recognise, rotates, accompanies, attach, undertake, earn, communicates, imagine, contradicts, specialize, accuses, obtain, caters, welcomes, interprets, await, communicate, templates, qualify, reverts, achieve, achieves, govern, restricts, violate, behave, emit, contend, adopt, overlaps, reproduces, rotate, defends, submit, revolve, lend, pertain, disagree, concentrate, detects, endorses, detect, predate, persists, consume, locates, earns, predict, interact, merge, consumes, behaves, locate, predates, enhances, predicts, integrates, inhabits, satisfy, contradict, swear, activate, restrict, satisfies, redirects, excludes, violates, interacts, admires, speculate, blame, drag, qualifies, activates, criticize, assures, welcome, depart, characterizes, defend, obtains, lends, strives, accuse, recognises, characterize, contends, perceive, complain, awaits 7 toss, spills, tosses, affirms, spill, melt, approves, affirm Table 2 : Results on the EN NONCE <cite>(Gulordava et al., 2018)</cite> stimuli. While not strictly comparable, the numbers reported by <cite>Gulordava et al. (2018)</cite> for the LSTM in this condition (on All) is 74.1 \u00b1 1.6. ---------------------------------- **REPRODUCABILITY** Code is available at https://github.com/yoavg/bert-syntax.",
  "y": "uses"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_11",
  "x": "Marvin and Linzen (2018) . The BERT and M&L numbers are not directly comparable, as the experimental setup differs in many ways. ---------------------------------- **DISCUSSION** The BERT models perform remarkably well on all the syntactic test cases. I expected the attentionbased mechanism to fail on these (compared to the LSTM-based models), and am surprised by these results. The <cite>Gulordava et al. (2018)</cite> and Marvin and Linzen (2018) conditions rule out the possibility of overly relying on selectional preference cues or memorizing the wikipedia training data, and suggest real syntactic generalization is taking place.",
  "y": "background uses"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_0",
  "x": "However, they dramatically increase runtime and can be difficult to employ in practice. We demonstrate that applying constituency model combination techniques to n-best lists instead of n different parsers results in significant parsing accuracy improvements. Parses are weighted by their probabilities and combined using an adapted version of<cite> Sagae and Lavie (2006)</cite> . These accuracy gains come with marginal computational costs and are obtained on top of existing parsing techniques such as discriminative reranking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23. On out-of-domain corpora, accuracy is improved by 0.4% on average. We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_1",
  "x": "We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains. ---------------------------------- **INTRODUCTION** Researchers have proposed many algorithms to combine parses from multiple parsers into one final parse (Henderson and Brill, 1999; Zeman an\u010f Zabokrtsk\u1ef3, 2005;<cite> Sagae and Lavie, 2006</cite>; Nowson and Dale, 2007; Fossum and Knight, 2009; Petrov, 2010; Johnson and Ural, 2010; Huang et al., 2010; McDonald and Nivre, 2011; Shindo et al., 2012; Narayan and Cohen, 2015) . These new parses are substantially better than the originals: Zhang et al. (2009) combine outputs from multiple n-best parsers and achieve an F 1 of 92.6% on the WSJ test set, a 0.5% improvement over their best n-best parser. Model combination approaches tend to fall into the following categories: hybridization, where multiple parses are combined into a single parse; switching, which picks a single parse according to some criteria (usually a form of voting); grammar merging where grammars are combined before or during parsing; and stacking, where one parser sends its prediction to another at runtime.",
  "y": "background"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_2",
  "x": "We then adapt parse combination methods by Henderson and Brill (1999) ,<cite> Sagae and Lavie (2006)</cite> , and Fossum and Knight (2009) to fuse the constituents from the n parses into a single tree. We empirically show that six n-best parsers benefit from parse fusion across six domains, obtaining stateof-the-art results. These improvements are complementary to other techniques such as reranking and self-training. Our best system obtains an F 1 of 92.6% on WSJ section 23, a score previously obtained only by combining the outputs from multiple parsers. A reference implementation is available as part of BLLIP Parser at http: //github.com/BLLIP/bllip-parser/ 2 Fusion Henderson and Brill (1999) propose a method to combine trees from m parsers in three steps: populate a chart with constituents along with the number of times they appear in the trees; remove any constituent with count less than m/2 from the chart; and finally create a final tree with all the remaining constituents. Intuitively their method constructs a tree with constituents from the majority of the trees, which boosts precision significantly. Henderson and Brill (1999) show that this process is guaranteed to produce a valid tree.",
  "y": "extends"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_3",
  "x": "Surprisingly, exploiting n-best trees does not lead to large improvement over combining 1-best trees in their experiments. Our extension takes the n-best trees from a parser as if they are 1-best parses from n parsers, then follows<cite> Sagae and Lavie (2006)</cite> . Parses are weighted by the estimated probabilities from the parser. Given n trees and their weights, the model computes a constituent's weight by summing weights of all trees containing that constituent. Concretely, the weight of a constituent spanning from ith word to jth word with label is where W (k) is the weight of kth tree and C k (i \u2192 j) is one if a constituent with label spanning from i to j is in kth tree, zero otherwise. After populating the chart with constituents and their weights, it throws out constituents with weights below a set threshold t. Using the threshold t = 0.5 emulates the method of Henderson and Brill (1999) in that it constructs the tree with the constituents in the majority of the trees.",
  "y": "uses"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_4",
  "x": "smoothing exponent (\u03b2) is best set around 1.0, with accuracy falling off if the value deviates too much. Finally, the threshold parameter is empirically optimized a little below t = 0.5 (the value suggested by Henderson and Brill (1999) ). Since score values are normalized, this means that constituents need roughly half the \"score mass\" in order to be included in the chart. Varying the threshold changes the precision/recall balance since a high threshold adds only the most confident constituents to the chart <cite>(Sagae and Lavie, 2006)</cite> . Baselines: Table 2 gives the accuracy of fusion and baselines for BLLIP on the development corpora. Majority voting sets n = 50, \u03b2 = 0, t = 0.5 giving all parses equal weight and results in constituent-level majority voting. We explore a rank-based weighting which ignores parse probabilities and weight parses only using the rank: W rank (k) = 1/(2 k ).",
  "y": "background"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_5",
  "x": "Fusion generally improves F 1 for in-domain and out-of-domain parsing by a significant margin. For the self-trained BLLIP parser, in-domain F 1 increases by 0.4% and out-of-domain F 1 increases by 0.4% on average. Berkeley parser obtains the smallest gains from fusion since Berkeley's n-best lists are ordered by factors other than probabilities. As a result, the probabilities from Berkeley can mislead the fusion process. We also compare against model combination using our reimplementation of<cite> Sagae and Lavie (2006)</cite> . For these results, all six parsers were given equal weight. The threshold was set to 0.42 to optimize model combination F 1 on development data (similar to Setting 2 for constituency parsing in<cite> Sagae and Lavie (2006)</cite> ).",
  "y": "uses"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_6",
  "x": "For the self-trained BLLIP parser, in-domain F 1 increases by 0.4% and out-of-domain F 1 increases by 0.4% on average. Berkeley parser obtains the smallest gains from fusion since Berkeley's n-best lists are ordered by factors other than probabilities. As a result, the probabilities from Berkeley can mislead the fusion process. We also compare against model combination using our reimplementation of<cite> Sagae and Lavie (2006)</cite> . For these results, all six parsers were given equal weight. The threshold was set to 0.42 to optimize model combination F 1 on development data (similar to Setting 2 for constituency parsing in<cite> Sagae and Lavie (2006)</cite> ). Model combination Table 3 : Evaluation of the constituency fusion method on six parsers across six domains.",
  "y": "similarities"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_7",
  "x": "The first extension explores applying fusion to dependency parsing. We explored two ways to apply fusion when starting from constituency parses: (1) fuse constituents and then convert them to dependencies and (2) convert to dependencies then fuse the dependencies as in<cite> Sagae and Lavie (2006)</cite> . Approach (1) does not provide any benefit (LAS drops between 0.5% and 2.4%). This may result from fusion's artifacts including unusual unary chains or nodes with a large number of children -it is possible that adjusting unary handling and the precision/recall tradeoff may reduce these issues. Approach (2) provided only modest benefits compared to those from constituency parsing fusion. The largest LAS increase for (2) is 0.6% for the Stanford Parser, though for Berkeley and Self-trained BLLIP, dependency fusion results in small losses (-0.1% LAS). Two possible reasons are that the dependency baseline is higher than its constituency counterpart and some dependency graphs from the n-best list are duplicates which lowers diversity and may need special handling, but this remains an open question.",
  "y": "similarities uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_0",
  "x": "We use PMB release 2.2.0, which contains gold standard (fully manually annotated) data of which we use 4,597 as train, 682 as dev and 650 as test instances. It also contains 67,965 silver (partially manually annotated) and 120,662 bronze (no manual annotations) instances. Most sentences are between 5 and 15 tokens in length. Since we will compare our results mainly to<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> , we will only employ the gold and silver data. Name x3 \"tom\" Figure 1: DRS in box format (a), gold clause representation (b) and example system output (c) for I am not working for Tom, with precision of 5/8 and recall of 5/9, resulting in an F-score of 58.8. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_1",
  "x": "In our experiments, we only use the English texts and corresponding DRSs. We use PMB release 2.2.0, which contains gold standard (fully manually annotated) data of which we use 4,597 as train, 682 as dev and 650 as test instances. It also contains 67,965 silver (partially manually annotated) and 120,662 bronze (no manual annotations) instances. Most sentences are between 5 and 15 tokens in length. Since we will compare our results mainly to<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> , we will only employ the gold and silver data. Name x3 \"tom\" Figure 1: DRS in box format (a), gold clause representation (b) and example system output (c) for I am not working for Tom, with precision of 5/8 and recall of 5/9, resulting in an F-score of 58.8.",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_2",
  "x": "---------------------------------- **REPRESENTING INPUT AND OUTPUT** We represent the source and target data in the same way as<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> , who represent the source sentence as a sequence of characters, with a special character indicating uppercase characters. The target DRS is also represented as a sequence of characters, with the exception of DRS operators, thematic roles and DRS variables, which are represented as super characters (Van Noord and Bos, 2017b), i.e. individual tokens. Since the variable names itself are meaningless, the DRS variables are rewritten to a more general representation, using the De Bruijn index (de Bruijn, 1972) . In a post-processing step, the original clause structured is restored. 1 To include morphological and syntactic information, we apply a lemmatizer, POS-tagger and dependency parser using Stanford CoreNLP (Manning et al., 2014) , similar to Sennrich and Haddow (2016) for machine translation.",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_3",
  "x": "---------------------------------- **NEURAL ARCHITECTURE** We employ a recurrent sequence-to-sequence neural network with attention (Bahdanau et al., 2014) and two bi-LSTM layers, similar to the one used by<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> . However, their model was trained with OpenNMT (Klein et al., 2017) , which does not support multiple encoders. Therefore, we switch to the sequence-to-sequence framework implemented in Marian (Junczys-Dowmunt et al., 2018). We use model-type s2s (for a single encoder) or multi-s2s (for multiple encoders). For the latter, this means that the multiple inputs are encoded separately by an identical RNN (without sharing parameters).",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_4",
  "x": "**EVALUATION PROCEDURE** Produced DRSs are compared with the gold standard representations by using COUNTER (Van Noord, Abzianidze, Haagsma, and Bos, 2018) . This is a tool that calculates micro precision, recall and F-score over matching clauses, similar to the SMATCH (Cai and Knight, 2013 ) evaluation tool for AMR parsing. All clauses have the same weight in matching, except for REF clauses, which are ignored. An example of the matching procedure is shown in Figure 1 . The produced DRSs go through a strict syntactic and semantic validation process, as described in<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> . If a produced DRS is invalid, it is replaced by a dummy DRS, which gets an F-score of 0.0.",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_5",
  "x": "Lemmatization shows the highest improvement, most likely because the DRS concepts that need to be produced are often lemmatized versions of the source words. When we stack the linguistic features, we observe an improvement for each addition, resulting in a final 2.7 point F-score increase over the baseline. If we also employ silver data, we again observe that the multi-encoder setup is preferable over a single encoder, for both isolating and stacking the linguistic features. On isolation, the results are similar to only using gold data, with the exception of the semantic tags, which even hurt the performance now. Interestingly, when stacking the linguistic features, there is no improvement over only using the lemma of the source words. We now compare our best models to previous parsers 4 (Bos, 2015;<cite> Van Noord, Abzianidze, Toral, and Bos, 2018)</cite> and two baseline systems, SPAR and SIM-SPAR. As previously indicated,<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> used a similar sequence-to-sequence model as our current approach, but implemented in OpenNMT and without the linguistic features.",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_6",
  "x": "When we stack the linguistic features, we observe an improvement for each addition, resulting in a final 2.7 point F-score increase over the baseline. If we also employ silver data, we again observe that the multi-encoder setup is preferable over a single encoder, for both isolating and stacking the linguistic features. On isolation, the results are similar to only using gold data, with the exception of the semantic tags, which even hurt the performance now. Interestingly, when stacking the linguistic features, there is no improvement over only using the lemma of the source words. We now compare our best models to previous parsers 4 (Bos, 2015;<cite> Van Noord, Abzianidze, Toral, and Bos, 2018)</cite> and two baseline systems, SPAR and SIM-SPAR. As previously indicated,<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> used a similar sequence-to-sequence model as our current approach, but implemented in OpenNMT and without the linguistic features. Boxer (Bos, 2008 (Bos, , 2015 ) is a DRS parser that uses a statistical CCG parser for syntactic analysis and a compositional semantics based on \u03bb-calculus, followed by pronoun and presupposition resolution.",
  "y": "uses similarities differences"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_7",
  "x": "We now compare our best models to previous parsers 4 (Bos, 2015;<cite> Van Noord, Abzianidze, Toral, and Bos, 2018)</cite> and two baseline systems, SPAR and SIM-SPAR. As previously indicated,<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> used a similar sequence-to-sequence model as our current approach, but implemented in OpenNMT and without the linguistic features. Boxer (Bos, 2008 (Bos, , 2015 ) is a DRS parser that uses a statistical CCG parser for syntactic analysis and a compositional semantics based on \u03bb-calculus, followed by pronoun and presupposition resolution. SPAR is a baseline system that outputs the same DRS for each test instance 5 , while SIM-SPAR outputs the DRS of the most similar sentence in the training set, based on a simple word embedding metric. 6 The results are shown in Table 5 . Our model clearly outperforms the previous systems, even when only using gold standard data. When compared to<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> , retrained with the same data used in our systems, the largest improvement (3.6 and 3.5 for dev and test) comes from switching framework and changing certain parameters such as the optimizer and learning rate.",
  "y": "differences"
 },
 {
  "id": "c293e9fb8d6382f185a3efeaf0dbf7_0",
  "x": "Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences). Different from Ma and Hovy (2016) and<cite> Liu et al. (2018)</cite> , choose a different data split on the POS dataset. Liu et al. (2018) and Hashimoto et al. (2017) use different development sets for chunking. \u2022 Preprocessing. A typical data preprocessing step is to normize digit characters (Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Strubell et al., 2017) . Reimers and Gurevych (2017b) use fine-grained representations for less frequent words. Ma and Hovy (2016) do not use preprocessing.",
  "y": "extends differences"
 },
 {
  "id": "c293e9fb8d6382f185a3efeaf0dbf7_1",
  "x": "\u2022 Evaluation. Some literature reports results using mean and standard deviation under different random seeds (Chiu and Nichols, 2016; Peters et al., 2017; <cite>Liu et al., 2018)</cite> . Others report the best result among different trials (Ma and Hovy, 2016) , which cannot be compared directly. \u2022 Hardware environment can also affect system accuracy. Liu et al. (2018) observes that the system gives better accuracy on NER task when trained using GPU as compared to using CPU. Besides, the running speeds are highly affected by the hardware environment. To address the above concerns, we systematically analyze neural sequence labeling models on three benchmarks: CoNLL 2003 NER (Tjong Kim Sang and De Meulder, 2003) , CoNLL 2000 chunking (Tjong Kim Sang and Buchholz, 2000) and PTB POS tagging (Marcus et al., 1993) .",
  "y": "background"
 },
 {
  "id": "c293e9fb8d6382f185a3efeaf0dbf7_2",
  "x": "Hammerton (2003) was the first to exploit LSTM for sequence labeling. built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (Lample et al., 2016; <cite>Liu et al., 2018)</cite> , GRU (Yang et al., 2016) , and CNN (Chiu and Nichols, 2016; Ma and Hovy, 2016) features. Yang et al. (2017a) proposed a neural reranking model to improve NER models. These models achieve state-of-the-art results in the literature. Reimers and Gurevych (2017b) compared several word-based LSTM models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value. They investigated the influence of various hyperparameters and configurations. Our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects: 1) Their experiments are based on a BiLSTM with handcrafted word features, while our experiments are based on end-to-end neural models without human knowledge.",
  "y": "background"
 },
 {
  "id": "c293e9fb8d6382f185a3efeaf0dbf7_3",
  "x": "We examined both structures and found that they give comparable accuracies on sequence labeling tasks. We choose Lample et al. (2016) 's structure as its character LSTMs can be calculated in parallel, making the system more efficient. ---------------------------------- **WORD SEQUENCE REPRESENTATIONS** Similar to character sequences in words, we can model word sequence information through LSTM or CNN structures. LSTM has been widely used in sequence labeling (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; <cite>Liu et al., 2018)</cite> . CNN can be much faster than LSTM due to the fact that convolution calculation can be parallel on the input sequence (Collobert et al., 2011; dos Santos et al., 2015; Strubell et al., 2017) .",
  "y": "background"
 },
 {
  "id": "c293e9fb8d6382f185a3efeaf0dbf7_4",
  "x": "**SETTINGS** Data. The NER dataset has been standardly split in Tjong Kim Sang and De Meulder (2003 (Toutanova et al., 2003; Santos and Zadrozny, 2014; Ma and Hovy, 2016; <cite>Liu et al., 2018)</cite> , we adopt the standard splits by using sections 0-18 as training set, sections 19-21 as development set and sections 22-24 as test set. No preprocessing is performed on either dataset except for normalizing digits. The dataset statistics are listed in Table 2 . Hyperparameters. Table 3 shows the hyperparameters used in our experiments, which mostly follow Ma and Hovy (2016) , including the learning rate \u03b7 = 0.015 for word LSTM models.",
  "y": "differences"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_0",
  "x": "While the seminal work by Bolukbasi et al. (2016a <cite>Bolukbasi et al. ( , 2016b</cite> concerns the identification and mitigation of gender bias in pretrained word embeddings, Zhao et al. (2018) provide insights into the possibilities of learning embeddings that are gender neutral. Bordia and Bowman (2019) outline a way of training a recurrent neural network for word-based language modelling such that the model is gender neutral. Park et al. (2018) discuss different ways of mitigating gender bias, in the context of abusive language detection, ranging from debiasing a model by using the hard debiased word embeddings produced by<cite> Bolukbasi et al. (2016b)</cite> , to manipulating the data prior to training a model by swapping masculine and feminine mentions, and employing transfer learning from a model learned from less biased text. Gonen and Goldberg (2019) contest the approaches to debiasing word embeddings presented by<cite> Bolukbasi et al. (2016b)</cite> and Zhao et al. (2018) , arguing that while the bias is reduced when measured according to its definition, i.e., dampening the impact of the general gender direction in the vector space, \"the actual effect is mostly hiding the bias, not removing it\". Further, Gonen and Gold-berg (2019) claim that a lot of the supposedly removed bias can be recovered due to the geometry of the vector representation of the gender neutralized words. Our contribution consists of an investigation of the presence of gender bias in pretrained embeddings for Swedish. We are less interested in bias as a theoretical construct, and more interested in the effects of gender bias in actual applications where pretrained embeddings are employed.",
  "y": "background"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_1",
  "x": "Bordia and Bowman (2019) outline a way of training a recurrent neural network for word-based language modelling such that the model is gender neutral. Park et al. (2018) discuss different ways of mitigating gender bias, in the context of abusive language detection, ranging from debiasing a model by using the hard debiased word embeddings produced by<cite> Bolukbasi et al. (2016b)</cite> , to manipulating the data prior to training a model by swapping masculine and feminine mentions, and employing transfer learning from a model learned from less biased text. Gonen and Goldberg (2019) contest the approaches to debiasing word embeddings presented by<cite> Bolukbasi et al. (2016b)</cite> and Zhao et al. (2018) , arguing that while the bias is reduced when measured according to its definition, i.e., dampening the impact of the general gender direction in the vector space, \"the actual effect is mostly hiding the bias, not removing it\". Further, Gonen and Gold-berg (2019) claim that a lot of the supposedly removed bias can be recovered due to the geometry of the vector representation of the gender neutralized words. Our contribution consists of an investigation of the presence of gender bias in pretrained embeddings for Swedish. We are less interested in bias as a theoretical construct, and more interested in the effects of gender bias in actual applications where pretrained embeddings are employed. Our experiments are therefore tightly tied to a real-world use case where gender bias would have potentially serious ramifications.",
  "y": "background"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_2",
  "x": "Caliskan et al. (2017) show that learned embeddings exhibit every linguistic bias documented in the field of psychology (such as that flowers are more pleasant than insects, musical instruments are preferred to weapons, and personal names are used to infer race). Garg et al. (2018) show that temporal changes of the embeddings can be used to quantify gender and ethnic stereotypes over time, and Zhao et al. (2017) suggest that biases might in fact be amplified by embedding models. Several researchers have also investigated ways to counter stereotypes and biases in learned language models. While the seminal work by Bolukbasi et al. (2016a <cite>Bolukbasi et al. ( , 2016b</cite> concerns the identification and mitigation of gender bias in pretrained word embeddings, Zhao et al. (2018) provide insights into the possibilities of learning embeddings that are gender neutral. Bordia and Bowman (2019) outline a way of training a recurrent neural network for word-based language modelling such that the model is gender neutral. Park et al. (2018) discuss different ways of mitigating gender bias, in the context of abusive language detection, ranging from debiasing a model by using the hard debiased word embeddings produced by<cite> Bolukbasi et al. (2016b)</cite> , to manipulating the data prior to training a model by swapping masculine and feminine mentions, and employing transfer learning from a model learned from less biased text. Gonen and Goldberg (2019) contest the approaches to debiasing word embeddings presented by<cite> Bolukbasi et al. (2016b)</cite> and Zhao et al. (2018) , arguing that while the bias is reduced when measured according to its definition, i.e., dampening the impact of the general gender direction in the vector space, \"the actual effect is mostly hiding the bias, not removing it\".",
  "y": "background"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_3",
  "x": "We also provide further evidence of the inability of the debiasing method proposed by<cite> Bolukbasi et al. (2016b)</cite> to handle the type of bias we are concerned with. ---------------------------------- **EMBEDDINGS** We include four different standard embeddings in these experiments: word2vec, fastText, ELMo and BERT. There are several pre-trained models available in various web repositories. We select one representative instance per model, summarized in Table 2 (next page). These models represent different types of embeddings.",
  "y": "similarities uses"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_4",
  "x": "When counting only the single most similar occupation, ELMo shows a similar tendency as word2vec and groups both male and female names with male occupations. BERT, on the other hand, seems slightly more balanced, with a tendency similar to fastText when counting the average similarities. When only counting the single most similar occupation, BERT is almost perfectly balanced between female and male occupations. ---------------------------------- **DEBIASING EMBEDDINGS** We apply the debiasing methodology in<cite> (Bolukbasi et al., 2016b)</cite> to the pretrained embedddings. Debiasing a given vector space involves finding the general direction in it that signifies gender using a set of predefined definitional pairs, and then removing the direction from all vectors except those corresponding to words that are naturally gender specific.",
  "y": "similarities uses"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_5",
  "x": "Debiasing a given vector space involves finding the general direction in it that signifies gender using a set of predefined definitional pairs, and then removing the direction from all vectors except those corresponding to words that are naturally gender specific. The definitional pairs are word pairs expressing among themselves a natural distinction between the genders, e.g., he -she, and motherfather. In our setting, there are 10 such pairs. The gender specific words are words that also carry a natural gender dimension that should not be corrected during the debiasing phase of the vector space. We use the same methodology for growing a seed set of gender specific words into a larger set as described in<cite> (Bolukbasi et al., 2016b)</cite> , and end up with 486 manually curated gender specific words, including e.g., farfar (paternal grandfather), tvillingsystrar (twin sisters), and matriark (matriarch). The definitional pairs are used to find a gender direction in the embedding space, which is done by taking the difference vector of each of the definitional pairs (i.e. w 1 \u2212 w 2 ), and then factorizing the mean-centered difference vectors using PCA, retaining only the first principal component, which will act as the gender direction. The vector space is then hard debiased 1 in the sense that the gen- der direction b is removed from the embeddings of all non-gender specific words w using orthogonal projection: w = w \u2212 b \u00d7 w\u00b7b b\u00b7b .",
  "y": "similarities uses"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_6",
  "x": "The definitional pairs are word pairs expressing among themselves a natural distinction between the genders, e.g., he -she, and motherfather. In our setting, there are 10 such pairs. The gender specific words are words that also carry a natural gender dimension that should not be corrected during the debiasing phase of the vector space. We use the same methodology for growing a seed set of gender specific words into a larger set as described in<cite> (Bolukbasi et al., 2016b)</cite> , and end up with 486 manually curated gender specific words, including e.g., farfar (paternal grandfather), tvillingsystrar (twin sisters), and matriark (matriarch). The definitional pairs are used to find a gender direction in the embedding space, which is done by taking the difference vector of each of the definitional pairs (i.e. w 1 \u2212 w 2 ), and then factorizing the mean-centered difference vectors using PCA, retaining only the first principal component, which will act as the gender direction. The vector space is then hard debiased 1 in the sense that the gen- der direction b is removed from the embeddings of all non-gender specific words w using orthogonal projection: w = w \u2212 b \u00d7 w\u00b7b b\u00b7b . The approach described by<cite> (Bolukbasi et al., 2016b)</cite> includes an equalize step to make all gender neutral words equidistant to each of the members of a given equality set of word pairs.",
  "y": "background"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_7",
  "x": "We hypothesize that the results for BERT are negatively affected by artefacts of the WordPiece tokenization, as discussed in Section 3. ---------------------------------- **THE EFFECT OF DEBIASING ON EMBEDDINGS** So far, we have shown that all Swedish pretrained embeddings included in this study exhibit some degree of gender bias when applied to a real-world scenario. We now turn to investigate the effect the hard debiasing operation has on the embedding spaces, using the intrinsic evaluation methodology of<cite> Bolukbasi et al. (2016b)</cite> . In this setting, a number of analogy pairs are extracted for the original and debiased embeddings, and human evaluators are used to asses the number of appropriate and stereotypical pairs in the respective representations. Bolukbasi et al. (2016b) used 10 crowdworkers to classify the analogy pairs as being appropriate or stereotypical.",
  "y": "uses"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_8",
  "x": "This is manifested by an increase of the number of uncertain analogy pairs that the annotators agree on between the original and debiased models (both for word2vec and fastText). However, the most interesting findings have to do with the number of stereotypical analogy pairs. The number of stereotypical analogy pairs output by the Swedish models is small compared to the numbers reported by<cite> Bolukbasi et al. (2016b)</cite> . Further, the number of stereotypical pairs is larger in the debiased word2vec model than in the original model (we anticipated that it should be lower). It thus seems as if the debiasing operation makes the word2vec embedding space more biased. For fastText, the number of such pairs are slightly fewer in the debiased model compared to its original counterpart. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_0",
  "x": "Pang and Lee (2005) suggested that doing so should be useful. We demonstrate that the answer is 'Yes.' Our approach is graph-based semi-supervised learning. Semi-supervised learning is an active research area in machine learning. It builds better classifiers or regressors using both labeled and unlabeled data, under appropriate assumptions (Zhu, 2005; Seeger, 2001 ). This paper contains three contributions: \u2022 We present a novel adaptation of graph-based semi-supervised learning (Zhu et al., 2003) to the sentiment analysis domain, extending past supervised learning work by<cite> Pang and Lee (2005)</cite> ; \u2022 We design a special graph which encodes our assumptions for rating-inference problems (section 2), and present the associated optimization problem in section 3;",
  "y": "extends"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_1",
  "x": "We make two assumptions: 1. We are given a similarity measure w ij \u2265 0 between documents x i and x j . w ij should be computable from features, so that we can measure similarities between any documents, including unlabeled ones. A large w ij implies that the two documents tend to express the same sentiment (i.e., rating). We experiment with positive-sentence percentage (PSP) based similarity which is proposed in <cite>(Pang and Lee, 2005)</cite> , and mutual-information modulated word-vector cosine similarity. Details can be found in section 4. 2. Optionally, we are given numerical rating predictions\u0177 l+1 , . . . ,\u0177 n on the unlabeled documents from a separate learner, for instance -insensitive support vector regression (Joachims, 1999; Smola and Sch\u00f6lkopf, 2004) used by <cite>(Pang and Lee, 2005)</cite> .",
  "y": "uses"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_2",
  "x": "2. Optionally, we are given numerical rating predictions\u0177 l+1 , . . . ,\u0177 n on the unlabeled documents from a separate learner, for instance -insensitive support vector regression (Joachims, 1999; Smola and Sch\u00f6lkopf, 2004) used by <cite>(Pang and Lee, 2005)</cite> . This acts as an extra knowledge source for our semisupervised learning framework to improve upon. We note our framework is general and works without the separate learner, too. (For this to work in practice, a reliable similarity measure is required.) We now describe our graph for the semisupervised rating-inference problem. We do this piece by piece with reference to Figure 1 . Our undirected graph G = (V, E) has 2n nodes V , and weighted edges E among some of the nodes.",
  "y": "uses"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_3",
  "x": "A large w ij implies that the two documents tend to express the same sentiment (i.e., rating). We experiment with positive-sentence percentage (PSP) based similarity which is proposed in <cite>(Pang and Lee, 2005)</cite> , and mutual-information modulated word-vector cosine similarity. Details can be found in section 4. 2. Optionally, we are given numerical rating predictions\u0177 l+1 , . . . ,\u0177 n on the unlabeled documents from a separate learner, for instance -insensitive support vector regression (Joachims, 1999; Smola and Sch\u00f6lkopf, 2004) used by <cite>(Pang and Lee, 2005)</cite> . This acts as an extra knowledge source for our semisupervised learning framework to improve upon. We note our framework is general and works without the separate learner, too. (For this to work in practice, a reliable similarity measure is required.)",
  "y": "motivation"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_4",
  "x": "This is a quadratic function in f . Setting the gradient to zero, \u2202L(f )/\u2202f = 0 , we find the minimum loss function Cy. Because C has strictly positive eigenvalues, the inverse is well defined. All our semi-supervised learning experiments use (7) in what follows. Before moving on to experiments, we note an interesting connection to the supervised learning method in <cite>(Pang and Lee, 2005)</cite> , which formulates rating inference as a metric labeling problem (Kleinberg and Tardos, 2002) . Consider a special case of our loss function (1) when b = 0 and M \u2192 \u221e. It is easy to show for labeled nodes j \u2208 L, the optimal value is the given label: f (x j ) = y j .",
  "y": "similarities"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_5",
  "x": "All our semi-supervised learning experiments use (7) in what follows. Before moving on to experiments, we note an interesting connection to the supervised learning method in <cite>(Pang and Lee, 2005)</cite> , which formulates rating inference as a metric labeling problem (Kleinberg and Tardos, 2002) . Consider a special case of our loss function (1) when b = 0 and M \u2192 \u221e. It is easy to show for labeled nodes j \u2208 L, the optimal value is the given label: f (x j ) = y j . Then the optimization problem decouples into a set of onedimensional problems, one for each unlabeled node The above problem is easy to solve. It corresponds exactly to the supervised, non-transductive version of metric labeling, except we use squared difference while <cite>(Pang and Lee, 2005)</cite> used absolute difference. Indeed in experiments comparing the two (not reported here), their differences are not statistically significant.",
  "y": "differences"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_6",
  "x": "Indeed in experiments comparing the two (not reported here), their differences are not statistically significant. From this perspective, our semisupervised learning method is an extension with interacting terms among unlabeled data. ---------------------------------- **EXPERIMENTS** We performed experiments using the movie review documents and accompanying 4-class (C = {0, 1, 2, 3}) labels found in the \"scale dataset v1.0\" available at http://www.cs.cornell.edu/people/pabo/ movie-review-data/ and first used in <cite>(Pang and Lee, 2005)</cite> . We chose 4-class instead of 3-class labeling because it is harder. The dataset is divided into four author-specific corpora, containing 1770, 902, 1307, and 1027 documents.",
  "y": "uses"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_8",
  "x": "We systematically vary labeled set size |L| \u2208 {0.9n, 800, 400, 200, 100, 50, 25, 12, 6} to observe the effect of semi-supervised learning. |L| = 0.9n is included to match 10-fold cross validation used by <cite>(Pang and Lee, 2005)</cite> . For each |L| we run 20 trials where we randomly split the corpus into labeled and test (unlabeled) sets. We ensure that all four classes are represented in each labeled set. The same random splits are used for all methods, allowing paired t-tests for statistical significance. All reported results are average test set accuracy. We compare our graph-based semi-supervised method with two previously studied methods: regression and metric labeling as in <cite>(Pang and Lee, 2005)</cite> .",
  "y": "uses"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_9",
  "x": "---------------------------------- **METRIC LABELING** We ran Pang and Lee's method based on metric labeling, using SVM regression as the initial label preference function. The method requires an itemsimilarity function, which is equivalent to our similarity measure w ij . Among others, we experimented with PSP-based similarity. For consistency with <cite>(Pang and Lee, 2005)</cite> , supervised metric labeling results with this measure are reported under 'reg+PSP. ' Note this method does not use unlabeled data for training either.",
  "y": "similarities"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_10",
  "x": "For consistency with <cite>(Pang and Lee, 2005)</cite> , supervised metric labeling results with this measure are reported under 'reg+PSP. ' Note this method does not use unlabeled data for training either. PSP i is defined in <cite>(Pang and Lee, 2005)</cite> as the percentage of positive sentences in review x i . The similarity between reviews x i , x j is the cosine angle Figure 2 : PSP for reviews expressing each fine-grain rating. We identified positive sentences using SVM instead of Na\u00efve Bayes, but the trend is qualitatively the same as in <cite>(Pang and Lee, 2005)</cite> . between the vectors (PSP i , 1\u2212PSP i ) and (PSP j , 1\u2212 PSP j ). Positive sentences are identified using a binary classifier trained on a separate \"snippet data set\" located at the same URL as above.",
  "y": "uses"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_11",
  "x": "' Note this method does not use unlabeled data for training either. PSP i is defined in <cite>(Pang and Lee, 2005)</cite> as the percentage of positive sentences in review x i . The similarity between reviews x i , x j is the cosine angle Figure 2 : PSP for reviews expressing each fine-grain rating. We identified positive sentences using SVM instead of Na\u00efve Bayes, but the trend is qualitatively the same as in <cite>(Pang and Lee, 2005)</cite> . between the vectors (PSP i , 1\u2212PSP i ) and (PSP j , 1\u2212 PSP j ). Positive sentences are identified using a binary classifier trained on a separate \"snippet data set\" located at the same URL as above. The snippet data set contains 10662 short quotations taken from movie reviews appearing on the rottentomatoes.com Web site.",
  "y": "similarities"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_12",
  "x": "Pang and Lee tuned them on a per-author basis using cross validation but did not report the optimal parameters. We were interested in learning a single set of parameters for use with all authors. In addition, since we varied labeled set size, it is convenient to tune c = k/|L|, the fraction of labeled reviews used as neighbors, instead of k. We then used the same c, \u03b1 for all authors at all labeled set sizes in experiments involving PSP. Because c is fixed, k varies directly with |L| (i.e., when less labeled data is available, our algorithm considers fewer nearby labeled examples). In an attempt to reproduce the findings in <cite>(Pang and Lee, 2005)</cite> , we tuned c, \u03b1 with cross validation. Tuning ranges are c \u2208 {0.05, 0.1, 0.15, 0.2, 0.25, 0.3} and \u03b1 \u2208 {0.01, 0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0}. The optimal parameters we found are c = 0.2 and \u03b1 = 1.5. (In section 4.4, we discuss an alternative similarity measure, for which we re-tuned these parameters.) Note that we learned a single set of shared parameters for all authors, whereas <cite>(Pang and Lee, 2005)</cite> tuned k and \u03b1 on a per-author basis.",
  "y": "similarities"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_13",
  "x": "Pang and Lee tuned them on a per-author basis using cross validation but did not report the optimal parameters. We were interested in learning a single set of parameters for use with all authors. In addition, since we varied labeled set size, it is convenient to tune c = k/|L|, the fraction of labeled reviews used as neighbors, instead of k. We then used the same c, \u03b1 for all authors at all labeled set sizes in experiments involving PSP. Because c is fixed, k varies directly with |L| (i.e., when less labeled data is available, our algorithm considers fewer nearby labeled examples). In an attempt to reproduce the findings in <cite>(Pang and Lee, 2005)</cite> , we tuned c, \u03b1 with cross validation. Tuning ranges are c \u2208 {0.05, 0.1, 0.15, 0.2, 0.25, 0.3} and \u03b1 \u2208 {0.01, 0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0}. The optimal parameters we found are c = 0.2 and \u03b1 = 1.5. (In section 4.4, we discuss an alternative similarity measure, for which we re-tuned these parameters.) Note that we learned a single set of shared parameters for all authors, whereas <cite>(Pang and Lee, 2005)</cite> tuned k and \u03b1 on a per-author basis.",
  "y": "differences"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_14",
  "x": "The best result in each row of the table is highlighted in bold. We also show in bold any results that cannot be distinguished from the best result using a paired t-test at the 0.05 level. <cite>(Pang and Lee, 2005)</cite> found that their metric labeling method, when applied to the 4-class data we are using, was not statistically better than regression, though they observed some improvement for authors (c) and (d). Using author-specific parameters, we obtained the same qualitative result, but the improvement for (c) and (d) appears even less significant in our results. Possible explanations for this difference are the fact that we derived our PSP measurements using an SVM classifier instead of an NB classifier, and that we did not use the same range of parameters for tuning. The optimal shared parameters produced almost the same results as the optimal author-specific parameters, and were used in subsequent experiments. ----------------------------------",
  "y": "background"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_15",
  "x": "**DISCUSSION** We have demonstrated the benefit of using unlabeled data for rating inference. There are several directions to improve the work: 1. We will investigate better document representations and similarity measures based on parsing and other linguistic knowledge, as well as reviews' sentiment patterns. For example, several positive sentences followed by a few concluding negative sentences could indicate an overall negative review, as observed in prior work <cite>(Pang and Lee, 2005)</cite> . 2. Our method is transductive: new reviews must be added to the graph before they can be classified. We will extend it to the inductive learning setting based on .",
  "y": "background"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_0",
  "x": "The richness of proverbs in terms of metaphors is very fascinating from a linguistic and cultural point of view. Due to this richness, proverbs constitute a challenging benchmark for existing computational models of metaphoricity. In this paper, we devise novel feature sets especially tailored to cope with the peculiarities of proverbs, which are generally short and figuratively rich. To the best of our knowledge, this is the first attempt to design a word-level metaphor recognizer specifically tailored to such metaphorically rich data. Even though some of the resources that we use (e.g., imageability and concreteness) have been used for this task before, we propose new ways of encoding this information, especially with respect to the density of the feature space and the way that the context of each word is modeled. On the proverb data, the novel features result in compact models that significantly outperform existing features designed for word-level metaphor detection in other genres <cite>(Klebanov et al., 2014)</cite> , such as news and essays. By also testing the new features on these other genres, we show that their generalization power is not limited to proverbs.",
  "y": "differences"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_1",
  "x": "**WORD-LEVEL METAPHOR DETECTION** Similarly to<cite> Klebanov et al. (2014)</cite> , we classify each content word (i.e., adjective, noun, verb or adverb) appearing in a proverb as being used metaphorically or not. Out of 1,054 proverbs in PROMETHEUS, we randomly sample 800 for training, 127 for development and 127 for testing. We carry out the development of new features on the development set; then we compare the performance of different feature sets using 10-fold cross validation on the combination of the development and training data. Finally, we test the most meaningful configurations on the held-out test data. As a baseline, we use a set of features very similar to the one proposed by<cite> Klebanov et al. (2014)</cite> . To obtain results more easily comparable with<cite> Klebanov et al. (2014)</cite>, we use the same classifier, i.e., logistic regression, in the implementation bundled with the scikit-learn package (Pedregosa et al., 2011) .",
  "y": "similarities uses"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_2",
  "x": "For all the experiments, we adjust the weight of the examples proportionally to the inverse of the class frequency. ---------------------------------- **BASELINE FEATURES (B)** Unigrams (u B ):<cite> Klebanov et al. (2014)</cite> use all content word forms as features without stemming or lemmatization. To reduce sparsity, we consider lemmas along with their POS tag. Part-of-speech (p B ): The coarse-grained part-ofspeech (i.e., noun, adjective, verb or adverb) of content words 2 . Concreteness (c B ): We extract the concreteness features from the resource compiled by Brysbaert et al. (2014) .",
  "y": "differences"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_3",
  "x": "---------------------------------- **BASELINE FEATURES (B)** Unigrams (u B ):<cite> Klebanov et al. (2014)</cite> use all content word forms as features without stemming or lemmatization. To reduce sparsity, we consider lemmas along with their POS tag. Part-of-speech (p B ): The coarse-grained part-ofspeech (i.e., noun, adjective, verb or adverb) of content words 2 . Concreteness (c B ): We extract the concreteness features from the resource compiled by Brysbaert et al. (2014) . Similarly to<cite> Klebanov et al. (2014)</cite> , the mean concreteness ratings, ranging from 1 to 5, are binned in 0.25 increments.",
  "y": "similarities"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_4",
  "x": "Among these, we can find several examples including words that belong to domains often used as a metaphor source, such as \"grist\" (domain: \"gastronomy\") in \"All is grist that comes to the mill\", or \"horse\" (domain: \"animals\") in \"You can take a horse to the water , but you can't make him drink\". Finally, Table 3 shows the effect of the different feature sets on VUAMC used by<cite> Klebanov et al. (2014)</cite> . We use the same 12-fold data split as<cite> Klebanov et al. (2014)</cite> , and also in this case we perform a grid-search to optimize the meta-parameter C of the logistic regression classifier. The best value of C identified for each genre and feature set is shown in the column labeled C. On this data, N features alone are significantly outperformed by B (p < 0.01). On the other hand, for the genres \"academic\" and \"fiction\", combining N and B features improves classification performance over B, and the difference is always statistically significant. Besides, the addition of N always leads to more balanced models, by compensating for the relatively lower precision of B. Due to the lack of a separate test set, as in the original setup by<cite> Klebanov et al. (2014)</cite> , and to the high dimensionality of B's lexicalized features, we cannot rule out over-fitting as an explanation for the relatively good performance of B on this benchmark. It should also be noted that the results reported in <cite>(Klebanov et al., 2014)</cite> are not the same, due to the mentioned differences in the implementation of the features and possibly other differences in the experimental setup (e.g., data filtering, pre-processing and meta-parameter optimization).",
  "y": "background"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_5",
  "x": "As shown by the feature ablation experi-ments, one of the main reasons for the performance difference between N and B is the ability of the former to model domain information. This finding can be further confirmed by inspecting the cases where B misclassifies metaphors that are correctly detected by N . Among these, we can find several examples including words that belong to domains often used as a metaphor source, such as \"grist\" (domain: \"gastronomy\") in \"All is grist that comes to the mill\", or \"horse\" (domain: \"animals\") in \"You can take a horse to the water , but you can't make him drink\". Finally, Table 3 shows the effect of the different feature sets on VUAMC used by<cite> Klebanov et al. (2014)</cite> . We use the same 12-fold data split as<cite> Klebanov et al. (2014)</cite> , and also in this case we perform a grid-search to optimize the meta-parameter C of the logistic regression classifier. The best value of C identified for each genre and feature set is shown in the column labeled C. On this data, N features alone are significantly outperformed by B (p < 0.01). On the other hand, for the genres \"academic\" and \"fiction\", combining N and B features improves classification performance over B, and the difference is always statistically significant.",
  "y": "similarities uses"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_6",
  "x": "On the other hand, for the genres \"academic\" and \"fiction\", combining N and B features improves classification performance over B, and the difference is always statistically significant. Besides, the addition of N always leads to more balanced models, by compensating for the relatively lower precision of B. Due to the lack of a separate test set, as in the original setup by<cite> Klebanov et al. (2014)</cite> , and to the high dimensionality of B's lexicalized features, we cannot rule out over-fitting as an explanation for the relatively good performance of B on this benchmark. It should also be noted that the results reported in <cite>(Klebanov et al., 2014)</cite> are not the same, due to the mentioned differences in the implementation of the features and possibly other differences in the experimental setup (e.g., data filtering, pre-processing and meta-parameter optimization). In particular, our implementation of the B features performs better than reported by<cite> Klebanov et al. (2014)</cite> on all four genres, namely: 0.52 vs. 0.51 for \"news\", 0.51 vs. 0.28 for \"academic\", 0.39 vs. 0.28 for \"conversation\" and 0.42 vs. 0.33 for \"fiction\". Even though the evidence is not conclusive, these results suggest that the insights derived from the analysis of PROMETHEUS and captured by the feature set N can also be applied to model word-level metaphor detection across very different genres. In particular, we believe that our initial attempt to encode context and domain information for metaphor detection deserves further investigation. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_0",
  "x": "Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008) , dual decomposition , or multi-commodity flows (Martins et al., 2009<cite> (Martins et al., , 2011</cite> . These are all instances of turbo parsers, as shown by Martins et al. (2010) : the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects. While this line of research has led to accuracy gains, none of these parsers use third-order contexts, and their speeds are well behind those of projective parsers. This paper bridges the gap above by presenting the following contributions: \u2022 We apply the third-order feature models of to non-projective parsing. \u2022 This extension is non-trivial since exact dynamic programming is not applicable. Instead, we adapt AD 3 , the dual decomposition algorithm proposed by<cite> Martins et al. (2011)</cite> , to handle third-order features, by introducing specialized head automata.",
  "y": "background"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_1",
  "x": "These are all instances of turbo parsers, as shown by Martins et al. (2010) : the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects. While this line of research has led to accuracy gains, none of these parsers use third-order contexts, and their speeds are well behind those of projective parsers. This paper bridges the gap above by presenting the following contributions: \u2022 We apply the third-order feature models of to non-projective parsing. \u2022 This extension is non-trivial since exact dynamic programming is not applicable. Instead, we adapt AD 3 , the dual decomposition algorithm proposed by<cite> Martins et al. (2011)</cite> , to handle third-order features, by introducing specialized head automata. \u2022 We make our parser substantially faster than the many-components approach of<cite> Martins et al. (2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_2",
  "x": "Instead, we adapt AD 3 , the dual decomposition algorithm proposed by<cite> Martins et al. (2011)</cite> , to handle third-order features, by introducing specialized head automata. \u2022 We make our parser substantially faster than the many-components approach of<cite> Martins et al. (2011)</cite> . While AD 3 requires solving quadratic subproblems as an intermediate step, recent results (Martins et al., 2012) show that they can be addressed with the same oracles used in the subgradient method . This enables AD 3 to exploit combinatorial subproblems like the the head automata above. Along with this paper, we provide a free distribution of our parsers, including training code. 1 ----------------------------------",
  "y": "differences"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_3",
  "x": "**DEPENDENCY PARSING WITH AD** ---------------------------------- **3** Dual decomposition is a class of optimization techniques that tackle the dual of combinatorial Figure 1 : Parts considered in this paper. Firstorder models factor over arcs (Eisner, 1996; McDonald et al., 2005) , and second-order models include also consecutive siblings and grandparents (Carreras, 2007) . Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in<cite> Martins et al. (2011)</cite> , in addition to third-order features for grand-and tri-siblings . problems in a modular and extensible manner (Komodakis et al., 2007; .",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_4",
  "x": "Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in<cite> Martins et al. (2011)</cite> , in addition to third-order features for grand-and tri-siblings . problems in a modular and extensible manner (Komodakis et al., 2007; . In this paper, we employ alternating directions dual decomposition (AD 3 ;<cite> Martins et al., 2011)</cite> . Like the subgradient algorithm of , AD 3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables. The difference is that the AD 3 subproblems have an additional quadratic term to accelerate consensus. Recent analysis (Martins et al., 2012 ) has shown that: (i) AD 3 converges at a faster rate, 2 and (ii) the quadratic subproblems can be solved using the same combinatorial machinery that is used in the subgradient algorithm. This opens the door for larger subproblems (such as the combination of trees and head automata in instead of a many-components approach <cite>(Martins et al., 2011)</cite> , while still enjoying faster convergence.",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_5",
  "x": "Like the subgradient algorithm of , AD 3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables. The difference is that the AD 3 subproblems have an additional quadratic term to accelerate consensus. Recent analysis (Martins et al., 2012 ) has shown that: (i) AD 3 converges at a faster rate, 2 and (ii) the quadratic subproblems can be solved using the same combinatorial machinery that is used in the subgradient algorithm. This opens the door for larger subproblems (such as the combination of trees and head automata in instead of a many-components approach <cite>(Martins et al., 2011)</cite> , while still enjoying faster convergence. ---------------------------------- **OUR SETUP** Given a sentence with L words, to which we prepend a root symbol $, let A := { h, m | h \u2208 {0, . .",
  "y": "motivation differences background"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_6",
  "x": "**OUR SETUP** Given a sentence with L words, to which we prepend a root symbol $, let A := { h, m | h \u2208 {0, . . . , L}, m \u2208 {1, . . . , L}, h = m} be the set of possible dependency arcs. We parameterize a dependency tree via an indicator vector u := u a a\u2208A , where u a is 1 if the arc a is in the tree, and 0 otherwise, and we denote by Y \u2286 R |A| the set of such vectors that are indicators of well-2 Concretely, AD 3 needs O(1/ ) iterations to converge to a -accurate solution, while subgradient needs O(1/ 2 ). formed trees. Let {A s } S s=1 be a cover of A, where each A s \u2286 A. We assume that the score of a parse tree u \u2208 Y decomposes as f (u) := S s=1 f s (z s ), where each z s := z s,a a\u2208As is a \"partial view\" of u, and each local score function f s comes from a feature-based linear model. Past work in dependency parsing considered either (i) a few \"large\" components, such as trees and head automata (Smith and Eisner, 2008; , or (ii) many \"small\" components, coming from a multi-commodity flow formulation (Martins et al., 2009<cite> (Martins et al., , 2011</cite> ).",
  "y": "background"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_7",
  "x": "Following<cite> Martins et al. (2011)</cite> , the problem of obtaining the best-scored tree can be written as follows: where the equality constraint ensures that the partial views \"glue\" together to form a coherent parse tree. 3 ---------------------------------- **DUAL DECOMPOSITION AND AD 3** Dual decomposition methods dualize out the equality constraint in Eq. 1 by introducing Lagrange multipliers \u03bb s,a . In doing so, they solve a relaxation where the combinatorial sets Y s are replaced by their convex hulls Z s := conv(Y s ).",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_8",
  "x": "The convex hull of Ys is the set conv(Ys) := { y s \u2208Ys \u03b1y s y s | \u03b1 \u2208 \u2206 |Ys| }. Its members represent marginal probabilities over the arcs in As. The AD 3 algorithm <cite>(Martins et al., 2011)</cite> alternates among the following iterative updates: \u2022 z-updates, which decouple over s = 1, . . . , S, and solve a penalized version of Eq. 2: Above, \u03c1 is a constant and the quadratic term penalizes deviations from the current global solution (stored in u (t) ). 5 We will see (Prop. 2) that this problem can be solved iteratively using only the Local-Max Oracle (Eq. 2). \u2022 u-updates, a simple averaging operation:",
  "y": "background"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_9",
  "x": "\u2022 \u03bb-updates, where the Lagrange multipliers are adjusted to penalize disagreements: In sum, the only difference between AD 3 and the subgradient method is in the z-updates, which in AD 3 require solving a quadratic problem. While closed-form solutions have been developed for some specialized components <cite>(Martins et al., 2011)</cite> , this problem is in general more difficult than the one arising in the subgradient algorithm. However, the following result, proved in Martins et al. (2012) , allows to expand the scope of AD 3 to any problem which satisfies Assumption 1. Proposition 2. The problem in Eq. 3 admits a solution z * s which is spanned by a sparse basis W \u2286 Y s with cardinality at most |W| \u2264 O(|A s |). In other words, there is a distribution \u03b1 with support in W such that z * s = y s \u2208W \u03b1 y s y s . 6 Prop.",
  "y": "motivation background"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_10",
  "x": "5 We will see (Prop. 2) that this problem can be solved iteratively using only the Local-Max Oracle (Eq. 2). \u2022 u-updates, a simple averaging operation: \u2022 \u03bb-updates, where the Lagrange multipliers are adjusted to penalize disagreements: In sum, the only difference between AD 3 and the subgradient method is in the z-updates, which in AD 3 require solving a quadratic problem. While closed-form solutions have been developed for some specialized components <cite>(Martins et al., 2011)</cite> , this problem is in general more difficult than the one arising in the subgradient algorithm. However, the following result, proved in Martins et al. (2012) , allows to expand the scope of AD 3 to any problem which satisfies Assumption 1. Proposition 2. The problem in Eq. 3 admits a solution z * s which is spanned by a sparse basis W \u2286 Y s with cardinality at most |W| \u2264 O(|A s |).",
  "y": "extends"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_11",
  "x": "This corresponds to the following component function (for the right-side case): Again, each of these functions can be maximized Sequential head bigram model. Head bigrams can be captured with a simple sequence model: Each score \u03c3 HB (m, h, h ) is obtained via features that look at the heads of consecutive words (as in<cite> Martins et al. (2011)</cite> ). This function can be maximized in time O(L 3 ) with the Viterbi algorithm. Arbitrary siblings.",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_12",
  "x": "This function can be maximized in time O(L 3 ) with the Viterbi algorithm. Arbitrary siblings. We handle arbitrary siblings as in<cite> Martins et al. (2011)</cite> , defining O(L 3 ) component functions of the form f ASIB h,m,s (z h,m , z h,s ) = \u03c3 ASIB (h, m, s). In this case, the quadratic problem in Eq. 3 can be solved directly in constant time. Tab. 1 details the time complexities of each subproblem. Without pruning, each iteration of AD 3 has O(L 4 ) runtime.",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_13",
  "x": "We first evaluated our non-projective parser in a projective English dataset, to see how its speed and accuracy compares with recent projective parsers, which can take advantage of dynamic programming. To this end, we converted the Penn Treebank to dependencies through (i) the head rules of Yamada and Matsumoto (2003) (PTB-YM) and (ii) basic dependencies from the Stanford parser 2.0.5 (PTB-S). 11 We trained by running 10 epochs of cost-augmented MIRA (Crammer et al., 2006) . To ensure valid parse trees at test time, we rounded fractional solutions as in Martins et al. (2009 )-yet, solutions were integral \u2248 95% of the time. Tab. 2 shows the results in the dev-set (top block) and in the test-set (two bottom blocks). In the dev-set, we see consistent gains when more expressive features are added, the best accuracies being achieved with the full third-order model; this comes at the cost of a 6-fold drop in runtime compared with a first-order model. By looking at the two bottom blocks, we observe that our parser has slightly better accuracies than recent projective parsers, with comparable speed levels (with the exception of the highly optimized vine cascade approach of Rush and Petrov, 2012 Martins et al. (2010<cite> Martins et al. ( , 2011</cite> , , Rush and Petrov (2012) , Zhang and McDonald (2012) .",
  "y": "similarities"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_0",
  "x": "One line of work has tackled the problem using partially observable Markov decision processes and reinforcement learning with carefully designed action spaces (Young et al., 2013) . However, the large, hand-designed action and state spaces make this class of models brittle and unscalable, and in practice most deployed dialogue systems remain hand-written, rule-based systems. Recently, neural network models have achieved success on a variety of natural language processing tasks (Bahdanau et al., 2015; Sutskever et al., 2014; Vinyals et al., 2015) , due to their ability to implicitly learn powerful distributed representations from data in an end-to-end trainable fashion. This paper extends recent work examining the utility of distributed state representations for taskoriented dialogue agents, without providing rules or manually tuning features. One prominent line of recent neural dialogue work has continued to build systems with modularly-connected representation, belief state, and generation components (Wen et al., 2016b) . These models must learn to explicitly represent user intent through intermediate supervision, and hence suffer from not being truly end-to-end trainable. Other work stores dialogue context in a memory module and repeatedly queries and reasons about this context to select an adequate system response<cite> (Bordes and Weston, 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_1",
  "x": "Other work stores dialogue context in a memory module and repeatedly queries and reasons about this context to select an adequate system response<cite> (Bordes and Weston, 2016)</cite> . While reasoning over memory is appealing, these models simply choose among a set of utterances rather than generating text and also must have temporal dialogue features explicitly encoded. However, the present literature lacks results for now standard sequence-to-sequence architectures, and we aim to fill this gap by building increasingly complex models of text generation, starting with a vanilla sequence-to-sequence recurrent architecture. The result is a simple, intuitive, and highly competitive model, which outperforms the more complex model of<cite> Bordes and Weston (2016)</cite> by 6.9%. Our contributions are as follows: 1) We perform a systematic, empirical analysis of increasingly complex sequence-to-sequence models for task-oriented dialogue, and 2) we develop a recurrent neural dialogue architecture augmented with an attention-based copy mechanism that is able to significantly outperform more complex models on a variety of metrics on realistic data. We use neural encoder-decoder architectures to frame dialogue as a sequence-to-sequence learning problem. Given a dialogue between a user (u) and a system (s), we represent the dialogue utterances as {(u 1 , s 1 ), (u 2 , s 2 ), . . . , (u k , s k )} where k denotes the number of turns in the dialogue.",
  "y": "differences"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_2",
  "x": "Other cited work using the DSTC2 dataset <cite>(Bordes and Weston, 2016</cite>; Liu and Perez, 2016; Seo et al., 2016) implement similar mechanisms whereby they expand the feature representations of candidate system responses based on whether there is lexical entity class matching with provided dialogue context. In these works, such features are referred to as match features. All of our architectures use an LSTM cell as the recurrent unit (Hochreiter and Schmidhuber, 1997) with a bias of 1 added to the forget gate in the style of (Pham et al., 2014) . ---------------------------------- **EXPERIMENTS** ---------------------------------- **DATA**",
  "y": "background"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_3",
  "x": "For our experiments, we used dialogues extracted from the Dialogue State Tracking Challenge 2 (DSTC2) (Henderson et al., 2014) , a restaurant reservation system dataset. While the goal of the original challenge was building a system for inferring dialogue state, for our study, we use the version of the data from<cite> Bordes and Weston (2016)</cite> , which ignores the dialogue state annotations, using only the raw text of the dialogues. The raw text includes user and system utterances as well as the API calls the system would make to the underlying KB in response to the user's queries. Our model then aims to predict both these system utterances and API calls, each of which is regarded as a turn of the dialogue. We use the train/validation/test splits from this modified version of the dataset. The dataset is appealing for a number of reasons: 1) It is derived from a real-world system so it presents the kind of linguistic diversity and conversational abilities we would hope for in an effective dialogue agent. 2) It is grounded via an underlying knowledge base of restaurant entities and their attributes.",
  "y": "uses"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_4",
  "x": "**METRICS** Evaluation of dialogue systems is known to be difficult . We employ several metrics for assessing specific aspects of our model, drawn from previous work: \u2022 Per-Response Accuracy:<cite> Bordes and Weston (2016)</cite> report a per-turn response accuracy, which tests their model's ability to select the system response at a certain timestep. Their system does a multiclass classification over a predefined candidate set of responses, which was created by aggregating all system responses seen in the training, validation, and test sets. Our model actually generates each individual token of the response, and we consider a prediction to be correct only if every token of the model output matches the corresponding token in the gold response. Evaluating using this metric on our model is therefore significantly more stringent a test than for the model of<cite> Bordes and Weston (2016)</cite> . \u2022 Per-Dialogue Accuracy:<cite> Bordes and Weston (2016)</cite> also report a per-dialogue accuracy, which assesses their model's ability to produce every system response of the dialogue correctly.",
  "y": "uses"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_5",
  "x": "---------------------------------- **METRICS** Evaluation of dialogue systems is known to be difficult . We employ several metrics for assessing specific aspects of our model, drawn from previous work: \u2022 Per-Response Accuracy:<cite> Bordes and Weston (2016)</cite> report a per-turn response accuracy, which tests their model's ability to select the system response at a certain timestep. Their system does a multiclass classification over a predefined candidate set of responses, which was created by aggregating all system responses seen in the training, validation, and test sets. Our model actually generates each individual token of the response, and we consider a prediction to be correct only if every token of the model output matches the corresponding token in the gold response. Evaluating using this metric on our model is therefore significantly more stringent a test than for the model of<cite> Bordes and Weston (2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_6",
  "x": "\u2022 Per-Dialogue Accuracy:<cite> Bordes and Weston (2016)</cite> also report a per-dialogue accuracy, which assesses their model's ability to produce every system response of the dialogue correctly. We calculate a similar value of dialogue accuracy, though again our model generates every token of every response. \u2022 BLEU: We use the BLEU metric, commonly employed in evaluating machine translation systems (Papineni et al., 2002) , which has also been used in past literature for evaluating dialogue systems (Ritter et al., 2011; . We calculate average BLEU score over all responses generated by the system, and primarily report these scores to gauge our model's ability to accurately generate the language patterns seen in DSTC2. \u2022 Entity F 1 : Each system response in the test data defines a gold set of entities. To compute an entity F 1 , we micro-average over the entire set of system dialogue responses. This metric evaluates the model's ability to generate relevant entities from the underlying knowledge base and to capture the semantics of the user-initiated dialogue flow.",
  "y": "similarities differences"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_7",
  "x": "---------------------------------- **RESULTS** In Table 2 , we present the results of our models compared to the reported performance of the best performing model of<cite> (Bordes and Weston, 2016)</cite> , which is a variant of an end-to-end memory network (Sukhbaatar et al., 2015) . Their model is referred to as MemNN. We also include the model of (Liu and Perez, 2016) , referred to as GMemNN, and the model of (Seo et al., 2016) , referred to as QRN, which currently is the stateof-the-art. In the table, Seq2Seq refers to our vanilla encoder-decoder architecture with (1), (2), and (3) LSTM layers respectively. +Attn refers to a 1-layer Seq2Seq with attention-based decoding.",
  "y": "motivation"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_0",
  "x": "To address this shortcoming, (Lao et al., 2012) augmented the knowledge graph with paths obtained from an external corpus. The added paths consisted of unlexicalized dependency labels obtained from a dependency parsed external corpus. To improve the expressivity of the added paths, instead of the unlexicalized labels, <cite>(Gardner et al., 2013)</cite> augmented the KB graph with verbs (surface relations) from a corpus containing over 600 million Subject-Verb-Object (SVO) triples. These verbs act as edges that connect previously unconnected entities thereby increasing the connectivity of the KB graph which can potentially improve PRA performance. However, na\u00efvely adding these edges increases the feature sparsity which degrades the discriminative ability of the logistic regression classifier used in PRA. This can be addressed by adding latent relations obtained by clustering the surface relations, instead of directly adding the surface relations. This reduces feature sparsity and has been shown to improve PRA inference <cite>(Gardner et al., 2013)</cite> , (Gardner et al., 2014) .",
  "y": "background"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_1",
  "x": "This can be addressed by adding latent relations obtained by clustering the surface relations, instead of directly adding the surface relations. This reduces feature sparsity and has been shown to improve PRA inference <cite>(Gardner et al., 2013)</cite> , (Gardner et al., 2014) . In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in <cite>(Gardner et al., 2013)</cite> and (Gardner et al., 2014) , which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner.",
  "y": "background"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_2",
  "x": "This can be addressed by adding latent relations obtained by clustering the surface relations, instead of directly adding the surface relations. This reduces feature sparsity and has been shown to improve PRA inference <cite>(Gardner et al., 2013)</cite> , (Gardner et al., 2014) . In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in <cite>(Gardner et al., 2013)</cite> and (Gardner et al., 2014) , which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner.",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_3",
  "x": "We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB <cite>(Gardner et al., 2013)</cite> , and vector space random walk PRA (Gardner et al., 2014) are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in <cite>(Gardner et al., 2013</cite>; Gardner et al., 2014) . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than <cite>(Gardner et al., 2013)</cite> and nearly the same prediction performance as provided by (Gardner et al., 2014) , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda.",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_4",
  "x": "In contrast, the previous approaches of adding edges or embeddings to the KB <cite>(Gardner et al., 2013)</cite> , and vector space random walk PRA (Gardner et al., 2014) are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in <cite>(Gardner et al., 2013</cite>; Gardner et al., 2014) . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than <cite>(Gardner et al., 2013)</cite> and nearly the same prediction performance as provided by (Gardner et al., 2014) , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. ---------------------------------- **RELATED WORK**",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_5",
  "x": "Our experiments suggest that ODA provides better performance than <cite>(Gardner et al., 2013)</cite> and nearly the same prediction performance as provided by (Gardner et al., 2014) , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. ---------------------------------- **RELATED WORK** Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992) , (Brin, 1999) , (Etzioni et al., 2004) . However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011) .",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_6",
  "x": "The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011) . It was extended by (Lao et al., 2012) , to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in <cite>(Gardner et al., 2013)</cite> . Instead of hard mapping of surface relations to latent embeddings, (Gardner et al., 2014 ) perform a 'soft' mapping using vector space random walks. This allows the random walker to traverse an edge semantically similar to the current edge type more frequently than other edges. Although, like others, we too use an external corpus to augment the KB, the crucial difference in our approach is that apart from adding surface relations, we also add bridging entities that enable us to create new paths in the KB. Furthermore, the procedure is targeted so that only paths that play a part in inferring the relations that are of interest are added.",
  "y": "background"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_7",
  "x": "It was extended by (Lao et al., 2012) , to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in <cite>(Gardner et al., 2013)</cite> . Instead of hard mapping of surface relations to latent embeddings, (Gardner et al., 2014 ) perform a 'soft' mapping using vector space random walks. This allows the random walker to traverse an edge semantically similar to the current edge type more frequently than other edges. Although, like others, we too use an external corpus to augment the KB, the crucial difference in our approach is that apart from adding surface relations, we also add bridging entities that enable us to create new paths in the KB. Furthermore, the procedure is targeted so that only paths that play a part in inferring the relations that are of interest are added. Thus, the number of paths added in this manner is much lower than the number of surface relations added using the procedure in <cite>(Gardner et al., 2013)</cite> .",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_9",
  "x": "<cite>PRA-SVO</cite> and PRA-VS are the systems proposed in <cite>(Gardner et al., 2013)</cite> and (Gardner et al., 2014) respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner. ---------------------------------- **PRA ON-DEMAND AUGMENTATION** (PRA-ODA) Training: Let s and t be any two KB entities and let s (n) and t (n) be their corresponding noun phrase representations or aliases. We search for bridging entities x 1 , x 2 , ..x n by performing limited depth first search (DFS) starting with s n such that we obtain a path s",
  "y": "background"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_10",
  "x": "**<cite>PRA-SVO</cite> AND PRA-VS** <cite>PRA-SVO</cite> and PRA-VS are the systems proposed in <cite>(Gardner et al., 2013)</cite> and (Gardner et al., 2014) respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner. ---------------------------------- **PRA ON-DEMAND AUGMENTATION** (PRA-ODA) Training: Let s and t be any two KB entities and let s (n) and t (n) be their corresponding noun phrase representations or aliases.",
  "y": "differences motivation"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_12",
  "x": "Improvements in PRA-ODA over <cite>PRA-SVO</cite> is statistically significant with p < 0.007, with <cite>PRA-SVO</cite> as null hypothesis. classifier. Query Time: The set of target entities corresponding to a source entity and the relation being predicted is not available during query (test) time. We use all the entities included in the range of the relation being predicted as candidate target entities. For example, if the relation is riverFlowsThroughCity, the candidate target set would include entities in the KB that are cities. The DFS is now performed starting from source entities as during training, but this time only restricting to paths with positive weights learned during training. Any path (along with bridging entities) found during this search are added to the KB, and the PRA algorithm is now run over this augmented graph.",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_14",
  "x": "Between the two top performing systems, i.e., PRA-ODA and PRA-VS, PRA-ODA is faster by a factor of 1.8. and journalistwritesforpublication). The hyperparameter values d max = 2, K = 10 reported the highest MRR and were used for the rest of the relations. For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (<cite>Gardner et al., 2013</cite>; Gardner et al., 2014) , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (<cite>PRA-SVO</cite>) (<cite>Gardner et al., 2013</cite>) and vector space random walk PRA (PRA-VS) (Gardner et al., 2014) . The run times, i.e, the time taken to perform an entire experiment for <cite>PRA-SVO</cite> and PRA-VS includes the time taken to augment NELL KB with SVO edges.",
  "y": "similarities uses"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_16",
  "x": "The run times, i.e, the time taken to perform an entire experiment for <cite>PRA-SVO</cite> and PRA-VS includes the time taken to augment NELL KB with SVO edges. The PRA-VS runtime also includes the time taken for generating embeddings to perform the vector space random walk. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, PRA-VS takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of <cite>PRA-SVO</cite> and PRA-VS, and embedding computation in case of PRA-VS are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing. Runtime gains with PRA-ODA are likely to be even more pronounced in such settings.",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_17",
  "x": "We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (<cite>PRA-SVO</cite>) (<cite>Gardner et al., 2013</cite>) and vector space random walk PRA (PRA-VS) (Gardner et al., 2014) . The run times, i.e, the time taken to perform an entire experiment for <cite>PRA-SVO</cite> and PRA-VS includes the time taken to augment NELL KB with SVO edges. The PRA-VS runtime also includes the time taken for generating embeddings to perform the vector space random walk. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, PRA-VS takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of <cite>PRA-SVO</cite> and PRA-VS, and embedding computation in case of PRA-VS are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing.",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_18",
  "x": "The PRA-VS runtime also includes the time taken for generating embeddings to perform the vector space random walk. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, PRA-VS takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of <cite>PRA-SVO</cite> and PRA-VS, and embedding computation in case of PRA-VS are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing. Runtime gains with PRA-ODA are likely to be even more pronounced in such settings. An additional advantage of the proposed algorithm is that it can also be run on the top of any PRA based algorithm such as the <cite>PRA-SVO</cite> and PRA-VS.",
  "y": "differences"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_0",
  "x": "Next, for SNLI, we compare our shortcutstacked encoder with the current state-of-the-art encoders from the SNLI leaderboard (https:// nlp.stanford.edu/projects/snli/). We also compare to the recent biLSTM-Max Encoder of <cite>Conneau et al. (2017)</cite> , which served as our model's 1-layer starting point. 1 The results indicate that 'Our Shortcut-Stacked Encoder' sur-passes all the previous state-of-the-art encoders, and achieves the new best encoding-based result on SNLI, suggesting the general effectiveness of simple shortcut-connected stacked layers in sentence encoders. ---------------------------------- **CONCLUSION** We explored various simple combinations and connections of biLSTM-RNN layered architectures and developed a Shortcut-Stacked Sentence Encoder for natural language inference. Our model is the top single result in the EMNLP RepEval 2017 Multi-NLI Shared Task, and it also surpasses the state-of-the-art encoders for the SNLI dataset.",
  "y": "uses"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_1",
  "x": "Next, for SNLI, we compare our shortcutstacked encoder with the current state-of-the-art encoders from the SNLI leaderboard (https:// nlp.stanford.edu/projects/snli/). We also compare to the recent biLSTM-Max Encoder of <cite>Conneau et al. (2017)</cite> , which served as our model's 1-layer starting point. 1 The results indicate that 'Our Shortcut-Stacked Encoder' sur-passes all the previous state-of-the-art encoders, and achieves the new best encoding-based result on SNLI, suggesting the general effectiveness of simple shortcut-connected stacked layers in sentence encoders. ---------------------------------- **CONCLUSION** We explored various simple combinations and connections of biLSTM-RNN layered architectures and developed a Shortcut-Stacked Sentence Encoder for natural language inference. Our model is the top single result in the EMNLP RepEval 2017 Multi-NLI Shared Task, and it also surpasses the state-of-the-art encoders for the SNLI dataset.",
  "y": "differences"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_2",
  "x": "In this paper, we follow the former approach of encoding-based models, and propose a novel yet simple sequential sentence encoder for the Multi-NLI problem. Our encoder does not require any syntactic information of the sentence. It also does not contain any attention or memory structure. It is basically a stacked (multi-layered) bidirectional LSTM-RNN with shortcut connections (feeding all previous layers' outputs and word embeddings to each layer) and word embedding fine-tuning. The overall supervised model uses these shortcutstacked encoders to encode two input sentences into two vectors, and then we use a classifier over the vector combination to label the relationship between these two sentences as that of entailment, contradiction, or neural (similar to the classifier setup of Bowman et al. (2015) and <cite>Conneau et al. (2017)</cite> ). Our simple shortcut-stacked encoders achieve strong improvements over existing encoders due to its multi-layered and shortcutconnected properties, on both matched and mis- matched evaluation settings for multi-domain natural language inference, as well as on the original SNLI dataset. It is the top single-model (nonensemble) result in the EMNLP RepEval 2017 Multi-NLI Shared Task , and the new state-of-the-art for encoding-based results on the SNLI dataset (Bowman et al., 2015) .",
  "y": "similarities uses"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_3",
  "x": "Github Code Link: https://github.com/ easonnie/multiNLI_encoder 2 Model Our model mainly consists of two separate components, a sentence encoder and an entailment classifier. The sentence encoder compresses each source sentence into a vector representation and the classifier makes a three-way classification based on the two vectors of the two source sentences. The model follows the 'encoding-based rule', i.e., the encoder will encode each source sentence into a fixed length vector without any information or function based on the other sentence (e.g., cross-attention or memory comparing the two sentences). In order to fully explore the generalization of the sentence encoder, the same encoder is applied to both the premise and the hypothesis with shared parameters projecting them into the same space. This setting follows the idea of Siamese Networks in Bromley et al. (1994) . Figure 1 shows the overview of our encoding model (the standard classifier setup is not shown here; see Bowman et al. (2015) and <cite>Conneau et al. (2017)</cite> for that).",
  "y": "background"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_4",
  "x": "In our settings, the input sequences for the ith biLSTM layer are the concatenated outputs of all the previous layers, plus the original word embedding sequence. This gives a shortcut connection style setup, related to the widely used idea of residual connections in CNNs for computer vision (He et al., 2016) , highway networks for RNNs in speech processing , and shortcut connections in hierarchical multitasking learning (Hashimoto et al., 2016) ; but in our case we feed in all the previous layers' output se-quences as well as the word embedding sequence to every layer. Let W = (w 1 , w 2 , ..., w n ) represent words in the source sentence. We assume w i \u2208 R d is a word embedding vector which are initialized using some pre-trained vector embeddings (and is then fine-tuned end-to-end via the NLI supervision). Then, the input of ith biLSTM layer at time t is defined as: Then, assuming we have m layers of biLSTM, the final vector representation will be obtained by applying row-max-pool over the output of the last biLSTM layer, similar to <cite>Conneau et al. (2017)</cite> . The final layer is defined as:",
  "y": "similarities uses"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_5",
  "x": "We assume w i \u2208 R d is a word embedding vector which are initialized using some pre-trained vector embeddings (and is then fine-tuned end-to-end via the NLI supervision). Then, the input of ith biLSTM layer at time t is defined as: Then, assuming we have m layers of biLSTM, the final vector representation will be obtained by applying row-max-pool over the output of the last biLSTM layer, similar to <cite>Conneau et al. (2017)</cite> . The final layer is defined as: , d m is the dimension of the hidden state of the last forward and backward LSTM layers, and v is the final vector representation for the source sentence (which is later fed to the NLI classifier). The closest encoder architecture to ours is that of <cite>Conneau et al. (2017)</cite> , whose model consists of a single-layer biLSTM with a max-pooling layer, which we treat as our starting point. Our experiments (Section 4) demonstrate that our enhancements of the stacked-biRNN with shortcut connections provide significant gains on top of this baseline (for both SNLI and Multi-NLI).",
  "y": "similarities extends"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_7",
  "x": "First, Table 1 shows the performance changes for different number of biLSTM layers and their varying dimension size. The dimension size of a biLSTM layer is referring to the dimension of the hidden state for both the forward and backward LSTM-RNNs. As shown, each added layer model improves the accuracy and we achieve a substantial improvement in accuracy (around 2%) on both matched and mismatched settings, compared to the single-layer biLSTM in <cite>Conneau et al. (2017)</cite> . We only experimented with up to 3 layers with 512, 1024, 2048 dimensions each, so the model still has potential to improve the result further with a larger dimension and more layers. Next, in Table 2 , we show that the shortcut connections among the biLSTM layers is also an important contributor to accuracy improvement (around 1.5% on top of the full 3-layered stacked-RNN model). This demonstrates that simply stacking the biLSTM layers is not sufficient to handle a complex task like Multi-NLI and it is significantly better to have the higher layer connected to both the output and the original input of all the previous layers (note that Table 1 results are based on multi-layered models with shortcut connections). Next, in Table 3 , we show that fine-tuning the word embeddings also improves results, again for both the in-domain task and cross-domain tasks (the ablation results are based on a smaller model with a 128+256 2-layer biLSTM).",
  "y": "differences"
 },
 {
  "id": "c78464cfe0fc44f5fd0da2e4f9d90e_0",
  "x": "In exploring these questions, I attempt to answer the overarching question of whether social media data is the friend or foe of NLP. I approach the question first from the perspective of what challenges social media language poses for NLP. The most immediate answer is the infamously free-form nature of language in social media, encompassing spelling inconsistencies, the free-form adoption of new terms, and regular violations of English grammar norms. Unsurprisingly, when NLP tools are applied directly to social media data, the results tend to be miserable when compared to data sets such as the Wall Street Journal component of the Penn Treebank. However, there have been recent successes in adapting parsers and POS taggers to social media data (Foster et al., 2011; <cite>Gimpel et al., 2011)</cite> . Additionally, lexical normalisation and other preprocessing strategies have been shown to enhance the performance of NLP tools over social media data (Lui and Baldwin, 2012; Han et al., to appear) . Furthermore, social media posts tend to be short and the content highly varied, meaning it is difficult to adapt a tool to the domain, or harness textual context to disambiguate the content.",
  "y": "background"
 },
 {
  "id": "c7e304499654516cce43c550256eae_0",
  "x": "Prior work on low-resource NLP has primarily focused on exploiting parallel corpora to project information between a high-and low-resource language (Yarowsky and Ngai, 2001; T\u00e4ckstr\u00f6m et al., 2013; Guo et al., 2015; Agi\u0107 et al., 2016; Buys and Botha, 2016) . For example, POS tags can be projected via word alignments, and the projected POS is then used to train a model in the lowresource language Zhang et al., 2016;<cite> Fang and Cohn, 2016)</cite> . These methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages. They also assume a large parallel corpus, which may not be available for many low-resource languages. To address these limitations, we propose a new technique for low resource tagging, with more modest resource requirements: 1) a bilingual dictionary; 2) monolingual corpora in the high and low resource languages; and 3) a small annotated corpus of around 1, 000 tokens in the low-resource language. The first two resources are used as a form of distant supervision through learning crosslingual word embeddings over the monolingual corpora and bilingual dictionary (Ammar et al., 2016) . Additionally, our model jointly incorporates the language-dependent information from the small set of gold annotations.",
  "y": "background"
 },
 {
  "id": "c7e304499654516cce43c550256eae_1",
  "x": "Prior work on low-resource NLP has primarily focused on exploiting parallel corpora to project information between a high-and low-resource language (Yarowsky and Ngai, 2001; T\u00e4ckstr\u00f6m et al., 2013; Guo et al., 2015; Agi\u0107 et al., 2016; Buys and Botha, 2016) . For example, POS tags can be projected via word alignments, and the projected POS is then used to train a model in the lowresource language Zhang et al., 2016;<cite> Fang and Cohn, 2016)</cite> . These methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages. They also assume a large parallel corpus, which may not be available for many low-resource languages. To address these limitations, we propose a new technique for low resource tagging, with more modest resource requirements: 1) a bilingual dictionary; 2) monolingual corpora in the high and low resource languages; and 3) a small annotated corpus of around 1, 000 tokens in the low-resource language. The first two resources are used as a form of distant supervision through learning crosslingual word embeddings over the monolingual corpora and bilingual dictionary (Ammar et al., 2016) . Additionally, our model jointly incorporates the language-dependent information from the small set of gold annotations.",
  "y": "motivation background"
 },
 {
  "id": "c7e304499654516cce43c550256eae_2",
  "x": "For example, POS tags can be projected via word alignments, and the projected POS is then used to train a model in the lowresource language Zhang et al., 2016;<cite> Fang and Cohn, 2016)</cite> . These methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages. They also assume a large parallel corpus, which may not be available for many low-resource languages. To address these limitations, we propose a new technique for low resource tagging, with more modest resource requirements: 1) a bilingual dictionary; 2) monolingual corpora in the high and low resource languages; and 3) a small annotated corpus of around 1, 000 tokens in the low-resource language. The first two resources are used as a form of distant supervision through learning crosslingual word embeddings over the monolingual corpora and bilingual dictionary (Ammar et al., 2016) . Additionally, our model jointly incorporates the language-dependent information from the small set of gold annotations. Our approach combines these two sources of supervision using multi-task learning, such that the kinds of errors that occur in cross-lingual transfer can be accounted for, and corrected automatically.",
  "y": "background differences motivation"
 },
 {
  "id": "c7e304499654516cce43c550256eae_3",
  "x": "For example, the CRFBiLSTM POS tagger obtained the state-of-theart performance on Penn Treebank WSJ corpus (Huang et al., 2015) . However, in low-resource languages, these models are seldom used because of limited labelled data. Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (Yarowsky and Ngai, 2001; T\u00e4ckstr\u00f6m et al., 2013; <cite>Fang and Cohn, 2016</cite>; Zhang et al., 2016) . Yarowsky and Ngai (2001) pioneered the use of parallel data for projecting POS tag information from one language to another language. used parallel data and exploited graph-based label propagation to expand the coverage of labelled tokens. T\u00e4ckstr\u00f6m et al. (2013) constructed tag dictionaries by projecting tag information from a highresource language to a low-resource language via alignments in the parallel text. Fang and Cohn (2016) used parallel data to obtain projected tags as distant labels and proposed a joint BiLSTM model trained on both the distant data and 1, 000 tagged tokens.",
  "y": "background"
 },
 {
  "id": "c7e304499654516cce43c550256eae_4",
  "x": "Instead we use minimal supervision to refine 'distant' labels through modelling the tag transformation, based on a small set of annotations. ---------------------------------- **MODEL** We now describe the modelling framework for POS tagging in a low-resource language, based on very limited linguistic resources. Our approach extends the work of<cite> Fang and Cohn (2016)</cite> , who present a model based on distant supervision in the form of cross-lingual projection and use projected tags generated from parallel corpora as distant annotations. There are three main differences between their work and ours: 1) We do not use parallel corpora, but instead use a bilingual dictionary for knowledge transfer. 2) Our model uses a more expressive multi-layer perceptron when generating the gold standard tags.",
  "y": "extends"
 },
 {
  "id": "c7e304499654516cce43c550256eae_5",
  "x": "To model this data we employ the same model structure as above but augmented with a second perceptron output layer, as illustrated in Figure 1 (right) . Formally,\u1ef9 t \u223c Categorial(\u00f5 t ) where\u00f5 t = MLP(o t ) is a single hidden layer perceptron with tanh activation and softmax output transformation. This component allows for a more expressive label mapping than<cite> Fang and Cohn (2016)</cite>'s linear matrix translation. Joint multi-task learning To combine the two sources of information, we use a joint objective, where N and M index the token positions in the distant and ground truth corpora, respectively, and \u03b3 is a constant balancing the two components which we set for uniform weighting, \u03b3 = |M| |N | . Consider the training effect of the true POS tags: when performing error backpropagation, the cross-entropy error signal must pass through the transformation linking\u00f5 with o, which can be seen as a language-specific step, after which the generalised error signal can be further backpropagated to the rest of the model. Active learning Given the scarcity of ground truth labels and the high cost of annotation, a natural question is whether we can optimise which text to be annotated in order achieve the high accuracy for the lowest cost.",
  "y": "differences"
 },
 {
  "id": "c7e304499654516cce43c550256eae_6",
  "x": "4 The BiLSTM layer uses 128 hidden units, and 32 hidden units for the transformation step. We used SGD with momentum to train models, with early stopping based on development performance. For benchmarks, we compare the proposed model against various state-of-the-art supervised learning methods, namely: a BILSTM tagger, BILSTM-CRF tagger (Huang et al., 2015) , and a state-of-the-art semi-supervised POS tagging algorithm, MINITAGGER (Stratos and Collins, 2015) , which is also focusing on minimising the amount of labelled data. Note these methods do not use cross-lingual supervision. For a more direct comparison, we include BILSTM-DEBIAS<cite> (Fang and Cohn, 2016)</cite> , applied using our proposed cross-lingual supervision based on dictionaries, instead of parallel corpora; accordingly the key difference is their linear transformation for the distant data, versus our non-linear transformation to the gold data. Results Table 1 reports the tagging accuracy, showing that our models consistently outperform the baseline techniques. The poor performance of the supervised methods suggests they are overfitting the small training set, however this is much less of a problem for our approach (labelled Joint).",
  "y": "differences uses"
 },
 {
  "id": "c7e304499654516cce43c550256eae_7",
  "x": "Results Table 1 reports the tagging accuracy, showing that our models consistently outperform the baseline techniques. The poor performance of the supervised methods suggests they are overfitting the small training set, however this is much less of a problem for our approach (labelled Joint). Note that distant supervision alone gives reasonable performance (labelled DISTANT) however the joint modelling of the ground truth and distant data yields significant improvements in almost all cases. BILSTM-DEBIAS<cite> (Fang and Cohn, 2016)</cite> performs worse than our proposed method, indicating that a linear transformation is insufficient for modelling distant supervision. The accuracies are higher overall for the European cf. Turkic languages, presumably because these languages are Table 1 : POS tagging accuracy on over the ten target languages, showing first approaches using only the gold data; next methods using only distant cross-lingual supervision, and lastly joint multi-task learning. English is used as the source language and columns correspond to a specific target language.",
  "y": "differences"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_0",
  "x": "The major distinction between these methods is in the contrast between the approaches based exclusively on the information contained in the text to be segmented, such as lexical repetition (e.g., <cite>Choi 2000</cite>; Hearst 1997; Heinonen 1998; Kehagias, Pavlina, and Petridis 2003; Utiyama and Isahara 2001) , and those approaches that rest on complementary semantic knowledge extracted from dictionaries and thesauruses (e.g., Kozima 1993; Lin et al. 2004; Morris and Hirst 1991) , or from collocations collected in large corpora (Bolshakov and Gelbukh 2001; Brants, Chen, and Tsochantaridis 2002; Choi et al. 2001; Ferret 2002; Kaufmann 1999; Ponte and Croft 1997) . According to their authors, methods that use additional knowledge allow for a solution to problems encountered when sentences belonging to a unique topic do not share common words due to the use of hyperonyms or synonyms and allow words that are semantically related to be taken as positive evidence for topic continuity. Empirical arguments in favor of these methods have been provided recently by Choi et al. (2001) in a study using Latent Semantic Analysis (Latent Semantic Indexing, Deerwester et al. 1990 ) to extract a semantic space from a corpus allowing determination of the similarity of meanings of words, sentences, or paragraphs. By comparing the accuracy of the very same algorithm according to whether or not it takes into account complementary semantic knowledge, they were able to show the benefit derived from such knowledge. However, implications of Choi et al.'s study for text segmentation and for the use of LSA in natural language processing are unclear due to the methodology employed. In their experiments, semantic knowledge was acquired from a corpus containing the materials to be segmented in the test phase. One could speculate whether the largest part of the benefit obtained thanks to the addition of semantic knowledge was not due to this hyper-specificity of the LSA corpus (i.e., the inclusion of the test materials).",
  "y": "differences background"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_1",
  "x": "Before reporting these experiments, <cite>Choi's algorithm</cite> and the use of LSA within this framework are described. ---------------------------------- **THE TWO VERSIONS OF <cite>CHOI'S ALGORITHM</cite>** The segmentation algorithm proposed by <cite>Choi (2000)</cite> is made up of the three steps usually found in any segmentation procedure based on lexical cohesion. Firstly, the document to be segmented is divided into minimal textual units, usually sentences. Then, a similarity index between every pair of adjacent units is calculated. Each raw similarity value is cast on an ordinal scale by taking the proportion of neighboring values that are smaller than it.",
  "y": "background"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_3",
  "x": "Before reporting these experiments, <cite>Choi's algorithm</cite> and the use of LSA within this framework are described. ---------------------------------- **THE TWO VERSIONS OF <cite>CHOI'S ALGORITHM</cite>** The segmentation algorithm proposed by <cite>Choi (2000)</cite> is made up of the three steps usually found in any segmentation procedure based on lexical cohesion. Firstly, the document to be segmented is divided into minimal textual units, usually sentences. Then, a similarity index between every pair of adjacent units is calculated. Each raw similarity value is cast on an ordinal scale by taking the proportion of neighboring values that are smaller than it.",
  "y": "background"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_4",
  "x": "Each raw similarity value is cast on an ordinal scale by taking the proportion of neighboring values that are smaller than it. Lastly, the document is segmented recursively according to the boundaries between the units that maximize the sum of the average similarities inside the segments thus comprised (divisive clustering). The step of greatest interest here is the one that calculates the inter-sentence similarities. The procedure initially proposed by <cite>Choi (2000)</cite> , C99, rests exclusively on the information contained in the text to be segmented. According to the vector space model, each sentence is represented by a vector of word frequency count, and the similarity between two sentences is calculated by means of the cosine measure between the corresponding vectors. In a first evaluation based on the procedure described below, <cite>Choi</cite> showed that its algorithm outperforms several other approaches such as TextTiling (Hearst 1997) and Segmenter (Kan, Klavans, and McKeown 1998) . Choi et al. (2001) claimed that it was possible to improve the inter-sentence similarities index by taking into account the semantic proximities between words estimated on the basis of Latent Semantic Analysis (LSA).",
  "y": "background"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_5",
  "x": "According to the vector space model, each sentence is represented by a vector of word frequency count, and the similarity between two sentences is calculated by means of the cosine measure between the corresponding vectors. In a first evaluation based on the procedure described below, <cite>Choi</cite> showed that its algorithm outperforms several other approaches such as TextTiling (Hearst 1997) and Segmenter (Kan, Klavans, and McKeown 1998) . Choi et al. (2001) claimed that it was possible to improve the inter-sentence similarities index by taking into account the semantic proximities between words estimated on the basis of Latent Semantic Analysis (LSA). Briefly stated, LSA rests on the thesis that analyzing the contexts in which words occur permits an estimation of their similarity in meaning (Deerwester et al. 1990; Landauer and Dumais 1997) . The first step in the analysis is to construct a lexical table containing an information-theoretic weighting of the frequencies of the words occurrence in each document (i.e. sentence, paragraph, or text) included in the corpus. This frequency table undergoes a Singular Value Decomposition that extracts the most important orthogonal dimensions, and, consequently, discards the small sources of variability in term usage. After this step, every word is represented by a vector of weights indicating its strength of association with each of the dimensions.",
  "y": "background"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_6",
  "x": "**EXPERIMENT 1** The aim of this experiment is to determine the impact of the presence of the test materials in the LSA corpus on the results obtained by Choi et al. (2001) . Does semantic knowledge acquired from a corpus that does not include the test materials also improve the segmentation accuracy? ---------------------------------- **METHOD** This experiment was based on the procedure and test materials designed by <cite>Choi (2000)</cite> , which was also used by several authors as a benchmark for comparing segmentation systems (Brants et al. 2002; Ferret 2002; Kehagias et al. 2003; Utiyama and Isahara 2001) . The task consists in finding the boundaries between concatenated texts.",
  "y": "uses"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_7",
  "x": "This experiment was based on the procedure and test materials designed by <cite>Choi (2000)</cite> , which was also used by several authors as a benchmark for comparing segmentation systems (Brants et al. 2002; Ferret 2002; Kehagias et al. 2003; Utiyama and Isahara 2001) . The task consists in finding the boundaries between concatenated texts. Each test sample is a concatenation of ten text segments. Each segment consisted in the first n sentences of a randomly selected text from two sub-sections of the Brown corpus. For the present experiment, I used the most general test materials built by <cite>Choi (2000)</cite> , in which the size of the segments within each sample varies randomly from 3 to 11 sentences. It is composed of 400 samples. The analysis related to the comparison between the accuracy of the algorithm when the test materials were included in the LSA corpus (Within) and when it was not (Without).",
  "y": "uses"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_8",
  "x": "The words on Choi's stoplist were removed, as were those that appeared only once in the whole corpus. Words were not stemmed, as in Choi et al. (2001) . To build the LSA space, the singular value decomposition was realized using the program SVDPACKC (Berry 1992; Berry et al. 1993) , and the first 300 singular vectors were retained. Concerning the segmentation algorithm, I used the version in which the number of boundaries to be found is imposed, and thus fixed at nine. An 11 \u00d7 11 rank mask was used for the ordinal transformation, as recommended by <cite>Choi (2000)</cite> . ---------------------------------- **RESULTS**",
  "y": "uses"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_9",
  "x": "In this corpus, the test materials from each sample account for-on average-0.0066% of the complete corpus. This second experiment also made it possible to compare the Within and Without spaces with a Former space composed of articles published in the same newspaper, but during the years 1995 and 1996 (roughly 50,000 articles and more than 22,000,000 words). This condition will show the possibility of using LSA to build even more generic semantic knowledge, since the LSA corpus is earlier than the text to segment. ---------------------------------- **METHOD** The test materials were extracted from the 1997-1998 corpus following the guidelines given in <cite>Choi (2000)</cite> . It is composed of 400 samples of ten segments, of which the length varies randomly from 3 to 11 sentences.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_0",
  "x": "Our TRANS-BLSTM model obtains an F1 score of 94.01% on the SQuAD 1.1 development dataset, which is comparable to the state-of-the-art result. ---------------------------------- **INTRODUCTION** Learning representations (Mikolov et al., 2013) of natural language and language model pre-training <cite>(Devlin et al., 2018</cite>; Radford et al., 2019) has shown promising results recently. These pretrained models serve as generic up-stream models and they can be used to improve down-stream applications such as natural language inference, paraphrasing, named entity recognition, and question answering. The innovation of BERT <cite>(Devlin et al., 2018)</cite> comes from the \"masked language model\" with a pre-training objective, inspired by the Cloze task (Taylor, 1953) . The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original token based only on its context.",
  "y": "background"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_1",
  "x": "Learning representations (Mikolov et al., 2013) of natural language and language model pre-training <cite>(Devlin et al., 2018</cite>; Radford et al., 2019) has shown promising results recently. These pretrained models serve as generic up-stream models and they can be used to improve down-stream applications such as natural language inference, paraphrasing, named entity recognition, and question answering. The innovation of BERT <cite>(Devlin et al., 2018)</cite> comes from the \"masked language model\" with a pre-training objective, inspired by the Cloze task (Taylor, 1953) . The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original token based only on its context. Follow-up work including RoBERTa (Liu et al., 2019b) investigated hyper-parameter design choices and suggested longer model training time. In addition, XLNet (Yang et al., 2019) has been proposed to address the BERT pre-training and fine-tuning discrepancy where masked tokens were found in the former but not in the latter. Nearly all existing work suggests that a large network is crucial to achieve the state-of-the-art performance.",
  "y": "background"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_2",
  "x": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original token based only on its context. Follow-up work including RoBERTa (Liu et al., 2019b) investigated hyper-parameter design choices and suggested longer model training time. In addition, XLNet (Yang et al., 2019) has been proposed to address the BERT pre-training and fine-tuning discrepancy where masked tokens were found in the former but not in the latter. Nearly all existing work suggests that a large network is crucial to achieve the state-of-the-art performance. For example, <cite>(Devlin et al., 2018)</cite> has shown that across natural language understanding tasks, using larger hidden layer size, more hidden layers, and more attention heads always leads to better performance. However, <cite>they</cite> stop at a hidden layer size of 1024. ALBERT (Lan et al., 2019) showed that it is not the case that simply increasing the model size would lead to better accuracy.",
  "y": "background"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_3",
  "x": "However, <cite>they</cite> stop at a hidden layer size of 1024. ALBERT (Lan et al., 2019) showed that it is not the case that simply increasing the model size would lead to better accuracy. In fact, they observed that simply increasing the hidden layer size of a model such as BERT-large can lead to significantly worse performance. On the other hand, model distillation (Hinton et al., 2015; Tang et al., 2019; Sun et al., 2019; Sanh et al., 2019) has been proposed to reduce the BERT model size while maintaining high performance. In this paper, we attempt to improve the performance of BERT via architecture enhancement. BERT is based on the encoder of the transformer model (Vaswani et al., 2017) , which has been proven to obtain state-of-the-art accuracy across a broad range of NLP applications <cite>(Devlin et al., 2018)</cite> . Prior to BERT, bidirectional LSTM (BLSTM) has dominated sequential modeling for many tasks including machine translation (Chiu and Nichols, 2016) and speech recognition (Graves et al., 2013) .",
  "y": "motivation"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_4",
  "x": "For example, <cite>(Devlin et al., 2018)</cite> has shown that across natural language understanding tasks, using larger hidden layer size, more hidden layers, and more attention heads always leads to better performance. However, <cite>they</cite> stop at a hidden layer size of 1024. ALBERT (Lan et al., 2019) showed that it is not the case that simply increasing the model size would lead to better accuracy. In fact, they observed that simply increasing the hidden layer size of a model such as BERT-large can lead to significantly worse performance. On the other hand, model distillation (Hinton et al., 2015; Tang et al., 2019; Sun et al., 2019; Sanh et al., 2019) has been proposed to reduce the BERT model size while maintaining high performance. In this paper, we attempt to improve the performance of BERT via architecture enhancement. BERT is based on the encoder of the transformer model (Vaswani et al., 2017) , which has been proven to obtain state-of-the-art accuracy across a broad range of NLP applications <cite>(Devlin et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_5",
  "x": "In this paper, we attempt to answer this question by proposing a transformer BLSTM joint modeling framework. Our major contribution in this paper is two fold: 1) We propose the TRANS-BLSTM model architectures, which combine the transformer and BLSTM into one single modeling framework, leveraging the modeling capability from both the transformer and BLSTM. 2) We show that the TRANS-BLSTM models can effectively boost the accuracy of BERT baseline models on SQuAD 1.1 and GLUE NLP benchmark datasets. 2 Related work 2.1 BERT Our work focuses on improving the transformer architecture (Vaswani et al., 2017) , which motivated the recent breakthrough in language representation, BERT <cite>(Devlin et al., 2018)</cite> . Our work builds on top of the transformer architecture, integrating each transformer block with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) . Related to our work, XLNet (Yang et al., 2019) proposes twostream self-attention as opposed to single-stream self-attention used in classic transformers. With two-stream attention, XLNet can be treated as a general language model that does not suffer from the pretrain-finetune discrepancy (the mask tokens are seen during pretraining but not during finetuning) thanks to its autoregressive formulation.",
  "y": "motivation"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_6",
  "x": "We used the same multi-head selfattention from the original paper (Vaswani et al., 2017) . We used the same input and output representations, i.e., the embedding and positional encoding, and the same loss objective, i.e., masked LM prediction and next sentence prediction, from the BERT paper <cite>(Devlin et al., 2018)</cite> . ---------------------------------- **PROPOSED TRANSFORMER BIDIRECTIONAL LSTM (TRANS-BLSTM) ARCHITECTURES** Previous experiments indicated that a bidirectional LSTM model alone may not perform on par with a transformer. For example, the distillation from a transformer model to a single-layer bidirectional LSTM model (Tang et al., 2019) resulted in significantly lower accuracy. We also confirmed this on our experiments in Section 4.3.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_7",
  "x": "---------------------------------- **OBJECTIVE FUNCTIONS** Following the BERT <cite>(Devlin et al., 2018)</cite> , we use masked language model loss and next sentence prediction (NSP) loss to train the models. The masked LM (MLM) is often referred to as a Cloze task in the literature (Taylor, 1953) . The encoder output, corresponding to the mask tokens, are fed into an output softmax over the vocabulary. In our experiments, we randomly mask 15% of all whole word wordpiece tokens in each sequence (Wu et al., 2016) . We also use the next sentence prediction loss as introduced in <cite>(Devlin et al., 2018)</cite> to train our models.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_8",
  "x": "The masked LM (MLM) is often referred to as a Cloze task in the literature (Taylor, 1953) . The encoder output, corresponding to the mask tokens, are fed into an output softmax over the vocabulary. In our experiments, we randomly mask 15% of all whole word wordpiece tokens in each sequence (Wu et al., 2016) . We also use the next sentence prediction loss as introduced in <cite>(Devlin et al., 2018)</cite> to train our models. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A, and 50% of the time it is a random sentence from the corpus. We note that recent work (Yang et al., 2019; Liu et al., 2019a; Lan et al., 2019; Raffel et al., 2019) has argued that the NSP loss may not be useful in improving model accuracy. Nevertheless, we used the NSP loss in our experiments to have a fair comparison between the proposed models and the original BERT models.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_9",
  "x": "We use the same large-scale data which has been used for BERT model pre-training, the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2.5B words) (Wikipedia contributors, 2004; <cite>Devlin et al., 2018)</cite> . The two corpora consist of about 16GB of text. Following the original BERT setup <cite>(Devlin et al., 2018)</cite> , we format the inputs as \"[CLS] TRANS/BERT  108M  12  768  768  12  6.0X  Base  TRANS-BLSTM-SMALL  152M  12  768  768  12  3.3X  TRANS-BLSTM  237M  12  768  768  12  2.5X  Large TRANS/BERT  334M  24  1024  1024  16  2.8X  TRANS-BLSTM-SMALL  487M  24  1024  1024  16  1.4X  TRANS-BLSTM  789M  24  1024  1024  16  1   Table 1 : Parameter size and training speed for TRANS/BERT, TRANS-BLSTM-SMALL, and TRANS-BLSTM on base and large settings respectively. x 1 = x 11 , x 12 . . . and x 2 = x 21 , x 22 . . . are two segments. To reduce the training memory consumption, we set the maximum input length to 256 (as opposed to 512 in the original BERT paper). We note that this setting may adversely affect the best accuracy we report in our paper 1 , but the relative accuracy gain by the proposed models are still valid. Similar to BERT, we use a vocabulary size of 30k with wordpiece tokenization.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_10",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** We use the same large-scale data which has been used for BERT model pre-training, the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2.5B words) (Wikipedia contributors, 2004; <cite>Devlin et al., 2018)</cite> . The two corpora consist of about 16GB of text. Following the original BERT setup <cite>(Devlin et al., 2018)</cite> , we format the inputs as \"[CLS] TRANS/BERT  108M  12  768  768  12  6.0X  Base  TRANS-BLSTM-SMALL  152M  12  768  768  12  3.3X  TRANS-BLSTM  237M  12  768  768  12  2.5X  Large TRANS/BERT  334M  24  1024  1024  16  2.8X  TRANS-BLSTM-SMALL  487M  24  1024  1024  16  1.4X  TRANS-BLSTM  789M  24  1024  1024  16  1   Table 1 : Parameter size and training speed for TRANS/BERT, TRANS-BLSTM-SMALL, and TRANS-BLSTM on base and large settings respectively. x 1 = x 11 , x 12 . . . and x 2 = x 21 , x 22 . . . are two segments. To reduce the training memory consumption, we set the maximum input length to 256 (as opposed to 512 in the original BERT paper).",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_11",
  "x": "Similar to <cite>(Devlin et al., 2018)</cite> , the training data generator chooses 15% of the token positions at random for making. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. The model updates use a batch size of 256 and Adam optimizer with learning rate starting from 1e-4. Training was done on a cluster of nodes, where each node consists of 8 Nvidia Tesla V100 GPUs. We vary the node size from 1 to 8 depending on the model size. Our TRANS-BLSTM is implemented on top of Pytorch transformer repository 2 . 1 Nevertheless, our implementation of baseline BERT model obtained higher accuracy than that reported by the original BERT paper <cite>(Devlin et al., 2018)</cite> .",
  "y": "similarities"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_12",
  "x": "If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. The model updates use a batch size of 256 and Adam optimizer with learning rate starting from 1e-4. Training was done on a cluster of nodes, where each node consists of 8 Nvidia Tesla V100 GPUs. We vary the node size from 1 to 8 depending on the model size. Our TRANS-BLSTM is implemented on top of Pytorch transformer repository 2 . 1 Nevertheless, our implementation of baseline BERT model obtained higher accuracy than that reported by the original BERT paper <cite>(Devlin et al., 2018)</cite> . 2 https://github.com/huggingface/pytorch-transformers.",
  "y": "differences"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_13",
  "x": "We vary the node size from 1 to 8 depending on the model size. Our TRANS-BLSTM is implemented on top of Pytorch transformer repository 2 . 1 Nevertheless, our implementation of baseline BERT model obtained higher accuracy than that reported by the original BERT paper <cite>(Devlin et al., 2018)</cite> . 2 https://github.com/huggingface/pytorch-transformers. ---------------------------------- **DOWNSTREAM EVALUATION DATASETS** Following the previous work <cite>(Devlin et al., 2018</cite>; Yang et al., 2019; Liu et al., 2019a; Lan et al., 2019) , we evaluate our models on the General Language Understanding Evaluation (GLUE) benchmark and the Stanford Question Answering Dataset (SQuAD 1.1) (Rajpurkar et al., 2016) .",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_14",
  "x": "For the down-stream fine-tuning experiments on SQuAD 1.1 dataset, we have the following hyperparameters for training. We set the learning rate to be 3e-5, training batch size to be 12, and the number of training epochs to be 2. We first run the experiment by replacing the transformer in BERT base with a bidirectional LSTM model with the same number of layers. That is, we replace the 12 transformer layers with 12 BLSTM layers. Table 2 shows the BERT base models, including the original BERT-base model in <cite>(Devlin et al., 2018)</cite> and our implementation, and the bidirectional LSTM model accuracy over SQuAD 1.1 development dataset. Our implementation results in a higher F1 score (90.05%) compared to the original BERT-base one (88.50%). This may be due to the fact that we use the whole word masking while BERT-base used partial word masking (an easier task, which may prevent from learning a better model).",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_16",
  "x": "This may be due to the fact that we use the whole word masking while BERT-base used partial word masking (an easier task, which may prevent from learning a better model). We found that the BLSTM model has F1 score of 83.43%, which is significantly worse than our TRANS/BERT baseline (90.05%). ---------------------------------- **MODEL** EM F1 BERT-base <cite>(Devlin et al., 2018)</cite> ---------------------------------- **MODELS PRE-TRAINING**",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_17",
  "x": "---------------------------------- **MODEL EVALUATION ON GLUE DATASETS** Following <cite>(Devlin et al., 2018)</cite> , we use a batch size of 32 and 3-epoch fine-tuning over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the development set. Additionally similar to <cite>(Devlin et al., 2018)</cite> , for large BERT and TRANS-BLSTM models, we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the development set. Table 5 shows the results of GLUE datasets for original BERT <cite>(Devlin et al., 2018)</cite> , ours TRANS/BERT, TRANS-BLSTM-SMALL and TRANS-BLSTM on base and large settings respectively. Following the BERT setting <cite>(Devlin et al., 2018)</cite> , we exclude the problematic WNLI set.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_18",
  "x": "**MODEL EVALUATION ON GLUE DATASETS** Following <cite>(Devlin et al., 2018)</cite> , we use a batch size of 32 and 3-epoch fine-tuning over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the development set. Additionally similar to <cite>(Devlin et al., 2018)</cite> , for large BERT and TRANS-BLSTM models, we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the development set. Table 5 shows the results of GLUE datasets for original BERT <cite>(Devlin et al., 2018)</cite> , ours TRANS/BERT, TRANS-BLSTM-SMALL and TRANS-BLSTM on base and large settings respectively. Following the BERT setting <cite>(Devlin et al., 2018)</cite> , we exclude the problematic WNLI set. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks.",
  "y": "similarities"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_19",
  "x": "---------------------------------- **MODEL EVALUATION ON GLUE DATASETS** Following <cite>(Devlin et al., 2018)</cite> , we use a batch size of 32 and 3-epoch fine-tuning over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the development set. Additionally similar to <cite>(Devlin et al., 2018)</cite> , for large BERT and TRANS-BLSTM models, we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the development set. Table 5 shows the results of GLUE datasets for original BERT <cite>(Devlin et al., 2018)</cite> , ours TRANS/BERT, TRANS-BLSTM-SMALL and TRANS-BLSTM on base and large settings respectively. Following the BERT setting <cite>(Devlin et al., 2018)</cite> , we exclude the problematic WNLI set.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_20",
  "x": "**MODEL EVALUATION ON GLUE DATASETS** Following <cite>(Devlin et al., 2018)</cite> , we use a batch size of 32 and 3-epoch fine-tuning over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the development set. Additionally similar to <cite>(Devlin et al., 2018)</cite> , for large BERT and TRANS-BLSTM models, we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the development set. Table 5 shows the results of GLUE datasets for original BERT <cite>(Devlin et al., 2018)</cite> , ours TRANS/BERT, TRANS-BLSTM-SMALL and TRANS-BLSTM on base and large settings respectively. Following the BERT setting <cite>(Devlin et al., 2018)</cite> , we exclude the problematic WNLI set. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks.",
  "y": "uses"
 },
 {
  "id": "cb57b8886be9ea4f0c50fd2c3a178a_0",
  "x": "Recently, there has been much work designing ranking architectures to effectively score query-document pairs, with encouraging results [5,<cite> 6,</cite> 20] . Meanwhile, pretrained contextualized language models (such as ELMo [1<cite>6</cite>] and BERT [4] ) have shown great promise on various natural language processing tasks [4, 1<cite>6</cite>] . These models work by pre-training LSTM-based or transformer-based [19] language models on a large corpus, and then by performing minimal task fine-tuning (akin to ImageNet [3, 23] ). Prior work has suggested that contextual information can be valuable when ranking. ConvKNRM [1] , a recent neural ranking model, uses a convolutional neural network atop word representations, allowing the model to learn representations aware of context in local proximity. In a similar vein, McDonald et al. [12] proposes Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.",
  "y": "background"
 },
 {
  "id": "cb57b8886be9ea4f0c50fd2c3a178a_4",
  "x": "We evaluate our methods on three neural relevance matching methods: PACRR<cite> [6]</cite> , KNRM [20] , and DRMM [5] . Relevance matching models have generally shown to be more effective than semantic matching models, while not requiring massive amounts of behavioral data (e.g., query logs). For PACRR, we increase k max = 30 to allow for more term matches and better back-propagation to the language model. Contextualized language models. We use the pretrained ELMo (Original, 5 .5B) and BERT (BERT-Base, Uncased) language models in our experiments. For ELMo, the query and document are encoded separately. Since BERT enables encoding multiple texts at the same time using Segment A and Segment B embeddings, we encode the query (Segment A) and document (Segment B) simultaneously.",
  "y": "uses"
 },
 {
  "id": "cb57b8886be9ea4f0c50fd2c3a178a_7",
  "x": "All models are trained using Adam [8] with a learning rate of 0.001 while BERT layers are trained at a rate of 2e-5. 5 Following prior work<cite> [6]</cite> , documents are truncated to 800 tokens. Baselines. We compare contextualized language model performance to the following strong baselines: -BM25 and SDM [13] , as implemented by Anserini [21] . Finetuning is conducted on the test set, representing the maximum performance of the model when using static parameters over each dataset. <cite>6</cite> We do not report SDM performance on WebTrack due to its high cost of retrieval on the large ClueWeb collections.",
  "y": "uses"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_0",
  "x": "They share the need to evaluate free-text answers but differ in task setting and grading/evaluation criteria, among others. This paper has the intention of fostering synergy between the different research strands. It discusses the different research strands, details the crucial differences, and explores under which circumstances systems can be compared given publicly available data. To that end, we present results with the CoMiC-EN Content Assessment system (Meurers et al., 2011a) on the dataset published by<cite> Mohler et al. (2011)</cite> and outline what was necessary to perform this comparison. We conclude with a general discussion on comparability and evaluation of short answer assessment systems. ---------------------------------- **INTRODUCTION**",
  "y": "uses background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_1",
  "x": "Short answer assessment systems compare students' responses to questions with manually defined target responses or answer keys in order to judge the appropriateness of the responses, or in order to automatically assign a grade. A number of approaches have emerged in recent years, each of them with different aims and different backgrounds. In this paper, we will draw a map of the short answer assessment landscape, highlighting the similarities and differences between approaches and the data used for evaluation. We will provide an overview of 12 systems and sketch their attributes. Subsequently, we will zoom into the comparison of two of them, namely CoMiC-EN (Meurers et al., 2011a ) and the one which we call the Texas system <cite>(Mohler et al., 2011)</cite> and discuss the issues that arise with this endeavor. Returning to the bigger picture, we will explore how such systems could be compared in general, in the belief that meaningful comparison of approaches across research strands will be an important ingredient in advancing this relatively new research field. 2 The short answer assessment landscape ----------------------------------",
  "y": "motivation background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_2",
  "x": "Another recent approach is described by<cite> Mohler et al. (2011)</cite> , hereafter referred to as the Texas system. Student responses and target responses are annotated using a dependency parser. Thereupon, subgraphs of the dependency structures are constructed in order to map one response to the other. These alignments are generated using machine learning. Dealing with subgraphs allows for variation in word order between the two responses that are to be compared. In order to account for meaning, they combine lexical semantic similarity with the aforementioned alignment. They make use of several WordNet-based measures and two corpus-based measures, namely Latent Semantic Analysis and Explicit Semantic Analysis (ESA, Gabrilovich and Markovitch 2007) .",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_3",
  "x": "For evaluating their system,<cite> Mohler et al. (2011)</cite> collected student responses from an online learning environment. 80 questions from ten introductory computer science assignments spread across two exams were gathered together with 2,273 student responses. These responses were graded by two human judges on a scale from zero to five. The judges fully agreed in 57% of all cases, their Pearson correlation computes to r = 0.586. The gold standard has been created by computing the arithmetic mean of the two judgments for each response. The Texas system achieves r = 0.518 and a Root Mean Square Error of 0.978 as its best result. Mohler et al. (2011) mention that \"[t]he dataset is biased towards correct answers\".",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_4",
  "x": [
   "The Texas system achieves r = 0.518 and a Root Mean Square Error of 0.978 as its best result. Mohler et al. (2011) mention that \"[t]he dataset is biased towards correct answers\". Data are publicly available. We used these in an evaluation experiment with the CoMiC-EN system, discussed in Section 3. While almost all short answer assessment research has targeted answers written in English, there are two recent approaches dealing with German answers. The CoMiC-EN reimplementation of CAM discussed above was motivated by the need for a modular architecture supporting a transfer of the system to German, resulting in its counterpart named CoMiC-DE (Meurers et al., 2011b) . The German system utilizes the same strategies as the English one, but with language-dependent processing modules being replaced."
  ],
  "y": "uses background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_5",
  "x": "**A CONCRETE SYSTEM COMPARISON** After discussing the broad landscape of Short Answer Evaluation systems, the main characteristics and differences, we now turn to a comparison of two concrete systems, namely CoMiC-EN (Meurers et al., 2011a ) and the Texas system<cite> Mohler et al. (2011)</cite> , to explore what is involved in such a concrete comparison of two systems from different contexts. While CoMiC-EN was developed with meaning comparison in mind, the purpose of the Texas system is answer grading. We pick these two systems because they constitute recent and interesting instances of their respective fields and the corresponding data are freely available. ---------------------------------- **DATA** In evaluating the Texas system,<cite> Mohler et al. (2011)</cite> used a corpus of ten assignments and two exams from an introductory computer science class.",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_6",
  "x": "In evaluating the Texas system,<cite> Mohler et al. (2011)</cite> used a corpus of ten assignments and two exams from an introductory computer science class. In total, the Texas corpus consists of 2,442 responses, which were collected using an online learning platform. Each response is rated by two annotators with a numerical grade on a 0-5 scale. Annotators were not given any specific instructions besides the scale itself, which resulted in an exact agreement of 57.7%. In order to arrive at a gold standard rating, the numerical average of the two ratings was computed. The data exist in raw, sentence-segmented and parsed versions and are freely available for research use. Table 2 presents a breakdown of the score counts and distribution statistics of the Texas corpus.",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_7",
  "x": "In order to arrive at a gold standard rating, the numerical average of the two ratings was computed. The data exist in raw, sentence-segmented and parsed versions and are freely available for research use. Table 2 presents a breakdown of the score counts and distribution statistics of the Texas corpus. A bias towards correct answers can be observed, which is also mentioned by<cite> Mohler et al. (2011)</cite> . Table 2 : Details on the gold standard scores in the Texas corpus. Non-integer scores result from averaging between raters and normalization onto the 0-5 scale. ----------------------------------",
  "y": "motivation background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_8",
  "x": [
   "Synonym Match Percent of token alignments that were synonym-resolved 13. Variety of Match Number of kinds of (0-5) token-level alignments dependency graph alignment in connection with two different machine learning approaches. Among the BOW features are WordNet-based similarity measures such as the one by Lesk (1986) and vector space measures such as tf * idf (Salton and McGill, 1983 ) and the more advanced LSA (Landauer et al., 1998) . The dependency graph alignment approach builds on a node-to-node matching stage which computes a score for each possible match between nodes of the student and target response. In the next stage, the optimal graph alignment is computed based on the node-to-node scores using the Hungarian algorithm. Mohler et al. (2011) also employ a technique they call \"question demoting\", which refers to the exclusion of words from the alignment process if they already appeared in the question string. Incidentally, the technique is also used in the earlier CAM system (Bailey and Meurers, 2008) , but called \"Givenness filter\" there, following the long tradition of research on givenness (Schwarzschild, 1999) as a notion of information structure investigated in formal pragmatics."
  ],
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_9",
  "x": "Both techniques are trained with several combinations of the dependency alignment and BOW features. While with SVR one trains a function to produce a score on the 0-5 scale itself, SVMRank produces a ranking of student answers which does not produce a 0-5 grade. Therefore,<cite> Mohler et al. (2011)</cite> employ isotonic regression to map the ranking to the 0-5 scale. In terms of performance,<cite> Mohler et al. (2011)</cite> report that the SVMRank system produces a better correlation measure (r = 0.518) while the SVR system yields a better RMSE (0.978). ---------------------------------- **EVALUATION** We now turn to the evaluation of CoMiC-EN on the Texas corpus as it is a publicly available dataset.",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_10",
  "x": "However, a classification approach has no knowledge of numerical relationships, i.e., it does not 'know' that 4 is a higher grade than 3 and a much higher grade than 1 (assuming a 0-5 scale). As a result, if an evaluation metric such as Pearson correlation is used, classification systems are at a disadvantage because some misclassifications are punished more than others. We discuss this point further in Section 4. For these reasons, to obtain a more interesting comparison, we modified CoMiC-EN to perform scoring instead of meaning comparison. This means that the memory-based learning approach CoMiC-EN had employed so far was no longer applicable and had to be replaced with a regression-capable learning strategy. We chose Support Vector Regression (SVR) using libSVM 4 since that is one of the methods employed by<cite> Mohler et al. (2011)</cite> . However, all other parts of CoMiC-EN such as the processing pipeline and the alignment approach and the extracted features remained the same.",
  "y": "extends"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_11",
  "x": [
   "So in order for researchers to learn from other approaches and also compare their results to those of other systems which tackle a different task, changes to systems seem necessary and should be preferred over changes to the gold standard data. In the case presented here, a meaning comparison system was turned into a scoring system by changing the machine learning component from classification to regression, which requires a certain level of system modularity. Having compared the two systems using Pearson correlation and RMSE, it also makes sense to consider the relevance of these evaluation metrics. For example, it is the case that pairwise correlation assumes a normal distribution whereas datasets like the Texas corpus are heavily skewed towards correct answers (see Table 2 ). Mohler et al. (2011) also note that in distributions with zero variance, correlation is undefined, which is not a problem as such but limits the use of correlation as evaluation metric. Mohler et al. (2011) propose that RMSE is better suited to the task since it captures the relative error a system makes when trying to predict scores. However, RMSE is scale-dependent and thus RMSE values across different studies cannot be compared."
  ],
  "y": "motivation background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_12",
  "x": [
   "In the case presented here, a meaning comparison system was turned into a scoring system by changing the machine learning component from classification to regression, which requires a certain level of system modularity. Having compared the two systems using Pearson correlation and RMSE, it also makes sense to consider the relevance of these evaluation metrics. For example, it is the case that pairwise correlation assumes a normal distribution whereas datasets like the Texas corpus are heavily skewed towards correct answers (see Table 2 ). Mohler et al. (2011) also note that in distributions with zero variance, correlation is undefined, which is not a problem as such but limits the use of correlation as evaluation metric. Mohler et al. (2011) propose that RMSE is better suited to the task since it captures the relative error a system makes when trying to predict scores. However, RMSE is scale-dependent and thus RMSE values across different studies cannot be compared. We can only suggest that in order to sufficiently describe a system's performance, several metrics need to be reported."
  ],
  "y": "motivation background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_14",
  "x": "---------------------------------- **CONCLUSION** We discussed several issues in the comparison of short answer evaluation systems. To that end, we gave an overview of the existing systems and picked two for a concrete comparison on the same data, the CoMiC-EN system (Meurers et al., 2011a ) and the Texas system <cite>(Mohler et al., 2011)</cite> . In comparing the two, it was necessary to turn CoMiC-EN into a scoring system because the Texas corpus as the chosen gold standard contains numeric scores assigned by humans. Taking a step back from the concrete comparison, we gave a more general description of what is necessary to compare short answer evaluation systems. We observed that more datasets need to be publicly available in order for performance comparisons to have meaning, a point also made earlier by Pulman and Sukkarieh (2005) .",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_15",
  "x": "Incidentally, the technique is also used in the earlier CAM system (Bailey and Meurers, 2008) , but called \"Givenness filter\" there, following the long tradition of research on givenness (Schwarzschild, 1999) as a notion of information structure investigated in formal pragmatics. To produce the final system score, the Texas system uses two machine learning techniques based on Support Vector Machines (SVMs), SVMRank and Support Vector Regression (SVR). Both techniques are trained with several combinations of the dependency alignment and BOW features. While with SVR one trains a function to produce a score on the 0-5 scale itself, SVMRank produces a ranking of student answers which does not produce a 0-5 grade. Therefore,<cite> Mohler et al. (2011)</cite> employ isotonic regression to map the ranking to the 0-5 scale. In terms of performance,<cite> Mohler et al. (2011)</cite> report that the SVMRank system produces a better correlation measure (r = 0.518) while the SVR system yields a better RMSE (0.978). ----------------------------------",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_16",
  "x": [
   "As mentioned before, CoMiC-EN performs meaning comparison based on a system of categories while the Texas system is a scoring approach, trying to predict a grade. While the former is a classification task, the latter is better characterized as a regression problem because of the desired numerical outcome. Of course, one could simply pretend that individual grades are classes and treat scoring as a classification task. However, a classification approach has no knowledge of numerical relationships, i.e., it does not 'know' that 4 is a higher grade than 3 and a much higher grade than 1 (assuming a 0-5 scale). As a result, if an evaluation metric such as Pearson correlation is used, classification systems are at a disadvantage because some misclassifications are punished more than others. We discuss this point further in Section 4. For these reasons, to obtain a more interesting comparison, we modified CoMiC-EN to perform scoring instead of meaning comparison."
  ],
  "y": "motivation background"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_0",
  "x": "This task, which we refer to as selective generation, is often formulated as two subproblems: content selection, which involves choosing a subset of relevant records to talk about from the exhaustive database, and surface realization, which is concerned with generating natural language descriptions for this subset. Learning to perform these tasks jointly is challenging due to the ambiguity in deciding which records are relevant, the complex dependencies between selected records, and the multiple ways in which these records can be described. Previous work has made significant progress on this task (Chen and Mooney, 2008;<cite> Angeli et al., 2010</cite>; Konstas and Lapata, 2012) . However, most approaches solve the two content selection and surface realization subtasks separately, use manual domain-dependent resources (e.g., semantic parsers) and features, or employ template-based generation. This limits domain adaptability and reduces coherence. We take an alternative, neural encoder-aligner-decoder approach to free-form selective generation that jointly performs content selection and surface realization, without using any specialized features, resources, or generation templates. This enables our approach to generalize to new domains.",
  "y": "background"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_1",
  "x": "Learning to perform these tasks jointly is challenging due to the ambiguity in deciding which records are relevant, the complex dependencies between selected records, and the multiple ways in which these records can be described. Previous work has made significant progress on this task (Chen and Mooney, 2008;<cite> Angeli et al., 2010</cite>; Konstas and Lapata, 2012) . However, most approaches solve the two content selection and surface realization subtasks separately, use manual domain-dependent resources (e.g., semantic parsers) and features, or employ template-based generation. This limits domain adaptability and reduces coherence. We take an alternative, neural encoder-aligner-decoder approach to free-form selective generation that jointly performs content selection and surface realization, without using any specialized features, resources, or generation templates. This enables our approach to generalize to new domains. Further, our memorybased model captures the long-range contextual dependencies among records and descriptions, which are integral to this task<cite> (Angeli et al., 2010)</cite> .",
  "y": "motivation"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_2",
  "x": "This enables our approach to generalize to new domains. Further, our memorybased model captures the long-range contextual dependencies among records and descriptions, which are integral to this task<cite> (Angeli et al., 2010)</cite> . We formulate our model as an encoder-alignerdecoder framework that uses recurrent neural networks with long short-term memory units (LSTMRNNs) (Hochreiter and Schmidhuber, 1997) together with a coarse-to-fine aligner to select and \"translate\" the rich world state into a natural language description. Our model first encodes the full set of over-determined event records using a bidirectional LSTM-RNN. A novel coarse-to-fine aligner then reasons over multiple abstractions of the input to decide which of the records to discuss. The model next employs an LSTM decoder to generate natural language descriptions of the selected records. The use of LSTMs, which have proven effective for similar long-range generation tasks Vinyals et al., 2015b; Karpathy and FeiFei, 2015) , allows our model to capture the longrange contextual dependencies that exist in selective generation.",
  "y": "motivation"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_3",
  "x": "Soricut and Marcu (2006) propose a language generation system that uses the WIDL-representation, a formalism used to compactly represent probability distributions over finite sets of strings. Wong and Mooney (2007) and Lu and Ng (2011) use synchronous context-free grammars to generate natural language sentences from formal meaning representations. Similarly, Belz (2008) employs probabilistic context-free grammars to perform surface realization. Other effective approaches include the use of tree conditional random fields (Lu et al., 2009) and template extraction within a log-linear framework<cite> (Angeli et al., 2010)</cite> . Recent work seeks to solve the full selective generation problem through a single framework. Chen and Mooney (2008) and Chen et al. (2010) learn alignments between comments and their corresponding event records using a translation model for parsing and generation. implement a two-stage framework that decides what to discuss using a combination of the methods of Lu et al. (2008) and Liang et al. (2009) , and then produces the text based on the generation system of Wong and Mooney (2007) .",
  "y": "background"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_4",
  "x": "Similarly, Belz (2008) employs probabilistic context-free grammars to perform surface realization. Other effective approaches include the use of tree conditional random fields (Lu et al., 2009) and template extraction within a log-linear framework<cite> (Angeli et al., 2010)</cite> . Recent work seeks to solve the full selective generation problem through a single framework. Chen and Mooney (2008) and Chen et al. (2010) learn alignments between comments and their corresponding event records using a translation model for parsing and generation. implement a two-stage framework that decides what to discuss using a combination of the methods of Lu et al. (2008) and Liang et al. (2009) , and then produces the text based on the generation system of Wong and Mooney (2007) . <cite>Angeli et al. (2010)</cite> propose a unified conceptto-text model that treats joint content selection and surface realization as a sequence of local decisions represented by a log-linear model. Similar to other work, they train their model using external alignments from Liang et al. (2009) .",
  "y": "background"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_5",
  "x": "Note that when \u03b3 is equal to N , the pre-selector is forced to select all the records (p j = 1.0 for all j), and the coarse-to-fine alignment reverts to the standard alignment introduced by Bahdanau et al. (2014) . Together with the negative loglikelihood of the ground-truth description x * 1:T , our loss function becomes Having trained the model, we generate the natural language description by finding the maximum a posteriori words under the learned model (Eqn. 1). For inference, we perform greedy search starting with the first word x 1 . Beam search offers a way to perform approximate joint inference -however, we empirically found that beam search does not perform any better than greedy search on the datasets that we consider, an observation that is shared with previous work<cite> (Angeli et al., 2010)</cite> . We later discuss an alternative k-nearest neighbor-based beam filter (see Sec 6.2). ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_6",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** Datasets We analyze our model on the benchmark WEATHERGOV dataset, and use the data-starved ROBOCUP dataset to demonstrate the model's generalizability. Following <cite>Angeli et al. (2010)</cite> , we use WEATHERGOV training, development, and test splits of size 25000, 1000, and 3528, respectively. For ROBOCUP, we follow the evaluation methodology of previous work (Chen and Mooney, 2008) , performing three-fold cross-validation whereby we train on three games (approximately 1000 scenarios) and test on the fourth. Within each split, we hold out 10% of the training data as the development set to tune the early-stopping criterion and \u03b3. We then report the standard average performance (weighted by the number of scenarios) over these four splits.",
  "y": "uses"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_7",
  "x": "**RESULTS AND ANALYSIS** We analyze the effectiveness of our model on the benchmark WEATHERGOV (as primary) and ROBOCUP (as generalization) datasets. We also present several ablations to illustrate the contributions of the primary model components. ---------------------------------- **PRIMARY RESULTS (WEATHERGOV)** We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of <cite>Angeli et al. (2010)</cite>), respectively (Sec. 5). Table 1 compares our test results against previous methods that include KL12 (Konstas and Lapata, 2012) , KL13 (Konstas and Lapata, 2013) , and ALK10<cite> (Angeli et al., 2010)</cite> .",
  "y": "uses"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_8",
  "x": "**PRIMARY RESULTS (WEATHERGOV)** We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of <cite>Angeli et al. (2010)</cite>), respectively (Sec. 5). Table 1 compares our test results against previous methods that include KL12 (Konstas and Lapata, 2012) , KL13 (Konstas and Lapata, 2013) , and ALK10<cite> (Angeli et al., 2010)</cite> . Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11.94% (F-1), 58.88% (sBLEU), and 36.68% (cBLEU) over the previous state-of-the-art. ---------------------------------- **BEAM FILTER WITH K-NEAREST NEIGHBORS** We considered beam search as an alternative to greedy search in our primary setup (Eqn. 1), but this performs worse, similar to what previous work found on this dataset<cite> (Angeli et al., 2010)</cite> .",
  "y": "differences"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_9",
  "x": "We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of <cite>Angeli et al. (2010)</cite>), respectively (Sec. 5). Table 1 compares our test results against previous methods that include KL12 (Konstas and Lapata, 2012) , KL13 (Konstas and Lapata, 2013) , and ALK10<cite> (Angeli et al., 2010)</cite> . Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11.94% (F-1), 58.88% (sBLEU), and 36.68% (cBLEU) over the previous state-of-the-art. ---------------------------------- **BEAM FILTER WITH K-NEAREST NEIGHBORS** We considered beam search as an alternative to greedy search in our primary setup (Eqn. 1), but this performs worse, similar to what previous work found on this dataset<cite> (Angeli et al., 2010)</cite> . As an alternative, we consider a beam filter based on a knearest neighborhood.",
  "y": "differences"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_10",
  "x": "Table 3 reports the results demonstrating that our aligner yields superior F-1 and BLEU scores relative to a standard aligner. Encoder Ablation Next, we consider the effectiveness of the encoder. Table 4 compares the results with and without the encoder on the development set, and demonstrates that there is a significant gain from encoding the event records using the LSTM-RNN. We attribute this improvement to the LSTM-RNN's ability to capture the relationships that exist among the records, which is known to be essential to selective generation (Barzilay and Lapata, 2005;<cite> Angeli et al., 2010)</cite> . ---------------------------------- **QUALITATIVE ANALYSIS (WEATHERGOV)** Output Examples Fig. 3 shows an example record set with its output description and recordword alignment heat map.",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_0",
  "x": "RTMs are a computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants. RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain specific information or resource. RTMs become the 2nd system out of 13 systems participating in Paraphrase and Semantic Similarity in Twitter, 6th out of 16 submissions in Semantic Textual Similarity Spanish, and 50th out of 73 submissions in Semantic Textual Similarity English. We present positive results from a fully automated judge for semantic similarity based on Referential Translation Machines<cite> (Bi\u00e7ici and Way, 2014b)</cite> in two semantic similarity tasks at SemEval-2015, Semantic Evaluation Exercises -International Workshop on Semantic Evaluation (Nakov et al., 2015). Referential translation machine (RTM) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. Each RTM model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation.",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_1",
  "x": "Each RTM model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation. RTMs present an accurate and language independent solution for making semantic similarity judgments. RTMs pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bi\u00e7ici and Yuret, 2015) as interpretants for reaching shared semantics. RTMs achieve (i) top performance when predicting the quality of translations (Bi\u00e7ici, 2013; Bi\u00e7ici and Way, 2014a) ; (ii) top performance when predicting monolingual cross-level semantic similarity; (iii) second performance when predicting paraphrase and semantic similarity in Twitter (iv) good performance when judging the semantic similarity of sentences; (iv) good performance when evaluating the semantic relatedness of sentences and their entailment<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation. MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. MTPP features for translation acts are provided in<cite> (Bi\u00e7ici and Way, 2014b)</cite> .",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_2",
  "x": "RTMs present an accurate and language independent solution for making semantic similarity judgments. RTMs pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bi\u00e7ici and Yuret, 2015) as interpretants for reaching shared semantics. RTMs achieve (i) top performance when predicting the quality of translations (Bi\u00e7ici, 2013; Bi\u00e7ici and Way, 2014a) ; (ii) top performance when predicting monolingual cross-level semantic similarity; (iii) second performance when predicting paraphrase and semantic similarity in Twitter (iv) good performance when judging the semantic similarity of sentences; (iv) good performance when evaluating the semantic relatedness of sentences and their entailment<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation. MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. MTPP features for translation acts are provided in<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs become the 2nd system out of 13 systems participating in Paraphrase and Semantic Similarity in Twitter (Task 1) (Xu et al., 2015) and achieve good results in Semantic Tex-56",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_3",
  "x": "RTMs pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bi\u00e7ici and Yuret, 2015) as interpretants for reaching shared semantics. RTMs achieve (i) top performance when predicting the quality of translations (Bi\u00e7ici, 2013; Bi\u00e7ici and Way, 2014a) ; (ii) top performance when predicting monolingual cross-level semantic similarity; (iii) second performance when predicting paraphrase and semantic similarity in Twitter (iv) good performance when judging the semantic similarity of sentences; (iv) good performance when evaluating the semantic relatedness of sentences and their entailment<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation. MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. MTPP features for translation acts are provided in<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs become the 2nd system out of 13 systems participating in Paraphrase and Semantic Similarity in Twitter (Task 1) (Xu et al., 2015) and achieve good results in Semantic Tex-56 ----------------------------------",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_4",
  "x": "**REFERENTIAL TRANSLATION MACHINE (RTM)** We present positive results from a fully automated judge for semantic similarity based on Referential Translation Machines<cite> (Bi\u00e7ici and Way, 2014b)</cite> in two semantic similarity tasks at SemEval-2015, Semantic Evaluation Exercises -International Workshop on Semantic Evaluation (Nakov et al., 2015) . Referential translation machine (RTM) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. Each RTM model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation. RTMs present an accurate and language independent solution for making semantic similarity judgments. RTMs pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bi\u00e7ici and Yuret, 2015) as interpretants for reaching shared semantics.",
  "y": "similarities uses"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_5",
  "x": "Referential translation machine (RTM) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. Each RTM model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation. RTMs present an accurate and language independent solution for making semantic similarity judgments. RTMs pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bi\u00e7ici and Yuret, 2015) as interpretants for reaching shared semantics. RTMs achieve (i) top performance when predicting the quality of translations (Bi\u00e7ici, 2013; Bi\u00e7ici and Way, 2014a) ; (ii) top performance when predicting monolingual cross-level semantic similarity; (iii) second performance when predicting paraphrase and semantic similarity in Twitter (iv) good performance when judging the semantic similarity of sentences; (iv) good performance when evaluating the semantic relatedness of sentences and their entailment<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation.",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_6",
  "x": "RTMs achieve (i) top performance when predicting the quality of translations (Bi\u00e7ici, 2013; Bi\u00e7ici and Way, 2014a) ; (ii) top performance when predicting monolingual cross-level semantic similarity; (iii) second performance when predicting paraphrase and semantic similarity in Twitter (iv) good performance when judging the semantic similarity of sentences; (iv) good performance when evaluating the semantic relatedness of sentences and their entailment<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation. MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. MTPP features for translation acts are provided in<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs become the 2nd system out of 13 systems participating in Paraphrase and Semantic Similarity in Twitter (Task 1) (Xu et al., 2015) and achieve good results in Semantic Tex- tual Similarity (Task 2) (Agirre et al., 2015) becoming 6th out of 16 submissions in Spanish. We use the Parallel FDA5 instance selection model for selecting the interpretants (Bi\u00e7ici et al., 2014; Bi\u00e7ici and Yuret, 2015) , which allows efficient parameterization, optimization, and implementation of Feature Decay Algorithms (FDA), and build an MTPP model.",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_7",
  "x": "RTMs achieve (i) top performance when predicting the quality of translations (Bi\u00e7ici, 2013; Bi\u00e7ici and Way, 2014a) ; (ii) top performance when predicting monolingual cross-level semantic similarity; (iii) second performance when predicting paraphrase and semantic similarity in Twitter (iv) good performance when judging the semantic similarity of sentences; (iv) good performance when evaluating the semantic relatedness of sentences and their entailment<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation. MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. MTPP features for translation acts are provided in<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs become the 2nd system out of 13 systems participating in Paraphrase and Semantic Similarity in Twitter (Task 1) (Xu et al., 2015) and achieve good results in Semantic Tex- tual Similarity (Task 2) (Agirre et al., 2015) becoming 6th out of 16 submissions in Spanish. We use the Parallel FDA5 instance selection model for selecting the interpretants (Bi\u00e7ici et al., 2014; Bi\u00e7ici and Yuret, 2015) , which allows efficient parameterization, optimization, and implementation of Feature Decay Algorithms (FDA), and build an MTPP model.",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_8",
  "x": "We optimize the learning parameters, the number of dimensions used for PLS, and the parameters for parallel FDA5. More details about the optimization processes are in <cite>(Bi\u00e7ici and Way, 2014b</cite>; Bi\u00e7ici et al., 2014) . We optimize the learning parameters by selecting \u03b5 close to the standard deviation of the noise in the training set (Bi\u00e7ici, 2013) since the optimal value for \u03b5 is shown to have linear dependence to the noise level for different noise models (Smola et al., 1998) . At testing time, the predictions are bounded to obtain scores in the corresponding ranges. We use Pearson's correlation (r P ), mean absolute error (MAE), and relative absolute error (RAE) for evaluation: We define MAER and MRAER for easier replication and comparability with relative errors for each instance: MAER is the mean absolute error relative to the magnitude of the target and MRAER is the mean absolute error relative to the absolute error of a predictor always predicting the target mean assuming that target mean is known.",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_9",
  "x": "We build separate RTM models for headlines and images domains for STS English. Domain specific RTM models obtain improved performance in those domains<cite> (Bi\u00e7ici and Way, 2014b)</cite> . STS English test set contains 2000, 1500, 2000, 1500, and 1500 sentences respectively from the specified domains however for evaluation, STS use a subset of the test set, 375, 750, 375, 750, and 750 instances respectively from the corresponding domains. This may lower the performance of RTMs by causing FDA5 to select more domain specific data and less task specific since RTMs use the test set to select interpretants and build a task specific RTM prediction model. (Bi\u00e7ici and Way, 2014b) are presented in Table 6, where we have used the top results from domain specific RTM models for headlines and images domains in the overall model results. Top 3 individual RTM model performance on the training set with further optimized learning model parameters after the challenge are presented in Table 7 . Better r P , RAE, and MRAER on the test set than on the training set in STS 2015 English may be attributed to RTMs.",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_10",
  "x": "Better r P , RAE, and MRAER on the test set than on the training set in STS 2015 English may be attributed to RTMs. ---------------------------------- **RTMS ACROSS TASKS AND YEARS** We compare the difficulty of tasks according to MRAER where the correlation of RAE and MRAER is 0.89. In Table 8 , we list the RAE, MAER, and MRAER obtained for different tasks and subtasks, also listing RTM results from SemEval-2013 , from SemEval-2014<cite> (Bi\u00e7ici and Way, 2014b)</cite> , and and from quality estimation task (QET) (Bi\u00e7ici and Way, 2014a ) of machine translation (Bojar et al., 2014) . RTMs at SemEval-2013 contain results from STS. RTMs at SemEval-2014 contain results from STS, semantic relatedness and entailment (SRE) (Marelli et al., 2014) , and cross-level semantic similarity (CLSS) tasks (Jurgens et al., 2014) .",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_0",
  "x": "The experimental results show that: (1) the WSM is able to achieve tonal (syllables input with four tones) and toneless (syllables input without four tones) syllable-to-word (STW) accuracies of 99% and 92%, respectively, among the converted words; and (2) while applying the WSM as an adaptation processing, together with the Microsoft Input Method Editor 2003 (MSIME) and an optimized bigram model, the average tonal and toneless STW improvements are 37% and 35%, respectively. ---------------------------------- **INTRODUCTION** According to (Becker, 1985; Huang, 1985; Gu et al., 1991; Chung, 1993; Kuo, 1995; Fu et al., 1996; Lee et al., 1997; Hsu et al., 1999; Chen et al., 2000; Tsai and Hsu, 2002; Gao et al., 2002; Lee, 2003;<cite> Tsai, 2005)</cite> , the approaches of Chinese input methods (i.e. Chinese input systems) can be classified into two types: (1) keyboard based approach: including phonetic and pinyin based (Chang et al., 1991; Hsu et al., 1993; Hsu, 1994; Hsu et al., 1999; Kuo, 1995; Lua and Gan, 1992) , arbitrary codes based (Fan et al., 1988) and structure scheme based (Huang, 1985) ; and (2) non-keyboard based approach: including optical character recognition (OCR) (Chung, 1993) , online handwriting and speech recognition (Fu et al., 1996; Chen et al., 2000) . Currently, the most popular Chinese input system is phonetic and pinyin based approach, because Chinese people are taught to write phonetic and pinyin syllables of each Chinese character in primary school. In Chinese, each Chinese word can be a mono-syllabic word, such as \"\u9f20(mouse)\", a bisyllabic word, such as \"\u888b\u9f20(kangaroo)\", or a multi-syllabic word, such as \"\u7c73\uf934\u9f20(Mickey mouse).\" The corresponding phonetic and pinyin syllables of each Chinese word is called syllable-words, such as \"dai4 shu3\" is the pinyin syllable-word of \"\u888b\u9f20(kangaroo).\" According to our computation, the {minimum, maximum, average} words per each distinct mono-syllableword and poly-syllable-word (including bisyllable-word and multi-syllable-word) in the CKIP dictionary (Chinese Knowledge Information Processing Group, 1995) are {1, 28, 2.8} and {1, 7, 1.1}, respectively. The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP).",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_1",
  "x": "---------------------------------- **INTRODUCTION** According to (Becker, 1985; Huang, 1985; Gu et al., 1991; Chung, 1993; Kuo, 1995; Fu et al., 1996; Lee et al., 1997; Hsu et al., 1999; Chen et al., 2000; Tsai and Hsu, 2002; Gao et al., 2002; Lee, 2003;<cite> Tsai, 2005)</cite> , the approaches of Chinese input methods (i.e. Chinese input systems) can be classified into two types: (1) keyboard based approach: including phonetic and pinyin based (Chang et al., 1991; Hsu et al., 1993; Hsu, 1994; Hsu et al., 1999; Kuo, 1995; Lua and Gan, 1992) , arbitrary codes based (Fan et al., 1988) and structure scheme based (Huang, 1985) ; and (2) non-keyboard based approach: including optical character recognition (OCR) (Chung, 1993) , online handwriting and speech recognition (Fu et al., 1996; Chen et al., 2000) . Currently, the most popular Chinese input system is phonetic and pinyin based approach, because Chinese people are taught to write phonetic and pinyin syllables of each Chinese character in primary school. In Chinese, each Chinese word can be a mono-syllabic word, such as \"\u9f20(mouse)\", a bisyllabic word, such as \"\u888b\u9f20(kangaroo)\", or a multi-syllabic word, such as \"\u7c73\uf934\u9f20(Mickey mouse).\" The corresponding phonetic and pinyin syllables of each Chinese word is called syllable-words, such as \"dai4 shu3\" is the pinyin syllable-word of \"\u888b\u9f20(kangaroo).\" According to our computation, the {minimum, maximum, average} words per each distinct mono-syllableword and poly-syllable-word (including bisyllable-word and multi-syllable-word) in the CKIP dictionary (Chinese Knowledge Information Processing Group, 1995) are {1, 28, 2.8} and {1, 7, 1.1}, respectively. The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP). Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (Hsu, 1994; Hsu et al., 1999; Tsai and Hsu, 2002; Gao et al., 2002; Microsoft Research Center in Beijing;<cite> Tsai, 2005)</cite> are addressed on STW conversion.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_2",
  "x": "Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (Hsu, 1994; Hsu et al., 1999; Tsai and Hsu, 2002; Gao et al., 2002; Microsoft Research Center in Beijing;<cite> Tsai, 2005)</cite> are addressed on STW conversion. On the other hand, STW conversion is the main task of Chinese Language Processing in typical Chinese speech recognition systems (Fu et al., 1996; Lee et al., 1993; Chien et al., 1993; Su et al., 1992) . As per (Chung, 1993; Fong and Chung, 1994; Tsai and Hsu, 2002; Gao et al., 2002; Lee, 2003;<cite> Tsai, 2005)</cite> , homophone selection and syllableword segmentation are two critical problems in developing a Chinese input system. Incorrect homophone selection and syllable-word seg-mentation will directly influence the STW conversion accuracy. Conventionally, there are two approaches to resolve the two critical problems: (1) linguistic approach: based on syntax parsing, semantic template matching and contextual information (Hsu, 1994; Fu et al., 1996; Hsu et al., 1999; Kuo, 1995; Tsai and Hsu, 2002) ; and (2) statistical approach: based on the n-gram models where n is usually 2, i.e. bigram model (Lin and Tsai, 1987; Gu et al., 1991; Fu et al., 1996; Ho et al., 1997; Sproat, 1990; Gao et al., 2002; Lee 2003) . From the studies (Hsu 1994; Tsai and Hsu, 2002; Gao et al., 2002; Kee, 2003;<cite> Tsai, 2005)</cite> , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake. The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_3",
  "x": "The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP). Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (Hsu, 1994; Hsu et al., 1999; Tsai and Hsu, 2002; Gao et al., 2002; Microsoft Research Center in Beijing;<cite> Tsai, 2005)</cite> are addressed on STW conversion. On the other hand, STW conversion is the main task of Chinese Language Processing in typical Chinese speech recognition systems (Fu et al., 1996; Lee et al., 1993; Chien et al., 1993; Su et al., 1992) . As per (Chung, 1993; Fong and Chung, 1994; Tsai and Hsu, 2002; Gao et al., 2002; Lee, 2003;<cite> Tsai, 2005)</cite> , homophone selection and syllableword segmentation are two critical problems in developing a Chinese input system. Incorrect homophone selection and syllable-word seg-mentation will directly influence the STW conversion accuracy. Conventionally, there are two approaches to resolve the two critical problems: (1) linguistic approach: based on syntax parsing, semantic template matching and contextual information (Hsu, 1994; Fu et al., 1996; Hsu et al., 1999; Kuo, 1995; Tsai and Hsu, 2002) ; and (2) statistical approach: based on the n-gram models where n is usually 2, i.e. bigram model (Lin and Tsai, 1987; Gu et al., 1991; Fu et al., 1996; Ho et al., 1997; Sproat, 1990; Gao et al., 2002; Lee 2003) . From the studies (Hsu 1994; Tsai and Hsu, 2002; Gao et al., 2002; Kee, 2003;<cite> Tsai, 2005)</cite> , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_4",
  "x": "Conventionally, there are two approaches to resolve the two critical problems: (1) linguistic approach: based on syntax parsing, semantic template matching and contextual information (Hsu, 1994; Fu et al., 1996; Hsu et al., 1999; Kuo, 1995; Tsai and Hsu, 2002) ; and (2) statistical approach: based on the n-gram models where n is usually 2, i.e. bigram model (Lin and Tsai, 1987; Gu et al., 1991; Fu et al., 1996; Ho et al., 1997; Sproat, 1990; Gao et al., 2002; Lee 2003) . From the studies (Hsu 1994; Tsai and Hsu, 2002; Gao et al., 2002; Kee, 2003;<cite> Tsai, 2005)</cite> , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake. The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems. In our previous work<cite> (Tsai, 2005)</cite> , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively. In<cite> (Tsai, 2005)</cite> , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems. As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences. Since the identified character ratio of the WP identifier <cite>(Tsai, 2005</cite> ) is about 55%, there are still about 15% improving room left.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_5",
  "x": "In our previous work<cite> (Tsai, 2005)</cite> , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively. In<cite> (Tsai, 2005)</cite> , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems. As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences. Since the identified character ratio of the WP identifier <cite>(Tsai, 2005</cite> ) is about 55%, there are still about 15% improving room left. The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database. We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method Editor 2003, MSIME) , and an optimized bigram model, BiGram<cite> (Tsai, 2005)</cite> , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier. The remainder of this paper is arranged as follows.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_6",
  "x": "In<cite> (Tsai, 2005)</cite> , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems. As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences. Since the identified character ratio of the WP identifier <cite>(Tsai, 2005</cite> ) is about 55%, there are still about 15% improving room left. The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database. We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method Editor 2003, MSIME) , and an optimized bigram model, BiGram<cite> (Tsai, 2005)</cite> , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier. The remainder of this paper is arranged as follows. In Section 2, we present an auto wordpair (AUTO-WP) generation used to generate the WP database.",
  "y": "motivation"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_7",
  "x": "The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems. In our previous work<cite> (Tsai, 2005)</cite> , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively. In<cite> (Tsai, 2005)</cite> , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems. As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences. Since the identified character ratio of the WP identifier <cite>(Tsai, 2005</cite> ) is about 55%, there are still about 15% improving room left. The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database. We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method Editor 2003, MSIME) , and an optimized bigram model, BiGram<cite> (Tsai, 2005)</cite> , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier.",
  "y": "uses"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_8",
  "x": "The UDN2001 corpus is a collection of 4,539624 Chinese sentences extracted from whole 2001 UDN (United Daily News, 2001) Website in Taiwan (Tsai and Hsu, 2002) . The system dictionary provides the knowledge of words and their corresponding pinyin syllable-words. The pinyin syllable-words were translated by phoneme-to-pinyin mappings, such as \"\u3129\u02ca\"-to-\"ju2.\" ---------------------------------- **AUTO-GENERATION OF WP DATABASE** Following<cite> (Tsai, 2005)</cite> , the three steps of autogenerating word-pairs (AUTO-WP) for a given Chinese sentence are as below: (the details of AUTO-WP can be found in<cite> (Tsai, 2005))</cite> Step 1. Get forward and backward word segmentations: Generate two types of word segmentations for a given Chinese sentence by forward maximum matching (FMM) and backward maximum matching (BMM) techniques (Chen et al., 1986; Tsai et al., 2004) with the system dictionary. Step 2. Get initial WP set: Extract all the combinations of word-pairs from the FMM and the BMM segmentations of Step 1 to be the initial WP set.",
  "y": "uses"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_9",
  "x": "**STW EXPERIMENT RESULTS OF THE WSM** The purpose of this experiment is to demonstrate the tonal and toneless STW accuracies among the identified words by using the WSM with the system WP database. The comparative system is the WP identifier<cite> (Tsai, 2005)</cite> . Table  2 is the experimental results. The WP database and system dictionary of the WP identifier is same with that of the WSM. From Table 2 ----------------------------------",
  "y": "uses"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_10",
  "x": "We selected Microsoft Input Method Editor 2003 for Traditional Chinese (MSIME) as our experimental commercial Chinese input system. In addition, following<cite> (Tsai, 2005)</cite> , an optimized bigram model called BiGram was developed. The BiGram STW system is a bigrambased model developing by SRILM (Stolcke, 2002) with Good-Turing back-off smoothing (Manning and Schuetze, 1999) , as well as forward and backward longest syllable-word first strategies (Chen et al., 1986; Tsai et al., 2004) . The system dictionary of the BiGram is same with that of the WP identifier and the WSM. Table 3a compares the results of the MSIME, the MSIME with the WP identifier and the MSIME with the WSM on the closed and open test sentences. a STW accuracies and improvements of the words identified by the MSIME (Ms) with the WP identifier b STW accuracies and improvements of the words identified by the MSIME (Ms) with the WSM Table 3a . The results of tonal and toneless STW experiments for the MSIME, the MSIME with the WP identifier and with the WSM.",
  "y": "uses"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_11",
  "x": "(Note that, as per<cite> (Tsai, 2005)</cite> , the differences between the tonal and toneless STW accuracies of the BiGram and the TriGram are less than 0.3%). Table 3c is the results of the MSIME and the BiGram by using the WSM as an adaptation processing with both system and user WP database. From Table 3c , we get the average tonal and toneless STW improvements of the MSIME and the BiGram by using the WSM as an adaptation processing are 37.2% and 34.6%, respectively. Table 3c . The results of tonal and toneless STW experiments for the MSIME and the BiGram using the WSM as an adaptation processing. ---------------------------------- **MS+WSM (ICR**",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_12",
  "x": "This observation is similarly with that of our previous work<cite> (Tsai, 2005)</cite> . (2) The major problem of error conversions in tonal and toneless STW systems is different. This observation is similarly with that of<cite> (Tsai, 2005)</cite> . From Table 4 , the major improving targets of tonal STW performance are the HS errors because more than 50% tonal STW errors caused by HS problem. On the other hand, since the ISWS errors cover more than 50% toneless STW errors, the major targets of improving toneless STW performance are the ISWS errors. This observation should answer the question \"Why the STW performance of Chinese input systems (MSIME and BiGram) with the WSM is better than that of these systems with the WP-identifier?\" To sum up the above three observations and all the STW experimental results, we conclude that the WSM is able to achieve better STW improvements than that of the WP identifier is because: (1) the identified character ratio of the WSM is 15% greater than that of the WP identifier with the same WP database and dictionary, and meantime (2) the WSM not only can maintain the ratio of the three STW error types but also can reduce the total number of error characters of converted words than that of the WP identifier.",
  "y": "similarities"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_13",
  "x": "(1) The coverage of unknown word problem for tonal and toneless STW conversions is similar. In most Chinese input systems, unknown word extraction is not specifically a STW problem, therefore, it is usually taken care of through online and offline manual editing processing (Hsu et al, 1999) . The results of Table 4 show that the most STW errors should be caused by ISWS and HS problems, not UW problem. This observation is similarly with that of our previous work<cite> (Tsai, 2005)</cite> . (2) The major problem of error conversions in tonal and toneless STW systems is different. This observation is similarly with that of<cite> (Tsai, 2005)</cite> . From Table 4 , the major improving targets of tonal STW performance are the HS errors because more than 50% tonal STW errors caused by HS problem.",
  "y": "similarities"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_14",
  "x": "In this paper, we present a word support model (WSM) to improve the WP identifier<cite> (Tsai, 2005)</cite> and support the Chinese Language Processing on the STW conversion problem. All of the WP data can be generated fully automatically by applying the AUTO-WP on the given corpus. We are encouraged by the fact that the WSM with WP knowledge is able to achieve state-of-the-art tonal and toneless STW accuracies of 99% and 92%, respectively, for the identified poly-syllabic words. The WSM can be easily integrated into existing Chinese input systems by identifying words as a post processing. Our experimental results show that, by applying the WSM as an adaptation processing together with the MSIME (a trigram-like model) and the BiGram (an optimized bigram model), the average tonal and toneless STW improvements of the two Chinese input systems are 37% and 35%, respectively. Currently, our WSM with the mixed WP database comprised of UDN2001 and AS WP database is able to achieve more than 98% identified character ratios of poly-syllabic words in tonal and toneless STW conversions among the UDN2001 and the AS corpus. Although there is room for improvement, we believe it would not produce a noticeable effect as far as the STW accuracy of poly-syllabic words is concerned.",
  "y": "extends"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_0",
  "x": "It is unclear whether such an approach can be directly applied to E2E models. Phoneme-only E2E systems have been shown to have inferior performance compared to grapheme or wordpiece models (WPM) in general <cite>[16,</cite> 17] , but shows better recognition of rare words and proper nouns. In this work we propose to incorporate phonemes to a wordpiece E2E model as modeling units and use phoneme-level FST for contextual biasing. We propose a word-frequency based sampling strategy to randomly tokenize rare words into phonemes in the target sequence using a lexicon. This approach also mitigates accuracy regressions that have been observed when using phoneme-only E2E models <cite>[16,</cite> 17] . We train our model using only American English data and thus its wordpieces and phoneme set (no data from foreign languages). In inference, given a list of foreign words, we bias the recognition using an English phoneme-level biasing FST, which is built by first tokenizing the words into foreign phonemes and then mapping them to English phonemes using [15] .",
  "y": "background"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_1",
  "x": "In [15] , contextual biasing has been used to assist recognition of foreign words. With the phoneme mapping from a foreign language phoneme set to the recognizer's phoneme set, foreign words are modeled as a phoneme-level contextual FST for biasing. It is unclear whether such an approach can be directly applied to E2E models. Phoneme-only E2E systems have been shown to have inferior performance compared to grapheme or wordpiece models (WPM) in general <cite>[16,</cite> 17] , but shows better recognition of rare words and proper nouns. In this work we propose to incorporate phonemes to a wordpiece E2E model as modeling units and use phoneme-level FST for contextual biasing. We propose a word-frequency based sampling strategy to randomly tokenize rare words into phonemes in the target sequence using a lexicon. This approach also mitigates accuracy regressions that have been observed when using phoneme-only E2E models <cite>[16,</cite> 17] .",
  "y": "motivation"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_2",
  "x": "A wordpiece-phoneme model differs from a wordpiece-only model in that it may decompose a few words to phonemes in training. The output of the model is a single softmax whose symbol set is the union of wordpiece and phoneme symbols. We use a pronunciation lexicon to obtain phoneme sequences of words. Since phonemes show strength in recognizing rare words<cite> [16]</cite> , we want to present these words as phonemes more often. In a target sentence, we decide to randomly present the i th word as phonemes with a probability , 1.0) where p0 and T are constants and c(i) is an integer representing the number of time the word appears in our entire training corpus. Therefore, the words that appear T times or less will be presented as phonemes with probability p0.",
  "y": "motivation"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_3",
  "x": "In a target sentence, we decide to randomly present the i th word as phonemes with a probability , 1.0) where p0 and T are constants and c(i) is an integer representing the number of time the word appears in our entire training corpus. Therefore, the words that appear T times or less will be presented as phonemes with probability p0. For words that appear more than T times, the more frequent they are, the less likely they are presented as phonemes 2 . Note that the decision of whether to use wordpieces or phonemes is made randomly at each gradient iteration, and thus a given sentence could have different target sequences at different epochs. We use context-independent phonemes as in<cite> [16]</cite> . ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_4",
  "x": "Figure 1 : Contextual FST for the word \"Cr\u00e9teil\" using a sequence of English phonemes \"k r\\ E t E j\". ---------------------------------- **DECODING GRAPH** To generate words as outputs, we search through a decoding graph similar to<cite> [16]</cite> but accept both phonemes and wordpieces. An example is shown in Figure 2 . The decoding FST has wordpiece loops around state 0 (we show only a few for simplicity), but also has a pronunciation section (states 1 through 14) . The pronunciation section is a prefix tree with phonemes as inputs, and outputs are wordpieces of the corresponding word produced by the WPM in Section 3.1.",
  "y": "differences similarities"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_5",
  "x": "For clarity, we omitted most wordpieces for the state 0. Based on<cite> [16]</cite> , we add two improvements to the decoding strategy. First, during decoding we consume as many input epsilon arcs as possible thus guaranteeing that all wordpieces in word are produced when all corresponding phonemes are seen in the input. Second, we merge paths that have the same output symbols. Given the nature of our training and decoding, a given word can be output either directly in wordpieces, or transduced from phonemes to wordpieces. Since the input symbols are different, each hypothesis has a different probability. We keep track of equivalent hypotheses and recombine them by adding their probabilities, assigning the total probability to the most likely hypothesis, and dropping the others from the beam.",
  "y": "extends"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_6",
  "x": "We attribute the superior per- formance of the wordpiece-phoneme model to the robustness of phonemes to OOV words, as observed in<cite> [16]</cite> . Since the wordpiece-phoneme model contains both wordpieces and phonemes as modeling units, we can further perform wordpiece biasing in addition to phoneme-based biasing by building a wordpiece FST in parallel to the phoneme FST. This further reduces the WER by 2%, as shown in the bottom row in Table 1 . This shows that wordpiece and phoneme biasing are complementary to each other. We note that the same weights are used for both phoneme and wordpiece biasing, and empirically we did not find significant improvements by using different weights. On the other hand, for wordpiece model biasing, our results are consistent with the observation in [13] that the wordpieces perform better than graphemes because of its sparsity in matching longer units. To further understand how biasing helps recognizing French place names, we present some wins of wordpiecephoneme model in Table 2 .",
  "y": "similarities"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_7",
  "x": "One potential approach to improve regression is to incorporate an English external language model for phonemes in rescoring, similarly to the wordpiece-based rescoring in [10] . However, we note that the regression is significantly smaller than the all-phoneme model in<cite> [16]</cite> . ---------------------------------- **EFFECT OF NUMBER OF BIASING WORDS** Given examples in Table 3 , we are curious how competing biasing words affect recognition. We thus randomly choose a fixed number of biasing words (including the ground truth one) and vary the number to see how WER changes. Figure 3 shows that WER for the Directions set is 9.1% when only the ground truth word is present (i.e. number of biasing words is 1), and the rate increases quickly when the total number of biasing words increases.",
  "y": "differences"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_0",
  "x": "To answer this question we explore zero-shot cross-lingual semantic parsing where we train an available coarse-to-fine semantic parser (Liu et al., 2018) using cross-lingual word embeddings and universal dependencies in English and test it on Italian, German and Dutch. Results on the Parallel Meaning Bank -a multilingual semantic graphbank, show that Universal Dependency features significantly boost performance when used in conjunction with other lexical features but modeling the UD structure directly when encoding the input does not. ---------------------------------- **INTRODUCTION** Semantic parsing is a task of transducing natural language to meaning representations, which in turn can be expressed through many different semantic formalisms including lambda calculus (Zettlemoyer and Collins, 2012) , DCS (Liang et al., 2013) , Discourse Representation Theory (DRT) (Kamp and Reyle, 2013) , AMR (Banarescu et al., 2013) and so on. This availability of annotated data in English has translated into the development of a plethora of models, including encoder-decoders (Dong and Lapata, 2016; Jia and Liang, 2016) as well as tree or graph-structured decoders Lapata, 2016, 2018;<cite> Liu et al., 2018</cite>; Yin and Neubig, 2017) . Whereas the majority of semantic banks focus on English, recent effort has focussed on *Work done when Jingfeng Yang was an intern and Federico Fancellu a post-doc at the University of Edinburgh building multilingual representations, e.g. PMB (Abzianidze et al., 2017) , MRS (Copestake et al., 1995) and FrameNet (Pad\u00f3 and Lapata, 2005) .",
  "y": "background"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_1",
  "x": "Figure 1 shows an example DRT for the sentence 'I sat down and opened my laptop' in its canonical 'box' representation. A DRS is a nested structure with the top part containing the discourse references and the bottom with unary and binary predicates, as well as semantic constants (e.g. 'speaker'). DRS can be linked to each other via logic operator (e.g. \u00ac, \u2192, \u22c4) or, as in this case, discourse relations (e.g. CONTINUATION, RESULT, ELABORA-TION, etc.). To test our approach we leverage the DRT parser of <cite>Liu et al. (2018)</cite> , an encoder-decoder architecture where the meaning representation is reconstructed in three stages, coarse-to-fine, by first building the DRS skeleton (i.e. the 'box' structures) and then fill each DRS with predicates and variables. Whereas the original parser utilizes a sequential Bi-LSTM encoder with monolingual lexical features, we experiment with languageindependent features in the form of cross-lingual word-embeddings, universal PoS tags and universal dependencies. In particular, we also make use of tree encoders to assess whether modelling syntax can be beneficial in cross-lingual settings, as shown for other semantic tasks (e.g. negation scope detection (Fancellu et al., 2018) ). Results show that language-independent features are a valid alternative to projection methods for cross-lingual semantic parsing.",
  "y": "extends"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_2",
  "x": "We show that adding dependency relation as features is beneficial, even when they are the only feature used during encoding. However, we also show that modeling the dependency structure directly via tree encoders does not outperform a sequential BiLSTM architecture for the three languages we have experimented with. ---------------------------------- **METHODS** ---------------------------------- **MODEL** In this section, we describe the modifications to the coarse-to-fine encoder-decoder architecture of <cite>Liu et al. (2018)</cite> ; for more detail, we refer the reader to the original paper.",
  "y": "extends"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_3",
  "x": "---------------------------------- **ENCODER** BiLSTM. We use <cite>Liu et al. (2018)</cite> 's Bi-LSTM as baseline. However, whereas the original model represents each token in the input sentence as the concatenation of word (e w i ) and lemma embeddings, we discard the latter and add a POS tag embedding (e p i ) and dependency relation embedding (e d i ) feature. These embeddings are concatenated to represent the input token. The final encoder representation is obtained by concatenating both final forward and backward hidden states.",
  "y": "uses"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_4",
  "x": "This is computed following Equation (2) where i is the position of the word, j is the jth dimension in total d dimensions. Bi/treeLSTM. Finally, similarly to Chen et al. (2017) , we combine tree-LSTM and Bi-LSTM, where a tree-LSTM come is initialized using the last layer of a Bi-LSTM, which encodes order information. Computation is shown in Equation (3). ---------------------------------- **DECODER** The decoder of <cite>Liu et al. (2018)</cite> reconstructs the DRS in three steps, by first predicting the overall structure (the 'boxes'), then the predicates and finally the referents, with each subsequent step being conditioned on the output of the previous.",
  "y": "background"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_5",
  "x": "During predicate prediction, the decoder uses a copying mechanism to predict those unary predicates that are also lemmas in the input sentence (e.g. 'eat'). For the those that are not, soft attention is used instead. No modifications were done to the decoder; for more detail, we refer the reader to the original paper. ---------------------------------- **DATA** We use the PMB v. In order to be used as input to the parser, <cite>Liu et al. (2018)</cite> first convert the DRS into treebased representations, which are subsequently linearized into PTB-style bracketed sequences. This transformation is lossless in that re-entrancies are duplicated to fit in the tree structure.",
  "y": "background"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_6",
  "x": "**EVALUATION** We use Counter (Van Noord et al., 2018) to evaluate the performance of our models. Counter looks for the best alignment between the predicted and gold DRS and computes precision, recall and F1. For further details about Counter, the reader is referred to Van Noord et al. (2018) . It is worth reminding that unlike other work on the PMB (e.g. van Noord et al., 2018), <cite>Liu et al. (2018)</cite> does not deal with presupposition. In the PMB, presupposed variables are extracted from a main box and included in a separate one. In our work, we revert this process so to ignore presupposed boxes.",
  "y": "background"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_7",
  "x": "Previous work have explored two main methods for cross-lingual semantic parsing. One method requires parallel corpora to extract alignments between source and target languages using machine translation (Pad\u00f3 and Lapata, 2005; Damonte and Cohen, 2017; Zhang et al., 2018) The other method is to use parameter-shared models in the target language and the source language by leveraging language-independent features such as multilingual word embeddings, Universal POS tags and UD Duong et al., 2017; Susanto and Lu, 2017; Mulcaire et al., 2018) . For semantic parsing, encoder-decoder mod- els have achieved great success. Amongst these, tree or graph-structured decoders have recently shown to be state-of-the-art Lapata, 2016, 2018;<cite> Liu et al., 2018</cite>; Cheng et al., 2017; Yin and Neubig, 2017) . ---------------------------------- **CONCLUSIONS** We go back to the questions in the introduction: Can we train a semantic parser in a language where annotation is available?.",
  "y": "background"
 },
 {
  "id": "ce0441b3ae7b957520d329799f8b9f_0",
  "x": "It is straight-forward to compare the performance of the set of systems that produce the same form of output, e.g., Penn Treebank-based parsers can be compared in terms of how well they reproduce the Penn Treebank. Alternative systems based on a standard are largely interchangeable. Thus a system that uses one PennTreebank-based parser as a component can easily be adapted to use another better performing PennTreebank-based parser. Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; <cite>Meyers et al., 2004)</cite> . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable?",
  "y": "background"
 },
 {
  "id": "ce0441b3ae7b957520d329799f8b9f_1",
  "x": "Alternative systems based on a standard are largely interchangeable. Thus a system that uses one PennTreebank-based parser as a component can easily be adapted to use another better performing PennTreebank-based parser. Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; <cite>Meyers et al., 2004)</cite> . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004), English (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; <cite>Meyers et al., 2004)</cite> , Czech (Sgall et al., 2004) and German (Baumann et al., 2004).",
  "y": "background"
 },
 {
  "id": "ce0441b3ae7b957520d329799f8b9f_2",
  "x": "Alternative systems based on a standard are largely interchangeable. Thus a system that uses one PennTreebank-based parser as a component can easily be adapted to use another better performing PennTreebank-based parser. Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; <cite>Meyers et al., 2004)</cite> . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004) , English (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; <cite>Meyers et al., 2004)</cite> , Czech (Sgall et al., 2004) and German (Baumann et al., 2004) .",
  "y": "background"
 },
 {
  "id": "ce0441b3ae7b957520d329799f8b9f_3",
  "x": "For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; <cite>Meyers et al., 2004)</cite> . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004) , English (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; <cite>Meyers et al., 2004)</cite> , Czech (Sgall et al., 2004) and German (Baumann et al., 2004) . The sixth, seventh and eighth papers (Baumann et al., 2004; \u00c7 mejrek et al., 2004; Helmreich et al., 2004) explore questions of multilingual annotation of syntax and semantics, beginning to answer the question of how annotation systems can be made compatible across languages. Indeed (Helmreich et al., 2004) explores the question of integration across languages, as well as levels of annotation. (Baumann et al., 2004) also describes how a number of different linguistic levels can be related in annotation (pragmatic and prosodic) among two languages (English and German).",
  "y": "background"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_0",
  "x": "The role of the discriminator is to try to distinguish between generated and real poems during training. We propose to add inductive bias via the choice of discriminator architecture: We require the discriminator to reason about poems through pairwise comparisons between line ending words. These learned word comparisons form a similarity matrix for the poem within the discriminator's architecture. Finally, the discriminator evaluates the poem through a 2D convolutional classifier applied directly to this matrix. This final convolution is naturally biased to identify spatial patterns across word comparisons, which, in turn, biases learned word comparisons to pick up rhyming since rhymes are typically the most salient spatial patterns. Recent work by <cite>Lau et al. (2018)</cite> proposes a quatrain generation method that relies on specific domain knowledge about the dataset to train a classifier for learning the notion of rhyming: that a line ending word always rhymes with exactly one more ending word in the poem. This limits the applicability of their method to other forms of poetry with different rhyming patterns.",
  "y": "background"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_1",
  "x": "This final convolution is naturally biased to identify spatial patterns across word comparisons, which, in turn, biases learned word comparisons to pick up rhyming since rhymes are typically the most salient spatial patterns. Recent work by <cite>Lau et al. (2018)</cite> proposes a quatrain generation method that relies on specific domain knowledge about the dataset to train a classifier for learning the notion of rhyming: that a line ending word always rhymes with exactly one more ending word in the poem. This limits the applicability of their method to other forms of poetry with different rhyming patterns. They train the classifier along with a language model in a multi-task setup. Further, at generation time, they heavily rely on rejection sampling to produce quatrains which satisfy any valid rhyming pattern. In contrast, we find that generators trained using our structured adversary produce samples that satisfy rhyming constraints with much higher frequency. We propose a structured discriminator to learn a poetry generator in a generative adversarial setup.",
  "y": "motivation"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_2",
  "x": "Finally, the discriminator evaluates the poem through a 2D convolutional classifier applied directly to this matrix. This final convolution is naturally biased to identify spatial patterns across word comparisons, which, in turn, biases learned word comparisons to pick up rhyming since rhymes are typically the most salient spatial patterns. Recent work by <cite>Lau et al. (2018)</cite> proposes a quatrain generation method that relies on specific domain knowledge about the dataset to train a classifier for learning the notion of rhyming: that a line ending word always rhymes with exactly one more ending word in the poem. This limits the applicability of their method to other forms of poetry with different rhyming patterns. They train the classifier along with a language model in a multi-task setup. Further, at generation time, they heavily rely on rejection sampling to produce quatrains which satisfy any valid rhyming pattern. In contrast, we find that generators trained using our structured adversary produce samples that satisfy rhyming constraints with much higher frequency.",
  "y": "differences"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_3",
  "x": "---------------------------------- **NEURAL GENERATION MODEL** Our generator is a hierarchical neural language model ( Figure 1 ) that first generates a sequence of line-ending words, and thereafter generates the poem's lines conditioned on the ending words. We use recurrent neural networks for ending word generation as well line generation conditioned on ending words. Following prior work<cite> (Lau et al., 2018)</cite>, we generate words in each line in reverse order (i.e. right to left), and begin generation with the last line first. Letx represent a sample from the current generator distribution, denoted by p \u03b8 , where \u03b8 represents the generator parameters. We initialize the word embeddings in the generator with pre-trained word embeddings<cite> (Lau et al., 2018)</cite> trained on a separate non-sonnet corpus.",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_4",
  "x": "Following prior work<cite> (Lau et al., 2018)</cite>, we generate words in each line in reverse order (i.e. right to left), and begin generation with the last line first. Letx represent a sample from the current generator distribution, denoted by p \u03b8 , where \u03b8 represents the generator parameters. We initialize the word embeddings in the generator with pre-trained word embeddings<cite> (Lau et al., 2018)</cite> trained on a separate non-sonnet corpus. ---------------------------------- **STRUCTURED DISCRIMINATOR** We introduce a structured discriminator, denoted by function f \u03c6 (x), which outputs the probability that x is a sample from the dataset as opposed to generated. Our architecture defines an intermediate matrix S \u2208 R T \u00d7T , where T denotes the number of lines in the poem, which encodes pair-wise similarities between line ending words in order to capture rhyming structure.",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_5",
  "x": "**EXPERIMENTS AND RESULTS** ---------------------------------- **DATASETS** We work with the Shakespeare SONNET dataset<cite> (Lau et al., 2018</cite> ) and a new LIMERICK corpus. Each sonnet in the Sonnet dataset is made up of 3 quatrains of 4 lines each, and a couplet. The dataset consists of 2685 sonnets in train, and 335 each in validation and test splits. The quatrains typically have one of the following rhyming structures: AABB, ABAB, ABBA, though some deviations are observed in the dataset.",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_6",
  "x": "---------------------------------- **POEM GENERATOR** Sampling efficiency We compute the expected number of samples needed before we sample a quatrain which satisfies one of the hand-defined rhyming patterns. Towards this end, we get 10K samples from each model without any constraints (except avoiding UNK -unknown tokens). Fol-lowing prior work<cite> (Lau et al., 2018)</cite> , words are sampled with a temperature value between 0.6 and 0.8. We use CMU dictionary (Weide, 1998) to look up the phonetic representation of a word, and extract the sequence of phonemes from the last stressed syllable onward. Two words are considered to be rhyming if their extracted sequences match (Parrish, 2015) .",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_7",
  "x": "Our model without adversarial learning i.e. RHYME-LM, achieves a test set NLL of 3.97. DEEP-SPEARE reports a test set NLL of 4.38. Note that our language model is hierarchical while DEEP-SPEARE has a linear model. The NLL for RHYME-LM and RHYME-GAN are very similar, though RHYME-GAN gets much better sampling efficiency scores than RHYME-LM. Our generator implementation is largely based on that of <cite>Lau et al. (2018)</cite> . The main difference is that we first generate all the line-ending words and then condition on them to generate the remaining words. The change was made to make it more amenable to our proposed discriminator.",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_8",
  "x": "Note that our language model is hierarchical while DEEP-SPEARE has a linear model. The NLL for RHYME-LM and RHYME-GAN are very similar, though RHYME-GAN gets much better sampling efficiency scores than RHYME-LM. Our generator implementation is largely based on that of <cite>Lau et al. (2018)</cite> . The main difference is that we first generate all the line-ending words and then condition on them to generate the remaining words. The change was made to make it more amenable to our proposed discriminator. However, our hierarchical language model (RHYME-LM) performs worse than DEEP-SPEARE as per sampling efficiency. Therefore, structured discriminator is the driving factor behind the observed improvement with RHYME-GAN.",
  "y": "extends"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_9",
  "x": "Following prior work<cite> (Lau et al., 2018)</cite> , we requested human annotators to identify the humanwritten poem when presented with two samples at a time -a quatrain from the Sonnet corpus and a machine-generated quatrain, and report the annotator accuracy on this task. Note that a lower accuracy value is favorable as it signifies higher quality of machine-generated samples. Using 150 valid samples (i.e. samples belonging to one of the allowed rhyming patterns), we observe 56.0% annotator accuracy for RHYME-GAN, and 53.3% for DEEP-SPEARE -indicating that the post-rejection sampling outputs from the two methods are of comparable quality (the difference in annotator accuracy is not statistically significant as per McNemar's test under p < 0.05). If we use pre-rejection samples, we observe 60.0% annotator accuracy for RHYME-GAN, and 81.3% for DEEP-SPEARE (the difference being statistically significant as per McNemar's test under p < 0.05) -indicating that unfiltered samples from RHYME-GAN are of higher quality compared to DEEP-SPEARE. ---------------------------------- **RELATED WORK** Early works on poetry generation mostly used rule based methods (Gerv\u00e1s, 2000; Wu et al., 2009; Oliveira, 2017) .",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_10",
  "x": "More recently, neural models for poetry generation have been proposed (Zhang and Lapata, 2014; Ghazvininejad et al., 2016 Ghazvininejad et al., , 2017 Hopkins and Kiela, 2017;<cite> Lau et al., 2018</cite>; Liu et al., 2019) . Yan et al. (2013) retrieve high ranking sentences for a given user query, and repeatedly swap words to satisfy poetry constraints. Ghazvininejad et al. (2018) worked on poetry translation using an unconstrained machine translation model and separately learned Finite State Automata for enforcing rhythm and rhyme. Similar to rhyming and rhythm patterns in poetry, certain types of musical compositions showcase rhythm and repetition patterns, and some prior works model such patterns in music generation (Walder and Kim, 2018; Jhamtani and BergKirkpatrick, 2019) . Generative adversarial learning (Goodfellow et al., 2014) for text generation has been used in prior works (Fedus et al., 2018; Wang et al., 2018 Wang et al., , 2019 Rao and Daum\u00e9 III, 2019) , though has not been explored with regard to the similarity structure proposed in this paper. ---------------------------------- **CONCLUSIONS**",
  "y": "background"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_0",
  "x": "Progress in this area has seemed swift and impressive, but the community is now scrutinising the results to understand whether enthusiasm is warranted. Several diagnostic datasets have been proposed with this goal in mind, highlighting various flaws in existing tasks (Johnson et al., 2017; Zhang et al., 2015) . Our paper is a contribution to these efforts, showing that the field may have moved too fast from noun to sentence interpretation, overlooking difficulties in understanding other parts-of-speech. Our paper expands the existing FOIL dataset <cite>(Shekhar et al., 2017)</cite> . FOIL consists of a set of images matched with captions containing one single mistake. The mistakes are always nouns referring to objects not actually present in the image. The work demonstrates that the language and vision modalities are not truly integrated in current computational models, as they fail to spot the mistake in the caption and to correct it appropriately (humans, on the other hand, obtain almost 100% accuracy on those tasks).",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_1",
  "x": "The results obtained by state-of-the-art systems on this data demonstrate that current models are indeed little able to move beyond object understanding. Figure 1 : Sample image, corresponding original caption and the generated foil caption for the different parts of speech. The model has to be able to classify the caption as 'correct' or 'foil' (Task 1); detect the foil word in the foil caption (see words highlighted in red) (Task 2); and correct the foil word with an appropriate replacement (see words highlighted in green) (Task 3). ---------------------------------- **THE FOIL METHODOLOGY** We follow the methodology highlighted in<cite> Shekhar et al. (2017)</cite> , which consists of replacing a single word in a human-generated caption with a 'foil' item, making the caption unsuitable to describe the original image. Given such replacements, the system should be able to perform three tasks: a) a classification task (T1): given an image and a caption, the model has to predict whether the caption is correct or inappropriate for the image (evaluating whether the model has a coarse understanding of the linguistic and visual inputs and their relations); b) a foil word detection task (T2): given an image and a foil caption, detect the foil word in the caption (evaluating whether the model reaches a fine-grained representation of the linguistic input); a foil word correction task (T3): given an image, a foil caption and the foil word, the model has to correct the mistake (verifying whether the model reaches a fine-grained representation of the image).",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_2",
  "x": "For T2, we subsequently occlude one word (Goyal et al. (2016) ) at a time and calculate the probability of the new caption to be good vs. foil. The model selects as foil word, the one which has generated the caption with the highest probability. For T3, we regress over all<cite> Shekhar et al. (2017)</cite> . the target words on the position of the foil word and select the one which generates the caption with the highest probability to be \"good\". Due to the generative nature of IC models, adapting IC-Wang for the classification purpose is less straightforward. For T1, we generate all possible captions by subsequently predicting one word at a time provided all other words in the caption and the image. We compare the probability of these generated captions with the given caption.",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_3",
  "x": "Following<cite> Shekhar et al. (2017)</cite> , we aim at creating a dataset of images associated with both correct and foil captions, where the latter are obtained by replacing one word in the original text. Expanding on the original paper, our target/foil pairs do not merely consist of nouns. The introduced error can also be an adjective (an object's attribute), a verb (an action), a preposition (a relation between objects) or an adverb (a manner of action). In total, we produce 196,284 datapoints, each corresponding to an <image, original, foil> triple. The starting point for images and correct captions is Microsoft's Common Objects in Context (MS-COCO) (Lin et al. (2014) ). ---------------------------------- **CREATING NEW TARGET/FOIL PAIRS**",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_4",
  "x": "We compare the probability of these generated captions with the given caption. When the test caption probability is higher than generated captions probabilities, we classify the given caption as good caption, else as foil caption. ---------------------------------- **DATASET CREATION** Following<cite> Shekhar et al. (2017)</cite> , we aim at creating a dataset of images associated with both correct and foil captions, where the latter are obtained by replacing one word in the original text. Expanding on the original paper, our target/foil pairs do not merely consist of nouns. The introduced error can also be an adjective (an object's attribute), a verb (an action), a preposition (a relation between objects) or an adverb (a manner of action).",
  "y": "extends"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_5",
  "x": "Using these prepositions, we generate target/foil pairs by coupling prepositions which belong to the same class. We obtain a total of 206 pairs (110, 90 and 6 for place, direction and device respectively). Nouns: The target/foil noun pairs are built using words that belong to the same category in MS-COCO (e.g., bird/dog, from the MS-COCO category ANIMAL). In order to obtain a balanced dataset across the various PoS, we only use a subset of the FOIL-COCO dataset of<cite> Shekhar et al. (2017)</cite> . From the FOIL dataset, we retain the 37,536 images for which foil captions could be generated, using the target/foil pairs extracted from the resources mentioned above. Of the FOIL datapoints generated for the noun pairs, only those containing images used for the other PoS are selected. Hence, the number of unique images of the whole dataset is the same of those used for nouns (see Table 1 for details of the train/test set division.)",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_6",
  "x": "From each resource, we randomly split the target/foil pairs into training and test sets. The number of unique pairs per PoS is provided in Table 1 . ---------------------------------- **FOIL CAPTION GENERATION** From the word pair lists above, foil captions are generated from MS-COCO original captions. The foil captions are generated by replacing nouns are directly extracted from the FOIL dataset by<cite> Shekhar et al. (2017)</cite> . In this case, for each original MS-COCO caption, several foil ones are generated and subsequently filtered using several heuristics.",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_7",
  "x": "These numbers, however, do not show to which extent the models are able to avoid the trap of the dataset:<cite> Shekhar et al. (2017)</cite> showed that on the FOIL data, models tend to detect correct captions with reasonable accuracy but fail to identify the incorrect ones, leading to a large bias in classification. Taking this insight into account, for the rest of this paper, we focus on the accuracy of the systems in dealing with foil captions, across all three tasks. As shown in Table 3 , the blind model's accuracy is still reasonable on T1, but lower than chance for nouns and adverbs. In the case of nouns, the visual input helps obtaining a higher accuracy, whereas this is not the case for the other PoS. This could be due to the ability of vision models to 'see' objects but not their properties (adjectives) or relations (verbs, prepositions) . It is a known shortcoming of such systems that they have difficulties in recognising anything that is not straightforwardly defined by a bounding box (Johnson et al. (2017) ). IC-Wang performs very poorly on verbs, adjectives and prepositions, even though it is the best system for nouns. Other models improve minimally on the baseline, with prepositions getting the best improvement: +7% for HieCoAtt.",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_8",
  "x": "HieCoAtt is the overall best performing model, but we note that it only outperforms the blind model by a few points. These numbers, however, do not show to which extent the models are able to avoid the trap of the dataset:<cite> Shekhar et al. (2017)</cite> showed that on the FOIL data, models tend to detect correct captions with reasonable accuracy but fail to identify the incorrect ones, leading to a large bias in classification. Taking this insight into account, for the rest of this paper, we focus on the accuracy of the systems in dealing with foil captions, across all three tasks. As shown in Table 3 , the blind model's accuracy is still reasonable on T1, but lower than chance for nouns and adverbs. In the case of nouns, the visual input helps obtaining a higher accuracy, whereas this is not the case for the other PoS. This could be due to the ability of vision models to 'see' objects but not their properties (adjectives) or relations (verbs, prepositions) . It is a known shortcoming of such systems that they have difficulties in recognising anything that is not straightforwardly defined by a bounding box (Johnson et al. (2017) ). IC-Wang performs very poorly on verbs, adjectives and prepositions, even though it is the best system for nouns.",
  "y": "background"
 },
 {
  "id": "cf7d01faf555f09973e44be400e768_0",
  "x": "In particular, we start with the gentle introduction to the fundamentals of reinforcement learning (Sutton and Barto, 1998; Sutton et al., 2000) . We further discuss We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017) . We then focus on a new case study of hierarchical deep reinforcement learning for video captioning<cite> (Wang et al., 2018b)</cite> , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems. This tutorial aims at introducing deep reinforcement learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in reinforcement learning. The intended length of the tutorial is 3 hours, including a coffee break.",
  "y": "uses"
 },
 {
  "id": "cf7d01faf555f09973e44be400e768_1",
  "x": "In particular, we start with the gentle introduction to the fundamentals of reinforcement learning (Sutton and Barto, 1998; Sutton et al., 2000) . We further discuss their modern deep learning extensions such as Deep QNetworks (Mnih et al., 2015) , Policy Networks (Silver et al., 2016) , and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016) . We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016) , semi-supervised text classification (Wu et al., 2018) , coreference (Clark and Manning, 2016; Yin et al., 2018) , knowledge graph reasoning (Xiong et al., 2017 ), text games (Narasimhan et al., 2015; He et al., 2016a) , social media (He et al., 2016b; Zhou and Wang, 2018) , information extraction (Narasimhan et al., 2016; Qin et al., 2018) , language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018) , etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017) . We then focus on a new case study of hierarchical deep reinforcement learning for video captioning<cite> (Wang et al., 2018b)</cite> , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems. This tutorial aims at introducing deep reinforcement learning methods to researchers in the NLP community.",
  "y": "uses"
 },
 {
  "id": "cf7d01faf555f09973e44be400e768_2",
  "x": "We describe the main contributions of this work: including its design of the reward functions, and why they are necessary for dialog. We then introduce the gigantic action space issue for deep Q-learning in NLP (He et al., 2016a,b) , including several solutions. To conclude this part, we discuss interesting applications of DRL in NLP, including information extraction and reasoning. \u2022 Lessons Learned, Future Directions, and Practical Advices for DRL in NLP Third, we switch from the theoretical presentations to an interactive demonstration and discussion session: we aim at providing an interactive session to transfer the theories of DRL into practical insights. More specifically, we will discuss three important issues, including problem formulation/model design, exploration vs. exploitation, and the integration of linguistic structures in DRL. We will show case a recent study<cite> (Wang et al., 2018b</cite> ) that leverages hierarchical deep reinforcement learning for language and vision, and extend the discussion. Practical advice including programming advice will be provided as a part of the demonstration.",
  "y": "uses"
 },
 {
  "id": "cffa735deb802118640005a1d527ee_0",
  "x": "While for the general language English newspaper domain syntactic (Marcus et al., 1993) , semantic (Palmer et al., 2005; Pustejovsky et al., 2003) , and even discourse (Carlson et al., 2003; Miltsakaki et al., 2008) annotations are increasingly made available, any language, domain, or genre shift pushes the severe burden on developers of NLP systems to supply comparably sized high-quality annotations. Even inner-domain shifts, such as, e.g., moving from hematology (Ohta et al., 2002) to the genetics of cancer (Kulick et al., 2004) within the field of molecular biology may have drastic consequences in the sense that entirely new meta data sets have to produced by annotation teams. Thus, reducing the human efforts for the creation of adequate training material is a major challenge. Active learning (AL) copes with this problem as it intelligently selects the data to be labeled. It is a sampling strategy where the learner has control over the training material to be manually annotated by selecting those examples which are of high utility for the learning process. AL has been successfully applied to speed up the annotation process for many NLP tasks without sacrificing annotation quality<cite> (Engelson and Dagan, 1996</cite>; Ngai and Yarowsky, 2000; Hwa, 2001; Tomanek et al., 2007a) . Once we decide to use AL for meta-data annotation and a reasonable, stable level of annotation quality is reachedafter having run through only a fraction of the documents compared with the traditional annotation approach where a randomly and independently selected amount of documents is sequentially annotated -an obvious question turns up: When do we stop the annotation process to cash in the time savings?",
  "y": "background"
 },
 {
  "id": "cffa735deb802118640005a1d527ee_1",
  "x": "**APPROXIMATING THE LEARNING CURVE** Given the idea that from the learning curve one can read the trade-off between annotation effort and classifier performance gain, we here propose an approach to approximate the progression of the learning curve which comes at no extra annotation costs. This approach is designed for use in committee-based AL (Seung et al., 1992) . A committee consists of k classifiers of the same type trained on different subsets of the already labeled (training) data. Each committee member then makes its predictions on the pool of unlabeled examples, and those examples on which the committee members express the highest disagreement are considered most informative for learning and are thus selected for manual annotation. To calculate the disagreement among the committee members several metrics have been proposed including the vote entropy<cite> (Engelson and Dagan, 1996)</cite> as possibly the most well-known one. Our approach to approximating the learning curve is based on the disagreement within a committee.",
  "y": "background"
 },
 {
  "id": "cffa735deb802118640005a1d527ee_2",
  "x": "To calculate the disagreement among the committee members several metrics have been proposed including the vote entropy<cite> (Engelson and Dagan, 1996)</cite> as possibly the most well-known one. Our approach to approximating the learning curve is based on the disagreement within a committee. However, it is independent of the actual metric used to calculate the disagreement. Although in our experiments we considered the NLP task of named entity recognition (NER) only, our approach is not limited to this scenario and can be expected to be applicable to other tasks as well. In Tomanek et al. (2007a) we introduced the selection agreement (SA) curve -the average agreement amongst the selected examples plotted over time. When the SA values are close to '1', the committee members almost perfectly agree. So, any further AL iteration would resemble a random selection.",
  "y": "similarities background"
 },
 {
  "id": "cffa735deb802118640005a1d527ee_3",
  "x": "For our experiments on approximating the learning curves for AL-based selection, we chose named entity recognition (NER) as the annotation task in focus. We employed the committee-based AL approach described in Tomanek et al. (2007a) . The committee consists of k = 3 Maximum Entropy (ME) classifiers (Berger et al., 1996) . In each AL iteration, each classifier is trained on a randomly , L being the set of all examples seen so far. Disagreement is measured by vote entropy<cite> (Engelson and Dagan, 1996)</cite> . In our NER scenario, complete sentences are selected by AL.",
  "y": "uses"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_0",
  "x": "We then compare the relative gains afforded by several other techniques proposed in the literature when starting with vanilla systems versus our stronger baselines, showing that experimental conclusions may change depending on the baseline chosen. This indicates that choosing a strong baseline is crucial for reporting reliable experimental results. ---------------------------------- **INTRODUCTION** In the relatively short time since its introduction, neural machine translation has risen to prominence in both academia and industry. Neural models have consistently shown top performance in shared evaluation tasks (Bojar et al., 2016; Cettolo et al., 2016) and are becoming the technology of choice for commercial MT service providers<cite> (Wu et al., 2016</cite>; Crego et al., 2016) . New work from the research community regularly introduces model extensions and algorithms that show significant gains over baseline NMT.",
  "y": "background"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_1",
  "x": "Our starting point for experimentation is a standard baseline neural machine translation system implemented using the Lamtram 1 and DyNet 2 toolkits (Neubig, 2015; Neubig et al., 2017) . This system uses the attentional encoder-decoder architecture described by Bahdanau et al. (2015) , building on work by Sutskever et al. (2014) . The translation model uses a bi-directional encoder with a single LSTM layer of size 1024, multilayer perceptron attention with a layer size of 1024, and word representations of size 512. Translation models are trained until perplexity convergence on held-out data using the Adam algorithm with a maximum step size of 0.0002 (Kingma and Ba, 2015; <cite>Wu et al., 2016)</cite> . Maximum training sentence length is set to 100 words. Model vocabulary is limited to the top 50K source words and 50K target words by frequency, with all others mapped to an unk token. A post-processing step replaces any unk tokens in system output by attempting a dictionary lookup 3 of the corresponding source word (highest attention score) and backing off to copying the source word directly (Luong et al., 2015) .",
  "y": "uses"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_2",
  "x": "This system uses the attentional encoder-decoder architecture described by Bahdanau et al. (2015) , building on work by Sutskever et al. (2014) . The translation model uses a bi-directional encoder with a single LSTM layer of size 1024, multilayer perceptron attention with a layer size of 1024, and word representations of size 512. Translation models are trained until perplexity convergence on held-out data using the Adam algorithm with a maximum step size of 0.0002 (Kingma and Ba, 2015; <cite>Wu et al., 2016)</cite> . Maximum training sentence length is set to 100 words. Model vocabulary is limited to the top 50K source words and 50K target words by frequency, with all others mapped to an unk token. A post-processing step replaces any unk tokens in system output by attempting a dictionary lookup 3 of the corresponding source word (highest attention score) and backing off to copying the source word directly (Luong et al., 2015) . Experiments in each section evaluate this system against incremental extensions such as improved model vocabulary or training algorithm.",
  "y": "background"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_3",
  "x": "To eliminate the need for schedules, subsequent NMT work trained models using the Adadelta algorithm, which automatically and continuously adapts learning rates for individual parameters during training (Zeiler, 2012) . Model performance is reported to be equivalent to SGD with annealing, though training still takes a considerable amount of time (Bahdanau et al., 2015; Sennrich et al., 2016b) . More recent work seeks to accelerate training with the Adam algorithm, which applies momentum on a per-parameter basis and automatically adapts step size subject to a user-specified maximum (Kingma and Ba, 2015) . While this can lead to much faster convergence, the resulting models are shown to slightly underperform compared to annealing SGD<cite> (Wu et al., 2016)</cite> . However, Adam's speed and reputation of generally being \"good enough\" have made it a popular choice for researchers and NMT toolkit authors 6 (Arthur et al., 2016; Lee et al., 2016; Britz et al., 2017; Sennrich et al., 2017) . While differences in automatic metric scores between SGD and Adam-trained systems may be relatively small, they raise the more general question of training effectiveness. In the following section, we explore the relative quality of the optima found by these training algorithms.",
  "y": "background differences motivation"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_4",
  "x": "More recent work seeks to accelerate training with the Adam algorithm, which applies momentum on a per-parameter basis and automatically adapts step size subject to a user-specified maximum (Kingma and Ba, 2015) . While this can lead to much faster convergence, the resulting models are shown to slightly underperform compared to annealing SGD<cite> (Wu et al., 2016)</cite> . However, Adam's speed and reputation of generally being \"good enough\" have made it a popular choice for researchers and NMT toolkit authors 6 (Arthur et al., 2016; Lee et al., 2016; Britz et al., 2017; Sennrich et al., 2017) . While differences in automatic metric scores between SGD and Adam-trained systems may be relatively small, they raise the more general question of training effectiveness. In the following section, we explore the relative quality of the optima found by these training algorithms. ---------------------------------- **RESULTS AND ANALYSIS**",
  "y": "motivation background"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_5",
  "x": "Model performance is reported to be equivalent to SGD with annealing, though training still takes a considerable amount of time (Bahdanau et al., 2015; Sennrich et al., 2016b) . More recent work seeks to accelerate training with the Adam algorithm, which applies momentum on a per-parameter basis and automatically adapts step size subject to a user-specified maximum (Kingma and Ba, 2015) . While this can lead to much faster convergence, the resulting models are shown to slightly underperform compared to annealing SGD<cite> (Wu et al., 2016)</cite> . However, Adam's speed and reputation of generally being \"good enough\" have made it a popular choice for researchers and NMT toolkit authors 6 (Arthur et al., 2016; Lee et al., 2016; Britz et al., 2017; Sennrich et al., 2017) . While differences in automatic metric scores between SGD and Adam-trained systems may be relatively small, they raise the more general question of training effectiveness. In the following section, we explore the relative quality of the optima found by these training algorithms. ----------------------------------",
  "y": "background"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_6",
  "x": "While Adam's use of momentum can be considered a form of \"self-annealing\", we also evaluate the novel extension of explicitly annealing the maximum step size by applying the same halving and restarting process used for SGD. It is important to note that while restarting SGD has no effect beyond changing the learning rate, restarting Adam causes the optimizer to \"forget\" the per-parameter learning rates and start fresh. For all training, we use a mini-batch size of 512 words. 8 For WMT systems, we evaluate dev 6 Adam is the default optimizer for the Lamtram, Nematus (https://github.com/rsennrich/nematus), and Marian toolkits (https://github.com/amunmt/ marian). 7 Learning rates of 0.5 for SGD and 0.0002 for Adam or very similar are shown to work well in NMT implementations including GNMT<cite> (Wu et al., 2016)</cite> , Nematus, Marian, and OpenNMT (http://opennmt.net). 8 For each mini-batch, sentences are added until the word set perplexity every 50K training sentences for the first training run and every 25K sentences for subsequent runs. For IWSLT systems, we evaluate every 25K sentences and then every 6,250 sentences.",
  "y": "uses"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_8",
  "x": "As<cite> Wu et al. (2016)</cite> show different levels of effectiveness for different sub-word vocabulary sizes, we evaluate running BPE with 16K and 32K merge operations. As shown in Table 2 , sub-word systems outperform full-word systems across the board, despite having fewer total parameters. Systems built on larger data generally benefit from larger vocabularies while smaller systems perform better with smaller vocabularies. Based on these results, we recommend 32K as a generally effective vocabulary size and 16K as a contrastive condition when building systems on less than 1 million parallel sentences. To understand the origin of these improvements, we divide the words in each test set into classes based on how the full-word and BPE models handle them and report the unigram F-1 score for each model on each class. We also plot the fullword and BPE vocabularies for context. As shown in Figure 2 , performance is comparable for the most frequent words that both models represent as single units.",
  "y": "similarities uses"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_9",
  "x": "When full-word NMT models struggle to learn translations for infrequent words, they can learn to simply trust the lexical or phrase-based model. However, when annealing Adam and BPE alleviate these underlying problems, the neural model's accuracy can match or exceed that of the pretrained model, making external scores either completely redundant or (in the worst case) harmful bias that must be overcome to produce correct translations. While pre-translation fares better than lexicon bias, it suffers a reversal in one scenario and a significant degradation in the other when moving from a single model to an ensemble. Even when bias from an external model improves translation, it does so at the cost of diversity by pushing the neural model's preferences toward those of the pre-trained model. These results further validate claims of the importance of diversity in model ensembles. Applying dropout significantly improves all configurations of the Czech-English system and some configurations of the English-French system, leveling off with the strongest. This trend follows previous work showing that dropout combats overfitting of small data, though the point of inflection is worth noting (Sennrich et al., 2016a; <cite>Wu et al., 2016)</cite> .",
  "y": "similarities"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_1",
  "x": "However, most of this research has focused on supervised methods requiring large amounts of labeled data. The supervision was either given in the form of meaning representations aligned with sentences (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007) or in a somewhat more relaxed form, such as lists of candidate meanings for each sentence (Kate and Mooney, 2007; Chen and Mooney, 2008) or formal representations of the described world state for each text<cite> (Liang et al., 2009)</cite> . Such annotated resources are scarce and expensive to create, motivating the need for unsupervised or semi-supervised techniques (Poon and Domingos, 2009 ). However, unsupervised methods have their own challenges: they are not always able to discover semantic equivalences of lexical entries or logical forms or, on the contrary, cluster semantically different or even opposite expressions (Poon and Domingos, 2009 ). Unsupervised approaches can only rely on distributional similarity of contexts (Harris, 1968) to decide on semantic relatedness of terms, but this information may be sparse and not reliable (Weeds and Weir, 2005) . For example, when analyzing weather forecasts it is very hard to discover in an unsupervised way which of the expressions among \"south wind\", \"wind from west\" and \"southerly\" denote the same wind direction and which are not, as they all have a very similar distribution of their contexts. The same challenges affect the problem of identification of argument roles and predicates.",
  "y": "background"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_3",
  "x": "We follow their set-up, but assume that instead of having access to the full semantic state for every training example, we have a very small amount of data annotated with semantic states and a larger number of unannotated texts with noncontradictory semantics. We study our set-up on the weather forecast data<cite> (Liang et al., 2009)</cite> where the original textual weather forecasts were complemented by additional forecasts describing the same weather states (see figure 1 for an example). The average overlap between the verbalized fields in each group of noncontradictory forecasts was below 35%, and more than 60% of fields are mentioned only in a single forecast from a group. Our model, learned from 100 labeled forecasts and 259 groups of unannotated non-contradictory forecasts (750 texts in total), achieved 73.9% F 1 . This compares favorably with 69.1% shown by a semi-supervised learning approach, though, as expected, does not reach the score of the model which, in training, observed semantics states for all the 750 documents (77.7% F 1 ). The rest of the paper is structured as follows. In section 2 we describe our inference algorithm for groups of non-contradictory documents.",
  "y": "uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_4",
  "x": "Our model, learned from 100 labeled forecasts and 259 groups of unannotated non-contradictory forecasts (750 texts in total), achieved 73.9% F 1 . This compares favorably with 69.1% shown by a semi-supervised learning approach, though, as expected, does not reach the score of the model which, in training, observed semantics states for all the 750 documents (77.7% F 1 ). The rest of the paper is structured as follows. In section 2 we describe our inference algorithm for groups of non-contradictory documents. Section 3 redescribes the semantics-text correspondence model<cite> (Liang et al., 2009)</cite> in the context of our learning scenario. In section 4 we provide an empirical evaluation of the proposed method. We conclude in section 5 with an examination of additional related work.",
  "y": "background"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_5",
  "x": "---------------------------------- **INFERENCE WITH NON-CONTRADICTORY DOCUMENTS** In this section we will describe our inference method on a higher conceptual level, not specifying the underlying meaning representation and the probabilistic model. An instantiation of the algorithm for the semantics-text correspondence model is given in section 3.2. Statistical models of parsing can often be regarded as defining the probability distribution of meaning m and its alignment a with the given text w, P (m, a, w) = P (a, w|m)P (m). The semantics m can be represented either as a logical formula (see, e.g., (Poon and Domingos, 2009 )) or as a set of field values if database records are used as a meaning representation<cite> (Liang et al., 2009</cite> ). The alignment a defines how semantics is verbalized in the text w, and it can be represented by a meaning derivation tree in case of full semantic parsing (Poon and Domingos, 2009) or, e.g., by a hierarchical segmentation into utterances along with an utterance-field alignment in a more shallow variation of the problem.",
  "y": "background"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_6",
  "x": "It starts with an empty ordering n = () and an empty list of meanings m = () (line 1). Then it iteratively predicts meaning representationsm j conditioned on the list of semantics m = (m 1 ,..., m i\u22121 ) fixed on the previous stages and does it for all the remaining texts w j (lines 3-5). The algorithm selects a single meaningm j which maximizes the probability of all the remaining texts and excludes the text j from future consideration (lines 6-7). Though the semantics m k (k / \u2208 n\u222a{j}) used in the estimates (line 6) can be inconsistent with each other, the final list of meanings m is guaranteed to be consistent. It holds because on each iteration we add a single meaningm n i to m (line 7), and m n i is guaranteed to be consistent with m , as the semanticsm n i was conditioned on the meaning m during inference (line 4). An important aspect of this algorithm is that unlike usual greedy inference, the remaining ('future') texts do affect the choice of meaning representations made on the earlier stages. As soon as semantics m k are inferred for every k, we find ourselves in the set-up of learning with unaligned semantic states considered in<cite> (Liang et al., 2009)</cite> .",
  "y": "uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_7",
  "x": "---------------------------------- **A MODEL OF SEMANTICS** In this section we redescribe the semantics-text correspondence model<cite> (Liang et al., 2009</cite> ) with an extension needed to model examples with latent states, and also explain how the inference algorithm defined in section 2 can be applied to this model. ---------------------------------- **MODEL DEFINITION** Liang et al. (2009) As explained in the introduction, the world states s are represented by sets of records (see the block in the middle of figure 1 for an example of a world state). Each record is characterized by a record type t \u2208 {1,..., T }, which defines the set of fields F (t) .",
  "y": "extends"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_9",
  "x": "When the world state is observable, learning does not require any approximations, as dynamic programming (a form of the forward-backward algorithm) can be used to infer the posterior distribution on the E-step<cite> (Liang et al., 2009)</cite> . However, when the state is latent, dependencies are not local anymore, and approximate inference is required. We use the algorithm described in section 2 (figure 2) to infer the state. In the context of the semantics-text correspondence model, as we discussed above, semantics m defines the subset of admissible world states. In order to use the algorithm, we need to understand how the conditional probabilities of the form P (m |m) are computed, as they play the key role in the inference procedure (see equation (2)). If there is a contradiction (m \u22a5m) then P (m |m) = 0, conversely, if m is subsumed by m (m \u2192 m ) then this probability is 1. Otherwise, P (m |m) equals the probability of new assignments \u2227 |m \\m| q=1 (s (defined by m \\m) conditioned on the previously fixed values of s (given by m).",
  "y": "background"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_10",
  "x": "The exact computation of the most probable semantics (line 4 of the algorithm) is intractable, and we have to resort to an approximation. Instead of predicting the most probable semanticsm j we search for the most probable pair (\u00e2 j ,m j ), thus assuming that the probability mass is mostly concentrated on a single alignment. The alignment a j is then discarded and not used in any other computations. Though the most likely alignment\u00e2 j for a fixed semantic representationm j can be found efficiently using a Viterbi algorithm, computing the most probable pair (\u00e2 j ,m j ) is still intractable. We use a modification of the beam search algorithm, where we keep a set of candidate meanings (partial semantic representations) and compute an alignment for each of them using a form of the Viterbi algorithm. As soon as the meaning representations m are inferred, we find ourselves in the set-up studied in<cite> (Liang et al., 2009</cite> ): the state s is no longer latent and we can run efficient inference on the E-step. Though some fields of the state s may still not be specified by m , we prohibit utterances from aligning to these non-specified fields.",
  "y": "similarities uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_11",
  "x": "---------------------------------- **EXPERIMENTS** To perform the experiments we used a subset of the weather dataset introduced in<cite> (Liang et al., 2009</cite> ). The original dataset contains 22,146 texts of 28.7 words on average, there are 12 types of records (predicates) and 36.0 records per forecast on average. We randomly chose 100 texts along with their world states to be used as the labeled data. 6 To produce groups of noncontradictory texts we have randomly selected a subset of weather states, represented them in a visual form (icons accompanied by numerical and symbolic parameters) and then manually annotated these illustrations. These newly-produced forecasts, when combined with the original texts, resulted in 259 groups of non-contradictory texts (650 texts, 2.5 texts per group).",
  "y": "uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_12",
  "x": "The overlap between the verbalized fields in each group was estimated to be below 35%. Around 60% of fields are mentioned only in a single forecast from a group, consequently, the texts cannot be regarded as paraphrases of each other. The test set consists of 150 texts, each corresponding to a different weather state. Note that during testing we no longer assume that documents share the state, we treat each document in isolation. We aimed to preserve approximately the same proportion of new and original examples as we had in the training set, therefore, we combined 50 texts originally present in the weather dataset with additional 100 newly-produced texts. We annotated these 100 texts by aligning each line to one or more records, 7 whereas for the original texts the alignments were already present. Following<cite> Liang et al. (2009)</cite> we evaluate the models on how well they predict these alignments.",
  "y": "uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_13",
  "x": "Around 60% of fields are mentioned only in a single forecast from a group, consequently, the texts cannot be regarded as paraphrases of each other. The test set consists of 150 texts, each corresponding to a different weather state. Note that during testing we no longer assume that documents share the state, we treat each document in isolation. We aimed to preserve approximately the same proportion of new and original examples as we had in the training set, therefore, we combined 50 texts originally present in the weather dataset with additional 100 newly-produced texts. We annotated these 100 texts by aligning each line to one or more records, 7 whereas for the original texts the alignments were already present. Following<cite> Liang et al. (2009)</cite> we evaluate the models on how well they predict these alignments. When estimating the model parameters, we followed the training regime prescribed in<cite> (Liang et al., 2009)</cite> .",
  "y": "uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_14",
  "x": "Instead of prohibiting records from crossing punctuation, as suggested by<cite> Liang et al. (2009)</cite> , in our implementation we disregard the words not attached to specific fields (attached to the nullfield, see section 3.1) when computing spans of records. To speed-up training, only a single record of each type is allowed to be generated when running inference for unlabeled examples on the E- Table 1 : Results (precision, recall and F 1 ) on the weather forecast dataset. step of the EM algorithm, as it significantly reduces the search space. Similarly, though we preserved all records which refer to the first time period, for other time periods we removed all the records which declare that the corresponding event (e.g., rain or snowfall) is not expected to happen. This preprocessing results in the oracle recall of 93%. We compare our approach (Semi-superv, noncontr) with two baselines: the basic supervised training on 100 labeled forecasts (Supervised BL) and with the semi-supervised training which disregards the non-contradiction relations (Semi-superv BL). The learning regime, the inference procedure and the texts for the semi-supervised baseline were identical to the ones used for our approach, the only difference is that all the documents were modeled as independent.",
  "y": "differences"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_15",
  "x": "However, correlation between rain and overcast, as also noted in<cite> (Liang et al., 2009)</cite> , results in the wrong assignment of the rain-related words to the field value corresponding to very cloudy weather. ---------------------------------- **RELATED WORK** Probably the most relevant prior work is an approach to bootstrapping lexical choice of a generation system using a corpus of alternative pas-sages (Barzilay and Lee, 2002) , however, in their work all the passages were annotated with unaligned semantic expressions. Also, they assumed that the passages are paraphrases of each other, which is stronger than our non-contradiction assumption. Sentence and text alignment has also been considered in the related context of paraphrase extraction (see, e.g., (Dolan et al., 2004; Barzilay and Lee, 2003) ) but this prior work did not focus on inducing or learning semantic representations. Similarly, in information extraction, there have been approaches for pattern discovery using comparable monolingual corpora (Shinyama and Sekine, 2003) but they generally focused only on discovery of a single pattern from a pair of sentences or texts.",
  "y": "similarities"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_16",
  "x": "However, exact inference for groups of documents with overlapping semantic representation is generally prohibitively expensive, as the shared latent semantics introduces nonlocal dependences between semantic representations of individual documents. To combat it, we proposed a simple iterative inference algorithm. We showed how it can be instantiated for the semantics-text correspondence model<cite> (Liang et al., 2009</cite> ) and evaluated it on a dataset of weather forecasts. Our approach resulted in an improvement over the scores of both the supervised baseline and of the traditional semi-supervised learning. There are many directions we plan on investigating in the future for the problem of learning semantics with non-contradictory relations. A promising and challenging possibility is to consider models which induce full semantic representations of meaning. Another direction would be to investigate purely unsupervised set-up, though it would make evaluation of the resulting method much more complex.",
  "y": "uses"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_0",
  "x": "Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., Dyer et al. (2016) with 92.4 F 1 on Penn Treebank constituency parsing and<cite> Vinyals et al. (2015)</cite> with 92.8 F 1 . In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture. In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem. Section 2 looks more closely at three of the most relevant previous papers. We then describe our exact model (Section 3), followed by the experimental setup and results (Sections 4 and 5). There is a one-to-one mapping between a tree and its sequential form. (Part-of-speech tags are not used.)",
  "y": "background"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_1",
  "x": "We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing -93.8 F 1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%. ---------------------------------- **INTRODUCTION** Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., Dyer et al. (2016) with 92.4 F 1 on Penn Treebank constituency parsing and<cite> Vinyals et al. (2015)</cite> with 92.8 F 1 . In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture. In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem.",
  "y": "uses"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_3",
  "x": "which is equivalent to Equation (1). We have reduced parsing to language modeling and can use language modeling techniques of estimating P (z t |z 1 , \u00b7 \u00b7 \u00b7 , z t\u22121 ) for parsing. ---------------------------------- **PREVIOUS WORK** We look here at three neural net (NN) models closest to our research along various dimensions. The first (Zaremba et al., 2014) gives the basic language modeling architecture that we have adopted, while the other two<cite> (Vinyals et al., 2015</cite>; Dyer et al., 2016) are parsing models that have the current best results in NN parsing. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_4",
  "x": "---------------------------------- **DATA** We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing (23) and millions of auto-parsed \"silver\" trees (McClosky et al., 2006; Huang et al., 2010; <cite>Vinyals et al., 2015)</cite> for tritraining. To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (Parker et al., 2011 ) with a product of eight Berkeley parsers (Petrov, 2010) 2 and ZPar (Zhu et al., 2013) and select 24 million trees on which both parsers agree (Li et al., 2014) . We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et 1 The code and trained models used for experiments are available at github.com/cdg720/emnlp2016. 2 We use the reimplementation by Huang et al. (2010) . (Charniak, 2000) performed better when trained on all of 24 million trees than when trained on resampled two million trees.",
  "y": "uses"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_6",
  "x": "We train LSTM-LM (GS) on the WSJ and a different set of 400,000 NYT trees for each epoch except for the last one during which we use the WSJ only. Training takes 26 epochs and 68 hours on a Titan X. LSTM-LM (GS) achieves 92.5 F 1 on the development. ---------------------------------- **RESULTS** ---------------------------------- **SUPERVISION** As shown in Table 2 , with 92.6 F 1 LSTM-LM (G) outperforms an ensemble of five MTPs<cite> (Vinyals et al., 2015)</cite> and RNNG (Dyer et al., 2016) , both of which are trained on the WSJ only.",
  "y": "differences"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_7",
  "x": "As shown in Table 2 , with 92.6 F 1 LSTM-LM (G) outperforms an ensemble of five MTPs<cite> (Vinyals et al., 2015)</cite> and RNNG (Dyer et al., 2016) , both of which are trained on the WSJ only. ---------------------------------- **SEMI-SUPERVISION** We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus 4 (HC)<cite> (Vinyals et al., 2015)</cite> ; and an ensemble of six one-to-many sequence models trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (Luong et al., 2016) . We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature. Parsers' parsing performance along with their training data is reported in Table 3 . LSTM-LM (GS) outperforms all the other parsers with 93.1 F 1 .",
  "y": "uses"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_8",
  "x": "In fact, we see that a generative parsing model, LSTM-LM, is more effective than discriminative parsing models (Dyer et al., 2016) . We suspect building large models with character embeddings would lead to further improvement as in language modeling (Kim et al., 2016; Jozefowicz et al., 2016) . We also wish to develop a complete parsing model using the LSTM-LM framework. Table 3 : Evaluation of models trained on the WSJ and additional resources. Note that the numbers of<cite> Vinyals et al. (2015)</cite> and Luong et al. (2016) are not directly comparable as their models are evaluated on OntoNotesstyle trees instead of PTB-style trees. E(LSTM-LMs (GS)) is an ensemble of eight LSTM-LMs (GS). X/Y in Silver column indicates the number of silver trees used to train Charniak parser and LSTM-LM.",
  "y": "differences"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_0",
  "x": "The remainder of the paper is organized as follows: Section 2 briefly reviews the previous related research; Section 3 describes the corpus we collected from TED talks; Section 4 describes the text classification methods; Section 5 reports on our experiments; and finally Section 6 discusses the findings of our study and plans for future work. ---------------------------------- **PREVIOUS RESEARCH** Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies (Mihalcea and Strapparava, 2005; Purandare and Litman, 2006; <cite>Yang et al., 2015)</cite> , humor recognition was modeled as a binary classification task In the seminal work (Mihalcea and Strapparava, 2005) , a corpus of 16,000 \"one-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers.",
  "y": "background"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_1",
  "x": "Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work<cite> (Yang et al., 2015)</cite> , a new corpus was constructed from a Pun of the Day website. It systematically explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style. In addition, Word2Vec (Mikolov et al., 2013) distributed representations were utilized in the model building. Beyond lexical cues from text inputs, other research has also utilized speakers' acoustic cues (Purandare and Litman, 2006; Bertero and Fung, 2016b) . These studies have typically used audio tracks from TV shows and their corresponding captions in order to categorize characters' speaking turns as humorous or non-humorous. Utterances prior to canned laughter that was manually inserted into the shows were treated as humorous, while other utterances were treated as negative cases.",
  "y": "background"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_2",
  "x": "By using Long Short Time Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997), Bertero and Fung (2016a) showed that Recurrent Neural Networks (RNNs) perform better on modeling sequential information than Conditional Random Fields (CRFs) (Lafferty et al., 2001) . From the brief review, we can find that the limited number of previously created corpora only cover one-line puns or jokes and conversations from TV comedy shows. There is a great need for an open corpus that can support investigating humor in presentations. 1 CNN-based text categorization methods have been applied for humor recognition (e.g., in (Bertero and Fung, 2016b) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in<cite> Yang et al. (2015)</cite> is missing; (b) CNN's performance in the previous research is not quite clear 2 ; and (c) some important techniques that can improve CNN performance (e.g., using variedsized filters and dropout regularization (Hinton et al., 2012) ) were missing. Therefore, the present study is meant to address these limitations. ---------------------------------- **TED TALK DATA**",
  "y": "motivation"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_3",
  "x": "This special markup was used to determine utterance labels. We collected 1,192 TED Talk transcripts 4 . An example transcription is given in Figure 1 . The collected transcripts were split into sentences using the Stanford CoreNLP tool (Manning et al., 2014) . In this study, sentences containing or immediately followed by '(Laughter)' were used as humorous sentences, as shown in Figure 1 ; all other sentences were defined as non-humorous sentences. Following (Mihalcea and Strapparava, 2005; <cite>Yang et al., 2015)</cite> , we selected the same sizes (n = 4726) of humorous and non-humorous sentences. To minimize possible topic shifts between positive and negative instances, for each positive instance, we picked up one negative instance nearby (the context window was 7 sentences in this study).",
  "y": "background uses"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_4",
  "x": "Kim (2014) ; Johnson and Zhang (2015) ; Zhang and Wallace (2015) suggested that using a simple CNN setup, which entails one layer of convolution on top of word embedding vectors, achieves excellent results on multiple tasks. Deep learning is rapidly being applied to computational humor research (Bertero and Fung, 2016b,a) . In Bertero and Fung (2016b), CNN was found to be the best model that uses both acoustic and lexical cues for humor recognition. By using Long Short Time Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997), Bertero and Fung (2016a) showed that Recurrent Neural Networks (RNNs) perform better on modeling sequential information than Conditional Random Fields (CRFs) (Lafferty et al., 2001) . From the brief review, we can find that the limited number of previously created corpora only cover one-line puns or jokes and conversations from TV comedy shows. There is a great need for an open corpus that can support investigating humor in presentations. 1 CNN-based text categorization methods have been applied for humor recognition (e.g., in (Bertero and Fung, 2016b) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in<cite> Yang et al. (2015)</cite> is missing; (b) CNN's performance in the previous research is not quite clear 2 ; and (c) some important techniques that can improve CNN performance (e.g., using variedsized filters and dropout regularization (Hinton et al., 2012) ) were missing.",
  "y": "motivation"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_5",
  "x": "---------------------------------- **METHODS** ---------------------------------- **CONVENTIONAL MODEL** Following<cite> Yang et al. (2015)</cite> , we applied Random Forest (Breiman, 2001 ) to do humor recognition by using the following two groups of features. The first group are latent semantic structural features covering the following 4 categories 5 : Incongruity (2), Ambiguity (6), Interpersonal Effect (4), and Phonetic Pattern (4). The second group are semantic distance features, including the humor label classes from 5 sentences in the training set that are closest to this sentence (found by using a k-Nearest Neighbors (kNN) method), and each sentence's averaged Word2Vec representations (n = 300).",
  "y": "uses"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_6",
  "x": "Note that for 1D convolution and FC layer's input, we applied 'dropout' (Hinton et al., 2012) regularization, which entails randomly setting a proportion of network weights to be zero during model training, to overcome overfitting. By using cross-entropy as the learning metric, the whole sequential network (all weights and bias) could be optimized by using any SGD optimization, e.g., Adam (Kingma and Ba, 2014) , Adadelta (Zeiler, 2012) , and so on. ---------------------------------- **EXPERIMENTS** We used two corpora: the TED Talk corpus (denoted as TED) and the Pun of the Day corpus 6 (denoted as Pun). Note that we normalized words in the Pun data to lowercase to avoid avoid a possibly elevated result caused by a special pattern: in the original format, all negative instances started with capital letters. The Pun data allows us to verify that our implementation is consistent with the work reported in<cite> Yang et al. (2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_7",
  "x": "When training the CNN model, we randomly selected 10% of the training data as the validation set for using early stopping to avoid overfitting. 7 https://github.com/ EducationalTestingService/skll 8 https://github.com/fchollet/keras 9 The implementation will be released with the paper On the Pun data, the CNN model shows consistent improved performance over the conventional model, as suggested in<cite> Yang et al. (2015)</cite> . In particular, precision has been greatly increased from 0.762 to 0.864. On the TED data, we also observed that the CNN model helps to increase precision (from 0.515 to 0.582) and accuracy (from 52.0% to 58.9%). The empirical evaluation results suggests that the CNN-based model has an advantage on the humor recognition task. In addition, focusing on the system development time, generating and implementing those features in the conventional model would take days or even weeks. However, the CNN model automatically learns its optimal feature representation and can adjust the features automatically across data sets.",
  "y": "similarities background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_0",
  "x": "They locate the source and target fragments independently, making the extracted fragments unreliable<cite> (Munteanu and Marcu, 2006)</cite> . Some studies develop alignment models for comparable sentences to extract parallel fragments (Quirk et al., 2007) . Because the comparable sentences are quite noisy, the extracted fragments are not accurate. In this paper, we propose an accurate parallel fragment extraction system. We locate parallel fragment candidates using an alignment model, and use an accurate lexicon filter to identify the truly parallel ones. Experimental results on Chinese-Japanese corpora show that our proposed method significantly outperforms a state-of-theart approach, which indicate the effectiveness of our parallel fragment extraction system. Moreover, we investigate the factors that may affect the performance of our system in detail.",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_1",
  "x": "Because the comparable sentences are quite noisy, the extracted fragments are not accurate. In this paper, we propose an accurate parallel fragment extraction system. We locate parallel fragment candidates using an alignment model, and use an accurate lexicon filter to identify the truly parallel ones. Experimental results on Chinese-Japanese corpora show that our proposed method significantly outperforms a state-of-theart approach, which indicate the effectiveness of our parallel fragment extraction system. Moreover, we investigate the factors that may affect the performance of our system in detail. 2 Related Work<cite> (Munteanu and Marcu, 2006)</cite> is the first attempt to extract parallel fragments from comparable sentences. They extract sub-sentential parallel fragments by using a Log-Likelihood-Ratio (LLR) lexicon estimated on an external parallel corpus and a smoothing filter.",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_2",
  "x": "The second lexicon we use is the LLR lexicon. <cite>Munteanu and Marcu (2006)</cite> show that the LLR lexicon performs better than the IBM Model 1 lexicon for parallel fragment extraction. One advantage of the LLR lexicon is that it can produce both positive and negative associations. <cite>Munteanu and Marcu (2006)</cite> develop a smoothing filter applying this advantage. We extract the LLR lexicon from a word-aligned parallel corpus using the same method as<cite> (Munteanu and Marcu, 2006)</cite> . The last lexicon we use is the SampLEX lexicon. Vuli\u0107 and Moens (2012) propose an associative approach for lexicon extraction from par-allel corpora that relies on the paradigm of data reduction.",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_3",
  "x": "<cite>Munteanu and Marcu (2006)</cite> show that the LLR lexicon performs better than the IBM Model 1 lexicon for parallel fragment extraction. One advantage of the LLR lexicon is that it can produce both positive and negative associations. <cite>Munteanu and Marcu (2006)</cite> develop a smoothing filter applying this advantage. We extract the LLR lexicon from a word-aligned parallel corpus using the same method as<cite> (Munteanu and Marcu, 2006)</cite> . The last lexicon we use is the SampLEX lexicon. Vuli\u0107 and Moens (2012) propose an associative approach for lexicon extraction from par-allel corpora that relies on the paradigm of data reduction. They extract translation pairs from many smaller sub-corpora that are randomly sampled from the original corpus, based on some frequency-based criteria of similarity.",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_4",
  "x": "One advantage of the LLR lexicon is that it can produce both positive and negative associations. <cite>Munteanu and Marcu (2006)</cite> develop a smoothing filter applying this advantage. We extract the LLR lexicon from a word-aligned parallel corpus using the same method as<cite> (Munteanu and Marcu, 2006)</cite> . The last lexicon we use is the SampLEX lexicon. Vuli\u0107 and Moens (2012) propose an associative approach for lexicon extraction from par-allel corpora that relies on the paradigm of data reduction. They extract translation pairs from many smaller sub-corpora that are randomly sampled from the original corpus, based on some frequency-based criteria of similarity. They show that their method outperforms IBM Model 1 and other associative methods such as LLR in terms of precision and F-measure.",
  "y": "similarities uses"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_5",
  "x": "They extract translation pairs from many smaller sub-corpora that are randomly sampled from the original corpus, based on some frequency-based criteria of similarity. They show that their method outperforms IBM Model 1 and other associative methods such as LLR in terms of precision and F-measure. We extract SampLEX lexicon from a parallel corpus using the same method as (Vuli\u0107 and Moens, 2012) . Aiming to gain new knowledge that does not exist in the lexicon, we apply a smoothing filter similar to<cite> (Munteanu and Marcu, 2006)</cite> . For each aligned word pair in the fragment candidates, we set scores to the words in two directions according to the extracted lexicon. If the aligned word pair exists in the lexicon, we set the corresponding translation probabilities as scores. For LLR lexicon, we use both positive and negative association values.",
  "y": "similarities uses"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_6",
  "x": "In this case, we set the scores to 1 without considering the existence of the word pair in the lexicon. After this process, we get initial scores for the words in the fragment candidates in two directions. We then apply an averaging filter to the initial scores to obtain filtered scores in both directions. The averaging filter sets the score of one word to the average score of several words around it. We think the words with initial positive scores are reliable, because they satisfy two strong constraints, namely alignment by IBM models and existence in the lexicon. Therefore, unlike<cite> (Munteanu and Marcu, 2006)</cite>, we only apply the averaging filter to the words with negative scores. Moreover, we add another constraint that only filtering a word when both the left and right words around it have positive scores, which can further guarantee accuracy.",
  "y": "extends differences"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_7",
  "x": "In our experiments, we compared our proposed fragment extraction method with<cite> (Munteanu and Marcu, 2006)</cite> . We manually evaluated the accuracy of the extracted fragments. Moreover, we used the extracted fragments as additional MT training data, and evaluated the effectiveness of the fragments for MT. We conducted experiments on Chinese-Japanese data. In all our experiments, we preprocessed the data by segmenting Chinese and Japanese sentences using a segmenter proposed by Chu et al. (2012) and JUMAN (Kurohashi et al., 1994) respectively. ---------------------------------- **DATA**",
  "y": "similarities"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_8",
  "x": "Note that since the articles in these two websites were written by Chinese and Japanese researchers respectively, the collected corpora are very-non-parallel. ---------------------------------- **EXTRACTION EXPERIMENTS** We first applied sentence extraction on the quasicomparable corpora using our system, and 30k comparable sentences of chemistry domain were extracted. We then applied fragment extraction on the extracted comparable sentences. We compared our proposed method with<cite> (Munteanu and Marcu, 2006)</cite>. We applied word alignment using GIZA++.",
  "y": "similarities"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_9",
  "x": "We also compared IBM Model 1, LLR and SampLEX lexicon for filtering. All lexicons were extracted from the parallel corpus. Table 1 shows the results for fragment extraction. We can see that the average size of fragments extracted by<cite> (Munteanu and Marcu, 2006</cite> ) is unusually long, which is also reported in (Quirk et al., 2007) . Our proposed method extracts shorter fragments. The number of extracted fragments and the average size are similar among the three lexicons when using the same alignment setting. Using the external parallel data for alignment extracts more fragments than only using the comparable sentences, and the average size is slightly larger.",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_10",
  "x": "To evaluate accuracy, we randomly selected 100 fragments extracted by the different methods. We manually evaluated the accuracy based on the number of exact match. Note that exact match criteria has a bias against<cite> (Munteanu and Marcu, 2006)</cite> , because their method extacts subsentential fragments which are quite long. We found that only one of the fragments extracted by \"Munteanu+, 2006\" is exact match, while for the remainder only partial matches are contained in long fragments. Our proposed method have a accuracy over 80%, while the remainder are partial matches. For the effects of different lexicons, LLR and SampLEX shows better performance than IBM Model 1 lexicon. We think the reason is the same one reported in previous studies that LLR and SampLEX lexicon are more accurate than IBM Model 1 lexicon.",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_11",
  "x": "The baseline system used the parallel corpus (680k sentences). We used another 368 and 367 sentences from the chemistry domain for tuning and testing respectively. We trained a 5-gram language model on the Japanese side of the parallel corpus using the SRILM toolkit 8 . Table 2 : Results for Chinese-to-Japanese translation experiments (\" \u2020\" and \" \u2021\"denotes the result is better than \"Baseline\" significantly at p < 0.05 and p < 0.01 respectively, \" * \" denotes the result is better than \"+Munteanu+, 2006\" significantly at p < 0.05). Translation results evaluated on BLEU-4, are shown in Table 2 . We can see that appending the extracted comparable sentences have a positive effect on translation quality. Adding the fragments extracted by<cite> (Munteanu and Marcu, 2006)</cite> has a negative impact, compared to appending the sentences.",
  "y": "differences"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_0",
  "x": "****LEARNING TO PREDICT SCRIPT EVENTS FROM DOMAIN-SPECIFIC TEXT**** **ABSTRACT** The automatic induction of scripts (Schank and Abelson, 1977) has been the focus of many recent works. In this paper, we employ a variety of these methods to learn Schank and Abelson's canonical restaurant script, using a novel dataset of restaurant narratives we have compiled from a website called \"Dinners from Hell.\" Our models learn narrative chains, script-like structures that we evaluate with the \"narrative cloze\" task<cite> (Chambers and Jurafsky, 2008)</cite> . ---------------------------------- **INTRODUCTION** A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977) .",
  "y": "uses"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_1",
  "x": "In this paper, we employ a variety of these methods to learn Schank and Abelson's canonical restaurant script, using a novel dataset of restaurant narratives we have compiled from a website called \"Dinners from Hell.\" Our models learn narrative chains, script-like structures that we evaluate with the \"narrative cloze\" task<cite> (Chambers and Jurafsky, 2008)</cite> . ---------------------------------- **INTRODUCTION** A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977) . Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora<cite> (Chambers and Jurafsky, 2008</cite>; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014) . These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed.",
  "y": "background"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_2",
  "x": "Previous work in the automatic induction of scripts or script-like structures has taken a number of different approaches. Regneri et al. (2010) attempt to learn the structure of specific scripts by eliciting event sequence descriptions (ESDs) from humans to which they apply multiple sequence alignment (MSA) to yield one global structure per script. (Orr et al. (2014) learn similar structures in a probabilistic framework with Hidden Markov Models.) Although Regneri et al. (2010) , like us, are concerned with learning pre-specified scripts, our approach is different in that we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learning may appear limited to domains for which a corpus conveniently already exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem-plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001) ,<cite> Chambers and Jurafsky (2008)</cite> propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014) .",
  "y": "background"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_3",
  "x": "(Orr et al. (2014) learn similar structures in a probabilistic framework with Hidden Markov Models.) Although Regneri et al. (2010) , like us, are concerned with learning pre-specified scripts, our approach is different in that we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learning may appear limited to domains for which a corpus conveniently already exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem-plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001) ,<cite> Chambers and Jurafsky (2008)</cite> propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014) . It is from this body of work that we borrow techniques to apply to the Dinners from Hell dataset. As defined by<cite> Chambers and Jurafsky (2008)</cite> , a narrative chain is \"a partially ordered set of narrative events that share a common actor,\" where a narrative event is \"a tuple of an event (most simply a verb) and its participants, represented as typed dependencies.\" To learn narrative chains from text, Chambers and Jurafsky extract chains of narrative events linked by a common coreferent within a document.",
  "y": "background"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_4",
  "x": "Over these extracted chains of narrative events, pointwise mutual information (PMI) is computed between all pairs of events. These PMI scores are then used to predict missing events from such chains, i.e. the narrative cloze evaluation. Jans et al. (2012) expand on this approach, introducing an ordered PMI model, a bigram probability model, skip n-gram counting methods, coreference chain selection, and an alternative scoring metric (recall at 50). Their bigram probability model outperforms the original PMI model on the narrative cloze task under many conditions. Pichotta and Mooney (2014) introduce an extended notion of narrative event that includes information about subjects and objects. They also introduce a competitive \"unigram model\" as a baseline for the narrative cloze task. To learn the restaurant script from our dataset, we implement the models of<cite> Chambers and Jurafsky (2008)</cite> and Jans et al. (2012) , as well as the unigram baseline of Pichotta and Mooney (2014) .",
  "y": "similarities uses"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_5",
  "x": "---------------------------------- **METHODS** This section provides an overview of each of the different methods and parameter settings we employ to learn narrative chains from the Dinners from Hell corpus, starting with the original model<cite> (Chambers and Jurafsky, 2008)</cite> and extending to the modifications of Jans et al. (2012) . As part of this work, we are releasing a program called NaChos, our integrated Python implementation of each of the methods for learning narrative chains described in this section. 1 ---------------------------------- **COUNTING METHODS FOR PMI**",
  "y": "extends differences"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_6",
  "x": "**PREDICTIVE MODELS FOR NARRATIVE CLOZE** In order to perform the narrative cloze task, we need a model for predicting the missing narrative event, e, from a chain of observed narrative events, e 1 . . . e n , at insertion point k. The original model, proposed by<cite> Chambers and Jurafsky (2008)</cite> , predicts the event that maximizes unordered pmi, where V is the set of all observed events (the vocabulary) and C(e 1 , e 2 ) is symmetric. Two additional models are introduced by Jans et al. (2012) and we use them here, as well. First, the ordered pmi model, pmi(e, e i ) (4) where C(e 1 , e 2 ) is asymmetric, i.e., C(e 1 , e 2 ) counts only cases in which e 1 occurs before e 2 .",
  "y": "background"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_7",
  "x": "Discounting For each model, we add an option for discounting the computed scores. In the case of the two PMI-based models, we use the discount score described in Pantel and Ravichandran (2004) and used by<cite> Chambers and Jurafsky (2008)</cite> . For the bigram probability model, this PMI discount score would be inappropriate, so we instead use absolute discounting. Document Threshold We include a document threshold parameter, D, that ensures that, in any narrative cloze test, any event e that was observed during training in fewer than D distinct documents will receive a worse score (i.e. be ranked behind) any event e whose count meets the document threshold. ---------------------------------- **DATASET: DINNERS FROM HELL** The source of our data for this experiment is a blog called \"Dinners From Hell\" 2 where readers submit stories about their terrible restaurant experiences.",
  "y": "similarities uses"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_8",
  "x": "Because our dataset is small (143 documents), we perform leave-one-out testing at the document level, training on 133 folds total. (Ten documents are excluded for a development set.) For each fold of training, we extract all of the narrative chains (mapped directly from coreference chains) in the held out test document. For each test chain, we generate one narrative cloze test per \"script-relevant\" event in that chain. For example, if a chain contains ten events, three of which are \"script-relevant,\" then three cloze tests will be generated, each containing nine \"observed\" events. Chains with fewer than two events are excluded. In this way, we generate a total of 2,273 cloze tests. Scoring We employ three different scoring metrics: average rank<cite> (Chambers and Jurafsky, 2008)</cite> , mean reciprocal rank, and recall at 50 (Jans et al., 2012) .",
  "y": "similarities uses"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_0",
  "x": "An all-neural E2E model eliminates the need to individually train components of a conventional model (i.e., acoustic, pronunciation, and language models), and directly outputs subword (or word) symbols [1, 2, 3, 4, 5] . In large scale training, E2E models perform competitively compared to more sophisticated conventional systems on Google traffic [6, 7] . Given its all-neural nature, an E2E model can be reasonably downsized to fit on mobile devices [6] . Despite the rapid progress made by E2E models, they still face challenges compared to state-of-the-art conventional models [8, 9] . To bridge the quality gap between a streaming recurrent neural network transducer (RNN-T) [6] and a large conventional model [8] , a two-pass framework has been proposed in<cite> [10]</cite> , which uses a non-streaming LAS decoder to rescore the RNN-T hypotheses. The rescorer attends to audio encoding from the encoder, and computes sequence-level log-likelihoods of first-pass hypotheses. The two-pass model achieves 17%-22% relative WER reduction (WERR) compared to RNN-T [6] and has a similar WER to a large conventional model [8] .",
  "y": "background"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_1",
  "x": "We encode first-pass hypotheses bidirectionally to leverage context information for decoding. Note that the first-pass hypotheses are sequences of wordpieces [19] and are usually short in VS, and thus the encoding should have limited impact on latency. Our experiments are conducted using the same training data as in [20, 21] , which is from multiple domains such as Voice Search, YouTube, Farfield and Telephony. We first analyze the behavior of the deliberation model, including performance when attending to multiple RNN-T hypotheses, contribution of different attention, and rescoring vs. beam search. We apply additional encoder (AE) layers and minimum WER (MWER) training [22] to further improve quality. The results show that our MWER trained 8-hypothesis deliberation model performs 11% relatively better than LAS rescoring<cite> [10]</cite> in VS WER, and up to 15% for proper noun recognition. Joint training further improves VS slightly (2%) but significantly for a proper noun test set: 9%.",
  "y": "differences"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_2",
  "x": "As a result, our best deliberation model achieves a WER of 5.0% on VS, which is 21% relatively better than the large conventional model [8] (6.3% VS WER). Lastly, we analyze the computational complexity of the deliberation model, and show some decoding examples to understand its strength. ---------------------------------- **DELIBERATION BASED TWO-PASS E2E ASR** ---------------------------------- **MODEL ARCHITECTURE** As shown in Fig. 1 , our deliberation network consists of three major components: A shared encoder, an RNN-T decoder [1] , and a deliberation decoder, similar to<cite> [10,</cite> 16] .",
  "y": "similarities"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_3",
  "x": "We keep the audio encoder unidirectional due to latency considerations. Then, two attention layers are followed to attend to acoustic encoding and first-pass hypothesis encoding separately. The two context vectors, c b and ce, are concatenated as inputs to a LAS decoder. There are two major differences between our model and the LAS rescoring<cite> [10]</cite> . First, the deliberation model attends to both e and yr, while<cite> [10]</cite> only attends to the acoustic embedding, e. Second, our deliberation model encodes yr bidirectionally, while<cite> [10]</cite> only relies on unidirectional encoding e for decoding. [10] shows that the incompatibility between an RNN-T encoder and a LAS decoder leads to a gap between the rescoring model and LASonly model. To help adaptation, we introduce a 2-layer LSTM as an additional encoder (dashed box in Fig. 1 to indicate optional) to further encode e.",
  "y": "differences"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_4",
  "x": "---------------------------------- **TRAINING** A deliberation model is typically trained from scratch by jointly optimizing all components [16] . However, we find training a two-pass model from scratch tends to be unstable in practice<cite> [10]</cite> , and thus use a two-step training process: Train the RNN-T as in [6] , and then fix the RNN-T parameters and only train the deliberation decoder and additional encoder layers as in [7, <cite>10]</cite> . ---------------------------------- **MWER LOSS** We apply the MWER loss [22] in training which optimizes the expected word error rate by using n-best hypotheses:",
  "y": "uses"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_5",
  "x": "where LRNNT(\u00b7) is the RNN-T loss, and LCE(\u00b7) the CE loss for the deliberation decoder. \u03b8e, \u03b81, and \u03b82 denote the parameters of shared encoder, RNN-T decoder, and deliberation decoder, respectively. Note that a jointly trained model can be further trained with MWER loss. The joint training is similar to \"deep finetuning\" in<cite> [10]</cite> but without a pre-trained decoder. ---------------------------------- **DECODING** Our decoding consists of two passes: 1) Decode using the RNN-T model to obtain the first-pass sequence yr, and 2) Attend to both yr and e, and perform the second beam search to generate y d .",
  "y": "similarities"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_6",
  "x": "**DECODING** Our decoding consists of two passes: 1) Decode using the RNN-T model to obtain the first-pass sequence yr, and 2) Attend to both yr and e, and perform the second beam search to generate y d . We are also curious how rescoring performs given bidirectional encoding from yr. In rescoring, we run the deliberation decoder on yr in a teacher-forcing mode<cite> [10]</cite> . Note the difference from<cite> [10]</cite> when rescoring a hypothesis is that the deliberation network sees all candidate hypotheses. We compare rescoring and beam search in Sect. 4.",
  "y": "uses"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_7",
  "x": "The joint training is similar to \"deep finetuning\" in<cite> [10]</cite> but without a pre-trained decoder. ---------------------------------- **DECODING** Our decoding consists of two passes: 1) Decode using the RNN-T model to obtain the first-pass sequence yr, and 2) Attend to both yr and e, and perform the second beam search to generate y d . We are also curious how rescoring performs given bidirectional encoding from yr. In rescoring, we run the deliberation decoder on yr in a teacher-forcing mode<cite> [10]</cite> . Note the difference from<cite> [10]</cite> when rescoring a hypothesis is that the deliberation network sees all candidate hypotheses.",
  "y": "differences"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_8",
  "x": "For training, we use the same multidomain datasets as in [20, 21] which include anonymized and hand-transcribed English utterances from general Google traffic, far-field environments, telephony conversations, and YouTube. We augment the clean training utterances by artificially corrupting them by using a room simulator, varying degrees of noise, and reverberation such that the signal-to-noise ratio (SNR) is between 0dB and 30dB [23] . We also use mixed-bandwidth utterances at 8kHz or 16 kHz for training [24] . Our main test set includes~14K anonymized hand-transcribed VS utterances sampled from Google traffic. To evaluate the performance of proper noun recognition, we report performance on a side-by-side (SxS) test set, and 4 voice command test sets [6] . The SxS set contains utterances where the LAS rescoring model<cite> [10]</cite> performs inferior to a state-of-the-art conventional model [8] , and one reason is due to proper nouns. The voice command test sets include 3 TTS test sets created using parallel-wavenet [25] : Songs, Contacts-TTS, and Apps, where the commands include song, contact, and app names, respectively.",
  "y": "differences"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_9",
  "x": "The AE consists of a 2-layer LSTM with 2,048 hidden units followed by 640-dimensional projection per layer. Beam search is used for decoding. In Table 3 Table 3 . WERs (%) with or without AE layers. ---------------------------------- **RESCORING** We propose to use the deliberation decoder to rescore first-pass RNN-T results, and expect bidirectional encoding to help compared to LAS rescoring<cite> [10]</cite> .",
  "y": "differences"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_10",
  "x": "We propose to use the deliberation decoder to rescore first-pass RNN-T results, and expect bidirectional encoding to help compared to LAS rescoring<cite> [10]</cite> . Table 5 shows that the deliberation rescoring (E8) performs 5% relatively better than LAS rescoring (B3). AE layers are added to both models. ---------------------------------- **COMPARISONS** From the above analysis, an MWER trained 8-hypothesis deliberation model with AE layers performs the best, and thus we use that for comparison below. In Table 4 , we compare deliberation models with an RNN-T [6] and LAS rescoring model<cite> [10]</cite> To understand where the improvement comes from, in Fig. 2 we show an example of deliberation attention distribution on the RNN-T hypotheses (x-axis) at every step of the second-pass decoding (yaxis).",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_0",
  "x": "Table 1 lists the number of sentences in the training and test sets for each task and the number of instances used as interpretants in the RTM models (M for million). We use referential translation machine (RTM) <cite>(Bi\u00e7ici, 2018</cite>; Bi\u00e7ici and Way, 2015) models for building our prediction models. RTMs predict data translation between the instances in the training set and the test set using interpretants, data close to the task instances. Interpretants provide context for the prediction task and are used during the derivation of the features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and to identify translation acts between any two data sets for building prediction models. With the enlarging parallel and monolingual corpora made available by WMT, the capability of the interpretant datasets selected by RTM models to provide context for the training and test sets improve as can be seen in the data statistics of parfda instance selection (Bi\u00e7ici, 2019). Figure 1 depicts RTMs and explains the model building process. RTMs use parfda for instance selection and machine translation performance prediction system (MTPPS) for obtaining the features, which includes additional features from word alignment and also from GLMd for word-level prediction.",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_1",
  "x": "The neural network architecture they used is also hybrid with different types of layers: input word embedding use 64 dimensional vectors, the next three layers are two feedforward layers with 400 nodes and a bidirectional gated recurrent units layer with 200 units, followed by similar three layers with half nodes, followed by a feedforward layer with 50 nodes and a softmax layer. We use Global Linear Models (GLM) (Collins, 2002) with dynamic learning (GLMd)<cite> (Bi\u00e7ici, 2018)</cite> for word-and phrase-level translation performance prediction. GLMd uses weights in a range [a, b] to update the learning rate dynamically according to the error rate. Evaluation metrics listed are Pearson's correlation (r), mean absolute error (MAE), and root mean squared error (RMSE). We use prediction averaging<cite> (Bi\u00e7ici, 2018)</cite> to obtain a combined prediction from various prediction outputs better than the components, where the performance on the training set is used to obtain weighted average of the top k predictions,\u0177 with evaluation metrics indexed by j \u2208 J and weights with w: We assume independent predictions and use p i /(1 \u2212 p i ) for weights where p i represents the accuracy of the independent classifier i in a weighted majority ensemble (Kuncheva and Rodr\u00edguez, 2014) . We only use the MIX prediction if we obtain better results on the training set.",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_2",
  "x": "We use prediction averaging<cite> (Bi\u00e7ici, 2018)</cite> to obtain a combined prediction from various prediction outputs better than the components, where the performance on the training set is used to obtain weighted average of the top k predictions,\u0177 with evaluation metrics indexed by j \u2208 J and weights with w: We assume independent predictions and use p i /(1 \u2212 p i ) for weights where p i represents the accuracy of the independent classifier i in a weighted majority ensemble (Kuncheva and Rodr\u00edguez, 2014) . We only use the MIX prediction if we obtain better results on the training set. We select the best model using r and mix the results using r, RAE, MRAER, and MAER. We filter out those results with higher than 1 relative evaluation metric scores. We also use stacking to build higher level models using predictions from base prediction models where they can also use the probability associated with the predictions (Ting and Witten, 1999) . The stacking models use the predictions from predictors as features and build second level predictors.",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_3",
  "x": "For the document-level RTM model, instead of running separate MTPPS instances for each training or test document to obtain specific features for each document, we concatenate the sentences from each document to obtain a single sentence representing each and then run an RTM model. This conversion decreases the number of features and obtains close results<cite> (Bi\u00e7ici, 2018)</cite> . Before model combination, we further filter prediction results from different machine learn- ing models based on the results on the training set to decrease the number of models combined and improve the results. A criteria that we use is to include results that are better than the best RR model's results. In general, the combined model is better than the best model in the set and stacking achieves better results than MIX. We tokenize and truecase all of the corpora using Moses ' (Koehn et al., 2007) processing tools. 2 LMs are built using kenlm (Heafield et al., 2013) .",
  "y": "similarities"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_4",
  "x": "We use referential translation machine (RTM) <cite>(Bi\u00e7ici, 2018</cite>; Bi\u00e7ici and Way, 2015) models for building our prediction models. RTMs predict data translation between the instances in the training set and the test set using interpretants, data close to the task instances. Interpretants provide context for the prediction task and are used during the derivation of the features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and to identify translation acts between any two data sets for building prediction models. With the enlarging parallel and monolingual corpora made available by WMT, the capability of the interpretant datasets selected by RTM models to provide context for the training and test sets improve as can be seen in the data statistics of parfda instance selection (Bi\u00e7ici, 2019) . Figure 1 depicts RTMs and explains the model building process. RTMs use parfda for instance selection and machine translation performance prediction system (MTPPS) for obtaining the features, which includes additional features from word alignment and also from GLMd for word-level prediction. We use ridge regression, kernel ridge regression, k-nearest neighors, support vector regression, AdaBoost (Freund and Schapire, 1997), gradient tree boosting, gaussian process regressor, extremely randomized trees (Geurts et al., 2006) , and multi-layer perceptron (Bishop, 2006) as learning models in combination with feature selection (FS) (Guyon et al., 2002) and partial least squares (PLS) (Wold et al., 1984) where most of these models can be found in scikit-learn.",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_5",
  "x": "\u2022 including the statistics of the binary tags obtained as features extracted from word-level tag predictions for sentence-level prediction, \u2022 using KNN to estimate the noise level for Figure 1 : RTM depiction: parfda selects interpretants close to the training and test data using parallel corpus in bilingual settings and monolingual corpus in the target language or just the monolingual target corpus in monolingual settings; an MTPPS use interpretants and training data to generate training features and another use interpretants and test data to generate test features in the same feature space; learning and prediction takes place taking these features as input. SVR, which obtains accuracy with 5% error compared with estimates obtained with known noise level (Cherkassky and Ma, 2004) and set = \u03c3/2. Martins et al. (2017) used a hybrid stacking model to combine the word-level predictions from 15 predictors using neural networks with different initializations together with the previous features from a linear model. The neural network architecture they used is also hybrid with different types of layers: input word embedding use 64 dimensional vectors, the next three layers are two feedforward layers with 400 nodes and a bidirectional gated recurrent units layer with 200 units, followed by similar three layers with half nodes, followed by a feedforward layer with 50 nodes and a softmax layer. We use Global Linear Models (GLM) (Collins, 2002) with dynamic learning (GLMd)<cite> (Bi\u00e7ici, 2018)</cite> for word-and phrase-level translation performance prediction. GLMd uses weights in a range [a, b] to update the learning rate dynamically according to the error rate.",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_6",
  "x": "GLMd uses weights in a range [a, b] to update the learning rate dynamically according to the error rate. Evaluation metrics listed are Pearson's correlation (r), mean absolute error (MAE), and root mean squared error (RMSE). ---------------------------------- **MIXTURE OF EXPERTS MODELS** We use prediction averaging<cite> (Bi\u00e7ici, 2018)</cite> to obtain a combined prediction from various prediction outputs better than the components, where the performance on the training set is used to obtain weighted average of the top k predictions,\u0177 with evaluation metrics indexed by j \u2208 J and weights with w: We assume independent predictions and use p i /(1 \u2212 p i ) for weights where p i represents the accuracy of the independent classifier i in a weighted majority ensemble (Kuncheva and Rodr\u00edguez, 2014) . We only use the MIX prediction if we obtain better results on the training set.",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_7",
  "x": "For the document-level RTM model, instead of running separate MTPPS instances for each training or test document to obtain specific features for each document, we concatenate the sentences from each document to obtain a single sentence representing each and then run an RTM model. This conversion decreases the number of features and obtains close results<cite> (Bi\u00e7ici, 2018)</cite> . Before model combination, we further filter prediction results from different machine learn- ing models based on the results on the training set to decrease the number of models combined and improve the results. A criteria that we use is to include results that are better than the best RR model's results. In general, the combined model is better than the best model in the set and stacking achieves better results than MIX. ---------------------------------- **RESULTS**",
  "y": "similarities"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_0",
  "x": "Similarly, the focus of previous investigations has mainly been readability from native language (L1) readers' perspective, but aspects of L2 readability have been less widely studied. To our knowledge no previous research have explored this latter dimension for Swedish before, hence we aim at filling this gap, which can be useful, besides the purposes mentioned above, also in future sentence and text simplification and adaptation tasks. We propose a rule-based as well as a combination of rule-based and machine learning methods for the identification of sentences understandable by L2 learners and suitable as exercise items. During the selection of linguistic indicators, we have taken into consideration previously studied features of readability (Fran\u00e7ois and Fairon, 2012; <cite>Heimann M\u00fchlenbock, 2013</cite>; Vajjala and Meurers, 2012) , L2 Swedish curricula (Levy Scherrer and Lindemalm, 2009; Folkuniversitet, 2013) and aspects of Good Dictionary Examples (GDEX) (Hus\u00e1k, 2010; Kilgarriff et al., 2008) , being that we believe they have some properties in common with exercise items. The current version of the machine learning model distinguishes sentences readable by students at an intermediate level of proficiency from sentences of a higher readability level. The approaches have been implemented and integrated into an online Intelligent ComputerAssisted Language Learning (ICALL) platform, L\u00e4rka . Besides a module where users can experiment with the filtering of corpus hits, a module with inflectional and vocabulary exercises (making use of the selected sentences with our method) is also available.",
  "y": "similarities uses"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_1",
  "x": "Language models have also been commonly used for readability predictions (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005) . A recently proposed measure, the Coh-Metrix (Graesser et al., 2011) , aims at a multilevel analysis of texts, inspired by psycholinguistic principles. It measures not only linguistic difficulty, but also cohesion in texts. Research on L1 readability for Swedish, using machine learning, is described in Heimann M\u00fchlenbock (2013) and<cite> Falkenjack et al. (2013)</cite> . Heimann M\u00fchlenbock (2013) examined readability along five dimensions: surface features, word usage, sentence structure, idea density and human interest. Mean dependency distance, subordinate clauses and modifiers proved good predictors for L1 Swedish. Although a number of readability formulas exist for native language users, these might not be suitable predictors of L2 difficulty being that the acquisition processes of L1 and L2 present a number of differences (Beinborn et al., 2012) .",
  "y": "background"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_2",
  "x": "These required less sophisticated text processing and had previously been used in several studies with success (Beinborn et al., 2012; Dell'Orletta et al., 2011; Fran\u00e7ois and Fairon, 2012; <cite>Heimann M\u00fchlenbock, 2013</cite>; Vajjala and Meurers, 2012) . We computed sentence length as the number of tokens including punctuation, and token length as the number of characters per token. Part of the syntactic features was based on the depth (length) and direction of dependency arcs (features 5-8). Another group of these features relied on the type of dependency relations. In feature 9 (Mod) nominal pre-modifiers (e.g. adjectives) and post-modifiers (e.g. relative clauses, prepositional phrases) were counted, similarly to Heimann M\u00fchlenbock (2013). Variation features (ModVar, AdvVar) measured the ratio of a morphosyntactic category to the number of lexical (content) words in the sentence, as in Vajjala and Meurers (2012) . These lexical categories comprised nouns, verbs, adverbs and adjectives.",
  "y": "similarities uses"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_3",
  "x": "We computed sentence length as the number of tokens including punctuation, and token length as the number of characters per token. Part of the syntactic features was based on the depth (length) and direction of dependency arcs (features 5-8). Another group of these features relied on the type of dependency relations. In feature 9 (Mod) nominal pre-modifiers (e.g. adjectives) and post-modifiers (e.g. relative clauses, prepositional phrases) were counted, similarly to Heimann M\u00fchlenbock (2013). Variation features (ModVar, AdvVar) measured the ratio of a morphosyntactic category to the number of lexical (content) words in the sentence, as in Vajjala and Meurers (2012) . These lexical categories comprised nouns, verbs, adverbs and adjectives. Subordinates (11) were detected on the basis of the \"UA\" (subordinate clause minus subordinating conjunction) dependency relation tag<cite> (Heimann M\u00fchlenbock, 2013)</cite> .",
  "y": "similarities"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_4",
  "x": "In feature 9 (Mod) nominal pre-modifiers (e.g. adjectives) and post-modifiers (e.g. relative clauses, prepositional phrases) were counted, similarly to Heimann M\u00fchlenbock (2013). Variation features (ModVar, AdvVar) measured the ratio of a morphosyntactic category to the number of lexical (content) words in the sentence, as in Vajjala and Meurers (2012) . These lexical categories comprised nouns, verbs, adverbs and adjectives. Subordinates (11) were detected on the basis of the \"UA\" (subordinate clause minus subordinating conjunction) dependency relation tag<cite> (Heimann M\u00fchlenbock, 2013)</cite> . Features DepDepth, Mod, Sub and RightDep, PrepComp have previously been empoyed for Swedish L1 readability at the text level in Heimann M\u00fchlenbock (2013) and<cite> Falkenjack et al. (2013)</cite> respectively. The lexical-morphological features (features 13-25) constituted the largest group. Difficulty at the lexical level was determined based on both the TTR feature mentioned above, expressing vocabulary diversity, and on the basis of the rarity of words (features 13-17) according to the Kelly list and the Wikipedia word list.",
  "y": "similarities uses"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_5",
  "x": "Features DepDepth, Mod, Sub and RightDep, PrepComp have previously been empoyed for Swedish L1 readability at the text level in Heimann M\u00fchlenbock (2013) and<cite> Falkenjack et al. (2013)</cite> respectively. The lexical-morphological features (features 13-25) constituted the largest group. Difficulty at the lexical level was determined based on both the TTR feature mentioned above, expressing vocabulary diversity, and on the basis of the rarity of words (features 13-17) according to the Kelly list and the Wikipedia word list. An analogous approach was adopted also by Fran\u00e7ois and Fairon (2012) , Vajjala and Meurers (2012) and Heimann M\u00fchlenbock (2013) with positive results. The LexD feature considers the ratio of lexical words (nouns, verbs, adjectives and adverbs) to the sum of tokens in the sentence (Vajjala and Meurers, 2012) . The NN/VB ratio feature, which has a higher value in written text, can also indicate a more complex sentence (Biber et al., 2004;<cite> Heimann M\u00fchlenbock, 2013)</cite> . Features 21-25 are based on evidence from the content of L2 Swedish course syllabuses (Folkuniversitet, 2013) and course books (Levy Scherrer and Lindemalm, 2009), part of them being language-dependent, namely S-VB/VB and S-VB%.",
  "y": "background"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_6",
  "x": "This feature was computed by counting the number of sense IDs per token according to a lexical-semantic resource for Swedish, SALDO (Borin et al., 2013) , and dividing this value by the number of tokens in the sentence. As pronouns indicate a potentially more difficult text (Graesser et al., 2011), we included PN/NN in our set. Both NomR and PN/NN capture idea density, i.e. how complex the relation between the ideas expressed are<cite> (Heimann M\u00fchlenbock, 2013)</cite>. ---------------------------------- **CLASSIFICATION RESULTS** The results obtained using the complete set of 28 features is shown in Table 3 . The results of the SVM are presented in comparison to a baseline classifier assigning the most frequent output label in the dataset to each instance.",
  "y": "background"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_7",
  "x": "The results of the SVM are presented in comparison to a baseline classifier assigning the most frequent output label in the dataset to each instance. classes was about 50-50%. The SVM classified 7 out of 10 sentences accurately. The precision and recall values for the identification of B1 sentences was 73% and 68%. Previous classification results for a similar task obtained an average of 77.25% of precision for the classification of easy-to-read texts within an L1 Swedish text-level readability study<cite> (Heimann M\u00fchlenbock, 2013)</cite> . Another classification at the sentence level, but for Italian and from an L1 perspective achieved an accuracy of 78.2%, thus 7% higher compared to our results (Dell'Orletta et al., 2011) . The 73% precision of our SVM model for classifying B1 sentences was close to the precision of 75.1% obtained for the easy-to-read sentences from Dell'Orletta et al. (2011) .",
  "y": "differences"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_8",
  "x": "terestingly, although semantic features represented the smallest group, they performed 2% better than traditional or syntactic features. The largest group of features including lexical-morphological indicators performed around 10% more accurately than other feature groups. Among the 10 features that influenced most the decisions of our SVM classifier, we can find attributes from different feature groups. The ID of these features together with the SVM weights are reported in Table 5 . An informative traditional measure was sentence length, similarly to the results of previous studies (Beinborn et al., 2012; Dell'Orletta et al., 2011; Fran\u00e7ois and Fairon, 2012; <cite>Heimann M\u00fchlenbock, 2013</cite>; Vajjala and Meurers, 2012) . Lexical-morphological features based on information about the frequency and the CEFR level of items in the Kelly list (DiffW%, DiffWs and KellyFr) also proved to be influential for the classification, as well as AdvVar. Two out of our three semantic features, namely NomR and, in particular, Sense/W, were also highly predictive.",
  "y": "similarities"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_9",
  "x": "An informative traditional measure was sentence length, similarly to the results of previous studies (Beinborn et al., 2012; Dell'Orletta et al., 2011; Fran\u00e7ois and Fairon, 2012; <cite>Heimann M\u00fchlenbock, 2013</cite>; Vajjala and Meurers, 2012) . Lexical-morphological features based on information about the frequency and the CEFR level of items in the Kelly list (DiffW%, DiffWs and KellyFr) also proved to be influential for the classification, as well as AdvVar. Two out of our three semantic features, namely NomR and, in particular, Sense/W, were also highly predictive. Syntactic features Ddep/SentLen and DeepDep, based on information about dependency arcs, were also among the ten features with highest weights, but they were somewhat less useful, as the weights in Table 5 show. Contrary to our results, Fran\u00e7ois and Fairon (2012) found syntactic features more informative than semantic ones for L2 French. This may depend either on the difference between the features used or the target languages. Moreover, in the case of Swedish L1 text readability the noun/pronoun ratio and modifiers proved to be indicative of textlevel difficulty<cite> (Heimann M\u00fchlenbock, 2013</cite> ), but at the sentence level from the L2 perspective only the latter seemed influential in our experiments.",
  "y": "differences"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_0",
  "x": "Against these data, we test both static and dynamic predictions of two neutral models, including the relation between corpus size and vocabulary size, frequency distributions, and turnover within those frequency distributions. Although a commonly used Neutral model fails to replicate all these emergent properties at once, we find that modified two-stage Neutral model does replicate the static and dynamic properties of the corpus data. This two-stage model is meant to represent a relatively small corpus (population) of English books, analogous to a 'canon', sampled by an exponentially increasing corpus of books in the wider population of authors. More broadly, this model-a smaller neutral model within a larger neutral model-could represent more broadly those situations where mass attention is focused on a small subset of the cultural variants. English has evolved continually over the centuries, in the branching off from antecedent languages in Indo-European prehistory [34, 39] , in the rates of regularisation of verbs [34] and in the waxing and waning in the popularity of individual words [3, 13, 37] . At a much finer scale of time and population, languages change through modifications and errors in the learning process [14, 27] . This continual change and diversity contrasts with the simplicity and consistency of Zipf's law, by which the frequency a word, f , is inversely proportional to its rank k, as f \u223c k \u2212\u03b3 and Heaps law, by which vocabulary size scales sub-linearly with total number of words, across diverse textual and spoken samples [32, 41, 46, 49, 15, 21, 48,<cite> 42]</cite> .",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_1",
  "x": "English has evolved continually over the centuries, in the branching off from antecedent languages in Indo-European prehistory [34, 39] , in the rates of regularisation of verbs [34] and in the waxing and waning in the popularity of individual words [3, 13, 37] . At a much finer scale of time and population, languages change through modifications and errors in the learning process [14, 27] . This continual change and diversity contrasts with the simplicity and consistency of Zipf's law, by which the frequency a word, f , is inversely proportional to its rank k, as f \u223c k \u2212\u03b3 and Heaps law, by which vocabulary size scales sub-linearly with total number of words, across diverse textual and spoken samples [32, 41, 46, 49, 15, 21, 48,<cite> 42]</cite> . The Google Ngram corpus [37] provides new support for these statistical regularities in word frequency dynamics at timescales from decades to centuries [22, 41,<cite> 42,</cite> 1, 28] of n-grams -an n-gram being n consecutive character strings, separated by spaces -derived from millions of books over multiple centuries [35] , the n-gram data now covers English books from the year 1500 to year 2008. In English, the Zipf's law in the n-gram data [41] exhibits two regimes: one among words with frequencies above about 0.01% (Zipf's exponent \u03b3 \u2248 1) and another (\u03b3 \u2248 1.4) among words with frequency below 0.0001% <cite>[42]</cite> . The latter Zipf's law exponent \u03b3 of 1.4 is equivalent to a probability distribution function (PDF) exponent, \u03b1, of about 1.7 (\u03b1 = 1 + 1/\u03b3). In addition to the well-known Zipf's law, word frequency data have at least two other statistical properties.",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_2",
  "x": "In English, the Zipf's law in the n-gram data [41] exhibits two regimes: one among words with frequencies above about 0.01% (Zipf's exponent \u03b3 \u2248 1) and another (\u03b3 \u2248 1.4) among words with frequency below 0.0001% <cite>[42]</cite> . The latter Zipf's law exponent \u03b3 of 1.4 is equivalent to a probability distribution function (PDF) exponent, \u03b1, of about 1.7 (\u03b1 = 1 + 1/\u03b3). In addition to the well-known Zipf's law, word frequency data have at least two other statistical properties. One, known as Heaps law, refers to the way that vocabulary size scales sub-linearly with corpus size (raw word count). The n-gram data show Heaps law in that, if N t is corpus size and v t is vocabulary size at time t, then v t \u2248 N \u03b2 t , with \u03b2 \u2248 0.5, for all English words in the corpus <cite>[42]</cite> . If the n-gram corpus is truncated by a minimum word count, then as that minimum is raised the Heaps scaling exponent increases from \u03b2 < 0.5, approaching \u03b2 < 1 <cite>[42]</cite> . The other statistical property is dynamic turnover in the ranked list of most commonly used words.",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_3",
  "x": "The n-gram data show Heaps law in that, if N t is corpus size and v t is vocabulary size at time t, then v t \u2248 N \u03b2 t , with \u03b2 \u2248 0.5, for all English words in the corpus <cite>[42]</cite> . If the n-gram corpus is truncated by a minimum word count, then as that minimum is raised the Heaps scaling exponent increases from \u03b2 < 0.5, approaching \u03b2 < 1 <cite>[42]</cite> . The other statistical property is dynamic turnover in the ranked list of most commonly used words. This can be measured in terms of how many words are replaced through time on \"Top y\" ranked lists of different sizes y of most frequently-used words [12, 17, 19, 23] . We can define this turnover z y (t) as the number of new words to have entered the top y most common words in year t, which is equivalent to the the top y in that year. The plotting of turnover z y for different list sizes y can therefore be useful in characterising turnover dynamics [2] . Many functional or network models readily yield the static Zipf distribution [21, 15] and Heaps law [36] , but not the dynamic aspects such as turnover.",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_4",
  "x": "One, known as Heaps law, refers to the way that vocabulary size scales sub-linearly with corpus size (raw word count). The n-gram data show Heaps law in that, if N t is corpus size and v t is vocabulary size at time t, then v t \u2248 N \u03b2 t , with \u03b2 \u2248 0.5, for all English words in the corpus <cite>[42]</cite> . If the n-gram corpus is truncated by a minimum word count, then as that minimum is raised the Heaps scaling exponent increases from \u03b2 < 0.5, approaching \u03b2 < 1 <cite>[42]</cite> . The other statistical property is dynamic turnover in the ranked list of most commonly used words. This can be measured in terms of how many words are replaced through time on \"Top y\" ranked lists of different sizes y of most frequently-used words [12, 17, 19, 23] . We can define this turnover z y (t) as the number of new words to have entered the top y most common words in year t, which is equivalent to the the top y in that year. The plotting of turnover z y for different list sizes y can therefore be useful in characterising turnover dynamics [2] .",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_5",
  "x": "The FNM does not, however, readily yield Heaps law (v t = N \u03b2 t , where \u03b2 < 1), for which \u03b2 \u2248 0.5 among the 1-gram data for English <cite>[42]</cite> . In the FNM, the expected exponent \u03b2 is 1.0, as the number of different variants (vocabulary) normally scales linearly with \u00b5N t [11] . While the FNM has been a powerful null model, in the case of books, we can make a notable improvement to account for the fact that most published material goes unnoticed while a relatively small portion of the corpus is highly visible. To name a few examples across the centuries, literally billions of copies of the Bible and the works of Shakespeare have been read since the seventeenth century, as well as tens or hundreds of millions of copies of works by Voltaire, Swift, Austen, Dickens, Tolkien, Fleming, Rawling and so on. While these and hundreds more books become considered part of the \"Western Canon,\" that canon is constantly evolving [28] and many books that were enormously popular in their time -e.g., Arabian Nights or the works of Fanny Burney-fall out of favour. As the published corpus has grown exponentially over the centuries, early authors were more able to sample the full range of historically published works, whereas contemporary authors sample from an increasingly small and more recent fraction of the corpus, simply due to its exponential expansion [28, 40] . As a simple way of capturing this, we propose a modified neutral model, called the partial-sampling Neutral model (PNM), of an evolving \"canon\" that is sampled by an exponentially-growing corpus of books.",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_6",
  "x": "What did ultimately work very well was our partial-sampling Neutral model, or PNM (Fig 1b) , which models a growing sample from a fixed-sized FNM. Our PNM, which takes exponentially increasing sample sizes from a neutrally evolved latent population, replicated the Zipf's law, Heaps law, and turnover patterns in the 1-gram data. Although it did not replicate exactly the particular 1-gram corpus we used here, the Heaps law exponent yielded by the PNM does fall within the range-from 0.44 to 0.54-observed in different English 1-gram corpora <cite>[42]</cite> . Among all features we attempted to replicate, the one mismatch between PNM and the 1-gram data is that the PNM yielded an order of magnitude fewer vocabulary words for a given corpus size, while increasing with corpus size according to the same Heaps law exponent. The reason for this mismatch appears to be a computational constraint: we could not run the PNM with exponential growth quite as large as that of the actual 300 years of exponential growth in the real English corpus. As a heuristic device, we consider the fixed-size FNM to represent a canonical literature, while the growing sample represents the real world of exponentially growing numbers of books published ever year in English. Of course, the world is not as simple as our model; there is no official fixed canon, that canon does not strictly copy words from the previous year only and there are plenty of words being invented that occur outside this canon.",
  "y": "similarities"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_7",
  "x": "Our PNM, which takes exponentially increasing sample sizes from a neutrally evolved latent population, replicated the Zipf's law, Heaps law, and turnover patterns in the 1-gram data. Although it did not replicate exactly the particular 1-gram corpus we used here, the Heaps law exponent yielded by the PNM does fall within the range-from 0.44 to 0.54-observed in different English 1-gram corpora <cite>[42]</cite> . Among all features we attempted to replicate, the one mismatch between PNM and the 1-gram data is that the PNM yielded an order of magnitude fewer vocabulary words for a given corpus size, while increasing with corpus size according to the same Heaps law exponent. The reason for this mismatch appears to be a computational constraint: we could not run the PNM with exponential growth quite as large as that of the actual 300 years of exponential growth in the real English corpus. As a heuristic device, we consider the fixed-size FNM to represent a canonical literature, while the growing sample represents the real world of exponentially growing numbers of books published ever year in English. Of course, the world is not as simple as our model; there is no official fixed canon, that canon does not strictly copy words from the previous year only and there are plenty of words being invented that occur outside this canon. Our canonical model of the PNM differs somewhat from the explanation by <cite>[42]</cite> , in which a \"decreasing marginal need for additional words\" as the corpus grows is underlain by the \"dependency network between the common words ... and their more esoteric counterparts.",
  "y": "differences"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_8",
  "x": "It was not computationally possible with either the FNM or PNM to replicate the Zipf across all nine orders of magnitude, as the modeled corpus size N t grows exponentially (Fig 2d) . Fig 3a illustrates the relationship between corpus size and vocabulary size in our partial-sampling Neutral model. Due to the exponentially increasing sample size, the ratio of vocabulary size over corpus size becomes increasingly small, thus the model gives us the sub-linear relationship described by v t = N \u03b2 t , where \u03b2 < 1. On the double-logarithmic plot in Fig 3a, the Heaps law exponent is equivalent to the slope of the data series. The PNM matches the 1-gram data with Heaps exponent (slope) of about 0.5, whereas the FNM, with exponent about 1.0, does not. Fig 3b shows how 100 runs of the PNM yields a Heaps law exponent within the range derived by <cite>[42]</cite> for several different n-grams corpora (all English, English fiction, English GB, English US and English 1M). We also The PNM yields Heaps law exponent \u03b2 \u2248 0.52 \u00b1 0.006, within the range of English corpora, whereas the FNM yields a mismatch with the data of \u03b2 \u2248 1 \u00b1 0.002 (Fig 3b) .",
  "y": "similarities background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_9",
  "x": "Although we track 1-grams from the year 1700, for turnover statistics we follow other studies <cite>[42]</cite> in being cautious about the n-grams record before the year 1800, due to misspelled words before 1800 that were surely digital scanning errors related to antique printing styles of that may conflate letters such as 's' and 'f' (e.g., myfelf, yourfelf, provifions, increafe, afked etc). The code used for modeling is available at: https://github.com/dr2g08/Neutral-evolution-and-turnover-over-centuries-of-English-word-popularity. ---------------------------------- **INTRODUCTION** English has evolved continually over the centuries, in the branching off from antecedent languages in Indo-European prehistory [34, 39] , in the rates of regularisation of verbs [34] and in the waxing and waning in the popularity of individual words [3, 13, 37] . At a much finer scale of time and population, languages change through modifications and errors in the learning process [14, 27] . This continual change and diversity contrasts with the simplicity and consistency of Zipf's law, by which the frequency a word, f , is inversely proportional to its rank k, as f \u223c k \u2212\u03b3 and Heaps law, by which vocabulary size scales sub-linearly with total number of words, across diverse textual and spoken samples [32, 41, 46, 49, 15, 21, 48,<cite> 42]</cite> .",
  "y": "uses"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_10",
  "x": "**INTRODUCTION** English has evolved continually over the centuries, in the branching off from antecedent languages in Indo-European prehistory [34, 39] , in the rates of regularisation of verbs [34] and in the waxing and waning in the popularity of individual words [3, 13, 37] . At a much finer scale of time and population, languages change through modifications and errors in the learning process [14, 27] . This continual change and diversity contrasts with the simplicity and consistency of Zipf's law, by which the frequency a word, f , is inversely proportional to its rank k, as f \u223c k \u2212\u03b3 and Heaps law, by which vocabulary size scales sub-linearly with total number of words, across diverse textual and spoken samples [32, 41, 46, 49, 15, 21, 48,<cite> 42]</cite> . The Google Ngram corpus [37] provides new support for these statistical regularities in word frequency dynamics at timescales from decades to centuries [22, 41,<cite> 42,</cite> 1, 28] . With annual counts of n-grams -an n-gram being n consecutive character strings, separated by spaces -derived from millions of books over multiple centuries [35] , the n-gram data now covers English books from the year 1500 to year 2008. In English, the Zipf's law in the n-gram data [41] exhibits two regimes: one among words with frequencies above about 0.01% (Zipf's exponent \u03b3 \u2248 1) and another (\u03b3 \u2248 1.4) among words with frequency below 0.0001% <cite>[42]</cite> .",
  "y": "background differences"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_11",
  "x": "The Google Ngram corpus [37] provides new support for these statistical regularities in word frequency dynamics at timescales from decades to centuries [22, 41,<cite> 42,</cite> 1, 28] . With annual counts of n-grams -an n-gram being n consecutive character strings, separated by spaces -derived from millions of books over multiple centuries [35] , the n-gram data now covers English books from the year 1500 to year 2008. In English, the Zipf's law in the n-gram data [41] exhibits two regimes: one among words with frequencies above about 0.01% (Zipf's exponent \u03b3 \u2248 1) and another (\u03b3 \u2248 1.4) among words with frequency below 0.0001% <cite>[42]</cite> . The latter Zipf's law exponent \u03b3 of 1.4 is equivalent to a probability distribution function (PDF) exponent, \u03b1, of about 1.7 (\u03b1 = 1 + 1/\u03b3). In addition to the well-known Zipf's law, word frequency data have at least two other statistical properties. One, known as Heaps law, refers to the way that vocabulary size scales sub-linearly with corpus size (raw word count). The n-gram data show Heaps law in that, if N t is corpus size and v t is vocabulary size at time t, then v t \u2248 N \u03b2 t , with \u03b2 \u2248 0.5, for all English words in the corpus <cite>[42]</cite> .",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_12",
  "x": "With annual counts of n-grams -an n-gram being n consecutive character strings, separated by spaces -derived from millions of books over multiple centuries [35] , the n-gram data now covers English books from the year 1500 to year 2008. In English, the Zipf's law in the n-gram data [41] exhibits two regimes: one among words with frequencies above about 0.01% (Zipf's exponent \u03b3 \u2248 1) and another (\u03b3 \u2248 1.4) among words with frequency below 0.0001% <cite>[42]</cite> . The latter Zipf's law exponent \u03b3 of 1.4 is equivalent to a probability distribution function (PDF) exponent, \u03b1, of about 1.7 (\u03b1 = 1 + 1/\u03b3). In addition to the well-known Zipf's law, word frequency data have at least two other statistical properties. One, known as Heaps law, refers to the way that vocabulary size scales sub-linearly with corpus size (raw word count). The n-gram data show Heaps law in that, if N t is corpus size and v t is vocabulary size at time t, then v t \u2248 N \u03b2 t , with \u03b2 \u2248 0.5, for all English words in the corpus <cite>[42]</cite> . If the n-gram corpus is truncated by a minimum word count, then as that minimum is raised the Heaps scaling exponent increases from \u03b2 < 0.5, approaching \u03b2 < 1 <cite>[42]</cite> .",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_13",
  "x": "The Google Ngram corpus [37] provides new support for these statistical regularities in word frequency dynamics at timescales from decades to centuries [22, 41,<cite> 42,</cite> 1, 28] . With annual counts of n-grams -an n-gram being n consecutive character strings, separated by spaces -derived from millions of books over multiple centuries [35] , the n-gram data now covers English books from the year 1500 to year 2008. In English, the Zipf's law in the n-gram data [41] exhibits two regimes: one among words with frequencies above about 0.01% (Zipf's exponent \u03b3 \u2248 1) and another (\u03b3 \u2248 1.4) among words with frequency below 0.0001% <cite>[42]</cite> . The latter Zipf's law exponent \u03b3 of 1.4 is equivalent to a probability distribution function (PDF) exponent, \u03b1, of about 1.7 (\u03b1 = 1 + 1/\u03b3). In addition to the well-known Zipf's law, word frequency data have at least two other statistical properties. One, known as Heaps law, refers to the way that vocabulary size scales sub-linearly with corpus size (raw word count). The n-gram data show Heaps law in that, if N t is corpus size and v t is vocabulary size at time t, then v t \u2248 N \u03b2 t , with \u03b2 \u2248 0.5, for all English words in the corpus <cite>[42]</cite> .",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_14",
  "x": "In English, the Zipf's law in the n-gram data [41] exhibits two regimes: one among words with frequencies above about 0.01% (Zipf's exponent \u03b3 \u2248 1) and another (\u03b3 \u2248 1.4) among words with frequency below 0.0001% <cite>[42]</cite> . The latter Zipf's law exponent \u03b3 of 1.4 is equivalent to a probability distribution function (PDF) exponent, \u03b1, of about 1.7 (\u03b1 = 1 + 1/\u03b3). In addition to the well-known Zipf's law, word frequency data have at least two other statistical properties. One, known as Heaps law, refers to the way that vocabulary size scales sub-linearly with corpus size (raw word count). The n-gram data show Heaps law in that, if N t is corpus size and v t is vocabulary size at time t, then v t \u2248 N \u03b2 t , with \u03b2 \u2248 0.5, for all English words in the corpus <cite>[42]</cite> . If the n-gram corpus is truncated by a minimum word count, then as that minimum is raised the Heaps scaling exponent increases from \u03b2 < 0.5, approaching \u03b2 < 1 <cite>[42]</cite> . The other statistical property is dynamic turnover in the ranked list of most commonly used words.",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_15",
  "x": "In this case we predict that the turnover z y can actually decrease with time as N t increases. This is because newly invented words start with frequency one, and under the neutral model they must essentially make a stochastic walk into the top 100, say. As N t grows, so does the minimum frequency needed to break into the top 100. As the \"bar\" is raised, words are more likely to 'die' before they ever reach the bar by stochastic walk [43] . As a result, turnover in the Top y can slow down over time and growth of N t . The FNM does not, however, readily yield Heaps law (v t = N \u03b2 t , where \u03b2 < 1), for which \u03b2 \u2248 0.5 among the 1-gram data for English <cite>[42]</cite> . In the FNM, the expected exponent \u03b2 is 1.0, as the number of different variants (vocabulary) normally scales linearly with \u00b5N t [11] .",
  "y": "differences"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_16",
  "x": "Fig 3a illustrates the relationship between corpus size and vocabulary size in our partial-sampling Neutral model. Due to the exponentially increasing sample size, the ratio of vocabulary size over corpus size becomes increasingly small, thus the model gives us the sub-linear relationship described by v t = N \u03b2 t , where \u03b2 < 1. On the double-logarithmic plot in Fig 3a, the Heaps law exponent is equivalent to the slope of the data series. The PNM matches the 1-gram data with Heaps exponent (slope) of about 0.5, whereas the FNM, with exponent about 1.0, does not. Fig 3b shows how 100 runs of the PNM yields a Heaps law exponent within the range derived by <cite>[42]</cite> for several different n-grams corpora (all English, English fiction, English GB, English US and English 1M). We also The PNM yields Heaps law exponent \u03b2 \u2248 0.52 \u00b1 0.006, within the range of English corpora, whereas the FNM yields a mismatch with the data of \u03b2 \u2248 1 \u00b1 0.002 (Fig 3b) . In Fig 3a, there is a constant offset on the y-axis between vocabulary size in the PNM (\u03b1 = 0.02, N = 10000) versus the 1-gram data.",
  "y": "similarities background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_19",
  "x": "In the case of conformity bias-where agents choose high-frequency words with even greater probability than just in proportion to frequency-both the Zipf law and turnover deteriorate under strong conformity in ways that mis-match with the data. What did ultimately work very well was our partial-sampling Neutral model, or PNM (Fig 1b) , which models a growing sample from a fixed-sized FNM. Our PNM, which takes exponentially increasing sample sizes from a neutrally evolved latent population, replicated the Zipf's law, Heaps law, and turnover patterns in the 1-gram data. Although it did not replicate exactly the particular 1-gram corpus we used here, the Heaps law exponent yielded by the PNM does fall within the range-from 0.44 to 0.54-observed in different English 1-gram corpora <cite>[42]</cite> . Among all features we attempted to replicate, the one mismatch between PNM and the 1-gram data is that the PNM yielded an order of magnitude fewer vocabulary words for a given corpus size, while increasing with corpus size according to the same Heaps law exponent. The reason for this mismatch appears to be a computational constraint: we could not run the PNM with exponential growth quite as large as that of the actual 300 years of exponential growth in the real English corpus. As a heuristic device, we consider the fixed-size FNM to represent a canonical literature, while the growing sample represents the real world of exponentially growing numbers of books published ever year in English.",
  "y": "similarities"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_20",
  "x": "Although it did not replicate exactly the particular 1-gram corpus we used here, the Heaps law exponent yielded by the PNM does fall within the range-from 0.44 to 0.54-observed in different English 1-gram corpora <cite>[42]</cite> . Among all features we attempted to replicate, the one mismatch between PNM and the 1-gram data is that the PNM yielded an order of magnitude fewer vocabulary words for a given corpus size, while increasing with corpus size according to the same Heaps law exponent. The reason for this mismatch appears to be a computational constraint: we could not run the PNM with exponential growth quite as large as that of the actual 300 years of exponential growth in the real English corpus. As a heuristic device, we consider the fixed-size FNM to represent a canonical literature, while the growing sample represents the real world of exponentially growing numbers of books published ever year in English. Of course, the world is not as simple as our model; there is no official fixed canon, that canon does not strictly copy words from the previous year only and there are plenty of words being invented that occur outside this canon. Our canonical model of the PNM differs somewhat from the explanation by <cite>[42]</cite> , in which a \"decreasing marginal need for additional words\" as the corpus grows is underlain by the \"dependency network between the common words ... and their more esoteric counterparts. \" In our PNM representation, there is no network structure between words at all, such as \"inter-word statistical dependencies\" [44] or grammar as a hierarchical network structure between words [20] .",
  "y": "differences"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_21",
  "x": "We set \u00b5 = 0.003 and run for t = 301 time steps representing years between 1700 and 2000, which are the same parameters used in the FNM. ---------------------------------- **1-GRAM DATA** The 1-gram data are available as csv files directly from Google's Ngrams site [25] . As in a previous study [1] , we removed 1-grams that are common symbols or numbers, and 1-grams containing the same consonant three or more times consecutively. As in our other studies [1, 8, 6] , we normalized the count of 1-grams using the yearly occurrences of the most common English word, the. Although we track 1-grams from the year 1700, for turnover statistics we follow other studies <cite>[42]</cite> in being cautious about the n-grams record before the year 1800, due to misspelled words before 1800 that were surely digital scanning errors related to antique printing styles of that may conflate letters such as 's' and 'f' (e.g., myfelf, yourfelf, provifions, increafe, afked etc).",
  "y": "uses"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_0",
  "x": "The sub-tasks are learned independently, and exact inference is used to find highest-scoring maximum spanning connected acyclic graph that contains all the concepts identified in the first stage. Later work by <cite>Wang et al. (2015b)</cite> adopted a different strategy based on the similarity between the dependency parse of a sentence and the semantic AMR graph. They start from the dependency parse and learn a transition-based parser that converts it into an AMR graph. To learn the parser, <cite>Wang et al. (2015b)</cite> define an algorithm that for each instance in the training data infers the action sequence that convert the input dependency tree into the corresponding AMR graph and train a classifier to predict the actions to be taken during testing. This strategy is also referred to as exact imitation learning, while the algorithm that infers the action sequence in the training instances is commonly referred to as the expert policy. In our submission to SemEval Task 8 on AMR parsing, we follow the transition-based paradigm of <cite>Wang et al. (2015b)</cite> with modifications to the parsing algorithm, and also use the DAGGER imitation learning algorithm (Ross et al., 2011) to generalise better to unseen data. The central idea of DAGGER is that the distribution of states encountered by the expert policy during training may not be a good approximation to those seen in testing by the trained policy.",
  "y": "background"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_1",
  "x": "This strategy is also referred to as exact imitation learning, while the algorithm that infers the action sequence in the training instances is commonly referred to as the expert policy. In our submission to SemEval Task 8 on AMR parsing, we follow the transition-based paradigm of <cite>Wang et al. (2015b)</cite> with modifications to the parsing algorithm, and also use the DAGGER imitation learning algorithm (Ross et al., 2011) to generalise better to unseen data. The central idea of DAGGER is that the distribution of states encountered by the expert policy during training may not be a good approximation to those seen in testing by the trained policy. Previous work by Rao et al. (2015) used SEARN, a similar imitation learning algorithm, on the AMR problem, with an algorithm that constructs the AMR graph directly from the sentence tokens. Imitation learning has also been used successfully in other semantic parsing tasks (Vlachos and Clark, 2014; Berant and Liang, 2015) . In imitation learning approaches such as DAG-GER the previous actions become features for classification learning. However the partial graphs in AMR parsing are rather complex to represent in this way, and combined with the finite amount of training data different actions can be chosen by the expert even though the feature representations for them can be very similar.",
  "y": "differences extends"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_2",
  "x": "In the following subsections we focus on the differences from previous work and in particular that of <cite>Wang et al. (2015b)</cite> who introduced the transitionbased dependency-to-AMR paradigm we follow. We initialise the main algorithm with a stack of the nodes in the dependency tree, root node first. This stack is termed \u03c3. A second stack, \u03b2 is initialised with all children of the top node in \u03c3. The state at any time is described by \u03c3, \u03b2, and the current graph (which starts as the dependency tree). Each action manipulates the top nodes in each stack, \u03c3 0 and \u03b2 0 . We reach a terminal state when \u03c3 is empty.",
  "y": "differences"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_3",
  "x": "inserted. All our experiments use node alignments from the system of Pourdamghani et al. (2014) . ---------------------------------- **ACTION SPACE** Flanigan et al. (2014) and <cite>Wang et al. (2015b)</cite> , both use AMR fragments as their smallest unit, which may consist of more than one AMR concept. Instead, we always work with the individual AMR nodes, and rely on Insert actions to learn how to build common fragments, such as country names. The main adaptations to the actions, summarised in Table 1 , stem from this.",
  "y": "differences"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_4",
  "x": "ReplaceHead covers two distinct actions in <cite>Wang et al. (2015b)</cite> ; ReplaceHead and Merge. Their Merge action merges \u03c3 0 and \u03b2 0 into a composite node; this is not required without composite nodes and retention of a 1:1 mapping between nodes and AMR concept. Unlike <cite>Wang et al. (2015b)</cite> we do not parameterise Swap or Reattach actions with a label. We leave that decision to a later NextEdge action. We permit a Reattach action to use parameter \u03ba equal to any node within six edges from \u03c3 0 , excluding any that would disconnect the graph or creating a cycle. The Insert action inserts a new node as a parent of the current \u03c3 0 . Wang et al. (2015a) later introduced an 'Infer' action similar to our Insert action.",
  "y": "background"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_5",
  "x": "NextNode and NextEdge form the core action set, labelling nodes and edges respectively without changing the graph structure. Swap, Reattach and ReplaceHead change this structure, but always retain a tree structure. ReplaceHead covers two distinct actions in <cite>Wang et al. (2015b)</cite> ; ReplaceHead and Merge. Their Merge action merges \u03c3 0 and \u03b2 0 into a composite node; this is not required without composite nodes and retention of a 1:1 mapping between nodes and AMR concept. Unlike <cite>Wang et al. (2015b)</cite> we do not parameterise Swap or Reattach actions with a label. We leave that decision to a later NextEdge action. We permit a Reattach action to use parameter \u03ba equal to any node within six edges from \u03c3 0 , excluding any that would disconnect the graph or creating a cycle.",
  "y": "differences"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_6",
  "x": "The current \u03c3 0 node is shown dashed and in red. From the top the actions are Insert(dateentity); NextNode(WORD); NextEdge(year); second diagram; NextNode(WORD); ReplaceHead to remove \"in\"; third diagram; NextNode(WORD); NextEdge(mod); Reattach to move \"date-entity\"; fourth diagram; NextNode(VERB); ReplaceHead to remove \"by\"; NextEdge(ARG0); NextEdge(time); NextNode(strike-01). <cite>Wang et al. (2015b)</cite> use all AMR concepts and relations that appear in the training set as possible parameters (l c and l r ) if they appear in any sentence containing the same lemma as \u03c3 0 and \u03b2. We reduce this to just concepts that have been aligned to the current lemma. We initially run the expert policy over the training set, and track the AMR concept assigned for each lemma. These provide the possible l c that will be used for NextNode actions. Similarly we track the lemmas at head and tail of each expert-assigned AMR relation, and compile possible l r from these.",
  "y": "differences"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_7",
  "x": "At each further iteration the AMR size threshold for the training set was increased. The motivation was to train the classifier on 'easy' sentences, before introducing more complex ones. We start with up to 30 nodes, and increase this by 10 each iteration. Rao et al. (2015) similarly use the smallest sentences for training, but do not increase the size threshold as training proceeds. ---------------------------------- **FEATURES** All features used are detailed in Table 2 , largely based on <cite>Wang et al. (2015b)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_8",
  "x": "Rao et al. (2015) similarly use the smallest sentences for training, but do not increase the size threshold as training proceeds. ---------------------------------- **FEATURES** All features used are detailed in Table 2 , largely based on <cite>Wang et al. (2015b)</cite> . All are 0-1 indicator functions. inserted is 1 if the node was inserted by the parser; dl is the dependency label in the original dependency tree; ner the named entity tag; POS the part-of-speech tag; prefix is the string before the hyphen if word is hyphenated; suffix is the string after the hyphen; brown is the 100-class Brown cluster id with cuts at 4, 6, 10 and 20 2 ; deleted is the lemma of any child node previously deleted by the parser; merged is the lemma of any node merged into this node by a ReplaceHead action; distance is the distance between the tokens in the sentence; path concatenates lemmas and dls between the tokens in the dependency tree; POSpath concatenates POS tags between the tokens; NERpath concatenates NER tags between the tokens. The key differences to <cite>Wang et al. (2015b)</cite> are the inclusion of the brown, POSpath, NERpath, prefix and suffix feature types.",
  "y": "extends differences"
 },
 {
  "id": "d9567072d2df6c0010b32e1d1eb676_0",
  "x": "Models trained in this manner often struggle to overcome previous prediction errors. Generative Adversarial Networks (Goodfellow et al., 2014) offer a solution for exposure bias. * The authors contributed equally Originally introduced for images, GANs leverage a discriminator, which is trained to discriminate between real images and generated images via an adversarial loss. In such a framework, the generator is not directly exposed to the ground truth data, but instead learns to imitate it using global feedback from the discriminator. This has led to several attempts to use GANs for text generation, with a generator using either a recurrent neural network (RNN) Guo et al., 2017;<cite> Press et al., 2017</cite>; Rajeswar et al., 2017) , or a Convolutional Neural Network (CNN) (Gulrajani et al., 2017; Rajeswar et al., 2017) . However, evaluating GANs is more difficult than evaluating LMs. While in language modeling, evaluation is based on the log-probability of a model on held-out text, this cannot be straightforwardly extended to GAN-based text generation, because the generator outputs discrete tokens, rather than a probability distribution.",
  "y": "background"
 },
 {
  "id": "d9567072d2df6c0010b32e1d1eb676_1",
  "x": "A main challenge in applying GANs for text is that generating discrete symbols is a nondifferentiable operation. One solution is to perform a continuous relaxation of the GAN output, which leads to generators that emit a nearly discrete continuous distribution<cite> (Press et al., 2017)</cite> . This keeps the model differentiable and enables end-to-end training through the discriminator. Alternatively, SeqGAN and Leak-GAN (Guo et al., 2017) used policy gradient methods to overcome the differentiablity requirement. We apply our approximation to both model types. 3 Evaluating GANs and LMs LM Evaluation. Text generation from LMs is commonly evaluated using probabilistic metrics.",
  "y": "background"
 },
 {
  "id": "d9567072d2df6c0010b32e1d1eb676_2",
  "x": "However, perplexity often correlates with extrinsic measures (Jurafsky and Martin, 2018) , and is the de-facto metric for evaluating the quality of language models today. ---------------------------------- **GAN-BASED TEXT GENERATION EVALUATION.** By definition, a text GAN outputs a discrete sequence of symbols rather than a probability distribution. As a result, LM metrics cannot be applied to evaluate the generated text. Consequently, other metrics have been proposed: \u2022 N-gram overlap:<cite> Press et al., 2017)</cite> : Inspired by BLEU (Papineni et al., 2002) , this measures whether n-grams generated by the model appear in a held-out corpus.",
  "y": "background"
 },
 {
  "id": "d9567072d2df6c0010b32e1d1eb676_3",
  "x": "We propose a method for approximating a distribution over tokens from a GAN, and then evaluate the model with standard LM metrics. We will describe our approach given an RNN-based LM, which is the most commonly-used architecture, but the approximation can be applied to other auto-regressive models (Vaswani et al., 2017) . ---------------------------------- **LANGUAGE MODEL APPROXIMATION** The inputs to an RNN at time step t, are the state vector h t and the current input token x t . The output token (one-hot) is denoted by o t . In RNNbased GANs, the previous output token is used at inference time as the input x t Guo et al., 2017;<cite> Press et al., 2017</cite>; Rajeswar et al., 2017) .",
  "y": "background"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_0",
  "x": "Enter keyword spotting systems. They solve the aforementioned issues by implementing an on-device mechanism to \"wake up\" the intelligent agent, e.g., \"Okay, Google\" for triggering the Android assistant. This then allows the device to record and transmit a limited segment of relevant speech only, obviating the need to be always-listening. Specifically, the task of keyword spotting (KWS) is to detect the presence of pre-specified phrases in a stream of audio, often with the end goal of wake-word detection or simple command recognition on device. Currently, state of the art uses lightweight neural networks [1, 2,<cite> 3,</cite> 4] , which can perform inference in real-time even on low-end devices [4, 5] . Despite the popularity of voice-enabled products, web applications have yet to make use of keyword spotting. This is surprising, since modern web applications are supported on billions of devices ranging from desktops to smartphones.",
  "y": "background"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_1",
  "x": "Unfortunately, the browser is a highly inefficient platform for deploying neural networks, mainly due to poorly optimized matrix multiply routines. Fortunately, in recent years, the art of compressing neural networks has made significant advances in both general [6, 7, 8] and keyword spotting literature [4, 9] . On our task, we demonstrate that network slimming [6] is a simple yet highly effective method to achieve low latency with minimal impact on accuracy. Thus, our main contributions are as follows: first, we develop a novel web application with an in-browser KWS system based on previous state-of-the-art<cite> [3]</cite> models. Second, we provide the first set of comprehensive experimental results for the latency of an in-browser KWS system on a broad range of devices. Finally, to the best of our knowledge, we are the first to apply network slimming to examine various accuracyefficiency operating points of a state-of-the-art KWS model. On the Google Speech Commands dataset [10] , our most accurate in-browser model achieves an accuracy of 94% while performing inference in less than 10 milliseconds.",
  "y": "background"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_2",
  "x": "Finally, to the best of our knowledge, we are the first to apply network slimming to examine various accuracyefficiency operating points of a state-of-the-art KWS model. On the Google Speech Commands dataset [10] , our most accurate in-browser model achieves an accuracy of 94% while performing inference in less than 10 milliseconds. With network slimming, we further reduce latency by 66% while increasing the error rate by only 4%. ---------------------------------- **BACKGROUND AND RELATED WORK** Keyword spotting. KWS is the task of detecting a spoken phrase in audio, applicable to simple command recognition <cite>[3,</cite> 10] and wake-word detection [2, 1] .",
  "y": "background"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_3",
  "x": "Keyword spotting. KWS is the task of detecting a spoken phrase in audio, applicable to simple command recognition <cite>[3,</cite> 10] and wake-word detection [2, 1] . A typical requirement is that such a KWS system must be small-footprint at inference time, since the target platforms are mobile phones, Internet-of-things (IoT) devices, and other portable electronics. To achieve this goal, resource-efficient architectures using convolutional neural networks (CNNs) <cite>[3,</cite> 1] and recurrent neural networks (RNNs) [2] have been proposed, while other works make use of low-bitwidth weights [4, 9] . However, despite the pervasiveness of modern web browsers in devices from smartphones to desktops, and in spite of the availability of JavaScript-based deep learning toolkits, implementing on-device KWS systems in web applications has never been done before. Compressing neural networks. Sparse matrix storage leads to inefficient computation and storage in general-purpose hardware; thus, inducing structured sparsity in neural networks, e.g., on entire rows and columns, has been the cornerstone of various compression techniques [6, 8] .",
  "y": "background"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_4",
  "x": "**DATA AND IMPLEMENTATION** For consistency with past results <cite>[3,</cite> 5] , we train our models on the first version of the Google Speech Commands dataset [10] , which comprises a total of 65,000 spoken utterances for 30 short, one-second phrases. To compare with past work<cite> [3]</cite> , we pick the following twelve classes: \"yes,\" \"no,\" \"stop,\" \"go,\" \"left,\" \"right,\" \"on,\" \"off,\" unknown, and silence. It contains roughly 2,000 examples per class, including a few background noise samples of both man-made and artificial noise, e.g., washing dishes and white noise. As is standard in speech processing literature, all audio is in 16-bit PCM, 16kHz mono-channel WAV format. We use the standard 80%, 10%, and 10% splits for the training, validation, and test sets, respectively <cite>[3,</cite> 10] . ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_5",
  "x": "For consistency with past results <cite>[3,</cite> 5] , we train our models on the first version of the Google Speech Commands dataset [10] , which comprises a total of 65,000 spoken utterances for 30 short, one-second phrases. To compare with past work<cite> [3]</cite> , we pick the following twelve classes: \"yes,\" \"no,\" \"stop,\" \"go,\" \"left,\" \"right,\" \"on,\" \"off,\" unknown, and silence. It contains roughly 2,000 examples per class, including a few background noise samples of both man-made and artificial noise, e.g., washing dishes and white noise. As is standard in speech processing literature, all audio is in 16-bit PCM, 16kHz mono-channel WAV format. We use the standard 80%, 10%, and 10% splits for the training, validation, and test sets, respectively <cite>[3,</cite> 10] . ---------------------------------- **INPUT PREPROCESSING**",
  "y": "similarities uses"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_6",
  "x": "**DATA AND IMPLEMENTATION** For consistency with past results <cite>[3,</cite> 5] , we train our models on the first version of the Google Speech Commands dataset [10] , which comprises a total of 65,000 spoken utterances for 30 short, one-second phrases. To compare with past work<cite> [3]</cite> , we pick the following twelve classes: \"yes,\" \"no,\" \"stop,\" \"go,\" \"left,\" \"right,\" \"on,\" \"off,\" unknown, and silence. It contains roughly 2,000 examples per class, including a few background noise samples of both man-made and artificial noise, e.g., washing dishes and white noise. As is standard in speech processing literature, all audio is in 16-bit PCM, 16kHz mono-channel WAV format. We use the standard 80%, 10%, and 10% splits for the training, validation, and test sets, respectively <cite>[3,</cite> 10] . ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_7",
  "x": "Then, for the feature extraction step, 40-dimensional Mel-frequency cepstral coefficients (MFCCs) are computed, with a window size of 30 milliseconds and a frame shift of 10 milliseconds, yielding a final preprocessed input size of 101 \u00d7 40 for each one-second audio sample. ---------------------------------- **MODEL ARCHITECTURE** We use the res8 and res8-narrow architectures from Tang and Lin<cite> [3]</cite> as a starting point, which represent prior state of the art in residual CNNs [13] for KWS. In both models, given the input X \u2208 R 101\u00d740 , we first expand the input channel-wise by applying a 2D convolution layer with weights W \u2208 R Cout\u00d71\u00d7(3\u00d73) and padding of one on all sides. This step results in an output ofX \u2208 R Cout\u00d7101\u00d740 , which we then downsample using an average pooling layer with a kernel size of (4, 3). Next, inspired by insights in image classification [13] , the output is passed through a series of three residual blocks comprising convolution and batch normalization [11] layers- Figure 2 illustrates one such block.",
  "y": "similarities uses"
 },
 {
  "id": "d9d5fce2b33c15bf073a5840930be1_1",
  "x": "Table 1 describes the algorithm for syntactic simplification to produce grammatically correct sentences. The algorithm has a time complexity of O(n 2 *R), where n is the number of tokens in the sentence and R is the number of simplifications rules. The average time complexity is, however, O(nlog(n)*R). One of the features of BioSimplify is avoidance of domainspecific rules. For example, we don't replace entity names (like genes) with shorter alternatives as is done with the noun phrases in the present version and with the gene names in our earlier version <cite>11</cite> . We also avoided hard-coding the words in the rules created to split sentences with relative clauses. These measures enhance the domain adaptability of the system.",
  "y": "differences"
 },
 {
  "id": "d9d5fce2b33c15bf073a5840930be1_2",
  "x": "PIE returns two kinds of results -one with a high precision, which we call tight PIE; and the other with low precision, which we call light PIE. We also compare the present version of BioSimplify with the older version <cite>11</cite> which is limited in its functionality because it only implements the rules described by Siddharthan 4 . The present version which has an average time complexity of O(nlog(n)*R) is faster than the older version which has a time complexity of O(n 3 *R), where n is the number of tokens in the sentence and R is the number of rules. The older version has domain specific optimizations (like replacing the gene names with single-word identifiers), which were not used in the newer version for portability. ---------------------------------- **RESULTS AND CONCLUSION** We used PIE to test for the presence of PPIs before and after simplification in both the versions.",
  "y": "uses"
 },
 {
  "id": "d9d5fce2b33c15bf073a5840930be1_3",
  "x": "---------------------------------- **PPI EXTRACTION EVALUATION** For the purpose of evaluating the impact of sentence simplification, we use AIMed 17 corpus (which is extensively used in comparing PPI extraction methods) and PIE 18 (a machine-learning based approach available as a web service that uses the parse tree information from the Collins statistical parser as its key component). PIE returns two kinds of results -one with a high precision, which we call tight PIE; and the other with low precision, which we call light PIE. We also compare the present version of BioSimplify with the older version <cite>11</cite> which is limited in its functionality because it only implements the rules described by Siddharthan 4 . The present version which has an average time complexity of O(nlog(n)*R) is faster than the older version which has a time complexity of O(n 3 *R), where n is the number of tokens in the sentence and R is the number of rules. The older version has domain specific optimizations (like replacing the gene names with single-word identifiers), which were not used in the newer version for portability.",
  "y": "differences"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_0",
  "x": "This concentration facilitates the capturing of dependencies between parts of the input and the output. After training the network, the attention mechanism enables the system to perform translations that can handle issues such as the movement of words and phrases, and fertility. However, even with these attention mechanisms, NMT models have their drawbacks, which include long training time and high computational requirements. Recent papers <cite>[3]</cite> , [4] in neural machine translation have proposed the strict use of attention mechanisms in networks such as the Transformer over previous approaches such as recurrent neural networks (RNNs) [5] and convolutional neural networks (CNNs) [6] . In other words, these approaches dispense with recurrences and convolutions entirely. In practice, attention mechanisms have mostly been used with recurrent architectures because removing the recurrent nature of the architecture makes the training more efficient by the removal of necessary sequential steps. This paper contributes by continuing to pursue the removal of sequential operations within encoder-decoder models.",
  "y": "background"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_1",
  "x": "Our work reported in this paper is based on another translation work by Google. Google's Vaswani et al. <cite>[3]</cite> proposed the reduction in the sequential steps seen in CNNs and RNNs. The sole use of attention mechanisms and feed-forward networks within the common encoder-decoder sequential model replaces the necessity of deep convolutions for distant dependent relationships, and the memory and computation intensive operations required within recurrent networks. Original training and testing by Vaswani et al. were over both the WMT 2014 English-French (EN-FE) and English-German (EN-DE) data sets, while this paper uses only the WMT 2014 EN-DE set and the IWSLT 2014 EN-DE and EN-FR data sets. This model is discussed later in the paper. Works in the field of NMT recommend a particular focus on the encoder. Analysis by Domhan [11] poses two questions: what type of attention is needed, and where.",
  "y": "background"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_2",
  "x": "**III. ARCHITECTURE** The Transformer architectures proposed by Vaswani et al. <cite>[3]</cite> , seen in Figure 1 , inspires this paper's work. We have made modifications to this architecture, to make it more efficient. However, our modifications can be applied to any encoder-decoder based model and is architecture-agnostic. These alterations follow from the following two hypotheses. 1) Reduction in the number of required sequential operations throughout the encoder section is likely to reduce training time without reducing performance. 2) Replacing the subsequent encoder attention stack is expected to result in discarding of inter-dependencies, and possibly incorrect, assumptions of encoder attention mechanisms and layers, improving performance.",
  "y": "similarities"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_3",
  "x": "Around each of the main sub-layers, a skip or residual connection [13] is also used. This same structure is used in the decoder with an attention mask to avoid attending to subsequent positions. The attention mechanism used by Vaswani et al. <cite>[3]</cite> can be thought of as a function that maps a query and set of keyvalue pairs to an output. The query, keys, values and output are all vectors. The output is obtained as a weighted sum of the values. The weight given to a value is learned by the system by considering how compatible the query is to the corresponding key. The particular form of attention used is called scaled dot-product attention.",
  "y": "background"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_4",
  "x": "---------------------------------- **A. PARALLEL ENCODING BRANCHES** A motivation for creating the Transformer model was the sluggish training and generation times of other common sequence-to-sequence models such as RNNs and CNNs <cite>[3]</cite> . This was done by simplifying and limiting sequential operations and computational requirements while also increasing the model's ability to exploit current hardware architecture. This paper proposes that removal of the previously stacked branches of the encoder (there is a stack of N encoder and other blocks on the left side of Figure 1 ), parallelizing these separate encoder 'trees', and incorporating their learned results for the decoder, will further eliminate sequential steps and accelerate learning within current sequence-to-sequence models. The architectures discussed are modeled in Figure 2 . Alterations to this parallel Transformer model were made and the following models were trained, tested, and are discussed in this paper:",
  "y": "background"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_5",
  "x": "**IV. EXPERIMENTS AND EVALUATION** All proposed architectures including the base Transformer model <cite>[3]</cite> are trained over the International Workshop on Spoken Language Translation (IWSLT) 2016 corpus and tested similarly over the IWSLT 2014 test corpus [15] . The training corpus includes over 200,000 parallel sentence pairs, and 4 million tokens for each language. The testing set contains 1,250 sentences, and 20-30 thousand tokens for French and German. This paper also performed experiments over the larger WMT data set including 4.5 and 36 million training sentence pairs for the EN-DE and EN-FR tasks respectively. The testing set for these experiments was the standard Newstest 2014 test set including around 3000 sentence pairs for each language task. These statistics are noted in Table I a full measure of the tested models and robustness to both short and long input.",
  "y": "background"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_6",
  "x": "This is one area of research this group wishes to pursue in the future. Table II shows that the AAPA model consistently performed on average nearly ten points higher in the BLEU metric on the English to German translation task on the IWSLT 2014 test set. It also performed very well on the English to French translation task. ---------------------------------- **B. MACHINE TRANSLATION** On the much larger WMT English-German test set, all our models achieve better results then Vaswani et al. <cite>[3]</cite> . Our model with five parallel encoding branches has a BLEU score of 62.69 compared to 60.95 and 61.00 for the two Transformers shown in Table III .",
  "y": "differences"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_7",
  "x": "Our approach also takes considerably less time than the large Transformer model with a stack of eight encoder attention heads, although it is a little slower than the smaller Transformer model reported by Vaswani et al. <cite>[3]</cite> . In terms of the BLEU metric, we establish state-of-the-art performance for both EN-DE and EN-FR translation considering the IWSLT 2014, and comparable results for the WMT data sets. Since our results came up very good, surpassing state of the art for the IWSLT 2014 dataset, we ran our experiments multiple times to ensure the results are correct. During the Transformer and attended parallel model's training lifetime, it can be seen that loss was consistently lower for our modified parallel model with five parallel stacks as seen in Figure 3 . In this task, loss doesn't always correspond to a higher metric, in this case our model also shows a continuous higher score in the BLEU metric over the validation set while the Transformer shows signs of plateauing early on Figure 4 . However, our parallelized model did have a slightly higher training time over a single GPU. One final experiment conducted to improve this drawback, also seen in the same table, is the reduction of number of parallel branches in the encoder.",
  "y": "differences"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_0",
  "x": "The PeerRead dataset<cite> (Kang et al., 2018)</cite> is an excellent resource towards research and study on this very impactful and crucial problem. With our ongoing effort towards the development of an Artificial Intelligence (AI)-assisted peer review system, we are intrigued with: What if there is an additional AI reviewer which predicts decisions by learning the high-level interplay between the review texts and the papers? How would the sentiment embedded within the review texts empower such decision-making? Although editors/program chairs usually go by the majority of the reviewer recommendations, they still need to go through all the review texts corresponding to all the submissions. A good use case of this research would be: slot-filling the missing reviewer, providing an additional perspective to the editor in cases of contrasting/borderline reviews. This work in no way attempts to replace the human reviewers; instead, we are intrigued to see how an AI can act as an additional reviewer with inputs from her human counterparts and aid the decision-making in the peer review process. We develop a deep neural architecture incorporating full paper information and review text along with the associated sentiment to predict the acceptability and recommendation score of a given research article. We perform two tasks, a classification (predicting accept/reject decision) and a regression (predicting recommendation score) one.",
  "y": "background"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_1",
  "x": "The famous Toronto Paper Matching system (Charlin and Zemel, 2013) was developed to match paper with reviewers. Recently we (Ghosal et al., 2018b,a) investigated the impact of various features in the editorial pre-screening process. Wang and Wan (2018) explored a multi-instance learning framework for sentiment analysis from the peer review texts. We carry our current investigations on a portion of the recently released PeerRead dataset<cite> (Kang et al., 2018)</cite> . Study towards automated support for peer review was otherwise not possible due to the lack of rejected paper instances and corresponding reviews. Our approach achieves significant performance improvement over the two tasks defined in<cite> Kang et al. (2018)</cite> . We attribute this to the use of deep neural networks and augmentation of review sentiment information in our architecture.",
  "y": "uses"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_2",
  "x": "Wang and Wan (2018) explored a multi-instance learning framework for sentiment analysis from the peer review texts. We carry our current investigations on a portion of the recently released PeerRead dataset<cite> (Kang et al., 2018)</cite> . Study towards automated support for peer review was otherwise not possible due to the lack of rejected paper instances and corresponding reviews. Our approach achieves significant performance improvement over the two tasks defined in<cite> Kang et al. (2018)</cite> . We attribute this to the use of deep neural networks and augmentation of review sentiment information in our architecture. ---------------------------------- **DATA DESCRIPTION AND ANALYSIS**",
  "y": "differences"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_3",
  "x": "We attribute this to the use of deep neural networks and augmentation of review sentiment information in our architecture. ---------------------------------- **DATA DESCRIPTION AND ANALYSIS** The PeerRead dataset consists of papers, a set of associated peer reviews, and corresponding accept/reject decisions with aspect specific scores of papers collected from several top-tier Artificial Intelligence (AI), Natural Language Processing (NLP) and Machine Learning (ML) conferences. Table 1 shows the data we consider in our experiments. We could not consider NIPS and arXiv portions of PeerRead due to the lack of aspect scores and reviews, respectively. For more details on the dataset creation and the task, we request the readers to refer to<cite> Kang et al. (2018)</cite> .",
  "y": "background"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_4",
  "x": "We further use the submissions of ICLR 2018, corresponding reviews and aspect scores to boost our training set for the decision prediction task. One motivation of our work stems from the finding that aspect scores for certain factors like Impact, Originality, Soundness/Correctness which are seemingly central to the merit of the paper, often have very low correlation with the final recommendation made by the reviewers as is made evident in<cite> Kang et al. (2018)</cite> . However, from the heatmap in Figure 1 we can see that the reviewer's sentiments (compound/positive) embedded within the review texts have visible correlations with the aspects like Recommendation, Appropriateness and Overall Decision. This also seconds our recent finding that determining the scope or appropriateness of an article to a venue is the first essential step in peer review (Ghosal et al., 2018a) . Since our study aims at deciding the fate of the paper, we take predicting recommendation score and overall decision as the objectives of our investigation. Thus our proposal to augment sentiment of reviews to the deep neural architecture seems intuitive. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_5",
  "x": "In the same way, we can get r as the output of the convolution operator for the Review R. We call the outputs p and r as the high-level representation feature vector of the paper and the review, respectively. We then concatenate these feature vectors (Feature-Level Fusion). The reason we extract features from both is to simulate the editorial workflow, wherein ideally, the editor/chair would look at both into the paper and the corresponding reviews to arrive at a judgement. ---------------------------------- **MULTI-LAYER PERCEPTRON** We employ a Multi-Layer Perceptron (MLP Predict) to take the joint paper+review representations x pr as input to get the final<cite> (Kang et al., 2018)</cite> , RMSE\u2192Root Mean Squared Error.",
  "y": "uses"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_6",
  "x": "We then concatenate these feature vectors (Feature-Level Fusion). The reason we extract features from both is to simulate the editorial workflow, wherein ideally, the editor/chair would look at both into the paper and the corresponding reviews to arrive at a judgement. ---------------------------------- **MULTI-LAYER PERCEPTRON** We employ a Multi-Layer Perceptron (MLP Predict) to take the joint paper+review representations x pr as input to get the final<cite> (Kang et al., 2018)</cite> , RMSE\u2192Root Mean Squared Error. CNN variant as in<cite> (Kang et al., 2018</cite> ) is used as the comparing system. representation as",
  "y": "uses"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_7",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** As we mention earlier, we undertake two tasks: Task 1: Predicting the overall recommendation score (Regression) and Task 2: Predicting the Accept/Reject Decision (Classification). To compare with<cite> Kang et al. (2018)</cite> , we keep the experimental setup (train vs test ratio) identical and re-implement their codes to generate the comparing figures. However,<cite> Kang et al. (2018)</cite> performed Task 2 on ICLR 2017 dataset with handcrafted features, and Task 1 in a deep learning setting. Since our approach is a deep neural network based, we crawl additional paper+reviews from ICLR 2018 to boost the training set. For Task 1, n 1 is 666 and n 2 is 98 while for Task 2, n 1 is 1494 and n 2 is 525.",
  "y": "uses"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_8",
  "x": "To compare with<cite> Kang et al. (2018)</cite> , we keep the experimental setup (train vs test ratio) identical and re-implement their codes to generate the comparing figures. However,<cite> Kang et al. (2018)</cite> performed Task 2 on ICLR 2017 dataset with handcrafted features, and Task 1 in a deep learning setting. Since our approach is a deep neural network based, we crawl additional paper+reviews from ICLR 2018 to boost the training set. For Task 1, n 1 is 666 and n 2 is 98 while for Task 2, n 1 is 1494 and n 2 is 525. We employ a grid search for hyperparameter optimization. For Task 1, F is 256, l is 5. ReLU is the non-linear function g(), learning rate is 0.007. We train the model with SGD optimizer, set momentum as 0.9<cite> (Kang et al., 2018</cite> ) is feature-based and considers only paper, and not the reviews.",
  "y": "differences extends"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_9",
  "x": "To compare with<cite> Kang et al. (2018)</cite> , we keep the experimental setup (train vs test ratio) identical and re-implement their codes to generate the comparing figures. However,<cite> Kang et al. (2018)</cite> performed Task 2 on ICLR 2017 dataset with handcrafted features, and Task 1 in a deep learning setting. Since our approach is a deep neural network based, we crawl additional paper+reviews from ICLR 2018 to boost the training set. For Task 1, n 1 is 666 and n 2 is 98 while for Task 2, n 1 is 1494 and n 2 is 525. We employ a grid search for hyperparameter optimization. For Task 1, F is 256, l is 5. ReLU is the non-linear function g(), learning rate is 0.007. We train the model with SGD optimizer, set momentum as 0.9<cite> (Kang et al., 2018</cite> ) is feature-based and considers only paper, and not the reviews.",
  "y": "uses"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_10",
  "x": "Again we train the model with Adam Optimizer, keep the batch size as 64 and use 0.7 as the dropout rate to prevent overfitting. We intentionally keep our CNN/MLP shallow due to less training data. We make our codes 4 available for further explorations. Table 2 and Table 3 show our results for both the tasks. We propose a simple but effective architecture in this work since our primary intent is to establish that a sentiment-aware deep architecture would better suit these two problems. For Task 1, we can see that our review sentiment augmented approach outperforms the baselines and the comparing systems by a wide margin (\u223c 29% reduction in error) on the ICLR 2017 dataset. With only using review+sentiment information, we are still able to outperform<cite> Kang et al. (2018)</cite> by a margin of 11% in terms of RMSE.",
  "y": "differences"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_11",
  "x": "---------------------------------- **RESULTS AND ANALYSIS** For Task 2, we observe that the handcrafted feature-based system by<cite> Kang et al. (2018)</cite> performs inferior compared to the baselines. This is because the features were very naive and did not 4 https://github.com/aritzzz/DeepSentiPeer address the complexity involved in such a task. We perform better with a relative improvement of 28% in terms of accuracy, and also our system is end-toend trained. Presumably, to some extent, our deep neural network learned to distinguish between the probable accept versus probable reject by extracting useful information from the paper and review data. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_12",
  "x": "With the additional (but less) data of ACL 2017 and CoNLL 2016 in PeerRead, we perform the cross-domain experiments. We do training with the ICLR data (core Machine Learning papers) and take the test set from the NLP conferences (ACL/CoNLL). NLP nowadays is mostly machine learning (ML) centric, where we find several applications and extensive usage of ML algorithms to address different NLP problems. Here we observe a relative error reduction of 4.8% and 14.5% over the comparing system for ACL 2017 and CoNLL 2016, respectively (Table 2 ). For the decision prediction task, the comparing system performs even worse, and we outperform them by a considerable margin of 28% (ACL 2017) and 26% (CoNLL 2017), respectively ( Table 3 ). The reason is that the work reported in<cite> Kang et al. (2018)</cite> relies on elementary handcrafted features extracted only from the paper; does not consider the review features whereas we include the review features along with the sentiment information in our deep neural architecture. However, we also find that our approach with only Review+Sentiment performs inferior to the Paper+Review method in<cite> Kang et al. (2018)</cite> for ACL 2017.",
  "y": "differences"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_13",
  "x": "Here we observe a relative error reduction of 4.8% and 14.5% over the comparing system for ACL 2017 and CoNLL 2016, respectively (Table 2 ). For the decision prediction task, the comparing system performs even worse, and we outperform them by a considerable margin of 28% (ACL 2017) and 26% (CoNLL 2017), respectively ( Table 3 ). The reason is that the work reported in<cite> Kang et al. (2018)</cite> relies on elementary handcrafted features extracted only from the paper; does not consider the review features whereas we include the review features along with the sentiment information in our deep neural architecture. However, we also find that our approach with only Review+Sentiment performs inferior to the Paper+Review method in<cite> Kang et al. (2018)</cite> for ACL 2017. This again seconds that inclusion of paper is vital in recommendation decisions. Only paper is enough for a human reviewer, but with the current state of AI, an AI reviewer would need the supervision of her human counterparts to arrive at a recommendation. So our system is suited to cases where the editor needs an additional judgment regarding a submission (such as dealing with missing/non-responding reviewers, an added layer of confidence with an AI which is aware of the past acceptances/rejections of a specific venue).",
  "y": "differences"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_0",
  "x": "Neural network models are an attractive alternative for this task for at least two reasons. First, they can model the argument of an implicit discourse relation as dense vectors and suffer less from the data sparsity problem that is typical of the traditional feature engineering paradigm. Second, they should be easily extended to other languages as they do not require human-annotated lexicons. However, despite the many nice properties of neural network models, it is not clear how well they will fare with a small dataset, typicalley found in discourse annotation projects. Moreover, it is not straightforward to construct a single vector that properly represents the \"semantics\" of the ar-guments. As a result, neural network models that use dense vectors have been shown to have inferior performance against traditional systems that use manually crafted features, unless the dense vectors are combined with the hand-crafted surface features <cite>(Ji and Eisenstein, 2015)</cite> . In this work, we explore multiple neural architectures in an attempt to find the best distributed representation and neural network architecture suitable for this task in both English and Chinese.",
  "y": "background"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_1",
  "x": "\u2022 We present the first neural CDTB-style Chinese discourse parser, confirming that our current results and other previous findings conducted on English data also hold crosslinguistically. ---------------------------------- **RELATED WORK** The prevailing approach for this task is to use surface features derived from various semantic lexicons (Pitler et al., 2009) , reducing the number of parameters by mapping raw word tokens in the arguments of discourse relations to a limited number of entries in a semantic lexicon such as polarity and verb classes. Along the same vein, Brown cluster assignments have also been used as a general purpose lexicon that requires no human manual annotation (Rutherford and Xue, 2014) . However, these solutions still suffer from the data sparsity problem and almost always require extensive feature selection to work well (Park and Cardie, 2012; Lin et al., 2009;<cite> Ji and Eisenstein, 2015)</cite> . The work we report here explores the use of the expressive power of distributed representations to overcome the data sparsity problem found in the traditional feature engineering paradigm.",
  "y": "background"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_2",
  "x": "For a fair comparison with the sequential model, we apply the same formulation of LSTM on the binarized constituent parse tree. The hidden state vector now corresponds to a constituent in the tree. These hidden state vectors are then used in the same fashion as the sequential LSTM. The mathematical formulation is the same as Tai et al. (2015) . This model is similar to the recursive neural networks proposed by<cite> Ji and Eisenstein (2015)</cite> . Our model differs from their model in several ways. We use the LSTM networks instead of the \"vanilla\" RNN formula and expect better results due to less complication with vanishing and exploding gradients during training.",
  "y": "similarities"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_4",
  "x": "---------------------------------- **CORPORA AND IMPLEMENTATION** The Penn Discourse Treebank (PDTB) We use the PDTB due to its theoretical simplicity in discourse analysis and its reasonably large size. The annotation is done as another layer on the Penn Treebank on Wall Street Journal sections. Each relation consists of two spans of text that are minimally required to infer the relation, and the sense is organized hierarchically. The classification problem can be formulated in various ways based on the hierarchy. Previous work in this task has been done over three schemes of evaluation: top-level 4-way classification (Pitler et al., 2009 ), second-level 11-way classification (Lin et al., 2009;<cite> Ji and Eisenstein, 2015)</cite> , and modified second-level classification introduced in the CoNLL 2015 Shared Task .",
  "y": "background"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_5",
  "x": "A tree LSTM model is trained on a GPU on Intel Xeon CPU E5-2660. All models converge within hours. ---------------------------------- **EXPERIMENT ON THE SECOND-LEVEL SENSE IN THE PDTB** We want to test the effectiveness of the interargument interaction and the three models described above on the fine-grained discourse relations in English. The data split and the label set are exactly the same as previous works that use this label set (Lin et al., 2009;<cite> Ji and Eisenstein, 2015)</cite> . Preprocessing All tokenization is taken from the gold standard tokenization in the PTB (Marcus et al., 1993) .",
  "y": "similarities uses"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_6",
  "x": "We use the Berkeley parser to parse all of the data (Petrov et al., 2006 too little data, 50-dimensional WSJ-trained word vectors have previously been shown to be the most effective in this task <cite>(Ji and Eisenstein, 2015)</cite> . Additionally, we also test the off-the-shelf word vectors trained on billions of tokens from Google News data freely available with the word2vec tool. All word vectors are trained on the Skipgram architecture (Mikolov et al., 2013b; Mikolov et al., 2013a) . Other models such as GloVe and continuous bag-of-words seem to yield broadly similar results (Pennington et al., 2014) . We keep the word vectors fixed, instead of fine-tuning during training. ---------------------------------- **RESULTS AND DISCUSSION**",
  "y": "background"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_7",
  "x": "We keep the word vectors fixed, instead of fine-tuning during training. ---------------------------------- **RESULTS AND DISCUSSION** The feedforward model performs best overall among all of the neural architectures we explore (Table 2) . It outperforms the recursive neural network with bilinear output layer introduced by<cite> Ji and Eisenstein (2015)</cite> (p < 0.05; bootstrap test) and performs comparably with the surface feature baseline (Lin et al., 2009) , which uses various lexical and syntactic features and extensive feature selection. Tree LSTM achieves inferior accuracy than our best feedforward model. The Figure 3: Inter-argument interaction can be modeled effectively with hidden layers.",
  "y": "differences"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_9",
  "x": "Why do sequential LSTM models outperform recursive neural networks or tree LSTM models? Although this first comes as a surprise to us, the results are consistent with recent works that use sequential LSTM to encode syntactic information. For example, Vinyals et al. (2015) use sequential LSTM to encode the features for syntactic parse output. Tree LSTM seems to show improvement when there is a need to model longdistance dependency in the data (Tai et al., 2015; Li et al., 2015) . Furthermore, the benefits of tree LSTM are not readily apparent for a model that discards the syntactic categories in the intermediate nodes and makes no distinction between heads and their dependents, which are at the core of syntactic representations. Another point of contrast between our work and<cite> Ji and Eisenstein's (2015)</cite> is the modeling choice for inter-argument interaction. Our experimental results show that the hidden layers are an important contributor to the performance for all of our models.",
  "y": "differences"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_10",
  "x": "We choose linear inter-argument interaction instead of bilinear interaction, and this decision gives us at least two advantages. Linear interaction allows us to stack up hidden layers without the exponential growth in the number of parameters. Secondly, using linear interaction allows us to use high dimensional word vectors, which we found to be another important component for the performance. The recursive model by<cite> Ji and Eisenstein (2015)</cite> is limited to 50 units due to the bilinear layer. Our choice of linear interargument interaction and high-dimensional word vectors turns out to be crucial to building a competitive neural network model for classifying implicit discourse relations. 6 Extending the results across label sets and languages Do our feedforward models perform well without surface features across different label sets and languages as well? We want to extend our results to another label set and language by evaluating our models on non-explicit discourse relation data used in English and Chinese CoNLL 2016 Shared Task.",
  "y": "differences"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_0",
  "x": "This poses many additional complexities to conventional VQA task, and we provide a baseline for approaching and evaluating the task, on top of which we invite the research community to build further improvements. ---------------------------------- **INTRODUCTION** The research community in artificial intelligence (AI) has witnessed a series of dramatic advances in the AI tasks concerning language and vision in recent years, thanks to the successful applications of deep learning techniques, particularly convolutional neural networks (CNN) and recurrent neural networks (RNN). AI has moved on from naming the entities in the image (Mei et al. 2008; Wang et al. 2009) , to describing the image with a natural sentence (Vinyals et al. 2015; Xu et al. 2015; Karpathy and Li 2015) and then to answering specific questions about the image with the advent of visual question answering (VQA) task <cite>(Antol et al. 2015)</cite> . However, current VQA task is focused on generating a short answer, mostly single words, which does not fully take advantage of the wide range of expressibility inherent in human natural language. Just as we moved from merely naming entities in the image to description of the images with natural sentence, it naturally follows that VQA will also move towards full-sentence answers.",
  "y": "background"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_1",
  "x": "A number of datasets on visual question answering have been introduced in recent years (Malinowski and Fritz 2014; Ren, Kiros, and Zemel 2015) , among which <cite>(Antol et al. 2015)</cite> in particular has gained the most attention and helped popularize the task. However, these datasets mostly consist of a small set of answers covering most of the questions, and most of the answers being single word. Our FSVQA dataset, derived from <cite>(Antol et al. 2015)</cite> , minimizes such limitation by converting the answers to full-sentences, thus widely expanding the set of answers. (Fukui et al. 2016) proposed multimodal compact bilinear pooling (MCB) to combine multimodal features of visual and text representations. This approach won the 1st place in 2016 VQA Challenge in real images category. (Saito et al. 2016) proposed DualNet, in which both addition and multiplication of the input features are performed, in order to fully take advantage of the discriminative features in the data. This method won the 1st place in 2016 VQA Challenge in abstract scenes category. ) was one of the first to propose attention model for VQA.",
  "y": "background"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_2",
  "x": "We also provide an augmented version of FSVQA by converting image captions to question and answers. We examine baseline approaches, and utilize complementary metrics for evaluation, providing a guideline upon which we invite the research community to build further improvements. Our primary contributions can be summarized as following: 1) introducing a novel task of full-sentence visual question answering, 2) building a large, publicly available dataset consisting of up to 1 million full-sentence Q&A pairs, and 3) examining baseline approaches along with a novel combination of evaluation metrics. ---------------------------------- **RELATED WORK** A number of datasets on visual question answering have been introduced in recent years (Malinowski and Fritz 2014; Ren, Kiros, and Zemel 2015) , among which <cite>(Antol et al. 2015)</cite> in particular has gained the most attention and helped popularize the task. However, these datasets mostly consist of a small set of answers covering most of the questions, and most of the answers being single word.",
  "y": "motivation background"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_3",
  "x": "**RELATED WORK** A number of datasets on visual question answering have been introduced in recent years (Malinowski and Fritz 2014; Ren, Kiros, and Zemel 2015) , among which <cite>(Antol et al. 2015)</cite> in particular has gained the most attention and helped popularize the task. However, these datasets mostly consist of a small set of answers covering most of the questions, and most of the answers being single word. Our FSVQA dataset, derived from <cite>(Antol et al. 2015)</cite> , minimizes such limitation by converting the answers to full-sentences, thus widely expanding the set of answers. (Fukui et al. 2016) proposed multimodal compact bilinear pooling (MCB) to combine multimodal features of visual and text representations. This approach won the 1st place in 2016 VQA Challenge in real images category. (Saito et al. 2016) proposed DualNet, in which both addition and multiplication of the input features are performed, in order to fully take advantage of the discriminative features in the data.",
  "y": "extends"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_4",
  "x": "Most of the works above limited the number of possible answers, which was possible due to a small number of answers covering the majority of the dataset. Our FSVQA dataset imposes additional complexity to existing approaches by having a much larger set of possible answers, in which no small set of labels can cover the majority of the dataset. ---------------------------------- **DATASET** Collecting full-sentence annotations from crowd-sourcing tools can be highly costly. We circumvent this financial cost by converting the answers in the original VQA dataset <cite>(Antol et al. 2015)</cite> to full-sentence answers by applying a number of linguistic rules using natural language processing techniques. Furthermore, we also provide an augmented version of dataset by converting the human-written captions provided in the MS COCO (Lin et al. 2014) .",
  "y": "extends"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_5",
  "x": "Words in the question were input to LSTM (Hochreiter and Schmidhuber 1997) one at a time as one-hot vector, where the dictionary contains only the words appearing more than once. Image features and question features are then mapped to common embedding space as a 1,024-dimensional vector. Batch size was 500 and training was performed for 300 epochs. We trained only with the answers that appear twice or more in train split, as using all unique answers in the dataset fails to run, with required memory far beyond the capacity of most of the contemporary GPUs, NVIDIA Tesla 40m in our case. 20,130 answers appear more than once in regular version, covering 95,340 questions from 62,292 images, and 23,400 answers appear more than once in the augmented version, which cover 105,563 questions from 64,060 images. This is only about 25% and 15.5% of the entire train split in respective version, which again shows a striking contrast with the original VQA dataset, in which only 1,000 answers covered up to 86.5% of the dataset. Following <cite>(Antol et al. 2015)</cite> , we examined the effect of Conversely, we also examined an approach where only image features are concerned.",
  "y": "uses"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_6",
  "x": "Likewise, answers generated from image features alone fit the images but are frequently out of textual context given by the question. It is notable that using image features alone performs very poorly, whereas using question features alone results in performances comparable to using both features. One plausible explanation is that, since using image features alone always generates the same answer for the same image regardless of the question, it can only get 1 out of k questions correctly at best, where k is the number of questions per image. On the contrary, using question features alone essentially reduces the problem to a semantic Q&A task, which can be handled one at a time. This tendency is consistent with the results reported in <cite>(Antol et al. 2015)</cite> . It must nevertheless be reminded that the best performances in both <cite>(Antol et al. 2015)</cite> and our experiment were achieved with the presence of both visual and textual clues. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_7",
  "x": "Answers from question features alone result in answers that match the questions but are frequently out of visual context given by the image. Likewise, answers generated from image features alone fit the images but are frequently out of textual context given by the question. It is notable that using image features alone performs very poorly, whereas using question features alone results in performances comparable to using both features. One plausible explanation is that, since using image features alone always generates the same answer for the same image regardless of the question, it can only get 1 out of k questions correctly at best, where k is the number of questions per image. On the contrary, using question features alone essentially reduces the problem to a semantic Q&A task, which can be handled one at a time. This tendency is consistent with the results reported in <cite>(Antol et al. 2015)</cite> . It must nevertheless be reminded that the best performances in both <cite>(Antol et al. 2015)</cite> and our experiment were achieved with the presence of both visual and textual clues.",
  "y": "similarities"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_0",
  "x": "****IMPROVING ROBUSTNESS OF MACHINE TRANSLATION WITH SYNTHETIC NOISE**** **ABSTRACT** Modern Machine Translation (MT) systems perform consistently well on clean, in-domain text. However human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of output translation. In this paper we leverage the Machine Translation of Noisy Text (MTNT) dataset<cite> (Michel and Neubig, 2018)</cite> to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system resilient to naturally occurring noise and partially mitigate loss in accuracy resulting therefrom. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_1",
  "x": "Machine Translation (MT) systems have been shown to exhibit severely degraded performance when presented with translation of out-of-domain or noisy data (Luong and Manning, 2015; Sakaguchi et al., 2016; Belinkov and Bisk, 2017) . This is particularly pronounced in systems trained on clean, formalized parallel data such as Europarl (Koehn, 2005) , are tasked with translation of unedited, human generated text such as is common in domains such as social media, where accurate translation is becoming of widespread relevance<cite> (Michel and Neubig, 2018)</cite> . Improving the robustness of MT systems to naturally occurring noise presents an important and interesting task. Recent work on MT robustness (Belinkov and Bisk, 2017) has further demonstrated the need to build or adapt systems that are resilient to such noise. We approach the problem of adapting to noisy data through two primary research questions: * These authors contributed equally 1. Can we artificially synthesize the types of noise common to social media text in otherwise clean data? 2. Are we able to improve the performance of vanilla MT systems on noisy data by leveraging artificially generated noise?",
  "y": "background"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_2",
  "x": "This is particularly pronounced in systems trained on clean, formalized parallel data such as Europarl (Koehn, 2005) , are tasked with translation of unedited, human generated text such as is common in domains such as social media, where accurate translation is becoming of widespread relevance<cite> (Michel and Neubig, 2018)</cite> . Improving the robustness of MT systems to naturally occurring noise presents an important and interesting task. Recent work on MT robustness (Belinkov and Bisk, 2017) has further demonstrated the need to build or adapt systems that are resilient to such noise. We approach the problem of adapting to noisy data through two primary research questions: * These authors contributed equally 1. Can we artificially synthesize the types of noise common to social media text in otherwise clean data? 2. Are we able to improve the performance of vanilla MT systems on noisy data by leveraging artificially generated noise? In this work we present two primary methods of synthesizing natural noise in accordance with the types of noise identified in prior work (Eisenstein, 2013;<cite> Michel and Neubig, 2018)</cite> as naturally occurring in internet and social media based text.",
  "y": "background"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_3",
  "x": "Recent work on MT robustness (Belinkov and Bisk, 2017) has further demonstrated the need to build or adapt systems that are resilient to such noise. We approach the problem of adapting to noisy data through two primary research questions: * These authors contributed equally 1. Can we artificially synthesize the types of noise common to social media text in otherwise clean data? 2. Are we able to improve the performance of vanilla MT systems on noisy data by leveraging artificially generated noise? In this work we present two primary methods of synthesizing natural noise in accordance with the types of noise identified in prior work (Eisenstein, 2013;<cite> Michel and Neubig, 2018)</cite> as naturally occurring in internet and social media based text. We present a series of experiments based on the Machine Translation of Noisy Text (MTNT) data set <cite>(Michel and Neubig, 2018</cite> ) through which we demonstrate improved resilience of a vanilla MT system by adaptation using artificially noised data. The primary contributions of this work are our Synthetic Noise Induction model which specifically introduces types of noise unique to social media text and the introduction of back translation (Sennrich et al., 2015a) as a means of emulating target noise.",
  "y": "uses"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_4",
  "x": "We present a series of experiments based on the Machine Translation of Noisy Text (MTNT) data set <cite>(Michel and Neubig, 2018</cite> ) through which we demonstrate improved resilience of a vanilla MT system by adaptation using artificially noised data. The primary contributions of this work are our Synthetic Noise Induction model which specifically introduces types of noise unique to social media text and the introduction of back translation (Sennrich et al., 2015a) as a means of emulating target noise. Szegedy et al. (2013) demonstrate the fragility of neural networks to noisy input. This fragility has been shown to extend to MT systems (Belinkov and Bisk, 2017; Khayrallah and Koehn, 2018) where both artificial and natural noise are shown to negatively affect performance. ---------------------------------- **RELATED WORK** Human generated text on the internet and social media are a particularly rich source of natural noise (Eisenstein, 2013; Baldwin et al., 2015) which causes pronounced problems for MT<cite> (Michel and Neubig, 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_5",
  "x": "We also employ weighttying (Press and Wolf, 2016) between the embedding layer and projection layer of the decoder. For expediency and convenience of experimentation we have chosen to deploy a smaller, faster variant of the model used in<cite> Michel and Neubig (2018)</cite> , which allows us to provide comparative results across a variety of settings. Other model parameters reflect the implementation outlined in<cite> Michel and Neubig (2018)</cite> . In all experimental settings we employ Byte-Pair Encoding (BPE) (Sennrich et al., 2015b) using Google's SentencePiece 2 . ---------------------------------- **EXPERIMENTAL APPROACHES** We propose two primary approaches to increasing the resilience of our baseline model to the MTNT data, outlined as follows:",
  "y": "extends differences"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_6",
  "x": "We also employ weighttying (Press and Wolf, 2016) between the embedding layer and projection layer of the decoder. For expediency and convenience of experimentation we have chosen to deploy a smaller, faster variant of the model used in<cite> Michel and Neubig (2018)</cite> , which allows us to provide comparative results across a variety of settings. Other model parameters reflect the implementation outlined in<cite> Michel and Neubig (2018)</cite> . In all experimental settings we employ Byte-Pair Encoding (BPE) (Sennrich et al., 2015b) using Google's SentencePiece 2 . ---------------------------------- **EXPERIMENTAL APPROACHES** We propose two primary approaches to increasing the resilience of our baseline model to the MTNT data, outlined as follows:",
  "y": "similarities"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_7",
  "x": "---------------------------------- **SYNTHETIC NOISE INDUCTION (SNI)** For this method, we inject artificial noise in the clean data according to the distribution of types of noise in MTNT specified in<cite> Michel and Neubig (2018)</cite> . For every token we choose to introduce the different types of noise with some probability on both French and English sides in 100k sentences of EP. Specifically, we fix the probabilities of error types as follows: spelling (0.04), profanity (0.007), grammar (0.015) and emoticons (0.002). To simulate spelling error, we randomly add or drop a character in a given word. For grammar error and profanity, we randomly select and insert a stop word or an expletive and its translation on either side.",
  "y": "similarities uses"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_0",
  "x": "---------------------------------- **INTRODUCTION** Qualia Structures have been originally introduced by <cite>(Pustejovsky, 1991)</cite> and are used for a variety of purposes in Natural Language processing such as the analysis of compounds (Johnston and Busa, 1996) , co-composition and coercion <cite>(Pustejovsky, 1991)</cite> as well as for bridging reference resolution (Bos et al., 1995) . Further, it has also been argued that qualia structures and lexical semantic relations in general have applications in information retrieval (Voorhees, 1994; Pustejovsky et al., 1993) . One major bottleneck however is that currently Qualia Structures need to be created by hand, which is probably also the reason why there are no practical system using qualia structures, but a lot of systems using globally available resources such as WordNet (Fellbaum, 1998) or FrameNet 1 1 http://framenet.icsi.berkeley.edu/ as source of lexical/world knowledge. The work described in this paper addresses this issue and presents an approach to automatically learning qualia structures for nominals from the Web. The approach is inspired in recent work on using the Web to identify instances of a relation of interest such as in (Markert et al., 2003) and (Cimiano and Staab, 2004) .",
  "y": "background"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_1",
  "x": "Section 3 describes our approach in detail and section 4 presents a quantitative and qualitative evaluation of our approach. Before concluding, we discuss some related work in Section 5. ---------------------------------- **QUALIA STRUCTURES** According to Aristotle, there are four basic factors or causes by which the nature of an object can be described (cf. (Kronlid, 2003) ): the material cause, i.e. the material an object is made of the agentive cause, i.e. the source of movement, creation or change the formal cause, i.e. its form or type the final cause, i.e. its purpose, intention or aim In his Generative Lexicon (GL) framework <cite>(Pustejovsky, 1991)</cite> reused Aristotle's basic factors for the description of the meaning of lexical elements. In fact he introduced so called Qualia Structures by which the meaning of a lexical element is described in terms of four roles: Constitutive: describing physical properties of an object, i.e. its weight, material as well as parts and components",
  "y": "background"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_2",
  "x": "---------------------------------- **QUALIA STRUCTURES** According to Aristotle, there are four basic factors or causes by which the nature of an object can be described (cf. (Kronlid, 2003) ): the material cause, i.e. the material an object is made of the agentive cause, i.e. the source of movement, creation or change the formal cause, i.e. its form or type the final cause, i.e. its purpose, intention or aim In his Generative Lexicon (GL) framework <cite>(Pustejovsky, 1991)</cite> reused Aristotle's basic factors for the description of the meaning of lexical elements. In fact he introduced so called Qualia Structures by which the meaning of a lexical element is described in terms of four roles: Constitutive: describing physical properties of an object, i.e. its weight, material as well as parts and components Agentive: describing factors involved in the bringing about of an object, i.e. its creator or the causal chain leading to its creation Formal: describing that properties which distinguish an object in a larger domain, i.e. orientation, magnitude, shape and dimensionality Telic: describing the purpose or function of an object Most of the qualia structures used in <cite>(Pustejovsky, 1991)</cite> however seem to have a more restricted interpretation. In fact, in most examples the Constitutive role seems to describe the parts or components of an object, while the Agentive role is typically described by a verb denoting an action which typically brings the object in question into existence.",
  "y": "background"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_3",
  "x": "In what follows we describe in detail the procedure for acquiring qualia elements for each qualia role. In particular, we describe in detail the clues and lexico-syntactic patterns used. In general, the patterns have been crafted by hand, testing and refining them in an iterative process, paying attention to maximize their coverage but also accuracy. In general it is important to mention that by this approach we are not able to detect and separate multiple meanings of words, i.e. to handle polysemy, which is appropriately accounted for in the framework of the Generative Lexicon <cite>(Pustejovsky, 1991)</cite> . ---------------------------------- **THE FORMAL ROLE** To derive qualia elements for the Formal role, we first download for each of the clues in Table 1 the first 10 abstracts matching the clue and then process them offline matching the patterns defined over part-of-speech-tags 5 thus yielding up to 10 different qualia element candidates per clue.",
  "y": "extends differences"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_4",
  "x": "for the nominal we want to acquire a qualia structure for as well as the following verbs: build, produce, make, write, plant, elect, create, cook, construct and design. If this value is over a threshold (0.0005 in our case), we assume that it is a valid filler of the Agentive qualia role. Busa, 1996) or <cite>(Pustejovsky, 1991)</cite> , as well as computer, an abstract noun, i.e. conversation, as well as two very specific multi-term words, i.e. natural language processing and data mining. We give the automatically learned weighted Qualia Structures for these entries in Figures 3,  4 , 5 and 6. The evaluation of our approach consists on the one hand of a discussion of the weighted qualia structures, in particular comparing them to the ideal structures form the literature. On the other hand, we also asked a student at our institute to assign credits to each of the qualia elements from 0 (incorrect) to 3 (totally correct) whereby 1 credit meaning 'not totally wrong' and 2 meaning 'still acceptable'. ----------------------------------",
  "y": "background"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_5",
  "x": "For book, the first four candidates of the Formal role, i.e. product, item, publication and document are very appropriate, but alluding to the physical object meaning of book as opposed to the meaning in the sense of information container (compare <cite>(Pustejovsky, 1991)</cite> . As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to <cite>(Pustejovsky, 1991)</cite> . For the Constitutive role of book we get -besides it at the first position which could be easily filtered out -sign (2nd position), letter (3rd position) and page (6th position), which are quite appropriate. The top four candidates for the Telic role are give, select, read and purchase. It seems that give is emphasizing the role of a book as a gift, read is referring to the most obvious purpose of a book as specified in the ideal qualia structures of <cite>(Pustejovsky, 1991)</cite> as well as (Johnston and Busa, 1996) and purchase denotes the more general purpose of a book, i.e. to be bought. The first element of the Formal role of knife unfortunately denotes the material it is typically made of, i.e. steel, but the next 5 elements are definitely appropriate: weapon, item, kitchenware, object and instrument. The ideal element artifact tool (compare (Johnston and Busa, 1996) ) can be found at the 10th position.",
  "y": "extends differences"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_6",
  "x": "As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to <cite>(Pustejovsky, 1991)</cite> . For the Constitutive role of book we get -besides it at the first position which could be easily filtered out -sign (2nd position), letter (3rd position) and page (6th position), which are quite appropriate. The top four candidates for the Telic role are give, select, read and purchase. It seems that give is emphasizing the role of a book as a gift, read is referring to the most obvious purpose of a book as specified in the ideal qualia structures of <cite>(Pustejovsky, 1991)</cite> as well as (Johnston and Busa, 1996) and purchase denotes the more general purpose of a book, i.e. to be bought. The first element of the Formal role of knife unfortunately denotes the material it is typically made of, i.e. steel, but the next 5 elements are definitely appropriate: weapon, item, kitchenware, object and instrument. The ideal element artifact tool (compare (Johnston and Busa, 1996) ) can be found at the 10th position. The results are interesting in that on the one hand the most prominent meaning of knife according to the web is the one of a weapon.",
  "y": "similarities uses"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_7",
  "x": "For the Constitutive role of book we get -besides it at the first position which could be easily filtered out -sign (2nd position), letter (3rd position) and page (6th position), which are quite appropriate. The top four candidates for the Telic role are give, select, read and purchase. It seems that give is emphasizing the role of a book as a gift, read is referring to the most obvious purpose of a book as specified in the ideal qualia structures of <cite>(Pustejovsky, 1991)</cite> as well as (Johnston and Busa, 1996) and purchase denotes the more general purpose of a book, i.e. to be bought. The first element of the Formal role of knife unfortunately denotes the material it is typically made of, i.e. steel, but the next 5 elements are definitely appropriate: weapon, item, kitchenware, object and instrument. The ideal element artifact tool (compare (Johnston and Busa, 1996) ) can be found at the 10th position. The results are interesting in that on the one hand the most prominent meaning of knife according to the web is the one of a weapon. On the other hand our results are more specific, classifying a knife as kitchenware instead of merely as an artifact tool.",
  "y": "motivation"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_8",
  "x": "Considering the qualia structure for beer, it is surprising that no purpose has been found. The reason is that currently no results are returned by Google for the clue a beer is used to and the four snippets returned for the purpose of a beer contain expressions of the form the purpose of a beer is to drink it which is not matched by our patterns as it is a pronoun and not matched by our NP pattern (unless it is matched by an error as in the Qualia Structure for book in Figure 4 ). Considering the results for the Formal role, the elements drink (1st), alcohol (2nd) and beverage (4th) are much more specific than liquid as given in <cite>(Pustejovsky, 1991)</cite> , while thing at the 3rd position is certainly too general. Furthermore, according to the automatically learned qualia structure, beer is made of rice, malt and hop, which are perfectly reasonable results. Very interesting are the results concoction and libation for the Formal role of beer, which unfortunately were rated low by our evaluator (compare Figure 3) . Overall, the discussion has shown that the results produced by our method are reasonable when compared to the qualia structures from the literature. In general, our method produces in some cases additional qualia candidates, such as the ones describing the material a knife is typically made of.",
  "y": "motivation"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_0",
  "x": "---------------------------------- **INTRODUCTION** Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) is rapidly proving itself to be a strong competitor to other statistical machine translation methods. However, it still lags behind other statistical methods on very lowresource language pairs<cite> (Zoph et al., 2016</cite>; Koehn and Knowles, 2017) . A common strategy to improve learning of lowresource languages is to use resources from related languages (Nakov and Ng, 2009) . However, adapting these resources is not trivial. NMT offers some simple ways of doing this.",
  "y": "motivation"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_1",
  "x": "In this paper, we explore the opposite scenario, where the parent language pair is also lowresource, but related to the child language pair. We show that, at least in the case of three Turkic languages (Turkish, Uzbek, and Uyghur), the original method of <cite>Zoph et al. (2016)</cite> does not always work, but it is still possible to use the parent model to considerably improve the child model. The basic idea is to exploit the relationship between the parent and child language lexicons. Zoph et al.'s original method makes no assumption about the relatedness of the parent and child languages, so it effectively makes a random assignment of the parent-language word embeddings to child-language words. But if we assume that the parent and child lexicons are related, it should be beneficial to transfer source word embeddings from parent-language words to their child-language equivalents. Thus, the problem amounts to finding a representation of the data that ensures a sufficient overlap between the vocabularies of the languages. To do this, we map the source languages to a common alphabet and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) on the union of the vocabularies to increase the number of common subwords. In our experiments, we show that transfer learning helps word-based translation, but not always significantly. But when used on top of a much stronger BPE baseline, it yields larger and statistically significant improvements.",
  "y": "uses motivation"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_2",
  "x": "**INTRODUCTION** Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) is rapidly proving itself to be a strong competitor to other statistical machine translation methods. However, it still lags behind other statistical methods on very lowresource language pairs<cite> (Zoph et al., 2016</cite>; Koehn and Knowles, 2017) . A common strategy to improve learning of lowresource languages is to use resources from related languages (Nakov and Ng, 2009) . However, adapting these resources is not trivial. NMT offers some simple ways of doing this. For example, <cite>Zoph et al. (2016)</cite> train a parent model on a (possibly unrelated) high-resource language pair, then use this model to initialize a child model which is further trained on a low-resource language pair.",
  "y": "background"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_3",
  "x": "---------------------------------- **LANGUAGE TRANSFER** We follow the transfer learning approach proposed by <cite>Zoph et al. (2016)</cite> . In their work, a parent model is first trained on a high-resource language pair. Then the child model's parameter values are copied from the parent's and are fine-tuned on its low-resource data. The source word embeddings are copied with the rest of the model, with the ith parent-language word embedding being assigned to the ith childlanguage word. Because the parent and child source languages have different vocabularies, this amounts to randomly assigning parent source word embeddings to child source words.",
  "y": "uses"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_4",
  "x": "**METHOD** The basic idea of our method is to extend the transfer method of <cite>Zoph et al. (2016)</cite> to share the parent and child's source vocabularies, so that when source word embeddings are transferred, a word that appears in both vocabularies keeps its embedding. In order for this to work, it must be the case that the parent and child languages have considerable vocabulary overlap, and that when a word occurs in both languages, it often has a similar meaning in both languages. Thus, we need to process the data to make these two assumptions hold as much as possible. ---------------------------------- **TRANSLITERATION** If the parent and child language have different orthographies, it should help to map them into a common orthography.",
  "y": "extends uses"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_5",
  "x": "After translation at test time, we rejoined BPE segments, recased, and detokenized. Finally, we evaluated using case-sensitive BLEU. As a baseline, we trained a child model using BPE but without transfer (that is, with weights randomly initialized). We also compared against a word-based baseline (without transfer) and two word-based systems using transfer without vocabulary-sharing, corresponding with the method of <cite>Zoph et al. (2016)</cite> ( \u00a72.2): one where the target word embeddings are fine-tuned, and one where they are frozen. ---------------------------------- **RESULTS AND ANALYSIS** Our results are shown in Table 3 .",
  "y": "uses"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_6",
  "x": "Whereas word-based transfer methods help very little for Turkish-English, and help or hurt slightly for Uyghur-English, our BPE-based transfer approach consistently improves over both the baseline and transfer word-based models. We surmise that the improvement is primarily due to the vocabulary overlap created by BPE (see Table 4 ). ---------------------------------- **CONCLUSION** In this paper, we have shown that the transfer learning method of <cite>Zoph et al. (2016)</cite> , while appealing, might not always work in a low-resource context. However, by combining it with BPE, we can improve NMT performance on a low-resource language pair by exploiting its lexical similarity with another related, low-resource language. Our results show consistent improvement in two Turkic languages.",
  "y": "uses"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_7",
  "x": "Whereas word-based transfer methods help very little for Turkish-English, and help or hurt slightly for Uyghur-English, our BPE-based transfer approach consistently improves over both the baseline and transfer word-based models. We surmise that the improvement is primarily due to the vocabulary overlap created by BPE (see Table 4 ). ---------------------------------- **CONCLUSION** In this paper, we have shown that the transfer learning method of <cite>Zoph et al. (2016)</cite> , while appealing, might not always work in a low-resource context. However, by combining it with BPE, we can improve NMT performance on a low-resource language pair by exploiting its lexical similarity with another related, low-resource language. Our results show consistent improvement in two Turkic languages.",
  "y": "extends"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_0",
  "x": "Recent Image Captioning models of this kind [1, 11, 12, 28] have shown impressive results, much thanks to the powerful language modelling capabilities of Long Short-Term Memory (LSTM) [15] RNNs. However, although MLE training enables models to confidently generate captions that have a high likelihood in the training set, it limits their capacity to generate novel descriptions. Their output exhibits a disproportionate replication of common n-grams and full captions seen in the training set [9, 11,<cite> 26]</cite> . Contributing to this problem is a combination of biased datasets and insufficient quality metrics. While the main benchmarking dataset for Image Captioning, MS COCO, makes available over 120k images with 5 human-annotated captions each [6] , the selection process for the images suggests a lack of diversity in both content and composition [11, 20] . Furthermore, the standard benchmarking metrics, based on ngram level overlap between generated captions and ground-truth captions, reward models with a bias towards common n-grams. This leads to the (indirect and unwanted) consequence of incentivizing models that output generic captions that are likely to fit a range of similar images, despite missing the goal of describing the relevant aspects specific to each image.",
  "y": "background"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_1",
  "x": "We review related work in Section 6 before presenting our conclusions and suggestions for future work in Section 7. ---------------------------------- **MEASURING CAPTION QUALITY** The subjectivity in what defines a good caption, has made it difficult to identify a single metric for the overall quality of Image Captioning models [5,<cite> 26]</cite> . Benchmarking methods from Machine Translation [3, 19, 23] have been appropriated, while other somewhat similar methods such as CIDEr [27] have been proposed specifically for assessing the quality of image captions. All these approaches unfortunately have a strong focus on replicating common n-grams from the ground-truth captions [5] and do not take into account the richness and diversity of human expression [9,<cite> 26]</cite> . Moreover, it has been found that this class of metrics suffers from poor correlations with human evaluation, with CIDEr and METEOR having the highest correlations among them [5] .",
  "y": "background"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_2",
  "x": "Benchmarking methods from Machine Translation [3, 19, 23] have been appropriated, while other somewhat similar methods such as CIDEr [27] have been proposed specifically for assessing the quality of image captions. All these approaches unfortunately have a strong focus on replicating common n-grams from the ground-truth captions [5] and do not take into account the richness and diversity of human expression [9,<cite> 26]</cite> . Moreover, it has been found that this class of metrics suffers from poor correlations with human evaluation, with CIDEr and METEOR having the highest correlations among them [5] . With the recognition of these limitations, there has been a growing interest in developing metrics that measure other desirable qualities in captions. SPICE [2] is a recent addition which measures the overlap of content by comparing automatically generated scene-graphs from the ground-truth and generated captions. While being a relevant addition, it does not solve the problem of generic captions. Rare occurrences and more detailed descriptions are more likely to incur a penalty than common concepts; e.g. correctly specifying a purple flower where the ground-truth text omits its color would register a false positive for the color.",
  "y": "background"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_3",
  "x": "This, again, encourages the ''safe'' generic captions that we want to move away from. ---------------------------------- **DIVERSITY METRICS** In an effort to measure the amount of generic captions produced by various Image Captioning models, [11] explores the concept of caption diversity. More recently, this concept has been employed as the focus for training and evaluation<cite> [26,</cite> 29] , and it has been proposed that improving caption diversity leads to more human-like captions <cite>[26]</cite> . This research direction is still new and lacks clear benchmarks and standardized metrics. We propose the following set of metrics to evaluate the diversity of a model:",
  "y": "background"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_4",
  "x": "We propose the following set of metrics to evaluate the diversity of a model: \u2500 novelty -percentage of generated captions where exact duplicates are not found in the training set [11,<cite> 26,</cite> 29 ] \u2500 diversity -percentage of distinct captions (where duplicates count as a single distinct caption) out of the total number of generated captions [11] \u2500 vocabulary size -number of unique words used in generated captions <cite>[26]</cite> ---------------------------------- **MEANINGFUL DIVERSITY THROUGH SPECIFICITY** The diversity metrics alone do not tell us if a diverse model is more meaningful or if it simply introduced more noise. We argue that improving the specificity of the captions is essential to producing a meaningful increase in diversity. Our hypothesis is that by directly increasing the specificity, we will also achieve a higher diversity since diversity is a necessity for specificity.",
  "y": "uses"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_5",
  "x": "In Image Retrieval tasks, a given query must be specific enough to retrieve the correct image among other, possibly similar, images. In this paper, we investigate whether the error signal from an Image Retrieval model can improve caption specificity in an Image Captioning model, and whether these more specific captions are also more diverse. The training process is inspired by [22] where the task is to generate Referring Expressions that unambiguously refer to a region of an image; their solution is to introduce a Region Discriminator that measures the quality of their generated expressions. Their method is in turn inspired by Generative Adversarial Networks (GANs) in which a Generator and a Discriminator are in constant competition -the Discriminator aims to distinguish between real and generated data, while the Generator aims to generate data that the Discriminator cannot tell apart from the real data [13] . In [22] , the training is cooperative rather than competitive; both systems adjust to the other to provide the best joint results. We take a slightly different approach from both the joint training in [22] and recent applications of GAN training in Image Captioning [9,<cite> 26]</cite> . Instead of allowing both systems to learn from each other, we freeze the NLU side and allow only the NLG to learn from the NLU; the NLU model is pre-trained on ground-truth captions, without any input from the NLG.",
  "y": "differences"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_7",
  "x": "The results for specificity would not be directly comparable to models using other external systems, but they are relevant when assessing our own models and verifying that our increase in diversity follows from an increase in specificity. Results from our contrastive models are averaged over 3 runs each. The non-contrastive models are based on single runs. As can be seen in Table 1 , our models demonstrate increased diversity and novelty, outperforming previously reported results. The vocabulary size also increases but is lower than in <cite>[26]</cite> . When it comes to the specificity metrics, our contrastive models have the advantage over our non-contrastive ones. They all improve the overall mean rank, but the latter do not show the increase in smaller k recalls that the contrastive models do.",
  "y": "differences"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_8",
  "x": "Diversity metrics for multi-candidate models Diversity within candidates Novelty within candidates CVAE [29] 11.8 82.0 GMM-CVAE [29] 59.4 80.9 AG-CVAE [29] 76.4 79.5 For completeness, we include the best models from [29] in Table 2 ; however, they only report diversity results on multiple (up to 10) candidates per image (where duplicates of a novel caption are counted as multiple novel captions), so they are not directly comparable to the single-best-caption models. Note that [12, 29] use different data splits, while our models and <cite>[26]</cite> use the Karpathy 5k splits [17] . Table 3 . Standard text metric results for single-best-caption models. All metrics are n-gram based except for SPICE which is based on scene graphs automatically inferred from the captions. [29] 0.698 0.521 0.372 0.265 0.506 0.225 0.834 0.158 GMM-CVAE [29] 0.718 0.538 0.388 0.278 0.516 0.238 0.932 0.170 AG-CVAE [29] 0 In Table 3 , we report results on the standard text metrics.",
  "y": "similarities uses"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_10",
  "x": "baseline: a man riding a snowboard down a snow covered slope DP: a snowboarder doing a trick in the air Cos: a snowboarder doing a trick in the air CDP: a snowboarder is jumping in the air on a snowboard CCos: a snowboarder is jumping in the air on a snowboard HUMAN CAPTIONS a picture of a man in the air on a snowboard a man doing tricks on a snowboard a man riding a snowboard through the air on a ski slope a snowboarder flies into the air under a chair lift a snowboarder does a trick while jumping through the air GENERATED CAPTIONS baseline: a man walking on the beach with a surfboard DP: a person walking on the beach with a surfboard Cos: a person walking on a beach with a surfboard CDP: a person walking on a beach with a dog CCos: a man walking on the beach with a dog HUMAN CAPTIONS a person walking their dog on the beach a man on a beach holding something while walking along it a single person walking the beach with a dog a person walking their dog on the beach a person walking their dog along the shoreline GENERATED CAPTIONS baseline: a man is doing a trick on a skateboard DP: a man doing a trick on a skateboard Cos: a man doing a trick on a skateboard CDP: a man is doing a trick on a wooden structure CCos: a man is doing a trick on a wooden structure HUMAN CAPTIONS a man on a skateboard performing a trick a man flying through the air on top of a skateboard a person on a skateboard in the air at a skate park a male skateboarder skateboards on a wall in an enclosed area a male on a skateboard performing a trick on a halfpipe Another example of GAN training is <cite>[26]</cite> where the Discriminator classifies whether a multi-sample set of captions are human-written or generated. In contrast, our evaluator only requires a single caption and uses a much simpler loss function. Furthermore, we let the NLU remain frozen during training, making the training stable and producing more informative learning curves. A similar approach can be found in [10] where Contrastive Learning is used in a GAN-like setting. In contrast to our approach which is unsupervised after pre-training, theirs require image-caption pairs both during and after pre-training. Similar to our work, they are motivated by a specificity goal; unfortunately, they do not report results on any diversity metrics.",
  "y": "differences"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_0",
  "x": "Thus, developing tools to identify gang member profiles on social media is an important step in the direction of using machine intelligence to fight crime. To help agencies monitor gang activity on social media, our past work investigated how features from Twitter profiles, including profile text, profile images, tweet text, emjoi use, and their links to YouTube, may be used to reliably find gang member profiles <cite>[BWDS16]</cite> . The diverse set of features, chosen to combat the fact that gang members often use local terms and hashtags in their posts, offered encouraging results. In this paper, we report our experience in integrating deep learning into our gang member profile classifier. Specifically, we investigate the effect of translating the features into a vector space using word embeddings [MSC + 13] . This idea is motivated by the recent success of word embeddings-based methods to learn syntactic and semantic structures automatically when provided with large datasets. A dataset of over 3,000 gang and non-gang member profiles that we previously curated is used to train the word embeddings.",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_1",
  "x": "Thus, developing tools to identify gang member profiles on social media is an important step in the direction of using machine intelligence to fight crime. To help agencies monitor gang activity on social media, our past work investigated how features from Twitter profiles, including profile text, profile images, tweet text, emjoi use, and their links to YouTube, may be used to reliably find gang member profiles <cite>[BWDS16]</cite> . The diverse set of features, chosen to combat the fact that gang members often use local terms and hashtags in their posts, offered encouraging results. In this paper, we report our experience in integrating deep learning into our gang member profile classifier. Specifically, we investigate the effect of translating the features into a vector space using word embeddings [MSC + 13] . This idea is motivated by the recent success of word embeddings-based methods to learn syntactic and semantic structures automatically when provided with large datasets. A dataset of over 3,000 gang and non-gang member profiles that we previously curated is used to train the word embeddings.",
  "y": "extends"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_2",
  "x": "These studies investigated a small set of manually curated gang member profiles, often from a small geographic area that may bias their findings. In our previous work <cite>[BWDS16]</cite> , we curated what may be the largest set of gang member profiles to study how gang member Twitter profiles can be automatically identified based on the content they share online. A data collection process involving location neutral keywords used by gang members, with an expanded search of their retweet, friends and follower networks, led to identifying 400 authentic gang member profiles on Twitter. Our study discovered that the text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to YouTube music videos, can help a classifier distinguish between gang and non-gang member profiles. While a very promising F 1 measure with low false positive rate was achieved, we hypothesize that the diverse kinds and the multitude of features employed (e.g. unigrams of tweet text) could be amenable to an improved representation for classification. We thus explore the possibility of mapping these features into a considerably smaller feature space through the use of word embeddings. Previous",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_3",
  "x": "In our previous work <cite>[BWDS16]</cite> , we curated what may be the largest set of gang member profiles to study how gang member Twitter profiles can be automatically identified based on the content they share online. A data collection process involving location neutral keywords used by gang members, with an expanded search of their retweet, friends and follower networks, led to identifying 400 authentic gang member profiles on Twitter. Our study discovered that the text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to YouTube music videos, can help a classifier distinguish between gang and non-gang member profiles. While a very promising F 1 measure with low false positive rate was achieved, we hypothesize that the diverse kinds and the multitude of features employed (e.g. unigrams of tweet text) could be amenable to an improved representation for classification. We thus explore the possibility of mapping these features into a considerably smaller feature space through the use of word embeddings. Previous ----------------------------------",
  "y": "extends"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_4",
  "x": "Gang member tweets and profile descriptions tend to have few textual indicators that demonstrate their gang affiliations or their tweets/profile text may carry acronyms which can only be deciphered by others involved in gang culture <cite>[BWDS16]</cite> . These gang-related terms are often local to gangs operating in neighborhoods and change rapidly when they form new gangs. Consequently, building a database of keywords, phrases, and other identifiers to find gang members nationally is not feasible. Instead, we use heterogeneous sets of features derived not only from profile and tweet text but also from the emoji usage, profile images, and links to YouTube videos reflecting their music preferences and affinity. In this section, we briefly discuss the feature types and their broad differences in gang and non-gang member profiles. An in-depth explanation of these feature selection can be found in <cite>[BWDS16]</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_5",
  "x": "An in-depth explanation of these feature selection can be found in <cite>[BWDS16]</cite> . ---------------------------------- **TWEET TEXT:** In our previous work, we observed that gang members use curse words nearly five times more than the average curse words use on Twitter <cite>[BWDS16]</cite> . Further, we noticed that gang members mainly use Twitter to discuss drugs and money using terms such as smoke, high, hit, money, got, and need while non-gang members mainly discuss their feelings using terms such as new, like, love, know, want, and look. ---------------------------------- **TWITTER PROFILE DESCRIPTION:**",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_6",
  "x": "---------------------------------- **TWEET TEXT:** In our previous work, we observed that gang members use curse words nearly five times more than the average curse words use on Twitter <cite>[BWDS16]</cite> . Further, we noticed that gang members mainly use Twitter to discuss drugs and money using terms such as smoke, high, hit, money, got, and need while non-gang members mainly discuss their feelings using terms such as new, like, love, know, want, and look. ---------------------------------- **TWITTER PROFILE DESCRIPTION:** We found gang member profile descriptions to be rife with curse words (nigga, fuck, and shit) while non-gang members use words related to their feelings or interests (love, life, music, and book).",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_7",
  "x": "Tags such as trigger, bullet, and worship were unique for gang member profiles while non-gang member images had unique tags such as beach, seashore, dawn, wildlife, sand, and pet. ---------------------------------- **YOUTUBE VIDEOS:** We found that 51.25% of the gang members in our dataset have a tweet that links to a YouTube video. Further, we found that 76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre <cite>[BWDS16]</cite> . Moreover, we found that eight YouTube links are shared on average by a gang member. The top 5 terms used in YouTube videos shared by gang members were shit, like, nigga, fuck, and lil while like, love, peopl, song, and get were the top 5 terms in nongang members' video data.",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_8",
  "x": "**EVALUATION SETUP** We consider a dataset of curated gang and non-gang members' Twitter profiles collected from our previous work <cite>[BWDS16]</cite> . It was developed by querying the Followerwonk Web service API 4 with location-neutral seed words known to be used by gang members across the U.S. in their Twitter profiles. The dataset was further expanded by examining the friends, follower, and retweet networks of the gang member profiles found by searching for seed words. Specific details about our data curation procedure are discussed in <cite>[BWDS16]</cite> . Ultimately, this dataset consists of 400 gang member profiles and 2,865 non-gang member profiles. For each user profile, we collected up to most recent 3,200 tweets from their Twitter timelines, profile description text, profile and cover images, and the comments and video descriptions for every YouTube video shared by them.",
  "y": "uses"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_9",
  "x": "We consider a dataset of curated gang and non-gang members' Twitter profiles collected from our previous work <cite>[BWDS16]</cite> . It was developed by querying the Followerwonk Web service API 4 with location-neutral seed words known to be used by gang members across the U.S. in their Twitter profiles. The dataset was further expanded by examining the friends, follower, and retweet networks of the gang member profiles found by searching for seed words. Specific details about our data curation procedure are discussed in <cite>[BWDS16]</cite> . Ultimately, this dataset consists of 400 gang member profiles and 2,865 non-gang member profiles. For each user profile, we collected up to most recent 3,200 tweets from their Twitter timelines, profile description text, profile and cover images, and the comments and video descriptions for every YouTube video shared by them. Table 1 provides statistics about the number of words found in each type of feature in the dataset.",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_10",
  "x": "fiers. An open source tool of Python, Gensim [\u0158S10] was used to generate the word embeddings. We compare our results with the two best performing systems reported in <cite>[BWDS16]</cite> which are the two state-of-theart models for identifying gang members in Twitter. Both baseline models are built from a random forest classifier trained over term frequencies for unigrams in tweet text, emoji, profile data, YouTube video data and image tags. Baseline Model(1) considers all 3,285 gang and non-gang member profiles in our dataset. Baseline Model(2) considers all Twitter profiles that contain every feature type discussed in Section 3.1. Because a Twitter profile may not have every feature type, baseline Model(1) represents a practical scenario where not every Twitter profile contains every type of feature.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_0",
  "x": "Separately, Gu et al. (2018) and Lee et al. (2018) developed non-autoregressive approaches to machine translation where the entire output can be generated in parallel in constant time. These models do away with order selection altogether but typically lag behind their autoregressive counterparts in translation quality. More recently, a number of novel insertionbased architectures have been developed for sequence generation (Gu et al., 2019;<cite> Stern et al., 2019</cite>; Welleck et al., 2019) . These frameworks license a diverse set of generation orders, including uniform (Welleck et al., 2019) , random (Gu et al., 2019) , or balanced binary trees <cite>(Stern et al., 2019)</cite> . Some of them also match the quality of state-of-the-art left-to-right models <cite>(Stern et al., 2019)</cite> . In this paper, we utilize one such framework to explore an extensive collection of generation orders, evaluating them on the WMT'14 English-German and WMT'18 English-Chinese translation tasks. We find that a number of nonstandard choices achieve BLEU scores comparable to those obtained with the classical approach, suggesting that left-to-right generation might not be a necessary ingredient for high-quality translation.",
  "y": "background"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_1",
  "x": "More recently, a number of novel insertionbased architectures have been developed for sequence generation (Gu et al., 2019;<cite> Stern et al., 2019</cite>; Welleck et al., 2019) . These frameworks license a diverse set of generation orders, including uniform (Welleck et al., 2019) , random (Gu et al., 2019) , or balanced binary trees <cite>(Stern et al., 2019)</cite> . Some of them also match the quality of state-of-the-art left-to-right models <cite>(Stern et al., 2019)</cite> . In this paper, we utilize one such framework to explore an extensive collection of generation orders, evaluating them on the WMT'14 English-German and WMT'18 English-Chinese translation tasks. We find that a number of nonstandard choices achieve BLEU scores comparable to those obtained with the classical approach, suggesting that left-to-right generation might not be a necessary ingredient for high-quality translation. Our contributions are as follows: \u2022 We introduce a general soft order-reward framework that can be used to teach insertion-based models to follow any specified ordering. \u2022 We perform a thorough empirical study of various orders, including: uniform, random, left-to-right, right-to-left, common-first, rarefirst, shortest-first, longest-first, alphabetical, and model-adaptive.",
  "y": "background"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_2",
  "x": "These models do away with order selection altogether but typically lag behind their autoregressive counterparts in translation quality. More recently, a number of novel insertionbased architectures have been developed for sequence generation (Gu et al., 2019;<cite> Stern et al., 2019</cite>; Welleck et al., 2019) . These frameworks license a diverse set of generation orders, including uniform (Welleck et al., 2019) , random (Gu et al., 2019) , or balanced binary trees <cite>(Stern et al., 2019)</cite> . Some of them also match the quality of state-of-the-art left-to-right models <cite>(Stern et al., 2019)</cite> . In this paper, we utilize one such framework to explore an extensive collection of generation orders, evaluating them on the WMT'14 English-German and WMT'18 English-Chinese translation tasks. We find that a number of nonstandard choices achieve BLEU scores comparable to those obtained with the classical approach, suggesting that left-to-right generation might not be a necessary ingredient for high-quality translation. Our contributions are as follows: \u2022 We introduce a general soft order-reward framework that can be used to teach insertion-based models to follow any specified ordering.",
  "y": "background"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_3",
  "x": "Separately, Gu et al. (2018) and Lee et al. (2018) developed non-autoregressive approaches to machine translation where the entire output can be generated in parallel in constant time. These models do away with order selection altogether but typically lag behind their autoregressive counterparts in translation quality. More recently, a number of novel insertionbased architectures have been developed for sequence generation (Gu et al., 2019;<cite> Stern et al., 2019</cite>; Welleck et al., 2019) . These frameworks license a diverse set of generation orders, including uniform (Welleck et al., 2019) , random (Gu et al., 2019) , or balanced binary trees <cite>(Stern et al., 2019)</cite> . Some of them also match the quality of state-of-the-art left-to-right models <cite>(Stern et al., 2019)</cite> . In this paper, we utilize one such framework to explore an extensive collection of generation orders, evaluating them on the WMT'14 English-German and WMT'18 English-Chinese translation tasks. We find that a number of nonstandard choices achieve BLEU scores comparable to those obtained with the classical approach, suggesting that left-to-right generation might not be a necessary ingredient for high-quality translation.",
  "y": "similarities background"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_4",
  "x": "Neural sequence models have traditionally been designed with left-to-right prediction in mind. In the classical setting, output sequences are produced by repeatedly appending tokens to the rightmost end of the hypothesis until an endof-sequence token is generated. Though highperforming across a wide range of application areas, this approach lacks the flexibility to accommodate other types of inference such as parallel generation, constrained decoding, infilling, etc. Moreover, it also leaves open the possibility that a non-left-to-right factorization of the joint distribution over output sequences could outperform the usual monotonic ordering. To address these concerns, several recent approaches have been proposed for insertion-based sequence modeling, in which sequences are con-structed by repeatedly inserting tokens at arbitrary locations in the output rather than only at the right-most position. We use one such insertion-based model, the Insertion Transformer <cite>(Stern et al., 2019)</cite> , for our empirical study. We give a brief overview of the model in this section before moving on to the details of our investigation.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_5",
  "x": "We note that<cite> Stern et al. (2019)</cite> also experimented with a number of other architectural variants, but we use the baseline version of the model described above in our experiments for simplicity. ---------------------------------- **DECODING** Once the model has been trained, it can be used for greedy autoregressive sequence generation as follows. At each step of decoding, we compute the joint argmax to determine what content\u0109 t should be inserted at which locationl t . We then apply this insertion, increasing the sequence length by one, and repeat this process until an end-of-sequence token is produced.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_6",
  "x": "Having presented our model of interest, we now describe a general soft order-reward framework that can be used to train the model to follow any oracle ordering for sequence generation. Let O(a) be an order function mapping insertion actions to real numbers, where lower values correspond to better actions, and let p(a) be the probability assigned by the model to action a. From these, we construct a reward function R(a), an oracle policy q oracle , and a per-slot loss L: Here, A * is the set of all valid actions. The temperature \u03c4 \u2208 (0, \u221e) controls the sharpness of the distribution, where \u03c4 \u2192 0 results in a one-hot distribution with all mass on the best-scoring action under the order function O(a), and \u03c4 \u2192 \u221e results in a uniform distribution over all valid actions. Intermediate values of \u03c4 result in distributions which are biased towards better-scoring actions but allow for other valid actions to be taken some of the time. Having defined the target distribution, we take the slot loss L for insertions within a particular slot to be the KL-divergence between the oracle distribution q oracle and the model distribution p. Substituting L in for the slot loss within the training framework of<cite> Stern et al. (2019)</cite> then gives the full sequence generation loss, which we can use to train an Insertion Transformer under any oracle policy rather than just the specific one they propose. We describe a wide variety of generation orders which can be characterized by different order functions O(a) in the subsections that follow.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_7",
  "x": "---------------------------------- **ROLL-IN POLICY** We follow<cite> Stern et al. (2019)</cite> and use a uniform roll-in policy when sampling partial outputs at training time in which we first select a subset size uniformly at random, then select a random subset of the output of that size. Repeated tokens are handled via greedy left or right alignment to the true output. Input: It would of course be a little simpler for the Germans if there were a coherent and standardised European policy, which is currently not the case. Output: Es w\u00e4re f\u00fcr die Deutschen nat\u00fcrlich ein wenig einfacher, wenn es eine koh\u00e4rente und einheitliche europ\u00e4ische Politik g\u00e4be, was derzeit nicht der Fall ist. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_8",
  "x": "Parallel decode (common-first): out using sacreBLEU 2 (Post, 2018) . In both cases, we train all models for 1M steps using sequencelevel knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016 ) from a base Transformer (Vaswani et al., 2017) . We perform a sweep over temperatures \u03c4 \u2208 {0.5, 1, 2} and EOS penalties \u2208 {0, 0.5, 1, 1.5, . . . , 8} <cite>(Stern et al., 2019)</cite> ---------------------------------- **ABILITY TO LEARN DIFFERENT ORDERS** By and large, we find that the Insertion Transformer is remarkably capable of learning to generate according to whichever order it was trained for. We give example decodes for three different generation orders in Figures 3, 4 , and 5.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_9",
  "x": "The BLEU scores are reported in Table 3 . The uniform loss proposed by<cite> Stern et al. (2019)</cite> serves as a strong baseline for both language pairs, coming within 0.6 points of the original Transformer for En-De at 26.72 BLEU, and attaining a respectable score of 33.1 BLEU on En-Zh. We note that there is a slightly larger gap between the normal Transformer and the Insertion Transformer for the latter of 2.7 points, which we hypothesize is a result of the larger discrepancy between word orders in the two languages combined with the more difficult nature of the Insertion Transformer training objective. Most of the content-based orderings (frequency-based, length-based, alphabetical) perform comparably to the uniform loss, and even the random order is not far behind. The adaptive orders perform similarly well, with easy-first attaining one of the highest scores on En-De. Curiously, in our model adaptive easy-order, we were unable to identify any strong patterns in the generation order. The model did have a slight preference towards functional words (i.e., \",\" and \"der\"), but the preference was weak.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_10",
  "x": "The model did have a slight preference towards functional words (i.e., \",\" and \"der\"), but the preference was weak. As for location-based losses, the binary tree loss is notable in that it achieves the highest score across all losses for both languages. On the other hand, we note that while the soft left-to-right and right-to-left losses perform substantially better than the hard loss employed in the original work by<cite> Stern et al. (2019)</cite> , performance does suffer when using parallel decoding for those models, which is generally untrue of the other orderings. We believe this is due in part to exposure bias issues arising from the monotonic ordering as compared with the uniform roll-in policy that are not shared by the other losses. ---------------------------------- **PERFORMANCE VS. SENTENCE LENGTH** For additional analysis, we consider how well our models perform relative to one another conditional on the length of the source sentence.",
  "y": "differences"
 },
 {
  "id": "e59bd02bb560d80ce08dfcd6b35317_0",
  "x": "For example, automatic speech transcription applied to call centre audio recordings is able to capture a level of detail that is otherwise unavailable unless the call audio is manually reviewed which is infeasible for large call volumes. In this case topic modelling can be applied to transcribed text to extract the key issues and emerging topics of discussion. In this study we propose a method for simulating various types of transcription errors. We then test the robustness of a popular topic modelling algorithm, Latent Dirichlet Allocation (LDA) using a topic stability measure introduced by <cite>Greene et al. (2014)</cite> over a variety of corpora. ---------------------------------- **TOPIC MODELLING AND METRICS** Blei et al. (Blei et al., 2003) introduced Latent Dirichlet Allocation (LDA) as a generative probabilistic model for text corpora.",
  "y": "uses"
 },
 {
  "id": "e59bd02bb560d80ce08dfcd6b35317_1",
  "x": "Blei et al. (Blei et al., 2003) introduced Latent Dirichlet Allocation (LDA) as a generative probabilistic model for text corpora. LDA regulates the probabilistic distributions between document, topic and word and it is an unsupervised learning model. For the evaluation of topic models, we follow the approach by <cite>Greene et al. (2014)</cite> for measuring topic model agreement . We can denote a topic list as S = {R 1 , ..., R k }, where R i is a topic with rank i. An individual topic can be described as R = {T 1 , ..., T m }, where T l is a term with rank l belong to the topic. Jaccard index (Jaccard, 1912) compares the number of identical items in two sets, but it neglects ranking order. Average Jaccard (AJ) similarity is a top-weighted version of the Jaccard index used to accommodate ranking information. AJ calculates the average of the Jaccard scores between every pair of subsets in two lists.",
  "y": "uses"
 },
 {
  "id": "e59bd02bb560d80ce08dfcd6b35317_2",
  "x": "The topic model agreement score between S 1 and S 2 is a mean value of the top similarity scores between each cross pair of R. The agreement score is solved using the Hungarian method (Kuhn, 1955) and is constrained in the range [0, 1] , where a perfect match between two identical k-way ranked sets results in 1 and a score 0 for nonoverlapping sets. (Greene et al., 2014) ---------------------------------- **DATASETS** In this paper, we explore two datasets bbc and wikilow<cite> (Greene et al., 2014)</cite> with different document size and corpus size. The bbc corpus includes general BBC news articles. This corpus contains 2225 documents in 5 topics and it uses 3121 terms.",
  "y": "uses"
 },
 {
  "id": "e59bd02bb560d80ce08dfcd6b35317_3",
  "x": "Moreover, the number of topics selected also affects topic agreement. With random noise or low-level systematic errors (below 20%), a correct or approximately correct number of topics brings the highest topic agreement scores. With high level systematic errors, topic models with 3 times the correct number of topics are most robust. In some corpora, redundant number of topics helps the LDA model through severe systematic errors (Figure 1(b) ). This complements previous work by <cite>Greene et al. (2014)</cite> who investigated how topic stability is influenced by number of topics over noisefree corpora. This suggests that transcribers should perhaps consider omitting words when the uncertainty is high. The topic model is less influenced with a random missing term than an erroneous replacement.",
  "y": "similarities"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_0",
  "x": "One of the remarkable perspectives to dig into natural linguistic laws is provided by social and behavior sciences, adaptation in language during communication as a result of changes in opinions and decisions. Opinions and decisions are personal in individual level, however, they are flexible while facing public opinions and decisions. Linguistic adaptation is twofold. In one part, collective voice unifies opinions and decisions in a complex process, ideas are biased, and consequently people start acting similarly, talking similarly, and so writing similarly. Twitter conversations<cite> (Danescu-Niculescu-Mizil, Gamon, and Dumais 2011</cite>; Purohit et al. 2013 ) and popular memes (Myers and Leskovec 2012;<cite> Coscia 2013)</cite> prove this similarity in social media. In the other part, when people have a well-defined goal at the end, they tend to reshape their arguments. In the presence of distinct winning and losing sides and social hierarchy, people at lower status show both cooperation through that at the higher status and competition among each other.",
  "y": "background"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_1",
  "x": "To measure commonsense for a particular situation is hard, however, adaptations can be easily captured in Twitter conversations<cite> (Danescu-NiculescuMizil, Gamon, and Dumais 2011</cite>; Purohit et al. 2013) , in memes (Myers and Leskovec 2012;<cite> Coscia 2013)</cite> , and faceto-face discussions<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> . In this paper, our main concerns are firstly to construct discussion groups including agents having different social powers and serving opposite aims. Secondly, we investigate how we can track the progress of opinions together with their influences on decisions in oral conversations. We claim that linguistic relations (Poria et al. 2015) preserve all rich phenomena, shortly discussed above, including collective voice, reshaping arguments, and so adaptation. To analyze adaptation induced by both cooperation and competition, we consider court conversations: they are held in clearly stated winner and loser groups with distinct hierarchy in decisionmaking due to the presence of Justices and lawyers. To this end, we evaluate the open access data of the United States Supreme Court (Hawes, Lin, and Resnik 2009; Hawes 2009; <cite>Danescu-Niculescu-Mizil et al. 2012</cite> ), prepare conversation groups with different adaptation levels, implement a suitable algorithm to extract linguistic relations in these group conversations, and finally provide a comparison between the groups and the discovered linguistic relations. The rest of the paper is organized as follows: the first section presents the dataset we consider and designed conversation groups out of the data; the second section describes our algorithm in detail; the following section explains how we implement pointwise mutual information for the conversation groups and then link with linguistic relations; finally, we provide experimental results and conclude the paper.",
  "y": "background"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_2",
  "x": "The resultant unified collective motion is extremely interesting in social groups (Borge-Holthoefer et al. 2011; Gonzalez-Bailon et al. 2011 ). Opinions and decisions are personal in individual level. However, as observed, they are quite flexible facing with a collective decision. Complex knowledge extraction process in micro state suddenly becomes less valuable and group decision gains (Conover et al. 2011) . We can argue that our opinions are biased when our decisions mostly rely on our previous knowledge, e.g., commonsense, and so richness of opinions kept in each individual is relatively unimportant. We can further argue that commonsense drives an adaptation in extracting knowledge. To measure commonsense for a particular situation is hard, however, adaptations can be easily captured in Twitter conversations<cite> (Danescu-NiculescuMizil, Gamon, and Dumais 2011</cite>; Purohit et al. 2013) , in memes (Myers and Leskovec 2012;<cite> Coscia 2013)</cite> , and faceto-face discussions<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> .",
  "y": "uses"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_3",
  "x": "We claim that linguistic relations (Poria et al. 2015) preserve all rich phenomena, shortly discussed above, including collective voice, reshaping arguments, and so adaptation. To analyze adaptation induced by both cooperation and competition, we consider court conversations: they are held in clearly stated winner and loser groups with distinct hierarchy in decisionmaking due to the presence of Justices and lawyers. To this end, we evaluate the open access data of the United States Supreme Court (Hawes, Lin, and Resnik 2009; Hawes 2009; <cite>Danescu-Niculescu-Mizil et al. 2012</cite> ), prepare conversation groups with different adaptation levels, implement a suitable algorithm to extract linguistic relations in these group conversations, and finally provide a comparison between the groups and the discovered linguistic relations. The rest of the paper is organized as follows: the first section presents the dataset we consider and designed conversation groups out of the data; the second section describes our algorithm in detail; the following section explains how we implement pointwise mutual information for the conversation groups and then link with linguistic relations; finally, we provide experimental results and conclude the paper. ---------------------------------- **SUPREME COURT DATA** We borrow the textual data of the conversations in the United States Supreme Court pre-processed by (Hawes, Lin, and Resnik 2009; Hawes 2009 ) and enriched by (DanescuNiculescu-Mizil et al. 2012) including the final votes of Justices.",
  "y": "uses"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_4",
  "x": "---------------------------------- **SUPREME COURT DATA** We borrow the textual data of the conversations in the United States Supreme Court pre-processed by (Hawes, Lin, and Resnik 2009; Hawes 2009 ) and enriched by (DanescuNiculescu-Mizil et al. 2012) including the final votes of Justices. Both the original data and the most updated version used here are publicly available<cite> (Danescu-NiculescuMizil et al. 2012)</cite> . The data gathers oral speeches before the Supreme Court and hosts 50,389 conversational exchanges among Justices and lawyers. Distinct hierarchy between Justices (high power) and lawyers (low power) impose lawyers to tune their arguments under the perspective and understandings of Justices, and as a result, speech adaptation and linguistic coordination leaves their traces in a sudden occurrence of sharing the same adverbs, conjunctions, and pronouns. Tracking initial utterances, the sides present a unique and personal speaking, but after a while in the communication, word selections, their forms, and frequencies mirror each other's language preference.",
  "y": "uses"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_5",
  "x": "Tracking initial utterances, the sides present a unique and personal speaking, but after a while in the communication, word selections, their forms, and frequencies mirror each other's language preference. The linguistic coordination is systematically quantified by<cite> (Danescu-Niculescu-Mizil et al. 2012</cite> ) and the arguments follow the principles of exchange theory examining behavior dynamics in low and high power groups (Willer 1999; Thye, Willer, and Markovsky 2006) : Lawyers tend to cooperate more to Justices than conversely and demonstrate strong linguistic coordination in their speech. Moreover, lawyers show even more cooperation to unfavorable Justices than favorable ones. Here, we enrich the comparison including the identity of winners and losers in lawsuits. The data provides whether the petitioner or the respondent is the winner at the end of each lawsuit. In addition, the speaker of each utterance is labeled as their position, e.g., Justice or lawyer. Furthermore, Justice's votes and the side of lawyers are tagged with the utterances.",
  "y": "background"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_6",
  "x": "**SUPREME COURT DATA** We borrow the textual data of the conversations in the United States Supreme Court pre-processed by (Hawes, Lin, and Resnik 2009; Hawes 2009 ) and enriched by (DanescuNiculescu-Mizil et al. 2012) including the final votes of Justices. Both the original data and the most updated version used here are publicly available<cite> (Danescu-NiculescuMizil et al. 2012)</cite> . The data gathers oral speeches before the Supreme Court and hosts 50,389 conversational exchanges among Justices and lawyers. Distinct hierarchy between Justices (high power) and lawyers (low power) impose lawyers to tune their arguments under the perspective and understandings of Justices, and as a result, speech adaptation and linguistic coordination leaves their traces in a sudden occurrence of sharing the same adverbs, conjunctions, and pronouns. Tracking initial utterances, the sides present a unique and personal speaking, but after a while in the communication, word selections, their forms, and frequencies mirror each other's language preference. The linguistic coordination is systematically quantified by<cite> (Danescu-Niculescu-Mizil et al. 2012</cite> ) and the arguments follow the principles of exchange theory examining behavior dynamics in low and high power groups (Willer 1999; Thye, Willer, and Markovsky 2006) : Lawyers tend to cooperate more to Justices than conversely and demonstrate strong linguistic coordination in their speech.",
  "y": "uses"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_7",
  "x": "In 1-4, who supported by the Justice is given in the middle. Furthermore, the last indicates the side of lawyer the Justice speaks to. Referring exchange theory (Willer 1999; Thye, Willer, and Markovsky 2006) and the measured coordination<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> , one can order the relative power of each Justice and lawyer pair where J and l represent Justices and lawyers, respectively (note that for comparing individually following the social exchange theory, P (J) > P (l) for both supportive and unsupported Justices). The subscript u indicates that Justice doesn't support the side of lawyer and the supportive version is described by s. For instance, in Table 1 , in the communications of 1 and 5; 4 and 6, Justices show supports and play as J s , whereas that of 3 and 5; 2 and 6, lawyers are unsupported by J u . The scenarios and pairs guide to construct groups with different cooperation level induced by P as illustrated in Table 2 . We further add another dimension in the relative power: Winners and Losers, haven't been investigated in the previous study<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> .",
  "y": "uses"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_8",
  "x": "where J and l represent Justices and lawyers, respectively (note that for comparing individually following the social exchange theory, P (J) > P (l) for both supportive and unsupported Justices). The subscript u indicates that Justice doesn't support the side of lawyer and the supportive version is described by s. For instance, in Table 1 , in the communications of 1 and 5; 4 and 6, Justices show supports and play as J s , whereas that of 3 and 5; 2 and 6, lawyers are unsupported by J u . The scenarios and pairs guide to construct groups with different cooperation level induced by P as illustrated in Table 2 . We further add another dimension in the relative power: Winners and Losers, haven't been investigated in the previous study<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> . To this end, Eq. 1 is reformulated Group ID Cooperation Pool of J and l I.i supportive, P (J s , l) 1 and 5 I.ii unsupported, P (J u , l) 3 and 5 II.i unsupported, P (J u , l) 2 and 6 II.ii supportive, P (J s , l) 4 and 6 Table 2 : Grouping communications with respect to level of cooperation, based on the relative power of the partners in the conversations. 1-6 and the power pairs P (J s , l) and P (J u , l) as defined previously.",
  "y": "differences"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_9",
  "x": "So, while the numerator describes the probabilistic occurrence of R in \u03ba, the denominator provides individual probability of R and that of \u03ba in the pool. We expect high M I(R, \u03ba) while R appears in a specific \u03ba and that is an indicator of its rare presence in the other conversation groups. Unlike the previous study<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> , entirely tracking back and forth utterances and proving the adaptation, e.g., linguistic coordination, by identifying the frequency of selected keywords, we directly utilize their overall conclusion and claim that linguistic relations already preserve the adaptation and any other complex collective linguistic process induced by both cooperation and competition in different power groups. We expect that the variation in M I(R, \u03ba) of gathered utterances of each relative power group, independent of the utterance order, suggests which relations can distinguish the difference in the groups and the magnitude of M I(R, \u03ba) of that difference highlights which relative power groups drastically influence the applied language. We will analyze M I(R, \u03ba) following this discussed understanding in coming Section. ---------------------------------- **RESULTS**",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_0",
  "x": "The proposed new loss function allows training a model to detect parts and whole of a keyword, without strictly depending on frame-level labeling from LVCSR (Large vocabulary continuous speech recognition), making further optimization possible. The proposed system outperforms the baseline keyword spotting model in [<cite>1</cite>] due to increased optimizability. Further, it can be more easily adapted for on-device learning applications due to reduced dependency on LVCSR. ---------------------------------- **INTRODUCTION** Keyword detection has become an important frontend service for ASR-based assistant interfaces (e.g. Hey Google, Alexa, Hey Siri). As assistant technology spreads to more ubiquitous use-cases (mobile, IOT), reducing resource consumption (memory and computation) while improving accuracy has been the key success criteria of keyword spotting techniques.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_1",
  "x": "Such works include DNN + temporal integration [4, 5, 11, 12] , and HMM + DNN hybrid approaches [6, 7, 8, 9, 10] . Recently introduced end-to-end trainable DNN approaches [<cite>1,</cite> 13] further improved accuracy and lowered resource requirements using highly optimizable system design. In general, training of such DNN based systems required framelevel labels generated by LVCSR systems [14, <cite>1</cite>] . These approaches make end-to-end optimizable keyword spotting system depend on labels generated from non-end-to-end system trained for a different task. However, for keyword-spotting, the exact position of the keyword is not as relevant as its presence. Therefore, such strict dependency on frame-level labels may limit further optimization promised by the end-to-end approach. In [<cite>1</cite>] , the top level loss is derived by integrating frame-level losses, which are computed using frame-level labels from LVCSR.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_2",
  "x": "Such works include DNN + temporal integration [4, 5, 11, 12] , and HMM + DNN hybrid approaches [6, 7, 8, 9, 10] . Recently introduced end-to-end trainable DNN approaches [<cite>1,</cite> 13] further improved accuracy and lowered resource requirements using highly optimizable system design. In general, training of such DNN based systems required framelevel labels generated by LVCSR systems [14, <cite>1</cite>] . These approaches make end-to-end optimizable keyword spotting system depend on labels generated from non-end-to-end system trained for a different task. However, for keyword-spotting, the exact position of the keyword is not as relevant as its presence. Therefore, such strict dependency on frame-level labels may limit further optimization promised by the end-to-end approach. In [<cite>1</cite>] , the top level loss is derived by integrating frame-level losses, which are computed using frame-level labels from LVCSR.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_3",
  "x": "These approaches make end-to-end optimizable keyword spotting system depend on labels generated from non-end-to-end system trained for a different task. However, for keyword-spotting, the exact position of the keyword is not as relevant as its presence. Therefore, such strict dependency on frame-level labels may limit further optimization promised by the end-to-end approach. In [<cite>1</cite>] , the top level loss is derived by integrating frame-level losses, which are computed using frame-level labels from LVCSR. Integrating frame-level losses penalizes slightly mis-aligned correct predictions, which can limit detection accuracy, especially for difficult data (e.g. noisy or accented speech) where LVCSR labels may have higher-than-normal uncertainty. In such case, losses can be fully minimized only when the predicted value and position-in-time matches that of provided frame level labels, where exact position match is not highly relevant for high accuracy. Prior work of CTC-training [15] or sequence-to-sequence training [16] dont require on frame level alignment information.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_4",
  "x": "One can train models to generate stable activations for a target pattern even without exact location of the target specified. We describe the details of the proposed method in Section 2. Then we show experiment setup in Section 3, and results in Section 4. We conclude with discussions in Section 5. ---------------------------------- **SMOOTHED MAX POOLING LOSS FOR TRAINING ENCODER/DECODER KEYWORD SPOTTING MODEL** The proposed model uses the same encoder/decoder structure as [<cite>1</cite>] ( Fig.1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss.",
  "y": "differences similarities uses"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_5",
  "x": "Then we show experiment setup in Section 3, and results in Section 4. We conclude with discussions in Section 5. ---------------------------------- **SMOOTHED MAX POOLING LOSS FOR TRAINING ENCODER/DECODER KEYWORD SPOTTING MODEL** The proposed model uses the same encoder/decoder structure as [<cite>1</cite>] ( Fig.1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss. In [<cite>1</cite>] , both encoder and decoder models are trained with cross entropy (CE) loss using frame level labels. In the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_6",
  "x": "In [<cite>1</cite>] , both encoder and decoder models are trained with cross entropy (CE) loss using frame level labels. In the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously. The proposed smoothed max pooling loss doesnt strictly depend on phoneme-level alignment, allowing better optimization than the <cite>baseline</cite>. ---------------------------------- **BASELINE END-TO-END KEYWORD SPOTTING MODEL** Both the <cite>baseline</cite> and the proposed model have an encoder which takes spectral domain feature Xt as input and generate (K+1) outputs Y E corresponding to phoneme-like sound units (Fig.1 ). The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword .",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_7",
  "x": "Both the <cite>baseline</cite> and the proposed model have an encoder which takes spectral domain feature Xt as input and generate (K+1) outputs Y E corresponding to phoneme-like sound units (Fig.1 ). The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword . The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner. In [<cite>1</cite>] , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen. In [<cite>1</cite>] , the encoder model is trained to predict phonemelevel labels provided from LVCSR. Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt\u2212C l , \u00b7 \u00b7 \u00b7 , xt, \u00b7 \u00b7 \u00b7 , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t. In [<cite>1</cite>] , target label sequence consists of intervals of repeated labels which we call runs.",
  "y": "similarities"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_8",
  "x": "**BASELINE END-TO-END KEYWORD SPOTTING MODEL** Both the <cite>baseline</cite> and the proposed model have an encoder which takes spectral domain feature Xt as input and generate (K+1) outputs Y E corresponding to phoneme-like sound units (Fig.1 ). The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword . The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner. In [<cite>1</cite>] , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen. In [<cite>1</cite>] , the encoder model is trained to predict phonemelevel labels provided from LVCSR. Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt\u2212C l , \u00b7 \u00b7 \u00b7 , xt, \u00b7 \u00b7 \u00b7 , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_9",
  "x": "---------------------------------- **BASELINE END-TO-END KEYWORD SPOTTING MODEL** Both the <cite>baseline</cite> and the proposed model have an encoder which takes spectral domain feature Xt as input and generate (K+1) outputs Y E corresponding to phoneme-like sound units (Fig.1 ). The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword . The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner. In [<cite>1</cite>] , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen. In [<cite>1</cite>] , the encoder model is trained to predict phonemelevel labels provided from LVCSR.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_10",
  "x": "In [<cite>1</cite>] , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen. In [<cite>1</cite>] , the encoder model is trained to predict phonemelevel labels provided from LVCSR. Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt\u2212C l , \u00b7 \u00b7 \u00b7 , xt, \u00b7 \u00b7 \u00b7 , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t. In [<cite>1</cite>] , target label sequence consists of intervals of repeated labels which we call runs. These label runs define clearly defined intervals where a model should learn to generate strong activation in label output dimension. While such model behavior can be trained end-to-end, the labels need to be provided from a LVCSR system which is typically non-end-to-end system [2] . The timing and accuracy of labels from LVCSR system can limit the accuracy of the trained model.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_11",
  "x": "We compare the model trained with the new smoothed max pooling loss on encoder/decoder architecture with the baseline in [<cite>1</cite>] . Both the <cite>baseline</cite> and the proposed model have the same architecture. Only the training losses are different. Details of the setup are discussed below. ---------------------------------- **FRONT-END** We used the same frontend feature extract as the baseline [<cite>1</cite>] in our experiments.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_12",
  "x": "Both the <cite>baseline</cite> and the proposed model have the same architecture. Only the training losses are different. Details of the setup are discussed below. ---------------------------------- **FRONT-END** We used the same frontend feature extract as the baseline [<cite>1</cite>] in our experiments. The front-end extracts and stacks a 40-d feature vector of log-mel filter-bank energies at each frame and stacks them to generate input feature vector Xt. Refer to [<cite>1</cite>] for further details.",
  "y": "similarities"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_13",
  "x": "Details of the setup are discussed below. ---------------------------------- **FRONT-END** We used the same frontend feature extract as the baseline [<cite>1</cite>] in our experiments. The front-end extracts and stacks a 40-d feature vector of log-mel filter-bank energies at each frame and stacks them to generate input feature vector Xt. Refer to [<cite>1</cite>] for further details. ---------------------------------- **MODEL SETUP**",
  "y": "similarities uses"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_15",
  "x": "**FRONT-END** We used the same frontend feature extract as the baseline [<cite>1</cite>] in our experiments. The front-end extracts and stacks a 40-d feature vector of log-mel filter-bank energies at each frame and stacks them to generate input feature vector Xt. Refer to [<cite>1</cite>] for further details. ---------------------------------- **MODEL SETUP** We selected E2E 318K architecture in [<cite>1</cite>] as the baseline and use the same structure for testing all other models. As shown in Fig. 1 , the model has 7 SVDF layers and 3 linear bottleneck dense layers.",
  "y": "similarities uses"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_17",
  "x": "As shown in Fig. 1 , the model has 7 SVDF layers and 3 linear bottleneck dense layers. For detailed architectural parameters, please refer to [<cite>1</cite>] . We call the <cite>baseline model</cite> as Baseline CE CE where encoder and decoder submodels are trained with CE loss. We call the proposed model as Max4 SMP SMP where both encoder and decoder submodels are trained by SMP (smoothed max pooling) loss. We also performed ablation study by testing other models that use different losses. Table 1 summarizes all the tested models. Model Max1-Max3 uses SMP (smoothed max pooling) loss for the decoder, but uses different losses for the encoder.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_18",
  "x": "CTC loss doesnt need alignments, but it learns peaky activations whose peak values are not highly stable. Max2 NA SMP has no encoder loss (i.e. \u03b1 = 0), s.t. the entire network is trained by decoder loss only. Max3 CE SMP used <cite>baseline</cite> CE loss for encoder. Model Max4-Max7 are tested to measure the importance of the smoothing operation. MP means max pooling without smoothing (i.e. s(t) = 1). For the decoder SMP(smoothed max pooling) loss, we used truncated Gaussian as the smoothing filter s(t) with \u00b5 = 0, \u03c3 = 9 frames (90ms) and truncated length 21 frames.",
  "y": "uses"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_19",
  "x": "The training data consists of 2.1 million anonymized utterances with the keywords Ok Google and Hey Google. Data augmentation similar to [<cite>1</cite>] has been used for better robustness. Evaluation is done on four data sets separate from training data, representing diverse environmental conditions -Clean non-accented set contains 170K non-accented English utterances of keywords in quiet condition. Clean accented has 138K English utterances of keyword with Australian, British, and Indian accents in quiet conditions. Query logs contains 58K utterances from anonymized voice search queries. In-vehicle set has 144K utterances with the keywords recorded inside cars while driving, which includes significant amount of noises from road, engine, and fans. All sets are augmented with 64K negative utterances, which are re-recorded TV noise.",
  "y": "similarities"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_21",
  "x": "Table 2 summarizes FR rates of models in Fig.3 and 4 at selected FA rate (0.1 FA per hour measured on 64K re-recorded TV noise set). Fig.3 shows the ROC curves of various models (<cite>baseline</cite>, Max1-Max4) across different conditions. Figure 4 shows the ROC curves of Max4-Max7 models across different conditions. Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve. Max3 CE MP model also performs better than the <cite>baseline</cite> but not as good as Max4. Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than <cite>baseline.</cite> Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss).",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_22",
  "x": "Figure 4 shows the ROC curves of Max4-Max7 models across different conditions. Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve. Max3 CE MP model also performs better than the <cite>baseline</cite> but not as good as Max4. Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than <cite>baseline.</cite> Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss). Especially the proposed Max4 model reduces FR rate to nearly half of the <cite>baseline</cite> in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_23",
  "x": "Figure 4 shows the ROC curves of Max4-Max7 models across different conditions. Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve. Max3 CE MP model also performs better than the <cite>baseline</cite> but not as good as Max4. Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than <cite>baseline.</cite> Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss). Especially the proposed Max4 model reduces FR rate to nearly half of the <cite>baseline</cite> in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_24",
  "x": "Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss). Especially the proposed Max4 model reduces FR rate to nearly half of the <cite>baseline</cite> in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments. ---------------------------------- **CONCLUSION** We presented smoothed max pooling loss for training keyword spotting model with improved optimizability. Experiments show that the proposed approach outperforms the <cite>baseline model</cite> with CE loss by relative 22%-54% across a variety of conditions. Further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the <cite>baseline</cite>.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_25",
  "x": "Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss). Especially the proposed Max4 model reduces FR rate to nearly half of the <cite>baseline</cite> in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments. ---------------------------------- **CONCLUSION** We presented smoothed max pooling loss for training keyword spotting model with improved optimizability. Experiments show that the proposed approach outperforms the <cite>baseline model</cite> with CE loss by relative 22%-54% across a variety of conditions. Further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the <cite>baseline</cite>.",
  "y": "differences"
 },
 {
  "id": "e75e14ff2812f34ff456eb472a36d2_0",
  "x": "Secondly, we address a number of Bayesian models ranging from latent variable model to VB inference (Chien and Chang, 2014; Chien and Chueh, 2011; Chien, 2015b) , MCMC sampling (Watanabe and Chien, 2015) and BNP learning (Chien, 2016; Chien, 2015a; Chien, 2018) for hierarchical, thematic and sparse topics from natural language. In the third part, a series of deep models including deep unfolding (Chien and Lee, 2018) , Bayesian RNN (Gal and Ghahramani, 2016; Chien and Ku, 2016) , sequence-to-sequence learning (Graves et al., 2006; Gehring et al., 2017) , CNN (<cite>Kalchbrenner et al., 2014</cite>; Xingjian et al., 2015; , GAN (Tsai and Chien, 2017) and VAE are introduced. The coffee break is arranged within this part. Next, the fourth part focuses on a variety of advanced studies which illustrate how deep Bayesian learning is developed to infer the sophisticated recurrent models for natural language understanding. In particular, the memory network Chien and Lin, 2018) , neural variational learning (Serban et al., 2017; Chung et al., 2015) , neural discrete representation (Jang et al., 2016; Maddison et al., 2016; van den Oord et al., 2017) , recurrent ladder network (Rasmus et al., 2015; Pr\u00e9mont-Schwarz et al., 2017; S\u00f8nderby et al., 2016) , stochastic neural network (Fraccaro et al., 2016; Goyal et al., 2017; Shabanian et al., 2017) , Markov recurrent neural network (Venkatraman et al., 2017; Kuo and Chien, 2018) , sequence GAN (Yu et al., 2017) and reinforcement learning (Tegho et al., 2017) are introduced in various deep models which open a window to more practical tasks, e.g. reading comprehension, sentence generation, dialogue system, question answering and machine translation. In the final part, we spotlight on some future directions for deep language understanding which can handle the challenges of big data, heterogeneous condition and dynamic system. In particular, deep learning, structural learning, temporal modeling, long history representation and stochastic learning are emphasized.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_0",
  "x": "Distributed representations of multimodal embeddings (Feng & Lapata, 2010) are receiving increasing attention recently in the machine learning literature, and techniques developed have found a wide spectrum of applications in the real world. These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary<cite> (Lazaridou et al., 2015</cite>; Glenberg & Robertson, 2000; ). As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; <cite>Lazaridou et al., 2015</cite>; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in<cite> Lazaridou et al. (2015)</cite> sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language. Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words. As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_1",
  "x": "These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary<cite> (Lazaridou et al., 2015</cite>; Glenberg & Robertson, 2000; ). As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; <cite>Lazaridou et al., 2015</cite>; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in<cite> Lazaridou et al. (2015)</cite> sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language. Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words. As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval. In this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a; b) , , and<cite> Lazaridou et al. (2015)</cite> .",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_2",
  "x": "We present extensive results of the representational properties of these embeddings on various word similarity benchmarks to show the promise of this approach. ---------------------------------- **INTRODUCTION** Distributed representations of multimodal embeddings (Feng & Lapata, 2010) are receiving increasing attention recently in the machine learning literature, and techniques developed have found a wide spectrum of applications in the real world. These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary<cite> (Lazaridou et al., 2015</cite>; Glenberg & Robertson, 2000; ). As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; <cite>Lazaridou et al., 2015</cite>; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in<cite> Lazaridou et al. (2015)</cite> sought to address many of the drawbacks of these models.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_3",
  "x": "In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language. Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words. As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval. In this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a; b) , , and<cite> Lazaridou et al. (2015)</cite> . Rather than adding a visual term to the linguistic training objective, we directly situate terms in a visual context by replacing relevant words with multimodal pseudowords, derived by composing the textual representations with convolutional features projected into the multimodal space. In this way, we further address the grounding problem of Glenberg & Robertson (2000) by incorpo-rating the word-level visual modality directly into the sentence context. This model represents an advancement of the existing literature surrounding multimodal skip-gram, as well as multimodal distributional semantic models in general, by greatly simplifying the method of situating the words in the visual context and reducing the number of hyperparameters to tune by directly incorporating multimodal words into the existing objective function and hiearchical softmax formulations of the skip-gram models.",
  "y": "extends"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_4",
  "x": "**RELATED WORK** In the last few years, there has been a wealth of literature on multimodal representational models. As explained in<cite> Lazaridou et al. (2015)</cite> , the majority of this literature focuses on constructing textual and visual representations independently and then combining them under some metrics. Bruni et al. (2014) utilize a direct approach to \"mixing\" the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition. The image vectors used here, though, are constructed using the bag-of-visual-words method. In Kiela & Bottou (2014) , the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text. Similarly, Frome et al. (2013) also utilizes the skip-gram architecture and convolutional features; however the two modalities are then combined using a natural similarity metric.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_5",
  "x": "In Kiela & Bottou (2014) , the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text. Similarly, Frome et al. (2013) also utilizes the skip-gram architecture and convolutional features; however the two modalities are then combined using a natural similarity metric. Other recent work has presented several methods for directly incorporating visual context in neural language models. In Xu et al. (2014) , word context is enhanced by global visual context; i.e., a single image is used as the context for the whole sentence (conversely, the sentence acts as a caption for the image). The multimodal skip-gram architecture proposed by<cite> Lazaridou et al. (2015)</cite> takes a more fine-grained approach by incorporating word-level visual context and concurrently training words to predict other text words in the window as well as their visual representation. Our model makes this approach even more explicit, by training the word vectors to predict an additive composition of the textual and visual context and thus constructing an implicit mapping between the textual and visual modalities. Finally, the work introduced in Hill & Korhonen (2014) employs a similar \"pseudoword\" architecture to that proposed here.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_6",
  "x": "For our text corpus, keeping with the existing literature, we use a preprocessed dump of Wikipedia 3 containing approximately 800M tokens. For the visual data, we use the image data from ILSVRC 2012 (Russakovsky et al., 2015) and the corresponding Wordnet hierarchy (Miller, 1995) to represent a word visually if the word or any of its hyponyms has an entry in Imagenet and occurs more than 500 times in the text corpus. This yields approximately 5,100 \"visual\" words. To construct the vectors for the visual representations, we follow a similar experimental set-up as that used by<cite> Lazaridou et al. (2015)</cite> . In each of the cases described above-centroid and hypersphere-, we randomly sample 100 images from the corresponding synsets of Imagenet for each visual word and use a pre-trained convolutional neural network as described in Krizhevsky et al. (2012) via the Caffe toolkit (Jia et al., 2014) to extract a 4096-dimensional vector representation of each image. We then treat the 100 vectors corresponding to each of the 5,100 visual words as clusters in the 4096-dimensional visual space. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_7",
  "x": "Using the results published by<cite> Lazaridou et al. (2015)</cite> and a target word embedding of 300, we compare our results to their MMSKIP-GRAM-A and MMSKIP-GRAM-B, which maximize the similarity of the textual and visual representations Table 1 : Spearman correlation between the generated multimodal similarities and the benchmark human judgments. In all cases, results are reported on the full set of word similarity pairs. under a max-margin framework.; the former constrains the dimensionality of the visual features to be the same as the word embeddings, while the latter learns an explicit mapping between the textual and visual spaces. We also include baseline results for pure-text skip-gram embeddings (SKIP-GRAM)). ---------------------------------- **RESULTS** The results for the human judgment experiments are presented in Table 1 .",
  "y": "uses"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_8",
  "x": "**NEURAL WEIGHT INITIALIZATION** On the other hand, when the mapping is quickly pretrained on existing distributed word representations, the results are greatly improved. In the cases of capturing general relatedness and pure visual similarity, the multimodal model of<cite> Lazaridou et al. (2015)</cite> performs better. However, in the case of capturing semantic word similarity, our model performs signficantly better than MMSKIP-GRAM-B (although it should be noted that these results are roughly on par with the benchmark authors (Silberer & Lapata, 2014) and a point below the non-mapping MMSKIP-GRAM-A). Although further work is needed to examine this result, the performance of the model in this case can be visualized through an example. Table 2 provides some insights on the changes made to the word embeddings as a result of the inclusion of visual information in the learning process. In the two visual instances, our model captures many of the same nuances as MMSKIP-GRAM-B over the SKIP-GRAM model: donuts are more similar to other types of food than to places where you find donuts and owls are more similar to other birds of prey than just woodland creatures.",
  "y": "differences"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_0",
  "x": "Image understanding, as another example, benefits from recursive modeling over structures, which yielded the state-of-the-art performance on tasks like scene segmentation (Socher et al., 2011) . In this paper, we extend LSTM to tree structures, in which we learn memory cells that can reflect the history memories of multiple child cells and hence multiple descendant cells. We call the model S-LSTM. Compared with previous recursive neural networks (<cite>Socher et al., 2013</cite>; 2012) , S-LSTM has the potentials of avoiding gradient vanishing and hence may model long-distance interaction over trees. This is a desirable characteristic as many of such structures are deep. S-LSTM can be considered as bringing the merits of a recursive neural network and a recurrent neural network togetherStanford Sentiment Tree Bank (<cite>Socher et al., 2013</cite>) to determine the sentiment for different granularities of phrases in a tree. <cite>The dataset</cite> has favorable properties: in addition to being a benchmark for much previous work, <cite>it</cite> provides with human annotations at all nodes of the trees, enabling us to comprehensively explore the properties of S-LSTM.",
  "y": "differences motivation"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_1",
  "x": "S-LSTM can be considered as bringing the merits of a recursive neural network and a recurrent neural network togetherStanford Sentiment Tree Bank (<cite>Socher et al., 2013</cite>) to determine the sentiment for different granularities of phrases in a tree. <cite>The dataset</cite> has favorable properties: in addition to being a benchmark for much previous work, <cite>it</cite> provides with human annotations at all nodes of the trees, enabling us to comprehensively explore the properties of S-LSTM. We experimentally show that S-LSTM outperforms a stateof-the-art recursive model by simply replacing the original tensor-enhanced composition with the S-LSTM memory block we propose here. We showed that utilizing the given structures is helpful in achieving a better performance than that without considering the structures. ---------------------------------- **RELATED WORK** Recursive neural networks Recursion is a fundamental process in different modalities.",
  "y": "motivation"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_2",
  "x": "**RELATED WORK** Recursive neural networks Recursion is a fundamental process in different modalities. In recent years, recursive neural networks (RvNN) have been introduced and demonstrated to achieve state-of-the-art performances on different problems such as semantic analysis in natural language processing and image segmentation (<cite>Socher et al., 2013</cite>; 2011) . <cite>These networks</cite> are defined over recursive tree structures-a tree node is a vector computed from its children. In a recursive fashion, the information from the leaf nodes of a tree and its internal nodes are combined in a bottom-up manner through the tree. Derivatives of errors are computed with backpropagation over structures (Goller & Kchler, 1996) . In addition, the literature has also included many other efforts of applying feedforward-based neural network over structures, including (Goller & Kchler, 1996; Chater, 1992; Starzyk et al.; Hammer et al., 2004) , amongst others.",
  "y": "background"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_3",
  "x": "Nevertheless, over the often deep structures, the networks are potentially subject to the vanishing gradient problem, resulting in difficulties in leveraging long-distance dependencies in the structures. In this paper, we propose the S-LSTM model that wires memory blocks in recursive structures. We compare our model with the RvNN models presented in (<cite>Socher et al., 2013</cite>) , as we directly replaced the tensor-enhanced composition layer at each tree node with a S-LSTM memory block. We show the advantages of our proposed model in achieving significantly better results. Recurrent neural networks and LSTM Unlike a feedforward network, a recurrent neural network (RNN) shares their hidden states across time. The sequential history is summarized in a hidden vector. RNN also suffers from the decaying of gradient, or less frequently, blowing-up of gradient problem.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_4",
  "x": "In (Irsoy & Cardie, 2014) , a deep recursive neural network is proposed . Nevertheless, over the often deep structures, the networks are potentially subject to the vanishing gradient problem, resulting in difficulties in leveraging long-distance dependencies in the structures. In this paper, we propose the S-LSTM model that wires memory blocks in recursive structures. We compare our model with the RvNN models presented in (<cite>Socher et al., 2013</cite>) , as we directly replaced the tensor-enhanced composition layer at each tree node with a S-LSTM memory block. We show the advantages of our proposed model in achieving significantly better results. Recurrent neural networks and LSTM Unlike a feedforward network, a recurrent neural network (RNN) shares their hidden states across time. The sequential history is summarized in a hidden vector.",
  "y": "differences"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_5",
  "x": "The small circle (\"\u2022\") or short line (\"\u2212\") at each arrowhead indicates a pass or block of information, respectively, while in the real model the gating is a soft version of gating. Figure 1 is composed of a S-LSTM memory block. We present a specific wiring of such a block in Figure 2 . Each memory block contains one input gate and one output gate. The number of forget gates depends on the structure, i.e., the number of children of a node. In this paper, we assume there are two children at each nodes, same as in (<cite>Socher et al., 2013</cite>) and therefore we use <cite>their data</cite> in our experiments. That is, we have two forget gates.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_6",
  "x": "The subscripts of the weight matrices indicate what they are used for. For example, W ho is a matrix mapping a hidden vector to an output gate. Backpropagation over structures During training, the gradient of the objective function with respect to each parameter can be calculated efficiently via backpropagation over structures (Goller & Kchler, 1996; <cite>Socher et al., 2013</cite>) . The major difference from that of (<cite>Socher et al., 2013</cite> ) is we use LSTM-like backpropagation, where unlike a regular LSTM, pass of error needs to discriminate between the left and right children, or in a topology with more than two children, needs to discriminate between children. Obtaining the backprop formulas is tedious but we list them below to facilitate duplication of our work 2 . We will discuss the specific objective function later in experiments. For each memory block, assume that the error passed to the hidden vector is \u01eb",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_7",
  "x": "The major difference from that of (<cite>Socher et al., 2013</cite> ) is we use LSTM-like backpropagation, where unlike a regular LSTM, pass of error needs to discriminate between the left and right children, or in a topology with more than two children, needs to discriminate between children. Obtaining the backprop formulas is tedious but we list them below to facilitate duplication of our work 2 . We will discuss the specific objective function later in experiments. For each memory block, assume that the error passed to the hidden vector is \u01eb 2 The code will be published at www.icml-placeholderonly.com where \u03c3 \u2032 (x) is the element-wise derivative of the logistic function over vector x. Since it can be computed with the activation of x, we abuse the notation a bit to write it over the activated vectors in these equations. \u01eb c t is the derivative over the cell vector. So if the current node is the left child of its parent, we use Equation (13) to calculate \u01eb c t , otherwise Formula (14) is used:",
  "y": "differences"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_8",
  "x": "We checked the correctness of the S-LSTM implementation with the standard approximated gradient approach. Objective over trees The objective function defined over structures can be complicated, which could consider the output structures depending on the properties of problem. Following (<cite>Socher et al., 2013</cite>) , the overall objective function we used to learn S-LSTM in this paper is simply minimizing the overall cross-entropy errors and a sum of that at all nodes. ---------------------------------- **EXPERIMENT SET-UP** As discussed earlier, recursion is a basic process inherent to many problems. In this paper, we leverage the proposed model to solve semantic composition for the meanings of pieces of text, a fundamental problem in understanding human languages.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_9",
  "x": "As discussed earlier, recursion is a basic process inherent to many problems. In this paper, we leverage the proposed model to solve semantic composition for the meanings of pieces of text, a fundamental problem in understanding human languages. We specifically attempt to determine the sentiment of different granularities of phrases in a tree, within the Stanford Sentiment Tree Bank benchmark data (<cite>Socher et al., 2013</cite>) . In obtaining the sentiment of a long piece of text, early work often factorized the problem to consider smaller pieces of component words or phrases with bag-of-words or bag-ofphrases models (Pang & Lee, 2008; Liu & Zhang, 2012) . More recent work has started to model composition (Moilanen & Pulman, 2007; Choi & Cardie, 2008; Socher et al., 2012; Kalchbrenner et al., 2014) , a more principled approach to modeling the formation of semantics. In this paper, we put the proposed LSTM memory blocks at tree nodes-we replaced the tensorenhanced composition layer at each tree node presented in (<cite>Socher et al., 2013</cite> ) with a S-LSTM memory block. We used the <cite>same dataset</cite>, the Stanford Sentiment Tree Bank, to evaluate the performances of the models.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_10",
  "x": "In this paper, we leverage the proposed model to solve semantic composition for the meanings of pieces of text, a fundamental problem in understanding human languages. We specifically attempt to determine the sentiment of different granularities of phrases in a tree, within the Stanford Sentiment Tree Bank benchmark data (<cite>Socher et al., 2013</cite>) . In obtaining the sentiment of a long piece of text, early work often factorized the problem to consider smaller pieces of component words or phrases with bag-of-words or bag-ofphrases models (Pang & Lee, 2008; Liu & Zhang, 2012) . More recent work has started to model composition (Moilanen & Pulman, 2007; Choi & Cardie, 2008; Socher et al., 2012; Kalchbrenner et al., 2014) , a more principled approach to modeling the formation of semantics. In this paper, we put the proposed LSTM memory blocks at tree nodes-we replaced the tensorenhanced composition layer at each tree node presented in (<cite>Socher et al., 2013</cite> ) with a S-LSTM memory block. We used the <cite>same dataset</cite>, the Stanford Sentiment Tree Bank, to evaluate the performances of the models. In addition to being a benchmark for much previous work, <cite>the data</cite> provide with human annotations at all nodes of the trees, facilitating a more comprehensive exploration of the properties of S-LSTM.",
  "y": "extends"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_11",
  "x": "In this paper, we leverage the proposed model to solve semantic composition for the meanings of pieces of text, a fundamental problem in understanding human languages. We specifically attempt to determine the sentiment of different granularities of phrases in a tree, within the Stanford Sentiment Tree Bank benchmark data (<cite>Socher et al., 2013</cite>) . In obtaining the sentiment of a long piece of text, early work often factorized the problem to consider smaller pieces of component words or phrases with bag-of-words or bag-ofphrases models (Pang & Lee, 2008; Liu & Zhang, 2012) . More recent work has started to model composition (Moilanen & Pulman, 2007; Choi & Cardie, 2008; Socher et al., 2012; Kalchbrenner et al., 2014) , a more principled approach to modeling the formation of semantics. In this paper, we put the proposed LSTM memory blocks at tree nodes-we replaced the tensorenhanced composition layer at each tree node presented in (<cite>Socher et al., 2013</cite> ) with a S-LSTM memory block. We used the <cite>same dataset</cite>, the Stanford Sentiment Tree Bank, to evaluate the performances of the models. In addition to being a benchmark for much previous work, <cite>the data</cite> provide with human annotations at all nodes of the trees, facilitating a more comprehensive exploration of the properties of S-LSTM.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_12",
  "x": "The Stanford Sentiment Tree Bank (<cite>Socher et al., 2013</cite>) contains about 11,800 sentences from the movie reviews that were originally discussed in (Pang & Lee, 2005) . The sentences were parsed with the Stanford parser (Klein & Manning, 2003) . Phrases at all the tree nodes were manually annotated with sentiment values. We use the same split of the training and test data as in (<cite>Socher et al., 2013</cite>) to predict the sentiment categories of the roots (sentences) and all phrases (including sentences). For the root sentiment, the training, development, and test sentences are 8544, 1101, and 2210, respectively. The phrase sentiment task includes 318582, 41447, and 82600 phrases for the three sets. Following (<cite>Socher et al., 2013</cite>) , we also use the classification accuracy to measure the performances.",
  "y": "background"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_13",
  "x": "**DATA SET** The Stanford Sentiment Tree Bank (<cite>Socher et al., 2013</cite>) contains about 11,800 sentences from the movie reviews that were originally discussed in (Pang & Lee, 2005) . The sentences were parsed with the Stanford parser (Klein & Manning, 2003) . Phrases at all the tree nodes were manually annotated with sentiment values. We use the same split of the training and test data as in (<cite>Socher et al., 2013</cite>) to predict the sentiment categories of the roots (sentences) and all phrases (including sentences). For the root sentiment, the training, development, and test sentences are 8544, 1101, and 2210, respectively. The phrase sentiment task includes 318582, 41447, and 82600 phrases for the three sets.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_14",
  "x": "The phrase sentiment task includes 318582, 41447, and 82600 phrases for the three sets. Following (<cite>Socher et al., 2013</cite>) , we also use the classification accuracy to measure the performances. ---------------------------------- **TRAINING DETAILS** As mentioned before, we follow (<cite>Socher et al., 2013</cite>) to minimize the cross-entropy error for all nodes or for roots only, depending on specific experiment settings. For all phrases, the error is calculated as a regularized sum: where y seni \u2208 R c\u00d71 is predicted distribution and t i \u2208 R c\u00d71 the target distribution.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_15",
  "x": "We use the same split of the training and test data as in (<cite>Socher et al., 2013</cite>) to predict the sentiment categories of the roots (sentences) and all phrases (including sentences). For the root sentiment, the training, development, and test sentences are 8544, 1101, and 2210, respectively. The phrase sentiment task includes 318582, 41447, and 82600 phrases for the three sets. Following (<cite>Socher et al., 2013</cite>) , we also use the classification accuracy to measure the performances. ---------------------------------- **TRAINING DETAILS** As mentioned before, we follow (<cite>Socher et al., 2013</cite>) to minimize the cross-entropy error for all nodes or for roots only, depending on specific experiment settings.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_16",
  "x": "**TRAINING DETAILS** As mentioned before, we follow (<cite>Socher et al., 2013</cite>) to minimize the cross-entropy error for all nodes or for roots only, depending on specific experiment settings. For all phrases, the error is calculated as a regularized sum: where y seni \u2208 R c\u00d71 is predicted distribution and t i \u2208 R c\u00d71 the target distribution. c is the number of classes or categories, and j \u2208 c denotes the j-th element of the multinomial target distribution; i iterates over nodes, \u03b8 are model parameters, and \u03bb is a regularization parameter. We tuned our model against the development data set as split in (<cite>Socher et al., 2013</cite>) . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_17",
  "x": "**DEFAULT SETTING** In the default setting, we conducted experiments as in (<cite>Socher et al., 2013</cite>) . Table 1 shows the accuracies of different models on the test set of the <cite>Stanford Sentiment Tree Bank</cite>. We present the results on 5-category sentiment prediction at both the sentence level (i.e., the ROOTS column in the table) and for all phrases including roots (the PHRASES column) 3 . In Table 1 , NB and SVM are naive Bayes and support vector machine classifiers, respectively; <cite>RvNN</cite> corresponds to RNN in (<cite>Socher et al., 2013</cite>) . As described earlier, we refer to recursive neural networks to as RvNN to avoid confusion with recurrent neural networks. RNTN is different from <cite>RvNN</cite> in that when merging two nodes to obtain the hidden vector of their parent, tensor is used to obtain the second-degree polynomial interactions.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_20",
  "x": "**DEFAULT SETTING** In the default setting, we conducted experiments as in (<cite>Socher et al., 2013</cite>) . Table 1 shows the accuracies of different models on the test set of the <cite>Stanford Sentiment Tree Bank</cite>. We present the results on 5-category sentiment prediction at both the sentence level (i.e., the ROOTS column in the table) and for all phrases including roots (the PHRASES column) 3 . In Table 1 , NB and SVM are naive Bayes and support vector machine classifiers, respectively; <cite>RvNN</cite> corresponds to RNN in (<cite>Socher et al., 2013</cite>) . As described earlier, we refer to recursive neural networks to as RvNN to avoid confusion with recurrent neural networks. RNTN is different from <cite>RvNN</cite> in that when merging two nodes to obtain the hidden vector of their parent, tensor is used to obtain the second-degree polynomial interactions.",
  "y": "differences"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_21",
  "x": "Table 1 showed that S-LSTM achieved the best predictive performance, when compared to all the models reported in (<cite>Socher et al., 2013</cite>) . The S-LSTM results reported here were obtained by setting the size of the hidden units to be 100, batch size to be 10, and learning rate to be 0.1. In our experiments, we only tuned these hyper-parameters, and we feel that more finer tuning, such as discriminating the classification weights between the leaves (word embedding) and other nodes, using different numbers of hidden units for the memory blocks (e.g., for the hidden layers of words), or different initializations of word embedding, may further improve the performances reported here. To evaluate the S-SLTM model's convergence behavior, Figure 3 depicts the converging time during training. More specifically, we show two sub-figures: one for roots (uppermuch less parameters than RNTN and the forward and backward propagation can be computed efficiently. ---------------------------------- **MORE REAL-LIFE SETTINGS**",
  "y": "differences"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_23",
  "x": "ROOT LBLS besides the model names stands for root labels; that is, only the gold labels of the sentence level are used to train the model. In most sentiment analysis circumstances, phrase level annotations are not available: most nodes in a tree are fragments that may not be that interesting; e.g., the fragment \"of a good movie\" 4 . Also, annotating all phrases is expensive. However, these should not be regarded as comments on the value of the <cite>Sentiment Tree Bank</cite>. Detailed annotations in the <cite>tree bank</cite> enable much interesting work to be possible, e.g., the study of the effect of negation in changing sentiment (Zhu et al., 2014) . The second setting, corresponding to model (3) and (4) in Table 2 , is only slightly different, in which we keep annotation for the tree leafs as well, to simulate that a sentiment lexicon is available and it covers all leafs (words) (LEAF LBLS along the side of the model names stands for leaf labels), and so there is no out-of-vocabulary concern. Using real sentiment lexicons is expected to have a performance between the two settings here.",
  "y": "future_work"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_24",
  "x": "Explicit structures vs. no structures Some efforts in the literature attempt to learn distributed representation by utilizing input structures when available, and others prefer to assume chain-structured recurrent neural networks can actually capture the structures implicitly though a linear coding process. In this paper, we attempt to give some empirical evidences in our experiment setting by comparing several different models. First, a special case for the S-LSTM model is considered, in which no sentential structures are given. Instead, words are read from left to right and combined in that order. We call it left recursive S-LSTM, or S- LSTM-LR in short. Similarly, we also experimented with a right recursive S-LSTM, S-LSTM-RR, in which words are read from right to left instead. Since for these models, phrase-level training signals are not available-the nodes here do not correspond to that in the <cite>original Standford Sentiment Tree Bank</cite>, but the roots and leafs annotations are still the same, so we run two versions of our experiments: one uses only training signals from roots and the other includes also leaf annotations.",
  "y": "similarities differences"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_0",
  "x": "The system generates all possible splits and then provides a ranking of various splits, based on probabilistic ranking inferred from a dataset of 25000 split points. Using the same dataset, Natarajan and Charniak (2011) proposed a sandhi splitter for Sanskrit. The method is an extension of Bayesian word segmentation approach by Goldwater et al. (2006) . <cite>Krishna et al. (2016)</cite> is currently the state of the art in Sanskrit word segmentation. The system treats the problem as an iterative query expansion problem. Using a shallow parser for Sanskrit (Goyal et al., 2012) , an input sentence is first converted to a graph of possible candidates and desirable nodes are iteratively selected using Path Constrained Random Walks (Lao and Cohen, 2010) . To further catalyse the research in word segmentation for Sanskrit, Krishna et al. (2017) has released a dataset for the word segmentation task.",
  "y": "background"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_1",
  "x": "To further catalyse the research in word segmentation for Sanskrit, Krishna et al. (2017) has released a dataset for the word segmentation task. The work releases a dataset of 119,000 sentences in Sanskrit along with the lexical and morphological analysis from a shallow parser. The work emphasises the need for not just predicting the inflected word form but also the prediction of the associated morphological information of the word. The additional information will be beneficial in further processing of Sanskrit texts, such as Dependency parsing or summarisation (Krishna et al., 2017) .So far, no system successfully predicts the morphological information of the words in addition to the final word form. Though <cite>Krishna et al. (2016)</cite> has designed their system with this requirement in mind and outlined the possible extension of <cite>their</cite> system for the purpose, <cite>the system</cite> currently only predicts the final word-form. ---------------------------------- **METHOD**",
  "y": "motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_2",
  "x": "It needs to be noted that the average length of a string in the Digital Corpus of Sanskrit is 6.7 (<cite>Krishna et al., 2016</cite>) . The proportion of sentences with more than 10 words in our dataset is less than 1 %. The test dataset has slightly more than 4 % sentences with 10 or more words. The 'segSeq2Seq' model performs better than the state of the art for both Precision and Recall for strings with less than or equal to 6 words. Figure 2a shows the proportion of sentences in the test data based on the frequency of words in it. Figure 2b shows the proportion of strings in the test dataset based on the number of words in the strings. Our systems attnSegSeq2Seq takes overall 11 hours 40 minutes and for 80 epochs in a 'Titan X' 12GB GPU memory, 3584 GPU Cores, 62GB RAM and Intel Xeon CPU E5-2620 2.40GHz system.",
  "y": "background"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_3",
  "x": "The word-level indexing in retrieval systems is often affected by phonetic transformations in words due to sandhi. String matching approaches often result in low precision results. Though this is not semantically meaningful it is lexically valid. Such tools are put to use by some of the existing systems (<cite>Krishna et al., 2016</cite>; Mittal, 2010 ) to obtain additional morphological or syntactic information about the sentences. This limits the scalability of <cite>those systems</cite>, as they cannot handle out of vocabulary words. Scalability of <cite>such systems</cite> is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing. The systems by <cite>Krishna et al. (2016)</cite> and Krishna et al. (2017) assume that the parser by Goyal et al. (2012) , identifies all the possible candidate chunks. Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence.",
  "y": "motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_4",
  "x": "Using a lexicon driven system might alleviate the said issues, but can lead to possible splits which are not semantically compatible. For parame\u015bvarah . ', it can be split as 'parama' (ultimate), '\u015bva' (dog) and 'rah . ' (to facilitate). Though this is not semantically meaningful it is lexically valid. Such tools are put to use by some of the existing systems (<cite>Krishna et al., 2016</cite>; Mittal, 2010 ) to obtain additional morphological or syntactic information about the sentences. This limits the scalability of <cite>those systems</cite>, as they cannot handle out of vocabulary words. Scalability of <cite>such systems</cite> is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing.",
  "y": "background"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_5",
  "x": "Using a lexicon driven system might alleviate the said issues, but can lead to possible splits which are not semantically compatible. Such tools are put to use by some of the existing systems (<cite>Krishna et al., 2016</cite>; Mittal, 2010 ) to obtain additional morphological or syntactic information about the sentences. This limits the scalability of <cite>those systems</cite>, as they cannot handle out of vocabulary words. Scalability of <cite>such systems</cite> is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing. The systems by <cite>Krishna et al. (2016)</cite> and Krishna et al. (2017) assume that the parser by Goyal et al. (2012) , identifies all the possible candidate chunks.",
  "y": "background"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_6",
  "x": "Using a lexicon driven system might alleviate the said issues, but can lead to possible splits which are not semantically compatible. Such tools are put to use by some of the existing systems (<cite>Krishna et al., 2016</cite>; Mittal, 2010 ) to obtain additional morphological or syntactic information about the sentences. This limits the scalability of <cite>those systems</cite>, as they cannot handle out of vocabulary words. Scalability of <cite>such systems</cite> is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing.",
  "y": "motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_7",
  "x": "Such tools are put to use by some of the existing systems (<cite>Krishna et al., 2016</cite>; Mittal, 2010 ) to obtain additional morphological or syntactic information about the sentences. This limits the scalability of <cite>those systems</cite>, as they cannot handle out of vocabulary words. Scalability of <cite>such systems</cite> is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing. The systems by <cite>Krishna et al. (2016)</cite> and Krishna et al. (2017) assume that the parser by Goyal et al. (2012) , identifies all the possible candidate chunks. Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence. Krishna et al. (2017) states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit. The segmentation task is seen as a means and not an end itself.",
  "y": "motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_8",
  "x": "The systems by <cite>Krishna et al. (2016)</cite> and Krishna et al. (2017) assume that the parser by Goyal et al. (2012) , identifies all the possible candidate chunks. Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence. Krishna et al. (2017) states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit. The segmentation task is seen as a means and not an end itself. Here, we overlook this aspect and see the segmentation task as an end in itself. So we achieve scalability at the cost of missing out on providing valuable linguistic information. Models that use linguistic resources are at an advantage here.",
  "y": "background"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_9",
  "x": "Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence. Krishna et al. (2017) states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit. The segmentation task is seen as a means and not an end itself. Here, we overlook this aspect and see the segmentation task as an end in itself. So we achieve scalability at the cost of missing out on providing valuable linguistic information. Models that use linguistic resources are at an advantage here. Those systems such as <cite>Krishna et al. (2016)</cite> can be used to identify the morphological tags of the system as <cite>they</cite> currently store the morphological information of predicted candidates, but do not use them for evaluation as of now.",
  "y": "motivation background"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_10",
  "x": "Those systems such as <cite>Krishna et al. (2016)</cite> can be used to identify the morphological tags of the system as <cite>they</cite> currently store the morphological information of predicted candidates, but do not use them for evaluation as of now. Currently, no system exists that performs the prediction of wordform and morphological information jointly for Sanskrit. But, we leave this work for future.",
  "y": "motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_11",
  "x": "The segmentation task is seen as a means and not an end itself. Here, we overlook this aspect and see the segmentation task as an end in itself. So we achieve scalability at the cost of missing out on providing valuable linguistic information. Models that use linguistic resources are at an advantage here. Those systems such as <cite>Krishna et al. (2016)</cite> can be used to identify the morphological tags of the system as <cite>they</cite> currently store the morphological information of predicted candidates, but do not use them for evaluation as of now. Currently, no system exists that performs the prediction of wordform and morphological information jointly for Sanskrit. In our case, since we learn a new vocabulary altogether, the real word boundaries are opaque to the system.",
  "y": "differences"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_12",
  "x": "But, we leave this work for future. ---------------------------------- **CONCLUSION** In this work we presented a model for word segmentation in Sanskrit using a purely engineering based appraoch. Our model with attention outperforms the current state of the art (<cite>Krishna et al., 2016</cite>) . Since, we tackle the problem with a non-linguistic approach, we hope to extend the work to other Indic languages as well where sandhi is prevalent such as Hindi, Marathi, Malayalam, Telugu etc. Since we find that the inclusion of attention is highly beneficial in improving the performance of the system, we intend to experiment with recent advances in the encoder-decoder architectures, such as Vaswani et al. (2017) and Gehring et al. (2017) , where different novel approaches in using attention are experimented with.",
  "y": "differences"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_13",
  "x": "Our model with attention outperforms the current state of the art (<cite>Krishna et al., 2016</cite>) . Since, we tackle the problem with a non-linguistic approach, we hope to extend the work to other Indic languages as well where sandhi is prevalent such as Hindi, Marathi, Malayalam, Telugu etc. Since we find that the inclusion of attention is highly beneficial in improving the performance of the system, we intend to experiment with recent advances in the encoder-decoder architectures, such as Vaswani et al. (2017) and Gehring et al. (2017) , where different novel approaches in using attention are experimented with. Our experiments in line with the measures reported in <cite>Krishna et al. (2016)</cite> show that our system performs robustly across strings of varying word size. ---------------------------------- **CODE AND DATASET** All our working code can be downloaded at https: //github.com/cvikasreddy/skt.",
  "y": "uses"
 },
 {
  "id": "eb591565efc03df1706710218a8f19_0",
  "x": "Learning knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, <cite>5,</cite> 6] . How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, 9 ]. The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather",
  "y": "background"
 },
 {
  "id": "eb591565efc03df1706710218a8f19_1",
  "x": "The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather ---------------------------------- **** knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, <cite>5,</cite> 6] .",
  "y": "background"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_0",
  "x": "In particular, we use methods proposed by Marie and Fujita (2018) ,<cite> Artetxe et al. (2018b)</cite> , and Lample et al. (2018) . These methods are based on phrase-based statistical machine translation (SMT) and phrase table refinements. Forward refinement used by Marie and Fujita (2018) simply augments a learner corpus with automatic corrections. We also use forward refinement for improvement of phrase table. Unsupervised MT techniques do not require a parallel but a comparable corpus as training data. Therefore, we use comparable translated texts using Google Translation as the source-side data. Specifically, we use News Crawl written in English as target-side data and News Crawl written in another language translated into English as source-side data.",
  "y": "uses"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_1",
  "x": "The experimental results show that our system achieved an F 0.5 score of 28.31 points. 2 Unsupervised GEC Algorithm 1 shows the pseudocode for unsupervised GEC. This code is derived from<cite> Artetxe et al. (2018b)</cite> . First, the cross-lingual phrase embeddings are acquired. Second, a phrase table is created based on these cross-lingual embeddings. Third, the phrase table is combined with a language model trained by monolingual data to initialize a phrase-based SMT system. Finally, the SMT system is updated through iterative forwardtranslation.",
  "y": "uses"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_2",
  "x": "The score of the lexical translation model was calculated based on the product of respective phrase translation probabilities. \u03f5 is a constant term for the case where no alignments are found. As in<cite> Artetxe et al. (2018b)</cite> , the term was set to 0.001. The backward lexical translation probability lex(e|f ) is calculated in a similar manner. Refinement of SMT system The phrase table created is considered to include noisy phrase pairs. Therefore, we update the phrase table using an SMT system. The SMT system trained on synthetic data eliminates the noisy phrase pairs using 2 As in<cite> Artetxe et al. (2018b)</cite> , \u03c4 is estimated by maximizing the phrase translation probability between an embedding and the nearest embedding on the opposite side.",
  "y": "similarities uses"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_3",
  "x": "The backward lexical translation probability lex(e|f ) is calculated in a similar manner. Refinement of SMT system The phrase table created is considered to include noisy phrase pairs. Therefore, we update the phrase table using an SMT system. The SMT system trained on synthetic data eliminates the noisy phrase pairs using 2 As in<cite> Artetxe et al. (2018b)</cite> , \u03c4 is estimated by maximizing the phrase translation probability between an embedding and the nearest embedding on the opposite side. language models trained on the target-side corpus. This process corresponds to lines 6-10 in Algorithm 1. The phrase table is refined with forward refinement (Marie and Fujita, 2018) .",
  "y": "similarities uses"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_4",
  "x": "In fact, Lample et al. (2018) ,<cite> Artetxe et al. (2018b)</cite> and Marie and Fujita (2018) use the News Crawl of source and target language as training data. To make a comparable corpus for GEC, we use translated texts using Google Translation as the source-side data. Specifically, we use Finnish News Crawl translated into English as source-side. English News Crawl is used as the target-side as is. Finnish data is used because Finnish is not similar to English. This translated data does not include misspelled words. To address these words, we use a spell checker as a preprocessing step before inference.",
  "y": "background"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_5",
  "x": "These data are tokenized by spaCy v1.9.0 6 and the en_core_web_sm-1.2.0 model. We used moses truecaser for the training data; this truecaser model is learned from processed English News Crawl. We used byte-pair-encoding (Sennrich et al., 2016) learned from processed English News Crawl; the number of operations is 50K. The implementation proposed by<cite> Artetxe et al. (2018b)</cite> 7 was modified to conduct the experiments. Specifically, some features were added; word-level Levenshtein distance, word-, and character-level edit operation, operation sequence model, (Durrani et al., 2013) 8 and 9-gram word class language model, similar to Grundkiewicz and Junczys-Dowmunt (2018) without sparse features. Word class language model was trained with One Billion Word Benchmark data; the number of classes is 200, and the word class was estimated with fastText (Bojanowski et al., 2017) . The distortion feature was not used.",
  "y": "extends"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_6",
  "x": "---------------------------------- **RELATED WORK** Unsupervised Machine Translation Studies on unsupervised methods have been conducted for both NMT (Lample et al., 2018; Marie and Fujita, 2018) and SMT <cite>(Artetxe et al., 2018b</cite> Table 4 : Error types for which our best system corrected errors well or mostly did not correct on the dev data. Top2 denotes the top two errors, and Bottom2 denotes the lowest two errors in terms of the F 0.5 10 . this study, we apply the USMT method of<cite> Artetxe et al. (2018b)</cite> and Marie and Fujita (2018) to GEC. The UNMT method (Lample et al., 2018) was ineffective under the GEC setting in our preliminary experiments. GEC with NMT/SMT Several studies that introduce sequence-to-sequence models in GEC heavily rely on large amounts of training data.",
  "y": "background"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_7",
  "x": "**RELATED WORK** Unsupervised Machine Translation Studies on unsupervised methods have been conducted for both NMT (Lample et al., 2018; Marie and Fujita, 2018) and SMT <cite>(Artetxe et al., 2018b</cite> Table 4 : Error types for which our best system corrected errors well or mostly did not correct on the dev data. Top2 denotes the top two errors, and Bottom2 denotes the lowest two errors in terms of the F 0.5 10 . this study, we apply the USMT method of<cite> Artetxe et al. (2018b)</cite> and Marie and Fujita (2018) to GEC. The UNMT method (Lample et al., 2018) was ineffective under the GEC setting in our preliminary experiments. GEC with NMT/SMT Several studies that introduce sequence-to-sequence models in GEC heavily rely on large amounts of training data. Ge et al. (2018) , who presented state-of-the-art results in GEC, proposed a supervised NMT method trained on corpora of a total 5.4 M sentence pairs.",
  "y": "uses"
 },
 {
  "id": "ec0ae4e56c069e3efb4a2dc12199cd_0",
  "x": "Active learning (AL) copes with this problem as it intelligently selects the data to be labeled. It is a sampling strategy where the learner has control over the training material to be manually annotated by selecting those examples which are of high utility for the learning process. AL has been successfully applied to speed up the annotation process for many NLP tasks without sacrificing annotation quality (Engelson and Dagan, 1996; Ngai and Yarowsky, 2000; Hwa, 2001; <cite>Tomanek et al., 2007a)</cite> . Once we decide to use AL for meta-data annotation and a reasonable, stable level of annotation quality is reachedafter having run through only a fraction of the documents compared with the traditional annotation approach where a randomly and independently selected amount of documents is sequentially annotated -an obvious question turns up: When do we stop the annotation process to cash in the time savings? Stopping after a certain amount of time has elapsed or a certain amount of data has been annotated is clearly not the best choice since such criteria, easily applicable though, do not take into account how well a classifier trained on the annotated data really performs. An optimal stopping condition for any annotation would be to locate that point in time when no further improvement in terms of classifier performance can be achieved by additional annotations. Since learning curves show the classifier performance at different time steps, i.e., for different amounts of annotated training examples, we can observe that progression.",
  "y": "background"
 },
 {
  "id": "ec0ae4e56c069e3efb4a2dc12199cd_1",
  "x": "A committee consists of k classifiers of the same type trained on different subsets of the already labeled (training) data. Each committee member then makes its predictions on the pool of unlabeled examples, and those examples on which the committee members express the highest disagreement are considered most informative for learning and are thus selected for manual annotation. To calculate the disagreement among the committee members several metrics have been proposed including the vote entropy (Engelson and Dagan, 1996) as possibly the most well-known one. Our approach to approximating the learning curve is based on the disagreement within a committee. However, it is independent of the actual metric used to calculate the disagreement. Although in our experiments we considered the NLP task of named entity recognition (NER) only, our approach is not limited to this scenario and can be expected to be applicable to other tasks as well. In <cite>Tomanek et al. (2007a)</cite> we introduced the selection agreement (SA) curve -the average agreement amongst the selected examples plotted over time.",
  "y": "background"
 },
 {
  "id": "ec0ae4e56c069e3efb4a2dc12199cd_2",
  "x": "---------------------------------- **EXPERIMENTAL SETTINGS** For our experiments on approximating the learning curves for AL-based selection, we chose named entity recognition (NER) as the annotation task in focus. We employed the committee-based AL approach described in <cite>Tomanek et al. (2007a)</cite> . The committee consists of k = 3 Maximum Entropy (ME) classifiers (Berger et al., 1996) . In each AL iteration, each classifier is trained on a randomly , L being the set of all examples seen so far.",
  "y": "uses"
 },
 {
  "id": "ec0ae4e56c069e3efb4a2dc12199cd_3",
  "x": "We have already shown that in this scenario, ME classifiers perform equally well for AL-driven selection as CRFs when using the same features. This effect is truly beneficial, especially for real-world annotation projects, due to much lower training times and, by this, shorter annotator idle times <cite>(Tomanek et al., 2007a)</cite> . For the AL simulation, we employed two simulation corpora: The CONLL corpus, based on the English data set of the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) , which consists of newspaper articles annotated with respect to person, location, and organisation entities. This pool consists of about 14,000 sentences. As validation set and as gold standard for plotting the learning curve we used CoNLL's evaluation corpus which sums up to 3,453 sentences. The PBVAR corpus consists of biomedical abstracts and was derived from the PENNBIOIE corpus (Kulick et al., 2004) by keeping only those annotations related to variation event mentions. We have randomly split this corpus into a pool set and a validation/gold set.",
  "y": "background"
 },
 {
  "id": "ee66681690f2c92fe705a09bf7015d_0",
  "x": "First, our system produces better k-best transliterations than related approaches by training on multiple hypotheses ranked according to a userspecified loss function (Levenshtein edit distance). Hence, our method achieves a 19.2% error reduction in 5-best performance over a baseline only trained with 1-best transliterations. This is especially helpful when machine transliteration is part of a larger machine translation or information retrieval pipeline since additional sentence context can be used to choose the best among top-K transliterations. Second, our training procedure accounts for noise and non-separability in the data. Therefore, our transliteration system would work well in cases where person names were misspelled or in cases in which a single name had many reasonable translations in the foreign language. The training algorithm we propose in this paper is based on the K-best MIRA algorithm which has been used earlier in structured prediction problems (McDonald et al., 2005a; McDonald et al., 2005b ). Our results demonstrate a significant improvement in accuracy of 7.2% over a statistical machine translation (SMT) system (Zens et al., 2005) and of 2.2% over a perceptron-based edit model <cite>(Freitag and Khadivi, 2007)</cite> .",
  "y": "background differences"
 },
 {
  "id": "ee66681690f2c92fe705a09bf7015d_1",
  "x": "for a parameter vector w and a feature vector \u03a6(a, e, f ). Furthermore, let \u03a6(a, e, f ) = l k=1 \u03c6(a k , e, i, f , j) be the sum of feature vectors associated with individual alignment operations. Here i, j are positions in sequences e, f after performing operations a k 1 . For fixed sequences e and f the function s(e, f ) can be efficiently computed using a dynamic programming algorithm, Given a source sequence f computing the best scoring target sequence e = arg max e \u2032 s(e \u2032 , f ) among all possible sequences E * requires a beam search procedure <cite>(Freitag and Khadivi, 2007)</cite> . This procedure can also be used to produce K-best target sequences . In this paper, we employ the same features as those used by<cite> Freitag and Khadivi (2007)</cite> .",
  "y": "background uses"
 },
 {
  "id": "ee66681690f2c92fe705a09bf7015d_2",
  "x": "Here i, j are positions in sequences e, f after performing operations a k 1 . For fixed sequences e and f the function s(e, f ) can be efficiently computed using a dynamic programming algorithm, Given a source sequence f computing the best scoring target sequence e = arg max e \u2032 s(e \u2032 , f ) among all possible sequences E * requires a beam search procedure <cite>(Freitag and Khadivi, 2007)</cite> . This procedure can also be used to produce K-best target sequences . In this paper, we employ the same features as those used by<cite> Freitag and Khadivi (2007)</cite> . All local feature functions \u03c6(a k , e, i, f , j) are conjunctions of the alignment operation a k and forward or backward-looking character m-grams in sequences e and f at positions i and j respectively. For the source sequence f both forward and backwardlooking m-gram features are included.",
  "y": "uses"
 },
 {
  "id": "ee66681690f2c92fe705a09bf7015d_3",
  "x": "The data set consists of Arabic names in an ASCII-based alphabet and its English rendering. Table 1 shows a few examples of Arabic-English pairs in our data set. We use the same training/development/testing (8084/1000/1000) set as the one used in a previous benchmark study <cite>(Freitag and Khadivi, 2007)</cite> . The development and testing data were obtained by randomly removing entries from the training data. The absence of short vowels (e.g. \"a\" in NB\"I, nab'i ), doubled consonants (e.g. \"ww\" in FWAL, fawwal ) and other diacritics in Arabic make the transliteration a hard problem. Therefore, it is hard to achieve perfect accuracy on this data set. For training, we set K = 20 best hypotheses and",
  "y": "uses"
 },
 {
  "id": "ee66681690f2c92fe705a09bf7015d_4",
  "x": "K } given the current parameters w \u03c4 . Let the corresponding alignments for the targets be {a 3. Set w \u03c4 +1 to be the solution of : C = 1.0 and run the algorithm for T = 10 epochs. To evaluate our algorithm, we generate 1-best (or 5-best) hypotheses using the beam search procedure and measure accuracy as the percentage of instances in which the target sequence e is one of the 1-best (or 5-best) targets. The input features are based on character m-grams for m = 1, 2, 3. Unlike previ-ous generative transliteration models, no additional language model feature is used. We compare our model against a state-of-the-art statistical machine translation (SMT) system (Zens et al., 2005) and an averaged perceptron edit model (PTEM) with identical features <cite>(Freitag and Khadivi, 2007)</cite> .",
  "y": "uses"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_0",
  "x": "---------------------------------- **INTRODUCTION** The objective of bilingual lexicon induction is to find translation pairs between two languages. Specifically, we aim to pair each word in the source vocabulary with its translation in the target vocabulary. In this paper, we assume that the languages are sufficiently closely related to allow some translation pairs to be identified on the basis of orthographic similarity. Our setting is completely unsupervised: we extract the bilingual lexicons from non-parallel monolingual corpora representing the same domain. By contrast, most of the prior work depend on parallel data in the form of a small bitext (Genzel, 2005) , a gold seed lexicon <cite>(Mikolov et al., 2013b)</cite> , or document-aligned comparable corpora (Vuli\u0107 and Moens, 2015) .",
  "y": "background"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_1",
  "x": "Our setting is completely unsupervised: we extract the bilingual lexicons from non-parallel monolingual corpora representing the same domain. By contrast, most of the prior work depend on parallel data in the form of a small bitext (Genzel, 2005) , a gold seed lexicon <cite>(Mikolov et al., 2013b)</cite> , or document-aligned comparable corpora (Vuli\u0107 and Moens, 2015) . Other prior work assumes access to additional resources or features, such as dependency parsers (Dou and Knight, 2013; Dou et al., 2014) , temporal and web-based features (Irvine and Callison-Burch, 2013) , or BabelNet (Wang and Sitbon, 2014) . Our approach consists of two stages: we first create a seed set of translation pairs, and then iteratively expand the lexicon with a bootstrapping procedure. The seed set is constructed by identifying words with similar spelling (cognates). We filter out non-translation pairs that look similar but differ in meaning (false friends) by imposing a relative-frequency constraint. We then use this noisy seed lexicon to train context vectors via neural network <cite>(Mikolov et al., 2013b)</cite> , inducing a cross-lingual transformation that approximates semantic similarity.",
  "y": "background"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_2",
  "x": "We then use this noisy seed lexicon to train context vectors via neural network <cite>(Mikolov et al., 2013b)</cite> , inducing a cross-lingual transformation that approximates semantic similarity. Although the initial accuracy of the transformation is low, it is sufficient to identify a certain number of correct translation pairs. Adding the high-confidence pairs to the seed lexicon allows us to refine the cross-lingual transformation matrix. We proceed to iteratively expand our lexicon by alternating the two steps of translation pair identification, and transformation induction. We conduct a series of experiments on English, French, and Spanish. The results demonstrate a substantial error reduction with respect to a word-vector-based method of<cite> Mikolov et al. (2013b)</cite> , when using the same word vectors on six source-target pairs. We also improve on the results reported by Haghighi et al. (2008) with both automatically-extracted and gold seed lexicons.",
  "y": "similarities uses"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_3",
  "x": "---------------------------------- **LEXICON EXPANSION** Since our task is to find translations for each of a given set of source-language words, which we refer to as the source vocabulary, we must expand the seed lexicon to cover all such words. We adapt the approach of<cite> Mikolov et al. (2013b)</cite> for learning a linear transformation between the source and target vector spaces to enable it to function given only a small, noisy seed. We use WORD2VEC (Mikolov et al., 2013a) to map words in our source and target corpora to ndimensional vectors. The mapping is derived in a strictly monolingual context of both the source and target languages. While<cite> Mikolov et al. (2013b)</cite> derive the translation matrix using five thousand translation pairs obtained via Google Translate,",
  "y": "similarities uses"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_4",
  "x": "We use WORD2VEC (Mikolov et al., 2013a) to map words in our source and target corpora to ndimensional vectors. The mapping is derived in a strictly monolingual context of both the source and target languages. While<cite> Mikolov et al. (2013b)</cite> derive the translation matrix using five thousand translation pairs obtained via Google Translate, for c iterations do Train source-target TM T on R ---------------------------------- **5:**",
  "y": "differences"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_5",
  "x": "We use the cosine similarity sim(T \u00b7 u, v) to calculate the confidence score for the corresponding candidate translation pair (s, t). An important innovation of our algorithm is considering not only the fitness of t as a translation of s, but also of s as a translation of t. Distinct translation matrices are derived in both directions: source-to-target (T) and target-to-source (T ). We define the score of a pair (s, t) corresponding to the pair of vectors (u, v) as the average of the two cosine similarity values: Unlike<cite> Mikolov et al. (2013b)</cite> , our algorithm iteratively expands the lexicon, which gradually increases the accuracy of the translation matrices. The initial translation matrices, derived from a small, noisy seed, are sufficient to identify a small number of correct translation pairs, which are added to the lexicon. The expanded lexicon is then used to derive new translation matrices, leading to more accurate translations. In each iteration, we sort the candidate translation pairs by their current confidence scores, and add the highest-scoring k pairs to the lexicon.",
  "y": "differences"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_6",
  "x": "The next iteration uses the augmented lexicon to derive new translation matrices. We refer to this approach as bootstrapping, and continue the process for a set number of iterations, which is tuned on development data. The output of our algorithm is the set of translation pairs produced in the final iteration, with each source vocabulary word paired (not necessarily injectively) with one target vocabulary word. ---------------------------------- **EXPERIMENTS** In this section we compare our method to two prior methods, our reimplementation of the supervised word-vector-based method of<cite> Mikolov et al. (2013b)</cite> (using the same vectors as our method), and the reported results of an EM-based method of Haghighi et al. (2008) . ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_7",
  "x": "Table 1 shows that our extraction method produces seed lexicons of a reasonable size and accuracy, with, on average, 219 translation pairs at 85% accuracy. Less than 5% of words in any given seed are duplicates. ---------------------------------- **EVALUATION** We evaluate the induced lexicon after 40 iterations of bidirectional bootstrapping by comparing it to the lexicon after the first iteration in a single direction, which is equivalent to the method of<cite> Mikolov et al. (2013b)</cite> . Following Haghighi et al. (2008) , we also report the accuracy of an ED-ITDIST baseline method, which matches words in the source and target vocabularies. We use an implementation of the Hungarian algorithm 1 (Kuhn, 1955) to solve the minimum bipartite matching problem, where the edge cost for any source-target pair is the normalized edit distance between the two words.",
  "y": "similarities"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_8",
  "x": "We evaluate the induced lexicon after 40 iterations of bidirectional bootstrapping by comparing it to the lexicon after the first iteration in a single direction, which is equivalent to the method of<cite> Mikolov et al. (2013b)</cite> . Following Haghighi et al. (2008) , we also report the accuracy of an ED-ITDIST baseline method, which matches words in the source and target vocabularies. We use an implementation of the Hungarian algorithm 1 (Kuhn, 1955) to solve the minimum bipartite matching problem, where the edge cost for any source-target pair is the normalized edit distance between the two words. The results in Table 2 show that the method of<cite> Mikolov et al. (2013b)</cite> (MIK13-Auto) , represented by the first translation matrix derived on our automatically extracted the seed lexicon, performs well below the edit distance baseline. By contrast, our bootstrapping approach (BootstrapAuto) achieves an average accuracy of 85% on the six datasets. ---------------------------------- **UNIDIRECTIONAL SCORING**",
  "y": "similarities uses"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_0",
  "x": "**ABSTRACT** Recent advances in large-scale, broad coverage part-of-speech tagging and syntactic parsing have been achieved in no small part due to the availability of large amounts of online, human-annotated corpora. In this paper, I argue that a large, human sensetagged corpus is also critical as well as necessary to achieve broad coverage, high accuracy word sense disambiguation, where the sense distinction is at the level of a good desk-top dictionary such as WORD-NET. Using the sense-tagged corpus of 192,800 word occurrences reported in<cite> (Ng and Lee, 1996)</cite> , I examine the effect of the number of training examples on the accuracy of an exemplar-based classifier versus the base-line, most-frequent-sense classitier. I also estimate the amount of human sense-tagged corpus and the manual annotation effort needed to build a largescale, broad coverage word sense disambiguation program which can significantly outperform the most-frequent-sense classifier. Finally, I suggest that intelligent example selection techniques may significantly reduce the amount of sense-tagged corpus needed and offer this research problem as a fruitful area for word sense disambiguation research. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_1",
  "x": "In this view, a large sense-tagged corpus is critical as well as necessary to achieve broad coverage, high accuracy WSD. The rest of this paper is organized as follows. In Section 2, I briefly discuss the utility of WSD in practical NLP tasks like information retrieval and machine translation. I also address some objections to WSD research. In Section 3, I examine the size of the training corpus on the accuracy of WSD, using a corpus of 192,800 occurrences of 191 words hand tagged with WORDNET senses<cite> (Ng and Lee, 1996)</cite> . In Section 4, I estimate the amount of human sense-tagged corpus and the manual annotation effort needed to build a broad coverage, high accuracy WSD program. Finally, in Section 5, I suggest that intelligent example selection techniques may significantly reduce the amount of sense-tagged corpus needed and offer this research problem as a fruitful area for WSD research.",
  "y": "uses"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_2",
  "x": "I only focus on content word disambiguation (i.e., words in the part of speech noun t, verb, adjective and adverb). This is also the task addressed by other WSD research such as (Bruce and Wiebe, 1994; Miller et al., 1994) . When the task is to resolve word senses to the fine-grain distinction of WORD-NET senses, the accuracy figures achieved are generally not very high (Miller et al., 1994; <cite>Ng and Lee, 1996)</cite> . This indicates that WSD is a challenging task and much improvement is still needed. However, if one were to resolve word sense to the level of homograph, or coarse sense distinction, then quite high accuracy can be achieved (in excess of 90%), as reported in (Wilks and Stevenson, 1996) . Similarly, if the task is to distinguish between binary, coarse sense distinction, then current WSD techniques can achieve very high accuracy (in excess of 96% when tested on a dozen words in (Yarowsky, 1995) ). This is to be expected, since homograph contexts are quite distinct and hence it is a much simpler task to disambiguate among a small number of coarse sense classes.",
  "y": "background"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_3",
  "x": "This is also the task addressed by other WSD research such as (Bruce and Wiebe, 1994; Miller et al., 1994) . When the task is to resolve word senses to the fine-grain distinction of WORD-NET senses, the accuracy figures achieved are generally not very high (Miller et al., 1994; <cite>Ng and Lee, 1996)</cite> . This indicates that WSD is a challenging task and much improvement is still needed. However, if one were to resolve word sense to the level of homograph, or coarse sense distinction, then quite high accuracy can be achieved (in excess of 90%), as reported in (Wilks and Stevenson, 1996) . Similarly, if the task is to distinguish between binary, coarse sense distinction, then current WSD techniques can achieve very high accuracy (in excess of 96% when tested on a dozen words in (Yarowsky, 1995) ). This is to be expected, since homograph contexts are quite distinct and hence it is a much simpler task to disambiguate among a small number of coarse sense classes. This is in contrast to disambiguating word senses to the refined senses of WoRDNET, where for instance, the average number of senses per noun is 7.8 and the average number of senses per verb is 12.0 for the set of 191 most ambiguous words investigated in<cite> (Ng and Lee, 1996)</cite> .",
  "y": "motivation"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_4",
  "x": "This is also the task addressed by other WSD research such as (Bruce and Wiebe, 1994; Miller et al., 1994) . When the task is to resolve word senses to the fine-grain distinction of WORD-NET senses, the accuracy figures achieved are generally not very high (Miller et al., 1994; <cite>Ng and Lee, 1996)</cite> . This indicates that WSD is a challenging task and much improvement is still needed. However, if one were to resolve word sense to the level of homograph, or coarse sense distinction, then quite high accuracy can be achieved (in excess of 90%), as reported in (Wilks and Stevenson, 1996) . Similarly, if the task is to distinguish between binary, coarse sense distinction, then current WSD techniques can achieve very high accuracy (in excess of 96% when tested on a dozen words in (Yarowsky, 1995) ). This is to be expected, since homograph contexts are quite distinct and hence it is a much simpler task to disambiguate among a small number of coarse sense classes. This is in contrast to disambiguating word senses to the refined senses of WoRDNET, where for instance, the average number of senses per noun is 7.8 and the average number of senses per verb is 12.0 for the set of 191 most ambiguous words investigated in<cite> (Ng and Lee, 1996)</cite> .",
  "y": "background"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_5",
  "x": "However, if one were to resolve word sense to the level of homograph, or coarse sense distinction, then quite high accuracy can be achieved (in excess of 90%), as reported in (Wilks and Stevenson, 1996) . Similarly, if the task is to distinguish between binary, coarse sense distinction, then current WSD techniques can achieve very high accuracy (in excess of 96% when tested on a dozen words in (Yarowsky, 1995) ). This is to be expected, since homograph contexts are quite distinct and hence it is a much simpler task to disambiguate among a small number of coarse sense classes. This is in contrast to disambiguating word senses to the refined senses of WoRDNET, where for instance, the average number of senses per noun is 7.8 and the average number of senses per verb is 12.0 for the set of 191 most ambiguous words investigated in<cite> (Ng and Lee, 1996)</cite> . We can readily collapse the refined senses of WORDNET into a smaller set if only a coarse (hot I will only focus on common noun in this paper and ignore proper noun. 2 mographic) sense distinction is needed, say for some NLP applications. Indeed, the WORDNET software has an option for grouping noun senses into a smaller number of sense classes.",
  "y": "extends"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_6",
  "x": "To overcome this data sparseness problem of WSD, I initiated a mini-project in sense tagging and collected a corpus in which 192,800 occurrences of 191 words have been manually tagged with senses of WORDNET<cite> (Ng and Lee, 1996)</cite> . These 192,800 word occurrences consist of only 121 nouns and 70 verbs which are the most frequently occurring and most ambiguous words of English. 2 To investigate the effect of the number of training examples on WSD accuracy, I ran the exemplarbased WSD algorithm L~.XAS on varying number of training examples to obtain learning curves for the 191 words (details of LEXAS are described in<cite> (Ng and Lee, 1996)</cite> ). For each word, 10 random trials were conducted and the accuracy figures were averaged over the I0 trials. In each trial, I00 examples were randomly selected to form the test set, while the remaining examples (randomly shuffled) were used for training. LEXAS was given training examples in multiple s of i00, starting with I00,200,300, ... training examples, up to the maximum number of training examples (in a multiple of 100) available in the corpus.",
  "y": "uses"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_7",
  "x": "To investigate the effect of the number of training examples on WSD accuracy, I ran the exemplarbased WSD algorithm L~.XAS on varying number of training examples to obtain learning curves for the 191 words (details of LEXAS are described in<cite> (Ng and Lee, 1996)</cite> ). For each word, 10 random trials were conducted and the accuracy figures were averaged over the I0 trials. In each trial, I00 examples were randomly selected to form the test set, while the remaining examples (randomly shuffled) were used for training. LEXAS was given training examples in multiple s of i00, starting with I00,200,300, ... training examples, up to the maximum number of training examples (in a multiple of 100) available in the corpus. Note that each word w (of the 191 words) can have a different number of sense-tagged occurrences in our corpus. From the combination of Brown corpus (1 million words) and Wall Street Journal corpus (2.5 million words), up to 1,500 sentences each containing an occurrence of the word w are extracted from the combined corpus, with each sentence containing a sense-tagged occurrence of w. When the combined corpus has less than 1,500 occurrences of w, the max= imum number of available occurrences of w is used. For instance, while 137 words have at least 600 occurrences in the combined corpus, only a subset of 43 words has at least 1400 occurrences.",
  "y": "uses"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_8",
  "x": "I also report here the evaluation of LP.XAS on two 2This corpus is scheduled for release by the lAnguistic Data Consortium (LDC). Contact the LDC at ldc~unagi.cis.upenn.edu for details. The performance figures of LEXAS in Table 1 are higher than those reported in<cite> (Ng and Lee, 1996)</cite> . The classification accuracy of the nearest neighbor algorithm used by LEXAS (Cost and Salzberg, 1993) is quite sensitive to the number of nearest neighbors used to select the best matching example. By using 10-fold cross validation (Kohavi and John, 1995) to automatically pick the best number of nearest neighbors to use, the performance of LSXAS has improved. ---------------------------------- **WORD SENSE DISAMBIGUATION IN THE**",
  "y": "differences"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_0",
  "x": "Predicting mental health from smartphone and social media data on a longitudinal basis has recently attracted great interest, with very promising results being reported across many studies [3, 9, 13,<cite> 26]</cite> . Such approaches have the potential to revolutionise mental health assessment, if their development and evaluation follows a real world deployment setting. In this work we take a closer look at state-of-the-art approaches, using different mental health datasets and indicators, different feature sources and multiple simulations, in order to assess their ability to generalise. We demonstrate that under a pragmatic evaluation framework, none of the approaches deliver or even approach the reported performances. In fact, we show that current state-of-the-art approaches can barely outperform the most na\u00efve baselines in the real-world setting, posing serious questions not only about their deployment ability, but also about the contribution of the derived features for the mental health assessment task and how to make better use of such data in the future. ---------------------------------- **INTRODUCTION**",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_1",
  "x": "**INTRODUCTION** Establishing the right indicators of mental well-being is a grand challenge posed by the World Health Organisation [7] . Poor mental health is highly correlated with low motivation, lack of satisfaction, low productivity and a negative economic impact [20] . The current approach is to combine census data at the population level [19] , thus failing to capture well-being on an individual basis. The latter is only possible via self-reporting on the basis of established psychological scales, which are hard to acquire consistently on a longitudinal basis, and they capture long-term aggregates instead of the current state of the individual. The widespread use of smart-phones and social media offers new ways of assessing mental well-being, and recent research [1, 2, 3, 5, 9, 10, 13, 14, 22, 23,<cite> 26]</cite> has started exploring the effectiveness of these modalities for automatically assessing ----------------------------------",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_2",
  "x": "Having such longitudinal studies is highly desirable, as it can allow fine-grained monitoring of mental health. However, a crucial question is what constitutes an appropriate evaluation framework, in order for such approaches to be employable in a real world setting. Generalisation to previously unobserved users can only be assessed via leave-N-users-out cross-validation setups, where typically, N is equal to one (LOUOCV, see Table 1 ). However, due to the small number of subjects that are available, such generalisation is hard to achieve by any approach [13] . Alternatively, personalised models [3, 13] for every individual can be evaluated via a within-subject, leave-N-instances-out cross-validation (for N=1, LOIOCV), where an instance for a user u at time i is defined as a {X ui , y ui } tuple of {features(u, i), mental-health-score(u, i)}. In a real world setting, a LOIOCV model is trained on some user-specific instances, aiming to predict her mental health state at some future time points. Again however, the limited number of instances for every user make such models unable to generalize well. In order to overcome these issues, previous work [2, 5, 9, 10, 22,<cite> 26]</cite> has combined the instances {X uj i , y uj i } from different individuals u j and performed evaluation using randomised cross validation (MIXED).",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_3",
  "x": "This poses serious questions about the contribution of the features derived from social media, smartphones and sensors for the task of automatically assessing well-being on a longitudinal basis. Our goal is to flesh out, study and discuss such limitations through extensive experimentation across multiple settings, and to propose a pragmatic evaluation and model-building framework for future research in this domain. ---------------------------------- **RELATED WORK** Research in assessing mental health on a longitudinal basis aims to make use of relevant features extracted from various modalities, in order to train models for automatically predicting a user's mental state (target), either in a classification or a regression manner [1, 2, 3, 9, 10, 13,<cite> 26]</cite> . Examples of state-of-the-art work in this domain are listed in Table 2 , along with the number of subjects that was used and the method upon which evaluation took place. Most approaches have used the \"MIXED\" approach to evaluate models [1, 2, 5, 9, 10, 22,<cite> 26]</cite> , which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score.",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_4",
  "x": "This poses serious questions about the contribution of the features derived from social media, smartphones and sensors for the task of automatically assessing well-being on a longitudinal basis. Our goal is to flesh out, study and discuss such limitations through extensive experimentation across multiple settings, and to propose a pragmatic evaluation and model-building framework for future research in this domain. ---------------------------------- **RELATED WORK** Research in assessing mental health on a longitudinal basis aims to make use of relevant features extracted from various modalities, in order to train models for automatically predicting a user's mental state (target), either in a classification or a regression manner [1, 2, 3, 9, 10, 13,<cite> 26]</cite> . Examples of state-of-the-art work in this domain are listed in Table 2 , along with the number of subjects that was used and the method upon which evaluation took place. Most approaches have used the \"MIXED\" approach to evaluate models [1, 2, 5, 9, 10, 22,<cite> 26]</cite> , which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score.",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_5",
  "x": "Some past works have extracted features {f (t \u2212 N ), ..., f (t)} over N days, in order to predict the score t on day N + 1 [3, 13] . Such approaches are biased if there are overlapping days of train/test data. To illustrate this problem we have followed the approach by Canzian and Musolesi [3] , as one of the pioneering works on predicting depression with GPS traces, on a longitudinal basis. P3 Predicting users instead of mood scores: Most approaches merge all the instances from different subjects, in an attempt to build user-agnostic models in a randomised cross-validation framework [2, 9, 10,<cite> 26]</cite> . This is problematic, especially when dealing with a small number of subjects, whose behaviour (as captured through their data) and mental health scores differ on an individual basis. Such approaches are in danger of \"predicting\" the user in the test set, since her (test set) features might be highly correlated with her features in the training set, and thus infer her average well-being score, based on the corresponding observations of the training set. Such approaches cannot guarantee that they will generalise on either a population-wide (LOUOCV ) or a personalised (LOIOCV ) level.",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_6",
  "x": "Such approaches are biased if there are overlapping days of train/test data. To illustrate this problem we have followed the approach by Canzian and Musolesi [3] , as one of the pioneering works on predicting depression with GPS traces, on a longitudinal basis. P3 Predicting users instead of mood scores: Most approaches merge all the instances from different subjects, in an attempt to build user-agnostic models in a randomised cross-validation framework [2, 9, 10,<cite> 26]</cite> . This is problematic, especially when dealing with a small number of subjects, whose behaviour (as captured through their data) and mental health scores differ on an individual basis. Such approaches are in danger of \"predicting\" the user in the test set, since her (test set) features might be highly correlated with her features in the training set, and thus infer her average well-being score, based on the corresponding observations of the training set. Such approaches cannot guarantee that they will generalise on either a population-wide (LOUOCV ) or a personalised (LOIOCV ) level. In order to examine this effect in both a regression and a classification setting, we have followed the experimental framework by Tsakalidis et al. <cite>[26]</cite> and Jaques et al. [9] .",
  "y": "uses"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_7",
  "x": "While we focus on the approach in [3] , a similar approach with respect to feature extraction was also followed in LiKamWa et al. [13] and Bogomolov et al. [2] , extracting features from the past 2 and 2 to 5 days, respectively. ---------------------------------- **P3: PREDICTING USERS (LOUOCV)** Tsakalidis et al. <cite>[26]</cite> monitored the behaviour of 19 individuals over four months. The subjects were asked to complete two psychological scales [25, 29] on a daily basis, leading to three target scores (positive, negative, mental well-being); various features from smartphones (e.g., time spent on the preferred locations) and textual features (e.g., ngrams) were extracted passively over the 24 hours preceding a mood form timestamp. Model training and evaluation was performed in a randomised (MIXED) cross-validation setup, leading to high accuracy (R 2 = 0.76). However, a case demonstrating the potential user bias is when the models are trained on the textual sources: initially the highest R 2 (0.22) is achieved when a model is applied to the mental-wellbeing target; by normalising the textual features on a per-user basis, the R 2 increases to 0.65.",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_8",
  "x": "The authors labelled the instances with the top 30% of all the scores as \"happy\" and the lowest 30% as \"sad\" and randomly separated them into training, validation and test sets, leading to the same user bias issue. Since different users exhibit different mood scores on average <cite>[26]</cite> , by selecting instances from the top and bottom scores, one might end up separating users and convert the mood prediction task into a user identification one. A more suitable task could have been to try to predict the highest and lowest scores of every individual separately, either in a LOIOCV or in a LOUOCV setup. While we focus on the works of Tsakalidis et al. <cite>[26]</cite> and Jaques et al. [9] , similar experimental setups were also followed in [10] , using the median of scores to separate the instances and performing five-fold cross-validation, and by Bogomolov et al. in [2] , working on a user-agnostic validation setting on 117 subjects to predict their happiness levels, and in [1] , for the stress level classification task. ---------------------------------- **EXPERIMENTS** ----------------------------------",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_10",
  "x": "---------------------------------- **DATASET 1:** We employed the dataset obtained by Tsakalidis et al. <cite>[26]</cite> , a pioneering dataset which contains a mix of longitudinal textual and mobile phone usage data for 30 subjects. From a textual perspective, this dataset consists of social media posts (1,854/5,167 facebook/twitter posts) and private messages (64,221/132/47,043 facebook/twitter/ SMS messages) sent by the subjects. For our ground truth, we use the {positive, negative, mental well-being} mood scores (in the ranges of , , , respectively) derived from self-assessed psychological scales during the study period. ---------------------------------- **DATASET 2:**",
  "y": "uses"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_11",
  "x": "Finally, we replace our features with completely random data and train the same SVM with T HIST = 14 by keeping the same ground truth, performing 100 experiments and reporting averages of sensitivity and specificity (RAND). ---------------------------------- **P3: PREDICTING USERS:** We followed the evaluation settings of two past works (see section 3.3), with the only difference being the use of 5-fold CV instead of a train/dev/test split that was used in [9] . The features of every instance are extracted from the past day before the completion of a mood form. In Experiment 1 we follow the setup in <cite>[26]</cite> : we perform 5-fold CV (MIXED) using SVM (SVR RBF ) and evaluate performance based on R 2 and RM SE. We compare the performance when tested under the LOIOCV /LOUOCV setups, with and without the per-user feature normalisation step.",
  "y": "uses"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_13",
  "x": "P2: Performance (sensitivity/specificity) of the SVM classifier trained over 14 days of smartphone/social media features (FEAT) compared against 3 na\u00efve baselines. ---------------------------------- **P3: PREDICTING USERS** Experiment 1: Table 7 shows the results based on the evaluation setup of Tsakalidis et al. <cite>[26]</cite> . In the MIXED cases, the pattern is consistent with <cite>[26]</cite> , indicating that normalising the features on a per-user basis yields better results, when dealing with sparse textual features (positive, negative, wellbeing targets). The explanation of this effect lies within the danger of predicting the user's identity instead of her mood scores. This is why the per-user normalisation does not have any effect for the stress target, since for that we are using dense features derived from smartphones: the vocabulary used by the subjects for the other targets is more indicative of their identity.",
  "y": "uses"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_14",
  "x": "---------------------------------- **P3: PREDICTING USERS** Experiment 1: Table 7 shows the results based on the evaluation setup of Tsakalidis et al. <cite>[26]</cite> . In the MIXED cases, the pattern is consistent with <cite>[26]</cite> , indicating that normalising the features on a per-user basis yields better results, when dealing with sparse textual features (positive, negative, wellbeing targets). The explanation of this effect lies within the danger of predicting the user's identity instead of her mood scores. This is why the per-user normalisation does not have any effect for the stress target, since for that we are using dense features derived from smartphones: the vocabulary used by the subjects for the other targets is more indicative of their identity. In order to further support this statement, we trained the SVR model using only the one-hot encoded user id as a feature, without any textual features.",
  "y": "similarities"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_15",
  "x": "In the MIXED case, we train and test on the same users, while\u0233 is calculated as the mean of the mood scores across all users, whereas in the LOIOCV /LOUOCV cases,\u0233 is calculated for every user separately. In MIXED, by identifying who the user is, we have a rough estimate of her mood score, which is by itself a good predictor, if it is compared with the average predictor across all mood scores of all users. Thus, the effect of the features in this setting cannot be assessed with certainty. Table 7 . P3: Results following the evaluation setup in <cite>[26]</cite> (MIXED), along with the results obtained in the LOIOCV and LOUOCV settings with (+) and without (-) per-user input normalisation. Table 8 displays our results based on Jaques et al. [9] (see section 3.3). The average accuracy on the \"UNIQ\" setup is higher by 14% compared to the majority classifier in MIXED.",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_0",
  "x": "**ABSTRACT** We explore blindfold (question-only) baselines for <cite>Embodied Question Answering</cite>. The <cite>EmbodiedQA</cite> task requires an agent to answer a question by intelligently navigating in a simulated environment, gathering necessary visual information only through first-person vision before finally answering. Consequently, a blindfold baseline which ignores the environment and visual information is a degenerate solution, yet we show through our experiments on the EQAv1 dataset that a simple question-only baseline achieves state-of-the-art results on the EmbodiedQA task in all cases except when the agent is spawned extremely close to the object. ---------------------------------- **INTRODUCTION** Recent breakthroughs in static, unimodal tasks such as image classification [16] and language processing [18] has prompted research towards multimodal tasks [1, 8] and virtual environments [4, 15, 25] .",
  "y": "motivation background"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_1",
  "x": "**INTRODUCTION** Recent breakthroughs in static, unimodal tasks such as image classification [16] and language processing [18] has prompted research towards multimodal tasks [1, 8] and virtual environments [4, 15, 25] . This is substantiated by embodiment theories in cognitive science that have argued for agent learning to be interactive and multimodal, mimicking key aspects of human learning [9, 17] . To foster and measure progress in such virtual environments, new tasks have been introduced, one of them being <cite>Embodied Question Answering</cite> (<cite>EmbodiedQA</cite>) <cite>[5]</cite> . The <cite>EmbodiedQA</cite> task requires an agent to intelligently navigate in a simulated household environment [25] and answer questions through egocentric vision. Concretely, an agent is spawned at a random location in an environment (a house or building) and asked a question (e.g. 'What color is the car?'). The agent perceives its environment through first-person egocentric vision and can perform a few atomic actions (move-forward, turn, strafe, etc.).",
  "y": "background"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_2",
  "x": "Recent breakthroughs in static, unimodal tasks such as image classification [16] and language processing [18] has prompted research towards multimodal tasks [1, 8] and virtual environments [4, 15, 25] . This is substantiated by embodiment theories in cognitive science that have argued for agent learning to be interactive and multimodal, mimicking key aspects of human learning [9, 17] . To foster and measure progress in such virtual environments, new tasks have been introduced, one of them being <cite>Embodied Question Answering</cite> (<cite>EmbodiedQA</cite>) <cite>[5]</cite> . The <cite>EmbodiedQA</cite> task requires an agent to intelligently navigate in a simulated household environment [25] and answer questions through egocentric vision. Concretely, an agent is spawned at a random location in an environment (a house or building) and asked a question (e.g. 'What color is the car?'). The agent perceives its environment through first-person egocentric vision and can perform a few atomic actions (move-forward, turn, strafe, etc.). The goal of the agent is to intelligently navigate the environment and gather visual information necessary for answering the question.",
  "y": "background"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_3",
  "x": "The goal of the agent is to intelligently navigate the environment and gather visual information necessary for answering the question. Subsequent to the introduction of the task, several methods have been introduced to solve the <cite>EmbodiedQA</cite> task <cite>[5</cite>, 6] , using some combination of reinforcement learning, behavior cloning and hierarchical control. Apart from using the question and images from the environment, these methods also rely on varying degrees of expert supervision such as shortest path demonstrations and subgoal policy sketches. In this work, we evaluate simple question-only baselines that never see the environment and receive no form of expert supervision. We examine whether existing methods outperform baselines designed to solely capture dataset bias, in order to better understand the performance of these existing methods. To our surprise, blindfold baselines achieve state-of-the-art performance on the <cite>EmbodiedQA</cite> task, except in the case when the agent is spawned extremely close to the object. Even in the latter case, blindfold baselines perform surprisingly close to existing state-of-the-art methods.",
  "y": "background"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_4",
  "x": "The agent perceives its environment through first-person egocentric vision and can perform a few atomic actions (move-forward, turn, strafe, etc.). The goal of the agent is to intelligently navigate the environment and gather visual information necessary for answering the question. Subsequent to the introduction of the task, several methods have been introduced to solve the <cite>EmbodiedQA</cite> task <cite>[5</cite>, 6] , using some combination of reinforcement learning, behavior cloning and hierarchical control. Apart from using the question and images from the environment, these methods also rely on varying degrees of expert supervision such as shortest path demonstrations and subgoal policy sketches. In this work, we evaluate simple question-only baselines that never see the environment and receive no form of expert supervision. We examine whether existing methods outperform baselines designed to solely capture dataset bias, in order to better understand the performance of these existing methods. To our surprise, blindfold baselines achieve state-of-the-art performance on the <cite>EmbodiedQA</cite> task, except in the case when the agent is spawned extremely close to the object.",
  "y": "motivation"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_5",
  "x": "The goal of the agent is to intelligently navigate the environment and gather visual information necessary for answering the question. Subsequent to the introduction of the task, several methods have been introduced to solve the <cite>EmbodiedQA</cite> task <cite>[5</cite>, 6] , using some combination of reinforcement learning, behavior cloning and hierarchical control. Apart from using the question and images from the environment, these methods also rely on varying degrees of expert supervision such as shortest path demonstrations and subgoal policy sketches. In this work, we evaluate simple question-only baselines that never see the environment and receive no form of expert supervision. We examine whether existing methods outperform baselines designed to solely capture dataset bias, in order to better understand the performance of these existing methods. To our surprise, blindfold baselines achieve state-of-the-art performance on the <cite>EmbodiedQA</cite> task, except in the case when the agent is spawned extremely close to the object. Even in the latter case, blindfold baselines perform surprisingly close to existing state-of-the-art methods.",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_6",
  "x": "We note that this finding is reminiscent of several recent works in both Computer Vision and Natural Language Processing, where researchers have found that statistical irregularities in the dataset can enable degenerate methods to perform surprisingly well [11, 12, 14, 21] . Our findings suggest that current <cite>EmbodiedQA</cite> models are ineffective at leveraging the context from the environment, in fact this context or embodiment in the environment can negatively hamper them. We hope comparison with our baseline results can more effectively demonstrate how well a method is able to leverage embodiment in the environment. Upon further error analysis of our models and qualitative inspection of the dataset, we find that there exist biases in the EQAv1 dataset that allow blindfold models to perform so well. We acknowledge the active effort of Das et al. <cite>[5]</cite> in removing some biases via entropy-pruning but note that further efforts might be necessary to fully correct these biases. ---------------------------------- **RELATED WORK**",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_8",
  "x": "Upon further error analysis of our models and qualitative inspection of the dataset, we find that there exist biases in the EQAv1 dataset that allow blindfold models to perform so well. We acknowledge the active effort of Das et al. <cite>[5]</cite> in removing some biases via entropy-pruning but note that further efforts might be necessary to fully correct these biases. ---------------------------------- **RELATED WORK** <cite>EmbodiedQA</cite> Methods: Das et al. <cite>[5]</cite> introduced the <cite>PACMAN-RL+Q</cite> model which is bootstrapped with expert shortest-path demonstrations and later fine-tuned with REINFORCE [24] . This model consists of a hierarchical navigation module: a planner and a controller, and a question answering module that acts when the navigation module has given up control. In a later work, Das et al. [6] introduce Neural Modular Control (NMC) which is a hierarchical policy network that operates over expert sub-policy sketches.",
  "y": "background"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_9",
  "x": "To get rid of peaky answers, an entropy pruning method was applied by <cite>[5]</cite> where questions with normalized entropy below 0.5 were excluded. However this still leaves an uneven answer distribution that can be exploited. We also train the <cite>[5]</cite> text embedding model (an LSTM) with the optimization settings described in <cite>[5]</cite> for 200 epochs.",
  "y": "differences"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_10",
  "x": "However this still leaves an uneven answer distribution that can be exploited. Training Details 1 We evaluate the efficacy of our proposed baselines on the EQAv1 dataset. For the BoW model, we initialize the embeddings with Glove vectors [20] of size 100, which are allowed to be fine-tuned during the training procedure. We use the Adam optimizer (batch-size of 64) with a learning rate of 5e \u22123 which is annealed via a scheduling mechanism based on plateaus in the validation loss. The training procedure is run for 200 epochs and we use the checkpoint with minimum validation loss to compute accuracy on the test set. The NN-AnswerDist and the Majority baselines are self-descriptive and there are no specific training details that we apply. We also train the <cite>[5]</cite> text embedding model (an LSTM) with the optimization settings described in <cite>[5]</cite> for 200 epochs.",
  "y": "similarities"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_11",
  "x": "The Nearest Neighbour method also does pretty well, and only falls behind to <cite>PACMAN</cite> (BC+REINFORCE) and NMC(BC+A3C) in the T 10 case. The difference in performance b/w the Nearest Neighbour method and BoW is primarily due to the fact that the BoW method leverages validation metrics more effectively, uses distributed word representations and differs in optimization. We also observe that the majority baseline achieves an accuracy of only 17.15%, suggesting that the other question-only baselines leverage dataset biases separate from class modes. For completeness, we also include a question only baseline derived directly from the <cite>EmbodiedQA</cite> codebase, which uses only the Question LSTM in the <cite>PACMAN</cite> model, termed as <cite>PACMAN</cite> Q-only (LSTM). Note that we only compare the top-1 accuracy of different methods here, and not the navigation performance since it's not directly applicable to these blindfold baselines. To better understand the exact bias exploited by the text only models we observe that (a) The questions from training set are largely repeated in the validation and test set, with only 2 and 6 questions being unique to them respectively. As noted earlier, this means that models don't need to generalize across unseen combinations of rooms/objects/colors to perform well on this task (b) Despite entropy-pruning, there is a noticeable bias in the answer distribution of EQAv1 questions (see [<cite>5</cite>, Appendix A]).",
  "y": "differences"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_12",
  "x": "We observe that the BoW model outperforms all existing methods except NMC(BC+A3C) in the case where agent is spawned very close to the target. The Nearest Neighbour method also does pretty well, and only falls behind to <cite>PACMAN</cite> (BC+REINFORCE) and NMC(BC+A3C) in the T 10 case. The difference in performance b/w the Nearest Neighbour method and BoW is primarily due to the fact that the BoW method leverages validation metrics more effectively, uses distributed word representations and differs in optimization. We also observe that the majority baseline achieves an accuracy of only 17.15%, suggesting that the other question-only baselines leverage dataset biases separate from class modes. For completeness, we also include a question only baseline derived directly from the <cite>EmbodiedQA</cite> codebase, which uses only the Question LSTM in the <cite>PACMAN</cite> model, termed as <cite>PACMAN</cite> Q-only (LSTM). Note that we only compare the top-1 accuracy of different methods here, and not the navigation performance since it's not directly applicable to these blindfold baselines. To better understand the exact bias exploited by the text only models we observe that (a) The questions from training set are largely repeated in the validation and test set, with only 2 and 6 questions being unique to them respectively.",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_13",
  "x": "The difference in performance b/w the Nearest Neighbour method and BoW is primarily due to the fact that the BoW method leverages validation metrics more effectively, uses distributed word representations and differs in optimization. We also observe that the majority baseline achieves an accuracy of only 17.15%, suggesting that the other question-only baselines leverage dataset biases separate from class modes. For completeness, we also include a question only baseline derived directly from the <cite>EmbodiedQA</cite> codebase, which uses only the Question LSTM in the <cite>PACMAN</cite> model, termed as <cite>PACMAN</cite> Q-only (LSTM). Note that we only compare the top-1 accuracy of different methods here, and not the navigation performance since it's not directly applicable to these blindfold baselines. To better understand the exact bias exploited by the text only models we observe that (a) The questions from training set are largely repeated in the validation and test set, with only 2 and 6 questions being unique to them respectively. As noted earlier, this means that models don't need to generalize across unseen combinations of rooms/objects/colors to perform well on this task (b) Despite entropy-pruning, there is a noticeable bias in the answer distribution of EQAv1 questions (see [<cite>5</cite>, Appendix A]). Our results on the Nearest Neighbour baseline confirm this source of bias and explain largely the text model performance.",
  "y": "similarities"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_14",
  "x": "To better understand the exact bias exploited by the text only models we observe that (a) The questions from training set are largely repeated in the validation and test set, with only 2 and 6 questions being unique to them respectively. As noted earlier, this means that models don't need to generalize across unseen combinations of rooms/objects/colors to perform well on this task (b) Despite entropy-pruning, there is a noticeable bias in the answer distribution of EQAv1 questions (see [<cite>5</cite>, Appendix A]). Our results on the Nearest Neighbour baseline confirm this source of bias and explain largely the text model performance. Viewing these results holistically, we conclude that current methods for the <cite>EmbodiedQA</cite> task are not effective at using context from the environment, and in fact this negatively hampers them. This shows that there is room for building new models that leverage the context and embodiment in the environment. Oracles: We now examine whether the EQAv1 dataset and the proposed oracle navigation can improve over pure text baselines, to leverage visual information in the most ideal case. We reproduce the settings for training the VQA model 2 .",
  "y": "motivation uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_16",
  "x": "Oracles: We now examine whether the EQAv1 dataset and the proposed oracle navigation can improve over pure text baselines, to leverage visual information in the most ideal case. We reproduce the settings for training the VQA model 2 . Specifically we train the VQA model described in [6] on the last 5 frames of oracle navigation for 50 epochs with ADAM and a learning rate of 3e \u2212 4 using batch size 20. We observe the accuracy is improved over text baselines in this unrealistic setting, but the use of this model with navigation in <cite>PACMAN</cite> reduces performance to below the text baselines. For completeness we benchmark an oracle with our BoW embedding model in place of the LSTM with all other settings kept constant. As noted in <cite>[5]</cite> , we re-iterate that these oracles are far from perfect, as they may not contain the best vantage or context to answer the question. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_17",
  "x": "For completeness we benchmark an oracle with our BoW embedding model in place of the LSTM with all other settings kept constant. As noted in <cite>[5]</cite> , we re-iterate that these oracles are far from perfect, as they may not contain the best vantage or context to answer the question. ---------------------------------- **T 10** T 20 T 50 T any Navigation + VQA <cite>PACMAN</cite> (BC) <cite>[5]</cite> 48 BOW-CNN VQA-Only 56.5 Table 1 : We compare to the published results from [6] for agent spawned at various steps away from the target: 10, 30, 50, and anywhere in the environment. Question-only baselines outperform Navigation+VQA methods except when spawned 10 steps from the target object. A VQA-only system with oracle navigation can improve on a pure text baseline but isn't effective when combined with navigation.",
  "y": "uses differences"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_18",
  "x": "---------------------------------- **T 10** T 20 T 50 T any Navigation + VQA <cite>PACMAN</cite> (BC) <cite>[5]</cite> 48 BOW-CNN VQA-Only 56.5 Table 1 : We compare to the published results from [6] for agent spawned at various steps away from the target: 10, 30, 50, and anywhere in the environment. Question-only baselines outperform Navigation+VQA methods except when spawned 10 steps from the target object. A VQA-only system with oracle navigation can improve on a pure text baseline but isn't effective when combined with navigation. (*) indicates our reproduction of the model described in <cite>[5]</cite> Error Analysis: To better understand the shortcomings and limitations, we perform an error analysis of the one of the runs of the BoW model on different question types: Here, the color category Preposition Location Color 9.09 51.72 53.31 Table 2 : Accuracy of the BoW model on different question types subsumes color and color_room both. The particularly low accuracy on preposition questions is due to the fact that there exist very few questions of this type in the training set (2.44%), and the entropy of answer distribution in this class is much higher compared to color and location question types.",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_19",
  "x": "A VQA-only system with oracle navigation can improve on a pure text baseline but isn't effective when combined with navigation. (*) indicates our reproduction of the model described in <cite>[5]</cite> Error Analysis: To better understand the shortcomings and limitations, we perform an error analysis of the one of the runs of the BoW model on different question types: Here, the color category Preposition Location Color 9.09 51.72 53.31 Table 2 : Accuracy of the BoW model on different question types subsumes color and color_room both. The particularly low accuracy on preposition questions is due to the fact that there exist very few questions of this type in the training set (2.44%), and the entropy of answer distribution in this class is much higher compared to color and location question types. ---------------------------------- **CONCLUSION** We show that simple question only baselines largely outperform or closely compete with existing methods on the <cite>EmbodiedQA</cite> task. Our results indicate existing models are not able to convincingly use sensory inputs from the environment to perform question answering, although they have been demonstrated some ability navigate toward the object of interest.",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_20",
  "x": "A VQA-only system with oracle navigation can improve on a pure text baseline but isn't effective when combined with navigation. (*) indicates our reproduction of the model described in <cite>[5]</cite> Error Analysis: To better understand the shortcomings and limitations, we perform an error analysis of the one of the runs of the BoW model on different question types: Here, the color category Preposition Location Color 9.09 51.72 53.31 Table 2 : Accuracy of the BoW model on different question types subsumes color and color_room both. The particularly low accuracy on preposition questions is due to the fact that there exist very few questions of this type in the training set (2.44%), and the entropy of answer distribution in this class is much higher compared to color and location question types. ---------------------------------- **CONCLUSION** We show that simple question only baselines largely outperform or closely compete with existing methods on the <cite>EmbodiedQA</cite> task. Our results indicate existing models are not able to convincingly use sensory inputs from the environment to perform question answering, although they have been demonstrated some ability navigate toward the object of interest.",
  "y": "future_work"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_0",
  "x": "In order to obtain better embeddings for the questions and answers, we build a convolutional neural network (CNN) structure on top of biLSTM. Secondly, in order to better distinguish candidate answers according to the question, we introduce a simple but efficient attention model to this framework for the answer embedding generation according to the question context. We report experimental results for two answer selection datasets: (1) InsuranceQA<cite> (Feng et al., 2015)</cite> 1 , a recently released large-scale non-factoid QA dataset from the insurance domain. The rest of the paper is organized as follows: Section 2 describes the related work for answer selection; Section 3 provides the details of the proposed models; Experimental settings and results of InsuranceQA and TREC-QA datasets are discussed in section 4 and 5 respectively; Finally, we draw conclusions in section 6. ---------------------------------- **RELATED WORK** Previous work on answer selection normally used feature engineering, linguistic tools, or external resources.",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_1",
  "x": "While these methods show effectiveness, they might suffer from the availability of additional resources, the effort of feature engineering and the systematic complexity by introducing linguistic tools, such as parse trees and dependency trees. There were prior methods using deep learning technologies for the answer selection task. The approaches for non-factoid question answering generally pursue the solution on the following directions: Firstly, the question and answer representations are learned and matched by certain similarity metrics<cite> (Feng et al., 2015</cite>; Yu et al., 2014; dos Santos et al., 2015) . Secondly, a joint feature vector is constructed based on both the question and the answer, and then the task can be converted into a classification or learning-to-rank problem (Wang & Nyberg, 2015) . Finally, recently proposed models for textual generation can intrinsically be used for answer selection and generation (Bahdanau et al., 2015; Vinyals & Le, 2015) . The framework proposed in this work belongs to the first category. There are two major differences between our approaches and the work in<cite> (Feng et al., 2015)</cite> : (1) The architectures developed in<cite> (Feng et al., 2015)</cite> are only based on CNN, whereas our models are based on bidirectional LSTMs, which are more capable of exploiting long-range sequential context information.",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_2",
  "x": "There are two major differences between our approaches and the work in<cite> (Feng et al., 2015)</cite> : (1) The architectures developed in<cite> (Feng et al., 2015)</cite> are only based on CNN, whereas our models are based on bidirectional LSTMs, which are more capable of exploiting long-range sequential context information. Moreover, we also integrate the CNN structures on the top of biLSTM for better performance. (2)<cite> Feng et al. (2015)</cite> tackle the question and answer independently, while the proposed structures develop an efficient attentive models to generate answer embeddings according to the question. ---------------------------------- **APPROACH** In this section, we describe the proposed framework and its variations. We first introduce the general framework, which is to build bi-directional LSTM on both questions and their answer candidates, and then use the similarity metric to measure the distance of question answer pairs.",
  "y": "differences"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_3",
  "x": "Following the same ranking loss in<cite> (Feng et al., 2015</cite>; Weston et al., 2014; Hu et al., 2014) , we define the training objective as a hinge loss. where a + is a ground truth answer, a \u2212 is an incorrect answer randomly chosen from the entire answer space, and M is constant margin. We treat any question with more than one ground truth as multiple training examples, each for one ground truth. There are three simple ways to generate representations for questions and answers based on the word-level biLSTM outputs: (1) Average pooling; (2) max pooling; (3) the concatenation of the last vectors on both directions. The three strategies are compared with the experimental performance in Section 5. Dropout operation is performed on the QA representations before cosine similarity matching. Finally, from preliminary experiments, we observe that the architectures, in which both question and answer sides share the same network parameters, is significantly better than the one that the question and answer sides own their own parameters separately, and converges much faster.",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_4",
  "x": "There are three simple ways to generate representations for questions and answers based on the word-level biLSTM outputs: (1) Average pooling; (2) max pooling; (3) the concatenation of the last vectors on both directions. The three strategies are compared with the experimental performance in Section 5. Dropout operation is performed on the QA representations before cosine similarity matching. Finally, from preliminary experiments, we observe that the architectures, in which both question and answer sides share the same network parameters, is significantly better than the one that the question and answer sides own their own parameters separately, and converges much faster. As discussed in<cite> (Feng et al., 2015)</cite> , this is reasonable, because for a shared layer network, the corresponding elements in question and answer vectors represent the same biLSTM outputs. While for the network with separate question and answer parameters, there is no such constraint and the model has doublesized parameters, making it difficult to learn for the optimizer. In the previous subsection, we generate the question and answer representations only by simple operations, such as max or mean pooling.",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_5",
  "x": "The structure of CNN in this work is similar to the one in<cite> (Feng et al., 2015)</cite> , as shown in Figure 2 . Unlike the traditional forward neural network, where each output is interactive with each input, the convolutional structure only imposes local interactions between the inputs within a filter size m. In this work, for every window with the size of m in biLSTM output vectors, ie. , where t is a certain time step, the convolutional filter F = [F(0) \u00b7 \u00b7 \u00b7 F(m \u2212 1)] will generate one value as follows. where b is a bias, and F and b are the parameters of this single filter. Same as typical CNNs, a max-k pooling layer is built on the top of the convolutional layer. Intuitively, we want to emphasize the top-k values from each convolutional filter.",
  "y": "similarities"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_6",
  "x": "**INSURANCEQA EXPERIMENTS** Having described a number of models in the previous section, we evaluate the proposed approaches on the insurance domain dataset, InsuranceQA, provided by<cite> Feng et al. (2015)</cite> . The InsuranceQA dataset provides a training set, a validation set, and two test sets. We do not see obvious categorical differentiation between two tests' questions. One can see the details of InsuranceQA data in<cite> (Feng et al., 2015)</cite> . We list the numbers of questions and answers of the dataset in Table 1 . A question may correspond to multiple answers.",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_7",
  "x": "However, QA-LSTM/CNN with attention can outperform the baselines on both datasets. ---------------------------------- **INSURANCEQA EXPERIMENTS** Having described a number of models in the previous section, we evaluate the proposed approaches on the insurance domain dataset, InsuranceQA, provided by<cite> Feng et al. (2015)</cite> . The InsuranceQA dataset provides a training set, a validation set, and two test sets. We do not see obvious categorical differentiation between two tests' questions. One can see the details of InsuranceQA data in<cite> (Feng et al., 2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_8",
  "x": "Similar to this work, the candidates are re-ranked according the cosine similarity to a question. Metzler-Bendersky IR model: A state-of-the-art weighted dependency (WD) model, which employs a weighted combination of term-based and term proximity-based ranking features to score each candidate answer. Architecture-II in<cite> (Feng et al., 2015)</cite> : Instead of using LSTM, a CNN model is employed to learn a distributed vector representation of a given question and its answer candidates, and the answers are scored by cosine similarity with the question. No attention model is used in this baseline. ---------------------------------- **ARCHITECTURE-II WITH GEOMETRICMEAN OF EUCLIDEAN AND SIGMOID DOT PRODUCT (GESD):** GESD is used to measure the distance between the question and answers.",
  "y": "background"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_9",
  "x": "**ARCHITECTURE-II WITH GEOMETRICMEAN OF EUCLIDEAN AND SIGMOID DOT PRODUCT (GESD):** GESD is used to measure the distance between the question and answers. This is the model which achieved the best performance in<cite> (Feng et al., 2015)</cite> . ---------------------------------- **RESULTS AND DISCUSSIONS** In this section, detailed analysis on experimental results are given. or attention model. They vary on how to utilize the biLSTM output vectors to form sentential embeddings for questions and answers in shown in section 3.1.",
  "y": "background"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_10",
  "x": "We can observe that just concatenating of the last vectors from both direction (A) performs the worst. It is surprised to see using maxpooling (C) is much better than average pooling (B). The potential reason is that the max-pooling extracts more local values for each dimension, so that more local information can be reflected on the output embeddings. From Row (D) to (F), CNN layers are built on the top of the biLSTM with different filter numbers. We set the filter width m = 2, and we did not see better performance if we increase m to 3 or 4. Row (F) with 4000 filters gets the best validation accuracy, obtained a comparable performance with the best baseline (Row (D) in Table 2 ). Row F shared a highly analogous CNN structure with Architecture II in<cite> (Feng et al., 2015)</cite> , except that the later used a shallow hidden layer to transform the word embeddings into the input of CNN structure, while Row F take the output of biLSTM as CNN input.",
  "y": "differences similarities"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_11",
  "x": "Row (G) and (H) corresponds to QA-LSTM with the attention model. (G) connects the output vectors of answers after attention with a max pooling layer, and (H) with an average pooling. In comparison to Model (C), Model (G) shows over 2% improvement on both validation and Test2 sets. With respect to the model with mean pooling layers (B), the improvement from attention is more remarkable. Model (H) is over 8% higher on all datasets compared to (B), and gets improvements from the best baseline by 3%, 2.8% and 1.2% on the validation, Test1 and Test2 sets, respectively. Compared to Architecture II in<cite> (Feng et al., 2015)</cite> , which involved a large number of CNN filters, (H) model also has fewer parameters. Row (I) corresponds to section 3.4, where CNN and attention mechanism are combined.",
  "y": "differences"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_12",
  "x": "---------------------------------- **DATA, METRICS AND BASELINES** In this paper, we adopt TREC-QA, created by Wang et al. (2007) Following previous work on this task, we use Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) as evaluation metrics, which are calculated using the official evaluation scripts. In Table 4 , we list the performance of some prior work on this dataset, which can be referred to (Wang & Nyberg, 2015) . We implemented the Architecture II in<cite> (Feng et al., 2015)</cite> from scratch. Wang & Nyberg (2015) and<cite> Feng et al. (2015)</cite> are the best baselines on MAP and MRR respectively. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_13",
  "x": "Wang & Nyberg (2015) and<cite> Feng et al. (2015)</cite> are the best baselines on MAP and MRR respectively. ---------------------------------- **SETUP** We keep the configurations same as those in InsuranceQA in section 4.1, except the following differences: First, we set the minibatch size as 10; Second, we set the maximum length of questions and answers as 40 instead of 200. Third, following (Wang & Nyberg, 2015) , We use 300-dimensional vectors that were trained and provided by word2vec 3 . Finally, we use the models from the epoch with the best MAP on the validation set for training. Moreover, although TREC-QA dataset provided negative answer candidates for each training question, we randomly select the negative answers from all the candidate answers in the training set.",
  "y": "background"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_14",
  "x": "Third, following (Wang & Nyberg, 2015) , We use 300-dimensional vectors that were trained and provided by word2vec 3 . Finally, we use the models from the epoch with the best MAP on the validation set for training. Moreover, although TREC-QA dataset provided negative answer candidates for each training question, we randomly select the negative answers from all the candidate answers in the training set. Table 5 shows the performance of the proposed models. Compared to Model (A), which is with average pooling on top of biLSTM but without attention, Model (B) with attention improves MAP by 0.7% and MRR by approximately 2%. The combination of CNN with QA-LSTM (Model-C) gives greater improvement on both MAP and MRR from Model (A). Model (D), which combines the ideas of Model (B) and (C), achieves the performance, competitive to the best baselines on MAP, and 2\u223c4% improvement on MRR compared to (Wang & Nyberg, 2015) and<cite> (Feng et al., 2015)</cite> .",
  "y": "differences"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_0",
  "x": "Readability classification, is a task of mapping text onto a scale of readability levels. We explore the task of automatically classifying documents based on their different readability levels. As an input, this function operates on various statistics relating to different text features. In this paper, we train a readability classification model using a corpus compiled from textbooks and features inherited from our previous works Islam et al. (2012; and features from<cite> Sinha et al. (2012)</cite> . Later we use the model to classify Bangla news articles for children from different well-known news sources from Bangladesh and West Bengal. The paper is organized as follows: Section 2 discusses related work. Section 3 describes cognitive model of children in terms of readability followed by an introduction of the training corpus and news articles in Section 4.",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_1",
  "x": "Due to recent achievements in linguistic data processing, different linguistic features are now in the focus of readability studies. Islam et al. (2012) summarizes related work regarding language model-based features (Collins- Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Aluisio et al., 2010; Kate et al., 2010; Eickhoff et al., 2011) , POS-related features (Pitler and Nenkova, 2008; Feng et al., 2009; Aluisio et al., 2010; Feng et al., 2010) , syntactic features (Pitler and Nenkova, 2008; Barzilay and Lapata, 2008; Heilman et al., 2007; Heilman et al., 2008; Islam and Mehler, 2013) , and semantic features (Feng et al., 2009; Islam and Mehler, 2013) . Recently, Hancke et al. (2012) found that morphological features influence the readability of German texts. Due to unavailability of linguistic resources for Bangla, we did not explore any of the linguistically motivated features. We have inherited features from Islam et al. (2012; and<cite> Sinha et al. (2012)</cite> , these features achieve reasonable classification accuracy. Children's reading skills is influenced by their cognitive ability.",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_2",
  "x": "The entropy of a random variable is related to the difficulty of correctly guessing the value of the corresponding random variable. In our previous studies, Islam et al. (2012; and Islam and Mehler (2013) use different information-theoretic features for text readability classification. Our hypothesis was that the higher the entropy, the less readable the text along the feature represented by the corresponding random variable. We have inherited seven informationtheoretic features from our previous studies. ---------------------------------- **READABILITY MODELS FOR BANGLA** Recently,<cite> Sinha et al. (2012)</cite> proposed few computational models that are similar to the traditional English readability formulas.",
  "y": "background"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_3",
  "x": "We have inherited seven informationtheoretic features from our previous studies. ---------------------------------- **READABILITY MODELS FOR BANGLA** Recently,<cite> Sinha et al. (2012)</cite> proposed few computational models that are similar to the traditional English readability formulas. A user study was performed to evaluate their performance. We also inherited two of their best performing models: In their models, they use structural parameters such as average WL, number of jukta-akshars (JUK) or consonant-conjuncts, number of polysyllabic words (PSW).",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_4",
  "x": "A user study was performed to evaluate their performance. We also inherited two of their best performing models: In their models, they use structural parameters such as average WL, number of jukta-akshars (JUK) or consonant-conjuncts, number of polysyllabic words (PSW). The PSW30 shows that normalized value of PSW over 30 sentences. Table 3 : Performance of Bangla readability models proposed by<cite> Sinha et al. (Sinha et al., 2012)</cite> . In this paper, we use 20 features to generate feature vectors for the classifier. The following section describes our experiments and results on training corpus and news articles.",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_5",
  "x": "In this paper, we use 20 features to generate feature vectors for the classifier. The following section describes our experiments and results on training corpus and news articles. ---------------------------------- **EXPERIMENTS AND RESULTS** In order to find the best performing training model, we use 20 features from Islam et al. (2012; and<cite> Sinha et al. (2012)</cite> . Note that hundred data sets were randomly generated where 80% of the corpus was used for training and remaining 20% for evaluation. The weighted average of Accuracy and F-score is computed by considering results of all data sets.",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_6",
  "x": "We use the SMO (Platt, 1998; Keerthi et al., 2001) classifier model implemented in WEKA (Hall et al., 2009) together with the Pearson VII function-based universal kernel PUK (\u00dcst\u00fcn et al., 2006) . ---------------------------------- **TRAINING MODEL** The traditional readability formulas that were proposed for English texts do not work for Bangla texts (Islam et al., 2012; Islam et al., 2014;<cite> Sinha et al., 2012)</cite> . That is why, we did not explore any of the traditional formulas. At first we build a classifier using two readability models from <cite>Sinha et al(2012)</cite> . The output of these models are used as input for the readability classifier.",
  "y": "background"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_7",
  "x": "The traditional readability formulas that were proposed for English texts do not work for Bangla texts (Islam et al., 2012; Islam et al., 2014;<cite> Sinha et al., 2012)</cite> . That is why, we did not explore any of the traditional formulas. At first we build a classifier using two readability models from <cite>Sinha et al(2012)</cite> . The output of these models are used as input for the readability classifier. Table 3 shows the evaluation results. The classification accuracy is little over than 66%. In our previous study Islam et al. (2014) found better classification accuracy using these features.",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_8",
  "x": "The classification F-Score rises to 87.87 when we combine features from Islam et al. (2014) and<cite> Sinha et al. (Sinha et al., 2012)</cite> . ---------------------------------- **NEWS ARTICLES CLASSIFICATION** Total 250 children news articles are collected as candidate news articles for classification. We consider the whole training corpus in order to build a training model. The training model is used to classify the candidate news articles. Among all articles, 160 articles are labeled as very easy and 18 articles as easy.",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_9",
  "x": "Children news articles are cognitively and linguistically different than articles for adult readers. A readability classifier trained on a textbooks corpus is able to classify these articles. Although linguistically motivated features could capture linguistic properties of news articles. Lexical features and features related to information density also have good predictive power to identify text difficulties. The classification results show that candidate articles are appropriate for children. This study also validate that features in our previous study Islam et al. (2014) and features proposed by<cite> Sinha et al. (Sinha et al., 2012)</cite> are useful for Bangla text readability analysis. There are many languages in the world which lack a readability measurement tool.",
  "y": "similarities"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_0",
  "x": "****CONVOLUTIONAL NEURAL NETWORKS FOR TEXT CATEGORIZATION: SHALLOW WORD-LEVEL VS. DEEP CHARACTER-LEVEL**** **ABSTRACT** This paper reports the performances of shallow word-level convolutional neural networks (CNN), our earlier work (2015) [3,<cite> 4]</cite> , on the eight datasets with relatively large training data that were used for testing the very deep characterlevel CNN in Conneau et al. (2016) [1]. Our findings are as follows. The shallow word-level CNNs achieve better error rates than the error rates reported in [1] though the results should be interpreted with some consideration due to the unique pre-processing of [1]. The shallow word-level CNN uses more parameters and therefore requires more storage than the deep character-level CNN; however, the shallow word-level CNN computes much faster. ----------------------------------",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_1",
  "x": "The shallow word-level CNN uses more parameters and therefore requires more storage than the deep character-level CNN; however, the shallow word-level CNN computes much faster. ---------------------------------- **INTRODUCTION** Text categorization is the task of labeling documents, which has many important applications such as sentiment analysis and topic categorization. Recently, several variations of convolutional neural networks (CNNs) [7] have been shown to achieve high accuracy on text categorization (see e.g., [3, <cite>4,</cite> 9, 1] and references therein) in comparison with a number of methods including linear methods, which had long been the state of the art. Long-Short Term Memory networks (LSTMs) [2] have also been shown to perform well on this task, rivaling or sometimes exceeding CNNs [5, 8] . However, CNNs are particularly attractive since, due to their simplicity and parallel processing-friendly nature, training and testing of CNNs can be made much faster than LSTM to achieve similar accuracy [5] , and therefore CNNs have a potential to scale better to large training data.",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_2",
  "x": "Here we focus on two CNN studies that report high performances on categorizing long documents (as opposed to categorizing individual sentences): \u2022 Our earlier work (2015) [3,<cite> 4]</cite> : shallow word-level CNNs (taking sequences of words as input), which we abbreviate as word-CNN. \u2022 Conneau et al. (2016) [1]: very deep character-level CNNs (taking sequences of characters as input), which we abbreviate as char-CNN. Although both studies report higher accuracy than previous work on their respective datasets, it is not clear how they compare with each other due to lack of direct comparison. In [1] , the very deep char-CNN was shown to perform well with larger training data (up to 2.6M documents) but perform relatively poorly with smaller training data; e.g., it underperformed linear methods when trained with 120K documents. In [3,<cite> 4]</cite> the shallow word-CNN was shown to perform well, using training sets (most intensively, 25K documents) that are mostly smaller than those used in [1] . While these results imply that the shallow word-CNN is likely to outperform the deep char-CNN when trained with relatively small training sets such as those used in [3,<cite> 4]</cite> , the shallow word-CNN is untested on the training sets as large as those used in [1] .",
  "y": "motivation background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_3",
  "x": "Although both studies report higher accuracy than previous work on their respective datasets, it is not clear how they compare with each other due to lack of direct comparison. In [1] , the very deep char-CNN was shown to perform well with larger training data (up to 2.6M documents) but perform relatively poorly with smaller training data; e.g., it underperformed linear methods when trained with 120K documents. In [3,<cite> 4]</cite> the shallow word-CNN was shown to perform well, using training sets (most intensively, 25K documents) that are mostly smaller than those used in [1] . While these results imply that the shallow word-CNN is likely to outperform the deep char-CNN when trained with relatively small training sets such as those used in [3,<cite> 4]</cite> , the shallow word-CNN is untested on the training sets as large as those used in [1] . Hence, the purpose of this report is to fill the gap by testing the shallow word-CNNs as in [3,<cite> 4]</cite> on the datasets used in [1] , for direct comparison with the results of very deep char-CNNs reported in [1] . Limitation of work In this work, our new experiments are limited to the shallow word-CNN as in [3,<cite> 4]</cite> . We do not provide new error rate results for the very deep CNNs proposed by [1] , and we only cite their results.",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_4",
  "x": "Although both studies report higher accuracy than previous work on their respective datasets, it is not clear how they compare with each other due to lack of direct comparison. In [1] , the very deep char-CNN was shown to perform well with larger training data (up to 2.6M documents) but perform relatively poorly with smaller training data; e.g., it underperformed linear methods when trained with 120K documents. In [3,<cite> 4]</cite> the shallow word-CNN was shown to perform well, using training sets (most intensively, 25K documents) that are mostly smaller than those used in [1] . While these results imply that the shallow word-CNN is likely to outperform the deep char-CNN when trained with relatively small training sets such as those used in [3,<cite> 4]</cite> , the shallow word-CNN is untested on the training sets as large as those used in [1] . Hence, the purpose of this report is to fill the gap by testing the shallow word-CNNs as in [3,<cite> 4]</cite> on the datasets used in [1] , for direct comparison with the results of very deep char-CNNs reported in [1] . Limitation of work In this work, our new experiments are limited to the shallow word-CNN as in [3,<cite> 4]</cite> . We do not provide new error rate results for the very deep CNNs proposed by [1] , and we only cite their results.",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_5",
  "x": "Although both studies report higher accuracy than previous work on their respective datasets, it is not clear how they compare with each other due to lack of direct comparison. In [1] , the very deep char-CNN was shown to perform well with larger training data (up to 2.6M documents) but perform relatively poorly with smaller training data; e.g., it underperformed linear methods when trained with 120K documents. In [3,<cite> 4]</cite> the shallow word-CNN was shown to perform well, using training sets (most intensively, 25K documents) that are mostly smaller than those used in [1] . While these results imply that the shallow word-CNN is likely to outperform the deep char-CNN when trained with relatively small training sets such as those used in [3,<cite> 4]</cite> , the shallow word-CNN is untested on the training sets as large as those used in [1] . Hence, the purpose of this report is to fill the gap by testing the shallow word-CNNs as in [3,<cite> 4]</cite> on the datasets used in [1] , for direct comparison with the results of very deep char-CNNs reported in [1] . Limitation of work In this work, our new experiments are limited to the shallow word-CNN as in [3,<cite> 4]</cite> . We do not provide new error rate results for the very deep CNNs proposed by [1] , and we only cite their results.",
  "y": "motivation"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_6",
  "x": "In [3,<cite> 4]</cite> the shallow word-CNN was shown to perform well, using training sets (most intensively, 25K documents) that are mostly smaller than those used in [1] . While these results imply that the shallow word-CNN is likely to outperform the deep char-CNN when trained with relatively small training sets such as those used in [3,<cite> 4]</cite> , the shallow word-CNN is untested on the training sets as large as those used in [1] . Hence, the purpose of this report is to fill the gap by testing the shallow word-CNNs as in [3,<cite> 4]</cite> on the datasets used in [1] , for direct comparison with the results of very deep char-CNNs reported in [1] . Limitation of work In this work, our new experiments are limited to the shallow word-CNN as in [3,<cite> 4]</cite> . We do not provide new error rate results for the very deep CNNs proposed by [1] , and we only cite their results. Although it may be natural to assume that the error rates reported in [1] well represent the best performance that the deep char-CNNs can achieve, we note that in [1] , documents were clipped and padded so that they all became 1014 characters long, and we do not know how this pre-processing affected their model accuracy. To experiment with word-CNN, we handle variable-sized documents as variable-sized as we see no merit in making them fixed-sized, though we reduce the size of vocabulary to reduce storage requirements.",
  "y": "similarities"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_7",
  "x": "---------------------------------- **PRELIMINARY** We start with briefly reviewing the very deep word-CNN of [1] and the shallow word-CNN of [3,<cite> 4]</cite> . ---------------------------------- **VERY DEEP CHARACTER-LEVEL CNNS OF [1]** [1] proposed very deep char-CNNs and showed that their best performing models produced higher accuracy than their shallower models and previous deep char-CNNs of [9] . Their best architecture consisted of the following:",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_8",
  "x": "The kernel size ('region size' in our wording) was set to 3 in every convolution layer. In addition, the results obtained by two more shallower architectures were reported. [1] should be consulted for the exact architectures. ---------------------------------- **SHALLOW WORD-LEVEL CNNS AS IN [3,<cite> 4]</cite>** Two types of word-CNN were proposed in [3,<cite> 4]</cite> , which are illustrated in Figure 1 . One is a straightforward application of CNN to text (the base model), and the other involves training of tv-embedding ('tv' stands for two views) to produce additional input to the base model.",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_9",
  "x": "See also the supplementary material of<cite> [4]</cite> for the representation power analysis. As illustrated in Figure 1 (a), f (x) is applied to the text regions at every location of a document (ovals in the figure), and pooling aggregates the resulting region vectors into a document vector, which is used as features by a linear classifier. In our experiments with word-CNN without tv-embedding reported below, the one-hot representation used for x was fixed to the concatenation of one-hot vectors with a vocabulary of the 30K most frequent words, and the dimensionality of region embedding (i.e., the number of feature maps) was fixed to 500. That is, our one-hot vectors were 30K-dimensional while any out-of-vocabulary word was converted to a zero vector, and the region embedding f (x) produced 500-dimensional vectors for each region. Region size (the number of words in each region) was chosen from {3,5}. Based on our previous work, we performed max-pooling with k pooling units (each of which covers 1/k of a document) while setting k = 1 on sentiment analysis datasets and choosing k from {1, 10} on the others. The models described here also served as the base models of the word-CNN with tv-embedding described next. Word-CNNs with tv-embedding Training of word-CNNs with tv-embedding is done in two steps, as shown in Figure 1 (b) .",
  "y": "uses"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_10",
  "x": "Next, we use the tv-embedding to produce additional input to the base model and train it with labeled data. This model can be easily extended to use multiple tv-embeddings, each of which, for example, uses a distinct vector representation of region, and so the region embedding function in the final model (hollow ovals in Figure 1 (b) ) can be written as: is the output of the tv-embedding indexed by i applied to the corresponding text region. In<cite> [4]</cite> , tv-embedding training was done using unlabeled data as an additional resource; therefore, the proposed models were semi-supervised models. In the experiments reported below, due to the lack of standard unlabeled data for the tested datasets, we trained tv-embeddings on the labeled training data ignoring the labels; thus, the resulting models are supervised ones. We trained four tv-embeddings with four distinct one-hot representations of text regions (i.e., input to orange ovals in Figure 1 (b) ): bow representation with region size 5 or 9, and bag-of-{1,2,3}-gram representation with region size 5 or 9. To make bow representation for tv-embedding, we used a vocabulary of the 30K most frequent words, and to make the bag-of-{1,2,3}-gram representation, we used a vocabulary of the 200K most frequent {1,2,3}-grams.",
  "y": "differences"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_11",
  "x": "Tv-embedding training was done as in<cite> [4]</cite> ; weighted square loss was minimized without regularization while the target regions (adjacent regions) were represented by bow vectors, and the data weights were set so that the negative sampling effect was achieved. Tv-embeddings were fixed (i.e., no weight updating) during the final training with labeled data. Training with labels (either with or without tv-embedding) was done as follows. A log loss (or cross entropy) with softmax was minimized. Optimization was done by mini-batch SGD with momentum 0.9 and the mini-batch size was set to 100. The number of epochs was fixed to 30 (except for AG, the smallest, for which it was fixed to 100), and the learning rate was reduced once by multiplying 0.1 after 24 epochs (or 80 epochs on AG). In all layers, weights were initialized by the Gaussian distribution of zero mean and standard deviation 0.01.",
  "y": "uses"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_12",
  "x": "Error rates In Table 1 (b), we show the error rate results of the shallow word-CNN in comparison with the best results of the deep char-CNN reported in [1] and the best results of linear models reported in [9] . On each dataset, the best results are shown in bold and the second best results are shown in the italic font. On all datasets, the shallow word-CNN with tv-embeddings performs the best. The second best performer is the shallow word-CNN without tv-embedding on all but Ama.f (Amazon full). Whereas the deep char-CNN underperforms traditional linear models when training data is relatively small, the shallow word-CNNs with and without tv-embedding clearly outperform them on all the datasets. We observe that, as in our previous work<cite> [4]</cite> , additional input produced by tv-embeddings led to substantial improvements. The performances of word-CNN without tv-embedding might be further improved by having multiple region sizes [3, 6] , but for simplicity, we did not attempt it in this work.",
  "y": "similarities"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_13",
  "x": "If we reduce the dimensionality of tv-embedding from 300 to 100, the number of parameters can be reduced to a half with a small degradation of accuracy, as shown in Table 2 Table 3 : Error rates of the shallow word-CNN with tv-embeddings of 100 dimensions ('w/ 4 tv(100-dim'). 'w/ 4 tv (300-dim)' was copied from shown in Table 3 . Reducing the number of tv-embeddings from four to two also reduces the number of parameters with a small degradation of accuracy ('w/ 2 tv (100-dim)' in Table 2 ). ---------------------------------- **SUMMARY OF THE RESULTS** \u2022 The shallow word-CNNs as in [3,<cite> 4]</cite> generally achieved better error rates than those of the very deep char-CNNs reported in [1] . \u2022 The shallow word-CNN computes much faster than the very deep char-CNN.",
  "y": "similarities"
 },
 {
  "id": "f2dfc35b67e47c12cba3cd0ec743a5_0",
  "x": "More specifically, in (i), I propose 1 E-mail:diego.raphael@gmail.com, diego@icmc.usp.br December 4, 2014 the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors [8] . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [<cite>17</cite>] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in [9] , [19] and [20]. In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels.",
  "y": "background"
 },
 {
  "id": "f2dfc35b67e47c12cba3cd0ec743a5_1",
  "x": "the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors [8] . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [<cite>17</cite>] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in [9] , [19] and [20] . In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels. Despite the limitations imposed by the restrict understanding of the mechanisms behind the classification, it is worth noting that the such representation remains entirely generic, being therefore useful to many tasks as well as for analyzing the evolution of languages, cultures and emotional trends.",
  "y": "background"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_0",
  "x": "A first study of object descriptions in this corpus shows that references in installments are quite common in this scenario and suggests that contextual factors partly determine their use. We discuss what new challenges this creates for NLG systems. ---------------------------------- **INTRODUCTION** Referring expression generation is classically considered to be the problem of producing a single noun phrase that uniquely identifies a referent (Krahmer and van Deemter, 2012) . This approach is well suited for non-interactive, static contexts, but recently, there has been increased interest in generation for situated dialog<cite> (Stoia, 2007</cite>; . Most human language use takes place in dynamic situations, and psycholinguistic research on humanhuman dialog has proposed that the production of referring expressions should rather be seen as a process that not only depends on the context and the choices of the speaker, but also on the reactions of the addressee.",
  "y": "background"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_1",
  "x": "Most human language use takes place in dynamic situations, and psycholinguistic research on humanhuman dialog has proposed that the production of referring expressions should rather be seen as a process that not only depends on the context and the choices of the speaker, but also on the reactions of the addressee. Thus the result is often not a single noun phrase but a sequence of installments (Clark and Wilkes-Gibbs, 1986) , consisting of multiple utterances which may be interleaved with feedback from the addressee. In a setting where the dialog partners have access to a common workspace, they, furthermore, carefully monitor each other's non-linugistic actions, which often replace verbal feedback (Clark and Krych, 2004; Gergle et al., 2004) . The following example from our data illustrates this. A is instructing B to press a particular button. While computational models of this behavior are still scarce, some first steps have been taken. <cite>Stoia (2007)</cite> studies instruction giving in a virtual environment and finds that references to target objects are often not made when they first become visible.",
  "y": "background"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_2",
  "x": "In the remaining cases, IFs press a button in response to the reference. In (3) and (4) a first reference utterance is followed by a separate object manipulation utterance. While in (3) the first reference uniquely identifies the target, in (4) the first utterance simply directs the player's attention to a group of buttons. The second utterance then picks out the target. <cite>Stoia (2007)</cite> observed that IGs use move instructions to focus the IF's attention on a particular area. This is also common in our data. For instance in (5), the IF is asked to turn to directly face the group of buttons containing the target.",
  "y": "background"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_3",
  "x": "<cite>Stoia (2007)</cite> observed that IGs use move instructions to focus the IF's attention on a particular area. This is also common in our data. For instance in (5), the IF is asked to turn to directly face the group of buttons containing the target. (5) also shows how IGs monitor their partners' actions and respond to them. The IF is moving towards the wrong button causing the IG to repeat part of the previous description. Similarly, in (6) the IG produces an elaboration when the IF stops moving towards the target, indicating her confusion. In (7) the IG inserts affirmative feedback when the IF reacts correctly to a portion of his utterance.",
  "y": "similarities"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_4",
  "x": "This led to interactions in which instructions are given in installments and linguistic and non-linguistic actions are interleaved. This poses interesting new questions for NLG systems, which we have illustrated by discussing the patterns of utterance sequences that IGs and IFs use in our corpus to agree on the objects that need to be manipulated. In line with results from psycholinguistics, we found that the information necessary to establish a reference is often expressed in multiple installments and that IGs carefully monitor how their partners react to their instructions and quickly respond by giving feedback, repeating information or elaborating on previous utterance when necessary. The NLG system thus needs to be able to decide when a complete identifying description can be given in one utterance and when a description in installments is more effective. <cite>Stoia (2007)</cite> as well as have addressed this question, but their approaches only make a choice between generating an instruction to move or a uniquely identifying referring expression. They do not consider cases in which another type of utterance, for instance, one that refers to a group of objects or gives an initial ambiguous description, is used to draw the attention of the IF to a particular area and they do not generate referring expressions in installments. The system, furthermore, needs to be able to interpret the IF's actions and decide when to insert an acknowledgment, elaboration or correction.",
  "y": "differences"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_5",
  "x": "They do not consider cases in which another type of utterance, for instance, one that refers to a group of objects or gives an initial ambiguous description, is used to draw the attention of the IF to a particular area and they do not generate referring expressions in installments. The system, furthermore, needs to be able to interpret the IF's actions and decide when to insert an acknowledgment, elaboration or correction. It then has to decide how to formulate this feedback. The addressee, e.g., needs to be able to distinguish elaborations from corrections. If the feedback was inserted in the middle of a sentence, if finally has to decide whether this sentence should be completed and how the remainder may have to be adapted. Once we have finished the corpus collection, we plan to use it to study and address the questions discussed above. We are planning on building on the work by <cite>Stoia (2007)</cite> on using machine learning techniques to develop a model that takes into account various contextual factors and on the work by Thompson (2009) on generating references in installments.",
  "y": "future_work"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_0",
  "x": "By introducing an additional blank symbol and a specially defined loss function aggregating many allowed paths within a graph, CTC model can be optimized to generate the correct character sequences from the speech signals regardless of the blank symbols interspersed among. The seq2seq models, on the other hand, simply maximized the likelihood of observing the decoded sequence given the ground truth at every time step. With many recent results [9, 10, 11, 12, <cite>13]</cite> approaching the stateof-the-art, end-to-end deep learning has definitely been a very important direction for speech recognition. Most end-to-end speech recognition approaches require a considerable amount of paired audio-text data, which is costly and time-consuming. Semi-supervised approaches [14, 15, end-to-end speech recognition. The two steps here are conducted iteratively: (a) a Criticizing Language Model (CLM) is trained to evaluate the quality score given a text sequence, and (b) and ASR model is trained to minimize the sequence loss calculated with ground truth while maximizing the scores given by CLM. 16, 17] have been developed to address such problem by involving unpaired text data (which are relatively easy to obtain) in the training progress.",
  "y": "background"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_1",
  "x": "16, 17] have been developed to address such problem by involving unpaired text data (which are relatively easy to obtain) in the training progress. One approach is to utilize unpaired text data to produce a separately trained language model (LM) to rescore the output of the end-to-end approach [18,<cite> 13,</cite> 19, 20] , but at the price of extra computation during testing. Also, in this way the unpaired text data and paired audio-text data were used separately, and the machines could not learn from them jointly. Another approach is to back-translate (synthesize) speech signals or encoder state sequences [17, 21, 22] from the unpaired text data, so they can be jointly learned in training. However, the improvements achievable with such approaches were limited by the quality of the synthesized data, which is usually far from real. The Generative Adversarial Networks (GANs) [23] have been shown to be very successful in diversified application areas. Instead of learning from a set of ground truth taken as the upper bound for learning, a generator model and a discriminator model are trained iteratively to challenge and learn from each other step by step.",
  "y": "background"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_2",
  "x": "The reason a convolution-based network instead of a recurrent network is used in Fig 2 is twofold. Convolution with small window size captures local relation features, which can then be averaged over time. Also, CNN based network is relatively more computationally efficient, which is important in adversarial training. But other network architectures such as RNN-LM<cite> [13]</cite> can also be used here. Loss Function. A major problem here is that soft distribution vectors produced by the ASR model is very different from one-hot vectors for real text data, making the task of CLM trivial, and the ASR model almost always fail to compete against it. Thanks to Wasserstein GAN (WGAN) [24] which addressed the above problem to some good extent.",
  "y": "background"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_3",
  "x": "---------------------------------- **ASR MODEL** Network Architecture. Any network architecture for end-toend speech recognition can be used here, while Fig. 3 gives the one used in this work, following the previous work<cite> [13]</cite> of integrating attentioned Seq2seq with CTC. The model takes a sequence of speech features O = o 1 , o 2 , ..., o N with length N as the input. O is encoded into sequence of hidden state H = h 1 , h 2 , ..., h T by the encoder (consists of a VGG extractor performing input downsampling followed by several BLSTM layers) with T being the output sequence length. The decoder is a single layer LSTM maintaining its own hidden state q. For each time index t, location-aware attention mechanism [7] Attention is used to integrate H with the previous decoder state q t\u22121 to generate the context vector c t .",
  "y": "similarities uses"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_4",
  "x": "The ASR model outputs two character sequences,\u1ef9 =\u1ef9 1 ,\u1ef9 2 , ...,\u1ef9 T andy =y 1 ,y 2 , ...,y T , respectively supervised by Seq2seq loss and CTC loss. CLM only takes\u1ef9 as input. During testing,\u1ef9 andy are integrated into a single output sequence just as done in the previous work<cite> [13]</cite> . Seq2seq Loss. The Seq2seq ASR model is to estimate the posterior probability, The loss function of the Seq2seq model model can be then computed as below, except here y = y 1 , y 2 , ..., y T is the ground truth of O with length T .",
  "y": "similarities uses"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_5",
  "x": "100 hours of clean speech data and their transcriptions are used as the paired data. We took the text of other 360 hours of clean speech and 500 hours of noisy speech and utilized them as the unpaired data (text-only). The clean development set and clean test set were used for evaluation. We used the end-to-end speech processing toolkit ESPnet [28] for data preprocessing and customized it for our adversarial training processes. We followed the previous work<cite> [13,</cite> 21] to use 80-dimensional log Mel-filter bank and 3-dimensional pitch features as the acoustic features. Text data are represented by sequences of 5000 subword units one-hot vectors to avoid OOV. For the CLM model, the dimension of the output of all layers were set to 128 except the last.",
  "y": "similarities uses"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_6",
  "x": "Text data are represented by sequences of 5000 subword units one-hot vectors to avoid OOV. For the CLM model, the dimension of the output of all layers were set to 128 except the last. The first convolution had a window size of 2 and stride of 1, and the second had window size 3 and stride 1. Batch normalization is applied between layers. For the ASR model, the encoder included a 6layer VGG extractor with downsampling used in the previous work<cite> [13]</cite> and a 5-layer BLSTM with 320 units per direction. 300-dimensional location-aware attention [7] was used in the attention layer. The decoder was a single layer LSTM with 320 units.",
  "y": "similarities uses"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_7",
  "x": "\u03bb CLM is set to 10 \u22124 since CLM output value was usually much higher than other loss values. Also, the update frequency of CLM is set to 5 times less than the ASR model to stabilize AT process. Table 1 . Speech recognition performance. \"+LM\" refers to shallow fusion decoding jointly with RNN-LM<cite> [13]</cite> , \"+AT\" refers to the adversarial training proposed here, \"+Both\" indicates training with AT and joint decoding with RNN-LM, and BT is the prior work of back-translation [21] . ---------------------------------- **EXPERIMENTAL RESULTS** In the experiments, the ASR model was trained on the 100 hours speech data but combined with different amount of unpaired text utilized in different ways.",
  "y": "background"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_8",
  "x": "Table 1 . Speech recognition performance. \"+LM\" refers to shallow fusion decoding jointly with RNN-LM<cite> [13]</cite> , \"+AT\" refers to the adversarial training proposed here, \"+Both\" indicates training with AT and joint decoding with RNN-LM, and BT is the prior work of back-translation [21] . ---------------------------------- **EXPERIMENTAL RESULTS** In the experiments, the ASR model was trained on the 100 hours speech data but combined with different amount of unpaired text utilized in different ways. The results are listed in Table 1 , where \"Baseline\" refers to the plain end-toend speech recognition framework as described in Sec. 2.3, \"+LM\" refers to the shallow fusion decoding with a separately trained RNN language model (RNN-LM)<cite> [13,</cite> 20] and \"+AT\" refers to the adversarial training proposed here. AT is actually compatible with any existing end-to-end speech recognition decoding approach, so \"+Both\" refers to training with AT while jointly decoding with RNN-LM.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_0",
  "x": "---------------------------------- **INTRODUCTION** Knowledge and/or data is often modeled in a structure, such as indexes, tables, key-value pairs, or triplets. These data, by their nature (e.g., raw data or long time-series data), are not easily usable by humans; outlining their crucial need to be synthesized. Recently, numerous works have focused on leveraging structured data in various applications, such as question answering [24, 34] or table retrieval [7, 32] . One emerging research field consists in transcribing data-structures into natural language in order to ease their understandablity and their usablity. This field is referred to as \"data-to-text\" [8] and has its place in several application domains (such as journalism [22] or medical diagnosis [25] ) or wide-audience applications (such as financial [26] and weather reports [30] , or sport broadcasting [4, <cite>39]</cite> ).",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_1",
  "x": "One emerging research field consists in transcribing data-structures into natural language in order to ease their understandablity and their usablity. This field is referred to as \"data-to-text\" [8] and has its place in several application domains (such as journalism [22] or medical diagnosis [25] ) or wide-audience applications (such as financial [26] and weather reports [30] , or sport broadcasting [4, <cite>39]</cite> ). As an example, Figure 1 shows a data-structure containing statistics on NBA basketball games, paired with its corresponding journalistic description. Designing data-to-text models gives rise to two main challenges: 1) understanding structured data and 2) generating associated descriptions. Recent datato-text models [18, 28, 29, <cite>39]</cite> mostly rely on an encoder-decoder architecture [2] in which the data-structure is first encoded sequentially into a fixed-size vectorial representation by an encoder. Then, a decoder generates words conditioned on this representation. With the introduction of the attention mechanism [19] on one hand, which computes a context focused on important elements from the input at each decoding step and, on the other hand, the copy mechanism Fig. 1 : Example of structured data from the RotoWire dataset.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_2",
  "x": "To address these shortcomings, we propose a new structured-data encoder assuming that structures should be hierarchically captured. Our contribution focuses on the encoding of the data-structure, thus the decoder is chosen to be a classical module as used in [28, <cite>39]</cite> . Our contribution is threefold: -We model the general structure of the data using a two-level architecture, first encoding all entities on the basis of their elements, then encoding the data structure on the basis of its entities; -We introduce the Transformer encoder [36] in data-to-text models to ensure robust encoding of each element/entities in comparison to all others, no matter their initial positioning; -We integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder. We report experiments on the RotoWire benchmark<cite> [39]</cite> which contains around 5K statistical tables of NBA basketball games paired with humanwritten descriptions. Our model is compared to several state-of-the-art models. Results show that the proposed architecture outperforms previous models on BLEU score and is generally better on qualitative metrics.",
  "y": "similarities"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_3",
  "x": "We report experiments on the RotoWire benchmark<cite> [39]</cite> which contains around 5K statistical tables of NBA basketball games paired with humanwritten descriptions. Our model is compared to several state-of-the-art models. Results show that the proposed architecture outperforms previous models on BLEU score and is generally better on qualitative metrics. In the following, we first present a state-of-the art of data-to-text literature (Section 2), and then describe our proposed hierarchical data encoder (Section 3). The evaluation protocol is presented in Section 4, followed by the results (Section 5). Section 6 concludes the paper and presents perspectives. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_4",
  "x": "**RELATED WORK** Until recently, efforts to bring out semantics from structured-data relied heavily on expert knowledge [6, 30] . For example, in order to better transcribe numerical time series of weather data to a textual forecast, Reiter et al. [30] devise complex template schemes in collaboration with weather experts to build a consistent set of data-to-word rules. Modern approaches to the wide range of tasks based on structured-data (e.g. table retrieval [7, 41] , table classification [9] , question answering [12] ) now propose to leverage progress in deep learning to represent these data into a semantic vector space (also called embedding space). In parallel, an emerging task, called \"data-to-text\", aims at describing structured data into a natural language description. This task stems from the neural machine translation (NMT) domain, and early work [1, 15, <cite>39]</cite> represent the data records as a single sequence of facts to be entirely translated into natural language. Wiseman et al.<cite> [39]</cite> show the limits of traditional NMT systems on larger structured-data, where NMT systems fail to accurately extract salient elements.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_5",
  "x": "Until recently, efforts to bring out semantics from structured-data relied heavily on expert knowledge [6, 30] . For example, in order to better transcribe numerical time series of weather data to a textual forecast, Reiter et al. [30] devise complex template schemes in collaboration with weather experts to build a consistent set of data-to-word rules. Modern approaches to the wide range of tasks based on structured-data (e.g. table retrieval [7, 41] , table classification [9] , question answering [12] ) now propose to leverage progress in deep learning to represent these data into a semantic vector space (also called embedding space). In parallel, an emerging task, called \"data-to-text\", aims at describing structured data into a natural language description. This task stems from the neural machine translation (NMT) domain, and early work [1, 15, <cite>39]</cite> represent the data records as a single sequence of facts to be entirely translated into natural language. Wiseman et al.<cite> [39]</cite> show the limits of traditional NMT systems on larger structured-data, where NMT systems fail to accurately extract salient elements. To improve these models, a number of work [16, 28, 40] proposed innovating decoding modules based on planning and templates, to ensure factual and coherent mentions of records in generated descriptions.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_6",
  "x": "On the other hand, Liu et al. [18, 17] rather focus on introducing structure into the encoder. For instance, they propose a dual encoder [17] which encodes separately the sequence of element names and the sequence of element values. These approaches are however designed for single-entity data structures and do not account for delimitation between entities. Our contribution differs from previous work in several aspects. First, instead of flatly concatenating elements from the data-structure and encoding them as a sequence [18, 28, <cite>39]</cite> , we constrain the encoding to the underlying structure of the input data, so that the delimitation between entities remains clear throughout the process. Second, unlike all works in the domain, we exploit the Transformer architecture [36] and leverage its particularity to directly compare elements with each others in order to avoid arbitrary assumptions on their ordering. Finally, in contrast to [5, 29] that use a complex updating mechanism to obtain a dynamic representation of the input data and its entities, we argue that explicit hierarchical encoding naturally guides the decoding process via hierarchical attention.",
  "y": "differences"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_7",
  "x": "\u2022 The dataset D is a collection of N aligned (data-structure, description) pairs (s, y). For instance, Figure 1 illustrates a data-structure associated with a description. The data-structure includes a set of entities (Hawks, Magic, Al Horford, Jeff Teague, ...). The entity Jeff Teague is modeled as a set of records {(PTS, 17), (REB, 0), (AST, 7) ...} in which, e.g., the record (PTS, 17) is characterized by a key (PTS) and a value (17) . For each data-structure s in D, the objective function aims to generate a description\u0177 as close as possible to the ground truth y. This objective function optimizes the following log-likelihood over the whole dataset D: where \u03b8 stands for the model parameters and P (\u0177 = y | s; \u03b8) the probability of the model to generate the adequate description y for table s. During inference, we generate the sequence\u0177 * with the maximum a posteriori probability conditioned on table s. Using the chain rule, we get: This equation is intractable in practice, we approximate a solution using beam search, as in [18, 17, 28, 29, <cite>39]</cite> .",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_8",
  "x": "Our model follows the encoder-decoder architecture [2] . Because our contribution focuses on the encoding process, we chose the decoding module used in [28, <cite>39]</cite> : a two-layers LSTM network with a copy mechanism. In order to supervise this mechanism, we assume that each record value that also appears in the target is copied from the data-structure and we train the model to switch between freely generating words from the vocabulary and copying words from the input. We now describe the hierarchical encoder and the hierarchical attention. ---------------------------------- **HIERARCHICAL ENCODING MODEL** As outlined in Section 2, most previous work [16, 28, 29,<cite> 39,</cite> 40 ] make use of flat encoders that do not exploit the data structure.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_9",
  "x": "In order to supervise this mechanism, we assume that each record value that also appears in the target is copied from the data-structure and we train the model to switch between freely generating words from the vocabulary and copying words from the input. We now describe the hierarchical encoder and the hierarchical attention. ---------------------------------- **HIERARCHICAL ENCODING MODEL** As outlined in Section 2, most previous work [16, 28, 29,<cite> 39,</cite> 40 ] make use of flat encoders that do not exploit the data structure. To keep the semantics of each element from the data-structure, we propose a hierarchical encoder which relies on two modules. The first one (module A in Figure 2) is called low-level encoder and encodes entities on the basis of their records; the second one (module B), called high-level encoder, encodes the data-structure on the basis of its underlying entities.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_10",
  "x": "As outlined in Section 2, most previous work [16, 28, 29,<cite> 39,</cite> 40 ] make use of flat encoders that do not exploit the data structure. To keep the semantics of each element from the data-structure, we propose a hierarchical encoder which relies on two modules. The first one (module A in Figure 2) is called low-level encoder and encodes entities on the basis of their records; the second one (module B), called high-level encoder, encodes the data-structure on the basis of its underlying entities. In the low-level encoder, the traditional embedding layer is replaced by a record embedding layer as in [18, 28, <cite>39]</cite> . We present in what follows the record embedding layer and introduce our two hierarchical modules. Record Embedding Layer. The first layer of the network consists in learning two embedding matrices to embed the record keys and values.",
  "y": "similarities"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_11",
  "x": "Keys k i,j are embedded to k i,j \u2208 R d and values v i,j to v i,j \u2208 R d , with d the size of the embedding. As in previous work [18, 28, <cite>39]</cite> , each record embedding r i,j is computed by a linear projection on the concatenation [k i,j ; v i,j ] followed by a non linearity: where W r \u2208 R 2d\u00d7d and b r \u2208 R d are learnt parameters. The low-level encoder aims at encoding a collection of records belonging to the same entity while the high-level encoder encodes the whole set of entities. Both the low-level and high-level encoders consider their input elements as unordered. We use the Transformer architecture from [36] . For each encoder, we have the following peculiarities:",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_12",
  "x": "Please note that the different embeddings and the model parameters presented in the model components are learnt using Equation 1. ---------------------------------- **EXPERIMENTAL SETUP** ---------------------------------- **THE ROTOWIRE DATASET** To evaluate the effectiveness of our model, and demonstrate its flexibility at handling heavy data-structure made of several types of entities, we used the Ro-toWire dataset<cite> [39]</cite> . It includes basketball games statistical tables paired with journalistic descriptions of the games, as can be seen in the example of Figure 1 .",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_13",
  "x": "Entities are of two types, either team or player, and player descriptions depend on their involvement in the game. We followed the data partitions introduced with the dataset and used a train/validation/test sets of respectively 3, 398/727/728 (data-structure, description) pairs. ---------------------------------- **EVALUATION METRICS** We evaluate our model through two types of metrics. The BLEU score [23] aims at measuring to what extent the generated descriptions are literally closed to the ground truth. The second category designed by<cite> [39]</cite> is more qualitative.",
  "y": "uses background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_14",
  "x": "It estimates the correspondence between a machine output and that of a human by computing the number of co-occurrences for ngrams (n \u2208 1, 2, 3, 4) between the generated candidate and the ground truth. We use the implementation code released by [27] . Information extraction-oriented metrics. These metrics estimate the ability of our model to integrate elements from the table in its descriptions. Particularly, they compare the gold and generated descriptions and measure to what extent the extracted relations are aligned or differ. To do so, we follow the protocol presented in<cite> [39]</cite> . First, we apply an information extraction (IE) system trained on labeled relations from the gold descriptions of the RotoWire train dataset.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_15",
  "x": "**BASELINES** We compare our hierarchical model against three systems. For each of them, we report the results of the best performing models presented in each paper. \u2022 Wiseman<cite> [39]</cite> is a standard encoder-decoder system with copy mechanism. \u2022 Li [16] is a standard encoder-decoder with a delayed copy mechanism: text is first generated with placeholders, which are replaced by salient records extracted from the table by a pointer network. \u2022 Puduppully-plan [28] acts in two steps: a first standard encoder-decoder generates a plan, i.e. a list of salient records from the table; a second standard encoder-decoder generates text from this plan. \u2022 Puduppully-updt [29] .",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_16",
  "x": "---------------------------------- **BASELINES** We compare our hierarchical model against three systems. For each of them, we report the results of the best performing models presented in each paper. \u2022 Wiseman<cite> [39]</cite> is a standard encoder-decoder system with copy mechanism. \u2022 Li [16] is a standard encoder-decoder with a delayed copy mechanism: text is first generated with placeholders, which are replaced by salient records extracted from the table by a pointer network. \u2022 Puduppully-plan [28] acts in two steps: a first standard encoder-decoder generates a plan, i.e. a list of salient records from the table; a second standard encoder-decoder generates text from this plan.",
  "y": "uses background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_17",
  "x": "As a consequence, the model uses standard attention. is closest to Wiseman, with the exception that we use a Transformer to encode the input sequence instead of an RNN. \u2022 Hierarchical-kv is our full hierarchical model, with traditional hierarchical attention, i.e. where attention over records is computed on the full record encoding, as in equation (5). \u2022 Hierarchical-k is our full hierarchical model, with key-guided hierarchical attention, i.e. where attention over records is computed only on the record key representations, as in equation (6). ---------------------------------- **IMPLEMENTATION DETAILS** The decoder is the one used in [28, 29, <cite>39]</cite> with the same hyper-parameters. For the encoder module, both the low-level and high-level encoders use a two-layers multi-head self-attention with two heads.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_18",
  "x": "---------------------------------- **IMPLEMENTATION DETAILS** The decoder is the one used in [28, 29, <cite>39]</cite> with the same hyper-parameters. For the encoder module, both the low-level and high-level encoders use a two-layers multi-head self-attention with two heads. To fit with the small number of record keys in our dataset<cite> (39)</cite> , their embedding size is fixed to 20. The size of the record value embeddings and hidden layers of the Transformer encoders are both set to 300. We use dropout at rate 0.5.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_19",
  "x": "All the models are implemented in OpenNMT-py [14] . All code is available at https://github.com/KaijuML/data-to-text-hierarchical ---------------------------------- **RESULTS** Our results on the RotoWire testset are summarized in Table 1 . For each proposed variant of our architecture, we report the mean score over ten runs, as well as the standard deviation in subscript. Results are compared to baselines [28, 29, <cite>39]</cite> and variants of our models.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_20",
  "x": "Qualitative metrics are either better or on par with baselines. We show in Figure  4 a text generated by our best model, which can be directly compared to the gold description in Figure 1 . Generation is fluent and contains domain-specific expressions. As reflected in Table 1 , the number of correct mentions (in green) outweights the number of incorrect mentions (in red). Please note that, as in previous work [16, 28, 29, <cite>39]</cite> , generated texts still contain a number of incorrect facts, as well hallucinations (in blue): sentences that have no basis in the input data (e.g. \"[...] he's now averaging 22 points [...].\"). While not the direct focus of our work, this highlights that any operation meant to enrich the semantics of structured data can also enrich the data with incorrect facts. Specifically, regarding all baselines, we can outline the following statements.",
  "y": "background"
 },
 {
  "id": "f36b605a9088532e5f430c86ffb363_0",
  "x": "****MEASURING SEMANTIC RELATEDNESS WITH VECTOR SPACE MODELS AND RANDOM WALKS**** **ABSTRACT** Both vector space models and graph random walk models can be used to determine similarity between concepts. Noting that vectors can be regarded as local views of a graph, we directly compare vector space models and graph random walk models on standard tasks of predicting human similarity ratings, concept categorization, and semantic priming, varying the size of the dataset from which vector space and graph are extracted. ---------------------------------- **INTRODUCTION** Vector space models, representing word meanings as points in high-dimensional space, have been used in a variety of semantic relatedness tasks (Sahlgren, 2006; <cite>Pad\u00f3 and Lapata, 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "f36b605a9088532e5f430c86ffb363_1",
  "x": "Noting that vectors can be regarded as local views of a graph, we directly compare vector space models and graph random walk models on standard tasks of predicting human similarity ratings, concept categorization, and semantic priming, varying the size of the dataset from which vector space and graph are extracted. ---------------------------------- **INTRODUCTION** Vector space models, representing word meanings as points in high-dimensional space, have been used in a variety of semantic relatedness tasks (Sahlgren, 2006; <cite>Pad\u00f3 and Lapata, 2007)</cite> . Graphs are another way of representing relations between linguistic entities, and they have been used to capture semantic relatedness by using both corpus-based evidence and the graph structure of WordNet and Wikipedia (Pedersen et al., 2004; Widdows and Dorow, 2002; Minkov and Cohen, 2008) . We study the relationship between vector space models and graph random walk models by embedding vector space models in graphs. The flexibility offered by graph random walk models allows us to compare the vector space-based similarity measures to extended notions of relatedness and similarity.",
  "y": "background uses"
 },
 {
  "id": "f36b605a9088532e5f430c86ffb363_2",
  "x": "Dimensionality reduction techniques like Singular Value Decomposition (SVD) also capture these higher-order effects, and it has been argued that that makes them more resistant against sparseness (Sch\u00fctze, 1997) . To our knowledge, no systematic comparison of SVD and classical vector-based methods has been done on different corpus sizes. In our experiments, we will compare the performance of SVD and flexible-walk smoothing at different corpus sizes and for a variety of tasks. Implementation: We extract tuples from the 2-billion word ukWaC corpus, 1 dependency-parsed with MINIPAR. 2 Following<cite> Pad\u00f3 and Lapata (2007)</cite> , we only consider co-occurrences where two target words are connected by certain dependency paths, namely: the top 30 most frequent preposition-mediated noun-to-noun paths (soldier+with+gun), the top 50 transitive-verbmediated noun-to-noun paths (soldier+use+gun), the top 30 direct or preposition-mediated verbnoun paths (kill+obj+victim, kill+in+school), and the modifying and predicative adjective-to-noun paths. Pairs (w 1 , w 2 ) that account for 0.01% or less of the marginal frequency of w 1 were trimmed. The resulting tuple list, with raw counts converted to mutual information scores, contains about 25 million tuples.",
  "y": "uses"
 },
 {
  "id": "f36b605a9088532e5f430c86ffb363_3",
  "x": "Concept categorization: Almuhareb (2006) proposed a set of 402 nouns to be categorized into 21 classes of both concrete (animals, fruit. . . ) and abstract (feelings, times. . . ) concepts. Our results on this clustering task are given in Table 1 (line  2) . The difference between SVD and pure-vector models is negligible and they both obtain the best performance in terms of both cluster entropy (not shown in the table) and purity. Both models' performances are comparable with the previously reported studies, and above that of random walks. Semantic priming: The next dataset comes from Hodgson (1991) and it is of interest since it requires capturing different forms of semantic relatedness between prime-target pairs: synonyms (synonym), coordinates (coord), antonyms (antonym), free association pairs (conass), superand subordinate pairs (supersub) and phrasal associates (phrasacc). Following previous simulations of this data-set<cite> (Pad\u00f3 and Lapata, 2007)</cite> , we measure the similarity of each related target-prime pair, and we compare it to the average similarity of the target to all the other primes instantiating the same relation, treating the latter quantity as our surrogate of an unrelated target-prime pair. We report results in terms of differences between unrelated and related pairs, normalized to t-scores, marking significance according to twotailed paired t-tests for the relevant degrees of freedom.",
  "y": "uses"
 },
 {
  "id": "f3b1a39203ebf0725d8dd2b8f8c7a9_0",
  "x": "In this work, we use a Generative Adversarial Network framework to alleviate this problem. We evaluate our framework on poetry, lyrics and metaphor datasets, each with widely different characteristics, and report better performance of our objective function over other generative models. ---------------------------------- **INTRODUCTION AND RELATED WORK** Language models can be optimized to recognize syntax and semantics with great accuracy [1] . However, the output generated can be repetitive and generic leading to monotonous or uninteresting responses (e.g \"I don't know\") regardless of the input <cite>[2]</cite> . While application of attention [3, 4] and advanced decoding mechanisms like beam search and variation sampling [5] have shown improvements, it does not solve the underlying problem.",
  "y": "background"
 },
 {
  "id": "f3b1a39203ebf0725d8dd2b8f8c7a9_1",
  "x": "Previous work on handling the shortcomings of MLE include length-normalizing sentence probability [6] , future cost estimation [7] , diversity-boosting objective function [8,<cite> 2]</cite> or penalizing repeating tokens [9] . When it comes to poetry generation using generative text models, Zhang and Lapata [10] , Yi et al. [11] and Wang et al. [12] use language modeling to generate Chinese poems. However, none of these methods provide feedback on the quality of the generated sample and hence, do not address the qualitative objective required for creative decoding. For the task of text generation, MaskGAN [13] uses a Reinforcement Learning signal from the discriminator, FMD-GAN [14] uses an optimal transport mechanism as an objective function. GumbelGAN [15] uses Gumbel-Softmax distribution that replaces the non-differentiable sample from a categorical distribution with a differentiable sample to propagate stronger gradients. Li et al. <cite>[2]</cite> use a discriminator for a diversity promoting objective. Yu et al. [16] use SeqGAN to generate poetry and comment on the performance of SeqGAN over MLE in human evaluations, encouraging our study of GANs for creative text generation.",
  "y": "background"
 },
 {
  "id": "f3b1a39203ebf0725d8dd2b8f8c7a9_2",
  "x": "When it comes to poetry generation using generative text models, Zhang and Lapata [10] , Yi et al. [11] and Wang et al. [12] use language modeling to generate Chinese poems. However, none of these methods provide feedback on the quality of the generated sample and hence, do not address the qualitative objective required for creative decoding. For the task of text generation, MaskGAN [13] uses a Reinforcement Learning signal from the discriminator, FMD-GAN [14] uses an optimal transport mechanism as an objective function. GumbelGAN [15] uses Gumbel-Softmax distribution that replaces the non-differentiable sample from a categorical distribution with a differentiable sample to propagate stronger gradients. Li et al. <cite>[2]</cite> use a discriminator for a diversity promoting objective. Yu et al. [16] use SeqGAN to generate poetry and comment on the performance of SeqGAN over MLE in human evaluations, encouraging our study of GANs for creative text generation. However, these studies do not focus solely on creative text.",
  "y": "background"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_0",
  "x": "A topic is traditionally represented by a list of t terms with the highest probability. In recent works, short phrases [11, 4] , images<cite> [3]</cite> or summaries [19] have been used as alternatives. Particularly, images offer a language independent representation of the topic which can also be complementary to textual labels. The visual representation of a topic has been shown to be as effective as the textual labels on retrieving information using a topic browser while it can be understood quickly by the users [1, 2] . The task of labeling topics consists of two main components: (1) a candidate generation component where candidate labels are obtained for a given topic (usually using information retrieval techniques and knowledge bases [11, <cite>3]</cite> ), and (2) a ranking (or label selection) component that scores the candidates according to their relevance to the topic. In the case of labeling topics with images the candidate labels consist of images. The method presented by<cite> [3]</cite> generates a graph where the candidate images are its nodes.",
  "y": "background"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_1",
  "x": "Particularly, images offer a language independent representation of the topic which can also be complementary to textual labels. The visual representation of a topic has been shown to be as effective as the textual labels on retrieving information using a topic browser while it can be understood quickly by the users [1, 2] . The task of labeling topics consists of two main components: (1) a candidate generation component where candidate labels are obtained for a given topic (usually using information retrieval techniques and knowledge bases [11, <cite>3]</cite> ), and (2) a ranking (or label selection) component that scores the candidates according to their relevance to the topic. In the case of labeling topics with images the candidate labels consist of images. The method presented by<cite> [3]</cite> generates a graph where the candidate images are its nodes. The edges are weighted with a similarity score between the images that connect. Then, an image is selected by re-ranking the candidates using PageRank.",
  "y": "background"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_2",
  "x": "The visual representation of a topic has been shown to be as effective as the textual labels on retrieving information using a topic browser while it can be understood quickly by the users [1, 2] . The task of labeling topics consists of two main components: (1) a candidate generation component where candidate labels are obtained for a given topic (usually using information retrieval techniques and knowledge bases [11, <cite>3]</cite> ), and (2) a ranking (or label selection) component that scores the candidates according to their relevance to the topic. In the case of labeling topics with images the candidate labels consist of images. The method presented by<cite> [3]</cite> generates a graph where the candidate images are its nodes. The edges are weighted with a similarity score between the images that connect. Then, an image is selected by re-ranking the candidates using PageRank. The method is iterative and has a runtime complexity of O(n 2 ) which makes it infeasible to run over large number of images.",
  "y": "background"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_3",
  "x": "The visual information from the image V is converted into a dense vectorized representation, x v . That is the output of the publicly available 16-layer VGG-net [1<cite>3]</cite> trained over the ImageNet dataset [9] . VGG-net provides a 1000 dimensional vector which is the soft-max classification output of ImageNet classes. The input to the network is the concatenation of topic, caption and visual vectors. i.e., This results in a 1600-dimensional input vector. Then, X is passed through a series of four hidden layers, H 1 , ..., H 4 .",
  "y": "uses"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_4",
  "x": "**EXPERIMENTAL SETUP** We evaluate our model on the publicly available data set provided by<cite> [3]</cite> . It consists of 300 topics generated using Wikipedia articles and news articles taken from the New York Times. Each topic is represented by ten terms with the highest probability. They are also associated with 20 candidate image labels and their human ratings between 0 (lowest) and 3 (highest) denoting the appropriateness of these images for the topic. That results into a total of 6K images and their associated textual metadata which are considered as captions. The task is to choose the image with the highest rating from the set of the 20 candidates for a given topic.",
  "y": "uses"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_5",
  "x": "The task is to choose the image with the highest rating from the set of the 20 candidates for a given topic. The 20 candidate image labels per topic are collected by<cite> [3]</cite> using an information retrieval engine (Google). Hence most of them are expected to be relevant to the topic. This jeopardizes the training of our supervised model due to the lack of sufficient negative examples. To address this issue we generate extra negative examples. For each topic we sample another 20 images from random topics in the training set and assign them a relevance score of 0. These extra images are added into the training data.",
  "y": "uses"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_6",
  "x": "Hence most of them are expected to be relevant to the topic. This jeopardizes the training of our supervised model due to the lack of sufficient negative examples. To address this issue we generate extra negative examples. For each topic we sample another 20 images from random topics in the training set and assign them a relevance score of 0. These extra images are added into the training data. Our evaluation follows prior work [11, <cite>3]</cite> using two metrics. The Top-1 average rating is the average human rating assigned to the top-ranked label proposed by the topic labeling method.",
  "y": "uses"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_7",
  "x": "We train the model in a 5-fold crossvalidation for 30 epochs and set the batch size for training data to 16. In each fold, data from 240 topics are used for training which results into 9,600 examples (20 original, 20 negative candidates per topic). The rest completely unseen 60 topics are used for testing which results into 1,200 test examples (note that we do not add negative examples in the test data). ---------------------------------- **RESULTS AND DISCUSSION** We compare our approach to the state-of-the-art method that uses Personalized PageRank<cite> [3]</cite> to re-rank image candidates (Local PPR) and an adapted version that computes the PageRank scores of all the available images in the test set (Global PPR). We also test other baselines methods: (1) a relevant approach originally proposed for image annotation that learns a joint model of text and image features (WSABIE) [20] , (2) linear regression and SVM models that use the concatenation of the topic, the caption and the image vectors as input, LR (Topic+Caption+VGG) and SVM (Topic+Caption+VGG) respectively.",
  "y": "uses"
 },
 {
  "id": "f3e9e5d7fb4001e3d29a171b5eb4a4_1",
  "x": "In the last years, several methods based on neural networks have been devised to Figure 1 . Utterances are translated into SPARQL queries encoded as sequences of tokens. Using complex surface forms leads to more graph patterns. We aim at learning these compositions. tackle the KBQA problem <cite>(Liang et al., 2016</cite>; Hao et al., 2017; Lukovnikov et al., 2017; Sorokin & Gurevych, 2017) . We study the application of the Neural Machine Translation paradigm for question parsing using a sequence-to-sequence model within an architecture dubbed Neural SPARQL Machine, previously introduced in Soru et al. (2017) . Similarly to<cite> Liang et al. (2016)</cite> , we employ a sequence-to-sequence model to learn query expressions and their compositions.",
  "y": "similarities uses"
 },
 {
  "id": "f3e9e5d7fb4001e3d29a171b5eb4a4_2",
  "x": "Similarly to<cite> Liang et al. (2016)</cite> , we employ a sequence-to-sequence model to learn query expressions and their compositions. Instead of inducing the programs through question-answer pairs, we expect a semi-supervised approach, where alignments between questions and queries are built through templates. Although query induction can save a considerable amount of supervision effort <cite>(Liang et al., 2016</cite>; Zhong et al., 2017) , a pseudo-gold program is not guaranteed to be correct when the same answer can be found with more than one query (e.g., as the capital is often the largest city of a country, predicates might be confused). On the contrary, our proposed solution relies on manual annotation and a weakly-supervised expansion of question-query templates. ---------------------------------- **NEURAL SPARQL MACHINES** Inspired by the Neural Programmer-Interpreter pattern by (Reed & De Freitas, 2015) , a Neural SPARQL Machine is composed by three modules: a generator, a learner, and an interpreter (Soru et al., 2017) .",
  "y": "background"
 },
 {
  "id": "f3e9e5d7fb4001e3d29a171b5eb4a4_3",
  "x": "Here, a recurrent neural network based on (Bidirectional) Long Short-Term Memories (Hochreiter & Schmidhuber, 1997 ) is employed as a sequence-to-sequence translator (see example in Figure 1 ). The final structure is then reconstructed by the interpreter through rule-based heuristics. Note that a sequence can be represented by any LISP S-expression; therefore, alternatively, sentence dependency trees can be used to encode questions and ARQ algebra (Seaborne, 2010) can be used to encode SPARQL queries. Neural SPARQL Machines do not rely on entity linking methods, since entities and relations are detected within the query construction phase. External pre-trained word embeddings help deal with vocabulary mismatch. Knowledge graph jointly embedded with SPARQL operators (Wang et al., 2014) can be utilized in the target space. A curriculum learning (Bengio et al., 2009 ) paradigm can learn graph pattern and SPARQL operator composition, in a similar fashion of<cite> Liang et al. (2016)</cite> .",
  "y": "background"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_0",
  "x": "Most of the presented works study the interrelationship between words in a text snip- pet (Hill et al., 2016; Kiros et al., 2015; <cite>Le and Mikolov, 2014)</cite> in an unsupervised fashion. Other methods build a task specific representation (Kim, 2014; Collobert et al., 2011) . In this paper we propose to use the covariance matrix of the word vectors in some document to define a novel descriptor for a document. We call our representation DoCoV descriptor. Our descriptor obtains a fixed-length representation of the paragraph which captures the interrelationship between the dimensions of the word embedding via the covariance matrix elements. This makes our work distinguished from to the work of<cite> (Le and Mikolov, 2014</cite>; Hill et al., 2016; Kiros et al., 2015) where they study the interrelationship of words in the text snippet. ----------------------------------",
  "y": "background"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_1",
  "x": "In this paper we propose to use the covariance matrix of the word vectors in some document to define a novel descriptor for a document. We call our representation DoCoV descriptor. Our descriptor obtains a fixed-length representation of the paragraph which captures the interrelationship between the dimensions of the word embedding via the covariance matrix elements. This makes our work distinguished from to the work of<cite> (Le and Mikolov, 2014</cite>; Hill et al., 2016; Kiros et al., 2015) where they study the interrelationship of words in the text snippet. ---------------------------------- **TOY EXAMPLE** We show a toy example to highlight the differences between DoCoV vector, the Mean vector and paragraph vector<cite> (Le and Mikolov, 2014)</cite> .",
  "y": "differences"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_2",
  "x": "---------------------------------- **TOY EXAMPLE** We show a toy example to highlight the differences between DoCoV vector, the Mean vector and paragraph vector<cite> (Le and Mikolov, 2014)</cite> . First, we used Gensim library 1 to generate word vectors and paragraph vectors using a dummy training corpus. Next, we formed two hypothetical documents; first document contains words about \"pets\" and second document contains words about \"travel\". In figure 1 we show on the top part the first two dimensions of a word embedding for each document separately. On the bottom Left, we show embedding of the two documents' words in the same space.",
  "y": "differences"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_3",
  "x": "In the word embedding space the covariance matrices are represented via the confidence ellipses. On the bottom right we show the corresponding covariance matrices as points in a new space after vectorization step. ---------------------------------- **MOTIVATION AND CONTRIBUTIONS** Below we describe our motivation towards the proposal of our novel representation: (1) Some neural-based paragraph representations such as paragraph vectors<cite> (Le and Mikolov, 2014)</cite> , FastSent (Hill et al., 2016) use a shared space between the words and paragraphs. This is counter intuitive, as the paragraph is a different entity other than the words.",
  "y": "background"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_4",
  "x": "---------------------------------- **MOTIVATION AND CONTRIBUTIONS** Below we describe our motivation towards the proposal of our novel representation: (1) Some neural-based paragraph representations such as paragraph vectors<cite> (Le and Mikolov, 2014)</cite> , FastSent (Hill et al., 2016) use a shared space between the words and paragraphs. This is counter intuitive, as the paragraph is a different entity other than the words. Figure 1 illustrates that point, we do not see a clear interpretation of why the paragraph vectors<cite> (Le and Mikolov, 2014)</cite> are positioned in the space as in figure 1 . (2) The covariance matrix represents the second order summary statistic of multivariate data.",
  "y": "similarities"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_5",
  "x": "This is an advantage compared to existing methods for generating paragraph vectors, such as<cite> (Le and Mikolov, 2014</cite>; Hill et al., 2016) . Our contribution in this work is two-fold: (1) We propose the Document-Covariance descriptor (DoCoV) to represent every document as the covariance of the word embedding of its words. To the best of our knowledge, we are the first to explicitly compute covariance descriptors on word embedding such as word2vec (Mikolov et al., 2013b) or similar word vectors. (2) We empirically show the effectiveness of our novel descriptor in comparison to the state-of-theart methods in various unsupervised and supervised classification tasks. Our results show that our descriptor can attain comparable accuracy to state-ofthe-art methods in a diverse set of tasks. ---------------------------------- **RELATED WORK**",
  "y": "differences"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_6",
  "x": "Among the approaches of finding word embedding are (Pennington et al., 2014; Levy and Goldberg, 2014; Mikolov et al., 2013b) . These alternatives share the same objective of finding a fixed-length vectorized representation for words to capture the semantic and syntactic regularities between words. These efforts paved the way for many researchers to judge document similarity based on word embedding. Some efforts aimed at finding a global representation of a text snippet using a paragraph-level representation such as paragraph vectors<cite> (Le and Mikolov, 2014)</cite> . Recently other neural-based sentence and paragraph level representations appeared to provide a fixed length representation like Skip-Thought Vectors (Kiros et al., 2015) and FastSent (Hill et al., 2016) . Some efforts focused on defining a Word Mover Distance(WMD) based on word level representation (Kusner et al., 2015) . Prior to this work, we proposed earlier trials for using covariance features in community question answering (Malhas et al., 2016b,a; Torki et al., 2017) .",
  "y": "similarities"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_7",
  "x": "**UNSUPERVISED SEMANTIC TEXTUAL SIMILARITY** We conduct a comparative evaluation against the state-of-the-art approaches in unsupervised paragraph representation. We follow the setup used in (Hill et al., 2016) . ---------------------------------- **DATASETS AND BASELINES** We contrast our results against the methods reported in (Hill et al., 2016) . The competing methods are the paragraph vectors<cite> (Le and Mikolov, 2014)</cite> , skip-thought vectors (Kiros et al., 2015) , Fastsent (Hill et al., 2016) , Sequential (Denoising) Autoencoders (SDAE) (Hill et al., 2016) .",
  "y": "similarities uses"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_8",
  "x": "**RESULTS AND DISCUSSION** We show the correlation values between the similarities computed via DoCoV and the human judgements. We contrast the performance of other representations in table 2. We observe that DoCoV representation outperforms other representations in this task. Other models such as skipthought vectors (Kiros et al., 2015) and SDAE (Hill et al., 2016) requires building an encoder-decoder model which takes time 3 to learn. For other models like paragraph vectors<cite> (Le and Mikolov, 2014)</cite> and Fastsent vectors (Hill et al., 2016) , they require a gradient descent inference step to compute the paragraph/sentence vectors. Using the DoCoV, we just require a pre-trained word embedding model and we do not need any additional training like encoder-decoder models or inference steps via gradient descent.",
  "y": "background"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_9",
  "x": "The subsets used in comparative benchmark evaluation are: Movie Reviews MR (Pang and Lee, 2005) , Subjectivity Subj (Pang and Lee, 2004) ,Customer Reviews CR (Hu and Liu, 2004) and TREC Question TREC (Li and Roth, 2002) . Results and Discussion Table 3 shows the results of our variants against state-of-art algorithms that use unsupervised paragraph representation. We observe that DoCoV is consistently better than the Mean vector and BOW with tf-idf weights. Also, DoCoV is improving consistently when concatenated with baselines such as Mean vector and BOW vectors. This means each feature is capturing different discriminating information. This justifies the choice of concatenating DoCoV with other features. We further observe that DoCoV is consistently better than the paragraph vectors<cite> (Le and Mikolov, 2014)</cite> , Fastsent and SDAE (Hill et al., 2016) .",
  "y": "differences"
 },
 {
  "id": "f5ad574acf9ea27c0be3129238fd92_0",
  "x": "Recent work has shown that neural models can be successfully trained on multiple languages simultaneously. We investigate whether such models learn to share and exploit common syntactic knowledge among the languages on which they are trained. This extended abstract presents our preliminary results. ---------------------------------- **INTRODUCTION** Recent work has shown that state-of-the-art neural models of language and translation can be successfully trained on multiple languages simultaneously without changing the model architecture (\u00d6stling and Tiedemann, 2017; <cite>Johnson et al., 2017)</cite> . In some cases this leads to improved performance compared to models only trained on a specific language, suggesting that multilingual models learn to share useful knowledge crosslingually through their learned representations.",
  "y": "background"
 },
 {
  "id": "f5ad574acf9ea27c0be3129238fd92_1",
  "x": "We focus on the case of language models (LM) trained on two languages, one of which (L1) is over-resourced with respect to the other (L2), and investigate whether the syntactic knowledge learned for L1 is transferred to L2. To this end we use the long-distance agreement benchmark recently introduced by Gulordava et al. (2018) . ---------------------------------- **BACKGROUND** The recent advances in neural networks have opened the way to the design of architecturally simple multilingual models for various NLP tasks, such as language modeling or next word prediction (Tsvetkov et al., 2016; \u00d6stling and Tiedemann, 2017; Malaviya et al., 2017; Tiedemann, 2018) , translation (Dong et al., 2015; Zoph et al., 2016; Firat et al., 2016; <cite>Johnson et al., 2017)</cite> , morphological reinflection (Kann et al., 2017) and more (Bjerva, 2017) . A practical benefit of training models multilingually is to transfer knowledge from high-resource languages to lowresource ones and improve task performance in the latter. Here we aim at understanding how linguistic knowledge is transferred among languages, specifically at the syntactic level, which to our knowledge has not been studied so far.",
  "y": "background"
 },
 {
  "id": "f5ad574acf9ea27c0be3129238fd92_2",
  "x": "We consider the scenario where L1 is overresourced compared to L2 and train our bilingual models by joint training on a mixed L1/L2 corpus so that supervision is provided simultaneously in the two languages (\u00d6stling and Tiedemann, 2017; <cite>Johnson et al., 2017)</cite> . We leave the evaluation of pre-training (or transfer learning) methods (Zoph et al., 2016; Nguyen and Chiang, 2017) to future work. The monolingual LM is trained on a small L2 corpus (LM L2 ). The bilingual LM is trained on a shuffled mix of the same small L2 corpus and a large L1 corpus, where L2 is oversampled to approximately match the amount of L1 sentences (LM L1+L2 ). See Table 1 for the actual training sizes. For our preliminary experiments we have chosen French as the helper language (L1) and Italian as the target language (L2). Since French and Italian share many morphosyntactic patterns, accuracy on the Italian agreement tasks is expected to benefit from adding French sentences to the training data if syntactic transfer occurs.",
  "y": "uses"
 },
 {
  "id": "f5bf9a833c3d46b00d70498e4f1c1b_1",
  "x": "These methods make limited use of the social context in which the authors are tweeting -our research question is \"Can we identify the language of a tweet using the social graph of the tweeter?\". Label propagation approaches [8] are powerful techniques for semi-supervised learning where the domain can naturally be described using an undirected graph. Each node contains a probability distribution over labels, which may be empty for unlabelled nodes, and these labels are propagated over the graph in an iterative fashion. Modified Adsorption (mad) [6] , is an extension that allows more control of the random walk through the graph. Applications of lp and mad are varied, including video recommendation [1] and sentiment analysis over Twitter<cite> [5]</cite> . ---------------------------------- **METHOD**",
  "y": "background"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_0",
  "x": "****FRUSTRATINGLY EASY SEMI-SUPERVISED DOMAIN ADAPTATION**** **ABSTRACT** In this work, we propose a semisupervised extension to a well-known supervised domain adaptation approach (EA) <cite>(Daum\u00e9 III, 2007)</cite> . Our proposed approach (EA++) builds on the notion of augmented space (introduced in EA) and harnesses unlabeled data in target domain to ameliorate the transfer of information from source to target. This semisupervised approach to domain adaptation is extremely simple to implement, and can be applied as a pre-processing step to any supervised learner. Experimental results on sequential labeling tasks demonstrate the efficacy of the proposed method. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_1",
  "x": "In this work, we propose a semisupervised extension to a well-known supervised domain adaptation approach (EA) <cite>(Daum\u00e9 III, 2007)</cite> . Our proposed approach (EA++) builds on the notion of augmented space (introduced in EA) and harnesses unlabeled data in target domain to ameliorate the transfer of information from source to target. This semisupervised approach to domain adaptation is extremely simple to implement, and can be applied as a pre-processing step to any supervised learner. Experimental results on sequential labeling tasks demonstrate the efficacy of the proposed method. ---------------------------------- **INTRODUCTION** A domain adaptation approach for sequential labeling tasks in NLP was proposed in <cite>(Daum\u00e9 III, 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_2",
  "x": "We have a set of source labeled examples L s (\u223c D s (x, y)) and a set of target labeled examples We also have target unlabeled data denoted by U t (\u223c D t (x)), where |U t | = u t . Our goal is to learn a hypothesis h : X \u2192 Y having low expected error with respect to the target domain. In this paper, we consider linear hypotheses only. However, the proposed techniques extend to non-linear hypotheses, as mentioned in <cite>(Daum\u00e9 III, 2007)</cite> . Source and target empirical errors for hypothesis h are denoted b\u0177 \u01eb s (h, f s ) and\u01eb t (h, f t ) respectively, where f s and f t are source and target labeling functions. Similarly, the corresponding expected errors are denoted by \u01eb s (h, f s ) and \u01eb t (h, f t ).",
  "y": "extends differences"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_3",
  "x": "However, the proposed techniques extend to non-linear hypotheses, as mentioned in <cite>(Daum\u00e9 III, 2007)</cite> . Source and target empirical errors for hypothesis h are denoted b\u0177 \u01eb s (h, f s ) and\u01eb t (h, f t ) respectively, where f s and f t are source and target labeling functions. Similarly, the corresponding expected errors are denoted by \u01eb s (h, f s ) and \u01eb t (h, f t ). Shorthand notions of\u01eb s ,\u01eb t , \u01eb s and \u01eb t have also been used. ---------------------------------- **EASYADAPT (EA)** In this section, we give a brief overview of EASYADAPT proposed in <cite>(Daum\u00e9 III, 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_4",
  "x": "For k domains, the augmented space blows up to R (k+1)d . The augmented feature maps \u03a6 s , \u03a6 t : X \u2192X for source and target domains are defined as, Source and target domain features are transformed using these feature maps and the augmented feature space so constructed is passed onto the underlying supervised classifier. One of the most appealing properties of EASYADAPT is that it is agnostic of the underlying supervised classifier being used to learn in the augmented space. Almost any standard supervised learning approach for linear classifiers (for e.g., SVMs, perceptrons) can be used to learn a linear hypothesish \u2208 R 3d in the augmented space. As mentioned earlier, this work considers linear hypotheses only and the the proposed techniques can be extended <cite>(Daum\u00e9 III, 2007)</cite> to non-linear hypotheses. Let us denot\u0207 h = h c , h s , h t , where each of h c , h s , h t is of dimension d and represent the common, sourcespecific and target-specific components ofh, respectively. During prediction on target data, the incoming target feature x is transformed to obtain \u03a6 t (x) andh is applied on this transformed feature.",
  "y": "extends differences"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_5",
  "x": "During prediction on target data, the incoming target feature x is transformed to obtain \u03a6 t (x) andh is applied on this transformed feature. This is equivalent to applying (h c + h t ) on x. A good intuitive insight into why this simple algorithm works so well in practice and outperforms most state-of-the-art algorithms is given in <cite>(Daum\u00e9 III, 2007)</cite> . Briefly, it can be thought to be simultaneously training two hypotheses: w s = (h c + h s ) for source domain and w t = (h c + g t ) for target domain. The commonality between the domains is represented by h c whereas the source and target domain specific information is captured by h s and h t , respectively. This technique can be easily extended to a multi-domain scenario by making more copies of the original feature space ((K + 1) copies in case of K domains). A kernelized version of the algorithm has also been presented in <cite>(Daum\u00e9 III, 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_6",
  "x": "Briefly, it can be thought to be simultaneously training two hypotheses: w s = (h c + h s ) for source domain and w t = (h c + g t ) for target domain. The commonality between the domains is represented by h c whereas the source and target domain specific information is captured by h s and h t , respectively. This technique can be easily extended to a multi-domain scenario by making more copies of the original feature space ((K + 1) copies in case of K domains). A kernelized version of the algorithm has also been presented in <cite>(Daum\u00e9 III, 2007)</cite> . ---------------------------------- **USING UNLABELED DATA** As discussed in the previous section, the EASYADAPT algorithm is attractive because it performs very well empirically and can be used in conjunction with any underlying supervised clas-sifier.",
  "y": "background"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_7",
  "x": "We follow the same experimental setup used in <cite>(Daum\u00e9 III, 2007)</cite> and perform two sequence labelling tasks (a) named-entity-recognition (NER), and (b) part-of-speech-tagging (POS )on the following datasets: PubMed-POS: Introduced by (Blitzer et al., 2006) , this dataset consists of two domains. task is to perform part-of-speech tagging on unlabeled PubMed abstracts with a classifier trained on labeled WSJ and PubMed data. Treebank-Brown. Treebank-Chunk data consists of the following domains: the standard WSJ domain (the same data as for CoNLL 2000), the ATIS switchboard domain and the Brown corpus. The Brown corpus consists of data combined from six subdomains. TreebankChunk is a shallow parsing task based on the data from the Penn Treebank.",
  "y": "similarities uses"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_8",
  "x": "Similar trends were observed in other data sets as can be seen in Figure 3(b) . We also note that EA performs poorly for some cases, as was shown <cite>(Daum\u00e9 III, 2007)</cite> earlier. ---------------------------------- **SUMMARY** In this paper, we have proposed a semi-supervised extension to an existing domain adaptation technique (EA). Our approach EA++, leverages the unlabeled data to improve the performance of EA. Empirical results demonstrate improved accuracy for sequential labeling tasks performed on standardized datasets.",
  "y": "similarities"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_9",
  "x": "**FUTURE WORK** In both EA and EA++, we use features from source and target space to construct an augmented feature space. In other words, we are sharing features across source and target labeled data. We term such algorithms as Feature Sharing Algorithms. Feature sharing algorithms are effective for domain adaptation because they are simple, easy to implement as a preprocessing step and outperform many existing state-of-the-art techniques (shown previously for domain adaptation <cite>(Daum\u00e9 III, 2007)</cite> ). However, despite their simplicity and empirical success, it is not theoretically apparent why these algorithms perform so well. Prior work provides some intuitions but is mostly empirical and a formal theoretical analysis to justify FSAs (for domain adaptation) is clearly missing.",
  "y": "background"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_0",
  "x": "A common approach to obtain them is to train the embeddings in both languages independently and then learn a mapping that minimizes the distances between equivalences listed in a bilingual dictionary. The learned transformation can also be applied to words missing in the dictionary, which can be used to induce new translations with a direct application in machine translation (Mikolov et al., 2013b; Zhao et al., 2015) . The first method to learn bilingual word embedding mappings was proposed by Mikolov et al. (2013b) , who learn the linear transformation that minimizes the sum of squared Euclidean distances for the dictionary entries. Subsequent work has proposed alternative optimization objectives to learn better mappings. <cite>Xing et al. (2015)</cite> incorporate length normalization in the training of word embeddings and try to maximize the cosine similarity instead, introducing an orthogonality constraint to preserve the length normalization after the projection. Faruqui and Dyer (2014) use canonical correlation analysis to project the embeddings in both languages to a shared vector space. Beyond linear mappings, Lu et al. (2015) apply deep canonical correlation analysis to learn a nonlinear transformation for each language.",
  "y": "background"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_1",
  "x": "In this paper, we propose a general framework to learn bilingual word embeddings. We start with a basic optimization objective (Mikolov et al., 2013b) and introduce several meaningful and intuitive constraints that are equivalent or closely related to previously proposed methods (Faruqui and Dyer, 2014; <cite>Xing et al., 2015)</cite> . Our framework provides a more general view of bilingual word embedding mappings, showing the underlying connection between the existing methods, revealing some flaws in their theoretical justification and providing an alternative theoretical interpretation for them. Our experiments on an existing English-Italian word translation induction and an English word analogy task give strong empirical evidence in favor of our theoretical reasoning, while showing that one of our models clearly outperforms previous alternatives. ---------------------------------- **LEARNING BILINGUAL MAPPINGS** Let X and Z denote the word embedding matrices in two languages for a given bilingual dictionary so that their ith row X i * and Z i * are the word embeddings of the ith entry in the dictionary.",
  "y": "similarities"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_2",
  "x": "As long as W is orthogonal, this is equivalent to maximizing the sum of cosine similarities for the dictionary entries, which is commonly used for similarity computations: This last optimization objective coincides with <cite>Xing et al. (2015)</cite> , but <cite>their work</cite> was motivated by an hypothetical inconsistency in Mikolov et al. (2013b) , where the optimization objective to learn word embeddings uses dot product, the objective to learn mappings uses Euclidean distance and the similarity computations use cosine. However, the fact is that, as long as W is orthogonal, optimizing the squared Euclidean distance of length-normalized embeddings is equivalent to optimizing the cosine, and therefore, the mapping objective proposed by <cite>Xing et al. (2015)</cite> is equivalent to that used by Mikolov et al. (2013b) with orthogonality constraint and unit vectors. In fact, our experiments show that orthogonality is more relevant than length normalization, in contrast to <cite>Xing et al. (2015)</cite> , <cite>who introduce</cite> orthogonality only to ensure that unit length is preserved after mapping. ---------------------------------- **MEAN CENTERING FOR MAXIMUM COVARIANCE** Dimension-wise mean centering captures the intuition that two randomly taken words would not be expected to be semantically similar, ensuring that the expected product of two random embeddings in any dimension and, consequently, their cosine similarity, is zero.",
  "y": "differences"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_3",
  "x": "The code for Mikolov et al. (2013b) and <cite>Xing et al. (2015)</cite> is not publicly available, so we implemented and tested them as part of the proposed framework, which only differs from the original systems in the optimization method (exact solution instead of gradient descent) and the length normalization approach in the case of <cite>Xing et al. (2015)</cite> (postprocessing instead of constrained training). As for the method by Faruqui and Dyer (2014) , we used their original implementation in Python and MAT-LAB 6 , which we extended to cover cases where the dictionary contains more than one entry for the same word. ---------------------------------- **RESULTS OF OUR FRAMEWORK** The rows in Table 1 show, respectively, the results for the original embeddings, the basic mapping proposed by Mikolov et al. (2013b) (cf. Section 2) and the addition of orthogonality constraint (cf. Section 2.1), with and without length normalization and, incrementally, mean centering. In all the cases, length normalization and mean centering were applied to all embeddings, even if missing from the dictionary. The results show that the orthogonality constraint is key to preserve monolingual performance, and it also improves bilingual performance by enforcing a relevant property (monolingual invariance) that the transformation to learn should intuitively have.",
  "y": "uses"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_4",
  "x": "Table 2 shows the results for our best performing configuration in comparison to previous work. As discussed before, (Mikolov et al., 2013b) and <cite>(Xing et al., 2015)</cite> were implemented as part of our framework, so they correspond to our uncostrained mapping with no preprocessing and orthogonal mapping with length normalization, respectively. ---------------------------------- **COMPARISON TO OTHER WORK** As it can be seen, the method by <cite>Xing et al. (2015)</cite> performs better than that of Mikolov et al. (2013b) in the translation induction task, which is in line with what <cite>they report</cite> in their paper. Moreover, thanks to the orthogonality constraint <cite>their monolingual performance</cite> in the word analogy task does not degrade, whereas the accuracy of Mikolov et al. (2013b) drops by 2.86% in absolute terms with respect to the original embeddings. Since Faruqui and Dyer (2014) Mikolov et al. (2013b) 34.93% 73. 80% <cite>Xing et al. (2015)</cite> 36.87% 76.66% Faruqui and Dyer (2014) CCA to perform dimensionality reduction, we tested several values for it and report the best (180 dimensions).",
  "y": "similarities uses"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_5",
  "x": "**COMPARISON TO OTHER WORK** As it can be seen, the method by <cite>Xing et al. (2015)</cite> performs better than that of Mikolov et al. (2013b) in the translation induction task, which is in line with what <cite>they report</cite> in their paper. Moreover, thanks to the orthogonality constraint <cite>their monolingual performance</cite> in the word analogy task does not degrade, whereas the accuracy of Mikolov et al. (2013b) drops by 2.86% in absolute terms with respect to the original embeddings. Since Faruqui and Dyer (2014) Mikolov et al. (2013b) 34.93% 73. 80% <cite>Xing et al. (2015)</cite> 36.87% 76.66% Faruqui and Dyer (2014) CCA to perform dimensionality reduction, we tested several values for it and report the best (180 dimensions). This beats the method by <cite>Xing et al. (2015)</cite> in the bilingual task, although it comes at the price of a considerable degradation in monolingual quality. In any case, it is our proposed method with the orthogonality constraint and a global preprocessing with length normalization followed by dimensionwise mean centering that achieves the best accuracy in the word translation induction task. Moreover, it does not suffer from any considerable degradation in monolingual quality, with an anecdotal drop of only 0.07% in contrast with 2.86% for Mikolov et al. (2013b) and 7.02% for Faruqui and Dyer (2014) .",
  "y": "background"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_6",
  "x": "As it can be seen, the method by <cite>Xing et al. (2015)</cite> performs better than that of Mikolov et al. (2013b) in the translation induction task, which is in line with what <cite>they report</cite> in their paper. Moreover, thanks to the orthogonality constraint <cite>their monolingual performance</cite> in the word analogy task does not degrade, whereas the accuracy of Mikolov et al. (2013b) drops by 2.86% in absolute terms with respect to the original embeddings. Since Faruqui and Dyer (2014) Mikolov et al. (2013b) 34.93% 73. 80% <cite>Xing et al. (2015)</cite> 36.87% 76.66% Faruqui and Dyer (2014) CCA to perform dimensionality reduction, we tested several values for it and report the best (180 dimensions). This beats the method by <cite>Xing et al. (2015)</cite> in the bilingual task, although it comes at the price of a considerable degradation in monolingual quality. In any case, it is our proposed method with the orthogonality constraint and a global preprocessing with length normalization followed by dimensionwise mean centering that achieves the best accuracy in the word translation induction task. Moreover, it does not suffer from any considerable degradation in monolingual quality, with an anecdotal drop of only 0.07% in contrast with 2.86% for Mikolov et al. (2013b) and 7.02% for Faruqui and Dyer (2014) . When compared to <cite>Xing et al. (2015)</cite> , our results in Table 1 reinforce our theoretical interpretation for their method (cf. Section 2.2), as it empirically shows that its improvement with respect to Mikolov et al. (2013b) comes solely from the orthogonality constraint, and not from solving any inconsistency.",
  "y": "differences"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_7",
  "x": "However, our model performs considerably better than any configuration from Faruqui and Dyer (2014) in both the monolingual and the bilingual task, supporting our hypothesis that these two constraints that are implicit in their method are not only conceptually confusing, 2292 but also have a negative impact. ---------------------------------- **CONCLUSIONS** This paper develops a new framework to learn bilingual word embedding mappings, generalizing previous work and providing an efficient exact method to learn the optimal transformation. Our experiments show the effectiveness of the proposed model and give strong empirical evidence in favor of our reinterpretation of <cite>Xing et al. (2015)</cite> and Faruqui and Dyer (2014) . It is the proposed method with the orthogonality constraint and a global preprocessing with length normalization and dimension-wise mean centering that achieves the best overall results both in monolingual and bilingual terms, surpassing those previous methods. In the future, we would like to study non-linear mappings (Lu et al., 2015) and the additional techniques in .",
  "y": "extends differences"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_0",
  "x": "**ABSTRACT** Word embeddings have been shown to be useful across state-of-the-art systems in many natural language processing tasks, ranging from question answering systems to dependency parsing. <cite>(Herbelot and Vecchi, 2015)</cite> explored word embeddings and their utility for modeling language semantics. In particular, they presented an approach to automatically map a standard distributional semantic space onto a set-theoretic model using partial least squares regression. We show in this paper that a simple baseline achieves a +51% relative improvement compared to their model on one of the two datasets they used, and yields competitive results on the second dataset. ---------------------------------- **INTRODUCTION**",
  "y": "background"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_1",
  "x": "<cite>(Herbelot and Vecchi, 2015)</cite> investigated a method to map word embeddings to formal semantics, which is the center of interest of this paper. Specifically, given a feature and a word vector of a concept, they tried to automatically find how often the given concept has the given feature. For example, the concept yam is always a vegetable, the concept cat has a coat most of the time, the concept plug has sometimes 3 prongs, and the concept dog never has wings. The method they used was based on partial least squares regression (PLSR). We propose a simple baseline that outperforms their model. ---------------------------------- **TASK**",
  "y": "uses background"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_2",
  "x": "For example, the concept yam is always a vegetable, the concept cat has a coat most of the time, the concept plug has sometimes 3 prongs, and the concept dog never has wings. The method they used was based on partial least squares regression (PLSR). We propose a simple baseline that outperforms their model. ---------------------------------- **TASK** In this section, we summarize the task presented in <cite>(Herbelot and Vecchi, 2015)</cite> . The following is an example of a concept along with some of its features, as formatted in one of the two datasets used to evaluate the model:   yam  a vegetable  all  all  all  yam  eaten by cooking  all  most most  yam grows in the ground  all  all  all  yam  is edible  all  most  all  yam  is orange  some most most  yam  like a potato  all  all  all The concept yam has six features (a vegetable, eaten by cooking, grows in the ground, is edible, is orange, and like a potato).",
  "y": "uses"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_3",
  "x": "The dimension of the model-theoretic vectors will therefore be 2201, and each model-theoretic vector will have on average 2201 \u2212 11.4 = 2189.6 elements set to 0 due to unannotated features. ---------------------------------- **MODEL** In the previous section, we have seen how to convert a concept into a model-theoretic vector based on human annotations. The goal of <cite>(Herbelot and Vecchi, 2015)</cite> is to analyze whether there exists a transformation from the word embedding of a concept to its model-theoretic vector, the gold standard being the human annotations. The word embeddings are taken from the word embeddings pre-trained with word2vec GoogleNews-vectors-negative300 1 (300 dimensions), which were trained on part of the Google News dataset, consisting of approximately 100 billion words. The transformation used in <cite>(Herbelot and Vecchi, 2015)</cite> is based on Partial Least Squares Regression (PLSR).",
  "y": "background"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_4",
  "x": "The transformation used in <cite>(Herbelot and Vecchi, 2015)</cite> is based on Partial Least Squares Regression (PLSR). The PLSR is fitted on the training set: the inputs are the word embeddings for each concept, and the outputs are the model-theoretic vectors for each concept. To assess the quality of the predictions, the Spearman rank-order correlation coefficient is computed between the predictions and the gold modeltheoretic vectors, ignoring all features for which a concept has not been annotated. The idea is that some of the features might be present but not given as options during annotation. The method should therefore not be penalized for not suggesting them. Figure 1 illustrates the model. <cite>(Herbelot and Vecchi, 2015)</cite> 's system.",
  "y": "background"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_5",
  "x": "The transformation used in <cite>(Herbelot and Vecchi, 2015)</cite> is based on Partial Least Squares Regression (PLSR). The PLSR is fitted on the training set: the inputs are the word embeddings for each concept, and the outputs are the model-theoretic vectors for each concept. To assess the quality of the predictions, the Spearman rank-order correlation coefficient is computed between the predictions and the gold modeltheoretic vectors, ignoring all features for which a concept has not been annotated. The idea is that some of the features might be present but not given as options during annotation. The method should therefore not be penalized for not suggesting them. Figure 1 illustrates the model. <cite>(Herbelot and Vecchi, 2015)</cite> 's system.",
  "y": "uses"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_6",
  "x": "Such features are omitted when evaluating the Spearman rank-order correlation coefficient. Also, the dimension of the model-theoretic vectors could be larger or smaller than the dimension of the word embedding. Since the word embeddings we use have 300 dimensions, the model-theoretic vectors will be smaller than the word embeddings in the AD dataset, and larger in the QMR dataset. ---------------------------------- **EXPERIMENTS** We compare <cite>(Herbelot and Vecchi, 2015)</cite> 's model (<cite>PLSR + word2vec</cite>) against three baselines: random vectors, mode, and nearest neighbor. \u2022 Mode: A predictor that outputs, for each feature, the most common feature value (i.e., the mode) in the training set.",
  "y": "uses"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_7",
  "x": "Indeed, the predicted values are always the same, regardless of the concept. If a feature has the same value for most concepts, the predictor may perform reasonably well. \u2022 Nearest neighbor (NN): A predictor that outputs for any concept the model-theoretic vector from the training set corresponding to the most similar concept in the training set. Similarity is based on the cosine similarity of the word vectors. This is a simple nearest neighbor predictor. \u2022 Random vectors: <cite>(Herbelot and Vecchi, 2015)</cite> used pre-trained word embeddings as input to the PLSR, we instead simply use random vectors of same dimension (300, continuous uniform distribution between 0 and 1). We also apply retrofitting (Faruqui et al., 2014) on the word embeddings in order to leverage relational information from semantic lexicons by encouraging linked words to have similar vector representations.",
  "y": "differences"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_8",
  "x": "\u2022 Nearest neighbor (NN): A predictor that outputs for any concept the model-theoretic vector from the training set corresponding to the most similar concept in the training set. Similarity is based on the cosine similarity of the word vectors. This is a simple nearest neighbor predictor. \u2022 Random vectors: <cite>(Herbelot and Vecchi, 2015)</cite> used pre-trained word embeddings as input to the PLSR, we instead simply use random vectors of same dimension (300, continuous uniform distribution between 0 and 1). We also apply retrofitting (Faruqui et al., 2014) on the word embeddings in order to leverage relational information from semantic lexicons by encouraging linked words to have similar vector representations. Using (Faruqui et al., 2014 )'s retrofitting tool 2 , we retrofit the word embeddings (GoogleNews-vectorsnegative300) on each of the 4 datasets present in the retrofitting tool (framenet, ppdb-xl, wordnetsynonyms+, and wordnet-synonyms. <cite>(Herbelot and Vecchi, 2015)</cite> ) in the last row.",
  "y": "uses"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_9",
  "x": "We could reproduce the results for the QMR dataset using PLSR and word2vec embeddings (0.346 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.332 in our experiments, but we could not ex-3 https://github.com/Franck-Dernoncourt/ model-theoretic actly reproduce the results for the AD dataset (0.634 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.572 in our experiments): this discrepancy most likely results from the choice of the training set. Our experiments' results are averaged over 1000 runs, and for each run the training/test split is randomly chosen, the only constraint being having the same number of training samples as in <cite>(Herbelot and Vecchi, 2015)</cite> . For the AD dataset, our worst run achieved 0.435, and our best run achieved 0.713, which emphasizes the lack of robustness of the results with respect to the train/test split. QMR dataset (min: 0.244; max: 0.407), which is expected since QMR is significantly larger than AD. Furthermore, the mode baseline yields results that are good on the AD dataset (0.554, vs. 0.634 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.572 in our PLSR + word2vec implementation), and significantly better than all other models on the QMR dataset (0.522, vs. 0.346 in <cite>(Herbelot and Vecchi, 2015)</cite> , i.e. +51% improvement). To get an intuition of why the mode baseline works well, Figures 2 and 3 show that most features tend to have one clearly dominant quantifier in the AD dataset. A similar trend can be found in the QMR dataset.",
  "y": "uses"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_10",
  "x": "Using (Faruqui et al., 2014 )'s retrofitting tool 2 , we retrofit the word embeddings (GoogleNews-vectorsnegative300) on each of the 4 datasets present in the retrofitting tool (framenet, ppdb-xl, wordnetsynonyms+, and wordnet-synonyms. <cite>(Herbelot and Vecchi, 2015)</cite> ) in the last row. PLSR stands for partial least squares regression, NN for nearest neighbor, ppdb for the Paraphrase Database (Ganitkevitch et al., 2013) . There are two ways to compute the mode: either taking the mode of the means of the 3 annotations (mode), or the mode for all annotations (true-mode). QMR has 3 potentially different annotations for each concept-feature pair, while AD has 3 only one annotation for each concept-feature pair: as a result, mode and true-mode have similar results for AD, but potentially different results for QMR. For each run, a train/test split was randomly chosen (60 training samples for AD, 400 for QMR, in order to have the same number of training samples as in <cite>(Herbelot and Vecchi, 2015)</cite> 's Table 2 ). ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_11",
  "x": "For each run, a train/test split was randomly chosen (60 training samples for AD, 400 for QMR, in order to have the same number of training samples as in <cite>(Herbelot and Vecchi, 2015)</cite> 's Table 2 ). ---------------------------------- **AD** 6 Results and discussion Table 1 presents the results, using the Spearman correlation as the performance metric. The experiment was coded in Python using scikit-learn (Pedregosa et al., 2011) and the source as well as the complete result log and the two datasets are available online 3 . We could reproduce the results for the QMR dataset using PLSR and word2vec embeddings (0.346 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.332 in our experiments, but we could not ex-3 https://github.com/Franck-Dernoncourt/ model-theoretic actly reproduce the results for the AD dataset (0.634 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.572 in our experiments): this discrepancy most likely results from the choice of the training set. Our experiments' results are averaged over 1000 runs, and for each run the training/test split is randomly chosen, the only constraint being having the same number of training samples as in <cite>(Herbelot and Vecchi, 2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_12",
  "x": "The experiment was coded in Python using scikit-learn (Pedregosa et al., 2011) and the source as well as the complete result log and the two datasets are available online 3 . We could reproduce the results for the QMR dataset using PLSR and word2vec embeddings (0.346 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.332 in our experiments, but we could not ex-3 https://github.com/Franck-Dernoncourt/ model-theoretic actly reproduce the results for the AD dataset (0.634 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.572 in our experiments): this discrepancy most likely results from the choice of the training set. Our experiments' results are averaged over 1000 runs, and for each run the training/test split is randomly chosen, the only constraint being having the same number of training samples as in <cite>(Herbelot and Vecchi, 2015)</cite> . For the AD dataset, our worst run achieved 0.435, and our best run achieved 0.713, which emphasizes the lack of robustness of the results with respect to the train/test split. QMR dataset (min: 0.244; max: 0.407), which is expected since QMR is significantly larger than AD. Furthermore, the mode baseline yields results that are good on the AD dataset (0.554, vs. 0.634 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.572 in our PLSR + word2vec implementation), and significantly better than all other models on the QMR dataset (0.522, vs. 0.346 in <cite>(Herbelot and Vecchi, 2015)</cite> , i.e. +51% improvement). To get an intuition of why the mode baseline works well, Figures 2 and 3 show that most features tend to have one clearly dominant quantifier in the AD dataset.",
  "y": "differences"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_13",
  "x": "The nearest neighbor baseline yields some competitive results on the AD dataset, but lower results on the QMR dataset. Lastly, using retrofitting increases the performances on both AD and QMR datasets. This is expected as applying retrofitting to word embeddings leverages relational information from semantic lexicons by encouraging linked words to have similar vector representations. ---------------------------------- **CONCLUSION** In this paper we have presented several baselines for mapping distributional to model-theoretic semantic spaces. The mode baseline significantly outperforms <cite>(Herbelot and Vecchi, 2015)</cite> 's model on the QMR dataset, and yields competitive results on the AD dataset.",
  "y": "differences"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_0",
  "x": "Previous work has mostly treated AA as a supervised text classification or regression task. A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006) , often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al., 2003) , and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones <cite>(Yannakoudakis et al., 2011)</cite> . As multiple factors influence the linguistic quality of texts, such systems exploit features that correspond to different properties of texts, such as grammar, style, vocabulary usage, topic similarity, and discourse coherence and cohesion. Cohesion refers to the use of explicit linguistic cohesive devices (e.g., anaphora, lexical semantic relatedness, discourse markers, etc.) within a text that can signal primarily suprasentential discourse relations between textual units (Halliday and Hasan, 1976) . Cohesion is not the only mechanism of discourse coherence, which may also be inferred from meaning without presence of explicit linguistic cues. Coherence can be assessed locally in terms of transitions between adjacent clauses, parentheticals, and other textual units capable of standing in discourse relations, or more globally in terms of the overall topical coherence of text passages. There is a large body of work that has investigated a number of different coherence models on news texts (e.g., Lin et al. (2011) , Elsner and Charniak (2008) , and Soricut and Marcu (2006) ).",
  "y": "background"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_2",
  "x": "The goal of the experiments presented in this paper is to measure the effect a number of (previously-developed and new) coherence models have on performance when combined with an AA system that achieves competitive results, but does not use discourse coherence features. Our contribution is threefold: 1) we present the first systematic analysis of several methods for assessing discourse coherence in the framework of AA of learner free-text responses, 2) we identify new discourse features that serve as proxies for the level of (in)coherence in texts and outperform previously developed techniques, and 3) we improve the best results reported by<cite> Yannakoudakis et al. (2011)</cite> on the publically available 'English as a Second or Other Language' (ESOL) corpus of learner texts (to date, this is the only public-domain corpus that contains grades). Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in<cite> Yannakoudakis et al. (2011)</cite> . ---------------------------------- **EXPERIMENTAL DESIGN & BACKGROUND** We examine the predictive power of a number of different coherence models by measuring the effect on performance when combined with an AA system that achieves state-of-the-art results, but does not use discourse coherence features. Specifically, we describe a number of different experiments improving on the AA system presented in<cite> Yannakoudakis et al. (2011)</cite> ; AA is treated as a rank preference supervised learning problem and ranking Support Vector Machines (SVMs) (Joachims, 2002) are used to explicitly model the grade relationships between scripts.",
  "y": "extends"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_4",
  "x": "For example, an examinee might learn by rote a set of well-formed sentences and reproduce these in an exam in the knowledge that an AA system is not checking for prompt relevance or coherence 1 . ---------------------------------- **DATASET & EXPERIMENTAL SETUP** We use the First Certificate in English (FCE) ESOL examination scripts 2 (upper-intermediate level assessment) described in detail in<cite> Yannakoudakis et al. (2011)</cite> , extracted from the Cambridge Learner Corpus 3 (CLC). The dataset consists of 1,238 texts between 200 and 400 words produced by 1,238 distinct learners in response to two different prompts. An overall mark has been assigned in the range 1-40. For all experiments, we use a series of 5-fold cross-validation runs on 1,141 texts from the examination year 2000 to evaluate performance as well as generalization of numerous models.",
  "y": "uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_5",
  "x": "The dataset consists of 1,238 texts between 200 and 400 words produced by 1,238 distinct learners in response to two different prompts. An overall mark has been assigned in the range 1-40. For all experiments, we use a series of 5-fold cross-validation runs on 1,141 texts from the examination year 2000 to evaluate performance as well as generalization of numerous models. Moreover, we identify the best model on year 2000 and we also test it on 97 texts from the examination year 2001, previously used in<cite> Yannakoudakis et al. (2011)</cite> to report the best published results. Validating the results on a different examination year tests generalization to some prompts not used in 2000, and also allows us to test correlation between examiners and the AA system. Again, we treat AA as a rank preference learning problem and use SVMs, utilizing the SVM light package (Joachims, 2002) , to facilitate comparison with<cite> Yannakoudakis et al. (2011)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_7",
  "x": "As in<cite> Yannakoudakis et al. (2011)</cite> , we analyze all texts using the RASP toolkit (Briscoe et al., 2006) 4 . ---------------------------------- **'SUPERFICIAL' PROXIES** In this section we introduce diverse classes of 'superficial' cohesive features that serve as proxies for coherence. Surface text properties have been assessed in the framework of automatic summary evaluation (Pitler et al., 2010) , and have been shown to significantly correlate with the fluency of machinetranslated sentences (Chae and Nenkova, 2009 ). ---------------------------------- **PART-OF-SPEECH (POS) DISTRIBUTION**",
  "y": "similarities uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_8",
  "x": "Surface text properties have been assessed in the framework of automatic summary evaluation (Pitler et al., 2010) , and have been shown to significantly correlate with the fluency of machinetranslated sentences (Chae and Nenkova, 2009 ). ---------------------------------- **PART-OF-SPEECH (POS) DISTRIBUTION** The AA system described in<cite> Yannakoudakis et al. (2011)</cite> exploited features based on POS tag sequences, but did not consider the distribution of POS types across grades. In coherent texts, constituent clauses and sentences are related and depend on each other for their interpretation. Anaphors such as pronouns link the current sentence to those where the entities were previously mentioned. Pronouns can be directly related to (lack of) coherence and make intuitive sense as cohesive devices.",
  "y": "background"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_9",
  "x": "---------------------------------- **SEMANTIC SIMILARITY** We explore the utility of inter-sentential feature types for assessing discourse coherence. Among the features used in<cite> Yannakoudakis et al. (2011)</cite> , none explicitly captures coherence and none models intersentential relationships. Incremental Semantic analysis (ISA) (Baroni et al., 2007) is a word-level distributional model that induces a semantic space from input texts. ISA is a fully-incremental variation of Random Indexing (RI) (Sahlgren, 2005) , which can efficiently capture second-order effects in common with other dimensionality-reduction methods based on singular value decomposition, but does not rely on stoplists or global statistics for weighting purposes. Utilizing the S-Space package (Jurgens and Stevens, 2010), we trained an ISA model 5 using a subset of ukWaC (Ferraresi et al., 2008) , a large corpus of English containing more than 2 billion tokens.",
  "y": "differences"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_10",
  "x": "**EVALUATION** We examine the predictive power of each of the coherence models/features described in Section 4 by measuring the effect on performance when combined with an AA system that achieves state-of-theart results on the FCE dataset, but does not use discourse coherence features. In particular, we use the system described in<cite> Yannakoudakis et al. (2011)</cite> as our baseline AA system. Discourse coherence is a strong indicator of thorough knowledge of a second language and thus we expect coherence features to further improve performance of AA systems. We evaluate the grade predictions of our models against the gold standard grades in the dataset using Pearson's product-moment correlation coeffi-16 http://goo.gl/yQ0Q0 cient (r) and Spearman's rank correlation coefficient (\u03c1) as is standard in AA research (Briscoe et al., 2010) . Table 1 gives results obtained by augmenting the baseline model with each of the coherence features described above. In each of these experiments, we perform 5-fold cross-validation 17 using all 1,141 texts from the exam year 2000 (see Section 3).",
  "y": "uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_11",
  "x": "In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in<cite> Yannakoudakis et al. (2011)</cite> to report results of the final best system. Validating the model on a different exam year also shows us the extent to which it generalizes between years. Table 2 published results on the 2001 texts, getting closer to the upper-bound. The upper-bound on this dataset 20 is 0.796 and 0.792 r and \u03c1 respectively, calculated by taking the average correlation between the FCE grades and the ones provided by 4 senior ESOL examiners 21 . Table 3 also presents the average correlation between our extended AA system's predicted grades and the 4 examiners' grades, in addition to the original FCE grades from the dataset. Again, our extended model improves over the baseline. Finally, we explore the utility of our best model for assessing the publically available 'outlier' texts used in<cite> Yannakoudakis et al. (2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_12",
  "x": "In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in<cite> Yannakoudakis et al. (2011)</cite> to report results of the final best system. Validating the model on a different exam year also shows us the extent to which it generalizes between years. Table 2 published results on the 2001 texts, getting closer to the upper-bound. The upper-bound on this dataset 20 is 0.796 and 0.792 r and \u03c1 respectively, calculated by taking the average correlation between the FCE grades and the ones provided by 4 senior ESOL examiners 21 . Table 3 also presents the average correlation between our extended AA system's predicted grades and the 4 examiners' grades, in addition to the original FCE grades from the dataset. Again, our extended model improves over the baseline. Finally, we explore the utility of our best model for assessing the publically available 'outlier' texts used in<cite> Yannakoudakis et al. (2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_14",
  "x": "---------------------------------- **CONCLUSION** We presented the first systematic analysis of a wide variety of models for assessing discourse coherence on learner data, and evaluated their individual performance as well as their combinations for the AA grading task. We adapted the LOWBOW model for assessing sequential content in texts, and showed evidence supporting our hypothesis that local histograms are useful. We also successfully adapted ISA, an efficient and incremental variant distributional semantic model, to this task. ISA, LOWBOW, the POS IBM model and word length are the best individual features for assessing coherence. A significant improvement over the AA system presented in<cite> Yannakoudakis et al. (2011)</cite> and the best published result on the FCE dataset was obtained by augmenting the system with an ISA-based local coherence feature.",
  "y": "differences"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_15",
  "x": "Our contribution is threefold: 1) we present the first systematic analysis of several methods for assessing discourse coherence in the framework of AA of learner free-text responses, 2) we identify new discourse features that serve as proxies for the level of (in)coherence in texts and outperform previously developed techniques, and 3) we improve the best results reported by<cite> Yannakoudakis et al. (2011)</cite> on the publically available 'English as a Second or Other Language' (ESOL) corpus of learner texts (to date, this is the only public-domain corpus that contains grades). Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in<cite> Yannakoudakis et al. (2011)</cite> . ---------------------------------- **EXPERIMENTAL DESIGN & BACKGROUND** We examine the predictive power of a number of different coherence models by measuring the effect on performance when combined with an AA system that achieves state-of-the-art results, but does not use discourse coherence features. Specifically, we describe a number of different experiments improving on the AA system presented in<cite> Yannakoudakis et al. (2011)</cite> ; AA is treated as a rank preference supervised learning problem and ranking Support Vector Machines (SVMs) (Joachims, 2002) are used to explicitly model the grade relationships between scripts. This system uses a number of different linguistic features that achieve good performance on the AA task.",
  "y": "uses"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_0",
  "x": "Taking pre-trained CMLM models as the encoder and decoder, we significantly improve the performance of unsupervised machine translation. Our code is available at https://github.com/Imagist-Shuo/CMLM. ---------------------------------- **INTRODUCTION** Unsupervised machine translation has become an emerging research interest in recent years (Artetxe et al., 2017; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019;<cite> Lample and Conneau, 2019)</cite> . The common framework of unsupervised machine translation builds two initial translation models at first (i.e., source to target and target to source), and then does iterative back-translation (Sennrich et al., 2016a; Zhang et al., 2018) with the two models using pseudo data generated by each other. The initialization process is crucial to the final translation * Contribution during internship at MSRA.",
  "y": "background"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_1",
  "x": "**INTRODUCTION** Unsupervised machine translation has become an emerging research interest in recent years (Artetxe et al., 2017; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019;<cite> Lample and Conneau, 2019)</cite> . The common framework of unsupervised machine translation builds two initial translation models at first (i.e., source to target and target to source), and then does iterative back-translation (Sennrich et al., 2016a; Zhang et al., 2018) with the two models using pseudo data generated by each other. The initialization process is crucial to the final translation * Contribution during internship at MSRA. performance as pointed in , Artetxe et al. (2018b) and Ren et al. (2019) . Previous approaches benefit mostly from crosslingual n-gram embeddings, but recent work proves that cross-lingual language model pretraining could be a more effective way to build initial unsupervised machine translation models <cite>(Lample and Conneau, 2019)</cite> . However, in their method, the cross-lingual information is mostly obtained from shared Byte Piece Encoding (BPE) (Sennrich et al., 2016b) spaces during pre-training, which is inexplicit and limited.",
  "y": "motivation background"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_2",
  "x": "The core idea of BERT is pre-training a deep bidirectional Transfomer encoder (Vaswani et al., 2017) with two training tasks. The first one is Masked Language Model (MLM) referring to the Cloze task (Taylor, 1953) , which takes a straightforward approach of masking some percentage of the input tokens at random, and then predicting them with the corresponding Transformer hidden states, as shown in Figure 1 . The second one is Next Sentence Prediction, which means to predict whether two sentences are adjacent or not. This task is designed for some tasks that need modeling the relationship between two sentences such as Question Answering (QA) and Natural Language Inference (NLI). ---------------------------------- **XLM** Based on BERT,<cite> Lample and Conneau (2019)</cite> propose a cross-lingual version called XLM and reach the state-of-the-art performance on some crosslingual NLP tasks including cross-lingual classification , machine translation, and unsupervised cross-lingual word embedding.",
  "y": "background"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_3",
  "x": "Therefore, cross-lingual BPE embeddings are leveraged to calculate the normalized sim(x i , y j ) to approximate a(i|j, m, l). p(y j |x i ) is the model prediction in Softmax outputs. For each source ngram, all of the retrieved k translation candidates are used to calculate the cross entropy loss, which are weighted with their translation probabilities in the n-gram table. Given a language pair X \u2212 Y , we process both languages with the same shared BPE vocabulary using their monolingual sentences together during pre-training. Following Devlin et al. (2018) ;<cite> Lample and Conneau (2019)</cite> , in our CMLM objective, we randomly sample 15% of the BPE ngrams from the text streams, and replace them by [MASK] tokens 70% of the time. During pretraining, in each iteration, a batch is composed of sentences sampled from the same language, and we alternate between MLM and CMLM objectives. Different from the original MLM in BERT, in the half of the MLM time, we randomly choose some source n-grams in the input text stream, and replace them with their translation candidates to construct code-switching sentences.",
  "y": "extends differences"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_4",
  "x": "Then, we train the model with monolingual data until convergence via denoising auto-encoder and iterative backtranslation, as described in Artetxe et al. (2017) ; . Different from them, we step further and make another iteration with updated n-gram translation tables. Specifically, we translate the monolingual sentences with our latest translation model and run GIZA++ (Och and Ney, 2003) on the generated pseudo parallel data to extract updated n-gram translation pairs, which are used to tune the encoder as Section 3.3, together with the back-translation within a multi-task learning framework. Experimental results show that running another iteration can further improve the translation performance. It is also interesting to explore the usage of pre-trained decoders in the translation model. It seems that pre-training decoders has a smaller effect on the final performance than pre-training encoders <cite>(Lample and Conneau, 2019)</cite> , one reason for which could be that the encoder-to-decoder attention is not pre-trained. Therefore, the parameters of the decoder need to be re-adjusted substantially in the following tuning process for MT task.",
  "y": "differences"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_5",
  "x": "For each language, we use all the available sentences in NewsCrawl till 2018, monolingual datasets from WMT. The NewsCrawl data are used in both pretraining and the following unsupervised NMT iteration process. Our CMLM is optimized based on the pre-trained models released by<cite> Lample and Conneau (2019)</cite> 1 , which are trained with Wikipedia dumps. For fair comparison, we use newstest 2014 as the test set for en-fr, and newstest 2016 for en-de and en-ro. 1 https://github.com/facebookresearch/XLM We use Moses scripts for tokenization, and use fastBPE 2 to split words into subword units with their released BPE codes 1 . The number of shared BPE codes for each language pair is 60,000. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_6",
  "x": "From the table, we find that our proposed method significantly outperforms previous methods on all language pairs by the average BLEU score of 1.7, Method fr2en en2fr de2en en2de ro2en en2ro Baselines (Artetxe et al., 2017) 15.6 15.1 ---- 14.3 15.1 13.3 9.6 -- (Artetxe et al., 2018b) 25 and both the improvements of en2fr and ro2en are over 2 BLEU points. The results indicate that the explicit cross-lingual information incorporated by our proposed CMLM is beneficial to the unsupervised machine translation task. Notice that by doing another iteration (\"Iter 2\") with updated ngram tables as described in Section 3.4, we further improve the performance a bit for most translation directions with the improvements of en2fr and ro2en bigger than 0.5 BLEU point, which confirms the potential that fine-tuned machine translation models contain more beneficial cross-lingual information than the initial n-gram translation tables, which can be used to enhance the pre-trained model iteratively. The improvement made by<cite> Lample and Conneau (2019)</cite> compared with the first five baselines shows that cross-lingual pre-training can be necessary for unsupervised MT. However, the crosslingual information learned with this method during pre-training is mostly from the shared subword space, which is inexplicit and not strong enough. Our proposed method can give the model more explicit and strong cross-lingual training signals so that the pre-trained model contains much beneficial cross-lingual information for unsupervised machine translation. As a result, we can further improve the translation performance significantly, compared with<cite> Lample and Conneau (2019)</cite> (with the significance level of p<0.01).",
  "y": "future_work"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_7",
  "x": "The results indicate that the explicit cross-lingual information incorporated by our proposed CMLM is beneficial to the unsupervised machine translation task. Notice that by doing another iteration (\"Iter 2\") with updated ngram tables as described in Section 3.4, we further improve the performance a bit for most translation directions with the improvements of en2fr and ro2en bigger than 0.5 BLEU point, which confirms the potential that fine-tuned machine translation models contain more beneficial cross-lingual information than the initial n-gram translation tables, which can be used to enhance the pre-trained model iteratively. The improvement made by<cite> Lample and Conneau (2019)</cite> compared with the first five baselines shows that cross-lingual pre-training can be necessary for unsupervised MT. However, the crosslingual information learned with this method during pre-training is mostly from the shared subword space, which is inexplicit and not strong enough. Our proposed method can give the model more explicit and strong cross-lingual training signals so that the pre-trained model contains much beneficial cross-lingual information for unsupervised machine translation. As a result, we can further improve the translation performance significantly, compared with<cite> Lample and Conneau (2019)</cite> (with the significance level of p<0.01). ----------------------------------",
  "y": "future_work"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_8",
  "x": "We compare the context-unaware method (i.e., directly calculating the similarity scores between unsupervised cross-lingual embeddings (Artetxe et al., 2018a ) of source and target words), XLM <cite>(Lample and Conneau, 2019)</cite> and our proposed CMLM pre-training method in the Table 3 . In this experiment, we leave out all the OOV words and those torn apart by the BPE operations. Table 3 : Results of word alignment tasks using different cross-lingual word embeddings. In this table, \"P\" means \"precision\", \"R\" means recall\", \"F\" means \"Fmeasure\" and \"AER\" means the \"alignment error rate\". From this table, we find that, based on BERT, both XLM and our method can model crosslingual context information, indicating that context information can greatly enhance the crosslingual mapping between the source and target words. By leveraging the explicit cross-lingual information in the model training, our CMLM can outperform XLM significantly. This confirms that our CMLM does better to connect the source and target representation space, with which as pretrained models, the performance of unsupervised NMT can be improved.",
  "y": "similarities"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_10",
  "x": "The core idea is to constrain outputs of encoders of two languages into a same latent space with a weight sharing mechanism such as using a shared encoder. Denoising auto-encoder (Vincent et al., 2010) and adversarial training methods are also leveraged. Besides, they apply iterative backtranslation to generated pseudo data for crosslingual training. In addition to NMT methods for unsupervised machine translation, some following work shows that SMT methods and the hybrid of NMT and SMT can be more effective (Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019) . They all build unsupervised PBSMT systems, and all of their models are initialized with language models and phrase tables inferred from cross-lingual word or n-gram embeddings and then use the initial PBSMT models to do iterative back-translation. also build a hybrid system by combining the best pseudo data that SMT models generate into the training of the NMT model while Ren et al. (2019) alternately train SMT and NMT models with the framework of posterior regularization. More recently,<cite> Lample and Conneau (2019)</cite> reach new state-of-the-art performance on unsupervised en-fr and en-de translation tasks.",
  "y": "background"
 },
 {
  "id": "fa641aca676761c79c0469c195f336_0",
  "x": "They observed that English readability formulas did not work well on Bengali texts <cite>[11]</cite> , [21] . This observation is not surprising, because Bengali is very different than English. Bengali is a highly inflected language, follows subject-object-verb ordering in sentences, and has a rich morphology. Further, Bengali shows word compounding and diglossia, i.e. formal and informal language variants (sadhu bhasha and cholit bhasha). All these factors complicate readability scoring in Bengali. Since the concept of readability is highly subjective and reader-dependent, it is necessary to find out how much two native Bengali speakers agree on the readability level of a piece of text. Generalizing from there, we performed an inter-rater agreement study on readability assessment in Bengali.",
  "y": "background"
 },
 {
  "id": "fa641aca676761c79c0469c195f336_1",
  "x": "Sinha et al. classified English readability formulas into three broad categories -traditional methods, cognitively motivated methods, and machine learning methods [21] . Traditional methods assess readability using surface features and shallow linguistic features such as the ones mentioned in the preceding paragraph. Cognitively motivated methods take into account the cohesion and coherence of text, its latent topic structure, Kintsch's propositions, etc [1] , [8] , [13] . Finally, machine learning methods utilize sophisticated structures such as language models [3] , [4] , [18] , query logs [15] , and several other features to predict the readability of open-domain text data. There are very few studies on readability assessment in Bengali texts. We found only three lines of work that specifically looked into Bengali readability [6] , <cite>[11]</cite> , [21] . Das and Roychoudhury worked with a miniature model of two parameters in their pioneering study [6] .",
  "y": "background"
 },
 {
  "id": "fa641aca676761c79c0469c195f336_2",
  "x": "Das and Roychoudhury worked with a miniature model of two parameters in their pioneering study [6] . They found that the two-parameter model was a better predictor of readability than the one-parameter model. Note, however, that Das and Roychoudhury's corpus was small (only seven documents), thereby calling into question the validity of their results. Sinha et al. alleviated these problems by considering six parameters instead of just two [21] . They further showed that English readability indices were inadequate for Bengali, and built their own readability model on 16 texts. Around the same time, Islam et al. independently reached the same conclusion <cite>[11]</cite> . They designed a Bengali readability classifier on lexical and information-theoretic features, resulting in an F-score 50% higher than that from traditional scoring approaches.",
  "y": "background"
 },
 {
  "id": "fa641aca676761c79c0469c195f336_3",
  "x": "We found only three lines of work that specifically looked into Bengali readability [6] , <cite>[11]</cite> , [21] . Around the same time, Islam et al. independently reached the same conclusion <cite>[11]</cite> . While all the above studies are very important and insightful, none of them explicitly performed an inter-rater agreement study. Further, none of these studies made available their readability-annotated gold standard datasets, thereby stymieing further research. We attempt to bridge these gaps in our work.",
  "y": "motivation"
 },
 {
  "id": "fa641aca676761c79c0469c195f336_4",
  "x": "We obtained moderate to fair agreement among seven independent annotators on 30 text passages written by four eminent Bengali authors. As a byproduct of this study, we obtained a gold standard human annotated readability dataset for Bengali. We plan to release this dataset for future research. We are working on readability modeling in Bengali, and this dataset will be very helpful. An important limitation of our study is the small corpus size. We only have 30 annotated passages at our disposal, whereas Islam et al. <cite>[11]</cite> had around 300. But Islam et al.'s dataset is not annotated in as fine-grained a fashion as ours. Note also that our dataset is larger than both Sinha et al.'s 16document dataset [21] , and Das and Roychoudhury's seven document dataset [6] .",
  "y": "differences"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_0",
  "x": "Van Hee et al. (2015b) identifies discriminative remarks (racist, sexist) as a subset of \"insults\", whereas Nobata et al. (2016) classifies similar remarks as \"hate speech\" or \"derogatory language\". Waseem and Hovy (2016) only consider \"hate speech\" without regard to any potential overlap with bullying or otherwise offensive language, while <cite>Davidson et al. (2017)</cite> distinguish hate speech from generally offensive language. Wulczyn et al. (2017) annotates for personal attacks, which likely encompasses identifying cyberbullying, hate speech, and offensive language. The lack of consensus has resulted in contradictory annotation guidelines -some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al. (2016) and <cite>Davidson et al. (2017)</cite> . To help to bring together these literatures and to avoid these contradictions, we propose a typology that synthesizes these different subtasks. We argue that the differences between subtasks within abusive language can be reduced to two primary factors: 1. Is the language directed towards a specific individual or entity or is it directed towards a generalized group?",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_1",
  "x": "Wulczyn et al. (2017) annotates for personal attacks, which likely encompasses identifying cyberbullying, hate speech, and offensive language. The lack of consensus has resulted in contradictory annotation guidelines -some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al. (2016) and <cite>Davidson et al. (2017)</cite> . To help to bring together these literatures and to avoid these contradictions, we propose a typology that synthesizes these different subtasks. We argue that the differences between subtasks within abusive language can be reduced to two primary factors: 1. Is the language directed towards a specific individual or entity or is it directed towards a generalized group? ---------------------------------- **IS THE ABUSIVE CONTENT EXPLICIT OR IMPLICIT?**",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_2",
  "x": "Each of the different subtasks related to abusive language occupies one or more segments of this typology. Our aim is to clarify the similarities and differences between subtasks in abusive language detection to help researchers select appropriate strategies for data annotation and modeling. ---------------------------------- **A TYPOLOGY OF ABUSIVE LANGUAGE** Much of the work on abusive language subtasks can be synthesized in a two-fold typology that conExplicit Implicit Directed \"Go kill yourself\", \"You're a sad little f*ck\" (Van Hee et al., 2015a) , \"@User shut yo beaner ass up sp*c and hop your f*ggot ass back across the border little n*gga\"<cite> (Davidson et al., 2017)</cite> , \"Youre one of the ugliest b*tches Ive ever fucking seen\" (Kontostathis et al., 2013) . \"Hey Brendan, you look gorgeous today. What beauty salon did you visit?\" (Dinakar et al., 2012), \"(((@User) )) and what is your job? Writing cuck articles and slurping Google balls? #Dumbgoogles\" (Hine et al., 2017) , \"you're intelligence is so breathtaking!!!!!!\" (Dinakar et al., 2011) Generalized \"I am surprised they reported on this crap who cares about another dead n*gger?\", \"300 missiles are cool! Love to see um launched into Tel Aviv! Kill all the g*ys there!\" (Nobata et al., 2016) , \"So an 11 year old n*gger girl killed herself over my tweets?\u02c6\u02c6thats another n*gger off the streets!!\" (Kwok and Wang, 2013) .",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_3",
  "x": "Starting with the targets, abuse can either be directed towards a specific individual or entity, or it can be used towards a generalized Other, for example people with a certain ethnicity or sexual orientation. This is an important sociological distinction as the latter references a whole category of people rather than a specific individual, group, or organization (see Brubaker 2004 , Wimmer 2013 ) and, as we discuss below, entails a linguistic distinction that can be productively used by researchers. To better illustrate this, the first row of Table 1 shows examples from the literature of directed abuse, where someone is either mentioned by name, tagged by a username, or referenced by a pronoun. 2 Cyberbullying and trolling are instances of directed abuse, aimed at individuals and online communities respectively. The second row shows cases with abusive expressions towards generalized groups such as racial categories and sexual orientations. Previous work has identified instances of hate speech that are both directed and generalized (Burnap and Williams, 2015; Waseem and Hovy, 2016;<cite> Davidson et al., 2017)</cite> , although Nobata et al. (2016) come closest to making a distinction between directed and generalized hate. The other dimension is the extent to which abusive language is explicit or implicit.",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_4",
  "x": "2 Cyberbullying and trolling are instances of directed abuse, aimed at individuals and online communities respectively. The second row shows cases with abusive expressions towards generalized groups such as racial categories and sexual orientations. Previous work has identified instances of hate speech that are both directed and generalized (Burnap and Williams, 2015; Waseem and Hovy, 2016;<cite> Davidson et al., 2017)</cite> , although Nobata et al. (2016) come closest to making a distinction between directed and generalized hate. The other dimension is the extent to which abusive language is explicit or implicit. This is roughly analogous to the distinction in linguistics and semiotics between denotation, the literal meaning of a term or symbol, and connotation, its sociocultural associations, famously articulated by Barthes (1957) . Explicit abusive lan-guage is that which is unambiguous in its potential to be abusive, for example language that contains racial or homophobic slurs. Previous research has indicated a great deal of variation within such language (Warner and Hirschberg, 2012;<cite> Davidson et al., 2017)</cite> , with abusive terms being used in a colloquial manner or by people who are victims of abuse.",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_5",
  "x": "Cyberbullying involves a victim whom annotators can identify and relatively easily discern whether statements directed towards the victim should be considered abusive. In contrast, in work on annotating harassment, offensive language, and hate speech there appears to be little consensus on definitions and lower inter-annotator agreement (\u03ba \u2248 0.60\u22120.80) (Ross et al., 2016; Waseem, 2016a; Tulkens et al., 2016; Bretschneider and Peters, 2017) are obtained. Given that these tasks are often broadly defined and the target is often generalized, all else being equal, it is more difficult for annotators to determine whether statements should be considered abusive. Future work in these subtasks should aim to have annotators distinguish between targeted and generalized abuse so that each subtype can be modeled more effectively. Annotation (via crowd-sourcing and other methods) tends to be more straightforward when explicit instances of abusive language can be identified and agreed upon (Waseem, 2016b) , but is considerably more difficult when implicit abuse is considered (Dadvar et al., 2013; Justo et al., 2014; Dinakar et al., 2011) . The connotations of language can be difficult to classify without domainspecific knowledge. Furthermore, while some argue that detailed guidelines can help annotators to make more subtle distinctions<cite> (Davidson et al., 2017)</cite> , others find that they do not improve the reliability of non-expert classifications (Ross et al., 2016) .",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_6",
  "x": "Annotation (via crowd-sourcing and other methods) tends to be more straightforward when explicit instances of abusive language can be identified and agreed upon (Waseem, 2016b) , but is considerably more difficult when implicit abuse is considered (Dadvar et al., 2013; Justo et al., 2014; Dinakar et al., 2011) . The connotations of language can be difficult to classify without domainspecific knowledge. Furthermore, while some argue that detailed guidelines can help annotators to make more subtle distinctions<cite> (Davidson et al., 2017)</cite> , others find that they do not improve the reliability of non-expert classifications (Ross et al., 2016) . In such cases, expert annotators with domain specific knowledge are preferred as they tend to produce more accurate classifications (Waseem, 2016a) . Ultimately, the nature of abusive language can be extremely subjective, and researchers must endeavor to take this into account when using human annotators. <cite>Davidson et al. (2017)</cite> , for instance, show that annotators tend to code racism as hate speech at a higher rate than sexism. As such, it is important that researchers consider the social biases that may lead people to disregard certain types of abuse.",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_7",
  "x": "As such, we do not propose that particular features are necessarily unique to each phenomenon, rather that they provide different insights and should be employed depending on what the researcher is attempting to measure. Directed abuse. Features that help to identify the target of abuse are crucial to directed abuse detection. Mentions, proper nouns, named entities, and co-reference resolution can all be used in different contexts to identify targets. Bretschneider and Peters (2017) use a multi-tiered system, first identifying offensive statements, then their severity, and finally the target. Syntactical features have also proven to be successful in identifying abusive language. A number of studies on hate speech use part-of-speech sequences to model the expression of hatred (Warner and Hirschberg, 2012; Gitari et al., 2015;<cite> Davidson et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_8",
  "x": "Explicit abuse Explicit abuse, whether directed or generalized, is often indicated by specific keywords. Hence, dictionary-based approaches may be well suited to identify this type of abuse (Warner and Hirschberg, 2012; Nobata et al., 2016) , although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways (Kwok and Wang, 2013;<cite> Davidson et al., 2017)</cite> . Negative polarity and sentiment of the text are also likely indicators of explicit abuse that can be leveraged by researchers (Gitari et al., 2015) . Implicit abuse. Building a specific lexicon may prove impractical, as in the case of the appropriation of the term \"skype\" in some forums (Magu et al., 2017) . Still, even partial lexicons may be used as seeds to inductively discover other keywords by use of a semi-supervised method proposed by King et al. (2017) . Additionally, character n-grams have been shown to be apt for abusive language tasks due to their ability to capture variation of words associated with abuse (Nobata et al., 2016; Waseem, 2016a) .",
  "y": "future_work"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_0",
  "x": "Unlike most of today's Email and Forum systems that use threaded structure by default. However, the Instant Messaging systems (e.g., Slack) often require users to manually organize messages in threads. A recent study (Wang et al., 2019) found that users most likely do not manually create threads. On average, only 15.3 threads were created per Slack channel with 355 messages, when they discuss group projects. Prior work on conversation thread disentanglement is often based on pairwise message compar- ison. Some solutions use unsupervised clustering methods with hand-engineered features (Wang and Oard, 2009; Shen et al., 2006) , while others use supervised approaches with statistical (Du et al., 2017) or linguistic features (Wang et al., 2008; Wang and Ros\u00e9, 2010; Elsner and Charniak, 2008 , 2011 Mayfield et al., 2012) . Recent work by <cite>(Jiang et al., 2018</cite>; Mayfield et al., 2012) adopt deep learning approaches to compute message pair similarity, using a combination of message content and simple contextual features (e.g., authorship and timestamps).",
  "y": "background"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_1",
  "x": "Based on these hypotheses, we propose two contextaware thread detection (CATD) models. The first model (CATD-MATCH) captures contexts of existing threads and computes the distance between the context and the input message; the second model (CATD-FLOW) captures the conversational flow, and computes the language genre consistency while attaching the input message to a thread. We also combine them with a dynamic gate for further performance improvement, followed by an efficient beam search mechanism in the inference step. The evaluation proves our approach improves over the existing methods. The contribution of this work is two-fold: 1) We propose context-aware deep learning models for thread detection and it advances the state-ofthe-art; 2) Based on the dataset in<cite> (Jiang et al., 2018)</cite> , we develop and release a more realistic multi-party multi-thread conversation dataset for future research. ---------------------------------- **METHODOLOGY**",
  "y": "uses"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_2",
  "x": "---------------------------------- **CONTEXT-AWARE THREAD DETECTION** We first adopt the Universal Sentence Encoder 2 with deep averaging network (USE) (Cer et al., 2018) to get a static feature representation for each message in the form of sentence embeddings. We encode each message m j as enc(m j ), by concatenating the USE output with two 20dimensional embeddings: (1) User-identity difference between m j and m i . (2) Time difference by mapping the time difference between m j and m i into 11 ranges (from 1 minutes to 72 hours, details in Appendix A). These two features are also used in<cite> (Jiang et al., 2018)</cite> , and another baseline model GTM uses only these features (Elsner and Charniak, 2008) . Given a message sequence M i 1 , which has been detected with L threads",
  "y": "similarities"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_3",
  "x": "g is computed based on the difference of the MATCH vector of context and the input message. Intuitively, if they are close, both FLOW and MATCH will be considered equally for prediction. Otherwise, the model dynamically computes the weights of MATCH and FLOW. Training Procedure: Following<cite> (Jiang et al., 2018)</cite> , apart from a new thread, we consider the candidate threads (Active Threads) in Eq. 1 only from those appearing in one hour time-frame before m i . During training, we treat the messages of a channel as a single sequence, and optimize Eq. 1 with training examples, containing m i and its active threads. Though messages are sorted by time, the training examples are shuffled during training. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_4",
  "x": "The new states with scores lower than top-B candidates are discarded. Similar to training, the active threads are also pruned by the \"one-hour\" constraint. However, they are not extracted from the groundtruth, but from previously-detected threads. ---------------------------------- **EXPERIMENTS** Datasets: We conduct extensive experiments on three publicly available datasets from Reddit datasets. We strictly follow<cite> (Jiang et al., 2018)</cite> to construct our data.",
  "y": "similarities uses"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_5",
  "x": "We take three sub-reddits to build three datasets, Gadgets, IPhones and Politics. 3 The data statistics and examples are shown in Appendix B. Reddit Dataset Improvement: We use the same pre-processing method in<cite> (Jiang et al., 2018)</cite> : we discard the messages which have less than 10 words or more than 100 words. Conversations less than 10 messages are also discarded. We guarantee that no more than 10 conversations happen at the same time. In their work, different message pairs of the same thread might be included in both train and test sets. Instead, we split the datasets on the thread level because in realistic settings, test threads should be completely unseen in train set.",
  "y": "similarities uses"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_6",
  "x": "We set it as 20.0 by the validation accuracy on Politics. We set LSTM output dimensions to 400, the batch size to 10 and the beam size to 5 by default. We train 50 epochs and select the model with the best validation-set performance. Baseline: (1) CISIR-SHCNN<cite> (Jiang et al., 2018)</cite> : A recently proposed model based on CNN and ranking message pairs. (2) CISIR-USE: We replace CNN encoder in CISIR with a USE to test the effect of different sentence encoders. (3) GTM (Elsner and Charniak, 2008) : A graph-theoretical model with chat and content specific features. ----------------------------------",
  "y": "background"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_7",
  "x": "**MODEL VARIATIONS** NMI ARI F1 A COMBINE (share) .832 .420 .422 B COMBINE (concat) .828 .446 .448 C SPLIT .824 .417 .420 D FLOW (K=5) .813 .395 .398 E FLOW (K=10) .823 .414 .417 F FLOW (K=20) .826 .420 .423 G MATCH (K=5) .820 .399 .402 H MATCH (K=10) .823 .405 .408 I MATCH (K=20) .831 .427 .430 J MATCH (K=20, bi-LSTM) .832 .428 .430 K COMBINE (K=5) .811 .378 .381 L COMBINE (K=10) .822 .403 .405 M COMBINE (K=20, B=1) . 828 .452 .455 N COMBINE (K=20, B=5) .834 .461 .464 O COMBINE (K=20, B=10) . 833 .431 .433 Evaluation Metrics: Normalized mutual information (NMI), Adjusted rand index (ARI) and F1 score, following<cite> (Jiang et al., 2018)</cite> . F1 is computed based on all message pairs in a test set. Also, following their work, we assume the candidate threads of each message for our models and baselines are obtained from the ones which have messages in the previous hour. For examples, the CISIR-SHCNN models will take pairs only within the one-hour frame.",
  "y": "similarities uses"
 },
 {
  "id": "fc58a9813b80afc9811b8ee27679b7_0",
  "x": "The event extraction task is related to several subtasks: event mention detection, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009 ). Current approaches for learning such patterns include bootstrapping techniques<cite> (Huang and Riloff, 2012a</cite>; Yangarber et al., 2000) , weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006) , fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009 ) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to apply a system on a different domain with less annotated data without reconsidering the design of the features used.",
  "y": "background"
 },
 {
  "id": "fc58a9813b80afc9811b8ee27679b7_1",
  "x": "The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009 ). Current approaches for learning such patterns include bootstrapping techniques<cite> (Huang and Riloff, 2012a</cite>; Yangarber et al., 2000) , weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006) , fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009 ) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to apply a system on a different domain with less annotated data without reconsidering the design of the features used. An important step forwards is TIER light<cite> (Huang and Riloff, 2012a</cite> ) that targeted the minimization of human supervision with a bootstrapping technique for event roles detection.",
  "y": "background"
 },
 {
  "id": "fc58a9813b80afc9811b8ee27679b7_2",
  "x": "**APPROACH** In this work, we approach the event extraction task by learning word representations from a domainspecific data set and by using these representations to identify the event roles. This idea relies on the assumption that the different words used for a given event role in the text share some semantic properties, related to their context of use and that these similarities can be captured by specific representations that can be automatically induced from the text, in an unsupervised way. We then propose to rely only on these word representations to detect the event roles whereas, in most works (Riloff, 1996; Patwardhan and Riloff, 2007;<cite> Huang and Riloff, 2012a</cite>; Huang and Riloff, 2012b) , the role fillers are represented by a set of different features (raw words, their parts-ofspeech, syntactic or semantic roles in the sentence). Furthermore, we propose two additional contributions to the construction of the word representations. The first one is to exploit limited knowledge about the event types (seed words) to improve the learning procedure by better selecting the dictionary. The second one is to use a max operation 1 on the word vector representations in order to build noun phrase representations (since slot fillers are generally noun phrases), which represents a better way of aggregating the semantic information born by the word representations.",
  "y": "differences"
 },
 {
  "id": "fc58a9813b80afc9811b8ee27679b7_3",
  "x": "---------------------------------- **TASK DESCRIPTION** We conducted the experiments on the official MUC-4 training corpus that consists of 1,700 documents and instantiated templates for each document. The task consists in extracting information about terrorist events in Latin America from news articles. We classically considered the following 4 types of events: attack, bombing, kidnapping and arson. These are represented by templates containing various slots for each piece of information that should be extracted from the document (perpetrators, human targets, physical targets, etc). Following previous works (Huang and Riloff, 2011;<cite> Huang and Riloff, 2012a)</cite> , we only consider the \"String Slots\" in this work (other slots need different treatments) and we group certain slots to finally consider the five slot types PerpInd (individual perpetrator), PerpOrg (organizational perpetrator), Target (physical target), Victim (human target name or description) and Weapon (instrument id or type).",
  "y": "similarities uses"
 },
 {
  "id": "fc58a9813b80afc9811b8ee27679b7_4",
  "x": "In all the experiments involving our model, we established the following stable choices of parameters: 50-dimensional vectors obtained by training on sequences of 5 words, which is consistent with previous studies (Turian et al., 2010; Collobert and Weston, 2008) . All the hyper-parameters of our model (e.g. learning rate, size of the hidden layer, size of the word vectors) have been chosen by finetuning our event extraction system on the TST1+TST2 data set. For DRVR-50 and W2V-50, the embeddings were built from the whole training corpus (1,300 documents) and the dictionary was made of all the words of this corpus under their inflected form. We used the extra-trees ensemble classifier implemented in (Pedregosa et al., 2011) , with hyperparameters optimized on the validation data: forest of 500 trees and the maximum number of features to consider when looking for the best split is \u221a number f eatures. We present a 3-fold evaluation: first, we compare our system with state-of-the-art systems on the same task, then we compare our domain-relevant vector representations (DRVR-50) to more generic word embeddings (C&W50, HLBL-50) 3 and finally to another State-of-the-art systems PerpInd PerpOrg Target Victim Weapon Average (Riloff, 1996) 33 word representation construction on the domainspecific data (W2V-50) 4 . Figure 1: F1-score results for event role labeling on MUC-4 data, for different size of training data, of \"String Slots\" on the TST3+TST4 with different parameters, compared to the learning curve of TIER<cite> (Huang and Riloff, 2012a)</cite> . The grey points represent the performances of other IE systems.",
  "y": "differences"
 },
 {
  "id": "fd50c8cf386e3ce8c8dd8dc46c467f_0",
  "x": "Their algorithm of the study was based on the extraction of structural patterns and peculiar structure of jokes (Taylor and Mazlack, 2004) . Later,<cite> Yang et al. (2015)</cite> formulated a classifier to distinguish between humorous and non-humorous instances, and also created computational models to discover the latent semantic structure behind humor from four perspectives: incongruity, ambiguity, interpersonal effect and phonetic style. Recently, with the rise of artificial neural networks, many studies utilize the methods for humor recognition. Luke and Alfredo applied recurrent neural network (RNN) to humor detec-tion from reviews in Yelp dataset. In addition, they also applied convolutional neural networks (CNNs) to train a model and the work shows that the model trained with CNNs has more accurate humor recognition (de Oliveira and Rodrigo, 2015) . In other research (Bertero and Fung, 2016) , CNNs were found to be a better sentence encoder for humor recognition as well. In a recent work, Chen and Lee predicted audience's laughter also using convolutional neural network.",
  "y": "background"
 },
 {
  "id": "fd50c8cf386e3ce8c8dd8dc46c467f_2",
  "x": "(b) the datasets in most studies are English corpus. (c) the evaluations are isolated from other research. In our work, we build the humor recognizer by using CNNs with extensive filter size and number, and the result shows higher accuracy from previous CNNs models. We conducted experiments on two different dataset, which were used in the previous studies. One is Pun of the Day <cite>(Yang et al., 2015)</cite> , and the other is 16000 One-Liners (Mihalcea and Strapparava, 2005) . In addition, we constructed a Chinese dataset to evaluate the generality of the method performance on humor recognition against different languages. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "fd50c8cf386e3ce8c8dd8dc46c467f_3",
  "x": "We conducted experiments on two different dataset, which were used in the previous studies. One is Pun of the Day <cite>(Yang et al., 2015)</cite> , and the other is 16000 One-Liners (Mihalcea and Strapparava, 2005) . In addition, we constructed a Chinese dataset to evaluate the generality of the method performance on humor recognition against different languages. ---------------------------------- **DATA** To fairly evaluate the performance on humor recognition, we need the dataset to consist of both humorous (positive) and non-humorous (negative) samples. The datasets we use to construct humor recognition experiments includes four parts: Pun of the Day <cite>(Yang et al., 2015)</cite> , 16000 OneLiners (Mihalcea and Strapparava, 2005) , Short Jokes dataset and PTT jokes.",
  "y": "uses"
 },
 {
  "id": "fd50c8cf386e3ce8c8dd8dc46c467f_4",
  "x": "Table 2 shows the experiments on both 16000 One-Liners and Pun of the Day. We set the baseline on the previous works of<cite> Yang et al. (2015)</cite> by Random Forest with Word2Vec + Human Centric Feature (Word2Vec + HCF) and Chen and Lee (2017) by Convolutional Neural Networks. We choose a dropout rate at 0.5 and test our model's performance with two factors F and HN. F means the increase of filter size and number as we mentioned in section 4. Otherwise, the window sizes would be (5, 6, 7) and filter number is 100 that is the same with Chen and Lee (2017)'s. HN indicates that we use the highway layers to train deep networks and we set the HN layers = 3 because it has better stability and accuracy in training step. We could observe that when we use both F and 115 Table 3 presents the result of Short Jokes and PTT Jokes datasets. As we can see, for the datasets was construed, it achieve 0.924 on Short Jokes and 0.943 on PTT Jokes in terms of F1 score respectively.",
  "y": "uses"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_0",
  "x": "**INTRODUCTION** Paragraph vectors (Le and Mikolov, 2014 ) are a recent method for embedding pieces of natural language text as fixed-length, real-valued vectors. Extending the word2vec framework <cite>(Mikolov et al., 2013b)</cite> , paragraph vectors are typically presented as neural language models, and compute a single vector representation for each paragraph. Unlike word embeddings, paragraph vectors are not shared across the entire corpus, but are instead local to each paragraph. When interpreted as a latent variable, we expect them to have higher uncertainty when the paragraphs are short. Recently, Barkan (2017) proposed a probabilistic view of word2vec that has motivated research on combining word2vec with other priors (Bamler and Mandt, 2017) . Inspired by this progress, we extend paragraph vectors to a probabilistic model.",
  "y": "motivation background"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_1",
  "x": "Paragraph embeddings are built on top of word embeddings, a set of dimensionality reduction tools that map words from a large vocabulary to a dense vector representation. Most word embedding methods learn a point estimate for each embedding vector (Mikolov et al., 2013a,b; Mnih and Kavukcuoglu, 2013; Goldberg and Levy, 2014; Pennington et al., 2014) . Barkan (2017) pointed out that the skip-gram model with negative sampling, also known as word2vec <cite>(Mikolov et al., 2013b)</cite> , admits a Bayesian interpretation. The Bayesian skip-gram model allows uncertainty to be taken into account in a principled way, and lays the basis for our proposed Bayesian paragraph vector model. Many tasks in natural language processing require fixed-length features for text passages of variable length, such as sentences, paragraphs, or documents (in this paper, we treat these three terms interchangeably). Generalizing embeddings of single words, several methods have been proposed to find dense vector representations of paragraphs (Le and Mikolov, 2014; Kiros et al., 2015; Wieting et al., 2015; Palangi et al., 2016; Pagliardini et al., 2017) . Since paragraph embeddings are local to short pieces of text, we expect them to have high posterior uncertainty if the paragraphs are short.",
  "y": "background"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_2",
  "x": "**METHOD** In Sec. 3.1, we summarize the Bayesian skip-gram model on which our model is based. We then present our Bayesian paragraph model in Sec. 3.2, and discuss two inference methods in Sec. 3.3. ---------------------------------- **BAYESIAN SKIP-GRAM MODEL** The Bayesian skip-gram model (Barkan, 2017 ) is a probabilistic interpretation of word2vec <cite>(Mikolov et al., 2013b)</cite> . The left part of Figure 1 shows the generative process.",
  "y": "background"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_3",
  "x": "is the sigmoid function. The pairs with label z ij = 1 form the so-called positive examples, and are assumed to correspond to occurrences of the word i in the context of word j somewhere in the corpus. The so-called negative examples with label z ij = 0 do not correspond to any observation in the corpus. When training the model, we resort to the heuristics proposed in <cite>(Mikolov et al., 2013b)</cite> to create artificial evidence for the negative examples (see Section 3.2 below). ---------------------------------- **BAYESIAN PARAGRAPH VECTORS** Bayesian paragraph vectors (BPV) are a direct extension of the Bayesian skip-gram model.",
  "y": "uses"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_4",
  "x": "Following Le and Mikolov (2014) , we add d n to the context vector V j when we classify a given pair of words (i, j) as a positive or a negative example. Thus, the likelihood of a word pair (i, j) in document n to have label z n,ij \u2208 {0, 1} is We collect evidence for the positive examples X + n in each document n by forming pairs of words (w n,t , w n,t+\u03b4 ). Here, w n,t is the word class of the t th token, t runs over all tokens in document n, \u03b4 runs from \u2212c to c where c is a small context window size, and we exclude \u03b4 = 0. Negative examples are not observed in the corpus. Following<cite> Mikolov et al. (2013b)</cite> , we construct artificial evidence X \u2212 n for negative pairs by sampling from the noise distribution , where f is the empirical unigram frequency across the training corpus.",
  "y": "uses"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_5",
  "x": [
   "**ABSTRACT** Word2vec (Mikolov et al., 2013b ) has proven to be successful in natural language processing by capturing the semantic relationships between different words. Built on top of single-word embeddings, paragraph vectors (Le and Mikolov, 2014) find fixed-length representations for pieces of text with arbitrary lengths, such as documents, paragraphs, and sentences. In this work, we propose a novel interpretation for neural-network-based paragraph vectors by developing an unsupervised generative model whose maximum likelihood solution corresponds to traditional paragraph vectors. This probabilistic formulation allows us to go beyond point estimates of parameters and to perform Bayesian posterior inference. We find that the entropy of paragraph vectors decreases with the length of documents, and that information about posterior uncertainty improves performance in supervised learning tasks such as sentiment analysis and paraphrase detection. ----------------------------------"
  ],
  "y": "background"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_0",
  "x": "---------------------------------- **INTRODUCTION** Recent advances in word representation learning such as word2vec (Mikolov et al., 2013b) have significantly boosted the performance of numerous Natural Language Processing tasks (Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015) . Despite their empirical performance, the inherent one-vector-per-word setting limits its application on tasks that require contextual understanding due to the existence of polysemous words such as part-of-speech tagging and semantic relatedness (Li and Jurafsky, 2015) . To this end, various sense-specific word embeddings have been proposed to account for the contextual subtlety of language (Reisinger and Mooney, 2010b,a;<cite> Huang et al., 2012</cite>; Neelakantan et al., 2015; Tian et al., 2014; Li and Jurafsky, 2015; Arora et al., 2016) . A majority of these methods propose to learn multiple vectors for each word via clustering. (Reisinger and Mooney, 2010b;<cite> Huang et al., 2012</cite>; Neelakantan et al., 2015) uses neural networks to learn cluster embeddings in order to matcha polysemous word with its correct sense embeddings.",
  "y": "background"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_1",
  "x": "(Reisinger and Mooney, 2010b;<cite> Huang et al., 2012</cite>; Neelakantan et al., 2015) uses neural networks to learn cluster embeddings in order to matcha polysemous word with its correct sense embeddings. Side information such as topical understanding (Liu et al., 2015b,a) or paralleled foreign language data (Guo et al., 2014; \u0160uster et al., 2016; Shyam et al., 2017) have also been exploited for clustering different meanings of multi-sense words. Another trend is to forgo word embeddings in favor of sentence or paragraph embeddings for specific tasks (? Kiros et al., 2015; Le and Mikolov, 2014) . While being more flexible and adaptive to context, all these approaches require sophisticated neural network structures and are problem specific, taking away the advantage offered by the unsupervised embedding approaches of single-sense embeddings. This paper bridges this gap. In this paper we propose a novel and extremely simple approach to learn sense-specific word embeddings. The essence of our approach is to assign each word a global base vector and model the contextual embedding as a linear combination of its context base vectors.",
  "y": "background"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_2",
  "x": "We evaluate our approach on various tasks that require contextual understanding of words, combining existing and new test datasets and evaluation metrics: word-sense induction ( (Koeling et al., 2005; Bartunov et al., 2015) ), contextual word similarity<cite> ((Huang et al., 2012</cite> ) and a new test set), and relevance detection ( (Arora et al., 2016) and a new test set). To the best of our knowledge, no prior literature has provided a comprehensive evaluation of all these multisense-specific tasks. Our simple, intuitive model retains almost all advantages offered by more complicated multisense embedding models, and often surpasses the performance of nonlinear \"deep\" models. Our code and data are at https://github.com/ dingwc/multisense/ To summarize, the contributions of our paper are as follows: 1. We propose an extremely simple model for learning polysemous word representations 2. We propose several new larger test sets to evaluate polysemous word embeddings, supplementing those that already exist 3. We perform extensive comparisons of our model to other widely used multisense models in the literature, and show that the simplicity of our model does not tradeoff performance",
  "y": "uses"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_3",
  "x": "**OUR CONTEXTUAL EMBEDDING MODEL** Like unisense vectors, sense-specific vectors should be closely aligned to words in that sense. This idea of local similarity has been widely used to obtain context sense representation<cite> Huang et al., 2012</cite>; Le and Mikolov, 2014; Neelakantan et al., 2015) . It was also used to decompose unisense vector into sense specific vectors (Arora et al., 2016) . In this paper, we exploit this intuition and model the contextual embedding of a word as a linear combination of its contexts. Specifically, we consider a corpus drawn from a vocabulary V = (word 1 , . . . , word V ). We define the normalized cooccurence matrix as the V \u00d7 V (sparse 1 ) symmetric matrix W where",
  "y": "background"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_5",
  "x": "Cos-distance works best for all embeddings. However, for embeddings from<cite> (Huang et al., 2012)</cite> and (Neelakantan et al., 2015) , which all have norm \u2248 1, the choice of \u03b1 makes little difference. On the other hand, using the embeddings from and our method, which both have highly varying norms, the choice of \u03b1 greatly affects performance. ---------------------------------- **QUALITATIVE EXAMPLE** Having explained the norm-filtering property of our approach and the \u03b1-distance measure in Eq. (3), we are now able to show a few qualitative examples of our model. First, Table 2 shows closest words to bank in three different context.",
  "y": "differences"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_7",
  "x": "Therefore, in order to have our evaluations comparable with standard metrics, we keep \u03b1 = 1 (measuring cosine similarity for all tasks). We compare our method against multisense approaches in<cite> (Huang et al., 2012</cite>; Neelakantan et al., 2015; . In each case, we use their pre-trained model and choose the embedding of a target word that is closest to the context representation (as they suggest). Since the code in<cite> (Huang et al., 2012)</cite> allows choosing various distance functions, we pick all and report the best scores. For (Neelakantan et al., 2015; we use the cosine distance as recommended. Overall Performance Table 5 shows that our method consistently outperforms<cite> (Huang et al., 2012</cite>; Neelakantan et al., 2015) . We note that ) is learned using additional supervision from the WordNet knowledge-base in clustering; therefore, it achieves comparably much higher scores in WSR and CWS tasks in which the evaluation is also based on WordNet.",
  "y": "motivation"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_8",
  "x": "Since the code in<cite> (Huang et al., 2012)</cite> allows choosing various distance functions, we pick all and report the best scores. For (Neelakantan et al., 2015; we use the cosine distance as recommended. Overall Performance Table 5 shows that our method consistently outperforms<cite> (Huang et al., 2012</cite>; Neelakantan et al., 2015) . We note that ) is learned using additional supervision from the WordNet knowledge-base in clustering; therefore, it achieves comparably much higher scores in WSR and CWS tasks in which the evaluation is also based on WordNet. We now describe each task in detail. Word-Context Relevance (WCR) This task is proposed in (Arora et al., 2016) and aims to detect when word-context pairs are relevant. In<cite> (Huang et al., 2012</cite>; Neelakantan et al., 2015; , the relevance metric can be seen as the distance (cosine or Euclidean) between the query word and the context cluster center.",
  "y": "differences"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_9",
  "x": "Overall Performance Table 5 shows that our method consistently outperforms<cite> (Huang et al., 2012</cite>; Neelakantan et al., 2015) . We note that ) is learned using additional supervision from the WordNet knowledge-base in clustering; therefore, it achieves comparably much higher scores in WSR and CWS tasks in which the evaluation is also based on WordNet. We now describe each task in detail. Word-Context Relevance (WCR) This task is proposed in (Arora et al., 2016) and aims to detect when word-context pairs are relevant. In<cite> (Huang et al., 2012</cite>; Neelakantan et al., 2015; , the relevance metric can be seen as the distance (cosine or Euclidean) between the query word and the context cluster center. In our method, we rely on the filtering effect of W ij values to diminish the norm of words in irrelevant contexts; thus we propose the 2 -norms of the contextual embedding as the metric of relevance, where the target word is excluded from the context if present. 5 In all cases, the ability of this metric to capture relevancy is essential for the success of that embedding to be applied to real world corpora, where not all neighboring words are relevant.",
  "y": "background"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_12",
  "x": "An example is given in Table 4 . In our evaluation, we first prune the test (Neelakantan et al., 2015) texttt300d 10s 1.32c\u03bb 0mv embedding (top),<cite> (Huang et al., 2012)</cite> cossim embedding (middle), and (Chen et al., 2014) (bottom) . The amount of separation between the two distributions seems correlated with the success of the embedding on the WCR tasks. In comparison, our methoduses vector norms to distinguish relevance, the separation of which is shown in Figure 1 . set to only include words present in vocabularies available to all embeddngs. Following<cite> (Huang et al., 2012)</cite> , we sort all the n = 2003 test pairs based on predicted similarity score and compare such ranking against the ground-truth ranking indicated by the average human evaluation score. The distance between two rank-lists is measured using the Spearman correlation score.",
  "y": "uses"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_13",
  "x": "Following<cite> (Huang et al., 2012)</cite> , we sort all the n = 2003 test pairs based on predicted similarity score and compare such ranking against the ground-truth ranking indicated by the average human evaluation score. The distance between two rank-lists is measured using the Spearman correlation score. Example of SCWS test for admission and confession ... the reason the Buddha himself gave was that the admission of women would weaken the Sangha and shorten its lifetime ... ... They included a confession said to have been inadvertently included on a computer disk that was given to the press... avg. human-given sim. score: 2.3 Table 8 : An example of a pair of word-contexts for a single SCWS task. We note that in<cite> (Huang et al., 2012</cite> ) the similarity between two word-context pairs is the measured using avgSimC, a weighted average of cosine similarities between all possible representation vectors of w 1 and w 2 . This metric, however, can not be applied to our approach since we have an infinite number of possible contextual representation for each word. Therefore, we use the cosine similarity without averaging, which is reasonable for all the embedding approaches.",
  "y": "differences"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_15",
  "x": "C2 is much bigger, containing 783 words, 5,188 senses, and 961,670 examples. Given that such large datasets were already available, we did not need to create our own. For each word we create 80 \u2212 20% train-test splits, train a K-NN multiclass classifier with Euclidean distance between contextual word embeddings, and report the mean classification accuracy (Acc) averaged across all words. Discussion We have provided a wide array of evaluations for measuring different aspects of multisense word embeddings, both collected from existing test sources and formed ourselves through WordNet. Overall, we find that our simple model performs surprisingly well on all evaluations, with the only consistent competitor a WordNet based model. One thing to note is that the SCWS Spearman scores of the<cite> (Huang et al., 2012)</cite> listed here are much smaller than that first reported. This is entirely attributed to the fact that we use direct cosine similarity between word embeddings, whereas they use an averaged similarity across their provided context words.",
  "y": "differences"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_0",
  "x": "However, these approaches do not scale to complex domains (Mueller, 1998; Gordon, 2001 ). More recently, automatic induction of script knowledge from text have started to attract attention: these methods exploit either natural texts (Chambers & Jurafsky, 2008; 2009) or crowdsourced data<cite> (Regneri et al., 2010)</cite> , and, consequently, do not require expensive expert annotation. Given a text corpus, they extract structured representations (i.e. graphs), for example chains (Chambers & Jurafsky, 2008) or more gen- eral directed acyclic graphs<cite> (Regneri et al., 2010)</cite> . These graphs are scenario-specific, nodes in them correspond to events (and associated with sets of potential event mentions) and arcs encode the temporal precedence relation. These graphs can then be used to inform NLP applications (e.g., question answering) by providing information whether one event is likely to precede or succeed another. In this work we advocate constructing a statistical model which is capable of \"answering\" at least some of the questions these graphs can be used to answer, but doing this without explicitly representing the knowledge as a graph. In our method, the distributed representations (i.e. vectors of real numbers) of event realizations are computed based on distributed representations of predicates and their arguments, and then the event representations are used in a ranker to predict the expected ordering of events.",
  "y": "background"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_1",
  "x": "However, these approaches do not scale to complex domains (Mueller, 1998; Gordon, 2001 ). More recently, automatic induction of script knowledge from text have started to attract attention: these methods exploit either natural texts (Chambers & Jurafsky, 2008; 2009) or crowdsourced data<cite> (Regneri et al., 2010)</cite> , and, consequently, do not require expensive expert annotation. Given a text corpus, they extract structured representations (i.e. graphs), for example chains (Chambers & Jurafsky, 2008) or more gen- eral directed acyclic graphs<cite> (Regneri et al., 2010)</cite> . These graphs are scenario-specific, nodes in them correspond to events (and associated with sets of potential event mentions) and arcs encode the temporal precedence relation. These graphs can then be used to inform NLP applications (e.g., question answering) by providing information whether one event is likely to precede or succeed another. In this work we advocate constructing a statistical model which is capable of \"answering\" at least some of the questions these graphs can be used to answer, but doing this without explicitly representing the knowledge as a graph. In our method, the distributed representations (i.e. vectors of real numbers) of event realizations are computed based on distributed representations of predicates and their arguments, and then the event representations are used in a ranker to predict the expected ordering of events.",
  "y": "background"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_2",
  "x": "An approach based on frequency of predicate pairs (Chambers & Jurafsky, 2008) , is unlikely to make a right prediction as driving usually precedes disembarking. Similarly, an approach which treats the whole predicate-argument structure as an atomic unit<cite> (Regneri et al., 2010)</cite> will probably fail as well, as such a sparse model is unlikely to be effectively learnable even from large amounts of data. However, our embedding method would be expected to capture relevant features of the verb frames, namely, the transitive use for the predicate disembark and the effect of the particle away, and these features will then be used by the ranking component to make the correct prediction. In previous work on learning inference rules (Berant et al., 2011) , it has been shown that enforcing transitivity constraints on the inference rules results in significantly improved performance. The same is true for the event order- ing task, as scripts have largely linear structure, and observing that a \u227a b and b \u227a c is likely to imply a \u227a c. Interestingly, in our approach we implicitly learn the model which satisfies transitivity constraints, without the need for any explicit global optimization on a graph. The approach is evaluated on crowdsourced dataset of <cite>Regneri et al. (2010)</cite> and we demonstrate that using our model results in the 13.5% absolute improvement in F 1 on event ordering with respect to their graph induction method (84% vs. 71%). ----------------------------------",
  "y": "background"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_3",
  "x": "Similarly, an approach which treats the whole predicate-argument structure as an atomic unit<cite> (Regneri et al., 2010)</cite> will probably fail as well, as such a sparse model is unlikely to be effectively learnable even from large amounts of data. However, our embedding method would be expected to capture relevant features of the verb frames, namely, the transitive use for the predicate disembark and the effect of the particle away, and these features will then be used by the ranking component to make the correct prediction. In previous work on learning inference rules (Berant et al., 2011) , it has been shown that enforcing transitivity constraints on the inference rules results in significantly improved performance. The same is true for the event order- ing task, as scripts have largely linear structure, and observing that a \u227a b and b \u227a c is likely to imply a \u227a c. Interestingly, in our approach we implicitly learn the model which satisfies transitivity constraints, without the need for any explicit global optimization on a graph. The approach is evaluated on crowdsourced dataset of <cite>Regneri et al. (2010)</cite> and we demonstrate that using our model results in the 13.5% absolute improvement in F 1 on event ordering with respect to their graph induction method (84% vs. 71%). ---------------------------------- **MODEL**",
  "y": "uses"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_4",
  "x": "Additionally, we use a Gaussian prior on weights, regularizing both the embedding parameters and the vector w. We initialize word representations using the SENNA embeddings (Collobert et al., 2011) . ---------------------------------- **EXPERIMENTS** We evaluate our approach on crowdsourced data collected for script induction by <cite>Regneri et al. (2010)</cite> , though, in principle, the method is applicable in arguably more general setting of Chambers & Jurafsky (2008) . ---------------------------------- **DATA AND TASK** <cite>Regneri et al. (2010)</cite> collected short textual descriptions (called event sequence descriptions, ESDs) of various types of human activities (e.g., going to a restaurant, ironing clothes) using crowdsourcing (Amazon Mechanical Turk), this dataset was also complemented by descriptions provided in the OMICS corpus (Gupta & Kochenderfer, 2004) .",
  "y": "uses"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_5",
  "x": "---------------------------------- **DATA AND TASK** <cite>Regneri et al. (2010)</cite> collected short textual descriptions (called event sequence descriptions, ESDs) of various types of human activities (e.g., going to a restaurant, ironing clothes) using crowdsourcing (Amazon Mechanical Turk), this dataset was also complemented by descriptions provided in the OMICS corpus (Gupta & Kochenderfer, 2004) . The datasets are fairly small, containing 30 ESDs per activity type in average (we will refer to different activities as scenarios), but the collection can easily be extended given the low cost of crowdsourcing. The ESDs are written in a bullet-point style and the annotators were asked to follow the temporal order in writing. Consider an example ESD for the scenario prepare coffee : {go to coffee maker} \u2192 {fill water in coffee maker} \u2192 {place the filter in holder} \u2192 {place coffee in filter} \u2192 {place holder in coffee maker} \u2192 {turn on coffee maker}",
  "y": "background"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_6",
  "x": "Consider an example ESD for the scenario prepare coffee : {go to coffee maker} \u2192 {fill water in coffee maker} \u2192 {place the filter in holder} \u2192 {place coffee in filter} \u2192 {place holder in coffee maker} \u2192 {turn on coffee maker} Though individual ESDs may seem simple, the learning task is challenging because of the limited amount of training data, variability in the used vocabulary, optionality of events (e.g., going to the coffee machine may not be mentioned in a ESD), different granularity of events and variability in the ordering (e.g., coffee may be put in a filter before placing it in a coffee maker). Unlike our work, <cite>Regneri et al. (2010)</cite> relies on WordNet to provide extra signal when using the Multiple Sequence Alignment (MSA) algorithm. As in their work, each description was preprocessed to extract a predicate and heads of argument noun phrases to be used in the model. The methods are evaluated on human annotated scenariospecific tests: the goal is to classify event pairs as appearing in a given stereotypical order or not<cite> (Regneri et al., 2010</cite> ). ----------------------------------",
  "y": "differences"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_7",
  "x": "As in their work, each description was preprocessed to extract a predicate and heads of argument noun phrases to be used in the model. The methods are evaluated on human annotated scenariospecific tests: the goal is to classify event pairs as appearing in a given stereotypical order or not<cite> (Regneri et al., 2010</cite> ). ---------------------------------- **3** The model was estimated as explained in Section 2.2 with the order of events in ESDs treated as gold standard. We used 4 held-out scenarios to choose model parameters, no scenario-specific tuning was performed, and the 10 test scripts were not used to perform model selection. When testing, we predicted that the event pair (e 1 ,e 2 ) is in the stereotypical order (e 1 \u227a e 2 ) if the ranking score for e 1 exceeded the ranking score for e 2",
  "y": "background"
 },
 {
  "id": "fe2f22d3d25358b23d0b75a6edee57_0",
  "x": "We believe this is the first work that explores world-knowledge in the form of linked entities and fine grained entity types as features to improve neural question generation models. Recently, works on question generation have drifted towards neural-based approaches. These approaches typically involve end-to-end supervised learning to generate questions. Du et al. (2017) proposed sequence-to-sequence learning for question generation from text passages. <cite>Zhou et al. (2017)</cite> utilized the answer-position, and linguistic features such as named entity recognition (NER) and parts of speech (POS) information to further improve the QG performance as the model is aware that for which answer a question need to be generated. In the work of a multi-perspective context matching algorithm is employed. Harrison and Walker (2018) use a set of rich linguistic features along with a NQG model.",
  "y": "background"
 },
 {
  "id": "fe2f22d3d25358b23d0b75a6edee57_2",
  "x": "The word vectors, the embedded world knowledge feature vectors and the answer position indicator embedding vectors are concatenated and passed as input to the Bi-LSTM encoder. ---------------------------------- **ENTITY LINKING** In previous works<cite> (Zhou et al., 2017</cite>; Harrison and Walker, 2018) , named entity type features have been used. These features, however, only allow for the encoding of coarse level information such as knowledge of if an entity belongs to a set of predefined categories such as 'PERSON', 'LOCATION' and 'ORGANI-ZATION'. To alleviate this, we use the knowledge in the form of linked entities. In our experiments, we use Wikipedia as the knowledge base for which to link entities.",
  "y": "motivation"
 },
 {
  "id": "fe2f22d3d25358b23d0b75a6edee57_3",
  "x": "SQuAD is composed of more than 100K questions posed by crowd workers on 536 Wikipedia articles. We used the same split as<cite> (Zhou et al., 2017)</cite> . MS MARCO datasets contains 1 million queries with corresponding answers and passages. All questions are sampled from real anonymized user queries and context passages are extracted from real web documents. We picked a subset of MS MARCO data where answers (<= 10 words) are sub-spans within the passages (<= 600 words), and use dev set as test set (7, 849), and split train set with ratio 90%-10% into train (1, 36, 337) and dev (15, 148) sets. ---------------------------------- **EXPERIMENTAL SETTINGS**",
  "y": "similarities uses"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_0",
  "x": "For the computation of the data we follow the MapReduce (Dean and Ghemawat, 2004) paradigm. The computation of similarities between terms becomes challenging on large corpora, as both the numbers of terms to be compared and the number of context features increases. This makes standard similarity calculations as proposed in (Lin, 1998;<cite> Curran, 2002</cite>; Lund and Burgess, 1996; Weeds et al., 2004) computationally infeasible. These approaches first calculate an information measure between each word and the according context and then calculate the similarity between all words, based on the information measure for all shared contexts. ---------------------------------- **RELATED WORK** A variety of approaches to compute DTs have been proposed to tackle issues regarding size and runtime.",
  "y": "background"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_1",
  "x": "As an example the dependency relation (nsub;gave 2 ;I 1 ) could be transferred to <gave 2 ,(nsub;@;I 1 )> and <I 1 ,(nsub;gave 2 ;@)>. This representation scheme is more generic then the schemes introduced in (Lin, 1998;<cite> Curran, 2002)</cite> , as it allows to characterise pairs by several holes, which could be used to learn analogies, cf. (Turney and Littman, 2005) . ---------------------------------- **DISTRIBUTIONAL SIMILARITY** First, we count the frequency for each first-order relation and remove all features that occur with more than w terms, as these context features tend to be too general to characterise the similarity between other words (Rychl\u00fd and Kilgarriff, 2007; Goyal et al., 2010, cmp.) . From this. we calculate a significance score for all first-order relations. For this work, we implemented two different significance measures: Pointwise Mutual Information (PMI): (Church and Hanks, 1990 ) and Lexicographer's Mutual Information (LMI):",
  "y": "differences"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_2",
  "x": "Whereas the method introduced by (Pantel and Lin, 2002) is very similar to the one proposed in this paper (the similarity between terms is calculated solely by the number of features two terms share), they use PMI to rank features and do not use pruning to scale to large corpora, as they use a rather small corpus. Additionally, they do not evaluate the effect of such pruning. In contrast to the best measures proposed by Lin (1998;<cite> Curran (2002</cite>; Pantel et al. (2009; Goyal et al. (2010) we do not calculate any information measure using frequencies of features and terms (we use significance ranking instead), as shown in Table 1 . Additionally, we avoid any similarity measurement using the information measure, as also done in these approaches, to calculate the similarity over the feature counts of each term: we merely count how many salient features two terms share. All these constraints makes this approach more scalable to larger corpora, as we do not need to know the full list of Information Measures Lin's formula I(term, f eature) = lin(term, f eature) = log ---------------------------------- **SIMILARITY MEASURES**",
  "y": "differences"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_4",
  "x": "We create a gold standard, by extracting reasonable entries of these 2000 nouns using Roget's 1911 thesaurus, Moby Thesaurus, Merriam Webster's Thesaurus, the Big Huge Thesaurus and the OpenOffice Thesaurus and employ the inverse ranking measure<cite> (Curran, 2002)</cite> to evaluate the DTs. Furthermore, we introduce a WordNet-based method. To calculate the similarity between two terms, we use the WordNet::Similarity path (Pedersen et al., 2004) measure. While its absolute scores are hard to interpret due to inhomogenity in the granularity of WordNet, they are well-suited for relative comparison. The score between two terms is inversely proportional to the shortest path between all the synsets of both terms. The highest possible score is one, if two terms share a synset. We compare the average score of the top five (or ten) entries in the DT for each of the 2000 selected words for our comparison.",
  "y": "uses"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_5",
  "x": "Apart from the PMI measure, Curran's measure leads to the weakest results. We could not confirm that his measure outperforms Lin's measure as stated in<cite> (Curran, 2002)</cite> 1 . An explanation for this results might be the use of a different parser, very few test words and also a different gold standard thesaurus in his evaluation. Comparing our method using LMI to Lin's method, we achieve lower scores with our method using small corpora, but surpass Lin's measure from 10 million sentences onwards. Next, we show the results of the WordNet evaluation measure in Figure 2 . Comparing the top 10 (upper) to the top 5 words (lower) used for the evaluation, we can observe higher scores for the top 5 words, which validates the ranking. These results are highly correlated to the results achieved with the inverse ranking measure.",
  "y": "differences"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_6",
  "x": "We can see that our method using PMI does not decline for larger corpora, as the limit on first-order features is not reached and frequent features are still being used. Comparing our LMI DT is en par with Lin's measure for 10 million sentences, and makes better use of large data when using the complete dataset. Again, the inverse ranking and the WordNet Path measure are highly correlated. 2 Building a gold standard thesaurus following<cite> Curran (2002)</cite> needs access to all the used thesauri. Whereas for some, programming interfaces exist, often with limited access and licence restrictions, others have to be extracted manually. The results shown here validate our pruning approach. Whereas Lin and Curran propose approaches to filter features that have low word feature scores, they do not remove features that occur with too many words, which is done in this work.",
  "y": "background"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_0",
  "x": "Then, in (Hern\u00e1ndez Fusilier et al., 2015a ) the authors proposed a PU-learning variant for the same task, concluding the appropriateness of their approach for detecting opinion spam. In this paper we study the feasibility of the application of different features for representing safely information about clues related to fake reviews. We focus our study in a variant of the stylistic feature character n-grams named character n-grams in tokens. We also study an emotion-based feature and a linguistic processes feature based on LIWC variables. We evaluated the proposed features with a Support Vector Machines (SVM) classifier using a corpus of 1600 reviews of hotels (<cite>Ott et al., 2011</cite>; Ott et al., 2013) . We show an experimental study evaluating the single features and combining them with the intention to obtain better features. After that previous study, we selected the one with we obtained the best results and made direct and indirect comparisons with some other methods.",
  "y": "uses"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_1",
  "x": "---------------------------------- **EXPERIMENTAL STUDY** In order to evaluate our proposal, we have performed some experimental study on the first publicly available opinion spam dataset gathered and presented in (<cite>Ott et al., 2011</cite>; Ott et al., 2013) . We first describe the corpus and then we show the different experiments made. Finally we compare our results with those published previously. ---------------------------------- **OPINION SPAM CORPUS**",
  "y": "uses"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_2",
  "x": "Finally we compare our results with those published previously. ---------------------------------- **OPINION SPAM CORPUS** The Opinion Spam corpus presented in (<cite>Ott et al., 2011</cite>; Ott et al., 2013 ) is composed of 1600 positive and negative opinions for hotels with the corresponding gold-standard. From the 800 positive reviews (<cite>Ott et al., 2011</cite>) , the 400 truthful where mined from TripAdvisor 5-star reviews about the 20 most popular hotels in Chicago area. All reviews were written in English, have at least 150 characters and correspond to users who had posted opinions previously on TripAdvisor (non first-time authors). The 400 deceptive opinions correspond to the same 20 hotels and were gathered using Amazon Mechanical Turk crowdsourcing service.",
  "y": "background"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_3",
  "x": "The Opinion Spam corpus presented in (<cite>Ott et al., 2011</cite>; Ott et al., 2013 ) is composed of 1600 positive and negative opinions for hotels with the corresponding gold-standard. From the 800 positive reviews (<cite>Ott et al., 2011</cite>) , the 400 truthful where mined from TripAdvisor 5-star reviews about the 20 most popular hotels in Chicago area. All reviews were written in English, have at least 150 characters and correspond to users who had posted opinions previously on TripAdvisor (non first-time authors). The 400 deceptive opinions correspond to the same 20 hotels and were gathered using Amazon Mechanical Turk crowdsourcing service. From the 800 negative reviews (Ott et al., 2013) , the 400 truthful where mined from TripAdvisor, Expedia, Hotels.com, Orbitz, Priceline and Yelp. The reviews are 1 or 2-star category and are about the same 20 hotels in Chicago. The 400 deceptive reviews correspond to the same 20 hotels and were obtained using Amazon Mechanical Turk.",
  "y": "background"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_4",
  "x": "In (Ren et al., 2014) a semi-supervised model called mixing population and individual property PU learning, is presented. The model is then incorporated to a SVM classifier. In (<cite>Ott et al., 2011</cite> ) <cite>the authors</cite> used the 80 dimensions of LIWC2007, unigrams and bigrams as set of features with a SVM classifier. In (Feng and Hirst, 2013) , profile alignment compatibility features combined with unigrams, bigrams and syntactic production rules were proposed for representing the opinion spam corpus. Then, a multivariate performance measures version of SVM classifier (named SVM perf ) was trained. In (Hern\u00e1ndez Fusilier et al., 2015b ) the authors studied two different representations: character n-grams and word n-grams. In particular, the best results were obtained with a Na\u00efve Bayes classifier using character 4 and 5 grams as features.",
  "y": "background"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_5",
  "x": "In (Feng and Hirst, 2013) , profile alignment compatibility features combined with unigrams, bigrams and syntactic production rules were proposed for representing the opinion spam corpus. Then, a multivariate performance measures version of SVM classifier (named SVM perf ) was trained. In (Hern\u00e1ndez Fusilier et al., 2015b ) the authors studied two different representations: character n-grams and word n-grams. In particular, the best results were obtained with a Na\u00efve Bayes classifier using character 4 and 5 grams as features. As we stated before, two kinds of comparisons are shown: an indirect (we could not obtain the complete set of results reported by the authors) and a direct (the authors kindly made available the results and a statistical comparison can be performed). In Table 4 we can observe the indirect comparison of our results with those of (Banerjee and Chua, 2014) and (Ren et al., 2014) obtained with a 10 fold cross validation experiment, and then, with a 5 fold cross validation in order to make a fair comparison with the results of (<cite>Ott et al., 2011</cite>) and (Feng and Hirst, 2013) . Note that the results are expressed in terms of the accuracy as those were published by <cite>the authors</cite>; the results correspond only to positive reviews of the Opinion Spam corpus because the authors experimented in that corpus alone.",
  "y": "uses"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_6",
  "x": "In particular, the best results were obtained with a Na\u00efve Bayes classifier using character 4 and 5 grams as features. As we stated before, two kinds of comparisons are shown: an indirect (we could not obtain the complete set of results reported by the authors) and a direct (the authors kindly made available the results and a statistical comparison can be performed). In Table 4 we can observe the indirect comparison of our results with those of (Banerjee and Chua, 2014) and (Ren et al., 2014) obtained with a 10 fold cross validation experiment, and then, with a 5 fold cross validation in order to make a fair comparison with the results of (<cite>Ott et al., 2011</cite>) and (Feng and Hirst, 2013) . Note that the results are expressed in terms of the accuracy as those were published by <cite>the authors</cite>; the results correspond only to positive reviews of the Opinion Spam corpus because the authors experimented in that corpus alone. From the Table 4 we can observe that the combination of 13 independent variables seems to have the lowest prediction accuracy (accuracy = 70.50%). About the last result, the authors in (Banerjee and Chua, 2014) concluded that only articles and pronouns (over the 13 variables) could significantly distinguish true from false reviews. The accuracy of the semi-supervised model is slightly lower (86.69%) than that of our approach (89%), although good enough.",
  "y": "uses"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_7",
  "x": "Note that the results are expressed in terms of the accuracy as those were published by <cite>the authors</cite>; the results correspond only to positive reviews of the Opinion Spam corpus because the authors experimented in that corpus alone. From the Table 4 we can observe that the combination of 13 independent variables seems to have the lowest prediction accuracy (accuracy = 70.50%). About the last result, the authors in (Banerjee and Chua, 2014) concluded that only articles and pronouns (over the 13 variables) could significantly distinguish true from false reviews. The accuracy of the semi-supervised model is slightly lower (86.69%) than that of our approach (89%), although good enough. The authors concluded that the good performance of the semi-supervised model is due the topic information captured by the model combined with the examples and their similarity (Ren et al., 2014) . Then, they could obtain an accurate SVM classifier. Regarding the experiments with the 5 fold cross-validation, we obtained similar results to those of (<cite>Ott et al., 2011</cite>) and slightly lower than the ones of (Feng and Hirst, 2013) .",
  "y": "similarities"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_8",
  "x": "10 fold cross-validation (Banerjee and Chua, 2014) 70.50% (Ren et al., 2014) 86.69% Our approach 89% 5 fold cross-validation (<cite>Ott et al., 2011</cite>) 89.8% (Feng and Hirst, 2013) 91.3% Our approach 89.8% Table 4 : Indirect comparison of the performance. Deceptive opinions detection for positive reviews of Opinion Spam corpus (800 opinions). In Table 5 we can observe the direct comparison of the performance for the positive and negative polarities reviews of the Opinion Spam corpus considering the proposal of (Hern\u00e1ndez Fusilier et al., 2015b) . First column shows the representation proposed, the second one shows the amount of attributes (Attr.) of the representation, the third column shows the F-measure value (F) obtained after a 10 fold cross-validation process, and the last column shows the p-value obtained in the statistical significance test used to study the differences of performance between (Hern\u00e1ndez Fusilier et al., 2015b) It is interesting to note that the F-measure values obtained with both approaches are quite similar for positive and negative reviews, as we can observe in Table 5 . Regarding the amount of attributes used for each representation of the reviews, it is worth noting that our approach uses 97% and 95% fewer attributes for positive and negative reviews compared with the model of (Hern\u00e1ndez Fusilier et al., 2015b) . Even using a combination of two simple features as character 4-grams in tokens and LIWC variables as we have proposed, the amount of attributes is considerably lower than the traditional character n-grams without diminishing the quality of the classification.",
  "y": "similarities"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_9",
  "x": "Character n-grams in tokens seems to capture correctly the content and the writing style of the reviews helping this, in some way, to differentiate truthful from deceptive opinions. Many works have demonstrated that emotions-based features can discriminate deceptive text, but in our experimental study this feature seems not to provide too much useful information for detecting deception in reviews. We also have used some variables extracted from LIWC as pronouns, articles and verbs. That information combined with character 4-grams in tokens was selected for modeling the representation of the reviews. For the experimental study we have used the positive and negative polarities reviews corresponding to the corpora proposed by (<cite>Ott et al., 2011</cite>; Ott et al., 2013) with 800 reviews each one (400 true and 400 false opinions). We have used both corpora in a separate way but we have performed experiments joining both polarities reviews in a combined corpus of 1600 reviews. From the results obtained with the different features we have concluded that character 4-grams in tokens with LIWC variables performs the best using a SVM classifier.",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_0",
  "x": "****MODELING LATENT BIOGRAPHIC ATTRIBUTES IN CONVERSATIONAL GENRES**** **ABSTRACT** This paper presents and evaluates several original techniques for the latent classification of biographic attributes such as gender, age and native language, in diverse genres (conversation transcripts, email) and languages (Arabic, English). First, we present a novel partner-sensitive model for extracting biographic attributes in conversations, given the differences in lexical usage and discourse style such as observed between same-gender and mixedgender conversations. Then, we explore a rich variety of novel sociolinguistic and discourse-based features, including mean utterance length, passive/active usage, percentage domination of the conversation, speaking rate and filler word usage. Cumulatively up to 20% error reduction is achieved relative to the standard <cite>Boulis and Ostendorf (2005)</cite> algorithm for classifying individual conversations on Switchboard, and accuracy for gender detection on the Switchboard corpus (aggregate) and Gulf Arabic corpus exceeds 95%. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_1",
  "x": "While small-scale sociolinguistic studies on monologues have shed some light on important features, we focus on modeling attributes from spoken conversations, building upon the work of <cite>Boulis and Ostendorf (2005)</cite> and show how gender and other attributes can be accurately predicted based on the following original contributions: 1. Modeling Partner Effect: A speaker may adapt his or her conversation style depending on the partner and we show how conditioning on the predicted partner class using a stacked model can provide further performance gains in gender classification. 2. Sociolinguistic features: The paper explores a rich set of lexical and non-lexical features motivated by the sociolinguistic literature for gender classification, and show how they can effectively augment the standard ngrambased model of <cite>Boulis and Ostendorf (2005)</cite> . 3. Application to Arabic Language: We also report results for Arabic language and show that the ngram model gives reasonably high accuracy for Arabic as well. Furthmore, we also get consistent performance gains due to partner effect and sociolingusic features, as observed in English. 4. Application to Email Genre: We show how the models explored in this paper extend to email genre, showing the wide applicability of general text-based features.",
  "y": "extends"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_2",
  "x": "Another computational study for gender classification using approximately 30 weblog entries was done by Herring and Paolillo (2006) , making use of a logistic regression model to study the effect of different features. While small-scale sociolinguistic studies on monologues have shed some light on important features, we focus on modeling attributes from spoken conversations, building upon the work of <cite>Boulis and Ostendorf (2005)</cite> and show how gender and other attributes can be accurately predicted based on the following original contributions: 1. Modeling Partner Effect: A speaker may adapt his or her conversation style depending on the partner and we show how conditioning on the predicted partner class using a stacked model can provide further performance gains in gender classification. 2. Sociolinguistic features: The paper explores a rich set of lexical and non-lexical features motivated by the sociolinguistic literature for gender classification, and show how they can effectively augment the standard ngrambased model of <cite>Boulis and Ostendorf (2005)</cite> . 3. Application to Arabic Language: We also report results for Arabic language and show that the ngram model gives reasonably high accuracy for Arabic as well. Furthmore, we also get consistent performance gains due to partner effect and sociolingusic features, as observed in English.",
  "y": "extends"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_3",
  "x": "2. Sociolinguistic features: The paper explores a rich set of lexical and non-lexical features motivated by the sociolinguistic literature for gender classification, and show how they can effectively augment the standard ngrambased model of <cite>Boulis and Ostendorf (2005)</cite> . 3. Application to Arabic Language: We also report results for Arabic language and show that the ngram model gives reasonably high accuracy for Arabic as well. Furthmore, we also get consistent performance gains due to partner effect and sociolingusic features, as observed in English. 4. Application to Email Genre: We show how the models explored in this paper extend to email genre, showing the wide applicability of general text-based features. 5. Application to new attributes: We show how the lexical model of <cite>Boulis and Ostendorf (2005)</cite> can be extended to Age and Native vs. Non-native prediction, with further improvements gained from our partner-sensitive models and novel sociolinguistic features.",
  "y": "extends"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_4",
  "x": "Gender differences has been one of the primary areas of sociolinguistic research, including work such as Coates (1998) and Eckert and McConnell-Ginet (2003) . There has also been some work in developing computational models based on linguistically interesting clues suggested by the sociolinguistic literature for detecting gender on formal written texts (Singh, 2001; Koppel et al., 2002; Herring and Paolillo, 2006) but it has been primarily focused on using a small number of manually selected features, and on a small number of formal written texts. Another relevant line of work has been on the blog domain, using a bag of words feature set to discriminate age and gender (Schler et al., 2006; Burger and Henderson, 2006; Nowson and Oberlander, 2006) . Conversational speech presents a challenging domain due to the interaction of genders, recognition errors and sudden topic shifts. While prosodic features have been shown to be useful in gender/age classification (e.g. Shafran et al., 2003) , their work makes use of speech transcripts along the lines of <cite>Boulis and Ostendorf (2005)</cite> in order to build a general model that can be applied to electronic conversations as well. While <cite>Boulis and Ostendorf (2005)</cite> observe that the gender of the partner can have a substantial effect on <cite>their</cite> classifier accuracy, given that same-gender conversations are easier to classify than mixed-gender classifications, <cite>they</cite> don't utilize this observation in <cite>their</cite> work. In Section 5.3, we show how the predicted gender/age etc.",
  "y": "background"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_5",
  "x": "of the partner/interlocutor can be used to improve overall performance via both dyadic modeling and classifier stacking. <cite>Boulis and Ostendorf (2005)</cite> have also constrained themselves to lexical n-gram features, while we show improvements via the incorporation of non-lexical features such as the percentage domination of the conversation, degree of passive usage, usage of subordinate clauses, speaker rate, usage profiles for filler words (e.g. \"umm\"), mean-utterance length, and other such properties. We also report performance gains of our models for a new genre (email) and a new language (Arabic), indicating the robustness of the models explored in this paper. Finally, we also explore and evaluate original model performance on additional latent speaker attributes including age and native vs. non-native English speaking status. ---------------------------------- **CORPUS DETAILS** Consistent with <cite>Boulis and Ostendorf (2005)</cite> , we utilized the Fisher telephone conversation corpus (Cieri et al., 2004) and we also evaluated performance on the standard Switchboard conversational corpus (Godfrey et al., 1992) , both collected and annotated by the Linguistic Data Consortium.",
  "y": "background"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_6",
  "x": "Conversational speech presents a challenging domain due to the interaction of genders, recognition errors and sudden topic shifts. While prosodic features have been shown to be useful in gender/age classification (e.g. Shafran et al., 2003) , their work makes use of speech transcripts along the lines of <cite>Boulis and Ostendorf (2005)</cite> in order to build a general model that can be applied to electronic conversations as well. While <cite>Boulis and Ostendorf (2005)</cite> observe that the gender of the partner can have a substantial effect on <cite>their</cite> classifier accuracy, given that same-gender conversations are easier to classify than mixed-gender classifications, <cite>they</cite> don't utilize this observation in <cite>their</cite> work. In Section 5.3, we show how the predicted gender/age etc. of the partner/interlocutor can be used to improve overall performance via both dyadic modeling and classifier stacking. <cite>Boulis and Ostendorf (2005)</cite> have also constrained themselves to lexical n-gram features, while we show improvements via the incorporation of non-lexical features such as the percentage domination of the conversation, degree of passive usage, usage of subordinate clauses, speaker rate, usage profiles for filler words (e.g. \"umm\"), mean-utterance length, and other such properties. We also report performance gains of our models for a new genre (email) and a new language (Arabic), indicating the robustness of the models explored in this paper.",
  "y": "motivation"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_7",
  "x": "Consistent with <cite>Boulis and Ostendorf (2005)</cite> , we utilized the Fisher telephone conversation corpus (Cieri et al., 2004) and we also evaluated performance on the standard Switchboard conversational corpus (Godfrey et al., 1992) , both collected and annotated by the Linguistic Data Consortium. In both cases, we utilized the provided metadata (including true speaker gender, age, native language, etc.) as only class labels for both training and evaluation, but never as features in the classification. The primary task we employed was identical to <cite>Boulis and Ostendorf (2005)</cite> , namely the classification of gender, etc. of each speaker in an isolated conversation, but we also evaluate performance when classifying speaker attributes given the combination of multiple conversations in which the speaker has participated. The Fisher corpus contains a total of 11971 speakers and each speaker participated in 1-3 conversations, resulting in a total of 23398 conversation sides (i.e. the transcript of a single speaker in a single conversation). We followed the preprocessing steps and experimental setup of <cite>Boulis and Ostendorf (2005)</cite> as closely as possible given the details presented in <cite>their</cite> paper, although some details such as the exact training/test partition were not currently obtainable from either the paper or personal communication. This resulted in a training set of 9000 speakers with 17587 conversation sides and a test set of 1000 speakers with 2008 conversation sides.",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_8",
  "x": "In both cases, we utilized the provided metadata (including true speaker gender, age, native language, etc.) as only class labels for both training and evaluation, but never as features in the classification. The primary task we employed was identical to <cite>Boulis and Ostendorf (2005)</cite> , namely the classification of gender, etc. of each speaker in an isolated conversation, but we also evaluate performance when classifying speaker attributes given the combination of multiple conversations in which the speaker has participated. The Fisher corpus contains a total of 11971 speakers and each speaker participated in 1-3 conversations, resulting in a total of 23398 conversation sides (i.e. the transcript of a single speaker in a single conversation). We followed the preprocessing steps and experimental setup of <cite>Boulis and Ostendorf (2005)</cite> as closely as possible given the details presented in <cite>their</cite> paper, although some details such as the exact training/test partition were not currently obtainable from either the paper or personal communication. This resulted in a training set of 9000 speakers with 17587 conversation sides and a test set of 1000 speakers with 2008 conversation sides. The Switchboard corpus was much smaller and consisted of 543 speakers, with 443 speakers used for training and 100 speakers used for testing, resulting in a total of 4062 conversation sides for training and 808 conversation sides for testing.",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_9",
  "x": "**CORPUS DETAILS** Consistent with <cite>Boulis and Ostendorf (2005)</cite> , we utilized the Fisher telephone conversation corpus (Cieri et al., 2004) and we also evaluated performance on the standard Switchboard conversational corpus (Godfrey et al., 1992) , both collected and annotated by the Linguistic Data Consortium. In both cases, we utilized the provided metadata (including true speaker gender, age, native language, etc.) as only class labels for both training and evaluation, but never as features in the classification. The primary task we employed was identical to <cite>Boulis and Ostendorf (2005)</cite> , namely the classification of gender, etc. of each speaker in an isolated conversation, but we also evaluate performance when classifying speaker attributes given the combination of multiple conversations in which the speaker has participated. The Fisher corpus contains a total of 11971 speakers and each speaker participated in 1-3 conversations, resulting in a total of 23398 conversation sides (i.e. the transcript of a single speaker in a single conversation). We followed the preprocessing steps and experimental setup of <cite>Boulis and Ostendorf (2005)</cite> as closely as possible given the details presented in <cite>their</cite> paper, although some details such as the exact training/test partition were not currently obtainable from either the paper or personal communication.",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_11",
  "x": "The Switchboard corpus was much smaller and consisted of 543 speakers, with 443 speakers used for training and 100 speakers used for testing, resulting in a total of 4062 conversation sides for training and 808 conversation sides for testing. ---------------------------------- **MODELING GENDER VIA NGRAM FEATURES (<cite>BOULIS AND OSTENDORF, 2005</cite>)** As our reference algorithm, we used the current state-of-the-art system developed by <cite>Boulis and Ostendorf (2005)</cite> using unigram and bigram features in a SVM framework. We reimplemented <cite>this</cite> model as our reference for gender classification, further details of which are given below: ---------------------------------- **TRAINING VECTORS**",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_12",
  "x": "We reimplemented <cite>this</cite> model as our reference for gender classification, further details of which are given below: ---------------------------------- **TRAINING VECTORS** For each conversation side, a training example was created using unigram and bigram features with tf-idf weighting, as done in standard text classification approaches. However, stopwords were retained in the feature set as various sociolinguistic studies have shown that use of some of the stopwords, for instance, pronouns and determiners, are correlated with age and gender. Also, only the ngrams with frequency greater than 5 were retained in the feature set following <cite>Boulis and Ostendorf (2005)</cite> . This resulted in a total of 227,450 features for the Fisher corpus and 57,914 features for the Switchboard corpus.",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_13",
  "x": "For compatibility with <cite>Boulis and Ostendorf (2005)</cite> , no special pre- processing for names is performed, and they are treated as just any other unigrams or bigrams 1 . Furthermore, the ngram-based approach scales well with varying the amount of conversation utilized in training the model as shown in Figure 1 . The \"<cite>Boulis and Ostendorf</cite>, 05\" rows in Table 3 show the performance of this reimplemented algorithm on both the Fisher (90.84%) and Switchboard (90.22%) corpora, under the identical training and test conditions used elsewhere in our paper for direct comparison with subsequent results 2 . ---------------------------------- **EFFECT OF PARTNER'S GENDER** Our original contribution in this section is the successful modeling of speaker properties (e.g. gender/age) based on the prior and joint modeling of the partner speaker's gender/age in the same discourse. The motivation here is that people tend to use stronger gender-specific, age-specific or dialect-specific word/phrase usage and discourse properties when speaking with someone of a similar gender/age/dialect than when speaking with someone of a different gender/age/dialect, when they may adapt a more neutral speaking style.",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_14",
  "x": "For compatibility with <cite>Boulis and Ostendorf (2005)</cite> , no special pre- processing for names is performed, and they are treated as just any other unigrams or bigrams 1 . Furthermore, the ngram-based approach scales well with varying the amount of conversation utilized in training the model as shown in Figure 1 . The \"<cite>Boulis and Ostendorf</cite>, 05\" rows in Table 3 show the performance of this reimplemented algorithm on both the Fisher (90.84%) and Switchboard (90.22%) corpora, under the identical training and test conditions used elsewhere in our paper for direct comparison with subsequent results 2 . ---------------------------------- **EFFECT OF PARTNER'S GENDER** Our original contribution in this section is the successful modeling of speaker properties (e.g. gender/age) based on the prior and joint modeling of the partner speaker's gender/age in the same discourse. The motivation here is that people tend to use stronger gender-specific, age-specific or dialect-specific word/phrase usage and discourse properties when speaking with someone of a similar gender/age/dialect than when speaking with someone of a different gender/age/dialect, when they may adapt a more neutral speaking style.",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_15",
  "x": "Thus, on top of the standard <cite>Boulis and Ostendorf (2005)</cite> model, we also investigated the following features motivated by the sociolinguistic literature on gender differences in discourse (Macaulay, 2005) : 1. % of conversation spoken: We measured the speaker's fraction of conversation spoken via three features extracted from the transcripts: % of words, utterances and time. 2. Speaker rate: Some studies have shown that males speak faster than females (Yuan et al., 2006) as can also be observed in Figure 2 showing empirical data obtained from Switchboard corpus. The speaker rate was measured in words/sec., using starting and ending time-stamps for the discourse. 3. % of pronoun usage: Macaulay (2005) argues that females tend to use more third-person male/female pronouns (he, she, him, her and his) as compared to males. 4. % of back-channel responses such as \"(laughter)\" and \"(lipsmacks)\".",
  "y": "extends"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_16",
  "x": "The evaluation measure was the standard classifier accuracy, that is, the fraction of test conversation sides whose gender was correctly predicted. Baseline performance (always guessing female) yields 57.47% and 51.6% on Fisher and Switchboard respectively. As noted before, the standard reference algorithm is <cite>Boulis and Ostendorf (2005)</cite> , and all cited relative error reductions are based on this established standard, as implemented in this paper. Also, as a second reference, performance is also cited for the popular \"Gender Genie\", an online gender-detector 7 , based on the manually weighted word-level sociolinguistic features discussed in Argamon et al. (2003) . The additional table rows are described in Sections 4-6, and cumulatively yield substantial improvements over the <cite>Boulis and Ostendorf (2005)</cite> with the work reported by <cite>Boulis and Ostendorf (2005)</cite> ), all of the above models can be easily extended to per-speaker evaluation by pooling in the predictions from multiple conversations of the same speaker. Table 5 shows the result of each model on a per-speaker basis using a majority vote of the predictions made on the individual conversations of the respective speaker. The consensus model when applied to Switchboard corpus show larger gains as it has 9.38 conversations per speaker on average as compared to 1.95 conversations per speaker on average in Fisher.",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_17",
  "x": "Table 4 combines the results of the experiments reported in the previous sections, assessed on both the Fisher and Switchboard corpora for gender classification. The evaluation measure was the standard classifier accuracy, that is, the fraction of test conversation sides whose gender was correctly predicted. Baseline performance (always guessing female) yields 57.47% and 51.6% on Fisher and Switchboard respectively. As noted before, the standard reference algorithm is <cite>Boulis and Ostendorf (2005)</cite> , and all cited relative error reductions are based on this established standard, as implemented in this paper. Also, as a second reference, performance is also cited for the popular \"Gender Genie\", an online gender-detector 7 , based on the manually weighted word-level sociolinguistic features discussed in Argamon et al. (2003) . The additional table rows are described in Sections 4-6, and cumulatively yield substantial improvements over the <cite>Boulis and Ostendorf (2005)</cite> with the work reported by <cite>Boulis and Ostendorf (2005)</cite> ), all of the above models can be easily extended to per-speaker evaluation by pooling in the predictions from multiple conversations of the same speaker. Table 5 shows the result of each model on a per-speaker basis using a majority vote of the predictions made on the individual conversations of the respective speaker.",
  "y": "differences"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_21",
  "x": "Gulf Arabic (52.5% sides are male) Ngram (<cite>Boulis & Ostendorf, 05</cite>) Results for Age and Native/Non-Native: Based on the prior distribution, always guessing the most likely class for age ( age less-than-orequal-to 40) results in 62.59% accuracy and always guessing the most likely class for native language (non-native) yields 50.59% accuracy. Table 9 shows the results for age and native/nonnative speaker status. We can see that the ngrambased approach for gender also gives reasonable performance on other speaker attributes, and more importantly, both the partner-model and sociolinguistic features help in reducing the error rate on age and native language substantially, indicating their usefulness not just on gender but also on other diverse latent attributes. Table 8 shows the most discriminative ngrams for binary classification of age, it is interesting to see the use of \"well\" right on top of the list for older speakers, also found in the sociolinguistic studies for age (Macaulay, 2005) . We also see that older speakers talk about their children (\"my daughter\") and younger speakers talk about their parents (\"my mom\"), the use of words such as \"wow\", \"kinda\" and \"cool\" is also common in younger speakers. To give maximal consistency/benefit to the <cite>Boulis and Ostendorf (2005)</cite> n-gram-based model, we did not filter the self-reporting n-grams such as \"im forty\" and \"im thirty\", putting our sociolinguisticliterature-based and discourse-style-based features at a relative disadvantage. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_22",
  "x": [
   "---------------------------------- **CONCLUSION** This paper has presented and evaluated several original techniques for the latent classification of speaker gender, age and native language in diverse genres and languages. A novel partner-sensitve model shows performance gains from the joint modeling of speaker attributes along with partner speaker attributes, given the differences in lexical usage and discourse style such as observed between same-gender and mixed-gender conversations. The robustness of the partner-model is substantially supported based on the consistent performance gains achieved in diverse languages and attributes. This paper has also explored a rich variety of novel sociolinguistic and discourse-based features, including mean utterance length, passive/active usage, percentage domination of the conversation, speaking rate and filler word usage. In addition to these novel models, the paper also shows how these models and the previous work extend to new languages and genres."
  ],
  "y": "differences"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_1",
  "x": "This is the problem of determining if a given event, represented as an s-v-o triple, is physically plausible (Table 1) . We show that in the original supervised setting a distributional model, namely a novel application of BERT (Devlin et al., 2019) , significantly outperforms the best existing method which has access to manually labeled physical features<cite> (Wang et al., 2018)</cite> . Still, the generalization ability of supervised models is limited by the coverage of the training set. We therefore present the more difficult problem of learning physical plausibility directly from text. We create a training set by parsing and extracting attested s-v-o triples from English Wikipedia, and we provide a baseline for training on this dataset and evaluating on <cite>Wang et al. (2018)</cite> 's physical plausibility task. We also experiment training on a large set of s-v-o triples extracted from the web as part of the NELL project (Carlson et al., 2010) , and find that Wikipedia triples result in better performance. arXiv:1911.05689v1 [cs.CL] 13 Nov 2019 <cite>Wang et al. (2018)</cite> present the semantic plausibility dataset that we use for evaluation in this work, and they show that distributional methods fail on this dataset.",
  "y": "differences"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_2",
  "x": "We create a training set by parsing and extracting attested s-v-o triples from English Wikipedia, and we provide a baseline for training on this dataset and evaluating on <cite>Wang et al. (2018)</cite> 's physical plausibility task. We also experiment training on a large set of s-v-o triples extracted from the web as part of the NELL project (Carlson et al., 2010) , and find that Wikipedia triples result in better performance. arXiv:1911.05689v1 [cs.CL] 13 Nov 2019 <cite>Wang et al. (2018)</cite> present the semantic plausibility dataset that we use for evaluation in this work, and they show that distributional methods fail on this dataset. This conclusion aligns with other work showing that GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) embeddings do not encode some salient features of objects (Li and Gauthier, 2017) . More recent work has similarly concluded that large pretrained language models only learn attested physical knowledge (Forbes et al., 2019) . Other datasets which include plausibility ratings are smaller in size and missing atypical but plausible events (Keller and Lapata, 2003) , or concern the more complicated problem of multi-event inference in natural language (Zhang et al., 2017; Sap et al., 2019) . Complementary to our work are methods of extracting physical features from a text corpus (Wang et al., 2017; Forbes and Choi, 2017; Bagherinezhad et al., 2016) .",
  "y": "uses"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_3",
  "x": "arXiv:1911.05689v1 [cs.CL] 13 Nov 2019 <cite>Wang et al. (2018)</cite> present the semantic plausibility dataset that we use for evaluation in this work, and they show that distributional methods fail on this dataset. This conclusion aligns with other work showing that GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) embeddings do not encode some salient features of objects (Li and Gauthier, 2017) . More recent work has similarly concluded that large pretrained language models only learn attested physical knowledge (Forbes et al., 2019) . Other datasets which include plausibility ratings are smaller in size and missing atypical but plausible events (Keller and Lapata, 2003) , or concern the more complicated problem of multi-event inference in natural language (Zhang et al., 2017; Sap et al., 2019) . Complementary to our work are methods of extracting physical features from a text corpus (Wang et al., 2017; Forbes and Choi, 2017; Bagherinezhad et al., 2016) . ---------------------------------- **DISTRIBUTIONAL MODELS**",
  "y": "background uses"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_4",
  "x": "**SELECTIONAL PREFERENCE** Closely related to semantic plausibility is selectional preference (Resnik, 1996) which concerns the semantic preference of a predicate for its arguments. Here, preference refers to the typicality of arguments: while it is plausible that a gorilla rides a camel, it is not preferred. Current approaches to selectional preference are distributional (Erk et al., 2010; Van de Cruys, 2014) and have shown limited performance in capturing semantic plausibility<cite> (Wang et al., 2018)</cite> . O S\u00e9aghdha and Korhonen (2012) have investigated combining a lexical hierarchy with a distributional approach, and there have been related attempts at grounding selectional preference in visual perception (Bergsma and Goebel, 2011; Shutova et al., 2015) . Models of selectional preference are either evaluated on a pseudo-disambiguation task, where attested predicate-argument tuples must be disambiguated from pseudo-negative random tuples, or evaluated on their correlation with human plausibility judgments. Selectional preference is one factor in plausibility and thus the two should correlate.",
  "y": "background"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_5",
  "x": "Models of selectional preference are either evaluated on a pseudo-disambiguation task, where attested predicate-argument tuples must be disambiguated from pseudo-negative random tuples, or evaluated on their correlation with human plausibility judgments. Selectional preference is one factor in plausibility and thus the two should correlate. ---------------------------------- **TASK** Following existing work, we focus on the task of single-event, physical plausibility. This is the problem of determining if a given event, represented as an s-v-o triple, is physically plausible. We use <cite>Wang et al. (2018)</cite> 's physical plausibility dataset for evaluation.",
  "y": "uses"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_6",
  "x": "The set of events and ground truth labels were manually curated. ---------------------------------- **SUPERVISED** In the supervised setting, a model is trained and tested on labelled events from the same distribution. Therefore, both the training and test set capture typical and atypical plausibility. We follow the same evaluation procedure as previous work and perform cross validation on the 3,062 labeled triples<cite> (Wang et al., 2018)</cite> . ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_7",
  "x": "Therefore, only the test set covers both typical and atypical plausibility. We create two training sets based on separate corpora: first, we parse English Wikipedia using the StanfordNLP neural pipeline (Qi et al., 2018) and extract attested s-v-o triples. Wikipedia has led to relatively good results for selectional preference (Zhang et al., 2019) , and in total we extract 6 million unique triples with a cumulative 10 million occurrences. Second, we use the NELL (Carlson et al., 2010) dataset of 604 million s-v-o triples extracted from the dependency parsed ClueWeb09 dataset. For NELL, we filter out triples with nonalphabetic characters or less than 5 occurrences, resulting in a total 2.5 million unique triples with a cumulative 112 million occurrences. For evaluation, we split <cite>Wang et al. (2018)</cite>'s 3,062 triples into equal sized validation and test sets. Each set thus consists of 1,531 triples.",
  "y": "extends"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_8",
  "x": "As a baseline, we consider the performance of a neural method for selectional preference (Van de Cruys, 2014). This method is a two-layer artificial neural network (NN) over static embeddings. Supervised. We reproduce the results of <cite>Wang et al. (2018)</cite> using GloVe embeddings and the same hyperparameter settings. Self-Supervised. We use this same method for learning from text (Subsection 3.2). To do so, we turn the training data into a self-supervised train-ing set: attested events are considered to be plausible, and pseudo-implausible events are created by sampling each word in an s-v-o triple independently by occurrence frequency.",
  "y": "similarities"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_9",
  "x": "**RESULTS** ---------------------------------- **SUPERVISED** For the supervised setting, we follow the same evaluation procedure as <cite>Wang et al. (2018)</cite> : we perform 10-fold cross validation on the dataset of 3,062 s-v-o triples, and report the mean accuracy of running this procedure 20 times all with the same model initialization (Table 3) . BERT outperforms existing methods by a large margin, including those with access to manually labeled physical features. We conclude from Model Accuracy Random 0.50 NN (Van de Cruys, 2014) 0.68 NN+WK<cite> (Wang et al., 2018)</cite> 0.76 Fine-tuned BERT 0.89 these results that distributional data does provide a strong cue for semantic plausibility in the supervised setting of <cite>Wang et al. (2018)</cite> . Examples of positive and negative results for BERT are presented in Table 4 .",
  "y": "similarities uses"
 }
]