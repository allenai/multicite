[
 {
  "id": "02521fd9721c264ee05315dec9b31d_0",
  "x": "Moreover, compared to previous work that uses two models in tandem <cite>(Baevski et al., 2019b)</cite> , by using one model for both BERT pre-trainining and fine-tuning, our model provides an average relative WER reduction of 9%. 1 ----------------------------------",
  "y": "differences"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_1",
  "x": "The practical need for building ASR systems for new conditions with limited resources spurred a lot of work focused on unsupervised speech recognition and representation learning (Park and Glass, 2008; Glass; et. al., a,f; van den Oord et al., 2018; , in addition to semiand weakly-supervised learning techniques aiming at reducing the supervised data needed in realworld scenarios (Vesely et al.; Li et al., b; Krishnan Parthasarathi and Strom; Chrupa\u0142a et al.; Kamper et al., 2017) . Recently impressive results have been reported for representation learning, that generalizes to different downstream tasks, through self-supervised learning for text and speech (Devlin et al., 2018; Baevski et al., 2019a; van den Oord et al., 2018;<cite> Baevski et al., 2019b)</cite> .",
  "y": "background"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_2",
  "x": "Self-supervised representation learning is done through tasks to predict masked parts of the input, reconstruct inputs through low bit-rate channels, or contrast similar data points against different ones. Different from <cite>(Baevski et al., 2019b)</cite> where the a BERT-like model is trained with the masked language model loss, frozen, and then used as a feature extractor in tandem with a final fully supervised convolutional ASR model (Collobert et al., 2016) , in this work, our \"Discrete BERT\" approach achieves an average relative Word Error Rate (WER) reduction of 9% by pre-training and fine-tuning the same BERT model using a Connectionist Temporal Classification (Graves et al.) loss. In addition, we present a new approach for pre-training bi-directional transformer models on continuous speech data using the InfoNCE loss (van den Oord et al., 2018) -dubbed \"continuous BERT\".",
  "y": "differences"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_3",
  "x": "---------------------------------- **VQ-WAV2VEC** vq-wav2vec <cite>(Baevski et al., 2019b)</cite> learns vector quantized (VQ) representations of audio data using a future time-step prediction task.",
  "y": "background"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_4",
  "x": "---------------------------------- **DISCRETE BERT** Our work builds on the recently proposed work in <cite>(Baevski et al., 2019b)</cite> where audio is quantized using a contrastive loss, then features learned on top by a BERT model (Devlin et al., 2018) .",
  "y": "extends"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_5",
  "x": "Our work builds on the recently proposed work in <cite>(Baevski et al., 2019b)</cite> where audio is quantized using a contrastive loss, then features learned on top by a BERT model (Devlin et al., 2018) . For the vq-wav2vec quantization, we use the gumbelsoftmax vq-wav2vec model with the same setup as described in <cite>(Baevski et al., 2019b)</cite> . This model quantizes the Librispeech dataset into 13.5k unique codes.",
  "y": "similarities uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_6",
  "x": "To understand the impact of acoustic representations baked into the wav2vec features, as alternatives, we explore quantizing the standard melfrequency cepstral coefficients (MFCC) and logmel filterbanks coefficients (FBANK), choosing a subset small enough to fit into GPU memory and running k-means with 13.5k centroids (to match the vq-wav2vec setup) to convergence. We then assign the index of the closest centroid to represent each time-step. We train a standard BERT model (Devlin et al., 2018; with only the masked language modeling task on each set of inputs in the same way as described in <cite>(Baevski et al., 2019b)</cite> , namely by choosing tokens for masking with probability of 0.05, expanding each chosen token to a span of 10 masked tokens (spans may overlap) and then computing a cross-entropy loss which attempts to maximize the likelihood of predicting the true token for each one that was masked ( Figure  1a ).",
  "y": "similarities uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_7",
  "x": "We first train the vq-wav2vec quantization model following the gumbel-softmax recipe described in <cite>(Baevski et al., 2019b)</cite> . After training this model For quantizing MFCC and log-mel filterbanks we first compute dense features using the scripts from the Kaldi (Povey) toolkit. We then compute 13.5k K-Means centroids, to match the number of unique tokens produced by the vq-wav2vec model, using 8 32GB Volta GPUs.",
  "y": "uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_8",
  "x": "We train on 128 GPUs with a batch size of 3072 tokens per GPU giving a total batch size of 393k tokens (Ott et al., 2018) . Each token represents 10ms of audio data. To mask the input sequence, we follow <cite>(Baevski et al., 2019b)</cite> and randomly sample p = 0.05 of all tokens to be a starting index, without replacement, and mask M = 10 consecutive tokens from every sampled index; spans may overlap.",
  "y": "uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_9",
  "x": "We expect that increasing the size of the fixed positional embeddings, or switching to relative positional embeddings will improve performance on longer examples, but in this work we wanted to stay consistent with the setup in<cite> Baevski et al. (2019b)</cite> . The tandem model which uses the features extracted from the pre-trained BERT models is a character-based Wav2Letter setup of (Zeghidour et al., 2018) which uses seven consecutive blocks of convolutions (kernel size 5 with 1.000 \u00d7 10 3 channels), followed by a PReLU nonlinearity and a dropout rate of 1 \u00d7 10 \u22121 . The final representation is projected to a 28-dimensional probability over the vocabulary and decoded using the standard 4gram language model following the same protocol as for the fine-tuned models Table 1 presents WERs of different input features and pre-training methods on the standard Librispeech clean and other subsets using 10 hours and 1 hour of labeled data for fine-tuning.",
  "y": "uses"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_10",
  "x": "Compared to the two-model tandem system proposed in <cite>(Baevski et al., 2019b)</cite> , which uses a the discrete BERT features to train another ASR system from scratch, our discrete BERT model provides an average of 13% and 6% of WER reduction on clean and other subsets respectively, by pre-training and fine-tuning the same BERT model on the 10h labeled set. The wav2vec inputs represent one level of unsupervised feature discovery, which provides a better space for quantization compared to raw spectral features. The discrete BERT training augments the wav2vec features with a higher level of representation that captures the sequential structure of the full utterance through the masked language modeling loss.",
  "y": "differences"
 },
 {
  "id": "02521fd9721c264ee05315dec9b31d_11",
  "x": "---------------------------------- **DISCUSSION AND RELATED WORK** The the success of BERT (Devlin et al., 2018) and Word2Vec (Mikolov et al., 2013) for NLP tasks motivated more research on self-supervised approaches for acoustic word embedding and unsupervised acoustic feature representation (Bengio and Heigold; Levin et al.; Chung et al., b; He et al.; van den Oord et al., 2018;<cite> Baevski et al., 2019b)</cite> , either by predicting masked discrete or continuous input, or by contrastive prediction of neighboring or similarly sounding segments using distant supervision or proximity in the audio signal as an indication of similarity.",
  "y": "background"
 },
 {
  "id": "0593fb7ee345cf632e6a61f1f21e6c_0",
  "x": "In this survey, first we cover major datasets published for validating the Visual Question Answering task, such as VQA dataset [1] , DAQUAR [8] , Visual7W [9] and most recent datasets up to 2019 include Tally-QA [10] and KVQA [11] . Next, we discuss the stateof-the-art architectures designed for the task of Visual Question Answering such as Vanilla VQA [1] , Stacked Attention Networks [12] and Pythia v1.0 [13] . Next we present some of our computed results over the three architectures: vanilla VQA model [1] , Stacked Attention Network (SAN) [12] and Teney et al. model <cite>[14]</cite> .",
  "y": "uses"
 },
 {
  "id": "0593fb7ee345cf632e6a61f1f21e6c_1",
  "x": "This model is better suited for the VQA in videos which has more use cases than images. [20] VQA [1] , TDIUC [29] , COCO-QA [21] Faster-RCNN [22] , Differential Modules [30] , GRU [31] 68.59 (VQA-v2), 86.73 (TDIUC), 69.36 (COCO-QA) AAAI 2019 Pythia v1.0 [28] : Pythia v1.0 is the award winning architecture for VQA Challenge 2018 1 . The architecture is similar to Teney et al. <cite>[14]</cite> with reduced computations with elementwise multiplication, use of GloVe vectors [23] , and ensemble of 30 models.",
  "y": "similarities"
 },
 {
  "id": "0593fb7ee345cf632e6a61f1f21e6c_2",
  "x": "We considered the following three models for our experiments, 1) the baseline Vanilla VQA model [1] which uses the VGG16 CNN architecture [3] and LSTMs [7] , 2) the Stacked Attention Networks [12] architecture, and 3) the 2017 VQA challenge winner Teney et al. model <cite>[14]</cite> . We considered the widely adapted datasets such as standard VQA dataset [1] and Visual7W dataset [9] for the experiments. We used the Adam Optimizer for all models with Cross-Entropy loss function.",
  "y": "uses"
 },
 {
  "id": "0593fb7ee345cf632e6a61f1f21e6c_3",
  "x": "Each model is trained for 100 epochs for each dataset. The experimental results are presented in Table III in terms of the accuracy for three models over two datasets. In the experiments, we found that the Teney et al. <cite>[14]</cite> is the best performing model on both VQA and Visual7W Dataset.",
  "y": "uses"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_0",
  "x": "It is hence clear that one cannot learn all these diverse relations from the very small amounts of available training data. Instead, we would have to learn a more general representation of discourse expectations. Many recent discourse relation classification approaches have focused on cross-lingual data augmentation , training models to better represent the relational arguments by using various neural network models, including feed-forward network (Rutherford et al., 2017) , convolutional neural networks (Zhang et al., 2015) , recurrent neural network (Ji et al., 2016;<cite> Bai and Zhao, 2018)</cite> , character-based (Qin et al., 2016) or formulating relation classification as an adversarial task (Qin et al., 2017) .",
  "y": "background"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_1",
  "x": "Many recent discourse relation classification approaches have focused on cross-lingual data augmentation , training models to better represent the relational arguments by using various neural network models, including feed-forward network (Rutherford et al., 2017) , convolutional neural networks (Zhang et al., 2015) , recurrent neural network (Ji et al., 2016;<cite> Bai and Zhao, 2018)</cite> , character-based (Qin et al., 2016) or formulating relation classification as an adversarial task (Qin et al., 2017) . However, previously proposed neural models still crucially lack a representation of the typical relations between sentences: to solve the task properly, a model should ideally be able to form discourse expectations, i.e., to represent the typical causes, consequences, next events or contrasts to a given event described in one relational argument, and then assess the content of the second relational argument with respect to these expectations (see Example 1).",
  "y": "motivation"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_2",
  "x": "It provides a three level hierarchy of relation tags. Following the experimental settings and evaluation metrics in<cite> Bai and Zhao (2018)</cite> , we use two most-used splitting methods of PDTB data, denoted as PDTB-Lin (Lin et al., 2009) , which uses sections 2-21, 22, 23 as training, validation and test sets, and PDTB-Ji (Ji and Eisenstein, 2015) , which uses 2-20, 0-1, 21-22 as training, validation and test sets and report the overall accuracy score. In addition, we also performed 10-fold cross validation among sections 0-22, as promoted in .",
  "y": "uses"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_3",
  "x": "used a seq2seq model that learns better argument representations due to being trained to explicitate the implicit connective. In addition, their classifier also uses a memory network that is intended to help remember similar argument pairs encountered during training. The current best performance was achieved by<cite> Bai and Zhao (2018)</cite> , who combined representations from different grained em-beddings including contextualized word vectors from ELMo (Peters et al., 2018) , which has been proved very helpful.",
  "y": "background"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_4",
  "x": "In addition, we compared our results with a simple bidirectional LSTM network and pre-trained word embeddings from Word2Vec. We can see that on all settings, the model using BERT representations outperformed all existing systems with a substantial margin. It obtained improvements of 7.3% points on PDTB-Lin, 5.5% points on PDTB-Ji, compared with the ELMobased method proposed in <cite>(Bai and Zhao, 2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_5",
  "x": "We also tested the state of the art model of implicit discourse relation classification proposed by<cite> Bai and Zhao (2018)</cite> on BioDRB. From Table 2 , we can see that the BERT base model achieved almost 12% points improvement over the Bi-LSTM baseline and 15% points over<cite> Bai and Zhao (2018)</cite> . When fine-tuned on in-domain data in the crossvalidation setting, the improvement increases to around 17% points.",
  "y": "uses"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_6",
  "x": "From Table 2 , we can see that the BERT base model achieved almost 12% points improvement over the Bi-LSTM baseline and 15% points over<cite> Bai and Zhao (2018)</cite> . When fine-tuned on in-domain data in the crossvalidation setting, the improvement increases to around 17% points. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "0753a2be70f9844d353ec54c04fd53_8",
  "x": "From Table 2 , we can see that the BERT base model achieved almost 12% points improvement over the Bi-LSTM baseline and 15% points over<cite> Bai and Zhao (2018)</cite> . When fine-tuned on in-domain data in the crossvalidation setting, the improvement increases to around 17% points. Cross-Domain In-Domain Bi-LSTM + w2v 300 32.97 46.49<cite> Bai and Zhao (2018)</cite> 29.52 55.90",
  "y": "differences"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_0",
  "x": "Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ).",
  "y": "background"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_1",
  "x": "Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features. Many of the applications relying on network analysis outperform other traditional shallow strategies in natural language processing (see e.g. the extractive summarization task [10, 11] ).",
  "y": "future_work motivation"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_2",
  "x": "Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features.",
  "y": "motivation future_work"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_3",
  "x": "Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies <cite>[8,</cite> 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features.",
  "y": "motivation"
 },
 {
  "id": "07a2b256766020450c85eae2839db8_4",
  "x": "More specifically, in (i), I propose 1 E-mail:diego.raphael@gmail.com, diego@icmc.usp.br December 4, 2014 the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors <cite>[8]</cite> . Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art.",
  "y": "uses"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_0",
  "x": "Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; <cite>Huang, 2008)</cite> can be viewed as previous attempts to higher-order constituent parsing, using some parts containing more than one grammar rule as non-local features. In this paper, we present a higher-order constituent parsing model 1 based on these previous works.",
  "y": "background"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_1",
  "x": "Following<cite> Huang (2008)</cite> , this algorithm traverses a parse forest in a bottom-up manner. However, it determines and keeps the best derivation for every grammar rule instance instead of for each node. Because all structures above the current rule instance is not determined yet, the computation of its nonlocal structural features, e.g., parent and sibling features, has to be delayed until it joins an upper level structure.",
  "y": "uses background"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_2",
  "x": "**DECODING** The factorization of the parsing model allows us to develop an exact decoding algorithm for it. Following<cite> Huang (2008)</cite> , this algorithm traverses a parse forest in a bottom-up manner.",
  "y": "uses"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_3",
  "x": "The factorization of the parsing model allows us to develop an exact decoding algorithm for it. This algorithm is more complex than the approximate decoding algorithm of<cite> Huang (2008)</cite> .",
  "y": "differences"
 },
 {
  "id": "07cee5aa02b518d48e41b1d6010c2f_5",
  "x": "4 Zhang et al. (2009) 92.3 Petrov (2010) 91.85 41.9 Self-training Zhang et al. (2009) (Kiefer, 1953) . The parameters \u03b8 of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and<cite> Huang (2008)</cite> . The performance of our first-and higher-order parsing models on all sentences of the two test sets is presented in Table 3 , where \u03bb indicates a tuned balance factor.",
  "y": "uses"
 },
 {
  "id": "086619bef9b4e7851bf42bf36eb14a_0",
  "x": "Recent methods applied to study texts at sentence level include probability distributions [24, 25, 26, 27] and correlations [26, 27, 17, <cite>28]</cite> . In recent studies, sentence length analysis have been related to style and authorship [29, 26, 27] . In general, sentence lengths have been quantified by the number of words [24, 29, 25, <cite>28]</cite> or characters [30, 31, 26, 27] .",
  "y": "background"
 },
 {
  "id": "086619bef9b4e7851bf42bf36eb14a_1",
  "x": "In general, sentence lengths have been quantified by the number of words [24, 29, 25, <cite>28]</cite> or characters [30, 31, 26, 27] . Also, note that the recurrence time of full stops also quantifies the sentence length [3] . However, there are other possible variations, such as word length or word frequency mappings, where all words are brought to lower case and punctuation marks are removed.",
  "y": "background"
 },
 {
  "id": "086619bef9b4e7851bf42bf36eb14a_2",
  "x": "Basically, this law states that the bigger the whole, the smaller its parts and vice-versa. However, an issue about this relationship was addressed in [37] : \"Somewhat more problematic is the relation of sentence length to the word length.\" This comment is consistent with the one in<cite> [28]</cite> asserting that the Menzerath-Altmann law does not hold if the sentence length is measured in terms of characters instead of the number of words. In a similar way, the data of Fig. 2a indicates, in the context of this law, that the relationship between the number of words and characters is also problematic.",
  "y": "background"
 },
 {
  "id": "086619bef9b4e7851bf42bf36eb14a_3",
  "x": "We can infer that h differs very little from one series to another and that their values are close to 0.8, with h * \u223c 0.5, implying in long-range correlations. All the series from the other books reflects this behavior (h \u223c 0.75). This result is consistent with the multifractal analysis performed in<cite> [28]</cite> .",
  "y": "similarities"
 },
 {
  "id": "0888b30ae5dcc880761a92ffbdcd1b_0",
  "x": "Here we explore the possibility that Zipf's law is a consequence of compression, the minimization of the mean length of the words of a vocabulary [3] . This principle has already been used to explain the origins of other linguistic laws: Zipf's law of abbreviation, namely, the frequency of more frequent words to be shorter [3,<cite> 4]</cite> , and Menzerath's law, the tendency of a larger linguistic construct to be made of smaller components [5] . Our argument combines two constraints for compression: (1) non-singular coding, i.e. any two different words should not be represented by the same string of letters or phonemes, and (2) unique decipherability, i.e. given a continuous sequence of letters or phonemes, there should be only one way of segmenting it into words [6] .",
  "y": "background"
 },
 {
  "id": "0888b30ae5dcc880761a92ffbdcd1b_2",
  "x": "The last parameter has been introduced to accommodate other variants of Mandelbrot's model [9] . Mandelbrot's derivation assumes that typing at random determines the probability of a word, which has two key implications. First, a relationship between the length of a word and its probability<cite> [4]</cite> l = a log p + b,",
  "y": "background"
 },
 {
  "id": "0888b30ae5dcc880761a92ffbdcd1b_3",
  "x": "Third, its assumptions are far reaching: compression allows one to shed light on the origins of three linguistic laws at the same time: Zipf's law for word frequencies, Zipf's law of abbreviation and Menzerath's law with the unifying principle of compression [3, <cite>4,</cite> 5] . There are many ways of explaining the origins of power-law-like distributions such as Zipf's law for word frequencies [12] but compression appears to be as the only one that can lead to a compact theory of statistical laws of language. Although uniquely decipherable codes are a subset of non-singular codes, it is tempting to think that both optimal non-singular coding and optimal uniquely decipherable coding cannot be satisfied to a large extend simultaneously.",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_0",
  "x": "The second evaluation scheme, supervised evaluation, uses the training part of the dataset in order to map the automatically induced clusters to GS senses. In the next step, the testing corpus is used to measure the performance of systems in a Word Sense Disambiguation (WSD) setting. A significant limitation of F-Score is that it does not evaluate the make up of clusters beyond the majority class <cite>(Rosenberg and Hirschberg, 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_1",
  "x": "A significant limitation of F-Score is that it does not evaluate the make up of clusters beyond the majority class <cite>(Rosenberg and Hirschberg, 2007)</cite> . Moreover, F-Score might also fail to evaluate clusters which are not matched to any GS class due to their small size. These two limitations define the matching problem of F-Score <cite>(Rosenberg and Hirschberg, 2007)</cite> which can lead to: (1) identical scores between different clustering solutions, and (2) inaccurate assessment of the clustering quality.",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_2",
  "x": "In this paper, we focus on analysing the SemEval-2007 WSI evaluation schemes showing their deficiencies. Subsequently, we present the use of V-measure <cite>(Rosenberg and Hirschberg, 2007)</cite> as an evaluation measure that can overcome the current limitations of F-Score. Finally, we also suggest a small modification on the supervised evaluation scheme, which will possibly allow for a more reliable estimation of WSD performance.",
  "y": "extends differences"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_3",
  "x": "If the clustering is identical to the original classes in the datasets, F-Score will be equal to one. In the example of Table 1 , F-Score is equal to 0.714. As it can be observed, F-Score assesses the quality of a clustering solution by considering two different angles, i.e. homogeneity and completeness <cite>(Rosenberg and Hirschberg, 2007)</cite> .",
  "y": "similarities"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_4",
  "x": "However, F-Score suffers from the matching problem, which manifests itself either by not evaluating the entire membership of a cluster, or by not evaluating every cluster <cite>(Rosenberg and Hirschberg, 2007)</cite> . The former situation is present, due to the fact that F-Score does not consider the make-up of the clusters beyond the majority class <cite>(Rosenberg and Hirschberg, 2007)</cite> . For example, in Table 3 the F-Score of the clustering so- lution is 0.714 and equal to the F-Score of the clustering solution shown in Table 1 , although these are two significantly different clustering solutions.",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_5",
  "x": "The former situation is present, due to the fact that F-Score does not consider the make-up of the clusters beyond the majority class <cite>(Rosenberg and Hirschberg, 2007)</cite> . For example, in Table 3 the F-Score of the clustering so- lution is 0.714 and equal to the F-Score of the clustering solution shown in Table 1 , although these are two significantly different clustering solutions. In fact, the clustering shown in Table 3 should have a better homogeneity than the clustering shown in Table 1 , since intuitively speaking each cluster contains fewer classes.",
  "y": "differences"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_6",
  "x": "V-measure assesses the quality of a clustering solution by explicitly measuring its homogeneity and its completeness <cite>(Rosenberg and Hirschberg, 2007)</cite> . Recall that homogeneity refers to the degree that each cluster consists of data points which primarily belong to a single GS class. V-measure assesses homogeneity by examining the conditional entropy of the class distribution given the proposed clustering, i.e. H(GS|C).",
  "y": "similarities"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_7",
  "x": "When there is only a single class (H(GS) = 0), any clustering would produce a perfectly homogeneous solution. In the worst case, the class distribution within each cluster is equal to the overall class distribution (H(GS|C) = H(GS)), i.e. clustering provides no new information. Overall, in accordance with the convention of 1 being desirable and 0 undesirable, the homogeneity (h) of a clustering solution is 1 if there is only a single class, and 1\u2212 H(GS|C) H(GS) in any other case <cite>(Rosenberg and Hirschberg, 2007)</cite> .",
  "y": "similarities"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_8",
  "x": "This happens when each GS class is included in all clusters with a distribution equal to the distribution of sizes <cite>(Rosenberg and Hirschberg, 2007)</cite> . Formulas 4 and 5 define H(C) and H(C|GS). Finally h and c can be combined and produce V-measure, which is the harmonic mean of homogeneity and completeness.",
  "y": "background"
 },
 {
  "id": "09dad2fd96cd1d48936cd5b99a38e7_9",
  "x": "By the definition of homogeneity (section 3.1), this baseline is perfectly homogeneous, since each cluster contains one instance of a single sense. However, its completeness is not 0, as one might intuitively expect. This is due to the fact that V-measure considers as the worst solution in terms of completeness the one, in which each class is represented by every cluster, and specifically with a distribution equal to the distribution of cluster sizes <cite>(Rosenberg and Hirschberg, 2007)</cite> .",
  "y": "similarities"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_0",
  "x": "In addition, several works have demonstrated how combining robot affordance learning with language grounding can provide cognitive robots with new and useful skills, such as learning the association of spoken words with sensorimotor experience<cite> [10,</cite> 11] or sensorimotor representations [12] , learning tool use capabilities [13, 14] , and carrying out complex manipulation tasks expressed in natural language instructions which require planning and reasoning [15] . In <cite>[10]</cite> , a joint model is proposed to learn robot affordances (i. e., relationships between actions, objects and resulting effects) together with word meanings. The data contains robot manipulation experiments, each of them associated with a number of alternative verbal descriptions uttered by two speakers for a total of 1270 recordings.",
  "y": "background"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_1",
  "x": "In addition, several works have demonstrated how combining robot affordance learning with language grounding can provide cognitive robots with new and useful skills, such as learning the association of spoken words with sensorimotor experience<cite> [10,</cite> 11] or sensorimotor representations [12] , learning tool use capabilities [13, 14] , and carrying out complex manipulation tasks expressed in natural language instructions which require planning and reasoning [15] . In <cite>[10]</cite> , a joint model is proposed to learn robot affordances (i. e., relationships between actions, objects and resulting effects) together with word meanings. The data contains robot manipulation experiments, each of them associated with a number of alternative verbal descriptions uttered by two speakers for a total of 1270 recordings.",
  "y": "background"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_2",
  "x": "---------------------------------- **PROPOSED APPROACH** In this paper, we combine (1) the robot affordance model of <cite>[10]</cite> , which associates verbal descriptions to the physical interactions of an agent with the environment, with (2) the gesture recognition system of [4] , which infers the type of action from human user movements.",
  "y": "extends differences"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_3",
  "x": "In the complete framework, we will use Bayesian Networks (BNs), which are a probabilistic model that represents random variables and conditional dependencies on a graph, such as in Fig. 2 . One of the advantages of using BNs is that their expressive power allows the marginalization over any set of variables given any other set of variables. Our main contribution is that of extending <cite>[10]</cite> by relaxing the assumption that the action is known during the learning phase.",
  "y": "extends differences"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_4",
  "x": "Following the method adopted in <cite>[10]</cite> , we use a Bayesian probabilistic framework to allow a robot to ground the basic world behavior and verbal descriptions associated to it. The world behavior is defined by random variables describing: the actions A, defined over the set A = {ai}, object properties F , over F = {fi}, and effects E, over E = {ei}. We denote X = {A, F, E} the state of the world as experienced by the robot. The verbal descriptions are denoted by the set of words W = {wi}. Consequently, the relationships between words and concepts are expressed by the joint probability distribution p(X, W ) of actions, object features, effects, and words in the spoken utterance.",
  "y": "similarities uses"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_5",
  "x": "the probability of each word co-occurring in the verbal description associated to a robot experiment in the environment. This joint probability distribution, that is illustrated by the part of Fig. 2 enclosed in the dashed box, is estimated by the robot in an ego-centric way through interaction with the environment, as in <cite>[10]</cite> . As a consequence, during learning, the robot knows what action it is performing with certainty, and the variable A assumes a deterministic value.",
  "y": "background"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_6",
  "x": "In this study we wish to generalize the model of <cite>[10]</cite> by observing external (human) agents, as shown in Fig. 1 . For this reason, the full model is now extended with a perception module capable of inferring the action of the agent from visual inputs. This corresponds to the Gesture HMMs block in Fig. 2 .",
  "y": "extends"
 },
 {
  "id": "09dfa2f17283fe6b3fc28383f36732_7",
  "x": "In the experimental section, we will show that what the robot has learned subjectively or alone (by self-exploration, knowing the action identity as a prior <cite>[10]</cite> ), can subsequently be used when observing a new agent (human), provided that the actions can be estimated with Gesture HMMs as in [4] . ---------------------------------- **EXPERIMENTAL RESULTS**",
  "y": "background"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_0",
  "x": "**INTRODUCTION** Apart from its application to machine translation, the encoder-decoder or sequence-to-sequence (seq2seq) paradigm has been successfully applied to monolingual text-to-text tasks including simplification <cite>(Nisioi et al., 2017)</cite> , paraphrasing (Mallinson et al., 2017) , style transfer (Jhamtani et al., 2017) , sarcasm interpretation (Peled and Reichart, 2017) , automated lyric annotation (Sterckx et al., 2017) and dialogue systems (Serban et al., 2016) . A sequence of input tokens is encoded to a series of hidden states using an encoder network and decoded to a target domain by a decoder network.",
  "y": "background"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_1",
  "x": "We train on the Wikilarge collection used by Zhu (2010) . Wikilarge is a collection of 296,402 automatically aligned complex and simple sentences from the ordinary and simple English Wikipedia corpora, used extensively in previous work (Wubben et al., 2012; Woodsend and Lapata, 2011; Zhang and Lapata, 2017;<cite> Nisioi et al., 2017)</cite> . The training data includes 2,000 development and 359 test instances created by Xu et al. (2016) .",
  "y": "background"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_2",
  "x": "We extend the OpenNMT (Klein et al., 2017) framework with functions for attention generation and release our code as a submodule. We use a similar archi- Table 2 : Quantitative evaluation of existing baselines from previous work and seq2seq with prior attention from the CVAE when choosing an optimal z sample for BLEU scores. tecture as Zhu et al. (2010) and<cite> Nisioi et al. (2017)</cite> : 2 layers of stacked unidirectional LSTMs with bi-linear global attention as proposed by Luong et al. (2015) , with hidden states of 512 dimensions.",
  "y": "uses"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_3",
  "x": "Our standard seq2seq model with attention, without prior attention, obtains a score of 89.92 BLEU points, which is close to scores obtained by similar models used in existing 1 Fleish-Kincaid Grade Level index. work on neural text simplification (Zhang and Lapata, 2017;<cite> Nisioi et al., 2017)</cite> . In Table 2 , we compare our seq2seq model with attention and without prior attention.",
  "y": "similarities"
 },
 {
  "id": "0ab60c5c9ace058a5fbe3bc2643cba_4",
  "x": "For the same z value, a SARI value of 38.30 was reached. For comparison, we include the SMT-based model by (Wubben et al., 2012) , the NTS model by <cite>(Nisioi et al., 2017)</cite> and the EncDecA by (Zhang and Lapata, 2017) . For decreasing values of the first hidden dimension z 1 , we observe that attention becomes situated at the diagonal, thus keeping closer to the structure of the source sentence and having one-to-one word alignments.",
  "y": "uses"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_0",
  "x": "Our work is based on the recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> in the fashion domain. We introduce a multimodal extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model and show that this extension outperforms strong baselines in terms of text-based similarity metrics. We also showcase the shortcomings of current vision and language models by performing an error analysis on our system's output.",
  "y": "extends"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_1",
  "x": "While recent progress in deep learning has unified research at the intersection of vision and language, the availability of open-source multimodal dialogue datasets still remains a bottleneck. This research makes use of a recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> , which contains multiple dialogue sessions in the fashion domain. The <cite>MMD</cite> dataset provides an interesting new challenge, combining recent efforts on task-oriented dialogue systems, as well as visually grounded dialogue.",
  "y": "uses"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_2",
  "x": "This research makes use of a recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> , which contains multiple dialogue sessions in the fashion domain. The <cite>MMD</cite> dataset provides an interesting new challenge, combining recent efforts on task-oriented dialogue systems, as well as visually grounded dialogue. In contrast to simple QA tasks in visually grounded dialogue, e.g. (Antol et al., 2015) , <cite>it contains</cite> conversations with a clear end-goal.",
  "y": "background"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_3",
  "x": "In contrast to simple QA tasks in visually grounded dialogue, e.g. (Antol et al., 2015) , <cite>it contains</cite> conversations with a clear end-goal. However, in contrast to previous slot-filling dialogue systems, e.g. (Rieser and Lemon, 2011; Young et al., 2013) , <cite>it heavily relies on</cite> the extra visual modality to drive the conversation forward (see Figure 1) . In the following, we propose a fully data-driven response generation model for this task.",
  "y": "background"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_4",
  "x": "The <cite>MMD</cite> dataset provides an interesting new challenge, combining recent efforts on task-oriented dialogue systems, as well as visually grounded dialogue. In contrast to simple QA tasks in visually grounded dialogue, e.g. (Antol et al., 2015) , <cite>it contains</cite> conversations with a clear end-goal. However, in contrast to previous slot-filling dialogue systems, e.g. (Rieser and Lemon, 2011; Young et al., 2013) , <cite>it heavily relies on</cite> the extra visual modality to drive the conversation forward (see Figure 1) .",
  "y": "background"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_5",
  "x": "During generation, the decoder conditions on the previous output token. <cite>Saha et al. (2017)</cite> propose a similar baseline model for the <cite>MMD</cite> dataset, extending HREDs to include the visual modality. However, for simplicity's sake, they 'unroll' multiple images in a single utterance to include only one image per utterance.",
  "y": "similarities"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_6",
  "x": "For example in Figure 3 , when the user asks \"what about the 4th image?\", it is impossible to give a correct response without reasoning over all images in the previous response. In the following, we empirically show that our extension leads to better results in terms of text-based similarity measures, as well as quality of generated dialogues. Example contexts for a given system utterance; note the difference in our approach from <cite>Saha et al. (2017)</cite> when extracting the training data from the original chat logs.",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_7",
  "x": "<cite>The data</cite> contains complex user queries, which pose new challenges for multimodal, task-based dialogue, such as quantitative inference (sorting, counting and filtering): \"Show me more images of the 3rd product in some different directions\", inference using domain knowledge and long term context: \"Will the 5th result go well with a large sized messenger bag?\", inference over aggregate of images: \"List more in the upper material of the 5th image and style as the 3rd and the 5th\", co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of <cite>the dataset</cite> for the model. This is done since <cite>the authors</cite> originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3 ).",
  "y": "extends"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_8",
  "x": "<cite>The data</cite> contains complex user queries, which pose new challenges for multimodal, task-based dialogue, such as quantitative inference (sorting, counting and filtering): \"Show me more images of the 3rd product in some different directions\", inference using domain knowledge and long term context: \"Will the 5th result go well with a large sized messenger bag?\", inference over aggregate of images: \"List more in the upper material of the 5th image and style as the 3rd and the 5th\", co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of <cite>the dataset</cite> for the model. This is done since <cite>the authors</cite> originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3 ).",
  "y": "motivation differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_9",
  "x": "4 Note that the results reported in <cite>their paper</cite> are on a different version of the corpus, hence not directly comparable. Table 1 provides results for different configurations of our model (\"T\" stands for text-only in the encoder, \"M\" for multimodal, and \"attn\" for using attention in the decoder). We experimented with different context sizes and found that output quality improved with increased context size (models with 5-turn context perform better than those with a 2-turn context), confirming the observation by Serban et al. (2016 Serban et al. ( , 2017 .",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_10",
  "x": "We also tested whether multimodal input has an impact on the generated outputs. However, there was only a slight increase in BLEU score (M-HRED-attn vs T-HRED-attn). To summarize, our best performing model (M-HRED-attn) outperforms the model of <cite>Saha et al.</cite> by 7 BLEU points.",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_11",
  "x": "**CONCLUSION AND FUTURE WORK** In this research, we address the novel task of response generation in search-based multimodal dialogue by learning from the recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> . We introduce a novel extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model (Serban et al., 2016) and show that our implementation significantly outperforms the model of <cite>Saha et al. (2017)</cite> by modelling the full multimodal context.",
  "y": "motivation"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_12",
  "x": "**CONCLUSION AND FUTURE WORK** In this research, we address the novel task of response generation in search-based multimodal dialogue by learning from the recently released <cite>Multimodal Dialogue</cite> (<cite>MMD</cite>) dataset <cite>(Saha et al., 2017)</cite> . We introduce a novel extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model (Serban et al., 2016) and show that our implementation significantly outperforms the model of <cite>Saha et al. (2017)</cite> by modelling the full multimodal context.",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_13",
  "x": "We introduce a novel extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model (Serban et al., 2016) and show that our implementation significantly outperforms the model of <cite>Saha et al. (2017)</cite> by modelling the full multimodal context. Contrary to <cite>their results</cite>, our generation outputs improved by adding attention and increasing context size. However, we also show that multimodal HRED does not improve significantly over text-only HRED, similar to observations by Agrawal et al. (2016) and Qian et al. (2018) .",
  "y": "differences"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_14",
  "x": "**ANALYSIS AND RESULTS** We report sentence-level BLEU-4 (Papineni et al., 2002) , METEOR (Lavie and Agarwal, 2007) and ROUGE-L (Lin and Och, 2004) using the evaluation scripts provided by (Sharma et al., 2017) . We compare our results against <cite>Saha et al. (2017)</cite> by using <cite>their code</cite> and data-generation scripts.",
  "y": "uses"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_15",
  "x": "**DATASET** The <cite>MMD</cite> dataset <cite>(Saha et al., 2017)</cite> consists of 100/11/11k train/validation/test chat sessions comprising 3.5M context-response pairs for the model. Each session contains an average of 40 dialogue turns (average of 8 words per textual response, 4 images per image response).",
  "y": "background"
 },
 {
  "id": "0bb68718667b8850dc0110d10d1d3a_16",
  "x": "The <cite>MMD</cite> dataset <cite>(Saha et al., 2017)</cite> consists of 100/11/11k train/validation/test chat sessions comprising 3.5M context-response pairs for the model. Each session contains an average of 40 dialogue turns (average of 8 words per textual response, 4 images per image response). <cite>The data</cite> contains complex user queries, which pose new challenges for multimodal, task-based dialogue, such as quantitative inference (sorting, counting and filtering): \"Show me more images of the 3rd product in some different directions\", inference using domain knowledge and long term context: \"Will the 5th result go well with a large sized messenger bag?\", inference over aggregate of images: \"List more in the upper material of the 5th image and style as the 3rd and the 5th\", co-reference resolution.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_0",
  "x": "Stance detection (also called stance identi cation or stance classication) is one of the considerably recent research topics in natural language processing (NLP). It is usually de ned as a classi cation problem where for a text and target pair, the stance of the author of the text for that target is expected as a classi cation output from the set: {Favor, Against, Neither} <cite>[12]</cite> . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_1",
  "x": "Nevertheless, in sentiment analysis, the sentiment of the author of a piece of text usually as Positive, Negative, and Neutral is explored while in stance detection, the stance of the author of the text for a particular target (an entity, event, etc.) either explicitly or implicitly referred to in the text is considered. Like sentiment analysis, stance detection systems can be valuable components of information retrieval and other text analysis systems <cite>[12]</cite> . Previous work on stance detection include [16] where a stance classi er based on sentiment and arguing features is proposed in addition to an arguing lexicon automatically compiled.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_2",
  "x": "In [4] , the author claims that Wikipedia can be used to determine stances about controversial topics based on their previous work regarding controversy extraction on the Web. Among more recent related work, in [1] stance detection for unseen targets is studied and bidirectional conditional encoding is employed. e authors state that their approach achieves stateof-the art performance rates [1] on SemEval 2016 Twi er Stance Detection corpus <cite>[12]</cite> .",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_3",
  "x": "e authors indicate that the data set is also annotated with sentiment information in addition to stance, so it can help SIDEWAYS'17, July 2017, Prague, Czech Republic D. K\u00fc\u00e7\u00fck reveal associations between stance and sentiment [11] . Lastly, in <cite>[12]</cite> , SemEval 2016's aforementioned shared task on Twi er Stance Detection is described. Also provided are the results of the evaluations of 19 systems participating in two subtasks (one with training data set provided and the other without an annotated data set) of the shared task <cite>[12]</cite> .",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_4",
  "x": "Lastly, in <cite>[12]</cite> , SemEval 2016's aforementioned shared task on Twi er Stance Detection is described. Also provided are the results of the evaluations of 19 systems participating in two subtasks (one with training data set provided and the other without an annotated data set) of the shared task <cite>[12]</cite> . In this paper, we present a tweet data set in Turkish annotated with stance information, where the corresponding annotations are made publicly available.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_5",
  "x": "It is emphasized in the related literature that unigram-based methods are reliable for the stance detection task [16] and similarly unigram-based models have been used as baseline models in studies such as <cite>[12]</cite> . In order to be used as a baseline and reference system for further studies on stance detection in Turkish tweets, we have trained two SVM classi ers (one for each target) using unigrams as features. Before the extraction of unigrams, we have employed automated preprocessing to lter out the stopwords in our annotated data set of 700 tweets.",
  "y": "background"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_6",
  "x": "e 10-fold cross-validation results of the two classi ers are provided in Table 1 using the metrics of precision, recall, and F-Measure. e performance of the classi ers is be er for the Favor class for both targets when compared with the performance results for the Against class. e same percentage of common terms may not have been observed in tweets during the expression of negative stances towards the targets. Yet, completely the opposite pa ern is observed in stance detection results of baseline systems given in <cite>[12]</cite> , i.e., be er FMeasure rates have been obtained for the Against class when compared with the Favor class <cite>[12]</cite> .",
  "y": "differences"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_7",
  "x": "Yet, completely the opposite pa ern is observed in stance detection results of baseline systems given in <cite>[12]</cite> , i.e., be er FMeasure rates have been obtained for the Against class when compared with the Favor class <cite>[12]</cite> . Some of the baseline systems reported in <cite>[12]</cite> are SVM-based systems using unigrams and ngrams as features similar to our study, but their data sets include all three stance classes of Favor, Against, and Neither, while our data set comprises only tweets classi ed as belonging to Favor or Against classes. Another di erence is that the data sets in <cite>[12]</cite> have been divided into training and test sets, while in our study we provide 10-fold cross-validation results on the whole data set.",
  "y": "differences"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_8",
  "x": "Yet, completely the opposite pa ern is observed in stance detection results of baseline systems given in <cite>[12]</cite> , i.e., be er FMeasure rates have been obtained for the Against class when compared with the Favor class <cite>[12]</cite> . Some of the baseline systems reported in <cite>[12]</cite> are SVM-based systems using unigrams and ngrams as features similar to our study, but their data sets include all three stance classes of Favor, Against, and Neither, while our data set comprises only tweets classi ed as belonging to Favor or Against classes. Another di erence is that the data sets in <cite>[12]</cite> have been divided into training and test sets, while in our study we provide 10-fold cross-validation results on the whole data set.",
  "y": "differences"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_9",
  "x": "Yet, further experiments on the extended versions of our data set should be conducted and the results should again be compared with the stance detection results given in the literature. We have also evaluated SVM classi ers which use only bigrams as features, as ngram-based classi ers have been reported to perform be er for the stance detection problem <cite>[12]</cite> . However, we have observed that using bigrams as the sole features of the SVM classi ers leads to quite poor results.",
  "y": "uses"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_10",
  "x": "We have also evaluated SVM classi ers which use only bigrams as features, as ngram-based classi ers have been reported to perform be er for the stance detection problem <cite>[12]</cite> . However, we have observed that using bigrams as the sole features of the SVM classi ers leads to quite poor results. is observation may be due to the relatively limited size of the tweet data set employed.",
  "y": "differences"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_11",
  "x": "\u2022 Other features like emoticons (as commonly used for sentiment analysis), features based on hashtags, and ngram features can also be used by the classi ers and these classiers can be tested on larger data sets. Other classi cation approaches could also be implemented and tested against our baseline classi ers. Particularly, related methods presented in recent studies such as <cite>[12]</cite> can be tested on our data set.",
  "y": "uses future_work"
 },
 {
  "id": "0bd3236100730487986ade49af24b9_12",
  "x": "\u2022 Other features like emoticons (as commonly used for sentiment analysis), features based on hashtags, and ngram features can also be used by the classi ers and these classiers can be tested on larger data sets. Other classi cation approaches could also be implemented and tested against our baseline classi ers. Particularly, related methods presented in recent studies such as <cite>[12]</cite> can be tested on our data set.",
  "y": "future_work"
 },
 {
  "id": "0bfea881773f504206bef9c1394f20_0",
  "x": "We computed multiple random subsets of sentences from the UMBC WEBBASE CORPUS (\u223c 17.13GB) via a custom implementation using the SPARK distributed framework. We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC <cite>[4]</cite>), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) [5] .",
  "y": "uses"
 },
 {
  "id": "0bfea881773f504206bef9c1394f20_1",
  "x": "**POSTERIOR EVALUATION DISTRIBUTION OF SUBSETS** We computed multiple random subsets of sentences from the UMBC WEBBASE CORPUS (\u223c 17.13GB) via a custom implementation using the SPARK distributed framework. We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC <cite>[4]</cite> ), and of n-gram perplexity.",
  "y": "uses"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_0",
  "x": "Such an issue is particularly prevalent when employing VAE-RNN architectures for text modelling <cite>(Bowman et al., 2016)</cite> . In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse. Compared to existing VAE-RNN architectures, we show that our model can achieve much more stable training process and can generate text with significantly better quality.",
  "y": "motivation"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_1",
  "x": "Variational Autoencoder (VAE) (Kingma and Welling, 2013 ) is a powerful method for learning representations of high-dimensional data. However, recent attempts of applying VAEs to text modelling are still far less successful compared to its application to image and speech (Bachman, 2016; Fraccaro et al., 2016; Semeniuta et al., 2017) . When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018; Dieng et al., 2019) .",
  "y": "background"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_2",
  "x": "Various efforts have been made to alleviate the latent variable collapse issue. <cite>Bowman et al. (2016)</cite> uses KL annealing, where a variable weight is added to the KL term in the cost function at training time. Yang et al. (2017) discovered that there is a trade-off between the contextual capacity of the decoder and effective use of encoding information, and developed a dilated CNN as decoder which can vary the amount of conditioning context.",
  "y": "background"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_3",
  "x": "In a more recent work, Dieng et al. (2019) avoided latent variable collapse by including skip connections in the generative model, where the skip connections enforce strong links between the latent variables and the likelihood function. Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss <cite>(Bowman et al., 2016</cite>; , or resort to designing more sophisticated model structures (Yang et al., 2017; Xu and Durrett, 2018; Dieng et al., 2019) . In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse.",
  "y": "motivation"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_4",
  "x": "We evaluate our model against several strong baselines which apply VAE for text modelling <cite>(Bowman et al., 2016</cite>; Yang et al., 2017; Xu and Durrett, 2018) . We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation dataset (Novikova et al., 2017) . Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation.",
  "y": "differences"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_5",
  "x": "In this section, we discuss the technical details of the proposed holistic regularisation VAE (HR-VAE) model, a general architecture which can effectively mitigate the KL vanishing phenomenon. Our model design is motivated by one noticeable defect shared by the VAE-RNN based models in previous works <cite>(Bowman et al., 2016</cite>; Yang et al., 2017; Xu and Durrett, 2018; Dieng et al., 2019) . That is, all these models, as shown in Figure 1a , only impose a standard normal distribution prior on the last hidden state of the RNN encoder, which potentially leads to learning a suboptimal representation of the latent variable and results in model vulnerable to KL loss vanishing.",
  "y": "motivation"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_6",
  "x": "( 3) As can be seen in Eq. 3, our solution to the KL collapse issue does not require any engineering for balancing the weight between the reconstruction term and KL loss as commonly the case in existing works <cite>(Bowman et al., 2016</cite>; . The weight between these two terms of our model is simply 1 : 1. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_7",
  "x": "---------------------------------- **DATASETS** We evaluate our model on two public datasets, namely, Penn Treebank (PTB) (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation corpus (Novikova et al., 2017) , which have been used in a number of previous works for text generation <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018; Wiseman et al., 2018; Su et al., 2018) .",
  "y": "uses background"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_8",
  "x": "**IMPLEMENTATION DETAILS** For the PTB dataset, we used the train-test split following <cite>(Bowman et al., 2016</cite>; Xu and Durrett, 2018) . For the E2E dataset, we used the train-test split from the original dataset (Novikova et al., 2017) and indexed the words with a frequency higher than 3.",
  "y": "uses"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_9",
  "x": "**BASELINES** We compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base 3 : A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue <cite>(Bowman et al., 2016)</cite> ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder (Yang et al., 2017) ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (Xu and Durrett, 2018) .",
  "y": "uses"
 },
 {
  "id": "0da20f60adaff9637ebdbe2a27f2a4_10",
  "x": "These plots were obtained based on the E2E training set using the inputless setting. We can see that the KL loss of VAE-LSTMbase, which uses Sigmoid annealing <cite>(Bowman et al., 2016)</cite> , collapses to zero, leading to a poor generative performance as indicated by the high reconstruction loss. The KL loss for both VAE-CNN and vMF-VAE are nonzero, where the former mitigates the KL collapse issue with a KL loss clipping strategy and the latter by replacing the standard normal distribution for the prior with the vMF distribution (i.e., with the vMF distribution, the KL loss only depends on a fixed concentration parameter, and hence results in a constant KL loss).",
  "y": "uses"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_0",
  "x": "Representations that are both general and discriminative can serve as a tool for tackling various NLP tasks. While common sentence representation methods are unsupervised in nature, recently, an approach for learning universal sentence representation in a supervised setting was presented in <cite>(Conneau et al., 2017)</cite> . We argue that although promising results were obtained, an improvement can be reached by adding various unsupervised constraints that are motivated by auto-encoders and by language models.",
  "y": "background"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_1",
  "x": "While previous methods train sentence embeddings in an unsupervised manner, a recent work <cite>(Conneau et al., 2017)</cite> argued that better representations can be achieved via supervised training on a general sentence inference dataset (Bowman et al., 2015) . To this end, the authors use the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) to train different Table 1 : Sentence embedding results. BiLSTM refers to the original BiLSTM followed by MaxPooling implementation of <cite>(Conneau et al., 2017)</cite> which is the baseline for our work.",
  "y": "background"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_2",
  "x": "To this end, the authors use the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) to train different Table 1 : Sentence embedding results. BiLSTM refers to the original BiLSTM followed by MaxPooling implementation of <cite>(Conneau et al., 2017)</cite> which is the baseline for our work. AE Reg and LM Reg refers to the Auto-Encoder and Language-Model regularization terms described in 2.1 and Combined refers to optimizing with both terms.",
  "y": "uses"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_3",
  "x": "Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -Peters et al. (2018), CoVe -McCann et al. (2017 ) Peters et al. (2017 , Salant and Berant (2017) ) -in this work we jointly train the unsupervised and supervised tasks. To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by <cite>(Conneau et al., 2017)</cite> . We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of <cite>(Conneau et al., 2017)</cite> .",
  "y": "extends"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_4",
  "x": "Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -Peters et al. (2018), CoVe -McCann et al. (2017 ) Peters et al. (2017 , Salant and Berant (2017) ) -in this work we jointly train the unsupervised and supervised tasks. To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by <cite>(Conneau et al., 2017)</cite> . We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of <cite>(Conneau et al., 2017)</cite> .",
  "y": "differences"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_5",
  "x": "---------------------------------- **METHOD** Our approach builds upon the previous work of <cite>(Conneau et al., 2017)</cite> .",
  "y": "extends"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_6",
  "x": "The final sentence representation is obtained by taking the maximal value of each dimension of the {h t } hidden units (i.e.: max pooling). The original model of <cite>(Conneau et al., 2017)</cite> was trained on the SNLI dataset in a supervised fashion -given pairs of sentences s 1 and s 2 , denote their representation bys 1 and s 2 . During training, the concatenation ofs 1 ,s 2 , |s 1 \u2212s 2 | ands 1 * s 2 is fed to a three layer fully connected network followed by a softmax classifier.",
  "y": "uses"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_7",
  "x": "---------------------------------- **EXPERIMENTS** Following <cite>(Conneau et al., 2017)</cite> we have tested our approach on a wide array of classification tasks, including sentiment analysis (MR -Pang and Lee (2005) , SST -Socher et al. (2013) ), question-type (TREC -Li and Roth (2002) ), product reviews (CR - Hu and Liu (2004) ), subjectivity/objectivity (SUBJ - Pang and Lee (2005) ) and opinion polarity (MPQA -Wiebe et al. (2005) ).",
  "y": "uses"
 },
 {
  "id": "0f66e9a5c51cff004d97e4aaddf4d0_10",
  "x": "Leveraging supervision given by some general task aided in obtaining state-of-the-art sentence representations <cite>(Conneau et al., 2017)</cite> . However, every supervised learning tasks is prone to overfit. In this context, overfitting to the learning task will result in a model which generalizes less well to new tasks.",
  "y": "motivation"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_0",
  "x": "In contrast, using multi-step relation paths (e.g., husband(barack, michelle) \u2227 mother(michelle, sasha) to train KB embeddings has been proposed very recently <cite>(Guu et al., 2015</cite>; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015) . While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply.",
  "y": "background"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_1",
  "x": "In contrast, using multi-step relation paths (e.g., husband(barack, michelle) \u2227 mother(michelle, sasha) to train KB embeddings has been proposed very recently <cite>(Guu et al., 2015</cite>; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015) . While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply.",
  "y": "background motivation"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_2",
  "x": "---------------------------------- **PRIOR APPROACHES** The two approaches we consider here are: using relation paths to generate new auxiliary triples for training<cite> (Guu et al., 2015)</cite> and using relation paths as features for scoring (Lin et al., 2015) .",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_3",
  "x": "The intermediate nodes e i are neglected. The natural composition function of a BILINEAR model is matrix multiplication<cite> (Guu et al., 2015)</cite> . For this model, the embedding of a length-n path \u03a6 \u03c0 \u2208 R d\u00d7d is defined as the matrix product of the sequence of relation matrices for the relations in \u03c0.",
  "y": "background"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_4",
  "x": "**PRUNED-PATHS THIS METHOD COMPUTES AND** stores the values of the random walk probabilities for all pairs of nodes and relation paths, for which these probabilities are non-zero. This can be done in time O T where Triples is the same quantity used in the analysis of<cite> Guu et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_5",
  "x": "stores the values of the random walk probabilities for all pairs of nodes and relation paths, for which these probabilities are non-zero. This can be done in time O T where Triples is the same quantity used in the analysis of<cite> Guu et al. (2015)</cite> . The memory requirements of this method are the same as these of<cite> (Guu et al., 2015)</cite> , up to a constant to store random-walk probabilities for paths.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_6",
  "x": "The memory requirements of this method are the same as these of<cite> (Guu et al., 2015)</cite> , up to a constant to store random-walk probabilities for paths. The time requirements are different, however. At training time, we compute scores and update gradients for triples corresponding to direct 5 The computation uses the fact that the number of path type sequences of length l is N l r .",
  "y": "differences uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_7",
  "x": "We estimate the average number of active paths per node pair as T Ne 2 . Therefore the overall time for this method per training iteration is O 2d(\u03b7 + 1)E kb We should note that whether this method or the one of<cite> Guu et al. (2015)</cite> will be faster in training depends on whether the average number of paths per node pair multiplied by E kb is bigger or smaller than the total number of triples T .",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_8",
  "x": "Unlike the method of<cite> Guu et al. (2015)</cite> , the evaluation-time memory requirements of this approach are the same as its training memory requirements, or they could be reduced slightly to match the evaluation-time memory requirements of ALL-PATHS, if these are lower as determined by the specific problem instance. ---------------------------------- **ALL-PATHS THIS METHOD DOES NOT EXPLICITLY CONSTRUCT OR STORE FULLY CONSTRUCTED PATHS (S, \u03a0, T).**",
  "y": "differences"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_9",
  "x": "Based on this analysis, we computed training time and memory estimates for our NCI+Txt knowledge base. Given the values of the quantities from our knowledge graph and d = 50, \u03b7 = 50, and maximum path length of 5, the estimated memory for<cite> (Guu et al., 2015)</cite> and PRUNED-PATHS is 4.0 \u00d7 10 18 and for ALL-PATHS the memory is 1.9\u00d710 9 . The time estimates are 2.4\u00d710 21 , 2.6 \u00d7 10 25 , and 7.3 \u00d7 10 15 for<cite> (Guu et al., 2015)</cite> , PRUNED-PATHS, and ALL-PATHS, respectively.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_10",
  "x": "Based on this analysis, we computed training time and memory estimates for our NCI+Txt knowledge base. Given the values of the quantities from our knowledge graph and d = 50, \u03b7 = 50, and maximum path length of 5, the estimated memory for<cite> (Guu et al., 2015)</cite> and PRUNED-PATHS is 4.0 \u00d7 10 18 and for ALL-PATHS the memory is 1.9\u00d710 9 . The time estimates are 2.4\u00d710 21 , 2.6 \u00d7 10 25 , and 7.3 \u00d7 10 15 for<cite> (Guu et al., 2015)</cite> , PRUNED-PATHS, and ALL-PATHS, respectively.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_11",
  "x": "**EXPERIMENTS** Our experiments are designed to study three research questions: (i) What is the impact of using path representations as a source of compositional regularization as in<cite> (Guu et al., 2015)</cite> versus using them as features for scoring as in PRUNED-PATHS and ALL-PATHS? (ii) What is the impact of using textual mentions for KB completion in different models? (iii) Does modeling intermediate path nodes improve the accuracy of KB completion?",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_12",
  "x": "The triples are then split into train, dev, and test sets, of size 10224, 1315, 2784, respectively. We identified genes belonging to the same family via the common letter prefix in their names, which adds 1936 triples to training. As a second dataset, we used a WordNet KB with the same train, dev, and test splits as<cite> Guu et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_13",
  "x": "PRUNED-PATHS denotes the traditional approach that learns from sampled paths detailed in \u00a73.1.2; paths with occurrence less than a cutoff are pruned (c = 1 in Table 1 means that all sampled paths are used). The most relevant prior approach is<cite> Guu et al. (2015)</cite> . We ran experiments using both their publicly available code and our re-implementation.",
  "y": "background"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_14",
  "x": "The most relevant prior approach is<cite> Guu et al. (2015)</cite> . We ran experiments using both their publicly available code and our re-implementation. We also included the BILINEAR-DIAG baseline.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_15",
  "x": "The L 2 penalty \u03bb was set to 0.1 for all models, and the entity vectors x e were normalized to unit vectors. For each positive example we sample 500 negative examples. For our implementation of<cite> (Guu et al., 2015)</cite> , we run 5 random walks of each length starting from each node and we found that adding a weight \u03b2 to the multi-step path triples improves the results.",
  "y": "extends"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_16",
  "x": "**NCI-PID RESULTS** Comparison among the baselines also offers valuable insights. The implementation of<cite> Guu et al. (2015)</cite> with default parameters performed significantly worse than our re-implementation.",
  "y": "differences"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_17",
  "x": "11 Model MAP HITS@10 BILINEAR-DIAG<cite> (Guu et al., 2015)</cite> N/A 12.9 BILINEAR-DIAG 8.0 12.2 +Guu et al. (2015) N/A 14.4 PRUNED-PATHS l = 3 c=10 9.5 14.8 PRUNED-PATHS l = 3 c=1 9.5 14.9 PRUNED-PATHS l = 5 c=10 8.9 14.4 ALL-PATHS l = 3 9.4 14.7 ALL-PATHS+NODES l=3 9.4 15.2 ALL-PATHS l = 5 9.6 16.6 ALL-PATHS+NODES l=5 9.8 16.7 Table 2 : KB completion results on the WordNet test set: comparison of our compositional learning approach (ALL-PATHS) with baseline systems. The maximum length of paths is denoted by l. Sampled paths occurring less than c times were pruned in PRUNED-PATHS.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_18",
  "x": "The MAP results were not reported in<cite> Guu et al. (2015)</cite> ; hence the NA value for MAP in row one. 12 On this dataset, our implementation of the baseline model does not have substantially different results than<cite> Guu et al. (2015)</cite> and we use their reported results for the baseline and compositionally trained model. Compositional training improved performance in Hits@10 from 12.9 to 14.4 in<cite> Guu et al. (2015)</cite> , and we find that using PRUNED-PATHS as features gives similar, but a bit higher performance gains.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_19",
  "x": "The MAP results were not reported in<cite> Guu et al. (2015)</cite> ; hence the NA value for MAP in row one. 12 On this dataset, our implementation of the baseline model does not have substantially different results than<cite> Guu et al. (2015)</cite> and we use their reported results for the baseline and compositionally trained model. Compositional training improved performance in Hits@10 from 12.9 to 14.4 in<cite> Guu et al. (2015)</cite> , and we find that using PRUNED-PATHS as features gives similar, but a bit higher performance gains.",
  "y": "uses"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_20",
  "x": "12 On this dataset, our implementation of the baseline model does not have substantially different results than<cite> Guu et al. (2015)</cite> and we use their reported results for the baseline and compositionally trained model. Compositional training improved performance in Hits@10 from 12.9 to 14.4 in<cite> Guu et al. (2015)</cite> , and we find that using PRUNED-PATHS as features gives similar, but a bit higher performance gains. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "10b9ec42ac06344cd575a66161ad91_21",
  "x": "This performance degradation could be avoided with 12 We ran the trained model distributed by<cite> Guu et al. (2015)</cite> and obtained a much lower Hits@10 value of 6.4 and MAP of of 3.5. Due to the discrepancy, we report the original results from the authors' paper which lack MAP values instead. a staged training regiment where models with shorter paths are first trained and used to initialize models using longer paths.",
  "y": "uses"
 },
 {
  "id": "117d30ddacd28478c6cce1e6d39c12_0",
  "x": "Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, <cite>8,</cite> 9 ]. The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora.",
  "y": "motivation"
 },
 {
  "id": "117d30ddacd28478c6cce1e6d39c12_1",
  "x": "How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, <cite>8</cite>, 9] . The word2vec [10] is among the most widely used word embedding models today.",
  "y": "motivation"
 },
 {
  "id": "117d30ddacd28478c6cce1e6d39c12_2",
  "x": "The 2016 Conference on Computational Linguistics and Speech Processing ROCLING 2016, pp. 100-102 \uf0d3 The Association for Computational Linguistics and Chinese Language Processing 100 than window contexts in word2vec. Bansal et al. [<cite>8</cite>] and Melamud et al. [11] show the benefits of such modified-context embeddings in dependency parsing task.",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_0",
  "x": "Systems that are trained on clean data generally perform poorly when faced with such errors at test time (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) . One potential solution is to introduce noise at training time, an approach that is similar in spirit to the use of adversarial examples in other areas of machine learning (Goodfellow et al., 2014) and natural language processing (Ebrahimi et al., 2018) . So far, using synthetic noise at training time has been found to only improve performance on test data with exactly the same kind of synthetic noise, while at the same time impairing performance on clean test data (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) .",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_1",
  "x": "Systems that are trained on clean data generally perform poorly when faced with such errors at test time (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) . One potential solution is to introduce noise at training time, an approach that is similar in spirit to the use of adversarial examples in other areas of machine learning (Goodfellow et al., 2014) and natural language processing (Ebrahimi et al., 2018) . So far, using synthetic noise at training time has been found to only improve performance on test data with exactly the same kind of synthetic noise, while at the same time impairing performance on clean test data (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) . We desire methods that yield good performance on both clean text as well as naturally-occurring noise, but this is beyond the reach of current techniques.",
  "y": "motivation background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_2",
  "x": "When these n-grams are disrupted by orthographical noise, the resulting encoding may be radically different from the encoding of a \"clean\" version of the same text. <cite>Belinkov and Bisk (2018)</cite> report significant degradations in performance after applying noise to only a small fraction of input tokens. 1 Table 1 describes the four types of synthetic orthographic noise we used during training.",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_3",
  "x": "1 Table 1 describes the four types of synthetic orthographic noise we used during training. Substitutions and swaps were experimented with extensively in previous work (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) , but deletion and insertion were not. Deletion and insertion pose a different challenge to character encoders, since they alter the distances between character sequences in the word, as well as its overall length.",
  "y": "background motivation"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_4",
  "x": "We used beam search for generating the translations (5 beams), and computed BLEU scores to measure performance on the test 3 https://github.com/pytorch/fairseq set. Table 2 shows the performance of the model on data with varying amounts of natural orthographical errors (see Section 2.2). As observed in prior art (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) , when there are significant amounts of natural noise, the model's performance drops significantly.",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_5",
  "x": "Table 2 shows the performance of the model on data with varying amounts of natural orthographical errors (see Section 2.2). As observed in prior art (Heigold et al., 2017; <cite>Belinkov and Bisk, 2018</cite>) , when there are significant amounts of natural noise, the model's performance drops significantly. However, training on our synthetic noise cocktail greatly improves performance, regaining between 20% (Czech) and 50% (German) of the BLEU score that was lost to natural noise.",
  "y": "differences"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_6",
  "x": "We conjecture that the importance of deletion and insertion is that they distort the typical distances between characters, requiring the CNN character encoder to become more invariant to unexpected character movements. The fact that we use deletion and insertion also explains why our model was able to regain a significant portion of its original performance when confronted with natural noise at test time, while <cite>previous work</cite> that trained only on substitutions and swaps was not able to do so <cite>(Belinkov and Bisk, 2018)</cite> . ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_7",
  "x": "Similarly, the impact of speech-like noise is explored by Sperber et al. (2017) . Most relevant for us is the work of <cite>Belinkov and Bisk (2018)</cite> , who evaluated on natural noise obtained from Wikipedia edit histories (e.g., Max and Wisniewski, 2010) . <cite>They</cite> find that robustness to natural noise can be obtained by training on the same noise model, but that (a) training on synthetic noise does not yield robustness to natural noise at test time, and (b) training on natural noise significantly impairs performance on clean text.",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_8",
  "x": "Most relevant for us is the work of <cite>Belinkov and Bisk (2018)</cite> , who evaluated on natural noise obtained from Wikipedia edit histories (e.g., Max and Wisniewski, 2010) . <cite>They</cite> find that robustness to natural noise can be obtained by training on the same noise model, but that (a) training on synthetic noise does not yield robustness to natural noise at test time, and (b) training on natural noise significantly impairs performance on clean text. In contrast, we show that training on the right kind and the right amount of synthetic noise can yield substantial improvements on natural noise at test time, without significantly impairing performance on clean data. Our ablation results suggest that deletion and insertion noise -which were not included by <cite>Belinkov and Bisk</cite> -are essential to achieving robustness to natural noise.",
  "y": "extends differences"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_9",
  "x": "An alternative approach is to build robustness to character permutations directly into the design of the character-level encoder. <cite>Belinkov and Bisk (2018)</cite> experiment with a bag of characters, while Sakaguchi et al. (2017) use character RNNs combined with special representations for the first and last characters of each token. <cite>These models</cite> are particularly suited for specific types of swapping and scrambling noises, but are not robust to natural noise.",
  "y": "background"
 },
 {
  "id": "13091dd4d06e11957a5cb7785b92d4_10",
  "x": "An alternative approach is to build robustness to character permutations directly into the design of the character-level encoder. <cite>Belinkov and Bisk (2018)</cite> experiment with a bag of characters, while Sakaguchi et al. (2017) use character RNNs combined with special representations for the first and last characters of each token. <cite>These models</cite> are particularly suited for specific types of swapping and scrambling noises, but are not robust to natural noise.",
  "y": "motivation background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_0",
  "x": "If we compose the meaning representations for red and herring, we might expect to get a very different representation from the one which could be directly inferred from corpus observations of the phrase red herring. Thus any judgements of the similarity of two composed phrases may be confounded by the degree to which those phrases are compositional. In this paper, we use a compound noun compositionality dataset (<cite>Reddy et al., 2011</cite>) to investigate the extent to which the underlying definition of context has an effect on a model's ability to support composition.",
  "y": "uses"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_1",
  "x": "**COMPOSITIONALITY OF COMPOUND NOUNS** Compositionality detection (<cite>Reddy et al., 2011</cite>) involves deciding whether a given multiword expression is compositional or not i.e., whether the meaning can be understood from the literal meaning of its parts. <cite>Reddy et al. (2011)</cite> introduced a dataset consisting of 90 compound nouns along with human judgments of their literality or compositionally at both the constituent and the phrase level.",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_2",
  "x": "Accordingly, as observed elsewhere (<cite>Reddy et al., 2011</cite>; Salehi et al., 2015; Yazdani et al., 2015) , compositional methods can be evaluated by correlating the similarity of composed and observed phrase representations with the human judgments of compositionality. A similar idea is also explored by Kiela and Clark (2013) who detect noncompositional phrases by comparing the neighbourhoods of phrases where individual words have been substituted for similar words. <cite>Reddy et al. (2011)</cite> carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100).",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_3",
  "x": "<cite>Reddy et al. (2011)</cite> carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100). Used 3-fold cross-validation, <cite>they</cite> found that using weighted addition outperformed multiplication as a compositionality function. With <cite>their</cite> optimal settings, <cite>they</cite> achieved a Spearman's rank correlation coefficient of 0.714 with the human judgments, which remains the state-of-the-art on this dataset 1 .",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_4",
  "x": "For consistency with the experiments of <cite>Reddy et al. (2011)</cite> , the corpus used in this experiment is the same fullyannotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008) . This corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependency-parsed with the Malt Parser (Nivre, 2004) . It contains about 1.9 billion tokens.",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_5",
  "x": "For consistency with the experiments of <cite>Reddy et al. (2011)</cite> , the corpus used in this experiment is the same fullyannotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008) . This corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependency-parsed with the Malt Parser (Nivre, 2004) . In order to create a corpus which contains compound nouns, we further preprocessed the corpus by identifying occurrences of the 90 target compound nouns and recombining them into a single lexical item.",
  "y": "motivation"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_6",
  "x": "1 Hermann et al. (2012) proposed using generative models for modeling the compositionality of noun-noun compounds. Using interpolation to mitigate the sparse data problem, their model beat the baseline of weighted addition on the <cite>Reddy et al. (2011)</cite> evaluation task when trained on the BNC. However, these results were still significantly lower than those reported by <cite>Reddy et al. (2011)</cite> using the larger ukWaC corpus.",
  "y": "background"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_7",
  "x": "We investigate using INT , which takes the minimum of each of the constituent's feature values and UNI , which performs pointwise addition. Following <cite>Reddy et al. (2011)</cite> , when using the UNI operation, we experiment with weighting the contributions of each constituent to the composed APT representation using the parameter, h. For example, if A 2 is the APT associated with the head of the phrase and A \u03b4 1 is the properly aligned APT associated with the modifier where \u03b4 is the dependency path from the head to the modifier (e.g. NMOD or AMOD), the composition operations can be defined as: (1)",
  "y": "uses"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_8",
  "x": "Using the cbow model with 100 dimensions and a subsampling threshold of t = 10 \u22123 gives a performance of 0.74 which is significantly higher than the previous state-ofthe-art reported in <cite>Reddy et al. (2011)</cite> . Since both of these models are based on untyped cooccurrences, this performance gain can be seen as the result of implicit parameter optimisation. Table 3 : Average \u03c1 using APT representations.",
  "y": "differences"
 },
 {
  "id": "14529822630fb469f5fc8f37aaf473_9",
  "x": "APT representations. We see that the results using standard PPMI (\u03b1 = 1) significantly outperform the result reported in <cite>Reddy et al. (2011)</cite> , which demonstrates the superiority of a typed dependency space over an untyped dependency space. Smoothing the PPMI calculation with a value of \u03b1 = 0.75 generally has a further small positive effect.",
  "y": "differences"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_0",
  "x": "Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. The most successful models include word embeddings like Fast-Text (Bojanowski et al., 2017) , and, more recently, pretrained language models which can be used to produce contextual embeddings or directly fine-tuned for each task. Good examples of the later are character-based models like Flair (Akbik et al., 2018) and masked language models like BERT<cite> (Devlin et al., 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_1",
  "x": "In addition, pre-trained language models for non-English languages are not always available. In that case, only multilingual versions are available, where each language shares the quota of substrings and parameters with the rest of the languages, leading to a decrease in performance<cite> (Devlin et al., 2019)</cite> . The chances for smaller languages, as for instance Basque, seem even direr, as easily available public corpora is very limited, and the quota of substrings depends on corpus size.",
  "y": "background"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_2",
  "x": "Alternatively, multilingual versions have been tested in transfer learning scenarios for other languages, where they have not been compared to monolingual versions<cite> (Devlin et al., 2019)</cite> . The goal of this paper is to compare publicly available models for Basque with analogous models which have been trained with a larger, better quality corpus. This has been possible for the FastText and Flair models.",
  "y": "background"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_3",
  "x": "More recently,<cite> Devlin et al. (2019)</cite> introduced BERT, a model based on the transformer architecture trained as a masked language model, which has obtained very good results on a variety of NLP tasks. The multilingual counterpart of BERT, called mBERT, is a single language model pre-trained from corpora in more than 100 languages. mBERT enables to perform transfer knowledge techniques among languages, so that systems can be trained on datasets in languages different to the one used to fine tune them (Heinzerling and Strube, 2019; Pires et al., 2019) .",
  "y": "motivation"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_4",
  "x": "**CONTEXTUAL STRING EMBEDDINGS: FLAIR** Flair refers to both a deep learning system and to a specific type of character-based contextual word embeddings. Flair (embeddings and system) have been successfully applied to sequence labeling tasks obtaining state-of-the-art results for a number of English Named Entity Recognition (NER) and Part-of-Speech tagging benchmarks (Akbik et al., 2018) , outperforming other well-known approaches such as BERT and ELMO <cite>(Devlin et al., 2019</cite>; Peters et al., 2018) .",
  "y": "background"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_5",
  "x": "**BERT LANGUAGE MODELS** We have trained a BERT<cite> (Devlin et al., 2019)</cite> model for Basque Language using the BMC corpus motivated by the rather low representation this language has in the original multilingual BERT model. In this section we describe the methods used for creating the vocabulary, the model architecture, the pre-training objective and procedure.",
  "y": "uses"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_6",
  "x": "We set the coverage percentage to 99.95. Model Architecture In the same way as the original BERT architecture proposed by<cite> Devlin et al. (2019)</cite> our model is composed by stacked layers of Transformer encoders (Vaswani et al., 2017) . Our approach follows the BERT BASE configuration containing 12 Transformer encoder layers, a hidden size of 768 and 12 self-attention heads for a total of 110M parameters.",
  "y": "extends differences"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_7",
  "x": "An upgraded version of BERT LARGE 7 has proven that WWM has substantial benefits in comparison with previous masking that was done after the sub-word tokenization. Pre-training procedure Similar to<cite> (Devlin et al., 2019)</cite> we use Adam with learning rate of 1e \u2212 4, \u03b2 1 = 0.9, \u03b2 2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10, 0000 steps, and linear decay of the learning rate. The dropout probability is fixed to 0.1 on all the layers.",
  "y": "similarities"
 },
 {
  "id": "14fcaa3645771e9ca183558eb2e9a1_8",
  "x": "We do not use the development set for training. For comparison between BERT models we fine-tune on the training data provided for each of the four tasks with both the official multilingual BERT<cite> (Devlin et al., 2019)</cite> model and with our BERTeus model (trained as described in Section 3.3.). Every reported result for every system is the average of five randomly initialized runs.",
  "y": "similarities uses"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_0",
  "x": "Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) <cite>[27]</cite> , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems. In this work, we propose and evaluate fully self-attentional networks for CTC (SAN-CTC). We are motivated by practicality: selfattention could be used as a drop-in replacement in existing CTClike systems, where only attention has been evaluated in the past [30, 31] ; unlike encoder-decoder systems, SAN-CTC is able to predict tokens in parallel at inference time; an analysis of SAN-CTC is useful for future state-of-the-art ASR systems, which may equip self-attentive encoders with auxiliary CTC losses [17, 20] .",
  "y": "background"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_1",
  "x": "Unlike past works, we do not require convolutional frontends [19] or interleaved recurrences <cite>[27]</cite> to train self-attention for ASR. In Section 2, we motivate the model and relevant design choices (position, downsampling) for ASR. In Section 3, we validate SAN-CTC on the Wall Street Journal and LibriSpeech datasets by outperforming existing CTC models and most encoder-decoder models in character error rates (CERs), with fewer parameters or less training time.",
  "y": "differences"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_2",
  "x": "In this work, we propose and evaluate fully self-attentional networks for CTC (SAN-CTC). Unlike past works, we do not require convolutional frontends [19] or interleaved recurrences <cite>[27]</cite> to train self-attention for ASR.",
  "y": "motivation"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_3",
  "x": "While in theory, a relatively local context could suffices for ASR, this is complicated by alphabets L which violate the conditional independence assumption of CTC (e.g., English characters [36] ). Wide contexts also enable incorporation of noise/speaker contexts, as <cite>[27]</cite> suggest regarding the broad-context attention heads in the first layer of their self-attentional LAS model. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_4",
  "x": "We now replace recurrent and convolutional layers for CTC with self-attention [24] . Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder [22] , previous explorations of self-attention in ASR [19,<cite> 27]</cite> , and defined in Section 2.3. The other stages are downsampling, which reduces input length T via methods like those in Section 2.4; embedding, which learns a dh-dim. embedding that also describes token position (Section 2.5); and projection, where each final representation is mapped framewise to logits over the intermediate alphabet L .",
  "y": "similarities"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_5",
  "x": "We also see inspiration from convolutional blocks: residual connections, layer normalization, and tied dense layers with ReLU for representation learning. In particular, multi-head attention is akin to having a number of infinitely-wide filters whose weights adapt to the content (allowing fewer \"filters\" to suffice). One can also assign interpretations; for example, <cite>[27]</cite> argue their LAS self-attention heads are differentiated phoneme detectors.",
  "y": "background"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_6",
  "x": "Instead, we consider three fixed approaches, from least-to most-preserving of the input data: subsampling, which only takes every k-th frame; pooling, which aggregates every k consecutive frames via a statistic (average, maximum); reshaping, where one concatenates k consecutive frames into one <cite>[27]</cite> . Note that CTC will still require U \u2264 T /k, however. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_7",
  "x": "The latter was found necessary for self-attentional LAS <cite>[27]</cite> , as additive encodings did not give convergence. However, the monotonicity of CTC is a further positional inductive bias, which may enable the success of content-only and additive encodings. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_8",
  "x": "We see that unlike self-attentional LAS <cite>[27]</cite> , SAN-CTC works respectably even with no position en- coding; in fact, the contribution of position is relatively minor (compare with [21] , where location in an encoder-decoder system improved CER by 3% absolute). Lossy downsampling appears to preserve performance in CER while degrading WER (as information about frame transitions is lost). We believe these observations align with the monotonicity and independence assumptions of CTC.",
  "y": "differences"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_9",
  "x": "Inspired by <cite>[27]</cite> , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details. In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike <cite>[27]</cite> we still see mild differentiation of variances. Inspired by [26] , we further plot the attention weights relative to the current time position (here, per head).",
  "y": "similarities"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_10",
  "x": "We believe these observations align with the monotonicity and independence assumptions of CTC. Inspired by <cite>[27]</cite> , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details. In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike <cite>[27]</cite> we still see mild differentiation of variances.",
  "y": "differences"
 },
 {
  "id": "154fd8e6b625eb93da21c09906ee90_11",
  "x": "Inspired by <cite>[27]</cite> , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details. In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike <cite>[27]</cite> we still see mild differentiation of variances. Inspired by [26] , we further plot the attention weights relative to the current time position (here, per head).",
  "y": "similarities differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_0",
  "x": "Although traditional AES methods typically rely on handcrafted features (Larkey, 1998; Foltz et al., 1999; Attali and Burstein, 2006; Dikli, 2006; Wang and Brown, 2008; Chen and He, 2013; Somasundaran et al., 2014; Yannakoudakis et al., 2014; Phandi et al., 2015) , recent results indicate that state-of-the-art deep learning methods reach better performance (Alikaniotis et al., 2016; <cite>Dong and Zhang, 2016</cite>; Taghipour and Ng, 2016; Song et al., 2017; Tay et al., 2018) , perhaps because these methods are able to capture subtle and complex information that is relevant to the task <cite>(Dong and Zhang, 2016)</cite> . In this paper, we propose to combine string kernels (low-level character n-gram features) and word embeddings (high-level semantic features) to obtain state-of-the-art AES results. Since recent methods based on string kernels have demonstrated remarkable performance in various text classification tasks ranging from authorship identification (Popescu and Grozea, 2012) and sentiment analysis (Gim\u00e9nez-P\u00e9rez et al., 2017; to native language identification (Popescu and Ionescu, 2013; Ionescu et al., 2014; Ionescu, 2015; and dialect identification , we believe that string kernels can reach equally good results in AES.",
  "y": "background"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_1",
  "x": "To our knowledge, this is the first successful attempt to combine string kernels and word embeddings. We evaluate our approach on the Automated Student Assessment Prize data set, in both in-domain and cross-domain settings. The empirical results indicate that our approach yields a better performance than state-of-the-art approaches (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_2",
  "x": "Since the official test data of the ASAP competition is not released to the public, we, as well as others before us (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>;  1 https://www.kaggle.com/c/asap-aes/data Tay et al., 2018) , use only the training data in our experiments. Evaluation procedure. As <cite>Dong and Zhang (2016)</cite>, we scaled the essay scores into the range 0-1.",
  "y": "uses similarities"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_3",
  "x": "Evaluation procedure. As <cite>Dong and Zhang (2016)</cite>, we scaled the essay scores into the range 0-1. We closely followed the same settings for data preparation as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) .",
  "y": "uses similarities"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_4",
  "x": "Evaluation procedure. As <cite>Dong and Zhang (2016)</cite>, we scaled the essay scores into the range 0-1. We closely followed the same settings for data preparation as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) .",
  "y": "similarities uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_5",
  "x": "The 5-fold cross-validation procedure is repeated for 10 times and the results were averaged to reduce the accuracy variation introduced by randomly selecting the folds. We note that the standard deviation in all cases in below 0.2%. For the cross-domain experiments, we use the same source\u2192target domain pairs as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) , namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928.",
  "y": "uses similarities"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_6",
  "x": "All essays in the source domain are used as training data. Target domain samples are randomly divided into 5 folds, where one fold is used as test data, and the other 4 folds are collected together to sub-sample target domain train data. The sub-sample sizes are n t = {10, 25, 50, 100}. The sub-sampling is repeated for 5 times as in (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) to reduce bias.",
  "y": "uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_7",
  "x": "We compare our approach with stateof-the-art methods based on handcrafted features (Phandi et al., 2015) , as well as deep features (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . We note that results for the cross-domain setting are reported only in some of these recent works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Implementation choices.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_8",
  "x": "We compare our approach with stateof-the-art methods based on handcrafted features (Phandi et al., 2015) , as well as deep features (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . We note that results for the cross-domain setting are reported only in some of these recent works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Implementation choices.",
  "y": "background"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_9",
  "x": "We used functions from the VLFeat li- Table 2 : In-domain automatic essay scoring results of our approach versus several state-of-the-art methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using 5-fold cross-validation. The best QWK score (among the machine learning systems) for each prompt is highlighted in bold.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_10",
  "x": "In our empirical study, we also include feature ablation results. We report the QWK measure on each prompt as well as the overall average. We first note that the histogram intersection string kernel alone reaches better overall performance (0.780) than all previous works (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_11",
  "x": "Although the BOSWE model can be regarded as a shallow approach, its overall results are comparable to those of deep learning approaches (<cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . When we combine the two models (HISK and BOSWE), we obtain even better results. Indeed, the combination of string kernels and word embeddings attains the best performance on 7 out of 8 prompts.",
  "y": "similarities"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_12",
  "x": "Cross-domain results. The results for the crossdomain automatic essay scoring task are presented in Table 3 . For each and every source\u2192target pair, we report better results than both state-of-theart methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) .",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_13",
  "x": "Our score in this case (0.728) is even higher than both scores of Phandi et al. (2015) and <cite>Dong and Zhang (2016)</cite> when they use n t = 50. Different from the in-domain setting, we note that the combination of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples (n t ) added into the training set is less or equal to 25. Discussion.",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_14",
  "x": "We applied this method in the in-domain setting and we obtained a surprisingly low overall QWK score, around 0.251. We concluded that this simple apSource\u2192Target Method n t = 0 n t = 10 n t = 25 n t = 50 n t = 100 1\u21922 (Phandi et al., 2015) Table 3 : Corss-domain automatic essay scoring results of our approach versus two state-of-the-art methods (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using the same evaluation procedure as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) .",
  "y": "differences"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_15",
  "x": "Results are reported in terms of the quadratic weighted kappa (QWK) measure, using the same evaluation procedure as (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>) . The best QWK scores for each source\u2192target domain pair are highlighted in bold. proach is not useful, and decided to use BOSWE instead.",
  "y": "similarities uses"
 },
 {
  "id": "15dd59368074f3473b57d86568807f_16",
  "x": "We compared our approach on the Automated Student Assessment Prize data set, in both in-domain and crossdomain settings, with several state-of-the-art approaches (Phandi et al., 2015; <cite>Dong and Zhang, 2016</cite>; Tay et al., 2018) . Overall, the in-domain and the cross-domain comparative studies indicate that string kernels, both alone and in combination with word embeddings, attain the best performance on the automatic essay scoring task. Using a shallow approach, we report better results compared to recent deep learning approaches <cite>(Dong and Zhang, 2016</cite>; Tay et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_0",
  "x": "Kazemzadeh et al [19] introduced the first large-scale dataset of referring expressions for objects in real-world natural images, collected in a two-player game. This dataset was originally collected on top of the 20,000 image ImageCleft dataset, but has recently been extended to images from the MSCOCO collection. We make use of the RefCOCO and RefCOCO+ datasets in our work along with another recently collected referring expression dataset, released by Google, denoted in our paper as RefCOCOg <cite>[26]</cite> .",
  "y": "uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_1",
  "x": "This dataset was originally collected on top of the 20,000 image ImageCleft dataset, but has recently been extended to images from the MSCOCO collection. We make use of the RefCOCO and RefCOCO+ datasets in our work along with another recently collected referring expression dataset, released by Google, denoted in our paper as RefCOCOg <cite>[26]</cite> . The most relevant work to ours is Mao et al <cite>[26]</cite> which introduced the first deep learning approach to REG.",
  "y": "background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_2",
  "x": "Although our task is somewhat different, we borrow machinery from state of the art caption generation [3, 39, 27, 5, 18, 21, 41] using LSTM to generate captions based on CNN features computed on an input image. Three recent approaches for referring expression generation <cite>[26]</cite> and comprehension [14, 33] also take a deep learning approach. However, we add visual object comparisons and tie together language generation for multiple objects.",
  "y": "differences"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_3",
  "x": "Recently, a large-scale referring expression dataset was collected by Kazemzadeh et al [19] featuring natural objects in the real world. Since then, another three REG datasets based on the object labels in MSCOCO have been collected [19,<cite> 26]</cite> . The availability of large-scale referring expression datasets allows us to train deep learning models.",
  "y": "background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_4",
  "x": "The first set of models are recent state of the art deep learning approaches from Mao et al <cite>[26]</cite> . We use these as our baselines (Sec 3.1). Next, we investigate incorporating better visual context features into the models (Sec 3.2).",
  "y": "uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_5",
  "x": "**BASELINES** For comparison, we implement both the baseline and strong model of Mao et al <cite>[26]</cite> . Both models utilize a pre-trained CNN network to model the target object and its context within the image, and then use a LSTM for generation.",
  "y": "uses motivation"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_6",
  "x": "Location and size are modeled as a 5-d vector encoding the x and y locations of the top left and bottom right corners of the target object bounding box, as well as the bounding box size with respect to the image, i.e., l i = [ Language generation is handled by a long short-term memory network (LSTM) [10] where inputs are the above visual features and the network is trained to generate natural language referring expressions. In Mao et al's baseline <cite>[26]</cite> , the model uses maximum likelihood training and outputs the most likely referring expression given the target object, context, and location/size features.",
  "y": "background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_7",
  "x": "**JOINT LANGUAGE GENERATION** For the referring expression generation task, rather than generating sentences for each object in an image separately [15] <cite>[26]</cite>, we consider tying the generation process together into a single task to jointly generate expressions for all objects of the same object category depicted in an image. This makes sense intuitively -when a person attempts to generate a referring expression for an object in an image they inherently compose that expression while keeping in mind expressions for the other objects in the picture.",
  "y": "differences"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_8",
  "x": "**DATA** We make use of 3 referring expression datasets in our work, all collected on top of the Microsoft COCO image collection [24] . One dataset, RefCOCOg <cite>[26]</cite> is collected in a non-interactive setting, while the other two datasets, RefCOCO and RefCOCO+, are collected interactively in a two-player game [19] .",
  "y": "uses background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_9",
  "x": "This means that each object will only appear either in training or testing set, but that one object from an image may appear in the training set while another object from the same image may appear in the test set. We use this split for RefCOCOg since same division was used in the previous state-of-the-art approach <cite>[26]</cite> . The second type is people-vs-objects splits.",
  "y": "similarities uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_10",
  "x": "We first perform some experiments to analyze the use of context in referring expressions (Sec 5.1). Given these findings, we then perform experiments evaluating the usefulness of our proposed visual and language innovations on the comprehension (Sec 5.2) and generation tasks (Sec 5.3). In experiments for the referring expression comprehension task, we use the same evaluation as Mao et al <cite>[26]</cite> , namely we first predict the region referred by the given expression, then we compute the intersection over union (IOU) ratio between the true and predicted bounding box.",
  "y": "similarities uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_12",
  "x": "---------------------------------- **ANALYSIS EXPERIMENTS** Context Representation As previously discussed, we suggest that the approaches proposed in recent referring expression works<cite> [26,</cite> 14] make use of relatively weak contextual information, by only considering a single global image context for all objects.",
  "y": "background"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_13",
  "x": "Context Representation As previously discussed, we suggest that the approaches proposed in recent referring expression works<cite> [26,</cite> 14] make use of relatively weak contextual information, by only considering a single global image context for all objects. To verify this intuition, we implemented both the baseline and strong MMI models from Mao et al <cite>[26]</cite> , and compare the results for referring expression comprehension task with and without global context on RefCOCO and Refcoco+ in Table 1 . Surprisingly we find that the global context does not improve the performance of the model.",
  "y": "uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_14",
  "x": "For RefCOCO and RefCOCO+, we evaluate on the two subsets of people (testA) and all other objects (testB). For RefCOCOg, we evaluate on the per-object split as previous work <cite>[26]</cite> . Since the authors haven't released their testing set, we show the performance on their validation set only, using the optimized hyper-parameters on RefCOCO.",
  "y": "similarities uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_15",
  "x": "We observe that our implementation of Mao et al <cite>[26]</cite> achieves comparable performance to the numbers reported in their paper. We also find that adding visual comparison features to the Baseline model improves performance across all datasets and splits. Similar improvements are also observed on top of the MMI model.",
  "y": "similarities uses"
 },
 {
  "id": "163770df02c1110edc60e7cac90ad2_16",
  "x": "In order to make a fully automatic referring system, we also train a Fast-RCNN [9] detector and build our system on top of the detections. We train Fast-RCNN on the validation portion only as the RefCOCO and RefCOCO+ are collected using MSCOCO training data. For RefCOCOg, we use the detection results provided by <cite>[26]</cite> , which were trained uisng Multibox [4] .",
  "y": "uses"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_0",
  "x": "Previous works<cite> (Rubin et al., 2016</cite>; Rashkin et al., 2017) rely on various linguistic and handcrafted semantic features for differentiating between news articles. However, none of them try to model the interaction of sentences within the document. We observed a pattern in the way sentences cluster in different kind of news articles.",
  "y": "background"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_1",
  "x": "We present a series of experiments on News Corpus with Varying Reliability dataset (Rashkin et al., 2017) and Satirical Legitimate News dataset<cite> (Rubin et al., 2016)</cite> . Our results demonstrate that the proposed model achieves state-of-the-art performance on these datasets and provides interesting insights. Experiments performed in out-of-domain settings establish the generalizability of our proposed method.",
  "y": "uses"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_2",
  "x": "Satire, according to Simpson (2003) , is complicated because it occupies more than one place in the framework for humor, proposed by Ziv (1988) : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well. <cite>Rubin et al. (2016)</cite> defines news satire as a genre of satire that mimics the format and style of journalistic reporting. Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources<cite> (Rubin et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_3",
  "x": "Satire, according to Simpson (2003) , is complicated because it occupies more than one place in the framework for humor, proposed by Ziv (1988) : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well. <cite>Rubin et al. (2016)</cite> defines news satire as a genre of satire that mimics the format and style of journalistic reporting. Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources<cite> (Rubin et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_4",
  "x": "<cite>Rubin et al. (2016)</cite> 's work by offering a quantitative study of linguistic differences found in articles of different types of fake news such as hoax, propaganda and satire. They also proposed predictive models for graded deception across multiple domains. We show that our proposed neural network based on graph convolutional layers can outperform this model.",
  "y": "differences"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_5",
  "x": "We use SLN: Satirical and Legitimate News Database<cite> (Rubin et al., 2016)</cite> , RPN: Random Political News Dataset (Horne and Adali, 2017) and LUN: Labeled Unreliable News Dataset Rashkin et al. (2017) for our experiments. Table 1 shows the statistics. Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines,",
  "y": "uses"
 },
 {
  "id": "18ef4e4fafdf62839d6797d62eb76b_6",
  "x": "similarity model does not seem to have much impact on the GCN model, and considering the computing cost, we don't experiment with it for the 4-way classification scenario. Given that we use SLN as an out of domain test set (just one overlapping source, no overlap in articles), whereas the SoTA paper<cite> (Rubin et al., 2016)</cite> reports a 10fold cross validation number on SLN. We believe that our results are quite strong, the GAT + 2 Attn Heads model achieves an accuracy of 87% on the entire RPN dataset when used as an out-of-domain test set.",
  "y": "differences"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_0",
  "x": "Liu et al focus their review on the fundamental principle of dependency length minimization but understanding how it interacts with other principles is vital. In 2009, we put forward another fundamental word order principle, i.e. predictability maximization, and presented a theoretical framework culminating in a conflict between dependency length minimization and predictability maximization [19] . For sociological reasons, these arguments started appearing in print many years later [20, 5, <cite>21]</cite> .",
  "y": "background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_1",
  "x": "In 2009, we put forward another fundamental word order principle, i.e. predictability maximization, and presented a theoretical framework culminating in a conflict between dependency length minimization and predictability maximization [19] . For sociological reasons, these arguments started appearing in print many years later [20, 5, <cite>21]</cite> . For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise)<cite> [21,</cite> 20] .",
  "y": "background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_2",
  "x": "Lengths in phonemes or syllables shed light on why SVO languages show SOV order when the object is a short word such as a clitic [24] . Without addressing these issues, the anti-locality effects or long-distance dependencies reviewed by Liu et al can neither be attributed to predictability maximization nor be interpreted as a violation of dependency length minimization safely; an effective evaluation of the theoretical framework above can be impossible (as that framework makes theoretical predictions based on the calculation of full length costs). The real challenge for psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [20, <cite>21]</cite> : one principle beating the other, coexistence, collaboration between principles or the very same trade-off causing the delusion that word order constraints have relaxed dramatically or even disappeared.",
  "y": "uses background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_3",
  "x": "For sociological reasons, these arguments started appearing in print many years later [20, 5, <cite>21]</cite> . For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise)<cite> [21,</cite> 20] . Liu et al review two major sources of evidence of dependency length minimization: the analysis of dependency treebanks and psychological experiments.",
  "y": "background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_4",
  "x": "For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise)<cite> [21,</cite> 20] . Liu et al review two major sources of evidence of dependency length minimization: the analysis of dependency treebanks and psychological experiments. A critical difference between them is that the former is based on the calculation of the total cost of the sentence (as a sum or mean of all the dependency lengths of the sentence) while the latter is based on a partial calculation and thus it can be misleading.",
  "y": "background"
 },
 {
  "id": "19a62878f72c84d1c5c83a9a8cdeff_5",
  "x": "Without addressing these issues, the anti-locality effects or long-distance dependencies reviewed by Liu et al can neither be attributed to predictability maximization nor be interpreted as a violation of dependency length minimization safely; an effective evaluation of the theoretical framework above can be impossible (as that framework makes theoretical predictions based on the calculation of full length costs). The real challenge for psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [20, <cite>21]</cite> : one principle beating the other, coexistence, collaboration between principles or the very same trade-off causing the delusion that word order constraints have relaxed dramatically or even disappeared. This is the way of physics.",
  "y": "uses background"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_0",
  "x": "**INTRODUCTION** There is a deep tension in statistical modeling of grammatical structure between providing good expressivity -to allow accurate modeling of the data with sparse grammars -and low complexitymaking induction of the grammars and parsing of novel sentences computationally practical. Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the problem of segmenting training data trees into elementary parse tree fragments to form the grammar (Cohn et al., 2009;<cite> Cohn and Blunsom, 2010</cite>; Post and Gildea, 2009) .",
  "y": "background"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_1",
  "x": "In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = N P ), each of which uses a base distribution P 0 that generates an initial tree by making stepwise decisions. The canonical P 0 uses a probabilistic CFGP that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitutions should occur (Cohn et al., 2009;<cite> Cohn and Blunsom, 2010)</cite> . We extend this model by adding specialized DPs for left and right auxiliary trees.",
  "y": "background"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_2",
  "x": "The canonical P 0 uses a probabilistic CFGP that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitutions should occur (Cohn et al., 2009;<cite> Cohn and Blunsom, 2010)</cite> . We extend this model by adding specialized DPs for left and right auxiliary trees. 3",
  "y": "extends"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_3",
  "x": "Since TIG derivations are highly structured objects, a basic sampling strategy based on local node-level moves such as Gibbs sampling (Geman and Geman, 1984) would not hold much promise. Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion<cite> (Cohn and Blunsom, 2010</cite>; Shindo et al., 2011) . This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (Johnson et al., 2007) .",
  "y": "uses"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_4",
  "x": "Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a CFG rule CFG probability proposal distribution. Fortunately, Schabes and Waters (1995) provide an (exact) transformation from a fully general TIG into a TSG that generates the same string languages. It is then straightforward to represent this TSG as a CFG using the Goodman transform (Goodman, 2002;<cite> Cohn and Blunsom, 2010)</cite> .",
  "y": "uses"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_5",
  "x": "Parsing results are based on the maximum probability parse which was obtained by sampling derivations under the transform CFG. We compare our system (referred to as TIG) to our implementation of the TSG system of<cite> (Cohn and Blunsom, 2010</cite> ) (referred to as TSG) and the constrained TIG variant of (Shindo et al., 2011 ) (referred to as TIG 0 ). The upshot of our experiments is that, while on the large training set all models have similar performance (85.6, 85.3, 85 .4 for TSG, TIG 0 and TIG respectively), on the small dataset insertion helps nonparametric model to find more compact and generalizable representations for the data, which affects parsing performance (Figure 4 ).",
  "y": "uses"
 },
 {
  "id": "1a17ae4e5c8ea9e605f129aa96a6ee_6",
  "x": "Parsing results are based on the maximum probability parse which was obtained by sampling derivations under the transform CFG. We compare our system (referred to as TIG) to our implementation of the TSG system of<cite> (Cohn and Blunsom, 2010</cite> ) (referred to as TSG) and the constrained TIG variant of (Shindo et al., 2011 ) (referred to as TIG 0 ). The upshot of our experiments is that, while on the large training set all models have similar performance (85.6, 85.3, 85 .4 for TSG, TIG 0 and TIG respectively), on the small dataset insertion helps nonparametric model to find more compact and generalizable representations for the data, which affects parsing performance (Figure 4 ).",
  "y": "uses similarities"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_0",
  "x": "The first is the advent of deep learning techniques (Goodfellow et al., 2016) , which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data. The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries<cite> (Hill et al., 2015</cite>; Hermann et al., 2015) , which permit fast integration loops between model conception and experimental evaluation. Cloze-style queries (Taylor, 1953) are created by deleting a particular word in a natural-language statement.",
  "y": "background"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_1",
  "x": "The task is to guess which word was deleted. In a pragmatic approach, recent work<cite> (Hill et al., 2015)</cite> formed such questions by extracting a sentence from a larger document. In contrast to considering a stand-alone statement, the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word.",
  "y": "background"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_2",
  "x": "This paper makes the following contributions. We present a novel iterative, alternating attention mechanism that, unlike existing models<cite> (Hill et al., 2015</cite>; Kadlec et al., 2016) , does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time. Our architecture tightly integrates previous ideas related to bidirectional readers (Kadlec et al., 2016) and iterative attention processes<cite> (Hill et al., 2015</cite>; Sukhbaatar et al., 2015) .",
  "y": "differences"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_4",
  "x": "One of the advantages of using Cloze-style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention. The CBT<cite> (Hill et al., 2015)</cite> and CNN (Hermann et al., 2015) corpora are two such datasets. The CBT 1 corpus was generated from well-known children's books available through Project Gutenberg.",
  "y": "background"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_5",
  "x": "The subsets are named entity, common noun, verb, and preposition. We will focus our evaluation solely on the first two subsets, i.e. CBT-NE (named entity) and CBT-CN (common nouns), since the latter two are relatively simple as demonstrated by<cite> (Hill et al., 2015)</cite> . The CNN 2 corpus was generated from news articles available through the CNN website.",
  "y": "motivation"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_6",
  "x": "The attention we use here is similar to the formulation used in<cite> (Hill et al., 2015</cite>; Sukhbaatar et al., 2015) , but with two differences. First, we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step. This simple bilinear attention has been successfully used in (Luong et al., 2015) . Second, we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t\u22121 .",
  "y": "similarities differences"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_8",
  "x": "Table 2 reports our results on the CBT-CN and CBT-NE dataset. The Humans, LSTMs and Memory Networks (MemNNs) results are taken from<cite> (Hill et al., 2015)</cite> and the Attention-Sum Reader (AS Reader) is a state-of-the-art result recently obtained by (Kadlec et al., 2016) . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_10",
  "x": "In general, attention models keep a memory of states that can be accessed at will by learned attention policies. In our case, the memory is represented by the set of document and query contextual encodings. Our model is closely related to (Sukhbaatar et al., 2015; Kumar et al., 2015; Hermann et al., 2015; Kadlec et al., 2016;<cite> Hill et al., 2015)</cite> , which were also applied to question answering.",
  "y": "similarities"
 },
 {
  "id": "1cd671c60486a137377096cae435ec_11",
  "x": "The inference network is responsible for making sense of the current attention step with respect to what has been gathered before. In addition to achieving state-ofthe-art performance, this technique may also prove to be more scalable than alternative query attention models. Finally, our iterative inference process shares similarities to the iterative hops in Memory Networks (Sukhbaatar et al., 2015;<cite> Hill et al., 2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "1f463f2f87bc2d572299d96481084f_0",
  "x": "Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004;<cite> Koehn, 2004)</cite> , to assess for a pair of systems how likely a difference in BLEU scores occurred by chance. Empirical tests detailed in<cite> Koehn (2004)</cite> show that even for test sets as small as 300 translations, BLEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large. Approximate randomization was subsequently proposed as an alternate to bootstrap resampling (Riezler and Maxwell, 2005) .",
  "y": "background"
 },
 {
  "id": "1f463f2f87bc2d572299d96481084f_1",
  "x": "Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004;<cite> Koehn, 2004)</cite> , to assess for a pair of systems how likely a difference in BLEU scores occurred by chance. Empirical tests detailed in<cite> Koehn (2004)</cite> show that even for test sets as small as 300 translations, BLEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large. Approximate randomization was subsequently proposed as an alternate to bootstrap resampling (Riezler and Maxwell, 2005) .",
  "y": "background"
 },
 {
  "id": "1f463f2f87bc2d572299d96481084f_2",
  "x": "We also note that the pseudo-code contains an unconventional computation of mean pseudo-statistics, \u03c4 B , for shiftto-zero. Rather than speculate over whether these issues with the original paper were simply presentational glitches or the actual basis of the experiments reported on in the paper, we present a normalized version of the two-sided bootstrap algorithm in Figure 1 , and report on the results of our own experiments in Section 4. We compare this method with approximate randomization and also paired bootstrap resampling<cite> (Koehn, 2004)</cite> , which is widely used in MT evaluation.",
  "y": "differences motivation"
 },
 {
  "id": "1f463f2f87bc2d572299d96481084f_3",
  "x": "Bootstrap resampling provides a way of estimating the population distribution by sampling with replacement from a representative sample (Efron and Tibshirani, 1993) . The test statistic is taken as the difference in scores of the two systems, S X \u2212 S Y , which has an expected value of 0 under the null hypothesis that the two systems perform equally well. A bootstrap pseudo-sample consists of the translations by the two systems (X b , Y b ) of a bootstrapped test set<cite> (Koehn, 2004)</cite> , constructed by sampling with replacement from the original test set translations.",
  "y": "uses background"
 },
 {
  "id": "1f72d18331beaef7adf4a78d1619c6_0",
  "x": "We introduce QVEC-CCA-an intrinsic evaluation measure of the quality of word embeddings. Our method is a modification of QVEC-an evaluation based on alignment of embeddings to a matrix of features extracted from a linguistic resource <cite>(Tsvetkov et al., 2015)</cite> . We review QVEC, and then describe QVEC-CCA.",
  "y": "extends"
 },
 {
  "id": "1f72d18331beaef7adf4a78d1619c6_1",
  "x": "---------------------------------- **SEMANTIC VECTORS.** To evaluate the semantic content of word vectors,<cite> Tsvetkov et al. (2015)</cite> exploit supersense annotations in a WordNetannotated corpus-SemCor (Miller et al., 1993 Table 2 : Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB.",
  "y": "background"
 },
 {
  "id": "1f72d18331beaef7adf4a78d1619c6_2",
  "x": "\u2022 Finally, we compute the Pearson's correlation coefficient r to quantify the linear relationship between the intrinsic and extrinsic scorings. The higher the correlation, the better suited the intrinsic evaluation to be used as a proxy to the extrinsic task. We extend the setup of<cite> Tsvetkov et al. (2015)</cite> with two syntactic benchmarks, and evaluate QVEC-CCA with the syntactic matrix.",
  "y": "extends"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_0",
  "x": "The advantage of such an effort is that the resulting tool provides non-intrusive and cost-effective means to detect and warn at-risk individuals early, before they visit a doctor's office, and possibly influence their decision to visit a doctor. Previous work has demonstrated that intervention by social media has modest but significant success in decreasing obesity (Ashrafian et al., 2014) . Furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible<cite> (Fried et al., 2014</cite>; Culotta, 2014) .",
  "y": "background"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_1",
  "x": "Furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible<cite> (Fried et al., 2014</cite>; Culotta, 2014) . However, in all cases, classification is made on aggregated data from cities, counties, or states, so these models are not immediately applicable to the task of classifying individuals. Our work takes the first steps towards transferring a classification model that identifies communities that are more overweight than average to classifying overweight (and thus at-risk for T2DM) individuals.",
  "y": "extends"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_2",
  "x": "The contributions of our work are: 1. We introduce a random-forest (RF) model that classifies US states as more or less overweight than average using only 7 decision trees with a maximum depth of 3. Despite the model's simplicity, it outperforms<cite> Fried et al. (2014)</cite> 's best model by 2% accuracy.",
  "y": "differences"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_3",
  "x": "In the last couple of years, several variants of this problem have been considered<cite> (Fried et al., 2014</cite>; Abbar et al., 2015; Culotta, 2014; Ardehaly and Culotta, 2015) . food-related tweets and use it to predict several population characteristics, namely diabetes rate, overweight rate and political tendency. Generally, they use state-level populations, e.g., one of their classification tasks is to label whether a state is more overweight than the national median.",
  "y": "background"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_4",
  "x": "The classification task is to label whether a state is more overweight than the national median. Individuals' tweets are localized at state level as a single instance to train several classifier models, and the performance of models is evaluated using leave-one-out cross-validation. Importantly,<cite> Fried et al. (2014)</cite> train and test their models on communities rather than individuals, which limits the applicability of their approach to individualized public health.",
  "y": "motivation"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_5",
  "x": "Even though performing classification at state or county granularity tends to be robust and accurate<cite> (Fried et al., 2014)</cite> , characteristics that are specific to individuals are more meaningful and practical. A wave of computational work on the automatic identification of latent attributes of individuals has recently emerged. Ardehaly and Culotta (2015) utilize label regularization, a lightly supervised learning method, to infer latent attributes of individuals, such as age and ethnicity.",
  "y": "differences"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_6",
  "x": "Our approach is summarized in Figure 1 . The first step is to develop an interpretable predictive model that identifies communities that are more overweight than average, in a way that can be converted into fun, engaging natural language questions. To this end, we started with the same settings as<cite> Fried et al. (2014)</cite> : we used the 887,310 tweets they collected which were localizable to a specific state and contained at least one relevant hashtag, such as #breakfast or #dinner.",
  "y": "uses"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_7",
  "x": "Each state was assigned a binary label (more or less overweight than the median) by comparing the percentage of overweight adults against the median state. For each state, we extracted features based on unigram (i.e., single) words and hashtags from all the above tweets localized to the corresponding state. To mitigate sparsity, we also included topics generated using Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and all tweets collected by Fried et al. For example, one of the generated topics contains words that approximate the standard American diet (e.g., chicken, potatoes, cheese, baked, beans, fried, mac), which has already been shown to correlate with higher overweight and T2DM rates<cite> (Fried et al., 2014</cite> Figure 2 : A decision tree from the random forest classifier trained using state-level Twitter data.",
  "y": "uses"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_8",
  "x": "Majority baseline 50.89 SVM<cite> (Fried et al., 2014)</cite> 80.39 RF (food + hashtags) 82.35 Discretized RF (food + hashtags) 78.43 Table 2 : Random forest (RF) classifier performance on state-level data relative to majority baseline and<cite> Fried et al. (2014)</cite> 's best classifier. We include two versions of our classifier: the first keeps numeric features (e.g., word counts) as is, whereas the second discretizes numeric features to three bins. of How often do you mention \"fruit\" in your tweets?",
  "y": "uses"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_9",
  "x": "This data (specifically height and weight) is also immediately used to compute the participant's BMI, to verify whether the classifier was correct. identical experimental settings as<cite> (Fried et al., 2014)</cite> , i.e., leave-one-out-cross-validation on the 50 states plus the District of Columbia. The table shows that our best model performs 2% better than the best model of<cite> (Fried et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "1f77b780c98093cd85966243471a1d_10",
  "x": "identical experimental settings as<cite> (Fried et al., 2014)</cite> , i.e., leave-one-out-cross-validation on the 50 states plus the District of Columbia. The table shows that our best model performs 2% better than the best model of<cite> (Fried et al., 2014)</cite> . Our second classifier, which used discretized numeric features and was the source of the quiz, performed 2% worse, but it still had acceptable accuracy, nearing 80%.",
  "y": "differences"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_0",
  "x": "Given the multitude of terms and definitions used in the literature, recent studies have investigated common aspects of the abusive language detection sub-tasks (Waseem et al., 2017; Wiegand et al., 2018) . However, none of these initial studies focused on both the type and the target of the offensive language. Therefore, in conjunction with this task, we present the Offensive Language Identification Dataset (OLID)<cite> (Zampieri et al., 2019)</cite> .",
  "y": "similarities"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_1",
  "x": "Hate speech, for example, is commonly understood as an insult targeted at a group whereas cyberbulling is typically targeted at an individual). In OffensEval 1 we use OLID<cite> (Zampieri et al., 2019)</cite> and propose one sub-task for each layer of annotation as presented in Section 3. The remainder of this paper is organized as follows: Section 3 presents the shared task description and the sub-tasks included in OffensEval and Section 4 includes a brief description of OLID based on <cite>Zampieri et al. (2019)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_2",
  "x": "Hate speech, for example, is commonly understood as an insult targeted at a group whereas cyberbulling is typically targeted at an individual). In OffensEval 1 we use OLID<cite> (Zampieri et al., 2019)</cite> and propose one sub-task for each layer of annotation as presented in Section 3. The remainder of this paper is organized as follows: Section 3 presents the shared task description and the sub-tasks included in OffensEval and Section 4 includes a brief description of OLID based on <cite>Zampieri et al. (2019)</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_3",
  "x": "A dataset of over 8,500 annotated tweets was provided for a course-grained binary classification task in which systems were trained to discriminate between offensive and non-offensive tweets and a second task where the organizers broke down the offensive class into three classes: profanity, insult, and abuse. Toxic comments: The Toxic Comment Classification Challenge was an open competition at Kaggle which provided participants with comments from Wikipedia labeled in six classes: toxic, severe toxic, obscene, threat, insult, identity hate. While each of these sub-tasks tackle a particular type of abuse or offense, they share similar properties and the hierarchical annotation model pro-posed proposed in OLID<cite> (Zampieri et al., 2019)</cite> and used in OffensEval aims to capture this.",
  "y": "similarities uses"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_4",
  "x": "The training and testing material used for OffensEval is the aforementioned Offensive Language Identification Dataset (OLID) dataset, built for this task. OLID was annotated using a hierarchical three-level annotation model introduced in <cite>Zampieri et al. (2019)</cite> . We use the annotation of each of the three layers in OLID to each sub-task in OffensEval as follows:",
  "y": "uses similarities"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_5",
  "x": "A detailed description of the data collection process and annotation is presented in <cite>Zampieri et al. (2019)</cite> . OLID is a large collection of English tweets annotated using a hierarchical three-layer annotation model. It contains 14,100 annotated tweets divided in a training partition containing 13,240 tweets and a test partition containing 860 tweets.",
  "y": "similarities uses"
 },
 {
  "id": "1ffadfc2d4961beeb1621502298a70_6",
  "x": "In this paper, we presented the results of SemEval-2016 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval). In OffensEval we used OLID<cite> (Zampieri et al., 2019)</cite> , a dataset containing English tweets annotated with a hierarchical three-layer annotation model which considers 1) whether a message is offensive or not (sub-task A); 2) what is the type of the offensive 7 In the camera-ready version of this report we will be including a Table with references to all system descriptions papers. message (sub-task B); and 3) what is the target of the offensive (sub-task C).",
  "y": "similarities uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_0",
  "x": "As such, although PSMT has been very successful, it suffers from the lack of a principled mechanism for handling long-distance reordering phenomena due to word order differences between languages. One method for addressing this difficulty is the reordering-as-preprocessing approach, exemplified by <cite>Collins et al. (2005)</cite> and Xia and McCord (2004) , where PSMT is coupled with a preprocessing step that reorders input sentences to more closely parallel the target language word order. Although this leads to improved performance overall, <cite>Collins et al. (2005)</cite> show that the reordering-as-preprocessing system does not consistently provide better translations than the PSMT baseline on a sentence-by-sentence basis.",
  "y": "background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_1",
  "x": "Although this leads to improved performance overall, <cite>Collins et al. (2005)</cite> show that the reordering-as-preprocessing system does not consistently provide better translations than the PSMT baseline on a sentence-by-sentence basis. One possible reason could be errors in the parse or the consequent reordering. Chiang et al. (2009) used features indicating problematic use of syntax to improve performance within hierarchical and syntax-based translation.",
  "y": "background motivation"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_2",
  "x": "We reimplement the <cite>Collins et al. (2005)</cite> reordering preprocessing step and conduct some preliminary experiments in German-toEnglish translation ( \u00a74). Our results ( \u00a75) do not replicate the finding of <cite>Collins et al. (2005)</cite> that the preprocessing step produces better translation results overall. However, results for our dual-path PSMT system do show an improvement, with our plain system achieving a BLEU score (Papineni et al., 2002) of 21.39, an increase of 0.62 over the baseline.",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_3",
  "x": "Our results ( \u00a75) do not replicate the finding of <cite>Collins et al. (2005)</cite> that the preprocessing step produces better translation results overall. However, results for our dual-path PSMT system do show an improvement, with our plain system achieving a BLEU score (Papineni et al., 2002) of 21.39, an increase of 0.62 over the baseline. We therefore conclude that a syntactically-informed reordering preprocessing step is inconsistently of use in PSMT, and that enabling the system to choose when to use the reordering leads to improved translation performance.",
  "y": "differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_4",
  "x": "Our work builds on the reordering-aspreprocessing approach of <cite>Collins et al. (2005)</cite> . Working with German-to-English translation, <cite>Collins et al. (2005)</cite> parse input sentences with a constituent-structure parser and apply six hand-crafted rules to reorder the German text toward English word order. These rules target the placement of non-finite and finite verbs, subjects, particles and negation.",
  "y": "extends"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_5",
  "x": "Our work builds on the reordering-aspreprocessing approach of <cite>Collins et al. (2005)</cite> . Working with German-to-English translation, <cite>Collins et al. (2005)</cite> parse input sentences with a constituent-structure parser and apply six hand-crafted rules to reorder the German text toward English word order. These rules target the placement of non-finite and finite verbs, subjects, particles and negation.",
  "y": "background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_6",
  "x": "Other examples of systems include Wang et al. (2007) (manual rules, Chinese-to-English), Habash (2007) (automatic rules, Arabic-to-English) and Popovi\u0107 and Ney (2006) (manual rules, Spanish/English-toSpanish/English/German). Despite the success of the reordering-aspreprocessing approach overall, <cite>Collins et al. (2005)</cite> found that in a human evaluation on 100 sentences, there were still several cases in which the baseline system translation was preferred over that produced with the reordering. The authors note this finding but do not analyse it further.",
  "y": "background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_7",
  "x": "Zwarts and Dras (2008) explore the <cite>Collins et al. (2005)</cite> finding by examining whether machine learning techniques can be used to predict, on a sentence-by-sentence basis, whether the translation of the reordered sentence is to be preferred over the alternative. For features, they use sentence length, parse probability from the Collins parser and unlinked fragment count from the Link Grammar parser on the English side of the translation. The authors find that, when used on the source side (in English-to-Dutch translation), these features provide no significant improvement in BLEU score, while as target-side features (in Dutch-to-English translation) they improve the BLEU score by 1.7 points over and above the 1.3 point improvement from reordering.",
  "y": "background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_8",
  "x": "Zwarts and Dras (2008) explore the <cite>Collins et al. (2005)</cite> finding by examining whether machine learning techniques can be used to predict, on a sentence-by-sentence basis, whether the translation of the reordered sentence is to be preferred over the alternative. Our work has some similarities to that of Zwarts and Dras (2008) but uses the log-linear model of the translation system itself to include features, rather than a separate classifier that does not permit interaction between the confidence features and features used during translation.",
  "y": "differences background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_9",
  "x": "For reordering, we use the Berkeley parser (Petrov et al., 2006 ) and the rules given by <cite>Collins et al. (2005)</cite> , but any reordering preprocessing step could equally be used. Further details of our systems are given in \u00a74. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_10",
  "x": "5 For the reordering preprocessing step we reimplement the <cite>Collins et al. (2005)</cite> rules and use this to recreate the <cite>Collins et al. (2005)</cite> reordering-aspreprocessing system as our second baseline. We use the Berkeley parser (Petrov et al., 2006) , repository revision 14, 6 to provide the parse trees for the reordering process. Since the German parsing model provided on the parser website does not include the function labels needed by the <cite>Collins et al. (2005)</cite> rules, we trained a new parsing model on the Tiger corpus (version 1).",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_11",
  "x": "5 For the reordering preprocessing step we reimplement the <cite>Collins et al. (2005)</cite> rules and use this to recreate the <cite>Collins et al. (2005)</cite> reordering-aspreprocessing system as our second baseline. We use the Berkeley parser (Petrov et al., 2006) , repository revision 14, 6 to provide the parse trees for the reordering process. Since the German parsing model provided on the parser website does not include the function labels needed by the <cite>Collins et al. (2005)</cite> rules, we trained a new parsing model on the Tiger corpus (version 1).",
  "y": "differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_12",
  "x": "We compare four systems on German-toEnglish translation: the Moses baseline (MOSES), the <cite>Collins et al. (2005)</cite> baseline (REORDER), the lattice system with just the reordering indicator feature (LATTICE), and the lattice system with all 3 It is possible that in practice the imbalance in number of non-zero features between the two paths could cause the system some difficulty in assigning the weights for each feature. In future it would be interesting to investigate this possibility by introducing extra features to balance the two paths. confidence features (+FEATURES).",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_13",
  "x": "Table 3 gives the BLEU score for each of our four systems and the approximate oracle. We note that these numbers are lower than those reported by <cite>Collins et al. (2005)</cite> . However, this is most likely due to differences in the training and testing data; our results are roughly in line with the numbers reported in the Euromatrix project for this test set.",
  "y": "differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_14",
  "x": "However, this is most likely due to differences in the training and testing data; our results are roughly in line with the numbers reported in the Euromatrix project for this test set. 8 Interestingly, our reimplementation of the <cite>Collins et al. (2005)</cite> baseline does not outperform the plain PSMT baseline. Possible explanations include variability due to differences in training data, noisier parser output in our system, or differing interpretation of the description of the reordering rules.",
  "y": "uses"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_15",
  "x": "Possible explanations include variability due to differences in training data, noisier parser output in our system, or differing interpretation of the description of the reordering rules. It may also be that the inconsistency of improvement noted by <cite>Collins et al. (2005)</cite> is the cause; sometimes the reordering produces better results and sometimes the baseline, with the effect just by chance favouring the baseline here. To explore this, we look at the approximate oracle.",
  "y": "uses background"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_16",
  "x": "In our experiment, the oracle preferred the baseline output in 848 cases and the reordered in 1,070 cases. 215 sentences were identical between the two systems, while in 392 cases the sentences differed but had equal numbers of n-gram overlaps. The BLEU score for the oracle is higher than that of both baselines; from this and the distribution of the oracle's choices, we conclude that the difference between our findings and those of <cite>Collins et al. (2005)</cite> is at least partly due to the inconsistency that they identified.",
  "y": "differences"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_17",
  "x": "Alternatively, the multiple options could be created by the same preprocessor but based on different parses, say the n best parses returned by one parser, or the output of n different parsers with comparable outputs. This extension would be quite different from the lattice-based systems in \u00a72.4, which are all based on a single parse. For future systems, we would like to replace the <cite>Collins et al. (2005)</cite> reordering rules with a set of automatically-extracted reordering rules (as in Xia and McCord (2004) ) so that we may more easily explore the usefulness of our system and confidence features in new language pairs with a variety of reordering requirements.",
  "y": "future_work"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_18",
  "x": "The next major phase of this work is to extend and explore the feature space. This entails examining subsets of confidence features to establish which are the most useful indicators of reliable reordering, and possibly replacing the MERT tuning process with another algorithm, such as MIRA, to handle a greater quantity of features. In addition, we wish to explore more fully our negative result with the reimplementation of the <cite>Collins et al. (2005)</cite> system, to investigate the effect of balancing features in the lattice, and to examine the variability of the BLEU scores for each system.",
  "y": "future_work"
 },
 {
  "id": "20b605ec3596ccd204b60cf893b738_19",
  "x": "We then augment the translation model of our system with a number of features to express our confidence in the reordering. While these features do not yield further improvement, a rough upper bound provided by our approximate oracle suggests that other features may still be found to guide the system in choosing whether or not to use the syntactically-informed reordering. While our reordering step is a reimplementation of the <cite>Collins et al. (2005)</cite> system, contrary to their findings we do not see an improvement using the reordering step alone.",
  "y": "differences uses"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_0",
  "x": "The most widely used technique is the use of beam search with n-gram LMs proposed by<cite> Nuhn et al. (2013)</cite> . We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural LM. We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural LM.",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_1",
  "x": "Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (Ravi and Knight, 2011;<cite> Nuhn et al., 2013)</cite> . Some methods use the ExpectationMaximization (EM) algorithm (Knight et al., 2006) while most state-of-the-art approaches for decipherment of 1:1 and homophonic substitution ciphers use beam search and rely on the clever use of n-gram LMs (Nuhn et al., 2014; Hauer et al., 2014) . Neural LMs globally score the entire candidate plaintext sequence (Mikolov et al., 2010) .",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_2",
  "x": "---------------------------------- **DECIPHERMENT MODEL** We use the notation from<cite> Nuhn et al. (2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_3",
  "x": "where p(.) is the language model (LM). Finding this argmax is solved using a beam search algorithm<cite> (Nuhn et al., 2013)</cite> which incrementally finds the most likely substitutions using the language model scores as the ranking. ----------------------------------",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_4",
  "x": "---------------------------------- **BEAM SEARCH** Algorithm 1 is the beam search algorithm<cite> (Nuhn et al., 2013</cite> (Nuhn et al., , 2014 Hs.",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_5",
  "x": "In this experiment we use a synthetic 1:1 letter substitution cipher dataset following Ravi and Knight (2008) ,<cite> Nuhn et al. (2013)</cite> and Hauer et al. (2014) . The text is from English Wikipedia articles about history 3 , preprocessed by stripping the text of all images, tables, then lower-casing all characters, and removing all non-alphabetic and non-space characters. We create 50 cryptograms for each length 16, 32, 64, 128 and 256 using a random Caesar-cipher 1:1 substitution.",
  "y": "similarities uses"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_6",
  "x": "**AN EASY CIPHER: ZODIAC-408** Zodiac-408, a homophonic cipher, is commonly used to evaluate decipherment algorithms. Our neural LM model with global rest cost estimation and frequency matching heuristic with a beam size of 1M has SER of 1.2% compared to the beam search algorithm<cite> (Nuhn et al., 2013)</cite> with beam size of 10M with a 6-gram LM which gives an SER of 2%.",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_7",
  "x": "Nuhn et al. (2014) present various improvements to the beam search algorithm in<cite> Nuhn et al. (2013)</cite> including improved rest cost estimation and an optimized strategy for ordering decipherment of the cipher symbols. Hauer et al. (2014) propose a novel approach for solving mono-alphabetic substitution ciphers which combines character-level and word-level language model. They formulate decipherment as a tree search problem, and use Monte Carlo Tree Search (MCTS) as an alternative to beam search.",
  "y": "background"
 },
 {
  "id": "2357152e66ad3ae1c23738ac95f971_8",
  "x": "This paper presents, to our knowledge, the first application of large pre-trained neural LMs to the decipherment problem. We modify the beam search algorithm for decipherment from<cite> Nuhn et al. (2013</cite>; and extend it to use global scoring of the plaintext message using neural LMs. To enable full plaintext scoring we use the neural LM to sample plaintext characters which reduces the beam size required.",
  "y": "extends differences"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_0",
  "x": "Learning Rate Schedulers update the learning rate over the course of training. We provide several popular schedulers, e.g., the inverse square-root scheduler from <cite>Vaswani et al. (2017)</cite> and cyclical schedulers based on warm restarts (Loshchilov and Hutter, 2016) . Reproducibility and forward compatibility.",
  "y": "similarities uses"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_1",
  "x": "After the FP16 gradients are synchronized between workers, we convert them to FP32, restore the original scale, and update the weights. Inference. FAIRSEQ provides fast inference for non-recurrent models (Gehring et al., 2017;<cite> Vaswani et al., 2017</cite>; Fan et al., 2018b; Wu et al., 2019) through incremental decoding, where the model states of previously generated tokens are cached in each active beam and re-used.",
  "y": "background"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_2",
  "x": "We provide reference implementations of several popular sequence-to-sequence models which can be used for machine translation, including LSTM (Luong et al., 2015) , convolutional models (Gehring et al., 2017; Wu et al., 2019) and Transformer<cite> (Vaswani et al., 2017)</cite> . We evaluate a \"big\" Transformer encoderdecoder model on two language pairs, WMT English to German (En-De) and WMT English to French (En-Fr). For En-De we replicate the setup of <cite>Vaswani et al. (2017)</cite> which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14.",
  "y": "background"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_3",
  "x": "For En-De we replicate the setup of <cite>Vaswani et al. (2017)</cite> which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14. The 32K vocabulary is based on a joint source and target byte pair encoding (BPE; Sennrich et al. 2016 ). For En-Fr, we train on WMT'14 and borrow the setup of Gehring et al. (2017) with 36M training sentence pairs.",
  "y": "similarities uses"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_4",
  "x": "We measure case-sensitive tokenized BLEU with multi-bleu (Hoang et al., 2006) and detokenized BLEU with SacreBLEU 1 (Post, 2018) . All results use beam search with a beam width of 4 and length penalty of 0.6, following<cite> Vaswani et al. 2017</cite> . FAIRSEQ results are summarized in Table 2 .",
  "y": "similarities uses"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_5",
  "x": "FAIRSEQ results are summarized in Table 2 . We reported improved BLEU scores over <cite>Vaswani et al. (2017)</cite> by training with a bigger batch size and an increased learning rate . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_6",
  "x": "**LANGUAGE MODELING** FAIRSEQ supports language modeling with gated convolutional models and Transformer models<cite> (Vaswani et al., 2017)</cite> . Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. Gehring et al. (2017) 25.2 40.5 b. <cite>Vaswani et al. (2017)</cite> 28.4 41.0 c. Ahmed et al. (2017) 28.9 41.4 d. Shaw et al. (2018) 29 et al., 2016), adaptive softmax (Grave et al., 2017) , and adaptive inputs .",
  "y": "background"
 },
 {
  "id": "242aacd35fb92d836fea9eb33961a3_7",
  "x": "**LANGUAGE MODELING** FAIRSEQ supports language modeling with gated convolutional models and Transformer models<cite> (Vaswani et al., 2017)</cite> . Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. Gehring et al. (2017) 25.2 40.5 b. <cite>Vaswani et al. (2017)</cite> 28.4 41.0 c. Ahmed et al. (2017) 28.9 41.4 d. Shaw et al. (2018) 29 et al., 2016), adaptive softmax (Grave et al., 2017) , and adaptive inputs .",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_0",
  "x": "Neural sequence-to-sequence models define the state of the art for paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> Kann and Sch\u00fctze, 2016) , the task of generating inflected forms of a lemma's paradigm, e.g., filling the empty fields in Table 1 using one of the non-empty fields. However, those models are in general very datahungry, and do not reach good performances in low-resource settings. Therefore,<cite> Kann et al. (2017)</cite> propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language).",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_1",
  "x": "**INTRODUCTION** Neural sequence-to-sequence models define the state of the art for paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> Kann and Sch\u00fctze, 2016) , the task of generating inflected forms of a lemma's paradigm, e.g., filling the empty fields in Table 1 using one of the non-empty fields. However, those models are in general very datahungry, and do not reach good performances in low-resource settings.",
  "y": "motivation background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_2",
  "x": "Neural sequence-to-sequence models define the state of the art for paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> Kann and Sch\u00fctze, 2016) , the task of generating inflected forms of a lemma's paradigm, e.g., filling the empty fields in Table 1 using one of the non-empty fields. However, those models are in general very datahungry, and do not reach good performances in low-resource settings. While closer related languages seem to help more than distant ones, the mechanisms how this transfer works still remain largely obscure.",
  "y": "motivation"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_3",
  "x": "Therefore,<cite> Kann et al. (2017)</cite> propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language). However,<cite> Kann et al. (2017)</cite> show that transferring morphological knowledge from Spanish to Portuguese, two languages with similar morphology and 89% lexical similarity, works well and, more surprisingly, even supposedly very different languages like Arabic and Spanish can benefit from each other.",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_4",
  "x": "Therefore,<cite> Kann et al. (2017)</cite> propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language). While closer related languages seem to help more than distant ones, the mechanisms how this transfer works still remain largely obscure.",
  "y": "motivation"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_5",
  "x": "An extreme example is the infeasible task of transferring morphological knowledge from Chinese to Portuguese as Chinese does not make use of inflection at all. Even between two morphologically rich languages transfer is difficult if they are unrelated, since inflections often mark dissimilar subcategories and word forms do not share similarities. However,<cite> Kann et al. (2017)</cite> show that transferring morphological knowledge from Spanish to Portuguese, two languages with similar morphology and 89% lexical similarity, works well and, more surprisingly, even supposedly very different languages like Arabic and Spanish can benefit from each other.",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_6",
  "x": "Model description. The model<cite> Kann et al. (2017)</cite> use and we explore in more detail here is an encoder-decoder recurrent neural network (RNN) with attention (Bahdanau et al., 2015) . It is trained on maximizing the following log-likelihood:",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_7",
  "x": "---------------------------------- **DATA** We use the Romance and Arabic language data from<cite> Kann et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_8",
  "x": "This explains the unexpected result that l-emb performs best for Arabic (200) and Portuguese (200): both source languages potentially confuse the language model; in Portuguese we contribute this to a big overlap of lemmata in the two languages with Portuguese often inflecting in a different way<cite> (Kann et al., 2017)</cite> . Further, the differences in performance between original and t-emb show that the model indeed learns information from the tags, supposedly which output sequence is more likely to appear with which tag. The l-emb-sep and t-emb-sep results show that a separation symbol clearly improves the model's performance.",
  "y": "background"
 },
 {
  "id": "28900a293048fdb0c40dc540985cf1_9",
  "x": "Paradigm completion. SIGMORPHON hosted two shared tasks on paradigm completion (Cotterell et al., 2016 <cite>(Cotterell et al., , 2017</cite> , in order to encourage the development of systems for the task. One approach is to treat it as a string transduction problem by applying an alignment model with a semi-Markov model (Durrett and DeNero, 2013; Nicolai et al., 2015) .",
  "y": "background"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_0",
  "x": "In other words, as most of the pre-trained LMs are designed to be of help to the tasks which can be categorized as classification including extractive summarization, they are not guaranteed to be advantageous to abstractive summarization models that should be capable of generating language (Wang and Cho, 2019; Zhang et al., 2019b) . On the other hand, recent studies for abstractive summarization<cite> (Chen and Bansal, 2018</cite>; Hsu et al., 2018; Gehrmann et al., 2018) have attempted to exploit extractive models. Among these, a notable one is<cite> Chen and Bansal (2018)</cite> , in which a sophisticated model called Reinforce-Selected Sentence Rewriting is proposed.",
  "y": "background"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_1",
  "x": "In other words, as most of the pre-trained LMs are designed to be of help to the tasks which can be categorized as classification including extractive summarization, they are not guaranteed to be advantageous to abstractive summarization models that should be capable of generating language (Wang and Cho, 2019; Zhang et al., 2019b) . On the other hand, recent studies for abstractive summarization<cite> (Chen and Bansal, 2018</cite>; Hsu et al., 2018; Gehrmann et al., 2018) have attempted to exploit extractive models. Among these, a notable one is<cite> Chen and Bansal (2018)</cite> , in which a sophisticated model called Reinforce-Selected Sentence Rewriting is proposed.",
  "y": "background"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_2",
  "x": "It is further fine-tuned by training the extractor with the rewards derived from sentencelevel ROUGE scores of the summary generated from the abstractor. In this paper, we improve the model of<cite> Chen and Bansal (2018)</cite> , addressing two primary issues. Firstly, we argue there is a bottleneck in the existing extractor on the basis of the observation that its performance as an independent summarization model (i.e., without the abstractor) is no better than solid baselines such as selecting the first 3 sentences.",
  "y": "extends"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_3",
  "x": "In this paper, we focus on single-document multisentence summarization and propose a neural abstractive model based on the Sentence Rewriting framework<cite> (Chen and Bansal, 2018</cite>; Xu and Dur-rett, 2019) which consists of two parts: a neural network for the extractor and another network for the abstractor. The extractor network is designed to extract salient sentences from a source article. The abstractor network rewrites the extracted sentences into a short summary.",
  "y": "extends"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_4",
  "x": "This property is needed to avoid selecting sentences that have overlapping information with the sentences extracted already. As the decoder structure is almost the same with the previous work, we convey the equations of<cite> Chen and Bansal (2018)</cite> to avoid confusion, with minor modifications to agree with our notations. Formally, the extraction probability is calculated as:",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_5",
  "x": "Our abstractor is practically identical to the one proposed in<cite> Chen and Bansal (2018)</cite> . ---------------------------------- **TRAINING**",
  "y": "similarities"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_6",
  "x": "Extractor Pre-training Starting from a poor random policy makes it difficult to train the extractor agent to converge towards the optimal policy. Thus, we pre-train the network using cross entropy (CE) loss like previous work (Bahdanau et al., 2017;<cite> Chen and Bansal, 2018)</cite> . However, there is no gold label for extractive summarization in most of the summarization datasets.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_7",
  "x": "This encourages the model to extract additional sentences only when they are expected to increase the final return. Following<cite> Chen and Bansal (2018)</cite> , we use the Advantage Actor Critic (Mnih et al., 2016) method to train. We add a critic network to estimate a value function V t (D,\u015d 1 , \u00b7 \u00b7 \u00b7 ,\u015d t\u22121 ), which then is used to compute advantage of each action (we will omit the current state (D,\u015d 1 , \u00b7 \u00b7 \u00b7 ,\u015d t\u22121 ) to simplify):",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_8",
  "x": "**EVALUATION** We evaluate the performance of our method using different variants of ROUGE metric computed with respect to the gold summaries. On the CNN/Daily Mail and DUC-2002 dataset, we use standard ROUGE-1, ROUGE-2, and ROUGE- L (Lin, 2004) on full length F 1 with stemming as previous work did (Nallapati et al., 2017; See et al., 2017;<cite> Chen and Bansal, 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_9",
  "x": "Therefore, the very first objective of extractive models is to outperform the simple method which always returns 3 or 4 sentences at the top. However, as Table 2 shows, ROUGE scores of lead baselines and extractors from previous work in Sentence Rewrite framework<cite> (Chen and Bansal, 2018</cite>; Xu and Durrett, 2019) are almost tie. We can easily conjecture that the limited performances of their full model are due to their extractor networks.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_10",
  "x": "Abstractive Summarization Our abstractive approaches combine the extractor with the abstractor. The combined model (BERT-ext + abs) without additional RL training outperforms the Sentence Rewrite model<cite> (Chen and Bansal, 2018)</cite> without reranking, showing the effectiveness of our extractor network. With the proposed RL training procedure (BERT-ext + abs + RL), our model exceeds the best model of<cite> Chen and Bansal (2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_11",
  "x": "Abstractive Summarization Our abstractive approaches combine the extractor with the abstractor. The combined model (BERT-ext + abs) without additional RL training outperforms the Sentence Rewrite model<cite> (Chen and Bansal, 2018)</cite> without reranking, showing the effectiveness of our extractor network. With the proposed RL training procedure (BERT-ext + abs + RL), our model exceeds the best model of<cite> Chen and Bansal (2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_12",
  "x": "In addition, the result is better than those of all the other abstractive methods exploiting extractive approaches in them (Hsu et al., 2018; <cite>Chen and Bansal, 2018</cite>; Gehrmann et al., 2018) . Redundancy Control Although the proposed RL training inherently gives training signals that induce the model to avoid redundancy across sentences, there can be still remaining overlaps between extracted sentences. We found that the additional methods reducing redundancies can improve the summarization quality, especially on CNN/Daily Mail dataset.",
  "y": "differences"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_13",
  "x": "We found that the additional methods reducing redundancies can improve the summarization quality, especially on CNN/Daily Mail dataset. We tried Trigram Blocking (Liu, 2019) for extractor and Reranking<cite> (Chen and Bansal, 2018)</cite> for abstractor, and we empirically found that the reranking only improves the performance. This helps the model to compress the extracted sentences focusing on disjoint information, even if there are some partial overlaps between the sentences.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_14",
  "x": "All the document sentences are paraphrased with our trained abstractor, and then we find the best set for each search method. Sentence-matching finds sentences with the highest ROUGE-L score for each sentence in the gold summary. This search method matches with the best reward from<cite> Chen and Bansal (2018)</cite> .",
  "y": "similarities"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_15",
  "x": "Relevance Readability Total Sentence Rewrite<cite> (Chen and Bansal, 2018)</cite> 56 59 115 BERTSUM (Liu, 2019) 58 60 118 BERT-ext + abs + RL + rerank (ours) 66 61 127 which has the highest summary-level ROUGE-L score, from all the possible combinations of sentences. Due to time constraints, we limited the maximum number of sentences to 5. This method corresponds to our final return in RL training.",
  "y": "differences"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_16",
  "x": "Additionally, we experiment to verify the contribution of our training method. We train the same model with different training signals; Sentencelevel reward from<cite> Chen and Bansal (2018)</cite> and combinatorial reward from ours. The results are shown in Table 4 .",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_17",
  "x": "To evaluate both these criteria, we design a Amazon Mechanical Turk experiment based on ranking method, inspired by Kiritchenko and Mohammad (2017) . We randomly select 20 samples from the CNN/Daily Mail test set and ask the human testers (3 for each sample) to rank summaries (for relevance and readability) produced by 3 different models: our final model, that of<cite> Chen and Bansal (2018)</cite> and that of Liu (2019) . 2, 1 and 0 points were given according to the ranking.",
  "y": "uses"
 },
 {
  "id": "2a01240f628d7deb74e6e9fe750378_18",
  "x": "More recently, there have been several studies that have attempted to improve the performance of the abstractive summarization by explicitly combining them with extractive models. Some notable examples include the use of inconsistency loss (Hsu et al., 2018) , key phrase extraction (Li et al., 2018; Gehrmann et al., 2018) , and sentence extraction with rewriting<cite> (Chen and Bansal, 2018)</cite> . Our model improves Sentence Rewriting with BERT as an extractor and summary-level rewards to optimize the extractor.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_0",
  "x": "Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010) , the recent work of <cite>(Poon and Domingos, 2009</cite> ) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_1",
  "x": "Statistical approaches to semantic parsing have recently received considerable attention. However, most of this research has concentrated on supervised methods requiring large amounts of labeled data. These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001; Banko et al., 2007) . The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument relations. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010) , the recent work of <cite>(Poon and Domingos, 2009</cite> ) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_2",
  "x": "In this paper, we adopt the above definition of unsupervised semantic parsing and propose a Bayesian non-parametric approach which uses hierarchical Pitman-Yor (PY) processes (Pitman, 2002) to model statistical dependencies between predicate and argument clusters, as well as distributions over syntactic and lexical realizations of each cluster. Our non-parametric model automatically discovers granularity of clustering appropriate for the dataset, unlike the parametric method of<cite> (Poon and Domingos, 2009)</cite> which have to perform model selection and use heuristics to penalize more complex models of semantics. Additional benefits generally expected from Bayesian modeling include the ability to encode prior linguistic knowledge in the form of hyperpriors and the potential for more reliable modeling of smaller datasets.",
  "y": "differences"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_3",
  "x": "More detailed discussion of relation between the Markov Logic Network (MLN) approach of <cite>(Poon and Domingos, 2009</cite> ) and our non-parametric method is presented in Section 3. Hierarchical Pitman-Yor processes (or their special case, hierarchical Dirichlet processes) have previously been used in NLP, for example, in the context of syntactic parsing (Liang et al., 2007; Johnson et al., 2007) . However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al., 2007) , or the number of adapted productions in the adaptor grammar (Johnson et al., 2007) ) was not very large.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_4",
  "x": "In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus <cite>(Poon and Domingos, 2009</cite> ). This suggests that standard inference methods for hierarchical PY processes, such as Gibbs sampling, Metropolis-Hastings (MH) sampling with uniform proposals, or the structured mean-field algorithm, are unlikely to result in efficient inference: for example in standard Gibbs sampling all thousands of alternatives should be considered at each sampling move. Instead, we use a split-merge MH sampling algorithm, which is a standard and efficient inference tool for non-hierarchical PY processes (Jain and Neal, 2000; Dahl, 2003) but has not previously been used in hierarchical setting.",
  "y": "background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_5",
  "x": "We evaluate our model both qualitatively, examining the revealed clustering of syntactic structures, and quantitatively, on a question answering task. In both cases, we follow <cite>(Poon and Domingos, 2009</cite> ) in using the corpus of biomedical abstracts. Our model achieves favorable results significantly outperforming the baselines, including state-of-theart methods for relation extraction, and achieves scores comparable to those of the MLN model.",
  "y": "uses"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_6",
  "x": "Unlike <cite>(Poon and Domingos, 2009</cite> ), we do not use the lambda calculus formalism to define our task but rather treat it as an instance of frame-semantic parsing, or a specific type of semantic role labeling (Gildea and Jurafsky, 2002) . The reason for this is two-fold: first, the frame semantics view is more standard in computational linguistics, sufficient to describe induced semantic representation and convenient to relate our method to the previous work. Second, lambda calculus is a considerably more powerful formalism than the predicate-argument structure used in frame semantics, normally supporting quantification and logical connectors (for example, negation and disjunction), neither of which is modeled by our model or in<cite> (Poon and Domingos, 2009)</cite> .",
  "y": "differences background"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_7",
  "x": "Thus, the argument identification and labeling stages consist of labeling each syntactic arc with a semantic role label. In comparison, the MLN model does not explicitly assume contiguity of lexical items and does not make this directionality assumption but their clustering algorithm uses initialization and clusterization moves such that the resulting model also obeys both of these constraints. Third, as in <cite>(Poon and Domingos, 2009</cite> ), we do not model polysemy as we assume that each syntactic fragment corresponds to a single semantic class.",
  "y": "similarities"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_8",
  "x": "This is not a model assumption and is only used at inference as it reduces mixing time of the Markov chain. It is not likely to be restrictive for the biomedical domain studied in our experiments. As in some of the recent work on learning semantic representations (Eisenstein et al., 2009;<cite> Poon and Domingos, 2009</cite> ), we assume that dependency structures are provided for every sentence.",
  "y": "similarities"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_9",
  "x": "The work of <cite>(Poon and Domingos, 2009</cite> ) models joint probability of the dependency tree and its latent semantic representation using Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) , selecting parameters (weights of first-order clauses) to maximize the probability of the observed dependency structures. For each sentence, the MLN induces a Markov network, an undirected graphical model with nodes corresponding to ground atoms and cliques corresponding to ground clauses. The MLN is a powerful formalism and allows for modeling complex interaction between features of the input (syntactic trees) and latent output (semantic representation), however, unsupervised learning of semantics with general MLNs can be prohibitively expensive. The reason for this is that MLNs are undirected models and when learned to maximize likelihood of syntactically annotated sentences, they would require marginalization over semantic representation but also over the entire space of syntactic structures and lexical units. Given the complexity of the semantic parsing task and the need to tackle large datasets, even approximate methods are likely to be infeasible.",
  "y": "background motivation"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_10",
  "x": "In order to overcome this problem, <cite>(Poon and Domingos, 2009</cite> ) group parameters and impose local normalization constraints within each group. Given these normalization constraints, and additional structural constraints satisfied by the model, namely that the clauses should be engineered in such a way that they induce treestructured graphs for every sentence, the parameters can be estimated by a variant of the EM algorithm. The class of such restricted MLNs is equivalent to the class of directed graphical models over the same set of random variables corresponding to fragments of syntactic and semantic structure. Given that the above constraints do not directly fit into the MLN methodology, we believe that it is more natural to regard their model as a directed model with an underlying generative story specifying how the semantic structure is generated and how the syntactic parse is drawn for this semantic structure. This view would facilitate understanding what kind of features can easily be integrated into the model, simplify application of non-parametric Bayesian techniques and expedite the use of inference techniques designed specifically for directed models. Our approach makes one step in this direction by proposing a non-parametric version of such generative model.",
  "y": "differences background motivation"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_11",
  "x": "We used the GENIA corpus (Kim et al., 2003) , a dataset of 1999 biomedical abstracts, and a set of questions produced by<cite> (Poon and Domingos, 2009)</cite> . A example question is shown in Figure 3 . All model hyperpriors were set to maximize the posterior, except for w (A) and w (C) , which were set to 1.e \u2212 10 and 1.e \u2212 35, respectively.",
  "y": "uses"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_12",
  "x": "For example, an argument type of class 2 is modeled as a distribution over two argument parts, prep of and prep from. The corresponding arguments define the origin of the cells (transgenic mouse, smoker, volunteer, donor, . . . ) . We now turn to the QA task and compare our model (USP-BAYES) with the results of baselines considered in <cite>(Poon and Domingos, 2009</cite> ).",
  "y": "uses"
 },
 {
  "id": "2b836473cf682ed474b7cda1800f84_13",
  "x": "There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; ), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model <cite>(Poon and Domingos, 2009</cite> ), another unsupervised method has been proposed in (Goldwasser et al., 2011) . In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol.",
  "y": "background"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_0",
  "x": "Word embeddings based on singular value decomposition (SVD; historically popular in the form of Latent Semantic Analysis (Deerwester et al., 1990) ) are not affected by this problem. Levy et al. (2015) created SVD PPMI after investigating the implicit operations performed while training neural word embeddings (Levy and Goldberg, 2014) . As SVD PPMI performs very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the Digital Humanities<cite> (Hamilton et al., 2016</cite>; Hellrich and Hahn, 2016a) .",
  "y": "background"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_1",
  "x": "Most work is centered around word2vec (e.g., Kim et al. (2014) ; Kulkarni et al. (2015) ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., Jo (2016) using GloVe (Pennington et al., 2014) and<cite> Hamilton et al. (2016)</cite> using SVD PPMI . Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (Kim et al., 2014; Kulkarni et al., 2015;<cite> Hamilton et al., 2016)</cite> and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space. This information can then be exploited for automatic (Buechel et al., 2016) or manual (Jo, 2016) interpretation.",
  "y": "background"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_2",
  "x": "Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (Kim et al., 2014; Kulkarni et al., 2015;<cite> Hamilton et al., 2016)</cite> and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space. This information can then be exploited for automatic (Buechel et al., 2016) or manual (Jo, 2016) interpretation. research.",
  "y": "background"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_3",
  "x": "In accordance with this limit, we also discarded slices with less than 10k (5k for RSC) 7 Parameters were chosen in accordance with Levy et al. (2015) and <cite>Hamilton et al. (2016</cite> words above the minimum frequency threshold used during PPMI and \u03c7 2 calculation, e.g., the 1810s and 1820s COHA slices. Figure 1 illustrates this sequence of processing steps, while Table 1 summarizes the resulting models for each corpus. 5 Website and API JESEME provides both an interactive website and an API for querying the underlying database.",
  "y": "uses similarities"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_4",
  "x": "Similar Words depicts the words with the highest similarity relative to the query term for the first and last time slice and how their similarity values changed over time. We follow Kim et al. (2014) in choosing such a visualization, while we refrain from using the two-dimensional projection used in other studies (Kulkarni et al., 2015;<cite> Hamilton et al., 2016)</cite> . We stipulate that the latter could Figure 2 : Screenshot of JESEME's result page when searching for the lexical item \"heart\" in COHA.",
  "y": "differences"
 },
 {
  "id": "2db9f6c8540d8d2b7a5946c3c132e9_6",
  "x": "JESEME is also the first tool of its kind and under continuous development. Future technical work will add functionality to compare words across corpora which might require a mapping between embeddings (Kulkarni et al., 2015;<cite> Hamilton et al., 2016)</cite> and provide optional stemming routines. Both goals come with an increase in precomputed similarity values and will thus necessitate storage optimizations to ensure long-term availability.",
  "y": "future_work"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_0",
  "x": "Singular value decomposition (SVD) is a common factorization technique and has been explored in feedforward networks [9, 10, 21, 22] and recurrent neural networks (RNN) <cite>[11]</cite> . Neural network quantization refers to compressing the original network by reducing number of bits required to represent weight matrices, and it has been studied for different model architectures [12, 13, 14, 15, 16, 19, 20] . By reducing the bit-width of weights, model size is reduced, and it also brings considerable acceleration via efficient low bit-width arithmetic operations supports available on hardware.",
  "y": "background"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_1",
  "x": "Compression of neural networks has been explored in broad context. We focus on two widely used and effective methods for deep models: low-rank matrix factorization and and quantization. Singular value decomposition (SVD) is a common factorization technique and has been explored in feedforward networks [9, 10, 21, 22] and recurrent neural networks (RNN) <cite>[11]</cite> .",
  "y": "uses background"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_2",
  "x": "Low-rank matrix factorization The factorization of weight matrices is based on the SVD compression of LSTM <cite>[11]</cite> . Let W Quantization training Quantization refers to representing floating-point values with n-bit integers (n < 32).",
  "y": "background"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_3",
  "x": "We start by formulating the multi-class audio event detection problem. Low-rank matrix factorization The factorization of weight matrices is based on the SVD compression of LSTM <cite>[11]</cite> .",
  "y": "uses background"
 },
 {
  "id": "2dc830dd598102ee82f1b982b88be9_4",
  "x": "We compute area under curve (AUC) and equal error rate (EER) as the two quantitative measures. As the distribution of weight matrices' eigenvalues can be different across different LSTM layers, we follow the practice of <cite>[11]</cite> to set the same threshold \u03c4 across layers as the fraction of retained singular values, defined as \u03c4 = Table 1 summarizes the results of low-rank matrix factorization compared to our baseline 3-layer LSTM. There is no accuracy degradation when \u03c4 is reduced to 0.6, which we hypothesize to be related to the regularizing effects.",
  "y": "uses"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_0",
  "x": "With the success of computer-based speech recognition over the past decades, automatic lipreading has become an active field of research as well, with pioneering work by Petajan [3] , who used lipreading to augment conventional acoustic speech recognition, and Chiou and Hwang [4] , who were the first to perform lipreading without resorting to any acoustic signal at all. Since 2014, lipreading systems have systematically begun to use neural networks at part of the processing pipeline [5, 6] or for end-to-end-training<cite> [7,</cite> 8, 9] . In our previous work <cite>[7]</cite> , we proposed a fully neural network based system, using a stack of fully connected and recurrent (LSTM, Long ShortTerm Memory) [10, 11] neural network layers.",
  "y": "background"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_1",
  "x": "With the success of computer-based speech recognition over the past decades, automatic lipreading has become an active field of research as well, with pioneering work by Petajan [3] , who used lipreading to augment conventional acoustic speech recognition, and Chiou and Hwang [4] , who were the first to perform lipreading without resorting to any acoustic signal at all. Since 2014, lipreading systems have systematically begun to use neural networks at part of the processing pipeline [5, 6] or for end-to-end-training<cite> [7,</cite> 8, 9] . In our previous work <cite>[7]</cite> , we proposed a fully neural network based system, using a stack of fully connected and recurrent (LSTM, Long ShortTerm Memory) [10, 11] neural network layers.",
  "y": "similarities"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_2",
  "x": "In our previous work <cite>[7]</cite> , we proposed a fully neural network based system, using a stack of fully connected and recurrent (LSTM, Long ShortTerm Memory) [10, 11] neural network layers. The scope of this paper is the introduction of state-of-theart methods for speaker-independent lipreading with neural networks. We evaluate our established system <cite>[7]</cite> in a crossspeaker setting, observing a drastic performance drop on unknown speakers.",
  "y": "similarities"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_3",
  "x": "The first deep neural network for lipreading was a seven-layer convolutional net as a preprocessing stage for an HMM-based word recognizer [5] . Since then, several end-to-end trainable systems were presented<cite> [7,</cite> 8, 9] . The current state-of-the-art accuracy on the GRID corpus is 3.3% error [9] using a very large set of additional training data; so their result is not directly comparable to ours.",
  "y": "similarities"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_4",
  "x": "We follow the data preprocessing protocol from <cite>[7]</cite> . We use the GRID corpus [13] , which consists of video and audio recordings of 34 speakers (which we name s1 to s34) saying 1000 sentences each. All sentences have a fixed structure: command(4) + color(4) + preposition(4) + letter(25) + digit(10) + adverb(4), for example \"Place red at J 2, please\", where the number of alternative words is given in parentheses.",
  "y": "similarities uses"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_5",
  "x": "We converted the \"normal\" quality videos (360 \u00d7 288 pixels) to greyscale and extracted 40\u00d740 pixel windows containing the mouth area, as described in <cite>[7]</cite> . The frames were contrastnormalized and z-normalized over the training set, independently for each speaker. Unreadable videos were discarded.",
  "y": "uses similarities"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_6",
  "x": "**METHODS AND SYSTEM SETUP** The system is based on the lipreading setup from <cite>[7]</cite> , reimplemented in Tensorflow [36] . Raw 40 \u00d7 40 lip images are used as input data, without any further preprocessing except normalization.",
  "y": "extends differences"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_7",
  "x": "Table 1 : Baseline word accuracies on single speakers, averaged over the development set, with standard deviation. Layer types are FC (fully connected feedforward), DP (Dropout), and LSTM, followed by the number of neurons/cells. * marks the (reimplemented and recomputed) best system from <cite>[7]</cite> .",
  "y": "background"
 },
 {
  "id": "2e7df0912d9aac8bf97f4061de613f_8",
  "x": "---------------------------------- **BASELINE LIPREADER** The first experiment deals with establishing a baseline for our experiments, building on prior work <cite>[7]</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "2e95d98d5f9d4d6fc90e3d8453f945_0",
  "x": "1 <cite>(Tetreault and Chodorow, 2008b)</cite> challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability. For example, trained raters typically annotate preposition errors with a kappa around 0.60. This low rater reliability has repercussions for system evaluation: Their experiments showed that system precision could vary as much as 10% depending on which rater's judgments they used as the gold standard.",
  "y": "background"
 },
 {
  "id": "2e95d98d5f9d4d6fc90e3d8453f945_1",
  "x": "However, in our work we concentrate on tasks where there is no single gold standard, either because there are multiple prepositions that are acceptable in a given context or because the conventions of preposition usage simply do not conform to strict rules. Typically, an early step in developing a preposition or article error detection system is to test the system on well-formed text written by native speakers to see how well the system can predict, or select, the writer's preposition given the context around the preposition. <cite>(Tetreault and Chodorow, 2008b)</cite> showed that trained human raters can achieve very high agreement (78%) on this task.",
  "y": "background"
 },
 {
  "id": "2e95d98d5f9d4d6fc90e3d8453f945_2",
  "x": "<cite>(Tetreault and Chodorow, 2008b)</cite> showed that trained human raters can achieve very high agreement (78%) on this task. We replicate this experiment not with trained raters but with the AMT to answer two research questions: 1. Can untrained raters be as effective as trained 46 raters? 2. If so, how many raters does it take to match trained raters?",
  "y": "extends background"
 },
 {
  "id": "2e95d98d5f9d4d6fc90e3d8453f945_3",
  "x": "The set consisted of 152 prepositions in total, and we requested 20 judgments per preposition. Previous work has shown this task to be a difficult one for trainer raters to attain high reliability. For example, <cite>(Tetreault and Chodorow, 2008b)</cite> found kappa between two raters averaged 0.630.",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_0",
  "x": "Given the increasing need for learning material adapted to different audiences and the barrier-free access to information required for political and social participation, automatic readability assessment is of immediate social relevance. Accordingly, it has attracted considerable research interest over the last decades, particularly for the assessment of English (Crossley et al., 2011; Chen and Meurers, 2017; Feng et al., 2010) . For German readability assessment, however, little progress has been made in recent years, despite a series of promising results published around the turn of the decade (Vor der Br\u00fcck et al., 2008;<cite> Hancke et al., 2012)</cite> .",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_1",
  "x": "To address these issues, we first present two new data sets for German readability assessment in Section 3: a set of German news broadcast subtitles based on the primary German TV news outlet Tagesschau and the children's counterpart Logo!, and a GEO/GEOlino corpus crawled from the educational GEO magazine's web site, a source first identified by <cite>Hancke et al. (2012)</cite> , but double in size. 1 The longstanding success of these outlets with their target audiences provides some external validity to the nature of the implicit linguistic adaptation of the language used. As showed for German secondary school textbooks, this is not necessarily the case across all linguistic dimensions and adjustments may even be limited to only the surface level of text, sentence, and word length.",
  "y": "extends"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_2",
  "x": "They show, that the typical aggregation of word frequencies across documents are less informative than richer representations including frequency standard deviations. In contrast to English, research on readability assessment for other languages, such as German, is more limited. There was a series of articles on this issue from the late 2000s to the early 2010s that demonstrated the benefits of broad linguistic modeling, in particular the use of morphological complexity measures for languages with rich morphological systems like German (Vor der Br\u00fcck et al., 2008;<cite> Hancke et al., 2012)</cite> , but also Russian (Reynolds, 2016) or French (Fran\u00e7ois and Fairon, 2012) .",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_3",
  "x": "Later work by <cite>Hancke et al. (2012)</cite> also combines traditional readability formula measures, such as text or word length, with more sophisticated lexical, syntactic, and language model, and morphological features to assess German readability, but they employ an overall broader and more diverse feature set than DeLite. They investigate readability of educational magazines on the GEO/GEOlino data set, which they compiled from online articles freely available at the GEO magazine's web page. Their work illustrates the relevance of rich linguistic modeling for readability assessment and in particular the value of morphological complexity features for German.",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_4",
  "x": "3 They are comparable to the National Geographic magazine and cover a variety of topics ranging from culture and history to technology and nature. <cite>Hancke et al. (2012)</cite> first compiled and analyzed a data set from this web resource. We followed their lead and crawled 8,263 articles from the GEO/GEOlino online archive, almost doubling the size of the original corpus.",
  "y": "background"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_5",
  "x": "3 They are comparable to the National Geographic magazine and cover a variety of topics ranging from culture and history to technology and nature. <cite>Hancke et al. (2012)</cite> first compiled and analyzed a data set from this web resource. We followed their lead and crawled 8,263 articles from the GEO/GEOlino online archive, almost doubling the size of the original corpus.",
  "y": "uses"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_6",
  "x": "On GEO/GEOlino S , the performance is comparable to the performance observed by <cite>Hancke et al. (2012)</cite> on the original GEO/GEOlino data. 11 As Table 7a shows, erroneous classifications are roughly balanced across both classes, showing that the model does not prefer one class over the other. When training a model using only the 20 most informative measures identified in Study 1, we reach an accuracy of 85.1%, i.e., the additional measures only account only for 3.3%.",
  "y": "similarities"
 },
 {
  "id": "2e967f8560ffdb216135ae387776eb_7",
  "x": "The model is based on a broad range of features that are highly informative for both data sets. This model is a valuable contribution since i) it is based on a considerably broader data basis than previous approaches to German readability, and ii) it successfully generalizes across the data sets, illustrating surprising robustness across rather different text types. The approach presented thus extends the state-of-the-art in <cite>Hancke et al. (2012)</cite> in terms of the breadth of features integrated and the accuracy and generalizability of the model -and provides two new data sources for this line of research.",
  "y": "extends"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_0",
  "x": "We observe at least three types of noise common in bug reports. First, many bug reports have many spelling, grammatical and sentence structure errors. To address this we extend a suitable stateof-the-art technique that is robust to such corpora, i.e. (<cite>Barzilay and McKeown, 2001</cite>) .",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_1",
  "x": "To address this we extend a suitable stateof-the-art technique that is robust to such corpora, i.e. (<cite>Barzilay and McKeown, 2001</cite>) . Second, many duplicate bug report families contain sentences that are not truly parallel. An example is shown in Table 1 (middle).",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_2",
  "x": "Paraphrases can be extracted from non-parallel corpora using contextual similarity (Lin, 1998) . They can also be obtained from parallel corpora if such data is available (<cite>Barzilay and McKeown, 2001</cite>; Ibrahim et al., 2003) . Recently, there are also a number of studies that extract paraphrases from multilingual corpora (Bannard and CallisonBurch, 2005; Zhao et al., 2008) .",
  "y": "background"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_3",
  "x": "Recently, there are also a number of studies that extract paraphrases from multilingual corpora (Bannard and CallisonBurch, 2005; Zhao et al., 2008) . The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. Due to this reason, we build our technique on top of <cite>theirs</cite>.",
  "y": "background"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_4",
  "x": "Recently, there are also a number of studies that extract paraphrases from multilingual corpora (Bannard and CallisonBurch, 2005; Zhao et al., 2008) . The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. Due to this reason, we build our technique on top of <cite>theirs</cite>.",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_5",
  "x": "The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. The following provides a summary of <cite>their technique</cite>.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_6",
  "x": "The approach in (<cite>Barzilay and McKeown, 2001</cite>) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. <cite>The authors</cite> first used identical words and phrases as seeds to find and score contextual patterns.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_7",
  "x": "Our similarity score is based on the number of common words, bigrams and trigrams shared between two parallel sentences. We use a threshold of 5 to filter out non-parallel sentences. Global Context-Based Scoring Our contextbased paraphrase scoring method is an extension of (<cite>Barzilay and McKeown, 2001</cite> ) described in Sec. 2.",
  "y": "extends"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_8",
  "x": "In (<cite>Barzilay and McKeown, 2001</cite> ), a paraphrase is reported as long as there is a single good supporting pair of sentences. Although <cite>this</cite> works well for a relatively clean parallel corpus considered in their work, i.e., novels, <cite>this</cite> does not work well for bug reports. Consider the context-peculiar example in Table 1 (bottom).",
  "y": "background"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_9",
  "x": "At times, some words might be detected as paraphrases incidentally due to the noise. In (<cite>Barzilay and McKeown, 2001</cite> ), a paraphrase is reported as long as there is a single good supporting pair of sentences. Although <cite>this</cite> works well for a relatively clean parallel corpus considered in their work, i.e., novels, <cite>this</cite> does not work well for bug reports.",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_10",
  "x": "where n is the number of parallel bug reports with the two phrases occurring in parallel, and s i is the score for the i'th occurrence. s i is computed as follows: 1. We compute the set of patterns with affixed pattern scores based on (<cite>Barzilay and McKeown, 2001</cite> ).",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_11",
  "x": "All possible pairings of chunks are then formed. This set of chunk pairs are later fed to the method in (<cite>Barzilay and McKeown, 2001</cite> ) to produce a set of patterns with affixed scores. With this we compute our global-context based scores.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_12",
  "x": "As the first step, we employ parallel sentence selection, described in Sec. 3, to remove nonparallel duplicate bug report pairs. After this step, we find 5,935 parallel duplicate bug report pairs. Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_13",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_14",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_15",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_16",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. In our approach, we first form chunk pairs from the 5,935 pairs of parallel sentences and then use the <cite>baseline approach</cite> at a low threshold to ob-tain patterns.",
  "y": "uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_17",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases. This number is too small for practical purposes. Therefore, we reduce the threshold to get more paraphrases.",
  "y": "motivation extends"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_18",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. We call <cite>it</cite> BL. As described in Sec. 2, <cite>BL</cite> utilizes a threshold to control the number of patterns mined. In the experiment, we find that running <cite>BL</cite> using <cite>their</cite> default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases. This number is too small for practical purposes.",
  "y": "motivation"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_19",
  "x": "In our approach, we first form chunk pairs from the 5,935 pairs of parallel sentences and then use the <cite>baseline approach</cite> at a low threshold to ob-tain patterns. Using these patterns we compute the global context-based scores S g . We also compute the co-occurrence scores S c .",
  "y": "extends uses"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_20",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. From the figure we can see that our holistic approach using global-context score to rank and co-occurrence score to filter (i.e., Rk-S g +Ft-S c ) has higher precision than the <cite>baseline approach</cite> (i.e., <cite>BL</cite>) in all ks.",
  "y": "differences"
 },
 {
  "id": "2f3e2c81bed66fd020731b2475bb98_21",
  "x": "Experimental Setup The baseline method we consider is the one in (<cite>Barzilay and McKeown, 2001</cite> ) without sentence alignment -as the bug reports are usually of one sentence long. Interestingly, the graph shows that using only one of the scores alone (i.e., Rk-S g and Rk-S c ) does not result in a significantly higher precision than the <cite>baseline approach</cite>.",
  "y": "similarities differences"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_0",
  "x": "Bilingual text embeddings, while displaying a wider applicability and versatility than the two other paradigms, still suffer from one important limitation: a bilingual supervision signal is required to induce shared cross-lingual semantic spaces. This supervision takes form of sentence-aligned parallel data [5] , pre-built word translation pairs [11,<cite> 19]</cite> or document-aligned comparable data [21] . 1 Recently, methods for inducing shared cross-lingual embedding spaces without the need for any bilingual signal (not even word translation pairs) have been proposed [1, 3] .",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_1",
  "x": "---------------------------------- **CROSS-LINGUAL WORD VECTOR SPACES** For our proposed CLIR models, we investigate cross-lingual embedding spaces produced with state-of-the-art representative methods requiring different amount and type of bilingual supervision: 1) document-aligned comparable data [21] , 2) word translation pairs <cite>[19]</cite> ; and 3) no bilingual data at all [3] .",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_2",
  "x": "**CROSS-LINGUAL EMBEDDINGS FROM WORD TRANSLATION PAIRS (CL-WT).** This class of models [1, 11,<cite> 19]</cite> focuses on learning the projections (i.e., mappings) between independently trained monolingual embedding spaces. Let { S w i } V S i =1 , S w i \u2208 R ds be the monolingual word embedding space of the source language L S with V S vectors, and { T w i } V T i =1 , T w i \u2208 R dt the monolingual space for the target language L T containing V T vectors; ds and dt are the respective space dimensionalities.",
  "y": "background"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_3",
  "x": "We therefore opt for the recent model of Smith et al. <cite>[19]</cite> to serve as a baseline, due to its competitive performance, large coverage, and readily available implementation. 2 Technically, the method of Smith et al. <cite>[19]</cite> learns two projection functions f S ( S |\u03b8 S ) and f S ( T |\u03b8 T ), projecting the source and target monolingual embedding spaces, respectively, to the new shared space. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_4",
  "x": "According to the comparative evaluation from [18] , all projectionbased methods for inducing cross-lingual embedding spaces perform similarly. We therefore opt for the recent model of Smith et al. <cite>[19]</cite> to serve as a baseline, due to its competitive performance, large coverage, and readily available implementation. 2 Technically, the method of Smith et al. <cite>[19]</cite> learns two projection functions f S ( S |\u03b8 S ) and f S ( T |\u03b8 T ), projecting the source and target monolingual embedding spaces, respectively, to the new shared space.",
  "y": "background"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_5",
  "x": "**CROSS-LINGUAL EMBEDDINGS WITHOUT BILINGUAL SUPERVISION (CL-UNSUP).** Most recently, Conneau et al. [3] have proposed an adversarial learning-based model in order to automatically, in a fully unsupervised fashion, create word translation pairs that can then be used to learn the same projection functions f S and f T as in the model of Smith et al. <cite>[19]</cite> . Let X be the set of all monolingual word embeddings from the source language, and Y the set of all target language embeddings. In the first, adversarial learning step, they jointly learn (1) the projection matrix W that maps one embedding space to the other and (2) the parameters of the discriminator model which, given an embedding vector (either W x where x \u2208 X , or \u2208 Y ) needs to predict whether it is an original vector from the target embedding space ( ),nor a vector from the source embedding space mapped via projection W to the target embedding space (W x).",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_6",
  "x": "In the first, adversarial learning step, they jointly learn (1) the projection matrix W that maps one embedding space to the other and (2) the parameters of the discriminator model which, given an embedding vector (either W x where x \u2208 X , or \u2208 Y ) needs to predict whether it is an original vector from the target embedding space ( ),nor a vector from the source embedding space mapped via projection W to the target embedding space (W x). The discriminator model is a multi-layer perceptron network. In the second step, the projection matrix W trained with adversarial objective is used to find the mutual nearest neighbors between the two vocabularies -this set of automatically obtained word translation pairs becomes a synthetic training set for the refined projection functions f S and f T computed via the SVD-based method similar to the previously described model of Smith et al. <cite>[19]</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_7",
  "x": "For the CL-CD embeddings, the BWESG model trains on full documentaligned Wikipedias 8 using SGNS with suggested parameters from prior work [22] : 15 negative samples, global decreasing learning rate is .025, subsampling rate is 1e \u2212 4, window size is 16. The CL-WT embeddings of Smith et al. <cite>[19]</cite> use 10K translation pairs obtained from Google Translate to learn the linear mapping functions. The CL-UNSUP training setup closely follows the default setup of Conneau et al. [3] : we refer the reader to the original 4 Note that with both variants of BWE-Agg, we effectively ignore both query and document terms that are not represented in the cross-lingual embedding space.",
  "y": "differences"
 },
 {
  "id": "320a5c79d9884e652c42f85847172b_8",
  "x": "The reported performance on bilingual lexicon extraction (BLE) using cross-lingual embedding spaces is also lower for EN-NL compared to EN-IT (see, e.g., <cite>[19]</cite> ). We observe the same pattern (4-5% lower BLE performance for EN-NL than for EN-IT) with the CL-UNSUP embedding spaces. The weighted variant of BWE-Agg (BWE-Agg-IDF) outperforms the simpler non-weighted summation model (BWE-Agg-Add) across the board.",
  "y": "similarities"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_0",
  "x": "However, a big question w.r.t to these systems is their ability to generalize outside the specific datasets they are trained and tested on. Recently, <cite>Glockner et al. (2018)</cite> have shown that state-of-the-art NLI systems break considerably easily when instead of tested on the original SNLI test set, they are tested on a test set which Preprint. Work in progress. is constructed by taking premises from the training set and creating several hypotheses from them by changing at most one word within the premise.",
  "y": "motivation"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_1",
  "x": "In this paper we show that NLI systems that have been very successful in specific NLI benchmarks, fail to generalize when trained on a specific NLI datset and then tested across different NLI benchmarks. The results we get are in line with <cite>Glockner et al. (2018)</cite> , showing that the generalization capability of the individual NLI systems is very limited, but, what is more, they further show the only system that was less prone to breaking in <cite>Glockner et al. (2018)</cite> , breaks in the experiments we have conducted as well. ----------------------------------",
  "y": "similarities differences extends"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_2",
  "x": "**RELATED WORK** The ability of NLI systems to generalize and related skepticism has been raised in a recent paper by <cite>Glockner et al. (2018)</cite> . There, the authors show that the generalization capabilities of stateof-the-art NLI systems, in cases where some kind of external lexical knowledge is needed, drops dramatically when the SNLI test set is replaced by a test set where the premise and the hypothesis are otherwise identical except for at most one word.",
  "y": "background motivation"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_3",
  "x": "KIM is particularly interesting in this context as it performed significantly better than other models in the Breaking NLI experiment conducted by <cite>Glockner et al. (2018)</cite> . For BiLSTM-max we used the Adam optimizer (Kingma and Ba, 2014) and a learning rate of 5e-4. The learning rate was decreased by the factor of 0.2 after each epoch if the model did not improve.",
  "y": "background"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_4",
  "x": "ESIM has the highest average drop of 27.0 points. In contrast to the findings of <cite>Glockner et al. (2018)</cite> , utilizing external knowledge did not improve the model's generalization capability, as KIM performed equally poorly across all dataset combinations. Also including a pretrained language model did not improve the results significantly.",
  "y": "differences"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_5",
  "x": "We experimented with five state-of-the-art models covering both sentence encoding approaches and cross-sentence attention models. For all the systems, the accuracy drops between 7.9-33.7 points (the average drop being 25.4 points), when testing with a test set drawn from a separate corpus from that of the training data, as compared to when the test and training data are splits from the same corpus. Our findings, together with the previous negative findings e.g. by <cite>Glockner et al. (2018)</cite> and Gururangan et al. (2018) , indicate that the current state-of-the-art neural network models fail to capture the semantics of NLI in a way that will enable them to generalize across different NLI situations.",
  "y": "similarities"
 },
 {
  "id": "35ef3eba487c3cd97d32210670678a_6",
  "x": "Our findings, together with the previous negative findings e.g. by <cite>Glockner et al. (2018)</cite> and Gururangan et al. (2018) , indicate that the current state-of-the-art neural network models fail to capture the semantics of NLI in a way that will enable them to generalize across different NLI situations. The results indicate two issues to be taken into consideration: a) using datasets involving a fraction of what NLI is, will fail when tested in datasets that are testing for a slightly different definition. This is evident when we move from the SNLI to the SICK dataset.",
  "y": "future_work"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_0",
  "x": "At the same time, previous research on PCFG parsing using treebank training data present PAR-SEVAL measures in comparing the parsing performance for different languages and annotation schemes, reporting a number of striking differences. For example, Levy and Manning (2003) , K\u00fcbler (2005) , and <cite>K\u00fcbler et al. (2006)</cite> highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks. In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004;<cite> K\u00fcbler et al., 2006)</cite> .",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_1",
  "x": "For example, Levy and Manning (2003) , K\u00fcbler (2005) , and <cite>K\u00fcbler et al. (2006)</cite> highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks. In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004;<cite> K\u00fcbler et al., 2006)</cite> . Previous work has compared the similar Negra and Tiger corpora of German to the very different T\u00fcBa-D/Z corpus.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_2",
  "x": "<cite>K\u00fcbler et al. (2006)</cite> compares the Negra and T\u00fcBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure. 1 Using the Stanford Parser (Klein and Manning, 2002) , which employs a factored PCFG and dependency model, they claim that the model trained on T\u00fcBa-D/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical function evaluations. Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from <cite>K\u00fcbler et al. (2006)</cite> .",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_3",
  "x": "1 Using the Stanford Parser (Klein and Manning, 2002) , which employs a factored PCFG and dependency model, they claim that the model trained on T\u00fcBa-D/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical function evaluations. Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from <cite>K\u00fcbler et al. (2006)</cite> . 2 In recent related work, Rehbein and van Genabith (2007a) demonstrate using the Tiger and T\u00fcBa-D/Z 1 The evaluation is based only on the grammatical function; it does not identify the dependency pair that it labels.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_4",
  "x": "2 While the focus of <cite>K\u00fcbler et al. (2006)</cite> is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results. This difference in focus and additional differences in experimental setup mean that a finegrained comparison of the results is inappropriate -the relevant point here is that the gap between the results (23% for subjects, 35% for accusative objects) warrants further attention in the context of comparing parsing results across corpora. corpora of German that PARSEVAL is inappropriate for comparisons of the output of PCFG parsers trained on different treebank annotation schemes because PARSEVAL scores are affected by the ratio of terminal to non-terminal nodes.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_5",
  "x": "2 While the focus of <cite>K\u00fcbler et al. (2006)</cite> is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results. This difference in focus and additional differences in experimental setup mean that a finegrained comparison of the results is inappropriate -the relevant point here is that the gap between the results (23% for subjects, 35% for accusative objects) warrants further attention in the context of comparing parsing results across corpora. corpora of German that PARSEVAL is inappropriate for comparisons of the output of PCFG parsers trained on different treebank annotation schemes because PARSEVAL scores are affected by the ratio of terminal to non-terminal nodes. A dependencybased evaluation on triples of the form word-POShead shows better results for the parser trained on Tiger even though the much lower PARSEVAL scores, if meaningful, would predict that the output for Tiger is of lower quality. However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalismindependent evaluation of parsers (e.g., Clark and Curran, 2007) . 3 Addressing these issues, we resolve the apparent discrepancy between <cite>K\u00fcbler et al. (2006)</cite> and Dubey (2004) and establish a firm grammatical function comparison of Negra and T\u00fcBa-D/Z. We also extend the evaluation to a labeled dependency evaluation based on grammatical relations for both corpora.",
  "y": "motivation"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_6",
  "x": "However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalismindependent evaluation of parsers (e.g., Clark and Curran, 2007) . 3 Addressing these issues, we resolve the apparent discrepancy between <cite>K\u00fcbler et al. (2006)</cite> and Dubey (2004) and establish a firm grammatical function comparison of Negra and T\u00fcBa-D/Z. We also extend the evaluation to a labeled dependency evaluation based on grammatical relations for both corpora. Such an evaluation, which abstracts away from the specifics of the annotation schemes, shows that, in contrast to the claims made in <cite>K\u00fcbler et al. (2006)</cite> , the parsing results for PCFG parsers trained on these heterogeneous corpora are very similar.",
  "y": "extends"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_7",
  "x": "3 Addressing these issues, we resolve the apparent discrepancy between <cite>K\u00fcbler et al. (2006)</cite> and Dubey (2004) and establish a firm grammatical function comparison of Negra and T\u00fcBa-D/Z. We also extend the evaluation to a labeled dependency evaluation based on grammatical relations for both corpora. Such an evaluation, which abstracts away from the specifics of the annotation schemes, shows that, in contrast to the claims made in <cite>K\u00fcbler et al. (2006)</cite> , the parsing results for PCFG parsers trained on these heterogeneous corpora are very similar. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_8",
  "x": "Following <cite>K\u00fcbler et al. (2006)</cite> , only sentences with fewer than 35 words were used, which results in 20,002 sentences for Negra and 21,365 sentences for T\u00fcBa-D/Z. Because punctuation is not attached within the sentence in the corpus annotation, punctuation was removed. To be able to train PCFG parsing models, it is necessary to convert the syntax graphs encoding trees with discontinuities in Negra into traditional syntax trees. Around 30% of sentences in Negra contain at least one discontinuity.",
  "y": "uses"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_9",
  "x": "To remove discontinuities, we used the conversion program included with the Negra corpus annotation tools (Brants and Plaehn, 2000) , the same tool used in <cite>K\u00fcbler et al. (2006)</cite> , which raises non-head elements to a higher tree until there are no more discontinuities. For example, for the discontinuous tree with a fronted object we saw in Figure 1 , the PP containing the fronted NP Dieser Meinung is raised to become a daughter of the top S node. 4 Additionally, the edge labels used in both corpora need to be folded into the node labels to become a part of context-free grammar rules used by a PCFG parser.",
  "y": "uses"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_10",
  "x": "Unlike <cite>K\u00fcbler et al. (2006)</cite> , which ignored edge labels on words, we incorporate all edge labels present in both corpora. As a consequence of this, providing a parser with perfect lexical tags would also provide the edge label for that word. T\u00fcBa-D/Z does not annotate grammatical functions other than HD on words, but Negra includes many grammatical functions on words.",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_11",
  "x": "Thus, it is useful to per-7 Our experimental setup is designed to support a comparison between Negra and T\u00fcBa-D/Z for the three evaluation metrics and is intended to be comparable to the setup of <cite>K\u00fcbler et al. (2006)</cite> . For Negra, Dubey (2004) explores a range of parsing models and the corpus preparation he uses differs from the one discussed in this paper so that a discussion of his results is beyond the scope of the corpus comparison in this paper. 8 Scores were calculated using evalb.",
  "y": "similarities"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_12",
  "x": "**GRAMMATICAL FUNCTION LABEL EVALUATION** <cite>K\u00fcbler et al. (2006)</cite> present the results shown in Table 3 for the parsing performance of the unlexicalized model of the Stanford Parser (Klein and Manning, 2002) . In this grammatical function label evaluation, T\u00fcBa-D/Z outperforms Negra for subjects, accusative objects, and dative objects based on an evaluation of phrasal arguments.",
  "y": "uses"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_14",
  "x": "This results in an unbalanced comparison between Negra and T\u00fcBa-D/Z since, as discussed in section 2, T\u00fcBa-D/Z includes unary-branching phrases above all single-word arguments whereas Negra does not. In effect, single-word arguments in Negra -mainly pronouns and bare nouns -are not considered in the evaluation from <cite>K\u00fcbler et al. (2006)</cite> . The result is thus a comparison of multiword arguments in Negra to both single-and multiword arguments in T\u00fcBa-D/Z. Recall from section 3.1 that this is not a minor difference: single-word arguments account for 38% of subjects, accusative objects, and dative objects in Negra.",
  "y": "background"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_15",
  "x": "This evaluation handles multiple categories of arguments, not just NPs, so it focuses solely on the grammatical function labels, ignoring the phrasal categories. For example, in Negra an NP-OA in a parse is considered a correct accusative object even if the OA label in the gold standard has the category MPN. The results are shown in Table 4 In contrast to the results for NP grammatical functions of <cite>K\u00fcbler et al. (2006)</cite> we saw in Table 3 , Negra and T\u00fcBa-D/Z perform quite similarly overall, with Negra slightly outperforming T\u00fcBa-D/Z for all types of arguments.",
  "y": "uses"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_16",
  "x": "These results also form a clear contrast to the PARSEVAL results we saw in Table 2 . Contrary to the finding in <cite>K\u00fcbler et al. (2006)</cite> , the PAR-SEVAL evaluation does not echo the grammatical function label evaluation. In keeping with the results from Rehbein and van Genabith (2007a) , we find that PARSEVAL is not an adequate predictor of performance in an evaluation targeting the functorargument structure of the sentence for comparisons between PCFG parsers trained on corpora with different annotation schemes.",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_17",
  "x": "Shifting the focus to the grammatical function evaluation, we showed that a grammatical function evaluation based on phrasal arguments as provided by <cite>K\u00fcbler et al. (2006)</cite> is inadequate for comparing parsers trained on the Negra and T\u00fcBa-D/Z corpora. By introducing non-branching phrase nodes above single-word arguments in Negra, it is possible to provide a balanced comparison for the grammatical function label evaluation between Negra and T\u00fcBa-D/Z on both phrasal and single-word arguments. The models trained on both corpora perform very similarly in the grammatical function evaluation, in contrast to the claims in <cite>K\u00fcbler et al. (2006)</cite> .",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_18",
  "x": "The models trained on both corpora perform very similarly in the grammatical function evaluation, in contrast to the claims in <cite>K\u00fcbler et al. (2006)</cite> . When the grammatical function label evaluation is extended into a labeled dependency evaluation by finding the verbal head to complete the labeled dependency triple, the parser trained on Negra outperforms that trained on T\u00fcBa-D/Z. The more significant drop in results for T\u00fcBa-D/Z compared to the grammatical function label evaluation may be due to the fact that a verbal lexical head in T\u00fcBa-D/Z is not in the same local tree as its dependents, whereas it is in Negra. The presence of intervening topological field nodes in T\u00fcBa-D/Z may make it difficult for the parser to consistently identify the elements of the dependency triple across several subtrees.",
  "y": "differences"
 },
 {
  "id": "366231b855f226f63d637e6b2e1667_19",
  "x": "Addressing the general question of how to compare parsing results for different annotation schemes, we revisited the comparison of PCFG parsing results for the Negra and T\u00fcBa-D/Z corpora. We show that these different annotation schemes lead to very significant differences in PARSEVAL scores for unlexicalized PCFG parsing models, but grammatical function label and labeled dependency evaluations for arguments of verbs show that this difference does not carry over to measures which are relevant to the semantic functor-argument structure. In contrast to <cite>K\u00fcbler et al. (2006)</cite> a grammatical function evaluation on subjects, accusative objects, and dative objects establishes that Negra and T\u00fcBa-D/Z perform similarly when all types of words and phrases appearing as arguments are taken into consideration.",
  "y": "differences"
 },
 {
  "id": "3816a122d7f0847c01415fadef2d3d_0",
  "x": "This should also allow us to integrate the advantages of the realizations into one generic parsing technique, which yields the further advancement of the whole parsing community. In this paper, we compare CFG filtering techniques for LTAG<cite> (Harbusch, 1990</cite>; Poller and Becker, 1998) and HPSG (Torisawa et al., 2000; Kiefer and Krieger, 2000) , following an approach to parsing comparison among different grammar formalisms ). The key idea of the approach is to use strongly equivalent grammars, which generate equivalent parse results for the same input, obtained by a grammar conversion as demonstrated by .",
  "y": "uses"
 },
 {
  "id": "3816a122d7f0847c01415fadef2d3d_1",
  "x": "In this section, we introduce a grammar conversion ) and CFG filtering<cite> (Harbusch, 1990</cite>; Poller and Becker, 1998; Torisawa et al., 2000; Kiefer and Krieger, 2000) . ---------------------------------- **GRAMMAR CONVERSION**",
  "y": "uses"
 },
 {
  "id": "3816a122d7f0847c01415fadef2d3d_2",
  "x": "In CFG filtering techniques for LTAG<cite> (Harbusch, 1990</cite>; Poller and Becker, 1998) , every branching of elementary trees in a given grammar is extracted as a CFG rule as shown in Figure 1 . ---------------------------------- **CFG RULES**",
  "y": "uses"
 },
 {
  "id": "3816a122d7f0847c01415fadef2d3d_3",
  "x": "This implies that successful context-free derivations obtained by CFG TNT basically involve elementary trees in which all substitution and adjunction have succeeded. However, CFG PB (also a CFG produced by the other work<cite> (Harbusch, 1990)</cite> ) cannot avoid generating invalid parse trees that connect two lo-cal structures where adjunction takes place between them. We measured with G 2-21 the proportion of the number of ok-prop between two node numbers of nodes that take adjunction and its success rate.",
  "y": "uses motivation"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_0",
  "x": "Text adventure games, in which players must make sense of the world through text descriptions and declare actions through natural language, can provide a stepping stone toward more realworld environments where agents must communicate to understand the state of the world and affect change in the world. Despite the steadily increasing body of research on text-adventure games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Tao et al., 2018;<cite> Ammanabrolu and Riedl, 2019)</cite> , and in addition to the ubiquity of deep reinforcement learning applications (Parisotto et al., 2016; Zambaldi et al., 2019) , teaching an agent to play text-adventure games remains a challenging task. Learning a control policy for a text-adventure game requires a significant amount of exploration, resulting in training runs that take hundreds of thousands of simulations (Narasimhan et al., 2015;<cite> Ammanabrolu and Riedl, 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_1",
  "x": "Text adventure games, in which players must make sense of the world through text descriptions and declare actions through natural language, can provide a stepping stone toward more realworld environments where agents must communicate to understand the state of the world and affect change in the world. Despite the steadily increasing body of research on text-adventure games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Tao et al., 2018;<cite> Ammanabrolu and Riedl, 2019)</cite> , and in addition to the ubiquity of deep reinforcement learning applications (Parisotto et al., 2016; Zambaldi et al., 2019) , teaching an agent to play text-adventure games remains a challenging task. Learning a control policy for a text-adventure game requires a significant amount of exploration, resulting in training runs that take hundreds of thousands of simulations (Narasimhan et al., 2015;<cite> Ammanabrolu and Riedl, 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_2",
  "x": "---------------------------------- **BACKGROUND AND RELATED WORK** Text-adventure games, in which an agent must interact with the world entirely through natural language, provide us with two challenges that have proven difficult for deep reinforcement learning to solve (Narasimhan et al., 2015; Haroush et al., 2018;<cite> Ammanabrolu and Riedl, 2019)</cite> : (1) The agent must act based only on potentially incomplete textual descriptions of the world around it.",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_3",
  "x": "Text-adventure games can be treated as partially observable Markov decision processes (POMDPs). This can be represented as a 7-tuple of S, T, A, \u2126, O, R, \u03b3 : the set of environment states, conditional transition probabilities between states, words used to compose text commands, observations, conditional observation probabilities, the reward function, and the discount factor respectively . Multiple recent works have explored the challenges associated with these games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Tao et al., 2018;<cite> Ammanabrolu and Riedl, 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_4",
  "x": "The more different the domain of the two games, the harder this task becomes. Previous work <cite>(Ammanabrolu and Riedl, 2019)</cite> introduced the use of knowledge graphs and questionanswering pre-training to aid in the problems of partial observability and a combinatorial action space. This work made use of a system called TextWorld that uses grammars to generate a series of similar (but not exact same) games.",
  "y": "background"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_5",
  "x": "The graph also makes a distinction with respect to items that are in the agent's possession or in their immediate surrounding environment. We make minor modifications to the rules used in<cite> Ammanabrolu and Riedl (2019)</cite> to better achieve such a graph in general interactive fiction environments. The agent also has access to all actions accepted by the game's parser, following Narasimhan et al. (2015) .",
  "y": "extends"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_6",
  "x": "These OBJ tags are then filled in by looking at all possible objects in the given vocabulary for the game. This action space is of the order of A = O(|V | \u00d7 |O| 2 ) where V is the number of action verbs, and O is the number of distinct objects in the world that the agent can interact with. As this is too large a space for a RL agent to effectively explore, the knowledge graph is used to prune this space by ranking actions based on their presence in the current knowledge graph and the relations between the objects in the graph as in <cite>(Ammanabrolu and Riedl, 2019)</cite> The architecture for the deep Q-network consists of two separate neural networks-encoding state and action separately-with the final Q-value for a state-action pair being the result of a pairwise interaction function between the two (Figure 1 ).",
  "y": "uses similarities"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_7",
  "x": "The portions that are pre-trained are the same parts of the architecture as in<cite> Ammanabrolu and Riedl (2019)</cite> . This game is referred to as the source task. The seeding of the knowledge graph is not strictly necessary but given that state-of-theart DRL agents cannot complete real games, this makes the agent more effective at the source task.",
  "y": "similarities uses"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_8",
  "x": "For the metrics tested after convergence, we set = 0.1 following both Narasimhan et al. (2015) and<cite> Ammanabrolu and Riedl (2019)</cite> . We use similar hyperparameters to those reported in <cite>(Ammanabrolu and Riedl, 2019)</cite> for training the KG-DQN with action pruning, with the main difference being that we use 100 dimensional word embeddings instead of 50 dimensions for the horror genre. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_9",
  "x": "For the metrics tested after convergence, we set = 0.1 following both Narasimhan et al. (2015) and<cite> Ammanabrolu and Riedl (2019)</cite> . We use similar hyperparameters to those reported in <cite>(Ammanabrolu and Riedl, 2019)</cite> for training the KG-DQN with action pruning, with the main difference being that we use 100 dimensional word embeddings instead of 50 dimensions for the horror genre. ----------------------------------",
  "y": "similarities differences"
 },
 {
  "id": "385ce03aee1e3d3de193de09fa1278_10",
  "x": "Following<cite> Ammanabrolu and Riedl (2019)</cite>, we use TextWorld's \"home\" theme to generate the games for the question-answering system. TextWorld is a framework that uses a grammar to randomly generate game worlds and quests. This framework also gives us information such as instructions on how to finish the quest, and a list of actions that can be performed at each step based on the current world state.",
  "y": "uses"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_0",
  "x": "Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations Cheng et al., 2018; Clinchant and Perronnin, 2013; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 . Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (Mitchell and Lapata, 2010) , it seems that more complex approaches usually yield better performance (Cheng et al., 2018; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Kiros et al., 2015; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 . To this end, we propose a simple yet effective approach for aggregating word embeddings into document embeddings.",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_1",
  "x": "In recent years, word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have had a huge impact in natural language processing (NLP) and related fields, being used in many tasks including sentiment analysis (Dos Santos and Gatti, 2014; Fu et al., 2018) , information retrieval (Clinchant and Perronnin, 2013; Ye et al., 2016) and word sense disambiguation (Bhingardive et al., 2015; Chen et al., 2014; Iacobacci et al., 2016) , among many others. Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations Cheng et al., 2018; Clinchant and Perronnin, 2013; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 . Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (Mitchell and Lapata, 2010) , it seems that more complex approaches usually yield better performance (Cheng et al., 2018; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Kiros et al., 2015; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 .",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_2",
  "x": "We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier, namely Support Vector Machines (SVM), and show that it is useful for a diverse set of text classification tasks. We consider five benchmark data sets: Reuters-21578 (Lewis, 1997) , RT-2k (Pang and Lee, 2004) , MR (Pang and Lee, 2005) , TREC (Li and Roth, 2002) and Subj (Pang and Lee, 2004) . We compare VLAWE with recent stateof-the-art methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 , demonstrating the effectiveness of our approach.",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_3",
  "x": "There are various works Cheng et al., 2018; Conneau et al., 2017; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Clinchant and Perronnin, 2013; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2018 ) that propose to build effective sentence-level or document-level representations based on word embeddings. While most of these approaches are based on deep learning (Cheng et al., 2018; Conneau et al., 2017; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Zhao et al., 2015; Zhou et al., 2018) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (Clinchant and Perronnin, 2013) . The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (J\u00e9gou et al., 2012) .",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_4",
  "x": "There are various works Cheng et al., 2018; Conneau et al., 2017; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Clinchant and Perronnin, 2013; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2018 ) that propose to build effective sentence-level or document-level representations based on word embeddings. While most of these approaches are based on deep learning (Cheng et al., 2018; Conneau et al., 2017; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Zhao et al., 2015; Zhou et al., 2018) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (Clinchant and Perronnin, 2013) . The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (J\u00e9gou et al., 2012) .",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_5",
  "x": "We set the number of clusters (size of the codebook) to k = 10, leading to a VLAWE representation of k \u00b7 d = 10 \u00b7 300 = 3000 components. Similar to J\u00e9gou et al. (2012) , we set \u03b1 = 0.5 for the power normalization step in Equation (4), which consistently leads to near-optimal results on all data sets. In the learning stage, we employ the Support Vector Machines (SVM) implementation provided by LibSVM (Chang and Lin, 2011 Table 1 : Performance results (in %) of our approach (VLAWE) versus several state-of-the-art methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 on the Reuters-21578, RT-2k, MR, TREC and Subj data sets.",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_6",
  "x": "For optimal results, the VLAWE representation is combined with the BOSWE representation , which is based on the PQ kernel Popescu, 2013, 2015b) . We follow the same evaluation procedure as Kiros et al. (2015) and<cite> Hill et al. (2016)</cite> , using 10-fold cross-validation when a train and test split is not pre-defined for a given data set. As evaluation metrics, we employ the micro-averaged F 1 measure for the Reuters-21578 data set and the standard classification accuracy for the RT-2k, the MR, the TREC and the Subj data sets, in order to fairly compare with the related art.",
  "y": "similarities uses"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_7",
  "x": "We compare VLAWE with several state-of-theart methods Cheng et al., 2018; Fu et al., 2018; <cite>Hill et al., 2016</cite>; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW). The corresponding results are presented in Table 1 . First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (Le and Mikolov, 2014;<cite> Hill et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "3bb6243de9f77fc6ebf2dc24de7faa_8",
  "x": "The corresponding results are presented in Table 1 . First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (Le and Mikolov, 2014;<cite> Hill et al., 2016)</cite> . In most cases, our improvements over the baselines are higher than 5%.",
  "y": "differences"
 },
 {
  "id": "3ebfa05038431571701a7199163832_0",
  "x": "Previous work has explored learnable alternatives to speech features that rely on a similar computation to spectral representations [19, 20, 21, <cite>22,</cite> 5] . These approaches learn convolutions that are then passed through a non-linearity, eventually a pooling operator and then a log compression to replicate the dynamic range compression typically performed on spectrograms or mel-filterbanks. This compression function remains fixed and is chosen beforehand, which could impact the final performance, as various compression functions including logarithm, cubic root, or 10th root have been previously showed to perform better depending on the task (see Table 2 of [23] ).",
  "y": "background"
 },
 {
  "id": "3ebfa05038431571701a7199163832_1",
  "x": "It has since then been used in production speech recognition systems [25] . In this work, we start from an attention-based model on mel-filterbanks, which already outperforms an equivalent model trained on low-level descriptors (LLDs). Our experiments show that by training a PCEN block on top of mel-filterbanks or replacing them by learnable time-domain filterbanks from<cite> [22]</cite> , we get a gain in accuracy around 10% in absolute when training an identical neural network for dysarthria detection.",
  "y": "uses"
 },
 {
  "id": "3ebfa05038431571701a7199163832_2",
  "x": "**TIME-DOMAIN FILTERBANKS** As the first step of our computational pipeline, we use TimeDomain filterbanks from<cite> [22]</cite> . Time-Domain filterbanks are neural network layers that take the raw waveform as input.",
  "y": "uses"
 },
 {
  "id": "3ebfa05038431571701a7199163832_3",
  "x": "is the waveform windowed with an Hanning function \u03c6 centered in t, (\u03c8 n ) n=1...N the N melfilters andf denotes the Fourier transform of f . [26] shows that these coefficient can be approximated in the time domain by the following computation, referred as the first order scattering transform: where (\u03d5 n ) n=1...N are Gabor wavelets defined in<cite> [22]</cite> such that |\u03c6 n | 2 \u2248 |\u03c8 n | 2 .<cite> [22]</cite> shows that this computation can be implemented as neural network layers, referred as TimeDomain filterbanks (TD-filterbanks).",
  "y": "uses background"
 },
 {
  "id": "3ebfa05038431571701a7199163832_4",
  "x": "When not combined with PCEN, a log-compression is added on top of TD-filterbanks after adding 1 to their absolute value to avoid numerical issues. Table 1 shows the detailed layers. Following<cite> [22]</cite> , the first 1D convolution filters are initialized with Gabor wavelets, to replicate mel-filterbanks, and are then learnt at the same time as the rest of the model.",
  "y": "uses"
 },
 {
  "id": "3fe979e570992b79c8656ab6cb34fb_0",
  "x": "We have used Lexicalized Tree Adjoining Grammar (LTAG) (Joshi, 1985; Schabes, 1990) to capture the syntax associated with each verb class, and have added semantic predicates. We also show how regular extensions of verb meaning can be achieved through the adjunction of particular syntactic phrases. We base these regular extensions on intersective Levin classes, a fine-grained variation on Levin classes, as a source of semantic components associated with specific adjuncts <cite>(Dang et al., 1998)</cite> .",
  "y": "uses background"
 },
 {
  "id": "3fe979e570992b79c8656ab6cb34fb_1",
  "x": "Push/Pull verbs can appear in the conative construction, which emphasizes their forceful semantic component and ability to express an attempted action where any result that might be associated with the verb is not necessarily achieved; Carry verbs (used with a goal or directional phrase) cannot take the conative alternation because this would conflict with the causation of motion which is the intrinsic meaning of the class <cite>(Dang et al., 1998)</cite> . Palmer et al. (1999) and Bleam et al. (1998) also defined compositional semantics for classes of verbs implemented in FB-LTAG, but they represented general semantic components (e.g., motion, manner) as features on the nodes of the trees. Our use of separate logical forms gives a more detailed semantics for the sentence, so that for an event involving motion, it is possible to know not only that the event has a motion semantic component, but also which entity is actually in motion.",
  "y": "background"
 },
 {
  "id": "40d73d5fc22686c13a14946946dd18_0",
  "x": "He et al. (2017) simplified their architecture using a highway Bi-LSTM network. More recently, <cite>Tan et al. (2018)</cite> replaced the common recurrent architecture with a self-attention network, directly capturing relationships between tokens regardless of their distance, resulting in better results and faster training. The work in deep end-to-end SRL has focused heavily on applying deep learning advances without considering the multilingual aspect.",
  "y": "background"
 },
 {
  "id": "40d73d5fc22686c13a14946946dd18_1",
  "x": "DAMESRL facilitates exploration and fair evaluation of new SRL models for different languages by providing flexible neural model construction on different modeling levels, the handling of various input and output formats, and clear output visualization. Beyond the existing state-of-the-art models (Zhou and Xu, 2015; He et al., 2017; <cite>Tan et al., 2018</cite> ), we exploit character-level modeling, beneficial when considering multiple languages. To demonstrate the merits of easy cross-lingual exploration and evaluation of model structures for SRL provided by DAMESRL, we report performance of several distinct models integrated into our framework for English, German and Arabic, as they have very different linguistic characteristics.",
  "y": "background"
 },
 {
  "id": "40d73d5fc22686c13a14946946dd18_2",
  "x": "Despite the foreseen importance, character-level embeddings have not been used in previous work (Zhou and Xu, 2015; He et al., 2017; <cite>Tan et al., 2018</cite>) . Phase II: As core sequence representation component, users can choose between a self-attention encoding (<cite>Tan et al., 2018</cite>) , a regular Bi-LSTM (Hochreiter and Schmidhuber, 1997) or a highway Bi-LSTM (Zhang et al., 2016; He et al., 2017) . Phase III: To compute model probabilities, users can choose a regular softmax, or a linear chain CRF as proposed by (Zhou and Xu, 2015) , which can be useful for languages where word order is an important SRL cue, such as English, or when less training data is available (shown in Section 4).",
  "y": "background"
 },
 {
  "id": "40d73d5fc22686c13a14946946dd18_3",
  "x": "DAMESRL provides a Bi-LSTM network to learn character-level word representations helping for languages where important SRL cues are given through inflections, such as case markings in German and Arabic. Despite the foreseen importance, character-level embeddings have not been used in previous work (Zhou and Xu, 2015; He et al., 2017; <cite>Tan et al., 2018</cite>) . Phase II: As core sequence representation component, users can choose between a self-attention encoding (<cite>Tan et al., 2018</cite>) , a regular Bi-LSTM (Hochreiter and Schmidhuber, 1997) or a highway Bi-LSTM (Zhang et al., 2016; He et al., 2017) .",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_0",
  "x": "We describe the submission from the Columbia Arabic & Dialect Modeling group (CADIM) for the Shared Task at the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL'2013). We participate in the Arabic Dependency parsing task for predicted POS tags and features. Our system is based on <cite>Marton et al. (2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_1",
  "x": "---------------------------------- **APPROACH** In this section, we summarize <cite>Marton et al. (2013)</cite> .",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_2",
  "x": "However, it had been more difficult showing that agreement morphology helps parsing, with negative results for dependency parsing in several languages Eryigit et al., 2008; Nivre, 2009) . In contrast to these negative results, <cite>Marton et al. (2013)</cite> showed positive results for using agreement morphology for Arabic. ----------------------------------",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_3",
  "x": "However, it had been more difficult showing that agreement morphology helps parsing, with negative results for dependency parsing in several languages Eryigit et al., 2008; Nivre, 2009) . In contrast to these negative results, <cite>Marton et al. (2013)</cite> showed positive results for using agreement morphology for Arabic. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_5",
  "x": "---------------------------------- **METHODOLOGY** In <cite>Marton et al. (2013)</cite> , we investigated morphological features for dependency parsing of Modern Standard Arabic (MSA).",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_6",
  "x": "In <cite>Marton et al. (2013)</cite> , we investigated morphological features for dependency parsing of Modern Standard Arabic (MSA). <cite>We</cite> used both the MaltParser (Nivre, 2008) and the Easy-First Parser (Goldberg and Elhadad, 2010) . Since the Easy-First Parser performed better, we use it in all experiments reported in this paper.",
  "y": "uses"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_7",
  "x": "However, often the functional morphology (which is relevant to agreement, and relates to the meaning of the word) is at odds with the \"surface\" (form-based) morphology; a well-known example of this are the \"broken\" (irregular) plurals of nominals, which often have singular-form morphemes but are in fact plurals and show plural agreement if the referent is rational. In <cite>Marton et al. (2013)</cite> , we showed that by modeling the functional morphology rather than the form-based morphology, we obtain a further increase in parsing performance (2013) test (old split) 81.0 84.0 92.7 Table 2 : Results of our system on Shared Task test data, Gold Tokenization, Predicted Morphological Tags; and for reference also on the data splits used in our previous work <cite>(Marton et al., 2013)</cite> ; \"\u2264 70\" refers to the test sentences with 70 or fewer words. Training Set Test Set Labeled Tedeval Score Unlabeled Tedeval Score 5K (SPMRL'2013) test \u2264 70 86.4 89.9 All (SPMRL'2013) test \u2264 70 87.8 90.8 Table 3 : Results of our system on on Shared Task test data, Predicted Tokenization, Predicted Morphological Tags; \"\u2264 70\" refers to the test sentences with 70 or fewer words (again, both when using gold and when using predicted POS and morphological features).",
  "y": "background"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_8",
  "x": "**BEST PERFORMING FEATURE SET** The best performing set of features on non-gold input, obtained in <cite>Marton et al. (2013)</cite> , are shown in Table 1 . The features are clustered into three types.",
  "y": "uses"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_9",
  "x": "The data split used in the shared task is different from the data split we used in <cite>(Marton et al., 2013)</cite> , so we retrained our models on the new splits (Diab et al., 2013) . The data released for the Shared Task showed inconsistent availability of lemmas across gold and predicted input, so we used the ALMOR analyzer (Habash, 2007) with the SAMA databases (Graff et al., 2009 ) to determine a lemma given the word form and the provided (gold or predicted) POS tags. In addition to the lemmas, the ALMOR analyzer also provides morphological features in the feature-value representation our approach requires.",
  "y": "differences"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_10",
  "x": "For predicted tokenization, only the IMS/Szeged system which uses system combination (Run 2) outperformed our parser on all measures; our parser performed better than all other single-parser systems. For gold tokenization, our system is the second best single-parser system after the IMS/Szeged single system (Run 1). For gold tokenization and predicted morphology (Table 2) , we also give the performance reported in our previous work <cite>(Marton et al., 2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "4176674f83dec5389a23d9d45654c7_11",
  "x": "For predicted tokenization, only the IMS/Szeged system which uses system combination (Run 2) outperformed our parser on all measures; our parser performed better than all other single-parser systems. For gold tokenization, our system is the second best single-parser system after the IMS/Szeged single system (Run 1). For gold tokenization and predicted morphology (Table 2) , we also give the performance reported in our previous work <cite>(Marton et al., 2013)</cite> .",
  "y": "differences"
 },
 {
  "id": "41bd8c692ac513b8a9cabbd5aafbda_0",
  "x": "How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, 9 ]. The word2vec <cite>[10]</cite> is among the most widely used word embedding models today.",
  "y": "background"
 },
 {
  "id": "41bd8c692ac513b8a9cabbd5aafbda_1",
  "x": "The word2vec <cite>[10]</cite> is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words.",
  "y": "background"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_0",
  "x": "**INTRODUCTION** PredPatt 1 <cite>(White et al., 2016</cite> ) is a pattern-based framework for predicate-argument extraction. It defines a set of interpretable, extensible and non-lexicalized patterns based on Universal Dependencies (UD) (de Marneffe et al., 2014) , and extracts predicates and arguments through these manual patterns.",
  "y": "background"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_1",
  "x": "Zhang et al. (2017) adapts PredPatt to data generation for cross-lingual open information extraction. However, the evaluation of PredPatt has been restricted to manually-checked extractions over a small set of sentences<cite> (White et al., 2016)</cite> , which lacks gold annotations to conduct an objective and reproducible evaluation, and inhibits the updates of patterns in PredPatt. Chris , the designer , wants to launch a new brand .",
  "y": "background motivation"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_2",
  "x": "However, the evaluation of PredPatt has been restricted to manually-checked extractions over a small set of sentences<cite> (White et al., 2016)</cite> , which lacks gold annotations to conduct an objective and reproducible evaluation, and inhibits the updates of patterns in PredPatt. In this work, we aim to conduct a large-scale and reproducible evaluation of PredPatt by introducing a large set of gold annotations gathered from PropBank (Palmer et al., 2005) .",
  "y": "extends motivation"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_3",
  "x": "PredPatt extracts predicates and arguments in four stages<cite> (White et al., 2016)</cite> : (1) predicate and argument root identification, (2) argument resolution, (3) predicate and argument phrase extraction, and (4) optional post-processing. We analyze PredPatt extraction in each of these stages on the held-out set, and make 19 improvements to PredPatt patterns. Due to lack of space, we only highlight one improvement for each stage below.",
  "y": "background"
 },
 {
  "id": "4235dbd05a848d934f17f35894c051_4",
  "x": "We then update the existing PredPatt patterns and introduce new patterns by analyzing PredPatt annotations on the held-out set. PredPatt extracts predicates and arguments in four stages<cite> (White et al., 2016)</cite> : (1) predicate and argument root identification, (2) argument resolution, (3) predicate and argument phrase extraction, and (4) optional post-processing. We analyze PredPatt extraction in each of these stages on the held-out set, and make 19 improvements to PredPatt patterns.",
  "y": "extends background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_0",
  "x": "---------------------------------- **INTRODUCTION** Self-attention networks (SANs) (Parikh et al., 2016; Lin et al., 2017) have shown promising empirical results in various natural language processing (NLP) tasks, such as machine translation <cite>(Vaswani et al., 2017)</cite> , natural language inference (Shen et al., 2018a) , and acoustic modeling (Sperber et al., 2018) .",
  "y": "background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_1",
  "x": "One appealing strength of SANs lies in their ability to capture dependencies regardless of distance by explicitly attending to all the elements. In addition, the performance of SANs can be improved by multi-head attention <cite>(Vaswani et al., 2017)</cite> , which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace. Despite their success, SANs have two major limitations.",
  "y": "background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_2",
  "x": "We expect that the interaction across different subspaces can further improve the performance of SANs. We evaluate the effectiveness of the proposed model on three widely-used translation tasks: WMT14 English-to-German, WMT17 Chineseto-English, and WAT17 Japanese-to-English. Experimental results demonstrate that our approach consistently improves performance over the strong TRANSFORMER model <cite>(Vaswani et al., 2017)</cite> across language pairs.",
  "y": "differences"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_3",
  "x": "where where ATT(\u00b7) is an attention model (Bahdanau et al., 2015;<cite> Vaswani et al., 2017)</cite> that retrieves the keys K h with the query q h i . The final output representation O is the concatenation of outputs generated by multiple attention models:",
  "y": "background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_4",
  "x": "To make a fair comparison, we re-implemented the above approaches under a same framework. Empirical results on machine translation tasks show the superiority of our approach in both translation quality and training efficiency. Multi-Head Attention Multi-head attention mechanism <cite>(Vaswani et al., 2017)</cite> employs different attention heads to capture distinct features (Raganato and Tiedemann, 2018) .",
  "y": "background"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_5",
  "x": "We conducted experiments with the Transformer model <cite>(Vaswani et al., 2017)</cite> on English\u21d2German (En\u21d2De), Chinese\u21d2English (Zh\u21d2En) and Japanese\u21d2English (Ja\u21d2En) translation tasks. For the En\u21d2De and Zh\u21d2En tasks, the models were trained on widely-used WMT14 and WMT17 corpora, consisting of around 4.5 and 20.62 million sentence pairs, respectively. Concerning Ja\u21d2En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs.",
  "y": "uses"
 },
 {
  "id": "43622e43d6ef5291b64320d2d68b95_6",
  "x": "Prior studies revealed that modeling locality in lower layers can achieve better performance (Shen et al., 2018b; Yu et al., 2018; , we applied our approach to the lowest three layers of the encoder. About configurations of NMT models, we used the Base and Big settings same as<cite> Vaswani et al. (2017)</cite> , and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_0",
  "x": "Our interest here is the ability of capturing hierarchical structure without being equipped with explicit structural representations <cite>(Bowman et al., 2015b</cite>; Tran et al., 2016; Linzen et al., 2016) . We choose Transformer as a non-recurrent model to study in this paper. We refer to Transformer as Fully Attentional Network (FAN) to emphasize this characteristic.",
  "y": "uses"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_1",
  "x": "We choose two tasks to study in this work: (1) subject-verb agreement, and (2) logical inference. The first task was proposed by Linzen et al. (2016) to test the ability of recurrent neural networks to capture syntactic dependencies in natural language. The second task was introduced by<cite> Bowman et al. (2015b)</cite> to compare tree-based recursive neural networks against sequence-based recurrent networks with respect to their ability to exploit hierarchical structures to make accurate inferences.",
  "y": "uses"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_2",
  "x": "**LOGICAL INFERENCE** In this task, we choose the artificial language introduced by<cite> Bowman et al. (2015b)</cite> . The vocabulary of this language includes six word types {a, b, c, d, e, f } and three logical operators {or, and, not}. The task consists of predicting one of seven mutually exclusive logical relations that describe the relationship between a pair of sentences: entailment ( , ), equivalence (\u2261), exhaustive and non-exhaustive contradiction ( \u2227 , |), and two types of semantic independence (#, ).",
  "y": "uses"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_3",
  "x": "---------------------------------- **MODELS** We follow the general architecture proposed in<cite> (Bowman et al., 2015b)</cite> : Premise and hypothesis sentences are encoded by fixed-size vectors.",
  "y": "uses"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_4",
  "x": "The LSTM architecture used in this experiment is similar to that of<cite> Bowman et al. (2015b)</cite> . We simply take the last hidden state of the top LSTM layer as a fixed-size vector representation of the sentence. Here, we use a 2-layer LSTM with skip connections.",
  "y": "similarities"
 },
 {
  "id": "4590b1a4a0566915a6f2d6439a4e8a_5",
  "x": "Following the experimental protocol of<cite> Bowman et al. (2015b)</cite> , the data is divided into 13 bins based on the number of logical operators. Both FANs and LSTMs are trained on samples with at most n logical operators and tested on all bins. Figure 4 shows the result of the experiments with n \u2264 6 and n \u2264 12.",
  "y": "uses"
 },
 {
  "id": "45d804ec30d20bd7e484c3bbd8399f_0",
  "x": "We use two dependency relation extraction methods to extract dependency relations from each self-attention heads of BERT and RoBERTa. The first method-maximum attention weight (MAX)-designates the word with the highest incoming attention weight as the parent, and is meant to identify specialist heads that track specific dependencies like obj (in the style of <cite>Clark et al., 2019)</cite> . The second-maximum spanning tree (MST)-computes a maximum spanning tree over the attention matrix, and is meant to identify generalist heads that can form complete, syntactically informative dependency trees.",
  "y": "uses background"
 },
 {
  "id": "45d804ec30d20bd7e484c3bbd8399f_1",
  "x": "In contrast to probing models (Adi et al., 2017; Conneau et al., 2018) , our methods require no further training. In prior work,<cite> Clark et al. (2019)</cite> find that some heads of BERT exhibit the behavior of some dependency relation types, though they do not perform well at all types of relations in general. We are able to replicate their results on BERT using our MAX method.",
  "y": "extends background motivation"
 },
 {
  "id": "45d804ec30d20bd7e484c3bbd8399f_2",
  "x": "They find that the best dependency score is not significantly higher than a right-branching tree baseline. Voita et al. (2019) find the most confident attention heads of the Transformer NMT encoder based on a heuristic of the concentration of attention weights on a single token, and find that these heads mostly attend to relative positions, syntactic relations, and rare words. Additionally, researchers have investigated the syntactic knowledge that BERT learns by analyzing the contextualized embeddings (Warstadt et al., 2019a) and attention heads of BERT<cite> (Clark et al., 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "45d804ec30d20bd7e484c3bbd8399f_3",
  "x": "Method 1: Maximum Attention Weights (MAX) Given a token A in a sentence, a selfattention mechanism is designed to assign high attention weights on tokens that have some kind of relationship with token A (Vaswani et al., 2017) . Therefore, for a given token A, a token B that has the highest attention weight with respect to the token A should be related to token A. Our aim is to investigate whether this relation maps to a universal dependency relation. We assign a relation (w i , w j ) between word w i and w j if j = argmax W [i] for each row (that corresponds to a word in attention matrix) i in attention matrix W . Based on this simple strategy, we extract relations for all sentences in our evaluation datasets. This method is similar to<cite> Clark et al. (2019)</cite> , and attempts to recover individual arcs between words; the relations extracted using this method need not form a valid tree, or even be fully connected, and the resulting edge directions may or may not match the canonical directions. Hence, we evaluate the resulting arcs individually and ignore their direction. After extracting dependency relations from all heads at all layers, we take the maximum UUAS over all relations types.",
  "y": "extends similarities"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_0",
  "x": "To test whether BERT is sensitive to agreement relations, we use the cloze test (Taylor, 1953 , also called the \"masked language model\" objective), in which we mask out one of two words in an agreement relation and ask BERT to predict the masked word, one of the two tasks on which BERT is initially trained. <cite>Goldberg (2019)</cite> adapted the experimental setup of Linzen et al. (2016) , Gulordava et al. (2018) and Marvin and Linzen (2018) to use the cloze test to assess BERT's sensitivity to number agreement in English subject-verb agreement relations. The results showed that the single-language BERT model performed surprisingly well at this task (above 80% accuracy in all experiments), even when there were multiple \"distractors\" in the sentence (other nouns that differed from the subject in number).",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_1",
  "x": "To what extent does<cite> Goldberg's (2019)</cite> result hold for subject-verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations? Building on<cite> Goldberg's (2019)</cite> work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging examples. In Section 2, we define what is meant by agreement relations and outline the particular agreement relations under study.",
  "y": "motivation"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_2",
  "x": "To what extent does<cite> Goldberg's (2019)</cite> result hold for subject-verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations? Building on<cite> Goldberg's (2019)</cite> work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging examples. In Section 2, we define what is meant by agreement relations and outline the particular agreement relations under study.",
  "y": "extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_3",
  "x": "(2) Les cl\u00e9s de la porte se trouvent sur la table. 'The keys to the door are on the table.' 'The keys to the door are broken. ' Previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject-verb agreement in number<cite> (Goldberg, 2019</cite>; Gulordava et al., 2018; Linzen et al., 2016) .",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_4",
  "x": "'The keys to the door are on the table.' 'The keys to the door are broken. ' Previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject-verb agreement in number<cite> (Goldberg, 2019</cite>; Gulordava et al., 2018; Linzen et al., 2016) . In our work, we study all four types of agreement relations and all four features discussed above.",
  "y": "uses"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_5",
  "x": "' Previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject-verb agreement in number<cite> (Goldberg, 2019</cite>; Gulordava et al., 2018; Linzen et al., 2016) . In our work, we study all four types of agreement relations and all four features discussed above. Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model (Hewitt and Manning, 2019;<cite> Goldberg, 2019</cite>; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; Clark et al., 2019) .",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_6",
  "x": "In our work, we study all four types of agreement relations and all four features discussed above. Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model (Hewitt and Manning, 2019;<cite> Goldberg, 2019</cite>; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; Clark et al., 2019) . We expand this line of work to 26 languages.",
  "y": "extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_7",
  "x": "**EXPERIMENT** Our experiment is designed to measure BERT's ability to model syntactic structure. Our experimental set up is an adaptation of that of <cite>Goldberg (2019)</cite> .",
  "y": "extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_8",
  "x": "As in previous work, we mask one word involved in an agreement relation and ask BERT to predict it. <cite>Goldberg (2019)</cite> , following Linzen et al. (2016) , considered a correct prediction to be one in which the masked word receives a higher probability than other inflected forms of the lemma. For example, when dogs is masked, a correct response gives more probability to dogs than dog.",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_9",
  "x": "<cite>Goldberg (2019)</cite> , following Linzen et al. (2016) , considered a correct prediction to be one in which the masked word receives a higher probability than other inflected forms of the lemma. This evaluation leaves open the possibility that selectional restrictions or frequency are responsible for the results rather than sensitivity to syntactic structure (Gulordava et al., 2018) . To remove this possibility, we take into account all words of the same part-of-speech as the masked word.",
  "y": "differences"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_10",
  "x": "We ignore cloze examples in which there are fewer than 10 possible correct and 10 incorrect answers in our feature data. The average example in our cloze data is evaluated using 1,468 words, compared with 2 in <cite>Goldberg (2019)</cite> . Following <cite>Goldberg (2019)</cite>, we use the pretrained BERT models from the original authors 2 , but through the PyTorch implementation.",
  "y": "differences"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_11",
  "x": "Following <cite>Goldberg (2019)</cite>, we use the pretrained BERT models from the original authors 2 , but through the PyTorch implementation. 3 <cite>Goldberg (2019)</cite> showed that in his experiments the base BERT model performed better than the larger model, so we restrict our attention to the base model. For English, we use the model trained only on English data, whereas for all other languages we use the multilingual model.",
  "y": "uses"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_12",
  "x": "Following <cite>Goldberg (2019)</cite>, we use the pretrained BERT models from the original authors 2 , but through the PyTorch implementation. 3 <cite>Goldberg (2019)</cite> showed that in his experiments the base BERT model performed better than the larger model, so we restrict our attention to the base model. For English, we use the model trained only on English data, whereas for all other languages we use the multilingual model.",
  "y": "uses"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_13",
  "x": "Yet current state-of-the-art models in language representations, such as BERT, do not have explicit syntactic structural representations. Previous work by <cite>Goldberg (2019)</cite> showed that BERT captures English subject-verb number agreement well despite this lack of explicit structural representation. We replicated this result using a different evaluation methodology that addresses shortcomings in the original methodology and expanded the study to 26 languages.",
  "y": "background"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_14",
  "x": "Previous work by <cite>Goldberg (2019)</cite> showed that BERT captures English subject-verb number agreement well despite this lack of explicit structural representation. We replicated this result using a different evaluation methodology that addresses shortcomings in the original methodology and expanded the study to 26 languages. Our study further broadened existing work by considering the most cross-linguistically common agreement types as well as the most common morphosyntactic features.",
  "y": "extends"
 },
 {
  "id": "46050691971ea46ce7e18fef5f6d2d_15",
  "x": [
   "First, in certain languages some of the cloze examples we studied contain redundant information. Even when one word from an agreement relation is masked out, other cues remain in the sentence (e.g. when masking out the noun for a French attributive adjective agreement relation, number information is still available from the determiner). To counter this in future work, we plan to run our experiment twice, masking out the controller and then the target."
  ],
  "y": "differences"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_0",
  "x": "Increasingly complex neural networks have achieved highly competitive results for many NLP tasks (Vaswani et al., 2017; Devlin et al., 2018) , but they prevent human experts from understanding how and why a prediction is made. Understanding how a prediction is made can be very important for certain domains, such as the medical domain. Recent research has started to investigate models with self-explaining capability, i.e. extracting evidence to support their final predictions (Li et al., 2015; Lei et al., 2016;<cite> Lin et al., 2017</cite>; Mullenbach et al., 2018) .",
  "y": "background"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_1",
  "x": "Two methods have been proposed on how to jointly provide highlights along with classification. (1) an extraction-based method (Lei et al., 2016) , which first extracts evidences from the original text and then makes a prediction solely based on the extracted evidences; (2) an attentionbased method <cite>(Lin et al., 2017</cite>; Mullenbach et al., 2018) , which leverages the self-attention mechaMedical Report: The patient was admitted to the Neurological Intensive Care Unit for close observation. She was begun on heparin anticoagulated carefully secondary to the petechial bleed .",
  "y": "background"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_2",
  "x": [
   "nism to show the importance of basic units (words or ngrams) through their attention weights. However, previous work has several limitations. Lin et al. (2017) , for example, take single words as basic units, while meaningful information is usually carried by multi-word phrases."
  ],
  "y": "background motivation"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_3",
  "x": "Another issue of<cite> Lin et al. (2017)</cite> is that their attention model is applied on the representation vectors produced by an LSTM. Each LSTM output contains more than just the information of that position, thus the real range for the highlighted position is unclear. Mullenbach et al. (2018) defines all 4-grams of the input text as basic units and uses a convolutional layer to learn their representations, which still suffers from fixed-length highlighting.",
  "y": "background motivation"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_4",
  "x": "Our work leverages the attention-based selfexplaining method<cite> (Lin et al., 2017)</cite> , as shown in Figure 1 . First, our text encoder ( \u00a73) formulates an input text into a list of basic units, learning a vector representation for each, where the basic units can be words, phrases, or arbitrary ngrams. Then, the attention mechanism is leveraged over all basic units, and sums up all unit representations based on the attention weights {\u03b1 1 , ..., \u03b1 n }. Eventually, the attention weight \u03b1 i will be used to reveal how important a basic unit h i is. The last prediction layer takes the fixed-length text representation t as input, and makes the final prediction.",
  "y": "uses"
 },
 {
  "id": "46cc0df5c6ed25f735cc0afd301ec8_5",
  "x": "**BASELINES:** We compare two types of baseline text encoders in Figure 1 . (1)<cite> Lin et al. (2017)</cite> (BiLSTM), which formulates single word positions as basic units, and computes the vector h i for the i-th word position with a BiLSTM; (2) Extension of Mullenbach et al. (2018) (CNN) . The original model in (Mullenbach et al., 2018) only utilizes 4-grams.",
  "y": "uses"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_0",
  "x": "The learning algorithms applied including: decision tree, decisionlist [15] , neural networks [7] , na\u00efve Bayesian learning ( [5] , <cite>[11]</cite> ) and maximum entropy [10] . Among these leaning methods, the most important issue is what features will be used to construct the classifier. It is common in WSD to use contextual information that can be found in the neighborhood of the ambiguous word in training data ( [6] , [16] [17] [18] ).",
  "y": "uses"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_1",
  "x": "A useful result from this work based on (about one million words) the tagged People's Daily News shows that adding more features from richer levels of linguistic information such as PoS tagging yielded no significant improvement (less than 1%) over using only the bi-gram co-occurrences information. Another similar study for Chinese <cite>[11]</cite> is based on the Naive Bayes classifier model which has taken into consideration PoS with position information and bi-gram templates in the local context. The system has a reported 60.40% in both precision and recall based on the SENSEVAL-3 Chinese training data.",
  "y": "background"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_2",
  "x": "Another similar study for Chinese <cite>[11]</cite> is based on the Naive Bayes classifier model which has taken into consideration PoS with position information and bi-gram templates in the local context. Even though in both approaches, statistically significant bi-gram co-occurrence information is used, they are not necessarily true collocations. In our system, we do not rely on co-occurrence information.",
  "y": "differences background"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_3",
  "x": "Niu <cite>[11]</cite> proved in his experiments that Na\u00efve Bayes classifier achieved best disambiguation accuracy with small topical context window size (< 10 words). We follow their method and set the contextual window size as 10 in our system. Each of the Chinese words except the stop words inside the window range will be considered as one topical feature.",
  "y": "background"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_4",
  "x": "**TOPICAL CONTEXTUAL FEATURES** Niu <cite>[11]</cite> proved in his experiments that Na\u00efve Bayes classifier achieved best disambiguation accuracy with small topical context window size (< 10 words). We follow their method and set the contextual window size as 10 in our system.",
  "y": "uses background"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_5",
  "x": "The sources of the collocations will be explained in Section 4.1. In both Niu <cite>[11]</cite> and Dang's [10] work, topical features as well as the so called collocational features were used. However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features.",
  "y": "background"
 },
 {
  "id": "4705e8c0bac7bf29d8ef3193cf729b_6",
  "x": "In both Niu <cite>[11]</cite> and Dang's [10] work, topical features as well as the so called collocational features were used. Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD.",
  "y": "differences background"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_0",
  "x": "This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017;<cite> Ortega and Vu, 2017)</cite> . The main contributions of the paper are:",
  "y": "motivation"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_1",
  "x": "\u2022 MRDA: ICSI Meeting Recorder Dialog Act Corpus (Adam et al., 2003; Shriberg et al., 2004 ) is a dialog corpus of multiparty meetings with 5 tags of dialog acts. Table 1 summarizes dataset statistics. We use the train, validation and test splits as defined in (Lee and Dernoncourt, 2016;<cite> Ortega and Vu, 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_2",
  "x": "Therefore, we compare our research against these works. According to <cite>(Ortega and Vu, 2017)</cite> , prior work by (Ji and Bilmes, 2006) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly. For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work.",
  "y": "background"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_3",
  "x": "This is very impressive given that we work with very small memory footprint and we do not rely on pre-trained word embeddings. Our study also shows that the proposed method is very effective for such natural language tasks compared to more complex neural network architectures such as deep CNN (Lee and Dernoncourt, 2016) and RNN variants (Khanpour et al., 2016;<cite> Ortega and Vu, 2017)</cite> . We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing lowdimensional semantic text representations that are useful for text classification applications.",
  "y": "differences"
 },
 {
  "id": "473cf4603dea14ff89ca12d6e0cb50_5",
  "x": "Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods (Lee and Dernoncourt, 2016; Khanpour et al., 2016;<cite> Ortega and Vu, 2017)</cite> . We introduced a compression technique that effectively captures low-dimensional semantic representation and produces compact models that significantly save on storage and computational cost. Our approach does not rely on pre-trained embeddings and efficiently computes the projection vectors on the fly.",
  "y": "differences"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_0",
  "x": "We show how the theoretical computations of Coecke et al. (2010) instantiate in this concrete setting, and how the Frobenius Algebras, originating from group theory (Frobenius, 1903) and later extended to vector spaces (Coecke et al., 2008) , allow us to not only represent meanings of words with complex roles, such as verbs, adjectives, and prepositions, in an intuitive relational manner, but also to stay faithful to their original linguistic types. Equally as importantly, this model enables us to realize the concrete computations in lower dimensional spaces, thus reduce the space complexity of the implementation. We experiment in two different tasks with promising results: First, we repeat the disambiguation experiment of <cite>Grefenstette and Sadrzadeh (2011a)</cite> for transitive verbs.",
  "y": "similarities uses"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_1",
  "x": "The work of <cite>Grefenstette and Sadrzadeh (2011a)</cite> was the first large-scale practical implementation of this framework for intransitive and transitive sentences, and thus a first step towards providing some concrete answers to these questions. Following ideas from formal semantics that verbs are actually relations, the authors argue that the distributional meaning of a verb is a weighted relation representing the extent according to which the verb is related to its subjects and objects. In vector spaces, these relations are represented by linear maps, equivalent to matrices for the case of binary relations and to tensors for relations of arity n. Hence transitive verbs can be represented by matrices created by structurally mixing and summing up all the contexts (subject and object pairs) in which the verb appears.",
  "y": "background"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_2",
  "x": "An important consequence of our design decision is that it enables us to reduce the space complexity of the implementation from \u0398(d n )<cite> (Grefenstette and Sadrzadeh, 2011a)</cite> to \u0398(d), making the problem much more tractable. What remains to be solved is a theoretical issue, that in practice the meaning of relational words such as 'chase' as calculated by Equation 5 is a matrix living in N 2 -however, the mathematical framework above prescribes that it should be a rank-3 tensor in N 3 . The necessary expansions are achieved by using Frobenius algebraic operations, for which the following sections first provide the mathematical definitions and then a linguistic justification.",
  "y": "extends differences"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_3",
  "x": "In this notation, each tensor is represented by a triangle, and its rank can be determined by the outgoing wires. The tensor product is depicted as juxtaposition of triangles. We also remind to the reader that the relational method for constructing a tensor for the meaning of a verb<cite> (Grefenstette and Sadrzadeh, 2011a)</cite> provides us with a matrix in N 2 .",
  "y": "similarities"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_4",
  "x": "Furthermore, note that the nesting problem of <cite>Grefenstette and Sadrzadeh (2011a)</cite> does not arise here, since the linguistic and concrete types are the same. ---------------------------------- **EXPERIMENTS**",
  "y": "differences"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_5",
  "x": "We train our vectors from a lemmatised version of the British National Corpus (BNC), following closely the parameters of the setting described in Mitchell and Lapata (2008) , later used by <cite>Grefenstette and Sadrzadeh (2011a)</cite> . Specifically, we use the 2000 most frequent words as the basis for our vector space; this single space will serve as a semantic space for both nouns and sentences. The weights of the vectors are set to the ratio of the probability of the context word given the target word to the probability of the context word overall.",
  "y": "similarities uses"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_6",
  "x": "**DISAMBIGUATION** We first test our models against the disambiguation task for transitive sentences described in <cite>Grefenstette and Sadrzadeh (2011a)</cite> . The goal is to assess how well a model can discriminate between the different senses of an ambiguous verb, given the context (subject and object) of that verb.",
  "y": "similarities uses"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_7",
  "x": "The most successful S = N model for this task is the copyobject model, which is performing really close to the original relational model of <cite>Grefenstette and Sadrzadeh (2011a)</cite> , with the difference to be statistically insignificant. This is a promising result, since it suggests that the lower-dimensional new model performs similarly with the richer structure of the old model for transitive sentences, while at the same time allows generalisation to even more complex sentences 1 . More importantly, note that the categorical models are the only ones that respect the word order and grammatical structure of sentences; a feature completely dismissed in the simple multiplicative model.",
  "y": "extends differences"
 },
 {
  "id": "48e3715c55fcc188367dcfdc26c05f_8",
  "x": "1 The original relational model of <cite>Grefenstette and Sadrzadeh (2011a)</cite> with S = N 2 , provided a \u03c1 of 0.21. When computed with our program with the exact same parameters (without embedding them in the S = N model), we obtained a \u03c1 of 0.195. The differences between both of these and our best model are statistically insignificant.",
  "y": "extends differences"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_0",
  "x": "With vMF, the KL loss only depends on the concentration parameter which is fixed during training and testing, and hence results in a constant KL loss. In a more recent work, Dieng et al. (2019) avoided latent variable collapse by including skip connections in the generative model, where the skip connections enforce strong links between the latent variables and the likelihood function. Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss (Bowman et al., 2016; , or resort to designing more sophisticated model structures<cite> (Yang et al., 2017</cite>; Xu and Durrett, 2018; Dieng et al., 2019) .",
  "y": "background"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_1",
  "x": "In contrast to existing VAE-RNN models for text modelling which merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation for all hidden states of the RNN encoder. Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures. We evaluate our model against several strong baselines which apply VAE for text modelling (Bowman et al., 2016; <cite>Yang et al., 2017</cite>; Xu and Durrett, 2018) .",
  "y": "uses"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_2",
  "x": "We evaluate our model against several strong baselines which apply VAE for text modelling (Bowman et al., 2016; <cite>Yang et al., 2017</cite>; Xu and Durrett, 2018) . Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation.",
  "y": "differences uses"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_3",
  "x": "**VARIATIONAL AUTOENDODER WITH HOLISTIC REGULARISATION** In this section, we discuss the technical details of the proposed holistic regularisation VAE (HR-VAE) model, a general architecture which can effectively mitigate the KL vanishing phenomenon. Our model design is motivated by one noticeable defect shared by the VAE-RNN based models in previous works (Bowman et al., 2016; <cite>Yang et al., 2017</cite>; Xu and Durrett, 2018; Dieng et al., 2019) .",
  "y": "motivation"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_4",
  "x": "We compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base 3 : A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue (Bowman et al., 2016) ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder<cite> (Yang et al., 2017)</cite> ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (Xu and Durrett, 2018) . the decoder needs to predict the entire sequence with only the help of the given latent variable z.",
  "y": "background"
 },
 {
  "id": "493f353942929c1a015d5e0acbf564_5",
  "x": "We compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base 3 : A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue (Bowman et al., 2016) ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder<cite> (Yang et al., 2017)</cite> ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (Xu and Durrett, 2018) . the decoder needs to predict the entire sequence with only the help of the given latent variable z.",
  "y": "uses background"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_0",
  "x": "The key feature of this model is that it predicts not only the output word, but also the alignment between source and target using an additional step symbol which shifts the pointer to the next symbol. This model was further improved by<cite> (Makarov et al., 2017)</cite> , whose system was the winner of Sigmorphon 2017 evaluation campaign (Cotterell et al., 2017) . The approach of Makarov et al. was especially successful in low and medium resource setting, while in high resource setting it achieves an impressive accuracy of over 95% 1 .",
  "y": "background"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_1",
  "x": "In presence of fusion, like in Russian and other Slavonic languages, the decomposition is not that easy or even impossible. However, this decomposition is already realised in model of<cite> (Makarov et al., 2017)</cite> since the grammatical features are treated as a list of atomic elements, not as entire label. A new source of information about the whole language are the laws of its phonetics.",
  "y": "background"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_2",
  "x": "They were already applied to the problem of inflection in (Sorokin, 2016) and produced a strong boost over the baseline system. The work of Sorokin used simple ngram models, however, neural language models (Tran et al., 2016) has shown their superiority over earlier approaches for various tasks. Summarizing, our approach was to enrich the model of<cite> (Makarov et al., 2017</cite> ) with the language model component.",
  "y": "extends"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_3",
  "x": "We expected to improve performance especially in low and medium resource setting, however, our approach does not have clear advantages: our joint system is only slightly ahead the baseline system of<cite> (Makarov et al., 2017)</cite> for most of the languages. We conclude that the language model job is already executed by the decoder. However, given the vitality of language model approach in other areas of modern NLP (Peters et al., 2018), we describe our attempts in detail to give other researchers the ideas for future work in this direction.",
  "y": "differences"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_4",
  "x": "---------------------------------- **BASELINE MODEL** As the state-of-the-art baseline we choose the model of Makarov et al.<cite> (Makarov et al., 2017)</cite> , the winner of previous Sigmorphon Shared Task.",
  "y": "uses"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_5",
  "x": "We also use the copy gate from<cite> (Makarov et al., 2017)</cite> : since the neural network copies the vast majority of its symbols, the output distribution p i is obtained as a weighted sum of singleton distribution which outputs current input symbol and the preliminary distribution p i specified above. The weight \u03c3 i is the output of another one-layer perceptron: ----------------------------------",
  "y": "uses"
 },
 {
  "id": "49b42346795d541dbcac9e2b9ad00a_6",
  "x": "**RESULTS AND DISCUSSION** We submitted three systems, one replicating the algorithm of<cite> (Makarov et al., 2017)</cite> , the second equipped with language models. The third one used only the language models: we extracted all possible abstract inflection paradigms for a given set of grammatical features and created a set of possible candidate forms applying all paradigms to the lemma.",
  "y": "uses"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_0",
  "x": "This is shown when training word embeddings, a vector representation of words, in news sets with crowd-sourcing evaluation to quantify the presence of biases, such as gender bias, in those representation<cite> (Bolukbasi et al., 2016)</cite> . This can affect downstream applications (Zhao et al., 2018a) and are at risk of being amplified (Zhao et al., 2017) . The objective of this work is to study the presence of gender bias in MT and give insight on the impact of debiasing in such systems.",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_1",
  "x": "**DEBIASING WORD EMBEDDINGS** The presence of biases in word embeddings has aroused as a topic of discussion about fairness. More specifically, gender stereotypes are learned from human generated corpora as shown by<cite> (Bolukbasi et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_2",
  "x": "Several debiasing approaches have been proposed. Debiaswe is a postprocess method for debiasing previously generated embeddings<cite> (Bolukbasi et al., 2016)</cite> . GN-GloVe is a method for generating gender neutral embeddings (Zhao et al., 2018b) .",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_3",
  "x": "GN-GloVe is a method for generating gender neutral embeddings (Zhao et al., 2018b) . The main ideas behind these algorithms are described next. Debiaswe<cite> (Bolukbasi et al., 2016</cite> ) is a postprocess method for debiasing word embeddings.",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_4",
  "x": "The dimension of the vectors is settled to 512 as standard and kept through all the experiments in this study. The parameter values for training the word embedding models with GloVe and GN-GloVe methods are listed in Table 3 . Debiaswe<cite> (Bolukbasi et al., 2016</cite> ) is a debiasing post-process performed on trained embeddings.",
  "y": "background"
 },
 {
  "id": "4ad830d8377d2584798a30bed65254_5",
  "x": "We trained sets of word embeddings with the standard GloVe algorithm. Then, we debiased the embeddings using Debiaswe<cite> (Bolukbasi et al., 2016)</cite> and also trained its gender neutral version with GN-GloVe (Zhao et al., 2018b) . We used all these different models on the Transformer (Vaswani et al., 2017) .",
  "y": "uses"
 },
 {
  "id": "4bc5fc3bccb704b9978b294ffb07de_0",
  "x": "**TUTORIAL DESCRIPTION** This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (Saon and Chien, 2012; Chan et al., 2016) to document summarization (Chang and Chien, 2009 ), text classification (Blei et al., 2003; Zhang et al., 2015) , text segmentation (Chien and Chueh, 2012) , information extraction (Narasimhan et al., 2016) , image caption generation (Vinyals et al., 2015; Xu et al., 2015) , sentence generation (Li et al., 2016b) , dialogue control (Zhao and Eskenazi, 2016; <cite>Li et al., 2016a</cite>) , sentiment classification, recommendation system, question answering (Sukhbaatar et al., 2015) and machine translation , to name a few. Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model.",
  "y": "uses background"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_0",
  "x": "In contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking. For example,<cite> Bohnet and Nivre (2012)</cite> had to carefully discretize the real-valued POS tag score in order to combine it with the other discrete binary features in their system. Additionally, we also experiment with different transition systems, most notably the integrated parsing and part-of-speech (POS) tagging system of<cite> Bohnet and Nivre (2012)</cite> and also the swap system of Nivre (2009) .",
  "y": "background"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_1",
  "x": "Additionally, we also experiment with different transition systems, most notably the integrated parsing and part-of-speech (POS) tagging system of<cite> Bohnet and Nivre (2012)</cite> and also the swap system of Nivre (2009) . We evaluate our parser on the CoNLL '09 shared task dependency treebanks, as well as on two English setups, achieving the best published numbers in many cases. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_2",
  "x": "The non-linear network models of Weiss et al. (2015) and Chen and Manning (2014) embed the 1-best tag, according to a first-stage tagger, for a select set of tokens for any configuration. Inspired by the work of<cite> Bohnet and Nivre (2012)</cite> , we embed the set of top tags according to a first-stage tagger. Specifically, we define the feature group ktags as the matrix X ktags such that, for",
  "y": "similarities uses"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_3",
  "x": "While past work on neural network transitionbased parsing has focused exclusively on the arcstandard transition system, it is known that better results can often be obtained with more sophisticated transition systems that have a larger set of possible actions. The integrated arc-standard transition system of<cite> Bohnet and Nivre (2012)</cite> allows the parser to participate in tagging decisions, rather than being forced to treat the tagger's tags as given, as in the arc-standard system. It does this by replacing the shift action in the arc-standard system with an action shift p , which, aside from shifting the top token on the buffer also assigns it one of the k best POS tags from a first-stage tagger.",
  "y": "uses"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_4",
  "x": "For ease of experimentation, we deviate from<cite> Bohnet and Nivre (2012)</cite> and use a single unstructured beam, rather than separate beams for POS tag and parse differences. We train our neural networks on the standard training sets only, except for initializing with word embeddings generated by word2vec and using cluster features in our POS tagger. Unlike Weiss et al. (2015) we train our model only on the treebank training set and do not use tri-training, which can likely further improve the results.",
  "y": "differences"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_5",
  "x": "It is worth pointing out that Gesmundo et al. (2009) is itself a neural net parser. Our models achieve higher labeled accuracy than the winning systems in the shared task in all languages. Additionally, our pipelined neural network parser always outperforms its linear counterpart, an in-house reimplementation of the system of Zhang and Nivre (2011) , as well as the more recent and highly accurate parsers of Zhang and McDonald (2014) and Lei et al. (2014 again outperforms its linear counterpart<cite> (Bohnet and Nivre, 2012)</cite> , however, in some cases the addition of graph-based and cluster features<cite> (Bohnet and Nivre, 2012</cite> )+G+C can lead to even better results.",
  "y": "differences"
 },
 {
  "id": "4cf805818bed233fabb81f5f64f4cc_6",
  "x": "Results. The results shown in Table 4 , we find that our full model surpasses, to our knowledge, all previously reported supervised parsing models for the Stanford dependency conversions. It surpasses its linear analog, the work of<cite> Bohnet and Nivre (2012)</cite> on Stanford Dependencies UAS by 0.9% UAS and by 1.14% LAS.",
  "y": "differences"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_0",
  "x": "<cite>Zhang et al. (2017)</cite> utilize adversarial training to obtain cross-lingual word embeddings without any parallel data. However, their performance is still significantly worse than supervised methods. <cite>Zhang et al. (2017)</cite> apply adversarial training to align monolingual word vector spaces with no supervision.",
  "y": "background"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_1",
  "x": "Our experiments could be divided into two parts. In the first part, we conduct experiments on smallscale datasets and our main baseline is <cite>Zhang et al. (2017)</cite> . In the second part, we combine our model with several advanced techniques and we compare our model with Conneau et al. (2018)",
  "y": "similarities"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_2",
  "x": "**SMALL-SCALE DATASETS** In this section, our experiments focus on smallscale datasets and our main baseline model is adversarial autoencoder<cite> (Zhang et al., 2017)</cite> . For justice, we use the same model selection strategy with <cite>Zhang et al. (2017)</cite> , i.e. we choose the model whose sum of reconstruction loss and classification accuracy is the least.",
  "y": "similarities"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_3",
  "x": "**EXPERIMENTS ON CHINESE-ENGLISH DATASET** For this set of experiments, we use the same data as <cite>Zhang et al. (2017)</cite> . The statistics of the final training data is given in Table 1 .",
  "y": "uses"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_4",
  "x": "We use Chinese-English Translation Lexicon Version 3.0 (LDC2002L27) as our ground truth bilingual lexicon for evaluation. The baseline models are MonoGiza system (Dou et al., 2015) , translation matrix (TM) (Mikolov et al., 2013) , isometric alignment (IA) (Zhang et al., 2016b) and adversarial training approach<cite> (Zhang et al., 2017)</cite> . Table 2 summarizes the performance of baseline models and our approach.",
  "y": "similarities uses"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_5",
  "x": "Table 2 summarizes the performance of baseline models and our approach. The results of baseline models are cited from <cite>Zhang et al. (2017)</cite> . As we can see from the table, our model could achieve superior performance compared with other baseline models.",
  "y": "differences uses"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_6",
  "x": "**EXPERIMENTS ON OTHER LANGUAGE PAIRS DATASETS** We also conduct experiments on Spanish-English and Italian-English language pairs. Again, we use the same dataset with <cite>Zhang et al. (2017)</cite> . and the statistics are shown in The experimental results are shown in Table 4 .",
  "y": "similarities uses"
 },
 {
  "id": "4d528117dd7751d0cd6413430e1ec1_7",
  "x": "---------------------------------- **LARGE-SCALE DATASETS** In this section, we integrate our method with Conneau et al. (2018) , whose method improves <cite>Zhang et al. (2017)</cite> by more sophiscated refinement procedure and validation criterion.",
  "y": "extends differences"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_0",
  "x": "For instance, most approaches for natural language inference (NLI) rely on alignment techniques to establish the overlap between the given premise and a hypothesis before determining if the former entails the latter. Such monolingual alignment techniques are also frequently employed in systems for paraphrase generation, multi-document summarization, sentence fusion and question answering. Previous work<cite> (MacCartney et al., 2008)</cite> has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006) .",
  "y": "background"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_1",
  "x": "Previous work<cite> (MacCartney et al., 2008)</cite> has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006) . However, MANLI's use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, <cite>MacCartney et al. (2008)</cite> employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation.",
  "y": "motivation"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_2",
  "x": "Previous work<cite> (MacCartney et al., 2008)</cite> has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006) . However, MANLI's use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, <cite>MacCartney et al. (2008)</cite> employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation.",
  "y": "background"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_3",
  "x": "Previous work<cite> (MacCartney et al., 2008)</cite> has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006) . However, MANLI's use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, <cite>MacCartney et al. (2008)</cite> employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation.",
  "y": "motivation"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_4",
  "x": "**RELATED WORK** Alignment is an integral part of statistical MT (Vogel et al., 1996; Och and Ney, 2003; Liang et al., 2006) but the task is often substantively different from monolingual alignment, which poses unique challenges depending on the application<cite> (MacCartney et al., 2008)</cite> . Outside of NLI, prior research has also explored the task of monolingual word align-ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002) .",
  "y": "motivation background"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_5",
  "x": "In our experiments, we merge the annotations using majority rule in the same manner as <cite>MacCartney et al. (2008)</cite> . ---------------------------------- **FEATURES**",
  "y": "uses"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_6",
  "x": "**THE MANLI ALIGNER** Our alignment system is structured identically to MANLI<cite> (MacCartney et al., 2008)</cite> and uses the same phrase-based alignment representation. An alignment E between two fragments of text T 1 and T 2 is represented by a set of edits {e 1 , e 2 , . . .}, each belonging to one of the following types:",
  "y": "uses"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_7",
  "x": "Features encode the type of edit, the size of the phrases involved in SUB edits, whether the phrases are constituents and their similarity (determined by leveraging various lexical resources). Additionally, contextual features note the similarity of neighboring words and the relative positions of phrases while a positional distortion feature accounts for the difference between the relative positions of SUB edit phrases in their respective sentences. Our implementation uses the same set of features as <cite>MacCartney et al. (2008)</cite> with some minor changes: we use a shallow parser (Daum\u00e9 and Marcu, 2005) for detecting constituents and employ only string similarity and WordNet for determining semantic relatedness, forgoing NomBank and the distributional similarity resources used in the original MANLI implementation.",
  "y": "extends uses"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_8",
  "x": "Feature weights are learned using the averaged structured perceptron algorithm (Collins, 2002) , an intuitive structured prediction technique. We deviate from <cite>MacCartney et al. (2008)</cite> and do not introduce L2 normalization of weights during learning as this could have an unpredictable effect on the averaged parameters. For efficiency reasons, we parallelize the training procedure using iterative parameter mixing (McDonald et al., 2010) in our experiments.",
  "y": "extends differences"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_9",
  "x": "For evaluation purposes, we compare the performance of approximate search decoding against exact ILP-based decoding on a reimplementation of MANLI as described in \u00a73. All models are trained on the development section of the Microsoft Research RTE2 alignment corpus (cf. \u00a73.1) using the training parameters specified in <cite>MacCartney et al. (2008)</cite> .",
  "y": "uses"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_10",
  "x": "Aligner performance is determined by counting aligned token pairs per problem and macro-averaging over all problems. The results are shown in Table 1 . We first observe that our reimplemented version of MANLI improves over the results reported in <cite>MacCartney et al. (2008)</cite> , gaining 2% in precision, 1% in recall and 2-3% in the fraction of alignments that exactly matched human annotations.",
  "y": "differences"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_11",
  "x": "We present a simple exact decoding technique as an alternative to approximate search-based decoding in MANLI that exhibits a twenty-fold improvement in runtime performance in our experiments. In addition, we propose novel syntactically-informed constraints to increase precision. Our final system improves over the results reported in <cite>MacCartney et al. (2008)</cite> by about 4.5% in precision and 1% in recall, with a large gain in the number of perfect alignments over the test corpus.",
  "y": "differences"
 },
 {
  "id": "4f111ff06afd5523d65fc1d1a9ff83_12",
  "x": "The results of our evaluation indicate that exact decoding via ILP is a robust and efficient technique for solving alignment problems. Furthermore, the incorporation of simple constraints over a dependency parse can help to shape more accurate alignments. An examination of the alignments produced by our system reveals that many remaining errors can be tackled by the use of named-entity recognition and better paraphrase corpora; this was also noted by <cite>MacCartney et al. (2008)</cite> with regard to the original MANLI system.",
  "y": "similarities future_work"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_0",
  "x": "The initial state of this decoder is a weighted combination of the final states of the two encoders. Intuitively, such an integration of sourcelanguage information in APE should be useful in conveying the context information to improve the APE performance. To provide the awareness of errors in mt originating from src, the transformer architecture <cite>(Vaswani et al., 2017)</cite> , which is built solely upon attention mechanisms (Bahdanau et al., 2015) , makes it possible to model dependencies without regard to their distance in the input or output sequences and also captures global dependencies between input and output (for our case src, mt, and pe).",
  "y": "background"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_1",
  "x": "The transformer architecture replaces recurrence and convolutions by using positional encodings on both the input and output sequences. The encoder and decoder both use multi-head (facilitating parallel computations) self-attention to compute representations of their corresponding inputs, and also compute multi-head vanilla-attentions between encoder and decoder representations. Our APE system extends this transformer-based NMT architecture <cite>(Vaswani et al., 2017)</cite> by using two encoders, a joint encoder, and a single decoder.",
  "y": "extends differences"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_2",
  "x": "Our APE system extends this transformer-based NMT architecture <cite>(Vaswani et al., 2017)</cite> by using two encoders, a joint encoder, and a single decoder. Our model concatenates two separate selfattention-based encoders (enc src and enc mt ) and passes this sequence through another self-attended joint encoder (enc src,mt ) to ensure capturing dependencies between src and mt. Finally, this joint encoder is fed to the decoder which follows a similar architecture as described in<cite> Vaswani et al. (2017)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_3",
  "x": "**SINGLE-SOURCE TRANSFORMER FOR APE** (mt \u2192 pe) Our single-source model (SS) is based on an encoder-decoder-based transformer architecture <cite>(Vaswani et al., 2017)</cite> .",
  "y": "extends"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_4",
  "x": "Apart from enc src and enc mt , each of which is equivalent to the original transformer's encoder <cite>(Vaswani et al., 2017)</cite> , we use a joint encoder with an equivalent architecture, to maintain the homogeneity of the transformer model. For this, we extend<cite> Vaswani et al. (2017)</cite> by introducing an additional identical encoding block by which both the enc src and the enc mt encoders communicate with the decoder. Our multi-source neural APE computes intermediate states enc src and enc mt for the two encoders, enc src,mt for their combination, and dec pe for the decoder in sequence-to-sequence modeling.",
  "y": "extends differences"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_5",
  "x": "Apart from enc src and enc mt , each of which is equivalent to the original transformer's encoder <cite>(Vaswani et al., 2017)</cite> , we use a joint encoder with an equivalent architecture, to maintain the homogeneity of the transformer model. For this, we extend<cite> Vaswani et al. (2017)</cite> by introducing an additional identical encoding block by which both the enc src and the enc mt encoders communicate with the decoder. Our multi-source neural APE computes intermediate states enc src and enc mt for the two encoders, enc src,mt for their combination, and dec pe for the decoder in sequence-to-sequence modeling.",
  "y": "extends differences"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_6",
  "x": "This is a similar setting to<cite> Vaswani et al. (2017)</cite> 's C \u2212 model 1 . For the scaled dotproduct attention, the input consists of queries and keys of dimension d k , and values of dimension d v . As multi-head attention parameters, we employ h = 8 for parallel attention layers, or heads.",
  "y": "similarities"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_7",
  "x": "For optimization, we use the Adam optimizer (Kingma and Ba, 2015) with \u03b2 1 = 0.9, \u03b2 2 = 0.98 and \u03f5 = 10 \u22129 . The learning rate is varied throughout the training process, first increasing linearly for the first training steps warmup steps = 4000 and then adjusted as described in <cite>(Vaswani et al., 2017)</cite> . At training time, the batch size is set to 32 samples, with a maximum sentence length of 80 subwords, and a vocabulary of the 50K most frequent subwords.",
  "y": "similarities"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_8",
  "x": "For encoding the word order, our model uses learned positional embeddings (Gehring et al., 2017) , since<cite> Vaswani et al. (2017)</cite> reported nearly identical results to sinusoidal encodings. After finishing training, we save the 5 best checkpoints saved at each epoch. Finally, we use a single model obtained by averaging the last 5 checkpoints.",
  "y": "similarities"
 },
 {
  "id": "4f75f73b4eac8aecdde9312a846a1d_9",
  "x": "In the future, we will investigate if the performance of each system can be improved by using a different hyper-parameter setup. Unfortunately, we could not test either the 'big' or the 'base' hyper-parameter configuration in<cite> Vaswani et al. (2017)</cite> due to unavailable computing resources at the time of submission. As additional future work, we would like to explore whether using re-ranking and ensembling of different neural APEs helps to improve the performance further.",
  "y": "differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_0",
  "x": "Adding this layer to the neural RC models improved performance on multi-hop tasks. Recently, an attention based system<cite> (Zhong et al., 2019)</cite> utilizing both documentlevel and entity-level information achieved stateof-the-art results on WIKIHOP data set, proving that techniques like co-attention and self-attention widely employed in single-document RC tasks are also useful in multi-document RC tasks. The second type of research work is based on graph neural networks (GNN) for multi-hop reasoning.",
  "y": "background"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_1",
  "x": "In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network<cite> (Zhong et al., 2019)</cite> , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities; \u2022 The HDE graph enables rich information interaction among different types of nodes thus facilitate accurate reasoning. Different types of nodes are connected with different types of edges to highlight the various structural information presented among query, document and candidates.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_2",
  "x": "Evaluated on the blind test set of WIKIHOP, our proposed end-to-end trained single neural model beats the current stateof-the-art results in<cite> (Zhong et al., 2019)</cite> 1 , without using pretrained contextual ELMo embedding (Peters et al., 2018) . ---------------------------------- **RELATED WORK**",
  "y": "differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_3",
  "x": "The study presented in this paper is directly related to existing research on multi-hop reading comprehension across multiple documents (Dhingra et al., 2018; Song et al., 2018; De Cao et al., 2018;<cite> Zhong et al., 2019</cite>; Kundu et al., 2018) . The method presented in this paper is similar to previous studies using GNN for multi-hop reasoning (Song et al., 2018; De Cao et al., 2018) . Our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_4",
  "x": "Our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information. The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model<cite> (Zhong et al., 2019)</cite> because they show the effectiveness of attention mechanisms. Our model is very different from the other two studies (Dhingra et al., 2018; Kundu et al., 2018) : these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_5",
  "x": "We will show later that by including mentions of query subject the performance can be improved. We use simple exact match strategy (De Cao et al., 2018;<cite> Zhong et al., 2019)</cite> to find the locations of mentions of query subject and candidates, i.e. we need the start and end positions of each mention. Each mention is treated as an entity.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_6",
  "x": "We denote an entity's representation as M \u2208 R lm\u00d7h where l m is the length of the entity. Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (Seo et al., 2016; Xiong et al., 2016) , and recently was applied to multiple-hop reading comprehension<cite> (Zhong et al., 2019)</cite> . Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document.",
  "y": "background"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_7",
  "x": "Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document. We follow the implementation of coattention in<cite> (Zhong et al., 2019)</cite> . We use the co-attention between a query and a supporting document for illustration.",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_8",
  "x": "We expect S ca carries query-aware contextual information of supporting documents as shown by <cite>Zhong et al. (2019)</cite> . The same co-attention module can also be applied to query and candidates, and query and entities (as shown in Figure 2 ) to get C ca and E ca . Note that we do not do coattention between query and entities corresponding to query subject because query subject is already a part of the query. To keep the dimensionality consistent, we apply a single-layer multi-layer perceptron (MLP) with tanh activation function to increase the dimension of the query subject entities to 2h.",
  "y": "uses differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_9",
  "x": "The same co-attention module can also be applied to query and candidates, and query and entities (as shown in Figure 2 ) to get C ca and E ca . Note that we do not do coattention between query and entities corresponding to query subject because query subject is already a part of the query. To keep the dimensionality consistent, we apply a single-layer multi-layer perceptron (MLP) with tanh activation function to increase the dimension of the query subject entities to 2h. Self-attentive pooling: while co-attention yields a query-aware contextual representation of documents, self-attentive pooling is designed to convert the sequential contextual representation to a fixed dimensional non-sequential feature vector by selecting important query-aware information<cite> (Zhong et al., 2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_10",
  "x": "Similarly, after self-attentive pooling, we can get c sa and e sa for each candidate and entity. Our context encoding module is different from the one used in <cite>Zhong et al. (2019)</cite> in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model. 2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while <cite>Zhong et al. (2019)</cite> first do self-attention on entity word sequences to get a sequence of entity vectors in each documents.",
  "y": "differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_11",
  "x": "Our context encoding module is different from the one used in <cite>Zhong et al. (2019)</cite> in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model. 2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while <cite>Zhong et al. (2019)</cite> first do self-attention on entity word sequences to get a sequence of entity vectors in each documents. Then, they apply coattention with query.",
  "y": "differences"
 },
 {
  "id": "5095f2af3f0c51283c8fbee08a17ac_13",
  "x": "In Table 1 , we show the results of the our proposed HDE graph based model on both development and test set and compare it with previously published results. We show that our proposed HDE graph based model improves the state-of-the-art accuracy on development set from 67.1% (Kundu et al., 2018) to 68.1%, on the blind test set from 70.6%<cite> (Zhong et al., 2019)</cite> to 70.9%. Compared to two previous studies using GNN for multi-hop reading comprehension (Song et al., 2018; De Cao et al., 2018) , our model surpasses them by a large margin even though we do not use better pre-trained contextual embedding ELMo (Peters et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_0",
  "x": "The starting point of our study is the goal oriented dialogue task of <cite>Kottur et al. (2017)</cite> , summarized in Fig. 2 . During learning we periodically replace some agents with new ones (gray agents). These new agents do not know any language, but instead of creating one they learn it from older agents.",
  "y": "uses"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_1",
  "x": "For example, an agent who understands blue square and purple triangle should also understand purple square without directly experiencing it; we use this sort of generalization to measure compositionality. Existing work has investigated conditions under which compositional languages emerge between neural agents in simple environments (Mordatch & Abbeel, 2018;<cite> Kottur et al., 2017)</cite> , but it only investigates how language changes within a generation. Simulating cultural transmission, the iterated learning model (Kirby et al., 2008; Kirby, 2001; Kirby et al., 2014) has found that generational dynamics cause compositional language to emerge using experiments in simulation (Kirby, 2001 ) and with human subjects (Kirby et al., 2008) .",
  "y": "background motivation"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_2",
  "x": "Unlike regimes where agents are trained to learn an existing language, languages that emerge in this sort of setting are not necessarily easy to understand for a human. Even attempts to translate these emerged languages for other agents are not completely successful (Andreas et al., 2017) , possibly because the target languages can't express the same concepts as the source languages. This desire for structure motivates the previously mentioned work on compositional language emergence in neural agents<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018; Choi et al., 2018) .",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_3",
  "x": "In this paper, we study the following question -what are the conditions that lead to the emergence of a compositional language? Our key finding is evidence that cultural transmission leads to more compositional language in deep reinforcement learning agents, as it does in evolutionary linguistics. The starting point for our investigation is the recent work of <cite>Kottur et al. (2017)</cite> , which investigates compositionality using a cooperative reference game between two agents.",
  "y": "uses"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_4",
  "x": "Both agents are rewarded if both attributes are correct. As in <cite>Kottur et al. (2017)</cite> , we implement Q, A, and U as neural networks. Our model is trained to maximize the reward using policy gradients (Williams, 1992) .",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_5",
  "x": "This approach-summarized in the black lines (4-9) of Algorithm 1-is our starting point. In <cite>Kottur et al. (2017)</cite> it was used to generate a somewhat compositional language given Algorithm 1: Training with Replacement and Multiple Agents Policy gradient update w.r.t.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_6",
  "x": "This approach-summarized in the black lines (4-9) of Algorithm 1-is our starting point. In <cite>Kottur et al. (2017)</cite> it was used to generate a somewhat compositional language given Algorithm 1: Training with Replacement and Multiple Agents Policy gradient update w.r.t.",
  "y": "uses background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_7",
  "x": "Single Agent. In <cite>Kottur et al. (2017)</cite> there is only one pair of agents (N Q = N A = 1) so we cannot replace both agents at the same round because all existing language would be lost. Instead, we consider two strategies that only replace one bot at a time:",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_8",
  "x": "As in <cite>Kottur et al. (2017)</cite> , our world contains objects with 3 attributes (shape, size, color) such that each attribute has 4 possible values. Objects are represented 'symbolically' as 3-hot vectors and not rendered as RGB images. Evaluation with Compositional Dataset.",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_9",
  "x": "If the language created by interaction between agents can identify the held out instances (e.g., it has unique words for purple square which both agents understand) then it is compositional. This is simply measured by accuracy on the test set. Previous work also measures generalization to held out compositions of attributes to measure compositionality<cite> (Kottur et al., 2017</cite>; Kirby et al., 2015) .",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_10",
  "x": "Unlike <cite>Kottur et al. (2017)</cite> , we use a slightly harder version of their dataset which aligns better with the goal of compositional language. For a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. This disallows opportunities for non-compositional generalization.",
  "y": "differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_11",
  "x": "Our A-bots and Q-bots have the same architecture and hyperparameter variations as in <cite>Kottur et al. (2017)</cite> , but with our cultural transmission training procedure and some other differences identified below. Like <cite>Kottur et al. (2017)</cite> , our hyperparameter variations consider the number of vocab words Q-bot (V Q ) and A-bot (V A ) may utter and whether or not A-bot has memory between dialog rounds. The memoryless version of A-bot simply sets h t A = 0 between each round of dialog.",
  "y": "similarities differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_12",
  "x": "Our A-bots and Q-bots have the same architecture and hyperparameter variations as in <cite>Kottur et al. (2017)</cite> , but with our cultural transmission training procedure and some other differences identified below. Like <cite>Kottur et al. (2017)</cite> , our hyperparameter variations consider the number of vocab words Q-bot (V Q ) and A-bot (V A ) may utter and whether or not A-bot has memory between dialog rounds. The memoryless version of A-bot simply sets h t A = 0 between each round of dialog.",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_13",
  "x": "In the Multi Agent setting we use N A = N Q = 5. To decide when to stop we measure validation set accuracy averaged over all Q-bot-Abot pairs and choose the first population whose validation accuracy did not improve for 200k epochs. 7 This differs from <cite>Kottur et al. (2017)</cite> , which stopped once train accuracy reached 100%.",
  "y": "differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_14",
  "x": "Thus we compare to the Replace All baseline, which has the greatest chance of seeing a lucky initialization and thereby ensures that gains over the No Replacement baseline 6 This is slightly different from Small Vocab in<cite> (Kottur et al., 2017)</cite> . 7 There are few objects in the environment, so each batch contains all objects and is an entire epoch. are not simply due to luck.",
  "y": "differences"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_15",
  "x": "This suggests that while some agent replacement needs to occur, it does not much matter whether agents with worse language are replaced or whether there is a pool of similarly typed agents to remember knowledge lost from older generations. The main factor is that new agents learn in the presence of others who already know a language. Test set accuracies (with standard deviations) are reported against our new harder dataset using models similar to those in<cite> (Kottur et al., 2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_16",
  "x": "The models considered in <cite>Kottur et al. (2017)</cite> were ordered, from best to worse, as: Memoryless + Small Vocab > Small Vocab > Overcomplete. Our trends tend to agree with that conclusion though the differences are smaller-mainly comparing the Memoryless + Small Vocab model to others in cultural transmission settings. Only in the Oldest setting are the differences all significant enough to completely establish the above ordering.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_17",
  "x": "The models considered in <cite>Kottur et al. (2017)</cite> were ordered, from best to worse, as: Memoryless + Small Vocab > Small Vocab > Overcomplete. Our trends tend to agree with that conclusion though the differences are smaller-mainly comparing the Memoryless + Small Vocab model to others in cultural transmission settings. Only in the Oldest setting are the differences all significant enough to completely establish the above ordering.",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_18",
  "x": "Our trends tend to agree with that conclusion though the differences are smaller-mainly comparing the Memoryless + Small Vocab model to others in cultural transmission settings. Only in the Oldest setting are the differences all significant enough to completely establish the above ordering. This agrees with factors noted elsewhere<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018; Nowak et al., 2000) .",
  "y": "similarities"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_19",
  "x": "Even though cultural transmission may aid the emergence of compositionality, recent results in evolutionary linguistics (Raviv et al., 2018) and deep learning<cite> (Kottur et al., 2017</cite>; Mordatch & Abbeel, 2018) show cultural transmission may not be necessary for compositionality to emerge. While existing work in deep learning has focused on biases that encourage compositionality, it has not considered settings where language is permitted to evolve as it is passed down over generations of agents. We consider such a setting because of its potential to complement our existing understanding.",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_20",
  "x": "Recent work in deep learning has increasingly focused on multi-agent environments where deep agents learn to accomplish goals (possibly cooperative or competitive) by interacting appropriately with the environment and each other. Some of this work has shown that deep agents will develop their own language where none exists initially if driven by a task which requires communication (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017) . Most relevant is similar work which focuses on conditions under which compositional language emerges as deep agents learn to cooperate (Mordatch & Abbeel, 2018;<cite> Kottur et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "50cdfe539f84d793ec50873b5ab066_21",
  "x": "Both Mordatch & Abbeel (2018) and <cite>Kottur et al. (2017)</cite> find that limiting the vocabulary size so that there aren't too many more words than there are objects to refer to encourages compositionality, which follows earlier results in evolutionary linguistics (Nowak et al., 2000) . Follow up work has continued to investigate the emergence of compositional language among neural agents, mainly focusing on perceptual as opposed to symbolic input and how the structure of the input relates to the tendency for compositional language to emerge (Choi et al., 2018; Havrylov & Titov, 2017; Lazaridou et al., 2018) . Other work investigating emergent translation has shown that Multi Agent interaction leads to better translation (Lee et al., 2018) , but they do not measure compositionality.",
  "y": "background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_0",
  "x": "Recently, translation scholars have made some general claims about translation properties. Some of these are source language independent while others are not. <cite>Koppel and Ordan (2011)</cite> performed empirical studies to validate both types of properties using English source texts and other texts translated into English. Obviously, corpora of this sort, which focus on a single language, are not adequate for claiming universality of translation properties.",
  "y": "motivation background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_1",
  "x": "Recently, scholars in this area identified several properties of the translation process with the aid of corpora (Baker, 1993; Baker, 1996; Olohan, 2001; Laviosa, 2002; Hansen, 2003; Pym, 2005) . These properties are subsumed under four keywords: explicitation, simplification, normalization and levelling out. They focus on the general effects of the translation process. Toury (1995) has a different theory from these. That is, a translated text will carry some fingerprints of its source language. Recently, Pastor et al. (2008) and Ilisei et al. (2009; have provided empirical evidence of simplification translation properties using a comparable corpus of Spanish. <cite>Koppel and Ordan (2011)</cite> perform empirical studies to validate both theories, using a subcorpus extracted from the Europarl (Koehn, 2005) and IHT corpora<cite> (Koppel and Ordan, 2011)</cite> . They used a comparable corpus of original English and English translated from five other European languages. In addition, original English and English translated from Greek and Korean was also used in their experiment. They have found that a translated text contains both source language dependent and independent features. Obviously, corpora of this sort, which focus on a single language (e.g., English), are not adequate for claiming the universal validity of translation properties. Different languages (and language families) have different linguistic properties.",
  "y": "background motivation"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_2",
  "x": "<cite>Koppel and Ordan (2011)</cite> have built a classifier that can identify the correct source of the translated text (given different possible source languages). They have built another classifier which can identify source text and translated text. Furthermore, they have shown that the degree of difference between two translated texts, translated from two different languages into the same target language reflects, the degree of difference of the source languages. They have gained impressive results for both of the tasks. However, the limitation of this study is that they only used a corpus of English original text and English text translated from various European languages.",
  "y": "background motivation"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_3",
  "x": "The system performs poorly when the source language of the training corpus is different from the one of the test corpus. We can not compare our findings directly with <cite>Koppel and Ordan (2011)</cite> even though we use text from the same corpus and similar techniques. The English language is not considered for this study due to unavailability of English translations for some languages included in this work.",
  "y": "similarities"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_4",
  "x": "We can not compare our findings directly with <cite>Koppel and Ordan (2011)</cite> even though we use text from the same corpus and similar techniques. The English language is not considered for this study due to unavailability of English translations for some languages included in this work. Furthermore, instead of the list of 300 function words used by <cite>Koppel and Ordan (2011)</cite> , we used the 100 most frequent words for each candidate language.",
  "y": "differences"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_5",
  "x": "However, this corpus is not suitable for the experiment we are performing here. We extract a suitable corpus from the Europarl corpus in a way similar to Lembersky et al. (2011) and <cite>Koppel and Ordan (2011)</cite> . Our target is to extract texts that are translated from and to the languages considered here.",
  "y": "uses"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_6",
  "x": "Our target is to extract texts that are translated from and to the languages considered here. We trust the source language marker that has been put by the respective translator, as did Lembersky et al.(2011) and <cite>Koppel and Ordan (2011)</cite> . To experiment with stylistic differences in translated text, a list of function words and their",
  "y": "similarities"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_7",
  "x": "In this experiment, our goal is to validate the translation properties postulated by Toury (1995) . He stated that a translated text inherits some fingerprints from the source language. The experimental result of <cite>Koppel and Ordan (2011)</cite> shows that text translated into English holds this property.",
  "y": "background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_8",
  "x": "Our hypothesis is again similar to <cite>Koppel and Ordan (2011)</cite> , that is, if the classifier's accuracy is close to 20%, then we cannot say that there is an interference effect in translated text. If the classifier's accuracy is close to 100% then our conclusion will be that interference effects exist in translated text. Table 3 and Table 4 show the evaluation results.",
  "y": "similarities"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_9",
  "x": "In the past, researchers have used comparable corpora to validate these translation properties (Baroni and Bernardini, 2006; Pastor et al., 2008; Ilisei et al., 2009; Ilisei et al., 2010;<cite> Koppel and Ordan, 2011)</cite> . Most of them used comparable corpora for two-class classification, distinguishing translated texts from the original texts. Only<cite> Koppel and Ordan (Koppel and Ordan, 2011)</cite> used English texts translated from multiple source languages. We perform similar experiments only for six European languages as shown in Table 1 .",
  "y": "background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_10",
  "x": "The source language texts are extracted for the corresponding languages in a similar way from the Europarl corpus. <cite>Koppel and Ordan (2011)</cite> received the highest accuracy (96.7%) among all works noted above. The training and test data are generated in similar ways as in our previous experiment.",
  "y": "background"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_11",
  "x": "Expected F-Scores are calculated from 100 samples. Table 5 shows the evaluation results. Even though the classifier for German achieves around 99% accuracy, we cannot compare the result with<cite> Koppel and Ordan (Koppel and Ordan, 2011)</cite> as the amount of chunks for the classes are different.",
  "y": "differences"
 },
 {
  "id": "50d065b6b187f361f8e456df0a0bbe_12",
  "x": "The results show that training a classifier based on the 100 most frequent words of a language is sufficient to obtain interpretable results. We find our results to be compatible with <cite>Koppel and Ordan (2011)</cite> who used 300 function words. A list of the 100 most frequent words is easily obtainable for a vast number of languages, while lists consisting strictly of function words are rare and cannot be produced without considerable additional effort.",
  "y": "similarities"
 },
 {
  "id": "511c17a6cb6bd74e0216c3d50eb9c0_0",
  "x": "As opposed to the situation for Irish Gaelic (Lynn et al., 2012a;<cite> Lynn et al., 2012b</cite>; Lynn et al., 2013; Lynn et al., 2014) there are no treebanks or tagging schemes for Scottish Gaelic, although there are machine-readable dictionaries and databases available from Sabhal M\u00f2r Ostaig. A single paper in the ACL Anthology (Kessler, 1995) mentions Scottish Gaelic in the context of computational dialectology of Irish. There is also an LREC workshop paper (Scannell, 2006 ) on machine translation between Irish and Scottish Gaelic.",
  "y": "background"
 },
 {
  "id": "511c17a6cb6bd74e0216c3d50eb9c0_1",
  "x": "This contains 23 types and was developed originally for parser evaluation. Another popular scheme is the Stanford Dependency scheme (de Marneffe and Manning, 2008; de Marneffe and Manning, 2013) , which is more finely-grained with over twice the number of dependency types to deal specifically with noisy data and to make it more accessible to non-linguists building information extraction applications. A very important scheme is the Dublin scheme for Irish (Lynn et al., 2012a;<cite> Lynn et al., 2012b</cite>; Lynn et al., 2013) , which is of a similar size to the Stanford scheme, but the reason for its size relative to GR is that it includes a large number of dependencies intended to handle grammatical features found in Irish but not in English.",
  "y": "background"
 },
 {
  "id": "511c17a6cb6bd74e0216c3d50eb9c0_2",
  "x": "This, at least on the surface, equates pronoun e, with a noun described by a relative clause including the verb bi. Fig. 1 shows our dependency tree for this. Note that this is different from the scheme in <cite>Lynn et al. (2012b)</cite> because of a difference between the two languages.",
  "y": "differences"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_0",
  "x": "On the other hand, <cite>Noraset et al. (2017)</cite> attempted to generate a definition of a word from an embedding induced from massive text (which can be seen as global context). This is followed by Gadetsky et al. (2018) that refers to a local context to disambiguate polysemous words by choosing relevant dimensions of their word embeddings. Al-though these research efforts revealed that both local and global contexts are useful in generating definitions, none of these studies exploited both contexts directly to describe unknown phrases.",
  "y": "background"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_1",
  "x": "The model therefore combines both pieces of information to generate a natural language description. Considering various applications where we need definitions of expressions, we evaluated our method with four datasets including WordNet<cite> (Noraset et al., 2017)</cite> for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slang, and a newlycreated Wikipedia dataset for entities. Our contributions are as follows:",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_2",
  "x": "The challenge here is that the decoder needs to be conditioned not only on the local context, but also on its global context. To incorporate the different types of contexts, we propose to use a gate function similar to <cite>Noraset et al. (2017)</cite> to dynamically control how the global and local contexts influence the description. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_3",
  "x": "**USE OF CHARACTER INFORMATION** In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following<cite> (Noraset et al., 2017)</cite> . Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following <cite>Noraset et al. (2017)</cite>, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg .",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_4",
  "x": "**USE OF CHARACTER INFORMATION** In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following<cite> (Noraset et al., 2017)</cite> . Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following <cite>Noraset et al. (2017)</cite>, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg .",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_5",
  "x": "In order to capture the interaction between the local and global contexts, we adopt a GATE(\u00b7) function (Eq. (7)) which is similar to <cite>Noraset et al. (2017)</cite> . The GATE(\u00b7) function updates the LSTM output s t to s t depending on the global context x trg , local context d t , and character-level information c trg as where \u03c3(\u00b7), and ; denote the sigmoid function, element-wise multiplication, and vector concatenation, respectively.",
  "y": "similarities"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_6",
  "x": "8 For all of these datasets, a given word or phrase has an inventory of senses with corresponding definitions and usage examples. These definitions are regarded as groundtruth descriptions. Datasets To evaluate our model on the word description task on WordNet, we followed <cite>Noraset et al. (2017)</cite> and extracted data from WordNet using the dict-definition 9 toolkit.",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_7",
  "x": "Note that the words are mutually exclusive across the three sets. The only difference between our dataset and theirs is that we extract the tuples only if the words have their usage examples in WordNet. Since not all entries in WordNet have usage examples, our dataset is a small subset of <cite>Noraset et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_8",
  "x": "**MODELS WE IMPLEMENTED FOUR METHODS: (1)** Global<cite> (Noraset et al., 2017)</cite> , (2) Local (Ni and Wang, 2017) with CNN, (3) I-Attention (Gadetsky et al., 2018) , and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the best model (S + G + CH) in <cite>Noraset et al. (2017)</cite> . It can access the global context of a phrase to be described, but has no ability to read the local context.",
  "y": "uses"
 },
 {
  "id": "5203c1037fe57bd1b813c0bf1ff5c4_9",
  "x": "However, the target of paraphrase acquisition are words/phrases with no specified context. Although a few studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) consider subsentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word. Recently, <cite>Noraset et al. (2017)</cite> introduced a task of generating a definition sentence of a word from its pre-trained embedding.",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_0",
  "x": "This severely limits their application on low-resource languages, which may have not a well-designed pronunciation lexicon. Recent researches on sequence-to-sequence attention-based models try to remove this dependency on the pronunciation lexicon [8,<cite> 9,</cite> 10] . Chiu et al. shows that attention-based encoder-decoder architecture, namely listen, attend, and spell (LAS), achieves a new stateof-the-art WER on a 12500 hour English voice search task using the word piece models (WPM) [10] .",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_1",
  "x": "Recent researches on sequence-to-sequence attention-based models try to remove this dependency on the pronunciation lexicon [8,<cite> 9,</cite> 10] . Chiu et al. shows that attention-based encoder-decoder architecture, namely listen, attend, and spell (LAS), achieves a new stateof-the-art WER on a 12500 hour English voice search task using the word piece models (WPM) [10] . Our previous work <cite>[9]</cite> demonstrates that the lexicon independent models can outperform lexicon dependent models on Mandarin Chinese ASR tasks by the ASR Transformer and the character based model establishes a new state-of-the-art character error rate (CER) on HKUST datasets.",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_2",
  "x": "Building on our work <cite>[9]</cite> , we employ sub-words generated by byte pair encoding (BPE) [11] as the multilingual modeling unit, which do not need any pronunciation lexicon. The ASR Transformer is chosen to be the basic architecture of sequence-to-sequence attention-based model<cite> [9,</cite> 12] . To alleviate the problem of few training data on low-resource languages, a well-trained ASR Transformer from a high-resource language is adopted as the initial model rather than random initialization, whose softmax layer is replaced by the language-specific softmax layer.",
  "y": "extends differences"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_3",
  "x": "In this paper, we concentrate on multilingual ASR on low-resource languages. Building on our work <cite>[9]</cite> , we employ sub-words generated by byte pair encoding (BPE) [11] as the multilingual modeling unit, which do not need any pronunciation lexicon. The ASR Transformer is chosen to be the basic architecture of sequence-to-sequence attention-based model<cite> [9,</cite> 12] .",
  "y": "extends differences"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_4",
  "x": "**SYSTEM OVERVIEW 3.1. ASR TRANSFORMER MODEL ARCHITECTURE** The ASR Transformer architecture used in this work is the same as our work<cite> [9,</cite> 12] which is shown in Figure 1 . It stacks multihead attention (MHA) [17] and position-wise, fully connected layers for both the encode and decoder.",
  "y": "similarities uses"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_5",
  "x": "Similar to [20, 21] , at the current frame t, these features are stacked with 3 frames to the left and downsampled to a 30ms frame rate. We generate more training data by linearly scaling the audio lengths by factors of 0.9 and 1.1 [22] , since it is always beneficial for training the ASR Transformer <cite>[9]</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_6",
  "x": "We perform our experiments on the big model (D1024-H16)<cite> [9,</cite> 17] of the ASR Transformer. Table 3 lists our experimental parameters. The Adam algorithm [23] with gradient clipping and warmup is used for optimization.",
  "y": "background"
 },
 {
  "id": "520588fbf0643725153b07a09430d1_7",
  "x": "We propose that one reason for the poor performance could be the training data is too few but the parameters of the ASR Transformer are relatively large which is about 230M in this work. To compensate the lack of training data on low-resource languages, a well-trained ASR Transformer with a CER of 26.64% on HKUST dataset, a corpus of Mandarin Chinese conversational telephone speech, is adopted from our work <cite>[9]</cite> . Its softmax layer is replaced by the language-specific softmax layer which is initialized randomly.",
  "y": "uses"
 },
 {
  "id": "52af1f5378194ccdb7c8f755a6ae34_0",
  "x": "Although AT has recently been applied in NLP tasks (e.g., text classification (Miyato et al., 2017) ), this paper -to the best of our knowledge -is the first attempt investigating regularization effects of AT in a joint setting for two related tasks. We start from a baseline joint model that performs the tasks of named entity recognition (NER) and relation extraction at once. Previously proposed models (summarized in Section 2) exhibit several issues that the neural network-based baseline approach (detailed in Section 3.1) overcomes: (i) our model uses automatically extracted features without the need of external parsers nor manually extracted features (see Gupta et al. (2016) ; Miwa and Bansal (2016) ; Li et al. (2017) ), (ii) all entities and the corresponding relations within the sentence are extracted at once, instead of examining one pair of entities at a time (see <cite>Adel and Sch\u00fctze (2017)</cite> ), and (iii) we model relation extraction in a multi-label setting, allowing multiple relations per entity (see Katiyar and Cardie (2017) ; Bekoulis et al. (2018a) ).",
  "y": "differences"
 },
 {
  "id": "52af1f5378194ccdb7c8f755a6ae34_1",
  "x": "Specifically, Miwa and Bansal (2016) as well as Li et al. (2017) apply bidirectional tree-structured RNNs for different contexts (i.e., news, biomedical) to capture syntactic information (using external dependency parsers). Gupta et al. (2016) propose the use of various manually extracted features along with RNNs. <cite>Adel and Sch\u00fctze (2017)</cite> solve the simpler problem of entity classification (EC, assuming entity boundaries are given), instead of NER, and they replicate the context around the entities, feeding entity pairs to the relation extraction layer.",
  "y": "background"
 },
 {
  "id": "52af1f5378194ccdb7c8f755a6ae34_2",
  "x": "We evaluate our models on four datasets, using the code as available from our github codebase. 1 Specifically, we follow the 5-fold crossvalidation defined by Miwa and Bansal (2016) for the ACE04 (Doddington et al., 2004) dataset. For the CoNLL04 (Roth and Yih, 2004 ) EC task (assuming boundaries are given), we use the same splits as in Gupta et al. (2016) ; <cite>Adel and Sch\u00fctze (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "52af1f5378194ccdb7c8f755a6ae34_3",
  "x": "This indicates that NLP tools are not always accurate for various contexts. For the CoNLL04 dataset, we use two evaluation settings. We use the relaxed evaluation similar to Gupta et al. (2016) ; <cite>Adel and Sch\u00fctze (2017)</cite> on the EC task.",
  "y": "similarities"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_0",
  "x": "While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004 ) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010) . A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages. This idea was first explored for weakly supervised learning (Cohen and Smith, 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010) and recently by <cite>Naseem et al. (2012)</cite> for multisource cross-lingual transfer.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_1",
  "x": "A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages. This idea was first explored for weakly supervised learning (Cohen and Smith, 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010) and recently by <cite>Naseem et al. (2012)</cite> for multisource cross-lingual transfer. In particular, <cite>Naseem et al.</cite> showed that by selectively sharing parameters based on typological features of each language, substantial improvements can be achieved, compared to using a single set of parameters for all languages.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_2",
  "x": "This requires a careful decomposition of features into language-generic and language-specific sets in order to tie specific target language parameters to their relevant source language counterparts. The resulting parser outperforms the method of <cite>Naseem et al. (2012)</cite> on 12 out of 16 evaluated languages. Second, in \u00a75 we introduce a train-ing method that can incorporate diverse knowledge sources through ambiguously predicted labelings of unlabeled target language data.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_3",
  "x": "Here, we experiment with two different knowledge sources: arc sets, which are filtered by marginal probabilities from the cross-lingual transfer parser, are used in an ambiguity-aware self-training algorithm ( \u00a75.2); these arc sets are then combined with the predictions of a different transfer parser in an ambiguity-aware ensemble-training algorithm ( \u00a75.3). The resulting parser provides significant improvements over a strong baseline parser and achieves a 13% relative error reduction on average with respect to the best model of <cite>Naseem et al. (2012)</cite> , outperforming it on 15 out of the 16 evaluated languages. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_4",
  "x": "The aforementioned approaches work well for transfer between similar languages. However, their assumptions cease to hold for typologically divergent languages; a target language can rarely be described as a linear combination of data or model parameters from a set of source languages, as languages tend to share varied typological traits; this critical insight is discussed further in \u00a74. To account for this issue, <cite>Naseem et al. (2012)</cite> recently introduced a novel generative model of dependency parsing, in which the generative process is factored into separate steps for the selection of dependents and their ordering.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_5",
  "x": "The parameters used in the selection step are all language independent, capturing only head-dependent attachment preferences. In the ordering step, however, parameters are selectively shared between subsets of <cite>Naseem et al. (2012)</cite> restricts its potential performance. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_6",
  "x": "To account for this issue, <cite>Naseem et al. (2012)</cite> recently introduced a novel generative model of dependency parsing, in which the generative process is factored into separate steps for the selection of dependents and their ordering. In the ordering step, however, parameters are selectively shared between subsets of <cite>Naseem et al. (2012)</cite> restricts its potential performance.",
  "y": "motivation"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_7",
  "x": "**BASIC MODELS AND EXPERIMENTAL SETUP** Inspired by the superiority of discriminative graphbased parsing in the supervised scenario, we investigate whether the insights of <cite>Naseem et al. (2012)</cite> on selective parameter sharing can be incorporated into such models in the transfer scenario. We first review the basic graph-based parser framework and the experimental setup that we will use throughout.",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_8",
  "x": "To facilitate comparison with the state of the art, we use the same treebanks and experimental setup as <cite>Naseem et al. (2012)</cite> . Notably, we use the mapping proposed by Naseem et al. (2010) to map from fine-grained treebank specific part-of-speech tags to coarse-grained \"universal\" tags, rather than the more recent mapping proposed by Petrov et al. (2012) . For",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_9",
  "x": "We refer the reader to <cite>Naseem et al. (2012)</cite> for detailed information on the different treebanks. Due to divergent treebank annotation guidelines, which makes fine-grained evaluation difficult, all results are evaluated in terms of unlabeled attachment score (UAS). In line with <cite>Naseem et al. (2012)</cite>, we use gold part-of-speech tags and evaluate only on sentences of length 50 or less excluding punctuation.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_10",
  "x": "We refer the reader to <cite>Naseem et al. (2012)</cite> for detailed information on the different treebanks. Due to divergent treebank annotation guidelines, which makes fine-grained evaluation difficult, all results are evaluated in terms of unlabeled attachment score (UAS). In line with <cite>Naseem et al. (2012)</cite>, we use gold part-of-speech tags and evaluate only on sentences of length 50 or less excluding punctuation.",
  "y": "similarities uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_11",
  "x": "The first baseline, <cite>NBG</cite>, is the generative model with selective parameter sharing from <cite>Naseem et al. (2012)</cite> . 3 <cite>This model</cite> is trained without target language data, but we investigate the use of such data in \u00a75.4. The second baseline, Delex, is a delexicalized projective version of the well-known graph-based MSTParser (McDonald et al., 2005) .",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_12",
  "x": "The first baseline, <cite>NBG</cite>, is the generative model with selective parameter sharing from <cite>Naseem et al. (2012)</cite> . 3 <cite>This model</cite> is trained without target language data, but we investigate the use of such data in \u00a75.4. The second baseline, Delex, is a delexicalized projective version of the well-known graph-based MSTParser (McDonald et al., 2005) .",
  "y": "extends differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_13",
  "x": "We see that Delex performs well on target languages that are related to the majority of the source languages. However, for languages 3 Model \"D-,To\" in Table 2 from <cite>Naseem et al. (2012)</cite> . that diverge from the Indo-European majority family, the selective sharing model, <cite>NBG</cite>, achieves substantially higher accuracies.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_14",
  "x": "In order to verify that these issues are the cause of the poor performance of the Delex model, we remove all directional features and all features that model local word order from Delex. The feature templates of the resulting Bare model are shown in the center of Figure 2 . These features only model selectional preferences and dependency length, analogously to the selection component of <cite>NBG</cite>.",
  "y": "similarities"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_15",
  "x": "Inspired by <cite>Naseem et al.</cite> Table 2 from <cite>Naseem et al. (2012)</cite> . (2012), we make use of the typological features from WALS (Dryer and Haspelmath, 2011), listed in Table 1, to selectively share directional parameters between languages. As a natural first attempt at sharing parameters, one might consider forming the crossproduct of all features of Delex with all WALS properties, similarly to a common domain adaptation technique (Daum\u00e9 III, 2007; Finkel and Manning, 2009 ).",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_16",
  "x": "85A; yet they have the exact opposite direction of attachment preference when it comes to nouns and adjectives. This problem applies to any method for parameter mixing that treats all the parameters as equal. Like <cite>Naseem et al. (2012)</cite> , we instead share parameters more selectively.",
  "y": "similarities uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_17",
  "x": "Although this model still performs worse than <cite>NBG</cite>, it is an improvement over the Delex baseline and actually outperforms the former on 5 out of the 16 languages. ---------------------------------- **SHARING BASED ON LANGUAGE GROUPS**",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_18",
  "x": "We see that by adding these rich features back into the fold, but having them fire only for languages in the same group, we can significantly increase the performance -from 57.4% to 62.0% on average when considering Family. If we consider our original Delex baseline, we see an absolute improvement of 6.9% on average and a relative error reduction of 15%. Particular gains are seen for non-Indo-European languages; e.g., Japanese increases from 38.9% to 65.9%. Furthermore, Family achieves a 7% relative error reduction over the <cite>NBG</cite> baseline and outperforms it on 12 of the 16 languages.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_19",
  "x": "While some higher-level linguistic properties of the target language have been incorporated through selective sharing, so far no features specific to the target language have been employed. Cohen et al. (2011) and <cite>Naseem et al. (2012)</cite> have shown that using expectation-maximization (EM) to this end can in some cases bring substantial accuracy gains. For discriminative models, self-training has been shown to be quite effective for adapting monolingual parsers to new domains (McClosky et al., 2006) , as well as for relexicalizing delexicalized parsers using unlabeled target language data (Zeman and Resnik, 2008) .",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_20",
  "x": "While some higher-level linguistic properties of the target language have been incorporated through selective sharing, so far no features specific to the target language have been employed. Cohen et al. (2011) and <cite>Naseem et al. (2012)</cite> have shown that using expectation-maximization (EM) to this end can in some cases bring substantial accuracy gains. However, as discussed in \u00a75.2, standard self-training is not optimal for target language adaptation.",
  "y": "motivation"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_21",
  "x": "In this study, we combine the arc sets of two base parsers: first, the arc-marginal ambiguity set of the base parser ( \u00a75.2); and second, the Viterbi arc set from the <cite>NBG</cite> parser of <cite>Naseem et al. (2012)</cite> in Table 2 . 4 Thus, the latter will have singleton arc ambiguity sets, but when combined with the arc-marginal ambiguity sets of our base parser, the result will encode uncertainty derived from both parsers. ----------------------------------",
  "y": "uses background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_22",
  "x": "We now study the different approaches to target language adaptation empirically. As in <cite>Naseem et al. (2012)</cite> , we use the CoNLL training sets, stripped of all dependency information, as the unlabeled target language data in our experiments. We use the Family model as the base parser, which is used to label the unlabeled target data with the Viterbi parses as well as with the ambiguous labelings.",
  "y": "uses similarities"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_23",
  "x": "Although the latter may suggest that\u1ef9(x) contains a high degree of ambiguity, in reality, the marginal distributions of the base model have low entropy and after filtering with \u03c3 = 0.95, the average number of potential heads per dependent ranges from 1.4 to 3.2, depending on the target language. The ambiguity-aware training methods, that is ambiguity-aware self-training (AAST) and ambiguityaware ensemble-training (AAET), are compared to three baseline systems. First, <cite>NBG+EM</cite> is the generative model of <cite>Naseem et al. (2012)</cite> trained with expectation-maximization on additional unlabeled target language text.",
  "y": "uses"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_24",
  "x": "Although the latter may suggest that\u1ef9(x) contains a high degree of ambiguity, in reality, the marginal distributions of the base model have low entropy and after filtering with \u03c3 = 0.95, the average number of potential heads per dependent ranges from 1.4 to 3.2, depending on the target language. The ambiguity-aware training methods, that is ambiguity-aware self-training (AAST) and ambiguityaware ensemble-training (AAET), are compared to three baseline systems. First, <cite>NBG+EM</cite> is the generative model of <cite>Naseem et al. (2012)</cite> trained with expectation-maximization on additional unlabeled target language text.",
  "y": "extends"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_25",
  "x": "Second, AAST outperforms the Viterbi variant on all languages and nearly always improves on the base parser, although it sees a slight drop for Italian. AAST improves the accuracy over the base model by 2% absolute on average and by as much as 5% absolute for Turkish. Comparing this model to the <cite>NBG+EM</cite> baseline, we observe an improvement by 3.6% absolute, outperforming it on 14 of the 16 languages.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_26",
  "x": "<cite>Naseem et al.</cite> observed an increase from 59.3% to 60.4% on average by adding unlabeled target language data and the gains were not consistent across languages. AAST, on the other hand, achieves consistent gains, rising from 62.0% to 64.0% on average. Third, as shown in the rightmost column of Table 3 , ambiguity-aware ensemble-training is indeed a successful strategy; AAET outperforms the previous best self-trained model on 13 and NB&G+EM on 15 out of 16 languages.",
  "y": "background"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_27",
  "x": "<cite>Naseem et al.</cite> observed an increase from 59.3% to 60.4% on average by adding unlabeled target language data and the gains were not consistent across languages. AAST, on the other hand, achieves consistent gains, rising from 62.0% to 64.0% on average. Third, as shown in the rightmost column of Table 3 , ambiguity-aware ensemble-training is indeed a successful strategy; AAET outperforms the previous best self-trained model on 13 and NB&G+EM on 15 out of 16 languages.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_28",
  "x": "Third, as shown in the rightmost column of Table 3 , ambiguity-aware ensemble-training is indeed a successful strategy; AAET outperforms the previous best self-trained model on 13 and NB&G+EM on 15 out of 16 languages. The relative error reduction with respect to the base Family model is 9% on average, while the average reduction with respect to <cite>NBG+EM</cite> is 13%. Before concluding, two additional points are worth making.",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_29",
  "x": "Before concluding, two additional points are worth making. First, further gains may potentially be achievable with feature-rich discriminative models. While the best generative transfer model of <cite>Naseem et al. (2012)</cite> approaches its upper-bounding supervised accuracy (60.4% vs. 67.1%), our relaxed selftraining model is still far below its supervised counterpart (64.0% vs. 84.1%).",
  "y": "differences"
 },
 {
  "id": "52c52f6ce3663de49d5784630af1e7_30",
  "x": [
   "Two instantiations of this framework were explored. First, an ambiguity-aware self-training method that can be used to effectively relexicalize and adapt a delexicalized transfer parser using unlabeled target language data. Second, an ambiguityaware ensemble-training method, in which predictions from different parsers can be incorporated and further adapted."
  ],
  "y": "differences"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_0",
  "x": "Knowledge Base Population (KBP, e.g.: Riedel et al., 2013; Sterckx et al., 2016) attempts to identify facts within raw text and convert them into triples consisting of a subject, object and the relation between them. One common form of this task is slot filling (Surdeanu and Heng, 2014) , in which a knowledge base (KB) query, such as place of birth(Obama, ?) is applied to a set of documents and a set of slot fillers is returned. By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_1",
  "x": "One common form of this task is slot filling (Surdeanu and Heng, 2014) , in which a knowledge base (KB) query, such as place of birth(Obama, ?) is applied to a set of documents and a set of slot fillers is returned. By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task. However, <cite>their approach</cite> relied on a modified QA model architecture and a dedicated slot-filling training corpus.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_2",
  "x": "Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> . ---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK**",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_3",
  "x": "---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_4",
  "x": "In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zeroshot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_5",
  "x": "In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) both positive examples, containing answers, and negative examples, without answers.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_6",
  "x": "In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of <cite>Levy et al. (2017)</cite> . <cite>These examples</cite> were derived from a pre-existing relation extraction resource, as <cite>their</cite> intention was to show the utility of the QA model.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_7",
  "x": "Data We compare two sources of training data: The <cite>University of Washington relation extraction</cite> (<cite>UWRE</cite>) dataset created by <cite>Levy et al. (2017</cite>) and the Stanford Question Answering Dataset (SQuAD) created by Rajpurkar et al. (2016) . The <cite>UWRE</cite> data is derived from WikiReading (Hewlett et al., 2016) , which is itself derived from WikiData (Vrande\u010di\u0107, 2012) , and consists of a set of positive and negative examples for relation extraction from Wikipedia sentences. Each instance consists of an entity, a relation, a question template for the relation and a sentence drawn from the wikipedia article for that entity which may or may not answer the question.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_8",
  "x": "The <cite>UWRE</cite> data is derived from WikiReading (Hewlett et al., 2016) , which is itself derived from WikiData (Vrande\u010di\u0107, 2012) , and consists of a set of positive and negative examples for relation extraction from Wikipedia sentences. Each instance consists of an entity, a relation, a question template for the relation and a sentence drawn from the wikipedia article for that entity which may or may not answer the question. Under the assumption that each relation triple found in a Wikipedia info-box is also expressed in the text of its article, the positive examples contain the first sentence from the article that contains both the subject and object of the triple.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_9",
  "x": "<cite>Levy et al. (2017)</cite> provide a number of train/dev/test splits, to allow <cite>them</cite> to evaluate a variety of modes of generalisation. Here we use the relation and entity splits. The former tests the ability to generalise from one set of relations to another, i.e. to do zero-shot learning for the unseen relations in the test set.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_10",
  "x": "Alongside these negative examples, we also retain the original SQuAD instances as positive examples. This process is applied to both the train and dev sets, allowing us to evaluate a model that uses only question answering data at training time. We also construct a series of datasets that combine increasing quantities of the <cite>UWRE</cite> entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_11",
  "x": "We also construct a series of datasets that combine increasing quantities of the <cite>UWRE</cite> entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_12",
  "x": "We also construct a series of datasets that combine increasing quantities of the <cite>UWRE</cite> entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited. Random samples of 10 3 , 10 4 , 10 5 and 10 6 <cite>UWRE</cite> instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_13",
  "x": "Models We employ the same modified BiDAF (Seo et al., 2016 ) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_14",
  "x": "Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall. However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction on the relation split test set, using models trained on the original <cite>UWRE</cite> and SQuAD datasets.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_15",
  "x": "However, returning an answer for such an instance does reduce precision. Results Table 1 reports the F1 scores for zeroshot relation extraction on the relation split test set, using models trained on the original <cite>UWRE</cite> and SQuAD datasets. As can be seen, BIDAF is actually more effective at answering the questions for the unseen relation types in the <cite>UWRE</cite> test set when it is trained on a standard QA dataset, rather than a dedicated relation extraction dataset.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_16",
  "x": "We compare training purely on <cite>UWRE</cite> instances to those same instances combined with the whole SQuAD dataset. As can be seen, when only small amounts of relation extraction data is available, combining this with the QA data gives a substantial boost to performance. Discussion The SQuAD trained model appears to be effective in the limited data and zero-shot cases, but contributes little when large numbers of examples of the relations of interest are available.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_17",
  "x": "**GENERALISATION TO A CHALLENGE TEST SET** In this second experiment, we want to test the ability of the models decribed above to generalise to data beyond the <cite>UWRE</cite> test set. In particular, we want to verify that the BiDAF model is able to recognise the assertion of a relation between the entity and the answer, rather than just recognising an answer phrase of the right type.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_18",
  "x": "In particular, we want to verify that the BiDAF model is able to recognise the assertion of a relation between the entity and the answer, rather than just recognising an answer phrase of the right type. Data We construct a challenge test set of negative examples based on sentences which are about the wrong entity but which do contain potential answers that are valid for the question and relation type. Thus, each positive example from the original <cite>UWRE</cite> entity split test set is turned into a negative example by pairing the sentence with an equivalent question about another entity.",
  "y": "extends"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_19",
  "x": "As before, a series of datasets combining SQuAD with increasing amounts of this new data is also constructed. Models We re-use the <cite>UWRE</cite> and SQuAD trained models in addition to training on the UWRE+ datasets described in the previous section. Evaluation Here, F1 is not an appropriate measure, as there are no positive instances in the challenge data.",
  "y": "extends uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_20",
  "x": "Evaluation Here, F1 is not an appropriate measure, as there are no positive instances in the challenge data. Instead, we use accuracy of the predictions, which in this case is just the number of 'no answer' predictions divided by the total number of instances. Table 3 : Zero-shot Precision, Recall and F1 on the <cite>UWRE</cite> relation split test set.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_21",
  "x": "Results Table 2 reports the accuracy of predictions on the challenge test set of negative examples. Although the original <cite>UWRE</cite> model achieved an F1 of around 90% on the unmodified entity split test set, here it only manages to get 2% of its predictions correct. In contrast, the modified UWRE+ training data results in a model that is much more accurate, predicting over 70% of the negative examples correctly.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_22",
  "x": "Nonetheless, the performance of the SQuAD trained model is stronger still, even without modification to address this problem. Figure 3 shows the accuracy on the challenge test set as increasing quantities of relation extraction instances are added to SQuAD. Looking first at the effect of adding the original <cite>UWRE</cite> training instances, performance drops dramatically as the size of this expansion increases.",
  "y": "uses"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_23",
  "x": "In the case of SQuAD, the multi-sentence paragraph structure around the answer provides enough potential distractors to overcome this issue. Other models may show different patterns of strength and weakness, but to be able to investigate and exploit further QA systems quickly would require a means of producing 'no answer' predictions without the need to modify the model implementation. 4 Using an unmodified QA model for slot filling <cite>Levy et al. (2017)</cite> modify the BiDAF architecture to produce an additional output representing the probability that no answer is present in the text.",
  "y": "background"
 },
 {
  "id": "5428f8c196308c90618abfdbdf856a_24",
  "x": "Results Table 3 reveals that the unmodified BiDAF model is almost as effective as the <cite>Levy et al. (2017)</cite> model in terms of zero-shot F1 on the original <cite>UWRE</cite> test set. In contrast, FastQA's performance is substantially worse. However, Table 4 reveals that FastQA is extremely accurate on the challenge test set, while BiDAF's performance is comparable to the modified model trained on SQuAD.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_0",
  "x": "In this paper, we demonstrate how learning to use code idioms leads to an improvement in model accuracy and training time for the task of semantic parsing, i.e., mapping intents in NL into general purpose source code (Iyer et al., 2017; Ling et al., 2016) . State-of-the-art semantic parsers are neural encoder-decoder models, where decoding is guided by the grammar of the target programming language (Yin and Neubig, 2017; Rabinovich et al., 2017; <cite>Iyer et al., 2018</cite>) to ensure syntactically valid programs. For general purpose programming languages with large formal grammars, this can easily lead to long decoding paths even for short snippets of code.",
  "y": "background"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_1",
  "x": "Once idioms are extracted, we greedily apply them to semantic parsing training sets to provide supervision for learning to apply idioms. We evaluate our approach on a context dependent semantic parsing task (<cite>Iyer et al., 2018</cite>) using the CONCODE dataset, where we improve the state of the art by 2.2% of BLEU score. Furthermore, generating source code using idioms results in a more than 50% reduction in the number of decoding steps, which cuts down training time to less than half, from 27 to 13 hours.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_2",
  "x": "Neural encoder-decoder models have proved effective in mapping NL to logical forms (Dong and Lapata, 2016) and also for directly producing general purpose programs (Iyer et al., 2017 (<cite>Iyer et al., , 2018</cite> . Ling et al. (2016) use a sequence-tosequence model with attention and a copy mechanism to generate source code. Instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamically determined modular structure paralleling the structure of the abstract syntax tree (AST) of the code (Rabinovich et al., 2017; Yin and Neubig, 2017) .",
  "y": "background"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_3",
  "x": "Ling et al. (2016) use a sequence-tosequence model with attention and a copy mechanism to generate source code. Instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamically determined modular structure paralleling the structure of the abstract syntax tree (AST) of the code (Rabinovich et al., 2017; Yin and Neubig, 2017) . Iy<cite>er et al. (2018</cite>) use a similar decoding approach but use a specialized context encoder for the task of context-dependent code generation.",
  "y": "background"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_4",
  "x": "This process is illustrated in Figure 3 where we perform two applications of the first idiom from Figure 2 (b), followed by one application of the second idiom from Figure 2 (d) , after which, the tree cannot be further compressed using those two idioms. The final tree can be represented using |r i | = 2 rules instead of the original |p i | = 5 rules. The decoder is then trained similar to previous approaches (Yin and Neubig, 2017; <cite>Iyer et al., 2018</cite>) using the compressed set of rules.",
  "y": "similarities"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_5",
  "x": "**EXPERIMENTAL SETUP** We apply our approach to the context dependent encoder-decoder model of <cite>Iyer et al. (2018</cite>) on the CONCODE dataset, and compare performance to a better tuned instance of their best model. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_6",
  "x": "---------------------------------- **BASELINE MODEL** We follow the approach of <cite>Iyer et al. (2018</cite>) with three major modifications in their encoder, which yields improvements in speed and accuracy (IyerSimp) .",
  "y": "extends"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_7",
  "x": "Using Bi-LSTM f , the encoder then computes: Then, h 1 , . . . , h z , andt i ,v i ,r i ,m i are passed on to the attention mechanism in the decoder, exactly as in <cite>Iyer et al. (2018</cite>) . The decoder of <cite>Iyer et al. (2018)</cite> is left unchanged.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_8",
  "x": "Then, h 1 , . . . , h z , andt i ,v i ,r i ,m i are passed on to the attention mechanism in the decoder, exactly as in <cite>Iyer et al. (2018</cite>) . The decoder of <cite>Iyer et al. (2018)</cite> is left unchanged. This forms our baseline model (Iyer-Simp).",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_9",
  "x": "It contains 100K tuples of (NL, code, context) for training, 2,000 tuples for development, and an additional 2,000 tuples for testing. We use a BPE vocabulary of 10K tokens for embedding matrix B and get the best validation set results using the original hyperparameters used by <cite>Iyer et al. (2018)</cite> . Since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400K training examples that <cite>Iyer et al. (2018)</cite> released as part of CONCODE.",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_10",
  "x": "Since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400K training examples that <cite>Iyer et al. (2018)</cite> released as part of CONCODE. We report exact match accuracy, corpus level BLEU score (which serves as a measure of partial credit) (Papineni et al., 2002) , and training time for all these configurations. <cite>Iyer et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_12",
  "x": "We report exact match accuracy, corpus level BLEU score (which serves as a measure of partial credit) (Papineni et al., 2002) , and training time for all these configurations. <cite>Iyer et al. (2018)</cite> . Significant improvements in training speed after incorporating idioms makes training on large amounts of data possible.",
  "y": "similarities"
 },
 {
  "id": "55160a7ab2df9a86e677bcc72d9842_13",
  "x": "Finally, the amount of compression, and therefore the training time, plateaus after the top-600 idioms are incorporated. Compared to the model of <cite>Iyer et al. (2018)</cite> , our significantly reduced training time enables us to train on their extended training set. We run IyerSimp using 400 idioms (taking advantage of even lower training time) on up to 5 times the amount of data, making sure that we do not include in training any NL from the validation or the test sets.",
  "y": "differences"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_0",
  "x": "In this paper, we approach this very broad topic by focusing on the problem of detecting hyperpartisan news, namely news written with an extreme manipulation of the reality on the basis of an underlying, typically extreme, ideology. This problem has received little attention in the context of the automatic detection of fake news, despite the potential correlation between them. Seminal work from <cite>[5]</cite> presents a comparative style analysis of hyperpartisan news, evaluating features such as characters n-grams, stop words, part-of-speech, readability scores, and ratios of quoted words and external links.",
  "y": "background"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_2",
  "x": "<cite>The results</cite> indicate that a topic-based model outperforms a style-based one to separate the left, right and mainstream orientations. We build upon <cite>previous work</cite> and use the dataset from <cite>[5]</cite> : this way we can investigate hyperpartisan-biased news (i.e., extremely one-sided) that have been manually fact-checked by professional journalists from BuzzFeed. The articles originated from 9 well-known political publishers, three each from the mainstream, the hyperpartisan left-wing, and the hyperpartisan right-wing.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_3",
  "x": "This technique makes it possible for us to corroborate previous results that content matters more than style. However, perhaps surprisingly, we are able to achieve the overall best performance by simply using higher-length n-grams than those used in the original work from <cite>[5]</cite> : this seems to indicate a strong lexical overlap between different sources with the same orientation, which, in turn, calls for more challenging datasets and task formulations to encourage the development of models covering more subtle, i.e., implicit, forms of bias. The rest of the paper is structured as follows.",
  "y": "extends"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_4",
  "x": "**EXPERIMENTS** We used the <cite>BuzzedFeed-Webis Fake News Corpus 2016</cite> collected by <cite>[5]</cite> whose articles were labeled with respect to three political orientations: mainstream, left-wing, and right-wing (see Table 2 ). Each article was taken from one of 9 publishers known as hyperpartisan left/right or mainstream in a period close to the US presidential elections of 2016.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_5",
  "x": "We used the <cite>BuzzedFeed-Webis Fake News Corpus 2016</cite> collected by <cite>[5]</cite> whose articles were labeled with respect to three political orientations: mainstream, left-wing, and right-wing (see Table 2 ). During initial data analysis and prototyping we identified a variety of issues with the <cite>original dataset:</cite> we cleaned the data excluding articles with empty or bogus texts, e.g. 'The document has moved here' (23 and 14 articles respectively).",
  "y": "extends"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_6",
  "x": "As a result, we obtained a new dataset with 1555 articles out of 1627. 4 Following the settings of <cite>[5]</cite> , we balance the training set using random duplicate oversampling. 4 <cite>The dataset</cite> is available at <cite>https://github.com/jjsjunquera/ UnmaskingBiasInNews</cite>.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_7",
  "x": "4 Following the settings of <cite>[5]</cite> , we balance the training set using random duplicate oversampling. 4 <cite>The dataset</cite> is available at <cite>https://github.com/jjsjunquera/ UnmaskingBiasInNews</cite>. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_8",
  "x": "After the text transformation by the masking process in both the training and test sets, we represented the documents with character n-grams and compared the results obtained with the style-based and the topic-related models. Machine (SVM) and Random Forest (RF); for the three classifiers we used the versions implemented in sklearn with the parameters set by default. Evaluation: We performed 3-fold cross-validation with the same configuration used in <cite>[5]</cite> .",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_9",
  "x": "In order to compare our results with those reported in <cite>[5]</cite> , we also used accuracy, precision, and recall. Baseline: Our baseline method is based on the same text representation with the character n-grams features, but without masking any word. Table 3 shows the results of the proposed method.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_10",
  "x": "Baseline: Our baseline method is based on the same text representation with the character n-grams features, but without masking any word. Table 3 shows the results of the proposed method. We compare with <cite>[5]</cite> against their topic and style-based methods.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_11",
  "x": "We compare with <cite>[5]</cite> against their topic and style-based methods. In order to compare our results with those reported in <cite>[5]</cite> , we report the same measures the <cite>authors</cite> used. We also include the macro F 1 score because of the unbalance test set.",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_12",
  "x": "For these experiments we extract the character 5-grams from the transformed texts, taking into account that as more narrow is the domain more sense has the use of longer n-grams. We follow the steps of [8] and set k = 500 for this comparison results. The last two rows show the results obtained by applying the system from <cite>[5]</cite> 6 to our cleaned dataset (Section 3).",
  "y": "uses"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_13",
  "x": "We follow the steps of [8] and set k = 500 for this comparison results. The last two rows show the results obtained by applying the system from <cite>[5]</cite> 6 to our cleaned dataset (Section 3). Similar to <cite>[5]</cite> , the topic-based model achieves better results than the style-related model.",
  "y": "similarities"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_14",
  "x": "The last two rows show the results obtained by applying the system from <cite>[5]</cite> 6 to our cleaned dataset (Section 3). Similar to <cite>[5]</cite> , the topic-based model achieves better results than the style-related model. However, the differences between the results of the two evaluated approaches are much higher (0.66 vs. 0.57 according to Macro F 1 ) than those shown in <cite>[5]</cite> .",
  "y": "differences"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_15",
  "x": "**RESULTS AND DISCUSSION** In line with what was already pointed out in <cite>[5]</cite> , the left-wing orientation is harder to predict, possibly because this class is represented with fewer examples in the dataset. Another reason why our masking approach achieves better results could be that we use a higher length of character n-grams.",
  "y": "similarities"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_16",
  "x": "In fact, comparing the results of <cite>[5]</cite> against our baseline model, it is possible to note that even without masking any word, the classifier obtains better results. This suggests that the good results are due to the length of the character n-grams rather than the use of the masking technique. Robustness of the approach to different values of k and n. With the goals of: (i) understanding the robustness of the approach to different parameter values; and to see if (ii) it is possible to overcome the F 1 = 0.70 from the baseline model, we vary the values of k and n and evaluate the macro F 1 using SVM.",
  "y": "differences"
 },
 {
  "id": "55bcdca5052745160dc861e22e7401_17",
  "x": "From this experiment, we conclude that: (i) the topic-related model is less sensitive than the style-related model when k < 500, i.e. the k most frequent terms are stylerelated ones; and (ii) when we vary the value of k, both models achieve worse results than our baseline. On the other hand, the results of extracting character 5-grams are higher than extracting smaller n-grams, as can be seen in Figures 2. These results confirm that perhaps the performance of our approach overcomes the models proposed in <cite>[5]</cite> because of the length of the n-grams 7 .",
  "y": "differences"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_0",
  "x": "Figure 1: An English sentence re-ordered into Japanese order using the rule-based method of<cite> Isozaki et al. (2010b)</cite> , and its reference Japanese translation. Semi-supervised approaches for NMT are often based on automatically creating pseudoparallel sentences through methods such as backtranslation (Irvine and Callison-Burch, 2013; Sennrich et al., 2016) or adding an auxiliary autoencoding task on monolingual data (Cheng et al., 2016; Currey et al., 2017) . However, both methods have problems with lowresource and syntactically divergent language pairs.",
  "y": "uses"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_1",
  "x": "Importantly, even simple heuristic reordering methods with a few handcreated rules have been shown to be highly effective in closing syntactic gaps (Collins et al. (2005) ;<cite> Isozaki et al. (2010b)</cite> ; Fig. 1 ). Because these rules usually function solely in high-resourced languages such as English with high-quality synguages, but limited success on real low-resource settings and syntactically divergent language pairs (Neubig and Hu, 2018; Guzm\u00e1n et al., 2019) . Hence we focus on semi-supervised methods in this paper.",
  "y": "background"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_2",
  "x": "However, in low-resource scenarios it is less realistic to expect that NMT could learn this reordering from scratch on its own. Here we ask \"how can we efficiently leverage the monolingual target data to improve the performance of the NMT system in low-resource, syntactically divergent language pairs?\" We tackle this problem via a simple two-step data augmentation method: (1) we first reorder monolingual target sentences to create source-ordered target sentences as shown in Fig. 1 , (2) we then replace the words in the reordered sentences with source words using a bilingual dictionary, and add them as the source side of a pseudo-parallel corpus. Experiments demonstrate the effectiveness of our approach on translation from Japanese and Uyghur to English, with a simple, linguistically motivated method of head finalization (HF;<cite> Isozaki et al. (2010b)</cite> ) as our reordering method.",
  "y": "uses"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_3",
  "x": "Instead of devising an entirely new wordordering method, we can simply rely on methods that have already been widely studied and proven useful in SMT . Reordering can be done either using rules based on linguistic knowledge<cite> (Isozaki et al., 2010b</cite>; Collins et al., 2005) or learning from aligned parallel data (Xia and McCord, 2004; Habash, 2007) , and in principle our pseudo-corpus creation paradigm is compatible with any of these methods. Specifically, in this work we utilize rule-based methods, as our goal is to improve translation of low-resource languages, where large quantities of high-quality parallel data do not exist and we posit that current data-driven reordering methods are unlikely to function well.",
  "y": "uses"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_4",
  "x": "Examples of rule-based methods include those to reorder English into German (Navratil et al., 2012) , Arabic (Badr et al., 2009 ), or Japanese<cite> (Isozaki et al., 2010b)</cite> . In experiments we use<cite> Isozaki et al. (2010b)</cite> 's method of reordering SVO languages (e.g. English) into the order of SOV languages (e.g. Japanese) by simply (1) applying a syntactic parser to English (Tsuruoka et al., 2004) , (2) identifying the head constituent of each phrase and moving it to the end of the phrase, and (3) inserting special tokens after subjects and objects of predicates to mimic Japanese case markers. Word-by-word Translation To generate data for training MT models, we next perform wordby-word translation of t s into pseudo-source sentence\u015d using a bilingual dictionary (Xie et al., 2018) .",
  "y": "background"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_5",
  "x": "In experiments we use<cite> Isozaki et al. (2010b)</cite> 's method of reordering SVO languages (e.g. English) into the order of SOV languages (e.g. Japanese) by simply (1) applying a syntactic parser to English (Tsuruoka et al., 2004) , (2) identifying the head constituent of each phrase and moving it to the end of the phrase, and (3) inserting special tokens after subjects and objects of predicates to mimic Japanese case markers. Word-by-word Translation To generate data for training MT models, we next perform wordby-word translation of t s into pseudo-source sentence\u015d using a bilingual dictionary (Xie et al., 2018) . 3 There are many ways we can obtain this dictionary: even for many low-resource languages with a paucity of bilingual text, we can obtain manually-curated lexicons with reasonable coverage, or run unsupervised word alignment on whatever parallel data we have available.",
  "y": "uses"
 },
 {
  "id": "56d1812bec8abbdb31a2346d96e5ca_6",
  "x": "As noted above, we use HF<cite> (Isozaki et al., 2010b)</cite> as our re-ordering rule. HF was designed for transforming English into Japanese order, but we use it as-is for the Uyghur-English pair as well to demonstrate that simple, linguistically motivated rules can generalize across pairs with similar syntax with little or no modification. Further details regarding the experimental settings are in the supplementary material.",
  "y": "uses"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_0",
  "x": "We show that the proposed method outperforms the baseline of <cite>Richardson et al. (2013)</cite> , and despite its relative simplicity, is comparable to recent work using machine learning. We hope that our approach will inform future work on this task. Furthermore, we argue that MC500 is harder than MC160 due to the way question answer pairs were created.",
  "y": "differences"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_1",
  "x": "The academic community has proposed a variety of tasks, such as information extraction (Sarawagi, 2008) , semantic parsing (Mooney, 2007) and textual entailment (Androutsopoulos and Malakasiotis, 2010) . However, these tasks assess performance on each task individually, rather than on overall progress towards machine comprehension of text. To this end, <cite>Richardson et al. (2013)</cite> proposed the Machine Comprehension Test (MCTest), a new challenge that aims at evaluating machine comprehension.",
  "y": "background"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_2",
  "x": "It does so through an opendomain multiple-choice question answering task on fictional stories requiring the common sense reasoning typical of a 7-year-old child. It is easy to evaluate as it consists of multiple choice questions. <cite>Richardson et al. (2013)</cite> also showed how the creation of stories and questions can be crowdsourced efficiently, constructing two datasets for the task, namely MC160 and MC500.",
  "y": "background"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_3",
  "x": "In this paper we develop an approach based on lexical matching which we extend by taking into account the type of the question and coreference resolution. These components improve the performance on questions that are difficult to handle with pure lexical matching. When combined with BIUTEE, we achieved 74.27% accuracy on MC160 and 65.96% on MC500, which are significantly better than those reported by <cite>Richardson et al. (2013)</cite> .",
  "y": "differences"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_4",
  "x": "3 Scoring function <cite>Richardson et al. (2013)</cite> proposed a sliding window algorithm that ranks the answers by forming the bag-of-words vector of each answer paired with the question text and then scoring them according to their overlap with the story text. We propose a modified version of this algorithm, which combines the scores across a range of window sizes. More concretely, the algorithm of <cite>Richardson et al. (2013)</cite> passes a sliding window over the story, size of which is equal to the number of words in the question-answer pair.",
  "y": "background"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_5",
  "x": "3 Scoring function <cite>Richardson et al. (2013)</cite> proposed a sliding window algorithm that ranks the answers by forming the bag-of-words vector of each answer paired with the question text and then scoring them according to their overlap with the story text. We propose a modified version of this algorithm, which combines the scores across a range of window sizes. More concretely, the algorithm of <cite>Richardson et al. (2013)</cite> passes a sliding window over the story, size of which is equal to the number of words in the question-answer pair.",
  "y": "background"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_6",
  "x": "This enables our algorithm to catch long-distance relations in the story. Similar to <cite>Richardson et al. (2013)</cite> , we use a linear combination of this score with their distancebased scoring function, and we weigh tokens with their inverse document frequencies in each individual story. By itself, this simple enhancement gives substantial improvements over the MSR baseline as shown in Table 1 (Enhanced SW+D), as it measures the overlap of the question-answer pair with multiple portions of the story text.",
  "y": "similarities uses"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_7",
  "x": "We evaluated our system on MC160 and MC500 test sets and the results are shown in Table 2 . Our proposed baseline outperforms the baseline of <cite>Richardson et al. (2013)</cite> by 4 and 3 points in accuracy on MC160 and MC500 respectively. 3 Our system is comparable to the MSR baseline with the RTE system BIUTEE (Stern and Dagan, 2011) .",
  "y": "differences"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_8",
  "x": "Our proposed baseline outperforms the baseline of <cite>Richardson et al. (2013)</cite> by 4 and 3 points in accuracy on MC160 and MC500 respectively. 3 Our system is comparable to the MSR baseline with the RTE system BIUTEE (Stern and Dagan, 2011) . If we linearly combine the RTE scores used in the MSR baseline with our method, we achieve 5 and 2.5 accuracy points higher than the best results achieved by <cite>Richardson et al. (2013)</cite> .",
  "y": "differences"
 },
 {
  "id": "5a000efaa052588f6cfbb69f8ced2d_9",
  "x": "<cite>Richardson et al. (2013)</cite> demonstrate that the MC160 and MC500 have similar ratings for clarity and grammar, and that humans perform equally well on both. However, in many cases MC500 appears to be designed in such a way to confuse lexical algorithms and encourage the use of more sophisticated techniques necessary to deal with phenomena such as elimination questions, negation, and common knowledge not explicitly written in the story. ----------------------------------",
  "y": "background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_0",
  "x": "Current statistical machine translation systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. To alleviate this problem, we extract hierarchical rules from weighted alignment matrix<cite> (Liu et al., 2009)</cite> . Since the sub-phrase pairs would change the inside and outside areas in the weighted alignment matrix of the hierarchical rules, we propose a new algorithm to calculate the relative frequencies and lexical weights of hierarchical rules.",
  "y": "background motivation"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_1",
  "x": "For example, a wrong rule (X 1 de jingji, of X 1 's economy) would be extracted from the alignment in Figure 1 (a). Since<cite> Liu et al. (2009)</cite> show that weighted alignment matrix provides an elegant solution to these two drawbacks, we apply it to the hierarchical phrase-based model (Chiang, 2005) and the tree-to-string model Huang et al., 2006) . While such an idea seems intuitive, it is non-trivial to extract hierarchical rules from weighted alignment matrices.",
  "y": "uses"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_2",
  "x": "Another challenge is how to achieve a balance between performance and rule table size. Note that given a source phrase, there would be plenty of \"potential\" candidate target phrases in weighted matrices<cite> (Liu et al., 2009</cite> ). If we retain all of them, these phrase pairs would produce even more hierarchical rules. For computational tractability, we need to design a measure to score the phrase pairs and wipe out the low-quality ones. We propose a new algorithm to calculate the relative frequencies of rules, and construct a measure that incorporates both frequency and lexical weight to score target phrases.",
  "y": "differences background motivation"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_3",
  "x": "A weighted alignment matrix<cite> (Liu et al., 2009)</cite> m is a J \u00d7 I matrix to encode the probabilities of n-best alignments of the same sentence pair. Each element in the matrix stores a link probability p m (j, i), which is estimated from an n-best list by calculating relative frequencies: where Here N is an n-best list, p(a) is the probability of an alignment a in the n-best list.",
  "y": "background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_4",
  "x": [
   "Liu et al. (2009) use the product of inside and outside probabilities as the fractional count of a phrase pair. Liu et al. (2009) define that inside probability indicates the probability that at least one word in source phrase is aligned to a word in target phrase, and outside probability indicates the chance that no words in one phrase are aligned to a word outside the other phrase. The fractional count is calculated: where \u03b1(\u00b7) and \u03b2(\u00b7) denote the inside and outside probabilities respectively, which can be calculated as Here in(\u00b7) denotes the inside area, which includes elements that fall inside the phrase pair, while out(\u00b7) denotes the outside area including elements that fall outside the phrase pair while fall in the same row or the same column."
  ],
  "y": "background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_5",
  "x": "To calculate the lexical weights,<cite> Liu et al. (2009)</cite> adapt p m (j, i) as the fractional count count(f j , e i ). The fractional counts of NULL words can be calculated as: Then the lexical weight can be calculated as: where",
  "y": "background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_6",
  "x": "Since we can calculate relative frequencies and lexical weights of phrase pairs as in<cite> Liu et al. (2009)</cite> , we only focus on the calculation of variable rules. ---------------------------------- **EXTRACTION ALGORITHM**",
  "y": "uses background"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_7",
  "x": "---------------------------------- **CALCULATING RELATIVE FREQUENCIES** We follow<cite> Liu et al. (2009)</cite> to calculate relative frequencies using the product of inside and outside probabilities.",
  "y": "uses"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_8",
  "x": "We follow<cite> Liu et al. (2009)</cite> to prune rule table using a threshold of frequency. Table 2 lists some hierarchical rules generated from the phrase pair (zhongguo de jingji, China's economy) in Figure 3 . If the threshold is 0.2, we retain all the rules in Table 2 .",
  "y": "uses"
 },
 {
  "id": "5a6684d978c0dbcfaabb4bc2314aeb_9",
  "x": "We follow<cite> Liu et al. (2009)</cite> to use p s2t \u00d7 p t2s as the probabilities of an alignment pair. Analogously, we abandon duplicate alignments that are produced from different alignment pairs. After these steps, there are 110 candidate alignments on average for each sentence pair.",
  "y": "uses"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_0",
  "x": "It is the fundamental step to many natural language processing applications, like Information Extraction (IE), Information Retrieval (IR) and Question Answering (QA). Most empirical approaches currently employed in NER task make decision only on local context for extract inference, which is based on the data independent assumption<cite> (Krishnan and Manning, 2006)</cite> . But often this assumption does not hold because non-local dependencies are prevalent in natural language (including the NER task).",
  "y": "background motivation"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_1",
  "x": "In order to establish the long dependencies easily and overcome the disadvantage of the approximate inference,<cite> Krishnan and Manning (2006)</cite> propose a two-stage approach using Conditional Random Fields (CRFs) with extract inference. They represent the non-locality with non-local features, and extract the nonlocal features from the output of the first stage CRF using local context alone; then they incorporate the non-local features into the second CRF. But the features in this approach are only used to improve label consistency. To our best knowledge, up to now, non-local information has not been explored to improve NER recall in previous researches; on the other hand, NER is always impaired by its lower recall due to the imbalanced distribution where the NONE class dominates the entity classes.",
  "y": "background motivation"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_2",
  "x": "Similar to<cite> Krishnan and Manning (2006)</cite> , we also encode non-local information with features and apply the simple two-stage architecture. Different from their work for improve label consistency, their features are activated on the recognized entities coming from the first CRF, the non-local features we design are used to recall more missed entities which are seen in the training data or unseen entities but some of their occurrences being recognized correctly in the first stage, our features are fired on the raw token sequence directly with forward maximum match. Compared to their non-local information extracted from training data with 10-fold cross-validation, our non-local information is extracted from the training date directly; our approach obtaining the non-local features is simpler. Moreover, we design different non-local features encoding different useful information for NER two subtasks: entity boundary detection and entity semantic classification.",
  "y": "similarities differences"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_3",
  "x": "**OUR BASELINE NER SYSTEM** To validate the effectiveness of our approach of exploiting non-local features, we need to establish a baseline with state-of-the-art performance using local context alone. Similar to<cite> (Krishnan and Manning, 2006)</cite> , we employ two-stage architecture under conditional random fields (CRFs) framework.",
  "y": "similarities"
 },
 {
  "id": "5b98a80237182b2d506ea4c9d71aa1_4",
  "x": "Token-position & entity-majority features (F4): These features capture non-local information from F2 and F3 simultaneously. They take into account the entity boundary and semantic class information at the same time. These non-local features are applied in English NER in one-step approach<cite> (Krishnan and Manning, 2006</cite>; Wong and Ng, 2007) , they employ these features to improve entity consistence among their different occurrences. These features are assigned to token sequences that are matched exactly with the (entity, majority-type) list in forward maximum matching (FMM) way.",
  "y": "background"
 },
 {
  "id": "5ed24e18f892d7092c183acab4b175_0",
  "x": "A particular upturn was proposed by Bengio et al. in [1] , replacing sparse n-gram models with word embeddings which are more compact representations obtained using feed-forward or more advanced neural networks. Recently, high quality and easy to train Skip-gram shallow architectures were presented in <cite>[10]</cite> and considerably improved in [11] with the introduction of negative sampling and subsampling of frequent words. The \"magical\" ability of word embeddings to capture syntactic and semantic regularities on text words is applicable in various applications like machine translations, error correcting systems, sentiment analyzers etc.",
  "y": "background"
 },
 {
  "id": "5ed24e18f892d7092c183acab4b175_1",
  "x": "Wikipedia Dependency corpus is a collection of 1 billion tokens from Wikipedia. The method used for training it is a modified version of Skip-gram word2vec described in [7] . Google News is one of the biggest and richest text sets with 100 billion tokens and a vocabulary of 3 million words and phrases <cite>[10]</cite> .",
  "y": "background"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_0",
  "x": "In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of <cite>our previously published entity representation models</cite>. e toolkit provides a uni ed interface to di erent representation learning algorithms, ne-grained parsing con guration and can be used transparently with GPUs. In addition, users can easily modify existing models or implement their own models in the framework.",
  "y": "motivation extends"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_1",
  "x": "---------------------------------- **INTRODUCTION** e unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a ention for the entity-oriented tasks of expert nding [9] and product search [<cite>8</cite>] .",
  "y": "motivation background"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_2",
  "x": "Representations are learned from a document collection and domain-speci c associations between documents and entities. Expert nding is the task of nding the right person with the appropriate skills or knowledge [1] and an association indicates document authorship (e.g., academic papers) or involvement in a project (e.g., annual progress reports). In the case of product search, an associated document is a product description or review [<cite>8</cite>] .",
  "y": "background"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_3",
  "x": "In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models [<cite>8</cite>, 9] . Beyond a uni ed interface that combines di erent models, the toolkit allows for ne-grained parsing con guration and GPU-based training through integration with eano [3, 6] . Users can easily extend existing models or implement their own models within the uni ed framework.",
  "y": "extends"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_4",
  "x": "For example, in the case of expert nding [9] , this weight is the reciprocal of the document length of the document where the sequence was extracted from. is avoids a bias in the objective towards long documents. An alternative option that exists within the toolkit is to resample word sequence/entity pairs such that every entity is associated with the same number of word sequences, as used for product search [<cite>8</cite>] . Code snippet 1: Illustrative example of the SERT model interface.",
  "y": "similarities extends"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_5",
  "x": "e toolkit includes implementations of state-of-the-art representation learning models that were applied to expert nding [9] and product search [<cite>8</cite>] . Users of the toolkit can use these implementations to learn representations out-of-the-box or adapt the algorithms to their needs. In addition, users can implement their own models by extending an interface provided by the framework.",
  "y": "extends background"
 },
 {
  "id": "5f62958d0cdd32b15067c1afe458a5_7",
  "x": "e toolkit contains implementations of state-of-the-art entity representations algorithms [<cite>8</cite>, 9] and consists of three components: text processing, representation learning and inference. Users of the toolkit can easily make changes to existing model implementations or contribute their own models by extending an interface provided by the SERT framework. Future work includes integration with Pyndri [11] such that document collections indexed with Indri can transparently be used to train entity representations.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_0",
  "x": "Generating descriptions for videos has many applications including assisting blind people and human-robot interaction. The recent advances in image captioning as well as the release of large-scale movie description datasets such as <cite>MPII-MD</cite> <cite>[28]</cite> allow to study this task in more depth. Many of the proposed methods for image captioning rely on pre-trained object classifier CNNs and Long-Short Term Memory recurrent networks (LSTMs) for generating descriptions.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_2",
  "x": "Many of the proposed methods rely on Long-Short Term Memory networks (LSTMs) [13] . In the meanwhile, two large-scale <cite>movie description datasets</cite> have been proposed, namely <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . <cite>Both</cite> are based on movies with associated textual descriptions and allow studying the problem how to generate movie description for visually disabled people.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_3",
  "x": "In the meanwhile, two large-scale <cite>movie description datasets</cite> have been proposed, namely <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . <cite>Both</cite> are based on movies with associated textual descriptions and allow studying the problem how to generate movie description for visually disabled people. Works addressing these datasets <cite>[28,</cite> 33, 39] show that <cite>they</cite> are indeed challenging in terms of visual recognition and automatic description.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_4",
  "x": "Works addressing these datasets <cite>[28,</cite> 33, 39] show that <cite>they</cite> are indeed challenging in terms of visual recognition and automatic description. This results in a significantly lower performance then on simpler video datasets (e.g. MSVD [2] ), but a detailed analysis of the difficulties is missing. In this work we address this by taking a closer look at the performance of existing methods on the movie description task.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_5",
  "x": "This work contributes a) an approach to build robust visual classifiers which distinguish verbs, objects, and places extracted from weak sentence annotations; b) based on the visual classifiers we evaluate different design choices to train an LSTM for generating descriptions. This outperforms <cite>related work</cite> on the <cite>MPII-MD dataset</cite>, both using automatic and human evaluation; c) we perform a detailed analysis of prior work and our approach to understand the challenges of the movie description task. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_6",
  "x": "Recently two large-scale movie description datasets have been proposed, <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets. Consequently <cite>they</cite> are much more varied and challenging with respect to the visual content and the associated description.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_7",
  "x": "Recently two large-scale movie description datasets have been proposed, <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets. Consequently <cite>they</cite> are much more varied and challenging with respect to the visual content and the associated description.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_8",
  "x": "Recently two large-scale movie description datasets have been proposed, <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets. Consequently <cite>they</cite> are much more varied and challenging with respect to the visual content and the associated description.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_9",
  "x": "Recently two large-scale movie description datasets have been proposed, <cite>MPII Movie Description (MPII-MD)</cite> <cite>[28]</cite> and Montreal Video Annotation Dataset (M-VAD) [31] . Given that <cite>they</cite> are based on movies, <cite>they</cite> cover a much broader domain then previous video description datasets. Consequently <cite>they</cite> are much more varied and challenging with respect to the visual content and the associated description. <cite>They</cite> also do not have any additional annotations, as e.g. TACoS Multi-Level [27] , thus one has to rely on the weak annotations of the sentence descriptions.",
  "y": "background"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_10",
  "x": "To extract labels from sentences we rely on the semantic parser of <cite>[28]</cite> , however we treat the labels differently to handle the weak supervision (see Section 3.1). We show that this improves over <cite>[28]</cite> and [33] . Fig. 1 : Overview of our approach.",
  "y": "differences uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_11",
  "x": "To extract labels from sentences we rely on the semantic parser of <cite>[28]</cite> , however we treat the labels differently to handle the weak supervision (see Section 3.1). We show that this improves over <cite>[28]</cite> and [33] . Fig. 1 : Overview of our approach.",
  "y": "uses differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_12",
  "x": "As in <cite>[28]</cite> we parse the sentences to obtain a set of labels (single words or short phrases, e.g. look up) to train our visual classifiers. However, in contrast to <cite>[28]</cite> we do not want to keep all of these initial labels as they are noisy, but select only visual ones which actually can be robustly recognized. Avoiding parser failure.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_13",
  "x": "As in <cite>[28]</cite> we parse the sentences to obtain a set of labels (single words or short phrases, e.g. look up) to train our visual classifiers. However, in contrast to <cite>[28]</cite> we do not want to keep all of these initial labels as they are noisy, but select only visual ones which actually can be robustly recognized. Avoiding parser failure.",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_14",
  "x": "Now, how do we select visual labels for our semantic groups? In order to find the verbs among the labels we rely on the semantic parser of <cite>[28]</cite> . Next, we look up the list of \"places\" used in [41] and search for corresponding words among our labels.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_15",
  "x": "**EVALUATION** In this section we first analyze our approach on the <cite>MPII-MD</cite> <cite>[28]</cite> dataset and explore different design choices. Then, we compare our best system to <cite>prior work</cite>.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_17",
  "x": "Experimental setup. We build on the labels discovered by our semantic parser <cite>[28]</cite> and additionally match these labels to sentences which <cite>the parser</cite> failed to process. To be able to learn classifiers we select the labels that appear at least 30 times, resulting in 1,263 labels.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_18",
  "x": "We build on the labels discovered by our semantic parser <cite>[28]</cite> and additionally match these labels to sentences which <cite>the parser</cite> failed to process. To be able to learn classifiers we select the labels that appear at least 30 times, resulting in 1,263 labels. <cite>The parser</cite> additionally tells us whether the label is a verb.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_19",
  "x": "We use the visual features (DT, LSDA, PLACES) provided with the <cite>MPII-MD dataset</cite> <cite>[28]</cite> . The LSTM output/hidden unit as well as memory cell have each 500 dimensions. We train the SVM classifiers on the Training set (56,861 clips).",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_21",
  "x": "In most cases the ensemble improves over the single networks in terms of METEOR score (see Table 4 ). To summarize, the most important aspects that decrease over-fitting and lead to a better sentence generation are: (a) a correct learning rate and step size, (b) dropout after the LSTM layer, (c) choosing the training iteration based on METEOR score as opposed to only looking at the LSTM accuracy/loss which can be misleading, and (d) building ensembles of multiple networks with different random initializations. In the following section we evaluate our best ensemble (last line of Table 4 ) on the test set of <cite>MPII-MD</cite>.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_22",
  "x": "Experimental setup. We compare the best method of <cite>[28]</cite> , the recently proposed method S2VT [33] and our proposed \"Visual Labels\"-LSTM on the test set of the <cite>MPII-MD dataset</cite> (6,578 clips). We report all popular automatic evaluation measures, CIDEr [32] , BLEU [26] , ROUGE [22] and METEOR [21] , computed using the evaluation code of [3] .",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_23",
  "x": "We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank <cite>three systems</cite> (the best SMT of <cite>[28]</cite> , S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to <cite>[28]</cite> . Results. Moreover, we improve over the recent approach of [33] , which also uses LSTM to generate video descriptions.",
  "y": "similarities"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_24",
  "x": "Moreover, we improve over the recent approach of [33] , which also uses LSTM to generate video descriptions. Exploring different strategies to label selection and classifier training, as well as various LSTM configurations allows to obtain best result to date on the <cite>MPII-MD dataset</cite>. Human evaluation mainly agrees with the automatic measures.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_25",
  "x": "We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank <cite>three systems</cite> (the best SMT of <cite>[28]</cite> , S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to <cite>[28]</cite> . Results. Moreover, we improve over the recent approach of [33] , which also uses LSTM to generate video descriptions. Exploring different strategies to label selection and classifier training, as well as various LSTM configurations allows to obtain best result to date on the <cite>MPII-MD dataset</cite>. Human evaluation mainly agrees with the automatic measures. We outperform <cite>both prior works</cite> in terms of Correctness and Relevance, however we lose to S2VT in terms of Grammar.",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_26",
  "x": "The rather low METEOR score of 19.43 reflects the difficulty of the dataset. A closer look at the sentences produced by <cite>all three methods</cite> gives us additional insights. An interesting characteristic is the output vocabulary size, which is 94 for <cite>[28] ,</cite> 86 for [33] and 605 for our method, while the test set contains 6422 unique words.",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_27",
  "x": "A closer look at the sentences produced by <cite>all three methods</cite> gives us additional insights. An interesting characteristic is the output vocabulary size, which is 94 for <cite>[28] ,</cite> 86 for [33] and 605 for our method, while the test set contains 6422 unique words. This clearly shows a higher diversity of our output.",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_28",
  "x": "**ANALYSIS** Despite the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets (<cite>MPII-MD</cite> <cite>[28]</cite> and M-VAD [31] ) remains relatively low. In this section we want to take a closer look at <cite>three methods</cite>, best <cite>SMT</cite> of <cite>[28]</cite> , S2VT [33] and ours, in order to understand where these methods succeed and where they fail.",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_30",
  "x": "In the following we evaluate <cite>all three methods</cite> on the <cite>MPII-MD</cite> test set. ---------------------------------- **APPROACH SENTENCE**",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_35",
  "x": "We obtain the sense information from the semantic parser of <cite>[28]</cite> , thus senses might be noisy. We showcase the 5 most frequent verbs for each topic in Table 6 . We select sentences with a single verb, group them according to the verb topic and compute an average METEOR score for each topic, see Figure 6 .",
  "y": "uses"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_36",
  "x": "We find that our method is best for all topics except \"communication\", where <cite>[28]</cite> wins. The most frequent verbs in this topic are \"look up\" and \"nod\", which are also frequent in the dataset and in the sentences produced by <cite>[28]</cite> . The best performing topic, \"cognition\", is highly biased to \"look at\" verb.",
  "y": "differences"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_37",
  "x": "We select sentences with a single verb, group them according to the verb topic and compute an average METEOR score for each topic, see Figure 6 . We find that our method is best for all topics except \"communication\", where <cite>[28]</cite> wins. The most frequent verbs in this topic are \"look up\" and \"nod\", which are also frequent in the dataset and in the sentences produced by <cite>[28]</cite> .",
  "y": "similarities"
 },
 {
  "id": "5fa570cf5f37c7aae3b428a17de3e3_38",
  "x": "For sentence generation we show the benefits of exploring different LSTM architectures and learning configurations. As the result we obtain the highest performance on the <cite>MPII-MD</cite> dataset as shown by all automatic evaluation measures and extensive human evaluation. We analyze the challenges in the movie description task using our and <cite>two prior works</cite>.",
  "y": "uses"
 },
 {
  "id": "600317fc3ce88ea730993d3cc94f19_0",
  "x": "Since the first approach [Wright and Wrigley 1991] of combining a probabilistic method into the GLR technique was published, Some probabilistic GLR parsers also have been implemented in which probabilities are assigned to actions of LR parsing tables by using lookaheads or LR states as simple context information of <cite>[Briscoe and Carroll 1993]</cite> , [Kentaro et al. 1998 ], and [Ruland, 2000] which does not use the stack information of the GLR parser effectively, because of highly complex internal GLR stack. As a result, they have used relatively limited contextual information for disambiguation. [Kwak et al., 2001] have proposed a conditional action model that uses the partially constructed parse represented by the graph-structured stack as the additional context.",
  "y": "background"
 },
 {
  "id": "600317fc3ce88ea730993d3cc94f19_1",
  "x": "Since the first approach [Wright and Wrigley 1991] of combining a probabilistic method into the GLR technique was published, Some probabilistic GLR parsers also have been implemented in which probabilities are assigned to actions of LR parsing tables by using lookaheads or LR states as simple context information of <cite>[Briscoe and Carroll 1993]</cite> , [Kentaro et al. 1998 ], and [Ruland, 2000] which does not use the stack information of the GLR parser effectively, because of highly complex internal GLR stack. As a result, they have used relatively limited contextual information for disambiguation. [Kwak et al., 2001] have proposed a conditional action model that uses the partially constructed parse represented by the graph-structured stack as the additional context. However, this method inappropriately defined sub-tree structure. Our proposed model uses Surface Phrasal Types representing the structural characteristics of the sub-trees for its additional contextual information.",
  "y": "motivation"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_0",
  "x": "The aim of these models is to identify abusive language that directly targets certain individuals or groups, particularly people belonging to protected categories (Waseem et al., 2017) . Bias may reduce the accuracy of these models, and at worst, will mean that the models actively discriminate against the same groups they are designed to protect. Our study focuses on racial bias in hate speech and abusive language detection datasets (Waseem, 2016;<cite> Waseem and Hovy, 2016</cite>; Golbeck et al., 2017; Founta et al., 2018) , all of which use data collected from Twitter.",
  "y": "motivation background"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_1",
  "x": "We now briefly describe each of these datasets in chronological order. <cite>Waseem and Hovy (2016)</cite> collected 130k tweets containing one of seventeen different terms or phrases they considered to be hateful. They then annotated a sample of these tweets themselves, using guidelines inspired by critical race theory.",
  "y": "background"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_2",
  "x": "We observe substantial racial disparities in the performance of all classifiers. In all but one of the comparisons, there are statistically significant (p < 0.001) differences and in all but one of these we see that tweets in the black-aligned corpus are assigned negative labels more frequently than those by whites. The only case where blackaligned tweets are classified into a negative class less frequently than white-aligned tweets is the racism class in the <cite>Waseem and Hovy (2016)</cite> classifier.",
  "y": "differences"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_3",
  "x": "In most cases the racial disparities persist, although they are generally smaller in magnitude and in some cases the direction even changes. Table 3 shows that for tweets containing the word \"n*gga\", classifiers trained on <cite>Waseem and Hovy (2016)</cite> and Waseem (2016) are both predict black-aligned tweets to be instances of sexism approximately 1.5 times as often as white-aligned tweets. The classifier trained on the data is significantly less likely to classify black-aligned tweets as hate speech, although it is more likely to classify them as offensive.",
  "y": "similarities uses"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_4",
  "x": "For the Founta et al. (2018) classifier we see that black-aligned tweets are slightly less frequently considered to be hate speech but are much more frequently classified as abusive. The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 . We see similar results for <cite>Waseem and Hovy (2016)</cite> and Waseem (2016) .",
  "y": "similarities"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_5",
  "x": "In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism. The <cite>Waseem and Hovy (2016)</cite> classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class. For almost all of these tweets are classified as offensive, however those in the blackaligned corpus are 1.15 times as frequently classified as hate speech.",
  "y": "background"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_6",
  "x": "Classifiers trained on data from <cite>Waseem and Hovy (2016)</cite> and Waseem (2016) only predicted a small fraction of the tweets to be racism. We suspect that this is due to the composition of their dataset, since the majority of the racist training examples consist of anti-Muslim rather than anti- Table 4 : Experiment 2, t = \"b*tch\" black language. Across both datasets the words \"n*gger\" and \"n*gga\" appear in 4 and 10 tweets respectively.",
  "y": "differences"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_7",
  "x": "Some studies sampled tweets using small, ad hoc sets of keywords created by the authors<cite> (Waseem and Hovy, 2016</cite>; Waseem, 2016; Golbeck et al., 2017) , an approach demonstrated to produce poor results (King et al., 2017) . Others start with large crowdsourced dictionaries of keywords, which tend to include many irrelevant terms, resulting in high rates of false positives Founta et al., 2018) . In both cases, by using keywords to identify relevant tweets we are likely to get non-representative samples of training data that may over-or under-represent certain communities.",
  "y": "background"
 },
 {
  "id": "60c1245eff625441383913f947a8b1_8",
  "x": "Since individual biases in reflect societal prejudices, they aggregate into systematic biases in training data. The datasets considered here relied upon a range of different annotators, from the authors (Golbeck et al., 2017;<cite> Waseem and Hovy, 2016)</cite> and crowdworkers Founta et al., 2018) to activists (Waseem, 2016) . Even the classifier trained on expert-labeled data (Waseem, 2016) flags black-aligned tweets as sexist at almost twice the rate of white-aligned tweets.",
  "y": "background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_0",
  "x": "Apart from the expression of criticism and satisfaction in customer reviews, reviews might include suggestions for improvements. Suggestions can either be expressed explicitly (Brun, 2013) , or by expressing wishes regarding new features and improvements<cite> (Ramanand et al., 2010)</cite> (Table 1) . Extraction of suggestions goes beyond the scope of sentiment analysis, and also complements it by providing another valuable information that is worth analyzing.",
  "y": "background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_1",
  "x": "Table 1 presents some examples of occurrence of subjunctive mood collected from different forums on English grammar 1 . There seems to be a high probability of the occurrence of subjunctive mood in wish and suggestion expressing sentences. This observation can be exploited for the tasks of wish detection<cite> (Ramanand et al., 2010)</cite> , and suggestion extraction (Brun, 2013) . To the best of our knowledge, subjunctive mood has never been analysed in the context of wish and suggestion detection.",
  "y": "background motivation"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_2",
  "x": "Suggestion Detection: <cite>Ramanand et al. (2010)</cite> pointed out that wish is a broader category, which might not bear suggestions every time. They performed suggestion detection, where they focussed only on suggestion bearing wishes, and used manually formulated syntactic patterns for their detection. Brun (2013) also extracted suggestions from product reviews and used syntactico-semantic patterns for suggestion detection.",
  "y": "background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_3",
  "x": "<cite>Ramanand et al. (2010)</cite> worked on product review dataset of the wish corpus, with an objective to extract suggestions for improvements. They considered suggestions as a subset of wishes, and thus retained the labels of only suggestion bearing wishes. They also annotated additional product reviews, but their data is not available for open research.",
  "y": "background"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_4",
  "x": "Annotation Details: We had 2 annotators annotate each sentence with a suggestion or non-suggestion tag. We support the observation of <cite>Ramanand et al. (2010)</cite> that wishes for improvements and new features are implicit expression of suggestions. Therefore, annotators were also asked to annotate suggestions which were expressed as wishes.",
  "y": "similarities motivation"
 },
 {
  "id": "61f88b86c451fb6a5e5893c8c42a24_5",
  "x": "VerbNet is a wide coverage verb lexicon, which places verbs into classes whose members have common syntactic and semantic properties. We collect all members of the VerbNet verb classes advice, wish, want, urge, require; 28 different verbs were obtained. <cite>Ramanand et al. (2010)</cite> also used a similar but much smaller subset {love, like, prefer and suggest} in their rules.",
  "y": "similarities"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_0",
  "x": "Thus, this approach cannot answer the question of why these linguistic variables have the effects they do on eye movement behavior. Recently, <cite>Bicknell and Levy (2010)</cite> presented a model of eye movement control in reading that directly models the process of identifying the text from visual input, and makes eye movements to maximize the efficiency of the identification process. Bicknell and Levy (2012) demonstrated that this rational model produces effects of word frequency and predictability that qualitatively match those of humans: words that are less frequent and less predictable receive more and longer fixations.",
  "y": "background"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_1",
  "x": "Because this model makes eye movements to maximize the efficiency of the identification process, this result gives an answer for the reason why these variables should have the effects that they do on eye movement behavior: a model that works to efficiently identify the text makes more and longer fixations on 21 words of lower frequency and predictability because it needs more visual information to identify them. Bicknell (2011) showed, however, that the effects of word length produced by the rational model look quite different from those of human readers. Because<cite> Bicknell and Levy's (2010)</cite> model implements the main proposal for why word length effects should arise, i.e., visual acuity limitations, the fact that the model does not reproduce humanlike word length effects suggests that our understanding of the causes of word length effects may be incomplete.",
  "y": "background"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_2",
  "x": "Because<cite> Bicknell and Levy's (2010)</cite> model implements the main proposal for why word length effects should arise, i.e., visual acuity limitations, the fact that the model does not reproduce humanlike word length effects suggests that our understanding of the causes of word length effects may be incomplete. In this paper, we argue that this result arose because of a simplifying assumption made in the rational model, namely, the assumption that the reader has veridical knowledge about the number of characters in a word being identified. We present an extension of<cite> Bicknell and Levy's (2010)</cite> model which does not make this simplifying assumption, and show in two sets of simulations that effects of word length produced by the extended model look more like those of humans.",
  "y": "extends differences"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_3",
  "x": "Determining how these different factors should interact to produce word length effects requires a model of eye movements in reading that models the process of word identification from disambiguating visual input (Bicknell & Levy, in press ). The model presented by <cite>Bicknell and Levy (2010)</cite> fits this description, and includes visual acuity limitations (in fact, identical to the visual acuity function in SWIFT). As already mentioned, how-22 ever, Bicknell (2011) showed that the model did not yield a humanlike length effect.",
  "y": "similarities"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_4",
  "x": "This would agree with the intuition that an 8-character word should be more easily confused with a 9-character word than a 3-character word with a 4-character word. Including uncertainty about word length that is larger for longer words would have the effect of increasing the number of visual neighbors for longer words more than for shorter words, providing another reason (in addition to visual acuity limitations) that longer words may require more and longer fixations. In the remainder of this paper, we describe an extension of<cite> Bicknell and Levy's (2010)</cite> model in which visual input provides stochastic -rather than veridical -information about the length of words, yielding uncertainty about word length, and in which the amount of uncertainty grows with length.",
  "y": "extends differences"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_5",
  "x": "In this section, we describe our extension of<cite> Bicknell and Levy's (2010)</cite> rational model of eye movement control in reading. Except for the visual input system, and a small change to the behavior policy to allow for uncertainty about word length, the model is identical to that described by Bicknell and Levy. The reader is referred to that paper for further computational details beyond what is described here.",
  "y": "extends"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_6",
  "x": "---------------------------------- **LANGUAGE KNOWLEDGE** Following <cite>Bicknell and Levy (2010)</cite>, we use very simple probabilistic models of language knowledge: word n-gram models (Jurafsky & Martin, 2009) , which encode the probability of each word conditional on the n \u2212 1 previous words.",
  "y": "similarities uses"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_7",
  "x": "We now assess the effects of word length produced by the extended version of the model. Following Bicknell (2011), we use the model to simulate reading of a modified version of the Schilling, Rayner, and Chumbley (1998) corpus of typical sentences used in reading experiments. We compare three levels of length uncertainty: \u03b4 \u2208 {0, .05, .1}. The first of these (\u03b4 = 0) corresponds to<cite> Bicknell and Levy's (2010)</cite> model, which has no uncertainty about word length.",
  "y": "similarities"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_8",
  "x": "**TEST CORPUS** We test the model on a corpus of 33 sentences from the Schilling corpus slightly modified by <cite>Bicknell and Levy (2010)</cite> so that every bigram occurred in the BNC, ensuring that the results do not depend on smoothing. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "6388fd7167389982be2f01fbe594cd_9",
  "x": "**CONCLUSION** In this paper, we argued that the success of major models of eye movements in reading to reproduce the (positive) human effect of word length via acuity limitations may be a result of not including opposing factors such as the negative correlation between visual neighborhood size and word length. We described the failure of the rational model presented in <cite>Bicknell and Levy (2010)</cite> to obtain humanlike effects of word length, despite including all of these factors, suggesting that our understanding of word length effects in reading is incomplete.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_0",
  "x": "By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task. However, <cite>their</cite> approach relied on a modified QA model architecture and a dedicated slot-filling training corpus. Here, we investigate the utility of standard QA data and models for this task.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_1",
  "x": "One common form of this task is slot filling (Surdeanu and Heng, 2014) , in which a knowledge base (KB) query, such as place of birth(Obama, ?) is applied to a set of documents and a set of slot fillers is returned. By converting such KB queries to natural language questions, <cite>Levy et al. (2017)</cite> showed that a question answering (QA) system could be effectively applied to this task. However, <cite>their</cite> approach relied on a modified QA model architecture and a dedicated slot-filling training corpus.",
  "y": "motivation"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_2",
  "x": "Starting at the top right, a KB query is translated into a natural language question, which can then be fed into a QA model that has been trained on an appropriate resource. When applied to a set of texts, this model needs to predict the correct answer within each text, including the possibility that a text contains no answer. Within this framework, we consider different models and training and test datasets, but we keep the translation of KB queries into natural language questions fixed, based on the crowd-sourced templates used by <cite>Levy et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_3",
  "x": "---------------------------------- **PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a QA dataset in relation to the slot-filling task of <cite>Levy et al. (2017)</cite> .",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_4",
  "x": "**PERFORMANCE ON THE ORIGINAL TASK** In our first experiment, we examine the utility of a QA dataset in relation to the slot-filling task of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zero-shot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_5",
  "x": "In our first experiment, we examine the utility of a QA dataset in relation to the slot-filling task of <cite>Levy et al. (2017)</cite> . <cite>Their</cite> zero-shot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth. Identifying an instance of such a relation in text is then equivalent to finding an answer to the relevant question template, instantiated with the appropriate entity. However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this <cite>they</cite> trained a slightly modified version of BiDAF (Seo et al., 2016) (Rajpurkar et al., 2016) , can be applied to the relation extraction task.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_6",
  "x": "This is motivated both by curiosity about the generalisation abilities of such QA models and also a practical interest in relation extraction applications. We first investigate the zero-shot case, where no examples of the relations are available, and then evaluate how performance improves as more data becomes available. Data We compare two sources of training data: The University of Washington relation extraction (UWRE) dataset created by <cite>Levy et al. (2017)</cite> and the Stanford Question Answering Dataset (SQuAD) created by Rajpurkar et al. (2016) .",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_7",
  "x": "<cite>Levy et al. (2017)</cite> provide a number of train/dev/test splits, to allow <cite>them</cite> to evaluate a variety of modes of generalisation. Here we use the relation and entity splits. The former tests the ability to generalise from one set of relations to another, i.e. to do zero-shot learning for the unseen relations in the test set.",
  "y": "background"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_8",
  "x": "Random samples of 10 3 , 10 4 , 10 5 and 10 6 UWRE instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched. Models We employ the same modified BiDAF (Seo et al., 2016) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model.",
  "y": "uses similarities"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_9",
  "x": "Models We employ the same modified BiDAF (Seo et al., 2016) model as <cite>Levy et al. (2017)</cite> , which uses an additional bias term to allow the model to signal when no answer is predicted within the text. Evaluation Following the approach of <cite>Levy et al. (2017)</cite> , we report F1 scores on the answers returned by the model. Under this measure, predicting correctly that a negative instance has no answer does not contribute to either precision or recall.",
  "y": "uses"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_10",
  "x": "Other models may show different patterns of strength and weakness, but to be able to investigate and exploit further QA systems quickly would require a means of producing 'no answer' predictions without the need to modify the model implementation. 4 Using an unmodified QA model for slot filling <cite>Levy et al. (2017)</cite> modify the BiDAF architecture to produce an additional output representing the probability that no answer is present in the text. In this experiment, we investigate whether it is possible to adapt a QA model to the slot filling task without having to understand and modify its internal structure and implementation.",
  "y": "motivation"
 },
 {
  "id": "64d11e9efeaa735f74585d6998bab7_11",
  "x": "Results Table 3 reveals that the unmodified BiDAF model is almost as effective as the <cite>Levy et al. (2017)</cite> model in terms of zero-shot F1 on the original UWRE test set. In contrast, FastQA's performance is substantially worse. However, Table 4 reveals that FastQA is extremely accurate on the challenge test set, while BiDAF's performance is comparable to the modified model trained on SQuAD.",
  "y": "differences"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_0",
  "x": "Recent work by <cite>Nerbonne and Wiersma (2006)</cite> has provided a foundation for measuring syntactic differences between corpora. It uses part-of-speech trigrams as an approximation to syntactic structure, comparing the trigrams of two corpora for statistically significant differences. This paper extends the method and its application.",
  "y": "background"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_1",
  "x": "Heeringa and others have also done related work on phonological distance in Nerbonne and Heeringa (1997) and Gooskens and Heeringa (2004) . A measure of syntactic distance is the obvious next step: <cite>Nerbonne and Wiersma (2006)</cite> provide one such method. This method approximates internal syntactic structure using vectors of part-of-speech trigrams.",
  "y": "background motivation"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_2",
  "x": "The sentences of the corpus, being represented as parse trees rather than a vector of POS tags, are converted into a vector of leafancestor paths, which were developed by Sampson (2000) to aid in parser evaluation by providing a way to compare gold-standard trees with parser output trees. In this way, each sentence produces its own vec-tor of leaf-ancestor paths. Fortunately, the permutation test used by <cite>Nerbonne and Wiersma (2006)</cite> is already designed to normalize the effects of differing sentence length when combining POS trigrams into a single vector per region.",
  "y": "motivation"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_3",
  "x": "**METHODS** The methods used to implement the syntactic difference test come from two sources. The primary source is the syntactic comparison of <cite>Nerbonne and Wiersma (2006)</cite> , which uses a permutation test, explained in Good (1995) and in particular for linguistic purposes in Kessler (2001) .",
  "y": "similarities uses"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_4",
  "x": "The trigram frequencies are normalized to neutralize the effects of sentence length, then compared to the trigram frequencies of the complete corpora. The principal difference between the work of <cite>Nerbonne and Wiersma (2006)</cite> and ours is the use of leaf-ancestor paths. Leaf-ancestor paths were developed by Sampson (2000) for estimating parser performance by providing a measure of similarity of two trees, in particular a gold-standard tree and a machine-parsed tree.",
  "y": "differences"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_5",
  "x": "Leaf-ancestor paths were developed by Sampson (2000) for estimating parser performance by providing a measure of similarity of two trees, in particular a gold-standard tree and a machine-parsed tree. This distance is not used for our method, since for our purposes, it is enough that leaf-ancestor paths represent syntactic information, such as upper-level tree structure, more explicitly than trigrams. The permutation test used by <cite>Nerbonne and Wiersma (2006)</cite> is independent of the type of item whose frequency is measured, treating the items as atomic symbols.",
  "y": "background"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_6",
  "x": "Fortunately, this is not the case; <cite>Nerbonne and Wiersma (2006)</cite> generate N \u2212 2 POS trigrams from each sentence of length N ; we generate N leaf-ancestor paths from each parsed sentence in the corpus. Normalization is needed to account for the frequency differences caused by sentence length variation; it is presented below. Since the same number (minus two) of trigrams and leaf-ancestor paths are generated for each sentence the same normalization can be used for both methods.",
  "y": "differences"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_7",
  "x": "However, to find out if the value of R is significant, we must use a permutation test with a Monte Carlo technique described by Good (1995) , following closely the same usage by <cite>Nerbonne and Wiersma (2006)</cite> . The intuition behind the technique is to compare the R of the two corpora with the R of two random subsets of the combined corpora. For comparison to the experiment conducted by <cite>Nerbonne and Wiersma (2006)</cite> , the experiment was also run with POS trigrams.",
  "y": "similarities uses"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_8",
  "x": "The average corpus was smaller than the Norwegian L2 English corpora of <cite>Nerbonne and Wiersma (2006)</cite> , which had two groups, one with 221,000 words and the other with 84,000. Significant differences (at p < 0.05) were found when comparing the largest regions, but no significant differences were found when comparing small regions to other small regions. The significant differences found are given in table 2 and 3.",
  "y": "differences"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_9",
  "x": "---------------------------------- **DISCUSSION** Our work extends that of <cite>Nerbonne and Wiersma (2006)</cite> in a number of ways.",
  "y": "extends"
 },
 {
  "id": "6597d733f13b06f61cb653f86c4460_10",
  "x": "This is very likely, because corpus divisions that better reflect reality have a better chance of achieving a significant difference. In fact, even though leaf-ancestor paths should provide finer distinctions than trigrams and thus require more data for detectable significance, the regional corpora presented here were smaller than the Norwegian speakers' corpora in <cite>Nerbonne and Wiersma (2006)</cite> by up to a factor of 10. This raises the question of a lower limit on corpus size.",
  "y": "differences"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_0",
  "x": "****NRC: INFUSED PHRASE VECTORS FOR NAMED ENTITY RECOGNITION IN TWITTER**** **ABSTRACT** Our submission to the W-NUT Named Entity Recognition in Twitter task closely follows the approach detailed by <cite>Cherry and Guo (2015)</cite> , who use a discriminative, semi-Markov tagger, augmented with multiple word representations.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_1",
  "x": "Our submission to this competition closely follows <cite>Cherry and Guo (2015)</cite> , who advocate the use of a semi-Markov tagger trained online with standard discriminative tagging features, gazetteer matches, Brown clusters, and word embeddings. We augment this approach with updated gazetteers, phrase embeddings, and infused embeddings that have been adapted to better predict gazetteer membership. Our novel infusion technique allows us to adapt existing vectors to NER regardless of their source, by training a typelevel auto-encoder whose hidden layer must predict the corresponding phrase's gazetteer memberships while also recovering the original vector.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_2",
  "x": "We use unannotated tweets to build various word representations (see Section 3.1). Our unannotated corpus collects 98M tweets (1,995M tokens) from between May 2011 and April 2012. The same corpus is used by <cite>Cherry and Guo (2015)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_3",
  "x": "---------------------------------- **BASE TAGGER** We first summarize the approach of <cite>Cherry and Guo (2015)</cite> , which we build upon for our system.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_4",
  "x": "Note that we do not include part-of-speech tags as features, as they were not found to be useful by <cite>Cherry and Guo (2015)</cite> . ---------------------------------- **UPDATED GAZETTEERS**",
  "y": "extends differences"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_5",
  "x": "We employ an auto-encoder to leverage knowledge derived from domain-specific gazetteers to make the distributed phrase representations more relevant to our NER task. In recent years, two sources of information have been found to be valuable to boost the performance for NER: distributed representation learned from a large corpus and domain-specific lexicons (Turian et al., 2010;<cite> Cherry and Guo, 2015)</cite> . Research has also shown that merging these two forms of information can result in further predictive improvement for an NER system (Passos et al., 2014) .",
  "y": "background"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_6",
  "x": "C&G 2015 adds Brown clusters and word embeddings to create a complete re-implementation of <cite>Cherry and Guo (2015)</cite> . We can see that these representations have a huge impact on NER performance for all dev and test sets. We then performed a careful hyper-parameter sweep using the two provided development sets, resulting in the Inc. Regularization system.",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_7",
  "x": "We then performed a careful hyper-parameter sweep using the two provided development sets, resulting in the Inc. Regularization system. The hyper-parameters suggested by <cite>Cherry and Guo (2015)</cite> (E=10, C=0.01, P=10) were selected to work well with and without representations. We found that once we have committed to using representations, the tagger benefits from increased regularization, so long as we allow the model to converge (E=30, C=0.001, P=8).",
  "y": "similarities uses"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_8",
  "x": "**CONCLUSION** We have summarized our entry to the first W-NUT Named Entity Recognition in Twitter task. Our entry extends the work of <cite>Cherry and Guo (2015)</cite> with updated lexicons, phrase embeddings, and gazetteer-infused phrase embeddings.",
  "y": "extends differences"
 },
 {
  "id": "65f6a6fce98c511473a3ab144a73e4_9",
  "x": "Our gazetteer infusion technique is novel in that it allows us to adapt existing vectors, regardless of their source. Taken together with improved hyper-parameters, these extensions improve the approach of <cite>Cherry and Guo (2015)</cite> by 2.6 Fmeasure on a completely blind test. Our final submission achieves a test F-measure of 44.7, placing third in the competition, and could have achieved an F-measure of 54.2 had we included all development data as training data.",
  "y": "extends differences"
 },
 {
  "id": "65fbdd0397473763bca35376d581be_0",
  "x": "To cater to these target reader populations, language teachers, linguists and other editors are often called upon to manually adapt a text. To automate this time-consuming task, there has been much effort in developing systems for lexical simplification (Zhu et al., 2010; Biran et al., 2011) and syntactic simplification (Siddharthan, 2002; Siddharthan and Angrosh, 2014) . The performance of the state-of-the-art systems has improved significantly<cite> (Horn et al., 2014</cite>; Siddharthan and Angrosh, 2014) .",
  "y": "background"
 },
 {
  "id": "65fbdd0397473763bca35376d581be_1",
  "x": "Following <cite>Horn et al. (2014)</cite> , our system simplifies neither proper nouns, as identified by the Natural Language Toolkit (Bird et al., 2009) , nor words in our stoplist, which are already simple. In terms of the three-step framework described above, we use the word2vec model 1 to retrieve candidates for substitution in the first step. We trained the model with all sentences from Wikipedia.",
  "y": "uses"
 },
 {
  "id": "65fbdd0397473763bca35376d581be_2",
  "x": "---------------------------------- **EVALUATION** We evaluated the performance of our algorithm on the Mechanical Turk Lexical Simplification Data Set<cite> (Horn et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "65fbdd0397473763bca35376d581be_3",
  "x": "---------------------------------- **EVALUATION** We evaluated the quality of syntactic simplification on the first 300 sentences in the Mechanical Turk Lexical Simplification Data Set<cite> (Horn et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_0",
  "x": "In previous work <cite>(Lucena & Paraboni, 2008)</cite> we presented a frequency-based greedy attribute selection strategy submitted to the TUNA Challenge 2008. Presently we further the issue by taking additional information into accountnamely, the trial condition information available from the TUNA data -and report improved results for string-edit distance as required for the 2009 competition. ----------------------------------",
  "y": "background"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_1",
  "x": "**INTRODUCTION** In previous work <cite>(Lucena & Paraboni, 2008)</cite> we presented a frequency-based greedy attribute selection strategy submitted to the TUNA Challenge 2008. Presently we further the issue by taking additional information into accountnamely, the trial condition information available from the TUNA data -and report improved results for string-edit distance as required for the 2009 competition.",
  "y": "extends background"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_2",
  "x": "In<cite> Lucena & Paraboni (2008)</cite> we presented a combined strategy based on attribute frequency and certain aspects of a greedy attribute selection strategy for referring expressions generation. A list P of attributes sorted by frequency is the centre piece of the following selection strategy: \u2022 select all attributes whose relative frequency falls above a threshold value t (t was estimated to be 0.8 for both Furniture and People domains.) \u2022 if the resulting description uniquely describes the target object, then finalizes.",
  "y": "background"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_3",
  "x": "Once again, this may be comparable to what a human speaker may do when an appropriate attribute becomes sufficiently salient and all distractors in the context can be ruled out at once. The above approach performed fairly well (at least considering its simplicity) as reported in<cite> Lucena & Paraboni (2008)</cite> . However, there is one major source of information available from the TUNA data that was not taken into account in the above strategy: the trial condition represented by the +/-LOC feature.",
  "y": "background"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_4",
  "x": "The above approach performed fairly well (at least considering its simplicity) as reported in<cite> Lucena & Paraboni (2008)</cite> . However, there is one major source of information available from the TUNA data that was not taken into account in the above strategy: the trial condition represented by the +/-LOC feature. This clear gap in our previous work represents an opportunity for improvement discussed in the next section.",
  "y": "motivation"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_5",
  "x": "The present work is a refined version of the original frequency-based greedy attribute selection strategy submitted to the TUNA Challenge 2008 <cite>(Lucena & Paraboni, 2008)</cite> , now taking also the trial condition (+/-LOC) into account. In the TUNA data, +LOC indicates the instances of the experiment in which participants were told that they were allowed to refer to the X,Y coordinates of the screen (i.e., selecting the X-and/or Y-DIMENSION attributes), whereas -LOC indicates the trials in which they were discouraged (but not prevented) to do so. In practice, references in +LOC trials are more likely to convey the X-and Y-DIMENSION attributes than those in which the -LOC condition was applied.",
  "y": "extends"
 },
 {
  "id": "6781a5d131e68f7a7ac2fe239cc3d0_6",
  "x": "For ease of comparison with our previous work, we also present Dice and MASI scores computed as in the previous TUNA Challenge, although these scores were not required for the current competition. The most relevant comparison with our previous work is observed in the overall string-edit distance values in Figure 1 : considering that in<cite> Lucena & Paraboni (2008)</cite> we reported 6.12 editdistance for Furniture and 7.38 for People, the overall improvement (driven by the descriptions in the Furniture domain) may be explained by the fact that the current version makes more accurate decisions as to when to use these attributes according to the instructions given to the participants of the TUNA trials (the trial condition +/-LOC. ) On the other hand, the divide between +LOC and -LOC strategies does not have a significant effect on the results based on the semantics of the description (i.e., Dice and MASI scores), which remain the same as those obtained previously.",
  "y": "differences"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_0",
  "x": "It is therefore desirable for relation extraction models to have the capability to learn continuously without catastrophic forgetting of previously learned relations. This would enable them exploit newly available supervision to both identify novel relations and improve performance without substantial retraining. Recently, <cite>Wang et al. (2019)</cite> introduced an embedding alignment approach to enable continual learning for relation extraction models.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_1",
  "x": "This would enable them exploit newly available supervision to both identify novel relations and improve performance without substantial retraining. Recently, <cite>Wang et al. (2019)</cite> introduced an embedding alignment approach to enable continual learning for relation extraction models. <cite>They</cite> consider a setting with streaming tasks, where each task consists of a number of distinct relations, and proposed to align the representation of relation instances in the embedding space to enable continual learning of new relations without forgetting knowledge from past relations.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_2",
  "x": "Recently, <cite>Wang et al. (2019)</cite> introduced an embedding alignment approach to enable continual learning for relation extraction models. While <cite>they</cite> obtained promising results, a key weakness of the approach is that the use of an alignment model introduces additional parameters to already overparameterized relation extraction models, which may in turn lead to an increase in the quantity of supervision required for training.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_3",
  "x": "In this work, we extend the work of <cite>Wang et al. (2019)</cite> by exploiting ideas from both lifelong learning and meta-learning. We propose to consider lifelong relation extraction as a metalearning challenge, to which the machinery of current optimization-based meta-learning algorithms can be applied. Unlike the use of a separate alignment model as proposed in <cite>Wang et al. (2019)</cite> , the proposed approach does not introduce additional parameters.",
  "y": "extends"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_4",
  "x": "We propose to consider lifelong relation extraction as a metalearning challenge, to which the machinery of current optimization-based meta-learning algorithms can be applied. Unlike the use of a separate alignment model as proposed in <cite>Wang et al. (2019)</cite> , the proposed approach does not introduce additional parameters. In addition, the proposed approach is more data efficient since it explicitly optimizes for the transfer of knowledge from past relations, while avoiding the catastrophic forgetting of previously learned relations.",
  "y": "extends"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_5",
  "x": "In order to use the same number of parameters and ensure fair comparison to <cite>Wang et al. (2019)</cite> , we adopt as the relation extraction model f \u03b8 the Hier- arachical Residual BiLSTM (HR-BiLSTM) model of Yu et al. (2017) , which is the same model used by <cite>Wang et al. (2019)</cite> for their experiments. The HR-BILSTM is a relation classifier which accepts as input a sentence and a candidate relation, then utilizes two Bidirectional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005) (BiLSTM) units with shared parameters to process the Glove (Pennington et al., 2014) embeddings of words in the sentence and relation names, then selects the relation with the maximum cosine similarity to the sentence as its response. Hyperparameters Apart from the hyperparameters specific to meta-learning (such as the step size ), all other hyperparameters we use for the learner model are the same as used by <cite>Wang et al. (2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_6",
  "x": "The HR-BILSTM is a relation classifier which accepts as input a sentence and a candidate relation, then utilizes two Bidirectional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005) (BiLSTM) units with shared parameters to process the Glove (Pennington et al., 2014) embeddings of words in the sentence and relation names, then selects the relation with the maximum cosine similarity to the sentence as its response. Hyperparameters Apart from the hyperparameters specific to meta-learning (such as the step size ), all other hyperparameters we use for the learner model are the same as used by <cite>Wang et al. (2019)</cite> . We also use the same buffer memory size (50) for each task.",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_7",
  "x": "**DATASETS** We conduct experiments on <cite>Lifelong FewRel</cite> and Lifelong <cite>SimpleQuestions</cite> datasets, both introduced in <cite>Wang et al. (2019)</cite> . <cite>Lifelong FewRel</cite> is derived from the FewRel (Han et al., 2018) dataset, by partitioning its 80 relations into 10 distinct clusters made up of 8 relations each, with each cluster serving as a task where a sentence must be labeled with the correct relation.",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_8",
  "x": "**DATASETS** We conduct experiments on <cite>Lifelong FewRel</cite> and Lifelong <cite>SimpleQuestions</cite> datasets, both introduced in <cite>Wang et al. (2019)</cite> . <cite>Lifelong FewRel</cite> is derived from the FewRel (Han et al., 2018) dataset, by partitioning its 80 relations into 10 distinct clusters made up of 8 relations each, with each cluster serving as a task where a sentence must be labeled with the correct relation.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_10",
  "x": "**EVALUATION METRICS** We report two measures, <cite>ACC whole</cite> and <cite>ACC avg</cite> , both introduced in <cite>Wang et al. (2019)</cite> . <cite>ACC whole</cite> measures accuracy on the test set of all tasks and gives a balanced measure of model performance on both observed (seen) and unobserved (unseen) tasks, and is the primary metric we report for all experiments.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_11",
  "x": "**EVALUATION METRICS** We report two measures, <cite>ACC whole</cite> and <cite>ACC avg</cite> , both introduced in <cite>Wang et al. (2019)</cite> . <cite>ACC whole</cite> measures accuracy on the test set of all tasks and gives a balanced measure of model performance on both observed (seen) and unobserved (unseen) tasks, and is the primary metric we report for all experiments.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_12",
  "x": "We report two measures, <cite>ACC whole</cite> and <cite>ACC avg</cite> , both introduced in <cite>Wang et al. (2019)</cite> . We also report <cite>ACC avg</cite> , which measures the average accuracy on the test set of only observed (seen) tasks.",
  "y": "background"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_13",
  "x": "Full Supervision Results Table 1 gives both the <cite>ACC whole</cite> and <cite>ACC avg</cite> results of our approach compared to other approaches including Episodic Memory Replay (EMR) and its various embedding-aligned variants EA-EMR as proposed in <cite>Wang et al. (2019)</cite> . Across all metrics, our approach outperforms the previous approaches, demonstrating its effectiveness in this setting. This result is likely because our approach is able to efficiently learn new relations by exploiting knowledge from previously observed relations.",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_14",
  "x": "---------------------------------- **LIMITED SUPERVISION RESULTS** The aim of our limited supervision experiments is to compare the use of an alignment module as proposed by <cite>Wang et al. (2019)</cite> to using our approach when only limited supervision is available for all tasks.",
  "y": "uses"
 },
 {
  "id": "681c3e59adbfc09a28d267a4885598_15",
  "x": "We conduct experiments on <cite>Lifelong FewRel</cite> and Lifelong <cite>SimpleQuestions</cite> datasets, both introduced in <cite>Wang et al. (2019)</cite> . Figures 1(a) and 1(b) show results obtained using 100 supervision instances for each task on <cite>Lifelong FewRel</cite> and <cite>Lifelong SimpleQuestions</cite>.",
  "y": "uses"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_0",
  "x": "How many literary characters appear in a novel? Despite the seeming simplicity of the question, precisely identifying which characters appear in a story remains an open question in literary and narrative analysis. Characters form the core of many computational analyses, from inferring prototypical character types (Bamman et al., 2014) to identifying the structure of social networks in literature <cite>(Elson et al., 2010</cite>; Lee and Yeung, 2012; Agarwal et al., 2013; Ardanuy and Sporleder, 2014; Jayannavar et al., 2015) .",
  "y": "background"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_1",
  "x": "Second, we propose a new technique for character detection based on inducing character prototypes, and in comparisons with three state-of-the-art methods, demonstrate superior performance, achieving significant improvements in F1 over the next-best method. Third, as practical applications, we analyze literary trends in character density over 20 decades and revisit the character-based literary hypothesis tested by<cite> Elson et al. (2010)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_2",
  "x": "Two approaches mined social interaction net-works without relying on dialogue, unlike the methods of<cite> Elson et al. (2010)</cite> and He et al. (2013) . Lee and Yeung (2012) build social networks by recognizing characters from explicit markers (e.g., kinship) and implicit markers (e.g., physical collocation). Similarly, Agarwal and Rambow (2010) build character networks using tree kernels on parse trees to identify interacting agents.",
  "y": "background"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_3",
  "x": "The task of character recognition has largely been subsumed into the task of extracting the social network of novels. Therefore, three state-of-the-art systems for social network extraction were selected: the method described in<cite> Elson et al. (2010)</cite> , BookNLP (Bamman et al., 2014) , and the method described in Ardanuy and Sporleder (2014) . For each method, we follow their procedures for identifying the characters in the social network, which produces sets of one or more aliases associated with each identified character.",
  "y": "uses"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_4",
  "x": "Thus, we argue that our procedure can provide a reasonable estimate for the total number of characters. (For comparison, BookNLP, the next best system, extracted 69 and 72 characters for Pride and Prejudice and The Moonstone, respectively, and within 1.2, on average, on the Sherlock Holmes set.) Experiment 2: Literary Theories<cite> Elson et al. (2010)</cite> analyze 60 novels to computationally test literary theories for novels in urban and rural settings (Williams, 1975; Moretti, 1999) . Recently, Jayannavar et al. (2015) challenged this analysis, showing their improved method for social network extraction did not support the same conclusions.",
  "y": "uses"
 },
 {
  "id": "684349c06be7ff51c0944b25be10ce_5",
  "x": "While our work focuses only on character detection, we are nevertheless able to test the related hypothesis of whether the number of characters in novels with urban settings is more than those in rural. Character detection was run on the same novels from<cite> Elson et al. (2010)</cite> and we found no statistically-significant difference in the mean number of characters in urban and rural settings, even when accounting for text size. Thus, our work raises questions about how these character interact and whether the setting influences the structure of the social network, despite similar numbers of characters.",
  "y": "uses"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_0",
  "x": "Abstractive summarization, the task of rewriting a document into a short summary is a significantly more challenging (and natural) task than extractive summarization, which only involves choosing which sentence from the original document to keep or discard in the output summary. Neural sequence-to-sequence models have led to substantial improvements on this task of abstractive summarization, via machine translation inspired encoder-aligner-decoder approaches, further enhanced via convolutional encoders, pointer-copy mechanisms, and hierarchical attention (Rush et al., 2015;<cite> Nallapati et al., 2016</cite>; See et al., 2017) . Despite these promising recent improvements, Input Document: may is a pivotal month for moving and storage companies .",
  "y": "background"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_1",
  "x": "Neural sequence-to-sequence models have led to substantial improvements on this task of abstractive summarization, via machine translation inspired encoder-aligner-decoder approaches, further enhanced via convolutional encoders, pointer-copy mechanisms, and hierarchical attention (Rush et al., 2015;<cite> Nallapati et al., 2016</cite>; See et al., 2017) . Despite these promising recent improvements, Input Document: may is a pivotal month for moving and storage companies . Ground-truth Summary: moving companies hit bumps in economic road Baseline Summary: a month to move storage companies Multi-task Summary: pivotal month for storage firms there is still scope in better teaching summarization models about the general natural language inference skill of logical entailment generation.",
  "y": "motivation"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_2",
  "x": "Bigger datasets and neural models have allowed the addressing of the complex reasoning involved in abstractive summarization, i.e., rewriting and compressing the input document into a new summary. Several advances have been made in this direction using machine translation inspired encoder-aligner-decoder models, convolution-based encoders, switching pointer and copy mechanisms, and hierarchical attention models (Rush et al., 2015;<cite> Nallapati et al., 2016</cite>; See et al., 2017) . Recognizing textual entailment (RTE) is the classification task of predicting whether the relationship between a premise and hypothesis sentence is that of entailment (i.e., logically follows), contradiction, or independence (Dagan et al., 2006) .",
  "y": "background"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_3",
  "x": "**EVALUATION** Following previous work<cite> (Nallapati et al., 2016</cite>; Chopra et al., 2016; Rush et al., 2015) , we use the full-length F1 variant of Rouge (Lin, 2004) for the Gigaword results, and the 75-bytes length limited Recall variant of Rouge for DUC. Additionally, we also report other standard language generation metrics (as motivated recently by See et al. (2017) ): METEOR (Denkowski and Lavie, 2014) , BLEU-4 (Papineni et al., 2002) , and CIDEr-D , based on the MS-COCO evaluation script (Chen et al., 2015) .",
  "y": "uses"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_5",
  "x": "In Table 2 , we again see that et al. (2015) 28.18 8.49 23.81 Chopra et al. (2016) 28.97 8.26 24.06<cite> Nallapati et al. (2016)</cite> our Luong et al. (2015) baseline model achieves competitive performance with previous work, esp. on Rouge-2 and Rouge-L. Next, we show promising multi-task improvements over this baseline of around 0.4% across all metrics, despite being a test-only setting and also with the mismatch between the summarization and entailment domains. Figure 3 shows some additional interesting output examples of our multi-task model and how it generates summaries that are better at being logically entailed by the input document, whereas the baseline model contains some crucial contradictory or unrelated information.",
  "y": "differences"
 },
 {
  "id": "685b0b0da37b81765bb78f0f87505b_6",
  "x": [
   "**CONCLUSION AND NEXT STEPS** We presented a multi-task learning approach to incorporate entailment generation knowledge into summarization models. We demonstrated promising initial improvements based on multiple datasets and metrics, even when the entailment knowledge was extracted from a domain different from the summarization domain."
  ],
  "y": "future_work"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_0",
  "x": "In our study, we aim at exploring the usability of annotation projection for the transfer of automatically produced coreference chains. In particular, our idea is that using several source annotations produced by different systems could improve the performance of the projection method. Our approach to the annotation projection builds upon the approach recently introduced by <cite>(Grishina and Stede, 2017)</cite> , who experimented with projecting manually annotated coreference chains from two source languages to the target language.",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_1",
  "x": "Therefore, in contrast to <cite>(Grishina and Stede, 2017)</cite> , we use automatic source annotations produced by two state-of-the-art coreference systems, and we combine the output of our projection method for two source languages (English and German) to obtain target annotations for a third language (Russian). Through performing the error analysis of the projected annotations, we investigate the most common projection errors and assess the benefits and drawbacks of our method. The paper is organized as follows: Section 2 presents an overview of the related work and Section 3 describes the experimental setup.",
  "y": "extends differences"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_2",
  "x": "Thereafter, <cite>(Grishina and Stede, 2017)</cite> proposed a multi-source method for annotation projection: They used a manually annotated trilingual coreference corpus and two source languages (English-German, English-Russian) to transfer annotations to the target language (Russian and German, respectively). Although their approach showed promising results, it was based on transferring manually produced annotations, which are typically not available for other languages and, more importantly, can not be acquired large-scale due to the complexity of the annotation task. ----------------------------------",
  "y": "background"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_3",
  "x": "**ANNOTATION PROJECTION EXPERIMENT** In our experiment, we propose a fully automatic projection setup: First, we perform coreference resolution on the source language data and then we implement the single-and multi-source approaches to transfer the automatically produced annotations. We use the English-German-Russian unannotated corpus of <cite>(Grishina and Stede, 2017)</cite> as the basis for our experiment, which contains texts in two genres -newswire texts (229 sentences per language) and short stories (184 sentences per language).",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_4",
  "x": "Since the corpus was already sentence-and word-aligned 2 , we use the available alignments to transfer the annotations. Thereafter, we re-implement the multi-source approach as described in <cite>(Grishina and Stede, 2017)</cite> . In particular, they (a) looked at disjoint chains coming from different sources and (b) used the notion of chain overlap to measure the similarity between two coreference chains that contain some identical mentions 3 .",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_5",
  "x": "Thereafter, we re-implement the multi-source approach as described in <cite>(Grishina and Stede, 2017)</cite> . In particular, they (a) looked at disjoint chains coming from different sources and (b) used the notion of chain overlap to measure the similarity between two coreference chains that contain some identical mentions 3 . In our experiment, we apply the following strategies from <cite>(Grishina and Stede, 2017)</cite>:",
  "y": "uses similarities"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_6",
  "x": "Moreover, while Precision number are quite similar, projections from English exhibit higher Recall numbers. As for the multi-source settings, we were able to achieve the highest F1 of 36.2 by combining disjoint chains (Setting 1), which is 1.9 point higher than the best single-source projection scores and constitutes almost 62% of the quality of the projection of gold standard annotations reported in <cite>(Grishina and Stede, 2017)</cite> . We were able to achieve the highest Precision scores by intersecting the overlapping chains (Setting 2) and the highest Recall by concatenating them (Setting 3).",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_7",
  "x": "Furthermore, we computed the distribution of unaligned words: The highest percentage of unaligned tokens disregarding punctuation marks are prepositions; pronouns constitute only 3% and 5% of all unaligned words for the alignments between English-Russian and German-Russian respectively. However, these numbers do not constitute more than 5% of the overall number of pronouns in the corpus. Following the work of <cite>(Grishina and Stede, 2017)</cite> , we analyse the projection accuracy for common nouns ('Nc'), named entities ('Np') and pronouns ('P') separately 4 : Table 5 shows the percentage of correctly projected markables of each type out of all the projected markables of this type.",
  "y": "similarities uses"
 },
 {
  "id": "6891aebc7bb1152884d2236a893b55_8",
  "x": "Our results conform to the results of <cite>(Grishina and Stede, 2017)</cite> : For both languages, pronouns exhibit the highest projection quality, while common and proper nouns are projected slightly less accurately, which is probably due to the fact that pronouns typically consist of single tokens and are better aligned than multi-token common and proper names. Overall, for all the markables, the projection accuracy for English-Russian is around 10% better than projection accuracy for GermanRussian. en-ru de-ru Nc 64.5 60.7 Np 70.5 66.6 P 83.6 76.5 All 65.1 55.6 Table 5 : Projection accuracy for common nouns, proper nouns and pronouns (%) Moreover, we compare the projected annotations across the two genres.",
  "y": "differences"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_0",
  "x": "Deep learning's recent success in speech recognition is based on learning feature hierarchies atop these representations [1,<cite> 2]</cite> . There has been increasing focus on extending this end-toend learning approach down to the level of the raw waveform. A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8] .",
  "y": "background"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_1",
  "x": "The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12,<cite> 2]</cite> . However, to decrease the experimental latency, we train on a reduced version of the model and a subset of the training data. The basic architecture is shown in Table 1 .",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_2",
  "x": "While we vary the front end processing, the backend remains the same: a convolutional (through time) layer, followed by three bidirectional simple recurrent layers, and a fully connected layer. Batch normalization [13] , is employed between each layer, but not between individual timesteps <cite>[2]</cite> . Rectified linear unit (ReLU) activation functions are used for all layers, including between timesteps.",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_3",
  "x": "The training data is drawn from a diverse collection of sources including read, conversational, accented, and noisy speech <cite>[2]</cite> . At each epoch, 40% of the utterances are randomly selected to have background noise Table 2 : Single scale waveform convolution outperforms the spectrogram baseline at low strides. The trend is visualized in Figure 2 .",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_4",
  "x": "Following <cite>[2]</cite> , we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances. While the CTC-trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement it with a Kneser-Ney smoothed 5-gram model that is trained using the KenLM toolkit [15] on cleaned text from the Common Crawl Repository. Decoding is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function.",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_5",
  "x": "As previously observed <cite>[2]</cite> , deep neural networks trained on sufficient data perform better as the model size grows. In order to make fair comparisons, the number of parameters of the models used in our experiments is held constant at 35M. We are aware that the results are not directly comparable to literature due to the use of proprietary datasets.",
  "y": "uses similarities"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_6",
  "x": "As previously observed <cite>[2]</cite> , deep neural networks trained on sufficient data perform better as the model size grows. In order to make fair comparisons, the number of parameters of the models used in our experiments is held constant at 35M. We are aware that the results are not directly comparable to literature due to the use of proprietary datasets.",
  "y": "uses"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_7",
  "x": "While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs<cite> [2,</cite> 16] . Our learned features underperform in this context, which is understandable as they are not spectrally ordered, and lack spatial structure. Regularization techniques such as [17] could perhaps be key to learning ordered filter maps with useful structure.",
  "y": "similarities"
 },
 {
  "id": "68d41bee7361b6680103c9951a6570_8",
  "x": "While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs<cite> [2,</cite> 16] . Our learned features underperform in this context, which is understandable as they are not spectrally ordered, and lack spatial structure. Regularization techniques such as [17] could perhaps be key to learning ordered filter maps with useful structure.",
  "y": "differences"
 },
 {
  "id": "692f7edc151a9a833c7dd7943bb608_0",
  "x": "**RELATED WORK** Several research work have been reported since 2010 in this research field of hate speech detection (Kwok and Wang, 2013; Burnap and Williams, 2015; Djuric et al., 2015; <cite>Davidson et al., 2017</cite>; Malmasi and Zampieri, 2018; Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018; ElSherief et al., 2018; Gamb\u00e4ck and Sikdar, 2017; Zhang et al., 2018; Mathur et al., 2018) . Schmidt and Wiegand (2017) & Fortuna and Nunes (2018) reviewed the approaches used for hate speech detection.",
  "y": "background"
 },
 {
  "id": "692f7edc151a9a833c7dd7943bb608_1",
  "x": [
   "The instances are vectorized using TF-IDF score for traditional machine learning models with minimum count two. The classifiers namely Multinomial Naive Bayes and Support Vector Machine with Stochastic Gradient Descent optimizer were employed to build the models for sub tasks B and C. Deep learning with Scaled Luong attention, deep learning with Normed Bahdanau attention, traditional machine learning with SVM give better results for Task A, Task B and Task C respectively. Our models outperform the base line for all the three tasks."
  ],
  "y": "future_work"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_0",
  "x": "Fully supervised maximum entropy Markov models have been used for cascaded prediction of POS tags followed by supertags (Clark and Curran, 2007) . Here, we learn supertaggers given only a POS tag dictionary and supertag dictionary or a small amount of material labeled with both types of information. Previous work has used Bayesian HMMs to learn taggers for both POS tagging and supertagging<cite> (Baldridge, 2008)</cite> separately.",
  "y": "background"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_1",
  "x": "---------------------------------- **SUPERTAGGING WITH VARYING AMOUNTS OF TRAINING DATA** In this experiment, we use the training and test sets used by <cite>Baldridge (2008)</cite> from CCGbank.",
  "y": "uses similarities"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_2",
  "x": "State-of-the-art POS taggers report accuracies in the range of 96\u221297%; our model FHMMB was comparable (95.35% for \u03b1 = 0.05 and 94.41 for \u03b1 = 1.0). The FHMMA model and the HMM model achieved 91% and 92.5% accuracy on POS tags, respectively. The accuracy of our HMM is lower than the performance of <cite>Baldridge (2008)</cite> for supertags.",
  "y": "differences"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_3",
  "x": "We draw the initial sample of CCG tag sequences corresponding to the observation sequence, using probabilities based on grammar informed initialization<cite> (Baldridge, 2008)</cite> . We consider the prior probability of occurrence of categories based on their complexity: given a lexicon L, the probability of a category c i is inversely proportional to its complexity: where complexity(c i ) is defined as the number of sub-categories contained in category c i .",
  "y": "uses"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_4",
  "x": "The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of <cite>Baldridge (2008)</cite> that uses variational Bayes EM (33%). Our complexity based initialization is not directly comparable to the results in <cite>Baldridge (2008)</cite> because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism. However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of <cite>Baldridge (2008)</cite> .",
  "y": "similarities"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_5",
  "x": "We also report the CCG accuracy values inclusive of unambiguous types in Table 5 (b) for \u03b1 = 1.0 and Table 6 (b) respectively. The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of <cite>Baldridge (2008)</cite> that uses variational Bayes EM (33%). Our complexity based initialization is not directly comparable to the results in <cite>Baldridge (2008)</cite> because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism.",
  "y": "differences"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_6",
  "x": "However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of <cite>Baldridge (2008)</cite> . It is however, quite short of the 56.1% accuracy achieved by the model of <cite>Baldridge (2008)</cite> that uses grammar informed initialization (combination of category based initialization along with category transition rules). Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM.",
  "y": "differences"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_7",
  "x": "However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of <cite>Baldridge (2008)</cite> . It is however, quite short of the 56.1% accuracy achieved by the model of <cite>Baldridge (2008)</cite> that uses grammar informed initialization (combination of category based initialization along with category transition rules). Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM.",
  "y": "background"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_8",
  "x": "In the weakly supervised setting, the choice of the transition prior \u03b1 of 0.05 lead to severe degradation in the prediction accuracy of CCG tags. Unlike POS tagging, where a symmetric transition prior of \u03b1 = 0.05 captured the sparsity of the tag transition distribution , in supertagging the transition priors are asymmetric. We expect that CCG transition rules<cite> (Baldridge, 2008)</cite> when encoded as category specific transition priors, will lead to better performance with the FHMMs.",
  "y": "future_work"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_9",
  "x": "**RELATED WORK** This paper follows the work of Duh (2005) , <cite>Baldridge (2008)</cite> and Goldwater and Griffiths (2007) . Duh (2005) uses FHMMs for jointly labeling the POS and NP chunk tags for the CoNLL2000 dataset (Sang et al., 2000) .",
  "y": "extends"
 },
 {
  "id": "6a90ecc147618b3909609fc6c2e2b3_10",
  "x": "Overall, the discriminative C&C supertagger outperforms the FHMMs in all supervised settings. Despite this, the FHMMs are suited for estimating models with less supervision, such as from tag dictionaries alone and incorporating more informative prior distributions such as those in <cite>Baldridge (2008)</cite> . This may make them more appropriate for developing CCGbanks for other languages and domains.",
  "y": "similarities"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_0",
  "x": "The most prominent approaches include: (1) temporal ordering in terms of publication date (Barzilay, 2003) , (2) temporal ordering in terms of textual cues in sentences (Bollegala et al., 2006) , (3) the topic of the sentences (Barzilay, 2003) , (4) coherence theories (<cite>Barzilay and Lapata, 2008</cite>) , e.g., Centering Theory, (5) content models (Barzilay and Lee, 2004) , and (6) ordering(s) in the underlying documents in the case of summarisation (Bollegala et al., 2006; Barzilay, 2003) . ---------------------------------- **THE MODEL**",
  "y": "background"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_1",
  "x": "The sentence ordering task used in this paper can easily be transformed into a ranking problem. Hence, paralleling <cite>Barzilay and Lapata (2008)</cite> , our model has the following structure. The data consists of alternative orderings (x ij , x ik ) of the sentences of the same document d i .",
  "y": "similarities"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_2",
  "x": "**GROUP SIMILARITY** The features in this category are inspired by discourse entity-based accounts of local coherence. Yet, in contrast to <cite>Barzilay and Lapata (2008)</cite> , <cite>who</cite> employ the syntactic properties of the respective occurrences, we reduce the accounts to whether or not the entities occur in subsequent sentences (similar to Karamanis (2004) 's NOCB metric).",
  "y": "differences"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_3",
  "x": "The group similarity features only capture the relation between a sentence and its immediate successor. However, the coherence of a text is clearly not only defined by direct relations, but also requires longer range relations between sentences (e.g., <cite>Barzilay and Lapata (2008)</cite> ). The features in this section explore the impact of such relations on the coherence of the overall document as well as the appropriate way of modeling them.",
  "y": "motivation"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_4",
  "x": "The three datasets used for the automatic evaluation in this paper are based on human-generated texts (Table 1 ). The first two are the earthquake and accident datasets used by <cite>Barzilay and Lapata (2008)</cite> . Each of these sets consists of 100 datasets in the training and test sets, respectively, as well as 20 random permutations for each text.",
  "y": "uses"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_5",
  "x": "It comprises 300 human summaries on 50 document sets, resulting in a total of 6,000 pairwise rankings split into training and test sets. The source furthermore differs from <cite>Barzilay and Lapata (2008)</cite> 's datasets in that the content of each text is not based on one individual event (an earthquake or accident), but on more complex topics followed over a period of time (e.g., the espionage case between GM and VW along with the various actions taken to resolve it). Since the different document sets cover completely different topics the third dataset will mainly be used to evaluate the topic-independent properties of our model.",
  "y": "differences"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_6",
  "x": "In particular, the performance on the earthquake dataset is below standard. However, it seems that sentence ordering in that set is primarily defined by topics, as only content models perform well. (<cite>Barzilay and Lapata (2008)</cite> only perform well when using <cite>their</cite> coreference module, which determines antecedents based on the identified coreferences in the original sentence ordering, thereby biasing <cite>their</cite> orderings towards the correct ordering.) Longer range and WordNet relations together (Chunk+Temp-WN+LongRange+) achieve the best performance.",
  "y": "differences"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_7",
  "x": "Namely, for newswire summarization systems, the topics of the documents are Table 7 : Cross-Training between Accident and Earthquake datasets. The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from <cite>Barzilay and Lapata (2008)</cite>. unknown at the time of training.",
  "y": "similarities"
 },
 {
  "id": "6b147afca676882878e67bc10abd58_8",
  "x": "In fact, model performance is nearly independent of the training topic. Nevertheless, the results on the earthquake test set indicate that our model is missing essential components for the correct prediction of sentence orderings on this set. When compared to the results obtained by <cite>Barzilay and Lapata (2008)</cite> and Barzilay and Lee (2004) , it would appear that direct sentenceto-sentence similarity (as suggested by the <cite>Barzilay and Lapata</cite> baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset.",
  "y": "similarities"
 },
 {
  "id": "6bf17a793eaee0593596df0c2249b5_0",
  "x": "---------------------------------- **INTRODUCTION** Multi-hop QA requires finding multiple supporting evidence, and reasoning over them in order to answer a question (Welbl et al., 2018; Talmor and Berant, 2018; <cite>Yang et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "6bf17a793eaee0593596df0c2249b5_1",
  "x": "Our model out-performs them for multi-hop QA. To summarize, this paper presents an entitycentric IR approach that jointly performs entity linking and effectively finds relevant evidence required for questions that need multi-hop reasoning from a large corpus containing millions of paragraphs. When the retrieved paragraphs are supplied to the baseline QA model introduced in<cite> Yang et al. (2018)</cite> , it improved the QA performance on the hidden test set by 10.59 F1 points.",
  "y": "uses"
 },
 {
  "id": "6bf17a793eaee0593596df0c2249b5_2",
  "x": "---------------------------------- **EXPERIMENTS** For all our experiment, unless specified otherwise, we use the open domain corpus 4 released by<cite> Yang et al. (2018)</cite> which contains over 5.23 million Wikipedia abstracts (introductory paragraphs).",
  "y": "uses"
 },
 {
  "id": "6bf17a793eaee0593596df0c2249b5_4",
  "x": "Figure 3 shows that our model performs equally well on both type of queries and hence can be applied in a practical setting. Baseline Reader<cite> (Yang et al., 2018)</cite> Table 2 shows the performance on the QA task. We were able to achieve better scores than reported in the baseline reader model of<cite> Yang et al. (2018)</cite> by using Adam (Kingma and Ba, 2014) instead of standard SGD (our re-implementation).",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_0",
  "x": "Parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations (Gu et al., 2018) . One way to remedy this fundamental problem is to refine model output iteratively (Lee et al., 2018;<cite> Ghazvininejad et al., 2019)</cite> . This work pursues this iterative approach to non-autoregressive translation.",
  "y": "background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_1",
  "x": "Non-autoregressive machine translation (NAT, Gu et al. (2018) ), on the other hand, generates all words in one shot and speeds up decoding at the expense of performance drop. Parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations (Gu et al., 2018) . One way to remedy this fundamental problem is to refine model output iteratively (Lee et al., 2018;<cite> Ghazvininejad et al., 2019)</cite> .",
  "y": "motivation background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_2",
  "x": "Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words. Unlike the masked language models (Devlin et al., 2019;<cite> Ghazvininejad et al., 2019)</cite> where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large. We also introduce a new inference algorithm for iterative parallel decoding, parallel easy-first, where each word is predicted by attending to the words that the model is more confident about.",
  "y": "differences background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_3",
  "x": "1 In this work, we propose a transformer-based architecture with attention masking, which we call Disentangled Context (DisCo) transformer, and use it for non-autoregressive decoding. Specifically, our DisCo transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words. Unlike the masked language models (Devlin et al., 2019;<cite> Ghazvininejad et al., 2019)</cite> where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large.",
  "y": "differences"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_4",
  "x": "Unlike the masked language models (Devlin et al., 2019;<cite> Ghazvininejad et al., 2019)</cite> where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large. We also introduce a new inference algorithm for iterative parallel decoding, parallel easy-first, where each word is predicted by attending to the words that the model is more confident about. This decoding algorithm allows for predicting all tokens with different context in each iteration and terminates when the output prediction converges, contrasting with the constant number of iterations<cite> (Ghazvininejad et al., 2019)</cite> .",
  "y": "differences"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_5",
  "x": "This decoding algorithm allows for predicting all tokens with different context in each iteration and terminates when the output prediction converges, contrasting with the constant number of iterations<cite> (Ghazvininejad et al., 2019)</cite> . Indeed, we will show in a later section that this method substantially reduces the number of required iterations without loss in performance. Our extensive empirical evaluations on 7 translation directions from standard WMT benchmarks show that our approach achieves competitive performance to state-of-the-art non-autoregressive and autoregressive machine translation while significantly reducing decoding time on average.",
  "y": "differences background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_6",
  "x": "**DISCO OBJECTIVE** Similar to masked language models for contextual word representations (Devlin et al., 2019; Liu et al., 2019 ), a con- ditional masked language model (CMLM, <cite>Ghazvininejad et al. (2019)</cite> ) predicts randomly masked target tokens Y mask given a source text X and the rest of the target tokens Y obs . Namely, for every sentence pair in bitext X and Y ,",
  "y": "background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_7",
  "x": "where RS denotes random sampling of masked tokens. 2 CMLMs have proven successful in parallel decoding for machine translation<cite> (Ghazvininejad et al., 2019)</cite> , video captioning (Yang et al., 2019a) , and speech recognition (Nakayama et al., 2019) . However, the fundamental inefficiency with this masked language modeling objective is that the model can only be trained to predict a subset of the reference tokens (Y mask ) for each network pass unlike a normal autoregressive model where we predict all Y from left to right.",
  "y": "background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_8",
  "x": "**DISCO TRANSFORMER ARCHITECTURE** Simply computing conditional probabilities P (Y n |X, Y n obs ) with a vanilla transformer decoder will necessitate N separate transformer passes for each Y n obs . We introduce the 2 BERT (Devlin et al., 2019 ) masks a token with probability 0.15 while CMLMs<cite> (Ghazvininejad et al., 2019)</cite> sample the number of masked tokens uniformly from [1, N ].",
  "y": "differences"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_9",
  "x": "**TRAINING LOSS** We use a standard transformer as an encoder and stacked DisCo layers as a decoder. For each Y n in Y where |Y | = N , we uniformly sample the number of visible tokens from [0, N \u2212 1], and then we randomly choose that number of tokens from Y \\ Y n as Y n obs , similarly to CMLMs<cite> (Ghazvininejad et al., 2019)</cite> .",
  "y": "similarities"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_10",
  "x": "---------------------------------- **MASK-PREDICT** Mask-predict is an iterative inference algorithm introduced in <cite>Ghazvininejad et al. (2019)</cite> to decode a conditional masked language model (CMLM).",
  "y": "background"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_11",
  "x": "**LENGTH BEAM** Following <cite>Ghazvininejad et al. (2019)</cite> , we apply length beam. In particular, we predict top K lengths from the distribution in length prediction and run parallel easy-first simultaneously.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_12",
  "x": "See Alg. 1 for full pseudo-code. Notice that all for-loops are parallelizable except the one over iterations t. In the subsequent experiments, we use length beam size of 5<cite> (Ghazvininejad et al., 2019)</cite> unless otherwise noted.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_13",
  "x": "We evaluate performance with BLEU scores (Papineni et al., 2002) for all directions except that we use SacreBLEU (Post, 2018) 5 in en\u2192zh again for fair comparison with prior work<cite> (Ghazvininejad et al., 2019)</cite> . For all autoregressive models, we use beam search with b = 5 (Vaswani et al., 2017; Ott et al., 2018) and tune length penalty of \u03b1 \u2208 [0.0, 0.2, \u00b7 \u00b7 \u00b7 , 2.0] in validation. For parallel easy-first, we set the max number of iterations T = 10 and use T = 4, 10 for constant-time mask-predict.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_14",
  "x": "CMLM As discussed earlier, we can generate a translation with mask-predict from a CMLM<cite> (Ghazvininejad et al., 2019)</cite> . We can directly compare our DisCo transformer with this method by the number of iterations required. 6 We provide results obtained by running their code.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_15",
  "x": "See Gu et al. (2019) (Luong et al., 2015; Vaswani et al., 2017) . Unfortunately, we lack consensus in evaluation (Post, 2018) . Hyperparameters We generally follow the hyperparameters for a transformer base (Vaswani et al., 2017;<cite> Ghazvininejad et al., 2019)</cite> : 6 layers for both the encoder and decoder, 8 attention heads, 512 model dimensions, and 2048 hidden dimensions.",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_16",
  "x": "**RESULTS AND DISCUSSION** Seen in Table 1 are the results in the four directions from the WMT'14 EN-DE and WMT'16 EN-RO datasets. First, our re-implementations of CMLM + Mask-Predict outperform <cite>Ghazvininejad et al. (2019)</cite> (e.g. 31.24 vs. 30.53 in de\u2192en with 10 steps).",
  "y": "uses"
 },
 {
  "id": "6d5a52c29e4f91bc17502e250c9187_17",
  "x": "**RELATED AND FUTURE WORK** Recent work on non-autoregressive translation developed ways to mitigate the trade-off between decoding parallelism and performance. As in this work, several prior work proposed methods to iteratively refine output predictions (Lee et al., 2018;<cite> Ghazvininejad et al., 2019</cite>; Gu et al., 2019; Mansimov et al., 2019) .",
  "y": "similarities"
 },
 {
  "id": "6e5ee9176bcc54c3c9c32965765990_0",
  "x": "The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE (Sennrich et al., 2016) approach. The models were trained with multi-domain data and we improved performance following a domainmixing approach <cite>(Britz et al., 2017)</cite> . The domain information was prepended with special tokens for each target sequence.",
  "y": "uses"
 },
 {
  "id": "6e5ee9176bcc54c3c9c32965765990_1",
  "x": "The data was lowercased and extra embeddings were added in order to keep the case information. The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE (Sennrich et al., 2016) approach. The models were trained with multi-domain data and we improved performance following a domainmixing approach <cite>(Britz et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_0",
  "x": "Given the success of statistical parsing models on the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (Charniak, 2000; Collins, 2003, for example) , there has been a change in focus in recent years towards the problem of replicating this success on genres other than American financial news stories. The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples. A breakthrough has come in the form of research by McClosky et al. (2006a;<cite> 2006b</cite> ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005) .",
  "y": "background"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_2",
  "x": "McClosky et al. (2006a;<cite> 2006b</cite> ) proceed as follows: sentences * Now affiliated to Lalic, Universit\u00e9 Paris 4 La Sorbonne. from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker. The highest ranked parse trees are added to the training set of the parser and the parser is retrained.",
  "y": "background"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_3",
  "x": "The highest ranked parse trees are added to the training set of the parser and the parser is retrained. This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of McClosky et al. (2006a;<cite> 2006b)</cite> , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material.",
  "y": "background"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_4",
  "x": "Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. (<cite>2006b)</cite> report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training.",
  "y": "background"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_5",
  "x": "They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. (<cite>2006b)</cite> report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training. In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a;<cite> 2006b)</cite> .",
  "y": "uses"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_6",
  "x": "However, McCloskey et al. (<cite>2006b)</cite> report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training. In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a;<cite> 2006b)</cite> . We parse the BNC (Burnard, 2000) in its entirety using the reranking parser of Charniak and Johnson (2005) .",
  "y": "differences uses"
 },
 {
  "id": "6f3d6ad1f09c55a1a006abbeddb4ab_7",
  "x": "Labelled precision (LP), recall (LR) and f-score measures 2 for this parser are shown in the first row of Table 1 . The f-score of 83.7% is lower than the f-score of 85.2% reported by<cite> McClosky et al. (2006b)</cite> for the same parser on Brown corpus data. This difference is reasonable since there is greater domain variation between the WSJ and the BNC than between the WSJ and the Brown corpus, and all BNC gold standard sentences contain verbs not attested in WSJ Sections 2-21.",
  "y": "differences"
 },
 {
  "id": "6fbfc9f887e736472510bce30c9228_0",
  "x": "We apply various probabilistic methods to improve discourse modelling in the support services domain. In previous work, we collected a small corpus of task-oriented dialogues between customers and support representatives from the MSN Shopping online support service <cite>(Ivanovic, 2005b)</cite> . The service is designed to assist potential customers with finding various items for sale on the MSN Shopping web site.",
  "y": "uses background"
 },
 {
  "id": "6fbfc9f887e736472510bce30c9228_1",
  "x": "A sample from one of the dialogues in this corpus is shown in Table 1 . The research presented here advances previous work which examined various models and tech-niques to predict dialogue acts in task-oriented instant messaging. In <cite>Ivanovic (2005b)</cite> , the MSN Shopping corpus was collected and a gold standard produced by labelling the utterances with dialogue acts.",
  "y": "background"
 },
 {
  "id": "6fbfc9f887e736472510bce30c9228_2",
  "x": "**BACKGROUND** Our dialogue act tag set contains 12 dialogue acts, which are intended to represent the illocutionary force of an utterance. The tags were derived in <cite>Ivanovic (2005b)</cite> by manually labelling the MSN Shopping corpus using the tags that seemed appropriate from a list of 42 tags in Stolcke et al. (2000) .",
  "y": "uses"
 },
 {
  "id": "6fbfc9f887e736472510bce30c9228_3",
  "x": "<cite>Ivanovic (2005b)</cite> describes the manual process of segmenting the messages into utterances and labelling the utterances with dialogue act tags to produce a gold standard version of the data. Kappa analysis on both the labelling and segmentation tasks was conducted with results showing high interannotator agreement (Ivanovic, 2005a ). ----------------------------------",
  "y": "background"
 },
 {
  "id": "6fd0c2fbbe0c7fb669f2618f4d01f7_0",
  "x": "Wikipedia is an open and collaborative multilingual encyclopedia contributed by several collaborators currently having 5.6 million English articles [21] . Constantly, the articles are updated and new articles are added by its collaborators. About 74% of Wikipedia articles fall under the category of named entity classes <cite>[4]</cite> , therefore, we consider Wikipedia links among articles to construct the Hi-En-WP NER annotated corpus.",
  "y": "uses background"
 },
 {
  "id": "6fd0c2fbbe0c7fb669f2618f4d01f7_1",
  "x": "About 74% of Wikipedia articles fall under the category of named entity classes <cite>[4]</cite> , therefore, we consider Wikipedia links among articles to construct the Hi-En-WP NER annotated corpus. Wikipedia includes content pages which contain concepts and facts about the article, category pages provides a list of content pages into several classes based on specific criteria and disambiguation pages help to locate different content pages with same title. Wikipedia is an abundant resource for generation of different types of multilingual, cross lingual, cross script and language independent corpus, etc., its markup has been used extensively to automatically generate NER annotated corpus for training machine learning models <cite>[4]</cite> [5] [6] [11] [12] [13] [14] 19] .",
  "y": "background"
 },
 {
  "id": "6fd0c2fbbe0c7fb669f2618f4d01f7_2",
  "x": "This approach is similar to Nothman et al (2008) <cite>[4]</cite> to generate the NER data from wikilinks. A total number of 7285 tokens and multi-tokens expressions were extracted from the links by parsing the 13 identified Wikipedia webpages. We consider these expressions as probable NEs after the removal of duplicates from the set of tokens obtained.",
  "y": "similarities"
 },
 {
  "id": "6fd0c2fbbe0c7fb669f2618f4d01f7_3",
  "x": "Whereas, low precision value for LOC tag suggests that other entities are wrongly classified as location. The MISC F-score is expectedly low, in agreement with the results of Nothman et al (2008) <cite>[4]</cite> . The variation reflected in F-score among all may be the effect of diversity in linguistic attributes.",
  "y": "similarities"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_0",
  "x": "They have been used for both rule-based (Taboada et al., 2011) and unsupervised (Turney, 2002; Hu and Liu, 2004; or supervised<cite> (Mohammad et al., 2013</cite>; Tang et al., 2014b; Vo and Zhang, 2015) machine-learning-based sentiment analysis. As a result, constructing sentiment lexicons is one important research task in sentiment analysis. Many approaches have been proposed to construct sentiment lexicons.",
  "y": "background"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_1",
  "x": "One benefit of such lexicons is high quality. On the other hand, the methods are timeconsuming, requiring language and domain expertise. Recently, statistical methods have been exploited to learn sentiment lexicons automatically (Esuli and Sebastiani, 2006; Baccianella et al., 2010;<cite> Mohammad et al., 2013)</cite> .",
  "y": "background"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_2",
  "x": "Such methods leverage knowledge resources (Bravo-Marquez et al., 2015) or labeled sentiment data (Tang et al., 2014a) , giving significantly better coverage compared to manual lexicons. Among the automatic methods,<cite> Mohammad et al. (2013)</cite> proposed to use tweets with emoticons or hashtags as training data. The main advantage is that such training data are abundant, and manual annotation can be avoided.",
  "y": "background"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_3",
  "x": [
   "The main advantage is that such training data are abundant, and manual annotation can be avoided. Despite that emoticons or hashtags can be noisy in indicating the sentiment of a tweet, existing research (Go et al., 2009; Pak and Paroubek, 2010; Agarwal et al., 2011; Kalchbrenner et al., 2014; Ren et al., 2016b) has shown that effectiveness of such data when used to supervise sentiment classifiers. Mohammad et al. (2013) collect sentiment lexicons by calculating pointwise mutual information (PMI) between words and emoticons."
  ],
  "y": "background"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_4",
  "x": "In this paper, we show that a better lexicon can be learned by directly optimizing the prediction accuracy, taking the lexicon as input and emoticon as the output. The correlation between our method and the method of<cite> Mohammad et al. (2013)</cite> is analogous to the \"predicting\" vs \"counting\" correlation between distributional and distributed word representations (Baroni et al., 2014) . We follow Esuli and Sebastiani (2006) in using two simple attributes to represent each sentiment word, and take inspiration from Mikolov et al. (2013) in using a very simple neural network for sentiment prediction.",
  "y": "differences"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_5",
  "x": "We follow Esuli and Sebastiani (2006) in using two simple attributes to represent each sentiment word, and take inspiration from Mikolov et al. (2013) in using a very simple neural network for sentiment prediction. The method can leverage the same data as<cite> Mohammad et al. (2013)</cite> and therefore benefits from both scale and annotation independence. Experiments show that the neural model gives the best results on standard benchmarks across multiple languages.",
  "y": "uses"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_6",
  "x": "We use the same data source as<cite> Mohammad et al. (2013)</cite> to train lexicons. However, rather than relying on PMI, we take a machine-learning method in optimizing the prediction accuracy of emoticons using the lexicons. To leverage large data, we use a very simple neural network to train the lexicons.",
  "y": "uses"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_7",
  "x": "We compare our lexicon with the lexicons of NRC 4<cite> (Mohammad et al., 2013)</cite> , HIT 5 (Tang et al., 2014a) and WEKA 6 (Bravo-Marquez et al., 2015) . As shown in Table  3 , using the unsupervised sentiment classification method (unsup) in Section 5, our lexicon gives significantly better result in comparison with countbased lexicons of NRC. Under both settings, our lexicon yields the best results compared to other methods.",
  "y": "uses"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_8",
  "x": "Table 6 shows examples of our predicting-based lexicon and the counting-based lexicon of<cite> Mohammad et al. (2013)</cite> . First, both lexicons can correctly reflect the strength of emotional words (e.g. bad, worse, worst), which demonstrates that our method can learn statistical relevance as effectively as PMI. Second, we find many cases where our lexicon gives the correct polarity (e.g. suitable, lazy) but the lexicon of<cite> Mohammad et al. (2013)</cite> does not.",
  "y": "uses"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_9",
  "x": "Second, we find many cases where our lexicon gives the correct polarity (e.g. suitable, lazy) but the lexicon of<cite> Mohammad et al. (2013)</cite> does not. To quantitatively compare the lexicons, we calculated the accuracies of their polarities (i.e. sign) by using the manually-annotated lexicon of Hu and Liu (2004) as the gold standard. We take the intersection between the automatic lexicons and the lexicon of Hu and Liu (2004) as the test set, which contains 3270 words.",
  "y": "differences"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_10",
  "x": "Table 6 shows examples of our predicting-based lexicon and the counting-based lexicon of<cite> Mohammad et al. (2013)</cite> . First, both lexicons can correctly reflect the strength of emotional words (e.g. bad, worse, worst), which demonstrates that our method can learn statistical relevance as effectively as PMI. Second, we find many cases where our lexicon gives the correct polarity (e.g. suitable, lazy) but the lexicon of<cite> Mohammad et al. (2013)</cite> does not.",
  "y": "differences"
 },
 {
  "id": "710ec6f6d6d4c7c8c148833c0adfef_11",
  "x": "We take the intersection between the automatic lexicons and the lexicon of Hu and Liu (2004) as the test set, which contains 3270 words. The polarity accuracy of our lexicon is 78.2%, in contrast to 76.9% by the lexicon of<cite> Mohammad et al. (2013)</cite> , demonstrating the relative strength of our method. Third, by having two attributes (n, p) instead of one, our lexicon is better in compositionality (e.g. SS(strong memory) > 0, SS(strong snowstorm) < 0).",
  "y": "differences"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_0",
  "x": "To improve the precision of retrieval output, especially within the very few (e.g, 5 or 10) highest-ranked documents that are returned, a number of researchers [36, 13, 16, 7, 22, 34, 25, 1, <cite>18,</cite> 9] have considered a structural re-ranking strategy. The idea is to re-rank the top N documents that some initial search engine produces, where the re-ordering utilizes information about inter-document relationships within that set. Promising results have been previously obtained by using document centrality within the initially retrieved list to perform structural re-ranking, on the premise that if the quality of this list is reasonable to begin with, then the documents that are most related to most of the docuPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_1",
  "x": "Copyright 2006 ACM 1-59593-369-7/06/0008 ...$5.00. ments on the list are likely to be the most relevant ones. In particular, in our prior work<cite> [18]</cite> we adapted PageRank [3] -which, due to the success of Google, is surely the most well-established algorithm for defining and computing centrality within a directed graph -to the task of re-ranking non-hyperlinked document sets. The arguably most well-known alternative to PageRank is Kleinberg's HITS algorithm [16] .",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_2",
  "x": "The major conceptual way in which HITS differs from PageRank is that it defines two different types of central items: each node is assigned both a hub and an authority score as opposed to a single PageRank score. In the Web setting, in which HITS was originally proposed, good hubs correspond roughly to highquality resource lists or collections of pointers, whereas good authorities correspond to the high-quality resources themselves; thus, distinguishing between two differing but interdependent types of Webpages is quite appropriate. Our previous study<cite> [18]</cite> applied HITS to non-Web documents.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_3",
  "x": "We found that its performance was comparable to or better than that of algorithms that do not involve structural re-ranking; however, HITS was not as effective as PageRank<cite> [18]</cite> . Do these results imply that PageRank is better than HITS for structural re-ranking of non-Web documents? Not necessarily, because there may exist graph-construction methods that are more suitable for HITS.",
  "y": "background motivation"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_4",
  "x": "Specifically, our experimental results show that the centralityinduction methods that we previously studied solely in the context of document-only graphs<cite> [18]</cite> result in much better re-ranking performance if implemented over bipartite graphs of documents (on one side) and clusters (on the other side). For example, ranking documents by their \"authoritativeness\" as computed by HITS upon these cluster-document graphs yields better performance than that of a previously proposed PageRank implementation applied to documentonly graphs. Interestingly, we also find that cluster authority scores can be used to identify clusters containing a large percentage of relevant documents.",
  "y": "extends"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_5",
  "x": "**ALTERNATIVE SCORES: PAGERANK AND INFLUX** We will compare the results of using the HITS algorithm against those derived using PageRank instead. This is a natural comparison because PageRank is the most well-known centrality-induction algorithm utilized for ranking documents, and because in earlier work<cite> [18]</cite> , PageRank performed quite well as a tool for structural re-ranking of non-Web documents, at least when applied to document-to-document graphs.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_6",
  "x": "We will compare the results of using the HITS algorithm against those derived using PageRank instead. This is a natural comparison because PageRank is the most well-known centrality-induction algorithm utilized for ranking documents, and because in earlier work<cite> [18]</cite> , PageRank performed quite well as a tool for structural re-ranking of non-Web documents, at least when applied to document-to-document graphs. One can think of PageRank as a version of HITS in which the hub/authority distinction has been collapsed.",
  "y": "uses background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_7",
  "x": "is an affine transformation (with respect to positive constants) of, and therefore equivalent for ranking purposes to, the unique positive sum-normalized solution to Equation 4. (Proof omitted due to space constraints.) Interestingly, this result shows that while one might have thought that clusters and documents would \"compete\" for PageRank score when placed within the same graph, in our document-as-authority and document-as-hub graphs this is not the case. Earlier work<cite> [18]</cite> also considered scoring a node v by its influx, P u\u2208V w t(u \u2192 v).",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_8",
  "x": "Section 5.2 presents results for detecting such clusters using centrality-based cluster ranking. Recently, there has been a growing body of work on graphbased modeling for different language-processing tasks wherein links are induced by inter-entity textual similarities. Examples include document (re-)ranking [7, 24, 9, <cite>18,</cite> 39 ], text summarization [11, 26] , sentence retrieval [28] , and document representation [10] .",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_9",
  "x": "Most aspects of the evaluation framework described below are adopted from our previous experiments with noncluster-based structural re-ranking<cite> [18]</cite> so as to facilitate direct comparison. Section 4.1 of<cite> [18]</cite> provides a more detailed justification of the experimental design. The main conceptual changes 6 here are: a slightly larger parameter search-space for the \"out-degree\" parameter \u03b4 (called the \"ancestry\" parameter \u03b1 in<cite> [18]</cite> ); and, of course, the incorporation of clusters.",
  "y": "uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_10",
  "x": "Section 4.1 of<cite> [18]</cite> provides a more detailed justification of the experimental design. The main conceptual changes 6 here are: a slightly larger parameter search-space for the \"out-degree\" parameter \u03b4 (called the \"ancestry\" parameter \u03b1 in<cite> [18]</cite> ); and, of course, the incorporation of clusters. ----------------------------------",
  "y": "uses background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_11",
  "x": "Most aspects of the evaluation framework described below are adopted from our previous experiments with noncluster-based structural re-ranking<cite> [18]</cite> so as to facilitate direct comparison. Section 4.1 of<cite> [18]</cite> provides a more detailed justification of the experimental design. The main conceptual changes 6 here are: a slightly larger parameter search-space for the \"out-degree\" parameter \u03b4 (called the \"ancestry\" parameter \u03b1 in<cite> [18]</cite> ); and, of course, the incorporation of clusters.",
  "y": "differences"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_12",
  "x": "Relevance flow based on language models (LMs). To estimate the degree to which one item, if considered relevant, can vouch for the relevance of another, we follow our previous work on document-based graphs<cite> [18]</cite> and utilize p [\u00b5] d (\u00b7), the unigram Dirichlet-smoothed language model induced from a given document d (\u00b5 is the smoothing parameter) [38] . To adapt this estimation scheme to settings involving clusters, we derive the language model p c (\u00b7) for a cluster c by treating c as the (large) document formed by concatenating 7 its constituent (or most strongly associated) documents [17, 25, 19] .",
  "y": "uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_13",
  "x": "where D is the Kullback-Leibler divergence. The asymmetry of this measure corresponds nicely to the intuition that relevance flow is not symmetric<cite> [18]</cite> . Moreover, this function 6 Some of the PageRank results appearing in our previous paper<cite> [18]</cite> accidentally reflect experiments utilizing a suboptimal choice of Dinit.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_14",
  "x": "Moreover, this function 6 Some of the PageRank results appearing in our previous paper<cite> [18]</cite> accidentally reflect experiments utilizing a suboptimal choice of Dinit. For citation purposes, the numbers reported in the current paper should be used. 7 Concatenation order is irrelevant for unigram LMs.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_15",
  "x": "is somewhat insensitive to large length differences between the items in question<cite> [18]</cite> , which is advantageous when both documents and clusters (which we treat as very long documents) are considered. Previous work<cite> [18,</cite> 33] makes heavy use of the idea of nearest neighbors in language-model space. It is therefore convenient to introduce the notation N bhd(x | m, R), pronounced \"neighborhood\", to denote the m items y within the \"restriction set\" R that have the highest values of rflow(x, y) (we break ties by item ID, assuming that these have been assigned to documents and clusters).",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_16",
  "x": "is somewhat insensitive to large length differences between the items in question<cite> [18]</cite> , which is advantageous when both documents and clusters (which we treat as very long documents) are considered. Previous work<cite> [18,</cite> 33] makes heavy use of the idea of nearest neighbors in language-model space. It is therefore convenient to introduce the notation N bhd(x | m, R), pronounced \"neighborhood\", to denote the m items y within the \"restriction set\" R that have the highest values of rflow(x, y) (we break ties by item ID, assuming that these have been assigned to documents and clusters).",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_17",
  "x": "Previous work<cite> [18,</cite> 33] makes heavy use of the idea of nearest neighbors in language-model space. It is therefore convenient to introduce the notation N bhd(x | m, R), pronounced \"neighborhood\", to denote the m items y within the \"restriction set\" R that have the highest values of rflow(x, y) (we break ties by item ID, assuming that these have been assigned to documents and clusters). Note that the neighborhood of x corresponds to what we previously termed the \"top generators\" of x<cite> [18]</cite> .",
  "y": "similarities background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_18",
  "x": "Alternatively, the scores used to determine Dinit can be utilized, if available. Parameter selection for graph-based methods. There are two motivations underlying our approach to choosing values for our algorithms' parameters<cite> [18]</cite> .",
  "y": "motivation"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_19",
  "x": "Main result. We first consider our main question: can we substantially boost the effectiveness of HITS by applying it to cluster-to-document graphs, which we have argued are more suitable for it than the document-to-document graphs we constructed in our previous work<cite> [18]</cite> ? The answer, as shown in Table 1 , is clearly \"yes\": we see that moving to cluster-to-document graphs results in substantial improvement for HITS, and indeed boosts its results over those for PageRank on document-to-document graphs.",
  "y": "motivation"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_20",
  "x": "We now turn to Figure 2 , which gives the results for the re-ranking algorithms docInflux, doc-PageRank and doc-Auth as applied to either the document-based graph d\u2194d (as in<cite> [18]</cite> ) or the clusterdocument graph c\u2192d. (Discussion of doc-Hub is deferred to Section 5.3.) To focus our discussion, it is useful to first point out that in almost all of our nine evaluation settings (3 corpora \u00d7 3 evaluation measures), all three of the re-ranking algorithms perform better when applied to c\u2192d graphs than to d\u2194d graphs, as the number of dark bars in Figure 2 indicates.",
  "y": "uses"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_21",
  "x": "As the notation suggests, this corresponds to running HITS and PageRank on the same graph, d\u2194d. But an alternative interpretation<cite> [18]</cite> is that non-smoothed (or no-random-jump) PageRank, as expressed by Equation (3), is applied to a different version of d\u2194d wherein the original edge weights w t(u \u2192 v) have been smoothed as follows: (we ignore nodes with no positive-weight out-edges to simplify discussion, and omit the d\u2194d superscripts for clarity). How does HITS perform on document-to-document graphs that are \"truly equivalent\", in the sense of employing the above edge-weighting regime, to those that PageRank is applied to? One reason this is an interesting question is that HITS assigns scores of zero to nodes that are not in the graph's largest connected component (with respect to positive-weight edges, considered to be bi-directional).",
  "y": "differences"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_22",
  "x": "In previous work, we showed that PageRank centrality scores induced over documentbased graphs can be used as a multiplicative weight on document query-likelihood terms, the intent being to cope with cases in which centrality in Dinit and relevance are not strongly correlated<cite> [18]</cite> . Indeed, employing this technique on the AP, TREC8, and WSJ corpora, prec@5 increases from .519, .524 and .536, to .531, .56 and .572 respectively. The same modification could be applied to the c\u2192d-based algorithms, although it is not particularly well-motivated in the HITS case.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_23",
  "x": "The same modification could be applied to the c\u2192d-based algorithms, although it is not particularly well-motivated in the HITS case. While PageRank scores correspond to a stationary distribution that could be loosely interpreted as a prior<cite> [18]</cite> , in which case multiplicative combination with query likelihood is sensible, it is not usual to assign a probabilistic interpretation to hub or authority scores. Nonetheless, for the sake of comparison completeness, we applied this idea to the doc-Auth[c\u2192d] algorithm, yielding the following performance changes: from .541, .544, and .564 to .537, .572 and .572 respectively.",
  "y": "background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_24",
  "x": "The same modification could be applied to the c\u2192d-based algorithms, although it is not particularly well-motivated in the HITS case. While PageRank scores correspond to a stationary distribution that could be loosely interpreted as a prior<cite> [18]</cite> , in which case multiplicative combination with query likelihood is sensible, it is not usual to assign a probabilistic interpretation to hub or authority scores. Nonetheless, for the sake of comparison completeness, we applied this idea to the doc-Auth[c\u2192d] algorithm, yielding the following performance changes: from .541, .544, and .564 to .537, .572 and .572 respectively.",
  "y": "uses background"
 },
 {
  "id": "72323bc821355923b8c4444ee37ef9_25",
  "x": "These results are still as good as -and for two corpora better than -those for PageRank as a multiplicative weight on query likelihood. Thus, it may be the case that centrality scores induced over a document-based graph are more effective as a multiplicative bias on query-likelihood than as direct representations of relevance in Dinit (see also<cite> [18]</cite> ); but, modulo the caveat above, it seems that when centrality is induced over cluster-based one-way bipartite graphs, the correlation with relevance is much stronger, and hence this kind of centrality serves as a better \"bias\" on query-likelihood. ----------------------------------",
  "y": "background"
 },
 {
  "id": "74471d4e333ce76fd62b968045eba5_0",
  "x": "**TUTORIAL DESCRIPTION** This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (Saon and Chien, 2012; Chan et al., 2016) to document summarization (Chang and Chien, 2009 ), text classification (Blei et al., 2003; <cite>Zhang et al., 2015</cite>) , text segmentation (Chien and Chueh, 2012) , information extraction (Narasimhan et al., 2016) , image caption generation (Vinyals et al., 2015; Xu et al., 2015) , sentence generation (Li et al., 2016b) , dialogue control (Zhao and Eskenazi, 2016; Li et al., 2016a) , sentiment classification, recommendation system, question answering (Sukhbaatar et al., 2015) and machine translation , to name a few. Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model.",
  "y": "uses background"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_0",
  "x": "We present a modification to the standard LSTM-based sequence labeling model that handles both problems and operates linearly in the number of tokens and the number of possible output labels at any token. The proposed neural network approach additionally jointly models entity mention head 2 information, a subtask found to be useful for many information extraction applications. Our model significantly outperforms the previously mentioned hypergraph model of<cite> Lu and Roth (2015)</cite> and Muis and Lu (2017) on entity mention recognition for the ACE2004 and ACE2005 corpora.",
  "y": "differences"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_1",
  "x": "Their model achieved state-of-the-art results on the GENIA dataset. However, the time complexity of their model is O(n 3 ), where n is the number of tokens in the sentence, making inference slow. As a result, we do not adopt their parse tree-based representation of nested entities and propose instead a linear time directed hypergraph-based model similar to that of<cite> Lu and Roth (2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_2",
  "x": "While most previous efforts for nested entity recognition were limited to named entities,<cite> Lu and Roth (2015)</cite> addressed the problem of nested entity mention detection where mentions can either be named, nominal or pronominal. Their hypergraph-based approach is able to represent the potentially exponentially many combinations of nested mentions of different types. They adopted a CRF-like log-linear approach to learn these mention hypergraphs and employed several hand-crafted features defined over the input sentence and the output hypergraph structure.",
  "y": "background"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_3",
  "x": "Recently, Muis and Lu (2017) introduced the notion of mention separators for nested entity mention detection. In contrast to the hypergraph representation that we and<cite> Lu and Roth (2015)</cite> adopt, they learn a multigraph representation and are able to perform exact inference on their structure. It is an interesting orthogonal possible approach for nested entity mention detection.",
  "y": "uses"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_4",
  "x": "If we look closely at Figure 2 then we realise that there is an extra \"O\" node in the hypergraph corresponding to the token \"his\" which did not appear in any entity output sequence in Figure 1 : in our task-specific hypergraph construction we make sure that there is an \"O\" node at every timestep to model the possibility of beginning of a new entity. The need for this will become more clear in Section 4. Note that the hypergraph representation of our model is similar to<cite> Lu and Roth (2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_5",
  "x": "Also, the expressiveness of our model is exactly the same as<cite> Lu and Roth (2015)</cite> ; Muis and Lu (2017) . The major difference in the two approaches is in learning. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_6",
  "x": "**EVALUATION METRICS** We use a strict evaluation metric similar to<cite> Lu and Roth (2015)</cite> : an entity mention is considered correct if both the mention span and the mention type are exactly correct. Similarly, for the task of joint extraction of entity mentions and mention heads, the mention span, head span and the entity type should all exactly match the gold label.",
  "y": "similarities"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_9",
  "x": "We show the performance of our approaches in Table 1 compared to the previous state-of-the-art system<cite> (Lu and Roth, 2015</cite>; Muis and Lu, 2017) on both the ACE2004 and ACE2005 datasets. We find that our LSTM-flat baseline that ignores embedded entity mentions during training performs worse than<cite> Lu and Roth (2015)</cite> ; however, our other neural network-based approaches all outperform the previous feature-based approach. Among the neural network-based models, we find that our models that construct a hypergraph perform better than the LSTM-flat models.",
  "y": "differences"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_10",
  "x": "We also evaluate our model on the GENIA dataset (Ohta et al., 2002) for nested named entity recognition. We follow the same dataset split as Finkel and Manning (2009);<cite> Lu and Roth (2015)</cite> ; Muis and Lu (2017) . Thus, the first 90% of the sentences were used in training and the remaining 10% were used for evaluation.",
  "y": "similarities uses"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_12",
  "x": "Roth (2015) . We suspect that it is because we use pretrained word embeddings 5 trained on PubMed data (Pyysalo et al., 2013) whereas<cite> Lu and Roth (2015)</cite> did not have access to them. We again find that our neural network model outperforms the previous state-of-the-art (Finkel and Manning, 2009; Muis and Lu, 2017) system.",
  "y": "differences"
 },
 {
  "id": "74b8684eaabda30a2d8705adcb19a2_13",
  "x": "We show that our model significantly outperforms a feature based mention hypergraph model<cite> (Lu and Roth, 2015)</cite> and a recent multigraph model (Muis and Lu, 2017) on the ACE dataset. Our model also outperforms the constituency parser-based approach of Finkel and Manning (2009) on the GENIA dataset. In future work, it would be interesting to learn global dependencies between the output labels for such a hypergraph structure and training the model globally.",
  "y": "differences"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_0",
  "x": "LDA naturally answers two of the three main problems mentioned above, i.e. (C1) and (C2), of the WSI task (Brody and Lapata 2009) . However, it is not flexible with regards to (C3), or the sense granularity problem, as it requires the users to specify the number of senses: Current systems (<cite>Wang et al. 2015</cite>; Chang, Pei, and Chen 2014) required to set the number of senses to a small number (set to 3 or 5 in the literature) to get a good accuracy, however many words may have a large number of senses, e.g. play in Figure 1 . ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_1",
  "x": "In the most recent shared task on WSI (Jurgens and Klapaftis 2013) , top models used lexical substitution method (AI-KU) (Baskaya et al. 2013) and Hierarchical Dirichlet Process trained with additional instances (Unimelb) . Latent variable models such as LDA (Blei, Ng, and Jordan 2003) are used to induce the word sense of a target word after rigorous preprocessing and feature extraction (LDA, Spectral) (Goyal and Hovy 2014). More recent models introduced a latent variable for the sense of a word, with the assumption that a sense has multiple concepts (HC, HC+Zipf) (Chang, Pei, and Chen 2014) and that topics and senses should be inferred jointly (STM) (<cite>Wang et al. 2015</cite>) .",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_2",
  "x": "Recent inclusions to the WSI models are neural-based dense distributional representation models. STM also used word embeddings (Mikolov et al. 2013) to assign similarity weights during inference (STM+w2v) (<cite>Wang et al. 2015</cite>) . Existing sense embeddings are also used to perform word sense induction (CRP-PPMI, SE-WSI-fix, WG, DIVE) (Song 2016; Pelevina et al. 2016; Chang et al. 2018 ).",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_3",
  "x": "First, LDA tries to give instance assignments to all senses even when it is unnecessary. For example, when the number of senses S is set to 10, the model tries to assign all the senses to all instances even when the original number of senses of a target word is 3. LDA extensions (<cite>Wang et al. 2015</cite>; Chang, Pei, and Chen 2014) mitigated this problem by setting S to a small number (e.g. 3 or 5).",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_4",
  "x": "For example, when the number of senses S is set to 10, the model tries to assign all the senses to all instances even when the original number of senses of a target word is 3. LDA extensions (<cite>Wang et al. 2015</cite>; Chang, Pei, and Chen 2014) mitigated this problem by setting S to a small number (e.g. 3 or 5). However, this is not a good solution because there are many words with more than five senses.",
  "y": "motivation"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_5",
  "x": "Previous works also attempted to introduce a separate sense latent variable to generate all the words (Chang, Pei, and Chen 2014) , or to generate only the neighboring words within a local context, decided by a strict user-specified window (<cite>Wang et al. 2015</cite>) . We improve by softening the strict local context assumption by introducing a switch variable which decides whether a word not in a local context should be generated by conditioning also on the sense latent variable. Our experiments show that our sense representation provides superior improvements from previous models.",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_6",
  "x": "Datasets and preprocessing We use two publicly available datasets: SemEval 2010 Task 14 (Manandhar et al. 2010) For preprocessing, we do tokenization, lemmatization, and removing of symbols to build the word lists using Stanford CoreNLP (Manning et al. 2014) . We divide the word lists into two contexts: the local and global context. Following (<cite>Wang et al. 2015</cite>), we set the local context window to 10, with a maximum number of words of 21 (i.e. 10 words before and 10 words after).",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_7",
  "x": "Parameter setting We set the hyperparameters to \u03b1 = 0.1, \u03b2 = 0.01, \u03b3 = 0.3, following the conventional setup (Griffiths and Steyvers 2004; Chemudugunta, Smyth, and Steyvers 2006) . We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following (<cite>Wang et al. 2015</cite>) . We also include four other versions of our model: AutoSense \u2212wp removes the target-neighbor pair constraint and transforms the local context to that of STM, AutoSense \u2212sw removes the switch variable and transforms the global context to that of LDA, AutoSense s=X is a tuned and best version of the model, where the number of senses is tuned over a separate development set provided by the shared tasks and X is the tuned number of sense, different for each dataset, and AutoSense s=100 is the overestimated and worst version of the model, where we set the number of senses to an arbitrary large number, i.e. 100.",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_8",
  "x": "We set the number of iterations to 2000 and run the Gibbs sampler. Following the convention of previous works (Lau et al. 2012; Goyal and Hovy 2014; <cite>Wang et al. 2015</cite>) , we assume convergence when the number of iterations is high. However, due to the randomized nature of Gibbs sampling, we report the average scores over 5 runs of Gibbs sampling.",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_9",
  "x": "SemEval 2010 For the SemEval 2010 dataset, we compare models using two unsupervised metrics: V-measure (V-M) and paired F-score (F-S). V-M favors a high number of senses (e.g. assigning one cluster per instance), while F-S favors a small number of senses (e.g. all instances in one cluster) (Manandhar et al. 2010) . In order to get a common ground for comparison, we do a geometric average AVG of both metrics, following (<cite>Wang et al. 2015</cite>) .",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_10",
  "x": "Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in (<cite>Wang et al. 2015</cite>) . We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (Jurgens and Klapaftis 2013) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in (<cite>Wang et al. 2015</cite>) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (Chang et al. 2018) , and g) Multi Context Continuous model MCC as reported in (Komninos and Manandhar 2016) . Results are shown in Table 2b .",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_11",
  "x": "F-BC gives preference to labelling all instances with the same sense, while F-NMI gives preference to labelling all instances with distinct senses. Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in (<cite>Wang et al. 2015</cite>) . We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (Jurgens and Klapaftis 2013) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in (<cite>Wang et al. 2015</cite>) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (Chang et al. 2018) , and g) Multi Context Continuous model MCC as reported in (Komninos and Manandhar 2016) .",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_12",
  "x": "This means that introducing the target-neighbor pair is crucial to the improvement of the model. Finally, the overestimated AutoSense model performs as well as the other AutoSense models, even outperforming all previous models on AVG, which proves the effectiveness of AutoSense even when s is set to a large value. For completeness, we also report STM with additional contexts, STM+actual and STM+ukWac (<cite>Wang et al. 2015</cite>) , where they used the actual additional contexts from the original data and semantically similar contexts from ukWac, respectively, as additional global context.",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_13",
  "x": "Recent models such as HC (Chang, Pei, and Chen 2014) and STM (<cite>Wang et al. 2015</cite>) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error. However, such tuning, often empirically set to a small number such as S = 3 (<cite>Wang et al. 2015</cite>) , fails to infer varying number of senses of words, especially for words with a higher number of senses. Nonparametric models such as HDP and BNP-HC Chang, Pei, and Chen 2014) claim to automatically induce different S for each word.",
  "y": "background"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_14",
  "x": "Recent models such as HC (Chang, Pei, and Chen 2014) and STM (<cite>Wang et al. 2015</cite>) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error. However, such tuning, often empirically set to a small number such as S = 3 (<cite>Wang et al. 2015</cite>) , fails to infer varying number of senses of words, especially for words with a higher number of senses. Nonparametric models such as HDP and BNP-HC Chang, Pei, and Chen 2014) claim to automatically induce different S for each word.",
  "y": "motivation"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_15",
  "x": "We compare the competing models quantitatively on how they correctly detect the actual number of sense clusters using cluster error, which is the mean absolute error between the detected number and the actual number of sense clusters. We compare the cluster errors of LDA (Blei, Ng, and Jordan 2003) , STM (<cite>Wang et al. 2015</cite>) , HC (Chang, Pei, and Chen 2014) , and a nonparametric model HDP (Teh et al. 2004 ), with AutoSense. We report the results in Figure 4 .",
  "y": "uses"
 },
 {
  "id": "75b2aa54c363151130ca2146044922_16",
  "x": "We use LDA (Blei, Ng, and Jordan 2003) , HC (Chang, Pei, and Chen 2014) and STM (<cite>Wang et al. 2015</cite>) as baselines. We do not compare with non-text feature-based models (Tang et al. 2012; Cen et al. 2013 ) because our goal is to compare sense topic models on a task where the sense granularities are more varied. For STM and AutoSense, the title, publication venue and the author names are used as local contexts while the abstract is used as the global context.",
  "y": "uses"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_0",
  "x": "Despite a large body of work on neural ranking models for \"traditional\" ad hoc retrieval over web pages and newswire documents (Huang et al., 2013; Shen et al., 2014; Pang et al., 2016; Xiong et al., 2017; Mitra et al., 2017; Pang et al., 2017; Dai et al., 2018; McDonald et al., 2018) , there has been surprisingly little work on applying neural networks to searching short social media posts such as tweets on Twitter. <cite>Rao et al. (2018)</cite> identified short document length, informality of language, and heterogeneous relevance signals as main challenges in relevance modeling, and proposed a model specifically designed to handle these characteristics. Evaluation on a number of datasets from the TREC Microblog Tracks demonstrates state-of-the-art effectiveness as well as the necessity of different model components to capture a multitude of relevance signals.",
  "y": "background"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_1",
  "x": "<cite>Rao et al. (2018)</cite> identified short document length, informality of language, and heterogeneous relevance signals as main challenges in relevance modeling, and proposed a model specifically designed to handle these characteristics. In this paper, we also examine the problem of modeling relevance for ranking short social media posts, but from a complementary perspective.",
  "y": "similarities background"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_2",
  "x": "Following <cite>Rao et al. (2018)</cite> , we evaluate our models in a reranking task, where the inputs are up to the top 1000 tweets retrieved from the classical query likelihood (QL) language model (Ponte and Croft, 1998) . We run four-fold cross-validation test split by year (i.e., train on three year's data, test on one year's data), and we randomly sample 10 queries from each year in the training sets (in total 30 queries) as our validation set. The mean average precision (MAP) and precision at top 30 (P30) are adopted as our evaluation metrics.",
  "y": "uses"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_3",
  "x": "RM3 (Lavrenko and Croft, 2001 ) is an interpolation model combining the QL score with a relevance model using pseudo-relevance feedback. MP-HCNN<cite> (Rao et al., 2018)</cite> is the first neural model that captures the characteristics of social media domain. Their method improves current neural IR methods, e.g., K-NRM (Xiong et al., 2017) , DUET (Mitra et al., 2017) , by a signifi- 4 BiCNN+PAtt+QL 0.4728 1-3 5-7 0.4293 1-3 5-7 0.4147 1,2 5,6 0.2621 1-3 5-7 0.5367 1-3 5,6 0.2990 1-3 5 0.6806 1,2 5-8 0.4563 1-3",
  "y": "uses"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_4",
  "x": "Existing Models 5 QL 0.4000 1 0.3576 1 0.3311 1 0.2091 1 0.4450 1 0.2532 1 0.6182 1 0.3924 1 6 RM3 0.4211 1 0.3824 1 0.3452 1 0.2342 1 0.4733 1 0.2766 1 0.6339 1 0.4480 1 7 MP-HCNN(+URL) 0.4306 1 0.3940 1,2 0.3757 1,5 0.2313 1,5 0.5211 1,5 0.2856 1,5 0.6279 1 0.4178 1 8 MP-HCNN(+URL)+QL 0.4435 1-2 5,6 0.4040 1,2 5,6 0.3915 1,5 6 0.2482 1,2 5 0.5250 1,5 6 0.2937 1,2 5 0.6455 1 0.4403 1,5 Table 2 : Results of non-neural and neural models on the TREC Microblog Tracks datasets. Results from 5 -8 are adopted from <cite>Rao et al. (2018)</cite> . Models denoted with (+URL) represents utilizing the URL information.",
  "y": "uses"
 },
 {
  "id": "782517ae7688cf18b4bca37a8087dd_5",
  "x": "cant margin. To the best of our knowledge, <cite>Rao et al. (2018)</cite> is the best neural model to date, and there are no neural models from TREC evaluations for further comparison. We also compared to MP-HCNN+QL, which is a linear interpolation to combine the raw MP-HCNN and QL scores.",
  "y": "background"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_0",
  "x": "Crucially, our models can operate without language at test time: a more practical setting, since it is often unrealistic to assume that linguistic supervision is available for unseen classes encountered in the wild. Compared to meta-learning baselines and recent approaches which use language supervision as a more fundamental bottleneck in a model <cite>[1]</cite> , we find this simple auxiliary training objective results in learned representations that generalize better to new concepts. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_3",
  "x": "In this framework, learning with attributes and other domain-specific rationales has been tackled extensively [8, 9, 29] , but language remains relatively unexplored. [13] use METEOR scores between captions as a similarity metric for specializing embeddings for image retrieval, but do not directly Figure<cite> 1</cite> : Building on prototype networks [26] , we propose few-shot classification models whose learned representations are constrained to predict natural language descriptions of the task during training, in contrast to models <cite>[1]</cite> which explicitly use language as a bottleneck for classification. ground language explanations.",
  "y": "differences"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_5",
  "x": "[28] explore a supervision setting similar to ours, except in highly structured text and symbolic domains where descriptions can be easily converted to executable forms via semantic parsing. Another line of work studies models which generate natural language explanations of decisions for interpretability for both textual (e.g. natural language inference; [3] ) and visual [17,<cite> 1</cite>8] tasks, but here we examine whether this act of predicting language can actually improve downstream task performance; similar ideas have been explored in text [22] and reinforcement learning [2,<cite> 1</cite>4] domains. Our work is most similar to <cite>[1]</cite> , which we describe and compare to later.",
  "y": "similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_9",
  "x": "With this component, we call our approach language-shaped learning (LSL) (Figure<cite> 1</cite> ). Relation to L3. LSL is similar to another recent model for this setting: Learning with Latent Language (L3) <cite>[1]</cite> , which proposes to use language not only as a supervision source, but as a bottleneck for classification ( Figure<cite> 1</cite> ).",
  "y": "similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_10",
  "x": "For full training details and code, see Appendix A. ShapeWorld. First, we use the ShapeWorld [20] dataset devised by <cite>[1]</cite> , which consists of 9000 training,<cite> 1</cite>000 validation, and 4000 test tasks ( Figure 2 ).",
  "y": "uses"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_11",
  "x": "Each concept is a spatial relation between two objects, optionally qualified by color and/or shape; 2-3 distractor shapes are also present in each image. The task is to predict whether a single query image x belongs to the concept. Model details are identical to <cite>[1]</cite> for easy comparison.",
  "y": "uses"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_17",
  "x": "Our code is publicly available at https://github.com/jayelm/lsl. A.1 ShapeWorld f \u03b8 . Like <cite>[1]</cite> , f \u03b8 starts with features extracted from the last convolutional layer of a fixed ImageNetpretrained VGG-19 network [25] . These 4608-d embeddings are then fed into two fully connected layers \u2208 R 4608\u00d7512 , R 512\u00d7512 with one ReLU nonlinearity in between.",
  "y": "similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_19",
  "x": "h \u03b7 is a unidirectional<cite> 1</cite>-layer GRU with hidden size 512 sharing the same word embeddings as g \u03c6 . The output of the last hidden state is taken as the embedding of the description w (t) . Like <cite>[1]</cite> , a total of<cite> 1</cite>0 descriptions per task are sampled at test time.",
  "y": "similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_21",
  "x": "We train for 50 epochs, each epoch consisting of<cite> 1</cite>00 batches with<cite> 1</cite>00 tasks in each batch, with the Adam optimizer [19] and a learning rate of 0.001. We selected the model with highest epoch validation accuracy during training. This differs slightly from <cite>[1]</cite> , who use different numbers of epochs per model and did not specify how they were chosen; otherwise, the training and evaluation process is the same.",
  "y": "differences"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_22",
  "x": "Data. We recreated the ShapeWorld dataset using the same code as <cite>[1]</cite> , except generating 4x as many test tasks (4000 vs<cite> 1</cite>000) for more stable confidence intervals. Note that results for both L3 and Baseline (Meta) are 3-4 points lower than the scores of the corresponding implementations in <cite>[1]</cite> .",
  "y": "differences uses similarities"
 },
 {
  "id": "79ac70221fa28e577876425cad627f_23",
  "x": "Data. We recreated the ShapeWorld dataset using the same code as <cite>[1]</cite> , except generating 4x as many test tasks (4000 vs<cite> 1</cite>000) for more stable confidence intervals. Note that results for both L3 and Baseline (Meta) are 3-4 points lower than the scores of the corresponding implementations in <cite>[1]</cite> .",
  "y": "differences"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_0",
  "x": "The corpus data are drawn from the British National Corpus (BNC) and are analyzed at the levels of syntax, discourse structure, and compositional semantics. Following <cite>Webber et al. (2003)</cite> , the paper argues that in contrast crucially involves discourse anaphora and, thus, resembles other discourse adverbials such as then, otherwise, and nevertheless. The compositional semantics proposed for other discourse connectives, however, does not straightforwardly generalize to in contrast, for which the notions of contrast pairs and contrast properties are essential.",
  "y": "differences"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_1",
  "x": "2. While syntactic dependencies can be quite complex and may involve highly nested or even crossing dependencies of various kinds, dependencies expressed by discourse connectives tend to be much more limited, typically involving tree-like structures and not introducing structural ambiguities of scope or attachment. 3. More complex cases of discourse connectives that prima facie seem to involve crossing or partially overlapping arguments can be reduced to the independent discourse mechanisms of anaphora and attribution and thus do not introduce any added complexities. The third generalization is further elaborated by <cite>Webber et al. (2003)</cite> who distinguish between coordinating conjunctions such as and, or, so, and but and subordinating conjunctions such as although, whereas, and when on the one hand, and discourse adverbials such as then, otherwise, nevertheless, and instead on the other hand.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_2",
  "x": "The third generalization is further elaborated by <cite>Webber et al. (2003)</cite> who distinguish between coordinating conjunctions such as and, or, so, and but and subordinating conjunctions such as although, whereas, and when on the one hand, and discourse adverbials such as then, otherwise, nevertheless, and instead on the other hand. It is the latter group, namely discourse adverbials, that, according to <cite>Webber et al. (2003)</cite> , should be considered as anaphors in very much the same way as other anaphoric expressions such as definite descriptions and pronouns. The purpose of this paper is to further examine and refine the above hypotheses by looking in some detail at a family of discourse connectives, all involving the notion of contrast.",
  "y": "similarities"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_3",
  "x": "**DISCOURSE ANAPHORA** This section will focus on the discourse function of the adverbial phrase in contrast. Following <cite>Webber et al. (2003)</cite>, we will argue that it resembles other discourse adverbials such as then, otherwise, and nevertheless in that it crucially involves the notion of discourse anaphora.",
  "y": "similarities"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_4",
  "x": "(8) Sue grabbed one phone, as Tom darted to the other phone.<cite> (Webber et al. (2003)</cite>, p. 555) Here the referent of the other phone can be inferred from the antecedent one phone. The referential relation between the anaphor and the antecedent is not one of identity of reference. Rather, the referents of the antecedent and the anaphor together constitute the set of phones owned by Sue and Tom.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_5",
  "x": "B7G (0726) Note that the domain of the set of countries in the quantified NP few countries is subsequently narrowed so as to not include countries in Europe, North America and Japan. It is precisely the explicitly mentioned contrast that leads to this effect. <cite>Webber et al. (2003)</cite> observe that identification of the correct antecedent of a definite description such as the tower or this tower in (12a) or a discourse adverbial such as otherwise in (12b) may require reference to abstract discourse objects such as the result of stacking blocks (to form a tower) or the state of not wanting an apple as the logical antecedent of a definite description or of a discourse adverbial.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_6",
  "x": "<cite>Webber et al. (2003)</cite> observe that identification of the correct antecedent of a definite description such as the tower or this tower in (12a) or a discourse adverbial such as otherwise in (12b) may require reference to abstract discourse objects such as the result of stacking blocks (to form a tower) or the state of not wanting an apple as the logical antecedent of a definite description or of a discourse adverbial. (12) a. Stack five blocks on top of one another. Now close your eyes and try knocking the tower, this tower\u00a1 over with your nose.<cite> (Webber et al. (2003)</cite> , p. 552) b. Do you want an apple? Otherwise you can have a pear.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_7",
  "x": "B0H (0476) Another property that distinguishes anaphoric discourse adverbials from structural connectives in the sense of <cite>Webber et al. (2003)</cite> , i.e. coordinating and subordinating conjunctions, concerns the type of dependencies that the arguments of the types of connectives can enter into. While structural connectives only allow non-crossing adjacent material as their arguments, discourse adverbials may involve crossing dependencies among non-adjacent material -just like other anaphoric expressions. e.g. pronouns and definite descriptions.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_8",
  "x": "Teufel and Moens (2002) and Siddharthan and Teufel (2007) In the previous section we established at some length that in contrast shares with other discourse adverbials its anaphoric behavior. This naturally raises the question whether the semantics that has been proposed for this class of expressions can be naturally generalized to the semantics of in contrast. Following earlier proposals by Hinrichs (1986) and Kamp and Reyle (1993) , <cite>Webber et al. (2003)</cite> assume that the semantics of discourse adverbials such as then involves an anaphoric relation between two events.",
  "y": "background"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_9",
  "x": "The semantics proposed for in-contrast, thus, provides further evidence for the distinction between coordinating and subordinating conjunctions and discourse adverbials that has been put forth by <cite>Webber et al. (2003)</cite> . ---------------------------------- **CONCLUSION AND FUTURE WORK**",
  "y": "differences"
 },
 {
  "id": "79ff6e23cc951aa18ae53763e9c982_10",
  "x": "Following <cite>Webber et al. (2003)</cite> , the paper argues that in contrast crucially involves discourse anaphora and, thus, resembles other discourse adverbials such as then, otherwise, and nevertheless. The compositional semantics proposed for other discourse connectives, however, does not straightforwardly generalize to in contrast, for which the notions of contrast pairs and contrast properties are essential. In future work we plan to consider a wider range of contrast relations in discourse such as by comparison, contrary to and on the other hand in order to ascertain whether the properties of the discourse connective in contrast will generalize to these cases as well.",
  "y": "differences"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_0",
  "x": "More recent work has focused on reducing that constraint. Vuli\u0107 and Moens (2016) and Vulic and Korhonen (2016) use document-aligned data to learn bilingual embeddings instead of a seed dictionary. <cite>Artetxe et al. (2017)</cite> use a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces; Zhang et al. (2017) use an adversarial training method to learn a similar mapping.",
  "y": "background"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_1",
  "x": "<cite>Artetxe et al. (2017)</cite> use a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces; Zhang et al. (2017) use an adversarial training method to learn a similar mapping. These recent advances in unsupervised bilingual lexicon induction show promise for use in low-resource contexts. However, none of them make use of linguistic features of the languages themselves (with the arguable exception of syntactic/semantic information encoded in the word embeddings).",
  "y": "motivation"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_2",
  "x": "The addition of linguistic features led to increased performance in these earlier models, especially for related languages, yet these features have not been applied to more modern methods. In this work, we extend the modern embeddingbased approach of <cite>Artetxe et al. (2017)</cite> with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_3",
  "x": "This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . Following their work, let X \u2208 R |Vs|\u00d7d and Z \u2208 R |Vt|\u00d7d be the word embedding matrices of two distinct languages, referred to respectively as the source and target, such that each row corresponds to the d-dimensional embedding of a single word. We refer to the ith row of one of these matrices as X i * or Z i * .",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_4",
  "x": "This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . Following their work, let X \u2208 R |Vs|\u00d7d and Z \u2208 R |Vt|\u00d7d be the word embedding matrices of two distinct languages, referred to respectively as the source and target, such that each row corresponds to the d-dimensional embedding of a single word. We refer to the ith row of one of these matrices as X i * or Z i * .",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_5",
  "x": "We wish to find a mapping matrix W \u2208 R d\u00d7d that maps source embeddings onto their aligned target embeddings. <cite>Artetxe et al. (2017)</cite> define the optimal mapping matrix W * with the following equation, which minimizes the sum of the squared Euclidean distances between mapped source embeddings and their aligned target embeddings.",
  "y": "background"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_6",
  "x": "This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . <cite>Artetxe et al. (2017)</cite> define the optimal mapping matrix W * with the following equation, which minimizes the sum of the squared Euclidean distances between mapped source embeddings and their aligned target embeddings.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_7",
  "x": "To reduce the need for a large seed dictionary, <cite>Artetxe et al. (2017)</cite> propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates until convergence. We propose two methods for extending this system using orthographic information, described in the following two sections.",
  "y": "extends"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_8",
  "x": "To reduce the need for a large seed dictionary, <cite>Artetxe et al. (2017)</cite> propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates until convergence. In the dictionary induction step, they set We propose two methods for extending this system using orthographic information, described in the following two sections.",
  "y": "background"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_9",
  "x": "This work is directly based on the work of <cite>Artetxe et al. (2017)</cite> . To reduce the need for a large seed dictionary, <cite>Artetxe et al. (2017)</cite> propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates until convergence.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_10",
  "x": "---------------------------------- **ORTHOGRAPHIC EXTENSION OF WORD EMBEDDINGS** This method augments the embeddings for all words in both languages before using them in the self-learning framework of <cite>Artetxe et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_11",
  "x": "This method modifies the similarity score for each word pair during the dictionary induction phase of the self-learning framework of <cite>Artetxe et al. (2017)</cite> , which uses the dot product of two words' embeddings to quantify similarity. We modify this similarity score by adding a measure of orthographic similarity, which is a function of the normalized string edit distance of the two words. The normalized edit distance is defined as the Levenshtein distance (L(\u00b7, \u00b7)) (Levenshtein, 1966) divided by the length of the longer word.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_12",
  "x": "**ORTHOGRAPHIC SIMILARITY ADJUSTMENT** This method modifies the similarity score for each word pair during the dictionary induction phase of the self-learning framework of <cite>Artetxe et al. (2017)</cite> , which uses the dot product of two words' embeddings to quantify similarity. We modify this similarity score by adding a measure of orthographic similarity, which is a function of the normalized string edit distance of the two words.",
  "y": "extends"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_13",
  "x": "**EXPERIMENTS** We use the datasets used by <cite>Artetxe et al. (2017)</cite> , consisting of three language pairs: EnglishItalian, English-German, and English-Finnish. The English-Italian dataset was introduced in Dinu and Baroni (2014) ; the other datasets were created by <cite>Artetxe et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_14",
  "x": "We use the datasets used by <cite>Artetxe et al. (2017)</cite> , consisting of three language pairs: EnglishItalian, English-German, and English-Finnish. The English-Italian dataset was introduced in Dinu and Baroni (2014) ; the other datasets were created by <cite>Artetxe et al. (2017)</cite> . Each dataset includes monolingual word embeddings (trained with word2vec (Mikolov et al., 2013b) ) for both languages and a bilingual dictionary, separated into a training and test set.",
  "y": "uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_15",
  "x": "The English-Italian dataset was introduced in Dinu and Baroni (2014) ; the other datasets were created by <cite>Artetxe et al. (2017)</cite> . Each dataset includes monolingual word embeddings (trained with word2vec (Mikolov et al., 2013b) ) for both languages and a bilingual dictionary, separated into a training and test set. We do not use the training set as the input dictionary to the system, instead using an automatically-generated dictionary consisting only of numeral identity translations (such as 2-2, 3-3, et cetera) as in <cite>Artetxe et al. (2017)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_16",
  "x": "Table 1 compares our methods against the system of <cite>Artetxe et al. (2017)</cite> , using scaling factors selected based on development data results. Because approximately 20% of source-target pairs in the dictionary were identical, we also extended all systems to guess the identity translation if the source word appeared in the target vocabulary. This improved accuracy in most cases, with some exceptions for English-Italian.",
  "y": "differences"
 },
 {
  "id": "7b9fc52e4479dc5ff9b8796a558981_17",
  "x": "For our experiments with orthographic similarity adjustment, the heuristic identified approximately 2 million word pairs for each language pair out of a possible 40 billion, resulting in significant computation savings. and c s = 1 as our hyperparameters. The local optima were not identical for all three languages, but we felt that these values struck the best compromise among them. Table 1 compares our methods against the system of <cite>Artetxe et al. (2017)</cite> , using scaling factors selected based on development data results.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_0",
  "x": "The perceptual effects can be so dramatic that prominent entities may not even rise to the level of awareness when the viewer is attending to other things in the scene [3, 4, 5] . Yet attention has not been a transformative force in computer vision, possibly because many standard computer vision tasks like detection, segmentation, and classification do not involve the sort of complex reasoning which attention is thought to facilitate. Answering detailed questions about an image is a type of task which requires more sophisticated patterns of reasoning, and there has been a rapid recent proliferation of computer vision approaches for tackling the visual question answering (Visual QA) task [6,<cite> 7]</cite> .",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_1",
  "x": "It uses a hard attention mechanism that selects only the important visual features for the task for further processing. We base our architecture on the premise that the norm of the visual features correlates with their relevance, and that those feature vectors with high magnitudes correspond to image regions which contain important semantic content. to handle many objects and their complex relations while also integrating rich background knowledge, and attention has emerged as a promising strategy for achieving good performance<cite> [7,</cite> 8, 9, 10, 11, 12, 13, 14] .",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_2",
  "x": "Existing attention models<cite> [7,</cite> 8, 9, 10] are predominantly based on soft attention, in which all information is adaptively re-weighted before being aggregated. This can improve accuracy by isolating important information and avoiding interference from unimportant information. Learning becomes more data efficient as the complexity of the interactions among different pieces of information reduces; this, loosely speaking, allows for more unambiguous credit assignment.",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_3",
  "x": "In particular, selecting those feature vectors with the greatest L2-norm values proves to be a heuristic that can facilitate hard attention -and provide the performance and efficiency benefits associated with -without requiring specialized learning procedures (see Figure 1 ). This attentional signal results indirectly from a standard supervised task loss, and does not require explicit supervision to incentivize norms to be proportional to object presence, salience, or other potentially meaningful measures [20, 21] . We rely on a canonical Visual QA pipeline<cite> [7,</cite> 9, 22, 23, 24, 25] augmented with a hard attention mechanism that uses the L2-norms of the feature vectors to select subsets of the information for further processing.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_4",
  "x": "**RELATED WORK** Visual question answering, or more broadly the Visual Turing Test, asks \"Can machines understand a visual scene only from answering questions?\" [6, 23, 29, 30, 31, 32] . Creating a good Visual QA dataset has proved non-trivial: biases in the early datasets [6, 22, 23, 33] rewarded algorithms for exploiting spurious correlations, rather than tackling the reasoning problem head-on<cite> [7,</cite> 34, 35] .",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_5",
  "x": "Visual question answering, or more broadly the Visual Turing Test, asks \"Can machines understand a visual scene only from answering questions?\" [6, 23, 29, 30, 31, 32] . Creating a good Visual QA dataset has proved non-trivial: biases in the early datasets [6, 22, 23, 33] rewarded algorithms for exploiting spurious correlations, rather than tackling the reasoning problem head-on<cite> [7,</cite> 34, 35] . Thus, we focus on the recently-introduced VQA-CP <cite>[7]</cite> and CLEVR [34] datasets, which aim to reduce the dataset biases, providing a more difficult challenge for rich visual reasoning.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_6",
  "x": "The most successful Visual QA architectures build multimodal representations with a combined CNN+LSTM architecture [22, 33, 41] , and recently have begun including attention mechanisms inspired by soft and hard attention for image captioning [42] . However, only soft attention is used in the majority of Visual QA works<cite> [7,</cite> 8, 9, 10, 11, 12, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52] . In these architectures, a full-frame CNN representation is used to compute a spatial weighting (attention) over the CNN grid cells.",
  "y": "background motivation"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_7",
  "x": "As is common in question answering<cite> [7,</cite> 9, 22, 23, 24] , the question is a sequence of words q = [q 1 , ..., q n ], while the output is reduced to a classification problem between a set of common answers (this is limited compared to approaches that generate answers [41] , but works better in practice). Our architecture for learning a mapping from image and question, to answer, is shown in Figure 2 . We encode the image with a CNN [62] (in our case, a pre-trained ResNet-101 [63] , or a small CNN trained from scratch), and encode the question to a fixed-length vector representation with an LSTM [64] .",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_8",
  "x": "Our architecture for learning a mapping from image and question, to answer, is shown in Figure 2 . We encode the image with a CNN [62] (in our case, a pre-trained ResNet-101 [63] , or a small CNN trained from scratch), and encode the question to a fixed-length vector representation with an LSTM [64] . We compute a combined representation by copying the question representation to every spatial location in the CNN, and concatenating it with (or simply adding it to) the visual features, like previous Otherwise, we follow the canonical Visual QA pipeline<cite> [7,</cite> 9, 22, 23, 24, 25] .",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_10",
  "x": "Our attention mechanism selectively chooses a subset of the multimodal vectors that are next aggregated and processed by the answering module. work<cite> [7,</cite> 9, 22, 23, 24, 25] . After a few layers of combined processing, we apply attention over spatial locations, following previous works which often apply soft attention mechanisms<cite> [7,</cite> 8, 9, 10] at this point in the architecture.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_11",
  "x": "VQA-CP v2. This dataset <cite>[7]</cite> consists of about 121K (98K) images, 438K (220K) questions, and 4.4M (2.2M) answers in the train (test) set; and it is created so that the distribution of the answers between train and test splits differ, and hence the models cannot excessively rely on the language prior <cite>[7]</cite> . As expected, <cite>[7]</cite> show that performance of all Visual QA approaches they tested drops significantly between train to test sets.",
  "y": "background"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_12",
  "x": "This dataset <cite>[7]</cite> consists of about 121K (98K) images, 438K (220K) questions, and 4.4M (2.2M) answers in the train (test) set; and it is created so that the distribution of the answers between train and test splits differ, and hence the models cannot excessively rely on the language prior <cite>[7]</cite> . As expected, <cite>[7]</cite> show that performance of all Visual QA approaches they tested drops significantly between train to test sets. The dataset provides a standard traintest split, and also breaks questions into different question types: those where the answer is yes/no, those where the answer is a number, and those where the answer is something else. Thus, we report accuracy on each question type as well as the overall accuracy for each network architecture.",
  "y": "motivation"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_13",
  "x": "These results are shown in the middle of Table 1 , where we can see that hard attention (48 entitties) actually boosts performance over an analogous model without hard attention. Finally, we compare standard soft attention baselines in the bottom of Table 1. In particular, we include previous results using a basic soft attention network<cite> [7,</cite> 9] , as well as our own re-implementation of the soft attention pooling algorithm presented in<cite> [7,</cite> 9] with the same features used in other experiments.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_14",
  "x": "Surprisingly, soft attention does not outperform basic sum pooling, even with careful implementation that outperforms the previously reported results with the same method on this dataset; in fact, it performs slightly worse. The nonlocal pairwise aggregation performs better than SAN on its own, although the best result includes hard attention. Our results overall are somewhat worse than the state-of-the-art <cite>[7]</cite> , but this is likely due to several architectural decisions not included here, such as a split pathway for different kinds of questions, special question embeddings, and the use of the question extractor.",
  "y": "differences"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_15",
  "x": "Table 1 shows SAN's [9] results reported by <cite>[7]</cite> together with our in-house implementation (denoted as \"ours\"). Our implementation has 2 attention hops, 1024 dimensional multimodal embedding size, a fixed learning rate 0.0001, and ResNet-101. In these experiments we pool the attended representations by weighted average with the attention weights.",
  "y": "uses"
 },
 {
  "id": "7bdb51a3ca6c322ef6e04d18ba8483_16",
  "x": "Analysis. In our experiments, simple-SAN achieves about 21% performance on the test set. Surprisingly, simple-HAN+sum achieves about 24% performance on the same split, on-par with the performance of normal SAN that uses more complex and deeper visual architecture [67] ; the results are reported by <cite>[7]</cite> .",
  "y": "similarities"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_0",
  "x": "Besides improving readability and understandability of texts for readers, automated text segmentation is beneficial for NLP and IR tasks such as text summarization (Angheluta et al., 2002; Dias et al., 2007) and passage retrieval (Huang et al., 2003; Dias et al., 2007) . Whereas early approaches to unsupervised text segmentation measured the coherence of segments via raw term overlaps between sentences (Hearst, 1997; Choi, 2000) , more recent methods (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> addressed the issue of sparsity of term-based representations by replacing term-vectors with vectors of latent topics. A topical representation of text is, however, merely a vague approximation of its meaning.",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_1",
  "x": "Text segmentation can be performed in two different ways, namely (1) with the goal of obtaining linear segmentations (i.e. detecting the sequence of different segments in a text) , or (2) in order to obtain hierarchical segmentations (i.e. defining a structure of subtopics between the detected segments). Like the majority of TS methods (Hearst, 1994; Brants et al., 2002; Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> , in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical TS, where each toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009 ). Hearst (1994 introduced TextTiling, one of the first unsupervised algorithms for linear text segmentation.",
  "y": "similarities"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_2",
  "x": "Galley et al. (2003) combined contentbased information with acoustic cues in order to detect discourse shifts whereas Utiyama and Isahara (2001) and Fragkou et al. (2004) minimized different segmentation cost functions with dynamic programming. The first segmentation approach based on topic modeling (Brants et al., 2002) employed the probabilistic latent semantic analysis (pLSA) to derive latent representations of segments and determined the segmentation based on similarities of segments' latent vectors. More recent models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003) .",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_3",
  "x": "More recent models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003) . Misra et al. (2009) used dynamic programming to find globally optimal segmentation over the set of LDA-based segment representations, whereas <cite>Riedl and Biemann (2012)</cite> introduced TopicTiling, an LDA-driven extension of Hearst's TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors). <cite>Riedl and Biemann (2012)</cite> show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009 ) and that it is also faster than other LDA-based methods (Misra et al., 2009 ).",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_4",
  "x": "More recent models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003) . Misra et al. (2009) used dynamic programming to find globally optimal segmentation over the set of LDA-based segment representations, whereas <cite>Riedl and Biemann (2012)</cite> introduced TopicTiling, an LDA-driven extension of Hearst's TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors). <cite>Riedl and Biemann (2012)</cite> show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009 ) and that it is also faster than other LDA-based methods (Misra et al., 2009 ).",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_5",
  "x": "4 Both LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> and GRAPHSEG rely on corpus-derived word representations. Thus, we evaluated on the Manifesto dataset both the domainadapted and domain-unadapted variants of these methods. The domain-adapted variants of the models used the unlabeled domain corpus -a test set of 466 unlabeled political manifestos -to train the domain-specific word representations.",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_6",
  "x": "Thus, we evaluated on the Manifesto dataset both the domainadapted and domain-unadapted variants of these methods. This means that we obtain (1) in-domain topics for the LDAbased TopicTiling model of <cite>Riedl and Biemann (2012)</cite> and (2) domain-specific embeddings for the GRAPHSEG algorithm.",
  "y": "uses"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_7",
  "x": "This means that we obtain (1) in-domain topics for the LDAbased TopicTiling model of <cite>Riedl and Biemann (2012)</cite> and (2) domain-specific embeddings for the GRAPHSEG algorithm. On the Manifesto dataset we also evaluate a baseline that randomly (50% chance) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments. We evaluate the performance using two standard TS evaluation metrics -P k (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) .",
  "y": "uses"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_8",
  "x": "We evaluate the performance using two standard TS evaluation metrics -P k (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) . P k is the probability that two randomly drawn sentences mutually k sentences apart are classified incorrectly -either as belonging to the same segment when they are in different gold segments or as being in different segments when they are in the same gold segment. Following <cite>Riedl and Biemann (2012)</cite> , we set k to half of the document length divided by the number of gold segments.",
  "y": "uses"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_9",
  "x": "In all experiments we use grid-search in a folded cross-validation setting to jointly optimize both parameters. In view of comparison with other models, the parameter optimization is justified be-3-5 6-8 9-11 3-11 Brants et al. (2002) 7. cause other models, e.g., TopicTiling<cite> (Riedl and Biemann, 2012)</cite> , also have parameters (e.g., number of topics for the topic model) which are optimized using cross-validation.",
  "y": "similarities"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_10",
  "x": "GRAPHSEG performs competitively, outperforming all methods but (Fragkou et al., 2004) and domain-adapted versions of LDA-based models (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> . However, the approach by (Fragkou et al., 2004) uses the gold standard information -the average gold segment size -as input. On the other hand, the LDA-based models adapt their topic models on parts of the Choi dataset itself.",
  "y": "differences"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_11",
  "x": "However, the approach by (Fragkou et al., 2004) uses the gold standard information -the average gold segment size -as input. On the other hand, the LDA-based models adapt their topic models on parts of the Choi dataset itself. Despite the fact that they use different documents for training the topic models from those used for evaluating segmentation quality, the evaluation is still tainted because snippets from the original documents appear in multiple artificial documents -some of which belong to the the training set and others to the test set, as admitted by <cite>Riedl and Biemann (2012)</cite> and this is why their reported performance on this dataset is overestimated.",
  "y": "background"
 },
 {
  "id": "7be8bcb17980dee5e94df9faec8183_12",
  "x": "In-domain training of word representations, topics for TopicTiling and word embeddings for GraphSeg, does not significantly improve the performance for neither of the two models. This result contrasts previous findings (Misra et al., 2009;<cite> Riedl and Biemann, 2012)</cite> in which the performance boost was credited to the indomain trained topics and supports our hypothesis that the performance boost of the LDA-based methods' with in-domain trained topics originates from information leakage between different portions of the synthetic Choi dataset. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_0",
  "x": "Pidgin English is one of the the most widely spoken languages in West Africa with roughly 75 million speakers estimated in Nigeria; and over 5 million speakers estimated in Ghana<cite> (Ogueji & Ahia, 2019)</cite> . 1 While there have been recent efforts in popularizing the monolingual Pidgin English as seen in the BBC Pidgin 2 , it remains under-resourced in terms of the available parallel corpus for machine translation. Similarly, this low-resource scenario extends to other domains in natural language generation (NLG) such as summarization, data-to-text and so on (Lebret et al., 2016; Su et al., 2018; Shen et al., 2019a; b; Zhao et al., 2019; Hong et al., 2019; de Souza et al., 2018) \u2212 where Pidgin English generation is largely under-explored.",
  "y": "background"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_1",
  "x": "Similarly, this low-resource scenario extends to other domains in natural language generation (NLG) such as summarization, data-to-text and so on (Lebret et al., 2016; Su et al., 2018; Shen et al., 2019a; b; Zhao et al., 2019; Hong et al., 2019; de Souza et al., 2018) \u2212 where Pidgin English generation is largely under-explored. The scarcity is further aggravated when the pipeline language generation system includes other sub-modules that computes semantic textual similarity (Shen et al., 2017; Zhuang & Chang, 2017) , which exists solely in English. Previous works on unsupervised neural machine translation for Pidgin English constructed a monolingual corpus<cite> (Ogueji & Ahia, 2019)</cite> , and achieved a BLEU score of 5.18 from English to Pidgin.",
  "y": "background"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_2",
  "x": "The scarcity is further aggravated when the pipeline language generation system includes other sub-modules that computes semantic textual similarity (Shen et al., 2017; Zhuang & Chang, 2017) , which exists solely in English. Previous works on unsupervised neural machine translation for Pidgin English constructed a monolingual corpus<cite> (Ogueji & Ahia, 2019)</cite> , and achieved a BLEU score of 5.18 from English to Pidgin. However, there is an issue of domain mismatch between down-stream NLG tasks and the trained machine translation system.",
  "y": "motivation background"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_3",
  "x": "---------------------------------- **APPROACH** First phase of the approach requires training of an unsupervised NMT system similar to <cite>Ogueji & Ahia (2019)</cite> (PidginUNMT).",
  "y": "similarities"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_4",
  "x": "**APPROACH** First phase of the approach requires training of an unsupervised NMT system similar to <cite>Ogueji & Ahia (2019)</cite> (PidginUNMT). Similar to <cite>Ogueji & Ahia (2019)</cite> , we train the cross-lingual model using FastText Bojanowski et al. (2017) on the combined Pidgin-English corpus.",
  "y": "similarities"
 },
 {
  "id": "7c2586a172dc6f061817c3a8b3ebf0_5",
  "x": "First phase of the approach requires training of an unsupervised NMT system similar to <cite>Ogueji & Ahia (2019)</cite> (PidginUNMT). Similar to <cite>Ogueji & Ahia (2019)</cite> , we train the cross-lingual model using FastText Bojanowski et al. (2017) on the combined Pidgin-English corpus. Next, we train an unsupervised NMT similar to Lample et al. (2017) ; Artetxe et al. (2017) ; <cite>Ogueji & Ahia (2019)</cite> between them to obtain model unsup .",
  "y": "similarities"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_0",
  "x": "Unsupervised approaches could be a viable alternative in this regard. The unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling (e.g., Tomokiyo and Hurst (2003) ), graph-based ranking (e.g., Zha (2002) ,<cite> Mihalcea and Tarau (2004)</cite> , Wan et al. (2007) , Wan and Xiao (2008) , Liu et al. (2009a) ), and clustering (e.g., Matsuo and Ishizuka (2004) , Liu et al. (2009b) ). While these methods have been shown to work well on a particular domain of text such as short paper abstracts and news articles, their effectiveness and portability across different domains have remained an unexplored issue.",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_1",
  "x": "Unsupervised approaches could be a viable alternative in this regard. The unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling (e.g., Tomokiyo and Hurst (2003) ), graph-based ranking (e.g., Zha (2002) ,<cite> Mihalcea and Tarau (2004)</cite> , Wan et al. (2007) , Wan and Xiao (2008) , Liu et al. (2009a) ), and clustering (e.g., Matsuo and Ishizuka (2004) , Liu et al. (2009b) ). While these methods have been shown to work well on a particular domain of text such as short paper abstracts and news articles, their effectiveness and portability across different domains have remained an unexplored issue.",
  "y": "motivation"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_2",
  "x": "These algorithms represent the ma-jor directions in this research area, including TfIdf and four recently proposed systems, namely, TextRank <cite>(Mihalcea and Tarau, 2004)</cite> , SingleRank (Wan and Xiao, 2008) , ExpandRank (Wan and Xiao, 2008) , and a clustering-based approach (Liu et al., 2009b) . Since none of these systems (except TextRank) are publicly available, we reimplement all of them and make them freely available for research purposes. 1 To our knowledge, this is the first attempt to compare the performance of state-of-the-art unsupervised keyphrase extraction systems on multiple datasets.",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_3",
  "x": "More specifically, we compare five unsupervised keyphrase extraction algorithms on four corpora with varying domains and statistical characteristics. These algorithms represent the ma-jor directions in this research area, including TfIdf and four recently proposed systems, namely, TextRank <cite>(Mihalcea and Tarau, 2004)</cite> , SingleRank (Wan and Xiao, 2008) , ExpandRank (Wan and Xiao, 2008) , and a clustering-based approach (Liu et al., 2009b) . Since none of these systems (except TextRank) are publicly available, we reimplement all of them and make them freely available for research purposes.",
  "y": "uses"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_4",
  "x": "These algorithms represent the ma-jor directions in this research area, including TfIdf and four recently proposed systems, namely, TextRank <cite>(Mihalcea and Tarau, 2004)</cite> , SingleRank (Wan and Xiao, 2008) , ExpandRank (Wan and Xiao, 2008) , and a clustering-based approach (Liu et al., 2009b) . Since none of these systems (except TextRank) are publicly available, we reimplement all of them and make them freely available for research purposes. 1 To our knowledge, this is the first attempt to compare the performance of state-of-the-art unsupervised keyphrase extraction systems on multiple datasets.",
  "y": "uses"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_5",
  "x": "The Inspec dataset is a collection of 2,000 abstracts from journal papers including the paper title. Each document has two sets of keyphrases assigned by the indexers: the controlled keyphrases, which are keyphrases that appear in the Inspec thesaurus; and the uncontrolled keyphrases, which do not necessarily appear in the thesaurus. This is a relatively popular dataset for automatic keyphrase extraction, as it was first used by Hulth (2003) and later by<cite> Mihalcea and Tarau (2004)</cite> and Liu et al. (2009b) .",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_6",
  "x": "Step 1: Candidate lexical unit selection The first step is to filter out unnecessary word tokens from the input document and generate a list of potential keywords using heuristics. Commonly used heuristics include (1) using a stop word list to remove non-keywords (e.g., Liu et al. (2009b) ) and (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be considered candidate keywords<cite> (Mihalcea and Tarau (2004)</cite> , Liu et al. (2009a) , Wan and Xiao (2008) ). In all of our experiments, we follow Wan and Xiao (2008) and select as candidates words with the following Penn Treebank tags: NN, NNS, NNP, NNPS, and JJ, which are obtained using the Stanford POS tagger (Toutanova and Manning, 2000) .",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_7",
  "x": "Step 3: Keyphrase formation In the final step, the ranked list of candidate words is used to form keyphrases. A candidate phrase, typically a sequence of nouns and adjectives, is selected as a keyphrase if (1) it includes one or more of the top-ranked candidate words<cite> (Mihalcea and Tarau (2004)</cite> , Liu et al. (2009b) ), or (2) the sum of the ranking scores of its constituent words makes it a top scoring phrase (Wan and Xiao, 2008) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_8",
  "x": "In the TextRank algorithm <cite>(Mihalcea and Tarau, 2004)</cite> , a text is represented by a graph. Each vertex corresponds to a word type. A weight, w ij , is assigned to the edge connecting the two vertices, v i and v j , and its value is the number of times the corresponding word types co-occur within a window of W words in the associated text.",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_9",
  "x": "Adjacent keywords are then collapsed and output as a keyphrase. According to<cite> Mihalcea and Tarau (2004)</cite> , TextRank's best score on the Inspec dataset is achieved when only nouns and adjectives are used to create a uniformly weighted graph for the text under consideration, where an edge connects two word types only if they co-occur within a window of two words. Hence, our implementation of TextRank follows this configuration.",
  "y": "background"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_10",
  "x": "**EXPERIMENTAL SETUP** TextRank and SingleRank setup Following<cite> Mihalcea and Tarau (2004)</cite> and Wan and Xiao (2008) , we set the co-occurrence window size for TextRank and SingleRank to 2 and 10, respectively, as these parameter values have yielded the best results for their evaluation datasets. ExpandRank setup Following Wan and Xiao (2008), we find the 5 nearest neighbors for each document from the remaining documents in the same corpus.",
  "y": "uses"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_11",
  "x": "We generate the curves for each system as follows. For Tf-Idf, SingleRank, and ExpandRank, we vary the number of keyphrases, N , predicted by each system. On average, TextRank performs much worse compared to Tf-Idf. This certainly gives more insight into TextRank since it was evaluated on Inspec only for T=33% by<cite> Mihalcea and Tarau (2004)</cite> .",
  "y": "differences"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_12",
  "x": "An examination of Liu et al.'s (2009b) results reveals a subtle caveat in keyphrase extraction evaluations. In Inspec, not all gold-standard keyphrases appear in their associated document, and as a result, none of the five systems we consider in this paper can achieve a recall of 100. While<cite> Mihalcea and Tarau (2004)</cite> and our reimplementations use all of these gold-standard keyphrases in our evaluation, Hulth (2003) and Liu et al. address Table 3 : Original vs. re-implementation scores of TextRank 3 , and are confident that our implementation is correct.",
  "y": "similarities"
 },
 {
  "id": "7d5c01ec5d744747413e42dcbc1a3c_13",
  "x": "In Inspec, not all gold-standard keyphrases appear in their associated document, and as a result, none of the five systems we consider in this paper can achieve a recall of 100. While<cite> Mihalcea and Tarau (2004)</cite> and our reimplementations use all of these gold-standard keyphrases in our evaluation, Hulth (2003) and Liu et al. address Table 3 : Original vs. re-implementation scores of TextRank 3 , and are confident that our implementation is correct. It is also worth mentioning that using our re-implementation of SingleRank, we are able to match the best scores reported by<cite> Mihalcea and Tarau (2004)</cite> on Inspec.",
  "y": "similarities"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_0",
  "x": "Distantly-supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors. Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn * First two authors contributed equally to the paper. and surface form relations such as \"was born in\" extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations <cite>(Riedel et al., 2013</cite>; Fan et al., 2014; . Matrix factorization is at the core of this completion:<cite> Riedel et al. (2013)</cite> convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns.",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_1",
  "x": "Matrix factorization is at the core of this completion:<cite> Riedel et al. (2013)</cite> convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns. Factorization of this matrix results in low-dimensional factors for entity-pairs and relations, which are able to effectively combine multiple evidence for each entity pair to predict unseen relations. An important shortcoming of this matrix factorization model for universal schema is that no information is shared between the rows that contain the same entity.",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_2",
  "x": "Distantly-supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors. Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn * First two authors contributed equally to the paper. and surface form relations such as \"was born in\" extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations <cite>(Riedel et al., 2013</cite>; Fan et al., 2014; . Matrix factorization is at the core of this completion:<cite> Riedel et al. (2013)</cite> convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns.",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_3",
  "x": "---------------------------------- **UNIVERSAL SCHEMA** A universal schema is defined as the union of all OpenIE-like surface form patterns found in text and fixed canonical relations that exist in a knowledge base<cite> (Riedel et al., 2013)</cite> .",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_4",
  "x": "In matrix factorization for universal schema,<cite> Riedel et al. (2013)</cite> construct a sparse binary matrix of size |P| \u00d7 |R| whose rows are indexed by entity-pairs (a, b) \u2208 P and columns by surface form and Freebase relations s \u2208 R. Subsequently, generalized PCA (Collins et al., 2001 ) is used to find a rank-k factorization, i.e., with relation factors r \u2208 R |R|\u00d7k and entity-pair factors p \u2208 R |P|\u00d7k , the probability of a relation s and two entities a and b is: where \u03c3 is the sigmoid function. Using this factorization, similar entity-pairs and relations are embedded close to each other in a k-dimensional vector space.",
  "y": "background"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_5",
  "x": "**MODEL E** Furthermore, we isolate the entity factorization in<cite> Riedel et al. (2013)</cite> by viewing it as tensor factorization. In this model, each relation is assigned an embedding for each of its two arguments, i.e.,",
  "y": "extends"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_6",
  "x": "Furthermore, we isolate the entity factorization in<cite> Riedel et al. (2013)</cite> by viewing it as tensor factorization. In this model, each relation is assigned an embedding for each of its two arguments, i.e., Although not explored in isolation by<cite> Riedel et al. (2013)</cite> , model E can be used on its own to predict relations between entities, even if they have not been observed to be in a relation.",
  "y": "extends"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_7",
  "x": "Although matrix factorization performs well for universal schema<cite> (Riedel et al., 2013)</cite> , it is not robust to sparse data and does not capture latent entity types that can be crucial for accurate relation extraction. On the other hand, although tensor factorization models are able to compactly represent entity types using unary embeddings, they are unable to adequately represent the pair-specific information that is necessary for modeling relations. It is worth noting that tensor factorization for universal schema has been proposed by , who also observed that tensor factorization by itself performs poorly (even with additional type constraints), and the predictions need to be combined with matrix factorization to be accurate.",
  "y": "motivation"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_8",
  "x": "As the direct combination of a pairwise model (Eq. 1) with an entity model (Eq. 5), we consider the FE model from<cite> Riedel et al. (2013)</cite> , i.e., the additive combination of the two: P (s(a, b)) = \u03c3(r s \u00b7 e ab + r s,1 \u00b7 e a + r s,2 \u00b7 e b ) (6) Both the matrix factorization model F and entity model E can de defined as special cases of this model, by setting r s,1/2 or r s to zero, respectively. ---------------------------------- **RECTIFIER MODEL (RFE)**",
  "y": "uses"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_9",
  "x": "As the direct combination of a pairwise model (Eq. 1) with an entity model (Eq. 5), we consider the FE model from<cite> Riedel et al. (2013)</cite> , i.e., the additive combination of the two: P (s(a, b)) = \u03c3(r s \u00b7 e ab + r s,1 \u00b7 e a + r s,2 \u00b7 e b ) (6) Both the matrix factorization model F and entity model E can de defined as special cases of this model, by setting r s,1/2 or r s to zero, respectively. A problem with combining the two models additively, as in FE, is that one model can easily override the other.",
  "y": "motivation"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_10",
  "x": "As the direct combination of a pairwise model (Eq. 1) with an entity model (Eq. 5), we consider the FE model from<cite> Riedel et al. (2013)</cite> , i.e., the additive combination of the two: P (s(a, b)) = \u03c3(r s \u00b7 e ab + r s,1 \u00b7 e a + r s,2 \u00b7 e b ) (6) Both the matrix factorization model F and entity model E can de defined as special cases of this model, by setting r s,1/2 or r s to zero, respectively. A problem with combining the two models additively, as in FE, is that one model can easily override the other. To alleviate this shortcoming, we experimented with rectifier units (Nair and Hinton, 2010) so that a score of model F or model E first needs to reach a certain threshold to influence the overall prediction for a triplet.",
  "y": "extends"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_11",
  "x": "As by<cite> Riedel et al. (2013)</cite> , we use a Bayesian personalized ranking objective (Rendle et al., 2009 ) to estimate parameters, i.e., for each observed training fact, we sample an unobserved fact for the same relation, and maximize their relative ranking using AdaGrad. For all models we use k = 100 as dimension of latent representations, an initial learning rate of 0.1, and 2 -regularization of all parameters with a weight of 0.01. For CANDECOMP/PARAFAC and RESCAL we use the open-source scikit-tensor 2 package with default hyper-parameters.",
  "y": "uses"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_12",
  "x": "With the promising results shown on synthetic data, we now turn to evaluation on real-world information extraction. In particular, we evaluate the models on universal schema for distantly-supervised relation extraction. Following the experiment setup of<cite> Riedel et al. (2013)</cite> , we instantiate the universal schema matrix over entity pairs and text/Freebase relations for New York Times data, and compare the performance using average precision of the presented models.",
  "y": "uses"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_13",
  "x": "In particular, we evaluate the models on universal schema for distantly-supervised relation extraction. Following the experiment setup of<cite> Riedel et al. (2013)</cite> , we instantiate the universal schema matrix over entity pairs and text/Freebase relations for New York Times data, and compare the performance using average precision of the presented models. Table 1 summarizes the performance of our models, as compared to existing approaches (see<cite> Riedel et al. (2013)</cite> for an overview).",
  "y": "uses"
 },
 {
  "id": "7d80c3cc15453ddeaea72dcb9c04f9_14",
  "x": [
   "In particular, TR-R13 takes the output predictions of matrix factorization, and combines it with an entity-type aware RESCAL model . 3 Tensor factorization approaches perform poorly on this data. We present results for Model E, but other formulations such as PARAFAC, TransE, RESCAL, and Tucker2 achieved even lower accuracy; this is consistent with the results in ."
  ],
  "y": "differences"
 },
 {
  "id": "7ddd5b18d774575ae7acb97ae9eb33_0",
  "x": "It is a popular tool in semantic parsing, and treebank creation efforts have been made for Turkish (\u00c7 ak\u0131c\u0131, 2005) , German (Hockenmaier, 2006) , English <cite>(Hockenmaier and Steedman, 2007)</cite> , Italian (Bos et al., 2009) , Chinese (Tse and Curran, 2010) , Arabic (Boxwell and Brew, 2010) , Japanese (Uematsu et al., 2013) , and Hindi (Ambati et al., 2018) . However, all of these treebanks were not directly annotated according to the CCG formalism, but automatically converted from phrase structure or dependency treebanks, which is an error-prone process. Direct annotation in CCG has so far mostly been limited to small datasets for seeding or testing semantic parsers (e.g., Artzi et al., 2015) , and no graphical annotation interface is available to support such efforts, making the annotation process difficult to scale.",
  "y": "background"
 },
 {
  "id": "7ddd5b18d774575ae7acb97ae9eb33_1",
  "x": "It is a popular tool in semantic parsing, and treebank creation efforts have been made for Turkish (\u00c7 ak\u0131c\u0131, 2005) , German (Hockenmaier, 2006) , English <cite>(Hockenmaier and Steedman, 2007)</cite> , Italian (Bos et al., 2009) , Chinese (Tse and Curran, 2010) , Arabic (Boxwell and Brew, 2010) , Japanese (Uematsu et al., 2013) , and Hindi (Ambati et al., 2018) . However, all of these treebanks were not directly annotated according to the CCG formalism, but automatically converted from phrase structure or dependency treebanks, which is an error-prone process. Direct annotation in CCG has so far mostly been limited to small datasets for seeding or testing semantic parsers (e.g., Artzi et al., 2015) , and no graphical annotation interface is available to support such efforts, making the annotation process difficult to scale. The only exceptions we are aware of are the Groningen Meaning Bank and the Parallel Meaning Bank (Abzianidze et al., 2017) , two annotation efforts which use a graphical user interface for annotating sentences with CCG derivations and other annotation layers, and which have produced CCG treebanks for English, German, Italian, and Dutch. However, these efforts are focused on semantics and have not released explicit guidelines for syntactic annotation. Their annotation tool is limited in that annotators only have control over lexical categories, not larger constituents. Even though CCG is a lexicalized formalism, where most decisions can be made on the lexical level, there is no full control over attachment phenomena in the lexicon. Moreover, these annotation tools are not open-source and cannot easily be deployed to support other annotation efforts. In this paper, we present an open-source, lightweight, easy-to-use graphical annotation tool that employs a statistical parser to create initial CCG derivations for sentences, and allows annotators to correct these annotations via lexical category constraints and span constraints.",
  "y": "motivation"
 },
 {
  "id": "7ddd5b18d774575ae7acb97ae9eb33_2",
  "x": "**A QUADRILINGUAL PILOT CCG TREEBANK** To test the viability of creating multilingual CCG treebanks by direct annotation, we conducted an annotation experiment on 110 short sentences from the Tatoeba corpus (Tatoeba, 2019) , each in four translations (English, German, Italian, and Dutch). The main annotation guideline was to copy the annotation style of CCGrebank (Honnibal et al., 2010), a CCG treebank adapted from CCGbank <cite>(Hockenmaier and Steedman, 2007)</cite> , which is in turn based on the Penn Treebank (Marcus et al., 1993) .",
  "y": "uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_0",
  "x": "We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_1",
  "x": "We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> .",
  "y": "similarities"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_2",
  "x": "<cite>Agirre et al. (2008)</cite> trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004 ) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003) , where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser.",
  "y": "background"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_3",
  "x": "We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_4",
  "x": "We extend the tests made in <cite>Agirre et al. (2008)</cite> , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006) . We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_5",
  "x": "We will evaluate the parser on both the full PTB (Marcus et al. 1993 ) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in <cite>Agirre et al. (2008)</cite> . <cite>Agirre et al. (2008)</cite> trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003) , where related words, like scissors and knife cannot be generalized.",
  "y": "background"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_6",
  "x": "---------------------------------- **DATASET** We used two different datasets: the full PTB and the Semcor/PTB intersection<cite> (Agirre et al. 2008</cite> ).",
  "y": "uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_7",
  "x": "The Semcor/PTB intersection contains both gold-standard sense and parse tree annotations, and allows to set an upper bound of the relative impact of a given semantic representation on parsing. We use the same train-test split of <cite>Agirre et al. (2008)</cite> , with a total of 8,669 sentences containing 151,928 words partitioned into 3 sets: 80% training, 10% development and 10% test data. This dataset is available on request to the research community.",
  "y": "similarities uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_8",
  "x": "---------------------------------- **SEMANTIC REPRESENTATION AND DISAMBIGUATION METHODS** We will experiment with the range of semantic representations used in <cite>Agirre et al. (2008)</cite> , all of which are based on WordNet 2.1.",
  "y": "similarities uses"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_9",
  "x": "<cite>Agirre et al. (2008)</cite> used a simple method of substituting wordforms with semantic information, which only allowed using a single semantic feature. MaltParser allows the combination of several semantic features together with other features such as wordform, lemma or part of speech. Although tables 1 and 2 only show the best combination for each type of semantic information, this can be appreciated on GOLD and 1ST in Table 1 .",
  "y": "background"
 },
 {
  "id": "805935a672f5d706bd878a73fa8171_10",
  "x": "Due to space reasons, we only have showed the best combination, but we can say that in general combining features gives significant increases over using a single semantic feature. \u2022 The present work presents a statistically significant improvement for the full treebank using WordNet-based semantic information for the first time. Our results extend those of <cite>Agirre et al. (2008)</cite> , which showed improvements on a subset of the PTB.",
  "y": "extends differences"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_0",
  "x": "This latter type of model caused a new revolution in NLP and led to popular language models like GPT-2 (Radford et al., 2018 (Radford et al., , 2019 and ELMo (Peters et al., 2018) . BERT <cite>(Devlin et al., 2019)</cite> improved over previous transformer models and recurrent networks by allowing the system to learn from input text in a bidirectional way, rather than only from left-to-right or the other way around. This model was later reimplemented, critically evaluated and improved in the RoBERTa model .",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_1",
  "x": "These are usually trained on a large quantity of text in different languages. For example, Multilingual-BERT is trained on a collection of corpora in 104 different languages <cite>(Devlin et al., 2019)</cite> , and generalizes language components well across languages (Pires et al., 2019) . However, models trained on data from one specific language usually improve the performance of multilingual models for this particular language (Martin et al., 2019; de Vries et al., 2019) .",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_2",
  "x": "This transformer model architecture resulted in a new paradigm in NLP with the migration from sequence-to-sequence recurrent neural networks to transformer-based models by removing the recurrent component and only keeping attention. This cornerstone was used for BERT, a transformer model that obtained stateof-the-art results for eleven natural language processing tasks, such as question answering and natural language inference <cite>(Devlin et al., 2019)</cite> . BERT is pre-trained with large corpora of text using two unsupervised tasks.",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_3",
  "x": "This architecture has been shown to be a general language model that could be fine-tuned with little data in a relatively efficient way for a very distinct range of tasks and still outperform previous architectures <cite>(Devlin et al., 2019)</cite> . Transformer models are also capable of generating contextualized word embeddings. These contextualized embeddings were presented by Peters et al. (2018) and addressed the well known issue with a word's meaning being defined by its context (e.g. \"a stick\" versus \"let's stick to\").",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_4",
  "x": "That<cite> Devlin et al. (2019)</cite> trained a better model when using NSP than without NSP is likely due to the model learning long-range dependencies in text from its inputs, which are longer than just the single sentence on itself. As such, the RoBERTa model uses only the MLM task, and uses multiple full sentences in every input. Other research improved the NSP task by instead making the model predict the correct order of two sentences, where the model thus has to predict whether the sentences occur in the given order in the corpus, or occur in flipped order (Lan et al., 2019) .",
  "y": "background"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_5",
  "x": "RobBERT shares its architecture with RoBERTa's base model, which itself is a replication and improvement over BERT . The architecture of our language model is thus equal to the original BERT model with 12 self-attention layers with 12 heads <cite>(Devlin et al., 2019)</cite> . One difference with the original BERT is due to the different pre-training task specified by RoBERTa, using only the MLM task and not the NSP task.",
  "y": "similarities"
 },
 {
  "id": "817576dbe36f79ac3e0031211f400d_6",
  "x": "ZeroR (majority class) 66.70 mBERT <cite>(Devlin et al., 2019)</cite> 90.21 BERTje (de Vries et al., 2019) 94.94 RobBERT (ours) 98.03 RobBERT outperforms previous models as well as other BERT models both with as well as without fine-tuning (see Table 1 and Table 2 ). It is also able to reach similar performance using less data.",
  "y": "background"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_0",
  "x": "To deal with phonological variability alternate pronunciations are included in the lexicon, and optional phonological rules are applied during training and recognition. A trigram LM is used in a second acoustic decoding pass which makes use of the word graph generated using the bigram LM [6] . Experimental results are reported on the ARPA Wall Street Journal (WSJ) <cite>[19]</cite> and BREF [14] corpora, using for both corpora over 37k utterances for acoustic training and more than 37 million words of newspaper text for language model training. It is shown that for both corpora increasing the amount of training utterances by an order of magnitude reduces the word error by about 30%. The use of a trigram LM in a second pass also gives an error reduction of 20% to 30%.",
  "y": "background"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_1",
  "x": "A backoff mechanism [10] is used to smooth the estimates of the probabilities of rare n-grams by relying on a lower order n-gram when there is insufficient training data, and to provide a means of modeling unobserved n-grams. Another advantage of the backoff mechanism is that LM size can be arbitrarily reduced by relying more on the backoff, by increasing the minimum number of required n-gram observations needed to include the n-gram. This property can be used in the first bigram decod1While we have built n-gram-backoff LMs directly from the 37M-word standardized WSJ training text material, in these experiments all results are reported using the 5k or 20k, bigram and tfigram backoff LMs provided by Lincoln Labs<cite> [ 19]</cite> as required by ARPA so as to be compatible with the other sites participating in the tests.",
  "y": "uses"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_2",
  "x": "In order to be able to constnact LMs for BREF, it was necessary to normalize the text material of Le Monde newpaper, which entailed a pre-treatment rather different from that used to normalize the WSJ texts <cite>[19]</cite> . The main differences are in the treatment of compound words, abbreviations, and case. In BREF the distinction between the cases is kept if it designates a distinctive graphemic feature, but not when the upper case is simply due to the fact that the word occurs at the beginning of the sentence.",
  "y": "uses differences"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_3",
  "x": "---------------------------------- **WSJ:** The ARPA WSJ corpus <cite>[19]</cite> was designed to provide general-purpose speech data with large vocabularies.",
  "y": "background"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_4",
  "x": "The 20k open test is also referred to as a 64k test since all of the words in these sentences occur in the 63,495 most frequent words in the normalized WSJ text material<cite> [ 19]</cite> . Two sets of standard training material have been used for these experiments: The standard WSJ0 SI84 training data which include 7240 sentences from 84 speakers, and the standard set of 37,518 WSJ0/WSJ1 SI284 sentences from 284 speakers. Only the primary microphone data were used for training.",
  "y": "uses"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_5",
  "x": "The WSJ system was evaluated in the Nov92 ARPA evaluation test [17] for the 5k-closed vocabulary and in the Nov93 ARPA evaluation test [18] for the 5k and 64k hubs. Except when explicitly stated otherwise, all of the results reported for WSJ use the standard language models <cite>[19]</cite> . Using a set of 1084 CD models trained with the WSJ0 si84 training data, the word error is 6.6% on the Nov92 5k test data and 9.4% on the Nov93 test data.",
  "y": "uses"
 },
 {
  "id": "84ae490de92cb9d064993be751b3e0_6",
  "x": "The recognizer has been evaluated on 5k and 20k test data for the English and French languages using similar style corpora. For WSJ, paragraphs were selected ensuring not more than one word was out of the 5.6k most frequent words<cite> [ 19]</cite> , and these additional words were then included as part of the vocabulary.",
  "y": "uses"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_0",
  "x": "Antol et al. [9] propose an element-wise multiplication between image and question embeddings to generate spatial attention map. Fukui et al.<cite> [6]</cite> propose multimodal compact bilinear pooling (MCB) to efficiently implement an outer product operator that combines visual and textual representations. Yu et al. [26] extend this pooling scheme by introducing a multi-modal factorized bilinear pooling approach (MFB) that improves the representational capacity of the bilinear operator.",
  "y": "background"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_1",
  "x": "The common approach is to use a one-way attention scheme, where the embedding of the question is used to generate a set of attention coefficients over a set of predefined image regions. These coefficients are then used to weight the embedding of the image regions to obtain a suitable descriptor [19, 21,<cite> 6,</cite> 25, 26] . More elaborated forms of attention has also been proposed.",
  "y": "background"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_2",
  "x": "We mostly build upon the MCB model in<cite> [6]</cite> , which exemplifies current state-of-the-art techniques for this problem. Our main innovation to this model is the addition of an Attention Supervision Module that incorporates visual grounding as an auxiliary task. Next we describe the main modules behind this model.",
  "y": "extends differences"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_3",
  "x": "Afterwards, the fusion module models the joint relationship J attn \u2208 R O\u00d7H\u00d7W between questions and images, mapping them to a common space of dimension O. In the simplest case, one can implement the fusion module using either concatenation or Hadamard product [1] , but more effective pooling schemes can be applied <cite>[6,</cite> 11, 25, 26] . The design choice of the fusion module remains an on-going research topic. In general, it should both effectively capture the latent relationship between multi-modal features meanwhile be easy to optimize.",
  "y": "background"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_4",
  "x": "Afterwards, it computes the logits over a set of predefined candidate answers. Following previous work<cite> [6]</cite> , we use as candidate outputs the top 3000 most frequent answers in the VQA dataset. At the end of this process, we obtain the highest scoring answer\u00c2.",
  "y": "similarities uses"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_5",
  "x": "It is mostly based on the model presented in<cite> [6]</cite> . Main innovation is the Attention Supervision Module that incorporates visual grounding as an auxiliary task. This module is trained through the use of a set of image attention labels that are automatically mined from the Visual Genome dataset.",
  "y": "extends"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_6",
  "x": "We build the attention supervision on top of the opensourced implementation of MCB<cite> [6]</cite> and MFB [25] . Similar to them, We extract the image feature from res5c layer of Resnet-152, resulting in 14 \u00d7 14 spatial grid (H = 14, W = 14, C = 2048). We construct our ground-truth visual grounding labels to be G v = 2 glimpse maps per QA pair, where the first map is object-level grounding and the second map is region-level grounding, as discussed in Section 4. Let (x i min , y i min , x i max , y i max ) be the coordinate of i th selected object bounding box in the grounding labels, then the mined object-level attention maps C 0 gt are:",
  "y": "extends differences"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_7",
  "x": "Visual Genome: The Visual Genome (VG) dataset [12] contains 108077 images, with an average of 17 QA pairs per image. We follow the processing scheme from<cite> [6]</cite> , where non-informative words in the questions and answers such as \"a\" and \"is\" are removed. Afterwards, (I, Q, A) triplets with answers to be single keyword and overlapped with VQA-2.0 dataset are included in our training set.",
  "y": "similarities uses"
 },
 {
  "id": "866ae880aa0de1e60d306eac2e66fc_8",
  "x": "**RANK CORRELATION** Accuracy/% VQA-HAT VQA-X VQA-2.0 Human [5] 0.623 -80.62 PJ-X [17] 0.396 0.342 -MCB<cite> [6]</cite> 0 authors also collect 1374 \u00d7 3 = 4122 HAT maps for VQA-1.0 validation sets, where each of the 1374 (I, Q, A) were labeled by three different annotators, so one can compare the level of agreement among labels. We use VQA-HAT to evaluate visual grounding performance, by comparing the rank-correlation between human attention and model attention, as in [5, 17] .",
  "y": "similarities"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_0",
  "x": "Recent work has shown benefits of combining conventional lexical information into neural cross-lingual part-ofspeech (PoS) tagging <cite>(Plank and Agi\u0107, 2018)</cite> . However, little is known on how complementary such additional information is, and to what extent improvements depend on the coverage and quality of these external resources. The contribution of this paper is in the analysis of the contributions of models' components (tagger transfer through annotation projection vs. the contribution of encoding lexical and morphosyntactic resources).",
  "y": "background"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_1",
  "x": "However, particularly when working with low-resource languages, small amounts of symbolic lexical resources such as user-generated lexicons are often available even when gold-standard corpora are not. Recent work has shown benefits of combining conventional lexical information into neural cross-lingual part-ofspeech (PoS) tagging <cite>(Plank and Agi\u0107, 2018)</cite> . However, little is known on how complementary such additional information is, and to what extent improvements depend on the coverage and quality of these external resources.",
  "y": "motivation"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_2",
  "x": "1 The sizes of the dictionaries vary considerably, from a few thousand entries (e.g., for Hindi and Bulgarian) to 2M entries (Finnish Uni-Morph). We study the impact of smaller dictionary sizes in Section 4.1. The tagger we analyze in this paper is an extension of the base tagger, called distant supervision from disparate sources (DSDS) tagger <cite>(Plank and Agi\u0107, 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_3",
  "x": "The tagger we analyze in this paper is an extension of the base tagger, called distant supervision from disparate sources (DSDS) tagger <cite>(Plank and Agi\u0107, 2018)</cite> . It is trained on projected data and further differs from the base tagger by the integration of lexicon information. In particular, given a lexicon src, DSDS uses e src to embed the lexicon into an l-dimensional space, where e src is the concatenation of all embedded m properties of length l (empirically set, see Section 2.2), and a zero vector for words not in the lexicon.",
  "y": "extends"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_4",
  "x": "Data We use the 12 Universal PoS tags (Petrov et al., 2012) . The set of languages is motivated by accessibility to embeddings and dictionaries. We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by<cite> Plank and Agi\u0107 (2018)</cite> showing that DSDS provides a viable alternative.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_5",
  "x": "The set of languages is motivated by accessibility to embeddings and dictionaries. We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by<cite> Plank and Agi\u0107 (2018)</cite> showing that DSDS provides a viable alternative. Annotation projection To build the taggers for new languages, we resort to annotation projection following<cite> Plank and Agi\u0107 (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_6",
  "x": "Annotation projection To build the taggers for new languages, we resort to annotation projection following<cite> Plank and Agi\u0107 (2018)</cite> . In particular, they employ the approach by Agi\u0107 et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences. The wide-coverage Watchtower corpus (WTC) by Agi\u0107 et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following<cite> Plank and Agi\u0107 (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_7",
  "x": "Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (Li et al., 2012) and retrofitting initialization. Hyperparameters We use the same setup as<cite> Plank and Agi\u0107 (2018)</cite> , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions. This ensures that our probing tasks always get the same input dimensionality: 64 (2x32) dimensions for cw, which is the same dimension as the off-theshelf word embeddings.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_8",
  "x": "We use the off-the-shelf Polyglot word embeddings (Al-Rfou et al., 2013) . Word embedding initialization provides a consistent and considerable boost in this cross-lingual setup, up to 10% absolute improvements across 21 languages when only 500 projected training instances are available <cite>(Plank and Agi\u0107, 2018)</cite> . Note that we em- pirically find it to be best to not update the word embeddings in this noisy training setup, as that results in better performance, see Section 4.4.",
  "y": "differences"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_9",
  "x": "Word embedding initialization provides a consistent and considerable boost in this cross-lingual setup, up to 10% absolute improvements across 21 languages when only 500 projected training instances are available <cite>(Plank and Agi\u0107, 2018)</cite> . Note that we em- pirically find it to be best to not update the word embeddings in this noisy training setup, as that results in better performance, see Section 4.4. ----------------------------------",
  "y": "background"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_10",
  "x": "**INCLUSION OF LEXICAL INFORMATION** Combining the best of two worlds results in the overall best tagging accuracy, confirming<cite> Plank and Agi\u0107 (2018)</cite> : Embedding lexical information into a neural tagger improves tagging accuracy from 83.4 to 84.1 (means over 21 languages). On 15 out of 21 languages, DSDS is the best performing model.",
  "y": "similarities"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_11",
  "x": "---------------------------------- **LEARNING CURVES** The lexicons we use so far are of different sizes (shown in Table 1 of<cite> Plank and Agi\u0107 (2018)</cite> ), spanning from 1,000 entries to considerable dictionaries of several hundred thousands entries.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_12",
  "x": "They rely on end-to-end training without resorting to additional linguistic resources. Our study contributes to the increasing literature to show the utility of linguistic resources for deep learning models by providing a deep analysis of a recently proposed model <cite>(Plank and Agi\u0107, 2018)</cite> . Most prior work in this direction can be found on machine translation (Sennrich and Haddow, 2016; Chen et al., 2017; Li et al., 2017; Passban et al., 2018) , work on named entity recognition (Wu et al., 2018) and PoS tagging (Sagot and Mart\u00ednez Alonso, 2017) who use lexicons, but as n-hot features and without examining the crosslingual aspect.",
  "y": "uses"
 },
 {
  "id": "8853d810b364ae47a2da71c2502b3e_13",
  "x": "We analyze DSDS, a recently-proposed lowresource tagger that symbiotically leverages neural representations and symbolic linguistic knowledge by integrating them in a soft manner. We replicated the results of<cite> Plank and Agi\u0107 (2018)</cite> , showing that the more implicit use of embedding user-generated dictionaries turns out to be more beneficial than approaches that rely more explicitly on symbolic knowledge, such a type constraints or retrofitting. By analyzing the reliance of DSDS on the linguistic knowledge, we found that the composition of the lexicon is more important than its size.",
  "y": "extends uses"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_0",
  "x": "**ABSTRACT** In this paper, we present a model for improved discriminative semantic parsing. The model addresses an important limitation associated with our previous stateof-the-art discriminative semantic parsing model -the <cite>relaxed hybrid tree</cite> model by introducing our constrained semantic forests.",
  "y": "motivation differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_1",
  "x": "One state-of-the-art model for semantic parsing is our recently introduced <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> , which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference. <cite>The model</cite> allows natural language phrases to be recursively mapped to semantic units, where certain long-distance dependencies can be captured. <cite>It</cite> relies on representations called <cite>relaxed hybrid trees</cite> that can jointly represent both the sentences and semantics. <cite>The model</cite> is essentially discriminative, and allows rich features to be incorporated.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_2",
  "x": "One state-of-the-art model for semantic parsing is our recently introduced <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> , which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference. Unfortunately, the <cite>relaxed hybrid tree</cite> model has an important limitation: <cite>it</cite> essentially does not allow certain sentence-semantics pairs to be jointly encoded using the proposed <cite>relaxed hybrid tree</cite> representations. Thus, <cite>the model</cite> is unable to identify joint representations for certain sentence-semantics pairs during the training process, and is unable to produce desired outputs for certain inputs during the evaluation process. In this work, we propose a solution addressing the above limitation, which makes our model more robust.",
  "y": "motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_3",
  "x": "Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine translation problem, the UBL system (Kwiatkowski et al., 2010) which performs CCG-based semantic parsing using a log-linear model, as well as the <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> which extends the generative hybrid tree model. This extension results in a discriminative model that incorporates rich features and allows long-distance dependencies to be captured. The <cite>relaxed hybrid tree</cite> model has achieved the state-of-the-art results on standard benchmark datasets across different languages.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_4",
  "x": "Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine translation problem, the UBL system (Kwiatkowski et al., 2010) which performs CCG-based semantic parsing using a log-linear model, as well as the <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> which extends the generative hybrid tree model. This extension results in a discriminative model that incorporates rich features and allows long-distance dependencies to be captured. The <cite>relaxed hybrid tree</cite> model has achieved the state-of-the-art results on standard benchmark datasets across different languages.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_6",
  "x": "We briefly discuss our previously proposed <cite>relaxed hybrid tree</cite> model <cite>(Lu, 2014)</cite> in this section. <cite>The model</cite> is a discriminative semantic parsing model which extends the generative hybrid tree model (Lu et al., 2008) . Let us use m to denote a complete semantic representation, n to denote a complete natural language sentence, and h to denote a complete latent structure that jointly represents both m and n. <cite>The model</cite> defines the conditional probability for observing a (m, h) pair for a given natural language sentence n using a log-linear approach:",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_7",
  "x": "Typically, to limit the space of latent structures, certain assumptions have to be made to h. In our work, we assume that h must be from a space consisting of <cite>relaxed hybrid tree</cite> structures <cite>(Lu, 2014)</cite> . The <cite>relaxed hybrid trees</cite> are analogous to the hybrid trees, which was earlier introduced as a generative framework. One major distinction between these two types of representations is that the <cite>relaxed hybrid tree</cite> representations are able to capture unbounded long-distance dependencies in a principled way.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_8",
  "x": "One major distinction between these two types of representations is that the <cite>relaxed hybrid tree</cite> representations are able to capture unbounded long-distance dependencies in a principled way. Such dependencies were unable to be captured by hybrid tree representations largely due to their generative settings. Figure 1 gives an example of a hybrid tree and a <cite>relaxed hybrid tree</cite> representation encoding the sentence w 1 w 2 w 3 w 4 w 5 w 6 w 7 w 8 w 9 w 10 and the se-",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_9",
  "x": "In the <cite>relaxed hybrid tree</cite>, however, each word is not only directly associated with exactly one semantic unit m, but also indirectly associated with all other semantic units that are predecessors of m. For example, the word w 3 now is directly associated with m b , but is also indirectly associated with m a . Both the hybrid tree and <cite>relaxed hybrid tree</cite> models define patterns at each level of their latent structure which specify how the words and child semantic units are organized at each level.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_10",
  "x": "**LIMITATIONS** One important difference between the hybrid tree representations and the <cite>relaxed hybrid tree</cite> representations is the exclusion of the pattern X in the latter. This ensured <cite>relaxed hybrid trees</cite> with an infinite number of nodes were not considered <cite>(Lu, 2014)</cite> when computing the denominator term of Equation 1.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_11",
  "x": "This ensured <cite>relaxed hybrid trees</cite> with an infinite number of nodes were not considered <cite>(Lu, 2014)</cite> when computing the denominator term of Equation 1. In <cite>relaxed hybrid tree</cite>, H(n, m) was implemented as a packed forest representation for exponentially many possible <cite>relaxed hybrid trees</cite> where pattern X was excluded. By allowing pattern X, we allow certain semantic units with no natural language word counter- part to exist in the joint <cite>relaxed hybrid tree</cite> representation.",
  "y": "differences motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_13",
  "x": "When pattern X is allowed, both m a and m b are not directly associated with any natural language word, so we are able to further insert arbitrarily many (compatible) semantic units between the two units m a and m b while the resulting <cite>relaxed hybrid tree</cite> remains valid. Therefore we can construct a <cite>relaxed hybrid tree</cite> representation that contains the given natural language sentence w 1 w 2 with an infinite number of nodes. This issue essentially prevents us from computing the denominator term of Equation 1 since it involves an infinite number of possible m and h .",
  "y": "motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_14",
  "x": "This issue essentially prevents us from computing the denominator term of Equation 1 since it involves an infinite number of possible m and h . To eliminate <cite>relaxed hybrid trees</cite> consisting of an infinite number of nodes, pattern X is disallowed in the <cite>relaxed hybrid trees</cite> model <cite>(Lu, 2014)</cite> . However, disallowing pattern X has led to other issues.",
  "y": "motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_17",
  "x": "**CONSTRAINED SEMANTIC FORESTS** To address this limitation, we allow pattern X to be included when building our new discriminative semantic parsing model. However, as mentioned above, doing so will lead to latent structures (<cite>relaxed hybrid tree</cite> representations) of infinite heights.",
  "y": "motivation"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_18",
  "x": "This is essentially the same as those considered by the hybrid tree model. Our new objective function is as follows: where M refers to the set of all possible semantic trees whose heights are less than or equal to c, and H (n, m ) refers to the set of possible <cite>relaxed hybrid tree</cite> representations where the pattern X is allowed.",
  "y": "motivation uses"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_19",
  "x": "Such a constrained semantic forest is a packed forest representation of exponentially many possible unique semantic trees, where we set the height of the forest to c. By contrast, it was not possible in our previous <cite>relaxed hybrid tree</cite> model to introduce such a compact representation over all possible semantic trees. In our previous model's implementation, we directly constructed for each sentence n a different compact representation over all possible <cite>relaxed hybrid trees</cite> containing n. Setting the maximum height to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. We then constructed the (exponentially many) <cite>relaxed hybrid tree</cite> representations based on the constrained semantic forest M and each input sentence n. We used a single packed forest representation to represent all such <cite>relaxed hybrid tree</cite> representations.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_20",
  "x": "Such a constrained semantic forest is a packed forest representation of exponentially many possible unique semantic trees, where we set the height of the forest to c. By contrast, it was not possible in our previous <cite>relaxed hybrid tree</cite> model to introduce such a compact representation over all possible semantic trees. In our previous model's implementation, we directly constructed for each sentence n a different compact representation over all possible <cite>relaxed hybrid trees</cite> containing n. Setting the maximum height to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. We then constructed the (exponentially many) <cite>relaxed hybrid tree</cite> representations based on the constrained semantic forest M and each input sentence n. We used a single packed forest representation to represent all such <cite>relaxed hybrid tree</cite> representations.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_21",
  "x": "Setting the maximum height to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. We then constructed the (exponentially many) <cite>relaxed hybrid tree</cite> representations based on the constrained semantic forest M and each input sentence n. We used a single packed forest representation to represent all such <cite>relaxed hybrid tree</cite> representations. This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in <cite>(Lu, 2014)</cite> . Optimization of the model parameters were done by using L-BFGS (Liu and Nocedal, 1989) , where the gradients were computed efficiently using an analogous dynamic programming algorithm.",
  "y": "similarities"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_22",
  "x": "To make our system directly comparable to previous works, we used the same train/test split used in those works (Jones et al., 2012; <cite>Lu, 2014</cite>) for evaluation. We also followed the standard approach for evaluating the correctness of an output semantic representation from our system. Specifically, we used a standard script to construct Prolog queries based on the outputs, and used the queries to retrieve answers from the GeoQuery database.",
  "y": "uses"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_23",
  "x": "We also followed the standard approach for evaluating the correctness of an output semantic representation from our system. Specifically, we used a standard script to construct Prolog queries based on the outputs, and used the queries to retrieve answers from the GeoQuery database. Following previous works, we regarded an output semantic representation as correct if and only if it returned the same answers as the gold standard (Jones et al., 2012; <cite>Lu, 2014</cite>) .",
  "y": "uses"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_24",
  "x": "UBL-S (Kwiatkowski et al., 2010 ) is a CCG-based semantic parsing system. TREETRANS (Jones et al., 2012) is the system based on tree transducers. <cite>RHT</cite> <cite>(Lu, 2014)</cite> is the discriminative semantic parsing system based on <cite>relaxed hybrid trees</cite>.",
  "y": "background"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_25",
  "x": "<cite>RHT</cite> <cite>(Lu, 2014)</cite> is the discriminative semantic parsing system based on <cite>relaxed hybrid trees</cite>. In practice, we set c (the maximum height of a semantic representation) to 20 in our experi- ments, which we determined based on the heights of the semantic trees that appear in the training data. Results showed that our system consistently yielded higher results than all the previous systems, including our state-of-the-art <cite>relaxed hybrid tree</cite> system (<cite>the full model</cite>, when all the features are used), in terms of both accuracy score and F 1 -measure.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_26",
  "x": "Results showed that our system consistently yielded higher results than all the previous systems, including our state-of-the-art <cite>relaxed hybrid tree</cite> system (<cite>the full model</cite>, when all the features are used), in terms of both accuracy score and F 1 -measure. We would like to highlight two potential advantages of our new model over the old <cite>RHT</cite> model. First, our model is able to handle certain sentence-semantics pairs which could not be handled by <cite>RHT</cite> during both training and evaluation as discussed in Section 3.1.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_27",
  "x": "We note that in our experiments we used a small subset of the features used by our <cite>relaxed hybrid tree</cite> work. Specifically, we did not use any long-distance features, and also did not use any character-level features. As we have mentioned in <cite>(Lu, 2014)</cite> , although the <cite>RHT</cite> model is able to capture unbounded long-distance dependencies, for certain languages such as German such longdistance features appeared to be detrimental to the performance of the system (<cite>Lu, 2014</cite>, Table  4 ).",
  "y": "uses"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_28",
  "x": "As we have mentioned in <cite>(Lu, 2014)</cite> , although the <cite>RHT</cite> model is able to capture unbounded long-distance dependencies, for certain languages such as German such longdistance features appeared to be detrimental to the performance of the system (<cite>Lu, 2014</cite>, Table  4 ). Here in this work, we only used simple unigram features (concatenation of a semantic unit and an individual word that appears directly below that unit in the joint representation), pattern features (concatenation of a semantic unit and the pattern below that unit) as well as transition features (concatenation of two semantic units that form a parent-child relationship) described in <cite>(Lu, 2014)</cite> . While additional features could potentially lead to better results, using simpler features would make our model more compact and more interpretable.",
  "y": "extends differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_29",
  "x": "We summarized in Table 3 the number of features used in both the previous <cite>RHT</cite> system and our system across four different languages. It can be seen that our system only required about 2-3% of the Table 3 : Number of features involved for both the <cite>RHT</cite> system and our new system using constrained semantic forests, across four different languages. features used in the previous system.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_30",
  "x": "features used in the previous system. We also note that the training time for our model is longer than that of the <cite>relaxed hybrid tree</cite> model since the space for H (n, m ) is now much larger than the space for H(n, m ). In practice, to make the overall training process faster, we implemented a parallel version of the original <cite>RHT</cite> algorithm.",
  "y": "differences"
 },
 {
  "id": "88aca1aa7ab73cde8492adc0e7a059_31",
  "x": "features used in the previous system. We also note that the training time for our model is longer than that of the <cite>relaxed hybrid tree</cite> model since the space for H (n, m ) is now much larger than the space for H(n, m ). In practice, to make the overall training process faster, we implemented a parallel version of the original <cite>RHT</cite> algorithm.",
  "y": "uses"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_0",
  "x": "However in the best case these models can only reproduce the 1 http://www.macs.hw.ac.uk/InteractionLab/E2E/ style of the training data, and in actuality the outputs have reduced stylistic variation, because when particular stylistic variations are less frequent, they are treated similarly to noise. Browns Cambridge is a pub, also it is a moderately priced italian place near Adriatic, also it is family friendly, you know and it's in the city centre. In subsequent work, we showed that we could augment the E2E training data with synthetically generated stylistic variants and train a neural generator to reproduce these variants, however the models can still only generate what they have seen in training<cite> [5]</cite> .",
  "y": "background"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_1",
  "x": "The PERSONAGE corpus<cite> [5]</cite> provides a controlled environment for testing different models of neural generation and style generation. It consists of 88,500 restaurant domain utterances whose style varies according to models of personality, which were generated by an existing statistical NLG engine that has the capability of manipulating 67 different stylistic parameters [9] . Table 2 shows sample utterances that are output for the singlevoice models and for each of our multi- Table 2 : MultiVoice generation output and comparable singlevoice outputs for DISAGREEABLE, EXTRAVERT and CONSCIENTIOUS for the meaning representation in Figure 1 .",
  "y": "uses"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_2",
  "x": "The frequencies of longer utterances (more attribute MRs) vary across train and test with test MRs not seen during training. The training data has more smaller MRs, while the test set is more challenging, with more larger MRs. Previous work shows that a simple model trained on the whole corpus of 88,855 utterances produces semantically correct outputs, but with reduced stylistic variation<cite> [5]</cite> , while a model that allocates a variable corresponding to a label for each style learns to reproduce the stylistic variation.",
  "y": "differences background"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_3",
  "x": "This is interesting because each style variable (personality) actually encodes a set of 36 different stylistic parameters and their values: the model learns for example how the DISAGREEABLE personality tends to produce many shorter sentences in the output, as well as learning that it tends to use expletives like damn, e.g. see the outputs based on DISAGREEABLE personality in Table 2 . Model Description. Our NNLG model uses a single token to represent personality encoding, following the use of single language labels used in machine translation and other work on neural generation [10, <cite>5]</cite> .",
  "y": "uses"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_4",
  "x": "Our model differs from the TO-KEN model used in our previous work<cite> [5]</cite> because it is trained on unsorted inputs to allow us to add multiple CONVERT tags to the MR at generation time. Note that we do not train on multiple personalities, instead, we train one model that uses all the data, where each distinct single personality has a corresponding CONVERT(PERSONALITY = X) in the training instance. At generation time, we generate singlevoice data for all the test MRs (1,390 total realizations, 278 unique MRs, realized for each of 5 personalities).",
  "y": "differences"
 },
 {
  "id": "8905d5936a5b2a839bfd56783ff55d_5",
  "x": "Natural language generators for task-oriented dialog should be able to vary the style of the output while still effectively realizing the system dialog actions and their associated semantics. The use of neural natural language generation (NNLG) for training the response generation component of conversational agents promises to simplify the process of producing high quality responses in new domains by relying on the neural architecture to automatically learn how to map an input meaning representation to an output utterance. However, there has been little investigation of NNLGs for dialog that can vary their response style, and we know of no experiments on models that can generate responses that are different in style from those seen during training, while still maintaining semantic fidelity to the input meaning representation. Instead, work on stylistic transfer has focused on tasks where only coarse-grained semantic fidelity is needed, such as controlling the sentiment of the utterance (positive or negative), or the topic or entity under discussion [1, 2, 3] . Consider for example a training instance for the restaurant domain consisting of a meaning representation (MR) from the End-to-End (E2E) Generation Challenge 1 and a sample output from one of our neural generation models in Figure 1 [4, <cite>5]</cite> .",
  "y": "background"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_0",
  "x": "Second, we show that the accuracies of a local, greedy transition-based parser cannot be improved by adding the rich features of Zhang and Nivre (2011) . Our result suggests that global learning with beam-search accommodates more complex models with richer features than a local model with greedy search and therefore enables higher accuracies. One interesting aspect of using a global model with beam-search is that it narrows down the contrast between \"local, greedy, transition-based parsing\" and \"global, exhaustive, graph-based parsing\" as exemplified by<cite> McDonald and Nivre (2007)</cite> .",
  "y": "background"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_1",
  "x": "We follow<cite> McDonald and Nivre (2007)</cite> and perform a comparative error analysis of ZPar, MSTParser and MaltParser using the CoNLL-X shared task data. Our results show that beam-search im-proves the precision on long sentences and dependencies compared to greedy search, while the advantage of transition-based parsing on short dependencies is preserved. Under particular measures, such as precision for arcs at different levels of the trees, ZPar shows characteristics surprisingly similar to MSTParser.",
  "y": "uses"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_2",
  "x": "**THE PARSERS AND EVALUATION DATA** In this section we study the effect of global learning and beam-search on the error distributions of transition-based dependency parsing. We characterize the errors of ZPar and add it to the error comparison between MaltParser and MSTParser <cite>(McDonald and Nivre, 2007)</cite> .",
  "y": "uses"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_3",
  "x": "In this section we study the effect of global learning and beam-search on the error distributions of transition-based dependency parsing. We characterize the errors of ZPar and add it to the error comparison between MaltParser and MSTParser <cite>(McDonald and Nivre, 2007)</cite> . Following<cite> McDonald and Nivre (2007)</cite> we evaluate the parsers on the CoNLL-X Shared Task data (Buchholz and Marsi, 2006) , which include training and test sentences for 13 different languages.",
  "y": "uses"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_4",
  "x": "For each parser, we conjoin the outputs for all 13 languages in the same way as<cite> McDonald and Nivre (2007)</cite> , and calculate error distributions over the aggregated output. Accuracies are measured using the labeled attached score (LAS) evaluation metric, which is defined as the percentage of words (excluding punctuation) that are assigned both the correct head word and the correct arc label. To handle non-projectivity, pseudo-projective parsing (Nivre and Nilsson, 2005 ) is applied to ZPar and MaltParser, transforming non-projective trees into pseudo-projective trees in the training data, and post-processing pseudo-projective outputs by the parser to transform them into non-projective trees.",
  "y": "uses"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_5",
  "x": "Here the precision of MaltParser and MSTParser is very different, with MaltParser being more precise for arcs nearer to the leaves, but less precise for those nearer to the root. One possible reason is that arcs near the bottom of the tree require comparatively fewer shift-reduce actions to build, and are therefore less prone to the propagation of search errors. Another important reason, as pointed out by<cite> McDonald and Nivre (2007)</cite> , is the default single-root mechanism by MaltParser: all words that have not been attached as a modifier when the shift-reduce process finishes are attached as modifiers to the pseudo-root.",
  "y": "background"
 },
 {
  "id": "89b2b492b4319636ff2f28a4ba0d95_6",
  "x": "Here the precision of MaltParser and MSTParser is very different, with MaltParser being more precise for arcs nearer to the leaves, but less precise for those nearer to the root. One possible reason is that arcs near the bottom of the tree require comparatively fewer shift-reduce actions to build, and are therefore less prone to the propagation of search errors. Another important reason, as pointed out by<cite> McDonald and Nivre (2007)</cite> , is the default single-root mechanism by MaltParser: all words that have not been attached as a modifier when the shift-reduce process finishes are attached as modifiers to the pseudo-root.",
  "y": "similarities background"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_0",
  "x": "However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V\u00e9ronis, 1998; Murray and Green, 2004; <cite>Erk et al., 2009</cite>; Passonneau et al., 2012b) . Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question: if allowed to make labeling ambiguity explicit, will annotators agree? Furthermore, we adopt the goal of<cite> Erk et al. (2009)</cite> , which enabled annotators to weight each sense by its applicability to the given context, thereby quantifying the ambiguity.",
  "y": "motivation background"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_1",
  "x": "Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question: if allowed to make labeling ambiguity explicit, will annotators agree? Furthermore, we adopt the goal of<cite> Erk et al. (2009)</cite> , which enabled annotators to weight each sense by its applicability to the given context, thereby quantifying the ambiguity. This paper provides the following contributions.",
  "y": "similarities"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_3",
  "x": "Likert Ratings Likert rating scales provide the most direct way of gathering weighted sense labels; Turkers are presented with all senses of a word and then asked to rate each on a numeric scale. We adopt the annotation guidelines of<cite> Erk et al. (2009)</cite> which used a five-point scale, ranging from 1 to 5, indicating the sense does not apply or that it matches the contextual usage exactly, respectively. Select and Rate Recent efforts in crowdsourcing have proposed multi-stage processes for accomplishing complex tasks, where efforts by one group of workers are used to create new subtasks for other workers to complete (Bernstein et al., 2010; Kittur et al., 2011; Kulkarni et al., 2012) .",
  "y": "uses"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_4",
  "x": "MaxDiff MaxDiff is an alternative to scale-based ratings in which Turkers are presented with a only subset of all of a word's senses and then asked to select (1) the sense option that best matches the mean-add.v ask.v win.v argument.n interest.n paper.n different.a important.a<cite> Erk et al. (2009)</cite> ing in the example context and (2) the sense option that least matches (Louviere, 1991) . In our setting, we presented three options at a time for words with fewer than seven senses, and four options for those with seven senses. For a single context, multiple subsets of the senses are presented and then their relative ranking is used to produce the numeric rating.",
  "y": "uses"
 },
 {
  "id": "8a8670fd7cfb8db9ddd3f546ce4534_5",
  "x": "For the reference sense labeling, we use a subset of the GWS dataset of<cite> Erk et al. (2009)</cite> , where three annotators rated 50 instances each for eight words. For clarity, we refer to these individuals as the GWS annotators. Given a word usage in a sentence, GWS annotators rated the applicability of all WordNet 3.0 senses using the same Likert scale as described in Section 3.",
  "y": "uses"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_0",
  "x": "Most rely on convolutional neural nets (Zeng et al., 2014 (Zeng et al., , 2015 Grishman, 2015, 2016;<cite> Fu et al., 2017)</cite> or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016) . Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016;<cite> Fu et al., 2017</cite> ) that require additional parsers.",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_1",
  "x": "Our supervised base model will be similar to (Zhou et al., 2016) . Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016;<cite> Fu et al., 2017</cite> ) that require additional parsers. In order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation.",
  "y": "differences"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_2",
  "x": "**SUPERVISED NEURAL RELATION EXTRACTION MODEL** The supervised neural model on a single dataset was introduced by Zeng et al. (2014) and followed by many others (Nguyen and Grishman, 2015; Zhou et al., 2016; Miwa and Bansal, 2016; Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> . We use a similar model as our base model.",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_3",
  "x": "We use a similar model as our base model. It takes word tokens, position of arguments and their entity types as input. Some work (Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> used extra syntax features as input.",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_4",
  "x": "The entity embedding (Nguyen and Grishman, 2016;<cite> Fu et al., 2017</cite> ) is included for arguments that are entities rather than common nouns. At the end, each token is converted to an embedding w i as the concatenation of these three types of embeddings, where i \u2208 [0, T ), T is the length of the sentence. A wide range of encoders have been proposed for relation extraction.",
  "y": "similarities"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_5",
  "x": "The decoder uses this high level representation as features for relation classification. It usually contains one hidden layer (Zeng et al., 2014; Nguyen and Grishman, 2016;<cite> Fu et al., 2017</cite> ) and a softmax output layer. We use the same structure which can be formalized as the following:",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_6",
  "x": "Previous work (Gormley et al., 2015; Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> set, and the other half of bc, cts and wl as the test sets. We followed their split of documents and their split of the relation types for asymmetric relations. The ERE dataset has a similar relation schema to ACE05, but is different in some annotation guidelines (Aguilar et al., 2014) .",
  "y": "background"
 },
 {
  "id": "8ca479895b028ea6dedb0e99cacae6_7",
  "x": "Training separately on the two corpora (row \"Supervised\" in Table 1 ), we obtain results on ACE05 comparable to previous work (Gormley et al., 2015) with substantially fewer features. With syntactic features as (Nguyen and Grishman, 2016;<cite> Fu et al., 2017)</cite> did, it could be further improved. In this paper, however, we want to focus on representation learning from scratch first.",
  "y": "future_work"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_0",
  "x": "Evaluated on a recent large scale dataset<cite> (Hermann et al., 2015)</cite> , our model exhibits better results than previous research, and we find that max-pooling is suited for modeling the accumulation of information on entities. Further analysis suggests that our model can put together multiple pieces of information encoded in different sentences to answer complicated questions. Our code for the model is available at https://github.",
  "y": "uses"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_1",
  "x": "Machine reading systems (Poon et al., 2010; Richardson et al., 2013) can be tested on their ability to answer queries about contents of documents that they read, thus a central problem is how the information of documents should be organized in the system and retrieved by the queries. Recently, large scale datasets of document-queryanswer triples have been constructed from online newspaper articles and their summaries<cite> (Hermann et al., 2015)</cite> , by replacing named entities in the summaries with placeholders to form Cloze (Taylor, 1953 ) style questions ( Figure 1 ). These datasets have enabled training and testing of complicated neural network models of hypothesized machine readers<cite> (Hermann et al., 2015</cite>; Hill et al., 2015) .",
  "y": "background"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_2",
  "x": "These datasets have enabled training and testing of complicated neural network models of hypothesized machine readers<cite> (Hermann et al., 2015</cite>; Hill et al., 2015) . ( @entity1 ) @entity0 may be @entity2 in the popular @entity4 superhero films , but he recently dealt in some advanced bionic technology himself . @entity0 recently presented a robotic arm to young @entity7 , a @entity8 boy who is missing his right arm from just above his elbow .",
  "y": "background"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_3",
  "x": "Information about this entity can only be accumulated by its subsequent occurrence, such as \"Downey recently presented a robotic arm . . . \". Thus, named entities basically serve as anchors to link multiple pieces of information encoded in different sentences. This insight has been reflected by the anonymization process in construction of the dataset, in which coreferent entities (e.g. \"Robert Downey Jr.\" and \"Downey\") are replaced by randomly permuted abstract entity markers (e.g. \"@en-tity0\"), in order to prevent additional world knowledge from being attached to the surface form of the entities<cite> (Hermann et al., 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_4",
  "x": "Thus, named entities basically serve as anchors to link multiple pieces of information encoded in different sentences. This insight has been reflected by the anonymization process in construction of the dataset, in which coreferent entities (e.g. \"Robert Downey Jr.\" and \"Downey\") are replaced by randomly permuted abstract entity markers (e.g. \"@en-tity0\"), in order to prevent additional world knowledge from being attached to the surface form of the entities<cite> (Hermann et al., 2015)</cite> . We, however, take it as a strong motivation to implement a reader that dynamically builds meaning representations for each entity, by gathering and accumulating information on that entity as it reads a document (Section 2).",
  "y": "motivation"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_5",
  "x": "**MODEL** Following<cite> Hermann et al. (2015)</cite> , our model estimates the conditional probability p(e|D, q), where q is a query and D is a document. A candidate answer for the query is denoted by e, which in this paper is any named entity.",
  "y": "uses"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_6",
  "x": "A candidate answer for the query is denoted by e, which in this paper is any named entity. Our model can be factorized as: in which u(q) is the learned meaning for the query and v(e; D, q) the dynamically constructed meaning for an entity, depending on the document D and the query q. We note that (1) is in contrast to the factorization used by<cite> Hermann et al. (2015)</cite>:",
  "y": "differences"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_7",
  "x": "We use the CNN-QA dataset<cite> (Hermann et al., 2015)</cite> for evaluating our model's ability to answer questions about named entities. The dataset consists of (D, q, e)-triples, where the document D is taken from online news articles, and the query q is formed by hiding a named entity e in a summarizing bullet point of the document (Figure 1) . The training set has 90k articles and 380k queries, and both validation and test sets have 1k articles and 3k queries.",
  "y": "uses"
 },
 {
  "id": "91723cf7f22ba6405c85a929ac2d8e_8",
  "x": "Finally, we note that our model, full DER Network, shows the best results compared to several previous reader models<cite> (Hermann et al., 2015</cite>; Hill et al., 2015) , endorsing our approach as promising. The 99% confidence intervals of the results of full DER Network and the one initialized by word2vec on the test set were [0.700, 0.740] and [0.708, 0.749], respectively (measured by bootstrap tests). Analysis In the example shown in Figure 4 , our basic model missed by paying little attention to the second and third sentences, probably because it does not mention @entity0 (Downey).",
  "y": "differences"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_0",
  "x": "Compared with traditional pipeline solutions (Williams and Young, 2007; Young et al., 2013; Wen et al., 2017) , end-to-end approaches recently gain much attention (Zhao et al., 2017; Eric and Manning, 2017a; Lei et al., 2018) , because they directly map dialog history to the output responses and consequently reduce human effort for modular designs and hand-crafted state labels. To effectively incorporate KB information and perform knowledge- * Corresponding Author based reasoning, memory augmented models have been proposed (Bordes et al., 2017; Seo et al., 2017; Eric and Manning, 2017b; <cite>Madotto et al., 2018</cite>; Raghu et al., 2018; Reddy et al., 2019; Wu et al., 2019) . Bordes et al. (2017) and Seo et al. (2017) attended to retrieval models, lacking the ability of generation, while others incorporated the memory (i.e. end-to-end memory networks, abbreviated as MemNNs, Sukhbaatar et al. (2015) ) and copy mechanism (Gu et al., 2016 ) into a sequential generative architecture.",
  "y": "background"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_1",
  "x": "The symbols are defined in Table 1 , and more details can be found in the supplementary material. We omit the subscript E or S 2 , following <cite>Madotto et al. (2018)</cite> to define each pointer index set: Symbol Definition xi or yi a token in the dialog history or system response $ a special token used as a sentinel <cite>(Madotto et al., 2018 )</cite> X X = {x1, . . . , xn, $}, the dialog history Y Y = {y1, \u00b7 \u00b7 \u00b7 , ym}, the expected response bi one KB tuple, actually the corresponding entity B B = {b1, \u00b7 \u00b7 \u00b7 , b l , $}, the KB tuples P T RE = {ptrE,1, \u00b7 \u00b7 \u00b7 , ptrE,m}, dialog pointer index set.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_2",
  "x": "Symbol Definition xi or yi a token in the dialog history or system response $ a special token used as a sentinel <cite>(Madotto et al., 2018 )</cite> X X = {x1, . . . , xn, $}, the dialog history Y Y = {y1, \u00b7 \u00b7 \u00b7 , ym}, the expected response bi one KB tuple, actually the corresponding entity B B = {b1, \u00b7 \u00b7 \u00b7 , b l , $}, the KB tuples P T RE = {ptrE,1, \u00b7 \u00b7 \u00b7 , ptrE,m}, dialog pointer index set. P T RE supervised information for copying words in dialog history P T RS = {ptrS,1, \u00b7 \u00b7 \u00b7 , ptrS,m}, KB pointer index set. P T RS supervised information for copying entities in KB tuples Table 1 : Notation Table. where xb z \u2208 X or B is the dialog history or KB tuples according to the subscript (E or S) and n xb + 1 is the sentinel position index as n xb is equal to the dialog history length n or the number of KB triples l. The idea behind Eq. 1 is that we can obtain the positions of where to copy by matching the target text with the dialog history or KB information.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_3",
  "x": "As shown in Figure 1 , we adopt the E-MemNN to memorize the dialog history X as described in Section 2.1, and then store KB tuples into the S-MemNN without TRANS(\u00b7). We also incorporate additional temporal information and speaker information into dialog utterances as <cite>(Madotto et al., 2018)</cite> and adopt a (subject, relation, object) representation of KB information as (Eric and Manning, 2017b) . More details can be found in the supplementary material.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_4",
  "x": "Now, three distributions, P vocab , P E\u00b7ptr and P S\u00b7ptr , are activated and moved into the STS, and then a proper word is generated from the activated distributions. We here use a rule-based word selection strategy by extending the sentinel idea in <cite>(Madotto et al., 2018)</cite> , which is shown in Figure 1 . If the expected word is not appearing either in the episodic memory or the semantic memory, the two copy pointers are trained to produce the sentinel token and our WMM2Seq generates the token from P vocab ; otherwise, the token is generated by copying from either the dialog history or KB tuples and this is done by comparing the two copy distributions.",
  "y": "extends"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_5",
  "x": "We use Per-response/dialog Accuracy (Bordes et al., 2017) , BLEU (Papineni et al., 2002) and Entity F1 <cite>(Madotto et al., 2018)</cite> to compare the performance of different models. And the baseline models are Seq2Seq+Attn (Luong et al., 2015) , Pointer to Unknown (Ptr-Unk, Gulcehre et al. (2016) ), <cite>Mem2Seq</cite> <cite>(Madotto et al., 2018)</cite> , Hierarchical Pointer Generator Memory Network (HyP-MN, Raghu et al. (2018) ) and Global-to-Local Memory Pointer (GLMP, Wu et al. (2019) ). Automatic Evaluation: The results on the bAbI dialog dataset are given in Table 2 .",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_6",
  "x": "We use Per-response/dialog Accuracy (Bordes et al., 2017) , BLEU (Papineni et al., 2002) and Entity F1 <cite>(Madotto et al., 2018)</cite> to compare the performance of different models. And the baseline models are Seq2Seq+Attn (Luong et al., 2015) , Pointer to Unknown (Ptr-Unk, Gulcehre et al. (2016) ), <cite>Mem2Seq</cite> <cite>(Madotto et al., 2018)</cite> , Hierarchical Pointer Generator Memory Network (HyP-MN, Raghu et al. (2018) ) and Global-to-Local Memory Pointer (GLMP, Wu et al. (2019) ). Automatic Evaluation: The results on the bAbI dialog dataset are given in Table 2 .",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_7",
  "x": "First, we remove the context-sensitive transformation TRANS(\u00b7) and then find significant performance degradation. This suggests that perceptual processes are a necessary step before storing perceptual information (the dialog history) into the episodic memory and it is important for the performance of working memory. Second, we find that WMM2Seq outperforms <cite>Mem2Seq</cite>, which uses a unified memory to store dialog history and KB information.",
  "y": "differences"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_8",
  "x": "Second, we find that WMM2Seq outperforms <cite>Mem2Seq</cite>, which uses a unified memory to store dialog history and KB information. We can safely conclude that the separation of context memory and KB memory benefits the performance, as WMM2Seq performs well with less parameters than <cite>Mem2Seq</cite> on task 5. Finally, we additionally analysis how the multi-hop attention mechanism helps by showing the performance differences between the hop K = 1 and the default hop K = 3.",
  "y": "extends differences"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_9",
  "x": "Finally, we additionally analysis how the multi-hop attention mechanism helps by showing the performance differences between the hop K = 1 and the default hop K = 3. Though multi-hop attention strengthens the reasoning ability and improves the results, we find that the performance difference between the hops K = 1 and K = 3 is not so obvious as shown in <cite>(Madotto et al., 2018</cite>; Wu et al., 2019) . Furthermore, our model performs well even with one hop, which we mainly attribute to the reasoning ability of working memory.",
  "y": "differences"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_10",
  "x": "Human Evaluation: Following the methods in (Eric and Manning, 2017b; Wu et al., 2019) , we report human evaluation of the generated responses in Table 4 . We adopt <cite>Mem2Seq</cite> as the baseline for human evaluation considering its good performance and code release 3 . First we randomly select 100 samples from the DSTC2 test set, then generate the corresponding responses using WMM2Seq and <cite>Mem2Seq</cite>, and finally ask two human subjects to judge the quality of the generated responses according to the appropriateness and humanlikeness on a scale from 1 to 5.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_11",
  "x": "We adopt <cite>Mem2Seq</cite> as the baseline for human evaluation considering its good performance and code release 3 . First we randomly select 100 samples from the DSTC2 test set, then generate the corresponding responses using WMM2Seq and <cite>Mem2Seq</cite>, and finally ask two human subjects to judge the quality of the generated responses according to the appropriateness and humanlikeness on a scale from 1 to 5. As shown in Table 4 , WMM2Seq outperforms <cite>Mem2Seq</cite> in both measures, which is coherent to the automatic evaluation.",
  "y": "uses"
 },
 {
  "id": "91c82c4a49815fb2de300d99312754_12",
  "x": "First we randomly select 100 samples from the DSTC2 test set, then generate the corresponding responses using WMM2Seq and <cite>Mem2Seq</cite>, and finally ask two human subjects to judge the quality of the generated responses according to the appropriateness and humanlikeness on a scale from 1 to 5. As shown in Table 4 , WMM2Seq outperforms <cite>Mem2Seq</cite> in both measures, which is coherent to the automatic evaluation. More details about human evaluation are reported in the supplementary material.",
  "y": "differences"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_0",
  "x": "BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011) , summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; Stajner et al., 2015;<cite> Xu et al., 2016)</cite> , i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014) , BLEU became the main automatic metric for TS, despite its deficiencies (see \u00a72). Indeed, focusing on lexical simplification,<cite> Xu et al. (2016)</cite> argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used.",
  "y": "background"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_1",
  "x": "Indeed, focusing on lexical simplification,<cite> Xu et al. (2016)</cite> argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity, but obtained a higher correlation with grammaticality and meaning preservation, relative to the SARI metric they proposed. In this paper, we further explore the applicability of BLEU for TS evaluation, examining BLEU's informativeness where sentence splitting is involved.",
  "y": "background"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_2",
  "x": "For exploring the effect of sentence splitting on BLEU scores, we compile a human-generated gold standard sentence splitting corpus -HSplit, which will also be useful for future studies of splitting in TS, and perform correlation analyses with human judgments. We consider two reference sets. First, we experiment with the most common set, proposed by<cite> Xu et al. (2016)</cite> , evaluating a variety of system outputs, as well as HSplit.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_3",
  "x": "While BLEU is standardly used for TS evaluation (e.g.,<cite> Xu et al., 2016</cite>; Nisioi et al., 2017; Zhang and Lapata, 2017; Ma and Sun, 2017 ), only few works tested its correlation with human judgments. Using 20 source sentences from the PWKP test corpus (Zhu et al., 2010) with 5 simplified sentences for each of them, Wubben et al. (2012) reported positive correlation of BLEU with simplicity ratings, but no correlation with adequacy. T-BLEU (\u0160tajner et al., 2014), a variant of BLEU which uses lower n-grams when no overlapping 4-grams are found, was tested on outputs that applied only structural modifications to the source.",
  "y": "motivation"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_4",
  "x": "In order to investigate the effect of correctly splitting sentences on the automatic metric scores, we build a parallel corpus, where each sentence is modified by 4 annotators, according to specific sentence splitting guidelines. We use the complex side of the test corpus of<cite> Xu et al. (2016)</cite> . 3 While Narayan et al. (2017) recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_5",
  "x": "3 While Narayan et al. (2017) recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split. This corpus enriches the set of references focused on lexical operations that were collected by<cite> Xu et al. (2016)</cite> for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (Narayan et al., 2017) . We use two sets of guidelines.",
  "y": "extends"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_6",
  "x": "In addition to BLEU, 7 we also experiment with (1) iBLEU (Sun and Zhou, 2012) which was recently used for TS <cite>(Xu et al., 2016</cite>; and which takes into account the BLEU scores of the output against the input and against the references; (2) the Flesch-Kincaid Grade Level (FK; Kincaid et al., 1975 ), computed at the system level, which estimates the readability of the text with a lower value indicating higher 4 Examples are taken from Siddharthan (2006) . 5 Examples are not provided in the case of Set 2 so as not to give an a-priori notion of simplicity. The complete guidelines are found in the supplementary material.",
  "y": "background"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_7",
  "x": "Metrics. 7 System-level BLEU scores are computed using the multi-bleu Moses support tool. Sentence-level BLEU scores are computed using NLTK (Loper and Bird, 2002). readability; 8 (3) SARI<cite> (Xu et al., 2016)</cite> , which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_8",
  "x": "Systems include six MT-based simplification systems, including outputs of the state-of-the-art neural TS system of Nisioi et al. (2017) , in four variants: either default settings or initialization by word2vec, for each both the highest and the fourth ranked hypotheses in the beam are considered. 10 We further include Moses (Koehn et al., 2007) and SBMT-SARI<cite> (Xu et al., 2016)</cite> , a syntax-based MT system tuned against SARI, and the identity function (outputs are same as inputs). The case which evaluates outputs with sentence splitting additionally includes the four HSplit corpora and the HSplit average scores.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_9",
  "x": "9 We explore two settings. In one (\"Standard Reference Setting\", \u00a74.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by<cite> Xu et al. (2016)</cite> (yielding BLEU-8ref, iBLEU-8ref and SARI-8ref). In the other (\"HSplit as Reference Setting\", \u00a74.3), we use HSplit as the reference set.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_10",
  "x": "We use the evaluation benchmark provided by Sulem et al. (2018b) , 11 including system outputs and human evaluation scores corresponding to the first 70 sentences of the test corpus of<cite> Xu et al. (2016)</cite> , and extend it to apply to HSplit as well. The evaluation of HSplit is carried out by 3 in-house native English annotators, who rated the different input-output pairs for the different systems according to 4 parameters: Grammaticality (G), Meaning preservation (M), Simplicity (S) and Structural Simplicity (StS). G and M are measured using a 1 to 5 scale.",
  "y": "uses"
 },
 {
  "id": "91e869971f139d90e36f73b1089877_11",
  "x": "Indeed, BLEU-1ref obtains 59.85 for the input and 43.90 for the HSplit corpora (averaged over the 4 HSplit corpora). BLEU-8ref obtains 94.63 for the input and 73.03 for HSplit. 12 The high scores obtained for Identity, also observed by<cite> Xu et al. (2016)</cite> , indicate that BLEU is a not a good predictor for relative simplicity to the input.",
  "y": "similarities"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_0",
  "x": "1 User comments, however, and more generally user content can also be abusive (e.g., bullying, profanity, hate speech) (Cheng et al., 2015) . Social media are under pressure to combat abusive content, but so far rely mostly on user reports and tools that detect frequent words and phrases of reported posts. 2<cite> Wulczyn et al. (2017)</cite> estimated that only 17.9% of personal attacks in Wikipedia discussions were followed by moderator actions.",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_1",
  "x": "We also provide word embeddings pre-trained on 5.2M comments from the same portal. Furthermore, we experiment on the 'attacks' dataset of<cite> Wulczyn et al. (2017)</cite> , approx. 115K English Wikipedia talk page comments labeled as containing personal attacks or not.",
  "y": "uses"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_3",
  "x": "30% in all subsets, except for G-TEST-S-R where it drops to 22%, because there are no occasions where the moderators were instructed to be stricter in G-TEST-S-R. Each G-TEST-S-R comment was re-moderated by five annotators. Krippendorff's (2004) alpha was 0.4762, close to the value (0.45) reported by<cite> Wulczyn et al. (2017)</cite> for the Wikipedia 'attacks' dataset. Using Cohen's Kappa (Cohen, 1960) , the mean pairwise agreement was 0.4749.",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_4",
  "x": "The Wikipedia 'attacks' dataset <cite>(Wulczyn et al., 2017)</cite> contains approx. 115K English Wikipedia talk page comments, which were labeled as containing personal attacks or not. Each comment was labeled by at least 10 annotators. Inter-annotator agreement, measured on a random sample of 1K comments using Krippendorff's (2004) alpha, was 0.45. The gold label of each comment is determined by the majority of annotators, leading to binary labels (accept, reject). Alternatively, the gold label is the percentage of annotators that labeled the comment as 'accept' (or 'reject'), leading to probabilistic labels. 7 The dataset is split in three parts (Table 1) : training (W-ATT-TRAIN, 69,526 comments), development (W-ATT-DEV, 23,160), and test (W-ATT-TEST, 23,178). In all three parts, the rejected comments are 12%, but this is an artificial ratio (Wulczyn et al. oversampled comments posted by banned users). By contrast, the ratio of rejected comments in all the Gazzetta subsets is the truly observed one. The Wikipedia comments are also longer (median length 38 tokens) compared to Gazzetta's (median length 25 tokens).",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_5",
  "x": [
   "Wulczyn et al. (2017) also provide two additional datasets of English Wikipedia talk page comments, which are not used in this paper. The first one, called 'aggression' dataset, contains the same comments as the 'attacks' dataset, now labeled as 'aggressive' or not. The (probabilistic) labels of the 'attacks' and 'aggression' datasets are very highly correlated (0.8992 Spearman, 0.9718 Pearson) and we did not consider the aggression dataset any further. The second additional dataset, called 'toxicity' dataset, contains approx. 160K comments labeled as being toxic or not."
  ],
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_6",
  "x": "**METHODS** We experimented with an RNN operating on word embeddings, the same RNN enhanced with our attention mechanism (a-RNN), a vanilla convolutional neural network (CNN) also operating on word embeddings, the DETOX system of<cite> Wulczyn et al. (2017)</cite> , and a baseline that uses word lists. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_7",
  "x": "DETOX <cite>(Wulczyn et al., 2017)</cite> was the previous state of the art in comment moderation, in the sense that it had the best reported results on the Wikipedia datasets (Section 2.2), which were in turn the largest previous publicly available dataset of moderated user comments. 8 DETOX represents each comment as a bag of word n-grams (n \u2264 2, each comment becomes a bag containing its 1-grams and 2-grams) or a bag of character n-grams (n \u2264 5, each comment becomes a bag containing character 1-grams, . . . , 5-grams). DETOX can rely on a logistic regression (LR) or MLP classifier, and it can use binary or probabilistic gold labels (Section 2.2) during training.",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_8",
  "x": [
   "8 DETOX represents each comment as a bag of word n-grams (n \u2264 2, each comment becomes a bag containing its 1-grams and 2-grams) or a bag of character n-grams (n \u2264 5, each comment becomes a bag containing character 1-grams, . . . , 5-grams). DETOX can rely on a logistic regression (LR) or MLP classifier, and it can use binary or probabilistic gold labels (Section 2.2) during training. We used the DETOX implementation provided by Wulczyn et al. and the same grid search (and code) to tune the hyper-parameters of DETOX that select word or character n-grams, classifier (LR or MLP), and gold labels (binary or probabilistic)."
  ],
  "y": "uses"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_9",
  "x": [
   "We used the DETOX implementation provided by Wulczyn et al. and the same grid search (and code) to tune the hyper-parameters of DETOX that select word or character n-grams, classifier (LR or MLP), and gold labels (binary or probabilistic). For Gazzetta, only binary gold labels were possible, since G-TRAIN-L and G-TRAIN-S have a single gold label per comment. Unlike Wulczyn et al., we tuned the hyper-parameters by evaluating (computing AUC and Spearman, Section 4) on a random 2% of held-out comments of W-ATT-TRAIN or G-TRAIN-S, instead of the development subsets, to be able to obtain more realistic results from the development sets while developing the methods."
  ],
  "y": "differences"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_12",
  "x": "Scores reported by<cite> Wulczyn et al. (2017)</cite> are shown in brackets. always better than CNN and DETOX; there is no clear winner between CNN and DETOX. Furthermore, a-RNN is always better than RNN on Gazzetta comments, but not on Wikipedia comments, where RNN is overall slightly better according to Table 2 .",
  "y": "background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_13",
  "x": "We also repeated the annotator ensemble experiment of<cite> Wulczyn et al. (2017)</cite> on 8K randomly chosen comments of W-ATT-TEST (4K comments from random users, 4K comments from banned users). 19 The decisions of 10 randomly chosen annotators (possibly different per comment) were used to construct the gold label of each comment. The gold labels were then compared to the decisions of the systems and the decisions of an ensemble of k other annotators, k ranging from 1 to 10.",
  "y": "uses background"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_14",
  "x": "Table 3 shows the mean AUC and Spearman scores, averaged over 25 runs of the experiment, along with standard errrors (in brackets). We conclude that RNN and a-RNN are as good as an ensemble of 7 human annotators; CNN is as good as 4 annotators; DETOX is as good as 4 in AUC and 3 annotators in Spearman correlation, which is consistent with the results of<cite> Wulczyn et al. (2017)</cite> . ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "920f2b94270c0711fcc19ad23dbb0d_15",
  "x": [
   "Wulczyn et al. (2017) experimented with character and word n-grams. We included their dataset and moderation system (DETOX) in our experiments. Waseem et al. (2016) used approx."
  ],
  "y": "similarities uses"
 },
 {
  "id": "929020618e8e1daa6a769f552a4655_0",
  "x": "Nonetheless, the low accuracy of such formulas and their language dependency made way for more advanced and accurate readability assessment methods, which involve machine learning techniques. These models are highly accurate for their use of sophisticated NLP features and machine intelligence to associate the extracted features to a proper readability level. Models proposed by Vajjala and Meurers [17] , Xia et al. <cite>[18]</cite> , and Mohammadi and Khasteh [19] are examples of state-of-the-art models for their target languages and target audience.",
  "y": "background"
 },
 {
  "id": "929020618e8e1daa6a769f552a4655_1",
  "x": "Despite the widespread use of English as a second language, this field has seen a few thorough studies. Different models are required to assess the readability of English texts for the second language readers as a different set of characteristics of text is influential on its readability level for second language readers [35] . Xia et al. <cite>[18]</cite> has published a thorough study on second language text readability assessment.",
  "y": "background"
 },
 {
  "id": "929020618e8e1daa6a769f552a4655_2",
  "x": "Prior to this study, distinct models have been used to assess the readability of English texts for second language readers. Since the DRL model eliminates the need for specific feature engineering for different types of text readability assessment, the proposed model can be applied to second language datasets without any modifications. To examine the proposed DRL model regarding this ability, it is applied to the Cambridge Exams dataset <cite>[18]</cite> .",
  "y": "uses"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_0",
  "x": "The importance of each of these sub-component on a particular downstream application is also not very clear. For the task of question-answering, we instead make an attempt at an end-to-end approach which directly models the entities and relations in the text as memory slots. While incorporating existing knowledge (from curated knowledge bases) for the purpose of question-answering [12, 9, 16] is an important area of research, we consider the simpler setting where all the information is contained within the text itself -which is the approach taken by many recent memory based neural network models [17, <cite>18</cite>, 19, 20] .",
  "y": "motivation background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_1",
  "x": "Recently, Henaff et. al. <cite>[18]</cite> proposed a dynamic memory based neural network for implicitly modeling the state of entities present in the text for question answering. However, <cite>this model</cite> lacks any module for relational reasoning.",
  "y": "motivation"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_2",
  "x": "However, <cite>this model</cite> lacks any module for relational reasoning. In response, we propose RelNet, which extends memoryaugmented neural networks with a relational memory to reason about relationships between multiple entities present within the text. Our end-to-end method reads text, and writes to both memory slots and edges between them.",
  "y": "motivation"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_3",
  "x": "We demonstrate the utility of the model through experiments on the bAbI tasks [19] and find that the model achieves smaller mean error across the tasks than the best previously published result [<cite>18</cite>] in the 10k examples regime and achieves 0% error on 11 of the 20 tasks. ---------------------------------- **RELNET MODEL**",
  "y": "differences"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_4",
  "x": "We describe the RelNet model in this section. The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to <cite>Recurrent Entity Networks</cite> [<cite>18</cite>] and then equip it with an additional relational memory.",
  "y": "similarities extends"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_5",
  "x": "We will describe these three modules in details. The input encoder and output module implementations are similar to the <cite>Entity Network</cite> [<cite>18</cite>] and main novelty lies in the dynamic memory. We describe the operations executed by the network for a single example consisting of a document with T sentences, where each sentence consists of a sequence of words represented with K-dimensional word embeddings {e 1 , . . . , e N }, a question on the document represented as another sequence of words and an answer to the question.",
  "y": "similarities extends"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_6",
  "x": "The input at each time point is a sentence from the document which can be encoded into a fixed vector representation using some encoding mechanism, such as a recurrent neural network. We use a simple encoder with a learned multiplicative mask [<cite>18</cite>, 17] : Dynamic Relational Memory This is the main component of an end-to-end reasoning pipeline, where we need to process the information contained in the text such that it can be used to reason about the entities, their properties and the relationships among them.",
  "y": "uses"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_7",
  "x": "Intuitively the memory slots can be thought of as entities. Indeed, Henaff et. al. <cite>[18]</cite> found that if <cite>they</cite> tie the key vectors to entities in the text then the memories contain information about the state of those entities.",
  "y": "background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_8",
  "x": "Note that there can be multiple entites in a sentence hence a sigmoid operation is more suitable, and it is also more scalable <cite>[18]</cite> . After selecting the set of memories, there is an update step which stores information in the corresponding memory slots: where PReLU is a parametric Rectified linear unit [21] , and U , V and W are k \u00d7 k parameter matrices.",
  "y": "background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_9",
  "x": "Similar to [<cite>18</cite>] , we normalize the memories after each update step (that is after reading each sentence). This acts as a forget step and does not cause the memory to explode. The full memory consists of the entity memory slots {h j } and the relational memory slots {r ij }.",
  "y": "similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_10",
  "x": "Output Module This is a standard attention module used in memory networks [17, <cite>18</cite>] . The question is encoded as a K dimensional vector q using the same encoding mechanism as the sentences (though with a separate learned mask). We first concatenate the relational memory vectors with the corresponding entity vectors, and project the resulting memory vector to k dimension.",
  "y": "similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_11",
  "x": "There is a long line of work in textual question-answering systems [22, 23] . Recent successful approaches use memory based neural networks for question answering [24, 19, 25, 20, <cite>18</cite>] . Our model is also a memory network based model and is also related to the neural turing machine [26] .",
  "y": "background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_12",
  "x": "As described previously, the model is closely related to the <cite>Recurrent Entity Networks</cite> model [<cite>18</cite>] which describes an end-to-end approach to model entities in text but does not directly model relations. Other approaches to question answering use external knowledge, for instance external knowledge bases [27, 12, 28, 29, 10] or external text like Wikipedia [30, 31] . Very recently, and in parallel to this work, a method for relational reasoning called relation networks [32] was proposed.",
  "y": "background"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_14",
  "x": "Training Details: We used Adam and did a grid search for the learning rate in {0.01, 0.005, 0.001} and choose a fixed learning rate of 0.005 based on performance on the validation set, and clip the gradient norm at 2. We keep all other details similar to <cite>[18]</cite> for a fair comparison. embedding dimensions were fixed to be 100, models were trained for a maximum of 250 epochs with minibatches size of 32 for all tasks except 3 for which the batch size was 16.",
  "y": "similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_15",
  "x": "The RelNet models were run for 5 times with random seed on each task and the model with best validation performance was chosen as the final model. The baseline <cite>EntNet</cite> model was run for 10 times for each task <cite>[18]</cite> . The results are shown in Table 1 .",
  "y": "similarities"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_16",
  "x": "The baseline <cite>EntNet</cite> model was run for 10 times for each task <cite>[18]</cite> . The results are shown in Table 1 . The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the <cite>EntNet</cite> model <cite>[18]</cite> .",
  "y": "differences"
 },
 {
  "id": "92f4cc0d6516a19a860d5b9af80f59_17",
  "x": "The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the <cite>EntNet</cite> model <cite>[18]</cite> . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the <cite>EntNet</cite> model achieves 0% error on 7 of the tasks. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_0",
  "x": "Recently, sequence to sequence (seq2seq) learning, proposed by Kalchbrenner & Blunsom (2013) , Sutskever et al. (2014) , and Cho et al. (2014) , emerges as an effective paradigm for dealing with variable-length inputs and outputs. seq2seq learning, at its core, uses recurrent neural networks to map variable-length input sequences to variable-length output sequences. While relatively new, the seq2seq approach has achieved state-of-the-art results in not only its original application -machine translation - (Luong et al., 2015b; Jean et al., 2015a; Luong et al., 2015a; Jean et al., 2015b; Luong & Manning, 2015) , but also image caption generation , and constituency parsing<cite> (Vinyals et al., 2015a)</cite> .",
  "y": "background"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_1",
  "x": "While relatively new, the seq2seq approach has achieved state-of-the-art results in not only its original application -machine translation - (Luong et al., 2015b; Jean et al., 2015a; Luong et al., 2015a; Jean et al., 2015b; Luong & Manning, 2015) , but also image caption generation , and constituency parsing<cite> (Vinyals et al., 2015a)</cite> . Despite the popularity of multi-task learning and sequence to sequence learning, there has been little work in combining MTL with seq2seq learning. To the best of our knowledge, there is only one recent publication by Dong et al. (2015) which applies a seq2seq models for machine translation, where the goal is to translate from one language to multiple languages.",
  "y": "motivation"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_2",
  "x": "We show that syntactic parsing and image caption generation improves the translation quality between English (Sutskever et al., 2014) and (right) constituent parsing<cite> (Vinyals et al., 2015a)</cite> . and German by up to +1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F 1 .",
  "y": "differences"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_3",
  "x": "Recent work such as Jean et al., 2015a; Luong et al., 2015a; <cite>Vinyals et al., 2015a)</cite> has found that it is crucial to empower seq2seq models with the attention mechanism. ---------------------------------- **MULTI-TASK SEQUENCE-TO-SEQUENCE LEARNING**",
  "y": "background"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_5",
  "x": "4 Since in our experiments, unsupervised tasks are always coupled with translation tasks, we use the same validation and test sets as the accompanied translation tasks. For constituency parsing, we experiment with two types of corpora: 1. a small corpus -the widely used Penn Tree Bank (PTB) dataset (Marcus et al., 1993) and, 2. a large corpus -the high-confidence (HC) parse trees provided by<cite> Vinyals et al. (2015a)</cite> .",
  "y": "uses"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_6",
  "x": "Note also that the parse trees have been linearized following<cite> Vinyals et al. (2015a)</cite> . Lastly, for image caption generation, we use a dataset of image and caption pairs provided by Vinyals et al. (2015b) . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_7",
  "x": "Since the parsing task maps from a sequence of English words to a sequence of parsing tags<cite> (Vinyals et al., 2015a)</cite> , only the encoder can be shared with an English\u2192German translation task. As a result, this is a one-to-many MTL scenario ( \u00a73.1). To our surprise, the results in Table 2 suggest that by adding a very small number of parsing minibatches (with mixing ratio 0.01, i.e., one parsing mini-batch per 100 translation mini-batches), we can improve the translation quality substantially.",
  "y": "background"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_8",
  "x": "For parsing, as<cite> Vinyals et al. (2015a)</cite> have shown that attention is crucial to achieve good parsing performance when training on the small PTB corpus, we do not set a high bar for our attention-free systems in this setup (better performances are reported in Section 4.3.3). Nevertheless, the parsing results in Table 2 indicate that MTL is also beneficial for parsing, yielding an improvement of up to +8.9 F 1 points over the baseline. 6 It would be interesting to study how MTL can be useful with the presence of the attention mechanism, which we leave for future work.",
  "y": "similarities"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_9",
  "x": "data. Instead of using the small Penn Tree Bank corpus, we consider a large parsing resource, the high-confidence (HC) corpus, which is provided by<cite> Vinyals et al. (2015a)</cite> . As highlighted in Table 4 , the trend is consistent; MTL helps boost translation quality by up to +0.9 BLEU points.",
  "y": "uses"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_10",
  "x": "The second set of experiments shifts the attention to parsing by having it as the reference task. We show in Table 5 results that combine parsing with either (a) the English autoencoder task or (b) the English\u2192German translation task. Our models are compared against the best attention-based systems in<cite> (Vinyals et al., 2015a)</cite> , including the state-of-the-art result of 92.8 F 1 .",
  "y": "uses"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_11",
  "x": "7 Second, our baseline system can obtain a very competitive F 1 score of 92.2, rivaling<cite> Vinyals et al. (2015a)</cite> 's systems. This is rather surprising since our models do not use any attention mechanism. A closer look into these models reveal that there seems to be an architectural difference:<cite> Vinyals et al. (2015a)</cite> use 3-layer LSTM with 256 cells and 512-dimensional embeddings; whereas our models use 4-layer LSTM with 1000 cells and 1000-dimensional embeddings.",
  "y": "similarities"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_12",
  "x": "This is rather surprising since our models do not use any attention mechanism. A closer look into these models reveal that there seems to be an architectural difference:<cite> Vinyals et al. (2015a)</cite> use 3-layer LSTM with 256 cells and 512-dimensional embeddings; whereas our models use 4-layer LSTM with 1000 cells and 1000-dimensional embeddings. This further supports findings in (Jozefowicz et al., 2016) that larger networks matter for sequence models.",
  "y": "differences"
 },
 {
  "id": "9426b2faf2ba633033c7dfcee4118b_13",
  "x": "For the multi-task results, while autoencoder does not seem to help parsing, translation does. At the mixing ratio of 0.05, we obtain a non-negligible boost of 0.2 F 1 over the baseline and with 92.4 F 1 , our multi-task system is on par with the best single system reported in<cite> (Vinyals et al., 2015a)</cite> . Furthermore, by ensembling 6 different multi-task models (trained with the translation task at mixing ratios of 0.1, 0.05, and 0.01), we are able to establish a new state-of-the-art result in English constituent parsing with 93.0 F 1 score.",
  "y": "similarities"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_0",
  "x": "The same authors studied the contribution of the different feature types in SRL and concluded that the lexical features were the most salient features in argument classification (Pradhan et al., 2007) . In recent work, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected from a full-fledged SRL system. SPs introduce semantic generalizations on the type of arguments preferred by the predicates and, thus, they are expected to improve results on infrequent and unknown words.",
  "y": "background"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_1",
  "x": "The positive effect was especially relevant for out-of-domain data. In this paper we advance (<cite>Zapirain et al., 2009</cite>) in two directions: (1) We learn separate SPs for prepositions and verbs, showing improvement over using SPs for verbs alone. (2) We integrate the information of several SP models in a state-of-the-art SRL system (SwiRL 1 ) and show significant improvements in SR classification.",
  "y": "extends"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_2",
  "x": "In recent work, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected from a full-fledged SRL system. SPs introduce semantic generalizations on the type of arguments preferred by the predicates and, thus, they are expected to improve results on infrequent and unknown words. The positive effect was especially relevant for out-of-domain data.",
  "y": "motivation"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_3",
  "x": "Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. More recently, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored.",
  "y": "background motivation"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_4",
  "x": "More recently, <cite>we</cite> showed (<cite>Zapirain et al., 2009</cite> ) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored. In this work we apply a subset of the SP methods proposed in (<cite>Zapirain et al., 2009</cite> ). These methods can be split in two main families, depending on the resource used to compute similarity: WordNetbased methods and distributional methods.",
  "y": "extends"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_5",
  "x": "WordNet-based similarity: One of the models that we used is based on Resnik's similarity measure (1993), referring to it as res. The other model is an in-house method (<cite>Zapirain et al., 2009</cite> ), referred as <cite>wn</cite>, which only takes into account the depth of the most common ancestor, and returns SPs that are as specific as possible. Distributional similarity: Following (<cite>Zapirain et al., 2009</cite>) we considered both first order and second order similarity.",
  "y": "uses"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_6",
  "x": "When using SPs alone, we only use the headwords of the arguments, and each argument is classified independently of the rest. For each headword, we select the role (r) of the verb (c) which fits best the head word (w), where the goodness of fit (SP sim (v, r, w)) is modeled using one of the similarity models above, between the headword w and the headwords seen in training data for role r of verb v. This selection rule is formalized as follows: In <cite>our previous work</cite> (<cite>Zapirain et al., 2009</cite> ), we modelled SPs for pairs of predicates (verbs) and arguments, independently of the fact that the argument is a core argument (typically a noun) or an adjunct argument (typically a prepositional phrase).",
  "y": "background"
 },
 {
  "id": "9567cb276162a6e9d445f13f06f5a2_7",
  "x": "In <cite>our previous work</cite> (<cite>Zapirain et al., 2009</cite> ), we modelled SPs for pairs of predicates (verbs) and arguments, independently of the fact that the argument is a core argument (typically a noun) or an adjunct argument (typically a prepositional phrase). In contrast, (Litkowski and Hargraves, 2005) show that prepositions have SPs of their own, especially when functioning as adjuncts. We therefore decided to split SPs according to whether the potential argument is a Prepositional Phrase (PP) or a Noun Phrase (NP).",
  "y": "extends differences"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_0",
  "x": "Word alignment is at the basis of most statistical machine translation. The models that are generally used are often slow to train, and have a large number of parameters. <cite>Dyer et al. (2013)</cite> present a simple reparameterization of IBM Model 2 that is very fast to train, and achieves results similar to IBM Model 4.",
  "y": "background"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_1",
  "x": "However, using the reparameterization in<cite> (Dyer et al., 2013)</cite> would leave the model simple enough even with a relatively large amount of word classes. word class, and so can have different gradients for alignment probability over the english words. If the model has learned that prepositions and nouns are more likely to align to words later in the sentence, it could have a lower lambda for both word classes, resulting in a less steep slope.",
  "y": "background"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_2",
  "x": "We make use of a modified version of Model 2, from <cite>Dyer et al. (2013)</cite> , which has an alignment model that is parameterised in its original form solely on the variable \u03bb. Specifically, the probability of a sentence e given a sentence f is given as: here, m is the length of the target sentence e, n the same for source sentence f , \u03b4 is the alignment model and \u03b8 is the translation model.",
  "y": "extends"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_3",
  "x": "Here, \u03c9 can range from \u22121 to 1, and thus the calculation for the diagonal j \u2193 is clamped to be in a valid range for alignments. As the partition function (Z(\u00b7)) used in<cite> (Dyer et al., 2013)</cite> consists of 2 calculations for each target position i, one for above and one for below the diagonal, we can simply substitute \u03b3 for the geometric series calculations in order to use different parameters for each: where j \u2191 is j \u2193 + 1.",
  "y": "background"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_4",
  "x": "For obtaining this derivative, the arithmetico-geometric series (Fernandez et al., 2006) was originally used as an optimization, and for the gradient with respect to omega a geometric series should suffice, as an optimization, as there is no conditioning on the source words. This is not done in the current work however, so timing results will not be directly comparable to those found in<cite> (Dyer et al., 2013)</cite> . Conditioning on the POS of the target words then becomes as simple as using a different \u03bb, \u03b3, and \u03c9 for each POS tag in the input, and calculating a separate derivative for each of them, using only the derivatives at those target words that use the POS tag.",
  "y": "differences"
 },
 {
  "id": "9666fd26c7e9a02505ff26a687076d_5",
  "x": "The above described model is evaluated with experiments on a set of 3 language pairs, on which AER scores and BLEU scores are computed. We use similar corpora as used in<cite> (Dyer et al., 2013)</cite> : a French-English corpus made up of Europarl version 7 and news-commentary corpora, the ArabicEnglish parallel data consisting of the non-UN portions of the NIST training corpora, and the FBIS Chinese-English corpora. The models that are compared are the original reparameterization of Model 2, a version where \u03bb is split around the diagonal (split), one where pos tags are used, but \u03bb is not split around the diagonal (pos), one where an offset is used, but parameters aren't split about the diagonal (offset), one that's split about the diagonal and uses pos tags used as in<cite> (Dyer et al., 2013)</cite> , with stepsize for updates to \u03bb and \u03b3 during gradient ascent is 1000, and that for \u03c9 is 0.03, decaying after every gradient descent step by 0.9, using 8 steps every iteration.",
  "y": "similarities"
 },
 {
  "id": "9770f647c0b406462f4b941f136748_0",
  "x": "To my knowledge, the first researchers to propose an automatic procedure to determine the valence of words on the basis of corpora are Hatzivassiloglou and McKeown (1997) . Their algorithm aims to infer the semantic orientation of adjectives on the basis of an analysis of their co-occurrences with conjunctions. The main limitation of their algorithm is that it was developed specifically for adjectives and that the question of its application to other grammatical categories has not been solved <cite>(Turney & Littman, 2003)</cite> .",
  "y": "motivation"
 },
 {
  "id": "9770f647c0b406462f4b941f136748_1",
  "x": "---------------------------------- **SO-LSA** The technique proposed by<cite> Turney and Littman (2003)</cite> tries to infer semantic orientation from semantic association in a corpus.",
  "y": "background"
 },
 {
  "id": "9770f647c0b406462f4b941f136748_3",
  "x": "**METHOD** The two techniques described above were used in this experiment. The fourteen SO-LSA benchmarks chosen by<cite> Turney and Littman (2003)</cite> were translated into 2 Each sentence was automatically modified so as to replace the name and the description of the function of every individual by a generic first name of adequate sex (Mary, John, etc.) in order to prevent the judges being influenced by their prior positive or negative opinion about these people.",
  "y": "similarities"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_0",
  "x": "In particular, the technique proposed by <cite>Yuan et al. (2016)</cite> returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released. This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow). Our study showed that similar results can be obtained with much less data than hinted at by <cite>Yuan et al. (2016)</cite> .",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_1",
  "x": "LSTM-based language models have been shown effective in Word Sense Disambiguation (WSD). In particular, the technique proposed by <cite>Yuan et al. (2016)</cite> returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released. This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow).",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_2",
  "x": "This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow). Our study showed that similar results can be obtained with much less data than hinted at by <cite>Yuan et al. (2016)</cite> . Detailed analyses shed light on the strengths and weaknesses of this method.",
  "y": "differences"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_3",
  "x": "Among the best-performing ones is the approach by <cite>Yuan et al. (2016)</cite> , in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD. Even though the results obtained by <cite>Yuan et al. (2016)</cite> outperform the previous state-of-the-art, neither the used datasets nor the constructed models are available to the community. This is unfortunate because this makes the re-application of this technique a non-trivial process, and it hinders further studies for understanding which limitations prevent even higher accuracies.",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_4",
  "x": "Even though the results obtained by <cite>Yuan et al. (2016)</cite> outperform the previous state-of-the-art, neither the used datasets nor the constructed models are available to the community. This is unfortunate because this makes the re-application of this technique a non-trivial process, and it hinders further studies for understanding which limitations prevent even higher accuracies. These could be, for instance, of algorithmic nature or relate to the input (either size or quality), and a deeper understanding is crucial for enabling further improvements. In addition, some details are not reported, and this could prevent other attempts from replicating the results. To address these issues, we reimplemented <cite>Yuan et al. (2016)</cite> 's method with the goal of: 1) reproducing and making available the code, trained models, and results and 2) understanding which are the main factors that constitute the strengths and weaknesses of this method.",
  "y": "motivation"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_5",
  "x": "These could be, for instance, of algorithmic nature or relate to the input (either size or quality), and a deeper understanding is crucial for enabling further improvements. In addition, some details are not reported, and this could prevent other attempts from replicating the results. To address these issues, we reimplemented <cite>Yuan et al. (2016)</cite> 's method with the goal of: 1) reproducing and making available the code, trained models, and results and 2) understanding which are the main factors that constitute the strengths and weaknesses of this method.",
  "y": "extends"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_6",
  "x": "We anticipate some conclusions. First, a positive result is that we were able to reproduce the method from <cite>Yuan et al. (2016)</cite> and obtain similar results to the ones originally published. However, to our surprise, these results were obtained using a much smaller corpus of 1.8 billion tokens (Gigaword), which is less than 2% of the data used in the original study.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_7",
  "x": "First, a positive result is that we were able to reproduce the method from <cite>Yuan et al. (2016)</cite> and obtain similar results to the ones originally published. However, to our surprise, these results were obtained using a much smaller corpus of 1.8 billion tokens (Gigaword), which is less than 2% of the data used in the original study. In addition, we observe that the amount of unannotated data is important, but that the relationship between its size and the improvement is not linear, meaning that exponentially more unannotated data is needed in order to improve the performance.",
  "y": "differences"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_8",
  "x": "The work by <cite>Yuan et al. (2016)</cite> , which we consider in this paper, belongs to this last category. Different from Melamud et al. (2016) , it uses significantly more unannotated data, the model contains more hidden units (2048 vs. 600), and the sense assignment is more elaborated. We describe this approach in more detail in the following section.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_9",
  "x": "The method proposed by <cite>Yuan et al. (2016)</cite> performs WSD by annotating each lemma in a text with one WordNet synset that is associated with its meaning. Broadly speaking, the disambiguation is done by: 1) constructing a language model from a large unannotated dataset; 2) extracting sense embeddings from this model using a much smaller annotated dataset; 3) relying on the sense embeddings to make predictions on the lemmas in unseen sentences. Each operation is described below.",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_10",
  "x": "In <cite>Yuan et al. (2016)</cite> , the first operation consists of constructing an LSTM language model to capture the meaning of words in context. They use an LSTM network with a single hidden layer of h nodes. Given a sentence s = (w 1 , w 2 , . . . , w n ), they replace word w k (1 \u2264 k \u2264 n) by a special token $.",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_11",
  "x": "<cite>Yuan et al. (2016)</cite> argue that the averaging procedure is suboptimal because of two reasons. First, the distribution of occurrences of senses is unknown whereas averaging is only suitable for spherical clusters. Second, averaging reduces the representation of occurrences of each sense to a single vector and therefore ignores sense prior.",
  "y": "background"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_12",
  "x": "We leave the study of bigger corpora for future work. For the training of the sense embeddings, we use the same two corpora used by <cite>Yuan et al. (2016)</cite>: 1. SemCor (Miller et al., 1993 ) is a corpus containing approximately 240,000 sense annotated words. The tagged documents originate from the Brown corpus (Francis and Kucera, 1979) and cover various genres.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_13",
  "x": "Three scorers are used: \"framework\" refers to the WSD evaluation framework from Raganato et al. (2017a) ; \"mapping to WN3.0\" refers to the evaluation used by <cite>Yuan et al. (2016)</cite> while \"competition\" refers to the scorer provided by the competition itself (e.g., semeval2013). 1,644 test instances in total, which are all nouns. The application of the MFS baseline on this dataset yields an F 1 score of 63.0%.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_14",
  "x": "**RESULTS** In this section, we report our reproduction of the results of <cite>Yuan et al. (2016)</cite> and additional experiments to gain a deeper insight into the strengths and weaknesses of the approach. These experiments focus on the performance on the most-and less-frequent senses, coverage of the annotated dataset and the consequent impact on the overall predictions, the granularity of the sense representation, and the impact of the unannotated data and model complexity on the accuracy of WSD.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_15",
  "x": "Reproduction results. We trained the LSTM model with the best reported settings in <cite>Yuan et al. (2016)</cite> (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs. During our training, one epoch took about one day to finish with TensorFlow fully utilizing one GPU.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_16",
  "x": "Table 1 presents the results using the test sets senseval2 and semeval2013, respectively. The top part of the table presents our reproduction results, the middle part reports the results from <cite>Yuan et al. (2016)</cite> , while the bottom part reports a representative sample of the other state-of-the-art approaches. It should be noted that with the test set semeval2013, all scorers use WordNet 3.0, therefore the performance of the various methods can be directly compared.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_17",
  "x": "Table 1 presents the results using the test sets senseval2 and semeval2013, respectively. The top part of the table presents our reproduction results, the middle part reports the results from <cite>Yuan et al. (2016)</cite> , while the bottom part reports a representative sample of the other state-of-the-art approaches. It should be noted that with the test set semeval2013, all scorers use WordNet 3.0, therefore the performance of the various methods can be directly compared.",
  "y": "similarities"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_19",
  "x": "When we use both SemCor and OMSTI for the annotated data, our results drop 0.02 point for senseval2, whereas they increase by almost 0.01 for semeval2013. Different from <cite>Yuan et al. (2016)</cite>, we did not observe improvement by using label propagation (comparing T: SemCor, U: OMSTI against T:SemCor without propagation). However, the performance of the label propagation strategy is still competitive on both test sets.",
  "y": "differences"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_20",
  "x": "When we use both SemCor and OMSTI for the annotated data, our results drop 0.02 point for senseval2, whereas they increase by almost 0.01 for semeval2013. Different from <cite>Yuan et al. (2016)</cite>, we did not observe improvement by using label propagation (comparing T: SemCor, U: OMSTI against T:SemCor without propagation). However, the performance of the label propagation strategy is still competitive on both test sets.",
  "y": "similarities"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_21",
  "x": "The first type of instances are the ones for which the correct link is the most-frequent sense, whereas the second subset consists of the remaining ones. This analysis is important because it is well-known that the simple strategy of always choosing the MFS is a strong baseline in WSD, thus there is a tendency for WSD systems to overfit towards the MFS (Postma et al., 2016) . Table 2 shows that the method by <cite>Yuan et al. (2016)</cite> does not overfit towards the MFS to the same extent as other supervised systems since the recall on LFS instances is still quite high 0.41 (a lower recall on LFS instances than on MFS ones is expected due to the reduced training data for them).",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_23",
  "x": "The data points at 100 billion (10 11 ) tokens correspond to <cite>Yuan et al. (2016)</cite> 's reported results. As might be expected, a bigger corpus leads to more meaningful context vectors and therefore higher performance on WSD. However, the amount of data needed for 1% of improvement in F 1 grows exponentially fast (notice that the horizontal axis is in log scale).",
  "y": "uses similarities"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_24",
  "x": "**CONCLUSIONS** This paper reports the results of a reproduction study of the model proposed by <cite>Yuan et al. (2016)</cite> and an additional analysis to gain a deeper understanding of the impact of various factors on its performance. A number of interesting conclusions can be drawn from our results.",
  "y": "uses"
 },
 {
  "id": "97f8d0af85eda3e453fc4fb00819f0_25",
  "x": "First, we observed that we do not need a very large unannotated dataset to achieve state-of-the-art all-words WSD performance since we used the Gigaword corpus, which is two orders of magnitude smaller than <cite>Yuan et al. (2016)</cite> 's proprietary corpus, and got similar performance on senseval2 and semeval2013. A more detailed analysis hints that adding more unannotated data and increasing model capacity are subject to diminishing returns. Moreover, we observed that this approach has a more balanced sense assignment than other techniques, as shown by the relatively good performance on less-frequent-sense instances.",
  "y": "differences"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_0",
  "x": "Nothing is assumed about each document except that it was written by a single author. Koppel et al. (2011) formulated this problem in a paper focused on clustering five books from the Hebrew Bible. They also consider a 'multi-author document' version of the problem: decomposing sentences from a single composite document generated by merging randomly sampled chunks of text from k authors. Akiva and Koppel (2013) followed that work with an expanded method, and<cite> Aldebei et al. (2015)</cite> have since presented an improved technique in the 'multi-author document' context by exploiting posterior probabilities of a Naive-Bayesian Model.",
  "y": "background"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_1",
  "x": "In this paper, we argue that the biblical clustering done by Koppel et al. (2011) and by<cite> Aldebei et al. (2015)</cite> do not represent a grouping around true authorship within the Bible, but rather around common topics or shared style. We demonstrate a general technique that can accurately discern multiple authors contained within the Books of Ezekiel and Jeremiah. Prior work assumes that each prophetic book reflects a single source, and does not consider the consensus among modern biblical scholars that the books of Ezekiel and Jeremiah were written by multiple individuals.",
  "y": "differences"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_2",
  "x": "Both Zheng et al. (2006) and Layton et al. (2013) propose that syntactic feature sets are reliable predictors for authorial attribution, and Tschuggnall and Specht (2014) demonstrates, with modest success, authorial decomposition using pq-grams extracted from sentences' syntax trees. We found that by combining the feature set of POS n-grams with a clustering approach similar to the one presented by Akiva (2013) , our method of decomposition attains higher accuracy than Tschuggnall's method, which also considers grammatical style. Additionally, in cases where authors are rhetorically similar, our framework outperforms techniques outlined by Akiva (2013) and <cite>Aldebei (2015)</cite> , which both rely on word occurrences as features.",
  "y": "differences"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_3",
  "x": "**CLARIFYING DETAILS WITH NYT COLUMNS** We shall describe a clustering of New York Times columns to clarify our framework. The NYT cor- pus is used both because the author of each document is known with certainty and because it is a canonical dataset that has served as a benchmark for both Akiva and Koppel (2013) and<cite> Aldebei et al. (2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "98d8ea63896cc80f0989130e7cbbf1_4",
  "x": "Collecting 27 speeches from President Obama and 20 from Senator McCain, we expected our technique to excel in this context. We found instead that our method performed exceptionally poorly, clustering these speeches with only 74.2% accuracy. Indeed, we were further surprised to discover that by adjusting our framework to be similar to that presented in Akiva and Koppel (2013) and<cite> Aldebei et al. (2015)</cite> -by replacing POS n-grams with ordinary word occurrences in step one -our framework performed very well, clustering at 95.3%.",
  "y": "similarities"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_0",
  "x": "Abuse can also have different facets. [10] released one of the initial data sets from Twitter with the goal of identifying what constitutes racism and sexism. <cite>[9]</cite> in <cite>their work</cite> pointed out that hate speech is different from offensive language and released a data set of 25k tweets with the goal of distinguishing hate speech from offensive language.",
  "y": "background"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_1",
  "x": "Existing approaches to abusive text detection can be broadly divided into two categories: 1) Feature intensive machine learning algorithms such as Logistic Regression (LR), Multilayer Perceptron (MLP) and etc. 2) Deep Learning models which learn feature representations on their own. [10] released the popular data set of 16k tweets annotated as belonging to sexism, racism or none class 1 , and provided a feature engineered model for detection of abuse in their corpus. <cite>[9]</cite> use a similar handcrafted feature engineered model to identify offensive language and distinguish it from hate speech.",
  "y": "background"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_3",
  "x": "We have used the 3 benchmark data sets for abusive content detection on Twitter. At the time of the experiment, the [10] data set had a total of 15,844 tweets out of which 1,924 were labelled as belonging to racism, 3,058 as sexism and 10,862 as none. The <cite>[9]</cite> data set had a total of 25,112 tweets out of which 1498 were labelled as hate speech, 19,326 as offensive language and 4,288 as neither.",
  "y": "uses"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_4",
  "x": "We have used the 3 benchmark data sets for abusive content detection on Twitter. At the time of the experiment, the [10] data set had a total of 15,844 tweets out of which 1,924 were labelled as belonging to racism, 3,058 as sexism and 10,862 as none. The <cite>[9]</cite> data set had a total of 25,112 tweets out of which 1498 were labelled as hate speech, 19,326 as offensive language and 4,288 as neither. For the [4] data set, there were 20,362 tweets out of which 5,235 were positive harassment examples and 15,127 were negative. We call [10] data set as D1 , <cite>[9]</cite> data set as <cite>D2</cite> and [4] as D3 For tweet tokenization, we use Ekphrasis which is a text processing tool built specially from social platforms such as Twitter.",
  "y": "uses"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_5",
  "x": "The network is trained at a learning rate of 0.001 for 10 epochs, with a dropout of 0.2 to prevent over-fitting. The results are averaged over 10-fold cross-validations for D1 and D3 and 5 fold cross-validations for <cite>D2</cite> because <cite>[9]</cite> reported results using 5 fold CV. Because of class imbalance in all our data sets, we report weighted F1 scores.",
  "y": "motivation"
 },
 {
  "id": "9a8b29b10539be9e6c65172a97b16f_6",
  "x": "On closer investigation we find that most cases where our model fails are instances where annotation is either noisy or the difference between classes are very blurred and subtle. The first tweet is a tweet from [10] , the second tweet is a tweet from from <cite>[9]</cite> data set and the third from the [4] ----------------------------------",
  "y": "differences"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_0",
  "x": "The paper presents an application of Structural Correspondence Learning (SCL)<cite> (Blitzer et al., 2006)</cite> for domain adaptation of a stochastic attribute-value grammar (SAVG). So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; ). An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_1",
  "x": "So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; ). An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions. We report on our exploration of applying SCL to adapt a syntactic disambiguation model and show promising initial results on Wikipedia domains.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_2",
  "x": "The need for domain adaptation arises in many NLP tasks: Part-of-Speech tagging, Sentiment Analysis, Semantic Role Labeling or Statistical Parsing, to name but a few. For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001 ). The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daum\u00e9 III and Marcu, 2006; Daum\u00e9 III, 2007;<cite> Blitzer et al., 2006</cite>; McClosky et al., 2006; .",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_3",
  "x": "In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daum\u00e9 III, 2007) , besides the labeled source data, we have access to a comparably small, but labeled amount of target data. In contrast, semi-supervised domain adaptation<cite> (Blitzer et al., 2006</cite>; McClosky et al., 2006; is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain data. Semi-supervised adaptation is a much more realistic situation, while at the same time also considerably more difficult.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_5",
  "x": "While several authors have looked at the supervised adaptation case, there are less (and especially less successful) studies on semi-supervised domain adaptation (McClosky et al., 2006;<cite> Blitzer et al., 2006</cite>; . Of these, McClosky et al. (2006) deal specifically with selftraining for data-driven statistical parsing.",
  "y": "motivation background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_6",
  "x": "They show that together with a re-ranker, improvements are obtained. Similarly, Structural Correspondence Learning<cite> (Blitzer et al., 2006</cite>; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification. In contrast, report on \"frustrating\" results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. \"no team was able to improve target domain performance substantially over a state of the art baseline\".",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_7",
  "x": "The few studies on adapting disambiguation models (Hara et al., 2005; Plank and van Noord, 2008) have focused exclusively on the supervised scenario. Therefore, the direction we explore in this study is semi-supervised domain adaptation for parse disambiguation. We examine the effectiveness of Structural Correspondence Learning (SCL)<cite> (Blitzer et al., 2006)</cite> for this task, a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_8",
  "x": "(Structural Correspondence Learning)<cite> (Blitzer et al., 2006</cite>; Blitzer, 2008 ) is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains. Before describing the algorithm in detail, let us illustrate the intuition behind SCL with an example, borrowed from . Suppose we have a Sentiment Analysis system trained on book reviews (domain A), and we would like to adapt it to kitchen appliances (domain B).",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_9",
  "x": "Features such as \"boring\" and \"repetitive\" are common ways to express negative sentiment in A, while \"not working\" or \"defective\" are specific to B. If there are features across the domains, e.g. \"don't buy\", with which the domain specific features are highly correlated with, then we might tentatively align those features. Therefore, the key idea of SCL is to identify automatically correspondences among features from different domains by modeling their correlations with pivot features. Pivots are features occurring frequently and behaving similarly in both domains<cite> (Blitzer et al., 2006)</cite> .",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_10",
  "x": "They are inspired by auxiliary problems from Ando and Zhang (2005) . Non-pivot features that correspond with many of the same pivot-features are assumed to correspond. Intuitively, if we are able to find good correspondences among features, then the augmented labeled source domain data should transfer better to a target domain (where no labeled data is available)<cite> (Blitzer et al., 2006)</cite> .",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_11",
  "x": "Step 3 is to arrange the m weight vectors in a matrix W , where a column corresponds to a pivot predictor weight vector. Applying the projection W T x (where x is a training instance) would give us m new features, however, for \"both computational and statistical reasons\"<cite> (Blitzer et al., 2006</cite>; Ando and Zhang, 2005 ) a low-dimensional approximation of the original feature space is computed by applying Singular Value Decomposition (SVD) on W (step 4). Let \u03b8 = U T h\u00d7n be the top h left singular vectors of W (with h a dimension parameter and n the number of non-pivot features).",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_12",
  "x": "So far, pivot features on the word level were used<cite> (Blitzer et al., 2006</cite>; Blitzer, 2008) , e.g. \"Does the bigram not buy occur in this document?\" (Blitzer, 2008) . Pivot features are the key ingredient for SCL, and they should align well with the NLP task. For PoS tagging and Sentiment Analysis, features on the word level are intuitively well-related to the problem at hand.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_13",
  "x": "In all our experiments, we set t = 5000. In this way we obtained on average 360 pivot features, on the datasets described in Section 5. Predictive features As pointed out by<cite> Blitzer et al. (2006)</cite> , each instance will actually contain features which are totally predictive of the pivot features (i.e. the pivot itself).",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_14",
  "x": "In our case, we additionally have to pay attention to 'more specific' features, e.g. r2 is a feature that extends r1, in the sense that it incorporates more information than its parent (i.e. which grammar rules applied in the construction of daughter nodes). It is crucial to remove these predictive features when creating the training data for the pivot predictors. Following<cite> Blitzer et al. (2006)</cite> (which follow Ando and Zhang (2005)), we only use positive entries in the pivot predictors weight vectors to compute the SVD.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_15",
  "x": "**FURTHER PRACTICAL ISSUES OF SCL** In practice, there are more free parameters and model choices (Ando and Zhang, 2005; Ando, 2006;<cite> Blitzer et al., 2006</cite>; Blitzer, 2008) besides the ones discussed above. Feature normalization and feature scaling.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_16",
  "x": "They then rescaled the features by a factor \u03b1 found on heldout data: \u03b1\u03b8x. Restricted Regularization. When training the supervised model on the augmented feature space x, \u03b8x ,<cite> Blitzer et al. (2006)</cite> only regularize the weight vector of the original features, but not the one for the new low-dimensional features.",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_17",
  "x": "Due to the positive results in Ando (2006),<cite> Blitzer et al. (2006)</cite> include this in their standard setting of SCL and report results using block SVDs only. ---------------------------------- **EXPERIMENTS AND RESULTS**",
  "y": "background"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_18",
  "x": "Extract all pages that are related to p (through sharing a direct, sub or super category) 3. Optionally, filter out certain pages In our empirical setup, we followed<cite> Blitzer et al. (2006)</cite> and tried to balance the size of source and target data.",
  "y": "uses"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_19",
  "x": "Thus, our first instantiation of SCL for parse disambiguation indeed shows promising results. We can confirm that changing the dimensionality parameter h has rather little effect (Table 4) , which is in line with previous findings (Ando and Zhang, 2005;<cite> Blitzer et al., 2006)</cite> . Thus we might fix the parameter and prefer smaller dimensionalities, which saves space and time.",
  "y": "similarities"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_20",
  "x": "We also tested feature normalization (as described in Section 4.2). While<cite> Blitzer et al. (2006)</cite> found it necessary to normalize (and scale) the projection features, we did not observe any improvement by normalizing them (actually, it slightly degraded performance in our case). Thus, we found this step unnecessary, and currently did not look at this issue any further.",
  "y": "differences"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_21",
  "x": "biguation. While SCL has been successfully applied to PoS tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; , its effectiveness for parsing was rather unexplored. The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results, even without the many additional extensions on the feature level as done in<cite> Blitzer et al. (2006)</cite> .",
  "y": "background motivation"
 },
 {
  "id": "9bd974b0fa4732f361f1c397e6b4c7_22",
  "x": "biguation. While SCL has been successfully applied to PoS tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; , its effectiveness for parsing was rather unexplored. The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results, even without the many additional extensions on the feature level as done in<cite> Blitzer et al. (2006)</cite> .",
  "y": "differences"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_0",
  "x": "Previous work on paraphrase generation that used these datasets (Wang et al., 2019;<cite> Gupta et al., 2018</cite>; Li et al., 1 https://data.quora.com/ First-Quora-Dataset-Release-Question-Pairs 2018; Prakash et al., 2016) chose BLEU (Papineni et al., 2002) , METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006) as evaluation metrics. In this paper, we find that simply using the input sentence as output in an unsupervised manner (i.e. fully parroting the input) significantly outperforms the state-of-the-art on two metrics for TWITTER, and on one metric for QUORA. Even after changing part of the input sentence (i.e. partially parroting the input), state-of-the-art metric scores can still be surpassed.",
  "y": "background"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_1",
  "x": "<cite>Gupta et al. (2018)</cite> sampled 4K sentences as their test set, but did not specify which sentences they used. Li et al.(2018) sampled 30K sentences as their test set, also not specifying which sentences they used. To avoid selecting a subset of data that is biased in favor of our method, we perform evaluation on the entire QUORA dataset.",
  "y": "background"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_2",
  "x": "MSCOCO. This is an image captioning dataset, with multiple captions provided for a single image (Lin et al., 2014) . There have been multiple works which use it as a paraphrase generation dataset by treating captions of the same image as paraphrases (Wang et al., 2019;<cite> Gupta et al., 2018</cite>; Prakash et al., 2016) .",
  "y": "background"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_3",
  "x": "There have been multiple works which use it as a paraphrase generation dataset by treating captions of the same image as paraphrases (Wang et al., 2019;<cite> Gupta et al., 2018</cite>; Prakash et al., 2016) . The training and testing sets are available, containing 331,163 and 162,016 input sentences respectively. However, relevance scores for captions of the same image score only 3.38 out of 5 under human evaluation (in contrast, the score is 4.82 for QUORA)<cite> (Gupta et al., 2018)</cite> , due to the fact that different captions for the same image often vary in the semantic information conveyed.",
  "y": "background"
 },
 {
  "id": "9c8c5da4cdd13efb187690e7d3aa20_4",
  "x": "Furthermore, the score deviation between different samples is small. Consequently, although the exact test sets used by<cite> (Gupta et al., 2018)</cite> and (Li et al., 2018) are not available, it is logical to assume that parroting performance would still exceed or be on par with the state-of-the-art on those test sets. Partial parroting.",
  "y": "future_work"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_0",
  "x": "The importance of concepts is implemented by assigning weights w i to each concept i with binary variable c i , yielding the following coverage maximization objective, subject to the appropriate constraints: In proposing bigrams as concepts for their system,<cite> Gillick and Favre (2009)</cite> explain that: [c]oncepts could be words, named entities, syntactic subtrees or semantic relations, for example.",
  "y": "background"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_1",
  "x": "While deeper semantics make more appealing concepts, their extraction and weighting are much more error-prone. Any error in concept extraction can result in a biased objective function, leading to poor sentence selection. (Gillick and Favre, 2009) Several authors, e.g., Woodsend and Lapata (2012) , and Li et al. (2013) , have followed<cite> Gillick and Favre (2009)</cite> in assuming that bigrams would lead to better practical performance than more syntactic or semantic concepts, even though bigrams serve as only an approximation of these.",
  "y": "background"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_2",
  "x": "Concepts are weighted by the number of times they appear in a document. Moreover, due the NP-hardness of coverage maximization, for an exact solution to the concept coverage optimization problem, we resort to fast solvers for integer linear programming, under some appropriate constraints. Bigrams.<cite> Gillick and Favre (2009)</cite> proposed to use bigrams as concepts, and to weight their contribution to the objective function in Equation (1) by the frequency with which they occur in the document.",
  "y": "background"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_4",
  "x": "---------------------------------- **BASELINE AND SYSTEMS** Our baseline is the bigram-based extraction summarization system of<cite> Gillick and Favre (2009)</cite> , icsisumm 7 .",
  "y": "uses"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_5",
  "x": "Concept count cut-off is the minimum frequency of concepts from the document (set) that qualifies them for consideration in coverage maximization. For bigrams of the original system on TAC08, there are two types of document sets: 'A' and 'B'. For 'A' type documents,<cite> Gillick and Favre (2009)</cite> set this threshold to 3 and for 'B' type documents, they set this to 4.",
  "y": "background"
 },
 {
  "id": "9d1699d4ca3b4026ed5aab125a737d_6",
  "x": "For single concept systems, the results are shown in Table 1 , and concept combination system results are given in Table 2 . We first note that our runs of the current distribution of icsisumm yield significantly worse ROUGE-2 results than reported in<cite> (Gillick and Favre, 2009</cite> ) (see Table 1 , BIGRAMS): 0.081 compared to 0.110 respectively. On the TAC08 data, we observe no improvements over the baseline BIGRAM system for any ROUGE metric here.",
  "y": "differences"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_0",
  "x": "The purpose of cross-genre experiment is to see whether the model can work robustly across genres. Table 4 shows cross-genre experimental results of our neural network models on the training set of MASC+Wiki by treating each genre as one crossvalidation fold. As we expected, both the macroaverage F1-score and class-wise F1 scores are lower compared with the results in Table 2 where in-genre data were used for model training as well. But the performance drop on the paragraph-level models is little, which clearly outperform the previous system<cite> (Friedrich et al., 2016)</cite> and the baseline model by a large margin.",
  "y": "differences"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_1",
  "x": "We aim to categorize a clause based on its aspectual property and more specifically, based on the type of Situation Entity (SE) 1 (e.g., events, states, generalizing statements and generic statements) the clause introduces to the discourse, following the recent work by<cite> (Friedrich et al., 2016)</cite> . Understanding SE types of clauses is beneficial for many NLP tasks, including discourse mode identi-fication 2 (Smith, 2003 (Smith, , 2005 ), text summarization, information extraction and question answering. The situation entity type of a clause reflects discourse roles the clause plays in a paragraph and discourse role interpretation depends heavily on paragraph-wide contexts.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_2",
  "x": "Understanding SE types of clauses is beneficial for many NLP tasks, including discourse mode identi-fication 2 (Smith, 2003 (Smith, , 2005 ), text summarization, information extraction and question answering. The situation entity type of a clause reflects discourse roles the clause plays in a paragraph and discourse role interpretation depends heavily on paragraph-wide contexts. Recently, <cite>Friedrich et al. (2016)</cite> used insightful syntactic-semantic features extracted from the target clause itself for SE type classification, which has achieved good performance across several genres when evaluated on the newly created large dataset MASC+Wiki.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_3",
  "x": "In addition, <cite>Friedrich et al. (2016)</cite> implemented a sequence labeling model with conditional random fields (CRF) (Lafferty et al., 2001 ) for finetuning a sequence of predicted SE types. However, other than leveraging common SE label patterns (e.g., GENERIC clauses tend to cluster together.), this approach largely ignored the wider contexts a clause appears in when predicting its SE type. To further improve the performance and robustness of situation entity type classification, we argue that we should consider influences of wider contexts more extensively, not only by fine-tuning a sequence of SE type predictions, but also in deriving clause representations and obtaining precise individual SE type predictions.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_4",
  "x": "Recently, <cite>Friedrich et al. (2016)</cite> used insightful syntactic-semantic features extracted from the target clause itself for SE type classification, which has achieved good performance across several genres when evaluated on the newly created large dataset MASC+Wiki. In addition, <cite>Friedrich et al. (2016)</cite> implemented a sequence labeling model with conditional random fields (CRF) (Lafferty et al., 2001 ) for finetuning a sequence of predicted SE types. However, other than leveraging common SE label patterns (e.g., GENERIC clauses tend to cluster together.), this approach largely ignored the wider contexts a clause appears in when predicting its SE type.",
  "y": "motivation background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_5",
  "x": "In order to further improve SE type classification performance, we also add an extra CRF layer at the top of our paragraph-level model to fine-tune a sequence of SE type predictions over clauses<cite> (Friedrich et al., 2016)</cite> , which however is not our contribution. Experimental results show that our paragraphlevel neural network model greatly improves the performance of SE type classification on the same MASC+Wiki<cite> (Friedrich et al., 2016)</cite> corpus and achieves robust performance close to human level. In addition, the CRF layer further improves the SE type classification results, but by a small margin.",
  "y": "differences"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_6",
  "x": "In order to further improve SE type classification performance, we also add an extra CRF layer at the top of our paragraph-level model to fine-tune a sequence of SE type predictions over clauses<cite> (Friedrich et al., 2016)</cite> , which however is not our contribution. Experimental results show that our paragraphlevel neural network model greatly improves the performance of SE type classification on the same MASC+Wiki<cite> (Friedrich et al., 2016)</cite> corpus and achieves robust performance close to human level. In addition, the CRF layer further improves the SE type classification results, but by a small margin.",
  "y": "extends"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_7",
  "x": "The situation entity types annotated in the MASC+Wiki corpus<cite> (Friedrich et al., 2016)</cite> were initially introduced by Smith (2003) , which were then extended by (Palmer et al., 2007; Friedrich and Palmer, 2014b) . The situation entity types can be divided into the following broad categories: \u2022 Eventualities (EVENT, STATE and RE-PORT): for clauses representing actual happenings and world states.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_8",
  "x": "Palmer et al. (2007) first implemented a maximum entropy model for SE type classification relying on words, POS tags and some linguistic cues as main features. This work used a relatively small dataset (around 4300 clauses) and did not achieve satisfied performance (around 50% of accuracy). To bridge the gap, <cite>Friedrich et al. (2016)</cite> created a much larger dataset MASC+Wiki (more than 40,000 clauses) and achieved better SE type classification performance (around 75% accuracy) by using rich features extracted from the target clause.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_9",
  "x": "<cite>Friedrich et al. (2016)</cite> further improved the performance by implementing a sequence labeling (CRF) model to fine-tune a sequence of SE type predictions and noted that much of the performance gain came from modeling the label pattern that GENERIC clauses often occur together. In contrast, we focus on deriving dynamic clause representations informed by paragraph-level contexts and model context influences more extensively. Becker et al. (2017) proposed a GRU based neural network model that predicts the SE type for one clause each time, by encoding the content of the target clause using a GRU and incorporating several sources of context information, including contents and labels of preceding clauses as well as genre information, using additional separate GRUs (Chung et al., 2014) .",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_10",
  "x": "<cite>Friedrich et al. (2016)</cite> further improved the performance by implementing a sequence labeling (CRF) model to fine-tune a sequence of SE type predictions and noted that much of the performance gain came from modeling the label pattern that GENERIC clauses often occur together. In contrast, we focus on deriving dynamic clause representations informed by paragraph-level contexts and model context influences more extensively. Becker et al. (2017) proposed a GRU based neural network model that predicts the SE type for one clause each time, by encoding the content of the target clause using a GRU and incorporating several sources of context information, including contents and labels of preceding clauses as well as genre information, using additional separate GRUs (Chung et al., 2014) .",
  "y": "differences background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_11",
  "x": "**FINE-TUNE SITUATION ENTITY PREDICTIONS WITH A CRF LAYER** Previous studies (Friedrich et al., 2016; Becker et al., 2017) show that there exist common SE label patterns between adjacent clauses. For example, <cite>Friedrich et al. (2016)</cite> reported the fact that GENERIC sentences usually occur together in a paragraph.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_12",
  "x": "Following<cite> (Friedrich et al., 2016)</cite> , in order to capture SE label patterns in our hierarchical recurrent neural network model, we add a CRF layer at the top of the softmax prediction layer (shown in figure 2 ) to fine-tune predicted situation entity types. The CRF layer will update a state-transition matrix, which can effectively adjust the current label depending on its preceding and following labels. Both the training and decoding procedures of the CRF layer can be conducted efficiently using the Viterbi algorithm.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_13",
  "x": "---------------------------------- **DATASET AND PREPROCESSING** The MASC+Wiki Corpus: We evaluated our neural network model on the MASC+Wiki corpus 7<cite> (Friedrich et al., 2016)</cite> (Friedrich et al., 2016; Becker et al., 2017) , we used the same 80:20 traintest split with balanced genre distributions.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_14",
  "x": "The MASC+Wiki Corpus: We evaluated our neural network model on the MASC+Wiki corpus 7<cite> (Friedrich et al., 2016)</cite> (Friedrich et al., 2016; Becker et al., 2017) , we used the same 80:20 traintest split with balanced genre distributions. Preprocessing: As described in<cite> (Friedrich et al., 2016)</cite> , texts were split into clauses using SPADE (Soricut and Marcu, 2003) . There are 4,784 paragraphs in total in the corpus; and on average, each paragraph contains 9.6 clauses.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_17",
  "x": "Following the previous work<cite> (Friedrich et al., 2016)</cite> on the same task and dataset, we report accuracy and macro-average F1-score across SE types on the test set of MASC+Wiki. The first section of Table 3 shows the results of the previous works. The second section shows the result of our implemented clause-level Bi-LSTM baseline, which already outperforms the previous best model.",
  "y": "uses"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_18",
  "x": "We noticed that the previous work<cite> (Friedrich et al., 2016)</cite> did not publish the class-wise performance of their model on the test set, instead, they reported the detailed performance on the training set using 10-fold cross-validation. For direct comparisons, we also report our 10-fold cross-validation results 8 on the training set of MASC+Wiki. Table 2 reports the cross-validation classification results.",
  "y": "background"
 },
 {
  "id": "9e7f3943d8f8aff2eb91c291cd020e_19",
  "x": "We noticed that the previous work<cite> (Friedrich et al., 2016)</cite> did not publish the class-wise performance of their model on the test set, instead, they reported the detailed performance on the training set using 10-fold cross-validation. For direct comparisons, we also report our 10-fold cross-validation results 8 on the training set of MASC+Wiki. Table 2 reports the cross-validation classification results.",
  "y": "uses"
 },
 {
  "id": "a01a6ab7cf13c7916b1b3823a4b4de_0",
  "x": "In order to effectively incorporate the otherlanguage data, we apply SVM re-ranking in a manner that has previously been shown to provide significant improvement for grapheme-to-phoneme conversion (Bhargava and Kondrak, 2011) . This method is flexible enough to incorporate multiple languages; it employs features based on character alignments between potential outputs and existing transliterations from other languages, as well as scores of these alignments, which serve as a measure of similarity. We apply this approach on top of the same DIRECTL+ system as submitted last year <cite>(Jiampojamarn et al., 2010b)</cite> for English-to-Hindi machine transliteration. Compared to the base DI-RECTL+ performance, we are able to achieve significantly better results, with a relative performance increase of over 10%.",
  "y": "background"
 },
 {
  "id": "a01a6ab7cf13c7916b1b3823a4b4de_1",
  "x": "Our principal base system that generates the n-best output lists is DIRECTL+, which has produced excellent results in the NEWS 2010 Shared Task on Transliteration <cite>(Jiampojamarn et al., 2010b)</cite> . For re-ranking, note that training a re-ranker requires training data where the base system scores are representative of unseen data so that the re-ranker does not simply learn to follow the base system; we therefore split the training data into ten folds and perform a sort-of cross validation with DIRECTL+. This provides us with usable training data for reranking.",
  "y": "uses background"
 },
 {
  "id": "a01a6ab7cf13c7916b1b3823a4b4de_2",
  "x": "This provides us with usable training data for reranking. We tune the SVM's hyperparameter based on performance on the provided development data, and use the best DIRECTL+ settings established in the NEWS 2010 Shared Task <cite>(Jiampojamarn et al., 2010b)</cite> . Armed with optimal parameter settings, we combine the training and development data into a single set used to train our final DIRECTL+ system.",
  "y": "extends"
 },
 {
  "id": "a01a6ab7cf13c7916b1b3823a4b4de_4",
  "x": "**PREVIOUS WORK** There are three lines of research that are relevant to the work we have presented in this paper: (1) DI-RECTL+ and SEQUITUR for machine transliteration; (2) applying multiple languages; and (3) system combination. For the NEWS 2009 and 2010 Shared Tasks, the discriminative DIRECTL+ system that incorporates many-to-many alignments, online maxmargin training and a phrasal decoder was shown to function well as a general string transduction tool; while originally designed for grapheme-tophoneme conversion, it produced excellent results for machine transliteration (Jiampojamarn et al., 2009;<cite> Jiampojamarn et al., 2010b)</cite> , leading us to re-use it here.",
  "y": "uses background"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_0",
  "x": "---------------------------------- **RELATED WORK** Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) , or the prediction of the compositionality of MWE types (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) .",
  "y": "background"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_1",
  "x": "The prediction of the compositionality of MWE types has traditionally been couched as a binary classification task (compositional or non-compositional: Baldwin et al. (2003) , Bannard (2006) ), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . In either case, the modelling has been done either over the whole MWE (Reddy et al., 2011;<cite> Salehi and Cook, 2013)</cite> , or relative to each component within the MWE (Baldwin et al., 2003; Bannard, 2006) . In this paper, we focus on the binary classification of MWE types relative to each component of the",
  "y": "background"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_2",
  "x": "The prediction of the compositionality of MWE types has traditionally been couched as a binary classification task (compositional or non-compositional: Baldwin et al. (2003) , Bannard (2006) ), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale (Reddy et al., 2011;<cite> Salehi and Cook, 2013</cite>; Salehi et al., 2014) . In either case, the modelling has been done either over the whole MWE (Reddy et al., 2011;<cite> Salehi and Cook, 2013)</cite> , or relative to each component within the MWE (Baldwin et al., 2003; Bannard, 2006) . In this paper, we focus on the binary classification of MWE types relative to each component of the",
  "y": "background"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_3",
  "x": "The work that is perhaps most closely related to this paper is that of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , who use translation data to predict the compositionality of a given MWE relative to each of its components, and then combine those scores to derive an overall compositionality score. In both cases, translations of the MWE and its components are sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014) , and if there is greater similarity between the translated components and MWE in a range of languages, the MWE is predicted to be more compositional. The basis of the similarity calculation is unsupervised, using either string similarity<cite> (Salehi and Cook, 2013)</cite> or distributional similarity (Salehi et al., 2014) .",
  "y": "similarities"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_4",
  "x": "In both cases, translations of the MWE and its components are sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014) , and if there is greater similarity between the translated components and MWE in a range of languages, the MWE is predicted to be more compositional. The basis of the similarity calculation is unsupervised, using either string similarity<cite> (Salehi and Cook, 2013)</cite> or distributional similarity (Salehi et al., 2014) . However, the overall method is supervised, as training data is used to select the languages to aggregate scores across for a given MWE construction.",
  "y": "uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_5",
  "x": "To benchmark our method, we use two of the same datasets as these two papers, and repurpose the best-performing methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) for classification of the compositionality of each MWE component. ---------------------------------- **METHODOLOGY**",
  "y": "uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_6",
  "x": "Our approach is to take whatever translations happen to exist in Wiktionary for a given MWE, and where there are translations in that language for the component of interest, use the LCSbased method of<cite> Salehi and Cook (2013)</cite> to measure the string similarity between the translation of the MWE and the translation of the components. Unlike<cite> Salehi and Cook (2013)</cite> , however, we do not use development data to select the optimal set of languages in a supervised manner, and instead simply take the average of the string similarity scores across the available languages. In the case of more than one translation in a given language, we use the maximum string similarity for each pairing of MWE and component translation.",
  "y": "uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_7",
  "x": "Our approach is to take whatever translations happen to exist in Wiktionary for a given MWE, and where there are translations in that language for the component of interest, use the LCSbased method of<cite> Salehi and Cook (2013)</cite> to measure the string similarity between the translation of the MWE and the translation of the components. Unlike<cite> Salehi and Cook (2013)</cite> , however, we do not use development data to select the optimal set of languages in a supervised manner, and instead simply take the average of the string similarity scores across the available languages. In the case of more than one translation in a given language, we use the maximum string similarity for each pairing of MWE and component translation.",
  "y": "differences"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_8",
  "x": "As mentioned above, we evaluate our method over the same two datasets as<cite> Salehi and Cook (2013)</cite> (which were later used, in addition to a third dataset of German noun compounds, in Salehi et al. (2014) ): (1) 90 binary English noun compounds (ENCs, e.g. spelling bee or swimming pool); and (2) 160 English verb particle constructions (EVPCs, e.g. stand up and give away). Our results are not directly comparable with those of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , however, who evaluated in terms of a regression task, modelling the overall compositionality of the MWE. In our case, the task setup is a binary classification task relative to each of the two components of the MWE.",
  "y": "similarities"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_9",
  "x": "**DATASETS** As mentioned above, we evaluate our method over the same two datasets as<cite> Salehi and Cook (2013)</cite> (which were later used, in addition to a third dataset of German noun compounds, in Salehi et al. (2014) ): (1) 90 binary English noun compounds (ENCs, e.g. spelling bee or swimming pool); and (2) 160 English verb particle constructions (EVPCs, e.g. stand up and give away). Our results are not directly comparable with those of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , however, who evaluated in terms of a regression task, modelling the overall compositionality of the MWE.",
  "y": "similarities differences"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_10",
  "x": "Second, the baseline method assumes that for any non-compositional MWE, all components must be equally non-compositional, despite the wealth of MWEs where one or more components are compositional (e.g. from the Wiktionary guidelines for idiom inclusion, 3 computer chess, basketball player, telephone box). We also compare our method with: (1) \"LCS\", the string similarity-based method of<cite> Salehi and Cook (2013)</cite> , in which 54 languages are used; (2) \"DS\", the monolingual distributional similarity method of Salehi et al. (2014) ; (3) \"DS+DSL2\", the multilingual distributional similarity method of Salehi et al. (2014) , including supervised language selection for a given dataset, based on crossvalidation; and (4) \"LCS+DS+DSL2\", whereby the first three methods are combined using a supervised support vector regression model. In each case, the continuous output of the model is equal-width discretised to generate a binary classification.",
  "y": "uses"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_11",
  "x": "The inclusion of translation data was found to improve all of precision, recall and F-score across the board for all of the proposed methods. For reasons of space, results without translation data are therefore omitted from the paper. Overall, the simple unsupervised methods proposed in this paper are comparable with the unsupervised and supervised state-of-the-art methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , with ITAG achieving the highest F-score for the ENC dataset and for the verb components of the EVPC dataset.",
  "y": "uses differences"
 },
 {
  "id": "a0730efd9575800ba779516af1f440_12",
  "x": "Overall, the simple unsupervised methods proposed in this paper are comparable with the unsupervised and supervised state-of-the-art methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , with ITAG achieving the highest F-score for the ENC dataset and for the verb components of the EVPC dataset. The inclusion of synonyms boosts results in most cases. When we combine each of our proposed methods with the string and distributional similarity methods of<cite> Salehi and Cook (2013)</cite> and Salehi et al. (2014) , we see substantial improvements over the comparable combined method of \"LCS+DS+DSL2\" in most cases, demonstrating both the robustness of the proposed methods and their complementarity with the earlier methods.",
  "y": "uses"
 },
 {
  "id": "a229630a81020951ec0be27f54885a_0",
  "x": "Following<cite> (Androutsopoulos et al., 2000)</cite> , we have assigned 9 and 999 (9 and 999 times more important) penalties to the missclassification of legitimate messages as UCE. This means that every instance of a legitimate message has been replaced by 9 and 999 instances of the same message respectively for NB, C4.5 and PART. However, for kNN the data have been downsampled.",
  "y": "uses"
 },
 {
  "id": "a229630a81020951ec0be27f54885a_1",
  "x": "Recall and precision for the UCE class show how effective the filter is blocking UCE, and what is its effectiveness letting legitimate messages pass the filter, respectively<cite> (Androutsopoulos et al., 2000)</cite> . In Table 1 , no costs were used. Tables 2 and  3 show the results of our experiments for cost ratios of 9 and 999.",
  "y": "background"
 },
 {
  "id": "a229630a81020951ec0be27f54885a_2",
  "x": "In (Sahami et al., 1998; <cite>Androutsopoulos et al., 2000)</cite> , the method followed is the variation of the probability threshold, which leads to a high variation of results. In future experiments, we plan to apply the uniform method MetaCost (Domingos, 1999) to the algorithms tested in this work, for getting more comparable results. With respect to the use of heuristics, we can see that this information alone is not competitive, but it can improve classification based on words.",
  "y": "background"
 },
 {
  "id": "a229630a81020951ec0be27f54885a_3",
  "x": "The improvement shown in our experiments is modest, due to the heuristics used. We are not able to add other heuristics in this case because the Spambase collection comes in a preprocessed fashion. For future experiments, we will use the collection from<cite> (Androutsopoulos et al., 2000)</cite> , which is in raw form.",
  "y": "future_work"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_0",
  "x": "Distributed representations of words in the form of word embeddings Pennington et al., 2014) and contextualized word embeddings (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2018; McCann et al., 2017; Radford et al., 2019) have led to huge performance improvement on many NLP tasks. However, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human-produced data<cite> (Bolukbasi et al., 2016</cite>; Caliskan et al., 2017) . In this work, we extend these analyses to the ELMo contextualized word embeddings. Our work provides a new intrinsic analysis of how ELMo represents gender in biased ways.",
  "y": "background motivation"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_1",
  "x": "For word representations,<cite> Bolukbasi et al. (2016)</cite> and Caliskan et al. (2017) show that word embeddings encode societal biases about gender roles and occupations, e.g. engineers are stereotypically men, and nurses are stereotypically women. As a consequence, downstream applications that use these pretrained word embeddings also reflect this bias. For example, Zhao et al. (2018a) and Rudinger et al. (2018) show that coreference resolution systems relying on word embeddings encode such occupational stereotypes.",
  "y": "background motivation"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_2",
  "x": "In contrast, we analyze bias in contextualized word representations and its effect on a downstream task. To mitigate bias from word embeddings,<cite> Bolukbasi et al. (2016)</cite> propose a post-processing method to project out the bias subspace from the pre-trained embeddings. Their method is shown to reduce the gender information from the embeddings of gender-neutral words, and, remarkably, maintains the same level of performance on different downstream NLP tasks.",
  "y": "background"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_3",
  "x": "We then calculate the difference of ELMo embeddings between occupation words in corresponding sentences and conduct principal component analysis for all pairs of sentences. Figure 1 shows there are two principal components for gender in ELMo, in contrast to GloVe which only has one<cite> (Bolukbasi et al., 2016)</cite> . The two principal components in ELMo seem to represent the gender from the contextual information (Contextual Gender) as well as the gender embedded in the word itself (Occupational Gender).",
  "y": "differences"
 },
 {
  "id": "a5d7f5c5fed218149818463427d6a1_4",
  "x": "Data augmentation is performed by replacing gender revealing entities in the OntoNotes dataset with words indicating the opposite gender and then training on the union of the original data and this swapped data. In addition, they find it useful to also mitigate bias in supporting resources and therefore replace standard GloVe embeddings with bias mitigated word embeddings from<cite> Bolukbasi et al. (2016)</cite> . We evaluate the performance of both aspects of this approach.",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_0",
  "x": "As such, there are much fewer open resources available, such as annotated data sets or sentiment lexica. We therefore explore an alternative approach to Arabic SA on social media, using off-the-shelf Machine Translation systems to translate Arabic tweets into English and then use a state-of-the-art sentiment classifier <cite>(Socher et al., 2013)</cite> to assign sentiment labels. To the best of our knowledge, this is the first study to measure the impact of automatically translated data on the accuracy of sentiment analysis of Arabic tweets.",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_1",
  "x": "Our approach differs from the ones described, in that we use automatic MT to translate Arabic tweets into English and then perform SA using a stateof-the-art SA classifier for English <cite>(Socher et al., 2013)</cite> . Most importantly, we empirically benchmark its performance towards previous SA approaches, including lexicon-based, fully supervised and distant supervision SA. tweets from the Twitter public stream.",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_2",
  "x": "We then use the Stanford Sentiment Classifier (SSC) developed by<cite> Socher et al. (2013)</cite> to automatically assign sentiment labels (positive, negative) to translated tweets. The classifier is based on a deep learning (DL) approach, using recursive neural models to capture syntactic dependencies and compositionality of sentiments. Socher et al. (2013) show that this model significantly outperforms previous standard models, such as Na\u00efve Bayes (NB) and Support Vector Machines (SVM) with an accuracy score of 85.4% for binary classification (positive vs. negative) at sentence level 2 .",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_3",
  "x": "Using<cite> Socher et al. (2013)</cite> 's approach for directly training a sentiment classifier will require a larger training data-set, which is not available yet for Ara-bic 3 . ---------------------------------- **BASELINE SYSTEMS**",
  "y": "uses"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_4",
  "x": "This is in line with results reported by Refaee and Rieser (2014b) which evaluate DS approaches to Arabic SA. Only the lexiconapproach is balanced between the positive and negative class. Note that our ML baseline systems as well as the English SA classifier by<cite> Socher et al. (2013)</cite> are trained on balanced data sets, i.e. we can assume no prior bias towards one class.",
  "y": "similarities"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_5",
  "x": "This approach led to the correct SA label, and thus, confirmed that the cause of the problem is word-ordering. Note that the Stanford SA system pays particular attention to sentence structure due to its \"deep\" architecture that adds to the model the feature of being sensitive to word ordering <cite>(Socher et al., 2013)</cite> . In future work, we will verify this by comparing these results to other high performing English SA tools (see for example Abbasi et al. (2014) In sum, one of the major challenges of this approach seems to be the use of Arabic dialects in social media, such as Twitter.",
  "y": "future_work"
 },
 {
  "id": "a5f00f524fdf18e62a4e98a92a2d82_6",
  "x": "We then use the Stanford Sentiment Classifier <cite>(Socher et al., 2013)</cite> to automatically assign sentiment labels (positive, negative) to translated tweets. In contrast to previous work, we benchmark this approach on a gold-standard test set of 937 manually annotated tweets and compare its performance to standard SA approaches, including lexicon-based, supervised and distant supervision approaches. We find that MT approaches reach a comparable performance or significantly outperform more resourceintense standard approaches.",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_0",
  "x": "To address this problem, Ni and Wang (2017) has proposed a task of describing a phrase in a given context. However, they follow the strict assumption that the target phrase is unknown and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult (perhaps as difficult as a human comprehending the phrase itself). On the other hand, <cite>Noraset et al. (2017)</cite> attempted to generate a definition of a word from its word embedding induced from massive text, followed by Gadetsky et al. (2018) that refers to a local context to define a polysemous word with a local context by choosing relevant dimensions of their embeddings.",
  "y": "background"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_1",
  "x": "On the other hand, <cite>Noraset et al. (2017)</cite> attempted to generate a definition of a word from its word embedding induced from massive text, followed by Gadetsky et al. (2018) that refers to a local context to define a polysemous word with a local context by choosing relevant dimensions of their embeddings. Although these research efforts revealed that both local and global contexts of words are useful in generating their definitions, none of these studies exploited both local and global contexts directly. In this study, we tackle a task of describing (defining) a phrase when given its local context as (Ni and Wang, 2017) , while allowing access to other usage examples via word embeddings trained from massive text (global contexts) (Noraset et al., 2017; Gadetsky et al., 2018) .",
  "y": "motivation"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_2",
  "x": "Considering various contexts where we need definitions of phrases, we evaluated our method with four datasets including WordNet<cite> (Noraset et al., 2017)</cite> for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slangs, and a newlycreated Wikipedia dataset for entities. Experimental results demonstrate the effectiveness of our method against the three baselines stated above (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) . Our contributions are as follows:",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_3",
  "x": "Considering various contexts where we need definitions of phrases, we evaluated our method with four datasets including WordNet<cite> (Noraset et al., 2017)</cite> for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slangs, and a newlycreated Wikipedia dataset for entities. Experimental results demonstrate the effectiveness of our method against the three baselines stated above (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) . Our contributions are as follows:",
  "y": "differences"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_4",
  "x": "In the rest of this section, we first describe our idea of utilizing local and global contexts in the description generation task, then present the details of our model. Local & Global Contexts for Description Generation In this paper, we refer to the explicit contextual information included in a single sentence as \"local context,\" and the implicit contextual information in the word/phrase embedding trained in an unsupervised manner on largescale corpora as \"global context. \" Previous work on the definition generation task<cite> (Noraset et al., 2017)</cite> has shown that global contexts can be useful clues when generating definitions of unknown words.",
  "y": "background"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_5",
  "x": "To incorporate the different types of contexts, we propose to use a GATE function<cite> (Noraset et al., 2017)</cite> to dynamically control how the global and local contexts influence the generation of the description. We use bi-directional and uni-directional LSTMs (Hochreiter and Schmidhuber, 1997) as our context encoder and description decoder (Figure 1 ), respectively. Given a sentence X and a phrase X trg , the context encoder generates a sequence of continuous vectors",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_6",
  "x": "In order to capture prefixes and suffixes in X trg , we construct character-level CNNs (Eq. (5)) following<cite> (Noraset et al., 2017)</cite> . Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following <cite>Noraset et al. (2017)</cite> , we set the kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg . In addition to the local context and the character-information, we also utilize the global context obtained from massive text.",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_7",
  "x": "Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following <cite>Noraset et al. (2017)</cite> , we set the kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg . In addition to the local context and the character-information, we also utilize the global context obtained from massive text. We achieve this by two different strategies proposed by <cite>Noraset et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_8",
  "x": "Datasets To evaluate our model on the word description task on WordNet, we followed <cite>Noraset et al. (2017)</cite> and extracted data from WordNet 7 using the dict-definition 8 toolkit. Each entry in the data consists of three elements: (1) a word, (2) its definition, and (3) a usage example of the word. We split this dataset to obtain Train, Validation, and Test sets.",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_9",
  "x": "**MODELS** We implemented four methods including three baselines: (1) Global, (2) Local, (3) I-Attention, and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the strongest model (S + G + CH) in<cite> (Noraset et al., 2017)</cite> . It can access the embedding (global context) of the phrase to be described, but has no ability to read the usage examples (local context).",
  "y": "uses"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_10",
  "x": "Recently, <cite>Noraset et al. (2017)</cite> introduced a task of generating a definition sentence of a word from its pre-trained embedding. Since their task does not take local contexts of words as inputs, their method cannot generate an appropriate definition for a polysemous word for a specific context. To cope with this problem, Gadetsky et al. (2018) have proposed a definition generation method that works with polysemous words in dictionaries.",
  "y": "background"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_11",
  "x": "Focusing on non-standard English words (or phrases), Ni and Wang (2017) generated their explanations solely from sentences with those words. Their model does not take advantage of global contexts (word embeddings induced from massive text) as was used in <cite>Noraset et al. (2017)</cite> . Our task of describing phrases with its given context is a generalization of these three tasks (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) , and the proposed method naturally utilizes both local and global contexts of a word in question.",
  "y": "background"
 },
 {
  "id": "a5f33403d23cdc0532547266f1841a_12",
  "x": "Their model does not take advantage of global contexts (word embeddings induced from massive text) as was used in <cite>Noraset et al. (2017)</cite> . Our task of describing phrases with its given context is a generalization of these three tasks (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) , and the proposed method naturally utilizes both local and global contexts of a word in question. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "a6296d02f21ca5887c7686a2cbe56c_0",
  "x": "The earliest approach in (Bangalore et al., 2001 ) used edit distance based multiple string alignment (MSA) (Durbin et al., 1988) to build the confusion networks. The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts <cite>(Rosti et al., 2007)</cite> . The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment.",
  "y": "background"
 },
 {
  "id": "a6296d02f21ca5887c7686a2cbe56c_1",
  "x": "The earliest approach in (Bangalore et al., 2001 ) used edit distance based multiple string alignment (MSA) (Durbin et al., 1988) to build the confusion networks. The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts <cite>(Rosti et al., 2007)</cite> . The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment.",
  "y": "extends"
 },
 {
  "id": "a6296d02f21ca5887c7686a2cbe56c_2",
  "x": "All words from the previously aligned hypotheses are available, even if not present in the skeleton hypothesis, when aligning the following hypotheses. As in <cite>(Rosti et al., 2007)</cite> , confusion networks built around all skeletons are joined into a lattice which is expanded and rescored with language models. System weights and language model weights are tuned to optimize the quality of the decoding output on a development set.",
  "y": "similarities uses"
 },
 {
  "id": "a6296d02f21ca5887c7686a2cbe56c_3",
  "x": "Other scores for the word arc are set as in <cite>(Rosti et al., 2007)</cite> . ---------------------------------- **BENEFITS OVER PAIR-WISE TER ALIGNMENT**",
  "y": "similarities uses"
 },
 {
  "id": "a6b450d1113e0e6d3d2813c09d12a8_0",
  "x": "Several German datasets for evaluation of SS or SR have been created so far (see Table 1 ). <cite>Gurevych (2005)</cite> conducted experiments with a German translation of an English dataset (Rubenstein and Goodenough, 1965) , but argued that the dataset (Gur65) is too small (it contains only 65 noun pairs), and does not model SR. Thus, she created a German dataset containing 350 word pairs (Gur350) containing nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004) .",
  "y": "background"
 },
 {
  "id": "a6b450d1113e0e6d3d2813c09d12a8_1",
  "x": "Semantic wordnet based measures Lesk (1986) introduced a measure (Les) based on the number of word overlaps in the textual definitions (or glosses) of two terms, where higher overlap means higher similarity. As GermaNet does not contain glosses, this measure cannot be employed. <cite>Gurevych (2005)</cite> proposed an alternative algorithm (PG) generating surrogate glosses by using a concept's relations within the hierarchy.",
  "y": "background"
 },
 {
  "id": "a6b450d1113e0e6d3d2813c09d12a8_2",
  "x": "Table 2 gives an overview of our experimental results on three German datasets. Best values for each dataset and knowledge source are in bold. We use the P G measure in optimal configuration as reported by <cite>Gurevych (2005)</cite> .",
  "y": "uses"
 },
 {
  "id": "a6b450d1113e0e6d3d2813c09d12a8_3",
  "x": "Our results on Gur65 using GermaNet are very close to those published by <cite>Gurevych (2005)</cite> , ranging from 0.69-0.75. For Gur350, the performance drops to 0.38-0.50, due to the lower upper bound, and because GermaNet does not model SR well. These findings are endorsed by an even more significant performance drop on ZG222.",
  "y": "similarities"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_0",
  "x": "**INTRODUCTION** Neural machine translation (NMT) (Bahdanau et al., 2015; Wu et al., 2016;<cite> Vaswani et al., 2017)</cite> trains an encoder-decoder network on sentence pairs to maximize the likelihood of predicting a target-language sentence given the corresponding source-language sentence, without considering the document context. By ignoring discourse connections between sentences and other valuable contextual information, this simplification potentially degrades the coherence and cohesion of a translated document (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017) .",
  "y": "background"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_1",
  "x": "This integration method enables it to jointly optimize for multiple-sentences. Furthermore, we extend the original HAN with a multi-head attention<cite> (Vaswani et al., 2017)</cite> to capture different types of discourse phenomena. Our main contributions are the following: (i) We propose a HAN framework for translation to capture context and inter-sentence connections in a structured and dynamic manner.",
  "y": "uses"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_2",
  "x": "Furthermore, we extend the original HAN with a multi-head attention<cite> (Vaswani et al., 2017)</cite> to capture different types of discourse phenomena. Our main contributions are the following: (i) We propose a HAN framework for translation to capture context and inter-sentence connections in a structured and dynamic manner. (ii) We integrate the HAN in a very competitive NMT ar-chitecture<cite> (Vaswani et al., 2017)</cite> and show significant improvement over two strong baselines on multiple data sets.",
  "y": "uses"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_3",
  "x": "In particular, h t is the last hidden state of the word to be encoded, or decoded at time step t, and h j i is the last hidden state of the i-th word of the j-th sentence of the context. The function f w is a linear transformation to obtain the query q w . We used the MultiHead attention function proposed by<cite> (Vaswani et al., 2017)</cite> to capture different types of relations among words.",
  "y": "uses"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_4",
  "x": "where f s is a linear transformation, q s is the query for the attention function, FFN is a position-wise feed-forward layer<cite> (Vaswani et al., 2017</cite> ). Each layer is followed by a normalization layer (Lei Ba et al., 2016) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_5",
  "x": "The configuration is the same as the model called \"base model\" in the original paper<cite> (Vaswani et al., 2017)</cite> . The encoder and decoder are composed of 6 hidden layers each. All hidden states have dimension of 512, dropout of 0.1, and 8 heads for the multi-head attention.",
  "y": "similarities uses"
 },
 {
  "id": "a6f32017135e984fbe59f2171c50f4_6",
  "x": "The optimization and regularization methods were the same as proposed by<cite> Vaswani et al. (2017)</cite> . Inspired by we trained the models in two stages. First we optimize the parameters for the NMT without the HAN, then we proceed to optimize the parameters of the whole network.",
  "y": "similarities uses"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_0",
  "x": "There is an accumulation of evidence that the use of dense distributional lexical representations, known as word embeddings, often supports better performance on a range of NLP tasks (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; <cite>Mikolov et al., 2013a</cite>; Mikolov et al., 2013b; Levy et al., 2015) . Consequently, word embeddings have been commonly used in the last few years for lexical similarity tasks and as features in multiple, syntactic and semantic, NLP applications. However, keeping embedding vectors for hundreds of thousands of words for repeated use could take its toll both on storing the word vectors on disk and, even more so, on loading them into memory.",
  "y": "background"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_1",
  "x": "Moreover, we provide two ways to train existing algorithms<cite> (Mikolov et al., 2013a</cite>; Mikolov et al., 2013b ) when the memory is limited during training and show that, here, too, an order of magnitude saving in memory is possible without degrading performance. We conduct comprehensive experiments on existing word and phrase similarity and relatedness datasets as well as on dependency parsing, to evaluate these results. Our experiments show that, in all cases and without loss in performance, 8 bits can be used when the current standard is 64 and, in some cases, only 4 bits per dimension are sufficient, reducing the amount of space required by a factor of 16.",
  "y": "extends"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_2",
  "x": "**RELATED WORK** If we consider traditional cluster encoded word representation, e.g., Brown clusters (Brown et al., 1992) , it only uses a small number of bits to track the path on a hierarchical tree of word clusters to represent each word. In fact, word embedding generalized the idea of discrete clustering representation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and generalization ability (Bengio et al., 2003; <cite>Mikolov et al., 2013a</cite>; Mikolov et al., 2013b) .",
  "y": "background motivation"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_4",
  "x": "In this section, we describe a comprehensive study on tasks that have been used for evaluating word embeddings. We train the word embedding algorithms, word2vec<cite> (Mikolov et al., 2013a</cite>; Mikolov et al., 2013b) , based on the Oct. 2013 Wikipedia dump. 1 We first compare levels of truncation of word2vec embeddings, and then evaluate the stochastic rounding and the auxiliary vectors based methods for training word2vec vectors.",
  "y": "uses"
 },
 {
  "id": "a7e49bec53a2bfd7795b9c770f5d0c_5",
  "x": "---------------------------------- **ANALYSIS OF BITS NEEDED** We ran both CBOW and skipgram with negative sampling<cite> (Mikolov et al., 2013a</cite>; Mikolov et al., 2013b) on the Wikipedia dump data, and set the window size of context to be five.",
  "y": "uses"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_0",
  "x": "We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of<cite> Durrett and Klein (2014)</cite> and Nguyen et al. (2014) . ---------------------------------- **1**",
  "y": "motivation"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_1",
  "x": "Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013;<cite> Durrett and Klein, 2014)</cite> . But an even simpler approach is to use context information from just the words in the source document itself to make sure the entity is being resolved sensibly in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et 1 Source available at github.com/matthewfl/nlp-entity-convnet al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods.",
  "y": "background"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_2",
  "x": "CNNs have been shown to be effective for sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Iyyer et al., 2015) and for capturing similarity in models for entity linking (Sun et al., 2015) and other related tasks (Dong et al., 2015; Shen et al., 2014) , so we expect them to be effective at isolating the relevant topic semantics for entity linking. We show that convolutions over multiple granularities of the input document are useful for providing different notions of semantic context. Finally, we show how to integrate these networks with a preexisting entity linking system <cite>(Durrett and Klein, 2014)</cite> .",
  "y": "motivation"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_3",
  "x": "This information can serve as a useful prior, but only if we can leverage it effectively by targeting the most salient part of a mention. For example, we may have never observed President Barack Obama as a linked string on Wikipedia, even though we have seen the substring Barack Obama and it unambiguously indicates the correct answer. Following<cite> Durrett and Klein (2014)</cite> , we introduce a latent variable q to capture which subset of a mention (known as a query) we resolve.",
  "y": "extends differences"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_4",
  "x": "Our final model has the form P (t|x) = q P (t, q|x). We parameterize P (t, q|x) in a loglinear way with three separate components: f Q and f E are both sparse features vectors and are taken from previous work <cite>(Durrett and Klein, 2014)</cite> .",
  "y": "extends similarities"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_5",
  "x": "Note that f C has its own internal parameters \u03b8 because it relies on CNNs with learned filters; however, we can compute gradients for these parameters with standard backpropagation. The whole model is trained to maximize the log likelihood of a labeled training corpus using Adadelta (Zeiler, 2012) . The indicator features f Q and f E are described in more detail in<cite> Durrett and Klein (2014)</cite> .",
  "y": "background"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_6",
  "x": "It is designed to roughly capture the basic shape of a query to measure its desirability, indicating whether suffixes were removed and whether the query captures the capitalized subsequence of a mention, as well as standard lexical, POS, and named entity type features. f E mostly captures how likely the selected query is to correspond to a given entity based on factors like anchor text counts from Wikipedia, string match with proposed Wikipedia titles, and discretized cosine similarities of tf-idf vectors (Ratinov et al., 2011) . Adding tf-idf indicators is the only modification we made to the features of<cite> Durrett and Klein (2014)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_7",
  "x": "We performed experiments on 4 different entity linking datasets. \u2022 ACE (NIST, 2005; Bentivogli et al., 2010) : This corpus was used in Fahrni and Strube (2014) and<cite> Durrett and Klein (2014)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_8",
  "x": "Our results outperform those of<cite> Durrett and Klein (2014)</cite> and Nguyen et al. (2014) . We see that this system outperforms the results of<cite> Durrett and Klein (2014)</cite> and the AIDA-LIGHT system of Nguyen et al. (2014) .",
  "y": "differences"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_9",
  "x": "We can also compare to two ablations: using just the sparse features (a system which is a direct extension of<cite> Durrett and Klein (2014)</cite> ) or using just the CNNderived features. 5 Our CNN features generally outperform the sparse features and improve even further when stacked with them. This reflects that they capture orthogonal sources of information: for example, the sparse features can capture how frequently the target document was linked to, whereas the CNNs can capture document context in a more nuanced way.",
  "y": "extends differences"
 },
 {
  "id": "a7f4154081f4045390e662c6e6f3ac_10",
  "x": "**ACE** In the sparse feature system, the highest weighted features are typically those indicating the frequency that a page was linked to and those indicating specific lexical items in the choice of the latent query variable q. This suggests that the system of<cite> Durrett and Klein (2014)</cite> has the power to pick the right span of a mention to resolve, but then is left to generally pick the most common link target in Wikipedia, which is not always correct. By contrast, the full system has a greater ability to pick less common link targets if the topic indicators distilled from the CNNs indicate that it should do so.",
  "y": "motivation background"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_0",
  "x": "1 Roots combine with derivational (e.g. refut+able) and inflectional affixes (e.g. hold+ing). Computational segmentation approaches can be divided into rule-based (Porter, 1980) , supervised (Ruokolainen et al., 2013) , semi-supervised (Gr\u00f6nroos et al., 2014) , and unsupervised (Creutz and Lagus, 2002) . <cite>Bartlett et al. (2008)</cite> observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification.",
  "y": "background"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_1",
  "x": "<cite>Bartlett et al. (2008)</cite> observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification. In this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology. We augment the syllabification approach of <cite>Bartlett et al. (2008)</cite> , with features encoding morphological segmentation of words.",
  "y": "extends"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_2",
  "x": "In this section, we describe the original syllabification method of <cite>Bartlett et al. (2008)</cite> , which serves as our baseline system, and discuss various approaches to incorporating morphological information. <cite>Bartlett et al. (2008)</cite> present a discriminative approach to automatic syllabification. They formulate syllabification as a tagging problem, and learn a Structured SVM tagger from labeled data (Tsochantaridis et al., 2005) .",
  "y": "uses"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_3",
  "x": "In this section, we describe the original syllabification method of <cite>Bartlett et al. (2008)</cite> , which serves as our baseline system, and discuss various approaches to incorporating morphological information. <cite>Bartlett et al. (2008)</cite> present a discriminative approach to automatic syllabification. They formulate syllabification as a tagging problem, and learn a Structured SVM tagger from labeled data (Tsochantaridis et al., 2005) .",
  "y": "background"
 },
 {
  "id": "a8743bb89abd16f75bec9e72e446b3_5",
  "x": "As a baseline, we replicate the experiments of <cite>Bartlett et al. (2008)</cite> , and extend them to lowresource settings. Since the training sets are of slightly different sizes, we label each training size point as specified in Table 3 . We see that correct syllabification of approximately half of the words is achieved with as few as 100 English and 50 German training examples.",
  "y": "extends uses"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_0",
  "x": "There has been much work recently on developing computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer programming to generate and rank all possible rewrites of an input sentence (Dras, 1999; Woodsend and Lapata, 2011) . Machine Translation systems have been adapted to translate complex sentences into simple ones <cite>(Zhu et al., 2010</cite>; Wubben et al., 2012; Coster and Kauchak, 2011) .",
  "y": "background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_1",
  "x": "While previous simplification approaches starts from either the input sentence or its parse tree, our model takes as input a deep semantic representation namely, the Discourse Representation Structure (DRS, (Kamp, 1981) ) assigned by Boxer (Curran et al., 2007) to the input complex sentence. As we shall see in Section 4, this permits a linguistically principled account of the splitting operation in that semantically shared elements are taken to be the basis for splitting a complex sentence into several simpler ones; this facilitates completion (the re-creation of the shared element in the split sentences); and this provide a natural means to avoid deleting obligatory arguments. When compared against current state of the art methods <cite>(Zhu et al., 2010</cite>; Woodsend and Lapata, 2011; Wubben et al., 2012) , our model yields significantly simpler output that is both grammatical and meaning preserving.",
  "y": "differences"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_2",
  "x": "Machine Translation systems have been adapted to translate complex sentences into simple ones <cite>(Zhu et al., 2010</cite>; Wubben et al., 2012; Coster and Kauchak, 2011) . And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications (Siddharthan et al., 2004; Siddharthan, 2011; Chandrasekar et al., 1996) . In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways.",
  "y": "extends"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_3",
  "x": "Using both the PWKP corpus developed by<cite> Zhu et al. (2010)</cite> and the edit history of Simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar (Smith and Eisner, 2006) describing a loose alignment between parse trees of complex and of simple sentences. Following Dras (1999) , they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by<cite> Zhu et al. (2010)</cite> namely, an aligned corpus of 100/131 EWKP/SWKP sentences and show that they achieve better BLEU score.",
  "y": "background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_4",
  "x": "Following Dras (1999) , they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by<cite> Zhu et al. (2010)</cite> namely, an aligned corpus of 100/131 EWKP/SWKP sentences and show that they achieve better BLEU score. They also conducted a human evaluation on 64 of the 100 test sentences and showed again a better performance in terms of simplicity, grammaticality and meaning preservation.",
  "y": "uses background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_5",
  "x": "A human evaluation on 20 sentences randomly selected from the test data indicates that, in terms of fluency and adequacy, their system is judged to outperform both<cite> Zhu et al. (2010)</cite> and Woodsend and Lapata (2011) systems. ---------------------------------- **SIMPLIFICATION FRAMEWORK**",
  "y": "background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_6",
  "x": "While our semantic based approach naturally accounts for this by copying the phrase corresponding to the shared entity in both phrases, syntax based approach such as<cite> Zhu et al. (2010)</cite> and Woodsend and Lapata (2011) will often fail to appropriately reconstruct the shared phrase and introduce agreement mismatches because the alignment or rules they learn are based on syntax alone. For instance, in example (2),<cite> Zhu et al. (2010)</cite> fails to copy the shared argument \"The judge\" to the second clause whereas Woodsend and Lapata (2011) learns a synchronous rule matching (VP and VP) to (VP. NP(It) VP) thereby failing to produce the correct subject pronoun (\"he\" or \"she\") for the antecedent \"The judge\".",
  "y": "differences"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_7",
  "x": "By contrast, syntax based approaches <cite>(Zhu et al., 2010</cite>; Woodsend and Lapata, 2011) do not distinguish between optional and obligatory arguments. For instance<cite> Zhu et al. (2010)</cite> simplifies (3C) to (3S) thereby incorrectly deleting the obligatory theme (gifts) of the complex sentence and modifying its meaning to giving knights and warriors (instead of giving gifts to knights and warriors). (3) C. Women would also often give knights and warriors gifts that included thyme leaves as it was believed to bring courage to the bearer.",
  "y": "differences"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_8",
  "x": "Thus in our approach, semantic subformulae which are related to a predicate by a core thematic roles (e.g., agent and patient) are never considered for deletion. By contrast, syntax based approaches <cite>(Zhu et al., 2010</cite>; Woodsend and Lapata, 2011) do not distinguish between optional and obligatory arguments. For instance<cite> Zhu et al. (2010)</cite> simplifies (3C) to (3S) thereby incorrectly deleting the obligatory theme (gifts) of the complex sentence and modifying its meaning to giving knights and warriors (instead of giving gifts to knights and warriors).",
  "y": "differences"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_9",
  "x": "For an efficient implementation of EM algorithm, we follow the work of Yamada and Knight (2001) and<cite> Zhu et al. (2010)</cite> ; and build training graphs (Figure 2 ) from the pair of complex and simple sentence pairs in the training data. Each training graph represents a complexsimple sentence pair and consists of two types of nodes: major nodes (M-nodes) and operation nodes (O-nodes). Each deletion candidate creates a deletion O-node marking successful or failed deletion of the candidate and a result M-node.",
  "y": "uses"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_10",
  "x": "**EXPERIMENTS** We trained our simplification and translation models on the PWKP corpus. To evaluate performance, we compare our approach with three other state of the art systems using the test set provided by<cite> Zhu et al. (2010)</cite> and relying both on automatic metrics and on human judgments.",
  "y": "uses"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_11",
  "x": "The DRS-Based simplification model is trained on PWKP, a bi-text of complex and simple sentences provided by<cite> Zhu et al. (2010)</cite> . To construct this bi-text,<cite> Zhu et al. (2010)</cite> extracted complex and simple sentences from EWKP and SWKP respectively and automatically aligned them using TF*IDF as a similarity measure. PWKP contains 108016/114924 complex/simple sentence pairs.",
  "y": "uses"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_12",
  "x": "The DRS-Based simplification model is trained on PWKP, a bi-text of complex and simple sentences provided by<cite> Zhu et al. (2010)</cite> . To construct this bi-text,<cite> Zhu et al. (2010)</cite> extracted complex and simple sentences from EWKP and SWKP respectively and automatically aligned them using TF*IDF as a similarity measure. PWKP contains 108016/114924 complex/simple sentence pairs.",
  "y": "background"
 },
 {
  "id": "a8ba807b94f6f7ff4f7e77a9fcde35_13",
  "x": "Finally, our DRS-Based simplification model is trained on 97.75% of PWKP; we drop out 2.25% of the complex sentences in PWKP which are repeated in the test set or for which Boxer fails to produce DRSs. We evaluate our model on the test set used by<cite> Zhu et al. (2010)</cite> namely, an aligned corpus of 100/131 EWKP/SWKP sentences. Boxer produces a DRS for 96 of the 100 input sentences.",
  "y": "uses"
 },
 {
  "id": "a9897f66e05a0354c36daba0db9afe_0",
  "x": "The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples. A breakthrough has come in the form of research by <cite>McClosky et al. (2006a</cite>; 2006b ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005) . Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003) .",
  "y": "background"
 },
 {
  "id": "a9897f66e05a0354c36daba0db9afe_2",
  "x": "This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Ku\u010dera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of <cite>McClosky et al. (2006a</cite>; 2006b) , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences).",
  "y": "background"
 },
 {
  "id": "abe561b75389e026a9f140280f211c_0",
  "x": "It is the reason why opinion mining has recently become a topic of interest in both academia and business institutions. Sentiment analysis is a type of opinion mining where affective states are represented categorically or by multi-dimensional continuous values<cite> (Yu et al., 2015)</cite> . The categorical approach aims at classifying the sentiment into polarity classes (such as positive, neutral, and negative,) or Ekman's six basic emotions, i.e., anger, happiness, fear, sadness, disgust, and surprise (Ekman, 1992 ).",
  "y": "background"
 },
 {
  "id": "abe561b75389e026a9f140280f211c_1",
  "x": "On the other hand, the dimensional approach represents affective states as continuous numerical values in multiple dimensions, such as valencearousal space (Markus and Kitayama, 1991) , as shown in Fig. 1 gree of pleasant and unpleasant (i.e., positive and negative) feelings, while the arousal represents the degree of excitement. According to the twodimensional representation, any affective state can be represented as a point in the valence-arousal space by determining the degrees of valence and arousal of given words (Wei et al., 2011; <cite>Yu et al., 2015)</cite> or texts (Kim et al., 2010) . Dimen-sional sentiment analysis is an increasingly active research field with potential applications including antisocial behavior detection (Munezero et al., 2011) and mood analysis (De Choudhury et al., 2012) .",
  "y": "background"
 },
 {
  "id": "abe561b75389e026a9f140280f211c_2",
  "x": "The valence-arousal score of the unknown word can be obtained by averaging the matched CVA. Moreover, previous research suggested that it is possible to improve the performance by aggregating the results of a number of valence-arousal methods<cite> (Yu et al., 2015)</cite> . Thus, we use two sets of methods for the prediction of valence: (1) prediction based on WVA and CVA, and (2) a kNN valence prediction method.",
  "y": "background motivation"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_0",
  "x": "**ABSTRACT** We replicate the syntactic experiments of <cite>Mikolov et al. (2013b)</cite> on English, and expand them to include morphologically complex languages. We learn vector representations for Dutch, French, German, and Spanish with the WORD2VEC tool, and investigate to what extent inflectional information is preserved across vectors.",
  "y": "extends uses"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_1",
  "x": "---------------------------------- **** 1 Introduction <cite>Mikolov et al. (2013b)</cite> demonstrate that vector representations of words obtained from a neural network language model provide a way of capturing both semantic and syntactic regularities in language.",
  "y": "background"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_2",
  "x": "In order to to validate our methodology, we first replicate the results of <cite>Mikolov et al. (2013b)</cite> on English syntactic analogies. ---------------------------------- **TRAINING CORPUS FOR WORD VECTORS**",
  "y": "uses"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_3",
  "x": "The vectors of <cite>Mikolov et al. (2013b)</cite> were trained on 320M tokens of broadcast news data, as described by Mikolov et al. (2011) . Since we have no access to this data, we instead train English vectors on a corpus from the Polyglot project (Al-Rfou et al., 2013) , which contains tokenized Wikipedia dumps intended for the training of vector-space models. For comparison with the results of <cite>Mikolov et al. (2013b)</cite> , we limit the data to the first 320M lowercased tokens of the corpus.",
  "y": "background"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_4",
  "x": "For comparison with the results of <cite>Mikolov et al. (2013b)</cite> , we limit the data to the first 320M lowercased tokens of the corpus. <cite>Mikolov et al. (2013b)</cite> obtain their best results with vectors of size 1600 that combine several models, but do not elaborate how this composite model was constructed. Instead, we take as a point of reference their second-best model, which employs 640-dimensional vectors produced by a single recursive neural network (RNN) language model.",
  "y": "motivation"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_5",
  "x": "<cite>Mikolov et al. (2013b)</cite> obtain their best results with vectors of size 1600 that combine several models, but do not elaborate how this composite model was constructed. Instead, we take as a point of reference their second-best model, which employs 640-dimensional vectors produced by a single recursive neural network (RNN) language model. 1 Rather than use an RNN model to learn our own vectors, we employ the far simpler skip-gram model.",
  "y": "differences"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_7",
  "x": "The first, labeled as M13, is the result of applying the vectors of <cite>Mikolov et al. (2013b)</cite> to their test set. The results match the results reported in their paper, except for the nominal results, which reflect our modifications described in Section 2.2. The removal of the possessives improves the accuracy from 25.2% reported in the original paper to 40.1%.",
  "y": "uses"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_8",
  "x": "The removal of the possessives improves the accuracy from 25.2% reported in the original paper to 40.1%. The second column, labeled as Ours, reports the results for our vectors, which were trained using WORD2VEC on the English data described in Section 2.1. Our verbal and adjectival vectors obtain slightly lower accuracies than the RNN trained vectors of <cite>Mikolov et al. (2013b)</cite> , but they are not far off.",
  "y": "similarities differences"
 },
 {
  "id": "af6c68ef5f80eac2274bf33a894d1f_9",
  "x": "In order to make results between multiple languages comparable, we made several changes to the construction of syntactic analogy questions. We follow the methodology of <cite>Mikolov et al. (2013b)</cite> in limiting analogy questions to the 100 most frequent verbs or nouns. The frequencies are obtained from corpora tagged by TREETAGGER (Schmid, 1994) .",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_0",
  "x": "The N-gram-based SMT framework is based on tuples. Tuples are minimal translation units composed of source and target cepts 2 . N-gram-based models are Markov models over sequences of tuples or operations encapsulating tuples<cite> (Durrani et al., 2011)</cite> .",
  "y": "background"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_1",
  "x": "2 A cept is a group of words in one language that is translated as a minimal unit in one specific context (Brown et al., 1993). poor translation selection, ii) inaccurate future-cost estimates and iii) incorrect early pruning of hypotheses that would produce better model scores if allowed to continue. In order to deal with these problems, search is carried out only on a graph of pre-calculated orderings, and ad-hoc reordering limits are imposed to constrain the search space (Crego et al., 2005; , or a higher beam size is used in decoding<cite> (Durrani et al., 2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_2",
  "x": "Phrase-based systems, on the other hand, are likely to have access to the phrasal unit \"scho\u00df ein Tor -scored a goal\" and can generate it in a single step. Moreover, a more accurate future-cost estimate can be computed because of the available context internal to the phrase. In this work, we extend the N-gram model, based on operation sequences<cite> (Durrani et al., 2011)</cite> , to use phrases during decoding.",
  "y": "extends"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_3",
  "x": "<cite>Durrani et al. (2011)</cite> recently addressed these problems by proposing an operation sequence Ngram model which strongly couples translation and reordering, hypothesizes all possible reorderings and does not require POS-based rules. Representing bilingual sentences as a sequence of operations enables them to memorize phrases and lexical reordering triggers like PBSMT. However, using minimal units during decoding and searching over all possible reorderings means that hypotheses can no longer be arranged in 2 m stacks.",
  "y": "background"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_4",
  "x": "Several count-based features such as gap and open gap penalties and distance-based features such as gap-width and reordering distance are added to the model, along with the lexical weighting and length penalty features in a standard log-linear framework<cite> (Durrani et al., 2011)</cite> . ---------------------------------- **SEARCH 4.1 OVERVIEW OF DECODING FRAMEWORK**",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_5",
  "x": "(Refer to Algorithm 1 in <cite>Durrani et al. (2011)</cite> ). The probability of an operation depends on the n \u2212 1 previous operations. The model backs-off to the smaller n-grams of operations if the full history is unknown.",
  "y": "background"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_6",
  "x": "The reordering operations (gaps and jumps) are generated by looking at the position of the translator, the last foreign word generated etc. (Refer to Algorithm 1 in <cite>Durrani et al. (2011)</cite> ). The probability of an operation depends on the n \u2212 1 previous operations.",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_7",
  "x": "We extended the training steps in <cite>Durrani et al. (2011)</cite> to extract a phrase lexicon from the parallel data. We extract all phrase pairs of length 6 and below, that are consistent (Och et al., 1999) with the word alignments. Only continuous phrases as used in a traditional phrase-based system are extracted thus allowing only inside-out (Wu, 1997) type of alignments.",
  "y": "extends"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_8",
  "x": "---------------------------------- **DECODING** We extended the decoder developed by <cite>Durrani et al. (2011)</cite> and tried three ideas.",
  "y": "extends"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_9",
  "x": "Word alignments are obtained by running GIZA++ (Och and Ney, 2003) with the grow-diag-final-and (Koehn et al., 2005) symmetrization heuristic. We follow the training steps described in <cite>Durrani et al. (2011)</cite> , consisting of i) post-processing the alignments to remove discontinuous and unaligned target cepts, ii) conversion of bilingual alignments into operation sequences, iii) estimation of the n-gram language models. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "b0a50145121eb797cf8e6ebc2f49e0_10",
  "x": "We mark a result as sig-8 Discontinuous source-side units did not lead to any improvements in<cite> (Durrani et al., 2011)</cite> and increased the decoding times by multiple folds. We also found these to be less useful. 9 Imposing a hard reordering limit significantly reduced the decoding time and also slightly increased the BLEU scores.",
  "y": "differences"
 },
 {
  "id": "b21dfcb9854b0b48af47f4f13899b0_0",
  "x": "Moreover, because students are the target users of these systems, instructors typically can neither correct the errors made by the automatic analysis nor observe/assess the students' revision efforts. We argue that an intelligent writing assistant ought to be aware of the revision process; it should: 1) identify all significant changes made by a writer between the essay drafts, 2) automatically determine the purposes of these changes, 3) provide the writer the means to compare between drafts in an easy to understand visualization, and 4) support instructor monitoring and corrections in the revision process as well. In our previous work (Zhang and Litman, 2014; <cite>Zhang and Litman, 2015)</cite> , we focused on 1) and 2), the automatic extraction and classification of revisions for argumentative writings.",
  "y": "background"
 },
 {
  "id": "b21dfcb9854b0b48af47f4f13899b0_1",
  "x": "We argue that an intelligent writing assistant ought to be aware of the revision process; it should: 1) identify all significant changes made by a writer between the essay drafts, 2) automatically determine the purposes of these changes, 3) provide the writer the means to compare between drafts in an easy to understand visualization, and 4) support instructor monitoring and corrections in the revision process as well. In our previous work (Zhang and Litman, 2014; <cite>Zhang and Litman, 2015)</cite> , we focused on 1) and 2), the automatic extraction and classification of revisions for argumentative writings. In this work, we extend our framework to integrate the automatic analyzer with a web-based interface to support student argumentative writings.",
  "y": "extends background"
 },
 {
  "id": "b21dfcb9854b0b48af47f4f13899b0_2",
  "x": "Revision classification. Following the argumentative revision definition in our prior work<cite> (Zhang and Litman, 2015)</cite> , revisions are first categorized to Content (Text-based) and Surface 3 according to whether the revision changed the meaning of the essay or not. The Text-based revisions include Thesis/Ideas (Claim), Rebuttal, Reasoning (Warrant), Evidence, and Other content changes (General Content).",
  "y": "uses"
 },
 {
  "id": "b21dfcb9854b0b48af47f4f13899b0_3",
  "x": "Inspired by works on learning analytics (Liu et al., 2013; Verbert et al., 2013) , we design the revision overview interface which displays the statistics of the revisions. Following design principle #1, the revision purposes are color coded and each purpose corresponds to a specific color. Our prior work<cite> (Zhang and Litman, 2015)</cite> demonstrates that only Text-based revisions are significantly correlated with the writing improvement.",
  "y": "background"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_0",
  "x": "From a synchronic perspective, <cite>Reddy et al. (2011</cite> ), Schulte im Walde et al. (2013 and Schulte im Walde et al. (2016a) are closest to our approach, since they predict the compositionality of compounds using vector space representations. However, Schulte im Walde et al. (2013) use German data and do not investigate diachronic changes. They report a Spearman's \u03c1 of 0.65 for predicting the compositionality of compounds based on the features of their semantic space and find that the modifiers mainly influence the compositionality of the whole compound, contrary to their expectation that the head should be the main source of influence.",
  "y": "similarities background"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_1",
  "x": "**METHODS AND DATA** Several studies have been conducted in order to measure compositionality for compounds in different languages (von der Heide and Borgwaldt, 2009;<cite> Reddy et al., 2011</cite>; Schulte im Walde et al., 2016b) . Some of these works have used large corpora to extract vector-based representations of compounds and their parts to automatically determine the compositionality of a given compound.",
  "y": "background"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_2",
  "x": "However, as it is not possible to survey compositionality rating for diachronic data, we instead use the synchronic data provided by<cite> Reddy et al. (2011)</cite> (henceforth referred to as REDDY) for evaluating the quality of the Google Books Ngram data as a source for investigating the compositionality of compounds in general. Reddy et al. (2011) compiled a list of 90 English compounds and asked annotators to rate the compositionality of these compounds on a scale from 0 to 5. They provide three mean values of their ratings for the compounds (compound-mean), heads (head-mean) and modifiers (modifier-mean). We make use of REDDY in order to verify that our methods are capable of capturing compositionality (synchronically) and use the diachronic data of Google Books Ngram to investigate the temporal change of compositionality.",
  "y": "uses"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_3",
  "x": "To do so, we calculate correlation scores between our proposed metrics and the annotated compositionality ratings of REDDY. Like<cite> Reddy et al. (2011)</cite> and Schulte im Walde et al. (2013), we opt for Spearman's \u03c1. To find the best configuration of a time span and cut-off for the regression models, we use the R 2 metric.",
  "y": "similarities"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_4",
  "x": "This could be due to PPMI's property of inflating scores for rare events. As can also be seen from Table 2 , our correlation values are considerably lower than that of<cite> Reddy et al. (2011)</cite> , but on par with a replication study by Schulte im Walde et al. (2016a) for compound-mean. We speculate that these differences are potentially due to the use of different data sets, the fact that we use a considerably smaller context window for constructing the word vectors (5 due to the restrictions of Google Ngram corpus vs. 100 in<cite> Reddy et al. (2011)</cite> and 40 in Schulte im Walde et al. (2016b) ) and the use of a compound-centric setting (as described in 4.1).",
  "y": "differences"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_5",
  "x": "As can also be seen from Table 2 , our correlation values are considerably lower than that of<cite> Reddy et al. (2011)</cite> , but on par with a replication study by Schulte im Walde et al. (2016a) for compound-mean. We speculate that these differences are potentially due to the use of different data sets, the fact that we use a considerably smaller context window for constructing the word vectors (5 due to the restrictions of Google Ngram corpus vs. 100 in<cite> Reddy et al. (2011)</cite> and 40 in Schulte im Walde et al. (2016b) ) and the use of a compound-centric setting (as described in 4.1). From Table 1 we see that our best reported R 2 value occurs when observing time in stretches of 20 years (scores) and compounds having a frequency cut-off of at least 100.",
  "y": "differences"
 },
 {
  "id": "b49e6f8181d51a998c6c27a830b98e_6",
  "x": "Our current work was limited to English compounds from<cite> Reddy et al. (2011)</cite> . We plan to expand our models to other languages for which compositionality ratings are available, such as German. We would also like to further investigate the fact that the information theory based measures outperform the ones based on cosine similarity.",
  "y": "uses"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_0",
  "x": "**ABSTRACT** Abstract Ratnaparkhi (1996) introduced a method of inferring a tag dictionary from annotated data to speed up part-of-speech tagging by limiting the set of possible tags for each word. While Ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed<cite> (Moore, 2014)</cite> makes tagging as fast as with Ratnaparkhi's tag dictionary, but with no decrease in accuracy.",
  "y": "background motivation"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_1",
  "x": "Ratnaparkhi's method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state-of-the-art tagging model. We recently presented<cite> (Moore, 2014)</cite> a new method of constructing a tag dictionary that produces a tagging speed-up comparable to Ratnaparkhi's, but with no decrease in tagging accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging-more than 100,000 tokens per second even in a Perl implementation.",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_2",
  "x": "introduced a method of inferring a tag dictionary from annotated data to speed up part-of-speech tagging by limiting the set of possible tags for each word. While Ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed <cite>(Moore, 2014</cite> ) makes tagging as fast as with Ratnaparkhi's tag dictionary, but with no decrease in accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging-more than 100,000 tokens per second in Perl.",
  "y": "motivation differences"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_3",
  "x": "We recently presented<cite> (Moore, 2014)</cite> a new method of constructing a tag dictionary that produces a tagging speed-up comparable to Ratnaparkhi's, but with no decrease in tagging accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi's method results in a much tighter tag dictionary than either Ratnaparkhi's or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging-more than 100,000 tokens per second even in a Perl implementation. ----------------------------------",
  "y": "motivation differences"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_4",
  "x": "With a more accurate model, however, we found <cite>(Moore, 2014</cite> ) that while Ratnaparkhi's tag dictionary decreased the average number of tags per token from 45 to 3.7 on the current standard WSJ development set, it also decreased per-tag accuracy from 97.31% to 97.19%. This loss of accuracy can be explained by the fact that 0.5% of the development set tokens are known words with a tag not seen in the training set, for which our model achieved 44.5% accuracy with all word/tag pairs permitted. With Ratnaparkhi's dictionary, accuracy for these tokens is necessarily 0%.",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_5",
  "x": "---------------------------------- **OUR PREVIOUS METHOD** We previously presented<cite> (Moore, 2014)</cite> a tag dictionary constructed by using the annotated training set to compute a smoothed probability estimate for any possible tag given any possible word, and for each word in the training set, including in the dictionary the tags having an estimated probability greater than a fixed threshold T .",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_6",
  "x": "The model structure, feature set, and learning method we use for POS tagging are essentially the same as those in our earlier work, treating POS tagging as a single-token independent multiclass classification task. Word-class-sequence features obtained by supervised clustering of the annotated training set replace the hidden tag-sequence features frequently used for POS tagging, and additional word-class features obtained by unsupervised clustering of a very large unannotated corpus provide information about words not occurring in the training set. For full details of the feature set, see our previous paper<cite> (Moore, 2014)</cite> .",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_7",
  "x": "The tagger we used is based on the fastest of the methods described in our previous work <cite>(Moore, 2014</cite>, Section 3.1) . Tagging took about 26 hours using a single-threaded implementation in Perl on a Linux workstation equipped with Intel Xeon X5550 2.67 GHz processors. ----------------------------------",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_8",
  "x": "The last column shows the tagging speed in tokens per second for each of the three tag dictionaries, using the fast tagging method we previously described<cite> (Moore, 2014)</cite> , in a singlethreaded implementation in Perl on a Linux workstation equipped with Intel Xeon X5550 2.67 GHz processors. Speed is rounded to the nearest 1,000 tokens per second, because we measured times to a precision of only about one part in one hundred. For the pruned KN-smoothed dictionary, we previously reported a speed of 49,000 tokens per second under similar conditions.",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_9",
  "x": "The most restrictive tag dictionary, the pruned semi-supervised dictionary, allows only 1.51 tags per token, and our implementation runs at 103,000 tokens per second on the WSJ development set. For our final experiments, we tested our tagger with this dictionary on the standard Penn Treebank WSJ test set and on the Penn Treebank-3 parsed Brown corpus subset, as an out-of-domain evaluation. For comparison, we tested our previous tagger and the fast version (english-left3words-distsim) of the Stanford tagger (Toutanova et al., 2003; Manning, 2011) recommended for practical use on the Stanford tagger website, which we found to be by far the fastest of the six publicly available taggers tested in our previous work<cite> (Moore, 2014)</cite> .",
  "y": "background"
 },
 {
  "id": "b5149b6136c8baaed8356b562d3f96_10",
  "x": "**CONCLUSIONS** Our method of constructing a tag dictionary is technically very simple, but remarkably effective. It reduces the mean number of possible tags per token by 57% and increases the number of unambiguous tokens by by 47%, compared to the previous state of the art<cite> (Moore, 2014)</cite> for a tag dictionary that does not degrade tagging accuracy.",
  "y": "differences"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_0",
  "x": "We hypothesize that semantic roles can be especially beneficial in NMT, as 'argument switching' (flipping arguments corresponding to different roles) is one of frequent and severe mistakes made by NMT systems (Isabelle et al., 2017) . There is a limited amount of work on incorporating graph structures into neural sequence models. Though, unlike semantics in NMT, syntactically-aware NMT has been a relatively hot topic recently, with a number of approaches claiming improvements from using treebank syntax Eriguchi et al., 2016; Nadejde et al., 2017;<cite> Bastings et al., 2017</cite>; Aharoni and Goldberg, 2017) , our graphs are different from syntactic structures.",
  "y": "differences background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_1",
  "x": "Though, unlike semantics in NMT, syntactically-aware NMT has been a relatively hot topic recently, with a number of approaches claiming improvements from using treebank syntax Eriguchi et al., 2016; Nadejde et al., 2017;<cite> Bastings et al., 2017</cite>; Aharoni and Goldberg, 2017) , our graphs are different from syntactic structures. Unlike syntactic dependency graphs, they are not trees and thus cannot be processed in a bottom-up fashion as in Eriguchi et al. (2016) or easily linearized as in Aharoni and Goldberg (2017) . Luckily, the modeling approach of <cite>Bastings et al. (2017)</cite> does not make any assumptions about the graph structure, and thus we build on their method.",
  "y": "similarities background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_2",
  "x": "<cite>Bastings et al. (2017)</cite> used Graph Convolutional Networks (GCNs) to encode syntactic structure. GCNs were originally proposed by Kipf and Welling (2016) and modified to handle labeled and automatically predicted (hence noisy) syntactic dependency graphs by . Representations of nodes (i.e. words in a sentence) in GCNs are directly influenced by representations of their neighbors in the graph. The form of influence (e.g., transition matrices and parameters of gates) are learned in such a way as to benefit the end task (i.e. translation). These linguistically-aware word representations are used within a neural encoder.",
  "y": "background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_3",
  "x": "As we use exactly the same modeling approach as in the syntactic method of <cite>Bastings et al. (2017)</cite> , we can easily compare the influence of the types of linguistic structures (i.e., syntax vs. semantics). We observe that when using full WMT data we obtain better results with semantics than with syntax (23.9 BLEU for syntactic GCN). Using syntactic and semantic GCN together, we obtain a further gain (24.9 BLEU) that suggests the complementarity of syntax and semantics.",
  "y": "motivation uses similarities"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_4",
  "x": "Graph neural networks are a family of neural architectures (Scarselli et al., 2009; Gilmer et al., 2017) specifically devised to induce representation of nodes in a graph relying on its graph structure. Graph convolutional networks (GCNs) belong to this family. While GCNs were introduced BiRNN CNN Baseline<cite> (Bastings et al., 2017)</cite> 14.9 12.6 +Sem 15.6 13.4 +Syn<cite> (Bastings et al., 2017)</cite> 16.1 13.7 +Syn + Sem 15.8 14.3 for modeling undirected unlabeled graphs (Kipf and Welling, 2016) , in this paper we use a formulation of GCNs for labeled directed graphs, where the direction and the label of an edge are incorporated.",
  "y": "differences background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_5",
  "x": "In particular, we follow the formulation of and <cite>Bastings et al. (2017)</cite> for syntactic graphs and apply it to dependency-based semantic-role structures (Hajic et al., 2009 ) (as in Figure 1 ). More formally, consider a directed graph G = (V, E), where V is a set of nodes, and E is a set of edges. Each node v \u2208 V is represented by a feature vector x v \u2208 R d , where d is the latent space dimensionality.",
  "y": "uses"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_6",
  "x": "The functions g u,v are scalar gates which weight the importance of each edge. Gates are particularly useful when the graph is predicted BiRNN Baseline<cite> (Bastings et al., 2017)</cite> 23.3 +Sem 24.5 +Syn<cite> (Bastings et al., 2017)</cite> 23.9 +Syn + Sem 24.9 and thus may contain errors, i.e., wrong edges. In this scenario gates can down weight the influence of such edges.",
  "y": "background"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_7",
  "x": "The settings and the framework (Neural Monkey (Helcl and Libovick\u00fd, 2017) ) used for experiments are the ones used in <cite>Bastings et al. (2017)</cite> , which we use as baselines. As RNNs, we use GRUs (Cho et al., 2014) . We now discuss the impact that different architectures and linguistic information have on the translation quality.",
  "y": "uses"
 },
 {
  "id": "b71321a9252376308d627c439e85b7_8",
  "x": "First, we start with experiments with the smaller News Commentary training set (See Table 1 ). As in <cite>Bastings et al. (2017)</cite> , we used the standard attention-based encoder-decoder model as a baseline. We tested the impact of semantic GCNs when used on top of CNN and BiRNN encoders.",
  "y": "uses"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_0",
  "x": "In some cases, the code was initially made available, then removed, and is now back online <cite>(Tang et al., 2016a)</cite> . Similarly, when others (Tay et al., 2017; Chen et al., 2017) attempt to replicate the experiments of<cite> Tang et al. (2016a)</cite> they also produce different results to the original authors.",
  "y": "background"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_1",
  "x": "In some cases, when code has been released, it is difficult to use which could explain why the results were not reproduced. Of course, we would not expect researchers to produce industrial strength code, or provide continuing free ongoing support for multiple years after publication, but the situation is clearly problematic for the development of the new field in general. In this paper, we therefore reproduce three papers chosen as they employ widely differing methods: Neural Pooling (NP) , NP with dependency parsing (Wang et al., 2017) , and RNN <cite>(Tang et al., 2016a)</cite> , as well as having been applied largely to different datasets.",
  "y": "motivation"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_2",
  "x": "In IR, specific reproducible research tracks have been created 3 and we are pleased to see the same happening at COLING 2018 4 . Turning now to the focus of our investigations, Target Dependent sentiment analysis (TDSA) research (Nasukawa and Yi, 2003) arose as an extension to the coarse grained analysis of document level sentiment analysis (Pang et al., 2002; Turney, 2002) . Since its inception, papers have applied different methods such as feature based (Kiritchenko et al., 2014) , Recursive Neural Networks (RecNN) (Dong et al., 2014) , Recurrent Neural Networks (RNN) <cite>(Tang et al., 2016a)</cite> , attention applied to RNN (Wang et al., 2016; Chen et al., 2017; Tay et al., 2017) , Neural Pooling (NP) Wang et al., 2017) , RNN combined with NP (Zhang et al., 2016) , and attention based neural networks (Tang et al., 2016b) .",
  "y": "background"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_3",
  "x": "There are two papers taking a similar approach to our work in terms of generalisability although they do not combine them with the reproduction issues that we highlight. First, Chen et al. (2017) compared results across SemEval's laptop and restaurant reviews in English (Pontiki et al., 2014) , a Twitter dataset (Dong et al., 2014) and their own Chinese news comments dataset. They did perform a comparison across different languages, domains, corpora types, and different methods; SVM with features (Kiritchenko et al., 2014) , Rec-NN (Dong et al., 2014) , TDLSTM <cite>(Tang et al., 2016a)</cite> , Memory Neural Network (MNet) (Tang et al., 2016b) and their own attention method.",
  "y": "background"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_4",
  "x": "All of the methods outputs are fed into a softmax activation function. The experiments are performed on the Dong et al. (2014) dataset where we train and test on the specified splits. For the LSTMs we initialised the weights using uniform distribution U(0.003, 0.003), used Stochastic Gradient Descent (SGD) a learning rate of 0.01, cross entropy loss, padded and truncated sequence to the length of the maximum sequence in the training dataset as stated in the original paper, and we did not \"set the clipping threshold of softmax layer as 200\" <cite>(Tang et al., 2016a)</cite> as we were unsure what this meant.",
  "y": "extends differences"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_5",
  "x": "Overall, we were able to reproduce the results of all three papers. However for the neural network/deep learning approach of<cite> Tang et al. (2016a)</cite> we agree with Reimers and Gurevych (2017) that reporting multiple runs of the system over different seed values is required as the single performance scores can be misleading, which could explain why previous papers obtained different results to the original for the TDLSTM method (Chen et al., 2017; Tay et al., 2017) . ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "b7a718664f395f048abb3655fb1d8d_6",
  "x": "For all of the methods we pre-processed the text by lower casing and tokenising using Twokenizer (Gimpel et al., 2011) , and we used all three sentiment lexicons where applicable. We found the best word vectors from SSWE and the common crawl 42B 300 dimension Glove vectors by five fold stratified cross validation for the NP methods and the highest accuracy on the validation set for the LSTM methods. We chose these word vectors as they have very different sizes (50 and 300), also they have been shown to perform well in different text types; SSWE for social media <cite>(Tang et al., 2016a)</cite> and Glove for reviews (Chen et al., 2017) .",
  "y": "similarities uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_0",
  "x": "Lao and Cohen (2010) introduced a new method that predicts direct links based on paths that connect the source and target nodes. Such paths are not only useful for link prediction (Lao et al., 2011; <cite>Gardner et al., 2014)</cite> , but also for finding explanations for direct links and help with targeted information extraction to fill in incomplete knowledge repositories (Yin et al., 2018; Zhou and Nastase, 2018) . These approaches rely on the structure of the knowledge graph, which is inherently incomplete.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_1",
  "x": "We investigate whether in this graph that represents a more general version of the information in the original KG, good patterns/paths are stronger and easier to find, because the aggregated view compensates for individual missing edges throughout the graph. We test the extracted paths through the link prediction task on Freebase (Bollacker et al., 2008) and NELL (Carlson et al., 2010a) , using<cite> Gardner et al. (2014)</cite> 's experimental set-up: pairs of nodes are represented using their connected paths as fea-tures, and a model for predicting the direct relations is learned and tested on training and test sets for 24 relations in Freebase and 10 relations in NELL. Our analysis shows that we find different and much fewer paths than the PRA method does (mostly because the abstract paths do not contain back-and-forth sequences of generalizing or type relations).",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_2",
  "x": "Lao and Cohen (2010) introduced a novel way to exploit information in knowledge graphs: using weighted extracted paths as features in four different recommendation tasks, which can be modeled as typed proximity queries. The idea of using paths in the graph has then been applied to the task of link prediction (Lao et al., 2011) , and extended to incorporate textual information<cite> (Gardner et al., 2014)</cite> . Lao et al. (2011) obtain paths for given node pairs using random walks over the knowledge graph.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_3",
  "x": "The paths themselves can be incorporated in different ways in a model -as features (Lao et al., 2011; <cite>Gardner et al., 2014)</cite> , as Horn clauses to provide rules for inference in KGs whether directly or through scores that represent the strength of the path as a direct relation (Neelakantan et al., 2015; Guu et al., 2015) , also taking into account information about intermediary nodes (Das et al., 2017; Yin et al., 2018) . Gardner and Mitchell (2015) perform link prediction using random walks but do not attempt to connect a source and target node, but rather to characterize the local structure around a (source or target) node using such localized paths. Using these subgraph features leads to better results for the knowledge graph completion task.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_4",
  "x": "Wang et al. (2013) have a different approachthey start with patterns in the form of first-order probabilistic rules, which they then ground in a small subgraph of a large knowledge graph. The approach we present here combines different elements of these previous approaches in a novel way: we build an abstract graph to find pat-terns that would be similar to those used by (Wang et al., 2013) . To test the quality of these paths we ground them using the original KG and use these grounded paths in a learning framework similar to<cite> (Gardner et al., 2014)</cite> .",
  "y": "similarities"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_5",
  "x": "R is the set of relation types of KG, R set = {intersection, subset, superset} 1 . weighted edges where the weight of a set relation between KG A 's nodes quantifies the overlap between the two sets: Figure 1: Knowledge graphs statistics on a logarithmic scale: relation and nodes frequencies for Freebase and NELL (the version used by<cite> (Gardner et al., 2014)</cite> and in this paper).",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_6",
  "x": "---------------------------------- **ABSTRACT PATHS** The Path Ranking Algorithm formalism originally proposed by (Lao and Cohen, 2010) performs two main steps to represent of a pair of nodes in a graph: (i) feature selection -adding paths that connect the node pair; (ii) feature computation - Table 1 : Graph statistics on the datasets used by<cite> (Gardner et al., 2014)</cite> , and their abstract versions associating a value for each added path.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_7",
  "x": "Algorithms that harness path information often mine paths either by performing costly random walks (Guu et al., 2015) , traversals<cite> (Gardner et al., 2014</cite>; Neelakantan et al., 2015; Das et al., 2016) or by constructing paths through generative models (Das et al., 2017; Ding et al., 2018) . Here, we adopt a different approach, by abstracting the graph first, then finding paths in this graph through traversal algorithms. For a relation r i , we start at its domain (source) node V i,s and search for a path to its range (target) node V i,t using breadth first search.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_8",
  "x": "Figure 1a shows that about 60% of Freebase nodes have degree higher than 10, which leads to an exponential growth in the number of paths starting in a node. Algorithms that harness path information often mine paths either by performing costly random walks (Guu et al., 2015) , traversals<cite> (Gardner et al., 2014</cite>; Neelakantan et al., 2015; Das et al., 2016) or by constructing paths through generative models (Das et al., 2017; Ding et al., 2018) . Here, we adopt a different approach, by abstracting the graph first, then finding paths in this graph through traversal algorithms.",
  "y": "differences background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_9",
  "x": "**GROUNDED PATHS** The abstract paths are hypothetical paths that could connect the source s and target t of a < s, r, t > tuple. They can be used in different ways, e.g. (i) as features in a link prediction system (e.g.<cite> (Gardner et al., 2014)</cite> ), (ii) to fill in larger portions of the graph by producing, rather than finding, groundings of the path for specific instances.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_10",
  "x": "They can be used in different ways, e.g. (i) as features in a link prediction system (e.g.<cite> (Gardner et al., 2014)</cite> ), (ii) to fill in larger portions of the graph by producing, rather than finding, groundings of the path for specific instances. In the work presented here we test the abstract paths through the link prediction task, so we will try to ground abstract paths for relation instances in the training and test data. After finding the set of abstract paths {\u03c0 i,r } associated with a relation r, for a given instance of the relation r -< s, r, t > -we can (try to) ground the paths as follows: (i) we first eliminate set relations from the abstract paths: at this point set relations between relation types domain and ranges are not useful (they were necessary only for the connectivity and search process in the abstract graph).",
  "y": "uses background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_11",
  "x": "Because we want to compare the abstract paths found using the abstract graph with paths found using PRA, we use the experimental set-up of<cite> (Gardner et al., 2014)</cite> , where we replace the feature selection and feature computation steps with the approach presented here. A big difference will be caused by the negative sampling, which also makes the results not directly comparable. The issues are explained in the negative sampling paragraph below.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_12",
  "x": "A big difference will be caused by the negative sampling, which also makes the results not directly comparable. The issues are explained in the negative sampling paragraph below. The data thus obtained is used for training a linear regression model (similarly to<cite> (Gardner et al., 2014)</cite> ), and tested on the provided test sets and evaluated using mean average precision (MAP).",
  "y": "similarities"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_13",
  "x": "**DATA** We build abstract graphs and paths from the Freebase and NELL data described in<cite> (Gardner et al., 2014)</cite> . We then use the extracted paths for link prediction.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_14",
  "x": "The graphs built by<cite> Gardner et al. (2014)</cite> cover several variations, where the KGs were enhanced with < subject, verb, object > triples extracted from dependency parses of ClueWeb documents. Table 1 shows the statistics for each original and abstract graph. The generated abstract graph is several degrees of magnitude smaller compared to the original KG.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_15",
  "x": "We build abstract graphs and paths from the Freebase and NELL data described in<cite> (Gardner et al., 2014)</cite> . The graphs built by<cite> Gardner et al. (2014)</cite> cover several variations, where the KGs were enhanced with < subject, verb, object > triples extracted from dependency parses of ClueWeb documents.",
  "y": "uses background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_16",
  "x": "The SVO triples bring in numerous low frequency relations, that without additional processing are not beneficial. The results presented by<cite> Gardner et al. (2014)</cite> show that this configuration very rarely (and never overall) leads to better results than the other graph variations. The numerous relation types brought in by the SVO triples also lead to high computation time for the abstract graph: its shortcoming is the computation of set relations between the different relations' domains and ranges, which grows quadratically with the number of relation types.",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_17",
  "x": "Each relation has a training and test set, whose numbers vary quite a bit, as shown through the statistics in Table 2 . Negative sampling The number of negative instances used in<cite> (Gardner et al., 2014)</cite> is not clearly stated. Both the number and methods of generating the negative samples can impact the results (Kotnis and Nastase, 2018) .",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_18",
  "x": "---------------------------------- **RESULTS AND DISCUSSION** The overall results of the experiments are presented in Table 3, and the relation-level results are  in Tables 4 for NELL, and 5 Table 3 : Results on the three graph variations of Freebase and NELL as reported by<cite> (Gardner et al., 2014)</cite> (G) and using abstract graphs (KG A ).",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_19",
  "x": "**RESULTS AND DISCUSSION** The overall results of the experiments are presented in Table 3, and the relation-level results are  in Tables 4 for NELL, and 5 Table 3 : Results on the three graph variations of Freebase and NELL as reported by<cite> (Gardner et al., 2014)</cite> (G) and using abstract graphs (KG A ). Overall, the results indicate that enhancing Freebase and NELL with additional facts from textual sources leads to better results, particularly when these additional facts (< subject, verb, object > triples) are processed and clustered using low dimensional dense representations<cite> Gardner et al. (2014</cite>; use embeddings obtained by running PCA on the matrix of SVO triples).",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_20",
  "x": "More than 500 relation types in Freebase have less than 10 instances, wheres NELL does not have this issue (see Figures 1a and 1b) . Because we test the approach for knowledge graph completion using classification based on the patterns as features, having features that appear too Table 4 : Relation results for the NELL KB. The second column is the best result for each relation reported by<cite> (Gardner et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_21",
  "x": "A more complete picture emerges when we look at the paths found, and compare them with the paths obtained with the PRA approach 3 . For all Freebase KG configurations,<cite> Gardner et al. (2014)</cite> have 1000 paths for most relations (approx. 6 of the relations have between 230 and 973). For NELL the number varies more, between 58 and 5509, 6 of the relations have more than 1000 metapaths.",
  "y": "background"
 },
 {
  "id": "b7a7b7d9a6594dadb5853c49cddddf_22",
  "x": "NELL The results for each relation in terms of average precision are presented in Table 4 . We include the best result on PRA (on any variation of the graph), as reported by<cite> (Gardner et al., 2014)</cite> , although since we used different negative instances the results are not directly comparable. Several of the NELL target relations have interesting patterns in the abstract graph, in particular StadiumLocatedInCity, TeamPlaysInLeague.",
  "y": "differences uses"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_0",
  "x": "For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Sudo et al., 2001;<cite> Sudo et al., 2003</cite>; Yangarber, 2003) . In these approaches extraction patterns are essentially parts of the dependency tree.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_1",
  "x": "A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) while <cite>Sudo et al. (2003)</cite> allow any subtree within the dependency parse to act as an extraction pattern. Stevenson and Greenwood (2006) showed that the choice of pattern model has important implications for IE algorithms including significant differences between the various models in terms of their ability to identify information of interest in text.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_2",
  "x": "Predicate-Argument Model (SVO): A simple approach, used by Yangarber et al. (2000) , Yangarber (2003) and , is to use subject-verb-object tuples from the dependency parse as extraction patterns. This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1 The formalism used for representing dependency patterns is similar to the one introduced by <cite>Sudo et al. (2003)</cite> .",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_3",
  "x": "This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1 The formalism used for representing dependency patterns is similar to the one introduced by <cite>Sudo et al. (2003)</cite> . Each node in the tree is represented in the format a[b/c] (e.g. subj[N/Acme]) where c is the lexical item (Acme), b its grammatical tag (N) and a the dependency relation between this node and its parent (subj). The relationship between nodes is represented as X(A+B+C) which indicates that nodes A, B and C are direct descendents of node X.",
  "y": "uses"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_4",
  "x": "Example linked chains are shown in Figure 2 . This pattern representation encodes most of the information in the sentence with the advantage of being able to link together event participants which neither of the SVO or chain model can, for example the relation between \"Smith\" and \"Bloggs\" in Figure 1 . Subtrees: The final model to be considered is the subtree model<cite> (Sudo et al., 2003)</cite> .",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_5",
  "x": "**PREVIOUS COMPARISONS** There have been few direct comparisons of the various pattern models. <cite>Sudo et al. (2003)</cite> compared three models (SVO, chains and subtrees) on two IE scenarios using a entity extraction task.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_6",
  "x": "There is some agreement between these two studies, for example that the SVO model performs poorly in comparison with other models. However, Stevenson and Greenwood (2006) also found that the coverage of the chain model was significantly worse than the subtree model, although <cite>Sudo et al. (2003)</cite> found that in some cases their performance could not be distinguished. In addition to these disagreements, these studies are also limited by the fact that they are indirect; they do not evaluate the various pattern models on an IE task.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_7",
  "x": "**EXPERIMENTS** We compared each of the patterns models described in Section 2 using an unsupervised IE experiment similar to one described by <cite>Sudo et al. (2003)</cite> . Let D be a corpus of documents and R a set of documents which are relevant to a particular extraction task.",
  "y": "uses"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_8",
  "x": "Equation 1 combines two factors: the term frequency (in relevant documents) and inverse document frequency (across the corpus). Patterns which occur frequently in relevant documents without being too prevalent in the corpus are preferred. <cite>Sudo et al. (2003)</cite> found that it was important to find the appropriate balance between these two factors.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_9",
  "x": "We made use of a version of the MUC-6 corpus described by Soderland (1999) which consists of 598 documents. For these experiments relevant documents were identified using annotations in the corpus. However, this is not necessary since <cite>Sudo et al. (2003)</cite> showed that adequate knowledge about document relevance could be obtained automatically using an IR system.",
  "y": "differences"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_10",
  "x": "In addition, <cite>Sudo et al. (2003)</cite> only generated subtrees which appeared in at least three documents. Kudo et al. (2005) and <cite>Sudo et al. (2003)</cite> both used the rightmost extension algorithm to generate subtrees. To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_11",
  "x": "Kudo et al. (2005) and <cite>Sudo et al. (2003)</cite> both used the rightmost extension algorithm to generate subtrees. To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed. Table 1 shows the number of patterns generated for each of the four models when the patterns are both filtered and unfiltered. (Although the set of unfiltered subtree patterns were not generated it is possible to determine the number of patterns which would be generated using a process described by Stevenson and Greenwood (2006 Table 1 : Number of patterns generated by each model It can be seen that the various pattern models generate vastly different numbers of patterns and that the number of subtrees is significantly greater than the other three models.",
  "y": "background"
 },
 {
  "id": "b8244f9337456f1f90a576b2398680_12",
  "x": "**PARAMETER TUNING** The value of \u03b2 in equation 1 was set using a separate corpus from which the patterns were generated, a methodology suggested by <cite>Sudo et al. (2003)</cite> . To generate this additional text we used the Reuters Corpus (Rose et al., 2002 ) which consists of a year's worth of newswire output.",
  "y": "uses"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_0",
  "x": "---------------------------------- **INTRODUCTION** Neural machine translation has achieved great success in the last few years (Bahdanau et al., 2014; Gehring et al., 2017;<cite> Vaswani et al., 2017)</cite> .",
  "y": "motivation"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_1",
  "x": "**INTRODUCTION** Neural machine translation has achieved great success in the last few years (Bahdanau et al., 2014; Gehring et al., 2017;<cite> Vaswani et al., 2017)</cite> . The Transformer <cite>(Vaswani et al., 2017)</cite> , which has outperformed previous RNN/CNN based translation models (Bahdanau et al., 2014; Gehring et al., 2017) , is based on multi-layer self-attention networks and can be trained very efficiently.",
  "y": "motivation"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_2",
  "x": "However, even with residual connections and layer normalization, deep Transformers are still hard to train: the original Transformer <cite>(Vaswani et al., 2017)</cite> only contains 6 encoder/decoder layers. show that Transformer models with more than 12 encoder layers fail to converge, and propose the Transparent Attention (TA) mechanism which weighted combines outputs of all encoder layers as encoded representation. However, the TA mechanism has to value outputs of shallow encoder layers to feedback sufficient gradients during back-propagation to ensure their convergence, which implies that weights of deep layers are likely to be hampered and against the motivation when go very deep, and as a result cannot get further improvements with more than 16 layers. reveal that deep Transformers with proper use of layer normalization is able to converge and propose to aggregate previous layers' outputs for each layer instead of at the end of encoding.",
  "y": "background"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_3",
  "x": "The official implementation of the Transformer uses a different computation sequence (Figure 1 b) compared to the published version <cite>(Vaswani et al., 2017)</cite> (Figure 1 a), since it seems better for harder-to-learn models 1 . Though several papers Domhan, 2018) mentioned this change, how this modification impacts on the performance of the Transformer, especially for deep Transformers, has never been deeply studied before with empirical results to the best of our knowledge, except analyzed the difference between two computation orders during back-propagation, and Zhang et al. (2019) point out the same effects of normalization in concurrent work. In order to compare with , we used the datasets from the WMT 14 English to German task and the WMT 15 Czech to English task for experiments.",
  "y": "background"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_4",
  "x": "In order to compare with , we used the datasets from the WMT 14 English to German task and the WMT 15 Czech to English task for experiments. We applied joint Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 32k merge operations. We used the same setting as the Transformer base <cite>(Vaswani et al., 2017)</cite> except the number of warm-up steps was set to 8k.",
  "y": "extends differences"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_5",
  "x": "Parameters were initialized with Glorot Initialization 2 (Glorot and Bengio, 2010) like in many other Transformer implementation (Klein et al., 2017; Hieber et al., 2017; . Our experiments run on 2 GTX 1080 Ti GPUs, and a batch size of at least 25k target tokens is achieved through gradient accumulation of small batches. We used a beam size of 4 for decoding, and evaluated tokenized case-sensitive BLEU with the averaged model of the last 5 checkpoints saved with an interval of 1,500 training steps <cite>(Vaswani et al., 2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_6",
  "x": "v1 and v2 stand for the computation order of the proposed Transformer <cite>(Vaswani et al., 2017)</cite> and that of the official implementation respectively. \"\u00ac\" means fail to converge, \"None\" means not reported in original works, \"*\" indicates our implementation of their approach. \u2020 and \u2021 mean p < 0.01 and p < 0.05 while comparing between v1 and v2 of the same number of layers in significance test. ----------------------------------",
  "y": "background"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_7",
  "x": "---------------------------------- **CONCLUSION** In contrast to all previous works Wu et al., 2019) which show that deep Transformers with the computation order as in<cite> Vaswani et al. (2017)</cite> have difficulty in convergence.",
  "y": "background"
 },
 {
  "id": "bb133ba3dfe483412672b44b777c4a_8",
  "x": "In contrast to all previous works Wu et al., 2019) which show that deep Transformers with the computation order as in<cite> Vaswani et al. (2017)</cite> have difficulty in convergence. We empirically show that deep Transformers with the original computation order can converge as long as with proper parameter initialization. In this paper, we first investigate convergence differences between the published Transformer <cite>(Vaswani et al., 2017)</cite> and the official implementation of the Transformer , and compare the differences of computation orders between them.",
  "y": "differences"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_0",
  "x": "When all the classes are present, ROOT9 achieves an F1 score of 90.7%, against a baseline of 57.2% (vector cosine). When the classification is binary, ROOT9 achieves the following results against the baseline: hypernyms-co-hyponyms 95.7% vs. 69.8%, hypernyms-random 91.8% vs. 64.1% and co-hyponyms-random 97.8% vs. 79.4%. In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the<cite> Weeds et al. (2014)</cite> datasets, proving that it is in fact competitive.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_1",
  "x": "Hypernymy in fact represents a key organization principle of semantic memory (Murphy, 2002) , the backbone of taxonomies and ontologies, and one of the crucial semantic relations supporting lexical entailment (Geffet and Dagan, 2005) . Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar<cite> (Weeds et al., 2014)</cite> . The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on<cite> (Weeds et al., 2014</cite>; Tungthamthiti et al. 2015) .",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_2",
  "x": "The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on<cite> (Weeds et al., 2014</cite>; Tungthamthiti et al. 2015) . For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014<cite> , Weeds et al., 2014</cite> Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003) . Both supervised and unsupervised approaches have been investigated.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_3",
  "x": "For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014<cite> , Weeds et al., 2014</cite> Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003) . Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in<cite> Weeds et al. (2014)</cite> , even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b) , a supervised method based on a Random Forest algorithm and thirteen corpus-based features.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_4",
  "x": "For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014<cite> , Weeds et al., 2014</cite> Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003) . Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in<cite> Weeds et al. (2014)</cite> , even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b) , a supervised method based on a Random Forest algorithm and thirteen corpus-based features.",
  "y": "extends background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_5",
  "x": "On the 9,600 pairs, ROOT9 achieved an F1 score of 90.7% when the three classes were present, 95.7% when we had to discriminate hypernyms and co-hyponyms, 91.8% for hypernyms and randoms, and 97.8% for co-hyponyms and randoms. In order to compare ROOT9 with the state-of-the-art, we have also evaluated it in the<cite> Weeds et al. (2014)</cite> datasets. Unfortunately, ROOT9 was not able to cover the full datasets, as several words in their pairs were missing from our Distributional Semantic Model (DSM) because of their low frequency.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_6",
  "x": "Nevertheless, the authors kindly provided the results of their models on our subsets, so that the comparison can be considered reliable. Also in 1 The 9,600 pairs are available at https://github.com/esantus/ROOT9 relation to the state of the art, ROOT9 is proved to be competitive, being slightly outperformed in all the datasets only by the svmCAT model<cite> (Weeds et al., 2014)</cite> , which is a Support Vector Machine (SVM) classifier run on the concatenation of the distributional vectors of the words in the pairs. Finally, we carried out an extra test to verify whether the system was actually learning the semantic relation between two word pairs, or simply identifying prototypical hypernyms (Levy et al. 2015) .",
  "y": "differences"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_7",
  "x": "In their evaluation, the authors have shown that hypernyms' most typical contexts are in fact less informative than hyponyms' ones. Among the supervised methods, Baroni et al. (2012) proposed to use an SVM classifier on the concatenation (after having tried also subtraction and division) of the vectors. Roller et al. (2014) used the vectors' difference, while<cite> Weeds et al. (2014)</cite> implemented numerous combinations (difference, multiplication, sum, concatenation, etc.), comparing them against the most common unsupervised methods.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_8",
  "x": "---------------------------------- **TASKS** We have performed three tasks: i) an ablation test to evaluate the contribution of the features on our dataset (henceforth, ROOT9 Dataset; see Section 4.2); ii) an evaluation against the state of the art, and -in particularagainst the best performant models in<cite> Weeds et al. (2014)</cite> ; iii) an evaluation on switched pairs to verify whether the actual semantic relations or the prototypical hypernyms (Levy et al., 2015) were learnt.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_9",
  "x": "F1 score on a 10-fold cross validation was chosen as accuracy measure. The second task, which is described in Section 6, consisted in binary classification tasks on the four datasets proposed by<cite> Weeds et al. (2014)</cite> . These datasets are described below, in Section 4.3.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_10",
  "x": "These datasets are described below, in Section 4.3. The task allowed us to compare ROOT9 against the state of the art models reported in<cite> Weeds et al. (2014)</cite> . The last task is described in Section 7.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_11",
  "x": "---------------------------------- **WEEDS DATASET** In order to compare ROOT9 to the state-of-the-art, we have evaluated it with the datasets created by<cite> Weeds et al. (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_12",
  "x": "2 These are four datasets, containing respectively: i) hypernyms versus other relations (extracted from WordNet; henceforth WN Hyper); ii) co-hyponyms versus other relations (extracted from WordNet; henceforth WN Co-Hyp); iii) hypernyms versus other relations (extracted from BLESS; henceforth Bless Hyper); iv) co-hyponyms versus other relations (extracted from BLESS; henceforth Bless Co-Hyp). The WN dataset<cite> (Weeds et al., 2014</cite> ) -meaning both WN Hyper and WN Co-Hyp -in particular, was built after noticing that supervised systems tended to perform well also on random vectors. This happens because they are able to learn ontological information and re-use it whenever the words re-appear in other pairs.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_13",
  "x": "In order to compare ROOT9 to the state-of-the-art, we have evaluated it with the datasets created by<cite> Weeds et al. (2014)</cite> . The WN dataset<cite> (Weeds et al., 2014</cite> ) -meaning both WN Hyper and WN Co-Hyp -in particular, was built after noticing that supervised systems tended to perform well also on random vectors.",
  "y": "uses background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_15",
  "x": "The discrepancy with what found by<cite> Weeds et al. (2014)</cite> namely that random vectors perform particularly well when words are re-used in the dataset -may depend on the small number of features, which does not allow the system to identify discriminative random dimensions. In the second task (see Section 6), we have used as baselines the most competitive models reported in<cite> Weeds et al. (2014)</cite> , namely the SVM classifiers trained on the PPMI vector of the second word (svmSINGLE), or on the concatenated (svmCAT), summed (svmADD), multiplied (svmMULT) and subtracted (svmDIFF) PPMI vectors of the words in the pair. Such vectors contain as features all major grammatical dependency relations involving open class Parts Of Speech.",
  "y": "background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_16",
  "x": "While the vector cosine achieves a reasonable accuracy, which is anyway far below the results obtained by our model, the random baseline performs much worst. The discrepancy with what found by<cite> Weeds et al. (2014)</cite> namely that random vectors perform particularly well when words are re-used in the dataset -may depend on the small number of features, which does not allow the system to identify discriminative random dimensions. In the second task (see Section 6), we have used as baselines the most competitive models reported in<cite> Weeds et al. (2014)</cite> , namely the SVM classifiers trained on the PPMI vector of the second word (svmSINGLE), or on the concatenated (svmCAT), summed (svmADD), multiplied (svmMULT) and subtracted (svmDIFF) PPMI vectors of the words in the pair.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_17",
  "x": "However, it is worth noticing here that such difference disappears with the WN datasets proposed by<cite> Weeds et al. (2014)</cite> . See section 6, and -in particular - Table 3 . F1 scores on a 10-fold cross validation for binary classification tasks.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_18",
  "x": "---------------------------------- **TASK 2: ROOT9 VS. STATE OF THE ART** In Table 4 , we show ROOT9's performance compared to the best systems reported by<cite> Weeds et al. (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_19",
  "x": "**TASK 2: ROOT9 VS. STATE OF THE ART** In Table 4 , we show ROOT9's performance compared to the best systems reported by<cite> Weeds et al. (2014)</cite> . The scores are all calculated on subsets of<cite> Weeds et al. (2014)</cite> 's datasets, as reported in Section 4.3.",
  "y": "uses"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_20",
  "x": "The scores are all calculated on subsets of<cite> Weeds et al. (2014)</cite> 's datasets, as reported in Section 4.3. Considering all the datasets, ROOT9 is the second best performing system, after svmCAT<cite> (Weeds et al., 2014)</cite> , which uses the SVM classifier on the concatenation of PPMI vectors, containing as features all major grammatical dependency relations involving open class Parts Of Speech. The SVM classifier on the sum (svmADD) and the multiplication (svmMULT) of the same PPMI vectors performs better in identifying co-hyponyms, but worst in identifying hypernyms.",
  "y": "differences background"
 },
 {
  "id": "bbb91e450b3503166bcfae60e9ba72_21",
  "x": [
   "**CONCLUSIONS** In this paper, we have described ROOT9, a classifier for hypernyms, co-hyponyms and random words that is derived from an optimization of ROOT13 (Santus et al., 2016b) . The classifier, based on the Random Forest algorithm, uses only nine unsupervised corpus-based features, which have been described, and their contribution assessed."
  ],
  "y": "uses future_work"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_0",
  "x": "This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory<cite> (Wachsmuth et al., 2017a)</cite> and one with 17 reasons for quality differences phrased spontaneously in practice (Habernal and Gurevych, 2016a) . In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4).",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_2",
  "x": "<cite>Wachsmuth et al. (2017a)</cite> point out that dialectical builds on rhetorical, and rhetorical builds on logical quality. They derive a unifying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions. 5-1 B is attacking / abusive.",
  "y": "background"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_3",
  "x": "9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- <cite>Wachsmuth et al. (2017a)</cite> given for each of the 17+1 reason labels of Habernal and Gurevych (2016a) . Bold/gray: Highest/lowest value in each column. Bottom row: The number of labels for each dimension.",
  "y": "background"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_4",
  "x": "9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- <cite>Wachsmuth et al. (2017a)</cite> given for each of the 17+1 reason labels of Habernal and Gurevych (2016a) . These pairs represent the practical view in our experiments.",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_5",
  "x": "---------------------------------- **CORRELATIONS OF DIMENSIONS AND REASONS** For Hypotheses 1 and 2, we consider all 736 pairs of arguments from Habernal and Gurevych (2016a) where both have been annotated by <cite>Wachsmuth et al. (2017a)</cite> .",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_6",
  "x": "Only the comparably low \u03c4 of 6-1 (no credible evidence) for local acceptability (.49) and credibility (.52) seem really unexpected. Besides, the descriptions of 6-2 and 6-3 sound like local but cor- Table 4 : The mean rating for each quality dimension of those arguments from <cite>Wachsmuth et al. (2017a)</cite> given for each reason label (Habernal and Gurevych, 2016a) . The bottom rows show that the minimum maximum mean ratings are consistently higher for the positive properties than for the negative properties.",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_7",
  "x": "The correlations found imply that the relative quality differences captured are reflected in absolute differences. For explicitness, we computed the mean rating for each quality dimension of all arguments from <cite>Wachsmuth et al. (2017a)</cite> with a particular reason label from Habernal and Gurevych (2016a) . As each reason refers to one argument of a pair, this reveals whether the labels, although meant to signal relative differences, indicate absolute ratings.",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_8",
  "x": "**ABSOLUTE QUALITY RATINGS BY THE CROWD** We emulated the expert annotation process carried out by <cite>Wachsmuth et al. (2017a)</cite> on CrowdFlower in order to evaluate whether lay annotators suffice for a theory-based quality assessment. In particular, we asked the crowd to rate the same 304 arguments as the experts for all 15 given quality dimensions with scores from 1 to 3 (or choose \"cannot judge\").",
  "y": "uses"
 },
 {
  "id": "bd2a718f75d206ef3f2cb5648585d5_9",
  "x": "On one hand, we compared the mean of all 10 crowd ratings to the mean of the three ratings of <cite>Wachsmuth et al. (2017a)</cite> . On the other hand, we estimated a reliable rating from the crowd ratings using MACE (Hovy et al., 2013) and compared it to the experts. Table 5 : Mean and MACE Krippendorff's \u03b1 agreement between (a) the crowd and the experts, (b) two independent crowd groups and the experts, (c) group 1 and the experts, and (d) group 2 and the experts.",
  "y": "uses"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_0",
  "x": "Because SLMs can be trained from only unlabeled text, they can be applied for ADS even when the relations of interest are not specified in advance <cite>(Downey et al., 2007)</cite> . In this paper, we show that an ADS technique based on SLMs is improved substantially when the language model it employs becomes more accurate.",
  "y": "differences"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_1",
  "x": "The extraction task we consider is formalized as follows: given a corpus, a target relation R, a list of seed instances S R , and a list of candidate extractions U R , the task is to order elements of U R such that correct instances for R are ranked above extraction errors. Let U Ri denote the set of the ith arguments of the extractions in U R , and let S Ri be defined similarly for the seed set S R . For relations of arity greater than one, we consider the typechecking task, an important sub-task of extraction <cite>(Downey et al., 2007)</cite> .",
  "y": "uses"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_2",
  "x": "The accuracy of an n-gram model of a corpus depends on two key factors: the choice of n, and the smoothing technique employed to assign probabilities to word sequences seen infrequently in training. We experiment with choices of n from 2 to 4, and two popular smoothing approaches, Modified Kneser-Ney (Chen and Goodman, 1996) and Witten-Bell (Bell et al., 1990) . Unsupervised Hidden Markov Models (HMMs) are an alternative SLM approach previously shown to offer accuracy and scalability advantages over ngram models in ADS <cite>(Downey et al., 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_3",
  "x": "**PERFORMING ADS WITH SLMS** The Assessment by Distributional Similarity (ADS) technique is to rank extractions in U R in decreasing order of distributional similarity to the seeds, as estimated from the corpus. In our experiments, we utilize an ADS approach previously proposed for HMMs <cite>(Downey et al., 2007)</cite> and adapt it to also apply to n-gram models, as detailed below.",
  "y": "extends"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_4",
  "x": "We evaluated performance on three distinct data sets. The first two data sets evaluate ADS for unsupervised information extraction, and were taken from <cite>(Downey et al., 2007)</cite> . The first, Unary, was an extraction task for unary relations (Company, Country, Language, Film) and the second, Binary, was a type-checking task for binary relations (Conquered, Founded, Headquartered, Merged).",
  "y": "uses"
 },
 {
  "id": "bdcd8b0f3a56606427ee298d454b52_5",
  "x": "A model selection technique that picks the HMM model with lowest perplexity (HMM 1-100) results in better ADS performance than previous results. As shown in Table 2, HMM 1-100 reduces error over the HMM-T model in <cite>(Downey et al., 2007)</cite> by 26%, on average. The experiments also reveal an important difference between the HMM and n-gram approaches.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_0",
  "x": "This reduces feature sparsity and has been shown to improve PRA inference (Gardner et al., 2013) , <cite>(Gardner et al., 2014)</cite> . In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_1",
  "x": "This is different from the scheme in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> , which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_2",
  "x": "We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> .",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_3",
  "x": "We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> .",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_4",
  "x": "We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013) , and <cite>vector space random walk PRA</cite> <cite>(Gardner et al., 2014)</cite> are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> .",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_5",
  "x": "We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> .",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_6",
  "x": "Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by <cite>(Gardner et al., 2014)</cite> , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_7",
  "x": "It was extended by (Lao et al., 2012) , to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in (Gardner et al., 2013) . Instead of hard mapping of surface relations to latent embeddings, <cite>(Gardner et al., 2014 )</cite> perform a 'soft' mapping using <cite>vector space random walks</cite>.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_8",
  "x": "Instead of hard mapping of surface relations to latent embeddings, <cite>(Gardner et al., 2014 )</cite> perform a 'soft' mapping using <cite>vector space random walks</cite>. Although, like others, we too use an external corpus to augment the KB, the crucial difference in our approach is that apart from adding surface relations, we also add bridging entities that enable us to create new paths in the KB.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_10",
  "x": "PRA-SVO and <cite>PRA-VS</cite> are the systems proposed in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_11",
  "x": "---------------------------------- **PRA-SVO AND <cite>PRA-VS</cite>** PRA-SVO and <cite>PRA-VS</cite> are the systems proposed in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_12",
  "x": "**PRA-SVO AND <cite>PRA-VS</cite>** PRA-SVO and <cite>PRA-VS</cite> are the systems proposed in (Gardner et al., 2013) and <cite>(Gardner et al., 2014)</cite> respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_13",
  "x": "After augmenting the KB, we run the training phase of the PRA algorithm to obtain the feature (path) weights computed by the logistic regression Table 2 : Comparison of Mean Reciprocal Rank (MRR) metric for 10 relations from NELL (higher is better). PRA-SVO, <cite>PRA-VS</cite> are the systems proposed in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . PRA-ODA is the approach proposed in this paper.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_14",
  "x": "---------------------------------- **EXPERIMENTS** We used the implementation of PRA provided by the authors of <cite>(Gardner et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_15",
  "x": "**EXPERIMENTS** We used the implementation of PRA provided by the authors of <cite>(Gardner et al., 2014)</cite> . For our experiments, we used the same 10 NELL relation data as used in <cite>(Gardner et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_16",
  "x": "We split the NELL data into 60% training data, 15 % development data and 25% test data. Values for d max , and K, the most frequent paths, were obtained by tuning on a development set for 4 relations (athleteplaysforsport,actorstarredinmovie,citylocatedincountry (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> . PRA-ODA is the approach proposed in this paper.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_17",
  "x": "Between the two top performing systems, i.e., PRA-ODA and <cite>PRA-VS</cite>, PRA-ODA is faster by a factor of 1.8. and journalistwritesforpublication). The hyperparameter values d max = 2, K = 10 reported the highest MRR and were used for the rest of the relations.",
  "y": "differences"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_18",
  "x": "For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_19",
  "x": "For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (Gardner et al., 2013; <cite>Gardner et al., 2014)</cite> , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_20",
  "x": "We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_21",
  "x": "We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (PRA-SVO) (Gardner et al., 2013) and <cite>vector space random walk PRA</cite> (<cite>PRA-VS</cite>) <cite>(Gardner et al., 2014)</cite> . The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_22",
  "x": "The run times, i.e, the time taken to perform an entire experiment for PRA-SVO and <cite>PRA-VS</cite> includes the time taken to augment NELL KB with SVO edges. The <cite>PRA-VS</cite> runtime also includes the time taken for generating embeddings to perform the <cite>vector space random walk</cite>. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8).",
  "y": "similarities"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_23",
  "x": "In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost. In other words, these costs are likely to increase as more relations (and their instances) are included during training and testing.",
  "y": "background"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_24",
  "x": "As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, <cite>PRA-VS</cite> takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of PRA-SVO and <cite>PRA-VS</cite>, and embedding computation in case of <cite>PRA-VS</cite> are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost.",
  "y": "uses"
 },
 {
  "id": "bdd7a4dabf8a8d7c0a2b638eb6eb72_25",
  "x": "Runtime gains with PRA-ODA are likely to be even more pronounced in such settings. An additional advantage of the proposed algorithm is that it can also be run on the top of any PRA based algorithm such as the PRA-SVO and <cite>PRA-VS</cite>. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "be26538a785f9ec9edc1ea031194cf_0",
  "x": "Specifically, in the Indian subcontinent, number of Internet users has crossed 500 mi 1 , and is rising rapidly due to inexpensive data 2 . With this rise, comes the problem of hate speech, offensive and abusive posts on social media. Although there are many previous works which deal with Hindi and English hate speech (the top two languages in India), but very few on the code-switched version (Hinglish) of the two (<cite>Mathur et al. 2018</cite>) .",
  "y": "motivation"
 },
 {
  "id": "be26538a785f9ec9edc1ea031194cf_1",
  "x": "Thus, with this in mind, we build a transfer learning based model for the code-switched language Hinglish, which outperforms the baseline model of (<cite>Mathur et al. 2018</cite>) . We also release the embeddings and the model trained. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "be26538a785f9ec9edc1ea031194cf_2",
  "x": "---------------------------------- **PRE-PROCESSING** In this work, we use the datasets released by (Davidson et al. 2017 ) and HEOT dataset provided by (<cite>Mathur et al. 2018</cite>) .",
  "y": "uses"
 },
 {
  "id": "be26538a785f9ec9edc1ea031194cf_3",
  "x": "Results Table 3 shows the performance of our model (after getting trained on (Davidson et al. 2017) ) with two types of embeddings in comparison to the models by (<cite>Mathur et al. 2018</cite>) and (Davidson et al. 2017 ) on the HEOT dataset averaged over three runs. We also compare results on pre-trained embeddings. As shown in the table, our model when given Glove embeddings performs better than all other models.",
  "y": "differences"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_0",
  "x": "Since sentiment lexicons helped in improving the accuracy of sentiment classification models (Liu and Zhang, 2012; Al-Sallab et al., 2017; Badaro et al., 2014a Badaro et al., ,b, 2015 , several researchers are working on developing emotion lexicons for different languages such as English, French, Polish and Chinese (Mohammad, 2017; Bandhakavi et al., 2017; Yang et al., 2007; Mohammad and Turney, 2013; Abdaoui et al., 2017;<cite> Staiano and Guerini, 2014</cite>; Maziarz et al., 2016; Janz et al., 2017) . While sentiment is usually represented by three labels namely positive, negative or neutral, several representation models exist for emotions such as Ekman representation (Ekman, 1992) (happiness, sadness, fear, anger, surprise and disgust) or Plutchik model (Plutchik, 1994) that includes trust and anticipation in addition to Ekman's six emotions. Despite the efforts for creating large scale emotion lexicons for English, the size of existing emotion lexicons remain much smaller compared to sentiment lexicons.",
  "y": "background"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_1",
  "x": "Despite the efforts for creating large scale emotion lexicons for English, the size of existing emotion lexicons remain much smaller compared to sentiment lexicons. For example, DepecheMood<cite> (Staiano and Guerini, 2014)</cite> , one of the largest publicly available emotion lexicon for English, includes around 37K terms while SentiWordNet (SWN) (Esuli and Sebastiani, 2007; Baccianella et al., 2010) , a large scale English sentiment lexicon semi-automatically generated using English WordNet (EWN) (Fellbaum, 1998) , includes around 150K terms annotated with three sentiment scores: positive, negative and objective. In this paper, we focus on expanding coverage of existing emotion lexicon, namely DepecheMood, using the synonymy semantic relation available in English WordNet.",
  "y": "background"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_2",
  "x": "AffectNet (Cambria et al., 2012) , part of the SenticNet project, includes also around 10K terms extracted from ConceptNet (Liu and Singh, 2004) and aligned with WordNet Affect. They extended WordNet Affect using the concepts in ConceptNet. While WordNet Affect, EmoLex and AffectNet include terms with emotion labels, Affect database (Neviarouskaya et al., 2007) and DepecheMood<cite> (Staiano and Guerini, 2014)</cite> include words that have emotion scores instead, which can be useful for compositional computations of emotion scores.",
  "y": "background"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_3",
  "x": "<cite>Staiano and Guerini (2014)</cite> utilized news articles from rappler.com. The articles are accompanied by Rappler's Mood Meter, which allows readers to express their emotions about the article they are reading. DepecheMood includes around 37K lemmas along with their part of speech tags and the lemmas are aligned with EWN.",
  "y": "background"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_4",
  "x": "Three variations of score representations exist for DepecheMood. We select to expand the DepecheMood variation with normalized scores since this variation performed best according to the presented results in<cite> (Staiano and Guerini, 2014)</cite> . In Fig. 1 , we show an overview of the steps followed to expand DepecheMood.",
  "y": "extends differences"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_5",
  "x": "We considered the same emotion mapping assumptions presented in the work of<cite> (Staiano and Guerini, 2014)</cite> : Fear \u2192 Afraid, Anger \u2192 Angry, Joy \u2192 Happy, Sadness \u2192 Sad and Surprise \u2192 Inspired. Disgust was not aligned with any emotion in EmoWordNet and hence was discarded as also assumed in<cite> (Staiano and Guerini, 2014)</cite> . One important aspect of the extrinsic evaluation was checking the coverage of EmoWordNet against SemEval dataset.",
  "y": "similarities"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_6",
  "x": "The emotion labels used in the dataset correspond to the six emotions of the Ekman model (Ekman, 1992) while those in EmoWordNet, as well as DepecheMood, follow the ones provided by Rappler Mood Meter. We considered the same emotion mapping assumptions presented in the work of<cite> (Staiano and Guerini, 2014)</cite> : Fear \u2192 Afraid, Anger \u2192 Angry, Joy \u2192 Happy, Sadness \u2192 Sad and Surprise \u2192 Inspired. Disgust was not aligned with any emotion in EmoWordNet and hence was discarded as also assumed in<cite> (Staiano and Guerini, 2014)</cite> .",
  "y": "similarities"
 },
 {
  "id": "c0cac496ec0abdfd3f6bd9914f4cc4_7",
  "x": "As stated in<cite> (Staiano and Guerini, 2014)</cite> paper, 'Disgust' emotion was excluded since there was no corresponding mapping in EmoWordNet/DepecheMood. The first evaluation consisted of measuring Pearson Correlation between the scores computed using the lexicons and those provided in SemEval. The results are reported in Table 1.",
  "y": "similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_0",
  "x": "This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Malouf and van Noord, 2004; Kaplan et al., 2004; . Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; . An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur-ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; <cite>Ninomiya et al., 2006</cite>; , which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999) .",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_1",
  "x": "<cite>Ninomiya et al. (2006)</cite> showed the parsing model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures. This means that syntactic structures are almost determined by supertags as is claimed by Bangalore and Joshi (1999) . However, supertaggers themselves were heuristically used as an external tagger.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_2",
  "x": "They filter out unlikely lexical entries just to help parsing (Clark and Curran, 2004a) , or the probabilistic models for phrase structures were trained independently of the supertagger's probabilistic models (Wang and Harper, 2004; <cite>Ninomiya et al., 2006)</cite> . In the case of supertagging of Weighted CDG , parameters for Weighted CDG are manually tuned, i.e., their model is not a well-defined probabilistic model. We propose a log-linear model for probabilistic HPSG parsing in which the supertagging probabilities are introduced as a reference distribution for the probabilistic HPSG.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_4",
  "x": "Our model can be regarded as an extension of a unigram reference distribution to an n-gram reference distribution with features that are used in supertagging. We also compared with a probabilistic model in <cite>(Ninomiya et al., 2006)</cite> . The probabilities of <cite>their model</cite> are defined as the product of probabilities of supertagging and probabilities of the probabilistic model for phrase structures, but <cite>their model</cite> was trained independently of supertagging probabilities, i.e., the supertagging probabilities are not used for reference distributions.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_8",
  "x": "In our model, <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite> is used as a reference distribution. The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution.",
  "y": "uses"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_9",
  "x": "combinations , d, c, hw, hp, hl , r, d, c, hw, hp , r, d, c, hw, hl , r, d, c, sy, hw , r, c, sp, hw, hp, hl , r, c, sp, hw, hp , r, c, sp, hw, hl , r, c, sp, sy, hw , r, d, c, hp, hl , r, d, c, hp , r, d, c, hl , r, d, c, sy , r, c, sp, hp, hl , r, c, sp, hp , r, c, sp, hl , r, c, sp, sy combinations of feature templates for funary r, hw, hp, hl , r, hw, hp , r, hw, hl , r, sy, hw , r, hp, hl , r, hp , r, hl , r, sy combinations of feature templates for f root hw, hp, hl , hw, hp , hw, hl , sy, hw , hp, hl , hp , hl (Probabilistic HPSG with an n-gram reference distribution) In our model, <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite> is used as a reference distribution. The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>.",
  "y": "similarities"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_10",
  "x": "The probabilistic model of lexical entry selection and its feature templates are the same as defined in <cite>Ninomiya et al. (2006)</cite> <cite>'s model 1</cite>. The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula).",
  "y": "similarities differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_11",
  "x": "The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures.",
  "y": "similarities differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_12",
  "x": "The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures.",
  "y": "similarities differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_13",
  "x": "The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures.",
  "y": "similarities differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_14",
  "x": "The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures.",
  "y": "similarities differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_15",
  "x": "The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures. In our model, given the probabilities for lexical entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus. In <cite>their model</cite>, the parameters for phrase structures are trained without using the probabilities for lexical entries, i.e., the parameters for phrase structures are estimated so as to maximize the probabilities for phrase structures only.",
  "y": "similarities differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_16",
  "x": "The formula of our model is the same as <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite>. But, <cite>their model</cite> is not a probabilistic model with a reference distribution. Both our model and <cite>their model</cite> consist of the probabilities for lexical entries (= p model1 (T |w)) and the probabilities for phrase structures (= the rest of each formula). The only difference between our model and <cite>their model</cite> is the way of how to train model parameters for phrase structures. In both our model and <cite>their model</cite>, the parameters for lexical entries (= the parameters of p model1 (T |w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures. That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 (T |w) of both models are the same. Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures. The difference of our model and <cite>their model</cite> is the estimation of parameters for phrase structures. In our model, given the probabilities for lexical entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus. In <cite>their model</cite>, the parameters for phrase structures are trained without using the probabilities for lexical entries, i.e., the parameters for phrase structures are estimated so as to maximize the probabilities for phrase structures only. That is, the parameters for lexical entries and the parameters for phrase structures are trained independently in <cite>their model</cite>.",
  "y": "similarities differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_17",
  "x": "We must admit that the difference between our models and <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite> was not as great as the difference from 's model, but 'our model 1' achieved 0.56 points higher F-score, and 'our model 2' achieved 0.8 points higher F-score. When the automatic POS tagger was introduced, Fscore dropped by around 2.4 points for all models. We also compared our model with Matsuzaki et al. (2007) 's model.",
  "y": "differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_18",
  "x": "Their parser ran around 6 times faster than <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>, 9 times faster than 'our model 1' and 60 times faster than 'our model 2.' Instead, our models achieved better accuracy. 'our model 1' had around 0.5 higher F-score, and 'our model 2' had around 0.8 points higher F-score. Their efficiency is mainly due to elimination of ungrammatical lexical entries by the CFG filtering.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_19",
  "x": "Consequently, strings that represent the ngram information are very frequently changed into feature structures and vice versa when they go in and out of the kernel of the parser. On the other hand, <cite>Ninomiya et al. (2006)</cite><cite>'s model 3</cite> uses the supertagger as an external module. Once the parser acquires the supertagger's outputs, the n-gram information never goes in and out of the kernel.",
  "y": "background"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_23",
  "x": "In the experiments, we compared our model with the probabilistic HPSG with a unigram reference distribution and the probabilistic HPSG with supertagging <cite>(Ninomiya et al., 2006 )</cite>. Though our model was not as fast as <cite>Ninomiya et al. (2006)</cite> <cite>'s models</cite>, it achieved the highest accuracy among them. Our model had around 2.65 points higher F-score than 's model and around 0.56 points higher F-score than the <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>.",
  "y": "differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_24",
  "x": "Though our model was not as fast as <cite>Ninomiya et al. (2006)</cite> <cite>'s models</cite>, it achieved the highest accuracy among them. Our model had around 2.65 points higher F-score than 's model and around 0.56 points higher F-score than the <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>. When we sacrifice parsing speed, our model achieved around 2.9 points higher F-score than 's model and around 0.8 points higher F-score than <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>.",
  "y": "differences"
 },
 {
  "id": "c14d918f3b1b1248dc1d25a7e0b2e4_25",
  "x": "Though our model was not as fast as <cite>Ninomiya et al. (2006)</cite> <cite>'s models</cite>, it achieved the highest accuracy among them. Our model had around 2.65 points higher F-score than 's model and around 0.56 points higher F-score than the <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>. When we sacrifice parsing speed, our model achieved around 2.9 points higher F-score than 's model and around 0.8 points higher F-score than <cite>Ninomiya et al. (2006)</cite> <cite>'s model 3</cite>.",
  "y": "differences"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_0",
  "x": "(ii) The local approach might suffer from the data sparseness issue of unseen words/features, the difficulty of calibrating, and the failure to induce the underlying similarity structures at high levels of abstraction for EL (due to the extensive reliance on the hand-designed coarse features) (Sun et al., 2015;<cite> Francis-Landau et al., 2016)</cite> . The first drawback of the local approach has been overcome by the global models in which all entity mentions (or a group of entity mentions) within a document are disambiguated simultaneously to obtain a coherent set of target entities. The central idea is that the referent entities of some mentions in a document might in turn introduce useful information to link other mentions in that document due to the semantic relatedness among them.",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_1",
  "x": "In practice, the features automatically induced by NN are combined with the discrete features in the local approach to extend their coverage for EL (Sun et al., 2015;<cite> Francis-Landau et al., 2016)</cite> . However, as the previous NN models for EL are local, they cannot capture the global interdependence among the target entities in the same document (the first limitation of the local approach). Guided by these analyses, in this paper, we propose to use neural networks to model both the local mention-to-entity similarities and the global relatedness among target entities in an unified architecture. This allows us to inherit all the benefits from the previous systems as well as overcome their inherent issues.",
  "y": "background motivation"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_2",
  "x": "Our work is an extension of<cite> (Francis-Landau et al., 2016)</cite> which only considers the local similarities. Given a document, we simultaneously perform linking for every entity mention from the beginning to the end of the document. For each entity mention, we utilize convolutional neural networks (CNN) to obtain the distributed representations for the entity mention as well as its target candidates. These distributed representations are then used for two purposes: (i) computing the local similarities for the entity mention and target candidates, and (ii) functioning as the input for the recurrent neural networks (RNN) that runs over the entity mentions in the documents.",
  "y": "similarities background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_3",
  "x": "This essentially converts the word sequence x into a sequence of vectors that is padded with zero vectors to form a fixed-length sequence of vectors w = (w 1 , w 2 , . . . , w n ) of length n. In the next step, we apply the convolution operation over w to generate the hidden vector sequence, that is then transformed by a non-linear function G and pooled by the sum function<cite> (Francis-Landau et al., 2016)</cite> . Following the previous work on CNN (Nguyen and Grishman, (2015a; 2015b) ), we utilize the set L of multiple window sizes to parameterize the convolution operation.",
  "y": "uses background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_4",
  "x": "We employ the local similarities \u03c6 local (m i , p ij ) from<cite> (Francis-Landau et al., 2016)</cite> , the state-of-the-art neural network model for EL. In particular: In this formula, W sparse and W CN N are the weights for the feature vectors F sparse and W CN N respectively.",
  "y": "uses background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_5",
  "x": "F sparse (m i , p ij ) is the sparse feature vector obtained from (Durrett and Klein, 2014) . This vector captures various linguistic properties and statistics that have been discovered in the previous studies for EL. The representative features include the anchor text counts from Wikipedia, the string match indications with the title of the Wikipedia candidate pages, or the information about the shape of the queries for candidate generations<cite> (Francis-Landau et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_6",
  "x": "The intuition for this computation is that the similarities at different levels of contexts might help to enforce the potential topic compatibility between the contexts of the entity mentions and target candidates for EL<cite> (Francis-Landau et al., 2016)</cite> . ---------------------------------- **GLOBAL SIMILARITIES**",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_7",
  "x": "---------------------------------- **DATASETS** Following<cite> (Francis-Landau et al., 2016)</cite>, we evaluate the models on 4 different entity linking datasets: i) ACE (Bentivogli et al., 2010 ): This corpus is from the 2005 evaluation of NIST.",
  "y": "uses"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_8",
  "x": "The task is to disambiguate the links in each article 4 . For all the datasets, we use the standard data splits (for training data, test data and development data) as the previous works for comparable comparison<cite> (Francis-Landau et al., 2016)</cite>. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_9",
  "x": "The dimensionality of the word embeddings is 300. Note that every parameter and resource in this work is either taken from the previous work (Nguyen and Grishman, 2016b;<cite> Francis-Landau et al., 2016)</cite> or selected by the development data. ----------------------------------",
  "y": "similarities background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_10",
  "x": "This section compares the proposed system (called Global-RNN) with the state-of-the-art models on our four datasets. These systems include the neural network model in<cite> (Francis-Landau et al., 2016)</cite> , the joint model for entity analysis in (Durrett and Klein, 2014) and the AIDA-light system with two-stage mapping in (Nguyen et al., 2014b) 6 . Table 2 shows the performance of the systems on the test sets with the reference knowledge base of the June 2016 Wikipedia dump.",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_11",
  "x": "We also include the performance of the systems on the December 2014 Wikipedia dump that was used and provided by<cite> (Francis-Landau et al., 2016)</cite> for further and compatible comparison. ---------------------------------- **SYSTEMS**",
  "y": "similarities background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_12",
  "x": "For EL, we hypothesize that the global coherence is an effective domain-independent feature that would help to improve the crossdomain performance of the models. The intuition is that the entities mentioned in a document of any domains should be related to each other. Eventually, we expect that the proposed model with global coherence features would be more robust to domain shifts than the local approach<cite> (Francis-Landau et al., 2016)</cite> .",
  "y": "background motivation"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_13",
  "x": "We note that news consists of formally written documents while a majority of the other domains is informal text, making the source and target domains very divergent in terms of vocabulary and styles (Plank and Moschitti, 2013) . Table 3 compares Global-RNN with the neural network EL model in<cite> (Francis-Landau et al., 2016)</cite> , the best reported model on the ACE dataset in the literature 8 . In this table, the models are trained on the source domain news, and evaluated on news itself (in-domain performance) (via 5-fold cross validation) as well as on the 4 target domains bc, cts, wl, un (out-of-domain performance The first observation from the table is that the performance of all the compared systems on the target domains is much worse than the corresponding in-domain performance.",
  "y": "background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_14",
  "x": "Second and most importantly, Global-RNN is consistently better than the model with only local features in<cite> (Francis-Landau et al., 2016)</cite> over all the target domains (although it is less pronounced in the cts domain). This demonstrates the cross-domain robustness of the proposed model and confirms our hypothesis about the domain-independence of the global coherence features for EL. ----------------------------------",
  "y": "differences background"
 },
 {
  "id": "c2a956a6ae0fb1ab338da01e5a5645_15",
  "x": "Neural networks are applied to entity linking very recently. He et al. (2013b) learn enttiy representation via Stacked Denoising Auto-encoders. Sun et al. (2015) employ convolutional neural networks and neural tensor networks to model mentions, entities and contexts while <cite>Francis-Landau et al. (2016)</cite> combine CNN-based representations with sparse features to improve the performance.",
  "y": "differences background"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_0",
  "x": "Therefore, mapping documents in different languages into a common latent topic space can be of great benefit when detecting document translation pairs (Mimno et al., 2009;<cite> Platt et al., 2010)</cite> . Aside from the benefits that it offers in the task of detecting document translation pairs, topic models offer potential benefits to the task of creating translation lexica, aligning passages, etc. The process of discovering relationship between documents using topic models involves: (1) representing documents in the latent space by inferring their topic distributions and (2) comparing pairs of topic distributions to find close matches.",
  "y": "motivation"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_1",
  "x": "Hall et al. (2008) used LDA to analyze historical trends in the scientific literature; Wei and Croft (2006) showed improvements on an information retrieval task. More recently Eisenstein et al. (2010) modeled geographic linguistic variation using Twitter data. Aside from their widespread use on monolingual text, topic models have also been used to model multilingual data (Boyd-Graber and Blei, 2009;<cite> Platt et al., 2010</cite>; Jagarlamudi and Daum\u00e9, 2010; Fukumasu et al., 2012) , to name a few.",
  "y": "background"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_2",
  "x": "Mimno et al. (2009) introduced polylingual topic models (PLTM), an extension of latent Dirichlet allocation (LDA), and, more recently,<cite> Platt et al. (2010)</cite> proposed extensions of principal component analysis (PCA) and probabilistic latent semantic indexing (PLSI). Both the PLTM and PLSI represent bilingual documents in the probability simplex, and thus the task of finding document translation pairs is formulated as finding similar probability distributions. While the nature of both works was exploratory, results shown on fairly large collections of bilingual documents (less than 20k documents) offer convincing argument of their potential.",
  "y": "background"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_3",
  "x": "In addition, we show how the results as reported by<cite> Platt et al. (2010)</cite> can be obtained using the PLTM representation with a significant speed improvement. As in <cite>(Platt et al., 2010)</cite> and (Mimno et al., 2009 ) the task is to find document translation pairs in a multilingual collection of documents by representing documents in the probability simplex and computing similarity between their probability distribution representation across all document pairs. For this experimental setup, accuracy is defined as the number of times (in percentage) that the target language document was discovered at rank 1 (i.e. % @Rank 1.) across the whole test collection.",
  "y": "differences"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_4",
  "x": "We use PLTM representations of bilingual documents. In addition, we show how the results as reported by<cite> Platt et al. (2010)</cite> can be obtained using the PLTM representation with a significant speed improvement. As in <cite>(Platt et al., 2010)</cite> and (Mimno et al., 2009 ) the task is to find document translation pairs in a multilingual collection of documents by representing documents in the probability simplex and computing similarity between their probability distribution representation across all document pairs.",
  "y": "similarities"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_5",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** We use Mallet's (McCallum, 2002) implementation of the PLTM to train and infer topics on the same data set used in<cite> Platt et al. (2010)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_6",
  "x": "This same approach was used by <cite>(Platt et al., 2010)</cite> to show the absolute performance comparison. As in the case of the previous two tasks, in order to evaluate the approximate, LSH based, Hellinger distance we used values of R=0.4, R=0.6 and R=0.8. Since in <cite>(Platt et al., 2010)</cite> numbers were reported on the test speeches whose word length is greater or equal to 100, we used the same subset (total of 14150 speeches) of the original test collection.",
  "y": "uses similarities"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_7",
  "x": "This same approach was used by <cite>(Platt et al., 2010)</cite> to show the absolute performance comparison. As in the case of the previous two tasks, in order to evaluate the approximate, LSH based, Hellinger distance we used values of R=0.4, R=0.6 and R=0.8. Since in <cite>(Platt et al., 2010)</cite> numbers were reported on the test speeches whose word length is greater or equal to 100, we used the same subset (total of 14150 speeches) of the original test collection.",
  "y": "similarities uses"
 },
 {
  "id": "c2bfe3534597a8f192ec846619f6b1_8",
  "x": "When using approximate, kd-trees based, Hellinger distance, we outperform regular JS and Hellinger divergence across all topics and for T=500 we achieve the best overall accuracy of 99.61%. We believe that this is due to the small amount of error in the search introduced by ANN, due to its approximate nature, which for this task yields positive results. On the same data set, <cite>(Platt et al., 2010)</cite> report accuracy of 98.9% using 50 topics, a slightly different prior distribution, and MAP instead of posterior inference.",
  "y": "extends differences"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_0",
  "x": "It was observed by<cite> [Zeng et al., 2015]</cite> that 50% of the sentences in the Riedel2010 Distant Supervision dataset [Riedel et al., 2010] , a popular DS benchmark dataset, had 40 or more words in them. We note that not all the words in these long sentences contribute towards expressing the given relation. In this work, we formulate various word attention mechanisms to help the relation extraction model focus on the right context in a given sentence.",
  "y": "background"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_1",
  "x": "In our task, we consider binary relations where k \u2208 [1, 2], such as Born In(Barack Obama, Hawaii). Given a set of sentences S = {s i }; i \u2208 [1 . . . N ], where each sentence s i contains both the entities, the task of relation extraction with distantly supervised dataset is to learn a function F r : F r (S, (e 1 , e 2 )) = 1 if relation r is true for pair(e 1 , e 2 ) 0 Otherwise PCNN:<cite> [Zeng et al., 2015]</cite> proposed the Piecewise Convolution Neural Network (PCNN), a successful model for distantly supervised relation extraction. The Success of the relation extraction task depends on extracting the right structural features from the sentence containing the entity-pair.",
  "y": "background"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_2",
  "x": "For a given bag of sentences, learning is done using the setting proposed by<cite> [Zeng et al., 2015]</cite> , wherein the sentence with the highest probability of expressing a relation in a bag is selected to train the model in each iteration. The EA model has two components: 1) PCNN layer, and 2) Entity Attention Layer, as shown in Figure 4 . Consider an instance set S q with set of sentences, 1\u00d7d is a word embedding and {e emb q1 , e emb q2 } are the embeddings for the two entities.",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_3",
  "x": "The EA model has two components: 1) PCNN layer, and 2) Entity Attention Layer, as shown in Figure 4 . Consider an instance set S q with set of sentences, 1\u00d7d is a word embedding and {e emb q1 , e emb q2 } are the embeddings for the two entities. The PCNN layer is applied on the words in the sentence<cite> [Zeng et al., 2015]</cite> .",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_4",
  "x": "The entity attention model (EA) we propose is adapted to the distantly supervised setting by using two important variations from the original [Shen and Huang, 2016] model (a) The EA processes a set of sentences. It uses PCNN<cite> [Zeng et al., 2015]</cite> assumption to select the sentence with highest probability of any relation. The selected sentence is used to estimate the relation probabilities for an entity-pair and for back-propagation of the error for the bag-of-sentences.",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_5",
  "x": "Baselines: We compare proposed models with (a) Piecewise Convolution Neural Network (PCNN)<cite> [Zeng et al., 2015]</cite> and (b) Neural Relation Extraction with Selective Attention over Instances (NRE) [Lin et al., 2016] . Both NRE and PCNN baseline outperform traditional baselines like MIML-RE and hence we use them as a representative state-of-the-art baseline to compare with proposed models. Model Parameters: The parameters used for the various models are summarized in Table 4 .",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_6",
  "x": "Word Position feature embeddings (with respect to each entity) are randomly initialized and learned during training. Concatenation of the word embedding and position embedding results in a 60-dimensional (d w + (2 * d p )) embedding x ij for each word. We implemented PCNN model baseline following<cite> [Zeng et al., 2015]</cite> and used author provided results and implementation for NRE baseline.",
  "y": "uses"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_7",
  "x": "[Lin et al., 2016] improves upon PCNN results by introducing an attention mechanism to select a set of sentences from instance set for relation label prediction. [ <cite>Zheng et al., 2016]</cite> aimed to leverage inter-sentence information for relation extraction in a ranking model. The hypothesis explored is that for a particular entity-pair, each mention alone may not be expressive enough of the relation in question, but information from several mentions may be required to decisively make a prediction.",
  "y": "background"
 },
 {
  "id": "c3c09df34cf9f81c1cc4fc63a18bf0_8",
  "x": "Attention mechanisms in neural networks have been successfully applied to a variety of problems, like machine translation [Bahdanau et al., 2014] , image captioning [Xu et al., 2015] , supervised relation extraction [Shen and Huang, 2016] , distantly-supervised relation extraction<cite> [Zheng et al., 2016]</cite> etc. In our work, we focus on selecting the right words in a sentence using the word and entity-based attention mechanism. ----------------------------------",
  "y": "background"
 },
 {
  "id": "c42e9d10ca8876af80eee021c969d7_0",
  "x": "Automatically learning dialogue structure from corpora is an active area of research driven by a recognition of the value offered by data-driven approaches (e.g., Bangalore et al., 2006) . Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems <cite>(Forbes-Riley et al., 2007)</cite> , but also contribute to our understanding of the cognitive and affective processes involved in learning through tutoring (VanLehn et al., 2007) . Although traditional top-down approaches (e.g., Cade et al., 2008) and some empirical work on analyzing the structure of tutorial dialogue <cite>(Forbes-Riley et al., 2007)</cite> have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure.",
  "y": "background"
 },
 {
  "id": "c42e9d10ca8876af80eee021c969d7_1",
  "x": "Although traditional top-down approaches (e.g., Cade et al., 2008) and some empirical work on analyzing the structure of tutorial dialogue <cite>(Forbes-Riley et al., 2007)</cite> have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure. An empirical approach to identifying tutorial dialogue strategies, or modes, could address this limitation by providing a mechanism for describing in succinct probabilistic terms the tutorial strategies that actually occur in a corpus. Just as early work on dialogue act interpretation utilized hidden Markov models (HMMs) to capture linguistic structure (Stolcke et al., 2000) , we propose a system that uses HMMs to capture the structure of tutorial dialogue implicit within sequences of already-tagged dialogue acts.",
  "y": "background motivation"
 },
 {
  "id": "c42e9d10ca8876af80eee021c969d7_2",
  "x": "---------------------------------- **1** The importance of adjacency pairs is wellestablished in natural language dialogue (e.g., Schlegoff & Sacks, 1973) , and adjacency pair analysis has illuminated important phenomena in tutoring as well <cite>(Forbes-Riley et al., 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_0",
  "x": "We evaluate and compare our proposed system both on our new multi-target UK election dataset, as well as on the benchmarking dataset for single-target dependent sentiment<cite> (Dong et al., 2014)</cite> . We show a clear state-of-the-art performance of TDParse over existing approaches for tweets with multiple targets, which encourages further research on the multi-target-specific sentiment recognition task. 2 2 Related Work: Target-dependent Sentiment Classification on Twitter",
  "y": "uses"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_1",
  "x": "Early work tackling target-dependent sentiment in tweets (Jiang et al., 2011) designed targetdependent features manually, relying on the syntactic parse tree and a set of grammar-based rules, and incorporating the sentiment labels of related tweets to improve the classification performance. Recent work<cite> (Dong et al., 2014)</cite> used recursive neural networks and adaptively chose composition functions to combine child feature vectors according to their dependency type, to reflect sentiment signal propagation to the target. Their datadriven composition selection approach replies on the dependency types as features and a small set of rules for constructing target-dependent trees.",
  "y": "background"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_2",
  "x": "Vo and Zhang (2015) exploit the left and right context around a target in a tweet and combine low-dimensional embedding features from both contexts and the full tweet using a number of different pooling functions. Despite not fully capturing semantic and syntactic information given the target entity, they show a much better performance than<cite> Dong et al. (2014)</cite> , indicating useful signals in relation to the target can be drawn from such context representation. Both Tang et al. (2016a) and Zhang et al. (2016) adopt and integrate left-right target-dependent context into their recurrent neural network (RNN) respectively.",
  "y": "background"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_3",
  "x": "It is worth noting that the sentiment annotation for each target also involves choosing among not only positive/negative/neutral but also a fourth category 'doesnotapply'. The resulting dataset contains 4,077 tweets, with an average of 3.09 entity mentions (targets) per tweet. As many as 3,713 tweets have more than a single entity mention (target) per tweet, which makes the task different from 2015 Semeval 10 subtask C (Rosenthal et al., 2015) and a target-dependent benchmarking dataset of<cite> Dong et al. (2014)</cite> where each tweet has only one target annotated and thus one sentiment label assigned.",
  "y": "differences"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_4",
  "x": "Feature vectors generated from different contexts are concatenated into a final feature 7 Empirically the proximity/location of such syntactic relations have not made much difference when used in feature weighting and is thus ignored. vector as shown in (2), where P (X) presents a list of k different pooling functions on an embedding matrix X. Not only does this proposed framework make the learning process efficient without labor intensive manual feature engineering and heavy architecture engineering for neural models, it has also shown that complex syntactic and semantic information can be effectively drawn by simply concatenating different types of context together without the use of deep learning (other than pretrained word embeddings). Data set: We evaluate and compare our proposed system to the state-of-the-art baselines on a benchmarking corpus<cite> (Dong et al., 2014</cite> ) that has been used by several previous studies (Vo and Zhang, 2015; Tang et al., 2016a; Zhang et al., 2016) .",
  "y": "uses"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_6",
  "x": "**EXPERIMENTAL RESULTS AND COMPARISON WITH OTHER BASELINES** We report our experimental results in Table 2 on the single-target benchmarking corpus<cite> (Dong et al., 2014)</cite> , with three model categories: 1) tweet-level target-independent models, 2) targetdependent models without considering the 'sametarget-multi-appearance' scenario and 3) targetdependent models incorporating the 'same-targetmulti-appearance' scenario. We include the models presented in the previous section as well as models for target specific sentiment from the literature where possible.",
  "y": "uses"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_7",
  "x": "Among the target-independent baseline models Target-ind (Vo and Zhang, 2015) and Semevalbest have shown strong performance compared with SSWE and SVM-ind (Jiang et al., 2011) as they use more features, especially rich automatic features using the embeddings of Mikolov et al. (2013) . Interestingly they also perform better than some of the targetdependent baseline systems, namely SVM-dep (Jiang et al., 2011) , Recursive NN and AdaRNN<cite> (Dong et al., 2014)</cite> , showing the difficulty of fully extracting and incorporating target information in tweets. Basic LSTM models (Tang et al., 2016a) completely ignore such target information and as a result do not perform as well.",
  "y": "background"
 },
 {
  "id": "c4e2a9322471fb5988a5bd737fa51e_8",
  "x": "Basic LSTM models (Tang et al., 2016a) completely ignore such target information and as a result do not perform as well. Among the target-dependent systems neural network baselines have shown varying results. The adaptive recursive neural network, namely AdaRNN<cite> (Dong et al., 2014)</cite> , adaptively selects composition functions based on the input data and thus performs better than a standard recursive neural network model (Recursive NN<cite> (Dong et al., 2014)</cite> ).",
  "y": "background"
 },
 {
  "id": "c57e98c9c07dd5d8653e172136c901_0",
  "x": "The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs. The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (Blei et al., 2010) , hierarchical Pitman-Yor process (Teh, 2006) , Indian buffet process (Ghahramani and Griffiths, 2005) , recurrent neural network (Mikolov et al., 2010; Van Den Oord et al., 2016) , long short-term memory (Hochreiter and Schmidhuber, 1997; , sequence-to-sequence model (Sutskever et al., 2014), variational auto-encoder (Kingma and Welling, 2014) , generative adversarial network (Goodfellow et al., 2014) , attention mechanism (Chorowski et al., 2015; <cite>Seo et al., 2016)</cite> , memory-augmented neural network (Graves et al., 2014; Graves et al., 2014) , stochastic neural network Miao et al., 2016) , predictive state neural network (Downey et al., 2017) , policy gradient (Yu et al., 2017) and reinforcement learning (Mnih et al., 2015) .",
  "y": "background"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_0",
  "x": "A transitive verb can be viewed as a third-order tensor with input dimensions for the subject and object, and an output dimension for the meaning of the sentence as a whole. This approach has achieved promising initial results [6] <cite>[7]</cite> [8] [9] 14] , but many questions remain. Two outstanding questions are the best method of learning verb tensors from a corpus, and the best sentence space for a variety of different tasks.",
  "y": "motivation"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_2",
  "x": "The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6,<cite> 7]</cite> or defined a new space for sentence meaning, particularly plausibility space [11, 14] . If the verb function is a multi-linear map, then the verb is naturally represented by a third-order tensor. However, tensor training can be expensive and in practice, for some tasks, the verb can be approximated as a matrix<cite> [7,</cite> 14] .",
  "y": "background"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_3",
  "x": "However, tensor training can be expensive and in practice, for some tasks, the verb can be approximated as a matrix<cite> [7,</cite> 14] . Below we describe two ways of learning a verb matrix. In the regression method, the learnt matrix consists of parameters from a plausibility classifier.",
  "y": "background"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_4",
  "x": "**DISTRIBUTIONAL (DIST)** Following <cite>[7]</cite> , we generate a K \u00d7 K matrix for each verb as the average of outer products of subject and verb vectors from the positively labelled subset of the training data: where \u2297 is outer product and N p is the number of positive training examples.",
  "y": "uses"
 },
 {
  "id": "c594df62c01bef2ffb1a7ee9c5ea28_6",
  "x": "For the verb disambiguation task we use the GS2011 dataset <cite>[7]</cite> . This dataset consists of pairs of SVO triples in which the subject and object are held constant, and the verb is manipulated to highlight different word senses. For example, the verb draw has senses that correspond to attract and depict.",
  "y": "uses"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_0",
  "x": "Previous efforts focus on generating opendomain conversational responses as an unsupervised clustering process (Ritter et al., 2010) , a phrasebased statistical machine translation task (Ritter et al., 2011 ) and a search problem based on the vector space model (Banchs and Li, 2012) , etc. With the booming of deep learning, particularly the neural network based sequence-to-sequence models, generating open-domain conversational responses gradually turns into an end-to-end encoding and decoding process (Sutskever et al., 2014; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016b; Li et al., 2016a; Li et al., 2016b; Shao et al., 2017; Yao et al., 2017) . Despite the success of the above research on single-turn conversational response generation, human conversations are usually coherent (Li et al., 2016c) and context-sensitive (Tian et al., 2017;<cite> Xing et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_1",
  "x": "\u2022 First, existing studies of utterance modeling mainly focus on representing utterances by using bidirectional GRU<cite> (Xing et al., 2017)</cite> or unidirectional GRU (Tian et al., 2017 ). One is the attention-based approach<cite> (Xing et al., 2017)</cite> , the other is the sequential integration approach (Tian et al., 2017) . \u2022 Utterance Representations: Bidirectional GRU vs. Unidirectional GRU<cite> Xing et al. (2017)</cite> utilized a bidirectional GRU and a word-level attention mechanism to transfer word representations to utterance representations. \u2022 Inter-utterance Representations: Attention vs. Sequential Integration<cite> Xing et al. (2017)</cite> proposed a hierarchical attention mechanism to feed the utterance representations to a backward RNN to obtain contextual representation.",
  "y": "background"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_2",
  "x": "For utterance representation, we consider the advantages of the two state-of-the-art approaches to encoding contextual information for context-sensitive response generation <cite>(Xing et al., 2017</cite>; Tian et al., 2017) . weighing the importance of utterances for generating open-domain conversational responses<cite> (Xing et al., 2017)</cite> , we thus model the inter-utterance representation to obtain the context vector in two measures, namely static and dynamic attention, as shown in Figure 2 .",
  "y": "similarities uses"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_3",
  "x": "Rather than use a hierarchical attention neural network<cite> (Xing et al., 2017)</cite> to obtain the contextual representation of a conversation, we propose two utterance-level attentions for weighting the importance of each utterance in the context, which is more simple in structure and has less number of parameters than the hierarchical attention approach. Meanwhile, rather than use a heuristic approach to weigh the importance of each utterance in the context (Tian et al., 2017) , in our proposed approach, the weights of utterance in the context are learned by two attention mechanisms from the data, which is more reasonable and flexible than the heuristic based approach. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "c5ec401f42f79c4707770dac4f5013_4",
  "x": "\u2022 WSI and HRAN are proposed by Tian et al. (2017) and<cite> Xing et al. (2017)</cite> respectively. We detailed describe and compare the two models in Section 2.1 and 2.2 and their frameworks are shown in Figure 1 . ----------------------------------",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_0",
  "x": "Furthermore, we reproduce <cite>multisource projection</cite> <cite>(Tyers et al., 2018)</cite> , in which dependency trees of multiple sources are combined. Finally, we apply multi-treebank modelling to the projected treebanks, in addition to or alternatively to polyglot modelling on the source side. We find that polyglot training on the source languages produces an overall trend of better results on the target language but the single best result for the target language is obtained by projecting from monolingual source parsing models and then training multi-treebank POS tagging and parsing models on the target side.",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_1",
  "x": "This work focuses on dependency parsing for low-resource languages by means of annotation projection (Yarowsky et al., 2001) and synthetic treebank creation (Tiedemann and Agi\u0107, 2016) . We build on recent work by <cite>Tyers et al. (2018)</cite> who show that in the absence of annotated training data for the target language, a lexicalized treebank can be created by translating a target language corpus into a number of related source languages and parsing the translations using models trained on the source language treebanks. 1 These annotations are then projected to the target language using separate word alignments for each source language, combined into a single graph for each sentence and decoded (Sagae and Lavie, 2006) , resulting in a treebank for the target language, Faroese in the case of <cite>Tyers et al.'s</cite> and our experiments.",
  "y": "extends"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_2",
  "x": "This work focuses on dependency parsing for low-resource languages by means of annotation projection (Yarowsky et al., 2001) and synthetic treebank creation (Tiedemann and Agi\u0107, 2016) . We build on recent work by <cite>Tyers et al. (2018)</cite> who show that in the absence of annotated training data for the target language, a lexicalized treebank can be created by translating a target language corpus into a number of related source languages and parsing the translations using models trained on the source language treebanks. 1 These annotations are then projected to the target language using separate word alignments for each source language, combined into a single graph for each sentence and decoded (Sagae and Lavie, 2006) , resulting in a treebank for the target language, Faroese in the case of <cite>Tyers et al.'s</cite> and our experiments.",
  "y": "similarities"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_3",
  "x": "For the polyglot learning technique applied to multiple treebanks of a single language, we use the term multi-treebank learning. 2. training a multi-treebank model on the individually projected treebanks and the treebank produced with multi-source projections. The former differs from the approach of <cite>Tyers et al. (2018)</cite> , who use multiple discrete, monolingual models to parse the translated sentences, whereas in this work we use a single model trained on multiple source treebanks.",
  "y": "differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_4",
  "x": "Our best result of 71.5 -an absolute improvement of 7.2 points over the result reported by <cite>Tyers et al. (2018)</cite> -was achieved with multi-treebank target learning over the monolingual projections. <cite>Tyers et al. (2018)</cite> describe a method for creating synthetic treebanks for Faroese based on previous work which uses machine translation and word alignments to transfer trees from source language(s) to the target language. Sentences from Faroese are translated into the four source languages Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l.",
  "y": "differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_5",
  "x": "<cite>Tyers et al. (2018)</cite> describe a method for creating synthetic treebanks for Faroese based on previous work which uses machine translation and word alignments to transfer trees from source language(s) to the target language. Sentences from Faroese are translated into the four source languages Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l. The translated sentences are then tokenized, POS tagged and parsed using the relevant source language model trained on the source language treebank.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_6",
  "x": "The parser output is evaluated using the gold-standard Faroese test treebank developed by <cite>Tyers et al. (2018)</cite> . The approach is compared to a delexicalized baseline, which it outperforms by a large margin. It is also shown that, for Faroese, a combination of the four source languages (multi-source projection) is superior to individual language projection.",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_7",
  "x": "Various forms of projected annotation methods are compared to delexicalized baselines, and the use of machine translation instead of parallel corpora to produce synthetic treebanks in the target language is explored. In contrast to <cite>Tyers et al. (2018)</cite> , they translate a target sentence and project the source parse tree back to the target during test time instead of using this approach to obtain training data for the target language. leverage massively multilingual parallel corpora such as translations of the Bible and web-scraped data from the Watchtower Online Library website 3 for low-resource POS tagging and dependency parsing using annotation projection.",
  "y": "differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_8",
  "x": "We use the following resources: raw Faroese sentences taken from Wikipedia, a machine translation system to translate these sentences into all source languages (Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l), a word-aligner to provide word alignments between the words in the target and source sentences, treebanks for the four source languages on which to train parsing models, POS tagging and parsing tools, and, lastly a target language test set. We use the same raw corpus, alignments and tokenized and segmented versions of the source translations 4 as <cite>Tyers et al. (2018)</cite> who release all of <cite>their data</cite>. 5 In this way, the experimental pipeline is the same as <cite>theirs</cite> but we predict POS tags and dependency annotations using our own models.",
  "y": "similarities uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_9",
  "x": "We use the following resources: raw Faroese sentences taken from Wikipedia, a machine translation system to translate these sentences into all source languages (Danish, Swedish, Norwegian Nynorsk and Norwegian Bokm\u00e5l), a word-aligner to provide word alignments between the words in the target and source sentences, treebanks for the four source languages on which to train parsing models, POS tagging and parsing tools, and, lastly a target language test set. We use the same raw corpus, alignments and tokenized and segmented versions of the source translations 4 as <cite>Tyers et al. (2018)</cite> who release all of <cite>their data</cite>. 5 In this way, the experimental pipeline is the same as <cite>theirs</cite> but we predict POS tags and dependency annotations using our own models.",
  "y": "similarities differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_10",
  "x": "**TARGET LANGUAGE CORPUS** We use the target corpus built by <cite>Tyers et al. (2018)</cite> which comprises 28,862 sentences which were extracted from Faroese Wikipedia dumps 6 using the WikiExtractor script 7 and further pre-processed to remove any non-Faroese texts and other forms of unsuitable sentences. Machine Translation As noted by <cite>Tyers et al. (2018)</cite> , popular repositories for developing machine translation systems such as OPUS (Tiedemann, 2016) contain an inadequate amount of sentences to train a data-driven machine translation system for Faroese.",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_11",
  "x": "Machine Translation As noted by <cite>Tyers et al. (2018)</cite> , popular repositories for developing machine translation systems such as OPUS (Tiedemann, 2016) contain an inadequate amount of sentences to train a data-driven machine translation system for Faroese. For instance, there are fewer than 7,000 sentence pairs between Faroese and Danish, Faroese and English, Faroese and Norwegian and Faroese and Swedish. Consequently, to create parallel source sentences, <cite>Tyers et al. (2018)</cite> use a rule-based machine translation system available in Apertium 8 to translate from Faroese to Norwegian Bokm\u00e5l.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_12",
  "x": "Machine Translation As noted by <cite>Tyers et al. (2018)</cite> , popular repositories for developing machine translation systems such as OPUS (Tiedemann, 2016) contain an inadequate amount of sentences to train a data-driven machine translation system for Faroese. For instance, there are fewer than 7,000 sentence pairs between Faroese and Danish, Faroese and English, Faroese and Norwegian and Faroese and Swedish. Consequently, to create parallel source sentences, <cite>Tyers et al. (2018)</cite> use a rule-based machine translation system available in Apertium 8 to translate from Faroese to Norwegian Bokm\u00e5l.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_13",
  "x": "As a result, the authors use pivot translation from Norwegian Bokm\u00e5l into the other source languages. The process is illustrated in Fig. 1 . For a more thorough description of the machine translation process and for resource creation in general, see the work of <cite>Tyers et al. (2018)</cite> .",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_14",
  "x": "---------------------------------- **WORD ALIGNMENTS** We use word alignments between the Faroese text and the source translations generated by <cite>Tyers et al. (2018)</cite> using fast align (Dyer et al., 2013) , a word alignment tool based on IBM Model 2.",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_15",
  "x": "Synthetic Source Treebanks Source translations are tokenized with UDPipe (Straka and Strakov\u00e1, 2017) by <cite>Tyers et al. (2018)</cite> . For each source language, the POS model trained on the full training data (see previous section) is used to tag the tokenized translations. Once the text is tagged, we predict dependency arcs and labels with the parsing models of the previous section, and use annotation projection (described below) to provide syntactic annotations for the target sentences.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_16",
  "x": "Once the text is tagged, we predict dependency arcs and labels with the parsing models of the previous section, and use annotation projection (described below) to provide syntactic annotations for the target sentences. Annotation Projection Once the synthetic source treebanks are compiled, i. e. the translations are parsed, the annotations are then projected from the source translations to the target language using the word alignments and <cite>Tyers et al.'s projection tool</cite>, resulting in a Faroese treebank. In some cases, not all tokens are aligned and <cite>Tyers et al. (2018)</cite> work around this by falling back to a 1:1 mapping between the target index and the source index.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_17",
  "x": "In some cases, not all tokens are aligned and <cite>Tyers et al. (2018)</cite> work around this by falling back to a 1:1 mapping between the target index and the source index. There are also cases where there is a mismatch in length between the source and target sentences and some dependency structures cannot be projected to the target language. <cite>Tyers et al.'s projection setup</cite> removes unsuitable projected trees containing e. g. more than one root token, a token that is its own head or a token with a head outside the range of the sentence.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_18",
  "x": "<cite>Tyers et al.'s projection setup</cite> removes unsuitable projected trees containing e. g. more than one root token, a token that is its own head or a token with a head outside the range of the sentence. Multi-source Projection For multi-source projection, the four source-language dependency trees for a Faroese sentence are projected into a single graph, scoring edges according to the number of trees that contain them (Sagae and Lavie, 2006; Nivre et al., 2007) . The dependency structure is first built by voting over the directed edges.",
  "y": "background"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_19",
  "x": "---------------------------------- **EXPERIMENTS** In this section, we describe our experiments, which include a replication of the main findings of <cite>Tyers et al. (2018)</cite> , using AllenNLP for POS tagging and parsing instead of UDPipe (Straka and Strakov\u00e1, 2017 Figure 3 : Multi-source projection.",
  "y": "uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_20",
  "x": "Comparing the results in Tables 4 and 5, we see that LAS scores tend to be slightly lower than on the version which included all target sen-WORK RESULT Rosa and Mare\u010dek (2018) 49.4 <cite>Tyers et al. (2018)</cite> 64.4 Our implementation 68.0 of <cite>Tyers et al. (2018)</cite> Our Best Model 71.5 tences, indicating that we did lose some information by filtering out a large number of sentences. However, Norwegian Nynorsk still outperforms the multi-source model for the monolingual setting and both Norwegian models perform better than the multi-source model in the polyglot setting, suggesting that size alone does not explain the under-performance of the multi-source model. It is also worth noting that polyglot training is superior to all monolingual models which hints that for no nynorsk (the previously better performing model), the monolingual model was not able to achieve its full potential with the reduced data while the polyglot model was able to provide richer annotations.",
  "y": "differences uses"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_21",
  "x": "It is also worth noting that polyglot training is superior to all monolingual models which hints that for no nynorsk (the previously better performing model), the monolingual model was not able to achieve its full potential with the reduced data while the polyglot model was able to provide richer annotations. Another reason why the multi-source model does not work as well in our experiments as it does in those of <cite>Tyers et al. (2018)</cite> might be that we use pre-trained embeddings whereas <cite>Tyers et al. (2018)</cite> do not. In this way, our monolingual models are stronger and likely do not benefit as much from voting.",
  "y": "differences"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_22",
  "x": "Note that they use predicted tokenization and segmentation whereas our experiments and <cite>Tyers et al.'s</cite> use gold tokenization and segmentation, which provides a small artificial boost. <cite>Tyers et al. (2018)</cite> report an LAS of 64.43 with a monolingual multi-source approach. Our implementation which uses a different parser (AllenNLP versus UDPipe) and pre-trained word embeddings achieves an LAS of 68.",
  "y": "similarities"
 },
 {
  "id": "c684a2be8ca8ed8db25be6e080f921_23",
  "x": "<cite>Tyers et al. (2018)</cite> report an LAS of 64.43 with a monolingual multi-source approach. Our implementation which uses a different parser (AllenNLP versus UDPipe) and pre-trained word embeddings achieves an LAS of 68. Our highest score of 71.51 is achieved through the combination of projecting from strong monolingual source models and then training multi-treebank POS tagging and parsing models on the outputs.",
  "y": "background"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_0",
  "x": "Recent state-of-the-art models (Wang et al., 2018; Fried et al., 2018b;<cite> Ma et al., 2019)</cite> have demonstrated large gains in accuracy on the VLN task. However, it is unclear which modality these go past the couch \u2026 Figure 1 : We factor the grounding of language instructions into visual appearance, route structure, and object detections using a mixture-of-experts approach. substantial increases in task metrics can be attributed to, and, in particular, whether the gains in performance are due to stronger grounding into visual context or e.g. simply into the discrete, geometric structure of possible routes, such as turning left or moving forward (see Fig. 1",
  "y": "background"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_1",
  "x": "Most recently,<cite> Ma et al. (2019)</cite> introduce a visual and textual co-attention mechanism and a route progress predictor. These approaches have significantly improved performance on the VLN task, when evaluated by metrics such as success rate. However, it is unclear where the high performance comes from.",
  "y": "background"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_2",
  "x": "Most recently,<cite> Ma et al. (2019)</cite> introduce a visual and textual co-attention mechanism and a route progress predictor. These approaches have significantly improved performance on the VLN task, when evaluated by metrics such as success rate. However, it is unclear where the high performance comes from.",
  "y": "motivation"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_3",
  "x": "Work concurrent to ours evaluates the performance of single-modality models for several embodied tasks including VLN (Thomason et al., 2019) , finding that high performance can be achieved on the R2R dataset using a non-visual version of the baseline model (Anderson et al., 2018) . In this paper, we show that the same trends hold for two recent state-of-the-art architectures <cite>(Ma et al., 2019</cite>; Fried et al., 2018b) for the VLN task; we also analyze to what extent object-based representations and mixture-ofexperts methods can address these issues. in a connectivity graph determined by line-of-sight in the physical environment.",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_4",
  "x": "In this paper, we find that agents without any visual input can achieve competitive performance, matching or even outperforming their vision-based counterparts under two state-of-theart model models (Fried et al., 2018b;<cite> Ma et al., 2019)</cite> . We also explore two approaches to make the agents better utilize their visual inputs. The role of vision in vision-and-language tasks.",
  "y": "uses differences"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_5",
  "x": "In this work, we analyze two recent VLN models, which typify the visual grounding approaches of VLN work: the panoramic \"follower\" model from the Speaker-Follower (SF) system of Fried et al. (2018b) and the Self-Monitoring (SM) model of<cite> Ma et al. (2019)</cite> . These models obtained stateof-the-art results on the R2R dataset. Both models are based on the encoder-decoder approach (Cho et al., 2014 ) and map an instruction to a sequence of actions in context by encoding the instruction with an LSTM, and outputting actions using an LSTM decoder that conditions on the encoded instruction and visual features summarizing the agent's environmental context.",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_6",
  "x": "For SM, we use a reimplementation without the progress monitor, which was shown to be most important for search in inference<cite> (Ma et al., 2019)</cite> . We investigate how well these models ground instructions into visual features of the environment, by training and evaluating them without access to the visual context: setting their visual feature vectors to zeroes during training and testing. We compare performance on the validation sets of the R2R dataset: the val-seen split, consisting of the same environments as in training, and the val- Table 1 : Success rate (SR) of the vision-based full agent (\"RN\", using ResNet) and the non-visual agent (\"no vis.\", setting all visual features to zero) on the R2R dataset under different model architectures (SpeakerFollower (SF) (Fried et al., 2018b) and Self-Monitoring (SM)<cite> (Ma et al., 2019)</cite> ) and training schemes.",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_7",
  "x": "We compare performance on the validation sets of the R2R dataset: the val-seen split, consisting of the same environments as in training, and the val- Table 1 : Success rate (SR) of the vision-based full agent (\"RN\", using ResNet) and the non-visual agent (\"no vis.\", setting all visual features to zero) on the R2R dataset under different model architectures (SpeakerFollower (SF) (Fried et al., 2018b) and Self-Monitoring (SM)<cite> (Ma et al., 2019)</cite> ) and training schemes. unseen split of novel environments. Since we aim to evaluate how well the agents generalize to the unseen environments, we focus on the val-unseen split.",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_8",
  "x": "We then use the same visual attention mechanism as in Fried et al. (2018b) and<cite> Ma et al. (2019)</cite> to obtain an attended object representation x obj,att over these {x obj,j } vectors. We either substitute the ResNet CNN features x img,att (\"RN\") with our object representation x obj,att (\"Obj\"), or concatenate x img,att and x obj,att (\"RN+Obj\"). Then we train the SF model or the SM model using this object representation, with results shown in Table 2 . 3 For SF (lines 1-4), object representations substantially improve generalization ability: using either the object representation (\"Obj\") or the combined representation (\"RN+Obj\") obtains higher success rate on unseen environments than using only the ResNet features (\"RN\"), and the combined representation (\"RN+Obj\") obtains the highest overall performance.",
  "y": "uses"
 },
 {
  "id": "c705c0533600b9b93d2c89bcbc292b_11",
  "x": "---------------------------------- **A DETAILS ON THE COMPARED VLN MODELS** The Speaker-Follower (SF) model (Fried et al., 2018b ) and the Self-Monitoring (SM) model<cite> (Ma et al., 2019)</cite> which we analyze both use sequenceto-sequence model (Cho et al., 2014) with attention (Bahdanau et al., 2015) as their base instruction-following agent.",
  "y": "uses"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_0",
  "x": "Question generation (QG) task, which takes a context and an answer as input and generates a question that targets the given answer, have received tremendous interests in recent years from both industrial and academic communities (Zhao et al., 2018) (Zhou et al., 2017) <cite>(Du et al., 2017)</cite> . The state-of-the-art models mainly adopt neural approaches by training a neural network based on the sequence-to-sequence framework. So far, the best performing result is reported in (Zhao et al., 2018) , which advances the state-of-the-art results from 13.9 to 16.8 (BLEU 4) .",
  "y": "motivation"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_1",
  "x": "The existing QG models mainly rely on recurrent neural networks (RNN) augmented by attention mechanisms. However, the inherent sequential nature of the RNN models suffers from the problem of handling long sequences. As a result, the existing QG models <cite>(Du et al., 2017)</cite> (Zhou et al., 2017 ) mainly use only sentence-level information as context.",
  "y": "background"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_2",
  "x": "As a result, the existing QG models <cite>(Du et al., 2017)</cite> (Zhou et al., 2017 ) mainly use only sentence-level information as context. When applied to a paragraphlevel context, the existing models show significant performance degradation. However, as indicated by <cite>(Du et al., 2017)</cite> , providing paragraph-level information can improve QG performance.",
  "y": "motivation"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_3",
  "x": "Answers of the questions are text spans in the articles. We follow the same data split settings as previous work on the QG tasks <cite>(Du et al., 2017)</cite> (Zhao et al., 2018) to directly compare the state-of-theart results on QG tasks. Table 1 summarizes some statistics for the compared datasets.",
  "y": "uses similarities"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_4",
  "x": "\u2022 SQuAD 73K In this set, we follow the same setting as <cite>(Du et al., 2017)</cite> ; the accessible parts of the SQuAD training data are randomly divided into a training set (80%), a development set (10%), and a test set (10%). We report results on the 10% test set. \u2022 SQuAD 81K In this set, we follow the same setting as (Zhao et al., 2018) ; the accessible SQuAD development data set is divided into a development set (50%), and a test set (50%).",
  "y": "similarities uses"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_5",
  "x": "The <cite>(Du et al., 2017)</cite> , and SQuAD 81K is the setting of (Zhao et al., 2018) . SQuAD 73K 73240 11877 10570  SQuAD 81K 81577 8964  8964 pre-trained model uses the officially provided BERT base model (12 layers, 768 hidden dimensions, and 12 attention heads.) with a vocab of 30522 words. Dropout probability is set to 0.1 between transformer layers.",
  "y": "background"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_6",
  "x": "---------------------------------- **MODEL COMPARISON** In this paper, we compare our models with the best performing models <cite>(Du et al., 2017)</cite> (Zhao et al., 2018) in the literature.",
  "y": "similarities"
 },
 {
  "id": "c7821d22613ad91f77ea454d50a5ce_7",
  "x": "The compared models in the experiment are: \u2022 NQG-RC <cite>(Du et al., 2017)</cite> : A seq2seq question generation model based on bidirectional LSTM. \u2022 PLQG (Zhao et al., 2018) : A seq2seq network which contains a gated self-attention encoder and a maxout pointer decoder to enable the capability of handling long text input.",
  "y": "uses similarities"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_0",
  "x": "Recurrent neural networks (RNNs) (Elman, 1990) are temporal networks and cumulative in nature to effectively model sequential data such as text or speech. RNNs and their variants such as LSTM (Hochreiter and Schmidhuber, 1997) have shown success in several natural language processing (NLP) tasks, such as entity extraction (Lample et al., 2016; Ma and Hovy, 2016) , relation extraction <cite>(Vu et al., 2016a</cite>; Miwa and Bansal, 2016; Gupta et al., 2016 Gupta et al., , 2018c , language modeling (Mikolov et al., 2010; Peters et al., 2018) , slot filling (Mesnil et al., 2015; Vu et al., 2016b) , machine translation (Bahdanau et al., 2014) , sentiment analysis (Wang et al., 2016; Tang et al., 2015) , semantic textual similarity (Mueller and Thyagarajan, 2016; Gupta et al., 2018a) and dynamic topic modeling (Gupta et al., 2018d) . Past works (Zeiler and Fergus, 2014; have mostly analyzed deep neural network, especially CNN in the field of computer vision to study and visualize the features learned by neurons.",
  "y": "background"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_2",
  "x": "---------------------------------- **CONNECTIONIST BI-DIRECTIONAL RNN** We adopt the bi-directional recurrent neural network architecture with ranking loss, proposed by<cite> Vu et al. (2016a)</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_3",
  "x": "W bi \u2208 R D\u00d7D is the weight matrix connecting the hidden vectors of the combined forward and backward network. Following Gupta et al. (2015) during model training, we use 3-gram and 5-gram representation of each word w t at timestep t in the word sequence, where a 3-gram for w t is obtained by concatenating the corresponding word embeddings, i.e., w t\u22121 w t w t+1 . Ranking Objective: Similar to Santos et al. (2015) and<cite> Vu et al. (2016a)</cite> , we applied the ranking loss function to train C-BRNN.",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_4",
  "x": "The parameter \u03b3 controls the penalization of the prediction errors and m + and m are margins for the correct and incorrect classes. Following<cite> Vu et al. (2016a)</cite> , we set \u03b3 = 2, m + = 2.5 and m \u2212 = 0.5. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_5",
  "x": "As position features in relation classification experiments, we use position indicators (PI) (Zhang and Wang, 2015) in C-BRNN to annotate target entity/nominals in the word sequence, without necessity to change the input vectors, while it increases the length of the input word sequences, as four independent words, as position indicators (<e1>, </ e1>, <e2>, </e2>) around the relation arguments are introduced. In our analysis and interpretation of recurrent neural networks, we use the trained C-BRNN ( Figure 1 )<cite> (Vu et al., 2016a)</cite> model. ----------------------------------",
  "y": "uses similarities"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_6",
  "x": "To do so, we first compute N-gram for each word w t in the sentence S. For instance, a 3-gram representation of w t is given by w t\u22121 , w t , w t+1 . Therefore, an N-gram (for N=3) sequence S of words is represented as [[w t\u22121 , w t , w t+1 ] n t=1 ], where w 0 and w n+1 are PADDING (zero) vectors of embedding dimension. Following<cite> Vu et al. (2016a)</cite> , we use N-grams (e.g., tri-grams) representation for each word in each subsequence S \u2264k that is input to C-BRNN to compute P (R|S \u2264k ), where the N-gram (N=3) subsequence S \u2264k is given by,",
  "y": "similarities uses"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_7",
  "x": "Given a sentence and two annotated nominals, the task of binary relation classification is to predict the semantic relations between the pairs of nominals. In most cases, the context in between the two nominals define the relationship. However,<cite> Vu et al. (2016a)</cite> has shown that the extended context helps.",
  "y": "background"
 },
 {
  "id": "c82c31a3e7b229b5aed8faeff21efa_8",
  "x": "We use the similar experimental setup as<cite> Vu et al. (2016a)</cite> . LISA Analysis: As discussed in Section 3, we interpret C-BRNN by explaining its predictions via the semantic accumulation over the subsequences S \u2264k (Figure 2) for each sentence S. We select the example sentences S1-S7 (Table 1) for which the network predicts the correct relation type with high scores. For an example sentence S1, Table 2 illustrates how different subsequences are input to C-BRNN in order to compute prediction scores pp in the softmax layer for the relation cause-effect(e1, e2).",
  "y": "similarities uses"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_0",
  "x": "While some work focused on the representation of relations on the basis of triplets belonging to the KB [3] , other work proposed to enhance the distributed representation of words for representing their underlying concepts by taking into consideration the structure of the KB graph (e.g., concepts in the same category or their relationships with other concepts) <cite>[6,</cite> 18, 19] . A first work<cite> [6]</cite> proposes a \"retrofitting\" technique consisting in a leveraging of lexicon-derived relational information, namely adjacent words of concepts, to refine their associated word embeddings. The underlying intuition is that adjacent concepts in the KB should have similar embeddings while maintaining most of the semantic information in their prelearned distributed word representations.",
  "y": "background"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_1",
  "x": "Within the latter work focusing on KBs, the goal is to exploit concepts and their relationships to obtain a latent representation of the KB. While some work focused on the representation of relations on the basis of triplets belonging to the KB [3] , other work proposed to enhance the distributed representation of words for representing their underlying concepts by taking into consideration the structure of the KB graph (e.g., concepts in the same category or their relationships with other concepts) <cite>[6,</cite> 18, 19] . A first work<cite> [6]</cite> proposes a \"retrofitting\" technique consisting in a leveraging of lexicon-derived relational information, namely adjacent words of concepts, to refine their associated word embeddings.",
  "y": "background"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_2",
  "x": "In contrast to<cite> [6]</cite> , other work [18, 19] proposes an endto-end oriented approach that rather adjusts the objective function of the neural language model. For instance, Xu et al. [18] propose the RC-NET model that leverages the relational and categorical knowledge to learn a higher quality word embeddings. This model extends the objective function of the skip-gram model [10] with two regularization functions based on relational and categorical knowledge from the external resource, respectively.",
  "y": "background"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_3",
  "x": "While a naive approach would be to exploit the concept embeddings learned from the KB distributed representation <cite>[6,</cite> 18] as input of the deep neural network, we believe that a hybrid representation of the distributional semantic (namely, word embeddings) and the symbolic semantics (namely, concept embeddings taking into account the graph structure) would allow enhancing the document-query matching. Indeed, simply considering concepts belonging to the KB may lead to a partial mismatch with the text of queries and/or documents [4] . With this in mind, the document and query representations could be enhanced with a symbolic semantic layer expressing the projection of the plain text on the KB with the consideration of concepts and their relationships within the KB.",
  "y": "differences"
 },
 {
  "id": "c86271049ebbcf8eafe781a0af6a98_4",
  "x": "With this in mind, the document and query representations could be enhanced with a symbolic semantic layer expressing the projection of the plain text on the KB with the consideration of concepts and their relationships within the KB. On one hand, the representation of the plain text might be, as used in several previous work, a high-dimensional vector of terms [8, 17] or of their corresponding word embeddings [16] . On the other hand, the semantic layer could be built by the representation of concepts (and their relationships) extracted from the plain text through a concept embedding<cite> [6]</cite> or a richer embedding representation of a KB sub-graph, as suggested in [2] .",
  "y": "background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_0",
  "x": "Currently, most of the state-of-the-art VQA models (<cite>Antol et al. 2015</cite>; Chen et al. 2016; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) only focus on how to improve accuracy. However, accuracy is not the only metric to score a given VQA model. Robustness is also a crucial property.",
  "y": "motivation"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_1",
  "x": "In Module 1, we encode all of the questions, also by Skip-Thought Vectors, from the training and validation sets of <cite>VQA</cite> (<cite>Antol et al. 2015</cite>) dataset as a 4800 by 186027 dimension basic question (BQ) matrix, and then solve the LASSO optimization problem (Huang, Alfadly, and Ghanem 2017) , with MQ, to find the top 3 similar BQ of MQ. These BQ are the output of Module 1. Moreover, we take the direct concatenation of MQ and BQ and the given image as the input of Module 2, the general VQA module, and then it will output the answer prediction of MQ.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_2",
  "x": "We claim that the BQ of given MQ can be considered as the small noise of MQ and it will affect the accuracy of VQA model. Then, we verify the above claim by the detailed experiments and use the VQABQ method to analyze the robustness of 6 available pretrained stateof-the-art VQA models, provided by papers' authors, (<cite>Antol et al. 2015</cite>; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) . Note that the available pretrained VQA models can be categorized into two main categories, attention-based and non-attention-based VQA models.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_3",
  "x": "Recently, there are many papers (<cite>Antol et al. 2015</cite>; Shih, Singh, and Hoiem 2016; Chen et al. 2016; Kafle and Kanan 2016; Ma, Lu, and Li 2016; Ren, Kiros, and Zemel 2015; Zhu et al. 2016; Wu et al. 2016; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) have proposed methods to solve the challenging VQA task. Our VQABQ method involves in different areas in Machine Learning, Natural Language Processing (NLP) and Computer Vision. The following, we discuss recent works related to our approach.",
  "y": "background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_4",
  "x": "We take the most popular <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to develop our BQD. At the beginning, we take all of the training and validation questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our basic question candidates. Then, we take all of the testing questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our main question candidates.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_5",
  "x": "At the beginning, we take all of the training and validation questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our basic question candidates. Then, we take all of the testing questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our main question candidates. That is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite> ).",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_6",
  "x": "We take the most popular <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to develop our BQD. At the beginning, we take all of the training and validation questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our basic question candidates. Then, we take all of the testing questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our main question candidates.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_7",
  "x": "Then, we take all of the testing questions from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) to be our main question candidates. That is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite> ). Because we model the basic question generation problem by LASSO, we remove the repeated basic question candidate and the basic question candidate exactly the same as any main question candidate.",
  "y": "similarities uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_8",
  "x": "Our idea is the BQ generation for MQ and, at the same time, we only want the minimum number of BQ to represent the MQ, so modeling our problem as LASSO optimization problem is an appropriate way: (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". , where A is the matrix of encoded BQ, b is the encode MQ and \u03bb is a parameter of the regularization term.",
  "y": "background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_9",
  "x": "We now describe how to generate the BQ of a query question, illustrated in Figure 1 . According to the above subsections, Question Encoding and Problem Formulation, we can encode all basic question candidates from the training and validation question sets of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) by Skip-Thought Vectors, and then we have a matrix of basic question candidates. Each column of the matrix is a vector representation, 4800 by 1 dimensions, of a specific basic question candidate and we have 186027 columns.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_10",
  "x": "Finally, we find the ranked BQ of all 244302 testing questions from the testing question set of <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) and collect them together, with the format {Image, M Q, 21 (BQ + corresponding similarity score)}, as our Basic Question Dataset (BQD). ---------------------------------- **BASIC QUESTION DATASET**",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_11",
  "x": "We propose a novel large scale dataset, called Basic (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". (<cite>Antol et al. 2015</cite>) , and the corresponding similarity scores of BQ are generated by our basic question generation method, referring to Section 3. Note that we preprocess the training and validation question datasets from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) , so the total number of basic question candidates is less than the total number of training and validation question datasets in <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) .",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_12",
  "x": "In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". (<cite>Antol et al. 2015</cite>) , and the corresponding similarity scores of BQ are generated by our basic question generation method, referring to Section 3. Note that we preprocess the training and validation question datasets from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) , so the total number of basic question candidates is less than the total number of training and validation question datasets in <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . In BQD, we have 81434 images, 244302 MQ and 5130342 (BQ + corresponding similarity score).",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_13",
  "x": "(<cite>Antol et al. 2015</cite>) , and the corresponding similarity scores of BQ are generated by our basic question generation method, referring to Section 3. Note that we preprocess the training and validation question datasets from the <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) , so the total number of basic question candidates is less than the total number of training and validation question datasets in <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . In BQD, we have 81434 images, 244302 MQ and 5130342 (BQ + corresponding similarity score). Furthermore, we exploit the BQD to do robustness analysis of the 6 available pretrained state-of-the-art VQA models (<cite>Antol et al. 2015</cite>; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) in the next subsection.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_14",
  "x": "The noise to the question shouldn't be random and it should have some contextual semantics for the measure to be informative. For the image part, there is already a rapidly growing research on evaluating the robustness of deep learning models (Fawzi, Moosavi Dezfooli, and Frossard 2017;  Table 3 : MUTAN without Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\".",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_15",
  "x": "First, we measure the accuracy of the model on the clean <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) and we call it Acc vqa . Then, we append the top ranked k BQs to each of the MQs in the clean dataset and recompute the accuracy of the model on this noisy input and we call it Acc bqd . Finally, we compute the absolute difference Acc dif f = |Acc vqa \u2212 Acc bqd | and we report the robustness score R score .",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_16",
  "x": "**DATASETS** We conduct our experiments on BQD and <cite>VQA</cite> (<cite>Antol et al. 2015</cite>) dataset. <cite>VQA dataset</cite> is based on the MS COCO Table 4 : MUTAN with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) .",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_17",
  "x": "<cite>VQA dataset</cite> is based on the MS COCO Table 4 : MUTAN with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\".",
  "y": "uses background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_18",
  "x": "\"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\". dataset (Lin et al. 2014) and it contains a large number of questions.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_19",
  "x": "dataset (Lin et al. 2014) and it contains a large number of questions. There are questions, 248349 for training, 121512 for validation and 244302 for testing. In the <cite>VQA dataset</cite>, each question is associated with 10 answers annotated by different people from Amazon Mechanical Turk (AMT).",
  "y": "background"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_20",
  "x": "About 98% of answers do not exceed 3 words and 90% of answers have single words. Note that we only develop our work on the open-ended case in <cite>VQA dataset</cite> because it is the most popular task and we also think the open-ended task is closer to the real situation than multiple-choice one. Setup In our LASSO model, we use \u03bb = 10 \u22126 to be our parameter and in the later subsection, we will discuss how the \u03bb affects the quality of BQ.",
  "y": "uses motivation"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_21",
  "x": "<cite>VQA dataset</cite> provides multiple-choice and open-ended task for evaluation. Regarding open-ended task, the answer can be any phrase or word. However, in the multiple-choice task, an answer should be chosen from 18 candidate answers.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_22",
  "x": "For both cases, answers are evaluated by accuracy which can reflect human consensus. The accuracy is given by the following: Table 5 : MLB with Attention model evaluation results on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and <cite>VQA dataset</cite> and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and <cite>VQA dataset</cite>. In addition, dif f = Original dev All \u2212 X dev All , where X is equal to \"First\", \"Second\",..., \"Seventh\".",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_23",
  "x": "We will put some randomly selected BQ examples from our BQD in the supplementary material for references. Note that Figure 2 : The accuracy of state-of-the-art VQA models evaluated on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . Note that we divide the top 21 ranked BQs into 7 partitions and each partition contains 3 ranked BQs.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_24",
  "x": "We want to do more advanced analysis on this model, so we claim that if the quality of Figure 3 : The accuracy decrement of state-of-the-art VQA models evaluated on BQD and <cite>VQA dataset</cite> (<cite>Antol et al. 2015</cite>) . Note that we divide the top 21 ranked BQs into 7 partitions and each partition contains 3 ranked BQs. Here, \"First top 3\" means the first partition, \"Second top 3\" means the second partition,..., and \"Seventh top 3\" means the seventh partition.",
  "y": "uses"
 },
 {
  "id": "c8cf2d615cc47395a55bc8737cd9fd_26",
  "x": [
   "In this paper, we propose a novel VQABQ method and Basic Question Dataset (BQD) for robustness analysis of VQA models. The VQABQ method has two main modules, Basic Question Generation Module and VQA Module. The former one can generate the basic questions for the query question, and the latter one can take an image, basic and query questions as the input and then output the text-based answer of the query question about the given image."
  ],
  "y": "uses"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_0",
  "x": "Sequence labeling is one of the most fundamental NLP models, which is used for many tasks such as named entity recognition (NER), chunking, word segmentation and part-of-speech (POS) tagging. It has been traditionally investigated using statistical approaches (Lafferty et al., 2001; Ratinov and Roth, 2009) , where conditional random fields (CRF) (Lafferty et al., 2001) has been proven as an effective framework, by taking discrete features as the representation of input sequence (Sha and Pereira, 2003; Keerthi and Sundararajan, 2007) . With the advances of deep learning, neural sequence labeling models have achieved state-ofthe-art for many tasks (Ling et al., 2015;<cite> Ma and Hovy, 2016</cite>; Peters et al., 2017) .",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_1",
  "x": "Although many authors released their code along with their sequence labeling papers (Lample et al., 2016;<cite> Ma and Hovy, 2016</cite>; Liu et al., 2018) , the implementations are mostly focused on specific model structures and specific tasks. Modifying or extending can need enormous coding. In this paper, we present Neural CRF++ (NCRF++) 3 , a neural sequence labeling toolkit based on PyTorch, which is designed for solving general sequence labeling tasks with effective and efficient neural models.",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_2",
  "x": "\u2022 Fully configurable: users can design their neural models only through a configuration file without any code work. Figure 1 shows a segment of the configuration file. It builds a LSTM-CRF framework with CNN to encode character sequence (the same structure as <cite>Ma and Hovy (2016)</cite> ), plus POS and Cap features, within 10 lines.",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_3",
  "x": "Similar to the statistical toolkits, NCRF++ supports user-defined features but using distributed representations through lookup tables, which can be initialized randomly or from external pretrained embeddings (embedding directory: emb dir in Figure 1 ). In addition, NCRF++ integrates several state-of-the-art automatic feature extractors, such as CNN and LSTM for character sequences, leading easy reproduction of many recent work (Lample et al., 2016; Chiu and Nichols, 2016;<cite> Ma and Hovy, 2016)</cite> . \u2022 Effective and efficient: we reimplement several state-of-the-art neural models (Lample et al., 2016;<cite> Ma and Hovy, 2016)</cite> using NCRF++.",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_4",
  "x": "In addition, NCRF++ integrates several state-of-the-art automatic feature extractors, such as CNN and LSTM for character sequences, leading easy reproduction of many recent work (Lample et al., 2016; Chiu and Nichols, 2016;<cite> Ma and Hovy, 2016)</cite> . \u2022 Effective and efficient: we reimplement several state-of-the-art neural models (Lample et al., 2016;<cite> Ma and Hovy, 2016)</cite> using NCRF++. Experiments show models built in NCRF++ give comparable performance with reported results in the literature.",
  "y": "similarities uses"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_5",
  "x": "\u2022 Word RNN together with GRU and LSTM are available in NCRF++, which are popular structures in the recent literature (Huang et al., 2015; Lample et al., 2016;<cite> Ma and Hovy, 2016</cite>; Yang et al., 2017) . Bidirectional RNNs are supported to capture the left and right contexted information of each word. The hidden vectors for both directions on each word are concatenated to represent the corresponding word.",
  "y": "background"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_6",
  "x": "Sang and Buchholz, 2000) , data split is following Reimers and Gurevych (2017) . For POS tagging, we use the same data and split with <cite>Ma and Hovy (2016)</cite> . We test different combinations of character representations and word sequence representations on these three benchmarks.",
  "y": "similarities uses"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_7",
  "x": "Hyperparameters are mostly following <cite>Ma and Hovy (2016)</cite> and almost keep the same in all these experiments 5 . Standard SGD with a decaying learning rate is used as the optimizer. Table 1 shows the results of six CRF-based models with different character sequence and word sequence representations on three benchmarks.",
  "y": "similarities uses"
 },
 {
  "id": "c91781e76a8d7d6de7d7bc4407e799_8",
  "x": "Most of state-of-the-art models utilize the framework of word LSTM-CRF with character LSTM or CNN features (correspond to \"CLSTM+WLSTM+CRF\" and \"CCNN+WLSTM+CRF\" of our models) (Lample et al., 2016;<cite> Ma and Hovy, 2016</cite>; Yang et al., 2017; Peters et al., 2017) . Our implementations can achieve comparable results, with better NER and chunking performances and slightly lower POS tagging accuracy. Note that we use almost the same hyperparameters across all the experiments to achieve the results, which demonstrates the robustness of our implementation.",
  "y": "background"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_0",
  "x": "These are sometimes referred to as \"the three Vs\" of communication (Mehrabian et al., 1971) . Multimodal sentiment analysis research focuses on understanding how an individual modality conveys sentiment information (intra-modal dynamics), and how they interact with each other (intermodal dynamics). It is a challenging research area and state-of-the-art performance of automatic sentiment prediction has room for improvement compared to human performance<cite> (Zadeh et al., 2018a)</cite> .",
  "y": "background"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_1",
  "x": "We use the CMU Multimodal Data Software Development Kit (SDK)<cite> (Zadeh et al., 2018a)</cite> to load and pre-process the CMU-MOSI database, which splits the 2199 opinion segments into training (1283 segments), validation (229 segments), and test (686 segments) sets. 1 We implement the sentiment analysis models using the Keras deep learning library (Chollet et al., 2015) . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_2",
  "x": "We use the vocal modality at the bottom of the hierarchy while using the verbal modality at the top in HF fusion. This is because in previous studies (e.g., <cite>Zadeh et al. (2018a)</cite> ) the verbal modality was shown to be the most effective for unimodal sentiment analysis, while the vocal modality was shown to be the least effective. 4",
  "y": "similarities background"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_3",
  "x": "2 In Tables 2 and 3 , \"S\" is the singletask learning model; \"S+P\" is the bi-task learning model with polarity classification as the auxillary task; \"S+I\" is the bi-task learning model with intensity classification as the auxillary task; \"S+P+I\" is the tri-task learning model. To evaluate the performance of sentiment score prediction, following previous work<cite> (Zadeh et al., 2018a)</cite> , we Tables 2 and 3 , the numbers in bold are the best performance for each modality or fusion strategy. To identify the significant differences in results, we perform a two-sample Wilcoxon test on the sentiment score predictions given by each pair of models being compared and consider p < 0.05 as significant.",
  "y": "uses"
 },
 {
  "id": "c9d9997b61974a537915a2c90af3cf_4",
  "x": "The results of unimodal sentiment prediction experiments are shown in Table 2 . 3 The verbal models have the best performance here, which is consistent with previous sentiment analysis studies on multiple databases (e.g., <cite>Zadeh et al. (2018a)</cite> ). This suggests that lexical information remains the most effective for sentiment analysis.",
  "y": "similarities"
 },
 {
  "id": "cc5927700475b7abc0482a28ab209a_0",
  "x": "Pre-trained word embeddings such as Glove (Pennington et al., 2014) , word2vec (Mikolov et al., 2013) and fasttext (Bojanowski et al., 2017) , trained on large corpora are readily available for use in a variety of tasks. Subsequently, there has been emphasis on post-processing the embeddings to improve their performance on downstream tasks <cite>(Mu and Viswanath, 2018)</cite> or to induce linguistic properties (Mrk\u0161ic et al.; Faruqui et al., 2015) . In particular, the Principal Component Analysis (PCA) based post-processing algorithm proposed by <cite>(Mu and Viswanath, 2018)</cite> has led to significant gains in word and sentence similarity tasks, and has also proved useful in dimensionality reduction (Raunak, 2017) .",
  "y": "background"
 },
 {
  "id": "cc5927700475b7abc0482a28ab209a_1",
  "x": "Pre-trained word embeddings such as Glove (Pennington et al., 2014) , word2vec (Mikolov et al., 2013) and fasttext (Bojanowski et al., 2017) , trained on large corpora are readily available for use in a variety of tasks. Subsequently, there has been emphasis on post-processing the embeddings to improve their performance on downstream tasks <cite>(Mu and Viswanath, 2018)</cite> or to induce linguistic properties (Mrk\u0161ic et al.; Faruqui et al., 2015) . In particular, the Principal Component Analysis (PCA) based post-processing algorithm proposed by <cite>(Mu and Viswanath, 2018)</cite> has led to significant gains in word and sentence similarity tasks, and has also proved useful in dimensionality reduction (Raunak, 2017) .",
  "y": "background"
 },
 {
  "id": "cc5927700475b7abc0482a28ab209a_2",
  "x": "4. We point out the limitations of applying variance based post-processing <cite>(Mu and Viswanath, 2018)</cite> and demonstrate that it leads to a decrease in performance in sentence classification and machine translation arXiv:1910.02211v1 [cs.CL] 5 Oct 2019 In Section 1, we provide an introduction to the problem statement. In Section 2 we discuss dimensional properties of word embeddings. In Section we 3 conduct a variance based analysis by measuring performance of word embeddings on downstream tasks.",
  "y": "motivation"
 },
 {
  "id": "cc5927700475b7abc0482a28ab209a_3",
  "x": "**THE POST PROCESSING ALGORITHM (PPA)** In this section, we first describe and then evaluate the post-processing algorithm (PPA) proposed in <cite>(Mu and Viswanath, 2018)</cite> , which achieves high scores on Word and Semantic textual similarity tasks. The algorithm removes the projections of top principal components from each of the word vectors, making the individual word vectors more discriminative (Refer to Algorithm 1 for details).",
  "y": "uses"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_0",
  "x": "Since its inception, papers have applied different methods such as feature based (Kiritchenko et al., 2014) , Recursive Neural Networks (RecNN) <cite>(Dong et al., 2014)</cite> , Recurrent Neural Networks (RNN) (Tang et al., 2016a) , attention applied to RNN (Wang et al., 2016; Chen et al., 2017; Tay et al., 2017) , Neural Pooling (NP) Wang et al., 2017) , RNN combined with NP (Zhang et al., 2016) , and attention based neural networks (Tang et al., 2016b) . Others have tackled TDSA as a joint task with target extraction, thus treating it as a sequence labelling problem. Mitchell et al. (2013) carried out this task using Conditional Random Fields (CRF), and this work was then extended using a neural CRF .",
  "y": "background"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_1",
  "x": "First, Chen et al. (2017) compared results across SemEval's laptop and restaurant reviews in English (Pontiki et al., 2014) , a Twitter dataset <cite>(Dong et al., 2014)</cite> and their own Chinese news comments dataset. They did perform a comparison across different languages, domains, corpora types, and different methods; SVM with features (Kiritchenko et al., 2014) , Rec-NN <cite>(Dong et al., 2014)</cite> , TDLSTM (Tang et al., 2016a) , Memory Neural Network (MNet) (Tang et al., 2016b) and their own attention method. However, the Chinese dataset was not released, and the methods were not compared across all datasets.",
  "y": "background"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_2",
  "x": "They did perform a comparison across different languages, domains, corpora types, and different methods; SVM with features (Kiritchenko et al., 2014) , Rec-NN <cite>(Dong et al., 2014)</cite> , TDLSTM (Tang et al., 2016a) , Memory Neural Network (MNet) (Tang et al., 2016b) and their own attention method. However, the Chinese dataset was not released, and the methods were not compared across all datasets. By contrast, we compare all methods across all datasets, using techniques that are not just from the Recurrent Neural Network (RNN) family.",
  "y": "background"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_3",
  "x": "Thus making it impossible to use the second set of annotation and forcing us to only use a subset of the dataset. An as example of this: \"Got rid of bureaucrats 'and we put that money, into 9000 more doctors and nurses'... to turn the doctors into bureaucrats#BattleForNumber10\" in that Tweet 'bureaucrats' was annotated as negative but it does not state if it was the first or second instance of 'bureaucrats' since it does not use target spans. As we can see from table 2, generally the social media datasets (Twitter and YouTube) contain more targets per sentence with the exception of<cite> Dong et al. (2014)</cite> and Mitchell et al. (2013) .",
  "y": "differences"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_4",
  "x": "Vo and Zhang (2015) created the first NP method for TDSA. All of their experiments are performed on<cite> Dong et al. (2014)</cite> Twitter data set.",
  "y": "background"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_5",
  "x": "---------------------------------- **SCALING AND FINAL MODEL COMPARISON** We test all of the methods on the test data set of<cite> Dong et al. (2014)</cite> and show the difference between the original and reproduced models in figure 2 .",
  "y": "uses similarities"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_6",
  "x": "TDParse+ the features of TDParse and LS and RS contexts. The experiments are performed on the<cite> Dong et al. (2014)</cite> and Wang et al. (2017) Twitter datasets where we train and test on the previously specified train and test splits. We also scale our features using Max Min scaling before inputting into the SVM.",
  "y": "similarities uses"
 },
 {
  "id": "d0007c7f1f9ecfbdd7b6ad7c59cc92_7",
  "x": "All of the methods outputs are fed into a softmax activation function. The experiments are performed on the<cite> Dong et al. (2014)</cite> dataset where we train and test on the specified splits. For the LSTMs we initialised the weights using uniform distribution U(0.003, 0.003), used Stochastic Gradient Descent (SGD) a learning rate of 0.01, cross entropy loss, padded and truncated sequence to the length of the maximum sequence in the training dataset as stated in the original paper, and we did not \"set the clipping threshold of softmax layer as 200\" (Tang et al., 2016a) as we were unsure what this meant.",
  "y": "similarities uses"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_0",
  "x": "Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou' et al., 2013; <cite>Bansal et al., 2014</cite>; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015) . While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) .",
  "y": "background"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_1",
  "x": "Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou' et al., 2013; <cite>Bansal et al., 2014</cite>; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015) . While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) .",
  "y": "background"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_2",
  "x": "Secondly, it would also be useful to learn dense representations directly for the higher-order substructures (that structured NLP tasks factor on) so as to explicitly capture the useful, hidden relationships among these substructures, instead of relying on the sparse word-conjoined relationships. In this work, we propose to address both these issues by learning simple dependency link embeddings on 'head-argument' pairs (as a single concatenated unit), which allows us to work directly with linguistically-intuitive, higher-order substructures, and also fire significantly fewer and simpler features in dependency parsing, as opposed to word cluster and embedding features in previous work (Koo et al., 2008; <cite>Bansal et al., 2014</cite>) , while still maintaining <cite>their</cite> strong accuracies. Trained using appropriate dependency-based context in word2vec, the fast neural language model of Mikolov et al. (2013a) , these link vectors allow a substantially smaller set of unary link features (as opposed to n-ary, conjoined features) which provide savings in parsing time and memory.",
  "y": "differences motivation"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_3",
  "x": "At the same time, these link embedding features maintain dependency parsing improvements similar to the complex, template-based features on word clusters and embeddings by previous work (Koo et al., 2008; <cite>Bansal et al., 2014</cite> ) (up to 9% relative error reduction), and also stack statistically significantly over <cite>them</cite> (up to an additional 5% relative error reduction). Another advantage of this approach (versus <cite>previous work</cite> on feature embeddings or special neural networks for parsing) is that these link embeddings can be imported as off-the-shelf, dense, syntactic features into various other NLP tasks, similar to word embedding features, but now with richer, structured information, and in tasks where plain word embeddings have not proven useful . As an example, we incorporate them into a constituent parse reranker and see improvements that again match state-of-the-art, manually-defined, non-local reranking features and stack over them statistically significantly.",
  "y": "motivation differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_4",
  "x": "To train the link embeddings, we use the speedy, skip-gram neural language model of Mikolov et al. (2013a; 2013b) via their toolkit word2vec. 2 We use the original skip-gram model and simply change the context tuple data on which the model is trained, similar to <cite>Bansal et al. (2014)</cite> and Levy and Goldberg (2014) . The goal is to learn similar embeddings for links with similar syntactic contextual properties like label, signed distance, ancestors, etc.",
  "y": "similarities"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_5",
  "x": "**FEATURES** The BROWN cluster features are based on <cite>Bansal et al. (2014)</cite> , <cite>who</cite> follow Koo et al. (2008) Koo et al. (2008) ). We have another feature that additionally includes the signed, bucketed distance of the particular link in the given sentence.",
  "y": "uses"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_6",
  "x": "We have another feature that additionally includes the signed, bucketed distance of the particular link in the given sentence. Also note the difference of our unary bucket features from the binary bucket features of <cite>Bansal et al. (2014)</cite> , <cite>who</cite> had to work with pairwise, conjoined features of the head and the argument. Hence, <cite>they</cite> used features on conjunctions of the two bucket values from the head and argument word vectors, firing one pairwise feature per dimension, because firing features on all dimension pairs (corresponding to an outer product) led to an infeasible number of features.",
  "y": "differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_7",
  "x": "Also note the difference of our unary bucket features from the binary bucket features of <cite>Bansal et al. (2014)</cite> , <cite>who</cite> had to work with pairwise, conjoined features of the head and the argument. Hence, <cite>they</cite> used features on conjunctions of the two bucket values from the head and argument word vectors, firing one pairwise feature per dimension, because firing features on all dimension pairs (corresponding to an outer product) led to an infeasible number of features. The result discussion of these feature differences in presented in \u00a73.2.",
  "y": "differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_8",
  "x": "For all experiments (unless otherwise noted), we follow the 2nd-order MSTParser setup of <cite>Bansal et al. (2014)</cite> , in terms of data splits, parameters, preprocessing, and feature thresholding. Statistical significance is reported based on the bootstrap test (Efron and Tibshirani, 1994) with 1 million samples. First, we compare the number of features in Table 2 .",
  "y": "uses"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_9",
  "x": "First, we compare the number of features in Table 2 . Our dense, unary, link-embedding based Bucket and Bit-string features are substantially fewer than the sparse, n-ary, template-based features used in the MSTParser baseline, in BROWN, and in the word embedding SKIP DEP result of B<cite>ansal et al. (2014)</cite> . This in turn also improves our parsing speed and memory.",
  "y": "differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_10",
  "x": "BROWN (92.7), are statistically significant at p < 0.01. Moreover, the Bit-string result (92.6) is the same, i.e., has no statistically significant difference from the BROWN result (92.7), and also from the <cite>Bansal et al. (2014</cite>) SKIP DEP result (92.7). Therefore, the main contribution of these link embeddings is that their significantly simpler, smaller, and faster set of unary features can match the performance of complex, template-based BROWN features (and of the dependency-based word embedding features of <cite>Bansal et al. (2014)</cite> ), and also stack over them.",
  "y": "similarities"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_11",
  "x": "We also get similar trends of improvements on the labeled attachment score (LAS) metric. 8 Moreover, unlike <cite>Bansal et al. (2014)</cite> , our Bucket features achieve statistically significant improvements, most likely because <cite>they</cite> fired D pairwise, conjoined features, one per dimension d, consisting of the two bucket values from the head and argument word vectors. This would disallow the classifier to learn useful linear combinations of the various dimensions.",
  "y": "differences"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_12",
  "x": "**RELATED WORK** As mentioned earlier, there has been a lot of useful, previous work on using word embeddings for NLP tasks such as similarity, tagging, NER, sentiment analysis, and parsing (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Huang et al., 2012; Al-Rfou' et al., 2013; Hisamoto et al., 2013; Andreas and Klein, 2014; <cite>Bansal et al., 2014;</cite> Guo et al., 2014; Pennington et al., 2014; Wang et al., 2015) , inter alia. In related work, <cite>Bansal et al. (2014)</cite> also use dependency context to tailor word embeddings to dependency parsing.",
  "y": "background"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_13",
  "x": "As mentioned earlier, there has been a lot of useful, previous work on using word embeddings for NLP tasks such as similarity, tagging, NER, sentiment analysis, and parsing (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Huang et al., 2012; Al-Rfou' et al., 2013; Hisamoto et al., 2013; Andreas and Klein, 2014; <cite>Bansal et al., 2014;</cite> Guo et al., 2014; Pennington et al., 2014; Wang et al., 2015) , inter alia. In related work, <cite>Bansal et al. (2014)</cite> also use dependency context to tailor word embeddings to dependency parsing. However, <cite>their</cite> embedding features are still based on the sparse set of n-ary, word-based templates from previous work (McDonald et al., 2005a; Koo et al., 2008) .",
  "y": "background"
 },
 {
  "id": "d0b4d9566f16915cb5a5244f351e61_14",
  "x": "In related work, <cite>Bansal et al. (2014)</cite> also use dependency context to tailor word embeddings to dependency parsing. However, <cite>their</cite> embedding features are still based on the sparse set of n-ary, word-based templates from previous work (McDonald et al., 2005a; Koo et al., 2008) . Our structured link embeddings achieve similar improvements <cite>as theirs</cite> (and better in the case of direct, per-dimension bucket features) with a substantially smaller and simpler (unary) set of features that are aimed to directly capture hidden relationships between the substructures that dependency parsing factors on.",
  "y": "motivation differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_0",
  "x": "By using a larger value of k, the number of nearest neighbors to use for determining the class of a test example, and through 10-fold cross validation to automatically determine the best k, we have obtained improved disambiguation accuracy on a large sense-tagged corpus first used in <cite>(Ng and Lee, 1996)</cite> . The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in (Mooney, 1996) to have the highest disambiguation accuracy among seven state-of-the-art machine learning algorithms. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_1",
  "x": "Many different learning approaches have been used, including neural networks (Leacock et al., 1993) , probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al., 1992a; Gale et al., 1995; Leacock et al., 1993; Yarowsky, 1992) , decision lists (Yarowsky, 1994) , exemplar-based learning algorithms (Cardie, 1993; <cite>Ng and Lee, 1996)</cite> , etc. In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word \"line\". The seven algorithms that he evaluated are: a Naive-Bayes classifier (Duda and Hart, 1973) , a perceptron (Rosenblatt, 1958) , a decisiontree learner (Quinlan, 1993) , a k nearest-neighbor classifier (exemplar-based learner) (Cover and Hart, 1967) , logic-based DNF and CNF learners (Mooney, 1995) , and a decision-list learner (Rivest, 1987) .",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_2",
  "x": "This is in spite of the conditional independence assumption made by the Naive-Bayes algorithm, which may be unjustified in the domains tested. Gale, Church and Yarowsky (Gale et al., 1992a; Gale et al., 1995; Yarowsky, 1992) have also successfully used the Naive-Bayes algorithm (and several extensions and variations) for word sense disambiguation. On the other hand, <cite>our past work</cite> on WSD <cite>(Ng and Lee, 1996)</cite> used an exemplar-based (or nearest neighbor) learning approach.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_3",
  "x": "These parameters include the number of nearest neighbors to use for determining the class of a test example (i.e., k in a k nearest-neighbor classifier), exemplar weights, feature weights, etc. We found that the number k of nearest neighbors used has a considerable impact on the accuracy of the induced exemplar-based classifier. By using 10-fold cross validation (Kohavi and John, 1995) on the training set to automatically determine the best k to use, we have obtained improved disambiguation accuracy on a large sensetagged corpus first used in <cite>(Ng and Lee, 1996)</cite> .",
  "y": "differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_4",
  "x": "Section 4 presents the disambiguation accuracy of PEBLS and Naive-Bayes on the large corpus of <cite>(Ng and Lee, 1996)</cite> . Section 5 discusses the implications of the results. Section 6 gives the conclusion.",
  "y": "uses"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_5",
  "x": "These parameters include k (the number of nearest neighbors to use for determining the class of a test example), exemplar weights, feature weights, etc. Each of these parameters has a default value in PEBLS, eg., k = 1, no exemplar weighting, no feature weighting, etc. <cite>We</cite> have used the default values for all parameter settings in <cite>our previous work</cite> on exemplar-based WSD reported in <cite>(Ng and Lee, 1996)</cite> .",
  "y": "uses"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_6",
  "x": "In our present study, we evaluated PEBLS and Naive-Bayes on a much larger corpus containing sense-tagged occurrences of 121 nouns and 70 verbs. This corpus was first reported in <cite>(Ng and Lee, 1996)</cite> , and it contains about 192,800 sense-tagged word occurrences of 191 most frequently occurring and ambiguous words of English. 1 These 191 words have been tagged with senses from WOI:tDNET (Miller, 1990) , an on-line, electronic dictionary available publicly.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_7",
  "x": "However, his comparative study is done on only one word using a data set of 2,094 examples. In our present study, we evaluated PEBLS and Naive-Bayes on a much larger corpus containing sense-tagged occurrences of 121 nouns and 70 verbs. This corpus was first reported in <cite>(Ng and Lee, 1996)</cite> , and it contains about 192,800 sense-tagged word occurrences of 191 most frequently occurring and ambiguous words of English.",
  "y": "uses"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_8",
  "x": "Both test sets are identical to the ones reported in <cite>(Ng and Lee, 1996)</cite> . Since the primary aim of our present study is the comparative evaluation of learning algorithms, not feature representation, we have chosen, for simplicity, to use local collocations as the only features in the example representation. Local collocations have been found to be the single most informative set of features for WSD <cite>(Ng and Lee, 1996)</cite> .",
  "y": "similarities"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_9",
  "x": "Since the primary aim of our present study is the comparative evaluation of learning algorithms, not feature representation, we have chosen, for simplicity, to use local collocations as the only features in the example representation. Local collocations have been found to be the single most informative set of features for WSD <cite>(Ng and Lee, 1996)</cite> . That local collocation knowledge provides important clues to WSD has also been pointed out previously by Yarowsky (1993) .",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_10",
  "x": "Contact the LDC at ldc@unagi.cis.upenn.edu for details. 2The first five of these seven features were also used in <cite>(Ng and Lee, 1996)</cite> . , 1996) . The default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating WSD programs (Gale et al., 1992b; Miller et al., 1994) .",
  "y": "similarities"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_11",
  "x": "Another assignment method is to determine the most frequently occurring sense in the training examples, and to assign this sense to all test examples. We call this method \"Most Frequent\" in Table 1 . The accuracy figures of LEXAS as reported in <cite>(Ng and Lee, 1996)</cite> are reproduced in the third row of Table 1 .",
  "y": "uses"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_12",
  "x": "However, the feature value pruning method of <cite>(Ng and Lee, 1996)</cite> only selects surrounding words and local collocations as feature values if they are indicative of some sense class as measured by conditional probability (See <cite>(Ng and Lee, 1996)</cite> for details). The next three rows show the accuracy figures of PEBLS using the parameter setting of k = 1, k = 20, and 10-fold cross validation for finding the best k, respectively. The last row shows the accuracy figures of the Naive-Bayes algorithm.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_13",
  "x": "The last row shows the accuracy figures of the Naive-Bayes algorithm. Accuracy figures of the last four rows are all based on only seven collocation features as described earlier in this section. However, all possible feature values (collocated words) are used, without employing the feature value pruning method used in <cite>(Ng and Lee, 1996)</cite> .",
  "y": "differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_14",
  "x": "However, all possible feature values (collocated words) are used, without employing the feature value pruning method used in <cite>(Ng and Lee, 1996)</cite> . Note that the accuracy figures of PEBLS with k = 1 are 1.0% and 1.6% higher than the accuracy figures of <cite>(Ng and Lee, 1996)</cite> in the third row, also with k = 1. The feature value pruning method of <cite>(Ng and Lee, 1996)</cite> is intended to keep only feature values deemed important for classification.",
  "y": "differences"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_15",
  "x": "The feature value pruning method of <cite>(Ng and Lee, 1996)</cite> is intended to keep only feature values deemed important for classification. It seems that the pruning method has filtered out some usefifl collocation values that improve classification accuracy, such that this unfavorable effect outweighs the additional set of features (part of speech and morphological form, surrounding words, and verb-object syntactic relation) used. Our results indicate that although Naive-Bayes performs better than PEBLS with k = 1, PEBLS with k = 20 achieves comparable performance.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_16",
  "x": "<cite>Our past work</cite> <cite>(Ng and Lee, 1996)</cite> suggests that multiple sources of knowledge are indeed useful for WSD. Future work will explore the addition of these other features to further improve disambiguation accuracy. Besides the parameter k, PEBLS also contains other learning parameters such as exemplar weights and feature weights.",
  "y": "background"
 },
 {
  "id": "d160d5a44f795f2b694d5ee538d713_17",
  "x": "Exemplar weighting has been found to improve classification performance (Cost and Saizberg, 1993) . Also, given the relative importance of the various knowledge sources as reported in <cite>(Ng and Lee, 1996)</cite> , it may be possible to improve disambignation performance by introducing feature weighting. Future work can explore the effect of exemplar weighting and feature weighting on disambiguation accuracy.",
  "y": "future_work"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_0",
  "x": "More importantly, KBQG can improve the ability of machines to actively ask questions on human-machine conversations (Duan et al., 2017; Sun et al., 2018) . Therefore, this task has attracted more attention in recent years (Serban et al., 2016;<cite> Elsahar et al., 2018)</cite> . Specifically, KBQG is the task of generating natural language questions according to the input facts from a knowledge base with triplet form, like <subject, predicate, object>. For example, as illustrated in Figure 1 , KBQG aims at generating a question \"Which city is Statue of Liberty located in?\" (Q3) for the input factual triplet Which city is Statue of Liberty located in? Figure 1 : Examples of KBQG.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_1",
  "x": "In comparison, <cite>Elsahar et al. (2018)</cite> obtain obvious degradation on all metrics while there is only a slight decline in our model. We believe that it may owe to the contextaugmented fact encoder since our model drops to 40.87 on the BLEU4 score without contextaugmented fact encoder and transE embeddings. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_2",
  "x": "More recently, some researches have striven toward this task, where the behind intuition is to construct implicit associations between facts and texts. Specifically, Serban et al. (2016) designed an encoder-decoder architecture to generate questions from structured triplet facts. In order to improve the generalization for KBQG, <cite>Elsahar et al. (2018)</cite> utilized extra contexts as input via distant supervisions (Mintz et al., 2009) , then a decoder is equipped with attention and part-ofspeech (POS) copy mechanism to generate questions.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_3",
  "x": "In order to improve the generalization for KBQG, <cite>Elsahar et al. (2018)</cite> utilized extra contexts as input via distant supervisions (Mintz et al., 2009) , then a decoder is equipped with attention and part-ofspeech (POS) copy mechanism to generate questions. Nevertheless, we observe that there are still two important research issues (RIs) which are not processed well or even neglected.",
  "y": "motivation background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_4",
  "x": "The generated question is required to express the given predicate in the fact. For example in Figure 1 , Q1 does not express (match) the predicate (fb:location/containedby) while it is expressed in Q2 and Q3. Previous work<cite> (Elsahar et al., 2018)</cite> usually obtained predicate textual contexts through distant supervision.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_5",
  "x": "We make statistic in the resources released by <cite>Elsahar et al. (2018)</cite> , and find that only 44% predicates have predicate textual context 2 . Therefore, it is prone to generate error questions from such without-context predicates. RI-2: The generated question is required to contain a definitive answer.",
  "y": "uses motivation"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_6",
  "x": "In contrast, Q3 refers to a definitive answer (the object \"New York City\" in the given fact) by restraining the answer type to a city. We believe that Q3, which expresses the given predicate and refers to a definitive answer, is a better question than Q1 and Q2. In previous work, <cite>Elsahar et al. (2018)</cite> only regarded a most frequently mentioned entity type as the textual context for the subject or object in the triplet.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_7",
  "x": "In previous work, <cite>Elsahar et al. (2018)</cite> only regarded a most frequently mentioned entity type as the textual context for the subject or object in the triplet. In fact, most answer entities have multiple types, where the most frequently mentioned type tends to be universal (e.g. a broad type \"administrative region\" rather than a refined type \"US state\" for the entity \"New York\"). Therefore, generated questions from <cite>Elsahar et al. (2018)</cite> may be difficult to contain definitive answers.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_8",
  "x": "Specifically, besides using predicate contexts from the distant supervision utilized by <cite>Elsahar et al. (2018)</cite> , we further leverage the domain, range and even topic for the given predicate as contexts, which are off-the-shelf in KBs (e.g. the range and the topic for the predicate fb:location/containedby are \"location\" and \"containedby\", respectively 1 ). Therefore, 100% predicates (rather than 44% 2 of those in Elsahar et al.) have contexts. Furthermore, in addition to the most frequently mentioned entity type as contexts used by <cite>Elsahar et al. (2018)</cite> , we leverage the type that best describes the entity as contexts (e.g. a refined entity type 3 \"US state\" combines a broad type \"administrative region\" for the entity \"New York\"), which is helpful to refine the entity information.",
  "y": "extends"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_9",
  "x": "Therefore, 100% predicates (rather than 44% 2 of those in Elsahar et al.) have contexts. Furthermore, in addition to the most frequently mentioned entity type as contexts used by <cite>Elsahar et al. (2018)</cite> , we leverage the type that best describes the entity as contexts (e.g. a refined entity type 3 \"US state\" combines a broad type \"administrative region\" for the entity \"New York\"), which is helpful to refine the entity information. Finally, in order to make full use of these contexts, we propose context-augmented fact encoder and multi-level copy mechanism (KB copy and context copy) to integrate diversified contexts, where the multilevel copy mechanism can copy from KB and textual contexts simultaneously.",
  "y": "extends"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_10",
  "x": "Similarly, the predicate embedding e p and the object embedding e o are mapped from the KB embedding matrix E f , where E f is pre-trained using TransE (Bordes et al., 2013) to capture much more fact information in previous work<cite> (Elsahar et al., 2018)</cite> . In our model, E f can be pre-trained or randomly initiated (Details in Sec. 4.7.1). ----------------------------------",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_11",
  "x": "<cite>Elsahar et al. (2018)</cite> demonstrated the effectiveness of POS copy for the context. However, such a copy mechanism heavily relies on POS tagging. Inspired by the CopyNet (Gu et al., 2016) , we directly copy words in the textual contexts C, and it does not rely on any POS tagging.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_12",
  "x": "For the subject and object context, we combine the most frequently mentioned entity type<cite> (Elsahar et al., 2018)</cite> with the type that best describe the entity 3 . The KB copy needs subject names as the copy source, and we map entities with their names similar to those in Mohammed et al. (2018) . The data details are in Appendix A and submitted Supplementary Data.",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_13",
  "x": "---------------------------------- **EVALUATION METRICS** Following (Serban et al., 2016;<cite> Elsahar et al., 2018)</cite> , we adopt some word-overlap based metrics (WBMs) for natural language generation including BLEU-4 (Papineni et al., 2002) , ROUGE L (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) .",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_14",
  "x": "Following (Serban et al., 2016;<cite> Elsahar et al., 2018)</cite> , we adopt some word-overlap based metrics (WBMs) for natural language generation including BLEU-4 (Papineni et al., 2002) , ROUGE L (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) . To better evaluate generated questions, we run two further evaluations as follows.",
  "y": "extends"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_15",
  "x": "(2) Serban et al. (2016): We compare our methods with the single placeholder model, which performs best in Serban et al. (2016) . (3) <cite>Elsahar et al. (2018)</cite> : We compare our methods with the model utilizing copy actions, the best performing model in <cite>Elsahar et al. (2018)</cite> . Although this model is designed to a zero-shot setting (for unseen predicates and entity type), it has good abilities to generate better questions (on known or unknown predicates and entity types) represented in the additional context input and SPO copy mechanism.",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_16",
  "x": "---------------------------------- **IMPLEMENTATION DETAILS** To make our model comparable to the comparison methods, we keep most parameter values the same as <cite>Elsahar et al. (2018)</cite> .",
  "y": "similarities"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_17",
  "x": "We set the weight (\u03bb) of the answer-aware loss to 0.2. In Table 1 , we compare our model with the typical baselines on word-overlap based metrics. It is evident that our model is remarkably better than baselines on all metrics, where the BLEU4 score increases 4.53 compared with the strongest baseline<cite> (Elsahar et al., 2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_18",
  "x": "Identification Serban et al. (2016) 53.5 <cite>Elsahar et al. (2018)</cite> 71.5 Our Model ans loss 75.5 from each model, and then two annotators are employed to judge whether the generated question expresses the given predicate. The Kappa for inter-annotator statistics is 0.611, and p-value for all scores is less than 0.005. As shown in Table  2 , we can see that our model has a significant improvement in the predicate identification.",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_19",
  "x": "To investigate our model, by incorporating answer-aware loss, does not generate an answer type word in a mandatory way, we found 20.5% predicate corresponds to the generated questions without answer type words when our model obtains the highest Ans cov (\u03bb=0.5), and it is very close to 21.7% for the one in human-annotated questions. This demonstrates that the answer-aware loss does not force all predicates to generate questions with answer type words. Table 4 : Ablation study by removing the main components, where \"w/o\" means without, and \"w/o diversified contexts\" represents that diversified contexts are replaced by contexts used in <cite>Elsahar et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_20",
  "x": "It demonstrates that all these components are useful. Specifically, the last line in Table 4 , replacing diversified contexts with contexts used in <cite>Elsahar et al. (2018)</cite> , has more obvious performance degradation. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_21",
  "x": "Naturalness Serban et al. (2016) 2.96 <cite>Elsahar et al. (2018)</cite> 2.23 Our Model ans loss 3.56 Human evaluation is important for generated questions. Following <cite>Elsahar et al. (2018)</cite> , we sample 100 questions from each system, and then two annotators measure the naturalness by a score of 0-5. The Kappa coefficient for inter-annotator is 0.629, and p-value for all scores is less than 0.005.",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_22",
  "x": "Naturalness Serban et al. (2016) 2.96 <cite>Elsahar et al. (2018)</cite> 2.23 Our Model ans loss 3.56 Human evaluation is important for generated questions. Following <cite>Elsahar et al. (2018)</cite> , we sample 100 questions from each system, and then two annotators measure the naturalness by a score of 0-5. The Kappa coefficient for inter-annotator is 0.629, and p-value for all scores is less than 0.005.",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_23",
  "x": "The Kappa coefficient for inter-annotator is 0.629, and p-value for all scores is less than 0.005. As shown in Table 5 , <cite>Elsahar et al. (2018)</cite> Pre-trained KB embeddings may provide rich structured relational information among entities. However, it heavily relies on large-scale triplets, which is time and resource-intensive.",
  "y": "uses"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_24",
  "x": "---------------------------------- **THE EFFECTIVENESS OF GENERATED QUESTIONS FOR ENHANCING QUESTION ANSWERING OVER KNOWLEDGE BASES** Data Type Accuracy human-labeled data 68.97 + gen data (Serban et al., 2016) 68.53 + gen data<cite> (Elsahar et al., 2018)</cite> 69.13 + gen data (Our Model ans loss ) 69.57 Previous experiments demonstrate that our model can deliver more precise questions.",
  "y": "differences"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_25",
  "x": "Our work is also inspired by the recent work for KBQG based on encoderdecoder frameworks. Serban et al. (2016) first proposed a neural network for mapping KB facts into natural language questions. To improve the generalization, <cite>Elsahar et al. (2018)</cite> introduced extra contexts for the input fact, which achieved significant performances.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_26",
  "x": "To improve the generalization, <cite>Elsahar et al. (2018)</cite> introduced extra contexts for the input fact, which achieved significant performances. However, these contexts may make it difficult to generate questions that express the given predicate and associate with a definitive answer. Therefore, we focus on the two research issues: expressing the given predicate and referring to a definitive answer for generated questions.",
  "y": "background motivation"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_27",
  "x": "Point network predicted the output sequence directly from the input, and it can not generate new words while CopyNet (Gu et al., 2016) combined copying and generating. Bao et al. (2018) proposed to copy elements in the table (KB). <cite>Elsahar et al. (2018)</cite> exploited POS copy action to better capture textual contexts.",
  "y": "background"
 },
 {
  "id": "d2028986dc30ccc0ca840ca3b2f454_28",
  "x": "<cite>Elsahar et al. (2018)</cite> exploited POS copy action to better capture textual contexts. To incorporate advantages from above copy mechanisms, we introduce KB copy and context copy which can copy KB element and textual context, and they do not rely on POS tagging. ----------------------------------",
  "y": "differences background"
 },
 {
  "id": "d2c95c3198f21e793549d7b16bdaf8_0",
  "x": "Entity Linking (EL), also known as Named Entity Disambiguation (NED), is one of the most important Natural Language Processing (NLP) techniques for extracting knowledge automatically from this huge amount of data. The goal of an EL approach is as follows: Given a piece of text, a reference knowledge base K and a set of entity mentions in that text, map each entity mention to the corresponding resource in K <cite>[4]</cite> . A large number of challenges has to be addressed while performing a disambiguation.",
  "y": "background"
 },
 {
  "id": "d2c95c3198f21e793549d7b16bdaf8_1",
  "x": "A portion of these approaches claim to be multilingual and most of them rely on models which are trained on English corpora with cross-lingual dictionaries. However, MAG (Multilingual AGDISTIS) <cite>[4]</cite> showed that the underlying models being trained on English corpora make them prone to failure when migrated to a different language. Additionally, these approaches hardly make their models or data available on more than three languages [6] .",
  "y": "motivation"
 },
 {
  "id": "d2c95c3198f21e793549d7b16bdaf8_2",
  "x": "Independently of the chosen graph algorithm, the highest candidate score among the set of candidates is chosen as correct disambiguation for a given mention <cite>[4]</cite> . ---------------------------------- **DEMONSTRATION**",
  "y": "background"
 },
 {
  "id": "d2c95c3198f21e793549d7b16bdaf8_3",
  "x": "The current implementation offers HITS and PageRank as algorithms, algorithm=hits or algorithm =pagerank. -Search by Context -This boolean parameter provides a search of candidates using a context index <cite>[4]</cite> . -Acronyms -This parameter enables a search by acronyms.",
  "y": "uses background"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_0",
  "x": "This architecture is a standard approach that was originally proposed in Lin et al. (2014) and was followed with slight variations by systems in the last year competition (Xue et al., 2015) . Our design most closely resembles the pipeline proposed by the top system last year<cite> (Wang and Lan, 2015)</cite> , in that argument extraction for explicit relations is performed separately for Arg1 and Arg2, the non-explicit sense classifier is run twice. The overall architecture of the system is shown in Figure 1 .",
  "y": "similarities"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_1",
  "x": "Only consecutive connectives that contain up to three tokens are addressed. The features are based on previous work (Pitler et al., 2009; Lin et al., 2014;<cite> Wang and Lan, 2015)</cite> . Our classifier is a Maximum Entropy classifier implemented with the NLTK toolkit (Bird, 2006) .",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_2",
  "x": "We employ the features proposed in Lin et al. (2014) and additional features described in last year's top system<cite> (Wang and Lan, 2015)</cite> . The position classifier is trained using the Maximum Entropy algorithm and achieves an F1 score of 99.186% on the development data. In line with prior work<cite> (Wang and Lan, 2015)</cite> , we consider PS to be the sentence that immediately precedes the connective.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_3",
  "x": "SS Argument Extractor: SS argument extractor identifies spans of Arg1 and Arg2 of explicit relations where Arg1 occurs in the same sentence, as the connective and Arg2. We follow the constituent-based approach proposed in Kong et al. (2014) , without the joint inference and enhance it using features in<cite> Wang and Lan (2015)</cite> . This component is also trained with the Maximum Entropy algorithm.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_4",
  "x": "This component is also trained with the Maximum Entropy algorithm. PS Arg1 Extractor: We implement features described in<cite> Wang and Lan (2015)</cite> and add novel features. To identify candidate constituents, we follow Kong et al. (2014) , where constituents are defined loosely based on punctuation occurring in the sentence and clause boundaries as defined by SBAR tags.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_5",
  "x": "We used the constituent split implemented in<cite> Wang and Lan (2015)</cite> . Based on earlier work <cite>(Wang and Lan, 2015</cite>; Lin et al., 2014) , we implement the following features: surface form of the verbs in the sentence (three features), last word of the current constituent (curr), last word of the previous constituent (prev), the first word of curr, and the lowercased form of the connective. The novel features that we add are shown in Table 1.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_6",
  "x": "PS Arg2 Extractor: Similar to PS Arg1 extractor, for this component we implement features described in<cite> Wang and Lan (2015)</cite> and add novel features. The novel features are the same as those introduced for PS Arg1 but also include the following additional features: \u2022 nextFirstW&puncBefore -the first word token of next and the punctuation before next.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_7",
  "x": "\u2022 C (Connective) string, C POS, prev + C, proposed in Lin et al. (2014) . \u2022 C self-category, parent-category of C, leftsibling-category of C, right-sibling-category of C, 4 C-Syn interactions, and 6 Syn-Syn interactions, introduced in Pitler et al. (2009) . \u2022 C parent-category linked context, previous connective and its POS of \"as\"(the connective and its POS of previous relation, if the connective of current relation is \"as\"), previous connective and its POS of \"when\", adopted from<cite> Wang and Lan (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_8",
  "x": "The first step in identifying non-explicit relations is the generation of sentence pairs that are candidate arguments for a non-explicit relation. Following<cite> Wang and Lan (2015)</cite>, we extract sentence pairs that satisfy the following three criteria: \u2022 Sentences are adjacent \u2022 Sentences occur within the same paragraph \u2022 Neither sentence participates in an explicit relation For all pairs of sentences that meet those criteria, we take the first sentence to be the location of Arg1, and the second sentence -the location of Arg2.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_9",
  "x": "The argument extractors for implicit relations are implemented in a way similar to explicit relation argument extraction. Candidate sentences are split into constituents based on punctuation symbols and clause boundaries using the SBAR tag. We use features in Lin et al. (2009) and<cite> Wang and Lan (2015)</cite> and augment these with novel features.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_10",
  "x": "Overall, we generate seven features that use dependency relations. Implicit Arg2 Extractor: We use most of the features in Lin et al. (2014) and<cite> Wang and Lan (2015)</cite> to train the Arg2 extractor (for more details and explanation about the features, we refer the reader to the respective papers): \u2022 Lowercased and lemmatized verbs in curr",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_12",
  "x": "**MODEL** Base features refer to features used in<cite> Wang and Lan (2015)</cite> . sense classification indicated that Averaged Perceptron should be preferred for these sub-tasks.",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_13",
  "x": "Explicit Sense Classifier: Table 2 evaluates the explicit sense classifier. We compare our baseline model that implements the features proposed in<cite> Wang and Lan (2015)</cite> with the model that employs additional features introduced in 4.4. Our baseline model performs slightly better than the one reported in<cite> Wang and Lan (2015)</cite> : we obtain 90.55 vs. 90.14, as reported in<cite> Wang and Lan (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_14",
  "x": "Our baseline model performs slightly better than the one reported in<cite> Wang and Lan (2015)</cite> : we obtain 90.55 vs. 90.14, as reported in<cite> Wang and Lan (2015)</cite> . Table 6 : Evaluation of each component on the development set (no EP). duced.",
  "y": "differences"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_15",
  "x": "Table 6 : Evaluation of each component on the development set (no EP). duced. We implement the features in<cite> Wang and Lan (2015)</cite> and add our novel features shown in Table 1 .",
  "y": "uses"
 },
 {
  "id": "d2ce392240108203377d8e51e89d09_16",
  "x": "Adding new features improves the results by three points. We note that in<cite> Wang and Lan (2015)</cite> the numbers that correspond to the entire sentence baselines are not the same as those that we obtain, so we do not report a direct comparison with their models. However, our base models implement the features they use.",
  "y": "differences"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_0",
  "x": "Natural language texts are, more often than not, a result of a deliberate cognitive effort of an author and as such consist of semantically coherent segments. Text segmentation deals with automatically breaking down the structure of text into such topically contiguous segments, i.e., it aims to identify the points of topic shift (Hearst 1994; Choi 2000; Brants, Chen, and Tsochantaridis 2002; Riedl and Biemann 2012; Du, Buntine, and Johnson 2013; Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> . Reliable segmentation results with texts that are more readable for humans, but also facilitates downstream tasks like automated text summarization (Angheluta, De Busser, and Moens 2002; Bokaei, Sameti, and Liu 2016) , passage retrieval (Huang et al. 2003; Shtekh et al. 2018) , topical classification (Zirn et al. 2016) , or dialog modeling (Manuvinakurike et al. 2016; Zhao and Kawahara 2017) .",
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_1",
  "x": "Similarly, a recently proposed state-of-the-art supervised neural segmentation model<cite> (Koshorek et al. 2018</cite> ) directly learns to predict binary sentence-level segmentation decisions and has no explicit mechanism for modeling coherence. In this work, in contrast, we propose a supervised neural model for text segmentation that explicitly takes coherence into account: we augment the segmentation prediction objective with an auxiliary coherence modeling objective. Our proposed model, dubbed Coherence-Aware Text Segmentation (CATS), encodes a sentence sequence using two hierarchically connected Transformer networks (Vaswani et al. 2017; Devlin et al. 2018 ).",
  "y": "differences background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_2",
  "x": "Similar to<cite> (Koshorek et al. 2018)</cite> , CATS' main learning objective is a binary sentence-level segmentation prediction. However, CATS augments the segmentation objective with an auxiliary coherence-based objec-tive which pushes the model to predict higher coherence for original text snippets than for corrupt (i.e., fake) sentence sequences. We empirically show (1) that even without the auxiliary coherence objective, the Two-Level Transformer model for Text Segmentation (TLT-TS) yields state-of-the-art performance across multiple benchmarks, (2) that the full CATS model, with the auxiliary coherence modeling, further significantly improves the segmentation, and (3) that both TLT-TS and CATS are robust in domain transfer.",
  "y": "similarities differences"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_3",
  "x": [
   "Koshorek et al. (2018) leveraged the manual structuring of Wikipedia pages into sections to automatically create a large segmentation-annotated corpus. WIKI-727K consists of 727,746 documents created from English (EN) Wikipedia pages, divided into training (80%), development (10%), and test portions (10%). We train, optimize, and evaluate our models on respective portions of the WIKI-727K dataset."
  ],
  "y": "uses background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_4",
  "x": [
   "Koshorek et al. (2018) additionally created a small evaluation set WIKI-50 to allow for comparative evaluation against unsupervised segmentation models, e.g., the GRAPHSEG model of Glava\u0161, Nanni, and Ponzetto (2016) , for which evaluation on large datasets is prohibitively slow. For years, the synthetic dataset of Choi (2000) was used as a standard becnhmark for text segmentation models. CHOI dataset contains 920 documents, each of which is a concatenation of 10 paragraphs randomly sampled from the Brown corpus."
  ],
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_5",
  "x": "Other Languages. In order to test the performance of our Transformer-based models in zero-shot language transfer setup, we prepared small evaluation datasets in other languages. Analogous to the WIKI-50 dataset created by<cite> Koshorek et al. (2018)</cite> from English (EN) Wikipedia, we created WIKI-50-CS, WIKI-50-FI, and WIKI-50-TR datasets consisting of 50 randomly selected pages from Czech (CS), Finnish (FI), and Turkish (TR) Wikipedia, respectively.",
  "y": "similarities extends"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_6",
  "x": "P k score is the probability that a model makes a wrong prediction as to whether the first and last sentence of a randomly sampled snippet of k sentences belong to the same segment (i.e., the probability of the model predicting the same segment for the sentences from different segment or different segments for the sentences from the same segment). Following (Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we set k to the half of the average ground truth segment size of the dataset. Baseline Models.",
  "y": "uses"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_7",
  "x": "Following previous work (Riedl and Biemann 2012; Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we also adopt the standard text segmentation measure P k (Beeferman, Berger, and Lafferty 1999) as our evaluation metric. P k score is the probability that a model makes a wrong prediction as to whether the first and last sentence of a randomly sampled snippet of k sentences belong to the same segment (i.e., the probability of the model predicting the same segment for the sentences from different segment or different segments for the sentences from the same segment). Following (Glava\u0161, Nanni, and Ponzetto 2016; <cite>Koshorek et al. 2018)</cite> , we set k to the half of the average ground truth segment size of the dataset.",
  "y": "uses"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_8",
  "x": "We compare CATS against the state-ofthe-art neural segmentation model of<cite> Koshorek et al. (2018)</cite> and against GRAPHSEG (Glava\u0161, Nanni, and Ponzetto 2016) , the state-of-the-art unsupervised text segmentation model. Additionally, as a sanity check, we evaluate the RANDOM baseline -it assigns a positive segmentation label to a sentence with the probability that corresponds to the ratio of the total number of segments (according to the gold segmentation) and total number of sentences in the dataset. ----------------------------------",
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_10",
  "x": "The improved performance that TLT-TS has with respect to the model of<cite> Koshorek et al. (2018)</cite> is consistent with improvements that Transformer-based architectures yield in comparison with models based on recurrent components in other NLP tasks (Vaswani et al. 2017; Devlin et al. 2018) . The gap in performance is particularly wide (>20 P k points) for the EL-EMENTS dataset. Evaluation on the ELEMENTS test set is, arguably, closest to a true domain-transfer setting: 12 while the train portion of the WIKI-727K set contains pages similar in type to those found in WIKI-50 and CITIES test sets, it does not contain any Wikipedia pages about chemical elements (all such pages are in the ELEMENTS test set).",
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_12",
  "x": "This empirically confirms the usefulness of explicit coherence modeling for text segmentation. Moreover,<cite> Koshorek et al. (2018)</cite> report human performance on the WIKI-50 dataset of 14.97, which is a mere one P k point better than the performance of our coherence-aware CATS model. The unsupervised GRAPHSEG model of Glava\u0161, Nanni, and Ponzetto (2016) seems to outperform all supervised models on the synthetic CHOI dataset.",
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_13",
  "x": [
   "Text segmentation tasks come in two main flavors: (1) linear (i.e., sequential) text segmentation and (2) hierarchical segmentation in which top-level segments are further broken down into sub-segments. While the hierarchical segmentation received a non-negligible research attention (Yaari 1997; Eisenstein 2009; Du, Buntine, and John-son 2013) , the vast majority of the proposed models (including this work) focus on linear segmentation (Hearst 1994; Beeferman, Berger, and Lafferty 1999; Choi 2000; Brants, Chen, and Tsochantaridis 2002; Misra et al. 2009; Riedl and Biemann 2012; Glava\u0161, Nanni, and Ponzetto 2016; Koshorek et al. 2018, inter alia) . In one of the pioneering segmentation efforts, Hearst (1994) proposed an unsupervised TextTiling algorithm based on the lexical overlap between adjacent sentences and paragraphs."
  ],
  "y": "background"
 },
 {
  "id": "d3672a2d7129beef6703598f1558c4_14",
  "x": "Finally,<cite> Koshorek et al. (2018)</cite> identify Wikipedia as a free large-scale source of manually segmented texts that can be used to train a supervised segmentation model. They train a neural model that hierarchically combines two bidirectional LSTM networks and report massive improvements over unsupervised segmentation on a range of evaluation datasets. The model we presented in this work has a similar hierarchical architecture, but uses Transfomer networks instead of recurrent encoders. Crucially, CATS additionally defines an auxiliary coherence objective, which is coupled with the (primary) segmentation objective in a multi-task learning model.",
  "y": "differences background motivation"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_0",
  "x": "An interesting property of word vectors learned by the log-linear model is that the relations among relevant words seem linear and can be computed by simple vector addition and substraction (Mikolov et al., 2013d) . For example, the following relation approximately holds in the word vector space: ParisFrance + Rome = Italy. In<cite> (Mikolov et al., 2013b)</cite> , the linear relation is extended to the bilingual scenario, where a linear transform is learned to project semantically identical words from one language to another.",
  "y": "background"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_1",
  "x": "**RELATED WORK** This work largely follows the methodology and experimental settings of<cite> (Mikolov et al., 2013b)</cite> , while we normalize the embedding and use an orthogonal transform to conduct bilingual translation. Multilingual learning can be categorized into projection-based approaches and regularizationbased approaches.",
  "y": "uses similarities"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_2",
  "x": "Our method in this paper and the linear projection method in <cite>(Mikolov et al., 2013b</cite> ) both belong to this category. Another interesting work proposed by (Faruqui and Dyer, 2014) learns linear transforms that project word vectors of all languages to a common low-dimensional space, where the correlation of the multilingual word pairs is maximized with the canonical correlation analysis (CCA). The regularization-based approaches involve the multilingual constraint in the objective function for learning the embedding.",
  "y": "similarities"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_3",
  "x": "**ORTHOGONAL TRANSFORM** The bilingual word translation provided by <cite>(Mikolov et al., 2013b</cite> ) learns a linear transform from the source language to the target language by the linear regression. The objective function is as follows:",
  "y": "background"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_4",
  "x": "where W is the projection matrix to be learned, and x i and z i are word vectors in the source and target language respectively. The bilingual pair (x i , z i ) indicates that x i and z i are identical in semantic meaning. A high accuracy was reported on a word translation task, where a word projected to the vector space of the target language is expected to be as close as possible to its translation<cite> (Mikolov et al., 2013b)</cite> .",
  "y": "background"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_5",
  "x": "The monolingual word embedding is conducted with the data published by the EMNLP 2011 SMT workshop (WMT11) 2 . For an easy comparison, we largely follow Mikolov's settings in<cite> (Mikolov et al., 2013b)</cite> and set English and Spanish as the source and target language, respectively. The data preparation involves the following steps.",
  "y": "uses similarities"
 },
 {
  "id": "d3f5f9b1ef8bda3d33c563d252d58a_6",
  "x": "As can be seen, the best dimension setting is 800 for English and 200 for Spanish, and the corresponding P@1 and P@5 are 35.36% and 53.96%, respectively. These results are comparable with the results reported in <cite>(Mikolov et al., 2013b</cite> ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_0",
  "x": "Given a word f , its distributional profile is: V is the vocabulary and the surrounding words w i are taken from a monolingual corpus using a fixed window size. We use a window size of 4 words based on the experiments in<cite> (Razmara et al., 2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_1",
  "x": "DPs need an association measure A(\u00b7, \u00b7) to compute distances between potential paraphrases. A comparison of different association measures appears in (Marton et al., 2009; <cite>Razmara et al., 2013</cite>; Saluja et al., 2014) and our preliminary experiments validated the choice of the same association measure as in these papers, namely Pointwise Mutual Information (Lin, 1998) (PMI) . For each potential context word w i :",
  "y": "similarities"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_2",
  "x": "(2) w i 's are the words that appear in the context of f 1 or f 2 , otherwise the PMI values would be zero. Considering all possible candidate paraphrases is very expensive. Thus, we use the heuristic applied in previous works (Marton et al., 2009; <cite>Razmara et al., 2013</cite>; Saluja et al., 2014) to reduce the search space.",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_3",
  "x": "(1): 1) A graph of source phrases is constructed as in<cite> (Razmara et al., 2013)</cite> ; 2) translations are propagated as labels through the graph as explained in Fig. 2 ; and 3) new translation rules obtained from graph-propagation are integrated with the original phrase table. ---------------------------------- **GRAPH CONSTRUCTION**",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_4",
  "x": "We describe in Sec. 4.2.2 the reasons for choosing MAD over other graph propagation algorithms. The MAD graph propagation generalizes the approach used in<cite> (Razmara et al., 2013)</cite> . The Structured Label Propagation algorithm (SLP) was used in (Saluja et al., 2014; Zhao et al., 2015) which uses a graph structure on the target side phrases as well.",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_5",
  "x": "They pre-structure the graph into bipartite graphs (only connections between phrases with known translation and OOV phrases) and tripartite graphs (connections can also go from a known phrasal node to an OOV phrasal node through one node that is a paraphrase of both but does not have translations, i.e. it is an unlabeled node). In these pre-structured graphs there are no connections between nodes of the same type (known, OOV or unlabeled). We apply this method in our low resource setting experiments (Sec. 5.3) to compare our bipartite and tripartite results to<cite> Razmara et al. (2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_6",
  "x": "We use CDEC 1 (Dyer et al., 2010) as an endto-end SMT pipeline with its standard features 2 . fast align (Dyer et al., 2013 ) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA (Crammer and Singer, 2003) . This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec. 5.5). But as we wish to fairly compare our approach with<cite> Razmara et al. (2013)</cite> on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och, 2003) for tuning.",
  "y": "uses"
 },
 {
  "id": "d51bf6d22d21dcd91e080f6f0b5dcb_7",
  "x": "---------------------------------- **CASE 1: LIMITED PARALLEL DATA** In this experiment we use a setup similar to (Razmara et al., 2013 we use 10K French-English parallel sentences, randomly chosen from Europarl to train translation system, as reported in<cite> (Razmara et al., 2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_0",
  "x": "This kind of statistical stemming has been shown to be effective for many languages, including English, Turkish, and Malay. For example, Bhat introduced a method for Kannada where the similarity of two words is determined by three distance measures based on prefix and suffix matching and the first mismatch point in the words [2] . Defining precise rules for morphologically complex texts, especially for the purpose of infix removal is sometimes impossible<cite> [5]</cite> .",
  "y": "motivation"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_1",
  "x": "Therefore, in dictionary-based CLIR, retrieval systems are obliged either to stem documents and queries, or to leave them intact [8, 4, 12] , or expand the query with inflections. We opted the query expansion approach which is a widely used approach to compensate the shortage of inflections [9, 3, <cite>5]</cite> . We used the following probabilistic framework to this end<cite> [5]</cite> :",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_2",
  "x": "We opted the query expansion approach which is a widely used approach to compensate the shortage of inflections [9, 3, <cite>5]</cite> . We used the following probabilistic framework to this end<cite> [5]</cite> : where q i is a query term and c i is the set of translation candidates provided in a bilingual dictionary for q i .",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_3",
  "x": "c i is the set of the most probable inflections of the words appeared in c i selected by a tuned threshold. Then, we compute the translation probability of c i,j or c i,j for the given q i . To avoid adding noisy terms, we only compute the joint probabilities between either a pair of translation candidates from the dictionary (c i,j and c i ,j ) or a pair of a candidate from the dictionary and an inflection from the collection (c i,j and c i ,j )<cite> [5]</cite> .",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_4",
  "x": "Queries are expanded by the top 50 terms generated by the feedback model [14, 6] . We removed Persian stop words from the queries and documents [4, <cite>5]</cite> . We used STeP1 [13] in our stemming process in Persian.",
  "y": "uses"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_5",
  "x": "We use Google EnglishPersian dictionary 1 as the translation resource. Dadashkarimi et al., demonstrated that Google has better coverage compared to other English-Persian dictionaries<cite> [5]</cite> . We have exploited 40 Persian POS tags in our experiments.",
  "y": "background"
 },
 {
  "id": "d52dfb30158deae64a1c3d787d9b95_6",
  "x": "To this end we compare the proposed SS4MCT with a number of dictionary-based CLIR methods; the 5-gram truncation method (SPLIT) proposed in [11] , rule-based query expansion (RBQE) based on inflectional/derivation rules from Farazzin machine translator 3 , and the STeP1 stemmer [13] are the morphological processing approaches for the retrieval system. On the other hand, we run another set of experiments without applying any morphological processing method similar to the Persian state-of-the-art CLIR methods. Iterative translation disambiguation (ITD) [11] , joint cross-lingual topical relevance model (JCLTRLM) [7] , top-ranked translation (TOP-1), and the bi-gram coherence translation method (BiCTM), introduced in<cite> [5]</cite> (assume |c i | = 0), are the baselines without any morphological processing units.",
  "y": "uses"
 },
 {
  "id": "d5a71358168d262dd1e9734c80234b_0",
  "x": "For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable?",
  "y": "background"
 },
 {
  "id": "d5a71358168d262dd1e9734c80234b_1",
  "x": "The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004), English <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) , Czech (Sgall et al., 2004) and German (Baumann et al., 2004). The sixth, seventh and eighth papers (Baumann et al., 2004; \u00c7 mejrek et al., 2004; Helmreich et al., 2004 ) explore questions of multilingual annotation of syntax and semantics, beginning to answer the question of how annotation systems can be made compatible across languages. Indeed (Helmreich et al., 2004) explores the question of integration across languages, as well as levels of annotation.",
  "y": "background"
 },
 {
  "id": "d5a71358168d262dd1e9734c80234b_2",
  "x": "For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) . It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable?",
  "y": "background"
 },
 {
  "id": "d5a71358168d262dd1e9734c80234b_3",
  "x": "I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004) , English <cite>(Miltsakaki et al., 2004</cite>; Babko-Malaya et al., 2004; Meyers et al., 2004) , Czech (Sgall et al., 2004) and German (Baumann et al., 2004) . The sixth, seventh and eighth papers (Baumann et al., 2004; \u00c7 mejrek et al., 2004; Helmreich et al., 2004) explore questions of multilingual annotation of syntax and semantics, beginning to answer the question of how annotation systems can be made compatible across languages.",
  "y": "background"
 },
 {
  "id": "d61f75366022f043d4c3a005b5a73d_0",
  "x": "The morphology-driven models allow pooling evidence from different words which have the same base form. These models work by learning per-morpheme representations rather than just per-word ones, and compose the representing vector of each word from those of its morphemes -as derived from a supervised or unsupervised morphological analysis -and (optionally) its surface form (e.g. walking = f (v walk , v ing , v walking )). The works differ in the way they acquire morphological knowledge (from using linguistically derived morphological analyzers on one end, to approximating morphology using substrings while relying on the concatenative nature of morphology, on the other) and in the model form (cDSMs (Lazaridou et al., 2013) , RNN (Luong et al., 2013) , LBL (Botha and Blunsom, 2014) , CBOW (Qiu et al., 2014) , SkipGram (Soricut and Och, 2015; <cite>Bojanowski et al., 2016)</cite> , GGM (Cotterell et al., 2016) ).",
  "y": "background"
 },
 {
  "id": "d61f75366022f043d4c3a005b5a73d_1",
  "x": "Bojanowski et al (2016) replace the word representation v wt with the set of character ngrams appearing in it: v wt = g\u2208G(wt) v g where G(w t ) is the set of n-grams appearing in w t . The n-grams are used to approximate the morphemes in the target word. We generalize<cite> Bojanowski et al (2016)</cite> by replacing the set of ngrams G(w) with a set P(w) of explicit linguistic properties.",
  "y": "extends"
 },
 {
  "id": "d61f75366022f043d4c3a005b5a73d_2",
  "x": "**MODELS** Our model form is a generalization of the fastText model<cite> (Bojanowski et al., 2016)</cite> , which in turn extends the skip-gram model of Mikolov et al (2013) . The skip-gram model takes a sequence of words w 1 , ..., w T and a function s assigning scores to (word, context) pairs, and maximizes",
  "y": "similarities extends"
 },
 {
  "id": "d61f75366022f043d4c3a005b5a73d_3",
  "x": "Our implementation is based on the fastText 2 library<cite> (Bojanowski et al., 2016)</cite> , which we modify as described above. We train the models on the Hebrew Wikipedia (\u223c4M sentences), using a window size of 2 to each side of the focus word, and dimensionality of 200. We use the morphological disambiguator of Adler (2007) to assign words with their morphological tags, and the inflection dictionary of MILA (Itai and Wintner, 2008) Semantic Evaluation Measure The common datasets for semantic similarity 4 have some notable shortcomings as noted in (Avraham and Goldberg, 2016; Faruqui et al., 2016; Batchkarov et al., 2016; Linzen, 2016) .",
  "y": "extends"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_0",
  "x": "Recent trends in speech recognition [7, <cite>8,</cite> 9] have demonstrated impressive performance on Switchboard and Fisher data. Deep neural network (DNN) based acoustic modeling has become the state-of-the-art in automatic speech recognition (ASR) systems [10, 11] . It has demonstrated impressive performance gains for almost all tried languages and ___________________________________________________________ *The author performed this work while at SRI International and is currently working at Apple Inc. acoustic conditions.",
  "y": "background"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_1",
  "x": "Deep neural network (DNN) based acoustic modeling has become the state-of-the-art in automatic speech recognition (ASR) systems [10, 11] . It has demonstrated impressive performance gains for almost all tried languages and ___________________________________________________________ *The author performed this work while at SRI International and is currently working at Apple Inc. acoustic conditions. Advanced variants of DNNs, such as convolutional neural nets (CNNs) [12] , recurrent neural nets (RNNs) [13] , long short-term memory nets (LSTMs) [14] , time-delay neural nets (TDNNs) [15, 29] , <cite>VGG-nets</cite> <cite>[8]</cite> , have significantly improved recognition performance, bringing them closer to human performance [9] .",
  "y": "background"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_2",
  "x": "These findings indicate that the use of better acoustic features can help improve speech recognition performance when using standard acoustic modeling techniques, and can demonstrate performance as good as those obtained from more sophisticated acoustic models that exploit temporal memory. For the sake of simplicity, we used a CNN acoustic model in our experiment, where the baseline system's performance is directly comparable to the state-of-the-art CNN performance reported in <cite>[8]</cite> . We expect our results using the CNN to carry over into other neural network architectures as well.",
  "y": "uses"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_3",
  "x": "Table 1 demonstrates that the fMLLR transformed features always performed better than the features without fMLLR transform. Also, the CNN models always gave better results, confirming similar observations from studies reported earlier <cite>[8]</cite> . Also, note that Table 1 shows that the DOC features performed slightly better than the MFB features after the fMLLR transform, where the performance improvement was more pronounced for the CH subset of the NIST 2000 CTS test set.",
  "y": "background"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_4",
  "x": "The use of articulatory features helped to reduce the error rates for both SWB and CH test sets, indicating their robustness to model spontaneous speech in both native (SWB) and non-native (CH) speaking styles. The FSH corpus contains speech from quite a diverse set of speakers, helping to reduce the WER of the CH subset more significantly than the SWB subset, a trend reflected in results reported in the literature <cite>[8]</cite> . Table 4 shows the system fusion results after dumping 2000-best lists from the rescored lattices from each individual system of different front-end features with fMLLR, i.e., MFB, DOC, MFB+DOC, MFB+DOC+TV, then conducting M-way combination of the subsystems using N-best ROVER [27] implemented in SRILM [28] .",
  "y": "similarities"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_5",
  "x": "Table 1 and 2 also demonstrates that sequence training always gave additive performance gain over crossentropy training, supporting the in <cite>[8,</cite> 21] . As a next step, we focused on training the acoustic models using the 2000-hour SWB+FSH CTS data, focusing on the CNN acoustic models and multi-view features. Note that the MFB DNN baseline model was used to generate the alignments for the FSH part of the 2000 hours CTS training set and as a consequence the number of senone labels remained the same as the 360-hour SWB models.",
  "y": "uses similarities"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_6",
  "x": [
   "The robust features and articulatory features capture complementary information, and the addition of them resulted in the best single system performance, with 12% relative reduction of WER on SWB and CH evaluation sets respectively, compared to the MFB+fMLLR CNN baseline. Note that in this study the language model has not been optimized. Future studies should investigate RNN or other neural network-based language modeling techniques that are known to perform better than word n-gram LMs."
  ],
  "y": "future_work"
 },
 {
  "id": "d72f0608fddd1bf1cdef7ca6a20bdf_7",
  "x": "Table 1 presents the word error rates (WER) from the baseline CNN model trained with the SWB data when evaluated on the NIST 2000 CTS test set, for both cross-entropy (CE) training and sequence training (ST) using MMI. Also, the CNN models always gave better results, confirming similar observations from studies reported earlier <cite>[8]</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_0",
  "x": "This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer, syntactic representations of units of bilingual language models (BiLMs). The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline, as well as over the lexicalized BiLM by <cite>Niehues et al. (2011)</cite> .",
  "y": "background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_1",
  "x": "Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous. In this paper, we base our approach to reordering on bilingual language models (Marino et al., 2006;<cite> Niehues et al., 2011)</cite> . Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process.",
  "y": "similarities background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_2",
  "x": "Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process. 1 Originally, Marino et al. (2006) used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems<cite> (Niehues et al., 2011)</cite> . We adopt and generalize the approach of <cite>Niehues et al. (2011)</cite> to investigate several variations of bilingual language models.",
  "y": "background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_3",
  "x": "1 Originally, Marino et al. (2006) used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems<cite> (Niehues et al., 2011)</cite> . We adopt and generalize the approach of <cite>Niehues et al. (2011)</cite> to investigate several variations of bilingual language models. Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties.",
  "y": "extends uses"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_4",
  "x": "Lexical information has been used by Tillmann (2004) but is known to suffer from data sparsity (Galley and Manning, 2008) . Also previous contributions to bilingual language modeling (Marino et al., 2006;<cite> Niehues et al., 2011)</cite> have mostly used lexical information, although Crego and Yvon (2010a) and Crego and Yvon (2010b) label bilingual to-kens with a rich set of POS tags. But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (Fox, 2002; Cherry, 2008) .",
  "y": "background motivation"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_5",
  "x": "Our contributions can be summarized as follows: We argue that the contextual information used in the original bilingual models<cite> (Niehues et al., 2011)</cite> is insufficient and introduce a simple model that exploits source-side syntax to improve reordering (Sections 2 and 3). We perform a thorough comparison between different variants of our general model and compare them to the original approach. We carry out translation experiments on multiple test sets, two language pairs (ArabicEnglish and Chinese-English), and with respect to two metrics (BLEU and TER).",
  "y": "motivation background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_6",
  "x": "The richer representation allows for a finer distinction between reorderings. For example, Arabic has a morphological marker of definiteness on both nouns and adjectives. If we first translate a definite adjective and then an indefinite noun, it will probably not be a likely sequence according to the translation model. This kind of intuition underlies the model of <cite>Niehues et al. (2011)</cite> , a bilingual LM (BiLM), which defines elementary translation events t 1 , ..., t n as follows: where e i is the i-th target word and A : E \u2192 P(F ) is an alignment function, E and F referring to target and source sentences, and P(\u00b7) is the powerset function. In other words, the i-th translation event consists of the i-th target word and all source words aligned to it.",
  "y": "background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_7",
  "x": "<cite>Niehues et al. (2011)</cite> refer to the defined translation events t i as bilingual tokens and we adopt this terminology. Our choice of the above definition is supported by the fact that it produces an unambiguous segmentation of a parallel sentence into tokens.",
  "y": "similarities background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_8",
  "x": "Since <cite>Niehues et al. (2011)</cite> have shown their model to work successfully as an additional feature in combination with commonly used standard phrase-based features, we use their approach as the main point of reference and base our approach on their segmentation method. In the rest of the text we refer to <cite>Niehues et al. (2011)</cite> as the original BiLM. 4 At the same time, we do not see any specific obstacles for combining our work with MTUs.",
  "y": "uses"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_10",
  "x": "<cite>Niehues et al. (2011)</cite> also described an alternative variant of the original BiLM, where words are substituted by their POS tags (Figure 2 .a, shaded part). Also, however, POS information by itself may be insufficiently expressive to separate cor- , it still is a likely sequence. Indeed, the log-probabilities of the two sequences with respect to a 4-gram BiLM model 5 result in a higher probability of \u221210.25 for the incorrect reordering than for the correct one (\u221210.39). Since fully lexicalized bilingual tokens suffer from data sparsity and POS-based bilingual tokens are insufficiently expressive, the question is which level of syntactic information strikes the right balance between expressiveness and generality.",
  "y": "background motivation"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_11",
  "x": "In this section, we introduce our model which combines the BiLM from <cite>Niehues et al. (2011)</cite> with source dependency information. We further give details on how the proposed models are trained and integrated into a phrase-based decoder. ----------------------------------",
  "y": "similarities background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_13",
  "x": "ture performs as compared to the standard PB-SMT baseline and, more importantly, to the original BiLM model. We consider two variants of BiLM discussed by <cite>Niehues et al. (2011)</cite> : the standard one, Lex\u2022Lex, and the simplest syntactic one, Pos\u2022Pos. Results for the experiments can be found in Table 2 .",
  "y": "background"
 },
 {
  "id": "d8e73e9c00acffc34ade1331709d92_14",
  "x": "In this paper, we have introduced a simple, yet effective way to include syntactic information into phrase-based SMT. Our method consists of enriching the representation of units of a bilingual language model (BiLM). We argued that the very limited contextual information used in the original bilingual models<cite> (Niehues et al., 2011)</cite> can capture reorderings only to a limited degree and proposed a method to incorporate information from a source dependency tree in bilingual units.",
  "y": "motivation background"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_0",
  "x": "Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (Hill et al., 2016; McCann et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018 ). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings <cite>(C\u00edfka and Bojar, 2018)</cite> . However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear.",
  "y": "background"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_1",
  "x": "Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (Hill et al., 2016; McCann et al., 2017; Schwenk and Douze, 2017; Subramanian et al., 2018 ). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings <cite>(C\u00edfka and Bojar, 2018)</cite> . However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear.",
  "y": "motivation"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_2",
  "x": "However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear. Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (Schwenk, 2018) , on translation quality (Lu et al., 2018) , on speed comparison (Britz et al., 2017) , or only exploring a bilingual scenario <cite>(C\u00edfka and Bojar, 2018)</cite> . In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model.",
  "y": "background"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_3",
  "x": "We examine this model with a systematic evaluation on different sizes of the attention bridge and extensive experiments to study the abstractions it learns from multiple translation tasks. In contrast to previous work <cite>(C\u00edfka and Bojar, 2018)</cite> , we demonstrate that there is a correlation between translation performance and trainable downstream tasks when adjusting the size of the intermediate layer. The trend is different for non-trainable tasks that benefit from the increased compression that denser representations achieve, which typically hurts the translation performance because of the decreased capacity of the model.",
  "y": "differences"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_4",
  "x": "The overall architecture is illustrated in Figure 1 (see also V\u00e1zquez et al., 2019) . Due to the attentive connection between encoders and decoders we call this layer attention bridge, and its architecture is an adaptation from the model proposed by<cite> C\u00edfka and Bojar (2018)</cite> . Finally, each decoder follows a common attention mechanism in NMT, with the only exception that the context vector is computed on the attention bridge, and the initialization is performed by a mean pooling over it.",
  "y": "uses"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_5",
  "x": "4 SentEval: Classification tasks Table 1 shows the performance of our models on two popular tasks (SNLI and SICK-E) as in<cite> C\u00edfka and Bojar (2018)</cite> as well as the average of all 10 SentEval downstream tasks. The experiments reveal two important findings: (1) In contrast with the results from<cite> C\u00edfka and Bojar (2018)</cite>, our scores demonstrate that an increasing number of attention heads is beneficial for classification-based downstream tasks.",
  "y": "similarities"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_6",
  "x": "4 SentEval: Classification tasks Table 1 shows the performance of our models on two popular tasks (SNLI and SICK-E) as in<cite> C\u00edfka and Bojar (2018)</cite> as well as the average of all 10 SentEval downstream tasks. The experiments reveal two important findings: (1) In contrast with the results from<cite> C\u00edfka and Bojar (2018)</cite>, our scores demonstrate that an increasing number of attention heads is beneficial for classification-based downstream tasks.",
  "y": "differences"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_7",
  "x": "Results with \u2020 taken from<cite> C\u00edfka and Bojar (2018).</cite> of multilingual training. We can see that multilingual training objectives are generally helpful for the trainable downstream tasks.",
  "y": "uses"
 },
 {
  "id": "d92e92b9a375914f3dd74868f463fc_8",
  "x": "This is in line with the findings of<cite> C\u00edfka and Bojar (2018)</cite> and could also be expected as the model is more strongly pushed into a dense semantic abstraction that is beneficial for measuring similarities without further training. More surprising is the negative effect of the multilingual models. We believe that the multilingual information encoded jointly in the attention bridge hampers the results for the monolingual semantic similarity measured with the cosine distance, while it becomes easier in a bilingual scenario where the vector encodes only one source language, English in this case.",
  "y": "similarities"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_0",
  "x": "**INTRODUCTION** A number of approaches for question answering have been proposed recently that use reinforcement learning to reason over a knowledge graph<cite> (Das et al., 2018</cite>; Lin et al., 2018; Chen et al., 2018; Zhang et al., 2018) . In these methods the input question is first parsed into a constituent question entity and relation.",
  "y": "background"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_1",
  "x": "---------------------------------- **RELATED WORK** The closest works to ours are the works by Lin et al. (2018) , Zhang et al. (2018) and <cite>Das et al. (2018)</cite> , which consider the question answering task in a reinforcement learning setting in which the agent always chooses to answer.",
  "y": "similarities"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_2",
  "x": "We base our work on the recent reinforcement learning approaches introduced in <cite>Das et al. (2018)</cite> and Lin et al. (2018) . We denote the knowledge graph as G, the set of entities as E, the set of relations as R and the set of directed edges L between entities of the form l = (e 1 , r, e 2 ) with e 1 , e 2 \u2208 E and r \u2208 R. The goal is to find an answer entity e a given a question entity e q and the question relation r q , when (e q , r q , e a ) is not part of graph G. We formulate this problem as a Markov Decision Problem (MDP) (Sutton and Barto, 1998) with the following states, actions, transition function and rewards:",
  "y": "uses"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_3",
  "x": "The agent is rewarded based on the final state. For example, in <cite>Das et al. (2018)</cite> and Lin et al. (2018) the agent obtains a reward of 1 if the correct answer entity is reached as the final state and 0 otherwise (i.e., R(s T ) = I{e T = e a }). Figure 2a illustrates the LSTM which encodes history of the path taken.",
  "y": "background"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_4",
  "x": "This is illustrated in Figure 3 . In the framework of <cite>Das et al. (2018)</cite> a binary reward is used which rewards the learner for the answer being wrong or correct. Following a similar protocol, we could award a score of 1 to return 'no answer' when there is no answer available in the KG.",
  "y": "background"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_5",
  "x": "Both the public dataset and the proprietary dataset are <cite>Das et al. (2018)</cite> , using the same train/val/test splits for FB15k-237. We extend the publicly available implementation of <cite>Das et al. (2018)</cite> for our experimentation.",
  "y": "similarities uses"
 },
 {
  "id": "da8f30113f1126a78cefed06a15076_6",
  "x": "Unlike <cite>Das et al. (2018)</cite> , we also train entity embeddings after initializing them with random values. This resulted in the final QA Score of 47.58%, around 8% higher than standard RL and 12% higher than <cite>Das et al. (2018)</cite> . The final QA Score also increased from 28.72% to 39.55%, and also significantly improved over <cite>Das et al. (2018)</cite> and Lin et al. (2018) .",
  "y": "extends differences"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_0",
  "x": "We start from a baseline joint model that performs the tasks of named entity recognition (NER) and relation extraction at once. Previously proposed models (summarized in Section 2) exhibit several issues that the neural network-based baseline approach (detailed in Section 3.1) overcomes: (i) our model uses automatically extracted features without the need of external parsers nor manually extracted features (see Gupta et al. (2016) ; Miwa and Bansal (2016) ; Li et al. (2017) ), (ii) all entities and the corresponding relations within the sentence are extracted at once, instead of examining one pair of entities at a time (see Adel and Sch\u00fctze (2017) ), and (iii) we model relation extraction in a multi-label setting, allowing multiple relations per entity (see Katiyar and Cardie (2017) ; <cite>Bekoulis et al. (2018a)</cite> ). The core contribution of the paper is the use of AT as an extension in the training procedure for the joint extraction task (Section 3.2).",
  "y": "differences"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_1",
  "x": "Katiyar and Cardie (2017) investigate RNNs with attention without taking into account that relation labels are not mutually exclusive. Finally, <cite>Bekoulis et al. (2018a)</cite> use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part. Our baseline model enables simultaneous extraction of multiple relations from the same input.",
  "y": "background"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_2",
  "x": "Finally, <cite>Bekoulis et al. (2018a)</cite> use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part. Our baseline model enables simultaneous extraction of multiple relations from the same input. Then, we further extend this strong baseline using adversarial training.",
  "y": "differences"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_3",
  "x": "Finally, <cite>Bekoulis et al. (2018a)</cite> use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part. Our baseline model enables simultaneous extraction of multiple relations from the same input. Then, we further extend this strong baseline using adversarial training.",
  "y": "extends differences"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_4",
  "x": "We also evaluate our models on the NER task similar to Miwa and Sasaki (2014) in the same dataset using 10-fold cross validation. For the Dutch Real Estate Classifieds, DREC (Bekoulis et al., 2017) dataset, we use train-test splits as in <cite>Bekoulis et al. (2018a)</cite> . For the Adverse Drug Events, ADE (Gurulingappa et al., 2012) , we perform 10-fold cross-validation similar to Li et al. (2017) .",
  "y": "uses"
 },
 {
  "id": "db6c35071fe4e93c11acca4056e9ac_5",
  "x": "In the boundaries evaluation, the baseline has an improvement of \u223c3% on both tasks compared to <cite>Bekoulis et al. (2018a)</cite> , whose quadratic scoring layer complicates NER. Table 1 and Fig. 2 show the effectiveness of the adversarial training on top of the baseline model. In all of the experiments, AT improves the predictive performance of the baseline model in the joint setting.",
  "y": "differences"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_0",
  "x": "Much research has therefore focused on compositionality prediction of MWEs, primarily at the type level. One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; <cite>Salehi et al., 2015</cite>) . The hypothesis behind <cite>this line of work</cite> is that the representation of a compositional MWE will be more similar to the representations of its component words than the representation of a non-compositional MWE will be to those of its component words.",
  "y": "background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_1",
  "x": "Much research has therefore focused on compositionality prediction of MWEs, primarily at the type level. One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; <cite>Salehi et al., 2015</cite>) . The hypothesis behind <cite>this line of work</cite> is that the representation of a compositional MWE will be more similar to the representations of its component words than the representation of a non-compositional MWE will be to those of its component words.",
  "y": "background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_2",
  "x": "One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; <cite>Salehi et al., 2015</cite>) . One issue faced by <cite>such approaches</cite> is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them. Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012) .",
  "y": "background motivation"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_3",
  "x": "One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; <cite>Salehi et al., 2015</cite>) . One issue faced by <cite>such approaches</cite> is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them. Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012) . In this paper we consider whether character-level neural network language models capture knowledge of MWE compositionality.",
  "y": "motivation"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_4",
  "x": "These character sequences are fed to the neural network language model, and the hidden state of the neural network at the end of the sequence is taken as the vector representation for that sequence. 2 Once vector representations of an MWE and its component words are obtained, following <cite>Salehi et al. (2015)</cite> , the following equations are then used to compute the compositionality of an MWE: where MWE is the vector representation of the MWE, and C 1 and C 2 are vector representations for the first and second components of the MWE, respectively.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_5",
  "x": "**TRAINING CORPUS** We train language models over a portion of English and German Wikipedia dumps -following <cite>Salehi et al. (2015)</cite> -from 20 January 2018. The raw dumps are preprocessed using WP2TXT 6 to remove wikimarkup, metadata, and XML and HTML tags.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_6",
  "x": "The proposed model is evaluated over the same three datasets as <cite>Salehi et al. (2015)</cite> , <cite>which</cite> cover two languages (English and German) and two kinds of MWEs (noun compounds and verb-particle constructions). ENC This dataset contains 90 English noun compounds (e.g., game plan, gravy train) which are annotated on a scale of [0, 5] for both their overall compositionality, and the compositionality of each of their component words (Reddy et al., 2011) . (Mikolov et al., 2013) , are also shown.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_7",
  "x": "(Mikolov et al., 2013) , are also shown. EVPC This dataset consists of 160 English verb-particle constructions (e.g., add up, figure out) which are rated on a binary scale for the compositionality of each of the verb and particle component words (Bannard, 2006) by multiple annotators; no ratings for the overall compositionality of MWEs are provided in this dataset. The binary compositionality judgements are converted to continuous values as in <cite>Salehi et al. (2015)</cite> by dividing the number of judgements that an expression is compositional by the total number of judgements.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_8",
  "x": "We evaluate our proposed approach following <cite>Salehi et al. (2015)</cite> by computing Pearson's correlation between the predicted compositionality (i.e., from either comp 1 or comp 2 ) and human ratings for overall compositionality. For EVPC, no overall compositionality ratings are provided. In this case we report the correlation between the predicted compositionality scores and both the verb and particle compositionality judgements.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_9",
  "x": "We begin by considering results using the default settings (described in section \u00a73.1) using both comp 1 and comp 2 . For comp 1 , we set \u03b1 to 0.7 for ENC and GNC following <cite>Salehi et al. (2015)</cite> ; for EVPC we set \u03b1 to 0.5. Results are shown in table 2.",
  "y": "uses"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_10",
  "x": "These correlations are well below those of previous work. For example, using comp 1 with representations of the MWE and component words obtained from word2vec (Mikolov et al., 2013) , <cite>Salehi et al. (2015)</cite> achieve correlations of 0.717, 0.289, and 0.400 for ENC, the verb component of EVPC, and GNC, respectively. 8 Nevertheless, the results in table 2, and in particular the significant correlations for ENC and the particle component of EVPC, indicate that character-level neural network language models do capture some information about the compositionality of MWEs, at least for certain types of expressions.",
  "y": "background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_11",
  "x": "However, for GNC, and the verb component of EVPC, neither approach to predicting compositionality gives significant correlations. These correlations are well below those of previous work. For example, using comp 1 with representations of the MWE and component words obtained from word2vec (Mikolov et al., 2013) , <cite>Salehi et al. (2015)</cite> achieve correlations of 0.717, 0.289, and 0.400 for ENC, the verb component of EVPC, and GNC, respectively.",
  "y": "differences"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_12",
  "x": "For the compositionality of the particle component, comp 1 and comp 2 achieve correlations of 0.327 and 0.308, respectively. These correlations are significant (p < 0.05). Word embedding models -such as that used in the approach to predicting compositionality of <cite>Salehi et al. (2015)</cite> -typically do not learn representations for low frequency items.",
  "y": "background"
 },
 {
  "id": "dcf84cf05e3e7950cabbdd8d8f304c_13",
  "x": "These correlations are significant (p < 0.05). Word embedding models -such as that used in the approach to predicting compositionality of <cite>Salehi et al. (2015)</cite> -typically do not learn representations for low frequency items. 9 These results demonstrate that the proposed model is able to predict the compositionality for low frequency items, that would not typically be in-vocabulary for word embedding models, and for which compositionality models based only on word embeddings would not be able to make predictions.",
  "y": "differences"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_0",
  "x": "There have been quite a number of recent papers on parallel text: Brown et al (1990 Brown et al ( , 1991 Brown et al ( , 1993 , Chen (1993) , <cite>Church (1993)</cite> , , Dagan et al (1993) , Church (1991, 1993) , Isabelle (1992) , Kay and Rgsenschein (1993) , Klavans and Tzoukermann (1990) , Kupiec (1993) , Matsumoto (1991) , Ogden and Gonzales (1993) , Shemtov (1993) , Simard et al (1992) , WarwickArmstrong and Russell (1990) , Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese.",
  "y": "background"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_1",
  "x": "There have been quite a number of recent papers on parallel text: Brown et al (1990 Brown et al ( , 1991 Brown et al ( , 1993 , Chen (1993) , <cite>Church (1993)</cite> , , Dagan et al (1993) , Church (1991, 1993) , Isabelle (1992) , Kay and Rgsenschein (1993) , Klavans and Tzoukermann (1990) , Kupiec (1993) , Matsumoto (1991) , Ogden and Gonzales (1993) , Shemtov (1993) , Simard et al (1992) , WarwickArmstrong and Russell (1990) , Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese.",
  "y": "background motivation"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_2",
  "x": "It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese. In previous work , we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual (Aho, Kernighan, Weinberger (1980) ), using charalign<cite> (Church, 1993)</cite> , a method that looks for character sequences that are the same in both the source and target. The charalign method was designed for European language pairs, where cognates often share character sequences, e.g., government and gouvernement.",
  "y": "background"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_3",
  "x": "In previous work , we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual (Aho, Kernighan, Weinberger (1980) ), using charalign<cite> (Church, 1993)</cite> , a method that looks for character sequences that are the same in both the source and target. In general, this approach doesn't work between languages such as English and Japanese which are written in different alphabets.",
  "y": "motivation background"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_4",
  "x": "The concordances for fisheries and p~ches are shown in Tables 1 and 2 (at the end of this paper). 1 1. These tables were computed from a small fragment of the Canadian Hansards that has been used in a number of other studies: <cite>Church (1993)</cite> and Simard et al (1992 show where the concordances were found in the texts.",
  "y": "uses"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_5",
  "x": "---------------------------------- **RESULTS** This algorithm was applied to a fragment of the Canadian Hansards that has been used in a number of other studies: <cite>Church (1993)</cite> and Simard et al (1992) .",
  "y": "uses"
 },
 {
  "id": "dcfad33f4322738906e2fdffe2e721_6",
  "x": "Currently, word_align depends on charalign<cite> (Church, 1993)</cite> to generate a starting point, which limits its applicability to European languages since char_align was designed for language pairs that share a common alphabet. ---------------------------------- **REFERENCES**",
  "y": "uses background"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_0",
  "x": "Recent work demonstrated that word embeddings induced from large text collections encode many human biases (e.g., Bolukbasi et al., 2016;<cite> Caliskan et al., 2017)</cite> . This finding is not particularly surprising given that (1) we are likely project our biases in the text that we produce and (2) these biases in text are bound to be encoded in word vectors due to the distributional nature (Harris, 1954) of the word embedding models (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017) . For illustration, consider the famous analogy-based gender bias example from Bolukbasi et al. (2016) : \"Man is to computer programmer as woman is to homemaker\".",
  "y": "background motivation"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_1",
  "x": "In order to measure the extent to which various societal biases are captured by word embeddings,<cite> Caliskan et al. (2017)</cite> proposed the Word Embedding Association Test (WEAT). WEAT measures semantic similarity, computed through word embeddings, between two sets of target words (e.g., insects vs. flowers) and two sets of attribute words (e.g., pleasant vs. unpleasant words). While they test a number of biases, the analysis is limited in scope to English as the only language, GloVe (Pennington et al., 2014) as the embedding model, and Common Crawl as the type of text.",
  "y": "background"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_2",
  "x": "We first introduce the WEAT dataset<cite> (Caliskan et al., 2017)</cite> and then describe XWEAT, our multilingual and cross-lingual extension of WEAT designed for comparative bias analyses across languages and in cross-lingual embedding spaces. ---------------------------------- **WEAT**",
  "y": "uses"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_3",
  "x": "---------------------------------- **WEAT** The Word Embedding Association Test (WEAT)<cite> (Caliskan et al., 2017)</cite> is an adaptation of the Implicit Association Test (IAT) (Nosek et al., 2002) .",
  "y": "background"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_4",
  "x": "---------------------------------- **METHODOLOGY** We adopt the general bias-testing framework from<cite> Caliskan et al. (2017)</cite> , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora.",
  "y": "extends uses"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_5",
  "x": "**METHODOLOGY** We adopt the general bias-testing framework from<cite> Caliskan et al. (2017)</cite> , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora. Furthermore, unlike<cite> Caliskan et al. (2017)</cite> , we test whether biases depend on the selection of the similarity metric.",
  "y": "extends"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_6",
  "x": "We adopt the general bias-testing framework from<cite> Caliskan et al. (2017)</cite> , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora. Furthermore, unlike<cite> Caliskan et al. (2017)</cite> , we test whether biases depend on the selection of the similarity metric. Finally, given the ubiquitous adoption of cross-lingual embeddings (Ruder et al., 2017; Glava\u0161 et al., 2019) , we investigate biases in a variety of bilingual embedding spaces.",
  "y": "extends"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_7",
  "x": "Finally, given the ubiquitous adoption of cross-lingual embeddings (Ruder et al., 2017; Glava\u0161 et al., 2019) , we investigate biases in a variety of bilingual embedding spaces. Bias-Testing Framework. We first describe the WEAT framework<cite> (Caliskan et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_8",
  "x": "The tested statistic is the difference between X and Y in average similarity of their terms with terms from A and B: with association difference for term t computed as: where t is the distributional vector of term t and f is a similarity or distance metric, fixed to cosine similarity in the original work<cite> (Caliskan et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_9",
  "x": "Corpora. In Table 4 we compare the biases of embeddings trained with the same model (GLOVE) but on different corpora: Common Crawl (i.e., noisy web content), Wikipedia (i.e., encyclopedic the definition of A and vice versa (Tissier et al., 2017) . 5 This is consistent with the original results obtained by<cite> Caliskan et al. (2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "de9eb9b7dff69743252b3ff0ef8894_10",
  "x": "**CONCLUSION** In this paper, we have presented the largest study to date on biases encoded in distributional word vector spaces. To this end, we have extended previous analyses based on the WEAT test<cite> (Caliskan et al., 2017</cite>; McCurdy and Serbetci, 2017) in multiple dimensions: across seven languages, four embedding models, and three different types of text.",
  "y": "extends uses"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_0",
  "x": "1 The CGT transport federation have risen against \"the lack of consultation\" and consider that employees have \"nothing positive to expect from this restructuring.\" 2 While studies have shown that discourse usage of discourse connectives can be accurately identified for English [13, <cite>20]</cite> , only a few studies have focused on the disambiguation of discourse connectives in other languages. In this paper, we investigate the usefulness of features proposed in the literature for the disambiguation of English discourse connectives for French discourse connectives. 3 This paper is organized as follow: Section 2 reviews related work.",
  "y": "background motivation"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_1",
  "x": "Most of previous work on the disambiguation of discourse connectives have focused on English discourse connectives [13, 14, <cite>20]</cite> . One of earliest and pioneer work on the disambiguation of discourse connectives, Pitler and Nenkova<cite> [20]</cite> , showed that four syntactic features (see Section 3.4 for details about the features) and the connective itself can disambiguate discourse connectives with an accuracy of 95.04% within the PDTB [22] . Their approach used these features not only to disambiguate discourse connectives between discourse-usage and non-discourse-usage, but also to tag the discourse relation signalled by the discourse connectives.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_2",
  "x": "One of earliest and pioneer work on the disambiguation of discourse connectives, Pitler and Nenkova<cite> [20]</cite> , showed that four syntactic features (see Section 3.4 for details about the features) and the connective itself can disambiguate discourse connectives with an accuracy of 95.04% within the PDTB [22] . Their approach used these features not only to disambiguate discourse connectives between discourse-usage and non-discourse-usage, but also to tag the discourse relation signalled by the discourse connectives. Later, Lin et al. [13] used the context of the connective (i.e. the previous and the following word of the connective) and added seven lexico-syntactic features to the feature set proposed by Pitler and Nenkova<cite> [20]</cite> .",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_3",
  "x": "Later, Lin et al. [13] used the context of the connective (i.e. the previous and the following word of the connective) and added seven lexico-syntactic features to the feature set proposed by Pitler and Nenkova<cite> [20]</cite> . In doing so, Lin et al. achieved an accuracy of 97.34% for the disambiguation of discourse connectives in the PDTB. On the other hand, the disambiguation of discourse connectives in languages other than English has received much less attention.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_4",
  "x": "Inspired by Pitler and Nenkova<cite> [20]</cite> , Alsaif and Markert [4] proposed an approach for the disambiguation of Arabic Discourse connectives. Alsaif and Markert have shown that the features proposed by Pitler and Nenkova<cite> [20]</cite> work well for Arabic with an accuracy of 91.2%. Moreover, they further improved the result of their system by considering Arabic-specific morphological features and achieved an accuracy of 92.4%.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_5",
  "x": "Inspired by Pitler and Nenkova<cite> [20]</cite> , Alsaif and Markert [4] proposed an approach for the disambiguation of Arabic Discourse connectives. Alsaif and Markert have shown that the features proposed by Pitler and Nenkova<cite> [20]</cite> work well for Arabic with an accuracy of 91.2%. Moreover, they further improved the result of their system by considering Arabic-specific morphological features and achieved an accuracy of 92.4%.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_6",
  "x": "**FEATURES** As mentioned in Section 2, Pitler and Nenkova<cite> [20]</cite> have shown that the context of discourse connectives in the syntactic tree is very discriminating for the disambiguation of English discourse connectives. They proposed four syntactic features:",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_7",
  "x": "Regardless of its source, this disparity motivated us to investigate the applicability of features proposed for the disambiguation of English discourse connectives for French. As mentioned in Section 2, Pitler and Nenkova<cite> [20]</cite> have shown that the context of discourse connectives in the syntactic tree is very discriminating for the disambiguation of English discourse connectives.",
  "y": "uses"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_8",
  "x": "The SelfCat node is the 'ADV' node in the parse tree and its parent, left and right siblings are the 'S', 'VN' and 'PP' nodes, respectively. In addition to the four features above, Pitler and Nenkova<cite> [20]</cite> used the discourse connective itself (case sensitive) as an additional feature for the classifier. The purpose of using the case sensitive version is to distinguish connectives positioned at the beginning of sentences.",
  "y": "background"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_9",
  "x": "In addition to the four features above, Pitler and Nenkova<cite> [20]</cite> used the discourse connective itself (case sensitive) as an additional feature for the classifier. We slightly modified this feature by using the case-folded version of the discourse connective (called the Conn Feature).",
  "y": "extends"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_10",
  "x": "**AT-THE-BEGINNING OR NOT-AT-THE-BEGINNING).** These two features are as informative as the case-sensitive connective string proposed by Pitler and Nenkova<cite> [20]</cite> , however, separating these features gives the classifier more flexibility when building its model. In Example (1), these two features are 'ainsi' and 'not-at-the-beginning', respectively.",
  "y": "similarities differences"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_11",
  "x": "---------------------------------- **RESULTS** Similarly to Pitler and Nenkova<cite> [20]</cite> , we report results using a maximum entropy classifier using ten-fold cross-validation over the extracted datasets.",
  "y": "similarities"
 },
 {
  "id": "e0df566d073649431c3454a52813e9_12",
  "x": "In this paper, we have investigated the applicability of the syntactic and lexical features proposed by Pitler and Nenkova<cite> [20]</cite> for the disambiguation of English discourse connectives for French. Our experiments on the French Discourse Treebank (FDTB) show that even though the syntactic features are less informative for the disambiguation of French discourse connectives than for English discourse connectives, overall the features can effectively disambiguate French discourse connectives between discourse-usage and non-discourseusage as well in French as in English. The fact that the local syntactic features proposed for English can be used almost as effectively for French and Arabic [4] suggests that lexicalized discourse connectives share certain common structural features cross-linguistically and that these structures are potentially an important component in discourse processing.",
  "y": "extends"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_0",
  "x": "Our results on both MBN and MFCC features are significantly higher than the previous state-of-the-art. The largest improvement comes from using the learned MBN features but our approach also improves results for MFCCs, which are the same features as were used in <cite>[15]</cite> . The learned MBN features provide better performance whereas the MFCCs are more cognitively plausible input features.",
  "y": "similarities differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_1",
  "x": "In the current study we present an image-caption retrieval model that extends our previous work to spoken input. In [12, 13] , the authors adapted text based caption-image retrieval (e.g. [9] ) and showed that it is possible to perform speech-image retrieval using convolutional neural networks on spectral features. Our work is most closely related to the models presented in [12, 13, 14,<cite> 15]</cite> .",
  "y": "similarities"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_2",
  "x": "Our work is most closely related to the models presented in [12, 13, 14,<cite> 15]</cite> . In the current study we improve upon these previous approaches to visual grounding of speech and present state-of-the-art image-caption retrieval results. The work by [12, 13, 14,<cite> 15]</cite> and the results presented here are a step towards more cognitively plausible models of language learning as it is more natural to learn language without prior assumptions about the lexical level.",
  "y": "uses"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_3",
  "x": "The work by [12, 13, 14,<cite> 15]</cite> and the results presented here are a step towards more cognitively plausible models of language learning as it is more natural to learn language without prior assumptions about the lexical level. For instance, research indicates that the adult lexicon contains many relatively fixed multi-word expressions (e.g., 'how-are-you-doing') [16] . Furthermore, early during language acquisition the lexicon consists of entire utterances before a child's language use becomes more adult-like [16, 17, 18, 19] .",
  "y": "background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_4",
  "x": "The idea is to make matching images and captions lie close together and mismatched images and captions lie far apart in the embedding space. Our model consists of two parts; an image encoder and a sentence encoder as depicted in Figure 1 . The approach is based on our own text-based model described in [8] and on the speech-based models described in [13,<cite> 15]</cite> and we refer to those studies for more details.",
  "y": "uses background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_5",
  "x": "where the other caption-image pairs in the batch serve to create mismatched pairs (c, i \u2032 ) and (c \u2032 , i). We take the cosine similarity cos(x, y) and subtract the similarity of the mismatched pairs from the matching pairs such that the loss is only zero when the matching pair is more similar than the mismatched pairs by a margin \u03b1. We use importance sampling to select the mismatched pairs; rather than using all the other samples in the mini-batch as mismatched pairs (as done in [8,<cite> 15]</cite> ), we calculate the loss using only the hardest examples (i.e. mismatched pairs with high cosine similarity).",
  "y": "differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_6",
  "x": "The main differences with the approaches described in [13,<cite> 15]</cite> are the use of multi-layered GRUs, importance sampling, the cyclic learning rate, snapshot ensembling and the use of vectorial rather than scalar attention. ---------------------------------- **WORD PRESENCE DETECTION**",
  "y": "differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_7",
  "x": "---------------------------------- **WORD PRESENCE DETECTION** While our model is not explicitly trained to recognise words or segment the speech signal, previous work has shown that such information can be extracted by visual grounding models<cite> [15,</cite> 28] .",
  "y": "background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_8",
  "x": "While our model is not explicitly trained to recognise words or segment the speech signal, previous work has shown that such information can be extracted by visual grounding models<cite> [15,</cite> 28] . <cite>[15]</cite> use a binary decision task: given a word and a sentence embedding, decide if the word occurs in the sentence. Our approach is similar to the spoken-bag-of-words prediction task described in [28] .",
  "y": "background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_9",
  "x": "Table 1 shows the performance of our models on the imagecaption retrieval task. The caption embeddings are ranked by cosine distance to the image and vice versa where R@N is the percentage of test items for which the correct image or caption was in the top N results. We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison.",
  "y": "uses"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_10",
  "x": "We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison. [12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions (see [8] for a full description).",
  "y": "uses"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_11",
  "x": "[12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions (see [8] for a full description). Both our MFCC and MBN based model significantly outperform previous spoken captionto-image methods on the Flickr8k dataset.",
  "y": "background"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_12",
  "x": "Both our MFCC and MBN based model significantly outperform previous spoken captionto-image methods on the Flickr8k dataset. The largest improvement is the MBN model which outperforms the results reported in <cite>[15]</cite> by as much as 23.2 percentage points on R@10. The MFCC model also improves on previous results but scores significantly lower than the MBN model across the board, improving as much as 12.3 percentage points over previous work.",
  "y": "differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_13",
  "x": "Both our MFCC and MBN based model significantly outperform previous spoken captionto-image methods on the Flickr8k dataset. The largest improvement is the MBN model which outperforms the results reported in <cite>[15]</cite> by as much as 23.2 percentage points on R@10. The MFCC model also improves on previous results but scores significantly lower than the MBN model across the board, improving as much as 12.3 percentage points over previous work.",
  "y": "differences"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_14",
  "x": "In a probing task, we show that the model learns to recognise words in the input speech signal. We are currently collecting the Semantic Textual Similarity (STS) database in spoken format and the next step will be to investigate whether the model presented here also learns to capture sentence level semantic information and understand language in a deeper sense than recognising word presence. The work presented in <cite>[15]</cite> has made the first efforts in this regard and we aim to extend this to a larger database with sentences from multiple domains.",
  "y": "uses future_work"
 },
 {
  "id": "e41a1adcb5c9d91f2130bd249ed598_15",
  "x": "In a probing task, we show that the model learns to recognise words in the input speech signal. We are currently collecting the Semantic Textual Similarity (STS) database in spoken format and the next step will be to investigate whether the model presented here also learns to capture sentence level semantic information and understand language in a deeper sense than recognising word presence. The work presented in <cite>[15]</cite> has made the first efforts in this regard and we aim to extend this to a larger database with sentences from multiple domains.",
  "y": "future_work"
 },
 {
  "id": "e803782890224294066ce447671981_0",
  "x": "We describe an algorithm for recovering non-local dependencies in syntactic dependency structures. The <cite>patternmatching approach</cite> proposed by <cite>Johnson (2002)</cite> for a similar task for phrase structure trees is extended with machine learning techniques. The algorithm is essentially a classifier that predicts a nonlocal dependency given a connected fragment of a dependency structure and a set of structural features for this fragment.",
  "y": "extends"
 },
 {
  "id": "e803782890224294066ce447671981_1",
  "x": "The algorithm is essentially a classifier that predicts a nonlocal dependency given a connected fragment of a dependency structure and a set of structural features for this fragment. Evaluating the algorithm on the Penn Treebank shows an improvement of both precision and recall, compared to the results presented in <cite>(Johnson, 2002)</cite> . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e803782890224294066ce447671981_2",
  "x": "Non-local dependencies (also called long-distance, long-range or unbounded) appear in many frequent linguistic phenomena, such as passive, WHmovement, control and raising etc. Although much current research in natural language parsing focuses on extracting local syntactic relations from text, nonlocal dependencies have recently started to attract more attention. In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_3",
  "x": "In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while <cite>Johnson (2002)</cite> presents a method for recovering non-local dependencies after parsing has been performed. More specifically, <cite>Johnson (2002)</cite> describes a <cite>pattern-matching algorithm</cite> for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies. From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_4",
  "x": "From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent. In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_5",
  "x": "From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent. In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_6",
  "x": "From a training corpus with annotated empty nodes <cite>Johnson's algorithm</cite> first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies. Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, <cite>the algorithm</cite> introduces a corresponding non-local dependency, inserting an empty node and (possibly) coindexing it with a suitable antecedent. In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem.",
  "y": "motivation"
 },
 {
  "id": "e803782890224294066ce447671981_7",
  "x": "In <cite>(Johnson, 2002 )</cite> <cite>the author</cite> notes that the biggest weakness of <cite>the algorithm</cite> seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem. Moreover, <cite>the author</cite> suggests that <cite>the algorithm</cite> may suffer from overlearning, and using more abstract \"skeletal\" patterns may be helpful to avoid this. In an attempt to overcome these problems we developed a similar approach using dependency structures rather than phrase structure trees, which, moreover, extends bare pattern matching with machine learning techniques.",
  "y": "motivation"
 },
 {
  "id": "e803782890224294066ce447671981_8",
  "x": "To avoid this, we associate patterns with certain structural features and use statistical classifi- cation methods on top of pattern matching. The evaluation of our algorithm on data automatically derived from the Penn Treebank shows an increase in both precision and recall in recovery of non-local dependencies by approximately 10% over the results reported in <cite>(Johnson, 2002)</cite> . However, additional work remains to be done for our algorithm to perform well on the output of a parser.",
  "y": "differences"
 },
 {
  "id": "e803782890224294066ce447671981_9",
  "x": "As in <cite>(Johnson, 2002)</cite> , our patterns are minimal connected fragments containing both nodes involved in a non-local dependency. However, in our case these fragments are not connected sets of local trees, but shortest paths in local dependency graphs, leading from heads to non-local dependents. Patterns do not include POS tags of the involved words, but only labels of the dependencies.",
  "y": "uses similarities"
 },
 {
  "id": "e803782890224294066ce447671981_10",
  "x": "Figure 2 shows examples of dependency graphs (above) and extracted patterns (below, with filled bullets corresponding to the nodes of a nonlocal dependency). As before, dotted lines denote non-local dependencies. The definition of a structure matching a pattern, and the algorithms for pattern matching and pattern extraction from a corpus are straightforward and similar to those described in <cite>(Johnson, 2002)</cite> .",
  "y": "similarities"
 },
 {
  "id": "e803782890224294066ce447671981_12",
  "x": "The table also shows the number of times a pattern (together with a specific non-local dependency label) actually occurs in the whole Penn Treebank corpus (the column Dependency count). In order to compare our results to the results presented in <cite>(Johnson, 2002)</cite> , we measured the overall performance of the algorithm across patterns and non-local dependency labels. This corresponds to the row \"Overall\" of Table 4 in <cite>(Johnson, 2002)</cite> , repeated here in Table 4 .",
  "y": "similarities"
 },
 {
  "id": "e803782890224294066ce447671981_13",
  "x": "This corresponds to the row \"Overall\" of Table 4 in <cite>(Johnson, 2002)</cite> , repeated here in Table 4 . We also evaluated the procedure on NP traces across all patterns, i.e., on nonlocal dependencies with NP-SBJ, NP-OBJ or NP-PRD labels. This corresponds to rows 2, 3 and 4 of Table 4 in <cite>(Johnson, 2002)</cite> .",
  "y": "similarities"
 },
 {
  "id": "e803782890224294066ce447671981_17",
  "x": "It is difficult to make a strict comparison of our results and those in <cite>(Johnson, 2002)</cite> . The two algorithms are designed for slightly different purposes: while <cite>Johnson's approach</cite> allows one to recover free empty nodes (without antecedents), we look for nonlocal dependencies, which corresponds to identification of co-indexed empty nodes (note, however, the modifications we describe in Section 2, when we actually transform free empty nodes into co-indexed empty nodes). ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e803782890224294066ce447671981_18",
  "x": "The results presented in the previous section show that it is possible to improve over the simple <cite>pattern matching algorithm</cite> of <cite>(Johnson, 2002)</cite> , using dependency rather than phrase structure information, more skeletal patterns, as was suggested by <cite>Johnson</cite>, and a set of features associated with instances of patterns. One of the reasons for this improvement is that our approach allows us to discriminate between different syntactic phenomena involving non-local dependencies. In most cases our patterns correspond to linguistic phenomena.",
  "y": "differences"
 },
 {
  "id": "e803782890224294066ce447671981_19",
  "x": "Obviously, because of parsing errors the performance drops significantly: e.g., in the experiments reported in <cite>(Johnson, 2002 )</cite> the overall fscore decreases from 0.75 to 0.68 when evaluating on parser output (see Table 4 ). While experimenting with Collins' parser (Collins, 1999) , we found that for our algorithm the accuracy drops even more dramatically, when we train the classifier on Penn Treebank data and test it on parser output. One of the reasons is that, since we run our algorithm not on the parser's output itself but on the output automatically converted to dependency structures, conversion errors also contribute to the performance drop.",
  "y": "background"
 },
 {
  "id": "e803782890224294066ce447671981_20",
  "x": "We have presented an algorithm for recovering longdistance dependencies in local dependency structures. We extend the pattern matching approach of <cite>Johnson (2002)</cite> with machine learning techniques, and use dependency structures instead of constituency trees. Evaluation on the Penn Treebank shows an increase in accuracy.",
  "y": "extends differences"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_0",
  "x": "---------------------------------- **INTRODUCTION** Large-scale knowledge bases (KBs), such as Freebase [Bollacker et al., 2008] , WordNet<cite> [Miller, 1995]</cite> , Yago<cite> [Suchanek et al., 2007]</cite> , and NELL [Carlson et al., 2010] , are critical to natural language processing applications, e.g., question answering [Dong et al., 2015] , relation extraction<cite> [Riedel et al., 2013]</cite> , and language modeling [Ahn et al., 2016] .",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_1",
  "x": "These KBs generally contain billions of facts, and each fact is organized into a triple base format (head entity, relation, tail entity), abbreviated as (h,r,t). However, the coverage of such KBs is still far from complete compared with real-world knowledge [Dong et al., 2014] . Traditional KB completion approaches, such as Markov logic networks<cite> [Richardson and Domingos, 2006]</cite> , suffer from feature sparsity and low efficiency.",
  "y": "motivation"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_2",
  "x": "Among these methods, TransE [Bordes et al., 2013 ] is a classical neural-based model, which assumes that each relation can be regarded as a translation from head to tail and uses a score function S(h,r,t)= h + r \u2212 t to measure the plausibility for triples. TransH [Wang et al., 2014] and TransR<cite> [Lin et al., 2015b]</cite> are representative variants of TransE. These variants consider entities from multiple aspects and various relations on different aspects. However, the majority of these approaches only exploit direct links that connect head and tail entities to predict potential relations between entities.",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_3",
  "x": "Consequently, a promising new research direction is to use relation paths to learn knowledge embeddings<cite> [Neelakantan et al., 2015</cite>; Guu et al., 2015;<cite> Toutanova et al., 2016]</cite> . For a relation path, consistent semantics is a semantic interpretation via composition of the meaning of the component elements. Each relation path contains its respective consistent semantics.",
  "y": "motivation"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_4",
  "x": "Each relation path contains its respective consistent semantics. However, the consistent semantics expressed by some relation paths p is unreliable for reasoning new facts of that entity pair<cite> [Lin et al., 2015a]</cite> . For instance, there is a common relation path h",
  "y": "motivation"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_5",
  "x": "As the path ranking algorithm (PRA)<cite> [Lao et al., 2011]</cite> suggests, relation paths that end in many possible tail entities are more likely to be unreliable for the entity pair. Reliable relation paths can thus be filtered using PRA. Figure 1 illustrates the basic idea for relation-specific and path-specific projections.",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_6",
  "x": "Typeconstrained TransE [Krompass et al., 2015] imposes these constraints on the global margin-loss function to better distinguish similar embeddings in latent space. A third current related work is PTransE<cite> [Lin et al., 2015a</cite> ] and the path ranking algorithm (PRA)<cite> [Lao et al., 2011]</cite> . PTransE considers relation paths as translations between head and tail entities and primarily addresses two problems: 1) exploit a variant of PRA to select reliable relation paths, and 2) explore three path representations by compositions of relation embeddings.",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_7",
  "x": "PRA, as one of the most promising research innovations for knowledge base completion, has also attracted considerable attention<cite> [Lao et al., 2015</cite>; Gardner and Mitchell, 2015; Wang et al., 2016;<cite> Nickel et al., 2016]</cite> . PRA uses the path-constrained random walk probabilities as path features to train linear classifiers for different relations. In large-scale KBs, relation paths have great significance for enhancing the reasoning ability for more complicated situations.",
  "y": "background"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_8",
  "x": "We follow the same evaluation procedures as used in [Bordes et al., 2013; Wang et al., 2014;<cite> Lin et al., 2015b]</cite> . First, for each test triple (h,r,t), we replace h or t with every entity in \u03b6. Second, each corrupted triple is calculated by the corresponding score function S(h,r,t).",
  "y": "uses"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_9",
  "x": "In both settings, if the latent representations of entity and relation are better, then a lower mean rank and a higher Hits@10 should be achieved. Because we use the same dataset, the baseline results reported in<cite> [Lin et al., 2015b</cite>;<cite> Lin et al., 2015a</cite>; Ji et al., 2016] are directly used for comparison. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_10",
  "x": "From Table 3 , we can conclude that 1) RPE (ACOM) outperforms all baselines in all mapping properties of relations. In particular, for the 1-to-N, N-to-1, and N-to-N types of relations [Bordes et al., 2013] 75.9 70.9 77.8 TransE (bern) [Bordes et al., 2013] 75.9 81.5 85.3 TransH (unif) [Wang et al., 2014] 77.7 76.5 78.4 TransH (bern) [Wang et al., 2014] 78.8 83.3 85.8 TransR (unif)<cite> [Lin et al., 2015b]</cite> 85.5 74.7 79.2 TransR (bern)<cite> [Lin et al., 2015b]</cite> 85.9 82.5 87.0 PTransE (ADD, 2-hop)<cite> [Lin et al., 2015a]</cite> 80.9 73.5 83.4 PTransE (MUL, 2-hop)<cite> [Lin et al., 2015a]</cite> 79.4 73.6 79.3 PTransE (ADD, 3-hop)<cite> [Lin et al., 2015a]</cite> 80 that plague knowledge embedding models, RPE (ACOM) improves 4.1%, 4.6%, and 4.9% on head entity's prediction and 6.9%, 7.0%, and 5.1% on tail entity's prediction compared with previous state-of-the-art performances achieved by PTransE (ADD, 2-hop). 2) RPE (MCOM) does not perform as well as RPE (ACOM), and we believe that this result is because RPE's path representation is not consistent with RPE (MCOM)'s composition of projections.",
  "y": "differences"
 },
 {
  "id": "e826db8ca46b47ba56945c50512a03_11",
  "x": "---------------------------------- **IMPLEMENTATION** We directly compare our model with prior work using the results about knowledge embedding models reported in<cite> [Lin et al., 2015b]</cite> n=50, m=50, \u03b3 1 =5, \u03b3 2 =6, \u03b1=0.0001, B=1440, \u03bb=0.8, and \u03b7=0.05, taking the L 1 norm on WN11; n=100, m=100, \u03b3 1 =3, \u03b3 2 =6, \u03b1=0.0001, B=960, \u03bb=0.8, and \u03b7=0.05, taking the L 1 norm on FB13; and n=100, m=100, \u03b3 1 =4, \u03b3 2 =5, \u03b1=0.0001, B=4800, \u03bb=1, and \u03b7 =0.05, taking the L 1 norm on FB15K.",
  "y": "uses"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_0",
  "x": "State-of-the-art image annotation methods are currently based on deep neural net representations, where an image embedding (e.g., obtained from a convolutional neural network or CNN) and a text embedding (e.g., obtained from a recurrent neural network or RNN) are combined into a unique multimodal embedding space. While several techniques for merging both spaces have been proposed [<cite>1</cite>, 2, 3, 4, 5, 6] , little effort has been made in finding the most appropriate image embeddings to be used in that process. In fact, <cite>most approaches</cite> simply use a one-layer CNN embedding [7, 8] .",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_1",
  "x": "State-of-the-art image annotation methods are currently based on deep neural net representations, where an image embedding (e.g., obtained from a convolutional neural network or CNN) and a text embedding (e.g., obtained from a recurrent neural network or RNN) are combined into a unique multimodal embedding space. While several techniques for merging both spaces have been proposed [<cite>1</cite>, 2, 3, 4, 5, 6] , little effort has been made in finding the most appropriate image embeddings to be used in that process. In fact, <cite>most approaches</cite> simply use a one-layer CNN embedding [7, 8] .",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_2",
  "x": "In fact, <cite>most approaches</cite> simply use a one-layer CNN embedding [7, 8] . In this paper we explore the impact of using a Full-Network embedding (FNE) [9] to generate the required image embedding, replacing the one-layer embedding. We do so by integrating the FNE into the multimodal embedding pipeline defined by <cite>Kiros et al. [1]</cite> , which is based in the use of a Gated Recurrent Units neural network (GRU) [10] for text encoding and CNN for image encoding.",
  "y": "extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_3",
  "x": "These particularities result in a richer visual embedding space, which may be more reliably mapped to a common visual-textual embedding space. The generic pipeline defined by <cite>Kiros et al. [1]</cite> has been recently outperformed in image annotation and image search tasks by methods specifically targeting one of those tasks [4, 18] . We choose to test our contribution on <cite>this pipeline</cite> for its overall competitive performance, expecting that any conclusion may generalize when applied to other solutions and tasks (e.g., caption generation).",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_4",
  "x": "The generic pipeline defined by <cite>Kiros et al. [1]</cite> has been recently outperformed in image annotation and image search tasks by methods specifically targeting one of those tasks [4, 18] . We choose to test our contribution on <cite>this pipeline</cite> for its overall competitive performance, expecting that any conclusion may generalize when applied to other solutions and tasks (e.g., caption generation). This assumption would be dimmer if a more problem-specific methodology was chosen instead.",
  "y": "uses motivation"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_5",
  "x": "We test the suitability of this approach by evaluating its performance on both image annotation and image retrieval using three publicly available datasets: Flickr8k [11] , Flickr30k [12] and MSCOCO [13] . Results obtained by the pipeline including the FNE are compared with the original pipeline of <cite>Kiros et al. [1]</cite> using a one-layer embedding, and also with the methods currently obtaining state-of-the-art results on the three datasets. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_6",
  "x": "In the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross-domain search [<cite>1</cite>, 2, 3, 4, 5] . This paper builds upon the methodology described by <cite>Kiros et al. [1]</cite> , <cite>which</cite> is in turn based on previous works in the area of Neural Machine Translation [14] . In their work, <cite>Kiros et al. [1]</cite> define a vectorized representation of an input text by using GRU RNNs.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_7",
  "x": "**RELATED WORK** In the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross-domain search [<cite>1</cite>, 2, 3, 4, 5] . This paper builds upon the methodology described by <cite>Kiros et al. [1]</cite> , <cite>which</cite> is in turn based on previous works in the area of Neural Machine Translation [14] .",
  "y": "extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_8",
  "x": "In their work, <cite>Kiros et al. [1]</cite> define a vectorized representation of an input text by using GRU RNNs. In this setting, each word in the text is codified into a vector using a word dictionary, vectors which are then fed one by one into the GRUs. Once the last word vector has been processed, the activations of the GRUs at the last time step conveys the representation of the whole input text in the multimodal embedding space.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_9",
  "x": "Similarly to the approach of <cite>Kiros et al. [1]</cite> , most image annotation and image retrieval approaches rely on the use of CNN features for image representation. The current best overall performing model (considering both image annotation and image retrieval tasks) is the Fisher Vector (FV) [4] , although its performance is most competitive on the image retrieval task. FV are computed with respect to the parameters of a Gaussian Mixture Model (GMM) and an Hybrid Gaussian-Laplacian Mixture Model (HGLMM).",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_10",
  "x": "**MULTIMODAL EMBEDDING** In our approach, we integrate the FNE with the multimodal embedding pipeline of <cite>Kiros et al. [1]</cite> . To do so we use the FNE to obtain an image representation instead of the output of the last layer of a CNN, as the <cite>original model</cite> does.",
  "y": "extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_11",
  "x": "In our approach, we integrate the FNE with the multimodal embedding pipeline of <cite>Kiros et al. [1]</cite> . To do so we use the FNE to obtain an image representation instead of the output of the last layer of a CNN, as the <cite>original model</cite> does. The encoder architecture processing the text is used as it is, using a GRUs recurrent neural network to encode the sentences.",
  "y": "extends differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_12",
  "x": "To combine both embeddings, <cite>Kiros et al. [1]</cite> use an affine transformation on the image representation (in our case, the FNE) identical to a fully connected neural network layer. This extra layer is trained simultaneously with the GRUs. The elements of the <cite>multimodal pipeline</cite> that are tuned during the training phase of the model are shown in orange in Figure  1 .",
  "y": "differences background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_13",
  "x": "This extra layer is trained simultaneously with the GRUs. The elements of the <cite>multimodal pipeline</cite> that are tuned during the training phase of the model are shown in orange in Figure  1 . In simple terms, the training procedure consist on the optimization of the pairwise ranking loss between the correct image-caption pair and a random pair.",
  "y": "background"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_14",
  "x": "To properly measure the relevance of the FNE, we compare the results of the FN-MME with those of the <cite>original multimodal pipeline</cite> reported by <cite>Kiros et al. [1]</cite> (CNN-MME). Additionally, we define a second baseline by using the <cite>original multimodal pipeline</cite> with a training configuration closer to the one used for the FNE experiments (i.e., same source CNN, same MME dimensionality, etc.). We refer to this second baseline as CNN-MME*.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_15",
  "x": "Additionally, we define a second baseline by using the <cite>original multimodal pipeline</cite> with a training configuration closer to the one used for the FNE experiments (i.e., same source CNN, same MME dimensionality, etc.). We refer to this second baseline as CNN-MME*. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_16",
  "x": "Five correct captions are provided for each image. In our experiments 29,000 images are used for training, 1,014 conform the validation set and 1,000 are kept for test. These splits are the same ones used by <cite>Kiros et al. [1]</cite> and by Karpathy and Fei-Fei [22] .",
  "y": "uses"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_17",
  "x": "Using all the words present in the dataset is likely to produce overfitting problems when training on examples containing words that only occur a few times. This overfitting problem may not have a huge impact on performance, but it may add undesired noise in the multimodal representation. The original setup [<cite>1]</cite> limited the word embedding to the 300 most frequent words, while using 300 GRUs.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_18",
  "x": "**RESULTS** For both image annotation and image retrieval tasks on the Flickr8k dataset, Table 1 shows the results of the proposed FN-MME, the reported results of the <cite>original model</cite> <cite>CNN-MME</cite>, the results of the original model when using our configuration CNN-MME*, and the current state-of-the-art (SotA). Tables 2 and  3 are analogous for the Flickr30k and MSCOCO datasets.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_19",
  "x": "Tables 2 and  3 are analogous for the Flickr30k and MSCOCO datasets. Additional results of the CNN-MME model were made publicly available later on by the original authors [26] . We include these for the MSCOCO dataset, which was not evaluated in the <cite>original paper</cite> <cite>[1]</cite> .",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_20",
  "x": "We include these for the MSCOCO dataset, which was not evaluated in the <cite>original paper</cite> <cite>[1]</cite> . First, let us consider the impact of using the FNE. On all cases, the multimodal pipeline proposed by <cite>Kiros et al. [1]</cite> obtains equal or better results when using the FNE.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_21",
  "x": "On all cases, the multimodal pipeline proposed by <cite>Kiros et al. [1]</cite> obtains equal or better results when using the FNE. This is the case for the <cite>originally reported results</cite> (<cite>CNN-MME</cite>), for the results made available later on by the original authors (CNN-MME \u2020), and for the experiments we do using same configuration as the FN-MME (CNN-MME*). The comparison we consider to be the most relevant is the FN-MME against the CNN-MME*, as these contain the least differences besides the image embedding being used.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_22",
  "x": "---------------------------------- **CONCLUSIONS** For the multimodal pipeline of <cite>Kiros et al. [1]</cite> , using the Full-Network image embedding results in consistently higher performances than using a one-layer image embedding.",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_23",
  "x": "When compared to the current state-of-the-art, the results obtained by the FN-MME are significantly less competitive than problem-specific methods. Since this happens for all models using the same pipeline (CNN-MME, CNN-MME \u2020, CNN-MME*), these results indicate that the original architecture of <cite>Kiros et al. [1]</cite> is itself outperformed in general by more problem-specific techniques. Since the FNE is compatible with most multimodal pipelines based on CNN embeddings, as future work of this paper we intend to evaluate the performance of the FNE when integrated into the current state-of-the-art on image annotation (W2VV [18] ) and image retrieval (FV [4] ).",
  "y": "differences"
 },
 {
  "id": "e90c9a93ec445a636fcee924306d95_24",
  "x": "When compared to the current state-of-the-art, the results obtained by the FN-MME are significantly less competitive than problem-specific methods. Since this happens for all models using the same pipeline (CNN-MME, CNN-MME \u2020, CNN-MME*), these results indicate that the original architecture of <cite>Kiros et al. [1]</cite> is itself outperformed in general by more problem-specific techniques. Since the FNE is compatible with most multimodal pipelines based on CNN embeddings, as future work of this paper we intend to evaluate the performance of the FNE when integrated into the current state-of-the-art on image annotation (W2VV [18] ) and image retrieval (FV [4] ).",
  "y": "future_work"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_0",
  "x": "In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008) . For constituent-based parsing using the Chinese Treebank (CTB), <cite>Wang et al. (2006)</cite> have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.'s approach.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_1",
  "x": "For constituent-based parsing using the Chinese Treebank (CTB), <cite>Wang et al. (2006)</cite> have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.'s approach. We apply the same shift-reduce procedure as <cite>Wang et al. (2006)</cite> , but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses.",
  "y": "motivation"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_2",
  "x": "For constituent-based parsing using the Chinese Treebank (CTB), <cite>Wang et al. (2006)</cite> have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.'s approach. We apply the same shift-reduce procedure as <cite>Wang et al. (2006)</cite> , but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses.",
  "y": "differences uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_3",
  "x": "The shift-reduce process used by our beam-search decoder is based on the greedy shift-reduce parsers of Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> . The process assumes binary-branching trees; section 2.1 explains how these are obtained from the arbitrary-branching trees in the Chinese Treebank. The input is assumed to be segmented and POS tagged, and the word-POS pairs waiting to be processed are stored in a queue.",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_4",
  "x": "\u2022 TERMINATE, which pops the root node off the stack and ends parsing. This action is novel in our parser. Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_5",
  "x": "Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node. However, there are a small number of sentences (14 out of 3475 from the training data) that have unary-branching roots. For these sentences, Wang's parser will be unable to produce the unary-branching roots because the parsing process terminates as soon as the root is found. We define a separate action to terminate parsing, allowing unary reduces to be applied to the root item before parsing finishes.",
  "y": "extends"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_6",
  "x": "Note also that, since the parser is building binary trees, the X label in the REDUCE rules can be one of the temporary constituent labels, such as NP * , which are needed for the binarization process described in Section 2.1. Hence the number of left and right binary reduce rules is the number of constituent labels in the binarized grammar. <cite>Wang et al. (2006)</cite> give a detailed example showing how a segmented and POS-tagged sentence can be incrementally processed using the shift-reduce actions to produce a binary tree.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_7",
  "x": "Hence the number of left and right binary reduce rules is the number of constituent labels in the binarized grammar. <cite>Wang et al. (2006)</cite> give a detailed example showing how a segmented and POS-tagged sentence can be incrementally processed using the shift-reduce actions to produce a binary tree. We show this example in Figure 1 .",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_8",
  "x": "In the deterministic parser of <cite>Wang et al. (2006)</cite> , the highest scoring action predicted by the classifier may prevent a valid binary tree from being built. In this case, Wang et al. simply return a partial parse consisting of all the subtrees on the stack. In our parser a set of restrictions is applied which guarantees a valid parse tree.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_9",
  "x": "When the corresponding node is a terminal, c represents its POS-tag, whereas when the corresponding node is non-terminal, c represents its constituent label; t represents the POS-tag for a word. The context S 0 , S 1 , S 2 , S 3 and N 0 , N 1 , N 2 , N 3 for the feature templates is taken from <cite>Wang et al. (2006)</cite> . However, <cite>Wang et al. (2006)</cite> used a polynomial kernel function with an SVM and did not manually create feature combinations.",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_10",
  "x": "The context S 0 , S 1 , S 2 , S 3 and N 0 , N 1 , N 2 , N 3 for the feature templates is taken from <cite>Wang et al. (2006)</cite> . However, <cite>Wang et al. (2006)</cite> used a polynomial kernel function with an SVM and did not manually create feature combinations. Since we used the linear perceptron algorithm we manually combined Unigram features into Bigram and Trigram features.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_11",
  "x": "However, <cite>Wang et al. (2006)</cite> used a polynomial kernel function with an SVM and did not manually create feature combinations. Since we used the linear perceptron algorithm we manually combined Unigram features into Bigram and Trigram features. The \"Bracket\" row shows bracket-related features, which were inspired by <cite>Wang et al. (2006)</cite> .",
  "y": "differences"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_12",
  "x": "The \"Bracket\" row shows bracket-related features, which were inspired by <cite>Wang et al. (2006)</cite> . Here brackets refer to left brackets including \"\uff08\", \"\"\" and \"\u300a\" and right brackets including \"\uff09\", \"\"\" and \"\u300b\". In the table, b represents the matching status of the last left bracket (if any) on the stack.",
  "y": "similarities"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_13",
  "x": "There are around a million features in our experiments with the CTB2 dataset. <cite>Wang et al. (2006)</cite> used a range of other features, including rhythmic features of S 0 and S 1 (Sun and Jurafsky, 2003) , features from the most recently found node that is to the left or right of S 0 and S 1 , the number of words and the number of punctuations in S 0 and S 1 , the distance between S 0 and S 1 and so on. We did not include these features in our parser, because they did not lead to improved performance during development experiments.",
  "y": "background"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_14",
  "x": "<cite>Wang et al. (2006)</cite> used a range of other features, including rhythmic features of S 0 and S 1 (Sun and Jurafsky, 2003) , features from the most recently found node that is to the left or right of S 0 and S 1 , the number of words and the number of punctuations in S 0 and S 1 , the distance between S 0 and S 1 and so on. We did not include these features in our parser, because they did not lead to improved performance during development experiments. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_15",
  "x": "The rows represent the model from Bikel and Chiang (2000) , Bikel (2004) , the SVM and ensemble models from <cite>Wang et al. (2006)</cite> , and our parser, respectively. The accuracy of our parser is competitive using this test set. The results of various models using automatically assigned POS-tags are shown in Table 4 .",
  "y": "similarities"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_16",
  "x": "The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003) , Xiong et al. (2005) , Bikel (2004), Chiang and Bikel (2002) , the SVM model from <cite>Wang et al. (2006)</cite> and the ensemble system from <cite>Wang et al. (2006)</cite> , and the parser of this paper, respectively. Our parser gave comparable accuracies to the SVM and ensemble models from <cite>Wang et al. (2006)</cite> . However, comparison with Table 3 shows that our parser is more sensitive to POS-tagging errors than some of the other models.",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_17",
  "x": "Our parser gave comparable accuracies to the SVM and ensemble models from <cite>Wang et al. (2006)</cite> . However, comparison with Table 3 shows that our parser is more sensitive to POS-tagging errors than some of the other models. One possible reason is that some of the other parsers, e.g. Bikel (2004) , use the parser model itself to resolve tagging ambiguities, whereas we rely on a POS tagger to accurately assign a single tag to each word.",
  "y": "similarities"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_18",
  "x": "The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003) , Xiong et al. (2005) , Bikel (2004), Chiang and Bikel (2002) , the SVM model from <cite>Wang et al. (2006)</cite> and the ensemble system from <cite>Wang et al. (2006)</cite> , and the parser of this paper, respectively. Our parser gave comparable accuracies to the SVM and ensemble models from <cite>Wang et al. (2006)</cite> . However, comparison with Table 3 shows that our parser is more sensitive to POS-tagging errors than some of the other models.",
  "y": "differences"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_19",
  "x": "**RELATED WORK** Our parser is based on the shift-reduce parsing process from Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> , and therefore it can be classified as a transition-based parser (Nivre et al., 2006 ). An important difference between our parser and the <cite>Wang et al. (2006)</cite> parser is that our parser is based on a discriminative learning model with global features, whilst the parser from <cite>Wang et al. (2006)</cite> is based on a local classifier that optimizes each individual choice.",
  "y": "uses"
 },
 {
  "id": "e9a7e0d6d09fb2a2dd1972d6d16682_20",
  "x": "Our parser is based on the shift-reduce parsing process from Sagae and Lavie (2005) and <cite>Wang et al. (2006)</cite> , and therefore it can be classified as a transition-based parser (Nivre et al., 2006 ). An important difference between our parser and the <cite>Wang et al. (2006)</cite> parser is that our parser is based on a discriminative learning model with global features, whilst the parser from <cite>Wang et al. (2006)</cite> is based on a local classifier that optimizes each individual choice. Instead of greedy local decoding, we used beam search in the decoder.",
  "y": "differences"
 },
 {
  "id": "e9b2f32ed29589b4a6d49d3b30fc3a_0",
  "x": "**OVERVIEW** Morphologically rich languages like Arabic (Beesley, K. 1996 ) present significant challenges to many natural language processing applications as the one described above because a word often conveys complex meanings decomposable into several morphemes (i.e. prefix, stem, suffix) . By segmenting words into morphemes, we can improve the performance of natural language systems including machine translation <cite>(Brown et al. 1993</cite> ) and information retrieval (Franz, M. and McCarley, S. 2002) .",
  "y": "extends motivation"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_0",
  "x": "**INTRODUCTION** Neural network-based encoder-decoder models are cutting-edge methodologies for tackling natural language generation (NLG) tasks, i.e., machine translation (Cho et al., 2014) , image captioning (Vinyals et al., 2015) , video description (Venugopalan et al., 2015) , and headline generation (<cite>Rush et al., 2015</cite>) . This paper also shares a similar goal and motivation to <cite>previous work</cite>: improving the encoderdecoder models for natural language generation.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_1",
  "x": "However, to the best of our knowledge, there is no clear evidence that syntactic and semantic information can enhance the recently developed encoder-decoder models in NLG tasks. To answer this research question, this paper proposes and evaluates a headline generation method based on an encoder-decoder architecture on Abstract Meaning Representation (AMR). The method is essentially an extension of <cite>attention-based summarization</cite> (<cite>ABS</cite>) (<cite>Rush et al., 2015</cite>) .",
  "y": "extends"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_2",
  "x": "To answer this research question, this paper proposes and evaluates a headline generation method based on an encoder-decoder architecture on Abstract Meaning Representation (AMR). The method is essentially an extension of <cite>attention-based summarization</cite> (<cite>ABS</cite>) (<cite>Rush et al., 2015</cite>) . Our proposed method encodes results obtained from an AMR parser by using a modified version of Tree-LSTM encoder (Tai et al., 2015) as additional information of the <cite>baseline ABS model</cite>.",
  "y": "extends"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_5",
  "x": "**<cite>ATTENTION-BASED SUMMARIZATION (ABS)</cite>** <cite>ABS</cite> proposed in <cite>Rush et al. (2015)</cite> has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007) . Figure 1 illustrates the model structure of <cite>ABS</cite>.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_6",
  "x": "Figure 1 illustrates the model structure of <cite>ABS</cite>. <cite>The model</cite> predicts a word sequence (summary) based on the combination of the neural network language model and an input sentence encoder. Let V be a vocabulary.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_7",
  "x": "<cite>ABS</cite> proposed in <cite>Rush et al. (2015)</cite> has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007) . Figure 1 illustrates the model structure of <cite>ABS</cite>. <cite>The model</cite> predicts a word sequence (summary) based on the combination of the neural network language model and an input sentence encoder.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_8",
  "x": "We assume a one-hot vector for a special start symbol, such as \"\u27e8S\u27e9\", when i < 1. Then, <cite>ABS</cite> outputs a summary\u0176 given an input sentence X as follows: where nnlm(Y C,i ) is a feed-forward neural network language model proposed in (Bengio et al., 2003) , and enc(X, Y C,i ) is an input sentence encoder with attention mechanism.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_9",
  "x": "---------------------------------- **EXPERIMENTS** To demonstrate the effectiveness of our proposed method, we conducted experiments on benchmark data of the abstractive headline generation task described in <cite>Rush et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_11",
  "x": "The training data was obtained from the first sentence and the headline of a document in the annotated <cite>Gigaword</cite> corpus (Napoles et al., 2012) 4 . The development data is DUC-2003 data, and test data are both DUC-2004 (Over et al., 2007) and sentence-headline pairs obtained from the annotated <cite>Gigaword</cite> corpus as well as training data 5 . All of the generated headlines were evaluated by ROUGE (Lin, 2004) 6 .",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_12",
  "x": "The training data was obtained from the first sentence and the headline of a document in the annotated <cite>Gigaword</cite> corpus (Napoles et al., 2012) 4 . The development data is DUC-2003 data, and test data are both DUC-2004 (Over et al., 2007) and sentence-headline pairs obtained from the annotated <cite>Gigaword</cite> corpus as well as training data 5 . All of the generated headlines were evaluated by ROUGE (Lin, 2004) 6 .",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_13",
  "x": "All of the generated headlines were evaluated by ROUGE (Lin, 2004) 6 . For evaluation on DUC-2004, we removed strings after 75-characters for each generated headline as described in the DUC-2004 evaluation. For evaluation on <cite>Gigaword</cite>, we forced the system outputs to be at most 8 words as in <cite>Rush et al. (2015)</cite> since the average length of headline in <cite>Gigaword</cite> is 8.3 words.",
  "y": "uses similarities"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_14",
  "x": "For the preprocessing for all data, all letters were converted to lower case, all digits were replaced with '#', and words appearing less than five times with 'UNK'. Note that, for further evaluation, we prepared 2,000 sentence-headline pairs randomly sampled from the test data section of the <cite>Gigaword</cite> corpus as our additional test data. In our experiments, we refer to the baseline neural attention-based abstractive summarization method described in <cite>Rush et al. (2015)</cite> as \"<cite>ABS</cite>\", and our proposed method of incorporating AMR structural information by a neural encoder to the baseline method described in Section 3 as \"<cite>ABS</cite>+AMR\".",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_15",
  "x": "In our experiments, we refer to the baseline neural attention-based abstractive summarization method described in <cite>Rush et al. (2015)</cite> as \"<cite>ABS</cite>\", and our proposed method of incorporating AMR structural information by a neural encoder to the baseline method described in Section 3 as \"<cite>ABS</cite>+AMR\". Additionally, we also evaluated the performance of the AMR encoder without the attention mechanism, which we refer to as \"<cite>ABS</cite>+AMR(w/o attn)\", to investigate the contribution of the attention mechanism on the AMR encoder. For the parameter estimation (training), we used stochastic gradient descent to learn parameters.",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_17",
  "x": "For <cite>ABS</cite>+AMR, we used the two-step training scheme to accelerate the training speed. The first phase learns the parameters of the <cite>ABS</cite>. The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline <cite>ABS</cite> were fixed and unchanged to prevent overfitting.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_18",
  "x": "We re-normalized the embedding after each epoch (Hinton et al., 2012) . For <cite>ABS</cite>+AMR, we used the two-step training scheme to accelerate the training speed. The first phase learns the parameters of the <cite>ABS</cite>.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_19",
  "x": "The first phase learns the parameters of the <cite>ABS</cite>. The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline <cite>ABS</cite> were fixed and unchanged to prevent overfitting. Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_20",
  "x": "Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset. <cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets.",
  "y": "background"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_21",
  "x": "Table 1 shows the recall of ROUGE (Lin, 2004 ) on each dataset. <cite>ABS</cite> (re-run) represents the performance of <cite>ABS</cite> re-trained by the distributed scripts 7 . We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets.",
  "y": "differences"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_22",
  "x": "In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004. However in contrast, we observed that the improvements on <cite>Gigaword</cite> (the same test data as <cite>Rush et al. (2015)</cite> ) seem to be limited compared with the DUC-2004 dataset. We assume that this limited gain is caused largely by the quality of AMR parsing results.",
  "y": "differences"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_23",
  "x": "We can see that the proposed method, <cite>ABS</cite>+AMR, outperforms the baseline <cite>ABS</cite> on all datasets. In particular, <cite>ABS</cite>+AMR achieved statistically significant gain from <cite>ABS</cite> (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004. However in contrast, we observed that the improvements on <cite>Gigaword</cite> (the same test data as <cite>Rush et al. (2015)</cite> ) seem to be limited compared with the DUC-2004 dataset.",
  "y": "similarities uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_26",
  "x": "G: high-end retailers are scouting sites in brooklyn A: new yorkers are poised to go mainstream with chic P: new york city is poised to go mainstream chic Figure 3 : Examples of generated headlines on <cite>Gigaword</cite>. I: input, G: true headline, A: <cite>ABS</cite> (re-run), and P: <cite>ABS</cite>+AMR. <cite>Gigaword</cite> test data provided by <cite>Rush et al. (2015)</cite> is already pre-processed.",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_27",
  "x": "<cite>Gigaword</cite> test data provided by <cite>Rush et al. (2015)</cite> is already pre-processed. Therefore, the quality of the AMR parsing results seems relatively worse on this pre-processed data since, for example, many low-occurrence words in the data were already replaced with \"UNK\". To provide evidence of this assumption, we also evaluated the performance on our randomly selected 2,000 sentence-headline test data also taken from the test data section of the annotated <cite>Gigaword</cite> corpus.",
  "y": "uses"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_29",
  "x": "We found the statistical difference between <cite>ABS</cite>(re-run) and <cite>ABS</cite>+AMR on ROUGE-1 and ROUGE-2. We can also observe that <cite>ABS</cite>+AMR achieved the best ROUGE-1 scores on all of the test data. According to this fact, <cite>ABS</cite>+AMR tends to successfully yield semantically important words.",
  "y": "differences"
 },
 {
  "id": "eb51af7d0487fc0795616aecfae9fb_35",
  "x": "Recently, the Recurrent Neural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of <cite>Rush et al. (2015)</cite> : the combination of the feed-forward neural network language model and attention-based sentence encoder. also adapted the RNN encoder-decoder with attention for headline generation tasks. Moreover, they made some efforts such as hierarchical attention to improve the performance.",
  "y": "background"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_0",
  "x": "This paper describes our submission to Shared Task on Similar Language Translation in Fourth Conference on Machine Translation (WMT 2019). We submitted three systems for Hindi \u2192 Nepali direction in which we have examined the performance of a Recursive Neural Network (RNN) based Neural Machine Translation (NMT) system, a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by <cite>(Artetxe et al., 2017)</cite> and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_1",
  "x": "This task basically focuses on how to improve performance for languages which are similar but resource scarce. There are many language pairs for which parallel data does not exist or exist in a very small amount. In past, to improve the performance of NMT systems various techniques like Back-Translation (Sennrich et al., 2016a) , utilizing other similar language pairs through pivoting (Cheng et al., 2017) or transfer learning (Zoph et al., 2016) , complete unsupervised architectures <cite>(Artetxe et al., 2017)</cite> (Lample et al., 2018 ) and many others have been proposed.",
  "y": "background"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_2",
  "x": "In (Burlot and Yvon, 2018) , it is claimed that quality of back-translated sentences is important. Recently many systems have been proposed for Unsupervised NMT, where only monolingual data is utilized. The Unsupervised NMT approach proposed in <cite>(Artetxe et al., 2017)</cite> follows an architecture where encoder is shared and decoder is separate for each language.",
  "y": "background"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_3",
  "x": "To train this we have utilized only parallel corpora. Second system is trained using a semi-supervised NMT system where monolingual data from both languages is utilized. We have utilized architecture proposed in <cite>(Artetxe et al., 2017)</cite> where encoder is shared and decoders are separate for each language and model is trained by alternating between denoising and back-translation.",
  "y": "uses"
 },
 {
  "id": "ecdd75533aff56771f0320694efc9a_4",
  "x": "In this way the amount of available data becomes three times of the original data. All the data is combined together, shuffled and then provided to the NMT system, there is no identification provided to distinguish between parallel data and copy data. To train all three systems we have utilized the implementation of <cite>(Artetxe et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_0",
  "x": "---------------------------------- **INTRODUCTION** Several papers recently demonstrated the potential of very weakly supervised or entirely unsupervised approaches to bilingual dictionary induction (BDI) (Barone, 2016; Artetxe et al., 2017; Zhang et al., 2017; <cite>Conneau et al., 2018</cite>; , the task of identifying translational equivalents across two languages.",
  "y": "background"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_2",
  "x": "Successful unsupervised or weakly supervised alignment of word vector spaces would remove much of the data bottleneck for machine translation and push horizons for cross-lingual learning . In addition to an unsupervised approach to aligning monolingual word embedding spaces with adversarial training,<cite> Conneau et al. (2018)</cite> present a supervised alignment algorithm that assumes a gold-standard seed dictionary and performs Procrustes Analysis (Sch\u00f6nemann, 1966) . show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i.e. words with identical spelling across source and target language, is superior to the completely unsupervised approach.",
  "y": "background"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_3",
  "x": "show that this approach, weakly supervised with a dictionary seed of crosslingual homographs, i.e. words with identical spelling across source and target language, is superior to the completely unsupervised approach. We therefore focus on weakly-supervised Procrustes Analysis (PA) for BDI here. The implementation of PA in<cite> Conneau et al. (2018)</cite> yields notable improvements over earlier work on BDI, even though it learns a simple linear transform of the source language space into the target language space.",
  "y": "background"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_4",
  "x": "The relative success of the simple linear approach can be explained in terms of isomorphism across monolingual semantic spaces, 1 an idea that receives support from cognitive science (Youn et al., 1999) . Word vector spaces are not perfectly isomorphic, however, as shown by , who use a Laplacian graph similarity metric to measure this property. In this work, we show that projecting both source and target vector spaces into a third space (Faruqui and Dyer, 2014) , using a variant of PA known as Generalized Procrustes Analysis (Gower, 1975) , makes it easier to learn the alignment between two word vector spaces, as compared to the single linear transform used in<cite> Conneau et al. (2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_5",
  "x": "---------------------------------- **EXPERIMENTS** In our experiments, we generally use the same hyper-parameters as used in<cite> Conneau et al. (2018)</cite> , unless otherwise stated.",
  "y": "uses similarities"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_6",
  "x": "**EXPERIMENTS** In our experiments, we generally use the same hyper-parameters as used in<cite> Conneau et al. (2018)</cite> , unless otherwise stated. When extracting dictionaries for the bootstrapping procedure, we use cross-domain local scaling (CSLS, see<cite> Conneau et al. (2018)</cite> for details) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15,000.",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_7",
  "x": "We do not put any restrictions on the initial seed dictionaries, based on cross-lingual homographs: those vary considerably in size, from 17,012 for Hebrew to 85,912 for Spanish. Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in<cite> Conneau et al. (2018)</cite> , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS. Our metric is Precision at k\u00d7100 (P@k), i.e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set<cite> (Conneau et al., 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_8",
  "x": "Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in<cite> Conneau et al. (2018)</cite> , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS. Our metric is Precision at k\u00d7100 (P@k), i.e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set<cite> (Conneau et al., 2018)</cite> . Unless stated otherwise, experiments were carried out using the publicly available pre-trained fastText embeddings, trained on Wikipedia data, 5 and bilingual dictionaries-consisting of 5000 and 1500 unique word pairs for training and testing, respectively-provided by<cite> Conneau et al. (2018)</cite> 6 .",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_9",
  "x": "Unless stated otherwise, experiments were carried out using the publicly available pre-trained fastText embeddings, trained on Wikipedia data, 5 and bilingual dictionaries-consisting of 5000 and 1500 unique word pairs for training and testing, respectively-provided by<cite> Conneau et al. (2018)</cite> 6 . ---------------------------------- **COMPARISON OF PA AND GPA**",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_10",
  "x": "Common benchmarks For a more extensive comparison with previous work, we include results on English-{Finnish, German, Italian} dictionaries used in<cite> Conneau et al. (2018)</cite> and Artetxe et al. (2018) -the second best approach to BDI known to us, which also uses Procrustes Analysis. We conduct experiments using three forms of supervision: gold-standard seed dictionaries of 5000 word pairs, cross-lingual homographs, and numerals. We use train and test bilingual dictionaries from Dinu et al. (2015) for English-Italian and from Artetxe et al. (2017) for English-{Finnish, German}. Following<cite> Conneau et al. (2018)</cite> , we report results with a set of CBOW embeddings trained on the WaCky corpus (Barone, 2016) , and with Wikipedia embeddings.",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_11",
  "x": "We use train and test bilingual dictionaries from Dinu et al. (2015) for English-Italian and from Artetxe et al. (2017) for English-{Finnish, German}. Following<cite> Conneau et al. (2018)</cite> , we report results with a set of CBOW embeddings trained on the WaCky corpus (Barone, 2016) , and with Wikipedia embeddings. Results are reported in Table 3 . We observe that GPA outperforms PA consistently on Italian and German with the WaCky embeddings, and on all languages with the Wikipedia embeddings.",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_13",
  "x": "To explore the latter issue and to further compare the capabilities of PA and GPA, we perform a Procrustes fit test, where we learn alignments in a fully supervised fashion, using the test dictionaries of<cite> Conneau et al. (2018)</cite> 9 for both training and evaluation 10 . In the ideal case, i.e. if the subspaces defined by the words in the seed dictionaries are perfectly alignable, this setup should result in precision of 100%. We found the difference between the fit with PA and GPA to be negligible, 0.20 on average across all 10 languages (5 low-resource and 5 high-source languages).",
  "y": "uses"
 },
 {
  "id": "edfce6b99a4804c0908b39ea38d707_14",
  "x": "In most recent work, this mapping is constrained to be orthogonal and solved using Procrustes Analysis (Xing et al., 2015; Artetxe et al., 2017 Artetxe et al., , 2018 <cite>Conneau et al., 2018</cite>; Lu et al., 2015) . The approach most similar to ours, Faruqui and Dyer (2014) , uses canonical correlation analysis (CCA) to project both source and target language spaces into a third, joint space. In this setup, similarly to GPA, the third space is iteratively updated, such that at timestep t, it is a product of the two language spaces as transformed by the mapping learned at timestep t \u2212 1.",
  "y": "background"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_0",
  "x": "However, since new named entities arise regularly, it becomes increasingly difficult to maintain an up-to-date dictionary and/or adapt a named entity classifier to a new domain; for example, sequence labeling techniques that use feature templates (Finkel et al., 2005; Sarawagi and Cohen, 2004) are not robust for unknown named entities because their feature space is very sparse<cite> (Primadhanty et al., 2015)</cite> . This problem worsens when we attempt to use a combination of features for sparse named entity classification. Therefore, in this paper, we propose the use of matrix factorization for named entity classification to consider the relationships between sparse features.",
  "y": "background"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_1",
  "x": "However, since new named entities arise regularly, it becomes increasingly difficult to maintain an up-to-date dictionary and/or adapt a named entity classifier to a new domain; for example, sequence labeling techniques that use feature templates (Finkel et al., 2005; Sarawagi and Cohen, 2004) are not robust for unknown named entities because their feature space is very sparse<cite> (Primadhanty et al., 2015)</cite> . Therefore, in this paper, we propose the use of matrix factorization for named entity classification to consider the relationships between sparse features.",
  "y": "motivation"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_2",
  "x": "These studies heavily rely on feature templates for learning combinations of features; however, since combinations of features in conventional supervised learning are treated independently, this approach is not robust for named entities that do not appear in the training data. To address the task of unknown named entity classification, <cite>Primadhanty et al. (2015)</cite> explored the use of sparse combinatorial features. They proposed a log-bilinear model that defines a score function considering interactions between features; the score function is regularized via a nuclear norm on a feature weight matrix.",
  "y": "background"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_3",
  "x": "**EXPERIMENTS** As described above, we aim to classify named entities that rarely appear in a given training corpus. We compared factorization machines with a log-linear model, a polynomial-kernel SVM, and a state-ofthe-art log-bilinear model using nuclear norm for regularization<cite> (Primadhanty et al., 2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_4",
  "x": "Data. We used the dataset provided by <cite>Primadhanty et al. (2015)</cite> ; this dataset was created for evaluating unknown named entity classification and is context features: Right and left contexts of the candidate in a sentence (do not take the order into account). cap=1, cap=0: Whether the first letter of the candidate is uppercase, or not. all-low=1, all-low=0: Whether all letters of the candidate are lowercase, or not.",
  "y": "uses"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_5",
  "x": "We used a subset of features from experiments performed by <cite>Primadhanty et al. (2015)</cite> . Table 3 summarizes the features used in our experiment, including context and entity features. Tools.",
  "y": "uses"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_6",
  "x": "Table 2 presents results of our experiments. Note that <cite>Primadhanty et al. (2015)</cite> used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use. Table 4 and Figure 2 show the performance and precision-recall curves of named entity classification for each tag, respectively.",
  "y": "differences"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_7",
  "x": "We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by <cite>Primadhanty et al. (2015)</cite> with fewer features. Overall, the microaveraged F1 score improved by 1.4 points. From these results, we conclude that unknown named entity classification can be successfully achieved by taking combinatorial features into account using factorization machines.",
  "y": "similarities"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_8",
  "x": "The accuracy of LOC, however, was lower than that of the log-bilinear model<cite> (Primadhanty et al., 2015)</cite> . Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER. We therefore conclude here that clustering and POS features are necessary to distinguish these tags.",
  "y": "differences"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_9",
  "x": "Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER. We therefore conclude here that clustering and POS features are necessary to distinguish these tags. Figure 1 plots the F1-score of our proposed method as dimension k changes for matrix factor- ization using the same development data as that of <cite>Primadhanty et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_10",
  "x": "It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points. This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization. Both our approach and the methods of <cite>Primadhanty et al. (2015)</cite> address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized.",
  "y": "similarities differences"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_11",
  "x": "Both our approach and the methods of <cite>Primadhanty et al. (2015)</cite> address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. <cite>Primadhanty et al. (2015)</cite> use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines. ----------------------------------",
  "y": "background"
 },
 {
  "id": "eee36102d3feac0f673cd33562d40f_12",
  "x": "This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization. Both our approach and the methods of <cite>Primadhanty et al. (2015)</cite> address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. <cite>Primadhanty et al. (2015)</cite> use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines.",
  "y": "differences"
 },
 {
  "id": "f1a800c7cd47ac2edf2172cedb5889_0",
  "x": "Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by <cite>Tsarfaty et al. (2010</cite> Tsarfaty et al. ( , 2013 : \u2022 Can we represent words abstractly so as to reflect shared morphological aspects between them? \u2022 Which types of morphological information should we include in the parsing model? It is tempting to hypothesize that character-level models effectively solve the first problem.",
  "y": "motivation"
 },
 {
  "id": "f1a800c7cd47ac2edf2172cedb5889_1",
  "x": "For the second, <cite>Tsarfaty et al. (2010)</cite> and Seeker and Kuhn (2013) reported that morphological case is beneficial across morphologically rich languages with extensive case systems, where case syncretism is pervasive and often hurts parsing performance. But <cite>these studies</cite> focus on vintage parsers; do neural parsers with character-level representations also solve this second problem? We attempt to answer this question by asking whether an explicit model of morphological case helps dependency parsing, and our results show that it does. Furthermore, a pipeline model in which we feed predicted case to the parser outperforms multi-task learning in which case prediction is an auxiliary task.",
  "y": "background motivation"
 },
 {
  "id": "f1a800c7cd47ac2edf2172cedb5889_2",
  "x": "We found that predicted case improves accuracy, although the effect is different across languages. These results are interesting, since in vintage parsers, predicted case usually harmed accuracy (<cite>Tsarfaty et al., 2010</cite>) . However, we note that our taggers use gold POS, which might help.",
  "y": "background"
 },
 {
  "id": "f1a800c7cd47ac2edf2172cedb5889_3",
  "x": "We found that predicted case improves accuracy, although the effect is different across languages. These results are interesting, since in vintage parsers, predicted case usually harmed accuracy (<cite>Tsarfaty et al., 2010</cite>) . However, we note that our taggers use gold POS, which might help.",
  "y": "differences"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_0",
  "x": "This paper gives an Abstract Categorial Grammar (ACG) account of<cite> (Kallmeyer and Kuhlmann, 2012)</cite>'s process of transformation of the derivation trees of Tree Adjoining Grammar (TAG) into dependency trees. We make explicit how the requirement of keeping a direct interpretation of dependency trees into strings results into lexical ambiguity. Since the ACG framework has already been used to provide a logical semantics from TAG derivation trees, we have a unified picture where derivation trees and dependency trees are related but independent equivalent ways to account for the same surface-meaning relation.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_1",
  "x": "Solving these problems often leads to modifications of derivation tree structures (Schabes and Shieber, 1994; Kallmeyer, 2002; Joshi et al., 2003; Rambow et al., 2001; Chen-Main and Joshi, To appear) . While alternative proposals have succeeded in linking derivation trees to semantic representations using unification (Kallmeyer and Romero, 2004; Kallmeyer and Romero, 2007) or using an encoding (Pogodalla, 2004; Pogodalla, 2009) of TAG into the ACG framework (de Groote, 2001) , only recently<cite> (Kallmeyer and Kuhlmann, 2012)</cite> has proposed a transformation from standard derivation trees to dependency trees. This paper provides an ACG perspective on this transformation.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_2",
  "x": "The goal is twofold. First, it exhibits the underlying lexical blow up of the yield functions associated with the elementary trees in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> . Second, using the same framework as (Pogodalla, 2004; Pogodalla, 2009 ) allows us to have a shared perspective on a phrase-structure architecture and a dependency one and an equivalence on the surface-meaning relation they define.",
  "y": "motivation background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_3",
  "x": "2 We refer the reader to (Pogodalla, 2009 ) for the details. 3 The TAG literature typically uses this example, and<cite> (Kallmeyer and Kuhlmann, 2012)</cite> as well, to show the mismatch between the derivation trees and the expected se- This sentence is usually analyzed in TAG with a derivation tree where the to love component scopes over all the other arguments, and where claims and seems are unrelated, as Fig. 2(a) shows. The three higher-order signatures are: \u03a3 der\u03b8 : Its atomic types include s, vp, np, s A , vp A . . . where the X types stand for the categories X of the nodes where a substitution can occur while the X A types stand for the categories X of the nodes where an adjunction can occur.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_4",
  "x": "bol and L yield (X 0 ) = X. Then, the derivation tree, the derived tree, and the yield of Fig. 2 are represented by: Trees<cite> (Kallmeyer and Kuhlmann, 2012)</cite> 's process to translate derivation trees into dependency trees is a two-step process. The first one does the actual transformation, using macro-tree transduction, while the second one modifies the way to get the yield from the dependency trees rather than from the derivation ones. ----------------------------------",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_5",
  "x": "This transformation aims at modeling the differences in scope of the argument between the derivation tree for (1) shown in Fig. 2 (a) and the corresponding dependency tree shown in Fig. 2 (b). For instance, in the derivation trees, claims and seems are under the scope of to love while in the dependency tree this order is reversed. According to<cite> (Kallmeyer and Kuhlmann, 2012)</cite> , such edge reversal is due to the fact that an edge between a complement taking adjunction (CTA) and an initial tree has to be reversed, while the other edges remain unchanged.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_6",
  "x": "For instance, in the derivation trees, claims and seems are under the scope of to love while in the dependency tree this order is reversed. According to<cite> (Kallmeyer and Kuhlmann, 2012)</cite> , such edge reversal is due to the fact that an edge between a complement taking adjunction (CTA) and an initial tree has to be reversed, while the other edges remain unchanged. Moreover, in case an initial tree accepts several adjunction of CTAs,<cite> (Kallmeyer and Kuhlmann, 2012)</cite> hypothesizes that the farther from the head a CTA is, the higher it is in the dependency tree.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_7",
  "x": "In order to do such reversing operations,<cite> (Kallmeyer and Kuhlmann, 2012)</cite> uses Macro Tree Transducers (MTTs) (Engelfriet and Vogler, 1985) . Note that the MTTs they use are linear, i.e. non-copying. It means that any node of an input tree cannot be translated more than once.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_8",
  "x": "The yield of the derived tree resulting from the operations of the derivation tree \u03b3 of Fig. 3 defined in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> , w 2 where x, y denotes a tuple of strings. Because of the adjunction, the corresponding dependency structure has a reverse order (\u03b3 = \u03b3 a (\u03b3 i )), the requirement on yield dep imposes that",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_9",
  "x": "Were we not interested in the yields but only in the dependency structures, we wouldn't have to manage this ambiguity. This is true both for<cite> (Kallmeyer and Kuhlmann, 2012)</cite> 's approach and ours. But as we have here a unified framework for the two-step process they propose, this lexical blow up will result in a multiplicity of types as Section 5 shows.",
  "y": "similarities background motivation"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_10",
  "x": "In order to encode the MTT acting on derivation trees, we introduce a new abstract vocabulary \u03a3 der\u03b8 for disambiguated derivation trees as in (Yoshinaka, 2006 to love is used to model sentences where both adjunctions are performed into \u03b3 to love . C 10 to love and C 01 to love are used for sentences where only one adjunction at the s or at the vp node occurs respectively. C 00 to love : np np s is used when no adjunction occurs. 6 This really mimics (Yoshinaka, 2006) 's encoding of<cite> (Kallmeyer and Kuhlmann, 2012)</cite> MTT rules: . . . are designed in order to indicate that a given adjunction has n adjunctions above it (i.e. which scope over it).",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_11",
  "x": "In this paper, we have given an ACG perspective on the transformation of the derivation trees of TAG to the dependency trees proposed in<cite> (Kallmeyer and Kuhlmann, 2012)</cite> . Figure 4 illustrates the architecture we propose. This transformation is a two-step process using first a macrotree transduction then an interpretation of dependency trees as (tuples of) strings.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_12",
  "x": "The encoding of the second step then made explicit the lexical blow up for the interpretation of the functional symbols of the dependency trees in<cite> (Kallmeyer and Kuhlmann, 2012</cite> )'s construct. It also provides a push out (in the categorical sense) of the two morphisms from the disambiguated derivation trees to the derived trees and to the dependency trees. The diagram is completed with the yield function from the derived trees and from the dependency trees to the string vocabulary.",
  "y": "background"
 },
 {
  "id": "f1eae0918a246174b1866ba71d4efc_13",
  "x": "The encoding of the second step then made explicit the lexical blow up for the interpretation of the functional symbols of the dependency trees in<cite> (Kallmeyer and Kuhlmann, 2012</cite> )'s construct. It also provides a push out (in the categorical sense) of the two morphisms from the disambiguated derivation trees to the derived trees and to the dependency trees. The diagram is completed with the yield function from the derived trees and from the dependency trees to the string vocabulary.",
  "y": "background"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_0",
  "x": "In practice, e.g. in the context of a sophisticated voicemail front-end ) that is tightly integrated with an organization-wide voicemail system and private branch exchange (PBX), additional sources of information may be available: the voicemail system or the PBX might provide information about the originating station of a call, and speaker identification can be used to match a caller's voice against models of known callers ). Restricting our attention to voicemail transcripts means that our focus and goals are similar to those of <cite>Huang et al. (2001)</cite> , but the features and techniques we use are very different. While the present task may seem broadly similar to named entity extraction from broadcast news (Gotoh and Renals, 2000) , it is quite distinct from the latter: first, we are only interested in a small subset of the named entities; and second, the structure of the voicemail transcripts in our corpus is very different from broadcast news and certain aspects of this structure can be exploited for extracting caller names.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_1",
  "x": "<cite>Huang et al. (2001)</cite> discuss three approaches: hand-crafted rules; grammatical inference of subsequential transducers; and log-linear classifiers with bigram and trigram features used as taggers (Ratnaparkhi, 1996) . While the latter are reported to yield the best overall performance, the hand-crafted rules resulted in higher recall. Our phone number extractor is based on a two-phase procedure that employs a small hand-crafted component to propose candidate phrases, followed by a classifier that retains the desirable candidates.",
  "y": "background"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_2",
  "x": "We shall see that hand-crafted rules achieve very good recall, just as <cite>Huang et al. (2001)</cite> had observed, and the pruning phase successfully eliminates most undesirable candidates without affecting recall too much. Overall performance of our method is better than if we employ a log-linear model with trigram features. The success of the method proposed here is also due to the use of a rich set of features for candidate classification.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_3",
  "x": "These observations strongly suggest that when extracting caller phrases, positional cues should be taken into account. This is good news, especially since intrinsic features of the caller phrase may not be as reliable: a caller phrase is likely to contain names that are problematic for an automatic speech recognizer. While this is less of a problem when evaluating on manual transcriptions, the experience reported in<cite> (Huang et al., 2001)</cite> suggests that the relatively high error rate of speech recognizers may negatively affect performance of caller name extraction on automatically generated transcripts.",
  "y": "background"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_4",
  "x": "Since a direct comparison to the log-linear named entity tagger described in<cite> (Huang et al., 2001</cite> ) (we refer to this approach as HZP log-linear below) is not possible due to the use of different corpora and annotation standards, we applied a similar named entity tagger based on a log-linear model with trigram features to our data (we refer to this approach as Col log-linear as the tagger was provided by Michael Collins). Table 1 summarizes precision (P), recall (R), and F-measure (F) for three approaches evaluated on manual transcriptions: row HZP loglinear repeats the results of the best model from<cite> (Huang et al., 2001</cite> ); row Col log-linear contains the results we obtained using a similar named entity tagger on our own data; and row JA classifiers shows the performance of the classifier method proposed in this section. Like <cite>Huang et al. (2001)</cite> , we count a proposed caller phrase as correct if and only if it matches the annotation of the evaluation data perfectly.",
  "y": "uses"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_6",
  "x": "Like <cite>Huang et al. (2001)</cite> , we count a proposed caller phrase as correct if and only if it matches the annotation of the evaluation data perfectly. The numbers could be made to look better by using containment as the evaluation criterion, i.e., we would count a proposed phrase as correct if it contained an actual phrase plus perhaps some additional material. While this may be more useful in practice (see below), it is not the objective that was maximized during training, and so we prefer the stricter criterion for evaluation on previously annotated transcripts.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_7",
  "x": "While this may be more useful in practice (see below), it is not the objective that was maximized during training, and so we prefer the stricter criterion for evaluation on previously annotated transcripts. While the results for the approach proposed here appear clearly worse than those reported by <cite>Huang et al. (2001)</cite> , we hasten to point out that this is most likely not due to any difference in the corpora that were used. This is corroborated by the fact that we were able to obtain performance much closer to that of the best, finely tuned log-linear model from<cite> (Huang et al., 2001</cite> ) by using a generic named entity tagger that was not adapted in any way to the particular task at hand.",
  "y": "differences"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_8",
  "x": "While the results for the approach proposed here appear clearly worse than those reported by <cite>Huang et al. (2001)</cite> , we hasten to point out that this is most likely not due to any difference in the corpora that were used. This is corroborated by the fact that we were able to obtain performance much closer to that of the best, finely tuned log-linear model from<cite> (Huang et al., 2001</cite> ) by using a generic named entity tagger that was not adapted in any way to the particular task at hand. The log-linear taggers employ n-gram features based on family names and other particular aspects of the development data that do not necessarily generalize to other settings, where the family names of the callers may be different or may not be transcribed properly.",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_10",
  "x": "In fact, we count caller phrases as correct as long as they contain the full name of the caller, since this is the common denominator in the otherwise somewhat heterogeneous labeling of our training corpus; more on this issue in the next section. The difference between the approach in<cite> (Huang et al., 2001</cite> ) and ours may be partly due to the performance of the ASR components: <cite>Huang et al. (2001)</cite> report a word error rate of 'about 35%', whereas we used a recognizer (Bacchiani, 2001 ) with a word error rate of only 23%. Still, the reduced performance of the HZP model on ASR transcripts compared with manual transcripts is points toward overfitting, or reliance on features that do not generalize to ASR transcripts.",
  "y": "differences"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_11",
  "x": "This approach (which we call classify below) increases the precision of the combined two steps to acceptable levels without hurting recall too much. A comparison of performance results is presented in Table 4 . Rows HZP rules and HZP log-linear refer to the rule-based baseline and the best log-linear model of<cite> (Huang et al., 2001</cite> ) and the figures are simply taken from that paper; row Col log-linear refers to the same named entity tagger we used in the previous section and is included for comparison with the HZP models; row JA digits refers to the simple baseline where we extract strings of spoken digits of plausible lengths.",
  "y": "background"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_12",
  "x": "The results are summarized in Table 5 , which also repeats the best results from<cite> (Huang et al., 2001)</cite> , using the same terminology as earlier: rows HZP strict and HZP containment refer to the best model from<cite> (Huang et al., 2001</cite> ) -corresponding to row HZP log-linear in Table 4 -when evaluated using the strict criterion and containment, respectively; and row JA containment refers to our own best model -corresponding to row JA extract + classify in Ta It is not very plausible that the differences between the approaches in Table 5 would be due to a difference in the performance of the ASR components that generated the message transcripts. From inspecting our own data it is clear that ASR mistakes inside phone numbers are virtually absent, and we would expect the same to hold even of an automatic recognizer with an overall much higher word error rate. Also, for most phone numbers the labeling is uncontroversial, so we expect the corpora used by <cite>Huang et al. (2001)</cite> and ourselves to be extremely similar in terms of mark-up of phone numbers.",
  "y": "uses"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_13",
  "x": "Also, for most phone numbers the labeling is uncontroversial, so we expect the corpora used by <cite>Huang et al. (2001)</cite> and ourselves to be extremely similar in terms of mark-up of phone numbers. So the observed performance difference is most likely due to the difference in extraction methods. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_14",
  "x": "Our grammar-based extractor translates spoken numbers into such a numeric representation. \u2022 Our two-phase approach allows us to efficiently develop a simple extraction grammar for which the only requirement is high recall. This places less of a burden on the grammar developers than having to write an accurate set of rules like the baseline of<cite> (Huang et al., 2001</cite> ).",
  "y": "uses"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_15",
  "x": "\u2022 The combined performance of our simple extraction grammar and the second-phase classifier exceeded the performance of all other methods, including the current state of the art<cite> (Huang et al., 2001</cite> ). Our results point towards approaches that use a small inventory of features that have been tailored to specific tasks. Generic methods like the named entity tagger used by <cite>Huang et al. (2001)</cite> may not be the best tools for particular tasks; in fact, we do not expect the bigram and trigram features used by such taggers to be sufficient for accurately extracting phone numbers.",
  "y": "differences"
 },
 {
  "id": "f28720b1597ca1273303f3774167f8_16",
  "x": "Generic methods like the named entity tagger used by <cite>Huang et al. (2001)</cite> may not be the best tools for particular tasks; in fact, we do not expect the bigram and trigram features used by such taggers to be sufficient for accurately extracting phone numbers. We also believe that using all available lexical information for extracting caller information can easily lead to over-fitting, which can partly be avoid by not relying on names being transcribed correctly by an ASR component. In practice, determining the identity of a caller might have to take many diverse sources of information into account.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_0",
  "x": "**INTRODUCTION** Discourse structure annotations have been demonstrated to be of high utility for a number of NLP applications, including automatic text summarization (Marcu, 1998; Marcu, 1999; Cristea et al., 2005) , sentence compression<cite> (Sporleder and Lapata, 2005)</cite> , natural language generation (Prasad et al., 2005) and question answering (Verberne et al., 2006) . These annotations include sentence segmentation into discourse units along with the linking of discourse units, both within and across sentence boundaries, into a labeled hierarchical structure.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_1",
  "x": "Manual segmentation was primarily responsible for this performance boost over their fully automatic system, thus making the case that automatic discourse segmentation is the primary impediment to high accuracy automatic sentence-level discourse structure annotation. Their models and algorithm -subsequently packaged together into the publicly available SPADE discourse parser 1 -make use of the output of the Charniak (2000) parser to derive syntactic indicator features for segmentation and discourse parsing. <cite>Sporleder and Lapata (2005)</cite> also used the RST Treebank as training data for data-driven discourse parsing algorithms, though their focus, in contrast to Soricut and Marcu (2003) , was to avoid contextfree parsing and rely exclusively on features in their model that could be derived via finite-state chunkers and taggers.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_2",
  "x": "Once again, segmentation is the part of the process where the automatic algorithms most seriously underperform. In this paper we take up the question posed by the results of <cite>Sporleder and Lapata (2005)</cite> : how much, if any, accuracy reduction should we expect if we choose to use only finite-state derived features, rather than those derived from full contextfree parses? If little accuracy is lost, as their results suggest, then it would make sense to avoid relatively expensive context-free parsing, particularly if the amount of text to be processed is large or if there are real-time processing constraints on the system. If, however, the accuracy loss is substantial, one might choose to avoid context-free parsing only in the most time-constrained scenarios.",
  "y": "motivation"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_3",
  "x": "While <cite>Sporleder and Lapata (2005)</cite> demonstrated that their finite-state system could perform as well as the SPADE system, which uses context-free parse trees, this does not directly answer the question of the utility of context-free derived features for this task. SPADE makes use of a particular kind of feature from the parse trees, and does not train a general classifier making use of other features beyond the parse-derived indicator features. As we shall show, its performance is not the highest that can be achieved via context-free parser derived features.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_4",
  "x": "By our count, this eliminates 40 of the 991 sentences in the test set from consideration. <cite>Sporleder and Lapata (2005)</cite> went further and established a smaller subset of 608 sentences, which omitted sentences with only one segment, i.e., sentences which themselves are atomic edus. Since the primary focus of this paper is on segmentation, there is no strong reason to omit any sentences from the test set, hence our results will evaluate on all 991 test sentences, with two exceptions.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_5",
  "x": "<cite>Sporleder and Lapata (2005)</cite> went further and established a smaller subset of 608 sentences, which omitted sentences with only one segment, i.e., sentences which themselves are atomic edus. Since the primary focus of this paper is on segmentation, there is no strong reason to omit any sentences from the test set, hence our results will evaluate on all 991 test sentences, with two exceptions. First, in Section 2.3, we compare SPADE results under our configuration with results from <cite>Sporleder and Lapata (2005)</cite> in order to establish comparability, and this is done on their 608 sentence subset.",
  "y": "differences"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_6",
  "x": "First, in Section 2.3, we compare SPADE results under our configuration with results from <cite>Sporleder and Lapata (2005)</cite> in order to establish comparability, and this is done on their 608 sentence subset. Second, in Section 3.2, we investigate feeding our segmentation into the SPADE system, in order to evaluate the impact of segmentation improvements on their sentence-level discourse parsing performance. For those trials, the 951 sentence subset from Soricut and Marcu (2003) is used.",
  "y": "uses"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_7",
  "x": "Hence Soricut and Marcu (2003) evaluate with respect to sentence internal segmentation boundaries, i.e., with indices j such that 0<j<k for a sentence of length k. Let g be the number of sentence-internal segmentation boundaries in the gold standard, t the number of sentence-internal segmentation boundaries in the system output, and m the number of correct sentence-internal segmentation boundaries in the system output. Then In <cite>Sporleder and Lapata (2005)</cite> , they were primarily interested in labeled segmentation, where the segment initial boundary was labeled with the segment type.",
  "y": "background"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_8",
  "x": "In such a scenario, the boundary at index 0 is no longer known, hence their evaluation included those boundaries, even when reporting unlabeled results. Thus, in section 2.3, for comparison with reported results in <cite>Sporleder and Lapata (2005)</cite> , our F1-score is defined accordingly, i.e., seg- mentation boundaries j such that 0 \u2264 j < k. In addition, we will report unlabeled bracketing precision, recall and F1-score, as defined in the PARSEVAL metrics (Black et al., 1991) and evaluated via the widely used evalb package. We also use evalb when reporting labeled and unlabeled discourse parsing results in Section 3.2.",
  "y": "uses"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_9",
  "x": "These two modifications to the Charniak parsing output used by the SPADE system lead to improvements in its performance compared to previously reported results. Table 1 compares segmentation results of three systems on the <cite>Sporleder and Lapata (2005)</cite> 608 sentence subset of the evaluation data: (1) their best reported system; (2) the SPADE system results reported in that paper; and (3) the SPADE system results with our current configuration. The evaluation uses the unlabeled F1 measure as defined in that paper, which counts sentence initial boundaries in the scoring, as discussed in the previous section.",
  "y": "uses"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_10",
  "x": "In other words, all n-grams in a six word window of boundary position i are included as features, except those that include neither w i nor w i+1 in the n-gram. The identical feature templates are used with POS-tag and shallow tag sequences as well, to define tag n-gram features. This feature set is very close to that used in <cite>Sporleder and Lapata (2005)</cite> , but not identical.",
  "y": "similarities"
 },
 {
  "id": "f4792ef9808a1a3c415f6f57351335_11",
  "x": "The results for three systems are presented in Table 3 : SPADE, our \"Full finite-state\" system, and our system with all features. Results for unlabeled bracketing are presented, along with results for labeled bracketing, where the label is either Nucleus or Satellite, depending upon whether or not the node is more central (Nucleus) to the coherence of the text than its sibling(s) (Satellite). This label set has been shown to be of particular utility for indicating which segments are more important to include in an automatically created summary or compressed sentence<cite> (Sporleder and Lapata, 2005</cite>; Marcu, 1998; Marcu, 1999; Cristea et al., 2005) .",
  "y": "background"
 },
 {
  "id": "f54235664f013f0fec918222be9198_0",
  "x": "Where van Zaanen (2000) and Clark (2001) induced unlabeled phrase structure for small domains like the ATIS, obtaining around 40% unlabeled f-score, Klein and Manning (2002) report 71.1% f-score on Penn WSJ part-of-speech strings \u2264 10 words (WSJ10) using a constituentcontext model called CCM. Klein and Manning (2004) further show that a hybrid approach which combines constituency and dependency models, yields 77.6% f-score on WSJ10. While Klein and Manning's approach may be described as an \"all-substrings\" approach to unsupervised parsing, an even richer model consists of an \"all-subtrees\" approach to unsupervised parsing, called U-DOP <cite>(Bod 2006)</cite> .",
  "y": "background"
 },
 {
  "id": "f54235664f013f0fec918222be9198_1",
  "x": "While we do not achieve as high an f-score as the UML-DOP model in<cite> Bod (2006)</cite> , we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in<cite> Bod (2006)</cite> . We will extend our experiments to 4 million sentences from the NANC corpus (Graff 1995) , showing that an f-score of 70.7% can be obtained on the standard Penn WSJ test set by means of unsupervised parsing. Moreover, U-DOP* can be directly put to use in bootstrapping structures for concrete applications such as syntax-based machine translation and speech recognition.",
  "y": "uses differences"
 },
 {
  "id": "f54235664f013f0fec918222be9198_2",
  "x": "Given the advantages of DOP*, we will generalize this model in the current paper to unsupervised parsing. We will use the same allsubtrees methodology as in<cite> Bod (2006)</cite> , but now by applying the efficient and consistent DOP*-based estimator. The resulting model, which we will call U-DOP*, roughly operates as follows:",
  "y": "extends"
 },
 {
  "id": "f54235664f013f0fec918222be9198_3",
  "x": "But equally important is the fact that this new DOP* model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003) . Moreover, DOP*'s estimation procedure is very efficient, while the EM training procedure for UML-DOP proposed in<cite> Bod (2006)</cite> is particularly time consuming and can only operate by randomly sampling trees. Given the advantages of DOP*, we will generalize this model in the current paper to unsupervised parsing.",
  "y": "background"
 },
 {
  "id": "f54235664f013f0fec918222be9198_4",
  "x": "A shared parse forest is an AND-OR graph where AND-nodes correspond to the usual parse tree nodes, while OR-nodes correspond to distinct subtrees occurring in the same context. The total number of nodes is cubic in sentence length n. This means that there are O(n 3 ) many nodes that receive a unique address as described above, to which next our PCFG reduction is applied. This is a huge reduction compared to<cite> Bod (2006)</cite> where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work.",
  "y": "differences"
 },
 {
  "id": "f54235664f013f0fec918222be9198_5",
  "x": "Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP <cite>(Bod 2006)</cite> . This can be accomplished by training the PCFG reduction on the held-out corpus HC by means of the expectation-maximization algorithm, where the weights in figure 1 are taken as initial parameters. Both U-DOP*'s and UML-DOP's estimators are known to be statistically consistent. But while U-DOP*'s training phase merely consists of the computation of the shortest derivations and the extraction of subtrees, UML-DOP involves iterative training of the parameters.",
  "y": "uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_6",
  "x": "To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Manning (2002, 2004) and<cite> Bod (2006)</cite> : Penn's WSJ10 which contains 7422 sentences \u2264 10 words after removing empty elements and punctuation, the German NEGRA10 corpus and the Chinese Treebank CTB10 both containing 2200+ sentences \u2264 10 words after removing punctuation. As with most other unsupervised parsing models, we train and test on p-o-s strings rather than on word strings. The extension to word strings is straightforward as there exist highly accurate unsupervised part-of-speech taggers (e.g. Sch\u00fctze 1995) which can be directly combined with unsupervised parsers, but for the moment we will stick to p-o-s strings (we will come back to word strings in section 5).",
  "y": "uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_7",
  "x": "We used the same evaluation metrics for unlabeled precision (UP) and unlabeled recall (UR) as in Manning (2002, 2004) . The two metrics of UP and UR are combined by the unlabeled f-score F1 = 2*UP*UR/(UP+UR). All trees in the test set were binarized beforehand, in the same way as in<cite> Bod (2006)</cite> .",
  "y": "uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_8",
  "x": "Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in<cite> Bod (2006)</cite> , the CCM model in Klein and Manning (2002) , the DMV dependency model in Klein and Manning (2004) It should be kept in mind that an exact comparison can only be made between U-DOP* and UML-DOP in table 1, since these two models were tested on 90%/10% splits, while the other models were applied to the full WSJ10, NEGRA10 and CTB10 corpora. Table 1 shows that U-DOP* performs worse than UML-DOP in all cases, although the differences are small and was statistically significant only for WSJ10 using paired t-testing. As explained above, the main advantage of U-DOP* over UML-DOP is that it works with a more succinct grammar extracted from the shortest derivations of HC.",
  "y": "uses"
 },
 {
  "id": "f54235664f013f0fec918222be9198_9",
  "x": "Comparison between the (best version of) U-DOP*, the supervised treebank PCFG and the supervised DOP* for section 23 of Penn's WSJ As seen in table 5, U-DOP* outperforms the binarized treebank PCFG on the WSJ test set. While a similar result was obtained in<cite> Bod (2006)</cite> , the absolute difference between unsupervised parsing and the treebank grammar was extremely small in<cite> Bod (2006)</cite>: 1.8%, while the difference in table 5 is 7.2%, corresponding to 19.7% error reduction. Our f-score remains behind the supervised version of DOP* but the gap gets narrower as more training data is being added to U-DOP*.",
  "y": "differences"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_0",
  "x": "Researchers have demonstrated impressive gains in a broad range of NLP tasks, from sentence classification to sequence labeling. Recently, <cite>Yang et al. (2019)</cite> showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark (Chen et al., 2017) . Interestingly, the approach of <cite>Yang et al. (2019)</cite> represents a simple method to combining BERT with off-the-shelf IR.",
  "y": "background"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_1",
  "x": "Recently, <cite>Yang et al. (2019)</cite> showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark (Chen et al., 2017) . Interestingly, the approach of <cite>Yang et al. (2019)</cite> represents a simple method to combining BERT with off-the-shelf IR. In this paper, we build on these initial successes to explore how much further we can push this simple architecture by data augmentation, taking advantage of distant supervision techniques to gather more and higher-quality * equal contribution training data to fine tune BERT.",
  "y": "background"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_2",
  "x": "Recently, <cite>Yang et al. (2019)</cite> showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark (Chen et al., 2017) . Interestingly, the approach of <cite>Yang et al. (2019)</cite> represents a simple method to combining BERT with off-the-shelf IR. In this paper, we build on these initial successes to explore how much further we can push this simple architecture by data augmentation, taking advantage of distant supervision techniques to gather more and higher-quality * equal contribution training data to fine tune BERT.",
  "y": "extends"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_3",
  "x": "Experiments show that, using the same reader model as <cite>Yang et al. (2019)</cite> , our simple data-augmentation techniques yield additional large improvements. To illustrate the robustness of our methods, we also demonstrate consistent gains on another English QA dataset and present baselines for two additional Chinese QA datasets (which have not to date been evaluated in an \"end-to-end\" manner). In addition to achieving state-of-the-art results, we contribute important lessons on how to leverage BERT effectively for question answering.",
  "y": "differences uses"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_4",
  "x": "We use the same exact setup as the \"paragraph\" variant of BERTserini<cite> (Yang et al., 2019)</cite> , where the input corpus is pre-segmented into paragraphs at index time, each of which is treated as a \"document\" for retrieval purposes. The question is used as a \"bag of words\" query to retrieve the top k candidate paragraphs using BM25 ranking. Each paragraph is then fed into the BERT reader along with the original natural language question for inference.",
  "y": "uses"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_5",
  "x": "For each candidate paragraph, we apply inference over the entire paragraph, and the reader selects the best text span and provides a score. We then combine the reader score with the retriever score via linear interpolation: S = (1 \u2212 \u00b5) \u00b7 S Anserini + \u00b5 \u00b7 S BERT , where \u00b5 \u2208 [0, 1] is a hyperparameter (tuned on a training sample). One major shortcoming with BERTserini is that <cite>Yang et al. (2019)</cite> only fine tune on SQuAD, which means that the BERT reader is exposed to an impoverished set of examples; all SQuAD data come from a total of only 442 documents.",
  "y": "differences"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_6",
  "x": "We apply hanziconv 1 to transform the corpus into simplified characters for CMRC and traditional characters for DRCD. Following <cite>Yang et al. (2019)</cite> , to evaluate answers in an end-to-end setup, we disregard the paragraph context from the original datasets and use only the answer spans. As in previous work, exact match (EM) score and F 1 score (at the token level) serve as the two primary evaluation metrics.",
  "y": "uses"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_7",
  "x": "As in previous work, exact match (EM) score and F 1 score (at the token level) serve as the two primary evaluation metrics. In addition, we compute recall (R), the fraction of questions for which the correct answer appears in any retrieved paragraph; to make our results comparable to <cite>Yang et al. (2019)</cite> , Anserini returns the top k = 100 paragraphs to feed into the BERT reader. Note that this recall is not the same as the token-level recall component in the F 1 score.",
  "y": "similarities"
 },
 {
  "id": "f633ceffdf53849159574a2891eda1_8",
  "x": "The row marked \"SRC\" indicates fine tuning with SQuAD data only and matches the BERTserini condition of <cite>Yang et al. (2019)</cite> ; we report higher scores due to engineering improvements (primarily a Lucene version upgrade). As expected, fine tuning with augmented data improves effectiveness, and experiments show that while training with positive examples using DS(+) definitely (Wang et al., 2017) 29.1 37.5 - Kratzwald and Feuerriegel (2018) 29.8 --Par. R. 28.5 -83.1 Par.",
  "y": "differences"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_0",
  "x": "In special, they have been more prevalent than syntactical networks because they represent a simplified representation of the complex syntactical analysis [7, 8] , as most of the syntactical links occur between neighboring words. Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition <cite>[9]</cite> , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Furthermore, such representation has also been useful in the analysis of the complexity [15] and quality of texts [16] .",
  "y": "background"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_1",
  "x": "Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies [8, <cite>9</cite> ] that novel topological measurements should be introduced to capture a wider range of linguistic features.",
  "y": "future_work background motivation"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_2",
  "x": "Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition <cite>[9]</cite> , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] . Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies [8, <cite>9</cite> ] that novel topological measurements should be introduced to capture a wider range of linguistic features.",
  "y": "motivation"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_3",
  "x": "Unfortunately, a major problem arising from the analyses performed with co-occurrence networks is the difficulty to provide a rigorous interpretation of the factors accounting for the success of the model. Therefore, future investigations should pursue a better interpretation at the network level aiming at the understanding of the fundamental properties of the language. Most importantly, it is clear from some recent studies [8, <cite>9</cite> ] that novel topological measurements should be introduced to capture a wider range of linguistic features.",
  "y": "motivation future_work"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_4",
  "x": "Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in <cite>[9]</cite> , [19] and [20].",
  "y": "background"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_5",
  "x": "Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in <cite>[9]</cite> , [19] and [20].",
  "y": "uses"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_6",
  "x": "Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in <cite>[9]</cite> , [19] and [20] .",
  "y": "background"
 },
 {
  "id": "f78b11352d01b6567392f8cb3c7642_7",
  "x": "Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in <cite>[9]</cite> , [19] and [20] .",
  "y": "uses"
 },
 {
  "id": "f856c4fb5e6e00729d33b15b24aff6_0",
  "x": "These systems give new evidence about the information and problem-solving that's involved. The challenge is that these models must describe semantics and pragmatics, as well as syntax and behavior. My own slow progress <cite>(Cassell et al., 2000</cite>; Koller and Stone, 2007) shows that there's still lots of hard work needed to develop suitable techniques.",
  "y": "future_work motivation"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_0",
  "x": "In that time, researchers have explored architectures ranging from convolutional neural networks (Kalchbrenner and Blunsom, 2013) to recurrent neural networks (Chung et al., 2014) to attentional models (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> and achieved better performance than traditional statistical or syntax-based MT techniques on many language pairs. NMT models first achieved state-of-the-art performance on the WMT English\u2192German news-domain task in 2015<cite> (Luong et al., 2015)</cite> and subsequent improvements have been reported since then (Sennrich et al., 2015a; Li and Jurafsky, 2016) . The problem of machine translation is fundamentally a sequence-to-sequence transduction task, and most approaches have been based on an encoder-decoder architecture (Sutskever et al., 2014; .",
  "y": "background"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_1",
  "x": "In that time, researchers have explored architectures ranging from convolutional neural networks (Kalchbrenner and Blunsom, 2013) to recurrent neural networks (Chung et al., 2014) to attentional models (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> and achieved better performance than traditional statistical or syntax-based MT techniques on many language pairs. NMT models first achieved state-of-the-art performance on the WMT English\u2192German news-domain task in 2015<cite> (Luong et al., 2015)</cite> and subsequent improvements have been reported since then (Sennrich et al., 2015a; Li and Jurafsky, 2016) . The problem of machine translation is fundamentally a sequence-to-sequence transduction task, and most approaches have been based on an encoder-decoder architecture (Sutskever et al., 2014; .",
  "y": "background"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_2",
  "x": "This entails coupled neural networks that encode the input sentence into a vector or set of vectors and decode that vector representation into an output sentence in a different language respectively. Recently, a third component has been added to many of these models: an attention mechanism, whereby the decoder can attend directly to localized information from the input sentence during the output generation process (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> . The encoder and decoder in these models typically consist of one-layer or multi-layer recurrent neural networks (RNNs); we use four-and five-layer long short-term memory (LSTM) RNNs.",
  "y": "background"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_3",
  "x": "Recently, a third component has been added to many of these models: an attention mechanism, whereby the decoder can attend directly to localized information from the input sentence during the output generation process (Bahdanau et al., 2015;<cite> Luong et al., 2015)</cite> . The encoder and decoder in these models typically consist of one-layer or multi-layer recurrent neural networks (RNNs); we use four-and five-layer long short-term memory (LSTM) RNNs. The attention mechanism in our four-layer model is what <cite>Luong (2015)</cite> describes as \"Global attention (dot)\"; the mechanism in our five-layer Y-LSTM model is described in Section 2.1.",
  "y": "similarities uses"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_4",
  "x": "There are three main techniques for achieving fully open-ended decoder output. Models may use computed alignments between source and target sentences to directly copy or transform a word from the input sentence whose corresponding translation is not present in the vocabulary<cite> (Luong et al., 2015)</cite> or they may conduct sentence tokenization at the level of individual characters (Ling et al., 2015) or subword units such as morphemes (Sennrich et al., 2015b) . The latter techniques allow the decoder to construct words it has not previously encountered out of known characters or morphemes; we apply the subword splitting strategy using Morfessor 2.0, an unsupervised morpheme segmentation model (Virpioja et al., 2013) .",
  "y": "background"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_5",
  "x": "---------------------------------- **MODEL DESCRIPTION** The model identified as metamind-single is based on the attention-based encoder-decoder framework described in <cite>Luong (2015)</cite> , using the attention mechanism referred to as \"Global attention (dot).",
  "y": "similarities uses"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_6",
  "x": "Decoding is performed using beam search, with beam width 16. The beam search decoder differs slightly from <cite>Luong (2015)</cite> in that we normalize output sentence probabilities by length, following , rather than performing ad-hoc adjustments to correct for short output sentences. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "fa33495582abd0c6efe8f599c73d0e_7",
  "x": "Our best single model matches the performance of the best model from U. Edinburgh, which applies a similar attentional framework, subword splitting, and back-translated augmentation. The Y-LSTM model underperformed relative to the model based on <cite>Luong (2015)</cite> , but provided a small additional boost to the ensemble. The primary contribution of this model is to demonstrate that purely attentional NMT is possible: the only inputs to the decoder are through the attention mechanism.",
  "y": "differences"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_0",
  "x": "While disentangling natural image representation has been studied extensively, much less work has focused on natural speech, leaving a rather large void in the understanding of this problem. In this paper, we first present a short review and comparison of two representative efforts on this topic [<cite>6</cite>, 7] , where both efforts involve using an auto-encoder and can be applied to the same task (i.e., voice conversion), but the key disentangling algorithms and underlying ideas are very different. In [<cite>6</cite>] , the authors proposed an unsupervised <cite>factorized hierarchical variational autoencoder (FHVAE)</cite>.",
  "y": "uses background"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_1",
  "x": "In this paper, we first present a short review and comparison of two representative efforts on this topic [<cite>6</cite>, 7] , where both efforts involve using an auto-encoder and can be applied to the same task (i.e., voice conversion), but the key disentangling algorithms and underlying ideas are very different. In [<cite>6</cite>] , the authors proposed an unsupervised <cite>factorized hierarchical variational autoencoder (FHVAE)</cite>. The key idea is that assuming that the speech data is generated from two separate latent variable sets z1 and z2, where z1 contains segment-level (short-term) variables and z2 contains sequencelevel (long-term) variables (z2 that are further conditioned on an s-vector \u00b52).",
  "y": "background"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_2",
  "x": "In [<cite>6</cite>] and subsequent efforts [8, 9, 10] , the authors further showed that the disentangled representation is also helpful in the speech recognition task. These efforts convey two primary insights: 1) by adding the appropriate prior assumptions on the latent variables, speech content information and speaker-level information can be separated out in an unsupervised learning manner; 2) the learned disentangled representations are useful to improve both speech synthesis and broader inference tasks. Different from [<cite>6</cite>] , in [7] , the authors propose a supervised approach based on adversarial training [11, 12, 13, 14] (illustrated in Figure 2 (left)).",
  "y": "background"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_3",
  "x": "Different from [<cite>6</cite>] , in [7] , the authors propose a supervised approach based on adversarial training [11, 12, 13, 14] (illustrated in Figure 2 (left)). In addition to a regular autoencoder, the authors add a regularization term in its objective function to force the latent variables (i.e., the encoding) to not contain speaker information. This is done by introducing an auxiliary speaker verification classifier C. C is trained to correctly identify the speaker y from the latent variables z (i.e., minimizing the misclassification loss Lc = \u2212logP (y|z)), while the encoder is trained to maximize Lc, i.e., to avoid encoding speaker information in z. Both z and speaker label y are fed to the decoder for reconstruction, and the complete objective function of the auto-encoder is hence minimizing Lrec \u2212 \u03bbLc (where Lrec is the point-wise L1-norm loss).",
  "y": "differences"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_4",
  "x": "That is, in [<cite>6</cite>] , z1 and z2 are in fact corresponding to general fast-changing and slow-changing information, i.e., z1 may contain other fast-changing information such as emotion, while z2 may contain slow-changing factors such as background and channel noise. In [7] , the authors actually separate out speaker information and general non-speaker information, which may contain a lot of detailed information. Coarse-grained disentangled representations are enough for some tasks, such as voice speaker conversion, but might be limited for other tasks.",
  "y": "background"
 },
 {
  "id": "fca14f99953b9dc30a594525ee92b5_5",
  "x": "In contrast, a coarse-grained disentangled representation [<cite>6</cite>, 7] may only support a simple voice speaker conversion task. Inference: Learning fine-grained disentangled representation can also help with more accurate inference and reasoning. When we attempt to predict one target variable, we usually want to eliminate the interference of other factors.",
  "y": "differences"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_0",
  "x": "In addition, conclusions from different reports can be contradictory. For example, most work observes that stochastic gradient descent (SGD) gives best performance on NER task (<cite>Chiu and Nichols, 2016;</cite> Lample et al., 2016; Ma and Hovy, 2016) , while Reimers and Gurevych (2017b) report that SGD is the worst optimizer on the same datasets. The comparison between different deep neural models is challenging due to sensitivity on experimental settings.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_1",
  "x": "\u2022 Dataset. Most work reports sequence labeling results on both CoNLL 2003 English NER (Tjong Kim Sang and De Meulder, 2003) and PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011; Ma and Hovy, 2016) . Ling et al. (2015) give results only on POS dataset, while some papers (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Strubell et al., 2017) report results on the NER dataset only.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_2",
  "x": "dos Santos et al. (2015) conducts experiments on NER for Portuguese and Spanish. Most work uses the development set to select hyperparameters (Lample et al., 2016; Ma and Hovy, 2016) , while others add development set into training set (<cite>Chiu and Nichols, 2016</cite>; Peters et al., 2017) . Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences).",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_3",
  "x": "A typical data preprocessing step is to normize digit characters (<cite>Chiu and Nichols, 2016;</cite> Lample et al., 2016; Yang et al., 2016; Strubell et al., 2017) . Reimers and Gurevych (2017b) use fine-grained representations for less frequent words. Ma and Hovy (2016) do not use preprocessing.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_4",
  "x": "\u2022 Features. Strubell et al. (2017) and <cite>Chiu and Nichols (2016)</cite> apply word spelling features and further integrate context features. Collobert et al. (2011) and use neural features to represent external gazetteer information.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_5",
  "x": "<cite>Chiu and Nichols (2016)</cite> search for the hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters. However, existing models use different parameter settings, which affects the fair comparison. \u2022 Evaluation.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_6",
  "x": "\u2022 Evaluation. Some literature reports results using mean and standard deviation under different random seeds (<cite>Chiu and Nichols, 2016</cite>; Peters et al., 2017; Liu et al., 2018) . Others report the best result among different trials (Ma and Hovy, 2016) , which cannot be compared directly.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_7",
  "x": "Strubell et al. (2017) built a deeper dilated CNN architecture to capture larger local features. Hammerton (2003) was the first to exploit LSTM for sequence labeling. built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (Lample et al., 2016; Liu et al., 2018) , GRU (Yang et al., 2016) , and CNN (<cite>Chiu and Nichols, 2016</cite>; Ma and Hovy, 2016) features.",
  "y": "extends"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_8",
  "x": "built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (Lample et al., 2016; Liu et al., 2018) , GRU (Yang et al., 2016) , and CNN (<cite>Chiu and Nichols, 2016</cite>; Ma and Hovy, 2016) features. <cite>These models</cite> achieve state-of-the-art results in the literature.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_9",
  "x": "3) Our findings are more consistent with <cite>most previous work</cite> on configurations such as usefulness of character information (Lample et al., 2016; Ma and Hovy, 2016) , optimizer (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Ma and Hovy, 2016) and tag scheme (Ratinov and Roth, 2009; Dai et al., 2015) . In contrast, many results of Reimers and Gurevych (2017b) contradict existing reports. 4) We conduct a wider range of comparison for word sequence representations, including all combinations of character CNN/LSTM and word CNN/LSTM structures, while Reimers and Gurevych (2017b) studied the word LSTM models only.",
  "y": "similarities"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_10",
  "x": "Character information has been proven to be critical for sequence labeling tasks (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Ma and Hovy, 2016) , with LSTM and CNN being used to model character sequence information (\"Char Rep.\"). Similarly, on the word level, LSTM or CNN structures can be leveraged to capture long-term information or local features (\"Word Rep.\"), respectively. Subsequently, the inference layer assigns labels to each word using the hidden states of word sequence representations.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_11",
  "x": "Character CNN. Using a CNN structure to encode character sequences was firstly proposed by Santos and Zadrozny (2014), and followed by many subsequent investigations (dos Santos et al., 2015; <cite>Chiu and Nichols, 2016</cite>; Ma and Hovy, 2016) . In our experiments, we take the same structure as Ma and Hovy (2016) , using one layer CNN structure with max-pooling to capture character-level representations.",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_12",
  "x": "**WORD SEQUENCE REPRESENTATIONS** Similar to character sequences in words, we can model word sequence information through LSTM or CNN structures. LSTM has been widely used in sequence labeling (Lample et al., 2016; Ma and Hovy, 2016; <cite>Chiu and Nichols, 2016;</cite> Liu et al., 2018) .",
  "y": "background"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_13",
  "x": "Similarly, \"WLSTM\" and \"WCNN\" represent word LSTM and word CNN structure, respectively. As shown in Table 4 , most NER work focuses on WLSTM+CRF structures with different character sequence representations. We re-implement the structure of several reports (<cite>Chiu and Nichols, 2016</cite>; Ma and Hovy, 2016; Peters et al., 2017) , which take the CCNN+WLSTM+CRF architecture.",
  "y": "uses"
 },
 {
  "id": "fd5a6307b398f37d8729c21cfce6c1_14",
  "x": "The results are shown in Figure 5 5 . In contrast to Reimers and Gurevych (2017b) , who reported that SGD is the worst optimizer, our results show that SGD outperforms all other optimizers significantly (p < 0.01), with a slower convergence process during training. Our observation is consistent with most literature (<cite>Chiu and Nichols, 2016</cite>; Lample et al., 2016; Ma and Hovy, 2016) .",
  "y": "similarities"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_0",
  "x": "**INTRODUCTION** Unknown words constitute a major source of difficulty for Chinese part-of-speech (POS) tagging, yet relatively little work has been done on POS guessing of Chinese unknown words. The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (Chen et al., 1997; <cite>Wu and Jiang, 2000</cite>; Goh, 2003) .",
  "y": "background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_1",
  "x": "This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models. The rule-based approach was rejected with the claim that rules are bound to overgenerate<cite> (Wu and Jiang, 2000)</cite> . In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task.",
  "y": "background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_2",
  "x": "Unknown words constitute a major source of difficulty for Chinese part-of-speech (POS) tagging, yet relatively little work has been done on POS guessing of Chinese unknown words. The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (Chen et al., 1997; <cite>Wu and Jiang, 2000</cite>; Goh, 2003) . This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models.",
  "y": "background motivation"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_3",
  "x": "Second,<cite> Wu and Jiang (2000)</cite> argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will \"result in massive overgeneration\" (p. 48). We will show that overgeneration can be controlled by additional constraints. ----------------------------------",
  "y": "background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_4",
  "x": "Second,<cite> Wu and Jiang (2000)</cite> argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will \"result in massive overgeneration\" (p. 48). We will show that overgeneration can be controlled by additional constraints. ----------------------------------",
  "y": "differences background"
 },
 {
  "id": "fd9122d20c390ea115c27092170739_5",
  "x": "The models we will consider are a rule-based model, the trigram model, and the statistical model developed by<cite> Wu and Jiang (2000)</cite> . Combination of the three models will be based on the evaluation of their individual performances on the training data. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "fddb1d19895976661babdc17d232ee_0",
  "x": "In this paper, we outline a fundamentally different approach to online video popularity analysis that allows social media creators both to predict video popularity as well as to understand the impact of its headline or video frames on the future popularity. To that end, we propose to use an attention-based model and gradient-weighted class activation maps [9] , inspired by the recent successes of the attention mechanism in other domains [15, <cite>16]</cite> . Although some works focused on understanding the influence of image parts on its popularity [6, 1] , our method addresses videos, not images, and exploits the temporal characteristics of video clips through the attention mechanism.",
  "y": "uses"
 },
 {
  "id": "fddb1d19895976661babdc17d232ee_1",
  "x": "For each frame feature vector we apply a learnable linear transformation followed by ReLU, obtaining a sequence of frame embeddings (q j ) N j=1 . The final video embedding is a weighted average of these embed- Weights \u03b1 i are computed with attention mechanism implemented as a two-layer neural network<cite> [16]</cite> : the first layer produces a hidden representation u i = tanh(W u q i + b u ) and the second layer outputs unnormalized importance a i = W a u i + b a .",
  "y": "uses"
 },
 {
  "id": "fddb1d19895976661babdc17d232ee_2",
  "x": "For visualizations in the text domain, we use attention weights \u03b2 t used to compute text representation d. These weights capture relative importance of words in their context to headline popularity, as shown in<cite> [16]</cite> in the context of sentiment analysis. ---------------------------------- **EXPERIMENTS**",
  "y": "uses background"
 },
 {
  "id": "fe1d6ca4a88c03cfb2ae94ef45030d_0",
  "x": "There has also been growing interest in deep learning models for keyphrase generation <cite>(Meng et al. 2017</cite>; Chan et al. 2019) . Inspired by these advances, we propose a new GAN architecture for keyphrase generation where the generator produces a sequence of keyphrases from a given document and the discriminator distinguishes between human-curated and machine-generated keyphrases. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "fe1d6ca4a88c03cfb2ae94ef45030d_1",
  "x": "As with most GAN architectures, our model also consists of a generator (G) and discriminator (D), which are trained in an alternating fashion<cite> (Goodfellow et al. 2014)</cite> . Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",
  "y": "uses"
 },
 {
  "id": "fe1d6ca4a88c03cfb2ae94ef45030d_2",
  "x": "All rights reserved. 1 Code is available at https://github.com/avinsit123/keyphrasegan Generator -Given a document d = {x 1 , x 2 , ..., x n }, where x i is the i th token, the generator produces a sequence of keyphrases: y = {y 1 , y 2 , ..., y m }, where each keyphrase y i is composed of tokens y 1 i , y 2 i , ..., y li i . We employ catSeq model<cite> (Yuan et al. 2018)</cite> for the generation process, which uses an encoder-decoder framework: the encoder being a bidirectional Gated Recurrent Unit (bi-GRU) and the decoder a forward GRU.",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_0",
  "x": "Kim and Hovy (2006) and Bethard et al. (2004) examine the usefulness of semantic roles provided by FrameNet 1 for both OH and opinion target extraction. More recently, <cite>Wiegand and Klakow (2010)</cite> explored convolution kernels for OH extraction and found that tree kernels outperform all other kernel types. In (Johansson and Moschitti, 2010) , a re-ranking approach modeling complex relations between multiple opinions in a sentence is presented.",
  "y": "background"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_1",
  "x": "As a large unlabeled (training) corpus, we chose the North American News Text Corpus. As a labeled (test) corpus, we use the MPQA corpus. 2 We use the definition of OHs as described in<cite> (Wiegand and Klakow, 2010)</cite> .",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_2",
  "x": "This means that on our unlabeled training corpus we consider each NP with the head being an agentive protoOH as a positive data instance and all the remaining NPs occurring in those sentences as negative instances. With this definition we train a supervised classifier based on convolution kernels (Collins and Duffy, 2001 ) as this method has been shown to be quite effective for OH extraction<cite> (Wiegand and Klakow, 2010)</cite> . Convolution kernels derive features automatically from complex discrete structures, such as syntactic parse trees or part-of-speech sequences, that are directly provided to the learner.",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_3",
  "x": "Moreover, the features in the vector kernel heavily rely on taskspecific resources, e.g. a sentiment lexicon, which are deliberately avoided in our low-resource classifier as our method should be applicable to any language (and for many languages sentiment resources are either sparse or do not exist at all). In addition to <cite>Wiegand and Klakow (2010)</cite> , we have to discard the content of candidate NPs (e.g. the candidate opinion holder NP [N P Cand [N N S advocates] ] is reduced to [N P Cand ]), the reason for this being that in our automatically generated training set, OHs will always be protoOHs. Retaining them in the training data would cause the learner to develop a detrimental bias towards these nouns (our resulting classifier should detect any OH and not only protoOHs).",
  "y": "extends"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_4",
  "x": "We also augment the tree kernels themselves with additional information by 3 wordnet.princeton.edu following <cite>Wiegand and Klakow (2010)</cite> who add for each word that belongs to a predictive semantic class another node that directly dominates the pertaining leaf node and assign it a label denoting that class. While <cite>Wiegand and Klakow (2010)</cite> made use of manually built lexicons, we use our predictive predicates extracted from contexts of protoOHs. For instance, if doubt is such a predicate, we would replace the subtree",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_5",
  "x": "Again, we use the supervised learner based on tree kernels ( \u00a74.1). We also augment the tree kernels themselves with additional information by 3 wordnet.princeton.edu following <cite>Wiegand and Klakow (2010)</cite> who add for each word that belongs to a predictive semantic class another node that directly dominates the pertaining leaf node and assign it a label denoting that class. While <cite>Wiegand and Klakow (2010)</cite> made use of manually built lexicons, we use our predictive predicates extracted from contexts of protoOHs.",
  "y": "differences"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_6",
  "x": "Both resources have been found predictive for OH extraction (Bloom et al., 2007;<cite> Wiegand and Klakow, 2010)</cite> . Table 3 (lower part) shows the performance of the rule-based classifiers based on protoOHs using different parts of speech. As hard baselines, the table also shows other rule-based classifiers using the same dependency relations as our rulebased classifier (see Table 2 ) but employing different predicates.",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_7",
  "x": "These are commonly accepted heuristics which have already been used in previous work as features (Choi et al., 2005;<cite> Wiegand and Klakow, 2010)</cite> . The latter rule requires the output of a named-entity recognizer 7 for checking proper nouns and WordNet for common nouns. As far as the classifier built with the help of protoOHs is concerned, adding highly ranked adjectives and nouns consistently improves the performance (mostly recall) when added to the set of 7 We use the Stanford tagger: nlp.stanford.edu/software/CRF-NER.shtml Table 4 : List of verbs most highly correlating with protoOHs; \u2020 : included in AL; * : included in SL.",
  "y": "background"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_8",
  "x": "The table also compares two different versions of the rule-based classifier being the classifier as presented in \u00a74.2 (left half of Table 3 ) and a classifier additionally incorporating the two heuristics (right half): \u2022 If the candidate NP follows according to, then it is labeled as an OH. \u2022 The candidate NP can only be an OH if it represents a person or a group of persons. These are commonly accepted heuristics which have already been used in previous work as features (Choi et al., 2005;<cite> Wiegand and Klakow, 2010)</cite> .",
  "y": "uses"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_9",
  "x": "**INCORPORATING KNOWLEDGE FROM PROTOOHS INTO SUPERVISED LEARNING** As a maximum amount of labeled training data we chose 60000 instances (i.e. NPs) which is even a bit more than used in<cite> (Wiegand and Klakow, 2010)</cite> . In addition, we also test 1%, 5%, 10%, 25% and 50% of the training set.",
  "y": "differences"
 },
 {
  "id": "fed51218e78d35aae39d287c95a95a_10",
  "x": "Tree augmentation causes both precision and recall to rise. This observation is consistent with<cite> (Wiegand and Klakow, 2010)</cite> where, however, AL and SL are considered for augmentation. When the vector kernel with the prediction of the rule-based classifier is also included, precision drops slightly but recall is notably boosted resulting in an even more increased F-Score.",
  "y": "similarities"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_0",
  "x": "<cite>Ziering and Van der Plas (2014)</cite> propose an approach that refrains from using any human annotation. They use the fact, that languages differ in their preference for open or closed compounding (i.e., multiword vs. one-word compounds), for inducing the English bracketing of 3NCs. English open 3NCs like human rights abuses can be translated to partially closed phrases as in German Verletzungen der Menschenrechte, (abuses of human rights), from which we can induce the LEFT-branching structure.",
  "y": "background"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_1",
  "x": "<cite>Ziering and Van der Plas (2014)</cite> propose an approach that refrains from using any human annotation. They use the fact, that languages differ in their preference for open or closed compounding (i.e., multiword vs. one-word compounds), for inducing the English bracketing of 3NCs. Although this approach achieves a solid accuracy, a crucial limitation is coverage, because restricting to six paraphrasing patterns ignores many other predictive cases. Moreover, the system needs part of speech (PoS) tags and splitting information for determining 2NCs and is therefore rather language-dependent.",
  "y": "motivation"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_2",
  "x": "The fact, that the alignment of the third noun, violations (violazioni), is separated from the rest, points us in the direction of LEFT-branching. Using less restricted forms of cross-lingual supervision, we achieve a much higher coverage than <cite>Ziering and Van der Plas (2014)</cite> . Furthermore, our results are more accurate.",
  "y": "differences"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_3",
  "x": "While AWDB is designed for bracketing NPs of any length, we first experiment with bracketing 3NCs, the largest class of 3 + NCs (93.8% on the basic dataset of <cite>Ziering and Van der Plas (2014)</cite>), for which bracketing is a binary classification (i.e., LEFT or RIGHT). For bracketing longer NCs we often have to make do with partial information from a language, instead of a full structure. In future work, we plan to investigate methods to combine these partial results.",
  "y": "background"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_4",
  "x": "Although AWDB can also process compounds including adjectives (e.g., active inclusion policy aligned to the Dutch beleid voor actieve insluiting (policy for active inclusion)), for a direct comparison with the system of <cite>Ziering and Van der Plas (2014)</cite> , that analyses 3NCs, we restrict ourselves to noun sequences. We use the Europarl 2 compound database 3 developed by <cite>Ziering and Van der Plas (2014)</cite> . This database has been compiled from the OPUS 4 corpus (Tiedemann, 2012) and comprises ten languages: Danish, Dutch, English, French, German, Greek, Italian, Portuguese, Spanish and Swedish.",
  "y": "uses"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_5",
  "x": "We consider the task of bracketing 3NCs composed of common nouns more ambitious, because named entities often form a single concept that is easy to spot, e.g., Apple II owners. Although AWDB can also process compounds including adjectives (e.g., active inclusion policy aligned to the Dutch beleid voor actieve insluiting (policy for active inclusion)), for a direct comparison with the system of <cite>Ziering and Van der Plas (2014)</cite> , that analyses 3NCs, we restrict ourselves to noun sequences. We use the Europarl 2 compound database 3 developed by <cite>Ziering and Van der Plas (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_6",
  "x": "We compare AWDB with the bracketing approach of <cite>Ziering and Van der Plas (2014)</cite>. For both systems, we use the majority vote across all nine aligned languages, in a token-and type-based version. We implemented an unsupervised method based on statistics on bi-grams extracted from the English part of the Europarl corpus.",
  "y": "uses"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_7",
  "x": "We consider both the adjacency (i.e., (N 1 , N 2 ) vs. (N 2 , N 3 ) , (Marcus, 1980) ) and the dependency (i.e., (N 1 , N 2 ) vs. (N 1 , N 3 ) , (Lauer, 1995) ) model. We created a back-off model for the bracketing system of <cite>Ziering and Van der Plas (2014)</cite> and for AWDB that falls back to using \u03c7 2 if no bracketing structure can be derived (system \u2192 \u03c7 2 ). Finally, we compare with a baseline, that always predicts the majority class: LEFT.",
  "y": "uses"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_8",
  "x": "We observed that there is only a very small overlap between test sets of previous work on NP bracketing and the Europarl database. The test set used by <cite>Ziering and Van der Plas (2014)</cite> is very small and the labeling is less fine-grained. Thus, we decided to create our own test set.",
  "y": "motivation"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_9",
  "x": "Our first result is that type-based cross-lingual bracketing outperforms token-based and achieves up to 91.2% in coverage. As expected, the system of <cite>Ziering and Van der Plas (2014)</cite> does not cover more than 48.1%. The \u03c7 2 method and the back-off models cover all 3NCs in our dataset.",
  "y": "differences"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_10",
  "x": "AWDB outperforms <cite>Ziering and Van der Plas (2014)</cite> significantly 7 . This can be explained with the flexible structure of AWDB, which can exploit more data and is thus more robust to word alignment errors. AWDB significantly outperforms \u03c7 2 in accuracy but is inferior in harmonic(com).",
  "y": "differences"
 },
 {
  "id": "ff5122ce817d506fbcb269b7ae41fe_11",
  "x": "AWDB's back-off model achieves the best harmonic(com) with 96.6% and an accuracy comparable to human performance. For AWDB, types and tokens show the same accuracy. The harmonic mean numbers for the system of <cite>Ziering and Van der Plas (2014)</cite> illustrate that coverage gain of types outweighs a higher accuracy of tokens.",
  "y": "background"
 },
 {
  "id": "ff73758fbef3ddc779a772e634b74e_0",
  "x": "Mental models, in cognitive theory, provide one view on how humans reason either functionally (understanding what the robot does) or structurally (understanding how it works). Mental models are important as they strongly impact how and whether robots and systems are used. In previous work, explanations have been categorised as either explaining 1) machine learning as in [11] who showed that they can increase trust; 2) explaining plans [2, 13] ; 3) verbalising robot [12] or agent rationalisation <cite>[3]</cite> .",
  "y": "background"
 },
 {
  "id": "ff73758fbef3ddc779a772e634b74e_1",
  "x": "This has the advantage of being agnostic to the method of autonomy and could be used to describe rule-based autonomous behaviours but also complex deep models. Similar human-provided rationalisation has been used to generate explanations of deep neural models for game play <cite>[3]</cite> . An interpretable model of autonomy was then derived from the expert, as partially shown in Figure 2 .",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_0",
  "x": "**INTRODUCTION** Recent work on deep learning syntactic parsing models has achieved notably good results, e.g.,<cite> Dyer et al. (2016)</cite> with 92.4 F 1 on Penn Treebank constituency parsing and Vinyals et al. (2015) with 92.8 F 1 . In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture.",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_1",
  "x": "The first (Zaremba et al., 2014) gives the basic language modeling architecture that we have adopted, while the other two (Vinyals et al., 2015; <cite>Dyer et al., 2016)</cite> are parsing models that have the current best results in NN parsing. ---------------------------------- **LSTM-LM**",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_2",
  "x": "---------------------------------- **RNNG** Recurrent Neural Network Grammars (RNNG), a generative parsing model, defines a joint distribution over a tree in terms of actions the model takes to generate the tree<cite> (Dyer et al., 2016)</cite> :",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_3",
  "x": "Given x, we produce Y (x), 50-best trees, with Charniak parser and find y with LSTM-LM as<cite> Dyer et al. (2016)</cite> do with their discriminative and generative models. 3 ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_5",
  "x": "---------------------------------- **SUPERVISION** As shown in Table 2 , with 92.6 F 1 LSTM-LM (G) outperforms an ensemble of five MTPs (Vinyals et al., 2015) and RNNG<cite> (Dyer et al., 2016)</cite> , both of which are trained on the WSJ only.",
  "y": "background"
 },
 {
  "id": "ffcefdc73338187d4a6b2dc2f0bb47_6",
  "x": "**CONCLUSION** The generative parsing model we presented in this paper is very powerful. In fact, we see that a generative parsing model, LSTM-LM, is more effective than discriminative parsing models<cite> (Dyer et al., 2016)</cite> .",
  "y": "differences"
 }
]