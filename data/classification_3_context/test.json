[
 {
  "id": "008d5261ee7385a2b7e39772938f51_0",
  "x": "Although this annotation was performed by the first author only, we know from previous work that similar styles of annotation can achieve acceptable interannotator agreement (Teufel et al., 2006 ). An example annotation for Smadja (1993) is given in Figure  2 , where the first column shows the line number and the second one shows the class label. To compare our work with <cite>Athar (2011)</cite> , we also applied a three-class annotation scheme.",
  "y": "uses"
 },
 {
  "id": "008d5261ee7385a2b7e39772938f51_1",
  "x": "This setup has been shown to produce good results earlier as well (Pang et al., 2002; <cite>Athar, 2011</cite>) . The first set of experiments focuses on simultaneous detection of sentiment and context sentences. For this purpose, we use the four-class annotated corpus described earlier.",
  "y": "background"
 },
 {
  "id": "008d5261ee7385a2b7e39772938f51_2",
  "x": "The baseline score, shown in bold, is obtained with no context window and is comparable to the results reported by <cite>Athar (2011)</cite> . However, we can observe that the F scores decrease as more context is introduced. This may be attributed to the increase in the vocabulary size of the n-grams and a consequent reduction in the discriminating power of the decision boundaries.",
  "y": "similarities"
 },
 {
  "id": "008d5261ee7385a2b7e39772938f51_3",
  "x": "We merge the text of the sentences in the context windows as well as their dependency triplets to obtain the features. The results are reported in Table 3 with best results in bold. Although these results are not better than the context-less baseline, the reason might be data sparsity since existing work on citation sentiment analysis uses more data <cite>(Athar, 2011)</cite> .",
  "y": "extends background"
 },
 {
  "id": "008d5261ee7385a2b7e39772938f51_4",
  "x": "**RELATED WORK** While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000) , the only recent work on citation sentiment detection using a relatively large corpus is by <cite>Athar (2011)</cite> . However, <cite>this work</cite> does not handle citation context.",
  "y": "background"
 },
 {
  "id": "008d5261ee7385a2b7e39772938f51_5",
  "x": "While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000) , the only recent work on citation sentiment detection using a relatively large corpus is by <cite>Athar (2011)</cite> . However, <cite>this work</cite> does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources.",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_0",
  "x": "Our proposed method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by <cite>Yang et al. (2018)</cite> . The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to two application tasks: machine translation and headline generation.",
  "y": "uses"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_1",
  "x": "However, <cite>Yang et al. (2018)</cite> proved that existing RNN language models have low expressive power due to the Softmax bottleneck, which means the output matrix of RNN language models is low rank when we interpret the training of RNN language models as a matrix factorization problem. To solve the Softmax bottleneck, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS), which increases the rank of the matrix by combining multiple probability distributions computed from the encoded fixed-length vector. In this study, we propose Direct Output Connection (DOC) as a generalization of MoS. For stacked RNNs, DOC computes the probability distributions from the middle layers including input embeddings.",
  "y": "motivation"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_2",
  "x": "However, <cite>Yang et al. (2018)</cite> proved that existing RNN language models have low expressive power due to the Softmax bottleneck, which means the output matrix of RNN language models is low rank when we interpret the training of RNN language models as a matrix factorization problem. To solve the Softmax bottleneck, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS), which increases the rank of the matrix by combining multiple probability distributions computed from the encoded fixed-length vector. In this study, we propose Direct Output Connection (DOC) as a generalization of MoS. For stacked RNNs, DOC computes the probability distributions from the middle layers including input embeddings.",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_3",
  "x": "Previous researches demonstrated that RNN language models achieve high performance by using several regularizations and selecting appropriate hyperparameters (Melis et al., 2018; Merity et al., 2018) . However, <cite>Yang et al. (2018)</cite> proved that existing RNN language models have low expressive power due to the Softmax bottleneck, which means the output matrix of RNN language models is low rank when we interpret the training of RNN language models as a matrix factorization problem. To solve the Softmax bottleneck, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS), which increases the rank of the matrix by combining multiple probability distributions computed from the encoded fixed-length vector.",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_4",
  "x": "To solve the Softmax bottleneck, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS), which increases the rank of the matrix by combining multiple probability distributions computed from the encoded fixed-length vector. In this study, we propose Direct Output Connection (DOC) as a generalization of MoS. For stacked RNNs, DOC computes the probability distributions from the middle layers including input embeddings. In addition to raising the rank, the proposed method helps weaken the vanishing gradient problem in backpropagation because DOC provides a shortcut connection to the output.",
  "y": "extends"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_5",
  "x": "Let f (\u00b7) represent an abstract function of an RNN, which might be the Elman network (Elman, 1990) , the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) , the Recurrent Highway Network (RHN) (Zilly et al., 2017) , or any other RNN variant. In this research, we stack three LSTM layers based on Merity et al. (2018) because they achieved high performance. 3 Language Modeling as Matrix Factorization <cite>Yang et al. (2018)</cite> indicated that the training of language models can be interpreted as a matrix 2 Actually, we apply a bias term in addition to the weight matrix but we omit it to simplify the following discussion.",
  "y": "extends"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_6",
  "x": "<cite>Yang et al. (2018)</cite> also argued that rank(A ) is as high as vocabulary size V based on the following two assumptions: 1. Natural language is highly context-dependent. In addition, since we can imagine many kinds of contexts, it is difficult to assume a basis that represents a conditional probability distribution for any contexts. 2. Since we also have many kinds of semantic meanings, it is difficult to assume basic meanings that can create all other semantic meanings by such simple operations as addition and subtraction; compressing V is difficult. In summary, <cite>Yang et al. (2018)</cite> indicated that D h N is much smaller than rank(A) because its scale is usually 10 2 and vocabulary size V is at least 10 4 .",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_7",
  "x": "**PROPOSED METHOD: DIRECT OUTPUT CONNECTION** To construct a high-rank matrix, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS). MoS computes multiple probability distributions from the hidden state of final RNN layer h N and regards the weighted average of the probability distributions as the final distribution.",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_8",
  "x": "To construct a high-rank matrix, <cite>Yang et al. (2018)</cite> proposed Mixture of Softmaxes (MoS). MoS computes multiple probability distributions from the hidden state of final RNN layer h N and regards the weighted average of the probability distributions as the final distribution. In this study, we propose Direct Output Connection (DOC), which is a generalization method of MoS. DOC computes probability distributions from the middle layers in addition to the final layer.",
  "y": "extends"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_9",
  "x": "The top row ( \u2020) represents MoS scores reported in <cite>Yang et al. (2018)</cite> as a baseline. \u2021 represents the perplexity obtained by the implementation of <cite>Yang et al. (2018)</cite> 6 with identical hyperparameters except for i 3 . dropout rate for vector k j,ct and the non-monotone interval.",
  "y": "uses"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_10",
  "x": "The top row ( \u2020) represents MoS scores reported in <cite>Yang et al. (2018)</cite> as a baseline. \u2021 represents the perplexity obtained by the implementation of <cite>Yang et al. (2018)</cite> 6 with identical hyperparameters except for i 3 . dropout rate for vector k j,ct and the non-monotone interval.",
  "y": "uses"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_11",
  "x": "Moreover, the top row of Table 3 shows the perplexity of AWD-LSTM with MoS reported in <cite>Yang et al. (2018)</cite> for comparison. Table 3 indicates that language models using middle layers outperformed one using only the final layer. In addition, Table  3 shows that increasing the distributions from the final layer (i 3 = 20) degraded the score from the language model with i 3 = 15 (the top row of Table 3).",
  "y": "uses"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_12",
  "x": "As shown by this table, the output of AWD-LSTM is restricted to D 3 7 . In contrast, AWD-LSTM-MoS<cite> (Yang et al., 2018)</cite> and AWD-LSTM-DOC outputted matrices whose ranks equal the vocabulary size. This fact indicates that DOC (including MoS) can output the same matrix as the true distributions in view of a rank.",
  "y": "similarities"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_13",
  "x": "We compare AWD-LSTM-DOC with AWD-LSTM (Merity et al., 2018) and AWD-LSTMMoS<cite> (Yang et al., 2018)</cite> . We trained each model with the same hyperparameters from our language modeling experiments (Section 5). We selected the model that achieved the best perplexity on the validation set during the training.",
  "y": "uses"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_14",
  "x": "As described in Section 3, <cite>Yang et al. (2018)</cite> interpreted training language modeling as matrix factorization and improved performance by computing multiple probability distributions. In this study, we generalized their approach to use the middle layers of RNNs. Finally, our proposed method, DOC, achieved the state-of-the-art score on the standard benchmark datasets.",
  "y": "background"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_15",
  "x": "As described in Section 3, <cite>Yang et al. (2018)</cite> interpreted training language modeling as matrix factorization and improved performance by computing multiple probability distributions. In this study, we generalized their approach to use the middle layers of RNNs. Finally, our proposed method, DOC, achieved the state-of-the-art score on the standard benchmark datasets.",
  "y": "extends"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_16",
  "x": "We proposed Direct Output Connection (DOC), a generalization method of MoS introduced by <cite>Yang et al. (2018)</cite> . DOC raises the expressive power of RNN language models and improves quality of the model. DOC outperformed MoS and achieved the best perplexities on the standard benchmark datasets of language modeling: PTB and WikiText-2.",
  "y": "extends"
 },
 {
  "id": "00a2e4d0cacfb1fb7098bd324d960a_17",
  "x": "---------------------------------- **CONCLUSION** We proposed Direct Output Connection (DOC), a generalization method of MoS introduced by <cite>Yang et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_0",
  "x": "**INTRODUCTION** Word embeddings are a crucial component in many NLP approaches (Mikolov et al., 2013; Pennington et al., 2014) since they capture latent semantics of words and thus allow models to better train and generalize. Recent work has moved away from the original \"one word, one embedding\" paradigm to investigate contextualized embedding models (Peters et al., 2017 (Peters et al., , 2018<cite> Akbik et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_1",
  "x": "Recent work has moved away from the original \"one word, one embedding\" paradigm to investigate contextualized embedding models (Peters et al., 2017 (Peters et al., , 2018<cite> Akbik et al., 2018)</cite> . Such approaches produce different embeddings for the same word depending on its context and are thus capable of capturing latent contextualized semantics of ambiguous words. Recently, <cite>Akbik et al. (2018)</cite> proposed a character-level contextualized embeddings ap- context.",
  "y": "background"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_2",
  "x": "Such approaches produce different embeddings for the same word depending on its context and are thus capable of capturing latent contextualized semantics of ambiguous words. Recently, <cite>Akbik et al. (2018)</cite> proposed a character-level contextualized embeddings ap- context. This leads to an underspecified contextual word embedding for the string \"Indra\" that ultimately causes a misclassification of \"Indra\" as an organization (ORG) instead of person (PER) in a downstream NER task.",
  "y": "motivation"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_3",
  "x": "Our proposed approach dynamically builds up a \"memory\" of contextualized embeddings and applies a pooling operation to distill a global contextualized embedding for each word. It requires an embed() function that produces a contextualized embedding for a given word in a sentence context (see <cite>Akbik et al. (2018)</cite> ). It also requires a memory that records for each unique word all previous contextual embeddings, and a pool() operation to pool embedding vectors.",
  "y": "uses"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_4",
  "x": "We contribute our approach and all pre-trained models to the open source FLAIR 1 framework (Akbik et al., 2019) , to ensure reproducibility of these results. Our proposed approach (see Figure 2 ) dynamically builds up a \"memory\" of contextualized embeddings and applies a pooling operation to distill a global contextualized embedding for each word. It requires an embed() function that produces a contextualized embedding for a given word in a 1 https://github.com/zalandoresearch/flair sentence context (see <cite>Akbik et al. (2018)</cite> ).",
  "y": "uses"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_5",
  "x": "To FLAIR, we add an implementation of our proposed pooled contextualized embeddings. Hyperparameters. For our experiments, we follow the training and evaluation procedure outlined in <cite>Akbik et al. (2018)</cite> and follow most hyperparameter suggestions as given by the in-depth study presented in Reimers and Gurevych (2017) .",
  "y": "uses"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_6",
  "x": "Standard word embeddings. The default setup of <cite>Akbik et al. (2018)</cite> recommends contextual string embeddings to be used in combination with standard word embeddings. We use GLOVE embeddings (Pennington et al., 2014) for the English tasks and FASTTEXT embeddings (Bojanowski et al., 2017) for all newswire tasks.",
  "y": "background"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_7",
  "x": "The default setup of <cite>Akbik et al. (2018)</cite> recommends contextual string embeddings to be used in combination with standard word embeddings. We use GLOVE embeddings (Pennington et al., 2014) for the English tasks and FASTTEXT embeddings (Bojanowski et al., 2017) for all newswire tasks. Baselines.",
  "y": "uses similarities"
 },
 {
  "id": "013f0e54384a8a4662a746eb4c30d9_8",
  "x": "Our baseline are contextual string embeddings without pooling, i.e. the original setup proposed in <cite>Akbik et al. (2018)</cite> 2 . By comparing against this baseline, we isolate the impact of our proposed pooled contextualized embeddings. Table 2 : Ablation experiment using contextual string embeddings without word embeddings.",
  "y": "uses"
 },
 {
  "id": "0143619c1c54129702aafb585463d2_0",
  "x": "Citation texts usually highlight certain contributions of the referenced paper and a set of citation texts to a reference paper can provide useful information about that paper. Therefore, citation texts have been previously used to enhance many downstream tasks in IR/NLP such as search and summarization (e.g. [2, 15, 16] ). While useful, citation texts might lack the appropriate context from the reference article<cite> [4,</cite> 5, 18] .",
  "y": "background"
 },
 {
  "id": "0143619c1c54129702aafb585463d2_1",
  "x": "Baselines. To our knowledge, the only published results on TAC 201<cite>4</cite> is<cite> [4]</cite> , where the authors utilized query reformulation (QR) based on UMLS ontology. In addition to<cite> [4]</cite> , we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM: Vector Space Model that was used in<cite> [4]</cite> ; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12] ; and <cite>4</cite>) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10] .",
  "y": "background"
 },
 {
  "id": "0143619c1c54129702aafb585463d2_2",
  "x": "To our knowledge, the only published results on TAC 201<cite>4</cite> is<cite> [4]</cite> , where the authors utilized query reformulation (QR) based on UMLS ontology. In addition to<cite> [4]</cite> , we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM: Vector Space Model that was used in<cite> [4]</cite> ; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12] ; and <cite>4</cite>) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10] . All the baseline parameters are tuned for the best performance, and the same preprocessing is applied to all the baselines and our methods.",
  "y": "extends differences"
 },
 {
  "id": "0143619c1c54129702aafb585463d2_3",
  "x": "This shows the e ectiveness of our models viewed from di erent aspects in comparison with the baselines. The best baseline performance is the query reformulation (QR) method by<cite> [4]</cite> which improves over other baselines. We observe that using general domain embeddings does not provide much advantage in comparison with the best baseline (compare WE wiki and QR in the Table) .",
  "y": "background"
 },
 {
  "id": "0143619c1c54129702aafb585463d2_4",
  "x": "The most relevant prior work to ours is<cite> [4]</cite> where the authors approached the problem using a vector space model similarity ranking and query reformulations. ---------------------------------- **CONCLUSIONS**",
  "y": "similarities"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_0",
  "x": "The key challenge in open RE is to reason jointly over the universal schema consisting of KB relations and surface relations<cite> (Riedel et al., 2013)</cite> . A number of matrix or tensor factorization models have recently been proposed in the context of relation extraction<cite> Riedel et al., 2013</cite>; Huang et al., 2014; . These models use the available data to learn latent semantic representations of entities (or entity pairs) and relations in a domain-independent way; the latent representations are subsequently used to predict new facts.",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_1",
  "x": "The key challenge in open RE is to reason jointly over the universal schema consisting of KB relations and surface relations<cite> (Riedel et al., 2013)</cite> . A number of matrix or tensor factorization models have recently been proposed in the context of relation extraction<cite> Riedel et al., 2013</cite>; Huang et al., 2014; . These models use the available data to learn latent semantic representations of entities (or entity pairs) and relations in a domain-independent way; the latent representations are subsequently used to predict new facts.",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_2",
  "x": "CORE is inspired by the combined factorization and entity model (FE) of<cite> Riedel et al. (2013)</cite> . As FE, CORE associates latent semantic representations with entities, relations, and arguments. In contrast to FE, CORE uses factorization machines (Rendle, 2012) as its underlying framework, which allows us to incorporate context in a flexible way.",
  "y": "extends"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_3",
  "x": "For this reason,<cite> Riedel et al. (2013)</cite> argued and experimentally validated that open RE models can outperform targeted IE methods. Open IE. In contrast to targeted IE, the goal of open IE is to extract all (or most) relations expressed in natural-language text, whether or not these relations are defined in a KB (Banko et al., 2007; Fader et al., 2011; Del Corro and Gemulla, 2013) .",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_4",
  "x": "This assumption generally does not hold for the surface relations extracted by open IE systems<cite> (Riedel et al., 2013)</cite> ; examples of other types of relationships between relations include implication or mutual exclusion. Tensor factorization. Matrix or tensor factorization approaches try to address the above problem: instead of clustering relations, they directly predict facts.",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_5",
  "x": "have also shown that a combination of matrix and tensor factorization models can be fruitful. Closest to our work is the \"universal schema\" matrix factorization approach of<cite> Riedel et al. (2013)</cite> , which combines a latent features model, a neighborhood model and an entity model but does not incorporate context. Our CORE model follows the universal schema idea, but uses a more general factorization model, which includes the information captured by the latent features and entity model (but not the neighborhood model), and incorporates contextual information.",
  "y": "similarities"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_6",
  "x": "This closed-world approach essentially assumes that all unobserved facts are false, which may not be a suitable assumption for the sparsely observed relations of open RE. Following<cite> Riedel et al. (2013)</cite> , we adopt the open-world assumption instead, i.e., we treat each unobserved facts as unknown. Since factorization machines originally require explicit target values (e.g., feedback in recommender systems), we need to adapt parameter estimation to the open-world setting.",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_7",
  "x": "To overcome this problem, the negative sample set needs to be related to x in some way. Since we ultimately use our model to rank tuples for each relation individually, we consider as negative evidence for x only unobserved facts from the same relation<cite> (Riedel et al., 2013)</cite> . In more detail, we (conceptually) build a negative sample set X \u2212 r for each relation r \u2208 R. We include into X \u2212 r all facts r(t)-again, along with their context-such that t \u2208 T is an observed tuple but r(t) is an unobserved fact.",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_8",
  "x": "**EXPERIMENTS** We conducted an experimental study on realworld data to compare our CORE model with other state-of-the-art approaches. 2 Our experimental study closely follows the one of<cite> Riedel et al. (2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_9",
  "x": "Dataset. We made use of the dataset of<cite> Riedel et al. (2013)</cite> , but extended it with contextual information. The dataset consisted of 2.5M surface facts extracted from the New York Times corpus (Sandhaus, 2008) , as well as 16k facts from Freebase.",
  "y": "extends"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_10",
  "x": "From the raw dataset described above, we filtered out all surface relations with less than 10 instances, and all tuples with less than two instances, as in<cite> Riedel et al. (2013)</cite> . Tab. 1 summarizes statistics of the resulting dataset.",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_11",
  "x": "Here we considered a fact or tuple as linked if both of its entities were linked to Freebase, as partiallylinked if only one of its entities was linked, and as non-linked otherwise. In contrast to previous work<cite> (Riedel et al., 2013</cite>; , we retain partially-linked and non-linked facts in our dataset. 3 Further information can be found at htps:// catalog.ldc.upenn.edu/LDC2008T19.",
  "y": "differences"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_12",
  "x": "Open RE models produce predictions for all relations and all tuples. To keep the experimental study feasible and comparable to previous studies, we use the full training data but evaluate each model's predictions on only the subsample of 10k tuples (\u2248 6% of all tuples) of<cite> Riedel et al. (2013)</cite> . The subsample consisted of 20% linked, 40% partially-linked and 40% non-linked tuples.",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_13",
  "x": "Our study focused on these two factorization models because they outperformed other models (including nonfactorization models) in previous studies<cite> (Riedel et al., 2013</cite>; . All models were trained with the full training data described above. PITF (Drumond et al., 2012) .",
  "y": "differences"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_15",
  "x": "PITF is based on factorization machines so that we used our scalable CORE implementation for training the model. NFE<cite> (Riedel et al., 2013)</cite> . NFE is the full model proposed in the \"universal schema\" work of<cite> Riedel et al. (2013)</cite> .",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_16",
  "x": "The F and E models together are similar (but not equal) to our CORE model without context. The NFE model outperformed tensor models as well as clustering methods and distantly supervised methods in the experimental study of<cite> Riedel et al. (2013)</cite> for open RE tasks. We use the original source code of<cite> Riedel et al. (2013)</cite> for training.",
  "y": "background"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_17",
  "x": "The F and E models together are similar (but not equal) to our CORE model without context. The NFE model outperformed tensor models as well as clustering methods and distantly supervised methods in the experimental study of<cite> Riedel et al. (2013)</cite> for open RE tasks. We use the original source code of<cite> Riedel et al. (2013)</cite> for training.",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_18",
  "x": "NFE<cite> (Riedel et al., 2013)</cite> . NFE is the full model proposed in the \"universal schema\" work of<cite> Riedel et al. (2013)</cite> . The NFE model outperformed tensor models as well as clustering methods and distantly supervised methods in the experimental study of<cite> Riedel et al. (2013)</cite> for open RE tasks. We use the original source code of<cite> Riedel et al. (2013)</cite> for training.",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_19",
  "x": "4 Our version adds support for BPR and parallelizes the training algorithm. Methodology. To evaluate the prediction performance of each method, we followed<cite> Riedel et al. (2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_20",
  "x": "The denominator is included to account for the fact that the evaluation set may include less than 100 true facts. MAP 100 # reflects how many true facts are found by each method as well as their ranking. If all # facts are found and ranked top, then MAP 100 # = 1. Note that our definition of MAP 100 # differs slightly from<cite> Riedel et al. (2013)</cite> ; our metric is more robust because it is based on completely labeled evaluation data.",
  "y": "differences"
 },
 {
  "id": "029def00e36495beb31bde4cc87298_21",
  "x": "These choices correspond to the ones of<cite> Riedel et al. (2013)</cite> ; no further tuning was performed. ---------------------------------- **RESULTS.**",
  "y": "uses"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_0",
  "x": "Ponzetto and Strube (2006) and <cite>Ratinov and Roth (2012)</cite> precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of mentions to the external entities is itself a difficult task, these systems favor high-precision linking.",
  "y": "background"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_1",
  "x": "Unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer (such as the non-transcript documents in <cite>Ratinov and Roth (2012)</cite> ). Alternatively, Rahman and Ng (2011) link each mention to multiple entities in the knowledge base, improving recall at the cost of lower precision; the attributes of all the linked entities are aggregated as features. Although this approach is more robust to noise in the documents, the features of a mention merge the different aspects of the entities, for example a \"Michael Jordan\" mention will contain features for both the scientist and basketball personas.",
  "y": "background"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_2",
  "x": "Our paper provides the following contributions: (1) an approach that jointly reasons about both within-doc entities and their alignment to KBentities by dynamically adjusting a ranked list of candidate alignments, during coreference, (2) Utilization of a larger set of surface string variations for each entity candidate by using links that appear all over the web (Spitkovsky and Chang, 2012) , (3) A combination of these approaches that improves upon a competitive baseline without a knowledge base by 1.09 B 3 F1 points on the ACE 2004 data, and outperforms the state-of-the-art coreference system (Stoyanov and Eisner, 2012) by 0.41 B 3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in <cite>Ratinov and Roth (2012)</cite> , and documents that contain a large number of mentions. ---------------------------------- **BASELINE PAIRWISE SYSTEM**",
  "y": "uses"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_3",
  "x": "These queries return scored, ranked lists of entity candidates (Wikipedia articles), which we associate with each proper noun mention, leaving the rest of the candidate lists empty. Linking is often noisy, so only selecting the high-precision links as in <cite>Ratinov and Roth (2012)</cite> results in too few matches, while picking an aggregation of all links results in more noise due to lower precision (Rahman and Ng, 2011) . Additionally, since linking is often performed in pre-processing, two mentions that are determined coreferent during inference could still be linked to different KB entities.",
  "y": "background"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_4",
  "x": "Linking is often noisy, so only selecting the high-precision links as in <cite>Ratinov and Roth (2012)</cite> results in too few matches, while picking an aggregation of all links results in more noise due to lower precision (Rahman and Ng, 2011) . To avoid these problems, we keep a list of candidate links for each mention, merging the lists when two mentions are determined coreferent, and rerank this list during inference.",
  "y": "differences"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_5",
  "x": "Each entity has access to external information keyed on the Wikipedia article, but this information could more generally come from any knowledge base. Given these entities, there are many possible features that may be used for disambiguation of the mentions, such as gender and fine-grained Wikipedia categories as used by <cite>Ratinov and Roth (2012)</cite> , however most of these features may not be relevant to the task of within-document coreference. Instead, an important resource for linking non-proper mentions of an entity is to identify the possible name variations of the entity.",
  "y": "background"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_6",
  "x": "Given these entities, there are many possible features that may be used for disambiguation of the mentions, such as gender and fine-grained Wikipedia categories as used by <cite>Ratinov and Roth (2012)</cite> , however most of these features may not be relevant to the task of within-document coreference. 1 We instead use the corpus described in Spitkovsky and Chang (2012) that consists of anchor texts of links to Wikipedia that appear on web pages.",
  "y": "differences"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_7",
  "x": "Static linking: Identical to dynamic linking except that entity candidate lists are not merged during inference (i.e., Algorithm 1 without line 17). This approach is comparable to the fixed alignment model, as in the approaches of Ponzetto and Strube (2006) and <cite>Ratinov and Roth (2012)</cite> . ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_8",
  "x": "Since these transcripts provide an additional challenge for alignment and coreference, <cite>Ratinov and Roth (2012)</cite> only use the set of non-transcripts for their evaluation. Using dynamic linking and a large set of surface string variations, our approach may be able to provide an improvement even on the transcripts. To identify the transcripts in the test set, we use the approximation from <cite>Ratinov and Roth (2012)</cite> that considers a document to be non-transcribed if it contains proper noun mentions and at least a third of those start with a capital letter.",
  "y": "differences"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_9",
  "x": "To identify the transcripts in the test set, we use the approximation from <cite>Ratinov and Roth (2012)</cite> that considers a document to be non-transcribed if it contains proper noun mentions and at least a third of those start with a capital letter. The performance is shown in Table 3 , while the improvement over our baseline is shown in Figure 3 . Our static linking matches the performance of <cite>Ratinov and Roth (2012)</cite> on the non-transcripts.",
  "y": "uses"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_10",
  "x": "To identify the transcripts in the test set, we use the approximation from <cite>Ratinov and Roth (2012)</cite> that considers a document to be non-transcribed if it contains proper noun mentions and at least a third of those start with a capital letter. The performance is shown in Table 3 , while the improvement over our baseline is shown in Figure 3 . Our static linking matches the performance of <cite>Ratinov and Roth (2012)</cite> on the non-transcripts.",
  "y": "similarities"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_11",
  "x": "We improve upon this approach by maintaining a list of the candidate entities for each mention cluster, modifying this list during the course of inference, and using features from only the top-ranked candidate at any time. Further, they do not provide a comparison on a standard dataset. <cite>Ratinov and Roth (2012)</cite> extend the multi-sieve coreference model (Raghunathan et al., 2010) by identifying at most a single candidate for each mention, and incorporating high-precision attributes extracted from Wikipedia.",
  "y": "background"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_12",
  "x": "<cite>Ratinov and Roth (2012)</cite> extend the multi-sieve coreference model (Raghunathan et al., 2010) by identifying at most a single candidate for each mention, and incorporating high-precision attributes extracted from Wikipedia. With these restrictions, they show improvements over the state-ofthe-art on a subset of ACE mentions that are more easily aligned to Wikipedia, while our approach demonstrates improvements on the complete set of mentions including the tougher to link mentions from the transcripts.",
  "y": "differences"
 },
 {
  "id": "03c57679549ff600a024d436d5a107_13",
  "x": "There are a number of approaches that provide an alignment from mentions in a document to Wikipedia. Wikifier (Ratinov et al., 2011) analyzes the context around the mentions and the entities jointly, and was used to align mentions for coreference in <cite>Ratinov and Roth (2012)</cite> . Dalton and Dietz (2013) introduce an approximation to the above approach, but incorporate retrieval-based supervised reranking that provides multiple candidates and scores; this approach performed competitively on previous TAC-KBP entity linking benchmarks (Dietz and Dalton, 2012) .",
  "y": "background"
 },
 {
  "id": "04461d946dadc759e4be1207655159_0",
  "x": "---------------------------------- **INTRODUCTION** Chiang's hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; <cite>Chiang, 2007)</cite> and has been widely adopted in statistical machine translation (SMT).",
  "y": "background"
 },
 {
  "id": "04461d946dadc759e4be1207655159_1",
  "x": "**HEAD-DRIVEN HPB TRANSLATION MODEL** Like Chiang (2005) and<cite> Chiang (2007)</cite> , our HD-HPB translation model adopts a synchronous context free grammar, a rewriting system which generates source and target side string pairs simultaneously using a context-free grammar. Instead of collapsing all non-terminals in the source language into a single symbol X as in<cite> Chiang (2007)</cite> , given a word sequence f i j from position i to position j, we first find heads and then concatenate the POS tags of these heads as f i j 's non-terminal symbol.",
  "y": "similarities"
 },
 {
  "id": "04461d946dadc759e4be1207655159_2",
  "x": "Like Chiang (2005) and<cite> Chiang (2007)</cite> , our HD-HPB translation model adopts a synchronous context free grammar, a rewriting system which generates source and target side string pairs simultaneously using a context-free grammar. Instead of collapsing all non-terminals in the source language into a single symbol X as in<cite> Chiang (2007)</cite> , given a word sequence f i j from position i to position j, we first find heads and then concatenate the POS tags of these heads as f i j 's non-terminal symbol. Specifically, we adopt unlabeled dependency structure to derive heads, which are defined as:",
  "y": "differences"
 },
 {
  "id": "04461d946dadc759e4be1207655159_3",
  "x": "It is worth noting that in this paper we only refine non-terminal X on the source side to headinformed ones, while still using X on the target side. According to the occurrence of terminals in translation rules, we group rules in the HD-HPB model into two categories: head-driven hierarchical rules (HD-HRs) and non-terminal reordering rules (NRRs), where the former have at least one terminal on both source and target sides and the later have no terminals. For rule extraction, we first identify initial phrase pairs on word-aligned sentence pairs by using the same criterion as most phrase-based translation models (Och and Ney, 2004 ) and Chiang's HPB model (Chiang, 2005; <cite>Chiang, 2007)</cite> .",
  "y": "similarities"
 },
 {
  "id": "04461d946dadc759e4be1207655159_4",
  "x": "As mentioned, a HD-HR has at least one terminal on both source and target sides. This is the same as the hierarchical rules defined in Chiang's HPB model<cite> (Chiang, 2007)</cite> , except that we use head POSinformed non-terminal symbols in the source language. We look for initial phrase pairs that contain other phrases and then replace sub-phrases with POS tags corresponding to their heads.",
  "y": "similarities differences"
 },
 {
  "id": "04461d946dadc759e4be1207655159_5",
  "x": "Given the word alignment in Figure 1 , Table 1 demonstrates the difference between hierarchical rules in<cite> Chiang (2007)</cite> and HD-HRs defined here. Similar to Chiang's HPB model, our HD-HPB model will result in a large number of rules causing problems in decoding. To alleviate these problems, we filter our HD-HRs according to the same constraints as described in<cite> Chiang (2007)</cite> .",
  "y": "differences"
 },
 {
  "id": "04461d946dadc759e4be1207655159_6",
  "x": "Similar to Chiang's HPB model, our HD-HPB model will result in a large number of rules causing problems in decoding. To alleviate these problems, we filter our HD-HRs according to the same constraints as described in<cite> Chiang (2007)</cite> . Moreover, we discard rules that have non-terminals with more than four heads.",
  "y": "similarities uses"
 },
 {
  "id": "04461d946dadc759e4be1207655159_8",
  "x": "Given e for the translation output in the target language, s and t for strings of terminals and nonterminals on the source and target side, respectively, we use a feature set analogous to the default feature set of<cite> Chiang (2007)</cite> , including: \u2022 P hd-hr (t|s) and P hd-hr (s|t), translation probabilities for HD-HRs; \u2022 P lex (t|s) and P lex (s|t), lexical translation probabilities for HD-HRs;",
  "y": "similarities"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_0",
  "x": "<cite>Weighted Textual Matrix Factorization [WTMF]</cite> (<cite>Guo and Diab, 2012b</cite> ) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large margin in the SS task, yielding state-of-the-art performance on the LI06 (Li et al., 2006 ) SS dataset. However, all of these models make harsh simplifying assumptions on how a token is generated: (1) in LSA/<cite>WTMF</cite>, a token is generated by the inner product of the word latent vector and the document latent vector; (2) in LDA, all the tokens in a document are sampled from the same document level topic distribution. Under this framework, they ignore rich linguistic phenomena such as inter-word dependency, semantic scope of words, etc.",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_1",
  "x": "SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012) , resulting in inadequate evidence to generalize to robust sentential semantics. <cite>Weighted Textual Matrix Factorization [WTMF]</cite> (<cite>Guo and Diab, 2012b</cite> ) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large margin in the SS task, yielding state-of-the-art performance on the LI06 (Li et al., 2006 ) SS dataset. However, all of these models make harsh simplifying assumptions on how a token is generated: (1) in LSA/<cite>WTMF</cite>, a token is generated by the inner product of the word latent vector and the document latent vector; (2) in LDA, all the tokens in a document are sampled from the same document level topic distribution.",
  "y": "motivation"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_2",
  "x": "However, in the SS setting, it is crucial to make good use of each word, given the limited number of words in a sentence. We believe a reasonable word generation story will avoid introducing noise in sentential semantics, encouraging robust lexical semantics which can further boost the sentential semantics. In this paper, we explicitly encode lexical semantics, both corpus-based and knowledge-based information, in the <cite>WTMF</cite> model, by which we are able to achieve even better results in SS task.",
  "y": "uses motivation"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_3",
  "x": "The additional corpus-based information we exploit is selectional preference semantics (Resnik, 1997) , a feature already existing in the data yet ignored by most latent variable models. Selectional preference focuses on the admissible arguments for a word, thus capturing more nuanced semantics than the sentence IDs (when applied to a corpus of sentences as opposed to documents). Consider the following example: In <cite>WTMF</cite>/LSA/LDA, a word will receive semantics from all the other words in a sentence, hence, the word oil, in the above example, will be assigned the incorrect finance topic that reflects the sentence level semantics.",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_4",
  "x": "To our best knowledge, this is the first work to model selectional preference for sentence/document semantics. We also integrate knowledge-based semantics in the <cite>WTMF</cite> framework. Knowledge-based semantics, a human-annotated clean resource, is an important complement to corpus-based noisy cooccurrence information.",
  "y": "extends"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_5",
  "x": "---------------------------------- **WEIGHTED TEXTUAL MATRIX FACTORIZATION** <cite>Our</cite> previous work (<cite>Guo and Diab, 2012b</cite> ) models the sentences in the weighted matrix factorization framework ( Figure 1 ).",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_6",
  "x": "Leveraging these pairs, an infrequent word such as purchase can exploit robust latent vectors from its synonyms such as buy. Similar words pairs can be seamlessly modeled in <cite>WTMF</cite>, since in the matrix factorization framework a latent vector profile is explicitly created for each word, while in LDA all the data structures are designed for documents/sentences. We construct a graph to connect words according to the extracted similar word pairs, to encourage similar words to share similar latent vector profiles.",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_7",
  "x": "Leveraging these pairs, an infrequent word such as purchase can exploit robust latent vectors from its synonyms such as buy. Similar words pairs can be seamlessly modeled in <cite>WTMF</cite>, since in the matrix factorization framework a latent vector profile is explicitly created for each word, while in LDA all the data structures are designed for documents/sentences. We construct a graph to connect words according to the extracted similar word pairs, to encourage similar words to share similar latent vector profiles.",
  "y": "motivation uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_8",
  "x": "The graphical model of <cite>WTMF</cite> is illustrated in Figure 2a . A w i /s j node is a latent vector P \u00b7,i /Q \u00b7,j , corresponding to a word/sentence, respectively. A shaded node is a non-zero cell in X, representing an observed token in a sentence.",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_9",
  "x": "In (<cite>Guo and Diab, 2012b</cite>) <cite>we</cite> use Alternating Least Square [ALS] for inference, which is to set the derivative of equation 1 for P/Q to 0 and iteratively compute P/Q by fixing the other matrix (Srebro and Jaakkola, 2003) . However, it is no longer applicable with the new term (equation 2) involving the length of word vectors |P \u00b7,i |. Therefore we approximate the objective function by treating the vector length |P \u00b7,i | as fixed values during the ALS iterations:",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_10",
  "x": "**INFERENCE** In (<cite>Guo and Diab, 2012b</cite>) <cite>we</cite> use Alternating Least Square [ALS] for inference, which is to set the derivative of equation 1 for P/Q to 0 and iteratively compute P/Q by fixing the other matrix (Srebro and Jaakkola, 2003) . However, it is no longer applicable with the new term (equation 2) involving the length of word vectors |P \u00b7,i |.",
  "y": "motivation"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_11",
  "x": "Therefore we approximate the objective function by treating the vector length |P \u00b7,i | as fixed values during the ALS iterations: where P \u00b7,s(i) are the latent vectors of similar words of word i; the length of these vectors in the current iteration are stored in L s(i) (similarly L i is the current length of P \u00b7,i ) (cf. (Steck, 2010; <cite>Guo and Diab, 2012b</cite>) for optimization details). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_12",
  "x": "In (<cite>Guo and Diab, 2012b</cite>) <cite>we</cite> use Alternating Least Square [ALS] for inference, which is to set the derivative of equation 1 for P/Q to 0 and iteratively compute P/Q by fixing the other matrix (Srebro and Jaakkola, 2003) . However, it is no longer applicable with the new term (equation 2) involving the length of word vectors |P \u00b7,i |. Therefore we approximate the objective function by treating the vector length |P \u00b7,i | as fixed values during the ALS iterations: where P \u00b7,s(i) are the latent vectors of similar words of word i; the length of these vectors in the current iteration are stored in L s(i) (similarly L i is the current length of P \u00b7,i ) (cf. (Steck, 2010; <cite>Guo and Diab, 2012b</cite>) for optimization details).",
  "y": "extends"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_13",
  "x": "We build the model WTMF+PK on the same corpora as used in <cite>our</cite> previous work (<cite>Guo and Diab, 2012b</cite>) , comprising the following: Brown corpus (each sentence is treated as a document), sense definitions from Wiktionary and Wordnet (only definitions without target words and usage examples). We follow the preprocessing steps in (Guo and Diab, 2012c) : tokenization, pos-tagging, lemmatization and further merge lemmas. The corpus is used for building matrix X. The evaluation datasets are LI06 dataset and Semeval-2012 STS [STS12] (Agirre et al., 2012) dataset.",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_14",
  "x": "We include three baselines LSA, LDA and <cite>WTMF</cite> using the setting described in (<cite>Guo and Diab, 2012b</cite>) . We run Gibbs Sampling based LDA for 2000 iterations and average the model over the last 10 iterations. For <cite>WTMF</cite>, we run 20 iterations and fix the missing words weight at w m = 0.01 with a regularization coefficient set at \u03bb = 20, which is the best condition found in (<cite>Guo and Diab, 2012b</cite>) .",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_15",
  "x": "For <cite>WTMF</cite>, we run 20 iterations and fix the missing words weight at w m = 0.01 with a regularization coefficient set at \u03bb = 20, which is the best condition found in (<cite>Guo and Diab, 2012b</cite>) . Table 1 summarizes the results at dimension K = 100 (the dimension of latent vectors). To remove randomness, each reported number is the averaged results of 10 runs.",
  "y": "uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_16",
  "x": "We include three baselines LSA, LDA and <cite>WTMF</cite> using the setting described in (<cite>Guo and Diab, 2012b</cite>) . Table 1 shows <cite>WTMF</cite> is already a very strong baseline: it outperforms LSA and LDA by a large margin.",
  "y": "uses background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_17",
  "x": "Based on the STS tuning set, we experiment with different values for the selectional preference weight (\u03b3 = {0, 1, 2}), and likewise for the similar word pairs weight varying the \u03b4 value as follows \u03b4 = {0, 0.1, 0.3, 0.5, 0.7}. The performance on STS12 tuning and test dataset as well as on the LI06 dataset are illustrated in Figures 3a,  3b and 3d . The parameters of model 6 in Table 1 (\u03b3 = 2, \u03b4 = 0.3) are the chosen values based on tuning set performance. Table 1 shows <cite>WTMF</cite> is already a very strong baseline: it outperforms LSA and LDA by a large margin.",
  "y": "differences uses"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_18",
  "x": "Same as in (<cite>Guo and Diab, 2012b</cite>) , LSA performance degrades dramatically when trained on a corpus of sentence sized documents, yielding results worse than the surface words baseline 31% (Agirre et al., 2012) . Using corpus-based selectional preference semantics alone (model 4 WTMF+P in Table  1 ) boosts the performance of <cite>WTMF</cite> by +1.17% on the test set, while using knowledge-based semantics alone (model 5 WTMF+K) improves the over the <cite>WTMF</cite> results by an absolute +2.31%. Combining them (model 6 WTMF+PK) yields the best results, with an absolute increase of +3.39%, which suggests that the two sources of semantic evidence are useful, but more importantly, they are complementary for each other.",
  "y": "similarities"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_19",
  "x": "Table 1 shows <cite>WTMF</cite> is already a very strong baseline: it outperforms LSA and LDA by a large margin. Same as in (<cite>Guo and Diab, 2012b</cite>) , LSA performance degrades dramatically when trained on a corpus of sentence sized documents, yielding results worse than the surface words baseline 31% (Agirre et al., 2012) . Using corpus-based selectional preference semantics alone (model 4 WTMF+P in Table  1 ) boosts the performance of <cite>WTMF</cite> by +1.17% on the test set, while using knowledge-based semantics alone (model 5 WTMF+K) improves the over the <cite>WTMF</cite> results by an absolute +2.31%.",
  "y": "extends differences"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_20",
  "x": "The trends hold in different parameter conditions with a consistent improvement. Figure 3c illustrates the impact of dimension K = {50, 75, 100, 125, 150} on <cite>WTMF</cite> and WTMF+PK. Generally a larger K leads to a higher Pearson correlation, but the improvement is tiny when K \u2265 100 (0.1% increase).",
  "y": "uses differences"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_21",
  "x": "3 While trained with this experimental setting, our model WTMF+PK (\u03b3 = 2, \u03b4 = 0.3, K = 100) is able to reach an even higher correlation of 72.0%. Figure 3d presents the results obtained on the LI06 data set at different weight values for the corpusbased selectional preference semantics \u03b3 and for the knowledge-based semantics \u03b4. Our previous experiments (<cite>Guo and Diab, 2012b</cite>) show that <cite>WTMF</cite> is the state-of-the-art model on LI06.",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_22",
  "x": "With lexical semantics explicitly modeled, WTMF+PK yields better results than <cite>WTMF</cite> (see Table 1 ). It should be noted that LI06 prefers a smaller similar word pair weight ( a \u03b4 = 0.1 yields the best performance around of 90.75%), yet in almost all conditions WTMF+PK outperforms <cite>WTMF</cite> as shown in Figure 3d . ----------------------------------",
  "y": "differences background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_23",
  "x": "We will refer to our proposed novel model as WTMF+PK. With lexical semantics explicitly modeled, WTMF+PK yields better results than <cite>WTMF</cite> (see Table 1 ).",
  "y": "differences"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_24",
  "x": "With lexical semantics explicitly modeled, WTMF+PK yields better results than <cite>WTMF</cite> (see Table 1 ). It should be noted that LI06 prefers a smaller similar word pair weight ( a \u03b4 = 0.1 yields the best performance around of 90.75%), yet in almost all conditions WTMF+PK outperforms <cite>WTMF</cite> as shown in Figure 3d . ----------------------------------",
  "y": "differences background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_25",
  "x": "We will refer to our proposed novel model as WTMF+PK. It should be noted that LI06 prefers a smaller similar word pair weight ( a \u03b4 = 0.1 yields the best performance around of 90.75%), yet in almost all conditions WTMF+PK outperforms <cite>WTMF</cite> as shown in Figure 3d .",
  "y": "differences"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_26",
  "x": "Researchers (O'Shea et al., 2008) find LSA does not yield good performance. In (<cite>Guo and Diab, 2012b</cite>; Guo and Diab, 2012c) , we show the superiority of the latent space approach in <cite>WTMF</cite>. In this paper, we improve the <cite>WTMF</cite> model and achieve state-of-the-art Pearson correlation on two standard SS datasets.",
  "y": "background"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_27",
  "x": "Researchers (O'Shea et al., 2008) find LSA does not yield good performance. In (<cite>Guo and Diab, 2012b</cite>; Guo and Diab, 2012c) , we show the superiority of the latent space approach in <cite>WTMF</cite>. In this paper, we improve the <cite>WTMF</cite> model and achieve state-of-the-art Pearson correlation on two standard SS datasets.",
  "y": "extends differences"
 },
 {
  "id": "048944feaff977c8cf057d52594c72_28",
  "x": "Researchers (O'Shea et al., 2008) find LSA does not yield good performance. In (<cite>Guo and Diab, 2012b</cite>; Guo and Diab, 2012c) , we show the superiority of the latent space approach in <cite>WTMF</cite>. In this paper, we improve the <cite>WTMF</cite> model and achieve state-of-the-art Pearson correlation on two standard SS datasets.",
  "y": "extends differences"
 },
 {
  "id": "053ce92029e643bfade157b3172c05_0",
  "x": "In order to ease the process of engineering such a large grammar, we have made use of the lexical knowledge representation language DATR (Evans & Gazdar, 1996) to compactly encode the elementary trees <cite>(Evans et al., 1995</cite>; Smets & Evans, 1998) . In Section 5 we present some figures that show how the size of the encoding of the grammar has increased during the grammar development process as the number and complexity of elementary trees has grown. We have addressed problems that result from trying to parse with such a large grammar by using a technique proposed by (Evans & Weir, 1997) and (Evans & Weir, 1998) in which all the trees that each word can anchor are compactly represented using a collection of finite state automata.",
  "y": "uses"
 },
 {
  "id": "053ce92029e643bfade157b3172c05_1",
  "x": "---------------------------------- **ENCODING FOR GRAMMAR DEVELOPMENT** Following<cite> (Evans et al., 1995)</cite> and (Smets & Evans, 1998 ) the LEXSYS grammar is encoded using DATR, a non-monotonic knowledge representation language.",
  "y": "similarities uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_0",
  "x": "In this paper, we describe the DeepNNNER entry to The 2nd Workshop on Noisy User-generated Text (WNUT) Shared Task #2: Named Entity Recognition in Twitter. Our shared task submission adopts the bidirectional LSTM-CNN model of<cite> Chiu and Nichols (2016)</cite>, as it has been shown to perform well on both newswire and Web texts. It uses word embeddings trained on large-scale Web text collections together with text normalization to cope with the diversity in Web texts, and lexicons for target named entity classes constructed from publicly-available sources.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_1",
  "x": "Recently, neural network models -especially those that use recursive models -have shown that state of the art performance can be achieved with little feature engineering (Collobert et al., 2011; Santos et al., 2015; <cite>Chiu and Nichols, 2016)</cite> . However, despite their popularity for NER on newswire texts, neural networks have not been widely adopted for NER on Web texts, with the exception of the feed-forward neural network (FFNN) model of Godin et al. (2015) . In this paper, we present the DeepNNNER entry to the WNUT 2016 Shared Task #2: Named Entity Recognition in Twitter.",
  "y": "background"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_2",
  "x": "However, despite their popularity for NER on newswire texts, neural networks have not been widely adopted for NER on Web texts, with the exception of the feed-forward neural network (FFNN) model of Godin et al. (2015) . In this paper, we present the DeepNNNER entry to the WNUT 2016 Shared Task #2: Named Entity Recognition in Twitter. Our shared task submission is based on the model of<cite> Chiu and Nichols (2016)</cite> , a hybrid model of bidirectional long short-term memory (BLSTM) networks and convolutional neural networks (CNN) that automatically learns both character-and word-level features, and which holds the current state-of-the-art on both newswire texts (CoNLL 2003) and diverse corpora including Web texts (OntoNotes 5.0).",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_3",
  "x": "The character-level CNN allows our model to learn relevant features from the orthography of words, which is important in task where unseen words are commonplace. Finally, it also encodes partial lexicon matches in neural networks, allowing it to make effective use of lexical knowledge. Our primary contribution is adapting the model of<cite> Chiu and Nichols (2016)</cite> to Twitter data by developing a text normalization method to effectively apply word embeddings to large vocabulary Web texts and automatically constructing lexicons for the shared task's target NE classes from publicly-available sources.",
  "y": "extends"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_4",
  "x": "In Section 2, we describe the adaptations made to<cite> Chiu and Nichols (2016)</cite> 's model. In Section 3, we describe the evaluation methodology. In Section 4, we discuss the results and present an error analysis.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_5",
  "x": "In this section, we describe the architecture of our shared task submission. An overview is given Figure 1 . Our system is based on the BLSTM-CNN model of<cite> Chiu and Nichols (2016)</cite> , and, unless otherwise noted, follows their training and tagging methodology, which the reader is referred to for more details.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_6",
  "x": "The neural embeddings of Collobert et al. (2011) were chosen because<cite> Chiu and Nichols (2016)</cite> reported them to be the highest performing on both CoNLL-2003 and OntoNotes 5.0 datasets. To evaluate embeddings trained on data closer to the WNUT dataset, we also selected the GloVe embeddings of Pennington et al. (2014) , trained on both Web text and tweets, and word2vec embeddings trained on Google News data (Mikolov et al., 2013 ) and on tweets (Godin et al., 2015) . Preliminary evaluation on the Dev1 data showed that GloVe 27B outperformed Collobert's embeddings (see Table 5 ) and word2vec 3B, so they were used in our submission.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_7",
  "x": "Following<cite> Chiu and Nichols (2016)</cite> , we use a CNN to extract features from 25 dim. character embeddings randomly-initialized from a uniform distribution between -0.5 and 0.5. To accommodate text normalization, we added embeddings for the normalization symbols described in Section 2.2, namely <url>, <user>, <smile>, <lolface>, <sadface>, <neutralface>, <heart>, <number> and <hashtag>. All experiments were conducted with the same character embeddings.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_8",
  "x": "To generate lexicon features, we apply the partial matching algorithm of<cite> Chiu and Nichols (2016)</cite> to the input text, as shown in Figure 2 . Each lexicon and match type (BIOES) is associated with a randomly-initialized 5 dim. embedding.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_9",
  "x": "---------------------------------- **CAPITALIZATION FEATURE** Following<cite> Chiu and Nichols (2016)</cite>, we used different symbols for word-level capitalization feature each assigned a randomly initialized embedding: allCaps, upperInitial, lowercase, mixedCaps and noinfo.",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_10",
  "x": "---------------------------------- **TRAINING AND INFERENCE** We follow the training and inference methodology of<cite> Chiu and Nichols (2016)</cite> , training our neural network to maximize the sentence-level log-likelihood from Collobert et al. (2011) .",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_11",
  "x": "For a more detailed survey, see<cite> (Chiu and Nichols, 2016)</cite> . Most recent approaches to NER have been characterized by the use of CRF, SVM, and perceptron models, where performance is heavily dependent on feature engineering. Ratinov and Roth (2009) used non-local features, a gazetteer extracted from Wikipedia, and Brown-cluster-like word representations.",
  "y": "background"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_12",
  "x": "Our system adopts the architecture of<cite> Chiu and Nichols (2016)</cite> , which combined BLSTMs to maximize context over the tagged word sequence and word-level CNNs to automatically generate characterlevel features with a partial-matching lexicon to achieve the state-of-the-art for NER on both CoNLL 2003 and OntoNotes datasets. Our system can be viewed as an investigation into how well state-of-theart neural approaches adapt to the challenges of NER on noisy Web data. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "05bf376f0a18cf313ead7189b029b6_13",
  "x": "**CONCLUSION** In this paper, we described the DeepNNNER entry to the WNUT 2016 Shared Task #2: Named Entity Recognition in Twitter, which adopted the BLSTM-CNN model of<cite> Chiu and Nichols (2016)</cite> . Extensive evaluation showed that high word type coverage for word embeddings is crucial to NER performance, likely due to rare words in entities, and that both text normalization and partial matching on lexicons constructed from DBpedia (Auer et al., 2007) contribute significantly to performance.",
  "y": "uses"
 },
 {
  "id": "062e9348de5fda68e61fff3ca4f186_0",
  "x": "P W captures the intuition that the connection between two sentences is stronger the more entities they share by means of weighted edges, where the weights equal the number of entities shared by sentences (Newman, 2004) . The third type of projection, P Acc , integrates syntactic information in the edge weights calculated by the following formula: While the entity grid <cite>(Barzilay and Lapata, 2008)</cite> uses information about sentences which do not share entities by means of the \"--\" transition, the entity graph cannot employ this negative information. Here, we propose a normalization for the entity graph and its corresponding one-mode projections which is based on the relative importance of entities and, in turn, the relative importance of sentences.",
  "y": "background motivation"
 },
 {
  "id": "062e9348de5fda68e61fff3ca4f186_1",
  "x": "Table 1 shows the results. Results for Guinaudeau and Strube (2013) , G&S, are reproduced, results for<cite> Barzilay and Lapata (2008)</cite> , B&L, and Elsner and Charniak (2011) , E&C, were reproduced by Guinaudeau and Strube (2013) . The unweighted graph, P U , does not need normalization.",
  "y": "background"
 },
 {
  "id": "062e9348de5fda68e61fff3ca4f186_2",
  "x": "**SUMMARY COHERENCE RATING** We follow<cite> Barzilay and Lapata (2008)</cite> for evaluating whether the normalized entity graph can decide whether automatic or human summaries are more coherent (80 pairs of summaries extracted from DUC 2003). Human coherence scores are associated with each pair of summarized documents <cite>(Barzilay and Lapata, 2008)</cite> .",
  "y": "uses"
 },
 {
  "id": "062e9348de5fda68e61fff3ca4f186_3",
  "x": "We follow<cite> Barzilay and Lapata (2008)</cite> for evaluating whether the normalized entity graph can decide whether automatic or human summaries are more coherent (80 pairs of summaries extracted from DUC 2003). Human coherence scores are associated with each pair of summarized documents <cite>(Barzilay and Lapata, 2008)</cite> . Table 3 displays reported results of B&L and reproduced results of the entity graph and our normalized entity graph.",
  "y": "uses"
 },
 {
  "id": "062e9348de5fda68e61fff3ca4f186_4",
  "x": "In experiments,<cite> Barzilay and Lapata (2008)</cite> assume that articles taken from Encyclopedia Britannica are more difficult to read (less coherent) than the corresponding articles from Encyclopedia Britannica Elementary, its version for children. We follow them with regard to data (107 article pairs), experimental setup and evaluation. Sentences in the Britannica Elementary are simpler and shorter than in the Encyclopedia Britannica. The entity graph does not take into account the effect of entities not shared between sentences while the normalized entity graph assigns a lower weight if there are more of these entities. Hence, Britannica Elementary receives a higher cohesion score than Encyclopedia Britannica in our model.",
  "y": "uses background motivation"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_0",
  "x": "Discourse analysis is one of the most challenging tasks in Natural Language Processing, that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc. (see (Webber et al., 2011) and (Taboada and Mann, 2006) for detailed review). With the availability of annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008) , statistical discourse parsers were developed (Lin et al., 2012;<cite> Ghosh et al., 2011</cite>; Xu et al., 2012) .",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_1",
  "x": "In the first approach the parser decision is not conditioned on whether the relation is intra-or inter-sentential (e.g.<cite> (Ghosh et al., 2011)</cite> ). In the second approach relations are parsed separately for each class (e.g. (Lin et al., 2012; Xu et al., 2012) ). In the former approach argument span extraction is applied right after discourse connective detection, while the latter approach also requires argument position classification.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_2",
  "x": "In the former approach argument span extraction is applied right after discourse connective detection, while the latter approach also requires argument position classification. The decision on argument span can be made on different levels: from token-level to sentence-level. In<cite> (Ghosh et al., 2011</cite> ) the decision is made on tokenlevel, and the problem is cast as sequence labeling using conditional random fields (CRFs) (Lafferty et al., 2001) .",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_3",
  "x": "In<cite> (Ghosh et al., 2011</cite> ) the decision is made on tokenlevel, and the problem is cast as sequence labeling using conditional random fields (CRFs) (Lafferty et al., 2001) . In this paper we focus on argument span extraction, and extend the token-level sequence labeling approach of<cite> (Ghosh et al., 2011)</cite> with the separate models for arguments of intra-sentential and intersentential explicit discourse relations. To compare to the other approaches (i.e. (Lin et al., 2012) and (Xu et al., 2012) ) we adopt the immediately previous sentence heuristic to select a candidate Arg1 sentence for the inter-sentential relations.",
  "y": "extends"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_4",
  "x": "In this paper we follows the approach of<cite> (Ghosh et al., 2011</cite> (Prasad et al., 2008) ); and distribution of Arg2 with respect to extent in inter-sentential explicit discourse relations. SS = same sentence as the connective; IPS = immediately previous sentence; NAPS = non-adjacent previous sentence; FS = some sentence following the sentence containing the connective; SingFull = Single Full sentence; SingPart = Part of single sentence; MultFull = Multiple full sentences; MultPart = Parts of multiple sentences. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_5",
  "x": "For the PS case, the Arg1 is located either in Immediately Previous Sentences (IPS: 30.1%) or in some Non-Adjacent Previous Sentences (NAPS: 9.0%). CRF-based discourse parser of<cite> Ghosh et al. (2011)</cite> , which processes SS and PS cases with the same model, uses \u00b12 sentence window as a hypothesis space (5 sentences: 1 sentence containing the connective, 2 preceding and 2 following sentences). The window size is motivated by the observation that it entirely covers arguments of 94% of all explicit relations.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_6",
  "x": "In the literature, the task of argument position classification was addressed by several researchers (e.g. (Prasad et al., 2010) , (Lin et al., 2012) ). Lin et al. (2012) , for instance, report F 1 of 97.94% for a classifier trained on PDTB sections 02-21, and tested on section 23. The task has a very high baseline and even higher performance on supervised machine learning, Table 3 : Feature sets for Arg2 and Arg1 argument span extraction in<cite> (Ghosh et al., 2011)</cite> which is an additional motivation to process intra-and inter-sentential relations separately.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_7",
  "x": "**PARSING MODELS** We replicate and evaluate the discourse parser of<cite> (Ghosh et al., 2011)</cite> , then modify it to process intraand inter-sentential explicit relations separately. This is achieved by integrating Argument Position Classification and Immediately Previous Sentence heuristic into the parsing pipe-line.",
  "y": "extends"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_8",
  "x": "Previous Sentence Feature (PREV) signals if a sentence immediately precedes the sentence starting with a connective, and its value is the first token of the connective<cite> (Ghosh et al., 2011)</cite> . For instance, if some sentence A is followed by a sentence B starting with discourse connective On the other hand, all the tokens of the sentence A have the PREV feature value 'On'. The feature is similar to a heuristic to select the sentence immediately preceding a sentence starting with a connective as a candidate for Arg1.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_9",
  "x": "Via templates these features are enriched with ngrams: tokens with 2-grams in the window of \u00b11 to- Figure 1: Single model discourse parser architecture of<cite> (Ghosh et al., 2011)</cite> . CRF argument span extraction models are in bold. kens, and the rest of the features with 2 & 3-grams in the window of \u00b12 tokens.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_10",
  "x": "The discourse parser of<cite> (Ghosh et al., 2011</cite> ) is a cascade of CRF models to sequentially label Arg2 and Arg1 spans (since Arg2 label is a feature for Arg1 model) (see Figure 1 ). There is no distinction between intra-and inter-sentential relations, rather the single model jointly decides on the position and the span of an argument (either Arg1 or Arg2, not both together) in the window of \u00b12 sentences (the parser will be further abbreviated as W5P -Window 5 Parser). The single model parser achieves F-measure of 81.7 for Arg2 and 60.3 for Arg1 using CONNL evaluation script.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_11",
  "x": "The performance is higher than<cite> (Ghosh et al., 2011</cite> ) -Arg2: F 1 of 79.1 and Arg1: F 1 of 57.3 -due to improvements in feature and instance extraction, such as the treatment of multi-word connectives. These models are the baseline for comparison with separate models architecture. However, we change the evaluation method (see Section 6).",
  "y": "differences"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_12",
  "x": "First, in this paper it is different from<cite> (Ghosh et al., 2011)</cite> ; thus, we first describe it and evaluate the difference. Second, in order to compare the baseline single and separate model parsers, the error from argument position classification has to be propagated for the latter one; and the process is described in 6.1.2. Since both versions of the parser are affected by automatic features, the evaluation is on gold features only.",
  "y": "differences"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_13",
  "x": "The exception is for Arg2 label; since it is generated within the segment of the pipeline we are in-terested in. Unless stated otherwise, all the results for Arg1 are reported for automatic Arg2 labels as a feature. Following<cite> (Ghosh et al., 2011)</cite> PDTB is split as Sections 02-22 for training, 00-01 for development, and 23-24 for testing.",
  "y": "uses"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_14",
  "x": "**CONLL VS. STRING-BASED EVALUATION** <cite>Ghosh et al. (2011)</cite> report using CONLL-based evaluation script. However, it is not well suited for the evaluation of argument spans because the unit of evaluation is a chunk -a segment delimited by any outof-chunk token or a sentence boundary.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_15",
  "x": "Thus, CONLL-based evaluation yields incorrect number of test instances:<cite> Ghosh et al. (2011)</cite> report 1,028 SS and 617 PS test instances for PDTB sections 23-24 (see caption of Table 7 in the original paper), which is 1,645 in total; whereas there is only 1,595 explicit relations in these sections. In this paper, the evaluation is string-based; i.e. an argument span is correct, if it matches the whole reference string. Following<cite> (Ghosh et al., 2011)</cite> and (Lin et al., 2012) , argument initial and final punctuation marks are removed; and precision (p), recall (r) and F 1 score are computed using the equations 1 -3.",
  "y": "background"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_16",
  "x": "In this paper, the evaluation is string-based; i.e. an argument span is correct, if it matches the whole reference string. Following<cite> (Ghosh et al., 2011)</cite> and (Lin et al., 2012) , argument initial and final punctuation marks are removed; and precision (p), recall (r) and F 1 score are computed using the equations 1 -3. In the equations, Exact Match is the count of correctly tagged argument spans; No Match is the count of argument spans that do not match the reference string exactly (even one token difference is counted as an error); and References in Gold is the total number of arguments in the reference.",
  "y": "uses"
 },
 {
  "id": "06917a1dd02d55c827e7e07eeae2da_17",
  "x": "**CONCLUSION** In this paper we compare two strategies for the argument span extraction: to process intra-and intersentential explicit relations by a single model, or separate ones. We extend the approach of<cite> (Ghosh et al., 2011)</cite> to argument span extraction cast as token-level sequence labeling using CRFs and integrate argument position classification and immediately previous sentence heuristic.",
  "y": "extends"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_0",
  "x": "Sequence-to-sequence (Seq2seq) models have successfully improved many well-studied NLP tasks, especially for natural language generation (NLG) tasks, such as machine translation (MT) (Sutskever et al., 2014; Cho et al., 2014) and abstractive summarization (Rush et al., 2015) . Seq2seq models have also been applied to constituency parsing <cite>(Vinyals et al., 2015)</cite> and provided a fairly good result. However one obvious, intuitive drawback of Seq2seq models when they are applied to constituency parsing is that they have no explicit architecture to model latent nested relationships among the words and phrases in constituency parse trees, Thus, models that directly model them, such as RNNG (Dyer et al., 2016) , are an intuitively more promising approach.",
  "y": "background"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_1",
  "x": "After the first proposal of an Seq2seq constituency parser, many task-independent techniques have been developed, mainly in the NLG research area. Our aim is to update the Seq2seq approach proposed in<cite> Vinyals et al. (2015)</cite> as a stronger baseline of constituency parsing. Our motivation is basically identical to that described in Denkowski and Neubig (2017) .",
  "y": "uses"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_2",
  "x": "**CONSTITUENCY PARSING BY SEQ2SEQ** Our starting point is an RNN-based Seq2seq model with an attention mechanism that was applied to constituency parsing <cite>(Vinyals et al., 2015)</cite> . We omit detailed descriptions due to space limitations, but note that our model architecture is identical to the one introduced in Luong et al. (2015a) 2 .",
  "y": "uses"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_4",
  "x": "In detail, the linearized forms with and without the POS-tag normalization are independently and simultaneously estimated as o j and q j , respectively, in the decoder output layer by following equation: 3.4 Output length controlling As described in<cite> Vinyals et al. (2015)</cite> , not all the outputs (predicted linearized parse trees) obtained from the Seq2seq parser are valid (well-formed) as a parse tree.",
  "y": "motivation background"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_5",
  "x": "3.4 Output length controlling As described in<cite> Vinyals et al. (2015)</cite> , not all the outputs (predicted linearized parse trees) obtained from the Seq2seq parser are valid (well-formed) as a parse tree. Toward guaranteeing that every output is a valid tree, we introduce a simple extension of the method for controlling the Seq2seq output length (Kikuchi et al., 2016) .",
  "y": "motivation"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_6",
  "x": "In constituency parsing, several systems also incorporate pre-trained word embeddings, such as<cite> Vinyals et al. (2015)</cite> ; Durrett and Klein (2015) . To maintain as much reproducibility of our experiments as possible, we simply applied publicly available pre-trained word embeddings, i.e., glove.840B.300d 7 , as initial values of the encoder embedding layer. ----------------------------------",
  "y": "background"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_7",
  "x": "The pre-trained word embeddings obtained from a large external corpora often boost the final task performance even if they only initialize the input embedding layer. In constituency parsing, several systems also incorporate pre-trained word embeddings, such as<cite> Vinyals et al. (2015)</cite> ; Durrett and Klein (2015) . To maintain as much reproducibility of our experiments as possible, we simply applied publicly available pre-trained word embeddings, i.e., glove.840B.300d 7 , as initial values of the encoder embedding layer.",
  "y": "motivation"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_8",
  "x": "We used the standard split of training (Sec.02-21), development (Sec.22), and test data (Sec.23) and strictly followed the instructions for the evaluation settings explained in<cite> Vinyals et al. (2015)</cite> . For data pre-processing, all the parse trees were transformed into linearized forms, which include standard UNK replacement for OOV words and POS-tag normalization by XX-tags. As explained in<cite> Vinyals et al. (2015)</cite> , we did not apply any parse tree binarization or special unary treatment, which were used as common techniques in the literature.",
  "y": "uses"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_9",
  "x": "For data pre-processing, all the parse trees were transformed into linearized forms, which include standard UNK replacement for OOV words and POS-tag normalization by XX-tags. As explained in<cite> Vinyals et al. (2015)</cite> , we did not apply any parse tree binarization or special unary treatment, which were used as common techniques in the literature. Table 7 : List of bracketing F-measures on test data (PTB Sec.23) reported in recent top-notch systems: scores with bold font represent our scores.",
  "y": "uses"
 },
 {
  "id": "0732eaa37366d7ae092f4de0ed72cb_10",
  "x": "We postprocessed such malformed parse trees by simple rules introduced in <cite>(Vinyals et al., 2015)</cite> . On the other hand, we confirmed that all the results applying the technique explained in Sec. 3.4 produced no malformed parse trees. Ensembling and Reranking: Table 5 shows the results of our models with model ensembling and LM-reranking.",
  "y": "uses"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_0",
  "x": "In <cite>(S\u00e1nchez and Bened\u00ed, 2006b</cite> ), SITGs were used for obtaining word phrases, reporting preliminary results on the EuroParl corpus. In this work, we extend that work by using bracketed corpora for estimating the STIGs. In section 2, we will briefly review the phrase-based SMT approach.",
  "y": "background"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_1",
  "x": "In <cite>(S\u00e1nchez and Bened\u00ed, 2006b</cite> ), SITGs were used for obtaining word phrases, reporting preliminary results on the EuroParl corpus. In this work, we extend that work by using bracketed corpora for estimating the STIGs. In section 2, we will briefly review the phrase-based SMT approach.",
  "y": "extends"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_2",
  "x": "---------------------------------- **SITGS FOR PHRASE EXTRACTION** First, we built an initial SITG by following the method described in <cite>(S\u00e1nchez and Bened\u00ed, 2006b</cite> ).",
  "y": "uses"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_3",
  "x": "Adding non-terminal symbols may provide more complexity to the grammar built, and hence increases its expressive power. <cite>(S\u00e1nchez and Bened\u00ed, 2006b</cite> ) Translation results of this setup can be seen in Table 1 . Here, all the weights of the log-linear model were adjusted my MERT training, and the language model used was a 5-gram interpolated with Knesser-Ney discount.",
  "y": "uses"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_4",
  "x": "Comparatively, the best result that<cite> (S\u00e1nchez and Bened\u00ed, 2006b)</cite> reported in the Spanish-English task was a BLEU score of 23.0, which they obtained by combining segments extracted from both the bracketed and the non-bracketed corpus. We have widely exceeded this baseline. On the other hand, the Moses toolkit (Philipp Koehn, 2007) , which is a state of the art statistical machine translation system, obtains in this task a score of 31.0 BLEU.",
  "y": "background"
 },
 {
  "id": "0751f2ced4f7ced37cf206fea051fa_5",
  "x": "Comparatively, the best result that<cite> (S\u00e1nchez and Bened\u00ed, 2006b)</cite> reported in the Spanish-English task was a BLEU score of 23.0, which they obtained by combining segments extracted from both the bracketed and the non-bracketed corpus. We have widely exceeded this baseline. On the other hand, the Moses toolkit (Philipp Koehn, 2007) , which is a state of the art statistical machine translation system, obtains in this task a score of 31.0 BLEU.",
  "y": "differences"
 },
 {
  "id": "0860b08831b01e7e98c66ced63b256_0",
  "x": "Bansal et al. [8] and Melamud et al. [11] show the benefits of such modified-context embeddings in dependency parsing task. The dependency-based word embedding can relieve the problem of data sparseness, since even without occurrence of dependency word pairs in a corpus, dependency scores can be still calculated by word embeddings <cite>[12]</cite> . In this paper, we proposed a rescoring approach for parsing, based on a combination of original parsing scores and dependency word embedding scores to assist the determination of the best parse tree among the n-best parse trees.",
  "y": "motivation"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_0",
  "x": "****A DETERMINISTIC ALGORITHM FOR BRIDGING ANAPHORA RESOLUTION**** **ABSTRACT** Previous work on bridging anaphora resolution (Poesio et al., 2004;<cite> Hou et al., 2013b)</cite> use syntactic preposition patterns to calculate word relatedness.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_1",
  "x": "Previous work on bridging anaphora resolution (Poesio et al., 2004;<cite> Hou et al., 2013b)</cite> use syntactic preposition patterns to calculate word relatedness. However, such patterns only consider NPs' head nouns and hence do not fully capture the semantics of NPs. Recently, Hou (2018) created word embeddings (embeddings PP) to capture associative similarity (i.e., relatedness) between nouns by exploring the syntactic structure of noun phrases. But embeddings PP only contains word representations for nouns.",
  "y": "background motivation"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_2",
  "x": "We show that this simple approach achieves the competitive results compared to the best system in<cite> Hou et al. (2013b)</cite> which explores Markov Logic Networks to model the problem. Additionally, we further improve the results for bridging anaphora resolution reported in Hou (2018) by combining our simple deterministic approach with<cite> Hou et al. (2013b)</cite>'s best system MLN II. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_3",
  "x": "We therefore develop a deterministic approach for bridging anaphora resolution, which represents the semantics of an NP based on its head noun and modifications. We show that this simple approach achieves the competitive results compared to the best system in<cite> Hou et al. (2013b)</cite> which explores Markov Logic Networks to model the problem. Additionally, we further improve the results for bridging anaphora resolution reported in Hou (2018) by combining our simple deterministic approach with<cite> Hou et al. (2013b)</cite>'s best system MLN II.",
  "y": "extends"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_4",
  "x": "Most previous empirical research on bridging (Poesio and Vieira, 1998; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011;<cite> Hou et al., 2013b)</cite> focus on bridging anaphora resolution, a subtask of bridging resolution that aims to choose the antecedents for bridging anaphors. For this substask, most previous work (Poesio et al., 2004; Lassalle and Denis, 2011;<cite> Hou et al., 2013b)</cite> calculate semantic relatedness between an anaphor and its antecedent based on word co-occurrence counts using certain syntactic patterns. However, such patterns only consider head noun knowledge and hence are not sufficient for bridging relations which require the semantics of modification.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_5",
  "x": "In addition, Delmed is exploring distribution arrangements with Fresenius USA, Delmed said. Most previous empirical research on bridging (Poesio and Vieira, 1998; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011;<cite> Hou et al., 2013b)</cite> focus on bridging anaphora resolution, a subtask of bridging resolution that aims to choose the antecedents for bridging anaphors. For this substask, most previous work (Poesio et al., 2004; Lassalle and Denis, 2011;<cite> Hou et al., 2013b)</cite> calculate semantic relatedness between an anaphor and its antecedent based on word co-occurrence counts using certain syntactic patterns.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_6",
  "x": "Our approach combines the semantics of an NP's head with the semantics of its modifications by vector average using embeddings bridging. We show that this simple, efficient method achieves the competitive results on ISNotes for the task of bridging anaphora resolution compared to the best system in<cite> Hou et al. (2013b)</cite> which explores Markov Logic Networks to model the problem. The main contributions of our work are: (1) a general word representation resource 2 for bridging; and (2) a simple yet competitive deterministic approach for bridging anaphora resolution which models the meaning of an NP based on its head noun and modifications.",
  "y": "differences"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_7",
  "x": "Pre-vious work on bridging anaphora resolution (Poesio et al., 2004; Lassalle and Denis, 2011;<cite> Hou et al., 2013b</cite> ) explored word co-occurrence counts in certain syntactic preposition patterns to calculate word relatedness. For instance, the big hit counts of the query \"the door of the house\" in large corpora could indicate that door and house stand in a part-of relation. These patterns encode associative relations between nouns which cover a variety of bridging relations.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_8",
  "x": "Markert et al. (2012) released a corpus called ISNotes which contains unrestricted bridging annotations. Based on this corpus,<cite> Hou et al. (2013b)</cite> proposed a joint inference framework for bridging anaphora resolution using Markov logic networks (Domingos and Lowd, 2009 framework resolves all bridging anaphors in one document together by modeling that semantically related anaphors are likely to share the same antecedent. ISNotes is a challenging corpus for bridging.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_9",
  "x": "Our approach is deterministic and simple, but achieves the competitive results compared to the advanced machine learning-based approach <cite>(Hou et al., 2013b)</cite> . We also improve the result reported in Hou (2018) on the same corpus by combining our deterministic approach with the best system from<cite> Hou et al. (2013b)</cite> . Just recently, two new corpora (R\u00f6siger, 2018a; Poesio et al., 2018) with bridging annotations have become available and we notice that the definitions of bridging in these corpora are different from the bridging definition in ISNotes.",
  "y": "differences"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_10",
  "x": "We also improve the result reported in Hou (2018) on the same corpus by combining our deterministic approach with the best system from<cite> Hou et al. (2013b)</cite> . Just recently, two new corpora (R\u00f6siger, 2018a; Poesio et al., 2018) with bridging annotations have become available and we notice that the definitions of bridging in these corpora are different from the bridging definition in ISNotes. We apply our algorithm with small adaptations to select antecedents for bridging anaphors on these corpora.",
  "y": "extends"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_11",
  "x": "**EXPERIMENTAL SETUP** Following<cite> Hou et al. (2013b)</cite> 's experimental setup, we resolve bridging anaphors to entity antecedents. Entity information is based on the OntoNotes coreference annotation.",
  "y": "uses"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_12",
  "x": "We also use the OntoNotes named entity annotation to assign 5 http://www.h-its.org/en/research/nlp/isnotes-corpus NPs the semantic type \"time\" if their entity types are \"date\" or \"time\". In<cite> Hou et al. (2013b)</cite> , features are extracted by using entity information. For instance, the raw hit counts of the preposition pattern query (e.g., arrangements of products) for a bridging anaphor a and its antecedent candidate e is the maximum count among all instantiations of e. In our experiments, we simply extend the list of antecedent candidates E a (described in Section 4) to include all instantiations of the original entities in E a .",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_13",
  "x": "In<cite> Hou et al. (2013b)</cite> , features are extracted by using entity information. For instance, the raw hit counts of the preposition pattern query (e.g., arrangements of products) for a bridging anaphor a and its antecedent candidate e is the maximum count among all instantiations of e. In our experiments, we simply extend the list of antecedent candidates E a (described in Section 4) to include all instantiations of the original entities in E a . Note that our simple antecedent candidate selection strategy (described in Section 4) allows us to include 76% of NP antecedents compared to 77% in pairwise model III from<cite> Hou et al. (2013b)</cite> where they add top 10% salient entities as additional antecedent candidates.",
  "y": "extends"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_14",
  "x": "For instance, the raw hit counts of the preposition pattern query (e.g., arrangements of products) for a bridging anaphor a and its antecedent candidate e is the maximum count among all instantiations of e. In our experiments, we simply extend the list of antecedent candidates E a (described in Section 4) to include all instantiations of the original entities in E a . Note that our simple antecedent candidate selection strategy (described in Section 4) allows us to include 76% of NP antecedents compared to 77% in pairwise model III from<cite> Hou et al. (2013b)</cite> where they add top 10% salient entities as additional antecedent candidates. In<cite> Hou et al. (2013b)</cite> , salient entities on each text are measured through the lengths of the coreference chains based on the gold coreference annotation.",
  "y": "differences"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_15",
  "x": "For instance, the raw hit counts of the preposition pattern query (e.g., arrangements of products) for a bridging anaphor a and its antecedent candidate e is the maximum count among all instantiations of e. In our experiments, we simply extend the list of antecedent candidates E a (described in Section 4) to include all instantiations of the original entities in E a . Note that our simple antecedent candidate selection strategy (described in Section 4) allows us to include 76% of NP antecedents compared to 77% in pairwise model III from<cite> Hou et al. (2013b)</cite> where they add top 10% salient entities as additional antecedent candidates. In<cite> Hou et al. (2013b)</cite> , salient entities on each text are measured through the lengths of the coreference chains based on the gold coreference annotation.",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_16",
  "x": "Note that our simple antecedent candidate selection strategy (described in Section 4) allows us to include 76% of NP antecedents compared to 77% in pairwise model III from<cite> Hou et al. (2013b)</cite> where they add top 10% salient entities as additional antecedent candidates. In<cite> Hou et al. (2013b)</cite> , salient entities on each text are measured through the lengths of the coreference chains based on the gold coreference annotation. Following<cite> Hou et al. (2013b)</cite> , we measure accuracy on the number of bridging anaphors, instead of on all links between bridging anaphors and their antecedent instantiations.",
  "y": "uses"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_17",
  "x": "Again we do not add the suffix \" PP\" to the bridging anaphors for GloVe GigaWiki14 and GloVe Giga. Table 6 lists the best results of the two models for bridging anaphora resolution from<cite> Hou et al. (2013b)</cite> . pairwise model III is a pairwise mentionentity model based on various semantic, syntactic and lexical features.",
  "y": "uses"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_18",
  "x": "For embeddings PP, the result on using NP head + modifiers (31.67%) is worse than the result on using NP head (33.03%). However, if we apply embeddings PP to a bridging anaphor's head and modifiers, and only apply embeddings PP to the head noun of an antecedent candidate, we get an due to the improved antecedent candidate selection strategy described in Section 4. acc models from<cite> Hou et al. (2013b)</cite> Table 6 : Results of using NP head plus modifications in different word representations for bridging anaphora resolution compared to the best results of two models from<cite> Hou et al. (2013b)</cite> .",
  "y": "uses"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_19",
  "x": "Although the differences are not significant, it confirms that the information from the modifiers of the antecedent candidates in embeddings PP hurts the performance. This corresponds to our observations in the previous section that the representations for words without the suffix \" PP\" in embeddings PP are not as good as in embeddings bridging due to less training instances. Finally, our method based on embeddings bridging achieves an accuracy of 39.52%, which is competitive to the best result (41.32%) reported in<cite> Hou et al. (2013b)</cite> .",
  "y": "similarities"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_20",
  "x": "---------------------------------- **COMBINING NP HEAD + MODIFIERS WITH MLN II** For bridging anaphora resolution, Hou (2018) integrates a much simpler deterministic approach by combining an NP head with its noun modifiers (appearing before the head) based on embeddings PP into the MLN II system <cite>(Hou et al., 2013b)</cite> .",
  "y": "background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_21",
  "x": "For bridging anaphora resolution, Hou (2018) integrates a much simpler deterministic approach by combining an NP head with its noun modifiers (appearing before the head) based on embeddings PP into the MLN II system <cite>(Hou et al., 2013b)</cite> . Similarly, we add a constraint on top of MLN II using our deterministic approach (NP head + modifiers) based on embeddings bridging. Table 8 lists the results of different systems 8 for bridging anaphora resolution in ISNotes.",
  "y": "uses background"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_22",
  "x": "The resulting word embeddings (embeddings bridging) are a more general word representation resource for bridging. Based on embeddings bridging, we propose a deterministic approach for choosing antecedents for bridging anaphors. We show that this simple and efficient method achieves the competitive result on bridging anaphora resolution compared to the advanced machine learning-based approach in<cite> Hou et al. (2013b)</cite> which is heavily dependent on a lot of carefully designed complex features.",
  "y": "differences"
 },
 {
  "id": "087a2a35ad2428d6b17c6906447349_23",
  "x": "We show that this simple and efficient method achieves the competitive result on bridging anaphora resolution compared to the advanced machine learning-based approach in<cite> Hou et al. (2013b)</cite> which is heavily dependent on a lot of carefully designed complex features. We also demonstrate that using embeddings bridging yields better results than using embeddings PP for bridging anaphora resolution. For the task of bridging anaphora resolution,<cite> Hou et al. (2013b)</cite> pointed out that considering only head noun knowledge is not enough and future work needs to explore wider context to resolve context-specific bridging relations.",
  "y": "future_work"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_0",
  "x": "<cite>Wang et al. (2014a)</cite> rely on Wikipedia anchors, making the applicable scope quite limited. In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors. We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description.",
  "y": "motivation"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_1",
  "x": "We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description. Extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of <cite>Wang et al. (2014a)</cite> , which is encouraging as we do not use any anchor information. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_2",
  "x": "An important milestone, the approach of <cite>Wang et al. (2014a)</cite> solves issue (1) by jointly embedding entities, relations, and words into the same vector space and hence is able to deal with words/phrases beyond entities in KBs. The key component is the so-called alignment model, which makes sure the embeddings of entities, relations, and words are in the same space. Two alignment models are introduced there: one uses entity names and another uses Wikipedia anchors.",
  "y": "background"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_3",
  "x": "We conduct extensive experiments on the tasks of triplet classification, link prediction, relational fact extraction, and analogical reasoning to compare with the previous approach <cite>(Wang et al., 2014a)</cite> . Results show that our approach consistently achieves better or comparable performance. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_4",
  "x": "TransE This is a representative knowledge embedding model proposed by . For a fact (h, r, t) in KBs, where h is the head entity, r is the relation, and t is the tail entity, TransE models the relation r as a translation vector r connecting the embeddings h and t of the two entities, i.e., h + r is close to t. The model is simple, effective and efficient. Most knowledge embedding models thereafter including this paper are variants of this model (Wang et al., 2014b; <cite>Wang et al., 2014a</cite>; Lin et al., 2015) .",
  "y": "uses background"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_5",
  "x": "Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as 'King' \u2212 'Queen' \u2248 'Man' \u2212 'Women'. However, as it is unsupervised, it cannot tell the exact relation between two words. <cite>Wang et al. (2014a)</cite> combines knowledge embedding and word embedding in a joint framework so that the entities/relations and words are in the same vector space and hence operators like inner product (similarity) between them are meaningful.",
  "y": "background"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_6",
  "x": "We follow the jointly embedding framework of <cite>(Wang et al., 2014a)</cite> , i.e., learning optimal embeddings by minimizing the following loss where L K , L T and L A are the component loss functions of the knowledge model, text model and alignment model respectively. Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in <cite>(Wang et al., 2014a)</cite> .",
  "y": "uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_7",
  "x": "where L K , L T and L A are the component loss functions of the knowledge model, text model and alignment model respectively. Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in <cite>(Wang et al., 2014a)</cite> . However, to make the content self-contained, we still need to briefly explain L K and L T .",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_8",
  "x": "However, to make the content self-contained, we still need to briefly explain L K and L T . Knowledge Model Describes the plausibility of a triplet (h, r, t) by defining where z(h, r, t) = b \u2212 0.5 \u00b7 h + r \u2212 t 2 2 , b = 7 as suggested by <cite>Wang et al. (2014a)</cite> .",
  "y": "uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_9",
  "x": "where Then the loss function of text model is Alignment Model This part is different from <cite>Wang et al. (2014a)</cite> .",
  "y": "differences"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_10",
  "x": "As to the methods, \"Separately\" denotes the method of separately embedding knowledge bases and text. \"Jointly(anchor)\" and \"Jointly(name)\" denote the jointly embedding methods based on Alignment by Wikipedia Anchors and Alignment by Entity Names in <cite>(Wang et al., 2014a)</cite> respectively. \"Jointly(desp)\" is the joint embedding method based on alignment by entity descriptions. Data For link prediction, FB15K from ) is used as the knowledge base.",
  "y": "uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_11",
  "x": "\"Jointly(desp)\" is the joint embedding method based on alignment by entity descriptions. Data For link prediction, FB15K from ) is used as the knowledge base. For triplet classification, a large dataset provided by <cite>(Wang et al., 2014a )</cite> is used as the knowledge base.",
  "y": "uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_12",
  "x": "As many Wikipedia articles can be mapped to Freebase entities, we regard a Wikipedia article as the description for the corresponding entity in Freebase. Following the settings in <cite>(Wang et al., 2014a)</cite> , we apply the same preprocessing steps, including sentence segmentation, tokenization, and named entity recognition. We combine the consecutive tokens covered by an anchor or identically tagged as \"Location/Person/Organization\" and regard them as phrases.",
  "y": "uses similarities"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_13",
  "x": "Triplet Classification This is a binary classification task, predicting whether a candidate triplet (h, r, t) is a correct fact or not. It is used in (Socher et al., 2013; Wang et al., 2014b; <cite>Wang et al., 2014a)</cite> . We follow the same protocol in <cite>(Wang et al., 2014a)</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_14",
  "x": "We follow the same protocol in <cite>(Wang et al., 2014a)</cite> . We train their models via our own implementation on our dataset. The results are in Table 2 . \"e-e\" means both sides of a triplet (h, r, t) are entities in KB, \"e-w\" means the tail side is a word out of KB entity vocabulary, similarly for \"w-e\" and \"w-w\".",
  "y": "uses similarities"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_15",
  "x": "**RELATIONAL FACT EXTRACTION** This task is to extract facts (h, r, t) from plain text. show that combing scores from TransE and some text side base extractor achieved much better precision-recall curve compared to the base extractor. <cite>Wang et al. (2014a)</cite> confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as <cite>(Wang et al., 2014a)</cite> to investigate the performance of our new alignment model.",
  "y": "motivation uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_16",
  "x": "This task is to extract facts (h, r, t) from plain text. show that combing scores from TransE and some text side base extractor achieved much better precision-recall curve compared to the base extractor. <cite>Wang et al. (2014a)</cite> confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as <cite>(Wang et al., 2014a)</cite> to investigate the performance of our new alignment model. We use the same public dataset NYT+FB, released by Riedel et al. (2010) and used in and <cite>(Wang et al., 2014a)</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_17",
  "x": "In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment. Since both Mintz and MIML are probabilistic models, we use the same method in <cite>(Wang et al., 2014a)</cite> to linearly combine the scores. The precision-recall curves are plot in Fig. (1) .",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_18",
  "x": "Moreover, \"Jointly(desp)\" is slightly better than \"Jointly(anchor)\", which is in accordance with the results from the link prediction experiment and the triplet classification experiment. Analogical Reasoning This task evaluates the quality of word embeddings (Mikolov et al., 2013b) . We use the original dataset released by (Mikolov et al., 2013b) and follow the same evaluation protocol of <cite>(Wang et al., 2014a)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_19",
  "x": "Both \"Jointly(anchor)\" and \"Skip-gram\" only consider the context of words, while \"Jointly(desp)\" not only consider the context but also use the whole document to disambiguate words. Intuitively, the whole document is also a valuable resource to disambiguate words. (3) We further verify that \"Jointly(name)\", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in <cite>(Wang et al., 2014a)</cite> .",
  "y": "similarities"
 },
 {
  "id": "08c64c92b77dbd9e999092a2fec3d1_20",
  "x": "Both \"Jointly(anchor)\" and \"Skip-gram\" only consider the context of words, while \"Jointly(desp)\" not only consider the context but also use the whole document to disambiguate words. Intuitively, the whole document is also a valuable resource to disambiguate words. (3) We further verify that \"Jointly(name)\", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in <cite>(Wang et al., 2014a)</cite> .",
  "y": "differences"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_0",
  "x": "They define a deduction system for (an isomorphic variant of)<cite> Attardi's (2006)</cite> transition system, which covers a subset of non-projective trees. The exact inference algorithm runs in Opn 7 q time, where n denotes sentence length. In this paper, we show how Cohen et al.'s (2011) system can be modified to generate a new family of deduction systems with corresponding transition systems.",
  "y": "background"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_4",
  "x": "We now introduce the widely-used<cite> Attardi (2006)</cite> system, which includes transitions that create arcs between non-consecutive subtrees, thus allowing it to produce some non-projective trees. To simplify exposition, here we present Cohen et al.'s (2011) isomorphic version. The set of transitions consists of a shift transition (sh) and four reduce transitions (re).",
  "y": "background"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_5",
  "x": "The distance between h and m in a re h,m transition is called its degree. A system limited to degree-1 transitions can only parse projective sentences. As shown in Fig. 1 ,<cite> Attardi's (2006)</cite> system has two degree-2 transitions (re s 0 ,s 2 and re s 2 ,s 0 ) that allow it to cover 87.24% of the nonprojective trees in UD 2.1.",
  "y": "background"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_6",
  "x": "**IMPROVING COVERAGE** A key observation is that a degree-D Attardi system does not contain all possible transitions of degree within D. Since prior empirical work has ascertained that transition systems using more transitions with degree greater than 1 can handle more non-projective treebank trees <cite>(Attardi, 2006</cite>; G\u00f3mez-Rodr\u00edguez, 2016) , we hypothesize that adding some of these \"missing\" reduce transitions into the system's inventory should increase coverage. The challenge is to simultaneously maintain run-time guarantees, as there exists a known trade-off between coverage and complexity (G\u00f3mez-Rodr\u00edguez, 2016 Cohen et al. (2011) , rather than Opn 3\u00a83`1 q; and (ii) another has degree 2 but better runtime than Cohen et al.'s (2011) system.",
  "y": "background"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_7",
  "x": "In this section, we modify Cohen et al.'s (2011) set of reduce deduction rules to improve coverage or time complexity. Since each such deduction rule corresponds to a reduce transition, each revision to the deduction system yields a variant of<cite> Attardi's (2006)</cite> parser. In other words, generalization of the deduction system gives rise to a family of nonprojective transition-based dependency parsers.",
  "y": "background"
 },
 {
  "id": "08d3f7a0938ab85d9a251b6a2364ed_8",
  "x": "On the other hand, since their addition doesn't affect the asymptotic run-time, we define ALLDEG1 to include all five degree-1 transitions from R into the<cite> Attardi (2006)</cite> system. Surprisingly, using ALLDEG1 improves non-projective coverage from 87.24% to 93.32%. Furthermore, recall that we argued above that,",
  "y": "uses"
 },
 {
  "id": "09493a62815b4b826248d6d9be47cb_0",
  "x": "This model outperforms MetaMap significantly, increasing the macro-averaged F-measure by 25% on an NCBI disease dataset. However, while these tools have proven to be effective for patient records and research papers, they achieve moderate results on social media texts (Nikfarjam et al., 2015;<cite> Limsopatham and Collier, 2016)</cite> . Recent works go beyond string matching: these works have tried to view the problem of matching a one-or multi-word expression against a knowledge base as a supervised sequence labeling problem.",
  "y": "motivation"
 },
 {
  "id": "09493a62815b4b826248d6d9be47cb_1",
  "x": "<cite>Limsopatham and Collier (2016)</cite> utilized convolutional neural networks (CNNs) for phrase normalization in user reviews, while Tutubalina et al. (2018) , Han et al. (2017) , and Belousov et al. (2017) applied recurrent neural networks (RNNs) to UGTs, achieving similar results. These works were among the first applications of deep learning techniques to medical concept normalization. The goal of this work is to study the use of deep neural models, i.e., contextualized word representation model BERT (Devlin et al., 2018) and Gated Recurrent Units (GRU) (Cho et al., 2014) with an attention mechanism, paired with word2vec word embeddings and contextualized ELMo embeddings (Peters et al., 2018) .",
  "y": "background"
 },
 {
  "id": "09493a62815b4b826248d6d9be47cb_2",
  "x": "In 2004, the research community started to address the needs to automatically detect biomedical entities in free texts through shared tasks. Huang and Lu (2015) survey the work done in the organization of biomedical NLP (BioNLP) challenge evaluations up to 2014. These tasks are devoted to the normalization of (1)<cite> (Limsopatham and Collier, 2016)</cite> 73.39 ----CNN<cite> (Limsopatham and Collier, 2016)</cite> 81.41 ----RNN<cite> (Limsopatham and Collier, 2016)</cite> 79.98 ----Attentional Char-CNN (Niu et al., 2018) 84.65 ----Hierarchical Char-CNN (Han et al., 2017) - Table 2 : The performance of the proposed models and the state-of-the-art methods in terms of accuracy.",
  "y": "background"
 },
 {
  "id": "0984f12a6fea858c7f18263cc2fb01_0",
  "x": "Image description is one of the core challenges at the intersection of Natural Language Processing (NLP) and Computer Vision (CV) (Bernardi et al., 2016) . This task has only received attention in a monolingual English setting, helped by the availability of English datasets, e.g. Flickr8K (Hodosh et al., 2013) , Flickr30K <cite>(Young et al., 2014)</cite> , and MS COCO (Chen et al., 2015) . However, the possible applications of image description are useful for all languages, such as searching for images using natural language, or providing alternative-description text for visually impaired Web users.",
  "y": "background"
 },
 {
  "id": "0984f12a6fea858c7f18263cc2fb01_1",
  "x": "Multi30K is an extension of the Flickr30K dataset <cite>(Young et al., 2014)</cite> with 31,014 German translations of English descriptions and 155,070 independently collected German descriptions. The translations were collected from professionally contracted translators, whereas the descriptions were collected from untrained crowdworkers. The key difference between these corpora is the relationship between the sentences in different languages.",
  "y": "extends"
 },
 {
  "id": "0984f12a6fea858c7f18263cc2fb01_2",
  "x": "---------------------------------- **THE MULTI30K DATASET** The Flickr30K Dataset contains 31,014 images sourced from online photo-sharing websites <cite>(Young et al., 2014)</cite> .",
  "y": "background"
 },
 {
  "id": "0984f12a6fea858c7f18263cc2fb01_3",
  "x": "The Flickr30K Dataset contains 31,014 images sourced from online photo-sharing websites <cite>(Young et al., 2014)</cite> . The Multi30K dataset extends the Flickr30K dataset with translated and independent German sentences.",
  "y": "extends background"
 },
 {
  "id": "0984f12a6fea858c7f18263cc2fb01_4",
  "x": "These differences are deliberate and part of the larger scope of studying multilingual multimodal data in different contexts. The descriptions were collected as similarly as possible to the original Flickr30K dataset by translating the instructions used by<cite> Young et al. (2014)</cite> into German. The translations were collected without showing the images to the translators to keep it as close to a standard translation task as possible.",
  "y": "uses"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_0",
  "x": "Syntactic analysis of search queries is important for a variety of tasks including better query refinement, improved matching and better ad targeting<cite> (Barr et al., 2008)</cite> . However, search queries differ substantially from traditional forms of written language (e.g., no capitalization, few function words, fairly free word order, etc.), and are therefore difficult to process with natural language processing tools trained on standard corpora<cite> (Barr et al., 2008)</cite> . In this paper we focus on part-of-speech (POS) tagging queries entered into commercial search engines and compare different strategies for learning from search logs.",
  "y": "background"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_1",
  "x": "Syntactic analysis of search queries is important for a variety of tasks including better query refinement, improved matching and better ad targeting<cite> (Barr et al., 2008)</cite> . However, search queries differ substantially from traditional forms of written language (e.g., no capitalization, few function words, fairly free word order, etc.), and are therefore difficult to process with natural language processing tools trained on standard corpora<cite> (Barr et al., 2008)</cite> . In this paper we focus on part-of-speech (POS) tagging queries entered into commercial search engines and compare different strategies for learning from search logs.",
  "y": "motivation"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_2",
  "x": "We had two annotators annotate the whole of the MS-251 data set. Before arbitration, the inter-annotator agreement was 90.2%. As a reference, <cite>Barr et al. (2008)</cite> report 79.3% when annotating queries with 19 POS tags.",
  "y": "uses"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_3",
  "x": "We had two annotators annotate the whole of the MS-251 data set. Before arbitration, the inter-annotator agreement was 90.2%. As a reference, <cite>Barr et al. (2008)</cite> report 79.3% when annotating queries with 19 POS tags. We then examined all the instances where the annotators disagreed, and corrected the discrepancy.",
  "y": "differences"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_4",
  "x": "However, transferring information from the snippets provides additional benefits, significantly improving even the uncased baseline taggers. This is consistent with the analysis in <cite>Barr et al. (2008)</cite> . Finally, we see that the direct transfer method from Section 2 significantly outperforms the method described in Bendersky et al. (2010) .",
  "y": "similarities"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_5",
  "x": "Our best system achieves a 21.2% relative reduction in error on their annotations. Some other trends become appar- ent in Table 2 . Firstly, a large part of the benefit of transfer has to do with case information that is available in the snippets but is missing in the query. The uncased tagger is insensitive to this mismatch and achieves significantly better results than the cased taggers. However, transferring information from the snippets provides additional benefits, significantly improving even the uncased baseline taggers. This is consistent with the analysis in <cite>Barr et al. (2008)</cite> .",
  "y": "similarities"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_6",
  "x": "For example one of the results for the query \"bell helmet\" had a snippet containing \"Bell cycling helmets\" and we failed to match helmet to helmets. Table 3 : Precision and recall of the NNP tag on the longtail data for the best baseline method and the three transfer methods using that baseline. 5 Related Work <cite>Barr et al. (2008)</cite> manually annotate a corpus of 2722 queries with 19 POS tags and use it to train and evaluate POS taggers, and also describe the linguistic structures they find.",
  "y": "background"
 },
 {
  "id": "0a226accf1fa8b471176916a76f1c6_7",
  "x": "5 Related Work <cite>Barr et al. (2008)</cite> manually annotate a corpus of 2722 queries with 19 POS tags and use it to train and evaluate POS taggers, and also describe the linguistic structures they find. Unfortunately their data is not available so we cannot use it to compare to their results. R\u00fcd et al. (2011) create features based on search engine results, that they use in an NER system applied to queries.",
  "y": "background"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_0",
  "x": [
   "**INTRODUCTION** Parallel corpora have been used to transfer information from source to target languages for Part-Of-Speech (POS) tagging, word sense disambiguation (Yarowsky et al., 2001) , syntactic parsing (Hwa et al., 2005; Ganchev et al., 2009; Jiang and Liu, 2010) and machine translation (Koehn, 2005; Tiedemann, 2002) . Analysis on the source sentences was induced onto the target sentence via projections across word aligned parallel corpora."
  ],
  "y": "background"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_1",
  "x": [
   "Due to the usage of automatic source parses, automatic word alignments and differences in the annotation schemes of source and target languages, the projected parses are not always fully connected and can have edges missing (Hwa et al., 2005; Ganchev et al., 2009 ). Nonliteral translations and divergences in the syntax of the two languages also lead to incomplete projected parse trees. Figure 1 shows an English-Hindi parallel sentence with correct source parse, alignments and target dependency parse."
  ],
  "y": "motivation background"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_2",
  "x": [
   "Projectivity can be relaxed in some parsers (McDonald et al., 2005; Nivre, 2009) . But these parsers can not directly be used to learn from partially connected parses (Hwa et al., 2005; Ganchev et al., 2009 ). In the projected Hindi treebank (section 4) that was extracted from English-Hindi parallel text, only 5.9% of the sentences had full trees."
  ],
  "y": "background motivation"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_3",
  "x": "In the projected Hindi treebank (section 4) that was extracted from English-Hindi parallel text, only 5.9% of the sentences had full trees. In Spanish and Bulgarian projected data extracted by<cite> Ganchev et al. (2009)</cite> In this paper, we present a dependency parsing algorithm which can train on partial projected parses and can take rich syntactic information as features for learning. The parsing algorithm constructs the partial parses in a bottom-up manner by performing a greedy search over all possible relations and choosing the best one at each step without following either left-to-right or right-to-left traversal.",
  "y": "background"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_4",
  "x": [
   "Ganchev et al. (2009) handle partial projected parses by avoiding committing to entire projected tree during training. The posterior regularization based framework constrains the projected syntactic relations to hold approximately and only in expectation. Jiang and Liu (2010) refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees."
  ],
  "y": "background motivation"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_5",
  "x": "They deal with partial projections by breaking down the projected parse into a set of edges and training on the set of projected relations rather than on trees. While Hwa et al. (2005) requires full projected parses to train their parser,<cite> Ganchev et al. (2009)</cite> and Jiang and Liu (2010) can learn from partially projected trees. However, the discriminative training in<cite> (Ganchev et al., 2009</cite> ) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse.",
  "y": "background motivation"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_6",
  "x": "We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in<cite> (Ganchev et al., 2009</cite> ) for comparison. The same could not be carried out for Chinese (which was the language (Jiang and Liu, 2010 ) worked on) due to the unavailability of projected data used in their work. Comparison with the traditional dependency parsers (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre, 2003; Goldberg and Elhadad, 2010) which train on complete dependency parsers is out of the scope of this work.",
  "y": "extends"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_7",
  "x": "While the Hindi projected treebank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in<cite> (Ganchev et al., 2009)</cite> . The datasets of Bulgarian and Spanish that contributed to the best accuracies for<cite> Ganchev et al. (2009)</cite> were used in our work (7 rules dataset for Bulgarian and 3 rules dataset for Spanish). The Hindi, Bulgarian and Spanish projected dependency treebanks have 44760, 39516 and 76958 sentences respectively.",
  "y": "uses"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_8",
  "x": "While the Hindi projected treebank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in<cite> (Ganchev et al., 2009)</cite> . The datasets of Bulgarian and Spanish that contributed to the best accuracies for<cite> Ganchev et al. (2009)</cite> were used in our work (7 rules dataset for Bulgarian and 3 rules dataset for Spanish). The Hindi, Bulgarian and Spanish projected dependency treebanks have 44760, 39516 and 76958 sentences respectively.",
  "y": "uses"
 },
 {
  "id": "0a7710557d020087035f4a94b5661c_9",
  "x": "For Bulgarian and Spanish, we used the same test data that was used in the work of<cite> Ganchev et al. (2009)</cite> . These test datasets had sentences from the training section of the CoNLL Shared Task (Nivre et al., 2007) that had lengths less than or equal to 10. All the test datasets have gold POS tags. A baseline parser was built to compare learning from partial parses with learning from fully connected parses.",
  "y": "uses background"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_0",
  "x": "In fact, emoji prediction, i.e., given a (usually short) message, predict its most likely associated emoji(s), may help to improve different NLP tasks (Novak et al., 2015) , such as information retrieval, generation of emojienriched social media content or suggestion of emojis when writing text messages or sharing pictures online. It has furthermore proven to be useful for sentiment analysis, emotion recognition and irony detection <cite>(Felbo et al., 2017)</cite> . The problem of emoji prediction, albeit recent, has already seen important developments.",
  "y": "background"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_1",
  "x": "First, we use the proposed label-wise mechanism to analyze the behavior of neural emoji classifiers, exploiting the attention weights to uncover and interpret emoji usages. Second, we experimentally compare the effect of the label-wise mechanism on the performance of an emoji classifier. We observed a performance improvement over competitive baselines such as FastText (FT) (Joulin et al., 2017) and Deepmoji <cite>(Felbo et al., 2017)</cite> , which is most noticeable in the case of infrequent emojis.",
  "y": "differences"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_2",
  "x": "**METHODOLOGY** Our base architecture is the Deepmoji model <cite>(Felbo et al., 2017)</cite> , which is based on two stacked word-based bi-directional LSTM recurrent neural networks with skip connections between the first and the second LSTM. The model also includes an attention module to increase its sensitivity to individual words during prediction.",
  "y": "uses"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_3",
  "x": "In<cite> Felbo et al. (2017)</cite> , attention is computed as follows: Here h i \u2208 R d is the hidden representation of the LSTM corresponding to the i th word, with N the total number of words in the sentence. The weight vector w a \u2208 R d and bias term b a \u2208 R map this hidden representation to a value that reflects the importance of this state for the considered classification problem.",
  "y": "background"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_4",
  "x": "The main architectural difference with respect to the typical attention is illustrated in Figure 1 . In<cite> Felbo et al. (2017)</cite> , attention is computed as follows: Here h i \u2208 R d is the hidden representation of the LSTM corresponding to the i th word, with N the total number of words in the sentence.",
  "y": "uses"
 },
 {
  "id": "0c2f7cea9f27b4799736fbcba48192_5",
  "x": "These extended experiments are performed on a corpus of around 100M tweets geolocalized in the United States and posted between October 2015 and May 2018. Models. In order to put our proposed labelwise attention mechanism in context, we compare its performance with a set of baselines: (1 <cite>(Felbo et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_0",
  "x": "Thus, CT has drawn more and more attention and became the dominant method in the Bakeoff 2005 and 2006. Although CT has shown its merits in word segmentation task, some researchers still hold the belief that on IV words DS can perform better than CT even in the restriction of Bakeoff closed test. Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005) , <cite>(Zhang et al., 2006a)</cite> .",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_1",
  "x": "Among these strategies, the confidence measure used to combine the results of CT and DS is a straight-forward one, which is introduced in <cite>(Zhang et al., 2006a)</cite> . The basic assumption of such combination is that DS method performs better on IV words and <cite>Zhang derives</cite> this belief from the fact that DS achieves higher IV recall rate as Table 1 shows. In which AS, CityU, MSRA and PKU are four corpora used in Bakeoff 2005 (also see Table 2 for detail).",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_2",
  "x": "Among these strategies, the confidence measure used to combine the results of CT and DS is a straight-forward one, which is introduced in <cite>(Zhang et al., 2006a)</cite> . The basic assumption of such combination is that DS method performs better on IV words and <cite>Zhang derives</cite> this belief from the fact that DS achieves higher IV recall rate as Table 1 shows. In which AS, CityU, MSRA and PKU are four corpora used in Bakeoff 2005 (also see Table 2 for detail).",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_3",
  "x": "Here we just employ them to evaluate segmentation results. Furthermore, our error analysis on the results of combination reveals that confidence measure in <cite>(Zhang et al., 2006a)</cite> has a representation flaw and we propose an EIV tag method to revise it. Finally, we give an empirical comparison between existing pure CT method and combination, which shows that pure CT method can produce state-of-the-art results on both IV word and overall segmentation.",
  "y": "motivation"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_6",
  "x": "Thus, we use the trigram language model to select top B (B is a constant predefined before search and in our experiment 3 is used) best candidates with highest probability at each stage so that the search algorithm can work in practice. Finally, when the whole sentence has been read, the best candidate with the highest probability will be selected as the segmentation result. Here, the term \"dictionary-based\" is exactly the method implemented in <cite>(Zhang et al., 2006a)</cite> , it does not mean the generative language model in general.",
  "y": "uses"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_7",
  "x": "Under CT scheme, each character in one sentence is labeled as \"B\" if it is the beginning of a word, \"O\" tag means the current character is a single-character word, other character is labeled as \"I\". For example, \"\u5168\u4e2d\u56fd (whole China)\" is labeled as \" In <cite>(Zhang et al., 2006a)</cite> , the above CT method is developed as subword-based tagging.",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_10",
  "x": "Here, we repeat two experiments described in <cite>(Zhang et al., 2006a)</cite> , namely dictionary-based approach and subword-based tagging. For CT method, top 2000 most frequent multi-character words and all single characters in training corpus are selected as subwords and the feature templates used for CRF model is listed in Table 3 . We present all the segmentation results in Table  6 to see the strength and weakness of each method conveniently.",
  "y": "uses"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_11",
  "x": "For CT method, top 2000 most frequent multi-character words and all single characters in training corpus are selected as subwords and the feature templates used for CRF model is listed in Table 3 . We present all the segmentation results in Table  6 to see the strength and weakness of each method conveniently. Based on IV and OOV recall as we show in Table 1 , <cite>Zhang argues</cite> that the DS performs better on IV word identification while CT performs better on OOV words. But we can see from the results in Table 6 (the lines about DS and CT), the IV precision of DS approach is much lower than that of CT on all the four corpora, which also causes a lower F measure of IV.",
  "y": "differences"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_12",
  "x": "In this way, the dictionary-based approach and the statistical model are combined. We choose the confidence measure to study because it is straight-forward. We show in this section that there is a representation flaw in the formula of confidence measure in <cite>(Zhang et al., 2006a )</cite>.",
  "y": "motivation"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_13",
  "x": "The basic idea of CM comes from the belief that CT performs better on OOV words while DS performs better on IV words. When both results of CT and DS are available, the CM can be calculated according to the following formula in <cite>(Zhang et al., 2006a)</cite> : Here, w is a subword, iob t is \"IOB\" tag given by CT and w t is \"IOB\" tag generated by DS.",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_14",
  "x": "Thus, the CM ultimately is the marginal probability of the \"IOB\" tag (MP). In the experiment of this paper, MP is used as CM because it is equivalent to <cite>Zhang\"s CM</cite> but more convenient to express. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_15",
  "x": "**EXPERIMENTS AND ERROR ANALYSIS ABOUT COMBINATION** We repeat the experiments about CM in <cite>Zhang\"s paper</cite> <cite>(Zhang et al., 2006a)</cite> and show that there is a representation flaw in the CM formula. Furthermore, we propose an EIV tag method to make CM yield a better result.",
  "y": "uses motivation"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_16",
  "x": "In this paper, \uf061 = 0.8 and t = 0.7 (Parameters in two papers, <cite>Zhang et al. 2006a</cite> and Zhang et al. 2006b , are different. And our parameters are consistent with Zhang et al. 2006b which is confirmed by Dr Zhang through email) are used in CM, namely MP= 0.875 is the threshold. Here, in Table 4 , we provide some statistics on the results of CT when MP is less than 0.875. From Table 4 we can see that even with MP less than 0.875, most of the subwords are still tagged correctly by CT and should not be revised by DS result.",
  "y": "differences"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_17",
  "x": "Table 5 illustrates that, after combining the two results, most original errors on IV words are corrected because DS can achieve higher IV recall as described in <cite>Zhang\"s paper</cite>. But on OOV part, more new errors are introduced by CM and these new errors decrease the precision of the IV words. For example, the OOV words \"\u8b66\u536b\u961f\u5458 (guard member)\" and \" \u8bbe\u8ba1\u8d39 (design fee)\" is recognized correctly by CT but with low CM. In the combining procedure, these words are wrongly split as IV errors: \"\u8b66\u536b (guard) \u961f\u5458 (member)\" and \"\u8bbe\u8ba1 (design) \u8d39 (fee)\".",
  "y": "background"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_18",
  "x": "We can see that on all four corpora the overall F measure of EIV result is higher than that of CT alone, which show that our EIV method works well. Now, let\"s check what changes happened in the number of error tags after EIV condition added into the CM. We can see from the Table 5 columns about EIV, there are more errors eliminated than the new errors introduced after EIV condition added into CM and most CT tags of subwords contained in OOV words maintained unchanged as we supposed. And then, our results (in Table  6 lines about EIV) are comparable with that in <cite>Zhang\"s paper</cite>.",
  "y": "similarities"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_20",
  "x": "We can see from the Table 6 this pure CT approach achieves the state-of-the-art results on all the corpora. On three of the four corpora (AS, MSRA and PKU) this pure CT method gets the best result. Even on IV word, this pure CT approach outperforms <cite>Zhang\"s CT method</cite> and produces comparable results with combination with EIV tags, which shows that pure CT method can perform well on IV words too.",
  "y": "differences"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_21",
  "x": "Table 6 Results of different approach used in our experiments (White background lines are the results we repeat <cite>Zhang\"s methods</cite> and they have some trivial difference with Table 1. ) Therefore, the most important thing worth to pay attention in future study is how to integrate linguistic information into the statistical model effectively, no matter character or word information. ---------------------------------- **CONCLUSIONS AND FUTURE WORK**",
  "y": "uses differences"
 },
 {
  "id": "0c8a99cac11953f26308128bfc058b_22",
  "x": "Furthermore, our experiments show that confidence measure in <cite>Zhang\"s paper</cite> has a representation flaw and we propose an EIV tag method to revise the combination. Finally, our experiments show that pure character-based approach also can achieve good IV word and overall performance. Perhaps, there are two reasons that existing combination results don\"t outperform the pure CT.",
  "y": "motivation"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_0",
  "x": "However, convolutional models must be significantly deeper to retrieve the same temporal receptive field [23] . Recently, the mechanism of self-attention<cite> [22,</cite> 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time. Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer<cite> [22]</cite> ), sentiment analysis [25] , and other tasks.",
  "y": "background"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_1",
  "x": "Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer<cite> [22]</cite> ), sentiment analysis [25] , and other tasks. These successes have motivated preliminary work in self-attention for ASR. Time-restricted self-attention was used as a drop-in replacement for individual layers in the state-of-theart lattice-free MMI model [26] , an HMM-NN system.",
  "y": "background"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_2",
  "x": "**MOTIVATING THE SELF-ATTENTION LAYER** We now replace recurrent and convolutional layers for CTC with self-attention [24] . Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder<cite> [22]</cite> , previous explorations of self-attention in ASR [19, 27] , and defined in Section 2.3.",
  "y": "uses similarities"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_3",
  "x": "**SEQUENTIAL OPERATIONS** Maximum path length Table 1 : Operation complexity of each layer type, based on<cite> [22]</cite> . T is input length, d is no. of hidden units, and k is filter/context width.",
  "y": "similarities uses"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_4",
  "x": "The first sublayer performs multi-head, scaled dot-product, self-attention<cite> [22]</cite> . For each head i of nhds, we learn linear maps W , and values V (i) of the i-th head, which combine to give",
  "y": "background"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_5",
  "x": "Heads are concatenated along the dh/nhds axis to give MltHdAtt = [HdAtt (1) , . . . , HdAtt (n hds ) ]. The second sublayer is a position-wise feed-forward network<cite> [22]</cite> FFN(H) = ReLU(HW1 + b1)W2 + b2 where parameters with the biases b1, b2 broadcasted over all T positions.",
  "y": "background"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_6",
  "x": "**POSITION** Self-attention is inherently content-based<cite> [22]</cite> , and so one often encodes position into the post-embedding vectors. We use standard trigonometric embeddings, where for 0 \u2264 i \u2264 demb/2, we define",
  "y": "background"
 },
 {
  "id": "0e4deda746127b97f68080bc8f13c8_7",
  "x": "We perform Nesterov-accelerated gradient descent on batches of 20 utterances. As self-attention architectures can be unstable in early training, we clip gradients to a global norm of 1 and use the standard linear warmup period before inverse square decay associated with these architectures [19, <cite>22]</cite> . Let n denote the global step number of the batch (across epochs); the learning rate is given by",
  "y": "extends differences"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_0",
  "x": "Experimental evidence shows the potential of the approach. In our previous work <cite>[7]</cite> , we applied a dual RNN in order to obtain a richer representation by blending the content and acoustic knowledge. In this paper, we improve upon our earlier work by incorporating an attention mechanism in the emotion recognition framework.",
  "y": "background"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_1",
  "x": "In our previous work <cite>[7]</cite> , we applied a dual RNN in order to obtain a richer representation by blending the content and acoustic knowledge. In this paper, we improve upon our earlier work by incorporating an attention mechanism in the emotion recognition framework. The proposed attention mechanism is trained to exploit both textual and acoustic information in tandem.",
  "y": "extends"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_2",
  "x": "Multi-modal approaches using acoustic features and textual information have been investigated. [5] identified emotional key phrases and salience of verbal cues from both phoneme sequences and words. Recently,<cite> [7,</cite> 18] combined acoustic information and conversation transcripts using a neural network-based model to improve emotion classification accuracy.",
  "y": "background"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_3",
  "x": "---------------------------------- **BIDIRECTIONAL RECURRENT ENCODER** Motivated by the architecture used in<cite> [7,</cite> 17, 19] , we train a recurrent encoder to predict the categorical class of a given audio signal.",
  "y": "motivation"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_4",
  "x": "To follow previous research <cite>[7]</cite> , we also add another prosodic feature vector, p, with each ot to generate a more informative vector representation of the signal, o A t . Finally, an emotion class is predicted from the acoustic signal by applying a softmax function to the final hidden representation at the last time step, o A last . We refer this model as audio-BRE with the objective function as follows:",
  "y": "uses"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_5",
  "x": "Figure 1 shows the architecture of the proposed MHA model. Previous research used multi-modal information independently using neural network model by concatenating features from each modality<cite> [7,</cite> 21] . As opposed to this approach, we propose a neural network architecture that exploits information in each modality by extracting relevant segments of the speech data using information from the lexical content (and vice-versa).",
  "y": "background"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_6",
  "x": "Previous research used multi-modal information independently using neural network model by concatenating features from each modality<cite> [7,</cite> 21] . As opposed to this approach, we propose a neural network architecture that exploits information in each modality by extracting relevant segments of the speech data using information from the lexical content (and vice-versa). First, the acoustic and textual data are encoded with the audio-BRE and text-BRE, respectively, using equation (1).",
  "y": "differences"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_7",
  "x": "For consistent comparison with previous works<cite> [7,</cite> 18] , all utterances labeled \"excitement\" are merged with those labeled \"happiness\". We assign single categorical emotion to the utterance with majority of annotators agreed on the emotion labels. The final dataset contains 5,531 utterances in total (1,636 happy, 1,084 sad, 1,103 angry and 1,708 neutral).",
  "y": "uses"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_8",
  "x": "---------------------------------- **FEATURE EXTRACTION AND IMPLEMENTATION DETAILS** As this research is extended work from previous research <cite>[7]</cite> , we use the same feature extraction method as done in our previous work.",
  "y": "extends"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_9",
  "x": "To measure the performance of systems, we report the weighted accuracy (WA) and unweighted accuracy (UA) averaging over the 10-fold cross-validation experiments. We use the same dataset and features as other researchers<cite> [7,</cite> 18] . Table 1 presents performances of proposed approaches for recognizing speech emotion in comparison with various models.",
  "y": "uses"
 },
 {
  "id": "0fd26c6dffab3fba2d120d2c58dff6_10",
  "x": "In audio-BRE ( Fig. 2(a) ), most of the emotion labels are frequently misclassified as neutral class, supporting the claims of<cite> [7,</cite> 25] . The text-BRE shows improvement in classifying most of the labels in Fig. 2(b) . In particular, angry and happy classes are correctly classified by 32% (57.14 to 75.41) and 63% (40.21 to 65.56) relative in accuracy with respect to audio-BRE, receptively.",
  "y": "similarities"
 },
 {
  "id": "0fd87fbdbe64e7d002ca31783448fb_0",
  "x": "Nowadays, AI demonstrates stronger potential for art creation. Many researches have been conducted to involve AI into poem generation [Zhang and Lapata, 2014; Cheng et al., 2018] , creation of classical or pop music<cite> [Manzelli et al., 2018</cite>; Hadjeres et al., 2017] and automatic images generation [van den Oord et al., 2016; Yan et al., 2016; Xu et al., 2018] . Whereas there are few researches exploring the possibility of artificial imagination for artwork creation.",
  "y": "background"
 },
 {
  "id": "0fd87fbdbe64e7d002ca31783448fb_1",
  "x": "We adopted open-source software jieba 2 for Chinese and Stanford parser<cite> [Toutanova and Manning, 2000]</cite> for English POS tagging. Only informative words such as noun, verb and adjective words are kept and TFIDF weights are calculated for further filtering. The output of this module is the keywords.",
  "y": "uses"
 },
 {
  "id": "0fd87fbdbe64e7d002ca31783448fb_2",
  "x": "As imagination is the soul for artistic Mind Map, Mappa Mundi employs several features to increase information variety<cite> [Liu et al., 2019]</cite> during topic expansion. It firstly uses word embeddings to find candidates based on semantic similarity <cite>[Mikolov et al., 2013</cite>;<cite> Pennington et al., 2014</cite>;<cite> Peters et al., 2018]</cite> . To enrich linguistic information of expansions, it also takes the morphological and phonological features into account.",
  "y": "background"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_0",
  "x": "(2013), infused with the bipartite matching dictionary prior of Haghighi et al. (2008) . However, like more recent approaches <cite>(Artetxe et al., 2017)</cite> , our model operates directly over pretrained word embeddings, induces a joint cross-lingual embedding space, and scales to large vocabulary sizes. To train our model, we derive a generalized expectationmaximization algorithm (EM; Neal and Hinton, 1998) and employ an efficient matching algorithm.",
  "y": "similarities"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_1",
  "x": "It also enables us to make implicit modeling assumptions explicit. To this end, we provide a reinterpretation of<cite> Artetxe et al. (2017)</cite> as a latent-variable model with an IBM Model 1-style (Brown et al., 1993 ) dictionary prior, which allows a clean side-by-side analytical comparison. Viewed in this light, the difference between our approach and<cite> Artetxe et al. (2017)</cite> , the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons.",
  "y": "extends"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_2",
  "x": "To this end, we provide a reinterpretation of<cite> Artetxe et al. (2017)</cite> as a latent-variable model with an IBM Model 1-style (Brown et al., 1993 ) dictionary prior, which allows a clean side-by-side analytical comparison. Viewed in this light, the difference between our approach and<cite> Artetxe et al. (2017)</cite> , the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons. Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over<cite> Artetxe et al. (2017)</cite> .",
  "y": "differences"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_3",
  "x": "Viewed in this light, the difference between our approach and<cite> Artetxe et al. (2017)</cite> , the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons. Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over<cite> Artetxe et al. (2017)</cite> . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_4",
  "x": "In addition, we experiment with priors that express different matchings in \u00a77. As for assumption (ii), previous work (Xing et al., 2015;<cite> Artetxe et al., 2017)</cite> has achieved some success using an orthogonal transformation; recently, however, demonstrated that monolingual embedding spaces are not approximately isomorphic and that there is a complex relationship between word form and meaning, which is only inadequately modeled by current approaches, which for example cannot model polysemy. Nevertheless, we will show that imbuing our model with these assumptions helps empirically in \u00a76, giving them practical utility.",
  "y": "background"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_5",
  "x": "In addition, we experiment with priors that express different matchings in \u00a77. As for assumption (ii), previous work (Xing et al., 2015;<cite> Artetxe et al., 2017)</cite> has achieved some success using an orthogonal transformation; recently, however, demonstrated that monolingual embedding spaces are not approximately isomorphic and that there is a complex relationship between word form and meaning, which is only inadequately modeled by current approaches, which for example cannot model polysemy. Nevertheless, we will show that imbuing our model with these assumptions helps empirically in \u00a76, giving them practical utility.",
  "y": "uses background"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_6",
  "x": "We additionally enforce the constraint that \u2126 is a real orthogonal matrix, i.e., \u2126 \u2126 = I. Previous work (Xing et al., 2015;<cite> Artetxe et al., 2017)</cite> found that the orthogonality constraint leads to noticeable improvements. Our M-step optimizes two objectives independently. First, making use of the result in equation (6), we optimize the following:",
  "y": "uses background"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_7",
  "x": "**REINTERPRETATION OF ARTETXE ET AL. (2017) AS A LATENT-VARIABLE MODEL** The self-training method of<cite> Artetxe et al. (2017)</cite> , our strongest baseline in \u00a76, may also be interpreted as a latent-variable model in the spirit of our exposition in \u00a73. Indeed, we only need to change the edge-set prior p(m) to allow for edge sets other than those that are matchings.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_8",
  "x": "We show how this corresponds to an alignment distribution that is equivalent to IBM Model 1 (Brown et al., 1993) , and that<cite> Artetxe et al. (2017)</cite> 's selftraining method is actually a form of Viterbi EM. To formalize<cite> Artetxe et al. (2017)</cite> 's contribution as a latent-variable model, we lay down some more notation. Let A = {1, . . . , n src + 1} ntrg , where we define (n src + 1) to be none, a distinguished symbol indicating unalignment.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_9",
  "x": "2011; Mena et al., 2018 ) may have been a computationally more effective manner to deal with the latent matchings. We show how this corresponds to an alignment distribution that is equivalent to IBM Model 1 (Brown et al., 1993) , and that<cite> Artetxe et al. (2017)</cite> 's selftraining method is actually a form of Viterbi EM. To formalize<cite> Artetxe et al. (2017)</cite> 's contribution as a latent-variable model, we lay down some more notation.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_10",
  "x": "thus, we can simply find a component-wise as follows: Artetxe et al. (2017) 's M-step The M-step remains unchanged from the exposition in \u00a73 with the exception that we fit \u2126 given matrices S a and T a formed from a one-to-many alignment a, rather than a matching m. Why a Reinterpretation? The reinterpretation of<cite> Artetxe et al. (2017)</cite> as a probabilistic model yields a clear analytical comparison between our method and theirs.",
  "y": "extends"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_11",
  "x": "Why a Reinterpretation? The reinterpretation of<cite> Artetxe et al. (2017)</cite> as a probabilistic model yields a clear analytical comparison between our method and theirs. The only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_12",
  "x": "**EXPERIMENTAL DETAILS** Datasets For bilingual dictionary induction, we use the English-Italian dataset by and the English-German and English-Finnish datasets by<cite> Artetxe et al. (2017)</cite> . For cross-lingual word similarity, we use the RG-65 and WordSim-353 cross-lingual datasets for English-German and the WordSim-353 cross-lingual dataset for EnglishItalian by Camacho-Collados et al. (2015) .",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_13",
  "x": "We follow<cite> Artetxe et al. (2017)</cite> and train monolingual embeddings with word2vec, CBOW, and negative sampling (Mikolov et al., 2013a ) on a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC), a 1.6 billion word corpus for Italian (itWaC), a 0.9 billion word corpus for German (SdeWaC), and a 2.8 billion word corpus for Finnish (Common Crawl). Seed dictionaries Following<cite> Artetxe et al. (2017)</cite>, we use dictionaries of 5,000 words, 25 words, and a numeral dictionary consisting of words matching the [0-9]+ regular expression in both vocabularies. 10 In line with , we additionally use a dictionary of identically spelled strings in both vocabularies.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_14",
  "x": "**MONOLINGUAL EMBEDDINGS** We follow<cite> Artetxe et al. (2017)</cite> and train monolingual embeddings with word2vec, CBOW, and negative sampling (Mikolov et al., 2013a ) on a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC), a 1.6 billion word corpus for Italian (itWaC), a 0.9 billion word corpus for German (SdeWaC), and a 2.8 billion word corpus for Finnish (Common Crawl). Seed dictionaries Following<cite> Artetxe et al. (2017)</cite>, we use dictionaries of 5,000 words, 25 words, and a numeral dictionary consisting of words matching the [0-9]+ regular expression in both vocabularies.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_15",
  "x": "Implementation details Similar to<cite> Artetxe et al. (2017)</cite> , we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 \u00d7 10 \u22126 between succeeding iterations. Unless stated otherwise, we induce a dictionary of 200,000 source and 200,000 target words as in previous work (Mikolov et al., 2013c; Artetxe et al., 2016) . For optimal 1:1 alignment, we have observed the best results by keeping the top k = 3 most similar target words.",
  "y": "similarities"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_16",
  "x": "Baselines We compare our approach with and without the rank constraint to the original bilingual mapping approach by Mikolov et al. (2013c) . In addition, we compare with Zhang et al. (2016) and Xing et al. (2015) who augment the former with an orthogonality constraint and normalization and an orthogonality constraint respectively. Finally, we compare with Artetxe et al. (2016) who add dimension-wise mean centering to Xing et al. (2015) , and<cite> Artetxe et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_17",
  "x": "Both Mikolov et al. (2013c) and<cite> Artetxe et al. (2017)</cite> are special cases of our famework and comparisons to these approaches thus act as an ablation study. Specifically, Mikolov et al. (2013c) does not employ orthogonal Procrustes, but rather allows the learned matrix \u2126 to range freely. Likewise, as discussed in \u00a75,<cite> Artetxe et al. (2017)</cite> make use of a Viterbi EM style algorithm with a different prior over edge sets.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_18",
  "x": "Likewise, as discussed in \u00a75,<cite> Artetxe et al. (2017)</cite> make use of a Viterbi EM style algorithm with a different prior over edge sets. 13 ----------------------------------",
  "y": "background"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_19",
  "x": "12 Training takes a similar amount of time as <cite>(Artetxe et al., 2017)</cite> due to faster convergence. 13 Other recent improvements such as symmetric reweighting (Artetxe et al., 2018) are orthogonal to our method, which is why we do not explicitly compare to them here. 14 Note that results are not directly comparable to (Conneau et al., 2018) due to the use of embeddings trained on different monolingual corpora (WaCKy vs. Wikipedia).",
  "y": "similarities"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_20",
  "x": "On cross-lingual word similarity, our approach yields the best performance on WordSim-353 and RG-65 for English-German and is only outperformed by<cite> Artetxe et al. (2017)</cite> on English-Italian Wordsim-353. ---------------------------------- **ANALYSIS**",
  "y": "differences"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_21",
  "x": "We show the target language words with the highest hubness using our method and<cite> Artetxe et al. (2017)</cite> for English-German with a 5,000 seed lexicon and the full vocabulary in Table 3 . 17 Hubs are fewer and occur less often with our method, demonstrating that the prior-to some en-tr en-bn en-hi et-fi<cite> Artetxe et al. (2017)</cite> extent-aids with resolving hubness. Interestingly, compared to , hubs seem to occur less often and are more meaningful in current cross-lingual word embedding models.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_22",
  "x": "where Q is a set of query source language words and NN k (x, G) denotes the k nearest neighbors of x in the graph G. 16 In accordance with Lazaridou et al. (2015), we set k = 20 and use the words in the evaluation dictionary as query terms. We show the target language words with the highest hubness using our method and<cite> Artetxe et al. (2017)</cite> for English-German with a 5,000 seed lexicon and the full vocabulary in Table 3 . 17 Hubs are fewer and occur less often with our method, demonstrating that the prior-to some en-tr en-bn en-hi et-fi<cite> Artetxe et al. (2017)</cite> extent-aids with resolving hubness.",
  "y": "differences"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_23",
  "x": "We perform experiments with our method with and without a rank constraint and<cite> Artetxe et al. (2017)</cite> for three truly lowresource language pairs, English-{Turkish, Bengali, Hindi}. We additionally conduct an experiment for Estonian-Finnish, similarly to . For all languages, we use fastText embeddings (Bojanowski et al., 2017) trained on Wikipedia, the evaluation dictionaries provided by Conneau et al. (2018) , and a seed lexicon based on identical strings to reflect a realistic use case. We note that English does not share scripts with Bengali and Hindi, making this even more challenging.",
  "y": "uses"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_24",
  "x": "Surprisingly, the method by<cite> Artetxe et al. (2017)</cite> a similar self-learning method that uses word embeddings, with an implicit one-to-many alignment based on nearest neighbor queries. Vuli\u0107 and Korhonen (2016) proposed a more strict one-to-many alignment based on symmetric translation pairs, which is also used by Conneau et al. (2018) . Our method bridges the gap between early latent variable and word embedding-based approaches and explicitly allows us to reason over its prior.",
  "y": "similarities"
 },
 {
  "id": "10319b67b6fcc1870791cf67b39299_25",
  "x": "**CONCLUSION** We have presented a novel latent-variable model for bilingual lexicon induction, building on the work of<cite> Artetxe et al. (2017)</cite> . Our model combines the prior over bipartite matchings inspired by Haghighi et al. (2008) and the discriminative, rather than generative, approach inspired by Irvine and CallisonBurch (2013) .",
  "y": "extends"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_0",
  "x": "Low-resource languages lack manually annotated data to learn even the most basic models such as part-of-speech (POS) taggers. To compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: -aligned parallel corpora to project POS annotations to target languages (Yarowsky et al., 2001; Agi\u0107 et al., 2015; Fang and Cohn, 2016) , -noisy tag dictionaries for type-level approximation of full supervision<cite> (Li et al., 2012)</cite> , -combination of projection and type constraints (Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013) , -rapid annotation of seed training data . However, only one or two compatible sources of distant supervision are typically employed.",
  "y": "background"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_1",
  "x": "To compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: -aligned parallel corpora to project POS annotations to target languages (Yarowsky et al., 2001; Agi\u0107 et al., 2015; Fang and Cohn, 2016) , -noisy tag dictionaries for type-level approximation of full supervision<cite> (Li et al., 2012)</cite> , -combination of projection and type constraints (Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013) , -rapid annotation of seed training data . However, only one or two compatible sources of distant supervision are typically employed. In reality severely under-resourced languages may require a more pragmatic \"take what you can get\" viewpoint.",
  "y": "motivation"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_2",
  "x": "Dictionaries are a useful source for distant supervision<cite> (Li et al., 2012</cite>; T\u00e4ckstr\u00f6m et al., 2013) . There are several ways to exploit such information: i) as type constraints during encoding (T\u00e4ckstr\u00f6m et al., 2013) , ii) to guide unsupervised learning<cite> (Li et al., 2012)</cite> , or iii) as additional signal at training. We focus on the latter and evaluate two ways to integrate lexical knowledge into neural models, while comparing to the former two: a) by representing lexicon properties as n-hot vector (e.g., if a word has two properties according to lexicon src, it results in a 2-hot vector, if the word is not present in src, a zero vector), with m the number of lexicon properties; b) by embedding the lexical features, i.e., e src is a lexicon src embedded into an l-dimensional space.",
  "y": "background"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_3",
  "x": "Dictionaries. Dictionaries are a useful source for distant supervision<cite> (Li et al., 2012</cite>; T\u00e4ckstr\u00f6m et al., 2013) . There are several ways to exploit such information: i) as type constraints during encoding (T\u00e4ckstr\u00f6m et al., 2013) , ii) to guide unsupervised learning<cite> (Li et al., 2012)</cite> , or iii) as additional signal at training.",
  "y": "background"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_4",
  "x": "We represent e src as concatenation of all embedded m properties of length l, and a zero vector otherwise. Tuning on the dev set, we found the second embedding approach to perform best, and simple concatenation outperformed mean vector representations. We evaluate two dictionary sources, motivated by ease of accessibility to many languages: WIK-TIONARY, a word type dictionary that maps tokens to one of the 12 Universal POS tags<cite> (Li et al., 2012</cite>; Petrov et al., 2012) ; and UNIMORPH, a morphological dictionary that provides inflectional paradigms across 350 languages (Kirov et al., 2016) .",
  "y": "uses"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_5",
  "x": "We evaluate two dictionary sources, motivated by ease of accessibility to many languages: WIK-TIONARY, a word type dictionary that maps tokens to one of the 12 Universal POS tags<cite> (Li et al., 2012</cite>; Petrov et al., 2012) ; and UNIMORPH, a morphological dictionary that provides inflectional paradigms across 350 languages (Kirov et al., 2016) . For Wiktionary, we use the freely available dictionaries from <cite>Li et al. (2012)</cite> and . The size of the dictionaries ranges from a few thousands (e.g., Hindi and Bulgarian) to 2M (Finnish UniMorph).",
  "y": "uses"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_6",
  "x": "We compare to the following weaklysupervised POS taggers: -AGIC: Multi-source annotation projection with Bible parallel data by Agi\u0107 et al. (2015) . -LI: Wiktionary supervision<cite> (Li et al., 2012)</cite> .",
  "y": "uses"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_7",
  "x": "It helps the most on morphological rich languages such as Uralic. On the test sets (Table 4 , right) DSDS reaches 87.2 over 8 test languages intersecting <cite>Li et al. (2012)</cite> and Agi\u0107 et al. (2016) . It reaches 86.2 over the more commonly used 8 languages of Das and Petrov (2011) , compared to their 83.4.",
  "y": "similarities"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_8",
  "x": "This is in slight contrast to 50 iterations that <cite>Li et al. (2012)</cite> recommend, although selecting 50 does not dramatically hurt the scores; ii) Our replication falls \u223c5 points short of their 84.9 accuracy. There is a large 33-point accuracy gap between the scores of <cite>Li et al. (2012)</cite> , where the dictionaries are large, and the other languages in Figure 4 , with smaller dictionaries. Compared to DAS, our tagger clearly benefits from pre-trained word embeddings, while theirs relies on label propagation through Europarl, a much cleaner corpus that lacks the coverage of the noisier WTC.",
  "y": "differences"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_9",
  "x": "We show: i) LI peaks at 10 iterations for their test languages, and at 35 iterations for all the rest. This is in slight contrast to 50 iterations that <cite>Li et al. (2012)</cite> recommend, although selecting 50 does not dramatically hurt the scores; ii) Our replication falls \u223c5 points short of their 84.9 accuracy. There is a large 33-point accuracy gap between the scores of <cite>Li et al. (2012)</cite> , where the dictionaries are large, and the other languages in Figure 4 , with smaller dictionaries.",
  "y": "background"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_10",
  "x": "---------------------------------- **RELATED WORK** Most successful work on low-resource POS tagging is based on projection (Yarowsky et al., 2001) , tag dictionaries<cite> (Li et al., 2012)</cite> , annotation of seed training data or even more recently some combination of these, e.g., via multi-task learning (Fang and not <cite>Li et al. (2012)</cite> Figure 4: The performance of LI with our dictionary data over EM iterations, separate for the languages from <cite>Li et al. (2012)</cite> and all the remaining languages in Table 1 . Cohn, 2016; Kann et al., 2018) .",
  "y": "background"
 },
 {
  "id": "1042e7b6ef7b73f29ad75b193f9e3b_11",
  "x": "Most successful work on low-resource POS tagging is based on projection (Yarowsky et al., 2001) , tag dictionaries<cite> (Li et al., 2012)</cite> , annotation of seed training data or even more recently some combination of these, e.g., via multi-task learning (Fang and not <cite>Li et al. (2012)</cite> Figure 4: The performance of LI with our dictionary data over EM iterations, separate for the languages from <cite>Li et al. (2012)</cite> and all the remaining languages in Table 1 . Cohn, 2016; Kann et al., 2018) . Our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed. Most prior work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are unnecessary for deep learning methods.",
  "y": "extends differences"
 },
 {
  "id": "10acbeba830b2f8b3feb30de542c56_0",
  "x": "The Touchdown dataset <cite>(Chen et al., 2019)</cite> provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively with the Touchdown tasks, we are publicly releasing the 29k raw Street View panoramas needed for Touchdown. We follow the process used for the StreetLearn data release (Mirowski et al., 2019) to check panoramas for personally identifiable information and blur them as necessary.",
  "y": "background"
 },
 {
  "id": "10acbeba830b2f8b3feb30de542c56_2",
  "x": "Until recently, the most commonly studied domains were map-based (Thompson et al., 1993) or game-like (Macmahon et al., 2006; Misra et al., 2017 Misra et al., , 2018 Hermann et al., 2017; Hill et al., 2017) . These environments enabled substantial progress, but the complexity and diversity of the visual input they provide is limited. This greatly simplifies both the language and vision challenges. To address this, recent tasks based on simulated environments include photo-realistic visual input, such as Room-to-Room (R2R; Anderson et al., 2018) , Talk-the-Walk (de Vries et al., 2018) and Touchdown <cite>(Chen et al., 2019)</cite> , all of which rely on panorama photos.",
  "y": "background"
 },
 {
  "id": "10acbeba830b2f8b3feb30de542c56_3",
  "x": "---------------------------------- **EXPERIMENTS** We re-implement the best-reported models on the navigation and spatial description resolution tasks from<cite> Chen et al. (2019)</cite> to compare performance with our data release to the original Touchdown paper.",
  "y": "uses"
 },
 {
  "id": "10acbeba830b2f8b3feb30de542c56_4",
  "x": "Spatial Description Resolution. SDR results are given in Table 1 . Following<cite> Chen et al. (2019)</cite> , we report mean distance error and accuracy with different thresholds (40px, 80px, and 120px), which measures the proportion of evaluation items where the pixel chosen by the model is within the specified pixel distance.",
  "y": "uses"
 },
 {
  "id": "10acbeba830b2f8b3feb30de542c56_6",
  "x": "\u2022 Normalized Dynamic Time Warping (nDTW): a minimized cumulative distance between the agent path and true path, normalized by path length. \u2022 Success weighted Dynamic Time Warping (SDTW): nDTW, with points awarded only for successful paths. TC, SPD, and SED are defined in<cite> Chen et al. (2019)</cite> and nDTW and SDTW are defined in Ilharco et al. (2019) .",
  "y": "background"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_0",
  "x": "Only Hamilton et al. (2016) used SVD PPMI in some of their very recent experiments and showed it to be adequate for exploring historical semantics. The Google Books Ngram corpus (GBN; Michel et al. (2011 ), Lin et al. (2012 ) is used in most of the studies we already mentioned, including our current study and its predecessor <cite>(Hellrich and Hahn, 2016a)</cite> . It contains about 6% of all books published between 1500 and 2009 in the form of n-grams (up to pentagrams), together with their frequency for each year.",
  "y": "uses background"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_1",
  "x": "Yet, we gathered evidence that the inherent randomness involved in their generation affects the reliability of word neighborhood judgments and demonstrate how this hampers qualitative conclusions based on such models. Our investigation was performed on both historical (for the time span of 1900 to 1904) and contemporary texts (for the time span of 2005 to 2009) in two languages, English and German. It is thus a continuation of prior work, in which we investigated historical English texts only <cite>(Hellrich and Hahn, 2016a)</cite> , and also influenced by the design decisions of Kim et al. (2014) and Kulkarni et al. (2015) which were the first to use word embeddings in diachronic studies.",
  "y": "background"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_2",
  "x": "There are two strategies for managing the huge number of potential contexts a word can appear in. Skip-gram hierarchical softmax (SGHS) uses a binary tree to more efficiently represent the vocabulary, whereas skip-gram negative sampling (SGNS) updates only a limited number of word vectors during each training step. SGNS is preferred in general, yet SGHS showed slight benefits in some reliability scenarios in our prior investigations <cite>(Hellrich and Hahn, 2016a)</cite> .",
  "y": "background"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_3",
  "x": "These models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor (Kim et al., 2014; Hellrich and Hahn, 2016b) , or a mapping between models for different points in time must be calculated (Kulkarni et al., 2015; Hamilton et al., 2016) . The first approach cannot be performed in parallel and is thus rather time-consuming, if texts are not subsampled. We nevertheless discourage using samples instead of full corpora, as we observed extremely low reliability values between different samples <cite>(Hellrich and Hahn, 2016a)</cite> .",
  "y": "background motivation"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_5",
  "x": "We processed the full subcorpora for each time span, due to the extremely low reliability values between samples we observed in previous investigations <cite>(Hellrich and Hahn, 2016a)</cite> . We tested both SGNS with 5 noise words and SGHS training strategies and trained for 10 iterations, saving the resulting embeddings after each epoch. During each epoch the learning rate was decreased from 0.025 to 0.0001.",
  "y": "motivation"
 },
 {
  "id": "1265a336e56a4535f0a904ca89b220_6",
  "x": "For German, we focus on the normalized version due to the overall similar performance and suitability for further applications. Influence of Neighborhood Size. Reliability at different top-n cut-offs is very similar for all languages and time spans under scrutiny, confirming previous observations in<cite> Hellrich and Hahn (2016a)</cite> and strengthening the suggestion to use only top-1 reliability for evaluation.",
  "y": "similarities"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_0",
  "x": "When combined with our two-stage fine-tuning pipeline, our method achieves improved common sense reasoning and state-of-the-art perplexity on the WritingPrompts<cite> (Fan et al., 2018)</cite> story generation dataset. ---------------------------------- **INTRODUCTION**",
  "y": "uses"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_1",
  "x": "When fine-tuning is combined with multi-task learning in a two-stage pipeline, we improve the model's CSR and outperform state-of-the-art perplexity on the WritingPrompts<cite> (Fan et al., 2018)</cite> Our primary task is to perform language modeling (Elman, 1990; Bengio et al., 2003; Dai and Le, 2015) on the WritingPrompts dataset. A language model learns to assign the probability of a text sequence X = x 1 , . . . , x T using the conditional probability factorization: We train our model using a standard cross-entropy loss between next-step true tokens and predicted probabilities given current tokens.",
  "y": "uses"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_2",
  "x": "A language model learns to assign the probability of a text sequence X = x 1 , . . . , x T using the conditional probability factorization: We train our model using a standard cross-entropy loss between next-step true tokens and predicted probabilities given current tokens. WritingPrompts<cite> (Fan et al., 2018</cite> ) is a dataset of prompts and short stories crawled from Reddit.",
  "y": "background"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_3",
  "x": "Readability is measured in terms of model perplexity on the test set of WritingPrompts. Because GPT2 uses subword tokenization (Sennrich et al., 2016) , it is not directly comparable to the wordlevel perplexity obtained in<cite> Fan et al. (2018)</cite> . We estimate the corresponding word-level perplexity by taking the product of each subword's probabilities to obtain probabilities for each word.",
  "y": "background"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_4",
  "x": "Both sub-word perplexity and word-level perplexities are reported in our experiments. Prompt ranking<cite> (Fan et al., 2018)</cite> assesses how well a model matches a story to its given prompt. This is measured by computing the likelihood of stories conditioned under ten different prompts, nine of which are randomly sampled and one is the true prompt.",
  "y": "background"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_5",
  "x": "Prompt ranking<cite> (Fan et al., 2018)</cite> assesses how well a model matches a story to its given prompt. This is measured by computing the likelihood of stories conditioned under ten different prompts, nine of which are randomly sampled and one is the true prompt. Following<cite> Fan et al. (2018)</cite> , we count a random story sample as correct when it ranks the true prompt with the lowest perplexity.",
  "y": "uses"
 },
 {
  "id": "13fe4afa75c5a02727cb8ce3a73297_7",
  "x": "---------------------------------- **RELATED WORK** Story Generation: Recent work in neural story generation (Kiros et al., 2015; Roemmele, 2016) has shown success in using hierarchical methods (Yao et al., 2018; <cite>Fan et al., 2018)</cite> to generate stories.",
  "y": "background"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_0",
  "x": "In particular, the method of <cite>Chen and Manning (2014)</cite> is the same as our NN baseline. Note that Zhou et al. (2015) reports a UAS of 91.47% by this parser, which is higher than the results we obtained. The main results include the use of different batch size during, while Zhou et al. (2015) used a batch size of 100,000, we used a batch size of 10,000 in all experiments.",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_1",
  "x": "All systems are deterministic. Our combined parser gives accuracies competitive to state-of-the-art deterministic parsers in the literature. In particular, the method of <cite>Chen and Manning (2014)</cite> is the same as our NN baseline.",
  "y": "similarities"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_2",
  "x": "Recently, <cite>Chen and Manning (2014)</cite> use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 A neural network model takes continuous vector representations of words as inputs, which can be pre-trained using large amounts of unlabeled data, thus containing more information. In addition, using an extra hidden layer, a neural network is capable of learning non-linear relations between automatic features, achieving feature combinations automatically.",
  "y": "background"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_3",
  "x": "---------------------------------- **PARSER** We take <cite>Chen and Manning (2014)</cite> , which uses the arc-standard transition system (Nivre, 2008) .",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_4",
  "x": "We take <cite>Chen and Manning (2014)</cite> , which uses the arc-standard transition system (Nivre, 2008) . Given an POS-tagged input sentence, it builds a projective output y by performing a sequence of state transition actions using greedy search. <cite>Chen and Manning (2014)</cite> can be viewed as a neutral alternative of MaltParser (Nivre, 2008) .",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_5",
  "x": "---------------------------------- **MODELS** Following <cite>Chen and Manning (2014)</cite> , training of all the models using a cross-entropy loss objective with a L2-regularization, and mini-batched AdaGrad (Duchi et al., 2011) .",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_6",
  "x": "We take the Neural model of <cite>Chen and Manning (2014)</cite> as another baseline (Figure 1(b) ). Given a parsing state x, the words are first mapped into continuous vectors by using a set of pre-trained word embeddings. Denote the mapping as \u03a6 e (x).",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_7",
  "x": "where \u2212 \u2192 \u03b8 c,a is the set of parameters between the hidden and output layers. We use the arc-standard features \u03a6 e as <cite>Chen and Manning (2014)</cite> , which is also based on the arc-eager templates of Zhang and Nivre (2011) , similar to those of the baseline model L. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_8",
  "x": "Following <cite>Chen and Manning (2014)</cite> , we use the pre-trained word embedding released by Collobert et al. (2011) , and set h = 200 for the hidden layer size, \u03bb = 10 \u22128 for L2 regularization, and \u03b1 = 0.01 for the initial learning rate of Adagrad. ---------------------------------- **DEVELOPMENT RESULTS**",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_9",
  "x": "<cite>Chen and Manning (2014)</cite> fine-tune word embeddings in supervised training, consistent with Socher et al. (2013) . Intuitively, fine-tuning embeddings allows in-vocabulary words to join the parameter space, thereby giving better fitting to in-domain data. However, it also forfeits the benefit of large-scale pre-training, because out-of-vocabulary (OOV) words do not have their embeddings fine-tuned.",
  "y": "background"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_10",
  "x": "The observation on Turian is similar. For the final experiments, we apply fine-tuning on the NN model, but not to the Turian and This. Note also tat for all experiments, the POS and label embedding features of <cite>Chen and Manning (2014)</cite> are fine-tuned, consistent with their original method.",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_11",
  "x": "Dropout rate. We test the effect of dropout (Hinton et al., 2012) during training, using a default ratio of 0.5 according to <cite>Chen and Manning (2014)</cite> . In our experiments, we find that the dense NN model and our combined model achieve better performances by using dropout, but the other models do not benefit from dropout.",
  "y": "uses"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_12",
  "x": "**FINAL RESULTS** The final results across web domains are shown in Table 2 . Our logistic regression linear parser and re-implementation of <cite>Chen and Manning (2014)</cite> give comparable accuracies to the perceptron ZPar 2 and Stanford NN Parser 3 , respectively.",
  "y": "uses similarities"
 },
 {
  "id": "14fa8c3b947667244d30dd30dae89a_13",
  "x": "Our parser enjoys the fast speed of deterministic parsers, and in particular the baseline NN parser<cite> (Chen and Manning, 2014)</cite> . ---------------------------------- **WSJ EXPERIMENTS**",
  "y": "similarities"
 },
 {
  "id": "1781b27c13dca15752cb6aa8a9fc38_0",
  "x": "The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs. The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (Blei et al., 2010) , hierarchical Pitman-Yor process (Teh, 2006) , Indian buffet process (Ghahramani and Griffiths, 2005) , recurrent neural network (Mikolov et al., 2010; Van Den Oord et al., 2016) , long short-term memory (Hochreiter and Schmidhuber, 1997; , sequence-to-sequence model (Sutskever et al., 2014), variational auto-encoder (Kingma and Welling, 2014) , generative adversarial network (Goodfellow et al., 2014) , attention mechanism (Chorowski et al., 2015; Seo et al., 2016) , memory-augmented neural network (Graves et al., 2014; Graves et al., 2014) , stochastic neural network <cite>Miao et al., 2016)</cite> , predictive state neural network (Downey et al., 2017) , policy gradient (Yu et al., 2017) and reinforcement learning (Mnih et al., 2015) .",
  "y": "uses background"
 },
 {
  "id": "1786b6c1c6532d5baa092cca40e389_0",
  "x": "In particular, data-driven models for lexical semantics require the creation of broad-coverage, hand-annotated corpora with predicateargument information, i.e. rich information about words expressing a semantic relation having argument slots filled by the interpretations of their grammatical complements. Corpora combining semantic and syntactic annotations constitute the backbone for the development of probabilistic models that automatically identify the semantic relationships, or semantic roles, conveyed by sentential constituents<cite> (Gildea and Jurafsky, 2002)</cite> . That is, given an input sentence and a target predicator the system labels constituents with general roles like Agent, Patient, Theme, etc., or more specific roles, as in (1) .",
  "y": "background"
 },
 {
  "id": "1786b6c1c6532d5baa092cca40e389_1",
  "x": "The representation of test (8), stereotype (<cite>10</cite>), and find out (11) in terms of two Notion relations, one of which is treated as more salient, reifies the concept of relative significance of Proto-Role properties in the verbal semantics. This concept is related to the weighting of entailments in the overall semantics of a verb, which plays a critical role in determining the syntactic patterns in which the verb appears (i.e. the grammatical realisations of its arguments). <cite>10</cite> The verbs in (8) and (9) involve an additional entailment of Intentionality.",
  "y": "uses"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_0",
  "x": "**INTRODUCTION** Recently, pre-trained language representation models such as GPT (Radford et al., 2018 (Radford et al., , 2019 , ELMo (Peters et al., 2018) , BERT<cite> (Devlin et al., 2019)</cite> and XLNet have achieved promising results in NLP tasks, including reading comprehension (Rajpurkar et al., 2016) , natural language inference (Bowman et al., 2015; Williams et al., 2018) and sentiment classification (Socher et al., 2013) . These models capture contextual information from large-scale unlabelled corpora via well-designed pre-training * Equal contribution \u2020 Corresponding author: Minlie Huang tasks.",
  "y": "background"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_1",
  "x": "The literature has commonly reported that pre-trained models can be used as effective feature extractors and achieve state-of-the-art performance on various downstream tasks . Although pre-trained language representation models have achieved transformative performance, the pre-training tasks like masked language model and next sentence prediction<cite> (Devlin et al., 2019)</cite> neglect to consider the linguistic knowledge. We argue that such knowledge is important for some NLP tasks, particularly for sentiment analysis.",
  "y": "motivation"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_2",
  "x": "Early work on pre-trained language representation models mainly focuses on distributed word representations, such as word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) . Since the distributed word representation is independent of context, it's challenging for such representation to model the complex word characteristics under different contexts. Thus contextual language representation based on pre-trained models including CoVe (McCann et al., 2017) , ELMo (Peters et al., 2018) , GPT (Radford et al., 2018 (Radford et al., , 2019 and BERT<cite> (Devlin et al., 2019)</cite> becomes prevalent recently.",
  "y": "background"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_3",
  "x": "These models use deep LSTM (Hochreiter and Schmidhuber, 1997) or Transformer (Vaswani et al., 2017) as the encoder to acquire contextual language representation. Various pre-training tasks were explored including traditional NLP tasks like machine translation (Mc-Cann et al., 2017) and language model (Peters et al., 2018; Radford et al., 2018 Radford et al., , 2019 , or other tasks such as masked language model and next sentence prediction<cite> (Devlin et al., 2019)</cite> . With the advent of BERT<cite> (Devlin et al., 2019)</cite> achieving state-of-the-art performances on various NLP tasks, many variants of BERT have been proposed.",
  "y": "background"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_4",
  "x": "These models use deep LSTM (Hochreiter and Schmidhuber, 1997) or Transformer (Vaswani et al., 2017) as the encoder to acquire contextual language representation. Various pre-training tasks were explored including traditional NLP tasks like machine translation (Mc-Cann et al., 2017) and language model (Peters et al., 2018; Radford et al., 2018 Radford et al., , 2019 , or other tasks such as masked language model and next sentence prediction<cite> (Devlin et al., 2019)</cite> . With the advent of BERT<cite> (Devlin et al., 2019)</cite> achieving state-of-the-art performances on various NLP tasks, many variants of BERT have been proposed.",
  "y": "background"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_5",
  "x": "Compared with the vanilla pretrained models like BERT<cite> (Devlin et al., 2019)</cite> , our model enriches the input sequence with its linguistic knowledge including part-of-speech tags and sentiment polarity labels, and utilizes a modified masked language model to capture the relationship between sentence-level sentiment labels and word-level knowledge in addition to context dependency. ---------------------------------- **LINGUISTIC KNOWLEDGE ACQUISITION**",
  "y": "differences"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_6",
  "x": "**PRE-TRAINING TASKS** During pre-training, Label-aware masked language model (LA-MLM) and next sentence prediction (NSP) are adopted as the pre-training tasks where the setting of NSP is identical to the one proposed by<cite> Devlin et al. (2019)</cite> . Label-aware masked language model is designed to utilize the linguistic knowledge to grasp the implicit dependency between sentence-level sentiment labels and words in addition to context dependency.",
  "y": "uses similarities"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_7",
  "x": "We follow the fine-tuning setting of the existing work <cite>(Devlin et al., 2019</cite>; : Sentence-level Sentiment Classification: The input of this task is a text sequence ([CLS], x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x n , [SEP]). The sentiment label is obtained based on the hidden state of [CLS]. Aspect-level Sentiment Classification: In addition to the text sequence, the input additionally contains an aspect term / aspect category sequence (a 1 , \u00b7 \u00b7 \u00b7 , a l ).",
  "y": "uses"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_8",
  "x": "Each review consists of 8.1 sentences and 127.8 words on average. Since our method can adapt to all the BERTstyle pre-training models, we used vanilla BERT<cite> (Devlin et al., 2019)</cite> as the base framework to construct Transformer blocks in this paper and leave the exploration of other models like RoBERTa as future work. The hyperparameters of the Transformer blocks were set to be the same as BERT-Base due to the limited computational power.",
  "y": "uses"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_10",
  "x": "Since the improvement of Macro-F1 is more notable than that of accuracy, it is convinced that our model actually does better in all the three sentiment classes. Due to the sparsity of aspect terms compared with aspect categories, our model improved a larger margin on the task of aspect category sentiment classification than the<cite> (Devlin et al., 2019)</cite> . To explore whether the performance of Sen-tiLR on common NLP tasks will improve or degrade, we evaluated our model on General Language Understanding Evaluation (GLUE) benchmark , which collects diverse language understanding tasks.",
  "y": "differences"
 },
 {
  "id": "183cf87042a3ad2180ead67555d247_11",
  "x": "Since the test sets of GLUE are not publicly available, we reported the results on development sets in Table 6 . Note that we directly used the results of BERT on SST-2, MNLI, QNLI and MRPC which are reported by<cite> Devlin et al. (2019)</cite> and reimplemented the BERT model fine-tuned on the rest of the tasks by ourselves. From Table 6 , SentiLR surely gets better results on the tasks in sentiment analysis like SST-2.",
  "y": "uses"
 },
 {
  "id": "19233e4954d7e75ac01112a4c07e64_0",
  "x": "**MODELS** First, we discuss our baseline model which is similar to the machine translation encoder-alignerdecoder model of Luong et al. (2015) , and presented by<cite> Chopra et al. (2016)</cite> . Next, we introduce our multi-task learning approach of sharing the parameters between abstractive summarization and entailment generation models.",
  "y": "uses"
 },
 {
  "id": "19233e4954d7e75ac01112a4c07e64_1",
  "x": "---------------------------------- **BASELINE MODEL** Our baseline model is a strong, multi-layered encoder-attention-decoder model with bilinear attention, similar to Luong et al. (2015) and following the details in<cite> Chopra et al. (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "19233e4954d7e75ac01112a4c07e64_2",
  "x": "**EVALUATION** Following previous work (Nallapati et al., 2016; <cite>Chopra et al., 2016</cite>; Rush et al., 2015) , we use the full-length F1 variant of Rouge (Lin, 2004) for the Gigaword results, and the 75-bytes length limited Recall variant of Rouge for DUC. Additionally, we also report other standard language generation metrics (as motivated recently by See et al. (2017) ): METEOR (Denkowski and Lavie, 2014) , BLEU-4 (Papineni et al., 2002) , and CIDEr-D , based on the MS-COCO evaluation script (Chen et al., 2015) .",
  "y": "uses"
 },
 {
  "id": "19233e4954d7e75ac01112a4c07e64_4",
  "x": "Baseline Results and Previous Work Our baseline is a strong encoder-attention-decoder model based on Luong et al. (2015) and presented by<cite> Chopra et al. (2016)</cite> . As shown in Table 1 , it is reasonably close to some of the state-of-theart (comparable) results in previous work, though making this baseline further strong (e.g., based on pointer-copy mechanism) is our next step. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "19233e4954d7e75ac01112a4c07e64_5",
  "x": "In Table 2 , we again see that et al. (2015) 28.18 8.49 23.81<cite> Chopra et al. (2016)</cite> 28.97 8.26 24.06 Nallapati et al. (2016) our Luong et al. (2015) baseline model achieves competitive performance with previous work, esp. on Rouge-2 and Rouge-L. Next, we show promising multi-task improvements over this baseline of around 0.4% across all metrics, despite being a test-only setting and also with the mismatch between the summarization and entailment domains. Figure 3 shows some additional interesting output examples of our multi-task model and how it generates summaries that are better at being logically entailed by the input document, whereas the baseline model contains some crucial contradictory or unrelated information.",
  "y": "similarities"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_0",
  "x": "**ABSTRACT** In this technical report, we introduce FastFusionNet, an efficient variant of FusionNet <cite>[12]</cite> . FusionNet is a high performing reading comprehension architecture, which was designed primarily for maximum retrieval accuracy with less regard towards computational requirements.",
  "y": "extends differences"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_1",
  "x": "---------------------------------- **** In this technical report, we analyze the inference bottlenecks of FusionNet <cite>[12]</cite> and introduce FastFusionNet that tackles them.",
  "y": "extends differences"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_2",
  "x": "FusionNet <cite>[12]</cite> is reading comprehension model built on top of DrQA by introducing Fully-aware attention layers (context-question attention and context self-attention), contextual embeddings [21] , and more RNN layers. Their proposed fully-aware attention mechanism uses the concatenation of layers of hidden representations as the query and the key to compute attention weights, which shares a similar intuition as DenseNet [11] . FusionNet was the state-of-the-art reading comprehension model at the time of writing (Oct. 4th 2017).",
  "y": "background"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_3",
  "x": "Their proposed fully-aware attention mechanism uses the concatenation of layers of hidden representations as the query and the key to compute attention weights, which shares a similar intuition as DenseNet [11] . FusionNet was the state-of-the-art reading comprehension model at the time of writing (Oct. 4th 2017). Figure 2 provides an analysis of the individual components of FusionNet that the contextual embedding layer, i.e. CoVe [21] , with several layers of wide LSTMs, takes up to 35.5% of the inference time while only contributing a 1.1% improvement of F1 Score (from 82.5% to 83.6%) Huang et al. <cite>[12]</cite> .",
  "y": "background"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_4",
  "x": "**FASTFUSIONNET** Here we introduce FastFusionNet which addresses the inference bottlenecks of FusionNet <cite>[12]</cite> . There are two differences compared to FusionNet: i) the CoVe [21] layers are removed and ii) each BiLSTM layer is replaced with two BiSRU layers.",
  "y": "extends differences"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_5",
  "x": "We closely follow the implementation of Huang et al. <cite>[12]</cite> described in their paper except for the changes above. Following Huang et al. <cite>[12]</cite> , the hidden size of each SRU is set to 125, resulting in a 250-d output feature of each BiSRU regardless of the input size. In the following explanation, we use [A; B] to represent concatenation in the feature dimension.",
  "y": "extends differences"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_6",
  "x": "Following Huang et al. <cite>[12]</cite> , the hidden size of each SRU is set to 125, resulting in a 250-d output feature of each BiSRU regardless of the input size. In the following explanation, we use [A; B] to represent concatenation in the feature dimension. Attn(Q, K, V) represents the attention mechanism taking the query Q, the key K, and the value V as inputs.",
  "y": "similarities uses"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_7",
  "x": "Like others <cite>[12]</cite> we use a randomly initialized the trainable embedding layer with 12 dimensions for POS tags and 8 dimensions for NER. We use question matching features proposed by Chen et al. [2] as well, which contains a hard version and a soft version. The hard version contains 3 binary features indicating where a context word's original form, lower case form, or lemmatized form appears in the question, respectively.",
  "y": "similarities"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_8",
  "x": "where The Question-Context Attention Layer is a fully-aware attention module <cite>[12]</cite> which takes the history (concatenation of GloVe, low-level, and high-level features) of each context word and question words as query and key for three attention modules, and represents each context word as three different vectors: ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_9",
  "x": "Our FastFusionNet reaches F1 75% in 4 epochs and achieves at F1 82.5% at the end which matches the reported F1 82.5% of FusionNet without CoVe on SQuAD development set <cite>[12]</cite> . The training time track aims to minimize the time to train a model up to at least 75% F1 score on SQuAD development set. Table 1 shows that our FastFusionNet reaches F1 75.0% within 20 minutes (after 4 epochs), which gives a 45% speedup compared to the winner DrQA(ParlAI) on the leaderboard.",
  "y": "differences"
 },
 {
  "id": "195f41862b929318787aad9d8e5a1c_10",
  "x": "When controlling the generation of GPUs and comparing our model with a DrQA (ParlAI) trained on an Nvidia V100, our model achieves a 3.1\u00d7 speedup. Compared to FusionNet, FastFusionNet is 23% faster to reach 75% F1 score; however, in terms of the training time per epoch, it is in fact 2.6\u00d7 as fast as FusionNet. <cite>[12]</cite> here since our reimplementation is about 0.5% F1 score worse.",
  "y": "differences"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_0",
  "x": "Our work is similar in spirit to e.g. (Roy, 2002; Skocaj et al., 2011) but advances it in several aspects <cite>(Yu et al., 2016)</cite> . In this demo paper, we present a dialogue agent that learns visually grounded word meanings interactively from a human tutor, which we call: VOILA (Visually Optimised Interactive Learning Agent). Our goal is to enable this agent to learn to identify and describe objects/attributes (colour 1 http://www.furhatrobotics.com/ and shape in this case) in its immediate visual environment through interaction with human users, incrementally, over time.",
  "y": "uses"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_1",
  "x": "What sets VOILA apart from other work in this area is: \u2022 VOILA's dialogue strategy is optimised via Reinforcement Learning to achieve an optimal trade-off between the accuracy of the concepts it learns/has learnt from users, and the effort that the dialogues incur on the users: this is a form of active learning where the agent only asks about something if it doesn't already know the answer with some appropriate confidence (see <cite>(Yu et al., 2016)</cite> for more detail). \u2022 VOILA is trained on a corpus of real HumanHuman conversations (Yu et al., 2017) , and is thus able to process natural human dialogue, which contains phenomena such as self-corrections, repetitions and restarts, pauses, fillers, and continuations VOILA is deployed onto Furhat, a humanlike robot head with a custom back-projected face, built-in stereo microphones, and a Microsoft",
  "y": "background"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_2",
  "x": "In this demonstration, VOILA plays the role of an interactive, concept learning agent that takes initiative in the dialogues and actively learns novel visual knowledge from the feedback from the human tutor. What sets VOILA apart from other work in this area is: \u2022 VOILA's dialogue strategy is optimised via Reinforcement Learning to achieve an optimal trade-off between the accuracy of the concepts it learns/has learnt from users, and the effort that the dialogues incur on the users: this is a form of active learning where the agent only asks about something if it doesn't already know the answer with some appropriate confidence (see <cite>(Yu et al., 2016)</cite> for more detail).",
  "y": "differences"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_3",
  "x": "---------------------------------- **INTERACTIVE MULTIMODAL FRAMEWORK** We developed a multimodal framework in support of building an interactive learning system, which loosely follows that of<cite> Yu et al. (2016)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_4",
  "x": "**WHEN TO LEARN: ADAPTIVE CONFIDENCE THRESHOLD** The first MDP performs a kind of active learning: the learner/agent only acquires the feedback from humans about a visual attribute if it is not confident enough already about its own predictions. Following previous work <cite>(Yu et al., 2016)</cite> , here we use a positive confidence threshold, which determines when the agent believes its own predictions.",
  "y": "uses"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_5",
  "x": "Following previous work <cite>(Yu et al., 2016)</cite> , here we use a positive confidence threshold, which determines when the agent believes its own predictions. For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as<cite> Yu et al. (2016)</cite> point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent. We therefore assign a separate but dependent component MDP for adjusting the threshold dynamically in order to optimise the trade-off between accuracy and cost.",
  "y": "motivation"
 },
 {
  "id": "196e7ca5ccd6754ac986137ec55cd3_6",
  "x": "Following previous work <cite>(Yu et al., 2016)</cite> , here we use a positive confidence threshold, which determines when the agent believes its own predictions. For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as<cite> Yu et al. (2016)</cite> point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent. We therefore assign a separate but dependent component MDP for adjusting the threshold dynamically in order to optimise the trade-off between accuracy and cost.",
  "y": "uses differences extends"
 },
 {
  "id": "1a8c7d22709cae34fbc1eb70fe5189_0",
  "x": "This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b) , which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007) , dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012) , context free grammar (CFG) parsing <cite>(Collins and Roark</cite>, 2004; Zhang and Clark, 2009; Zhu et al., 2013) , combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013) , achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010) , joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012) , joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013) , and joint segmentation, POS-tagging and parsing . In addition to the aforementioned tasks, the framework can be applied to all structural prediction tasks for which the output can be constructed using an incremental process.",
  "y": "uses"
 },
 {
  "id": "1a8c7d22709cae34fbc1eb70fe5189_1",
  "x": "In this tutorial, we make an introduction to the framework, illustrating how it can be applied to a range of NLP problems, giving theoretical discussions and demonstrating a software implementation. We start with a detailed introduction of the framework, describing the averaged perceptron algorithm (Collins, 2002) and its efficient implementation issues (Zhang and Clark, 2007) , as well as beam-search and the early-update strategy <cite>(Collins and Roark, 2004)</cite> . We then illustrate how the framework can be applied to NLP tasks, including word segmentation, joint segmentation & POS-tagging, labeled and unlabeled dependency parsing, joint POS-tagging and dependency parsing, CFG parsing, CCG parsing, and joint segmentation, POS-tagging and parsing.",
  "y": "uses"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_0",
  "x": "<cite>Levy et al. (2017)</cite> present a reformulation of RE, where the task is framed as reading comprehension. In this formulation, each relation type (e.g. author, occupation) is mapped to at least one natural language question template (e.g. \"Who is the author of x?\"), where x is filled with an entity (e.g. \"Inferno\"). The model is then tasked with finding an answer (\"Dante Alighieri\") to this question with respect to a given context. They show that this formulation of the problem both outperforms off-the-shelf RE systems in the typical RE setting and, in addition, enables generalization to unspecified and unseen types of relations.",
  "y": "background motivation"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_1",
  "x": "Open RE methods (Yates et al., 2007; Fader et al., 2011) do not require relationspecific data, but treat different phrasings of the same relation as different relations and rely on a combination of syntactic features (e.g. dependency parses) and normalisation rules, and so have limited generalization capacity. Zero-shot relation extraction <cite>Levy et al. (2017)</cite> propose a novel approach towards achieving this generalization by transforming relations into natural language question templates. For instance, the relation born in(x, y) can be expressed as \"Where was x born?\" or \"In which place was x born?\". Then, a reading comprehension model (Seo et al., 2016; Chen et al., 2017) can be trained on question, answer, and context examples where the x slot is filled with an entity and the y slot is either an answer if the answer is present in the context, or NIL. The model is then able to extract relation instances (given expressions of the relations as questions) from raw text.",
  "y": "background motivation"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_2",
  "x": "2 Slot-filling data To extract the contexts for each triple in our dataset we use the distant supervision method described by <cite>Levy et al. (2017)</cite> . For each Wikidata document belonging to a given entity 1 we take all the denormalized tuples (property, entity 2 ) and extract the first sentence in the text containing both entity 1 and entity 2 . Negatives (contexts without answers) are constructed by finding pairs of triples with common entity 2 type (to ensure they contain good distractors), swapping their context if entity 2 is not present in the context of the other triple.",
  "y": "uses"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_3",
  "x": "Querification <cite>Levy et al. (2017)</cite> created 1192 question templates for 120 Wikidata properties. A template contains a placeholder for an entity x (e.g. for property \"author\", some templates are \"Who wrote the novel x?\" and \"Who is the author of x?\"), which can be automatically filled in to create questions so that question \u2248 template(property, x)). For our multilingual dataset, we had these templates translated by human translators. The translators attempted to translate each of the original 1192 templates. If a template was difficult to translate, they were in- structed to discard it. They were also instructed to create their own templates, paraphrasing the original ones when possible. This resulted in a varying number of templates for each of the properties across languages. In addition to the entity placeholder, some languages with richer morphology (Spanish, Italian, and German) required extra placeholders in the templates because of agreement phenomena (gender). We added a placeholder for definite articles, as well as one for gender-dependent filler words. The gender is automatically inferred from the Wikipedia page statistics and a few heuristics.",
  "y": "extends"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_5",
  "x": "**EXPERIMENTS** Following <cite>Levy et al. (2017)</cite> , we distinguish between the traditional RE setting where the aim is to generalize to unseen entities (UnENT) and the zero-shot setting (UnREL) where the aim is to do so for unseen relation types (see Section 2). Our goal is to answer these three questions: A) how well can RE models be transferred across languages? B) in the difficult UnREL setting, can the variance between languages in the number of instances of relations (see Figure 2 ) be exploited to enable more robust RE ? C) can one jointly-trained multilingual model which performs RE in multiple languages perform comparably to or outperform its individual monolingual counterparts?",
  "y": "motivation similarities"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_6",
  "x": "For all experiments, we take the multiple templates approach where a model sees different paraphrases of the same question during training. This approach was shown by <cite>Levy et al. (2017)</cite> to have significantly better paraphrasing abilities than when only one question template or simpler relation descriptions are employed. Evaluation Our evaluation methodology follows <cite>Levy et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_7",
  "x": "Evaluation Our evaluation methodology follows <cite>Levy et al. (2017)</cite> . We compute precision, recall and F1 by comparing spans predicted by the 3 https://github.com/google-research/ bert/blob/master/multilingual.md models with gold answers. Precision is equal to the true positives divided by total number of nonnil answers predicted by a system. Recall is equal to the true positives divided by the total number of instances that are non-nil in the ground truth answers. Word order and punctuation are not considered.",
  "y": "uses"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_11",
  "x": "All monolingual models' word embeddings were initialised using fastText embeddings trained on each language's Wikipedia and common crawl corpora, 7 except for the comparison experiments described in sub-section 5.1 where GloVe (Pennington et al., 2014) was used for comparability with <cite>Levy et al. (2017)</cite> . ---------------------------------- **RELATED WORK**",
  "y": "background"
 },
 {
  "id": "1ac16c74cc5bb4099ae07f89d7f148_12",
  "x": "Until recently, however, the multilingual aspect of such tasks has not received as much attention. This is pri-Lang. UnENT UnREL <cite>Levy et al. (2017)</cite> Faruqui and Kumar (2015) employed a pipeline of machine translation systems to translate to English, then Open RE systems to perform RE on the translated text, followed by crosslingual projection back to source language.",
  "y": "background"
 },
 {
  "id": "1b424cab4d7008997a31be8c2e5198_0",
  "x": "In order to facilitate the matching, it is often beneficial to explicitly create new features like element-wise absolute difference (|u\u2212v|) and element-wise product (u \u00b7 v) that augment u and v. The combined feature vector is then processed by further layers in the task specific neural network. For example, Tai et al. (2015) use these heuristics to improve semantic representations. Most notably, for the natural language inference task, augmenting the hypothesis (u) and premise (v) representations with |u\u2212v| and u \u00b7 v considerably improves performance in a siamese architecture <cite>Mou et al. (2016)</cite> .",
  "y": "background"
 },
 {
  "id": "1b424cab4d7008997a31be8c2e5198_1",
  "x": "Most notably, for the natural language inference task, augmenting the hypothesis (u) and premise (v) representations with |u\u2212v| and u \u00b7 v considerably improves performance in a siamese architecture <cite>Mou et al. (2016)</cite> . In this paper we focus on polynomial features like u \u00b7 v for the natural language inference task, where it is trying to capture similarity between u and v. It is also a monomial of degree 2.",
  "y": "uses background"
 },
 {
  "id": "1b424cab4d7008997a31be8c2e5198_2",
  "x": "In the first model, both the premise and the hypothesis sentence are encoded using a bidirectional LSTM Hochreiter & Schmidhuber (1997) and the intermediate states are max-pooled to get the respective representations u and v. We refer to this as the InferSent model Conneau et al. (2017) . The standard matching feature of <cite>Mou et al. (2016)</cite> uses a concatenation of u, v, |u\u2212v| and u \u00b7 v. We define the following new matching feature vector that scales the multiplicative term by a constant factor \u03b7 > 0. (1)",
  "y": "uses background"
 },
 {
  "id": "1b424cab4d7008997a31be8c2e5198_3",
  "x": "Choosing \u03b7 = 1 in w poly2 reduces it to the matching feature vector proposed by <cite>Mou et al. (2016)</cite> . The same procedure is repeated for the other baseline model, namely ESIM Chen et al. (2017) . In this case, u represents one of the intermediate states of a bidirectional LSTM encoding of the premise (hypothesis) and v represents the hypothesis (premise) states weighted by relevance to the premise (hypothesis) state.",
  "y": "uses"
 },
 {
  "id": "1bcd442a685e5fb2d0f3f44d3c66c3_0",
  "x": "Several recent works<cite> (Lazaridou, Peysakhovich, and Baroni 2016</cite>; Havrylov and Titov 2017; Lazaridou et al. 2018; <cite>Mordatch and Abbeel 2018)</cite> , have shown that in multi-agent cooperative setting of referential games, deep reinforcement learning can successfully induce communication protocols. In these games, communication success is the only supervision during learning, and the meaning of the emergent messages gets grounded during the game. In<cite> (Lazaridou, Peysakhovich, and Baroni 2016)</cite> , the authors have restricted the message to be a single symbol token picked from a fixed vocabulary while in (Havrylov and Titov 2017) , the message is considered to be a sequence of symbols.",
  "y": "background motivation"
 },
 {
  "id": "1bcd442a685e5fb2d0f3f44d3c66c3_1",
  "x": "In<cite> (Lazaridou, Peysakhovich, and Baroni 2016)</cite> , the authors have restricted the message to be a single symbol token picked from a fixed vocabulary while in (Havrylov and Titov 2017) , the message is considered to be a sequence of symbols. (Lazaridou et al. 2018) demonstrates that successful communication can also emerge in environments which present raw pixel input. Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org).",
  "y": "background"
 },
 {
  "id": "1bcd442a685e5fb2d0f3f44d3c66c3_2",
  "x": "All rights reserved. (<cite>Mordatch and Abbeel 2018)</cite> further extends the scope of mode of communication by also studying the emergence of non-verbal communication. While these works have studied a wide variety of game setups as well as variations in communication rules, none of them have considered written language system as a mode of communication.",
  "y": "background"
 },
 {
  "id": "1bcd442a685e5fb2d0f3f44d3c66c3_3",
  "x": "**REFERENTIAL GAME FRAMEWORK** In our work, we have used two referential game setups that are slight modifications to the ones used in<cite> (Lazaridou, Peysakhovich, and Baroni 2016</cite>; Lazaridou et al. 2018 ). There are two players, a sender and a receiver.",
  "y": "extends differences"
 },
 {
  "id": "1bcd442a685e5fb2d0f3f44d3c66c3_4",
  "x": "The payoff is 1 for both the agents iff R \u03c6 (f s (S \u03b8 (R(M i ), h i , V )), U ) = t , where i is the last timestep of the episode. In all other cases and intermediate timesteps, the payoff is 0. Because of the high dimensional search space introduced due to brushstrokes, we use Proximal Policy Optimization (PPO)<cite> (Schulman et al. 2017)</cite> for optimizing the weights of sender and receiver agents. ----------------------------------",
  "y": "background"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_0",
  "x": "**ABSTRACT** Hate speech in the form of racism and sexism is commonplace on the internet <cite>(Waseem and Hovy, 2016)</cite> . For this reason, there has been both an academic and an industry interest in detection of hate speech.",
  "y": "background"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_1",
  "x": "We provide an evaluation on our own data set and run our models on <cite>the data set</cite> released by <cite>Waseem and Hovy (2016)</cite>. We find that amateur annotators are more likely than expert annotators to label items as hate speech, and that systems trained on expert annotations outperform systems trained on amateur annotations. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_2",
  "x": "In addition, it is important to understand how different manners of obtaining labeling can influence the classification models and how it is possible to obtain good annotations, while ensuring that annotators are not likely to experience adverse effects of annotating hate speech. Our contribution We provide annotations of 6, 909 tweets for hate speech by annotators from CrowdFlower and annotators that have a theoretical and applied knowledge of hate speech, henceforth amateur and expert annotators 1 . Our data set extends the <cite>Waseem and Hovy (2016)</cite> data set by 4, 033 tweets.",
  "y": "extends"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_3",
  "x": "Our data set is obtained by sampling tweets from the 130k tweets extracted by <cite>Waseem and Hovy (2016)</cite> . The order of the tweets is selected by our database connection, thus allowing for an overlap with the data set released by <cite>Waseem and Hovy (2016)</cite> . We find that there is an overlap of 2, 876 tweets (see Table 1) between the two data sets.",
  "y": "uses"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_4",
  "x": "Our data set is obtained by sampling tweets from the 130k tweets extracted by <cite>Waseem and Hovy (2016)</cite> . The order of the tweets is selected by our database connection, thus allowing for an overlap with the data set released by <cite>Waseem and Hovy (2016)</cite> . We find that there is an overlap of 2, 876 tweets (see Table 1) between the two data sets.",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_5",
  "x": "Racism Sexism Neither Count 1 95 2780 Given the distribution of the labels in <cite>Waseem and Hovy (2016)</cite> and our annotated data set (see Table  2 ), it is to be expected the largest overlap occurs with tweets annotated as negative for hate speech. Observing Table 2 , we see that the label distribution in our data set generally differs from the distribution in <cite>Waseem and Hovy (2016)</cite> . In fact, we see that the amateur majority voted labels is the only distribution that tends towards a label distribution similar to <cite>Waseem and Hovy (2016)</cite> Our annotation effort deviates from <cite>Waseem and Hovy (2016)</cite> .",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_6",
  "x": "We find that there is an overlap of 2, 876 tweets (see Table 1) between the two data sets. Racism Sexism Neither Count 1 95 2780 Given the distribution of the labels in <cite>Waseem and Hovy (2016)</cite> and our annotated data set (see Table  2 ), it is to be expected the largest overlap occurs with tweets annotated as negative for hate speech. Observing Table 2 , we see that the label distribution in our data set generally differs from the distribution in <cite>Waseem and Hovy (2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_7",
  "x": "Racism Sexism Neither Count 1 95 2780 Given the distribution of the labels in <cite>Waseem and Hovy (2016)</cite> and our annotated data set (see Table  2 ), it is to be expected the largest overlap occurs with tweets annotated as negative for hate speech. Observing Table 2 , we see that the label distribution in our data set generally differs from the distribution in <cite>Waseem and Hovy (2016)</cite> . In fact, we see that the amateur majority voted labels is the only distribution that tends towards a label distribution similar to <cite>Waseem and Hovy (2016)</cite> Our annotation effort deviates from <cite>Waseem and Hovy (2016)</cite> .",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_8",
  "x": "We present the annotators with <cite>the tests</cite> from <cite>Waseem and Hovy (2016)</cite> . If a tweet fails any of <cite>the tests</cite>, the annotators are instructed to label it as the relevant form of hate speech. Expert annotators are given the choice of skipping tweets, if they are not confident in which label to assign, and a \"Noise\" label in case the annotators are presented with non-English tweets.",
  "y": "uses"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_9",
  "x": "Table 2 suggests that if only cases of full agreement are considered, it is possible to obtain good annotations using crowdsourcing. Overlap Considering the overlap with the <cite>Waseem and Hovy (2016)</cite>, we see that the agreement is extremely low (mean pairwise \u03ba = 0.14 between all annotator groups and <cite>Waseem and Hovy (2016)</cite> ). Interestingly, we see that the vast majority of disagreements between our annotators and <cite>Waseem and Hovy (2016)</cite> , are disagreements where our annotators do not find hate speech but <cite>Waseem and Hovy (2016)</cite> the influence of the features listed in Table 4 for each annotator group.",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_10",
  "x": "Interestingly, we see that the vast majority of disagreements between our annotators and <cite>Waseem and Hovy (2016)</cite> , are disagreements where our annotators do not find hate speech but <cite>Waseem and Hovy (2016)</cite> the influence of the features listed in Table 4 for each annotator group. Model Selection We perform a grid search over all possible feature combinations to find the best performing features. We find that the features with the highest performance are not necessarily the features with the best performance.",
  "y": "differences"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_11",
  "x": "Interestingly, this feature performs worse than any other feature. Particularly when trained on expert annotations, suggesting that hate speech may be more situational or that users engaging in hate speech, do not only, or even primarily engage in hate speech. Gender Following the indication that gender can positively influence classification scores <cite>(Waseem and Hovy, 2016)</cite> , we compute the gender of the users in our data set.",
  "y": "uses"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_12",
  "x": "Gender Following the indication that gender can positively influence classification scores <cite>(Waseem and Hovy, 2016)</cite> , we compute the gender of the users in our data set. To counteract the low coverage in <cite>Waseem and Hovy (2016)</cite> , we use a lexicon trained on Twitter (Sap et al., 2014) to calculate the probability of gender. Using these probabilities we assign binary gender.",
  "y": "extends"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_13",
  "x": "Results Running our system on the <cite>Waseem and Hovy (2016)</cite> data set, we find that our best performing system does not substantially outperform on the binary classification task <cite>Waseem and Hovy (2016</cite> Interestingly, the main cause of error is false positives. This holds true using both amateur and expert annotations. We mitigate personal bias in our annotations, as multiple people have participated in the annotation process.",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_14",
  "x": "We mitigate personal bias in our annotations, as multiple people have participated in the annotation process. <cite>Waseem and Hovy (2016)</cite> may suffer from personal bias, as the only the authors annotated, and only the annotations positive for hate speech were reviewed by one other person. It is our contention that hate speech corpora should reflect real life, in that hate speech is a rare occurrence comparatively.",
  "y": "differences"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_15",
  "x": "Nobata et al. (2016) address this, by using comprehensive lists of slurs obtained from Hatebase 4 . <cite>Waseem and Hovy (2016)</cite> and Ross et al. (2016) focus on building corpora which <cite>they annotate</cite> for containing hate speech. Our work closely resembles <cite>Waseem and Hovy (2016)</cite> , as <cite>they also run</cite> classification experiments on a hate speech data set.",
  "y": "background"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_16",
  "x": "Our work closely resembles <cite>Waseem and Hovy (2016)</cite> , as <cite>they also run</cite> classification experiments on a hate speech data set. <cite>Waseem and Hovy (2016)</cite> obtain an F1-score of 73.91 on <cite>their data set</cite>, using character n-grams and gender information. Nobata et al. (2016) employ a wide array of features for abusive language detection, including but not limited to POS tags, the number of blacklisted words in a document, n-gram features including token and character n-grams and length features.",
  "y": "similarities"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_17",
  "x": "Our work closely resembles <cite>Waseem and Hovy (2016)</cite> , as <cite>they also run</cite> classification experiments on a hate speech data set. <cite>Waseem and Hovy (2016)</cite> obtain an F1-score of 73.91 on <cite>their data set</cite>, using character n-grams and gender information. Nobata et al. (2016) employ a wide array of features for abusive language detection, including but not limited to POS tags, the number of blacklisted words in a document, n-gram features including token and character n-grams and length features.",
  "y": "background"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_18",
  "x": "We find that using expert annotations can produce models that perform comparably to previous classification efforts. Our best model is on par with previous work on the <cite>Waseem and Hovy (2016)</cite> data set for the binary classification task but under-performs for the multi-class classification task. We suggest that a weighted F1-score be applied in evaluation of classification efforts on hate speech corpora, such that misclassification on minority classes is penalized.",
  "y": "similarities differences"
 },
 {
  "id": "1c51e45e2917268e0ab5ce43a69655_19",
  "x": [
   "This can allow for a significant decrease in the annotations burden of expert annotators by asking them to primarily consider the cases in which amateur annotators have disagreed. Future Work We will seek to further investigate the socio-linguistic features such as gender and location. Furthermore, we will expand to more forms of hate speech."
  ],
  "y": "motivation"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_0",
  "x": "Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field<cite> (Sproat and Jaitly, 2016)</cite> . In this paper, we present our approach to nonstandard text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively. ----------------------------------",
  "y": "background"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_1",
  "x": "Constructing such grammars is time consuming and error-prone and requires extensive linguistic knowledge and programming proficiency. Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field<cite> (Sproat and Jaitly, 2016)</cite> . In this paper, we present our approach to nonstandard text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively.",
  "y": "motivation"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_2",
  "x": "---------------------------------- **DATA-DRIVEN APPROACHES** Recently, methods based on neural networks have been applied to TN and ITN<cite> (Sproat and Jaitly, 2016</cite>; Pusateri et al., 2017; Yolchuyeva et al., 2018) .",
  "y": "background"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_3",
  "x": "Researchers hope a vast amount of such data can counteract the errors inherited in WFST-based models. Recent data-driven approaches examine window-based sequence-to-sequence (seq2seq) models and convolutional neural networks (CNN) to normalize a central piece of text with the help of context<cite> (Sproat and Jaitly, 2016</cite>; Yolchuyeva et al., 2018) . Window-based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special <self> token.",
  "y": "background"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_4",
  "x": "---------------------------------- **BASELINE MODELS** Following <cite>Sproat and Jaitly (2016)</cite>, we implement a seq2seq model trained on window-based data.",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_5",
  "x": "Table 1 illustrates the window-based model's training examples corresponding to one sentence \"wake me up at 8 AM .\" which is broken down into 6 pairs. <n> and </n> indicate the center of the window. A window center might contain 1 or more words (e.g., \"8 AM\") and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as TIME, DATE, ORDINAL<cite> (Sproat and Jaitly, 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_6",
  "x": "---------------------------------- **DATASET** The data for the window-based seq2seq model and full sentence seq2seq were generated from the publicly available release of parallel written/speech formatted text from <cite>Sproat and Jaitly (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_7",
  "x": "Our datasets were randomly sampled from a set of 4.9M sentences in the training data portion of the <cite>Sproat and Jaitly (2016)</cite> data release and split into training, validation, and test data. However, the training data for window-based and sentencebased models are not identical due to differences in input configurations. While the window-based model uses 500K randomly sampled windows, the sentence-based models use 500K sentences.",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_8",
  "x": "There are large numbers of <self> tokens present in the dataset. We follow <cite>Sproat and Jaitly (2016)</cite> in down-sampling window-based training data to constrain the proportion of \"<self>\" tokens to 10% of the data. For training sentence-based models, the source sentence is segmented into characters while the target sentence is broken into tokens.",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_9",
  "x": "---------------------------------- **BASELINE MODEL SETUP** Our first approach replicates the window-based seq2seq model of <cite>Sproat and Jaitly (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_10",
  "x": "* TELEPHONE is not reported in <cite>Sproat and Jaitly (2016)</cite> but included in the dataset; ** we removed ELECTRONIC category. As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with <cite>Sproat and Jaitly (2016)</cite> , considering our training set is much smaller. There are 16 different edit labels shown.",
  "y": "differences"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_11",
  "x": "As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with <cite>Sproat and Jaitly (2016)</cite> , considering our training set is much smaller. There are 16 different edit labels shown. Data with TELEPHONE labels were not included in the initial analysis of <cite>Sproat and Jaitly (2016)</cite> , but were made available in the dataset release.",
  "y": "similarities"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_12",
  "x": "There are 16 different edit labels shown. Data with TELEPHONE labels were not included in the initial analysis of <cite>Sproat and Jaitly (2016)</cite> , but were made available in the dataset release. For our second baseline model which operates on whole sentences, on the input side, we still use 250 common characters.",
  "y": "differences"
 },
 {
  "id": "1c89c8f4849d1c8214a3e5f6b9ff1a_13",
  "x": "upper, lower, mixed, nonalphanumerical, foreign characters; 2) position: (Bird et al., 2009) . Edit labels are the most expensive to obtain in real life. Our labels are generated directly from the Google FST<cite> (Sproat and Jaitly, 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "1e232f9dfa7d499d1ba39fcebf3d1a_0",
  "x": "**INTRODUCTION** Systems, such as treebank-based parsers<cite> (Charniak, 2001</cite>; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008) , are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data.",
  "y": "background"
 },
 {
  "id": "1e232f9dfa7d499d1ba39fcebf3d1a_1",
  "x": "For example, users of the Charniak parser<cite> (Charniak, 2001)</cite> should add the AUX category to the PTB parts of speech and adjust their systems to account for the conversion of the word ain't into the tokens IS and n't. Similarly, tokenization decisions with respect to hyphens vary among different versions of the Penn Treebank, as well as different parsers based on these treebanks. Thus if a system uses multiple parsers, such differences must be accounted for.",
  "y": "background"
 },
 {
  "id": "1e232f9dfa7d499d1ba39fcebf3d1a_2",
  "x": "**RUNNING THE GLARFER PROGRAMS** We use Charniak, UMD and KNP parsers<cite> (Charniak, 2001</cite>; Huang and Harper, 2009; Kurohashi and Nagao, 1998) , JET Named Entity tagger (Grishman et al., 2005; Ji and Grishman, 2006) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper. English GLARFer rules use Comlex (Macleod et al., 1998a) and the various NomBank lexicons (http:// nlp.cs.nyu.edu/meyers/nombank/) for lexical lookup.",
  "y": "uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_0",
  "x": "This paper attempts to marry the interpretability of statistical machine learning approaches with the more robust models of joke structure and joke semantics capable of being learned by neural models. Specifically, we explore the use of semantic relatedness features based on word associations, rather than the more common Word2Vec similarity, on a binary humour identification task and identify several factors that make word associations a better fit for humour. We also explore the effects of using joke structure, in the form of humour anchors<cite> (Yang et al., 2015)</cite> , for improving the performance of semantic features and show that, while an intriguing idea, humour anchors contain several pitfalls that can hurt performance.",
  "y": "uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_1",
  "x": "Works which take a more interpretable statistical machine learning approach have their own drawbacks. For example, the representation of joke semantics has been fairly basic, typically computing word embedding similarities between all word pairs in a document<cite> (Yang et al., 2015)</cite> , and bear little resemblance to the way humans actually interpret humour. Additionally, many works fail to take advantage of joke structure, treating texts as unordered bags-of-words (Bertero and Fung, 2016b; Mihalcea and Strapparava, 2005; Yan and Pedersen, 2017) .",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_2",
  "x": "For example, the representation of joke semantics has been fairly basic, typically computing word embedding similarities between all word pairs in a document<cite> (Yang et al., 2015)</cite> , and bear little resemblance to the way humans actually interpret humour. This paper aims to marry the interpretability of statistical machine learning approaches with the more nuanced models of joke structure and joke semantics of neural approaches.",
  "y": "uses background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_3",
  "x": "While such features have been explored in the past (Cattle and Ma, 2016; Cattle and Ma, 2017b) , this work presents a more in depth analysis, focusing on a more fundamental task (binary humour classification vs. relative humour ranking) on with a dataset that better represents natural language (oneliners and puns vs. Twitter hashtag games), and is the first work to incorporate interpolated word association strengths. Furthermore, we introduce a novel method for targetting our semantic features using joke structure to help reduce noise and increase reliability. Specifically, we experiment with integrating the extraction of humour anchors, the \"meaningful, complete, minimal set of word spans\"<cite> (Yang et al., 2015)</cite> that allow humour to occur, into the humour classification process itself, the first work to do so.",
  "y": "uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_4",
  "x": "Similarly, punchlines that are not sufficiently incongruent with their setup are unfunny as there is no re-evaluation. As overlap and incongruity are difficult to measure directly, one common approach is instead to use word embeddings, such as Word2Vec (Mikolov et al., 2013) , to calculate the cosine similarities between pairs of vectors representing words in a document <cite>(Yang et al., 2015</cite>; Shahaf et al., 2015; Kukova\u010dec et al., 2017) . However, measuring incongruity and overlap in terms of similarity is a rather odd choice. Just because two scripts overlap does not imply they are similar.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_5",
  "x": "Despite the issues mentioned in Section 2.1,<cite> Yang et al. (2015)</cite> 's \"incongruity\" feature set, maximum and minimum word embedding similarities between pairs of words in a document, perform fairly well. Cattle and Ma (2017b) takes a similar approach with their word association features. The problems comes from the fact that both works compute these values across all possible pairs of words in a document.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_6",
  "x": "Yang et al. (2015) , in addition to their humour classifier, also introduces a method for identifying jokes' humour anchors (HAs), the \"meaningful, complete, minimal set of word spans\" that allow humour to occur. While this is slightly different from identifying a joke's setup and punchline, focusing only on pairs of HAs would help reducing noise by increasing the precision of meaningful word pairs selection without sacrificing recall. However,<cite> Yang et al. (2015)</cite> does not use their extracted HAs to improve their humour classification performance.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_7",
  "x": "It should be noted that sequence-based humour models such as Bertero and Fung (2016b) or Donahue et al. (2017) should theoretically be capable of implicitly learning HAs, especially Bertero and Fung (2016a)'s Long Short-Term Memory-based approach. However, these models are much more complex than<cite> Yang et al. (2015)</cite> 's approach, require more training data, and suffer from a lack of interpretability. ----------------------------------",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_8",
  "x": "**DATASETS** We evaluate our classifiers across two separate datasets: Pun of the Day (PotD), collected in<cite> Yang et al. (2015)</cite> , and 16000 One-Liner (OL), collected in Mihalcea and Strapparava (2005) . PotD consists of positive examples collected from the Pun of the Day website 2 and negative examples collected from a combination of news sources, question/answer forums, and lists of proverbs<cite> (Yang et al., 2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_9",
  "x": "PotD consists of positive examples collected from the Pun of the Day website 2 and negative examples collected from a combination of news sources, question/answer forums, and lists of proverbs<cite> (Yang et al., 2015)</cite> . OL consists of positive examples scraped from humour websites and negative examples taken from a combination of new headlines, sentences from the British National Corpus, and proverbs. ----------------------------------",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_10",
  "x": "We evaluate our classifiers across two separate datasets: Pun of the Day (PotD), collected in<cite> Yang et al. (2015)</cite> , and 16000 One-Liner (OL), collected in Mihalcea and Strapparava (2005) . PotD consists of positive examples collected from the Pun of the Day website 2 and negative examples collected from a combination of news sources, question/answer forums, and lists of proverbs<cite> (Yang et al., 2015)</cite> . OL consists of positive examples scraped from humour websites and negative examples taken from a combination of new headlines, sentences from the British National Corpus, and proverbs.",
  "y": "uses background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_11",
  "x": "---------------------------------- **BASELINE** For our baseline we implemented our own version of<cite> Yang et al. (2015)</cite> 's highest performing classifier.",
  "y": "extends"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_12",
  "x": "---------------------------------- **SEMANTIC FEATURES** Similar to<cite> Yang et al. (2015)</cite> , we compute the minimum, maximum, and average Word2Vec similarity between ordered word pairs.",
  "y": "similarities"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_13",
  "x": "Similar to<cite> Yang et al. (2015)</cite> , we compute the minimum, maximum, and average Word2Vec similarity between ordered word pairs. Not only is this a common humour recognition feature <cite>(Yang et al., 2015</cite>; Shahaf et al., 2015; Kukova\u010dec et al., 2017) , but it also acts as a point of comparison for word association strength. For our word association features we compute the minimum, maximum, and average association strength between ordered word pairs, which we refer to this as the forward strength.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_14",
  "x": "**SEMANTIC FEATURES** Similar to<cite> Yang et al. (2015)</cite> , we compute the minimum, maximum, and average Word2Vec similarity between ordered word pairs. Not only is this a common humour recognition feature <cite>(Yang et al., 2015</cite>; Shahaf et al., 2015; Kukova\u010dec et al., 2017) , but it also acts as a point of comparison for word association strength.",
  "y": "similarities background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_15",
  "x": "HAs are extracted using the method described in<cite> Yang et al. (2015)</cite> using the same baseline humour model described in Section 3.2 for anchor candidate evaluation. HA extraction's requirement of a fully trained anchor candidate evaluator raises the problem of what data that evaluator should be trained on. Given that, as described in Section 3.1, we experiment on two separate humour datasets, we train the anchor candidate evaluator on the opposite dataset from the overall humour classifier.",
  "y": "uses"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_16",
  "x": "**RESULTS AND DISCUSSION** The results of our experiments are reported in Table 1 . In general, our model performs slightly worse than the<cite> Yang et al. (2015)</cite> baseline.",
  "y": "differences"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_17",
  "x": "In general, our model performs slightly worse than the<cite> Yang et al. (2015)</cite> baseline. One interesting aspect to note is that our model uses only 28 feature dimensions compared to<cite> Yang et al. (2015)</cite> 's 318. While this is not exactly a fair comparison in the case of our ML-based word association strengths (our ML strength predictor takes 415 feature dimensions as input), graph-based associations perform similarly and do truly use only 28 dimensions.",
  "y": "differences"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_18",
  "x": "The wonderfully simple extraction method described in<cite> Yang et al. (2015)</cite> only makes HAs more intriguing. Unfortunately, as can be seen in Table 1 , HA targetting actually hurts the performance of our humour model. One obvious suspect for this drop in performance is the quality of the extracted HAs, a sample of which is shown in extracted anchors were either incomplete, as is the case with Dark and Santa, or nonsensical, like profectionist.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_19",
  "x": "Unfortunately, as can be seen in Table 1 , HA targetting actually hurts the performance of our humour model. One obvious suspect for this drop in performance is the quality of the extracted HAs, a sample of which is shown in extracted anchors were either incomplete, as is the case with Dark and Santa, or nonsensical, like profectionist. As described in Section 2.2,<cite> Yang et al. (2015)</cite> 's HA extraction algorithm requires a fully trained humour model, the accuracy of which undoubtedly affects the quality of the extracted HAs.",
  "y": "background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_20",
  "x": "As described in Section 2.2,<cite> Yang et al. (2015)</cite> 's HA extraction algorithm requires a fully trained humour model, the accuracy of which undoubtedly affects the quality of the extracted HAs. For this reason we also experimented with training our anchor candidate scorer using the test data, to maximize its performance. While this approach is problematic, it does provide an upper bound for our HA extraction performance.",
  "y": "similarities background"
 },
 {
  "id": "1f5326cacca33bfc80a9ddcb4ae313_21",
  "x": "While this approach is problematic, it does provide an upper bound for our HA extraction performance. While using such HA targetted models did result in increased humour classification performance (ACC = 0.740, P = 0.735, R = 0.754, F 1 = 0.744 on PotD. ACC = 0.706, P = 0.678, R = 0.783, F 1 = 0.727 on OL.), it still failed to exceed our non-HA models. We chose our baseline<cite> Yang et al. (2015)</cite> humour classifier as our anchor candidate scorer for simplicity but their HA extraction algorithm is able to work with any humour recognition model so long as it is robust to word order and capable of generating a humour score (in our case, we used humour probability).",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_0",
  "x": "State-of-the-art NLG models are built using recurrent neural network (RNN) based sequence to sequence models<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> . Convolutional sequence to sequence based models have been used in the domain of machine translation but their application as natural language generators in dialogue systems is still unexplored. In this work, we propose a novel approach to NLG using convolutional neural network (CNN) based sequence to sequence learning.",
  "y": "background"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_1",
  "x": "Entrainment to users way of speaking is essential for generating more natural and high quality natural language responses. Most of the approaches for incorporating entrainment are rule-based models. Recent advances have been in the direction of developing a fully trainable context aware NLG model<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> .",
  "y": "background"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_2",
  "x": "These binary vectors are used to penalize the responses having missing and/or irrelevant information. We evaluate our model on the Alex Context natural language generation (NLG) dataset of <cite>Du\u0161ek and Jurcicek (2016a)</cite> and demonstrate that our model outperforms the RNNbased model of <cite>Du\u0161ek and Jurcicek (2016a)</cite> (TGen model) in automatic metrics. Training time of proposed model is observed to be significantly lower than TGen model.",
  "y": "differences uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_3",
  "x": "The semantic meaning which is required to be given in response to a query is very well modelled if context awareness is taken into account. This leads to generation of more informative response. Model proposed by <cite>Du\u0161ek and Jurcicek (2016a)</cite> serves as a baseline sequence to sequence generation model (TGen model) for SDS which takes into account the context.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_4",
  "x": "The model implemented three modifications to the model proposed by Du\u0161ek and Jurcicek (2016b) . The third modification was implementing a N-gram match reranker. This reranker is based on n-gram precision scores and promotes responses having phrase overlaps with user utterances<cite> (Du\u0161ek and Jurcicek, 2016a</cite> ).",
  "y": "background"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_5",
  "x": "This reranker is based on n-gram precision scores and promotes responses having phrase overlaps with user utterances<cite> (Du\u0161ek and Jurcicek, 2016a</cite> ). In the next section, we present the proposed CNN-based sequence to sequence generator for NLG. ----------------------------------",
  "y": "uses background"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_6",
  "x": "We implement the N-gram match reranker as given by <cite>Du\u0161ek and Jurcicek (2016a)</cite> . We describe the proposed convolutional sequence to sequence generator in Section 3.1 and convolutional reranker in Section 3.2. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_7",
  "x": "Each element of binary vector is a binary decision on the presence of DA type or slot-value combinations. For the dataset which we have used<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> , there are 19 such classes of DA types and slot-value combinations. These 19 classes are shown in Figure 3 .",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_8",
  "x": "Hamming distance between the classifier output and binary vector representation of input DA is considered as reranking penalty. The weighted reranking penalties of all the n-best responses are subtracted from their log-probabilities similar to <cite>Du\u0161ek and Jurcicek (2016a)</cite> . The architecture and working of the CNN reranker on an input instance from training dataset is shown in Figure 4 .",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_9",
  "x": "---------------------------------- **EXPERIMENTAL STUDIES** The studies in this work are performed on Alex Context natural language generation (NLG) dataset<cite> (Du\u0161ek and Jurcicek, 2016a</cite> ).",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_10",
  "x": "Each data instance consists of a preceding context (user utterance), source meaning representation and target natural language responses/sentences. Data is delexicalized and split into training, validation and test sets as done by <cite>Du\u0161ek and Jurcicek (2016a)</cite> . For training and validation, the three paraphrases are used as separate instances.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_11",
  "x": "The performance of the proposed ConvSeq2Seq model for NLG is compared with that of TGen model<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> . For comparison, we have considered NIST (Doddington, 2002) , BLEU (Papineni et al., 2002) , METEOR (Denkowski and Lavie, 2014) , ROUGE L (Lin, 2004) and CIDEr metrics (Vedantam et al., 2015) . For this study, we have considered script \"mtevalv13a-sig.pl\" (version 13a) that implements these metrics.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_12",
  "x": "Our model has also been evaluated using the metric script \"mtevalv11b.pl\" (version 11b) to compare our results with those stated in<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> . The 13a version takes into account the closest reference length with respect to candidate length for calculation of brevity penalty. This is in accordance with IBM BLEU.",
  "y": "uses"
 },
 {
  "id": "1fd85a350d9ec7ac12151cfe4412e4_13",
  "x": "BLEU and NIST scores of the TGen model given in Table 2 match with that represented in<cite> (Du\u0161ek and Jurcicek, 2016a)</cite> . The scores of our model shows slight improvement over TGen model. The studies done to compare the proposed model with the TGen model, show the effectiveness of considering the CNN-based approach to NLG.",
  "y": "similarities"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_0",
  "x": "**INTRODUCTION** Transformer (Vaswani et al., 2017 ) is a relative new architecture which outperforms traditional deep learning models such as Recurrent Neural Networks (RNNs) (Sutskever et al., 2014) and Temporal Convolutional Networks (TCNs) (Bai et al., 2018) for sequence modeling tasks across neural machine translations (Vaswani et al., 2017) , language understanding (Devlin et al., 2018) , sequence prediction<cite> (Dai et al., 2019)</cite> , image generation (Child et al., 2019) , video activity classification (Wang et al., 2018) , music generation (Huang et al., 2018a) , and multimodal sentiment analysis (Tsai et al., 2019a) . Instead of performing recurrence (e.g., RNN) or convolution (e.g., TCN) over the sequences, Transformer is a feed-forward model that concurrently processes the entire sequence.",
  "y": "background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_1",
  "x": "It should be noted that some applications has only the decoder self-attention such as sequence prediction<cite> (Dai et al., 2019)</cite> . In all cases, the Transformer's attentions follow the same general mechanism. At the high level, the attention can be seen as a weighted combination of the input sequence, where the weights are determined by the similarities between elements of the input sequence.",
  "y": "background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_2",
  "x": "We note that this operation is orderagnostic to the permutation in the input se-quence (order is encoded with extra positional embedding (Vaswani et al., 2017; Shaw et al., 2018;<cite> Dai et al., 2019)</cite> ). The above observation inspires us to connect Transformer's attention to kernel learning (Scholkopf and Smola, 2001) : they both concurrently and order-agnostically process all inputs by calculating the similarity between the inputs. Therefore, in the paper, we present a new formulation for Transformer's attention via the lens of kernel.",
  "y": "background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_3",
  "x": "We note that this operation is orderagnostic to the permutation in the input se-quence (order is encoded with extra positional embedding (Vaswani et al., 2017; Shaw et al., 2018;<cite> Dai et al., 2019)</cite> ). The above observation inspires us to connect Transformer's attention to kernel learning (Scholkopf and Smola, 2001) : they both concurrently and order-agnostically process all inputs by calculating the similarity between the inputs. Therefore, in the paper, we present a new formulation for Transformer's attention via the lens of kernel.",
  "y": "background motivation"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_4",
  "x": "The main advantage of connecting attention to kernel is that it opens up a new family of attention mechanisms that can relate to the well-established literature in kernel learning (Scholkopf and Smola, 2001) . As a result, we develop a new variant of attention which simply considers a product of symmetric kernels when modeling non-positional and positional embedding. Furthermore, our proposed formulation highlights naturally the main components of Transformer's attention, enabling a better understanding of this mechanism: recent variants of Transformers (Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019; Wang et al., 2018; Tsai et al., 2019a) can be expressed through these individual components.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_5",
  "x": "The inspiration for connecting the kernel (Scholkopf and Smola, 2001 ) and attention instantiates from the observation: both operations concurrently processes all inputs and calculate the similarity between the inputs. We first introduce the background (i.e., the original formulation) of attention and then provide a new reformulation within the class of kernel smoothers (Wasserman, 2006) . Next, we show that this new formulation allows us to explore new family of attention while at the same time offering a framework to categorize previous attention variants (Vaswani et al., 2017; Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019; Wang et al., 2018; Tsai et al., 2019a) .",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_6",
  "x": "Formally, a sequence x = [x 1 , x 2 , \u22ef, x T ] defines each element as x i = (f i , t i ) with f i \u2208 F being the nontemporal feature at time i and t i \u2208 T as an temporal feature (or we called it positional embedding). Note that f i can be the word representation (in neural machine translation (Vaswani et al., 2017) ), a pixel in a frame (in video activity recognition (Wang et al., 2018) ), or a music unit (in music generation (Huang et al., 2018b) ). t i can be a mixture of sine and cosine functions (Vaswani et al., 2017) or parameters that can be learned during back-propagation<cite> (Dai et al., 2019</cite>; Ott et al., 2019) .",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_7",
  "x": "On the contrary, encoder selfattention and decoder-encoder attention consider no additional mask to Eq. (1). Recent work (Shaw et al., 2018; <cite>Dai et al., 2019</cite>; Huang et al., 2018b; Child et al., 2019; Parmar et al., 2018; Tsai et al., 2019a) proposed modifications to the Transformer for the purpose of better modeling inputs positional relation (Shaw et al., 2018; Huang et al., 2018b;<cite> Dai et al., 2019)</cite> , appending additional keys in S x k<cite> (Dai et al., 2019)</cite> , modifying the mask applied to Eq. (1) (Child et al., 2019) , or applying to distinct feature types Parmar et al., 2018; Tsai et al., 2019a) . These works adopt different designs of attention as comparing to the original form (Eq. (1)).",
  "y": "background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_8",
  "x": "Recent work (Shaw et al., 2018; <cite>Dai et al., 2019</cite>; Huang et al., 2018b; Child et al., 2019; Parmar et al., 2018; Tsai et al., 2019a) proposed modifications to the Transformer for the purpose of better modeling inputs positional relation (Shaw et al., 2018; Huang et al., 2018b;<cite> Dai et al., 2019)</cite> , appending additional keys in S x k<cite> (Dai et al., 2019)</cite> , modifying the mask applied to Eq. (1) (Child et al., 2019) , or applying to distinct feature types Parmar et al., 2018; Tsai et al., 2019a) . These works adopt different designs of attention as comparing to the original form (Eq. (1)). In our paper, we aim at providing an unified view via the lens of kernel.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_9",
  "x": "In the prior work (Vaswani et al., 2017) Note that the kernel form k(x q , x k ) in the original Transformer (Vaswani et al., 2017 ) is a asymmetric exponential kernel with additional mapping W q and W k (Wilson et al., 2016; Li et al., 2017) 2 . The new formulation defines a larger space for composing attention by manipulating its individual components, and at the same time it is able to categorize different variants of attention in prior work (Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019; Wang et al., 2018; Tsai et al., 2019a) .",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_10",
  "x": "In addition to modeling sequences like word sentences (Vaswani et al., 2017) or music signals (Huang et al., 2018b) , the Transformer can also be applied to images (Parmar et al., 2018) , sets , and multimodal sequences (Tsai et al., 2019a) . Due to distinct data types, these applications admit various kernel feature space: (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> : with F being non-positional feature space and T being the positional embedding space of the position in the sequence.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_11",
  "x": "Positional Embedding k(\u22c5, \u22c5) The kernel construction on X = (F \u00d7 T ) has distinct design in variants of Transformers (Vaswani et al., 2017; <cite>Dai et al., 2019</cite>; Huang et al., 2018b; Shaw et al., 2018; Child et al., 2019) . Since now the kernel feature space considers a joint space, we will first discuss the kernel construction on F (the non-positional feature space) and then discuss how different variants integrate the positional embedding (with the positional feature space T ) into the kernel. Kernel construction on F. All the work considered the scaled asymmetric exponential kernel with the mapping W q and W k (Wilson et al., 2016; Li et al., 2017) for non-positional features f q and f k :",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_12",
  "x": "The designs for integrating the positional embedding t q and t k are listed in the following. (i) Absolute Positional Embedding (Vaswani et al., 2017; <cite>Dai et al., 2019</cite>; Ott et al., 2019) : For the original Transformer (Vaswani et al., 2017) , each t i is represented by a vector with each dimension being sine or cosine functions. For learned positional embedding<cite> (Dai et al., 2019</cite>; Ott et al., 2019) , each t i is a learned parameter and is fixed for the same position for different sequences.",
  "y": "uses background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_13",
  "x": "For learned positional embedding<cite> (Dai et al., 2019</cite>; Ott et al., 2019) , each t i is a learned parameter and is fixed for the same position for different sequences. These works defines the feature space as the direct sum of its temporal and non-temporal space: X = F \u2295 T . Via the lens of kernel, the kernel similarity is defined as",
  "y": "uses background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_14",
  "x": "Via the lens of kernel, the kernel similarity is defined as (ii) Relative Positional Embedding in Transformer-XL<cite> (Dai et al., 2019)</cite> : t represents the indicator of the position in the sequence, and the kernel is chosen to be asymmetric of mixing sine and cosine functions: with k fq t q , t k being an asymmetric kernel with coefficients inferred by f q : log k fq t q , t k = \u2211 (iii) Relative Positional Embedding of Shaw et al. (2018) and Music Transformer (Huang et al., 2018b) : t \u22c5 represents the indicator of the position in the sequence, and the kernel is modified to be indexed by a look-up table:",
  "y": "uses background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_16",
  "x": "In contrast, Eq. (5) represents the kernel as a product of two kernels (one for f i and another for t i ), which is able to capture the similarities for both temporal and non-temporal components. (ii) Transformer-XL<cite> (Dai et al., 2019)</cite> , Music Transformer (Huang et al., 2018b) , Self-Attention with Relative Positional Embedding (Shaw et al., 2018) :",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_17",
  "x": "Note that decoder self-attention considers x q = x k with x q being the decoded sequence. Since the decoded sequence is the output for previous timestep, the query at position i can only observe the keys being the tokens that are decoded with position < i. For convenience, let us define S 1 as the set returned by original Transformer (Vaswani et al., 2017 ) from M (x q , S x k ), which we will use it later. (iv) Decoder Self-Attention in Transformer-XL<cite> (Dai et al., 2019)</cite> : For each query x q in the decoded sequence, M (x q , S x k ) returns a set containing S 1 and additional memories (M (x q , S x k ) = S 1 + S mem , M (x q , S x k ) \u2283 S 1 ).",
  "y": "uses background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_18",
  "x": "For performance-wise comparisons, Transformer-XL<cite> (Dai et al., 2019)</cite> showed that, the additional memories in M (x q , S x k ) are able to capture longer-term dependency than the original Transformer (Vaswani et al., 2017) and hence results in better performance. Sparse Transformer (Child et al., 2019) showed that although having much fewer elements in M (x q , S x k ), if the elements are carefully chosen, the attention can still reach the same performance as Transformer-XL<cite> (Dai et al., 2019)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_19",
  "x": "For performance-wise comparisons, Transformer-XL<cite> (Dai et al., 2019)</cite> showed that, the additional memories in M (x q , S x k ) are able to capture longer-term dependency than the original Transformer (Vaswani et al., 2017) and hence results in better performance. Sparse Transformer (Child et al., 2019) showed that although having much fewer elements in M (x q , S x k ), if the elements are carefully chosen, the attention can still reach the same performance as Transformer-XL<cite> (Dai et al., 2019)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_20",
  "x": "Note that t i here is chosen as the mixture of sine and cosine functions as in the prior work (Vaswani et al., 2017; Ott et al., 2019) . In our experiment, we find it reaching competitive performance as comparing to the current state-of-the-art designs (Eq. (5) by<cite> Dai et al. (2019)</cite> ). We fix the size of the weight matrices W \u22c5 in Eq. (9) and Eq. (5) which means we save 33% of the parameters in attention from Eq. (9)",
  "y": "uses similarities"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_21",
  "x": "Q4. Is positional embedding required in value function? We conduct experiments on neural machine translation (NMT) and sequence prediction (SP) tasks since these two tasks are commonly chosen for studying Transformers (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> .",
  "y": "uses background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_22",
  "x": "We conduct experiments on neural machine translation (NMT) and sequence prediction (SP) tasks since these two tasks are commonly chosen for studying Transformers (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> . Note that NMT has three different types of attentions (e.g., encoder selfattention, decoder-encoder attention, decoder selfattention) and SP has only one type of attention (e.g., decoder self-attention). For the choice of datasets, we pick IWSLT'14 German-English (De-En) dataset (Edunov et al., 2017) for NMT and WikiText-103 dataset (Merity et al., 2016) for SP as suggested by Edunov et al. (Edunov et al., 2017) and<cite> Dai et al. (Dai et al., 2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_23",
  "x": "For fairness of comparisons, we train five random initializations and report test accuracy with the highest validation score. We fix the position-wise operations in Transformer 3 and only change the attention mechanism. Similar to prior work (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> , we report BLEU score for NMT and perplexity for SP.",
  "y": "similarities"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_24",
  "x": "Table 2 : Kernel Types. Other than manipulating the kernel choice of the non-positional features, we fix the configuration by Vaswani et al. (2017) for NMT and the configuration by<cite> Dai et al. (2019)</cite> for SP. 34.14 24.13 24.21 table, it outperforms the case with having PE as direct-sum in feature space, especially for SP task.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_25",
  "x": "34.14 24.13 24.21 table, it outperforms the case with having PE as direct-sum in feature space, especially for SP task. Note that the look-up table is indexed by the relative position (i.e., t q \u2212 t k ) instead of absolute position. Second, we see that PE in the product kernel proposed by<cite> Dai et al. (Dai et al., 2019)</cite> may not constantly outperform the other integration types (it has lower BLEU score for NMT).",
  "y": "differences uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_26",
  "x": "The numbers are shown in Table 2 . Note that, for fairness, other than manipulating the kernel choice of the non-positional features, we fix the configuration by Vaswani et al. (Vaswani et al., 2017) for NMT and the configuration by<cite> Dai et al. (Dai et al., 2019)</cite> for SP. We first observe that the linear kernel does not converge for both NMT and SP.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_27",
  "x": "---------------------------------- **ORDER-INVARIANCE IN ATTENTION** The need of the positional embedding (PE) in the attention mechanism is based on the argument that the attention mechanism is an order-agnostic (or, permutation equivariant) operation (Vaswani et al., 2017; Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019) .",
  "y": "background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_28",
  "x": "**ORDER-INVARIANCE IN ATTENTION** The need of the positional embedding (PE) in the attention mechanism is based on the argument that the attention mechanism is an order-agnostic (or, permutation equivariant) operation (Vaswani et al., 2017; Shaw et al., 2018; Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019) . However, we show that, for decoder self-attention, the operation is not order-agnostic.",
  "y": "differences background"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_29",
  "x": "For clarification, we are not attacking the claim made by the prior work (Vaswani et al., 2017; Shaw et al., 2018;  Huang et al., 2018b; <cite>Dai et al., 2019</cite>; Child et al., 2019 ), but we aim at providing a new look at the order-invariance problem when considering the attention mechanism with masks (masks refer to the set filtering function in our kernel formulation). In other words, previous work did not consider the mask between queries and keys when discussing the order-invariance problem (P\u00e9rez et al., 2019) . To put it formally, we first present the definition by for a permutation equivariance function:",
  "y": "differences"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_30",
  "x": "To put it formally, we first present the definition by for a permutation equivariance function: Definition 2. Denote \u03a0 as the set of all permutations over [n] = {1, \u22ef, n}. A function f unc \u2236 X n \u2192 Y n is permutation equivariant iff for any permutation \u03c0 \u2208 \u03a0, f unc(\u03c0x) = \u03c0f unc(x). showed that the standard attention (encoder self-attention (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> ) is permutation equivariant.",
  "y": "uses"
 },
 {
  "id": "20330d309c218dc2e1521b9644ed9c_31",
  "x": "Here, we present the non-permutation-equivariant problem on the decoder self-attention: (Vaswani et al., 2017;<cite> Dai et al., 2019)</cite> is not permutation equivariant. To proceed the proof, we need the following definition and propositions. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_0",
  "x": "Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009) , attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically<cite> (Plank and Moschitti, 2013)</cite> .",
  "y": "motivation"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_1",
  "x": "The only study explicitly targeting this problem so far is by<cite> Plank and Moschitti (2013)</cite> who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels. Although this idea is interesting, it suffers from two major limitations: + It does not incorporate word cluster information at different levels of granularity. In fact,<cite> Plank and Moschitti (2013)</cite> only use the 10-bit cluster prefix in their study.",
  "y": "background"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_2",
  "x": "The only study explicitly targeting this problem so far is by<cite> Plank and Moschitti (2013)</cite> who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels. Although this idea is interesting, it suffers from two major limitations: + It does not incorporate word cluster information at different levels of granularity. In fact,<cite> Plank and Moschitti (2013)</cite> only use the 10-bit cluster prefix in their study.",
  "y": "background motivation"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_3",
  "x": "As the real-valued features are able to capture latent yet useful properties of words, the augmentation of lexical terms with these features is desirable to provide a more general representation, potentially helping relation extractors perform more robustly across domains. In this work, we propose to avoid these limitations by applying a feature-based approach for RE which allows us to integrate various word features of generalization into a single system more natu-rally and effectively. The application of word representations such as word clusters in domain adaptation of RE <cite>(Plank and Moschitti, 2013</cite> ) is motivated by its successes in semi-supervised methods (Chan and Roth, 2010; Sun et al., 2011) where word representations help to reduce data-sparseness of lexical information in the training data.",
  "y": "background"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_4",
  "x": "Following<cite> Plank and Moschitti (2013)</cite> , we assume that we only have labeled data in a single source domain but no labeled as well as unlabeled target data. Moreover, we consider the singlesystem DA setting where we construct a single system able to work robustly with different but related domains (multiple target domains). This setting differs from most previous studies (Blitzer et al., 2006) on DA which have attempted to design a specialized system for every specific target domain.",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_5",
  "x": "Unfortunately, this feature set includes the human-annotated (gold-standard) information on entity and mention types which is often missing or noisy in reality<cite> (Plank and Moschitti, 2013)</cite> . Therefore, following the settings of<cite> Plank and Moschitti (2013)</cite> , we will only assume entity boundaries and not rely on the gold standard information in the experiments.",
  "y": "motivation uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_6",
  "x": "Unfortunately, this feature set includes the human-annotated (gold-standard) information on entity and mention types which is often missing or noisy in reality<cite> (Plank and Moschitti, 2013)</cite> . This issue becomes more serious in our setting of single-system DA where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available. Therefore, following the settings of<cite> Plank and Moschitti (2013)</cite> , we will only assume entity boundaries and not rely on the gold standard information in the experiments.",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_7",
  "x": "We use the ACE 2005 corpus for DA experiments (as in<cite> Plank and Moschitti (2013)</cite> ). It involves 6 relation types and 6 domains: broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversation (cts), weblogs (wl) and usenet (un). We follow the standard practices on ACE<cite> (Plank and Moschitti, 2013)</cite> and use news (the union of bn and nw) as the source domain and bc, cts and wl as our target domains.",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_8",
  "x": "We use the ACE 2005 corpus for DA experiments (as in<cite> Plank and Moschitti (2013)</cite> ). It involves 6 relation types and 6 domains: broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversation (cts), weblogs (wl) and usenet (un). We follow the standard practices on ACE<cite> (Plank and Moschitti, 2013)</cite> and use news (the union of bn and nw) as the source domain and bc, cts and wl as our target domains.",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_9",
  "x": "We take half of bc as the only target development set, and use the remaining data and domains for testing purposes (as they are small already). As noted in<cite> Plank and Moschitti (2013)</cite> , the distributions of relations as well as the vocabularies of the domains are quite different. ----------------------------------",
  "y": "background"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_10",
  "x": "Embeddings This section examines the effectiveness of word representations for RE across domains. We evaluate word cluster and embedding (denoted by ED) features by adding them individually as well as simultaneously into the baseline feature set. For word clusters, we experiment with two possibilities: (i) only using a single prefix length of 10 (as<cite> Plank and Moschitti (2013)</cite> did) (denoted by WC10) and (ii) applying multiple prefix lengths of 4, 6, 8, 10 together with the full string (denoted by WC).",
  "y": "uses"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_11",
  "x": "(i): The baseline system achieves a performance of 51.4% within its own domain while the performance on target domains bc, cts, wl drops to 49.7%, 41.5% and 36.6% respectively. Our baseline performance is worse than that of<cite> Plank and Moschitti (2013)</cite> only on the target domain cts and better in the other cases. This might be explained by the difference between our baseline feature set and the feature set underlying their kernel-based system.",
  "y": "differences"
 },
 {
  "id": "206b65ee4e69a01e8a0892dc0f2b30_12",
  "x": "However, in domain cts, the improvement that word embeddings provide for word clusters is modest. This is because the RCV1 corpus used to induce the word embeddings (Turian et al., 2010) does not cover spoken language words in cts very well. (v): Finally, the in-domain performance is also improved consistently demonstrating the robustness of word representations<cite> (Plank and Moschitti, 2013)</cite> .",
  "y": "differences"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_0",
  "x": "<cite>Ambati et al. (2013)</cite> showed that the performance of Malt (Nivre et al., 2007b) on the free word order language, Hindi, is improved by using lexical categories from Combinatory Categorial Grammar (CCG) (Steedman, 2000) . In this paper, we extend <cite>this work</cite> and show that CCG categories are useful even in the case of English, a typologically different language, where parsing accuracy of dependency parsers is already extremely high. In addition, we also demonstrate the utility of CCG categories to MST (McDonald et al., 2005) for both languages.",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_1",
  "x": "<cite>Ambati et al. (2013)</cite> showed that the performance of Malt (Nivre et al., 2007b) on the free word order language, Hindi, is improved by using lexical categories from Combinatory Categorial Grammar (CCG) (Steedman, 2000) . In this paper, we extend <cite>this work</cite> and show that CCG categories are useful even in the case of English, a typologically different language, where parsing accuracy of dependency parsers is already extremely high. In addition, we also demonstrate the utility of CCG categories to MST (McDonald et al., 2005) for both languages.",
  "y": "extends"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_2",
  "x": "Conversely, <cite>Ambati et al. (2013)</cite> showed that a Hindi dependency parser (Malt) could be improved by using CCG categories. Using an algorithm similar to Cakici (2005) and Uematsu et al. (2013) , <cite>they first created</cite> a <cite>Hindi CCGbank</cite> from a Hindi dependency treebank and built a supertagger. <cite>They provided</cite> CCG categories from a supertagger as features to Malt and obtained overall improvements of 0.3% and 0.4% in unlabelled and labelled attachment scores respectively.",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_3",
  "x": "features from a dependency parser to a re-ranker with an improvement of 0.35% in labelled F-score of the CCGbank test set. Conversely, <cite>Ambati et al. (2013)</cite> showed that a Hindi dependency parser (Malt) could be improved by using CCG categories. Using an algorithm similar to Cakici (2005) and Uematsu et al. (2013) , <cite>they first created</cite> a <cite>Hindi CCGbank</cite> from a Hindi dependency treebank and built a supertagger.",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_4",
  "x": "Using an algorithm similar to Cakici (2005) and Uematsu et al. (2013) , <cite>they first created</cite> a <cite>Hindi CCGbank</cite> from a Hindi dependency treebank and built a supertagger. <cite>They provided</cite> CCG categories from a supertagger as features to Malt and obtained overall improvements of 0.3% and 0.4% in unlabelled and labelled attachment scores respectively. Figure 1 shows a CCG derivation with CCG lexical categories for each word and Stanford scheme dependencies (De Marneffe et al., 2006) for an example English sentence.",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_5",
  "x": "For Hindi, we worked with the Hindi Dependency Treebank (HDT) released as part of Coling 2012 Shared Task (Bharati et al., 2012) . HDT contains 12,041 training, 1,233 development and 1,828 testing sentences. We used the English (Hockenmaier and Steedman, 2007) and <cite>Hindi CCGbanks</cite> (Ambati et al., 1 http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html 2013) for our experiments.",
  "y": "uses"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_6",
  "x": "---------------------------------- **SUPERTAGGERS** We used Clark and Curran (2004) 's supertagger for English, and <cite>Ambati et al. (2013)</cite> 's supertagger for Hindi.",
  "y": "uses"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_7",
  "x": "The Clark and Curran (2004) supertagger uses different features like word, partof-speech, and contextual and complex bi-gram features to obtain a 1-best accuracy of 91.5% on the development set. In addition to the above mentioned features, <cite>Ambati et al. (2013)</cite> employed morphological features useful for Hindi. The 1-best accuracy of Hindi supertagger for finegrained and coarse-grained lexicon is 82.92% and 84.40% respectively.",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_8",
  "x": "For Hindi, we also did all our experiments using automatic features <cite>Ambati et al. (2013)</cite> ). (POS, chunk and morphological information) extracted using a Hindi shallow parser 2 . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_9",
  "x": "**CCG CATEGORIES AS FEATURES** Following <cite>Ambati et al. (2013)</cite> , we used supertags which occurred at least K times in the training data, and backed off to coarse POS-tags otherwise. For English K=1, i.e., when we use CCG categories for all words, gave the best results.",
  "y": "uses"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_10",
  "x": "But for longer distances, 6\u221210, and >10, there was significant improvement of 1.3% and 1.3% respectively for MST. <cite>Ambati et al. (2013)</cite> reported similar improvements for Malt as well. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_11",
  "x": "We have shown that informative CCG categories, which contain both local subcategorization information and capture long distance dependencies elegantly, improve the performance of two dependency parsers, Malt and MST, by helping in recovering long distance relations for Malt and local verbal arguments for MST. This is true both in the case of English (a fixed word order language) and Hindi (free word order and morphologically richer language), extending the result of <cite>Ambati et al. (2013)</cite> . The result is particularly interesting in the case of Malt which cannot directly use valency information, which CCG categories provide indirectly.",
  "y": "extends"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_12",
  "x": "<cite>Ambati et al. (2013)</cite> showed that for Hindi, providing CCG categories as features improved Malt in better handling of long distance dependencies. The percentage of dependencies in the 1\u22125, 6\u221210 and >10 distance ranges are 82.2%, 8.6% and 9.2% respectively out of the total of around 40,000 dependencies. Similar to English, there was very slight improvement for short distance dependencies (1\u22125).",
  "y": "uses"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_13",
  "x": "We got final improvements of 0.5% and 0.3% in UAS and LAS respectively. In contrast, for Malt, <cite>Ambati et al. (2013)</cite> had shown that coarse-grained supertags gave larger improvements of 0.3% and 0.4% in UAS and LAS respectively. Due to better handling of error propagation in MST, the richer information in fine-grained categories may have surpassed the slightly lower supertagger performance, compared to coarse-grained categories.",
  "y": "background"
 },
 {
  "id": "21c2160667b3ff919e39285cd1ece7_14",
  "x": "We got final improvements of 0.5% and 0.3% in UAS and LAS respectively. In contrast, for Malt, <cite>Ambati et al. (2013)</cite> had shown that coarse-grained supertags gave larger improvements of 0.3% and 0.4% in UAS and LAS respectively. Due to better handling of error propagation in MST, the richer information in fine-grained categories may have surpassed the slightly lower supertagger performance, compared to coarse-grained categories.",
  "y": "differences"
 },
 {
  "id": "2292b2c0366ef12a5dd25e544f6b2d_0",
  "x": "First, we consider Berant and Liang (2014) 's own extension of the semantic parser of <cite>Berant et al. (2013)</cite> by using paraphrases. Second, we apply WordNet synonyms (Miller, 1995) for selected parts of speech to the queries in the Free917 dataset. The new pairs of queries and logical forms are added to the dataset on which the semantic parsers are retrained.",
  "y": "uses"
 },
 {
  "id": "2292b2c0366ef12a5dd25e544f6b2d_1",
  "x": "We extend their application of responsebased learning for SMT to a larger and lexically more diverse dataset and show how to perform model selection in the environment from which response signals are obtained. In contrast to their work where a monolingual SMT-based approach (Andreas et al., 2013 ) is used as semantic parser, our work builds on existing parsers for Freebase, with a focus on exploiting paraphrasing and synonym extension for scaling semantic parsers to open-domain database queries. Response-based learning has been applied in previous work to semantic parsing itself (Kwiatowski et al. (2013) , <cite>Berant et al. (2013)</cite> , Goldwasser and Roth (2013) , inter alia).",
  "y": "background"
 },
 {
  "id": "2292b2c0366ef12a5dd25e544f6b2d_2",
  "x": "Our baseline system is the parser of <cite>Berant et al. (2013)</cite> , called SEMPRE. We first consider the approach presented by Berant and Liang (2014) to scale the baseline to open-domain database queries: In their system, called PARASEMPRE, pairs of logical forms and utterances are generated from a given query and the database, and the pair whose utterance best paraphrases the input query is selected.",
  "y": "uses"
 },
 {
  "id": "2292b2c0366ef12a5dd25e544f6b2d_3",
  "x": "Training of the baseline SMT system was performed on the COMMON CRAWL 5 (Smith et al., 2013 ) dataset consisting of 7.5M parallel English-German segments extracted from the web. Response-based learning for SMT uses the code described in Riezler et al. (2014) 6 . For semantic parsing we use the SEMPRE and PARASEMPRE tools of <cite>Berant et al. (2013)</cite> and Berant and Liang (2014) which were trained on the training portion of the FREE917 corpus 7 .",
  "y": "uses"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_0",
  "x": "UKP-Athene <cite>(Hanselowski et al., 2018)</cite> , the highest document retrieval scoring team, uses MediaWiki API 1 to search the Wikipedia database for the claims noun phrases. ---------------------------------- **SENTENCE RETRIEVAL**",
  "y": "background"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_1",
  "x": "Enhanced Sequential Inference Model (ESIM) (Chen et al., 2016) with some small modifications has been used in (Nie et al., 2019;<cite> Hanselowski et al., 2018)</cite> . ESIM encodes premises and hypotheses using one Bidirectional Long Short-Term Memory (BiLSTM) with shared weights. The encoded sentences are later aligned by a bidirectional attention mechanism.",
  "y": "background"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_2",
  "x": "Finally, a softmax layer classifies the max and mean pooled representations of the second BiLSTM. The UKP-Athene team <cite>(Hanselowski et al., 2018)</cite> achieved the highest sentence retrieval recall using ESIM and pairwise training. Their model takes a claim and a pair of positive and negative sentences and predicts a similarity score for each sentence.",
  "y": "background"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_3",
  "x": "ESIM has been widely used among the FEVER challenge participants (Nie et al., 2019; Yoneda et al., 2018;<cite> Hanselowski et al., 2018)</cite> . UNC (Nie et al., 2019) , the winner of the competition, proposes a modified ESIM that takes the concatenation of the retrieved evidence sentences and claim along with ELMo embedding and three additional token-level features: Word-Net, number embedding, and semantic relatedness score from the document retrieval and sentence retrieval steps. Dream (Zhong et al., 2019) has the state of the art FEVER score.",
  "y": "background"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_4",
  "x": "**DOCUMENT RETRIEVAL** In the document retrieval step, the Wikipedia documents containing the evidence supporting or refuting the claim are retrieved. Following the UKP-Athene promising document retrieval component <cite>(Hanselowski et al., 2018)</cite> , which results in more than 93% development set document recall, we exactly use their method to collect a set of top documents D c l top for the claim c l .",
  "y": "uses"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_5",
  "x": "In addition, we experiment with the modified Hinge loss functions like <cite>(Hanselowski et al., 2018)</cite> : At testing time, for both pairwise loss functions, we sort the sentences by their output value o and similarly choose S c l top for the claim c l . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_8",
  "x": "The pointwise Model FEVER Score(%) Label Accuracy(%) DREAM (Zhong et al., 2019) 70 (Nie et al., 2019) 64.21 68.21 UCL (Yoneda et al., 2018) 62.52 67.62 UKP-Athene <cite>(Hanselowski et al., 2018)</cite> 61.58 65.46 Figure 5 : Recall and precision results on the development set. x shows the UNC, UCL, UPK-Athene, DREAM XLNet, and DREAM RoBERTa scores (Nie et al., 2019; Yoneda et al., 2018;<cite> Hanselowski et al., 2018</cite>; Zhong et al., 2019) methods surpass the pairwise methods in terms of recall-precision performance. Figure 5 also shows that HNM enhances both pairwise methods trained by the Ranknet and Hinge loss functions and preserves the pointwise performance.",
  "y": "differences"
 },
 {
  "id": "22da24997f66a6dafa911f83f061e5_9",
  "x": "The BERT claim verification system even if it is trained on the UKP-Athene sentence retrieval component <cite>(Hanselowski et al., 2018)</cite> , the state of the art method with the highest recall, improves both label accuracy and FEVER score. Training based on the BERT sentence retrieval predic-tions significantly enhances the verification results because while it explicitly improves the FEVER score by providing more correct evidence sentences, it provides a better training set for the verification system. The large BERTs are only trained on the best retrieval systems, and as expected significantly improve the performance.",
  "y": "differences"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_0",
  "x": "However, this treatment is not optimal because it ignores the fact that words can share similar morphemes which can be exploited to estimate the OOV word embedding better. Meanwhile, word representation models based on subword units, such as characters or word segments, have been shown to perform well in many NLP tasks such as POS tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015) , language modeling (Ling et al., 2015; Kim et al., 2016; Vania and Lopez, 2017) , machine translation (Vylomova et al., 2016; Lee et al., 2016; Sennrich et al., 2016) , dependency parsing (Ballesteros et al., 2015) , and sequence labeling<cite> (Rei et al., 2016</cite>; Lample et al., 2016) . These representations are effective because they can represent OOV words better by leveraging the orthographic similarity among words.",
  "y": "background"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_1",
  "x": "In this paper, we report the ability of a neural network-based approach for Indonesian NER in conversational data. We employed the neural sequence labeling model of<cite> (Rei et al., 2016)</cite> and experimented with two word representation models: word-level and character-level. We evaluated all models on relatively large, manually annotated Indonesian conversational texts.",
  "y": "uses"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_2",
  "x": "For the CRF model, we used an implementation provided by Okazaki (2007) 3 . Neural architectures for sequence labeling are pretty similar. They usually employ a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with CRF as the output layer, and a CNN (Ma and Hovy, 2016) or LSTM (Lample et al., 2016;<cite> Rei et al., 2016)</cite> composes the character embeddings.",
  "y": "background"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_3",
  "x": "They usually employ a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with CRF as the output layer, and a CNN (Ma and Hovy, 2016) or LSTM (Lample et al., 2016;<cite> Rei et al., 2016)</cite> composes the character embeddings. Also, we do not try to achieve state-of-theart results but only are interested whether neural sequence labeling models with character embedding can handle the OOV problem well. Therefore, for the neural models, we just picked the implementation provided in<cite> (Rei et al., 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_4",
  "x": "The loss function contains not only the log likelihood of the training data and the similarity score but also a language modeling loss, which is not mentioned in<cite> (Rei et al., 2016)</cite> but discussed in the subsequent work (Rei, 2017) . Thus, their implementation essentially does multi-task learning with sequence labeling as the primary task and language modeling as the auxiliary task. We used an almost identical setting to<cite> Rei et al. (2016)</cite> : words are lowercased, but characters are not, digits are replaced with zeros, singleton words in the training set are converted into unknown tokens, word and character embedding sizes are 300 and 50 respectively.",
  "y": "background"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_5",
  "x": "Thus, their implementation essentially does multi-task learning with sequence labeling as the primary task and language modeling as the auxiliary task. We used an almost identical setting to<cite> Rei et al. (2016)</cite> : words are lowercased, but characters are not, digits are replaced with zeros, singleton words in the training set are converted into unknown tokens, word and character embedding sizes are 300 and 50 respectively. The character embeddings were initialized randomly and learned during training.",
  "y": "similarities uses"
 },
 {
  "id": "237ac6f9b635e56119be956d7521e1_6",
  "x": "---------------------------------- **CONCLUSION AND FUTURE WORK** We reported an empirical evaluation of neural sequence labeling models by<cite> Rei et al. (2016)</cite> on NER in Indonesian conversational texts.",
  "y": "uses"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_0",
  "x": "To circumvent this problem,<cite> Khapra et al. (2009)</cite> proposed a WSD method that can be applied to a language even when no sense tagged corpus for that language is available. This is achieved by projecting Wordnet and corpus parameters from another language to the language in question. The approach is centered on a novel synset based multilingual dictionary (Mohanty et al., 2008) where the synsets of different languages are aligned and thereafter the words within the synsets are manually cross-linked.",
  "y": "motivation"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_1",
  "x": "In section 2 we present related work. In section 3 we describe the Synset based multilingual dictionary which enables parameter projection. In section 4 we discuss the work of<cite> Khapra et al. (2009)</cite> on parameter projection for multilingual WSD.",
  "y": "background"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_2",
  "x": "Recent work by<cite> Khapra et al. (2009)</cite> has shown that it is possible to project the parameters learnt from the annotation work of one language to another language provided aligned Wordnets for two languages are available. However, their work does not address the question of further improving the accuracy of WSD by using a small amount of training data from the target language. Some similar work has been done in the area of domain adaptation where Chan et al. (2007) showed that adding just 30% of the target data to the source data achieved the same performance as that obtained by taking the entire source and target data.",
  "y": "background"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_3",
  "x": "The second factor is the cost of sense annotated data from the target language. The third factor is the accuracy of WSD The first two factors in some sense relate to the cost of purchasing a commodity and the third factor relates to the commodity itself. The work of<cite> Khapra et al. (2009)</cite> as described above does not attempt to reach an optimal costbenefit point in this economic system.",
  "y": "background"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_4",
  "x": "Based on the above discussion, if k a is the cost of sense annotating one word, k c is the cost of manually cross-linking a word and A is the accuracy desired then the problem of multilingual WSD can be cast as an optimization problem: Accuracy \u2265 A where, w c and w a are the number of words to be manually cross linked and annotated respectively. Ours is thus a 3-factor economic model (crosslinking, annotation and accuracy) as opposed to the 2-factor model (cross-linking, accuracy) proposed by<cite> Khapra et al. (2009)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_5",
  "x": "The rationale here is simple: invest money in words which are bound to occur frequently in the test data and achieve maximum impact on the accuracy. In the following paragraphs, we explain our probabilistic cross linking model. The model proposed by <cite>Khapra et al. (2009</cite> ) is a deterministic model where the expected count for (Sense S, Marathi Word W ), i.e., the number of times the word W appears in sense S is approximated by the count for the corresponding cross linked Hindi word.",
  "y": "background"
 },
 {
  "id": "2407cfa8572ccbab7f9a081f45a4ad_6",
  "x": "The parameters thus learnt were then projected using the MultiDict (refer section 3 and 4) to build a resource conscious Marathi (T L ) WSD engine. We used the same dataset as described in<cite> Khapra et al. (2009)</cite> for all our experiments. The data was collected from two domains, viz., Tourism and Health.",
  "y": "similarities uses"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_0",
  "x": "However, in practical cases, appropriate methods are required to learn such models by automatically acquiring the necessary token-to-matrix assignments. In this paper, we introduce graded matrix grammars of natural language, a variant of the matrix grammars proposed by <cite>Rudolph and Giesbrecht (2010)</cite> , and show a close correspondence between this matrix-space model and weighted finite automata. We conclude that the problem of learning compositional matrix-space models can be mapped to the problem of learning weighted finite automata over the real numbers.",
  "y": "extends"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_1",
  "x": "To overcome the limitations of VSMs, <cite>Rudolph and Giesbrecht (2010)</cite> proposed Compositional Matrix-Space Models (CMSM) as a recent alternative model to work with distributional approaches. These models employ matrices instead of vectors and make use of iterated matrix multiplication as the only composition operation. They show that these models are powerful enough to subsume many known models, both quantitative (vector-space models with diverse composition operations) and qualitative ones (such as regular languages).",
  "y": "background"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_2",
  "x": "Therefore, methods for training such models should be developed e.g. by leveraging appropriate machine learning methods. In this paper, we are concerned with Graded Matrix Grammars, a variant of the Matrix Grammars of <cite>Rudolph and Giesbrecht (2010)</cite> , where instead of the \"yes or no\" decision, if a sequence is part of a language, a real-valued score is assigned. This is a popular task in NLP, used, e.g., in sentiment analysis settings (Yessenalina and Cardie, 2011) .",
  "y": "extends"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_4",
  "x": "The general principle of compositionality is that the meaning of a complex expression is a function of the meaning of its constituent tokens and some rules used to combine them (Frege, 1884) . More formally, according to <cite>Rudolph and Giesbrecht (2010)</cite> , the underlying idea can be described as follows: \"Given a mapping \u00b7 : \u03a3 \u2192 S from a set of tokens in \u03a3 into some semantical space S, the composition operation is defined by mapping sequences of meanings to meanings: : S \u2192 S. So, the meaning of the sequence of tokens \u03c3 1 \u00b7 \u00b7 \u00b7 \u03c3 n can be obtained by first applying the function \u00b7 to each token and then to the sequence \u03c3 1 \u00b7 \u00b7 \u00b7 \u03c3 n , as shown in Figure 2 \". Figure 2: Principle of compositionality, illustration taken from <cite>Rudolph and Giesbrecht (2010)</cite> In compositional matrix-space models, this general idea is instantiated as follows: we have S = R n\u00d7n , i.e., the semantical space consists of quadratic matrices of real numbers.",
  "y": "background"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_5",
  "x": "The general principle of compositionality is that the meaning of a complex expression is a function of the meaning of its constituent tokens and some rules used to combine them (Frege, 1884) . More formally, according to <cite>Rudolph and Giesbrecht (2010)</cite> , the underlying idea can be described as follows: \"Given a mapping \u00b7 : \u03a3 \u2192 S from a set of tokens in \u03a3 into some semantical space S, the composition operation is defined by mapping sequences of meanings to meanings: : S \u2192 S. So, the meaning of the sequence of tokens \u03c3 1 \u00b7 \u00b7 \u00b7 \u03c3 n can be obtained by first applying the function \u00b7 to each token and then to the sequence \u03c3 1 \u00b7 \u00b7 \u00b7 \u03c3 n , as shown in Figure 2 \". Figure 2: Principle of compositionality, illustration taken from <cite>Rudolph and Giesbrecht (2010)</cite> In compositional matrix-space models, this general idea is instantiated as follows: we have S = R n\u00d7n , i.e., the semantical space consists of quadratic matrices of real numbers.",
  "y": "uses"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_6",
  "x": "Then, using the standard matrix multiplication as the only composition operation , the semantics of complex phrases are also described by matrices. <cite>Rudolph and Giesbrecht (2010)</cite> showed theoretically that by employing matrices instead of vectors, CMSMs subsume a wide range of linguistic models such as statistical models (vector-space models and word space models). ----------------------------------",
  "y": "background"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_7",
  "x": "**GRADED MATRIX GRAMMARS AND WEIGHTED FINITE AUTOMATA** In some applications of NLP, we need to derive the meaning of a sequence of words in a language, which can be done with CMSMs as described in Section 2.2. In this section, we introduce the notion of a graded matrix grammar which constitutes a slight variation of matrix grammars as introduced by <cite>Rudolph and Giesbrecht (2010)</cite> .",
  "y": "extends"
 },
 {
  "id": "26658b95c9bac96f1206da96b95921_8",
  "x": "An application of CMSM has been shown in the work of Yessenalina and Cardie (2011) . They proposed a learning-based approach for phraselevel sentiment analysis. Inspired by the work of <cite>Rudolph and Giesbrecht (2010)</cite> they use CMSMs to model composition, and present an algorithm for learning a matrix for each word via ordered logistic regression, which is evaluated with promising results.",
  "y": "background"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_0",
  "x": "<cite>Xiang et al. (2013)</cite> formalized the problem as classifying each IP node (roughly corresponds to S and SBAR in Penn Treebank) in the phrase structure. In this paper, we propose a novel method for empty category detection for Japanese that uses conjunction features on phrase structure and word embeddings. We use the Keyaki Treebank (Butler et al., 2012) , which is a recent development.",
  "y": "background"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_1",
  "x": "As it has annotations for pro and trace, we show our method has substantial improvements over the state-of-the-art machine learning-based method<cite> (Xiang et al., 2013)</cite> for Chinese empty category detection as well as linguistically-motivated manually written rule-based method similar to (Campbell, 2004 ). ---------------------------------- **BASELINE SYSTEMS**",
  "y": "differences"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_2",
  "x": "We also use<cite> Xiang et al's (2013)</cite> model as another baseline. It formulates empty category detection as the classification of IP nodes. For example, in Figure 1 , empty nodes in the left tree are removed and encoded as additional labels with its position information to IP nodes in the right tree.",
  "y": "uses"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_3",
  "x": "The probability model of<cite> (Xiang et al., 2013)</cite> is formulated as MaxEnt model: where \u03c6 is a feature vector, \u03b8 is a weight vector to \u03c6 and Z is normalization factor: where E represents the set of all empty category types to be detected.",
  "y": "background"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_4",
  "x": "<cite>Xiang et al. (2013)</cite> grouped their features into four types: tree label features, lexical features, empty category features and conjunction features as shown in Table 1 . As the features for<cite> (Xiang et al., 2013)</cite> were developed for Chinese Penn Treebank, we modify their features for Keyaki Treebank: First, the traversal order is changed from post-order (bottom-up) to pre-order (top-down). As PROs are implicit in Keyaki Treebank, the decisions on IPs in lower levels depend on those on higher levels in the tree.",
  "y": "background"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_5",
  "x": "where E represents the set of all empty category types to be detected. <cite>Xiang et al. (2013)</cite> grouped their features into four types: tree label features, lexical features, empty category features and conjunction features as shown in Table 1 . As the features for<cite> (Xiang et al., 2013)</cite> were developed for Chinese Penn Treebank, we modify their features for Keyaki Treebank: First, the traversal order is changed from post-order (bottom-up) to pre-order (top-down).",
  "y": "extends"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_6",
  "x": "Table 2 shows the accuracies of Japanese empty category detection, using the original and our modification of the<cite> (Xiang et al., 2013)</cite> with ablation test. We find that the conjunction features left-sibling label or POS tag (up to two siblings) 10 right-sibling label or POS tag (up to two siblings) Lexical features 11 left-most word under the current node 12 right-most word under the current node 13 word immediately left to the span of the current node 14 word immediately right to the span of the current node 15 head word of the current node 16 head word of the parent node 17 is the current node head child of its parent? (binary) Empty category features 18 predicted empty categories of the left sibling 19* the set of detected empty categories of ancestor nodes Conjunction features 20 current node label with parent node label 21* current node label with features computed from ancestor nodes 22 current node label with features computed from leftsibling nodes 23 current node label with lexical features<cite> (Xiang et al., 2013)</cite> 68.2 \u22120.40 modified<cite> (Xiang et al., 2013)</cite> 68.6 -\u2212 Tree label 68.6 \u22120.00 \u2212 Empty category 68.3 \u22120.30 \u2212 Lexicon 68.6 \u22120.00 \u2212 Conjunction 58.5 \u221210.1 Table 2 : Ablation result of<cite> (Xiang et al., 2013)</cite> are highly effective compared to the three other features.",
  "y": "uses"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_8",
  "x": "In gold parse condition, we used the trees of Keyaki Treebank without empty categories as input to the systems. In system parse condition, we used the output of the Berkeley Parser model of HARUNIWA before rule-based empty category detection 1 . We evaluated them using the word-position-level identification metrics described in<cite> (Xiang et al., 2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_9",
  "x": "In the gold parse condition, the two baselines, the rule-based method (RULE) and the modified<cite> (Xiang et al., 2013)</cite> method, achieved the F-measure of 62.6% and 68.6% respectively. Among the proposed models, the combination of path feature and child feature (PATH \u00d7 CHILD) even outperformed the baselines.",
  "y": "differences"
 },
 {
  "id": "26743b7d006e485be1b850a4424a5f_10",
  "x": "In the gold parse condition, the two baselines, the rule-based method (RULE) and the modified<cite> (Xiang et al., 2013)</cite> method, achieved the F-measure of 62.6% and 68.6% respectively. We also implemented the third baseline based on (Johnson, 2002) . Minimal unlexicalized tree fragments from empty node to its antecedent were extracted as pattern rules based on corpus statistics.",
  "y": "uses"
 },
 {
  "id": "27aeffca1f7a9a6b40743284a2871d_0",
  "x": "It turns out there is a surprising connection between the two that suggests novel ways of extending both grammars and topic models. After explaining this connection, I go on to describe extensions which identify topical multiword collocations and automatically learn the internal structure of namedentity phrases. The adaptor grammar framework is a nonparametric extension of probabilistic context-free grammars (Johnson et al., 2007) , which was initially intended to allow fast prototyping of models of unsupervised language acquisition (Johnson, 2008), but it has been shown to have applications in text data mining and information retrieval as well <cite>Hardisty et al., 2010</cite>) .",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_0",
  "x": "Prior argumentative relation mining studies have often used features extracted from argument components to model different aspects of the relations between the components, e.g., relative distance, word pairs, semantic similarity, textual entailment (Cabrio and Villata, 2012;<cite> Stab and Gurevych, 2014b</cite>; Boltu\u017ei\u0107 and\u0160najder, 2014; Peldszus and Stede, 2015b) . Features extracted from the text surrounding the components have been less explored, e.g., using words and their part-of-speech from adjacent sentences (Peldszus, 2014) . The first hypothesis investigated in this paper is that the discourse relations of argument components with adjacent sentences (called context windows in this study, a formal definition is given in \u00a75.3) can help characterize the argumentative relations that connect pairs of argument components.",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_1",
  "x": "Unlike argument component identification where textual inputs are typically sentences or clauses (Moens et al., 2007;<cite> Stab and Gurevych, 2014b</cite>; Levy et al., 2014; Lippi and Torroni, 2015) , textual inputs of argumentative relation mining vary from clauses (Stab and Gurevych, 2014b; Peldszus, 2014 ) to multiple-sentences (Biran and Rambow, 2011; Cabrio and Villata, 2012; Boltu\u017ei\u0107 an\u010f Snajder, 2014) . Studying claim justification between user comments, Biran and Rambow (2011) proposed that the argumentation in justification of a claim can be characterized with discourse structure in the justification. They however only considered discourse markers but not discourse relations.",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_2",
  "x": "In our work, we follow <cite>Stab and Gurevych (2014b)</cite> and use the predicted labels of argument components as features during argumentative relation mining. We, however, take advantage of an enhanced argument component model (Nguyen and Litman, 2016 ) to obtain more reliable argument component labels than in<cite> (Stab and Gurevych, 2014b)</cite> . Argument mining research has studied different data-driven approaches for separating organizational content (shell) from topical content to improve argument component identification, e.g., supervised sequence model (Madnani et al., 2012) , unsupervised probabilistic topic models (S\u00e9aghdha and Teufel, 2014; Du et al., 2014) .",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_3",
  "x": "In our work, we follow <cite>Stab and Gurevych (2014b)</cite> and use the predicted labels of argument components as features during argumentative relation mining. We, however, take advantage of an enhanced argument component model (Nguyen and Litman, 2016 ) to obtain more reliable argument component labels than in<cite> (Stab and Gurevych, 2014b)</cite> . Argument mining research has studied different data-driven approaches for separating organizational content (shell) from topical content to improve argument component identification, e.g., supervised sequence model (Madnani et al., 2012) , unsupervised probabilistic topic models (S\u00e9aghdha and Teufel, 2014; Du et al., 2014) .",
  "y": "differences"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_4",
  "x": "Finally, prior research has explored predicting different argumentative relationship labels between pairs of argument components, e.g., attachment (Peldszus and Stede, 2015a) , support vs. non-support (Biran and Rambow, 2011; Cabrio and Villata, 2012;<cite> Stab and Gurevych, 2014b)</cite> , {implicit, explicit}\u00d7{support, attack} (Boltu\u017ei\u0107 and\u0160najder, 2014) , verifiability of support (Park and Cardie, 2014) . Our experiments use two such argumentative relation classification tasks (Support vs. Non-support, Support vs. Attack) to evaluate the effectiveness of our proposed features. ----------------------------------",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_5",
  "x": "Finally, prior research has explored predicting different argumentative relationship labels between pairs of argument components, e.g., attachment (Peldszus and Stede, 2015a) , support vs. non-support (Biran and Rambow, 2011; Cabrio and Villata, 2012;<cite> Stab and Gurevych, 2014b)</cite> , {implicit, explicit}\u00d7{support, attack} (Boltu\u017ei\u0107 and\u0160najder, 2014) , verifiability of support (Park and Cardie, 2014) . Our experiments use two such argumentative relation classification tasks (Support vs. Non-support, Support vs. Attack) to evaluate the effectiveness of our proposed features. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_6",
  "x": "Stab and Gurevych (2014a) compiled the Persuasive Essay Corpus consisting of 90 student argumentative essays and made it publicly available. 3 Because the corpus has been utilized for different argument mining tasks (Stab and Gurevych, 2014b; Nguyen and Litman, 2015; Nguyen and Litman, 2016) , we use this corpus to demonstrate our context-aware argumentative relation mining approach, and adapt the model developed by <cite>Stab and Gurevych (2014b)</cite> to serve as the baseline for evaluating our proposed approach. Three experts identified possible argument components of three types within each sentence in the corpus (MajorClaim -writer's stance toward the writing topic, Claim -controversial statements that support or attack MajorClaim, and Premiseevidence used to underpin the validity of Claim), and also connected the argument components using two argumentative relations (Support and Attack).",
  "y": "extends"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_7",
  "x": "The number of relations are shown in Table 1. 4 Argumentative Relation Tasks 4.1 Task 1: Support vs. Non-support Our first task follows<cite> (Stab and Gurevych, 2014b)</cite> : given a pair of source and target argument components, identify whether the source argumentatively supports the target or not. Note that when a support relation does not hold, the source may attack or has no relation with the target compo- <cite>Stab and Gurevych (2014b)</cite> split the corpus into an 80% training set and a 20% test set which have similar label distributions.",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_8",
  "x": "4 Argumentative Relation Tasks 4.1 Task 1: Support vs. Non-support Our first task follows<cite> (Stab and Gurevych, 2014b)</cite> : given a pair of source and target argument components, identify whether the source argumentatively supports the target or not. Note that when a support relation does not hold, the source may attack or has no relation with the target compo- <cite>Stab and Gurevych (2014b)</cite> split the corpus into an 80% training set and a 20% test set which have similar label distributions. We use this split to train and test our proposed models, and directly compare our models' performance to the reported performance in<cite> (Stab and Gurevych, 2014b)</cite> .",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_9",
  "x": "As shown in Table 1 , of the total 1473 relations, we have 1312 (89%) Support and 161 (11%) Attack relations. Because this task was not studied in<cite> (Stab and Gurevych, 2014b)</cite> , we adapt Stab and Gurevych's model to use as the baseline. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_10",
  "x": "Given a pair of argument components, we follow<cite> (Stab and Gurevych, 2014b)</cite> by first extracting 3 feature sets: structural (e.g., word counts, sentence position), lexical (e.g., word pairs, first words), and grammatical production rules (e.g., S\u2192NP,VP). Because a sentence may have more than one argument component, the relative component positions might provide useful information (Peldszus, 2014) . Thus, we also include 8 new component position features: whether the source and target components are the whole sentences or the beginning/end components of the sentences; if the source is before or after the target component; and the absolute difference of their positions.",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_11",
  "x": "<cite>Stab and Gurevych (2014b)</cite> used a 55-discourse marker set to extract indicator features. We expand their discourse maker set by combining them with a 298-discourse marker set developed in (Biran and Rambow, 2011). We expect the expanded set of discourse markers will represent better possible discourse relations in the texts.",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_12",
  "x": "Thus, we also include 8 new component position features: whether the source and target components are the whole sentences or the beginning/end components of the sentences; if the source is before or after the target component; and the absolute difference of their positions. <cite>Stab and Gurevych (2014b)</cite> used a 55-discourse marker set to extract indicator features. We expand their discourse maker set by combining them with a 298-discourse marker set developed in (Biran and Rambow, 2011).",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_13",
  "x": "We expand their discourse maker set by combining them with a 298-discourse marker set developed in (Biran and Rambow, 2011). We expect the expanded set of discourse markers will represent better possible discourse relations in the texts. <cite>Stab and Gurevych (2014b)</cite> used predicted label of argument components as features for both training and testing their argumentation structure identification model.",
  "y": "background"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_14",
  "x": "We expect the expanded set of discourse markers will represent better possible discourse relations in the texts. <cite>Stab and Gurevych (2014b)</cite> used predicted label of argument components as features for both training and testing their argumentation structure identification model. 5 As their predicted labels are not available to us, we adapt this feature set by using the argument component model in (Nguyen and Litman, 2016) which was shown to outperform the corresponding model of Stab and Gurevych.",
  "y": "extends"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_16",
  "x": "Because temporal relations were shown not helpful for argument mining tasks (Biran and Rambow, 2011;<cite> Stab and Gurevych, 2014b)</cite> , we exclude them here. Discourse marker: while the baseline model only considers discourse markers within the argument components, we define a boolean feature for each discourse marker classifying whether the marker is present before the covering sentence of the source and target components or not. This implementation aims to characterize the discourse of the preceding and following text segments of each argument component separately.",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_17",
  "x": "We use the training set as determined in<cite> (Stab and Gurevych, 2014b)</cite> to train/test 9 the models using LibLINEAR algorithm (Fan et al., 2008) without parameter or feature optimization. Cross-validations are conducted using Weka (Hall et al., 2009) . We use Stanford parser (Klein and Manning, 2003) to perform text processing.",
  "y": "uses"
 },
 {
  "id": "27be8a173136e48a15f637278fd831_18",
  "x": "We train all models using the training set and report their performances on the test set in Table 2 . We also compare our baseline to the reported performance (REPORT) for Support vs. Non-support classification in<cite> (Stab and Gurevych, 2014b)</cite> . The learning algorithm with parameters are kept the same as in the window-size tuning experiment.",
  "y": "uses"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_0",
  "x": "An example of the type of image and gold-standard descriptions available can be seen in Figure 1 . Recent approaches to this task have been based on slot-filling (Yang et al., 2011;<cite> Elliott and Keller, 2013)</cite> , combining web-scale ngrams , syntactic tree substitution (Mitchell et al., 2012) , and description-by-retrieval (Farhadi et al., 2010; Ordonez et al., 2011; Hodosh et al., 2013) . Image description has been compared to translating an image into text or summarising an image",
  "y": "background"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_1",
  "x": "**DATA** We perform the correlation analysis on the Flickr8K data set of Hodosh et al. (2013) , and the data set of <cite>Elliott and Keller (2013)</cite> . The test data of the Flickr8K data set contains 1,000 images paired with five reference descriptions.",
  "y": "uses"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_2",
  "x": "The test data of <cite>Elliott and Keller (2013)</cite> contains 101 images paired with three reference descriptions. The images were taken from the PAS-CAL VOC Action Recognition Task, the reference descriptions were collected from Mechanical Turk, and the judgements were also collected from Mechanical Turk. <cite>Elliott and Keller (2013)</cite> generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1-5 for each imagedescription pair, resulting in a total of 2,042 human judgement-description pairings.",
  "y": "background"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_3",
  "x": "The images were taken from the PAS-CAL VOC Action Recognition Task, the reference descriptions were collected from Mechanical Turk, and the judgements were also collected from Mechanical Turk. <cite>Elliott and Keller (2013)</cite> generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1-5 for each imagedescription pair, resulting in a total of 2,042 human judgement-description pairings. In this analysis, we use only the first sentence of the description, which describes the event depicted in the image.",
  "y": "background"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_4",
  "x": "We calculate automatic measures for each image-retrieved sentence pair against the five reference descriptions for the original image. The test data of <cite>Elliott and Keller (2013)</cite> contains 101 images paired with three reference descriptions. <cite>Elliott and Keller (2013)</cite> generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1-5 for each imagedescription pair, resulting in a total of 2,042 human judgement-description pairings.",
  "y": "uses"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_5",
  "x": "Unigram BLEU without a brevity penalty has been reported by ), Ordonez et al. (2011 , and Kuznetsova et al. (2012) ; to the best of our knowledge, the only image description work to use higher-order n-grams with BLEU is <cite>Elliott and Keller (2013)</cite> . In this paper we use the smoothed BLEU implementation of Clark et al. (2011) to perform a sentence-level analysis, setting n = 1 and no brevity penalty to get the unigram BLEU measure, or n = 4 with the brevity penalty to get the Smoothed BLEU measure. We note that a higher BLEU score is better.",
  "y": "background"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_6",
  "x": "Finally, Meteor is most strongly correlated measure against human judgements. A similar pattern is observed in the <cite>Elliott and Keller (2013)</cite> data set, though the correlations are lower across all measures. This could be caused by the smaller sample size or because the descriptions were generated by a computer, and not retrieved from a collection of human-written descriptions containing the goldstandard text, as in the Flickr8K data set.",
  "y": "similarities"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_7",
  "x": "There are no fluency judgements available for Flickr8K, but <cite>Elliott and Keller (2013)</cite> report grammaticality judgements for their data, which are comparable to fluency ratings. We failed to find significant correlations between grammatlicality judgements and any of the automatic measures on the <cite>Elliott and Keller (2013)</cite> data. This discrepancy could be explained in terms of the differences between the weather forecast generation and image description tasks, or because the image description data sets contain thousands of texts and a few human judgements per text, whereas the data sets of Reiter and Belz (2009) included hundreds of texts with 30 human judges.",
  "y": "uses"
 },
 {
  "id": "280affafa32147a63e7eeda8d5f763_8",
  "x": "They did, however, find significant correlations of automatic measures against fluency judgements. There are no fluency judgements available for Flickr8K, but <cite>Elliott and Keller (2013)</cite> report grammaticality judgements for their data, which are comparable to fluency ratings. We failed to find significant correlations between grammatlicality judgements and any of the automatic measures on the <cite>Elliott and Keller (2013)</cite> data.",
  "y": "uses"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_0",
  "x": "In this domain, vector-space methods give state-ofthe-art performance (Pad\u00f3 and Lapata, 2007) . Previously, a graph based framework has been proposed that models word semantic similarity from parsed text <cite>(Minkov and Cohen, 2008)</cite> . The underlying graph in this case describes a text corpus as connected dependency structures, according to the schema shown in Figure 1 .",
  "y": "background"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_1",
  "x": "Notably, different edge types, as well as the paths traversed, may have varying importance for different types of similarity sought. For example, in the parsed text domain, noun similarity and verb similarity are associated with different syntactic phenomena (Resnik and Diab, 2000) . To this end, we consider a path constrained graph walk (PCW) algorithm, which allows one to learn meaningful paths given a small number of labeled examples and incorporates this information in assessing node relatedness in the graph <cite>(Minkov and Cohen, 2008)</cite> .",
  "y": "uses"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_2",
  "x": "To this end, we consider a path constrained graph walk (PCW) algorithm, which allows one to learn meaningful paths given a small number of labeled examples and incorporates this information in assessing node relatedness in the graph <cite>(Minkov and Cohen, 2008)</cite> . PCW have been successfully applied to the extraction of named entity coordinate terms, including city and person names, from graphs representing newswire text <cite>(Minkov and Cohen, 2008)</cite> , where the specialized measures learned outperformed the state-ofthe-art dependency vectors method (Pad\u00f3 and Lapata, 2007) for small-and medium-sized corpora. In this work, we apply the path constrained graph walk method to the task of eliciting general word relatedness from parsed text, conducting a set of experiments on the task of synonym extraction.",
  "y": "background"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_3",
  "x": "PCW have been successfully applied to the extraction of named entity coordinate terms, including city and person names, from graphs representing newswire text <cite>(Minkov and Cohen, 2008)</cite> , where the specialized measures learned outperformed the state-ofthe-art dependency vectors method (Pad\u00f3 and Lapata, 2007) for small-and medium-sized corpora. In this work, we apply the path constrained graph walk method to the task of eliciting general word relatedness from parsed text, conducting a set of experiments on the task of synonym extraction. While the tasks of named entity extraction and synonym extraction from text have been treated separately in the literature, this work shows that both tasks can be addressed using the same general framework.",
  "y": "uses"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_4",
  "x": "**PATH CONSTRAINED GRAPH WALKS** PCW is a graph walk variant proposed recently that is intended to bias the random walk process to follow meaningful edge sequences (paths) <cite>(Minkov and Cohen, 2008)</cite> . In this approach, rather than assume fixed (possibly, uniform) edge weight parameters \u0398 for the various edge types in the graph, the probability of following an edge of type \u2113 from node x is evaluated dynamically, based on the history of the walk up to x.",
  "y": "background"
 },
 {
  "id": "28805bfa8f8b847110664d7e05b1b3_5",
  "x": "In conducting the constrained walk, we applied a threshold of 0.5 to truncate paths associated with lower probability of reaching a relevant response, following on previous work <cite>(Minkov and Cohen, 2008)</cite> . We implemented DV using code made available by its authors, 3 where we converted the syntactic patterns specified to Stanford dependency parser conventions. The parameters of the DV method were set to medium context and oblique edge weighting scheme, which were found to perform best (Pad\u00f3 and Lapata, 2007) .",
  "y": "uses"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_0",
  "x": "Most of the existing studies [6] - [9] have used conventional Machine Learning (ML) models to detect cyberbullying incidents. Recently Deep Neural Network Based (DNN) models have also been applied for detection of cyberbullying [10] , <cite>[11]</cite> . In <cite>[11]</cite> , authors have used DNN models for detection of cyberbullying and have expanded their models across multiple social media platforms.",
  "y": "background"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_1",
  "x": "Most of the existing studies [6] - [9] have used conventional Machine Learning (ML) models to detect cyberbullying incidents. Recently Deep Neural Network Based (DNN) models have also been applied for detection of cyberbullying [10] , <cite>[11]</cite> . In <cite>[11]</cite> , authors have used DNN models for detection of cyberbullying and have expanded their models across multiple social media platforms.",
  "y": "background"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_2",
  "x": "Based on their reported results, their models outperform traditional ML models, and most importantly authors have stated that they have applied transfer learning which means their developed models for detection of cyberbullying can be adapted and used on other datasets. In this contribution, we begin by reproducing and validating the <cite>[11]</cite> proposed models and their results on the three datasets, Formspring [12] , Twitter [13] and Wikipedia [14] , which have been used by the authors. Cyberbullying takes place in almost all of the online social networks; therefore, developing a detection model which is adaptable and transferable to different social networks is of great value.",
  "y": "uses"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_3",
  "x": "Recently Deep Neural Network Based (DNN) models have also been applied for detection of cyberbullying [10] , <cite>[11]</cite> . In <cite>[11]</cite> , authors have used DNN models for detection of cyberbullying and have expanded their models across multiple social media platforms. We expand our work by re-implementing the models on a new dataset.",
  "y": "extends"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_4",
  "x": "In this study, we have first reproduced the experiments conducted in <cite>[11]</cite> on the datasets used by the authors namely, Formspring [12] , Wikipedia [14] , and Twitter [13] . We have used the same models and experimental setup for our implementations. In this section, we have briefly introduced the datasets and explained the models and other experiment components.",
  "y": "uses"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_5",
  "x": "Transfer learning is the process of using a model which has been trained on one task for another related task. Following <cite>[11]</cite> we also implemented the transfer learning procedure to evaluate to what extent the DNN models trained on a social network, here Twitter, Formspring, and Wiki, can successfully detect cyberbullying posts in another social network, i.e., YouTube. For this purpose we used the BLSTM with attention model and experimented with three different approaches.",
  "y": "uses"
 },
 {
  "id": "289ea9be270f68e23ca1809f997be9_6",
  "x": "In this study, we successfully reproduced the reference literature <cite>[11]</cite> for detection of cyberbullying incidents in social media platforms using DNN based models. The source codes and materials were mostly well organized and accessible. However, there were some details and settings that were not clearly stated.",
  "y": "uses"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_0",
  "x": "<cite>Prabhumoye et al. (2018)</cite> propose to transfer style through backtranslation. The latter method is simpler to train and it attains the state-of-the-art performance in style transfer accuracy, confirming the efficacy of back-translation in grounding meaning. The goal of the current study is to investigate alternative back-translation setups that attain a better balance between meaning preservation and style transfer.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_1",
  "x": "The latter method is simpler to train and it attains the state-of-the-art performance in style transfer accuracy, confirming the efficacy of back-translation in grounding meaning. The goal of the current study is to investigate alternative back-translation setups that attain a better balance between meaning preservation and style transfer. We introduce two approaches which extend the back-translation models proposed by <cite>Prabhumoye et al. (2018)</cite> exploring back-translation setups that preserve the content of the sentence better.",
  "y": "extends"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_3",
  "x": "---------------------------------- **GROUNDING MEANING IN BACK-TRANSLATION** While the previous work (<cite>Prabhumoye et al., 2018</cite>) focuses on creating a representation by translating to a pivot language, preserving meaning in the generated sentences is still an unsolved question.",
  "y": "motivation"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_4",
  "x": "<cite>Prabhumoye et al. (2018)</cite> introduces the technique of back-translation to perform style transfer. <cite>They</cite> first transfer a sentence to one pivot language and use the encoding of the sentence in the pivot language to train the generative models corresponding to the two styles. <cite>They</cite> also use feedback from a pre-trained classifier to guide the generators to generate the desired style.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_5",
  "x": "Style transfer through Back-translation. <cite>Prabhumoye et al. (2018)</cite> introduces the technique of back-translation to perform style transfer. <cite>They</cite> first transfer a sentence to one pivot language and use the encoding of the sentence in the pivot language to train the generative models corresponding to the two styles.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_6",
  "x": "<cite>Prabhumoye et al. (2018)</cite> introduces the technique of back-translation to perform style transfer. <cite>They</cite> first transfer a sentence to one pivot language and use the encoding of the sentence in the pivot language to train the generative models corresponding to the two styles. <cite>They</cite> also use feedback from a pre-trained classifier to guide the generators to generate the desired style.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_7",
  "x": "This model is denoted as <cite>Back-translated Style Transfer</cite> (<cite>BST</cite>) in the future. Grounding meaning with multilingual backtranslation. Johnson et al. (2017) showed that multi-lingual neural machine translation systems using one-to-many and many-to-one frameworks can perform zero-shot learning.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_8",
  "x": "We also train a many to one translation system (Johnson et al., 2017) where we have a encoder-decoder network for two source languages and one target language. We use these translation systems for training the style specific decoders following the procedure in (<cite>Prabhumoye et al., 2018</cite>) . Specifically, a sentence is first translated from English to two pivot languages.",
  "y": "uses"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_9",
  "x": "**STYLE TRANSFER TASKS** We use three tasks described in (<cite>Prabhumoye et al., 2018</cite>) to evaluate our models. The three tasks correspond to: (1) gender transfer: we transfer the style of writing reviews of Yelp from male and female authors (Reddy and Knight, 2016) .",
  "y": "uses"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_11",
  "x": "Style Transfer Accuracy. We measure the accuracy of style transfer as described in (Shen et al., 2017) . We have reproduced the classifiers described in (<cite>Prabhumoye et al., 2018</cite>) .",
  "y": "uses"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_12",
  "x": "Meaning Preservation. We follow the procedure described in (Bennett, 2005) to perform A/B testing. We reuse the instructions provided by (<cite>Prabhumoye et al., 2018</cite>) for the three tasks. But unlike (<cite>Prabhumoye et al., 2018</cite>), we perform our evaluation on Amazon Mechanical Turk.",
  "y": "differences uses"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_14",
  "x": "As reported by <cite>them</cite>, the <cite>BST</cite> model performs better in preservation of meaning for the tasks of gender and political slant transfer. We present the results for the comparison between <cite>BST</cite> and MBST models; and the MBST and the MBST+F models. Fluency.",
  "y": "background"
 },
 {
  "id": "28eeecadd8d3348de6daec3c801ae4_16",
  "x": "The over-all averaged scores for the two models MBST and MBST+F is the same 3.08, whereas it is much lower 2.79 for <cite>BST</cite> and 2.57 for CAE. Table 4 : Fluency in generated sentences. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "2915e49791d14f5b802225d10f33fb_0",
  "x": "Machine Learning methods have been widely applied for sentiment analysis (Pang et al. 2008;<cite> Pang et al. 2002</cite>; Tan et al. 2008 ). Pang and Lee (2004) experimented with various features like unigrams, bi-grams and adjectives for sentiment classification of movie reviews using different machine learning algorithms namely Na\u00efve Bayes (NB), Support Vector Machines (SVM), and Maximum-Entropy (ME). Feature selection methods improve the performance of sentiment classification by eliminating the noisy and irrelevant features from feature vector.",
  "y": "background"
 },
 {
  "id": "2915e49791d14f5b802225d10f33fb_1",
  "x": "Documents are initially pre-processed as follows: (i) Negation handling is performed as<cite> Pang et al. (2002)</cite> , \"NOT_\" is added to every words occurring after the negation word (no, not, isn't, can't, never, couldn't, didn't, wouldn't, don't) and first punctuation mark in the sentence. (ii) Words occurring in less than 3 documents are removed from the feature set. Binary weighting scheme has been identified as a better weighting scheme as compared to frequency based schemes for sentiment classification<cite> (Pang et al. 2002)</cite> ; therefore we also used binary weighting method for representing text.",
  "y": "uses similarities"
 },
 {
  "id": "2915e49791d14f5b802225d10f33fb_2",
  "x": "(ii) Words occurring in less than 3 documents are removed from the feature set. Binary weighting scheme has been identified as a better weighting scheme as compared to frequency based schemes for sentiment classification<cite> (Pang et al. 2002)</cite> ; therefore we also used binary weighting method for representing text. In addition, there is no need of using separate discretisation method in case of binary weighting scheme as required by RSAR feature selection algorithm.",
  "y": "motivation background"
 },
 {
  "id": "2915e49791d14f5b802225d10f33fb_3",
  "x": "Support Vector Machine (SVM) and Na\u00efve Bayes (NB) classifiers are the mostly used for sentiment classification <cite>(Pang et al. 2002</cite>; Tan et al. 2008) . Therefore, we report the classification results of SVM and NB classifier for classifying review documents into positive or negative sentiment polarity. For the evaluation of proposed methods 10 fold cross validation method is used.",
  "y": "background"
 },
 {
  "id": "2abfa447cea31af26d06d4325c94ac_0",
  "x": "Recent successes in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees (Collins, 1999; Charniak, 2000;<cite> Henderson, 2003)</cite> have brought the hope that the same approach could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence. Moving towards a shallow semantic level of representation has immediate applications in question-answering and information extraction. For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to New York will need to know that from Geneva indicates the origin of the flight and to New York the destination.",
  "y": "background"
 },
 {
  "id": "2abfa447cea31af26d06d4325c94ac_1",
  "x": "We present work to test the hypothesis that a current statistical parser <cite>(Henderson, 2003)</cite> can output rich information comprising both a parse tree and semantic role labels robustly, that is without any significant degradation of the parser's accuracy on the original parsing task. We achieve promising results both on the simple parsing task, where the accuracy of the parser is measured on the standard Parseval measures, and also on the parsing task where more complex labels comprising both syntactic labels and semantic roles are taken into account. These results have several consequences.",
  "y": "uses"
 },
 {
  "id": "2abfa447cea31af26d06d4325c94ac_2",
  "x": "Arguments receiving labels A0-A5 or AA do not express consistent semantic roles and are specific to a verb, while arguments receiving an AM-X label are supposed to be adjuncts, and the roles they express are consistent across all verbs. To achieve the complex task of assigning semantic role labels while parsing, we use a family of state-of-the-art history-based statistical parsers, the Simple Synchrony Network (SSN) parsers <cite>(Henderson, 2003)</cite> , which use a form of left-corner parse strategy to map parse trees to sequences of derivation steps. These parsers do not impose any a priori independence assumptions, but instead smooth their parameters by means of the novel SSN neural network architecture.",
  "y": "uses"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_0",
  "x": "This definition of stance is used in several stance detection studies. For instance, in studies performed on the text genres web debate forums (Somasundaran and Wiebe, 2010; Anand et al., 2011; Walker et al., 2012; Hasan and Ng, 2013) , news paper text (Ferreira and Vlachos, 2016; Fake News Challenge, 2017) and tweets (Augenstein et al., 2016;<cite> Mohammad et al., 2017)</cite> . Stance detection is generally considered more difficult than sentiment analysis and thereby a task for which currently available methods achieve lower results.",
  "y": "background"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_1",
  "x": "Stance detection is generally considered more difficult than sentiment analysis and thereby a task for which currently available methods achieve lower results. This was, for instance, shown by a recent shared task on three-category stance classification of tweets, where an F-score of 0.59 was achieved by a classifier that outperformed submissions from 19 shared task teams <cite>(Mohammad et al., 2017)</cite> . For the task of stance classification of posts of two-sided discussion threads, an F-score of 0.70 is the best result we have been able to find in previous research (Hasan and Ng, 2013)",
  "y": "background"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_2",
  "x": "The 1,190 posts to include in the experiment were presented for manual classification in a random order, without revealing who the debater was or which thread the post belonged to. The annotation was performed by one of the authors of the paper. Following the principle of the guidelines by<cite> Mohammad et al. (2017)</cite> , we classified the posts as taking a stance against or for vaccination, or to be undecided.",
  "y": "similarities uses"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_3",
  "x": "This follows the approach of<cite> Mohammad et al. (2017)</cite> , as well as of many of the previously performed vaccine sentiment studies. The model was trained on all tokens in the training data, as well as on 2-, 3-and 4-grams that occurred at least twice in the data. The standard NLTK stop word list for English was used for removing non-content words when constructing one set of n-grams.",
  "y": "similarities uses"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_4",
  "x": "Our decision, to classify debate posts as against vaccination when they opposed an official vaccination policy, was based on that debaters often implicitly argue against such a policy. In addition, a system for surveilling increases in vaccine hesitancy is likely to take an official policy as its point of departure. The eight annotators that classified each tweet in the study by<cite> Mohammad et al. (2017)</cite> were employed through a crowdsourcing platform, which was made possible by that the stance targets were chosen with the criterion that they should be commonly known in the United States.",
  "y": "background"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_5",
  "x": "---------------------------------- **MACHINE LEARNING DECISIONS** The choice of machine learning model was primarily based on that a linear support vector machine was successful on data from the previously mentioned shared task of stance detection of tweets <cite>(Mohammad et al., 2017)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "2adb3a645a57b8f441a80bb5a46045_6",
  "x": "For instance, features constructed using an arguing lexicon (Somasun-daran and Wiebe, 2010), or word embeddings constructed in an unsupervised fashion using a large corpus from the same text genre as the text to classify <cite>(Mohammad et al., 2017)</cite> . Apart from making a decision on what type of classifier and features to use, it must also be decided on how to gather more training data. Strategies for reducing the manual labelling effort should be investigated, in particular since the annotation task, as discussed above, might not be suitable for crowdsourcing.",
  "y": "similarities"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_0",
  "x": "However, RNNs do have some drawbacks, of which the most relevant to real-world applications is the high number of sequential operations, which increases the processing time of both learning and inference. To address these limitations, Vaswani et al. have proposed the Transformer, a machine translation model that introduces a new deep learning architecture solely based on \"attention\" mechanisms <cite>[2]</cite> . We later clarify the meaning of attention in this context.",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_1",
  "x": "To address these limitations, Vaswani et al. have proposed the Transformer, a machine translation model that introduces a new deep learning architecture solely based on \"attention\" mechanisms <cite>[2]</cite> . We later clarify the meaning of attention in this context. Inspired by the positive results of Vaswani et al. in machine translation, we have applied a similar architecture to the domain of question-answering, a model that we have named Fully Attention-Based Information Retriever (FABIR).",
  "y": "extends"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_2",
  "x": "Besides the development of a new architecture, we identify three major contributions of our work that have made these results possible: \u2022 Convolutional attention: a novel attention mechanism that encodes many-to-many relationships between words, enabling richer contextual representations. \u2022 Reduction layer: a new layer design that fits the pipeline proposed by Vaswani et al. <cite>[2]</cite> and compresses the input embedding size for subsequent layers (this is especially beneficial when employing pre-trained embeddings).",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_3",
  "x": "\u2022 Convolutional attention: a novel attention mechanism that encodes many-to-many relationships between words, enabling richer contextual representations. \u2022 Reduction layer: a new layer design that fits the pipeline proposed by Vaswani et al. <cite>[2]</cite> and compresses the input embedding size for subsequent layers (this is especially beneficial when employing pre-trained embeddings). \u2022 Column-wise cross-attention: we modify the crossattention operation by <cite>[2]</cite> and propose a new technique that is better suited to question-answering.",
  "y": "extends"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_4",
  "x": "Vaswani et al. were the first to apply attention directly over the word-embeddings, and thus derived a new neural network architecture which, without any recurrence, achieved state-ofthe-art results in machine translation <cite>[2]</cite> . In this section, we briefly discuss both types of attention models. ----------------------------------",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_5",
  "x": "**A. TRADITIONAL ATTENTION MECHANISMS** In recent years, attention mechanisms have been used with success in a variety of NLP tasks, such as machine translation <cite>[2]</cite> , [27] and natural language inference [28] , [29] . Indeed, most models that target the SQuAD dataset use some form of attention to model the relationship between question and passage.",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_6",
  "x": "The Transformer is a machine translation model introduced in <cite>[2]</cite> that achieved state-of-the-art results by combining feedforward neural networks with a multiplicative attention mechanism applied over position-encoded embedding vectors. It defines three different matrices U, K and V that are associated with queries, keys, and values, respectively. Every attention operation in the Transformer is performed by multiplying these matrices as shown in (4) .",
  "y": "uses background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_7",
  "x": "Every attention operation in the Transformer is performed by multiplying these matrices as shown in (4) . where W U K , W V \u2208 R d model \u00d7d model are weight matrices and d model is the embedding size of each word. Additionally, Vaswani et al. <cite>[2]</cite> suggest a multi-head attention, in which U, K and V are divided into n heads heads and the attention in the i th head is computed as",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_8",
  "x": "where W O \u2208 R n heads * d head \u00d7d model . If one wants to model the interdependence of words within a single piece of text, U, K and V are all equal and consist of the text of interest embedded in some vectorial space. This type of attention is often called \"self-attention\" or \"self-alignment\" <cite>[2]</cite> , [21] .",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_9",
  "x": "We also identified another QA model [32] that is inspired by the architecture introduced by Vaswani et al. <cite>[2]</cite> . Their model differs from ours in that it heavily relies on convolutions (46 layers against 2 in FABIR), which approximates it to other CNN NLP models [33] , rather than purely attention based models. Although they report high F1 and EM scores (82.7% and 73.3%), our model is almost twice as fast in inference (259 samples/s against 440 in FABIR).",
  "y": "extends"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_10",
  "x": "In this section, we present FABIR's architecture and the main design decisions we have made to develop a lighter and faster question-answering model. In particular, we introduce the convolutional attention, the column-wise cross-attention, and the reduction layer, which build on the Transformer model <cite>[2]</cite> to enable its application to question-answering. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_11",
  "x": "In contrast to an RNN, FABIR does not process words in sequence, and hence needs to model the position of each word in a sentence differently. We add positional information to each word embedding using a trigonometric encoder as proposed in <cite>[2]</cite> . Therefore, given a sequence of embedding vectors of even size d model , the position of the i th word is encoded in a vector e i as follows:",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_12",
  "x": "We add positional information to each word embedding using a trigonometric encoder as proposed in <cite>[2]</cite> . Therefore, given a sequence of embedding vectors of even size d model , the position of the i th word is encoded in a vector e i as follows: where f k are scalars, which were chosen according to <cite>[2]</cite> .",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_13",
  "x": "where d model is the size of each position encoding, which is not necessarily equal to d input . The encoding E can be summed to \u2126 to include the information of the position of each word in the text. Indeed, in <cite>[2]</cite> , the final vectorial representation of a piece of text is defined by the sum of the embeddings \u2126 with the position encoding E, which would require d model = d input .",
  "y": "background"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_14",
  "x": "The encoding E can be summed to \u2126 to include the information of the position of each word in the text. Indeed, in <cite>[2]</cite> , the final vectorial representation of a piece of text is defined by the sum of the embeddings \u2126 with the position encoding E, which would require d model = d input . However, we introduce a layer that processes embeddings and encodings separately before summing them up.",
  "y": "differences"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_15",
  "x": "In FABIR the attention mechanism is inspired by the Transformer model introduced in <cite>[2]</cite> . However, we hypothesize the word-to-word relationship in (1a) fails to capture the complexity of expressions involving groups of words. To facilitate the modeling of the interdependence of surrounding words, we redefine s i,j as",
  "y": "extends"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_16",
  "x": "2) Column-wise Cross-attention: Cross-attention (att cross ) differs from other types of attention by relating two different pieces of text. Given P and Q, cross-attention of Q over P is defined as In contrast to Vaswani et al. <cite>[2]</cite> , where the softmax in (12d) is applied in a row-wise manner, we suggest column-wise cross-attention.",
  "y": "differences"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_17",
  "x": "However, in FABIR we have observed better results when only the former is used. 3) Feedforward: The feedforward sublayer is solely composed of a neural network with a single hidden layer, which is applied vector-wise. Following the architecture suggested by Vaswani et al. <cite>[2]</cite> , the feedforward sublayer is implemented in (15) with a two-layer neural network:",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_18",
  "x": "Nonetheless, we observed that the new architecture introduced by Vaswani et al. <cite>[2]</cite> is more susceptible to overfitting than RNNs when presented with large embedding sizes. Hence, we needed a method to compress the word representations, and thus facilitate and speed up training by reducing the number of parameters. A straightforward method to reduce the input embedding size is to multiply it by a matrix with the required dimensions:",
  "y": "motivation"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_19",
  "x": "We pre-processed the texts with the NLTK Tokenizer [40] . As suggested in <cite>[2]</cite> , we have chosen the Adam optimizer [41] with the same hyperparameters, except for the learning rate, which was divided by two in our implementation. For regularization, we applied residual and attention dropout <cite>[2]</cite> of 0.9 in processing layers and of 0.8 in the reduction layer.",
  "y": "differences uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_20",
  "x": "We pre-processed the texts with the NLTK Tokenizer [40] . As suggested in <cite>[2]</cite> , we have chosen the Adam optimizer [41] with the same hyperparameters, except for the learning rate, which was divided by two in our implementation. For regularization, we applied residual and attention dropout <cite>[2]</cite> of 0.9 in processing layers and of 0.8 in the reduction layer.",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_21",
  "x": "This analysis confirms the effectiveness of char-embeddings, as its addition increased the F1 and EM scores, by 2.7% and 3.1%, respectively. Most importantly, when the convolutional attention was replaced by the standard attention mechanism proposed in <cite>[2]</cite> , the performance dropped by 2.4% in F1 and 2.5% in EM. That validates the contribution of this new attention method in building elaborate contextual representations.",
  "y": "uses"
 },
 {
  "id": "2b25e38db7fd20c92d677b73af110c_22",
  "x": "FABIR also brings three significant contributions to this new class of neural network architectures. The convolutional attention, the reduction layer, and the column-wise crossattention individually increased the model's F1 and EM scores by more than 2%. Moreover, being thoroughly compatible with the Transformer <cite>[2]</cite> , these new mechanisms are valuable assets to further developments in attention models.",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_0",
  "x": "**INTRODUCTION** There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> . Neural network models have been exploited due to their strength in non-sparse representation learning and non-linear power in feature combination, which have led to advances in many NLP tasks.",
  "y": "background"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_1",
  "x": "In addition to character embeddings, distributed representations of character bigrams Pei et al., 2014) and words (Morita et al., 2015;<cite> Zhang et al., 2016b)</cite> have also been shown to improve segmentation accuracies. With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a) , as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> . For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> .",
  "y": "background"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_2",
  "x": "\u89d2(corner)\" (Zheng et al., 2013) , which is infeasible by using sparse one-hot character features. In addition to character embeddings, distributed representations of character bigrams Pei et al., 2014) and words (Morita et al., 2015;<cite> Zhang et al., 2016b)</cite> have also been shown to improve segmentation accuracies. With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a) , as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> .",
  "y": "background"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_3",
  "x": "In addition to character embeddings, distributed representations of character bigrams Pei et al., 2014) and words (Morita et al., 2015;<cite> Zhang et al., 2016b)</cite> have also been shown to improve segmentation accuracies. With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a) , as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> . For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016;<cite> Zhang et al., 2016b)</cite> .",
  "y": "background"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_4",
  "x": "To our knowledge, such rich external information has not been systematically investigated for neural segmentation. We fill this gap by investigating rich external pretraining for neural segmentation. Following Cai and Zhao (2016) and<cite> Zhang et al. (2016b)</cite> , we adopt a globally optimised beam-search framework for neural structured prediction (Andor et al., 2016; Zhou et al., 2015; Wiseman and Rush, 2016) , which allows word information to be modelled explicitly.",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_5",
  "x": "Our results show a similar degree of error reduction compared to theirs by using external data. Our model inherits from previous findings on context representations, such as character windows Pei et al., 2014; Chen et al., 2015a) and LSTMs (Chen et al., 2015b; Xu and Sun, 2016) . Similar to<cite> Zhang et al. (2016b)</cite> and Cai and Zhao (2016) , we use word context on top of character context.",
  "y": "similarities"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_6",
  "x": "Similar to<cite> Zhang et al. (2016b)</cite> and Cai and Zhao (2016) , we use word context on top of character context. However, words play a relatively less important role in our model, and we find that word LSTM, which has been used by all previous neural segmentation work, is unnecessary for our model. Our model is conceptually simpler and more modularised compared with Figure 1 : Overall model.",
  "y": "similarities differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_7",
  "x": "In the figure, V denotes the score of a state, given by a neural network model. The score of the initial state (i.e. axiom) is 0, and the score of a non-axiom state is the sum of scores of all incremental decisions resulting in the state. Similar to<cite> Zhang et al. (2016b)</cite> and Cai and Zhao (2016) , our model is a global structural model, using the overall score to disambiguate states, which correspond to sequences of inter-dependent transition actions.",
  "y": "similarities"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_8",
  "x": "For the former, we follow prior methods (Xue et al., 2003; Pei et al., 2014) , using five-character window [c \u22122 , c \u22121 , c 0 , c 1 , c 2 ] to represent incoming characters. Shown in Figure 3 , a multi-layer perceptron (MLP) is employed to derive a five-character window vector D C from single-character vector rep- For the latter, we follow recent work (Chen et al., 2015b;<cite> Zhang et al., 2016b)</cite> , using a bidirectional LSTM to encode input character sequence.",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_9",
  "x": "A hidden layer is employed to derive a two-word vector X W from single word embeddings e w (w \u22122 ) and e w (w \u22121 ). For the latter, we follow<cite> Zhang et al. (2016b)</cite> and Cai and Zhao (2016) , using an uni-directional LSTM on words that have been recognized. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_10",
  "x": "We use Adagrad (Duchi et al., 2011) to optimize model parameters, with an initial learning rate \u03b1. L2 regularization and dropout (Srivastava et al., 2014) on input are used to reduce overfitting, with a L2 weight \u03bb and a dropout rate p. All the parameters in our model are randomly initialized to a value (\u2212r, r), where r = 6.0 f an in +f anout (Bengio, 2012). We fine-tune character and character bigram embeddings, but not word embeddings, acccording to<cite> Zhang et al. (2016b)</cite> .",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_11",
  "x": "Using a context of only w \u22121 (1-word window), the F-measure increases to 95.78%. This shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word-based segmentors <cite>(Zhang et al., 2016b</cite>; Cai and Zhao, 2016) . This is likely due to the difference in our neural network structures, and that we fine-tune both character and character bigram embeddings, which significantly enlarges the adjustable parameter space as compared with<cite> Zhang et al. (2016b)</cite> .",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_12",
  "x": "This is likely due to the difference in our neural network structures, and that we fine-tune both character and character bigram embeddings, which significantly enlarges the adjustable parameter space as compared with<cite> Zhang et al. (2016b)</cite> . The fact that word contexts can contribute relatively less than characters in a word is also not surprising in the sense that word-based neural segmentors do not outperform the best character-based models by large margins. Given that character context is what we pretrain, our model relies more heavily With both w \u22122 and w \u22121 being used for the context, the F-score further increases to 95.86%, showing that a 2-word window is useful by offering more contextual information.",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_13",
  "x": "Finally, by integrating all above information via multi-task learning, the F-score is further improved to 96.48%, with a 15.0% relative error reduction. Zhang et al. (2016b) Both our model and<cite> Zhang et al. (2016b)</cite> use global learning and beam search, but our network is different. Zhang et al. (2016b) utilizes the action history with LSTM encoder, while we use partial word rather than action information.",
  "y": "similarities differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_14",
  "x": "Zhang et al. (2016b) utilizes the action history with LSTM encoder, while we use partial word rather than action information. Besides, the character and character bigram embeddings are fine-tuned in our model while<cite> Zhang et al. (2016b)</cite> set the embeddings fixed during training. We study the F-measure distribution with respect to sentence length on our baseline model, multitask pretraining model and<cite> Zhang et al. (2016b)</cite> .",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_15",
  "x": "We study the F-measure distribution with respect to sentence length on our baseline model, multitask pretraining model and<cite> Zhang et al. (2016b)</cite> . ---------------------------------- **PRETRAINING RESULTS**",
  "y": "uses"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_16",
  "x": "As shown in Figure 5 , the models give different error distributions, with our models being more robust to the sentence length compared with<cite> Zhang et al. (2016b)</cite> . Their model is better on very short sentences, but worse on all other cases. This shows the relative advantages of our model.",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_17",
  "x": "Our final results on CTB6 are shown in Table 7 , which lists the results of several current state-ofthe-art methods. Without multitask pretraining, our model gives an F-score of 95.44%, which is higher than the neural segmentor of<cite> Zhang et al. (2016b)</cite> , which gives the best accuracies among pure neural segments on this dataset. By using multitask pretraining, the result increases to 96.21%, with a relative error reduction of 16.9%.",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_18",
  "x": "Our final results compare favourably to the best statistical models, including those using semisupervised learning (Sun and Xu, 2011; Wang et al., 2011) , and those leveraging joint POS and syntactic information (Zhang et al., 2014) . In addition, it also outperforms the best neural models, in particular<cite> Zhang et al. (2016b)</cite> Table 7 : Main results on CTB6. ual discrete features into their word-based neural model.",
  "y": "differences"
 },
 {
  "id": "2b5d8cab263c9edbe005674910a7b1_19",
  "x": "Similar to Table 7 , our method gives the best accuracies on all corpora except for MSR, where it underperforms the hybrid model of<cite> Zhang et al. (2016b)</cite> by 0.2%. To our knowledge, we are the first to report results for a neural segmentor on more than 3 datasets, with competitive results consistently. It verifies that knowledge learned from a certain set of resources can be used to enhance cross-domain robustness in training a neural segmentor for different datasets, which is of practical importance.",
  "y": "differences"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_0",
  "x": "Such agents may act as mediators or can be helpful for pedagogical purposes (Johnson et al., 2019) . Efforts in agent-human negotiations involving free-form natural language as a means of communication are rather sparse. Researchers<cite> (He et al., 2018)</cite> recently studied natural language negotiations in buyer-seller bargaining setup, which is comparatively less restricted than previously studied game environments (Asher et al., 2016; Lewis et al., 2017) .",
  "y": "background"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_1",
  "x": "Researchers<cite> (He et al., 2018)</cite> recently studied natural language negotiations in buyer-seller bargaining setup, which is comparatively less restricted than previously studied game environments (Asher et al., 2016; Lewis et al., 2017) . Lack of a well-defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario. Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work.",
  "y": "background motivation"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_2",
  "x": "Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work. We focus on buyer-seller negotiations<cite> (He et al., 2018)</cite> where two individuals negotiate the price of a given product. Leveraging the recent advancements (Vaswani et al., 2017; Devlin et al., 2019) in pre-trained language encoders, we attempt to predict negotiation outcomes early on in the conversation, in a completely data-driven manner ( Figure  1 ).",
  "y": "uses"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_3",
  "x": "Our evaluations show that natural language allows the models to make better predictions by looking at only a fraction of the negotiation. Rather than just realizing the strategy in natural language, our empirical results suggest that language can be crucial in the planning as well. We provide a sample negotiation from the test set<cite> (He et al., 2018</cite> ) along with our model predictions in Table 1 .",
  "y": "uses"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_4",
  "x": "Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by<cite> He et al. (2018)</cite> . Instead of focusing on the previously studied game environments (Asher et al., 2016; Lewis et al., 2017) , the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist 1 . The dataset consists of 6682 dialogues between a buyer and a seller who converse in natural language to negotiate the price of a given product (sample in Table 1 ).",
  "y": "uses"
 },
 {
  "id": "2b7ba7f7aa2a03ad0de84e007c1f64_5",
  "x": "By constructing artificial negotiations, we observe that the model predictions at f =0.2 increase when the buyer shows more interest in the product, indicating more willingness to pay. With the capability to incorporate cues from natural language, such a framework can be used in the future to get negotiation feedback, in order to guide the planning of a negotiating agent. This can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning (Lewis et al., 2017; <cite>He et al., 2018)</cite> .",
  "y": "future_work"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_0",
  "x": "Much work that followed improved upon this strategy, by improving the features (Ng and Cardie, 2002b) , the type of classifier<cite> (Denis and Baldridge, 2007)</cite> , and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b) . This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. Ng and Cardie (2002a) and Ng (2004) highlight the problem of determining whether or not common noun phrases are anaphoric.",
  "y": "background"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_1",
  "x": "They use two classifiers, an anaphoricity classifier, which decides if a mention should have an antecedent and a pairwise classifier similar those just discussed, which are combined in a cascaded manner. More recently,<cite> Denis and Baldridge (2007)</cite> utilized an integer linear programming (ILP) solver to better combine the decisions made by these two complementary classifiers, by finding the globally optimal solution according to both classifiers. However, when encoding constraints into their ILP solver, they did not enforce transitivity.",
  "y": "background"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_2",
  "x": "For this task we are given a document which is annotated with a set of mentions, and the goal is to cluster the mentions which refer to the same entity. When describing our model, we build upon the notation used by<cite> Denis and Baldridge (2007)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_3",
  "x": "(2) where m is the set of mentions in the document, and x is the set of variables representing each pairwise coreference decision x i,j . Note that this model is degenerate, because it assigns probability mass to nonsensical clusterings. Specifically, it will allow Prior work (Soon et al., 2001; <cite>Denis and Baldridge, 2007)</cite> has generated training data for pairwise classifiers in the following manner.",
  "y": "background"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_4",
  "x": "Create negative examples for all intermediate mentions, and a positive example for the mention and its correct antecedent. This approach made sense for Soon et al. (2001) because testing proceeded in a similar manner: for each mention, work backwards until you find a previous mention which the classifier thinks is coreferent, add a link, and terminate the search. The COREF-ILP model of<cite> Denis and Baldridge (2007)</cite> took a different approach at test time: for each mention they would work backwards and add a link for all previous mentions which the classifier deemed coreferent.",
  "y": "background"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_5",
  "x": "Our D&B-STYLE baseline used the same test time method as<cite> Denis and Baldridge (2007)</cite> , however at training time we created data for all mention pairs. ---------------------------------- **INTEGER LINEAR PROGRAMMING TO ENFORCE TRANSITIVITY**",
  "y": "uses"
 },
 {
  "id": "2bb115f1c3e753e9dc66735887a52d_6",
  "x": "We show performance for both baseline classifiers, as well as our ILP-based classifier, which finds the most probable legal assignment to the variables representing coreference decisions over pairs of mentions. For comparison, we also give the results of the COREF-ILP system of<cite> Denis and Baldridge (2007)</cite> , which was also based on a na\u00efve pairwise classifier. They used an ILP solver to find an assignment for the variables, but as they note at the end of Section 5.1, it is equivalent to taking all links for which the classifier returns a probability \u2265 0.5, and so the ILP solver is not really necessary.",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_0",
  "x": "This task is a rather easier task which prepare a pair of complex and simple representations than a challenging task which changes the substitute pair in a given context (Specia et al., 2012; <cite>Kajiwara and Yamamoto, 2015</cite>) . Construction of a benchmark dataset is important to ensure the reliability and reproducibility of evaluation. However, few resources are available for the automatic evaluation of lexical simplification.",
  "y": "uses differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_1",
  "x": "<cite>Kajiwara and Yamamoto (2015)</cite> constructed an evaluation dataset for Japanese lexical simplification 1 in languages other than English. However, there are four drawbacks in the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> : (1) <cite>they</cite> extracted sentences only from a newswire corpus; (2) <cite>they</cite> substituted only a single target word; (3) <cite>they</cite> did not allow ties; and (4) <cite>they</cite> did not integrate simplification ranking considering the quality. Hence, we propose a new dataset addressing the problems in <cite>the dataset</cite> of <cite>Kajiwara and Yamamoto (2015)</cite> .",
  "y": "background"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_2",
  "x": "<cite>Kajiwara and Yamamoto (2015)</cite> constructed an evaluation dataset for Japanese lexical simplification 1 in languages other than English. However, there are four drawbacks in the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> : (1) <cite>they</cite> extracted sentences only from a newswire corpus; (2) <cite>they</cite> substituted only a single target word; (3) <cite>they</cite> did not allow ties; and (4) <cite>they</cite> did not integrate simplification ranking considering the quality. Hence, we propose a new dataset addressing the problems in <cite>the dataset</cite> of <cite>Kajiwara and Yamamoto (2015)</cite> .",
  "y": "motivation background"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_3",
  "x": "However, there are four drawbacks in the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> : (1) <cite>they</cite> extracted sentences only from a newswire corpus; (2) <cite>they</cite> substituted only a single target word; (3) <cite>they</cite> did not allow ties; and (4) <cite>they</cite> did not integrate simplification ranking considering the quality. Hence, we propose a new dataset addressing the problems in <cite>the dataset</cite> of <cite>Kajiwara and Yamamoto (2015)</cite> . The main contributions of our study are as follows:",
  "y": "extends"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_4",
  "x": "However, there are four drawbacks in the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> : (1) <cite>they</cite> extracted sentences only from a newswire corpus; (2) <cite>they</cite> substituted only a single target word; (3) <cite>they</cite> did not allow ties; and (4) <cite>they</cite> did not integrate simplification ranking considering the quality. Hence, we propose a new dataset addressing the problems in <cite>the dataset</cite> of <cite>Kajiwara and Yamamoto (2015)</cite> . The main contributions of our study are as follows:",
  "y": "motivation extends"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_5",
  "x": "**RELATED WORK** The evaluation dataset for the English Lexical Simplification task (Specia et al., 2012) Figure 1: A part of the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . notated on top of the evaluation dataset for English lexical substitution (McCarthy and Navigli, 2007) .",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_6",
  "x": "3 Problems in previous datasets for Japanese lexical simplification <cite>Kajiwara and Yamamoto (2015)</cite> followed Specia et al. (2012) to construct an evaluation dataset for Japanese lexical simplification. Namely, <cite>they</cite> split the data creation process into two steps: substitute extraction and simplification ranking. During the substitute extraction task, <cite>they</cite> collected substitutes of each target word in 10 different contexts.",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_7",
  "x": "3 Problems in previous datasets for Japanese lexical simplification <cite>Kajiwara and Yamamoto (2015)</cite> followed Specia et al. (2012) to construct an evaluation dataset for Japanese lexical simplification. Namely, <cite>they</cite> split the data creation process into two steps: substitute extraction and simplification ranking. During the substitute extraction task, <cite>they</cite> collected substitutes of each target word in 10 different contexts.",
  "y": "motivation"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_8",
  "x": "The target word was a content word (noun, verb, adjective, or adverb) , and was neither a simple word nor part of any compound words. <cite>They</cite> gathered substitutes from five annotators using crowdsourcing. These procedures were the same as for De Belder and Moens (2012) .",
  "y": "background"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_9",
  "x": "These procedures were the same as for De Belder and Moens (2012) . During the simplification ranking task, annotators were asked to reorder the target word and its substitutes in a single order without allowing ties. <cite>They</cite> used crowdsourcing to find five annotators different from those who performed the substitute extraction task.",
  "y": "background"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_10",
  "x": "Thus, there was a big blur between annotators, and the simplification ranking collected using crowdsourcing tended to have a lower quality. Figure 1 shows a part of the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . Our discussion in this paper is based on this example.",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_11",
  "x": "Figure 1 shows a part of the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . Our discussion in this paper is based on this example. Domain of the dataset is limited.",
  "y": "motivation"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_12",
  "x": "Because <cite>Kajiwara and Yamamoto (2015)</cite> extracted sentences from a newswire corpus, <cite>their</cite> dataset has a poor variety of expression. English lexical simplification datasets (Specia et al., 2012; De Belder and Moens, 2012) do not have this problem because both of them use a balanced corpus of English (Sharoff, 2006) . Complex words might exist in context.",
  "y": "motivation background"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_13",
  "x": "Ties are not permitted in simplification ranking. When each annotator assigns a simplification ranking to a substitution list, a tie cannot be assigned in previous datasets (Specia et al., 2012; <cite>Kajiwara and Yamamoto, 2015</cite>) . This deteriorates ranking consistency if some substitutes have a similar simplicity.",
  "y": "differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_14",
  "x": "<cite>Kajiwara and Yamamoto (2015)</cite> and Specia et al. (2012) use an average score to integrate rankings, but it might be biased by outliers. De Belder and Moens (2012) report a slight increase in agreement by greedily removing annotators to maximize the agreement score. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_15",
  "x": "Figure 2 illustrates how we constructed the dataset. It follows the data creation procedure of <cite>Kajiwara and Yamamoto's (2015)</cite> dataset with improvements to resolve the problems described in Section 3. We use a crowdsourcing application, Lancers, 3 3 http://www.lancers.jp/ Figure 3 : Example of annotation of extracting substitutes.",
  "y": "extends uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_16",
  "x": "It is about the same size as previous work (Specia et al., 2012; <cite>Kajiwara and Yamamoto, 2015</cite>) . Our dataset has two advantages: (1) improved correlation with human judgment by making a controlled and balanced dataset, and (2) enhanced consistency by allowing ties in ranking and removing outlier annotators. In the following subsections, we evaluate our dataset in detail.",
  "y": "similarities"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_17",
  "x": "The baseline integration ranking used an average score (<cite>Kajiwara and Yamamoto, 2015</cite>) . Our proposed method excludes outlier annotators by using a reliability score calculated using the method developed by Matsui et al. (2014) . Pairwise agreement is calculated between each pair of sets (p 1 , p 2 \u2208 P ) from all the possible pairings (P) (Equation 1).",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_18",
  "x": "the Spearman rank correlation coefficient of the substitute ranking phase was 0.522. This score is higher than that from <cite>Kajiwara and Yamamoto (2015)</cite> by 0.190. This clearly shows the importance of allowing ties during the substitute ranking task.",
  "y": "differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_19",
  "x": "Our method achieved better accuracy in ranking integration than previous methods (Specia et al., 2012; <cite>Kajiwara and Yamamoto, 2015</cite>) and is similar to the results from De Belder and Moens (2012) . This shows that the reliability score can be used for improving the quality. Table 3 shows the number of sentences and average substitutes in each genre.",
  "y": "differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_20",
  "x": "**EXTRINSIC EVALUATION** In this section, we evaluate our dataset using five simple lexical simplification methods. We calcu- late 1-best accuracy in our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_21",
  "x": "In this section, we evaluate our dataset using five simple lexical simplification methods. We calcu- late 1-best accuracy in our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . Annotated data is collected by our and <cite>Kajiwara and Yamamoto (2015)</cite>'s work in ranking substitutes task, and which size is 21,700 ((2010 + 2330) 5) rankings.",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_22",
  "x": "Annotated data is collected by our and <cite>Kajiwara and Yamamoto (2015)</cite>'s work in ranking substitutes task, and which size is 21,700 ((2010 + 2330) 5) rankings. Then, we calculate correlation between the accuracies of annotated data and either those of <cite>Kajiwara and Yamamoto (2015)</cite> or those of our dataset. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_23",
  "x": "The Pearson coefficient shows that our dataset correlates with human annotation better than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> , possibly because we controlled each sentence to include only one complex word. Because our dataset is balanced, the accuracy of Web corpus-based metrics (Frequency and Number of Users) closer than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_24",
  "x": "Because our dataset is balanced, the accuracy of Web corpus-based metrics (Frequency and Number of Users) closer than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> . ---------------------------------- **CONCLUSION**",
  "y": "differences"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_25",
  "x": "Finally, to compare two datasets, we used the Pearson product-moment correlation coefficient between our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> against the annotated data. Table 4 shows the result of this experiment. The Pearson coefficient shows that our dataset correlates with human annotation better than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> , possibly because we controlled each sentence to include only one complex word.",
  "y": "uses"
 },
 {
  "id": "2d3ec2e77947cb23af773926ec917b_26",
  "x": "Finally, to compare two datasets, we used the Pearson product-moment correlation coefficient between our dataset and the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> against the annotated data. The Pearson coefficient shows that our dataset correlates with human annotation better than the dataset of <cite>Kajiwara and Yamamoto (2015)</cite> , possibly because we controlled each sentence to include only one complex word.",
  "y": "extends differences"
 },
 {
  "id": "2e636754342e9bb857068922519dbc_0",
  "x": "More specifically, these techniques enhanced NLP algorithms through the use of contextualized text embeddings at word, sentence, and paragraph levels (Mikolov et al., 2013; Le and Mikolov, 2014; Peters et al., 2017; <cite>Devlin et al., 2018</cite>; Logeswaran and Lee, 2018; Radford et al., 2018) . More recently, Jin and Szolovits (2018) components from PubMed abstracts. To our knowledge, that study was the first in which a deep learning framework was used to extract PIO elements from PubMed abstracts.",
  "y": "background"
 },
 {
  "id": "2e636754342e9bb857068922519dbc_1",
  "x": "In the present paper, we build a dataset of PIO elements by improving the methodology found in (Jin and Szolovits, 2018) . Furthermore, we built a multi-label PIO classifier, along with a boosting framework, based on the state of the art text embedding, BERT. This embedding model has been proven to offer a better contextualization compared to a bidirectional LSTM model<cite> (Devlin et al., 2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "2e636754342e9bb857068922519dbc_2",
  "x": "The idea behind this model is to pre-train a bidirectional representation by jointly conditioning on both left and right contexts in all layers using a transformer (Vaswani et al., 2017;<cite> Devlin et al., 2018)</cite> . Like any other language model, BERT can be pre-trained on different contexts. A contextualized representation is generally optimized for downstream NLP tasks.",
  "y": "background motivation"
 },
 {
  "id": "2e636754342e9bb857068922519dbc_3",
  "x": "The first version is based on the original BERT release<cite> (Devlin et al., 2018)</cite> . This model is pre-trained on the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia, text passages were extracted while lists were ignored.",
  "y": "uses"
 },
 {
  "id": "2e636754342e9bb857068922519dbc_4",
  "x": "The model has 12 attention layers and all texts are converted to lowercase by the tokenizer<cite> (Devlin et al., 2018)</cite> . The architecture of the model is illustrated in Figure 1 . Using this framework, we trained the model using the two pretrained embedding models described in the previous section.",
  "y": "uses"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_0",
  "x": "In the past year, the field of Natural Language Processing (NLP) has seen the rise of pretrained language models such as as ELMo (Peters et al., 2018) , ULMFiT (Howard and Ruder, 2018) and <cite>BERT</cite> (<cite>Devlin et al., 2019</cite>) . <cite>These approaches</cite> train a deep-learning language model on large volumes of unlabeled text, which is subsequently fine-tuned for particular NLP tasks. Applying <cite>these models</cite> to the General Language Understanding Evaluation (GLUE) benchmark introduced by Wang et al. (2018) has achieved the best performance to date on tasks ranging from sentiment classification to question answering (<cite>Devlin et al., 2019</cite>) .",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_1",
  "x": "In the past year, the field of Natural Language Processing (NLP) has seen the rise of pretrained language models such as as ELMo (Peters et al., 2018) , ULMFiT (Howard and Ruder, 2018) and <cite>BERT</cite> (<cite>Devlin et al., 2019</cite>) . <cite>These approaches</cite> train a deep-learning language model on large volumes of unlabeled text, which is subsequently fine-tuned for particular NLP tasks. Applying <cite>these models</cite> to the General Language Understanding Evaluation (GLUE) benchmark introduced by Wang et al. (2018) has achieved the best performance to date on tasks ranging from sentiment classification to question answering (<cite>Devlin et al., 2019</cite>) .",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_2",
  "x": "In the past year, the field of Natural Language Processing (NLP) has seen the rise of pretrained language models such as as ELMo (Peters et al., 2018) , ULMFiT (Howard and Ruder, 2018) and <cite>BERT</cite> (<cite>Devlin et al., 2019</cite>) . Applying <cite>these models</cite> to the General Language Understanding Evaluation (GLUE) benchmark introduced by Wang et al. (2018) has achieved the best performance to date on tasks ranging from sentiment classification to question answering (<cite>Devlin et al., 2019</cite>) .",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_3",
  "x": "In the past year, the field of Natural Language Processing (NLP) has seen the rise of pretrained language models such as as ELMo (Peters et al., 2018) , ULMFiT (Howard and Ruder, 2018) and <cite>BERT</cite> (<cite>Devlin et al., 2019</cite>) . The benefit of <cite>these models</cite> has also been demonstrated in specialized NLP domains.",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_5",
  "x": "Further refining this model on clinical text produced an increase in performance in medical natural language inference (Alsentzer et al. 2019) . While large pretrained models offer significantly increased performance, they come with their own constraints, as the number of parameters in the <cite>classic BERT-base model</cite> exceeds 100 million. As such, <cite>their</cite> computational cost can thus be prohibitively high at both training and prediction time (<cite>Devlin et al., 2019</cite>) .",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_6",
  "x": "While large pretrained models offer significantly increased performance, they come with their own constraints, as the number of parameters in the <cite>classic BERT-base model</cite> exceeds 100 million. As such, <cite>their</cite> computational cost can thus be prohibitively high at both training and prediction time (<cite>Devlin et al., 2019</cite>) . More recent work has addressed this challenge by 'distilling' the models, training smaller versions of <cite>BERT</cite> which reduce the number of parameters to train by 40% while retaining more than 95% of the full model performance and even outperforming it on two out of eleven GLUE tasks .",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_7",
  "x": "More recent work has addressed this challenge by 'distilling' the models, training smaller versions of <cite>BERT</cite> which reduce the number of parameters to train by 40% while retaining more than 95% of the full model performance and even outperforming it on two out of eleven GLUE tasks . This paper shows that using pretrained models in learning analytics holds great potential for advancing the field. We apply the <cite>BERT</cite> approach to the following three previously explored LAK tasks on MOOC forum data (Wei et al., 2017) : Confusion detection, urgency of teacher intervention and sentimentality classification.",
  "y": "background"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_8",
  "x": "More recent work has addressed this challenge by 'distilling' the models, training smaller versions of <cite>BERT</cite> which reduce the number of parameters to train by 40% while retaining more than 95% of the full model performance and even outperforming it on two out of eleven GLUE tasks . This paper shows that using pretrained models in learning analytics holds great potential for advancing the field. We apply the <cite>BERT</cite> approach to the following three previously explored LAK tasks on MOOC forum data (Wei et al., 2017) : Confusion detection, urgency of teacher intervention and sentimentality classification.",
  "y": "extends"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_9",
  "x": "Scores are given on a Likert scale from 1 (low) to 7 (high). Language Models: We constructed two models, EduBERT and EduDistilBERT, which respectively refine <cite>BERT-base</cite> and DistilBERT , <cite>both of which</cite> were trained on general domain text from books and Wikipedia (<cite>Devlin et al., 2019</cite>) . Both models are initialized from their <cite>base model</cite> and finetuned on educational data, using the Transformers library .",
  "y": "extends"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_10",
  "x": "Scores are given on a Likert scale from 1 (low) to 7 (high). Language Models: We constructed two models, EduBERT and EduDistilBERT, which respectively refine <cite>BERT-base</cite> and DistilBERT , <cite>both of which</cite> were trained on general domain text from books and Wikipedia (<cite>Devlin et al., 2019</cite>) . Both models are initialized from their <cite>base model</cite> and finetuned on educational data, using the Transformers library .",
  "y": "extends"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_11",
  "x": "Following previous work by Guo et al. (2019) , we split the data into a 2/3 training set and 1/3 test set and consider a post to express sentiment, urgency or confusion if and only if its respective score is \u2265 4. We compare between the four classifiers <cite>BERT-base</cite>, DistilBERT, EduBERT and EduDistilBERT. We evaluated multiple sets of parameters.",
  "y": "uses"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_12",
  "x": "We compare between the four classifiers <cite>BERT-base</cite>, DistilBERT, EduBERT and EduDistilBERT. We evaluated multiple sets of parameters. Best results for these tasks were achieved with the following parameters: two learning epochs, maximal sequence length of 300 (<cite>BERT-base</cite>, EDUBERT) and 512 for the distilled models, all other parameter values were equal to the ones used for pre-training.",
  "y": "uses"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_13",
  "x": "Table 1 compares EduBERT, EduDistilBERT to <cite>their base versions</cite>, as well as the state-of-the-art (SoA) for urgency detection (Guo et al. 2019) . The table shows that all pretraining approaches outperformed the SoA for F1 and weighted F1 measures, with our distilled model EduDistilBERT achieving the best overall performance. Table 2 compares all of the <cite>models</cite> for all three tasks to the SoA using the same measures of accuracy as Wei et al. (2017) .",
  "y": "uses"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_14",
  "x": "Table 1 compares EduBERT, EduDistilBERT to <cite>their base versions</cite>, as well as the state-of-the-art (SoA) for urgency detection (Guo et al. 2019) . Table 2 compares all of the <cite>models</cite> for all three tasks to the SoA using the same measures of accuracy as Wei et al. (2017) .",
  "y": "uses"
 },
 {
  "id": "2eeffe385539b28c7d31eeb176e926_15",
  "x": "**RESULTS & DISCUSSIONS** . EduBERT and EduDistilBERT are fine-tuned on millions of tokens, in contrast to the billions of tokens required to make the most of the architecture potential (<cite>Devlin et al., 2019</cite>) . We are actively seeking more data to train models even more capable of producing contextualized word representations in the educational domain.",
  "y": "background"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_0",
  "x": "Social power structures are ubiquitous in human interactions, and since power is often reflected through language, computational research at the intersection of language and power has gained interest recently. This research has been applied to a wide array of domains such as Wikipedia talk pages (Strzalkowski et al., 2010; Taylor et al., 2012; Danescu-Niculescu-Mizil et al., 2012; Swayamdipta and Rambow, 2012) , blogs (Rosenthal, 2014) as well as workplace interactions <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012; Prabhakaran, 2015) . The corporate environment is one social context in which power dynamics have a clearly defined structure and shape the interactions between individuals, making it an interesting case study on how language and power interact.",
  "y": "background motivation"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_1",
  "x": "Prior work has investigated the use of NLP techniques to study manifestations of different types of power using the Enron email corpus (Diesner et al., 2005; Prabhakaran et al., 2012; Prabhakaran and Rambow, 2013; Prabhakaran and Rambow, 2014) . While early work <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) focused on surface level lexical features aggregated at corpus level, more recent work has looked into the thread structure of emails as well (Prabhakaran and Rambow, 2014) . However, both <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) and (Prabhakaran and Rambow, 2014 ) group all messages sent by an individual to another individual (at the corpus-level and at the thread-level, respectively) and rely on word-ngram * Authors (listed in alphabetical order) contributed equally.",
  "y": "background"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_2",
  "x": "However, both <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) and (Prabhakaran and Rambow, 2014 ) group all messages sent by an individual to another individual (at the corpus-level and at the thread-level, respectively) and rely on word-ngram * Authors (listed in alphabetical order) contributed equally. based features extracted from this concatenated text to infer power relations. They ignore the fact that the text comes from separate emails, and that there is a sequential order to them.",
  "y": "background"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_3",
  "x": "Grouped: Here, we group all emails A sent to B across all threads in the corpus, and vice versa, and use these sets of emails to predict the power relation between A and B. This formulation is similar those in <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) , but our results are not directly comparable since, unlike them, we rely on the ground truth of power relations from (Agarwal et al., 2012) ; however, we created an SVM model that uses word-ngram features similar to theirs as a baseline to our proposed neural architectures. ---------------------------------- **METHODS**",
  "y": "extends similarities differences"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_4",
  "x": "**EXPERIMENTS AND RESULTS** We use support vector machine (SVM) based approaches as our baseline, since they are the state-of-the art in this problem (Prabhakaran and Rambow, 2014;<cite> Bramsen et al., 2011</cite>; Gilbert, 2012) . We use the performance reported by (Prabhakaran and Rambow, 2014) using SVM as baseline for the Per-Thread formulation (using the same train-dev-test splits) and implemented an SVM baseline for the Grouped formulation (not directly comparable to performance reported by <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) ).",
  "y": "uses background"
 },
 {
  "id": "307c18e2928c4a45f574a9c3a36b76_5",
  "x": "**EXPERIMENTS AND RESULTS** We use support vector machine (SVM) based approaches as our baseline, since they are the state-of-the art in this problem (Prabhakaran and Rambow, 2014;<cite> Bramsen et al., 2011</cite>; Gilbert, 2012) . We use the performance reported by (Prabhakaran and Rambow, 2014) using SVM as baseline for the Per-Thread formulation (using the same train-dev-test splits) and implemented an SVM baseline for the Grouped formulation (not directly comparable to performance reported by <cite>(Bramsen et al., 2011</cite>; Gilbert, 2012) ).",
  "y": "extends differences"
 },
 {
  "id": "310272015a781b05c42015c0559b18_0",
  "x": "Several computational simulations of how children solve the word segmentation problem have been proposed, but most have been applied only to a limited number of languages. Saffran et al. (1996) assume transitional probability, but<cite> Brent (1999a)</cite> claims mutual information (MI) is more appropriate. Both assume predictability is measured locally, relative to neighboring segment-pairs.",
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_1",
  "x": "This paper replicates<cite> Brent's (1999a)</cite> mutualinformation model on a corpus of childdirected speech in Modern Greek, and introduces a variant model using a global threshold. Brent's finding regarding the superiority of MI is confirmed; the relative performance of local comparisons and global thresholds depends on the evaluation metric. ----------------------------------",
  "y": "differences similarities background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_2",
  "x": "While the infant studies discussed above focus primarily on the properties of particular cues, computational studies of word-segmentation must also choose between various implementations, which further complicates comparisons. Several models (e.g., Batchelder, 2002;<cite> Brent's (1999a)</cite> MBDP-1 model; Davis, 2000; de Marcken, 1996; Olivier, 1968) simultaneously address the question of vocabulary acquisition, using previously learned word-candidates to bootstrap later segmentations. (It is beyond the scope of this paper to discuss these in detail; see Brent 1999a,b for a review.)",
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_3",
  "x": [
   "Brent (1999a) points out one type of ambiguity, namely that Saffran and colleagues' (1996) results can be modeled as favoring word-breaks at points of either low transitional probability or low mutual information. Brent reports results for models relying on each of these measures. It should be noted that these models are not the main focus of his paper, but provided for illustrative purposes; nevertheless, these models provide the best comparison to Saffran and colleagues' experiment, and may be regarded as an implementation of the same."
  ],
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_4",
  "x": [
   "It should be noted that these models are not the main focus of his paper, but provided for illustrative purposes; nevertheless, these models provide the best comparison to Saffran and colleagues' experiment, and may be regarded as an implementation of the same. Brent (1999a) compares these two models in terms of word tokens correctly segmented (see Section 3 for exact criteria), reporting approximately 40% precision and 45% recall for transitional probability (TP) and 50% precision and 53% recall for mutual information (MI) on the first 1000 utterances of his corpus (with improvements given larger corpora). Indeed, their performance on word tokens is surpassed only by Brent's main model (MBDP-1), which seems to have about 73% precision and 67% recall for the same range."
  ],
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_5",
  "x": "Another question which Saffran et al. (1996) leave unanswered is whether the segmentation depends on local or global comparisons of predictability. Saffran et al. assume implicitly, and<cite> Brent (1999a)</cite> explicitly, that the proper comparison is local-in Brent, dependent solely on the adjacent pairs of segments. However, predictability measures for segmental bigrams (whether TP or MI) may be compared in any number of ways. One straightforward alternative to the local comparison is to compare the predictability measures compare to some global threshold.",
  "y": "background motivation"
 },
 {
  "id": "310272015a781b05c42015c0559b18_6",
  "x": "The global comparison, taken on its own, seems a rather simplistic and inflexible heuristic: for any pair of phonemes xy, either a word boundary is always hypothesized between x and y, or it never is. Clearly, there are many cases where x and y sometimes straddle a word boundary and sometimes do not. The heuristic also takes no account of lengths of possible words. However, the local comparison may take length into account too much, disallowing words of certain lengths. In order to see that, we must examine<cite> Brent's (1999a)</cite> suggested implementation of Saffran et al. (1996) more closely.",
  "y": "background motivation"
 },
 {
  "id": "310272015a781b05c42015c0559b18_7",
  "x": "In the local comparison, given some string \u2026wxyz\u2026, in order for a word boundary to be inserted between x and y, the predictability measure for xy must be lower than both that of wx and of yz. It follows that neither wx nor yz can have word boundaries between them, since they cannot simultaneously have a lower predictability measure than xy. This means that, within an utterance, word boundaries must have at least two segments between them, so this heuristic will not correctly segment utterance-internal one-phoneme words. 3 Granted, only a few one-phoneme word types exist in either English or Greek (or other languages). However, these words are often function words and so are less likely to appear at edges of utterances (e.g., ends of utterances for articles and prepositions; beginnings for postposed elements). Neither<cite> Brent's (1999a)</cite> implementation of Saffran's et al. (1996) heuristic nor utterance-boundary heuristic can explain how these might be learned.",
  "y": "background motivation"
 },
 {
  "id": "310272015a781b05c42015c0559b18_8",
  "x": [
   "Brent (1999a) himself points out another lengthrelated limitation-namely, the relative difficulty that the 'local comparison' heuristic has in segmenting learning longer words. The bigram MI frequencies may be most strongly influenced byand thus as an aggregate largely encode-the most frequent, shorter words. Longer words cannot be memorized in this representation (although common ends of words such as prefixes and suffixes might be)."
  ],
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_9",
  "x": [
   "Longer words cannot be memorized in this representation (although common ends of words such as prefixes and suffixes might be). In order to test for this, Brent proposes that precision for word types (which he calls \"lexicon precision\") be measured as well as for word tokens. While the word-token metric emphasizes the correct segmentation of frequent words, the word-type metric does not share this bias. Brent defines this metric as follows: \"After each block [of 500 utterances], each word type that the algorithm produced was labeled a true positive if that word type had occurred anywhere in the portion of the corpus processed so far; otherwise it is labeled a false positive. \" Measured this way, MI yields a word type precision of only about 27%; transitional probability yields a precision of approximately 24% for the first 1000 utterances, compared to 42% for MBDP-1. He does not measure word type recall. This same limitation in finding longer, less frequent types may apply to comparisons against a global threshold as well. This is also in need of testing. It seems that both global and local comparisons, used on their own as sole or decisive heuristics, may have serious limitations. It is not clear a priori which limitation is most serious; hence both comparisons are tested here."
  ],
  "y": "background motivation"
 },
 {
  "id": "310272015a781b05c42015c0559b18_10",
  "x": "While in its general approach the study reported here replicates the mutual-information and transitional-probability models in<cite> Brent (1999a)</cite> , it differs slightly in the details of their use. First, whereas Brent dynamically updated his measures over a single corpus, and thus blurred the line between training and testing data, our model precompiles statistics for each distinct bigram-type offline, over a separate training corpus. 4 Secondly, we compare the use of a global threshold (described in more detail in Section 2.3, below) to<cite> Brent's (1999a)</cite> use of the local context (as described in Section 1.3 above).",
  "y": "differences background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_11",
  "x": "First, whereas Brent dynamically updated his measures over a single corpus, and thus blurred the line between training and testing data, our model precompiles statistics for each distinct bigram-type offline, over a separate training corpus. 4 Secondly, we compare the use of a global threshold (described in more detail in Section 2.3, below) to<cite> Brent's (1999a)</cite> use of the local context (as described in Section 1.3 above). Like <cite>(Brent, 1999a)</cite> , but unlike Saffran et al. (1996) , our model focuses on pairs of segments, not on pairs of syllables.",
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_12",
  "x": "4 Secondly, we compare the use of a global threshold (described in more detail in Section 2.3, below) to<cite> Brent's (1999a)</cite> use of the local context (as described in Section 1.3 above). Like <cite>(Brent, 1999a)</cite> , but unlike Saffran et al. (1996) , our model focuses on pairs of segments, not on pairs of syllables. While Modern Greek syllabic structure is not as complicated as English's, it is still more complicated than the CV structure assumed in Saffran et al. (1996) ; hence, access to syllabification cannot be assumed.",
  "y": "similarities background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_13",
  "x": "Since the finite-state framework selects the best path over the whole utterance, it also allows for optimization over a sequence of decisions, rather than optimizing each local decision separately. 6 Unlike Belz (1998) , where the actual FSM structure (including classes of phonemes that could be group onto one arc) was learned, here the structure of each FSM is determined in advance. Only the weight on each arc is derived from data. No attempt is made to combine phonemes to produce more minimal FSMs; each phoneme (and phoneme-pair) is modeled separately. Like<cite> Brent (1999a)</cite> and indeed most models in the literature, this model assumes (for sake of convenience and simplicity) that the child hears each segment produced within an utterance without error. This assumption translates into the finitestate domain as a simple acceptor (or equivalently, an identity transducer) over the segment sequence for a given utterance.",
  "y": "uses background motivation"
 },
 {
  "id": "310272015a781b05c42015c0559b18_14",
  "x": "This boundary measure may be more conservative than that reported by other authors, but is easily convertible into other metrics. The second metric, the percentage of word tokens detected, is the same as<cite> Brent (1999a)</cite> . In order for a word to be counted as correctly found, three conditions must be met: (a) the word's beginning (left boundary) is correctly detected, (b) the word's ending (right boundary) is correctly detected, and (c) these two are consecutive (i.e., no false boundaries are posited within the word).",
  "y": "similarities"
 },
 {
  "id": "310272015a781b05c42015c0559b18_15",
  "x": "The last metric (word type) is slightly more conservative than<cite> Brent's (1999a)</cite> in that the word type must have been actually spoken in the same utterance (not the same block of 500 utterances) in which it was detected to count as a match. This lessens the possibility that a mismatch that happens to be segmentally identical to an actual word (but whose semantic context may not be conducive to learning its correct meaning) is counted as a match. However, this situation is presumably rather rare.",
  "y": "extends motivation"
 },
 {
  "id": "310272015a781b05c42015c0559b18_16",
  "x": "However, this situation is presumably rather rare. Tables 2 and 3 present the results over the test set for both the global and the local comparisons of the predictability statistics proposed by Saffran et al. (1996) and<cite> Brent (1999a)</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "310272015a781b05c42015c0559b18_17",
  "x": "---------------------------------- **COMPARING THE FOUR VARIANTS** The findings here confirm<cite> Brent's (1999a)</cite> contention that mutual information is a better measure of predictability than is transitional probability-at least for the task of identifying words, not just boundaries.",
  "y": "similarities"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_0",
  "x": "The present work differs from Galley et al. (2003) in two respects, viz. we focus solely on textual information and we directly address the problem of tutorial dialogue. In this study we apply the methods of Foltz et al. (1998) ,<cite> Hearst (1994</cite> Hearst ( , 1997 , and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue. All three are vector space methods that measure lexical cohesion to determine topic shifts.",
  "y": "uses"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_1",
  "x": "An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment. Both<cite> Hearst (1994</cite> Hearst ( , 1997 and Foltz et al. (1998) use vector space methods discussed below to represent and compare units of text. The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.",
  "y": "background"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_2",
  "x": "The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text. However,<cite> Hearst (1994</cite> Hearst ( , 1997 and Foltz et al. (1998) differ on how text units are defined and on how to interpret the results of a comparison. The text unit's definition in<cite> Hearst (1994</cite> Hearst ( , 1997 and Foltz et al. (1998) is generally task dependent, depending on what size gives the best results.",
  "y": "background"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_3",
  "x": "The text unit's definition in<cite> Hearst (1994</cite> Hearst ( , 1997 and Foltz et al. (1998) is generally task dependent, depending on what size gives the best results. For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level. However, when using LSA to segment text, Foltz et al. (1998) use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion.",
  "y": "background"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_4",
  "x": "For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level. However, when using LSA to segment text, Foltz et al. (1998) use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion. Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens <cite>(Hearst, 1994)</cite> , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size.",
  "y": "background"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_5",
  "x": "Hearst (1994 Hearst ( , 1997 was replicated using the JTextTile (Choi, 1999 ) Java software. A variant of<cite> Hearst (1994</cite> Hearst ( , 1997 was created by using LSA instead of the standard vector space method. The orthonormal basis method also used a moving window; however, in contrast to the previous methods, the window is not treated just as a large block of text.",
  "y": "differences"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_6",
  "x": "---------------------------------- **HEARST (1994, 1997)** The JTextTile software was used to implement<cite> Hearst (1994)</cite> on dialogue.",
  "y": "uses"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_7",
  "x": "The optimal combination of parameters (Fmeasure = .17) is a unit size of 16 words and a window size of 16 units. This combination matches<cite> Hearst (1994)</cite> 's heuristic of choosing the window size to be the average paragraph length. On the test set, this combination of parameters yielded an F-measure of .14 as opposed to the Fmeasure for monologue reported by Hearst (1997) , .70.",
  "y": "uses"
 },
 {
  "id": "3128481fa4e5d2c4af7deba2c28950_9",
  "x": "Unfortunately, the large difference in F-measure between the Foltz algorithm and the Hearst + LSA algorithm is more difficult to explain. These two methods differ by their segmentation criterion and by their training (Foltz is a regression model and Hearst is not). It may be that<cite> Hearst (1994</cite> Hearst ( , 1997 )'s segmentation criterion, i.e. depth scores, do not translate well to dialogue.",
  "y": "differences"
 },
 {
  "id": "318487ac270ca272ec11a3de6c0685_0",
  "x": "In this paper, we show how labeled data can considerably improve distributional methods for measuring semantic similarity. First, we develop a new discriminative term-weighting metric called TF-KLD, which is applied to the term-context matrix before factorization. On a standard paraphrase identification task (Dolan et al., 2004) , this method improves on both traditional TF-IDF and Weighted Textual Matrix Factorization (WTMF;<cite> Guo and Diab, 2012)</cite> .",
  "y": "differences"
 },
 {
  "id": "318487ac270ca272ec11a3de6c0685_2",
  "x": "---------------------------------- **SUPERVISED CLASSIFICATION** While previous work has performed paraphrase classification using distance or similarity in the latent space<cite> (Guo and Diab, 2012</cite>; Socher et al., 2011) , more direct supervision can be applied.",
  "y": "background"
 },
 {
  "id": "318487ac270ca272ec11a3de6c0685_4",
  "x": "As in prior work<cite> (Guo and Diab, 2012)</cite> , the threshold is tuned on held-out training data. We consider two distributional feature sets: FEAT 1 , which includes unigrams; and FEAT 2 , which also includes bigrams and unlabeled dependency pairs obtained from MaltParser (Nivre et al., 2007) . To compare with<cite> Guo and Diab (2012)</cite> , we set the latent dimensionality to K = 100, which was the same in their paper.",
  "y": "similarities uses"
 },
 {
  "id": "318487ac270ca272ec11a3de6c0685_5",
  "x": "To compare with<cite> Guo and Diab (2012)</cite> , we set the latent dimensionality to K = 100, which was the same in their paper. Both SVD and NMF factorization are evaluated; in both cases, we minimize the Frobenius norm of the reconstruction error. Table 2 compares the accuracy of a number of different configurations.",
  "y": "similarities uses"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_0",
  "x": "Translation from speech utterances is a challenging problem that has been studied both under statistical, symbolic approaches (Ney, 1999; Casacuberta et al., 2004; Kumar et al., 2015) and more recently using neural models <cite>(Sperber et al., 2017)</cite> . Most previous work rely on pipeline approaches, using the output of a speech recognition system (ASR) as an input to a machine translation (MT) one. These inputs can be simply the 1-best sentence returned by the ASR system or a more structured representation such as a lattice.",
  "y": "background"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_1",
  "x": "<cite>Sperber et al. (2017)</cite> proposes a lattice-tosequence model which, in theory, can address both problems above. However, <cite>their model</cite> suffers from training speed performance due to the lack of efficient batching procedures and they rely on transcriptions for pretraining. In this work, we address these two problems by applying lattice transformations and graph networks as encoders.",
  "y": "background"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_2",
  "x": "However, <cite>their model</cite> suffers from training speed performance due to the lack of efficient batching procedures and they rely on transcriptions for pretraining. In this work, we address these two problems by applying lattice transformations and graph networks as encoders. More specifically, we enrich the lattices by applying subword segmentation using byte-pair encoding (Sennrich et al., 2016, BPE) and perform a minimisation step to remove redundant nodes arising from this procedure.",
  "y": "motivation"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_3",
  "x": "To perform this step, we leverage an efficient algorithm for automata minimisation (Hopcroft, 1971) , which traverses the graph detecting redundant nodes by using equivalence classes, running in O(n log n) time, where n is the number of nodes. 1 This procedure is also done in <cite>Sperber et al. (2017)</cite> . The final step adds reverse and self-loop edges to the lattice, where these new edges have specific parameters in the encoder.",
  "y": "similarities"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_4",
  "x": "Following previous work (Post et al., 2013; <cite>Sperber et al., 2017)</cite> , we lowercase and remove punctuation from the English translations. To build the BPE models, we extract the vocabulary from the Spanish training lattices, using 8K split operations. Models and Evaluation All our models are trained on the Fisher training set.",
  "y": "uses"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_5",
  "x": "Table 1 : Out-of-the-box scenario results, in BLEU scores. \"L\" corresponds to word lattice inputs, \"L+S\" and \"L+S+M\" correspond to lattices after subword segmentation and after minimisation, respectively. Each model is trained using 5 different seeds and we report BLEU (Papineni et al., 2001) results using the median performance according to the dev set and an ensemble of the 5 models. For the word-based models, we remove any tokens with frequency lower than 2 (as in <cite>Sperber et al. (2017)</cite> ), while for subword models we do not perform any threshold pruning.",
  "y": "uses"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_6",
  "x": "The results shown in Table 2 are consistent with previous work: adding transcriptions further enhance the system performance. We also slightly outperform <cite>Sperber et al. (2017)</cite> in the setting where they ignore lattice scores, as in our approach. Most importantly, we are able to reach those results while being two orders of magnitude faster at training time: <cite>Sperber et al. (2017)</cite> report taking 1.5 days for each epoch while our architecture can process each epoch in 15min.",
  "y": "differences"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_7",
  "x": "Most importantly, we are able to reach those results while being two orders of magnitude faster at training time: <cite>Sperber et al. (2017)</cite> report taking 1.5 days for each epoch while our architecture can process each epoch in 15min. The reason is because <cite>their model</cite> relies on the CPU while our GGNN-based model can be easily batched and computed in a GPU. Given those differences in training time, it is worth mentioning that the best model in <cite>Sperber et al. (2017)</cite> is surpassed by our best ensemble using lattices only.",
  "y": "differences"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_8",
  "x": "We also slightly outperform <cite>Sperber et al. (2017)</cite> in the setting where they ignore lattice scores, as in our approach. Most importantly, we are able to reach those results while being two orders of magnitude faster at training time: <cite>Sperber et al. (2017)</cite> report taking 1.5 days for each epoch while our architecture can process each epoch in 15min. The reason is because <cite>their model</cite> relies on the CPU while our GGNN-based model can be easily batched and computed in a GPU.",
  "y": "differences"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_9",
  "x": "The reason is because <cite>their model</cite> relies on the CPU while our GGNN-based model can be easily batched and computed in a GPU. Given those differences in training time, it is worth mentioning that the best model in <cite>Sperber et al. (2017)</cite> is surpassed by our best ensemble using lattices only. This means that we can obtain state-of-the-art performance even in an out-of-thebox scenario, under the same training speed constraints.",
  "y": "differences"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_10",
  "x": "As a simple first approach to incorporate scores, we embed them using a multilayer perceptron, using the score as the input. This however did not produce good results: performance dropped to 32.9 BLEU in the single model setting and 38.4 for the ensemble. It is worth noticing that <cite>Sperber et al. (2017)</cite> has a more principled approach to incorporate scores: by modifying the attention module.",
  "y": "differences"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_11",
  "x": "For future work, we plan to investigate better approaches to incorporate scores in the lattices. The approaches used by <cite>Sperber et al. (2017)</cite> can provide a starting point in this direction. The same minimisation procedures we employ can be adapted to weighted lattices (Eisner, 2003 ).",
  "y": "uses"
 },
 {
  "id": "3251c6cd1afccf6ad8d5391a4360b0_12",
  "x": "For future work, we plan to investigate better approaches to incorporate scores in the lattices. The approaches used by <cite>Sperber et al. (2017)</cite> can provide a starting point in this direction. The same minimisation procedures we employ can be adapted to weighted lattices (Eisner, 2003 ).",
  "y": "future_work"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_0",
  "x": "Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations (Homola, 2011; Mager et al., 2018d; Klavans, 2018a) . Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017) , hybrid models (Mager et al., 2018b; Moeller et al., 2018) , and supervised machine learning, particularly deep learning approaches (Micher, 2017; <cite>Kann et al., 2018)</cite> . While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009 ), or Arapaho (Littell, 2018 Moeller et al., 2018) ), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages.",
  "y": "background"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_1",
  "x": "While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009 ), or Arapaho (Littell, 2018 Moeller et al., 2018) ), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages. We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (Johnson et al., 2007) . We experiment with four UtoAztecan languages: Mexicanero (MX), Nahuatl (NH), Wixarika (WX) and Yorem Nokki (YN)<cite> (Kann et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_2",
  "x": "We use the datasets introduced by<cite> Kann et al. (2018)</cite> in an unsupervised fashion (unsegmented words). We design several AG learning setups: 1) use the best-on-average AG setup from Eskander et al. (2016) ; 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from<cite> Kann et al. (2018)</cite> ; 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3). We show that the AG-based approaches outperform other unsupervised methods -M orf essor (Creutz and Lagus, 2007) and M orphoChain (Narasimhan et al., 2015) ) -, and that for two of the languages (NH and YN), the best AG-based approaches outperform the best supervised methods (Section 4).",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_3",
  "x": "We use the datasets introduced by<cite> Kann et al. (2018)</cite> in an unsupervised fashion (unsegmented words). We design several AG learning setups: 1) use the best-on-average AG setup from Eskander et al. (2016) ; 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from<cite> Kann et al. (2018)</cite> ; 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3). We show that the AG-based approaches outperform other unsupervised methods -M orf essor (Creutz and Lagus, 2007) and M orphoChain (Narasimhan et al., 2015) ) -, and that for two of the languages (NH and YN), the best AG-based approaches outperform the best supervised methods (Section 4).",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_4",
  "x": [
   "The rootmorpheme complexity and the word class \"squish\" makes developing segmented training data with reliability across annotators difficult to achieve. Kann et al. (2018) have made a first step by releasing a small set of morphologically segmented datasets although even in these carefully curated datasets, the distinction between affix and clitic is not always indicated. We use these datasets in an unsupervised fashion (i.e., we use the unsegmented words)."
  ],
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_5",
  "x": "They were constructed so they include both segmentable as well as non-<cite> Kann et al. (2018)</cite> , for training we do not use the segmented version of the data (our approach is unsupervised). In addition to the datasets, for NH and WX we also have available the Bible (Christodouloupoulos and Steedman, 2015; Mager et al., 2018a ), which we consider for one of our experimental setups as additional training data. In the dataset from<cite> (Kann et al., 2018)</cite> , the maximum number of morphemes per word for MX is seven with an average of 2.13; for NH, six with an average of 2.2; for WX, maximum of ten with an average of 3.3; and for YN, the maximum is ten, with an average of 2.13.",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_6",
  "x": "In addition to the datasets, for NH and WX we also have available the Bible (Christodouloupoulos and Steedman, 2015; Mager et al., 2018a ), which we consider for one of our experimental setups as additional training data. In the dataset from<cite> (Kann et al., 2018)</cite> , the maximum number of morphemes per word for MX is seven with an average of 2.13; for NH, six with an average of 2.2; for WX, maximum of ten with an average of 3.3; and for YN, the maximum is ten, with an average of 2.13. ----------------------------------",
  "y": "background"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_7",
  "x": "However, since affixes and stems are not distinguished in the training annotations from<cite> Kann et al. (2018)</cite> , we only consider the first and last morphemes that appear at least five times. We call this setup AG Scholar BestL . Multilingual Training.",
  "y": "motivation"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_8",
  "x": "Multilingual Training. Since the vocabulary in<cite> Kann et al. (2018)</cite> for each language is small, and the languages are from the same language family, one data augmentation approach is to train on all languages and test then on each language individually. We call this setup AG M ulti .",
  "y": "motivation"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_9",
  "x": "We evaluate the different AG setups on the blind test set from<cite> Kann et al. (2018)</cite> and compare our AG approaches to state-of-the-art unsupervised systems as well as supervised models including the best supervised deep learning models from<cite> Kann et al. (2018)</cite> . As the metric, we use the segmentation-boundary F1-score, which is standard for this task (Virpioja et al., 2011) . Evaluating different AG setups.",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_13",
  "x": "In addition, we report the results on their other supervised baselines: a supervised seq-to-seq model (S2S) and a supervised CRF approach. As can be seen in Table  4 , our unsupervised AG-based approaches outperform the best supervised approaches for NH and YN with absolute F1-scores of 0.010 and 0.012, respectively. An interesting observation is that for YN we only used the words in the training set of<cite> Kann et al. (2018)</cite> (unsegmented) , without any data augmentation.",
  "y": "uses"
 },
 {
  "id": "34346688a7e5166ee7b559ccbfe8e3_14",
  "x": "An interesting observation is that for YN we only used the words in the training set of<cite> Kann et al. (2018)</cite> (unsegmented) , without any data augmentation. For MX and WX, the neural models from<cite> Kann et al. (2018)</cite> (BestMTT and BestDA), outperform our unsupervised AG-based approaches. Error Analysis.",
  "y": "differences"
 },
 {
  "id": "3452953ac579f1c05870442456a49c_0",
  "x": "Contrast) Previous studies show that the presence of connectives can greatly help with classification of the relation and can be disambiguated with 0.93 accuracy (4-ways) solely on the discourse relation connectives (Pitler et al., 2008) . In implicit relations, no such strong cue is available and the discourse relation instead needs to be inferred based on the two textual arguments. In recent studies, various classes of features are explored to capture lexical and semantic regularities for identifying the sense of implicit relations, including linguistically informed features like polarity tags, Levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features <cite>(Lin et al., 2009</cite>; Pitler et al., 2009; Zhou et al., 2010; Zhang et al., 2015; Chen et al., 2016) .",
  "y": "background"
 },
 {
  "id": "3452953ac579f1c05870442456a49c_1",
  "x": "In recent studies, various classes of features are explored to capture lexical and semantic regularities for identifying the sense of implicit relations, including linguistically informed features like polarity tags, Levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features <cite>(Lin et al., 2009</cite>; Pitler et al., 2009; Zhou et al., 2010; Zhang et al., 2015; Chen et al., 2016) . For some of second-level relations (a level of granularity that should be much more meaningful to downstream tasks than the four-way distinction), there are only a dozen in-stances, so that it's important to make maximal use of both the data set for training and testing. The test set that is currently most often used for 11 way classification is section 23 <cite>(Lin et al., 2009</cite>; Ji and Eisenstein, 2015; Rutherford et al., 2017) , which contains only about 761 implicit relations.",
  "y": "background"
 },
 {
  "id": "3452953ac579f1c05870442456a49c_2",
  "x": "Previous work in this task has been done over two schemes of evaluation: first-level 4-ways classification (Pitler et al., 2009; Rutherford and Xue, 2014; Chen et al., 2016) , second-level 11-way classification <cite>(Lin et al., 2009</cite>; Ji and Eisenstein, 2015) . The distribution of second-level relations in PDTB is illustrated in Table 1 . We follow the preprocessing method in <cite>(Lin et al., 2009</cite>; Rutherford et al., 2017) .",
  "y": "background"
 },
 {
  "id": "3452953ac579f1c05870442456a49c_3",
  "x": "Previous work in this task has been done over two schemes of evaluation: first-level 4-ways classification (Pitler et al., 2009; Rutherford and Xue, 2014; Chen et al., 2016) , second-level 11-way classification <cite>(Lin et al., 2009</cite>; Ji and Eisenstein, 2015) . The distribution of second-level relations in PDTB is illustrated in Table 1 . We follow the preprocessing method in <cite>(Lin et al., 2009</cite>; Rutherford et al., 2017) .",
  "y": "uses"
 },
 {
  "id": "3452953ac579f1c05870442456a49c_4",
  "x": "**MODEL** The task is to predict the discourse relation given the two arguments of an implicit instance. As a label set, we use 11-way distinction as proposed in<cite> Lin et al., (2009)</cite>; Ji and Eisenstein (2015) .",
  "y": "uses"
 },
 {
  "id": "3477c0225d6a0e55365242d95a3dc9_0",
  "x": "Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks [7, 8, <cite>9]</cite> . The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora.",
  "y": "background"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_0",
  "x": "The complexity of the task is two-fold: (1) localised named entities (e.g. sporting team names) are not of interest; and (2) without semantic knowledge it is difficult to detect terms that are in general use but have a special meaning in a region. In this paper we propose a text-based geolocation method based on neural networks. Our contributions are as follows: (1) we achieve state-of-the-art results on benchmark Twitter geolocation datasets; (2) we show that the model is less sensitive to the specific location discretisation method; (3) we release the first broad-coverage dataset for evaluation of lexical dialectology models; (4) we incorporate our text-based model into a network-based model<cite> (Rahimi et al., 2015a)</cite> and improve the performance utilising both network and text; and (5) we use the model's embeddings for extraction of local terms and show that it outperforms two baselines.",
  "y": "extends background"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_1",
  "x": "The classification models often rely on less than 1% of geotagged tweets for supervision and discretise real-valued coordinates into equalsized grids (Serdyukov et al., 2009 ), administrative regions (Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Han et al., 2012 , or flat (Wing and Baldridge, 2011) or hierarchical k-d tree clusters (Wing and Baldridge, 2014) . Network-based methods also use either real-valued coordinates (Jurgens et al., 2015) or discretised regions<cite> (Rahimi et al., 2015a)</cite> as labels, and use label propagation over the interaction graph (e.g. @-mentions). More recent methods have focused on representation learning by using sparse coding (Cha et al., 2015) or neural networks (Liu and Inkpen, 2015) , utilising both text and network information<cite> (Rahimi et al., 2015a)</cite> .",
  "y": "background"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_2",
  "x": "More recent methods have focused on representation learning by using sparse coding (Cha et al., 2015) or neural networks (Liu and Inkpen, 2015) , utilising both text and network information<cite> (Rahimi et al., 2015a)</cite> . Dialect is a variety of language shared by a group of speakers (Wolfram and Schilling, 2015) . Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas.",
  "y": "background"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_3",
  "x": "Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161. 4 The results reported in Rahimi et al. (2015b;<cite> 2015a)</cite> for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset. While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user<cite> (Rahimi et al., 2015a)</cite> .",
  "y": "differences"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_4",
  "x": "Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161. 4 The results reported in Rahimi et al. (2015b;<cite> 2015a)</cite> for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset. While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user<cite> (Rahimi et al., 2015a)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "365171603fb13c6534ef4abab092d6_5",
  "x": "Our method outperforms both the flat and hierarchical text-based models by a large margin. Comparing the two discretisation strategies, k-means outperforms k-d tree by a reasonable margin. We also incorporated the MLP predictions into a network-based model based on the method of <cite>Rahimi et al. (2015a)</cite> , and improved upon their work.",
  "y": "extends background"
 },
 {
  "id": "373795850c8f182051214a8ee09461_0",
  "x": "They represent a well-established tool for modelling semantic relatedness between words and phrases (Bullinaria and Levy, 2007; Turney and Pantel, 2010) . In the last decade, standard DSMs using bag-of-words or syntactic cooccurrence counts have been enhanced by integration into neural networks Levy et al., 2015; Nguyen et al., 2016) , or by integrating perceptual information (Silberer and Lapata, 2014; Bruni et al., 2014; <cite>Kiela et al., 2014</cite>; Lazaridou et al., 2015) . While standard DSMs have been applied to a variety of semantic relatedness tasks such as word sense discrimination, selectional preferences, relation distinction (among others), multi-modal models have predominantly been evaluated on their general ability to model semantic similarity as captured by SimLex (Hill et al., 2015) , WordSim (Finkelstein et al., 2002) , etc.",
  "y": "background"
 },
 {
  "id": "373795850c8f182051214a8ee09461_1",
  "x": "Differently to most previous multimodal approaches, we thus address a semantically specific task that was traditionally addressed by standard DSMs, mainly for English and German (Baldwin, 2005; Bannard, 2005; Reddy et al., 2011; Salehi and Cook, 2013; Schulte im Walde et al., 2013; Salehi et al., 2014; Bott and Schulte im Walde, 2014; Bott and Schulte im Walde, 2015; Schulte im Walde et al., 2016a) . Furthermore, we zoom into factors that might influence the quality of predictions, such as lexical and empirical target properties (e.g., ambiguity, frequency, compositionality); and filters to optimise the visual space, such as dispersion and imageability filters<cite> (Kiela et al., 2014)</cite> , and a novel clustering filter. Our experiments demonstrate that the contributions of the textual and the visual models differ for predictions across the nominal vs. verbal compositions.",
  "y": "background"
 },
 {
  "id": "373795850c8f182051214a8ee09461_2",
  "x": "2 Furthermore, we zoom into factors that might influence the quality of predictions: (A) the impact of lexical and empirical target properties, i.e., ambiguity (relying on the DUDEN dictionary 3 , frequency (as provided by the gold standards), abstractness and imageability (as taken from K\u00f6per and Schulte im Walde (2016)); (B) optimisation of the visual space: (i) In accordance with human concept processing (Paivio, 1990) , including image representations should be more useful for words which are visual. We therefore apply the dispersion-based filter suggested by<cite> Kiela et al. (2014)</cite> . The filter decides whether to include perceptual information for a specific word or not, relying on a pairwise similarity between all images of a concept.",
  "y": "uses"
 },
 {
  "id": "373795850c8f182051214a8ee09461_3",
  "x": "For GS-NN, the compositionality of concrete and imaginable targets is predicted better than for abstract and less imaginable targets, as one would expect and has been shown by<cite> Kiela et al. (2014)</cite> ; for GS-PV, the opposite is the case. Similarly, while for GS-NN highly compositional targets are predicted worse than low-and mid-compositional targets, for GS-PV mid-compositional targets are predicted much worse than low-and high-compositional targets. These differences in results point to questions that have still been unsolved across research fields: while humans can easily grasp intuitions about the abstractness, imageability and compositionality of nouns, the categorisations are difficult to define for verbs (Glenberg and Kaschak, 2002; Brysbaert et al., 2014) .",
  "y": "similarities"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_0",
  "x": "In recent years, an increasing number of studies have investigated character-level models with subwords in both unsupervised<cite> (Bojanowski et al., 2017</cite>; Pagliardini et al., 2018) and supervised learning (Zhang et al., 2015; Sennrich et al., 2016; Wieting et al., 2016; Lee et al., 2017) . In these models, the notion of vocabularies is extended to include sub-words. By enriching the information of the word, sub-words are useful for capturing morphological changes<cite> (Bojanowski et al., 2017)</cite> and the meaning of short phrases (Wieting et al., 2016) .",
  "y": "background"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_1",
  "x": "In these models, the notion of vocabularies is extended to include sub-words. By enriching the information of the word, sub-words are useful for capturing morphological changes<cite> (Bojanowski et al., 2017)</cite> and the meaning of short phrases (Wieting et al., 2016) . In addition, OOV (or unseen) words can be composed from sub-words, which are present at training<cite> (Bojanowski et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_2",
  "x": "In these models, the notion of vocabularies is extended to include sub-words. By enriching the information of the word, sub-words are useful for capturing morphological changes<cite> (Bojanowski et al., 2017)</cite> and the meaning of short phrases (Wieting et al., 2016) . In addition, OOV (or unseen) words can be composed from sub-words, which are present at training<cite> (Bojanowski et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_3",
  "x": "In this paper, we propose a simple unsupervised method of character n-gram embedding for unsegmented languages, where the segmentation step is completely omitted thus words, phrases and sentences are treated seamlessly. Our model considers all possible character n-grams as embedding targets in a corpus. Each n-gram is explicitly modeled as a composition of its sub-n-grams just like each word is modeled as a composition of sub-words in the subword information skipgram model (SISG)<cite> (Bojanowski et al., 2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_5",
  "x": "As baseline systems, we use C-BOW, Skipgram (Mikolov et al., 2013) , Subword Information Skip-gram (SISG)<cite> (Bojanowski et al., 2017)</cite> and Segmentation-free word embedding for unsegmented languages (Sembei) (Oshikiri, 2017) for the word-level tasks. For the sentence-level task, baselines are PV-DBOW, PV-DM (Le and Mikolov, 2014) and Sent2vec (Pagliardini et al., 2018) . In addition, we test sentence embedding baselines obtained by simple averaging of word embeddings over the sentence, denoted as C-BOW * , Skip-gram * and SISG * .",
  "y": "uses"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_6",
  "x": "Most of the settings are the same as that of <cite>Bojanowski et al. (2017)</cite> . Two widely-used benchmark datasets are used: Chinese word similarity dataset (Jin and Wu, 2012) , which contains 297 pairs of words, and Japanese word similarity dataset (Sakaizawa and Komachi, 2017) , which contains 4429 pairs of words. Conventional word embedding methods, C-BOW, Skip-gram, and Sembei, cannot provide the embeddings of OOV words in the test data.",
  "y": "uses similarities"
 },
 {
  "id": "375b9c865d9f1b559387aa01a20a78_7",
  "x": "Conventional word embedding methods, C-BOW, Skip-gram, and Sembei, cannot provide the embeddings of OOV words in the test data. In contrast, SISG and our model can compute representations for almost all words, since both methods learn compositional n-gram features. In order to show comparable results, we use the null vector for these OOV words following <cite>Bojanowski et al. (2017)</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_0",
  "x": "For instance, some GEC systems use large parallel corpora and synthetic data (Ge et al., 2018; Xie et al., 2018) . We introduce an unsupervised method based on MT for GEC that does not almost use parallel learner data. In particular, we use methods proposed by Marie and Fujita (2018) , <cite>Artetxe et al. (2018b)</cite> , and Lample et al. (2018) .",
  "y": "similarities uses"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_1",
  "x": "The experimental results show that our system achieved an F 0.5 score of 28.31 points in the low resource track of the shared task at BEA2019. 2 Unsupervised GEC Algorithm 1 shows the pseudocode for unsupervised GEC. This code is derived from <cite>Artetxe et al. (2018b)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_2",
  "x": "\u01eb is a constant term for the case where no alignments are found. As in <cite>Artetxe et al. (2018b)</cite> , the term was set to 0.001. The backward lexical translation probability lex(e|f ) is calculated in a similar manner.",
  "y": "similarities"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_3",
  "x": "A new phrase table P (1) s\u2192t was then created with this target synthetic corpus. This operation was executed N times. For backward refinement<cite> (Artetxe et al., 2018b)</cite> , source synthetic data were generated from the target monolingual data using the target to source phrase table P (0) t\u2192s and source language model LM s .",
  "y": "background"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_4",
  "x": "Construction of a comparable corpus This unsupervised method is based on the assumption that the source and target corpora are comparable. In fact, Lample et al. (2018) , <cite>Artetxe et al. (2018b)</cite> and Marie and Fujita (2018) use the News Crawl of source and target language as training data. To make a comparable corpus for GEC, we use translated texts using Google Translation as 3 Experiment of low resource GEC 3.1 Experimental setting Table 1 shows the training and development data size.",
  "y": "background"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_5",
  "x": "We used moses truecaser for the training data; this truecaser model was learned from processed English News Crawl. We used byte-pair-encoding (Sennrich et al., 2016b) learned from processed English News Crawl; the number of operations was 50K. The implementation made by <cite>Artetxe et al. (2018b)</cite> 6 was modified to conduct the experiments.",
  "y": "extends differences"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_6",
  "x": "The number of USMT forward in iter 1 and iter 2 is 3,437 and 3,257, respectively, whereas that of USMT backward in iter 1 and iter 2 is 4,092 and 2,789. As for USMT backward , the number of corrections from iter 1 to iter 2 decreases by 1,303. <cite>Artetxe et al. (2018b)</cite> and Lample et al. (2018) reported that the BLEU score (Papineni et al., 2002) of unsupervised MT with backward-refinement improves with increasing iterations.",
  "y": "background"
 },
 {
  "id": "3bbc588f06e326e1d75985fe253a5f_8",
  "x": "Although we observed some usage error examples of 'watch' in the synthetic source data, our model was not able to replace 'watch' to 'see' based on the context. both NMT (Lample et al., 2018; Marie and Fujita, 2018) and SMT<cite> (Artetxe et al., 2018b)</cite> . In this study, we apply the USMT method of <cite>Artetxe et al. (2018b)</cite> and Marie and Fujita (2018) to GEC.",
  "y": "similarities uses"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_0",
  "x": "As MA is the first step in Japanese NLP, its accuracy directly affects the accuracy of NLP systems as a whole. In addition, with the proliferation of text in various domains, there is increasing need for methods that are both robust and adaptable to out-of-domain data (Escudero et al., 2000) . Previous approaches have used structured predictors such as hidden Markov models (HMMs) or conditional random fields (CRFs), which consider the interactions between neighboring words and parts of speech (Nagata, 1994; Asahara and Matsumoto, 2000;<cite> Kudo et al., 2004)</cite> .",
  "y": "background"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_1",
  "x": "We find experimental evidence that pointwise MA can exceed the accuracy of a state-of-the-art structured approach<cite> (Kudo et al., 2004)</cite> on in-domain data, and is significantly more robust to out-of-domain data. We also show that pointwise MA can be adapted to new domains with minimal effort through the combination of active learning and partial annotation (Tsuboi et al., 2008) , where only informative parts of a particular sentence are annotated. In a realistic domain adaptation scenario, we find that a combination of pointwise prediction, partial annotation, and active learning allows for easy adaptation.",
  "y": "differences"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_2",
  "x": "**JAPANESE MORPHOLOGICAL ANALYSIS** Japanese MA takes an unsegmented string of characters x I 1 as input, segments it into morphemes w J 1 , and annotates each morpheme with a part of speech t J 1 . This can be formulated as a two-step process of first segmenting words, then estimating POSs (Ng and Low, 2004) , or as a single joint process of finding a morpheme/POS string from unsegmented text <cite>(Kudo et al., 2004</cite>; Nakagawa, 2004; Kruengkrai et al., 2009) .",
  "y": "background"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_3",
  "x": "Feature Strings tire sentences as in Figure 1 (a). The CRF-based method presented by<cite> Kudo et al. (2004)</cite> is generally accepted as the state-of-the-art in this paradigm. CRFs are trained over segmentation lattices, which allows for the handling of variable length sequences that occur due to multiple segmentations.",
  "y": "background"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_4",
  "x": "We follow<cite> Kudo et al. (2004)</cite> in defining our feature set, as summarized in Table 1 1 . Lexical features were trained for the top 5000 most frequent words in the corpus. It should be noted that these are wordbased features, and information about transitions between POS tags is included.",
  "y": "uses"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_5",
  "x": "It should be noted that these are wordbased features, and information about transitions between POS tags is included. When creating training data, the use of word-based features indicates that word boundaries must be annotated, while the use of POS transition information further indicates that all of these words must be annotated with POSs. 1 More fine-grained POS tags have provided small boosts in accuracy in previous research<cite> (Kudo et al., 2004)</cite> , but these increase the annotation burden, which is contrary to our goal.",
  "y": "background"
 },
 {
  "id": "3ced64da2c64b0963c4c3d88fd60e0_7",
  "x": "In addition, we trained a CRF-based model with the CRFSuite toolkit (Okazaki, 2007) using the same features and set-up (for both word segmentation and POS tagging) to examine the contribution of context information (2-CRF). To create the dictionary, we added all of the words in the corpus, but left out a small portion of singletons to prevent overfitting on the training data 3 . As an evaluation measure, we follow Nagata (1994) and<cite> Kudo et al. (2004)</cite> and use Word/POS tag pair Fmeasure, so that both word boundaries and POS tags must be correct for a word to be considered correct.",
  "y": "uses"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_1",
  "x": "In each relation, EDUs can be Nucleus (more essential) or Satellite to the writer's purpose. Many approaches have been used in DP, the majority of them using machine learning algorithms, such as probabilistic models<cite> (Soricut and Marcu, 2003)</cite> , SVMs (Reitter, 2003; duVerle and Prendinger, 2009; Hernault et al., 2010; Feng and Hirst, 2012) and dynamic conditional random field (Joty et al., 2012) . To obtain acceptable results, these approaches need plenty of labeled data.",
  "y": "background"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_2",
  "x": "Many approaches have been used in DP, the majority of them using machine learning algorithms, such as probabilistic models<cite> (Soricut and Marcu, 2003)</cite> , SVMs (Reitter, 2003; duVerle and Prendinger, 2009; Hernault et al., 2010; Feng and Hirst, 2012) and dynamic conditional random field (Joty et al., 2012) . To obtain acceptable results, these approaches need plenty of labeled data. Given this fact, what can we do when there is not enough data to perform effective learning of DP, as in languages with little annotated data?",
  "y": "motivation"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_3",
  "x": "A framework of semi-supervised never-ending learning (SSNEL) (see Section 2.2 below) was created and evaluated with the adapted models. The results show that this approach improved the results to achieve nearhuman perfomance, even with the use of automatic tools (syntax parser and discourse segmenter). 2 Related Work 2.1 Supervised Discourse Parsing<cite> Soricut and Marcu (2003)</cite> use two probabilistic models to perform a sentence-level analysis, one for segmentation and other to identify the relations and build the rhetorical structure.",
  "y": "background"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_4",
  "x": "Their architecture runs 24 hours per day, forever, obtaining new information and performing a learning task. With the aim of surpassing the limitation of labeled RST in Portuguese to develop a good DP, we employ SSNEL in the task by adapting the work of<cite> Soricut and Marcu (2003)</cite> and Hernault et al. (2010) . This choice for SSLNEL was made considering the large and free availability of news texts on the web.",
  "y": "uses"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_5",
  "x": "Table 1 compares it with available Portuguese corpora labeled according to RST (these corpora will be referred to as RST-DT-PT hereafter). The corpora CSTNews (Cardoso et al., 2011) , Summ-it (Collovini et al., 2007) and two-thirds of Rhetalho (Pardo and Seno, (45) and many more words (55,536) than RST-DT-PT. This work focuses on the identification of rhetorical relations at the sentence level, and as is common since the work of<cite> Soricut and Marcu (2003)</cite> , fine-grained relations were grouped: 29 sentence-level rhetorical relations were found and grouped into 16 groups.",
  "y": "uses similarities background"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_6",
  "x": "**ADAPTED MODELS** Syntactic information is crucial in SPADE<cite> (Soricut and Marcu, 2003)</cite> and for Portuguese the parser most similar to that used by Soricut and Marcu is the LX-parser (Stanford parser trained to Portuguese (Silva et al., 2010) ). After the parsing of the text by the syntactic parser, the same lexicalization procedure (Magerman, 1995) was applied and adapted according to the tagset used by LX-parser.",
  "y": "uses"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_7",
  "x": "**ADAPTED MODELS** Syntactic information is crucial in SPADE<cite> (Soricut and Marcu, 2003)</cite> and for Portuguese the parser most similar to that used by Soricut and Marcu is the LX-parser (Stanford parser trained to Portuguese (Silva et al., 2010) ). After the parsing of the text by the syntactic parser, the same lexicalization procedure (Magerman, 1995) was applied and adapted according to the tagset used by LX-parser.",
  "y": "background"
 },
 {
  "id": "3d99ad1ba1696c8ef743f233530601_8",
  "x": "Using separated test data, we tried to avoid possible overfitting on training data, but the size of test data may not lead to a fair evaluation<cite> Soricut and Marcu (2003)</cite> or Joty et al. (2012) , since HILDA-PT used different corpora (RST-DT-PT instead of RST-DT), and some reported results are for the complete DP. However, our results show the potential of the SSNEL workflow when not enough labeled data is available for supervised learning, since the same approach for relation identification of Hernault et al. (2010) was used in HILDA-PT and 0.531 was initially obtained. These results constitute the state of art for rhetorical relation identification for Portuguese and it is believed that with more time (iterations in SSNEL), the results may increase.",
  "y": "background"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_0",
  "x": "Machine translation (MT) from Chinese to English has been a difficult problem: structural differences between Chinese and English, such as the different orderings of head nouns and relative clauses, cause BLEU scores to be consistently lower than for other difficult language pairs like Arabic-English. But use of hierarchical decoders has not solved the DE construction translation problem. An alternative way of dealing with structural differences is to reorder source language sentences to minimize structural divergence with the target language, (Xia and McCord, 2004; Collins et al., 2005;<cite> Wang et al., 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_1",
  "x": "For example <cite>Wang et al. (2007)</cite> introduced a set of rules to decide if a (DE) construction should be reordered or not before translating to English: \u2022 For DNPs (consisting of\"XP+DEG\"): -Reorder if XP is PP or LCP; -Reorder if XP is a non-pronominal NP \u2022 For CPs (typically formed by \"IP+DEC\"): -Reorder to align with the \"that+clause\" structure of English. Although this and previous reordering work has led to significant improvements, errors still remain. Indeed, <cite>Wang et al. (2007)</cite> found that the precision of their NP rules is only about 54.6% on a small human-judged set. One possible reason the (DE) construction remains unsolved is that previous work has paid insufficient attention to the many ways the (DE) construction can be translated and the rich structural cues to the translation.",
  "y": "background motivation"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_3",
  "x": "According to the Chinese Treebank tagging guidelines (Xia, 2000) , the character can be tagged as DEC, DEG, DEV, SP, DER, or AS. Similar to<cite> (Wang et al., 2007)</cite> , we only consider the majority case when the phrase with (DE) is a noun phrase modifier. The DEs in NPs have a part-of-speech tag of DEC (a complementizer or a nominalizer) or DEG (a genitive marker or an associative marker).",
  "y": "similarities background"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_4",
  "x": "The way we categorize the DEs is based on their behavior when translated into English. This is implicitly done in the work of <cite>Wang et al. (2007)</cite> where they use rules to decide if a certain DE and the words next to it will need to be reordered. Some NPs are translated into a hybrid of these categories, or just don't fit into one of the five categories, for instance, involving an adjectival premodifier and a relative clause.",
  "y": "similarities background"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_6",
  "x": "2 As a baseline, we use the rules introduced in <cite>Wang et al. (2007)</cite> to decide if the DEs require reordering or not. However, since their rules only decide if there is reordering in an NP with DE, their classification result only has two classes. So, in order to compare our classifier's performance with the rules in <cite>Wang et al. (2007)</cite> , we have to map our five-class results into two classes. We mapped our five-class results into two classes. So we mapped B preposition A and relative clause into the class \"reordered\", and the other three classes into \"not-reordered\".",
  "y": "extends"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_8",
  "x": "The 2-class accuracy is still lower than using the heuristic rules in<cite> (Wang et al., 2007)</cite> , which is reasonable because their rules encode more information than just the POS tags of DEs. A-pattern: Chinese syntactic patterns appearing before Secondly, we want to incorporate the rules in<cite> (Wang et al., 2007)</cite> as features in the log-linear classifier.",
  "y": "uses"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_9",
  "x": "4. A ends with VA: true if A+DE is a CP which is in the form of \"IP+DEC\", and the IP ends with a VP that's either just a VA or a VP preceded by a ADVP. Features 1-3 are inspired by the rules in<cite> (Wang et al., 2007)</cite> , and the fourth rule is based on the observation that even though the predicative adjective VA acts as a verb, it actually corresponds to adjectives in English as described in (Xia, 2000) . 3 We call these four features A-pattern.",
  "y": "uses similarities"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_11",
  "x": "We have two different settings as baseline experiments. Also, we reorder the training data, the tuning and the test sets with the NP rules in<cite> (Wang et al., 2007)</cite> and compare our results with this second baseline (WANG-NP). The NP reordering preprocessing (WANG-NP) showed consistent improvement in Table 5 on all test sets, with BLEU point gains ranging from 0.15 to 0.40. This confirms that having reordering around DEs in NP helps Chinese-English MT.",
  "y": "uses background"
 },
 {
  "id": "3dbdf61d07a3e35ac1b6ecc7ab3999_12",
  "x": "Our approach DE-Annotated reorders the Chinese sentence, which is similar to the approach proposed by <cite>Wang et al. (2007)</cite> (WANG-NP). However, our focus is on the annotation on DEs and how this can improve translation quality. Table 7 shows an example that contains a DE construction that translates into a relative clause in English.",
  "y": "similarities differences"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_0",
  "x": "In October 2017, the semi-autonomous region of Catalonia held a referendum on independence from Spain, where 92% of respondents voted for independence (Fotheringham, 2017) . To determine the role of the local language Catalan in * Equal contributions. this setting, we apply the methodology used by <cite>Shoemark et al. (2017)</cite> in the context of the 2014 Scottish independence referendum to a dataset of tweets related to the Catalonian referendum.",
  "y": "similarities uses"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_1",
  "x": "A relatively unexplored area of code-switching behavior is politically-motivated code-switching, which we assume has a different set of constraints compared to everyday code-switching. With respect to political separatism, <cite>Shoemark et al. (2017)</cite> studied the use of Scots, a language local to Scotland, in the context of the 2014 Scotland independence referendum. They found that Twitter users who openly supported Scottish independence were more likely to incorporate words from Scots in their tweets.",
  "y": "background"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_2",
  "x": "They also found that Twitter users who tweeted about the referendum were less likely to use Scots in referendum-related tweets than in non-referendum tweets. This study considers the similar scenario which took place in 2017 vis-\u00e0-vis the semi-autonomous region of Catalonia. Our main methodological divergence from <cite>Shoemark et al. (2017)</cite> relates to the linguistic phenomenon at hand: while Scots is mainly manifested as interleaving individual words within English text (code-mixing), Catalan is a distinct language which, when used, usually replaces Spanish altogether for the entire tweet (code-switching).",
  "y": "similarities"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_3",
  "x": "3 After including ASCII-equivalent variants of special characters, as well as lowercased variants, our final hashtag set comprises 111 unique strings. Next, all tweets containing any referendum hashtag were extracted from T , yielding 190,061 tweets. After removing retweets and tweets from users whose tweets frequently contained URLs (i.e., likely bots), our final \"Catalonian Independence Tweets\" (CT) dataset is made up of 11,670 tweets from 10,498 users (cf. the Scottish referendum set IT with 59,664 tweets and 18,589 users in <cite>Shoemark et al. (2017)</cite> ).",
  "y": "similarities uses"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_4",
  "x": "After removing retweets and tweets from users whose tweets frequently contained URLs (i.e., likely bots), our final \"Catalonian Independence Tweets\" (CT) dataset is made up of 11,670 tweets from 10,498 users (cf. the Scottish referendum set IT with 59,664 tweets and 18,589 users in <cite>Shoemark et al. (2017)</cite> ). 36 referendum-related hashtags appear in the filtered dataset. They are shown with their frequencies (including variants) in Table 1 (cf. the 47 hashtags and similar frequency distribution in Table 1 of <cite>Shoemark et al. (2017)</cite> ).",
  "y": "background"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_5",
  "x": "They are shown with their frequencies (including variants) in Table 1 (cf. the 47 hashtags and similar frequency distribution in Table 1 of <cite>Shoemark et al. (2017)</cite> ). To address the control condition, all authors of tweets in the CT dataset were collected to form a set U , and all other tweets in T written by these users were extracted into a control dataset (XT) of 45,222 tweets (cf. the 693,815 control tweets in Table 6 of <cite>Shoemark et al. (2017)</cite> ).",
  "y": "background"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_6",
  "x": "Results. Catalan is used more often among the pro-independence users compared to the antiindependence users, across both the hashtagonly and all-tweet conditions. Table 3 shows that the proportion of tweets in Catalan for proindependence users (p pro ) is significantly higher than the proportion for anti-independence users (p anti ). This is consistent with <cite>Shoemark et al. (2017)</cite> , who found more Scots usage among proindependence users (d = 0.00555 for pro/anti tweets, d = 0.00709 for all tweets).",
  "y": "similarities"
 },
 {
  "id": "3e0704e0928f2df8b7c2ffa9863a55_7",
  "x": "The lack of a significant difference between referendum-related hashtags and other hashtags suggests that the topic being discussed is not as central in choosing one's language, compared with the audience being targeted. Our second result is the opposite of the prior finding that there were significantly fewer Scots words in referendum-related tweets than in control tweets (cf. Table 7 in <cite>Shoemark et al. (2017)</cite> ; d u = \u22120.0015 for all controls).",
  "y": "differences"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_0",
  "x": "Since Ramshaw and Marcus approached NP chunking using a machine learning method, many researchers have used various machine learning techniques [2, 4, 5, <cite>6,</cite> 10, 11, 13, 14] . The chunking task was extended to the CoNLL-2000 shared task with standard datasets and evaluation metrics, which is now a standard evaluation task for text chunking [3] . Most previous works with relatively high performance in English used machine learning methods for chunking [4, 13] .",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_1",
  "x": "Unlike English, a rule-based chunking method [7, 8] is predominantly used in Korean because of its well-developed function words, which contain information such as grammatical relation, case, tense, modal, etc. Chunking in Korean texts with only simple heuristic rules obtained through observation on the text shows a good performance similar to other machine learning methods<cite> [6]</cite> . Park et al. proposed a hybrid of rule-based and machine learning method to handle exceptional cases of the rules, to improve the performance of chunking in Korean texts [5,<cite> 6]</cite> .",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_2",
  "x": "Park et al. proposed a hybrid of rule-based and machine learning method to handle exceptional cases of the rules, to improve the performance of chunking in Korean texts [5,<cite> 6]</cite> . In this paper, we present how CRFs, a recently introduced probabilistic model for labeling and segmenting sequence of data [12] , can be applied to the task of chunking in Korean texts. CRFs are undirected graphical models trained to maximize conditional probabilities of label sequence given input sequence.",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_3",
  "x": "Chunking in Korean texts with only simple heuristic rules obtained through observation on the text shows a good performance similar to other machine learning methods<cite> [6]</cite> . Park et al. proposed a hybrid of rule-based and machine learning method to handle exceptional cases of the rules, to improve the performance of chunking in Korean texts [5,<cite> 6]</cite> . In this paper, we present how CRFs, a recently introduced probabilistic model for labeling and segmenting sequence of data [12] , can be applied to the task of chunking in Korean texts.",
  "y": "uses background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_4",
  "x": "Well-developed function words in Korean help with chunking, especially NP and VP chunking. For example, when the part-of-speech of current word is one of determiner, pronoun and noun, the following seven rules for NP chunking in Table 1 can find most NP chunks in text, with about 89% accuracy<cite> [6]</cite> . For this reason, boundaries of chunks are easily found in Korean, compared to other languages such as English or Chinese.",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_5",
  "x": "This is why a rule-based chunking method is predominantly used. However, with sophisticated rules, the rule-based chunking method has limitations when handling exceptional cases. Park et al. proposed a hybrid of the rule-based and the machine learning method to resolve this problem [5,<cite> 6]</cite> .",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_6",
  "x": "In Korean, there are four basic phrases: noun phrase (NP), verb phrase (VP), adverb phrase (ADVP), and independent phrase (IP)<cite> [6]</cite> . As function words such as postposition or ending are well-developed, the number of chunk types is small compared to other languages such as English or Chinese. Like the CoNLL-2000 dataset, we use three types of chunk border tags, indicating whether a word is outside a chunk (O), starts a chunk (B), or continues a chunk (I).",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_7",
  "x": "Using more features such as previous chunk tag should be able to improve the performance of NP chunk type. Park et al. reported the performance of various chunking methods<cite> [6]</cite> . We add the experimental results of the chunking methods using HMMs-bigram and CRFs.",
  "y": "background"
 },
 {
  "id": "3e86788379f2c0074ff16687d68fc9_8",
  "x": "Using more features such as previous chunk tag should be able to improve the performance of NP chunk type. Park et al. reported the performance of various chunking methods<cite> [6]</cite> . We add the experimental results of the chunking methods using HMMs-bigram and CRFs.",
  "y": "similarities"
 },
 {
  "id": "3ec2dc9530699f55b8a4c234532daf_0",
  "x": "Chatbots aim to engage users in open-domain human-computer conversations and are currently receiving increasing attention. The existing work on building chatbots includes generation-based methods and retrieval-based methods. In this paper, we focus on the second type and study the problem of multi-turn response selection. This task aims to select the best-matched response from a set of candidates, given the context of a conversation which is composed of multiple utterances<cite> (Lowe et al., 2015</cite>; Lowe et al., 2017; Wu et al., 2017 ).",
  "y": "background"
 },
 {
  "id": "3ec2dc9530699f55b8a4c234532daf_1",
  "x": "Recently, some extended work has been made to incorporate external knowledge into generation with specific personas or emotions (Li et al., 2016; Zhou et al., 2018a) . Our work belongs to the retrieval-based methods, which learn a matching model for a pair of a conversational context and a response candidate. This approach has the advantage of providing informative and fluent responses because they select a proper response for the current conversation from a repository by means of response selection algorithms<cite> (Lowe et al., 2015</cite>; Lowe et al., 2017 EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EB  EB  EB  EB  EB   E0  E1  E2  E3  E4  E5  E18  E19  E6  E7  E8  E9  E10  E11  E17  E12  E13  E14  E15  E16  E20  E21  E22  E23  E24  E25 [ E0  E0  E0  E0  E0  E0  E0  E0  E1  E1  E1  E1  E1  E1  E0  E1  E1  E1  E1  E1  E0  E1  E1  E1  E1  E1 Speaker Embeddings + + + + + + + + + + + + + + + + + + + + + + + + + + Figure 1 : The input representation of SA-BERT.",
  "y": "similarities background"
 },
 {
  "id": "3ec2dc9530699f55b8a4c234532daf_2",
  "x": "We tested SA-BERT on five public multi-turn response selection datasets, Ubuntu Dialogue Corpus V1<cite> (Lowe et al., 2015)</cite> , Ubuntu Dialogue Corpus V2 (Lowe et al., 2017) , Douban Conversation Corpus (Wu et al., 2017) , E-commerce Dialogue Corpus (Zhang et al., 2018b) and DSTC 8-Track 2-Subtask 2 Corpus (Seokhwan Kim, 2019). The first four datasets have been disentangled in advance and our proposed speaker-aware disentanglement strategy has been applied to only the last DSTC 8-Track 2-Subtask 2 Corpus. Ubuntu Dialogue Corpus V1, V2 and DSTC 8-Track 2-Subtask 2 Corpus contain multi-turn dialogues about Ubuntu system troubleshooting in English.",
  "y": "uses background"
 },
 {
  "id": "3ec2dc9530699f55b8a4c234532daf_3",
  "x": "We used the same evaluation metrics as those used in previous work<cite> (Lowe et al., 2015</cite>; Lowe et al., 2017; Wu et al., 2017; Zhang et al., 2018b; Seokhwan Kim, 2019) . Each model was tasked with selecting the k best-matched responses from n available candidates for the given conversation context c, and we calculated the recall of the true positive replies among the k selected responses, denoted as R n @k, as the main evaluation metric. In addition to R n @k, we considered the mean average precision (MAP) (BaezaYates and Ribeiro-Neto, 1999), mean reciprocal rank (MRR) (Voorhees, 1999) and precision-at-one (P@1), especially for the Douban corpus, following the settings of previous work.",
  "y": "uses"
 },
 {
  "id": "400bd47879aaed0aa1195bafe54e76_0",
  "x": "Despite the existence of various Indonesian pretrained word embeddings, there are no publicly available Indonesian analogy task datasets on which to evaluate these embeddings. Consequently, it is unknown if Indonesian word embeddings introduced in, e.g., (Al-Rfou et al., 2013) and <cite>(Grave et al., 2018)</cite> , capture syntactic or semantic information as measured by analogy tasks. Also, such embeddings are usually trained on Indonesian Wikipedia (Al-Rfou et al., 2013; Bojanowski et al., 2017) whose size is relatively small, approximately 60M tokens.",
  "y": "background"
 },
 {
  "id": "400bd47879aaed0aa1195bafe54e76_1",
  "x": "1 One of the goals of this work is to evaluate and compare existing Indonesian pretrained word embeddings. We used fastText pretrained embeddings introduced in (Bojanowski et al., 2017 ) and <cite>(Grave et al., 2018)</cite> , which have been trained on Indonesian Wikipedia and Indonesian Wikipedia plus Common Crawl data respectively. We refer to them as Wiki/fastText and CC/fastText hereinafter.",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_0",
  "x": "Distributed word representations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; . Traditionally, word representations have been obtained using countbased methods (Baroni et al., 2014) , where the cooccurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; . Techniques for generating lower-rank representations have also been employed, such as PPMI-SVD <cite>(Levy et al., 2015)</cite> and GloVe (Pennington et al., 2014) , both achieving state-of-the-art performance on a variety of tasks.",
  "y": "background"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_1",
  "x": "The performance of the PPMI matrix on word similarity tasks can be further improved by using context-distribution smoothing <cite>(Levy et al., 2015)</cite> and subsampling the corpus (Mikolov et al., 2013b) . As word embeddings with lower dimensionality may improve efficiency and generalization <cite>(Levy et al., 2015)</cite> , the improved PPMI * matrix can be factorized as a product of two lower rank matrices. where W w andW c are d-dimensional row vectors corresponding to vector embeddings for the target and context words.",
  "y": "background"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_2",
  "x": "The performance of the PPMI matrix on word similarity tasks can be further improved by using context-distribution smoothing <cite>(Levy et al., 2015)</cite> and subsampling the corpus (Mikolov et al., 2013b) . As word embeddings with lower dimensionality may improve efficiency and generalization <cite>(Levy et al., 2015)</cite> , the improved PPMI * matrix can be factorized as a product of two lower rank matrices. where W w andW c are d-dimensional row vectors corresponding to vector embeddings for the target and context words.",
  "y": "background"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_3",
  "x": "Levy et al. (2015) recommend using W = U \u03a3 p as the word representations, as suggested by Bullinaria and Levy (2012) , who borrowed the idea of weighting singular values from the work of Caron (2001) on Latent Semantic Analysis. Although the optimal value of p is highly task-dependent (\u00d6sterlund et al., 2015) , we set p = 0.5 as it has been shown to perform well on the word similarity and analogy tasks we use in our experiments <cite>(Levy et al., 2015)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_4",
  "x": "All models generate embeddings of 300 dimensions. The PPMI* matrix used by both PPMI-SVD and LexVec was constructed using smoothing of \u03b1 = 3/4 suggested in <cite>(Levy et al., 2015)</cite> and an unweighted window of size 2. A dirty subsampling of the corpus is adopted for PPMI* and SGNS with threshold of t = 10 \u22125 (Mikolov et al., 2013b) .",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_5",
  "x": "The PPMI* matrix used by both PPMI-SVD and LexVec was constructed using smoothing of \u03b1 = 3/4 suggested in <cite>(Levy et al., 2015)</cite> and an unweighted window of size 2. A dirty subsampling of the corpus is adopted for PPMI* and SGNS with threshold of t = 10 \u22125 (Mikolov et al., 2013b) . 2 Additionally, SGNS uses 5 negative samples (Mikolov et al., 2013b) , a window of size 10 (<cite> Levy et al., 2015)</cite> , for 5 iterations with initial learning rate set to the default 0.025.",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_6",
  "x": "All methods generate both word and context matrices (W andW ): W is used for SGNS, PPMI-SVD and W +W for GloVe (following<cite> Levy et al. (2015)</cite> , and W and W +W for LexVec. For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Pennington et al., 2014;<cite> Levy et al., 2015</cite> factorization of logM ) and Skip-gram (implicit factorization of the shifted PMI matrix), and compare the stochastic and mini-batch approaches. Word similarity tasks are: 3 WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001) , MEN (Bruni et al., 2012 ), MTurk (Radinsky et al., 2011 , RW (Luong et al., 2013) , SimLex-999 (Hill et al., 2015) , MC (Miller and Charles, 1991) , RG (Rubenstein and Goodenough, 1965) , and SCWS (Huang et al., 2012) , calculated using cosine.",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_7",
  "x": "For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Pennington et al., 2014;<cite> Levy et al., 2015</cite> factorization of logM ) and Skip-gram (implicit factorization of the shifted PMI matrix), and compare the stochastic and mini-batch approaches. Word similarity tasks are: 3 WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001) , MEN (Bruni et al., 2012 ), MTurk (Radinsky et al., 2011 , RW (Luong et al., 2013) , SimLex-999 (Hill et al., 2015) , MC (Miller and Charles, 1991) , RG (Rubenstein and Goodenough, 1965) , and SCWS (Huang et al., 2012) , calculated using cosine. Word analogy tasks are: Google semantic (GSem) and syntactic (GSyn) (Mikolov et al., 2013a) and MSR syntactic analogy dataset (Mikolov et al., 2013c) , using 3CosAdd and 3CosM ul .",
  "y": "uses"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_8",
  "x": "In terms of configurations, WS SGN S performed marginally better than WS P P M I . This is inline with results for PPMI-SVD and SGNS models <cite>(Levy et al., 2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_9",
  "x": "We hypothesize it is simply because of the additional computation. While W and (W +W ) are roughly equivalent on word similarity tasks, W is better for analogies. This is inline with results for PPMI-SVD and SGNS models <cite>(Levy et al., 2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "40742bac72bbbaed4755ff0b74d599_10",
  "x": [
   "We believe the poor syntactic performance is a result of the PPMI measure. PPMI-SVD also struggled with syntactic analogies more than any other task. Levy et al. (2015) obtained similar results, and suggest that using positional contexts as done by might help in recovering syntactic analogies."
  ],
  "y": "similarities"
 },
 {
  "id": "40d370558d873499e493a83f106f17_0",
  "x": "Large-scale distributional thesauri created automatically from corpora (Grefenstette, 1994;<cite> Lin, 1998</cite>; Weeds et al., 2004; Ferret, 2012) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998 ) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts.",
  "y": "background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_1",
  "x": "To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures<cite> (Lin, 1998</cite>; Curran and Moens, 2002; Ferret, 2010) , identifying and demoting bad neighbors (Ferret, 2013) , or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013) .",
  "y": "background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_2",
  "x": "**RELATED WORK** In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears<cite> (Lin, 1998</cite>; McCarthy et al., 2003; Weeds et al., 2004) .",
  "y": "background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_3",
  "x": "In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears<cite> (Lin, 1998</cite>; McCarthy et al., 2003; Weeds et al., 2004) . The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like <cite>Lin's (1998)</cite> , cosine, Jensen-Shannon divergence, Dice or Jaccard.",
  "y": "background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_4",
  "x": "Evaluation of the quality of distributional thesauri is a well know problem in the area<cite> (Lin, 1998</cite>; Curran and Moens, 2002) . For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri<cite> (Lin, 1998)</cite> , and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004) . Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed.",
  "y": "motivation background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_5",
  "x": "For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri<cite> (Lin, 1998)</cite> , and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004) . Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed. For instance, Biemann and Riedl (2013) found that filtering a subset of contexts based on LMI increased the similarity of a thesaurus with WordNet.",
  "y": "background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_6",
  "x": "The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like <cite>Lin's (1998)</cite> , cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area<cite> (Lin, 1998</cite>; Curran and Moens, 2002) . For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri<cite> (Lin, 1998)</cite> , and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004) .",
  "y": "motivation background"
 },
 {
  "id": "40d370558d873499e493a83f106f17_7",
  "x": "For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri<cite> (Lin, 1998)</cite> , and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004) . Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed. For instance, Biemann and Riedl (2013) found that filtering a subset of contexts based on LMI increased the similarity of a thesaurus with WordNet.",
  "y": "motivation"
 },
 {
  "id": "40d370558d873499e493a83f106f17_8",
  "x": "The thesauri were constructed using <cite>Lin's (1998)</cite> method. Lin's version of the distributional hypothesis states that two words (verbs v 1 and v 2 in our case) are similar if they share a large proportion of contexts weighted by their information content, assessed with PMI (Bansal et al., 2012; Turney, 2013) . In the literature, little attention is paid to context filters.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_0",
  "x": "It is thus imperative to evaluate the extent to which they exhibit social and intersectional bias. May et al. <cite>[21]</cite> establish a preliminary study of social bias in BERT, but their analysis relies only on sentence level encodings. Our work extends beyond this to provide an analysis of gender and racial bias on a variety of state-of-the-art contextual word models.",
  "y": "background motivation"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_1",
  "x": "Our work extends beyond this to provide an analysis of gender and racial bias on a variety of state-of-the-art contextual word models. We also evaluate intersectional (gender + race) bias, since the lived experience of groups with multiple minority identities is cumulatively worse than that of each of the groups with a singular minority identity [9] . We adapt the Sentence Encoder Association Test (SEAT) <cite>[21]</cite> to evaluate how these techniques displays bias in contextual word representations.",
  "y": "extends"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_2",
  "x": "Zhao et al. [35] and Basta et al. [1] demonstrate gender bias in ELMo [25] word embeddings, whereas May et al. <cite>[21]</cite> evaluate various models of contextual word representations on a sentential generalization of WEAT. Our work extends such analyses in two ways: 1) we consider a wide variety of contextual word embedding tools including state-of-the-art approaches such as BERT and GPT-2, 2) we extend the evaluation to consider contextual word representations as opposed to prior work, which either used word embeddings without context or used sentence-level embeddings that can have additional confounds and obscure bias, and 3) we include additional embedding association tests targeting gender, race and intersectional identities. 1 and BooksCorpus (800M words) [36] .",
  "y": "background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_3",
  "x": "We adopt the methodology of Caliskan et al. [5] and May et al. <cite>[21]</cite> to test social and intersectional bias using embedding association tests with contextual word representations. Caliskan et al. [5] proposed Word Embedding Association Tests (WEATs), which follows human implicit association tests [14] in measuring the association between two target concepts and two attributes. We follow May et al. <cite>[21]</cite> in describing WEATs and SEATs.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_4",
  "x": "We adopt the methodology of Caliskan et al. [5] and May et al. <cite>[21]</cite> to test social and intersectional bias using embedding association tests with contextual word representations. Caliskan et al. [5] proposed Word Embedding Association Tests (WEATs), which follows human implicit association tests [14] in measuring the association between two target concepts and two attributes. We follow May et al. <cite>[21]</cite> in describing WEATs and SEATs.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_5",
  "x": "A larger effect size corresponds to more severe pro-stereotypical representations, controlling for significance. May et al. <cite>[21]</cite> adopt the WEAT tests [5] into Sentence Encoder Association Tests (SEATs) to test biases using sentence encodings. The embeddings used in the association tests are encodings of a sentence, which are obtained by pooling per token contextual representations, or by using the representation of the first or last token.",
  "y": "background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_6",
  "x": "**EXTENSION OF EMBEDDING ASSOCIATION TESTS TO CONTEXTUAL WORD REPRESENTATIONS** May et al. <cite>[21]</cite> suggest that although they find less bias in sentence encoders than context free word embeddings, the sentence templates may not be as semantically bleached as expected, and that a lack of evidence of bias should not be taken as a lack of bias. We propose to assess bias at the contextual word level.",
  "y": "background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_7",
  "x": "To investigate social and intersectional bias, we introduce new embedding association tests to more comprehensively target race, gender and intersectional identities. The new tests are prefixed by a \"+\" in Tables 3, 4 and 5. For race and gender, we are interested in attributes of pleasantness (P/U: Pleasant/Unpleasant), work (Career/Family), discipline (Science/Arts) [5] and the Heilman double bind [15,<cite> 21]</cite> .",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_8",
  "x": "The Heilman double bind refers to how women, when clearly succeeding in a stereotypically male occupation, are perceived as less likable than similar men, and how women, when success is more ambiguous, are perceived as less competent than similar men. Although the Heilman double bind originated in the context of gender, we also extend the attribute lists 3 of competence and likability to the context of race. We preserve and report the original WEATs, SEATs and tests introduced by May et al. <cite>[21]</cite> where possible.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_9",
  "x": "We also prefer tests using names (e.g. Alice) as concept words over group terms (e.g. European American), since names were demonstrated to have a significant association more often than group terms <cite>[21]</cite> For intersectional identities, we are focused primarily on the identity which is the subject of discussion in the work of Crenshaw [9] : being both African American and female. Specifically, <cite>[21]</cite> , which targets the stereotype of black women as loud, angry, and imposing [8] . 5 Empirical Analysis",
  "y": "uses background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_10",
  "x": "Specifically, <cite>[21]</cite> , which targets the stereotype of black women as loud, angry, and imposing [8] . 5 Empirical Analysis ----------------------------------",
  "y": "background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_11",
  "x": "**EXPERIMENTS** We investigate biases in GPT-2 [27] , one of the state-of-the-art models for contextual word representations, in both its 117M and 345M versions. For comparison with previous work [1,<cite> 21,</cite> 35] , we also report on other word representation models: CBoW-GLoVe [24] , ELMo [25] , BERT bert-base-cased (bbc) and bert-large-cased (blc) versions [10] , and GPT [26] .",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_12",
  "x": "For comparison with previous work [1,<cite> 21,</cite> 35] , we also report on other word representation models: CBoW-GLoVe [24] , ELMo [25] , BERT bert-base-cased (bbc) and bert-large-cased (blc) versions [10] , and GPT [26] . For all association tests, we use p = 0.01 for significance testing. We use PyTorch, as well as the framework and code from May et al. <cite>[21]</cite> , to conduct the experiments 6 .",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_13",
  "x": "Furthermore, methods for de-biasing specifically across race, gender, and intersectional identities remains a challenging open question. Similar to May et al. <cite>[21]</cite> , in the continuous bag of words (CBoW) model we encode sentences as the average of word embeddings using 300-dimensional GloVe vectors 7 trained on the Common Crawl corpus [24] . There is no corresponding contextual word level equivalent since GloVe is context-free.",
  "y": "similarities"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_14",
  "x": "**A.2 ELMO** Following May et al. <cite>[21]</cite> , the sentence encoding of ELMo is a sequence of vectors, one for each token. We use mean-pooling over the tokens, followed by summation over aggregated layer outputs to obtain the final 1024-dimensional sentence encoding.",
  "y": "uses"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_15",
  "x": "---------------------------------- **A.4 GPT** Similar to May et al. <cite>[21]</cite> and the original word [26] , we use the representation corresponding to the last word in the sequence as the sentence encoding.",
  "y": "similarities"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_16",
  "x": "Both encoding types are 768-dimensional. Different from May et al. <cite>[21]</cite> , we use the implementation of GPT from Hugging Face, and not the jiant project 10 . prefix weat but sentence level tests have the prefix sent-weat.",
  "y": "differences"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_17",
  "x": "The Caliskan Tests are detailed in Caliskan et al. [5] , and the double bind tests are detailed in May et al. <cite>[21]</cite> . The additional tests are detailed in the sections below. In the full results presented in Tables 7 and 8 , we also report results for neutral tests (C1-2) and tests regarding disability and age (C9-10).",
  "y": "background"
 },
 {
  "id": "41a23b1ae47d83315a047497691117_18",
  "x": "In both the race and gender double bind tests, sentence templates that are either bleached of semantic meaning or unbleached are used. Following May et al. <cite>[21]</cite> , examples of such templates (non-exhaustive) are as below. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_0",
  "x": "In NLP, Neural language model pre-training has shown to be effective for improving many tasks [12, 26] . Transformer<cite> [34]</cite> is based solely on the attention mechanism, and dispensing with recurrent and convolutions entirely. At present, this model has received extensive attentions and plays an key role in many neural language models, such as BERT [12] , GPT [27] and Universal Transformer [10] .",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_1",
  "x": "In Transformer, the multi-head attention is a key part and it is constructed by a large number of parameters. Specifically, Ashish et.al<cite> [34]</cite> compute the attention function on a set of queries simultaneously, packed together into a matrix Q, while the keys and values are also packed together into matrices K and V , respectively. The attention function then adopts a no-linear function sof tmax over three matrices Q, K and V .",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_2",
  "x": "Specifically, Ashish et.al<cite> [34]</cite> compute the attention function on a set of queries simultaneously, packed together into a matrix Q, while the keys and values are also packed together into matrices K and V , respectively. The attention function then adopts a no-linear function sof tmax over three matrices Q, K and V . There are two challenges to find a high-quality compression method to compress the multi-head attention in Transformer.",
  "y": "motivation"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_3",
  "x": "In order to address this challenge, we first prove that the output of the attention function of the self-attention model<cite> [34]</cite> can be linearly represented by a group of orthonormal base vectors. Q, K and V can be considered as factor matrices. Then, by initializing a low rank core tensor, we use Tucker-decomposition [32, 20] to reconstruct a new attention representation.",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_4",
  "x": "This process can lead to reduce many parameters. The second challenge is that the attention model after compressing can not be directly integrated into the encoder and decoder framework of Transformer <cite>[34,</cite> 7] . In order to address this challenge, there are three steps as follows.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_5",
  "x": "Third, the concatenation of these matrices can serve as the input to the next layer network in Transformer. After that, it can be integrated into the encoder and decoder framework of Transformer <cite>[34,</cite> 7] and trained end-to-end. Moreover, we also prove that the 3-order tensor can reconstruct the scaled dot-product attention in Transformer by a sum on a particular dimension.",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_6",
  "x": "We cover below in Section 2.1 basic background on Block-Term tensor decomposition [9] . Then, we describe in Section 2.2 multi-head attention<cite> [34]</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_7",
  "x": "In Transformer, the attention function is named as \"Scaled Dot-Product Attention\". In practice, Transformer<cite> [34]</cite> processes query, keys and values as matrices Q, K, and V respectively. The attention function can be written as follows:",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_8",
  "x": "where d is the number of columns of Q and K. In these work <cite>[34,</cite> 12, 7] , they all use the multi-head attention, as introduced in<cite> [34]</cite> , where matrices W Q i and In this work<cite> [34]</cite> , multiple groups of parameters (W Q i , W K i and W V i ) are used, which results in a large number of redundant parameters.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_9",
  "x": "**ANALYSIS OF COMPRESSION AND COMPLEXITY** Compression Our focus is on the compression of the multi-head mechanism in the multi-head attention of Transformer. Previous work<cite> [34]</cite> gets the multi-head attention by multiple groups of linear mappings.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_10",
  "x": "Previous work<cite> [34]</cite> gets the multi-head attention by multiple groups of linear mappings. We use three linear ma for matrices Q, K and V , respectively. For the output of three mappings, we choose to share them which are considered as three factor matrices in reconstructing the Multi-linear attention.",
  "y": "similarities"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_11",
  "x": "This process is shown in Figure 2 (left) . h is the number of heads in<cite> [34]</cite> , and d is the dimension of factor matrices. The compression ratios can be computed by (3 \u00d7 h \u00d7 d)/(3 \u00d7 d + h).",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_12",
  "x": "However, we can reorder the computations to reduce the model complexity O(R 2 d), where R is the rank of the tensor which can be set in our experiments. In our experiments, R is set as the number between 10 and 18 which is smaller than N . The minimum number of sequential operations in Multi-linear attention for different layers is lower than that of the self-attention in Transformer<cite> [34]</cite> .",
  "y": "differences"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_13",
  "x": "**RELATED WORK** The field of language modeling has witnessed many significant advances. Different from the architectures of convolutional neural network (CNNs) and recurrent neural networks (RNNs) language modeling, the Transformer<cite> [34]</cite> and its variants [7, 12, 10] achieve excellent results in language modeling processing.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_14",
  "x": "Vaswani et al.<cite> [34]</cite> uses a segment-level recurrence mechanism and a novel positional encoding scheme to resolve this question. BERT [12] is a kind of bidirectional encoder representations from transformers. It is designed to pre-train deep bidirectional representation and obtains new SoTA on some NLP tasks.",
  "y": "background"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_15",
  "x": "In this task, we have trained the Transformer model<cite> [34]</cite> on WMT 2016 English-German dataset [30] . Sentences were tokenized using the SentencePiece 3 . For our experiments, we have replaced each of the attention layers with Multi-linear attention.",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_16",
  "x": "For our experiments, we have replaced each of the attention layers with Multi-linear attention. For evaluation we used beam search with a beam size of 5 and length penalty \u03b1=0.6. In this section, we only compared the results with Transformer<cite> [34]</cite> .",
  "y": "uses"
 },
 {
  "id": "41de1ad534ca00b7a99260de7bb0b2_17",
  "x": "For the other baseline, we use the basic Transformer architecture<cite> [34]</cite> . The BLEU score is 34.5 for the basic architecture. We carry out two tensorized Transformer structures, namely core-1 and core-2 respectively.",
  "y": "uses"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_0",
  "x": "Retrieval-based systems are of interest because they admit a natural evaluation metric, namely the recall and precision measures. First introduced for evaluating user simulations by Schatzmann et al. (2005) , such a framework has gained recent prominence for the evaluation of end-to-end dialogue systems<cite> (Lowe et al., 2015a</cite>; Kadlec et al., 2015; Dodge et al., 2015) . These models are trained on the task of selecting the correct response from a candidate list, which we call Next-Utterance-Classification (NUC, detailed in Section 2), and are evaluated using the metric of recall.",
  "y": "background"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_1",
  "x": "With the huge size of current dialogue datasets that contain millions of utterances<cite> (Lowe et al., 2015a</cite>; Banchs, 2012; Ritter et al., 2010) and the increasing amount of natural language data, it is conceivable that retrieval-based systems will be able to have engaging conversations with humans. However, despite the current work with NUC, there has been no verification of whether machine and human performance differ on this task. This cannot be assumed; it is possible that no significant gap exists between the two, as is the case with many current automatic response generation metrics (Liu et al., 2016) .",
  "y": "background"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_2",
  "x": "In this paper, we consider to what extent NUC is achievable by humans, whether human performance varies according to expertise, and whether there is room for machine performance to improve (or has reached human performance already) and we should move to more complex conversational tasks. We performed a user study on three different datasets: the SubTle Corpus of movie dialogues (Banchs, 2012) , the Twitter Corpus (Ritter et al., 2010) , and the Ubuntu Dialogue Corpus<cite> (Lowe et al., 2015a)</cite> . Since conversations in the Ubuntu Dialogue Corpus are highly technical, we recruit 'expert' humans who are adept with the Ubuntu terminology, whom we compare with a state-of-the-art machine learning agent on all datasets.",
  "y": "uses"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_3",
  "x": "Next-Utterance-Classification (NUC, see Figure 1 ) is a task, which is straightforward to evaluate, designed for training and validation of dialogue systems. They are evaluated using the metric of Recall@k, which we define in this section. In NUC, a model or user, when presented with the context of a conversation and a (usually small) pre-defined list of responses, must select the most appropriate response from this list. This list includes the actual next response of the conversation, which is the desired prediction of the model. The other entries, which act as false positives, are sampled from elsewhere in the corpus. Note that no assumptions are made regarding the number of utterances in the context: these can be fixed or sampled from arbitrary distributions. Performance on this task is easy to assess by measuring the success rate of picking the correct next response; more specifically, we measure Recall@k (R@k), which is the percentage of correct responses (i.e. the actual response of the conversation) that are found in the top k responses with the highest rankings according to the model. This task has gained some popularity recently for evaluating dialogue systems<cite> (Lowe et al., 2015a</cite>; Kadlec et al., 2015) .",
  "y": "background"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_4",
  "x": "The number of utterances in the context were sampled according to the procedure in<cite> (Lowe et al., 2015a)</cite> , with a maximum context length of 6 turns -this was done for both the human trials and ANN model. All conversations were preprocessed in order to anonymize the utterances. Table 2 : Average results on each corpus.",
  "y": "uses"
 },
 {
  "id": "43d670e583caab9b38ddce999b8872_5",
  "x": "The human results are separated into AMT nonexperts, consisting of paid respondents who have 'Beginner' or no knowledge of Ubuntu terminology; AMT experts, who claimed to have 'Intermediate' or 'Advanced' knowledge of Ubuntu; and Lab experts. We also presents results on the same task for a state-of-the-art artificial neural network (ANN) dialogue model (see<cite> (Lowe et al., 2015a)</cite> for implementation details). We first observe that subjects perform above chance level (20% for R@1) on all domains, thus the task is doable for humans.",
  "y": "uses"
 },
 {
  "id": "4498072885df2a126e2db553cf3aca_0",
  "x": "Intuitively, the DIH states that we should be able to replace any occurrence of \"cat\" with \"animal\" and still have a valid utterance. An important insight from work on distributional methods is that the definition of context is often critical to the success of a system<cite> (Shwartz et al., 2017)</cite> . Some distributional representations, like positional or dependency-based contexts, may even capture crude Hearst pattern-like features (Levy et al., 2015; Roller and Erk, 2016) .",
  "y": "background"
 },
 {
  "id": "4498072885df2a126e2db553cf3aca_1",
  "x": "**DISTRIBUTIONAL HYPERNYM DETECTION** Most unsupervised distributional approaches for hypernymy detection are based on variants of the Distributional Inclusion Hypothesis (Weeds et al., 2004; Kotlerman et al., 2010; Santus et al., 2014; Lenci and Benotto, 2012;<cite> Shwartz et al., 2017)</cite> . Here, we compare to two methods with strong empirical results.",
  "y": "background"
 },
 {
  "id": "4498072885df2a126e2db553cf3aca_2",
  "x": "To measure both the inclusion of x in y and the non-inclusion of y in x, invCL is then defined as Although most unsupervised distributional approaches are based on the DIH, we also consider the distributional SLQS model based on on an alternative informativeness hypothesis (Santus et al., 2014;<cite> Shwartz et al., 2017)</cite> . Intuitively, the SLQS model presupposes that general words appear mostly in uninformative contexts, as measured by entropy.",
  "y": "uses"
 },
 {
  "id": "4498072885df2a126e2db553cf3aca_3",
  "x": "We chose to evaluate the global ranking using Average Precision. This allowed us to use the same metric on all detection benchmarks, and is consistent with evaluations in <cite>Shwartz et al. (2017)</cite> . Direction: In direction prediction, the task is to identify which term is broader in a given pair of words.",
  "y": "uses"
 },
 {
  "id": "4498072885df2a126e2db553cf3aca_4",
  "x": "The other pattern-based models do not have any hyperparameters. Distributional models: For the distributional baselines, we employ the large, sparse distributional space of <cite>Shwartz et al. (2017)</cite> , which is computed from UkWaC and Wikipedia, and is known to have strong performance on several of the detection tasks. The corpus was POS tagged and dependency parsed.",
  "y": "uses"
 },
 {
  "id": "45723171ec398550e687c57d42e7cc_0",
  "x": "The increasing use of social media platforms world wide offers an interesting application of natural language processing tools for monitoring public health and health-related events on the social media. The social media mining for health applications (SMM4H) shared task<cite> (Weissenbacher et al., 2018)</cite> hosts four tasks aiming to identify mentions of different aspects medication use on Twitter. Briefly, the tasks and their descriptions are: Task 1: Automatic detection of posts mentioning drug names.",
  "y": "background"
 },
 {
  "id": "457f9916ed4d7eafacea57e208c760_0",
  "x": "A client may connect to the server and open up a dialogue (see Figure 1 in <cite>(Busemann et al., 1997)</cite> ). During the dialogue, the client may request texts to be analyzed or semantic descriptions to be verbalized. When given a text, the server returns the semantic representation, and vice versa.",
  "y": "uses"
 },
 {
  "id": "457f9916ed4d7eafacea57e208c760_1",
  "x": "The use of SMES in COSMA, semantic analysis and inference, the dialogue model mapping between human and machine dialogue structures, utterance generation, the architectural framework of the server, and the PASHA agent system are described in <cite>(Busemann et al., 1997)</cite> . Both papers can be found in the ANLP '97 conference proceedings. We demonstrate extended versions of the systems described in <cite>(Busemann et al., 1997)</cite> .",
  "y": "background"
 },
 {
  "id": "457f9916ed4d7eafacea57e208c760_2",
  "x": "Both papers can be found in the ANLP '97 conference proceedings. We demonstrate extended versions of the systems described in <cite>(Busemann et al., 1997)</cite> . In particular, the systems to be demonstrated can process counterproposals, which form an important part of efficient and cooperative scheduling dialogues.",
  "y": "extends"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_0",
  "x": "In this paper we explore generalized pooling methods to enhance sentence embedding. Specifically, by extending scalar self-attention models such as those proposed in<cite> Lin et al. (2017)</cite> , we propose vectorbased multi-head attention, which includes the widely used max pooling, mean pooling, and scalar selfattention itself as special cases. On one hand, the proposed method allows for extracting different aspects of the sentence into multiple vector representations through the multi-head mechanism. On the other, it allows the models to focus on one of many possible interpretations of the words encoded in the context vector through the vector-based attention mechanism.",
  "y": "similarities background motivation"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_1",
  "x": [
   "Lin et al. (2017) proposed a scalar structure/multi-head self-attention method for sentence embedding. The multi-head self-attention is calculated by a MLP with only LSTM states as input. There are two main differences from our proposed method; i.e., (1) they used scalar attention instead of vectorial attention, (2) we propose different penalization terms which is suitable for vector-based multi-head self-attention, while their penalization term on attention matrix is only designed for scalar multi-head self-attention."
  ],
  "y": "differences background"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_2",
  "x": "Instead of using AA T \u2212 I 2 F to encourage the diversity for scalar attention matrix as in<cite> Lin et al. (2017)</cite> , we propose the following formula to encourage the diversity for vectorial attention matrices. The penalization term on attention matrices is where \u03bb and \u00b5 are hyper-parameters which need to be tuned based on a development set. Intuitively, we try to encourage the diversity of any two different A i under the threshold \u03bb.",
  "y": "differences"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_3",
  "x": "Note that, we do not use SNLI as an additional training/development set in our experiments. Age Dataset To compare our models with that of<cite> Lin et al. (2017)</cite> , we use the same Age dataset in our experiment here, which is an Author Profiling dataset. The dataset are extracted from the Author Profiling dataset 1 , which consists of tweets from English Twitter.",
  "y": "uses"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_4",
  "x": "We use the same data split as in<cite> Lin et al. (2017)</cite> , i.e., 68,485 samples for training, 4,000 for development, and 4,000 for testing. ---------------------------------- **YELP DATASET**",
  "y": "uses"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_5",
  "x": "The Yelp dataset 2 is a sentiment analysis task, which takes reviews as input and predicts the level of sentiment in terms of the number of stars, from 1 to 5 stars, where 5-star means the most positive. We use the same data split as in<cite> Lin et al. (2017)</cite> , i.e., 500,000 samples for training, 2,000 for development, and 2,000 for testing. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_7",
  "x": "The BiLSTM with self-attention proposed by<cite> Lin et al. (2017)</cite> achieves better result than CNN and BiLSTM with max pooling. One of our baseline models using max pooling on BiLSTM achieves accuracies of 65.00% and 82.30% on the Yelp and the Age dataset respectively, which is already better than the self-attention model proposed by<cite> Lin et al. (2017)</cite> . We also show that the results of baseline with mean pooling and last pooling, in which mean pooling achieves the best result on the Yelp dataset among three baseline models and max pooling achieves the best on the Age dataset among three baselines.",
  "y": "background"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_8",
  "x": "Table 3 shows the results of different models for the Yelp and the Age dataset. The BiLSTM with self-attention proposed by<cite> Lin et al. (2017)</cite> achieves better result than CNN and BiLSTM with max pooling. One of our baseline models using max pooling on BiLSTM achieves accuracies of 65.00% and 82.30% on the Yelp and the Age dataset respectively, which is already better than the self-attention model proposed by<cite> Lin et al. (2017)</cite> .",
  "y": "differences"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_10",
  "x": "The green lines indicate scalar selfattention pooling added on top of the BiLSTMs, same as in<cite> Lin et al. (2017)</cite> , and the blue lines indicate vector-based attention used in our generalized pooling methods. It is obvious that the vector-based attention achieves improvement over scalar attention. Different line styles are used to indicate selfattention using different numbers of multi-head, ranging from 1, 3, 5, 7 to 9. For vector-based attention, the 9-head model achieves the best accuracy of 86.8% on the development set. For scalar attention, the 7-head model achieves the best accuracy of 86.4% on the development set.",
  "y": "differences background"
 },
 {
  "id": "4588d13c734d1ca0f348e056b1d39e_11",
  "x": "The experiments show that the proposed model achieves significant improvement over strong sentence-encoding-based methods, resulting in state-of-the-art performances on four datasets. The proposed approach can be easily implemented for more problems than we discuss in this paper. Our future work includes exploring more effective MLP to use the structures of multi-head vectors, inspired by the idea from<cite> Lin et al. (2017)</cite> .",
  "y": "future_work background"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_0",
  "x": "In the related task of image captioning, most methods try to generate descriptions only for individual images or for short videos depicting a single activity. Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (Rohrbach et al., 2016; Pan et al., 2016; Lu and Grauman, 2013; <cite>Huang et al., 2016)</cite> . The type of data we consider in this paper provides input illustrations for story generation in the form of photo albums, sampled over a few minutes to a few days of time.",
  "y": "background"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_1",
  "x": "A more direct dataset was recently released<cite> (Huang et al., 2016)</cite> , where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk. In this paper, we make use of the Visual Storytelling Dataset<cite> (Huang et al., 2016)</cite> . While the authors provide a seq2seq baseline, they only deal with the task of generating stories given 5-representative (summary) photos hand-selected by people from an album.",
  "y": "background"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_2",
  "x": "In this paper, we make use of the Visual Storytelling Dataset<cite> (Huang et al., 2016)</cite> . While the authors provide a seq2seq baseline, they only deal with the task of generating stories given 5-representative (summary) photos hand-selected by people from an album. Instead, we focus on the more challenging and realistic problem of end-toend generation of stories from entire albums.",
  "y": "uses"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_3",
  "x": "Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set. Previous works include storyline graph modeling Xing, 2014), unsupervised mining (Sigurdsson et al., 2016) , blog-photo alignment , and language retelling<cite> (Huang et al., 2016</cite>; Park and Kim, 2015) . While (Park and collects data by mining Blog Posts,<cite> (Huang et al., 2016)</cite> collects stories using Mechanical Turk, providing more directly relevant stories.",
  "y": "background"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_4",
  "x": "Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set. Previous works include storyline graph modeling Xing, 2014), unsupervised mining (Sigurdsson et al., 2016) , blog-photo alignment , and language retelling<cite> (Huang et al., 2016</cite>; Park and Kim, 2015) . While (Park and collects data by mining Blog Posts,<cite> (Huang et al., 2016)</cite> collects stories using Mechanical Turk, providing more directly relevant stories.",
  "y": "background"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_5",
  "x": "Following<cite> (Huang et al., 2016)</cite> , we choose a Gated Recurrent Unit (GRU) as the RNN unit to encode the photo sequence. The sequence output at each time step encodes the local album context for each photo (from both directions). Fused with the visual representation followed by ReLU, our final photo representation is (top module in Fig. 1 ):",
  "y": "uses"
 },
 {
  "id": "460a83a07ca3aa4d56deabad4f9831_6",
  "x": "**EXPERIMENTS** We use the Visual Storytelling Dataset<cite> (Huang et al., 2016)</cite> , consisting of 10,000 albums with 200,000 photos. Each album contains 10-50 photos taken within a 48-hour span with two annotations: 1) 2 album summarizations, each with 5 selected representative photos, and 2) 5 stories describing the selected photos.",
  "y": "uses"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_0",
  "x": "We also offer an analysis of system performance and the impact of training data size on the task. For example, we show that training our best model for only one epoch with < 40% of the data enables better performance than the baseline reported by <cite>Klinger et al. (2018)</cite> for the task. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_1",
  "x": "Other works exploit emojis to capture emotion carrying data (Felbo et al., 2017) . Alhuzali et al. (2018) introduce a third effective approach that leverages firstperson seed phrases like \"I'm happy that\" to collect emotion data. <cite>Klinger et al. (2018)</cite> propose yet a fourth method for collecting emotion data that depends on the existence of the expression \"emotion-word + one of the following words (when, that or because)\" in a tweet, regardless of the position of the emotion word.",
  "y": "background"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_2",
  "x": "The data are partitioned into 153, 383 tweets for training, 9591 tweets for validation, and 28, 757 data points for testing. The training and validation sets were provided early for system development, while the test set was released one week before the deadline of system submission. The full details of the dataset can be found in <cite>Klinger et al. (2018)</cite> .",
  "y": "background"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_3",
  "x": "As an additional baseline, we compare to <cite>Klinger et al. (2018)</cite> who propose a model based on Logistic Regression with a bag of word unigrams (BOW). All our deep learning models are based on variations of recurrent neural networks (RNNs), which have proved useful for several NLP tasks. RNNs are able to capture sequential dependencies especially in time series data, of which language can be seen as an example.",
  "y": "uses"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_4",
  "x": "Table 3 shows results of all our models in F-score. As the Table shows, all our models achieve sizable gains over the logistic regression model introduced by<cite> (Klinger et al., 2018)</cite> as a baseline for the competition (F-score = 60%). Even though our models trained based on fastText and ELMo each has a single hidden layer, which is not that deep, these at least 1.5% higher than the logistic regression model.",
  "y": "differences"
 },
 {
  "id": "4625b603beb2308b8b306dd4c01823_5",
  "x": "Interestingly, as Figure 3 shows, the model exceeds the baseline model reported by the task organizers<cite> (Klinger et al., 2018)</cite> when trained on only 10% of the training data. Additionally, the model outperforms the fastText and ELMo models by only seeing 40% of the training data. Once the model has access to 80% of the training data, its gains start to increase relatively slowly.",
  "y": "differences"
 },
 {
  "id": "47e109fd12ddbeebba894cead282d2_0",
  "x": "Angelic Falling anyone? P4 CON I'm obviously ignorant. Look how many times i've been given the title. \"Gravity is a theory. Why aren't you giving it the same treatment you do to evolution?\" Because it doesn't carry the same weight. ;P bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011) ; (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003) ; and (3) online social and political public forums (Somasundaran and Wiebe, 2009;<cite> Somasundaran and Wiebe, 2010</cite>; Wang and Ros\u00e9, 2010; Biran and Rambow, 2011) .",
  "y": "background"
 },
 {
  "id": "47e109fd12ddbeebba894cead282d2_1",
  "x": "Look how many times i've been given the title (P4 in Fig. 1 ), questioning another's evidence or assumptions: Yes there is always room for human error, but is one accident that hasn't happened yet enough cause to get rid of a capital punishment? (P2 in Fig. 3 ), and insults: Or is it because you are ignorant? (P3 in Fig. 1 ). These properties may function to engage the audience and persuade them to form a particular opinion, but they make computational analysis of such debates challenging, with the best performance to date averaging 64% over several topics <cite>(Somasundaran and Wiebe, 2010</cite> Second, the affordances of different online debate sites provide differential support for dialogic relations between forum participants. For example, the research of<cite> Somasundaran and Wiebe (2010)</cite> , does not explicitly model dialogue or author relations.",
  "y": "background"
 },
 {
  "id": "47e109fd12ddbeebba894cead282d2_2",
  "x": "These properties may function to engage the audience and persuade them to form a particular opinion, but they make computational analysis of such debates challenging, with the best performance to date averaging 64% over several topics <cite>(Somasundaran and Wiebe, 2010</cite> Second, the affordances of different online debate sites provide differential support for dialogic relations between forum participants. For example, the research of<cite> Somasundaran and Wiebe (2010)</cite> , does not explicitly model dialogue or author relations. However debates in our corpus vary greatly by topic on two dialogic factors: (1) the percent of posts that are rebuttals to prior posts, and (2) the number of posts per author.",
  "y": "background"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_0",
  "x": "Although traditional AES methods typically rely on handcrafted features (Larkey, 1998; Foltz et al., 1999; Attali and Burstein, 2006; Dikli, 2006; Wang and Brown, 2008; Chen and He, 2013; Somasundaran et al., 2014; Yannakoudakis et al., 2014; <cite>Phandi et al., 2015</cite>) , recent results indicate that state-of-the-art deep learning methods reach better performance (Alikaniotis et al., 2016; Dong and Zhang, 2016; Taghipour and Ng, 2016; Song et al., 2017; Tay et al., 2018) , perhaps because these methods are able to capture subtle and complex information that is relevant to the task (Dong and Zhang, 2016) . In this paper, we propose to combine string kernels (low-level character n-gram features) and word embeddings (high-level semantic features) to obtain state-of-the-art AES results. Since recent methods based on string kernels have demonstrated remarkable performance in various text classification tasks ranging from authorship identification (Popescu and Grozea, 2012) and sentiment analysis (Gim\u00e9nez-P\u00e9rez et al., 2017; to native language identification (Popescu and Ionescu, 2013; Ionescu et al., 2014; Ionescu, 2015; and dialect identification , we believe that string kernels can reach equally good results in AES.",
  "y": "background"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_1",
  "x": "We evaluate our approach on the Automated Student Assessment Prize data set, in both in-domain and cross-domain settings. The empirical results indicate that our approach yields a better performance than state-of-the-art approaches (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016; Tay et al., 2018) . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_2",
  "x": "The number of essays per prompt along with the score ranges are presented in Table 1 . Since the official test data of the ASAP competition is not released to the public, we, as well as others before us (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016;  1 https://www.kaggle.com/c/asap-aes/data Tay et al., 2018) , use only the training data in our experiments. Evaluation procedure.",
  "y": "motivation uses similarities"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_3",
  "x": "As Dong and Zhang (2016), we scaled the essay scores into the range 0-1. We closely followed the same settings for data preparation as (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . For the in-domain experiments, we use 5-fold cross-validation.",
  "y": "similarities uses"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_4",
  "x": "For the cross-domain experiments, we use the same source\u2192target domain pairs as (<cite>Phandi et al., 2015;</cite> Dong and Zhang, 2016) , namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928. All essays in the source domain are used as training data. Target domain samples are randomly divided into 5 folds, where one fold is used as test data, and the other 4 folds are collected together to sub-sample target domain train data.",
  "y": "uses similarities"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_5",
  "x": "All essays in the source domain are used as training data. Target domain samples are randomly divided into 5 folds, where one fold is used as test data, and the other 4 folds are collected together to sub-sample target domain train data. The sub-sample sizes are n t = {10, 25, 50, 100}. The sub-sampling is repeated for 5 times as in (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) to reduce bias.",
  "y": "similarities"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_6",
  "x": "We compare our approach with stateof-the-art methods based on handcrafted features (<cite>Phandi et al., 2015</cite>) , as well as deep features (Dong and Zhang, 2016; Tay et al., 2018) . We note that results for the cross-domain setting are reported only in some of these recent works (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . Implementation choices.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_7",
  "x": "Baselines. We compare our approach with stateof-the-art methods based on handcrafted features (<cite>Phandi et al., 2015</cite>) , as well as deep features (Dong and Zhang, 2016; Tay et al., 2018) . We note that results for the cross-domain setting are reported only in some of these recent works (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) .",
  "y": "background"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_8",
  "x": "We used functions from the VLFeat li- Table 2 : In-domain automatic essay scoring results of our approach versus several state-of-the-art methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016; Tay et al., 2018) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using 5-fold cross-validation. The best QWK score (among the machine learning systems) for each prompt is highlighted in bold.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_9",
  "x": "We first note that the histogram intersection string kernel alone reaches better overall performance (0.780) than all previous works (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016; Tay et al., 2018) . Remarkably, the overall performance of the HISK is also higher than the inter-human agreement (0.754). Although the BOSWE model can be regarded as a shallow approach, its overall results are comparable to those of deep learning approaches (Dong and Zhang, 2016; Tay et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_10",
  "x": "The results for the crossdomain automatic essay scoring task are presented in Table 3 . For each and every source\u2192target pair, we report better results than both state-of-theart methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . We observe that the difference between our best QWK scores and the <cite>other approaches</cite> are sometimes much higher in the cross-domain setting than in the in-domain setting.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_11",
  "x": "The results for the crossdomain automatic essay scoring task are presented in Table 3 . For each and every source\u2192target pair, we report better results than both state-of-theart methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . We observe that the difference between our best QWK scores and the <cite>other approaches</cite> are sometimes much higher in the cross-domain setting than in the in-domain setting.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_12",
  "x": "For each and every source\u2192target pair, we report better results than both state-of-theart methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . We observe that the difference between our best QWK scores and the <cite>other approaches</cite> are sometimes much higher in the cross-domain setting than in the in-domain setting. We particularly notice that the difference from (<cite>Phandi et al., 2015</cite>) when n t = 0 is always higher than 10%.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_13",
  "x": "We observe that the difference between our best QWK scores and the <cite>other approaches</cite> are sometimes much higher in the cross-domain setting than in the in-domain setting. We particularly notice that the difference from (<cite>Phandi et al., 2015</cite>) when n t = 0 is always higher than 10%. Our highest improvement (more than 54%, from 0.187 to 0.728) over (<cite>Phandi et al., 2015</cite>) is recorded for the pair 5\u21926, when n t = 0.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_14",
  "x": "We particularly notice that the difference from (<cite>Phandi et al., 2015</cite>) when n t = 0 is always higher than 10%. Our highest improvement (more than 54%, from 0.187 to 0.728) over (<cite>Phandi et al., 2015</cite>) is recorded for the pair 5\u21926, when n t = 0. Our score in this case (0.728) is even higher than both scores of <cite>Phandi et al. (2015)</cite> and Dong and Zhang (2016) when they use n t = 50.",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_15",
  "x": "We applied this method in the in-domain setting and we obtained a surprisingly low overall QWK score, around 0.251. We concluded that this simple apSource\u2192Target Method n t = 0 n t = 10 n t = 25 n t = 50 n t = 100 1\u21922 (<cite>Phandi et al., 2015</cite>) Table 3 : Corss-domain automatic essay scoring results of our approach versus two state-of-the-art methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using the same evaluation procedure as (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) .",
  "y": "differences"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_16",
  "x": "We concluded that this simple apSource\u2192Target Method n t = 0 n t = 10 n t = 25 n t = 50 n t = 100 1\u21922 (<cite>Phandi et al., 2015</cite>) Table 3 : Corss-domain automatic essay scoring results of our approach versus two state-of-the-art methods (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using the same evaluation procedure as (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016) . The best QWK scores for each source\u2192target domain pair are highlighted in bold.",
  "y": "uses similarities"
 },
 {
  "id": "4821d5a283c1d4ba162b58e5fac8bc_17",
  "x": "We compared our approach on the Automated Student Assessment Prize data set, in both in-domain and crossdomain settings, with several state-of-the-art approaches (<cite>Phandi et al., 2015</cite>; Dong and Zhang, 2016; Tay et al., 2018) . Overall, the in-domain and the cross-domain comparative studies indicate that string kernels, both alone and in combination with word embeddings, attain the best performance on the automatic essay scoring task. Using a shallow approach, we report better results compared to recent deep learning approaches (Dong and Zhang, 2016; Tay et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "484bc7c9c66bf4028eef4103beec7f_0",
  "x": "Our previous work focused on only the segmentation part of the voice identification task <cite>(Brooke et al., 2012)</cite> . Here, we instead assume an initial segmentation and then try to create clusters corresponding to segments of the The Waste Land which are spoken by the same voice. Of particular interest is the influence of the initial segmentation on the success of this downstream task.",
  "y": "background motivation"
 },
 {
  "id": "484bc7c9c66bf4028eef4103beec7f_1",
  "x": "Our work here is built on our earlier work <cite>(Brooke et al., 2012)</cite> . Our segmentation model for The Waste Land was based on a stylistic change curve whose values are the distance between stylistic feature vectors derived from 50 token spans on either side of each point (spaces between tokens) in the text; the local maxima of this curve represent likely voice switches. Performance on The Waste Land was far from perfect, but evaluation using standard text segmentation metrics (Pevzner and Hearst, 2002) indicated that it was well above various baselines.",
  "y": "background"
 },
 {
  "id": "484bc7c9c66bf4028eef4103beec7f_2",
  "x": "Our approach to voice identification in The Waste Land consists first of identifying the boundaries of voice spans <cite>(Brooke et al., 2012)</cite> . Given a segmentation of the text, we consider each span as a data point in a clustering problem. The elements of the vector correspond to the best feature set from the segmentation task, with the rationale that features which were useful for detecting changes in style should also be useful for identifying stylistic similarities. Our features therefore include: a collection of readability metrics (including word length), frequency of punctuation, line breaks, and various parts-ofspeech, lexical density, average frequency in a large external corpus (Brants and Franz, 2006) , lexiconbased sentiment metrics using SentiWordNet (Baccianella et al., 2010) , formality score (Brooke et al., 2010) , and, perhaps most notably, the centroid of 20-dimensional distributional vectors built using latent semantic analysis (Landauer and Dumais, 1997), reflecting the use of words in a large web corpus (Burton et al., 2009) ; in previous work (Brooke et al., 2010) , we established that such vectors contain useful stylistic information about the English lexicon (including rare words that appear only occasionally in such a corpus), and indeed LSA vectors were the single most promising feature type for segmentation.",
  "y": "background"
 },
 {
  "id": "484bc7c9c66bf4028eef4103beec7f_3",
  "x": "For a more detailed discussion of the feature set, see<cite> Brooke et al. (2012)</cite> . All the features are normalized to a mean of zero and a standard deviation of 1. For clustering, we use a slightly modified version of the popular k-means algorithm (MacQueen, 1967) .",
  "y": "background"
 },
 {
  "id": "484bc7c9c66bf4028eef4103beec7f_4",
  "x": "The Even segmentation should be viewed as the baseline for segmentation, and the Gold segmentation an \"oracle\" representing an upper bound on segmentation performance. For the automatic segmentation model, we use the settings from<cite> Brooke et al. (2012)</cite> . We also compare three possible clusterings for each segmentation: no clustering at all (Initial), that is, we assume that each segment is a new voice; k-means clustering (k-means), as outlined above; and random clustering (Random), in which we randomly assign each voice to a cluster.",
  "y": "uses"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_0",
  "x": "To align the text and image embedding vectors between matching recipe-image pairs, cosine similarity loss with margin was applied. Carvalho et al. <cite>[3]</cite> proposed a similar multi-modal embedding method for aligning text and image representations in a shared latent space. In contrast to Salvador et al. [22] , they formulated a joint objective function which incorporates the loss for the cross-modal retrieval task and a classification loss, instead of using the latent space for a multitask learning setup.",
  "y": "background"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_1",
  "x": "The image analysis path is composed of a ResNet-50 model [9] , pretrained on the ImageNet Dataset [7] , with a custom top layer for mapping the image features to the joint embedding space. All word embeddings are pretrained with the word2vec algorithm [19] and fine tuned during the joint embedding learning phase. We chose 512-dimensional word embedding for our model with self-attention, whereas [17] and <cite>[3]</cite> chose a vector length of 300.",
  "y": "differences"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_2",
  "x": "We have trained our model using cosine similarity loss with margin as in [17] and with the triplet loss proposed by <cite>[3]</cite> . Both objective functions and the semantic regularization by [17] aim at maximizing intra-class correlation and minimizing inter-class correlation. Let us define the text query embedding as \u03d5 q and the embedding of the image query as \u03d5 d , then the cosine embedding loss can be defined as follows:",
  "y": "uses"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_3",
  "x": "Our optimization strategy differs from [17] in that we use an aggressive learning rate decay, namely exponential decay, so that the learning rate is halved all 20 epochs. Since the timing of freezing layers proved not to be of importance unless the recipe path is trained first, we used the same strategy under the cosine distance objective [17] and for the triplet loss <cite>[3]</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_4",
  "x": "[17] used 238,399 samples for their effective training set and for the validation and testing set 51,119 and 51,303 samples, respectively. By filtering out noisy instructions sentences (e.g. instructions containing only punctuation) we increased the effective dataset size to 254,238 samples for the training set and 54,565 and 54,885 for the validation and testing sets, respectively. Similarly to [17] and <cite>[3]</cite> , we evaluated our model on 10 subsets of 1000 samples each.",
  "y": "similarities"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_6",
  "x": "Image to Recipe MedR R@1 R@5 R@10 1k samples Random [17] 500.0 0.001 0.005 0.01 JNE [17] 5.0 \u00b1 0. Both [17] and <cite>[3]</cite> use time-consuming instruction text preprocessing over the skip-thought technique [15] .",
  "y": "background"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_7",
  "x": "Both [17] and <cite>[3]</cite> use time-consuming instruction text preprocessing over the skip-thought technique [15] . This process doubles the overall training time from three days to six days using two Nvidia Titan X GPU's. By using online-instruction encoding with the self-attention encoder, we were able train the model for its main task in under 30 hours.",
  "y": "differences"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_9",
  "x": "Depending on meal type, all baseline implementations as well as our Ingredient Attention based model exhibit a broad range of retrieval accuracy. In Figure 5 we present a few typical results on the intended recipe retrieval task. AdaMine <cite>[3]</cite> creates more distinct class clusters than in [17] .",
  "y": "uses"
 },
 {
  "id": "489d0077e05269327e7fe4e7f7e4a3_10",
  "x": "Our main contribution is the aforementioned ingredient attention, empowering our model to solve the recipe retrieval without any upstream skip instruction embedding, as well as the light-weight architecture provided by the transformer-like instruction encoder. On the recipe retrieval task, our method performs similarly to our baseline implementation of <cite>[3]</cite> . Regarding training time on the other hand, we increased the efficiency significantly for crossmodal based retrieval methods.",
  "y": "similarities"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_0",
  "x": "Before applying tools trained on specific languages, one must determine the language of the text. It has attracted considerable attention in recent years [1, 2,<cite> 3,</cite> 4, 5, 6, 7, 8] . Most of the existing approaches take words as features, and then adopt effective supervised classification algorithms to solve the problem.",
  "y": "background"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_1",
  "x": "For example, Simoes et al. [9] achieved 97% accuracy for discriminating among 25 unrelated languages. However, it is generally difficult to distinguish between related languages or variations of a specific language (see [9] and [10] for example). To deal with this problem, Huang and Lee<cite> [3]</cite> proposed a contrastive approach based on documentlevel top-bag-of-word similarity to reflect distances among the three varieties of Mandarin in China, Taiwan and Singapore, which is a kind of word-level uni-gram feature.",
  "y": "background"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_3",
  "x": "In fact, the word alignment-based dictionary can extract both fine-grained representative words and coarse-grained words simultaneously. The above observation indicates that character form, PMI-based and word alignment-based information are useful information to discriminate dialects in the GCR. In order to investigate the detailed characteristics of different dialects of Mandarin Chinese, we extend <cite>3</cite> dialects in Huang and Lee<cite> [3]</cite> to 6 dialects.",
  "y": "extends"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_5",
  "x": "For example, Murthy and Kumar [1] focused on Indian languages identification. Meanwhile, Ranaivo-Malancon [2] proposed features based on frequencies of character n-grams to identify Malay and Indonesian. Huang and Lee<cite> [3]</cite> presented the top-bag-of-word similarity based contrastive approach to reflect distances among the three varieties of Mandarin in Mainland China, Taiwan and Singapore.",
  "y": "background"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_7",
  "x": "Besides these works, other recent studies include: Spanish varieties identification [1<cite>3</cite>] , Arabic varieties discrimination [14, 15, 16, 17] , and Persian and Dari identification [18] . Among the above related works, study<cite> [3]</cite> is the most related work to ours. The differences between study<cite> [3]</cite> and our work are two-fold:",
  "y": "similarities"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_8",
  "x": "Among the above related works, study<cite> [3]</cite> is the most related work to ours. The differences between study<cite> [3]</cite> and our work are two-fold: (1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore.",
  "y": "differences"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_9",
  "x": "In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in Huang and Lee<cite> [3]</cite> to 6 dialects. Also, the more dialects there are, the more difficult the dialects discrimination becomes. (2)The top-bag-of-word they proposed in Huang and Lee<cite> [3]</cite> is word uni-gram feature essentially.",
  "y": "extends"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_10",
  "x": "The differences between study<cite> [3]</cite> and our work are two-fold: (1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore. In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in Huang and Lee<cite> [3]</cite> to 6 dialects. Also, the more dialects there are, the more difficult the dialects discrimination becomes. (2)The top-bag-of-word they proposed in Huang and Lee<cite> [3]</cite> is word uni-gram feature essentially. While in this paper, besides the traditional uni-gram feature, we propose some novel features, such as character form, PMI-based and word alignment-based features.",
  "y": "differences"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_11",
  "x": "Compared with English, no space exists between words in Chinese sentence. Therefore, we use character uni-grams, bi-grams and tri-grams as features. However, Huang and Lee<cite> [3]</cite> did not use character-level n-grams.",
  "y": "differences"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_19",
  "x": "Baseline system 1: As mentioned in Section 2, we take the Huang and Lee<cite> [3]</cite> 's top-bag-of-word similarity-based approach as one of our baseline system. We re-implement their method in this paper using the similar <cite>3</cite>-way news dataset. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_22",
  "x": "Also the bi-gram and word segmentation based features are better than the Huang and Lee<cite> [3]</cite> 's method (baseline system 1) for 6-way, <cite>3</cite>-way and 2-way dialect identification in the GCR. Obviously, the random method does not work for the GCR dialect identification. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "48add0c1226863808c3c3a8c29a12e_28",
  "x": "**RESULTS ON WIKIPEDIA DATASET** As shown in Table <cite>3</cite> , character form based features are very effective (94.<cite>3</cite>6% for 2-way dialects classification). Similar to Huang and Lee<cite> [3]</cite> 's work, in order to eliminate the trivial issue of character encoding (simplified and traditional character), we convert Taiwan and Hong Kong texts to the same simplified character set using Zhconvertor 6 utility to focus on actual linguistic and textual features.",
  "y": "similarities"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_0",
  "x": "Our proposed supervised and unsupervised approaches perform better than the supervised and unsupervised approaches of <cite>Fazly et al. (2009)</cite> , respectively. ---------------------------------- **VERB-NOUN IDIOMATIC COMBINATIONS**",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_1",
  "x": "Although there has been relatively less work on MWE identification than other type-level MWE prediction tasks, it is nevertheless important for NLP applications such as machine translation that must be able to distinguish MWEs from literal combinations in context. Some recent work has focused on token-level identification of a wide range of types of MWEs and other multiword units (e.g., Newman et al., 2012; Schneider et al., 2014; Brooke et al., 2014) . Many studies, however, have taken a word sense disambiguation-inspired approach to MWE identification (e.g., Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010) , treating literal combinations and MWEs as different word senses, and have exploited linguistic knowledge of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005; Hashimoto and Kawahara, 2008; <cite>Fazly et al., 2009</cite>; Fothergill and Baldwin, 2012) .",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_2",
  "x": "They are a common and productive type of English idiom, and occur cross-lingually (<cite>Fazly et al., 2009</cite>) . VNICs tend to be relatively lexico-syntactically fixed, e.g., whereas hit the roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages. <cite>Fazly et al. (2009)</cite> exploit this property in their unsupervised approach, referred to as CFORM.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_3",
  "x": "<cite>Fazly et al. (2009)</cite> exploit this property in their unsupervised approach, referred to as CFORM. <cite>They</cite> define lexico-syntactic patterns for VNIC token instances based on the noun's determiner (e.g., a, the, or possibly no determiner), the number of the noun (singular or plural), and the verb's voice (active or passive). <cite>They</cite> propose a statistical method for automatically determining a given VNIC type's canonical idiomatic form, based on the frequency of its usage in these patterns in a corpus.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_4",
  "x": "<cite>They</cite> define lexico-syntactic patterns for VNIC token instances based on the noun's determiner (e.g., a, the, or possibly no determiner), the number of the noun (singular or plural), and the verb's voice (active or passive). <cite>They</cite> propose a statistical method for automatically determining a given VNIC type's canonical idiomatic form, based on the frequency of its usage in these patterns in a corpus. 2 <cite>They</cite> then classify a given token instance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_5",
  "x": "<cite>Fazly et al. (2009)</cite> exploit this property in their unsupervised approach, referred to as CFORM. <cite>They</cite> define lexico-syntactic patterns for VNIC token instances based on the noun's determiner (e.g., a, the, or possibly no determiner), the number of the noun (singular or plural), and the verb's voice (active or passive). <cite>They</cite> propose a statistical method for automatically determining a given VNIC type's canonical idiomatic form, based on the frequency of its usage in these patterns in a corpus.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_6",
  "x": "<cite>They</cite> propose a statistical method for automatically determining a given VNIC type's canonical idiomatic form, based on the frequency of its usage in these patterns in a corpus. 2 <cite>They</cite> then classify a given token instance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise. <cite>Fazly et al</cite>. also consider a supervised approach that classifies a given VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_7",
  "x": "2 <cite>They</cite> then classify a given token instance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise. <cite>Fazly et al</cite>. also consider a supervised approach that classifies a given VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training. Distributed representations of word meaning in the form of word embeddings (Mikolov et al., 2013) have recently been demonstrated to benefit a wide range of NLP tasks including POS tagging (e.g., Ling et al., 2015) , question answering (e.g., Dong et al., 2015) , and machine translation (e.g., Zou et al., 2013) .",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_8",
  "x": "In this work we first propose a supervised approach to identifying VNIC token instances based on word embeddings that outperforms the supervised method of <cite>Fazly et al. (2009)</cite> . We then propose an unsupervised approach to this task, that combines word embeddings with <cite>Fazly et al</cite>.'s unsupervised CFORM approach, that improves over <cite>CFORM</cite>. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_9",
  "x": "Moreover, word embeddings have been shown to improve over count-based models of distributional similarity for predicting MWE compositionality (Salehi et al., 2015) . In this work we first propose a supervised approach to identifying VNIC token instances based on word embeddings that outperforms the supervised method of <cite>Fazly et al. (2009)</cite> . We then propose an unsupervised approach to this task, that combines word embeddings with <cite>Fazly et al</cite>.'s unsupervised CFORM approach, that improves over <cite>CFORM</cite>.",
  "y": "differences uses"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_10",
  "x": "The summation is done over the same window size that the word2vec model was trained on so that c j captures the same information that the word2vec model has learned to capture. After computing c verb and c noun these vectors are averaged to form c. Figure 1 shows the process for forming c for an example sentence. Finally, to form the feature vector representing a VNIC instance, we subtract e from c, and append to this vector a single binary feature representing whether the VNIC instance occurs in its canonical form, as determined by <cite>Fazly et al. (2009)</cite> .",
  "y": "uses"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_11",
  "x": "---------------------------------- **UNSUPERVISED VNIC IDENTIFICATION** Our unsupervised approach combines the word embedding-based representation used in the supervised approach (without relying on training a supervised classifier, of course) with the unsupervised CFORM method of <cite>Fazly et al. (2009)</cite> .",
  "y": "uses"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_12",
  "x": "---------------------------------- **SUPERVISED RESULTS** Following <cite>Fazly et al. (2009)</cite> , the supervised approach was evaluated using a leave-one-token-out strategy.",
  "y": "uses"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_13",
  "x": "The accuracy on DEV and TEST ranges from 85.5%-88.2% and 83.4%-88.3%, respectively. All of these accuracies are higher than those reported by <cite>Fazly et al. (2009)</cite> for their supervised approach. They are also substantially higher than the most-frequent class baseline, and the unsupervised CFORM method of <cite>Fazly et al</cite>.",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_14",
  "x": "The accuracy on DEV and TEST ranges from 85.5%-88.2% and 83.4%-88.3%, respectively. All of these accuracies are higher than those reported by <cite>Fazly et al. (2009)</cite> for their supervised approach. They are also substantially higher than the most-frequent class baseline, and the unsupervised CFORM method of <cite>Fazly et al</cite>.",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_15",
  "x": "A word2vec model with a smaller window size gives more syntactically-oriented word embeddings, whereas a larger window size gives more semantically-oriented embeddings (Trask et al., 2015) . The CFORM method of <cite>Fazly et al. (2009 )</cite> is a strong unsupervised benchmark for this task, and relies on the lexico-syntactic pattern in which an MWE token instance occurs. A smaller window size for the word embedding features might be better able to capture similar information to <cite>CFORM</cite>, which could explain the good performance of the model using a window size of 1.",
  "y": "background"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_16",
  "x": "A smaller window size for the word embedding features might be better able to capture similar information to <cite>CFORM</cite>, which could explain the good performance of the model using a window size of 1. ---------------------------------- **GENERALIZATION TO UNSEEN VNICS**",
  "y": "similarities"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_17",
  "x": "Moreover, the unsupervised CFORM method of <cite>Fazly et al. (2009)</cite> gives substantially higher accuracies than this supervised approach. The limited ability of this model to generalize to unseen MWE types further motivates exploring unsupervised approaches to this task. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_18",
  "x": "**UNSUPERVISED RESULTS** The k-means clustering for the unsupervised approach is repeated 100 times with randomlyselected initial centroids, for several values of k. The average accuracy and standard deviation of the unsupervised approach over these 100 runs are shown in the left panel of Table 2 . For k = 4 and 5 on TEST, this approach surpasses the unsupervised CFORM method of <cite>Fazly et al. (2009)</cite> ; however, on DEV this approach does not outperform <cite>Fazly et al</cite>.'s CFORM approach for any of the val-ues of k considered.",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_19",
  "x": "The k-means clustering for the unsupervised approach is repeated 100 times with randomlyselected initial centroids, for several values of k. The average accuracy and standard deviation of the unsupervised approach over these 100 runs are shown in the left panel of Table 2 . For k = 4 and 5 on TEST, this approach surpasses the unsupervised CFORM method of <cite>Fazly et al. (2009)</cite> ; however, on DEV this approach does not outperform <cite>Fazly et al</cite>.'s CFORM approach for any of the val-ues of k considered. Analyzing the results on individual expressions indicates that the unsupervised approach gives especially low accuracy for hit roof -which is in DEV-as compared to the CFORM method of <cite>Fazly et al.</cite>, which could contribute to the overall lower accuracy of the unsupervised approach on this dataset.",
  "y": "differences"
 },
 {
  "id": "491879c73f8aa9f11bfae01abc795d_20",
  "x": "---------------------------------- **CONCLUSIONS** In this paper we proposed supervised and unsupervised approaches, based on word embeddings, to identifying token instances of VNICs that performed better than the supervised approach, and unsupervised CFORM approach, of <cite>Fazly et al. (2009)</cite> , respectively.",
  "y": "differences"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_0",
  "x": "Researchers have found a variety of ways in which dangerous unintended bias can show up in NLP applications (Blodgett and O'Connor, 2017; Hovy and Spruit, 2016; Tatman, 2017) . Mitigating such biases is a difficult problem, and researchers have created many ways to make fairer NLP applications. Much of the focus for mitigating unintended bias in NLP is either targeted at reducing gender stereotypes in text (Bolukbasi et al., 2016b,a; Zhao et al., 2017; Zhang et al., 2018) , or inequality of sentiment or toxicity for various protected groups <cite>(Caliskan-Islam et al., 2016</cite>; Bakarov, 2018; Dixon et al.; Garg et al., 2018; Kiritchenko and Mohammad, 2018) .",
  "y": "background"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_1",
  "x": "(Bolukbasi et al., 2016b ) defines a useful metric for identifying gender bias and<cite> (Caliskan-Islam et al., 2016)</cite> defines a metric called the WEAT score for evaluating unfair correlations with sentiment for various demographics in text. Unfortunately metrics like these leverage vector space arguments between only two identities at a time like man vs woman (Bolukbasi et al., 2016a) , or European American names vs. African American names<cite> (Caliskan-Islam et al., 2016)</cite> . Though geometrically intuitive, these tests do not have a direct relation to discrimination in general.",
  "y": "background"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_2",
  "x": "More specifically, word embeddings has been an area of focus for evaluating unintended bias. (Bolukbasi et al., 2016b ) defines a useful metric for identifying gender bias and<cite> (Caliskan-Islam et al., 2016)</cite> defines a metric called the WEAT score for evaluating unfair correlations with sentiment for various demographics in text. Unfortunately metrics like these leverage vector space arguments between only two identities at a time like man vs woman (Bolukbasi et al., 2016a) , or European American names vs. African American names<cite> (Caliskan-Islam et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_3",
  "x": "Unfortunately metrics like these leverage vector space arguments between only two identities at a time like man vs woman (Bolukbasi et al., 2016a) , or European American names vs. African American names<cite> (Caliskan-Islam et al., 2016)</cite> . Our framework and RNSB metric enable a clear evaluation of discrimination with respect to word embedding bias for a whole class of demographics.",
  "y": "differences"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_4",
  "x": "**MODELS AND DATA** We evaluate three pretrained embedding models: GloVe (Pennington et al., 2014) , Word2vec (Mikolov et al., 2013 ) (trained on the large Google News corpus), and ConceptNet . GloVe and Word2vec embeddings have been shown to contain unintended bias in (Bolukbasi et al., 2016a;<cite> Caliskan-Islam et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_5",
  "x": "First, we compare the RNSB metric for 3 pretrained word embeddings, showing that our metric is consistent with other word embedding analysis like WEAT<cite> (Caliskan-Islam et al., 2016)</cite> . We then show that our framework enables an insightful view into word embedding bias. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "4a093e0ca9a499c19e4721366d88f6_6",
  "x": "Although the RNSB metric is not directly comparable to WEAT scores, these results are still consistent with some of the bias predicted by<cite> (Caliskan-Islam et al., 2016)</cite> . The WEAT score shows that word embeddings like Word2vec and GloVe are biased with respect to national origin because European-American names are more correlated with positive sentiment than AfricanAmerican names. RNSB captures the same types of biases, but has a clear and larger scope, measuring discrimination with respect to more than two demographics within a protected group.",
  "y": "similarities"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_0",
  "x": "However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk et al., 2005;<cite> Liu et al., 2007</cite>; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea, 2003; Galley et al., 2004; Bod, 2007) .",
  "y": "background motivation"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_1",
  "x": "Although remarkable progresses have been reported, the strict syntactic constraint (the both sides of the rules should strictly be a subtree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents. To alleviate this constraint, a few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks. For example, forest-to-string transformation rules have been integrated into the tree-to-string translation framework by (Liu et al., 2006;<cite> Liu et al., 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_2",
  "x": "A few researches have made some exploratory investigations towards the effects of different rules by classifying the translation rules into different subcategories<cite> (Liu et al., 2007</cite>; Zhang et al., 2008a; DeNeefe et al., 2007) . <cite>Liu et al. (2007)</cite> differentiated the rules in their tree-to-string model which integrated with forest 1 -to-string into fully lexicalized rules, non-lexicalized rules and partial lexicalized rules according to the lexicalization levels. As an extension, Zhang et al. (2008a) proposed two more categories: Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR).",
  "y": "background"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_3",
  "x": "A few researches have made some exploratory investigations towards the effects of different rules by classifying the translation rules into different subcategories<cite> (Liu et al., 2007</cite>; Zhang et al., 2008a; DeNeefe et al., 2007) . <cite>Liu et al. (2007)</cite> differentiated the rules in their tree-to-string model which integrated with forest 1 -to-string into fully lexicalized rules, non-lexicalized rules and partial lexicalized rules according to the lexicalization levels. As an extension, Zhang et al. (2008a) proposed two more categories: Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR).",
  "y": "background motivation"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_4",
  "x": "Currently, there have been several classifications in SMT research community. Generally, the rules can be classified into two main groups according to whether syntax information is involved: bilingual phrases (Phrase) and syntax rules (Syntax). Further, the syntax rules can be divided into three categories according to the lexicalization levels<cite> (Liu et al., 2007</cite>; Zhang et al., 2008a source and target sides are non-lexicons (nonterminals)",
  "y": "background"
 },
 {
  "id": "4a63ef4085639a66d1c7f6344f7548_5",
  "x": "In the future works, aiming to analyze the rule contributions and the redundances issues using the presented rule classification based on some real translation systems, we plan to implement some synchronous grammar based syntax translation models such as the one presented in<cite> (Liu et al., 2007)</cite> or in (Zhang et al., 2008a) . Taking such a system as the experimental platform, we can perform comprehensive statistics about distributions of different rule categories. What is more important, the contribution of each rule category can be evaluated seriatim.",
  "y": "future_work"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_0",
  "x": "This paper considers the reading comprehension task in which some discrete-reasoning abilities are needed to correctly answer questions. Specifically, we focus on a new reading comprehension dataset called DROP <cite>(Dua et al., 2019)</cite> , which requires Discrete Reasoning Over the content of Paragraphs to obtain the final answer. Unlike previous benchmarks such as CNN/DM (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016) that have been well solved Devlin et al., 2019) , DROP is substantially more challenging in three ways.",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_1",
  "x": "First, to produce various answer types,<cite> Dua et al. (2019)</cite> extend previous one-type answer prediction (Seo et al., 2017) to multi-type prediction that supports span extraction, counting, and addition/subtraction. However, they have not fully considered all potential types. Take the question \"What percent are not non-families?\" and the passage snippet \"39.9% were non-families\" as an example, a negation operation is required to infer the answer.",
  "y": "background"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_2",
  "x": "Existing approaches, when applied to this more realistic scenario, have three problems. First, to produce various answer types,<cite> Dua et al. (2019)</cite> extend previous one-type answer prediction (Seo et al., 2017) to multi-type prediction that supports span extraction, counting, and addition/subtraction. However, they have not fully considered all potential types.",
  "y": "motivation"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_3",
  "x": "Third, to support numerical reasoning, prior work <cite>(Dua et al., 2019)</cite> learns to predict signed numbers for obtaining an arithmetic expression that can be executed by a symbolic system. Nevertheless, the prediction of each signed number is isolated, and the expression's context information has not been considered. As a result, obviously-wrong expressions, such as all predicted signs are either minus or zero, are likely produced.",
  "y": "background"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_4",
  "x": "Existing approaches, when applied to this more realistic scenario, have three problems. Third, to support numerical reasoning, prior work <cite>(Dua et al., 2019)</cite> learns to predict signed numbers for obtaining an arithmetic expression that can be executed by a symbolic system. Nevertheless, the prediction of each signed number is isolated, and the expression's context information has not been considered.",
  "y": "motivation"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_5",
  "x": "Our model uses BERT (Devlin et al., 2019) as encoder: we map word embeddings into contextualized representations using pre-trained Transformer blocks (Vaswani et al., 2017 ) ( \u00a73.1). Based on the representations, we employ a multi-type answer predictor that is able to produce four answer types: (1) span from the text; (2) arithmetic expression; (3) count number; (4) negation on numbers ( \u00a73.2). Following<cite> Dua et al. (2019)</cite> , we first predict the answer type of a given passage-question pair, and then adopt individual prediction strategies.",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_6",
  "x": "Following<cite> Dua et al. (2019)</cite> , we design a multi-type answer predictor to selectively produce different kinds of answers such as span, count number, and arithmetic expression. To further increase answer coverage, we propose adding a new answer type to support logical negation. Moreover, unlike prior work that separately predicts passage spans and question spans, our approach directly extracts spans from the input sequence.",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_7",
  "x": "To further increase answer coverage, we propose adding a new answer type to support logical negation. Moreover, unlike prior work that separately predicts passage spans and question spans, our approach directly extracts spans from the input sequence. Answer type prediction Inspired by the Augmented QANet model <cite>(Dua et al., 2019)</cite> , we use the contextualized token representations from the last four blocks (H L\u22123 , ..., H L ) as the inputs to our answer predictor, which are denoted as M 0 , M 1 , M 2 , M 3 , respectively.",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_8",
  "x": "where \u2297 denotes the outer product between the vector g and each token representation in M. Arithmetic expression In order to model the process of performing addition or subtraction among multiple numbers mentioned in the passage, we assign a three-way categorical variable (plus, minus, or zero) for each number to indicate its sign, similar to<cite> Dua et al. (2019)</cite> . As a result, an arithmetic expression that has a number as the final answer can be obtained and easily evaluated.",
  "y": "similarities"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_9",
  "x": "Since DROP does not indicate the answer type but only provides the answer string, we therefore adopt the weakly supervised annotation scheme, as suggested in Berant et al. (2013);<cite> Dua et al. (2019)</cite> . We find all possible annotations that point to the gold answer, including matching spans, arithmetic expressions, correct count numbers, negation operations, and the number of spans. We use simple rules to search over all mentioned numbers to find potential negations.",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_10",
  "x": "**IMPLEMENTATION DETAILS** Dataset We consider the reading comprehension benchmark that requires Discrete Reasoning Over Paragraphs (DROP) <cite>(Dua et al., 2019)</cite> prehensive understanding of the context as well as the ability of numerical reasoning are required. Model settings We build our model upon two publicly available uncased versions of BERT: BERT BASE and BERT LARGE 2 , and refer readers to Devlin et al. (2019) for details on model sizes.",
  "y": "uses"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_11",
  "x": "Baselines Following the implementation of Augmented QANet (NAQANet) <cite>(Dua et al., 2019)</cite> , we introduce a similar baseline called Augmented BERT (NABERT). The main difference is that we replace the encoder of QANet (Yu et al., 2018) with the pre-trained Transformer blocks (Devlin et al., 2019) . Moreover, it also supports the prediction of various answer types such as span, arithmetic expression, and count number.",
  "y": "similarities"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_12",
  "x": "While early research used cloze-style tests (Hermann et al., 2015; Hill et al., 2016) , most of recent works (Rajpurkar et al., 2016; Joshi et al., 2017) are designed to extract answers from the passage. Despite their success, these datasets only require shallow pattern matching and simple logical reasoning, thus being well solved Devlin et al., 2019) . Recently,<cite> Dua et al. (2019)</cite> released a new benchmark named DROP that demands discrete reasoning as well as deeper paragraph understanding to find the answers.",
  "y": "background"
 },
 {
  "id": "4a90cd18be0df0c41a94febe2f68ef_13",
  "x": "Recently,<cite> Dua et al. (2019)</cite> released a new benchmark named DROP that demands discrete reasoning as well as deeper paragraph understanding to find the answers. We choose to work on DROP to test both the numerical reasoning and linguistic comprehension abilities.",
  "y": "uses"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_0",
  "x": "Recently, <cite>Locascio et al. (2016)</cite> designed the Deep-Regex model based on the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014) using minimal domain knowledge during the learning phase while still accurately predicting regular expressions from NLs. Later, Zhong et al. (2018a) improved the performance by training on not only syntactic content of the expressions (i.e. the exact textual representation of the expression that was used), but also the semantic content (the regular language described by the expression). However, the reward function in the SemRegex model (Zhong et al., 2018a) that determines if the predicted regular expression is semantically equivalent to the ground truth expression is known to be PSPACE-complete and is a bottleneck in practice (Stockmeyer and Meyer, 1973) .",
  "y": "background"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_1",
  "x": "Kushman and Barzilay (2013) built a parsing model that translates a natural language sentence to a regular expression, and provided a dataset which is now a popular benchmark dataset for related research. <cite>Locascio et al. (2016)</cite> proposed the Deep-Regex model based on Seq2Seq for generating regular expressions from natural language descriptions together with a dataset of 10,000 NL-RX pairs. However, due to the limitations of the standard Seq2Seq model, the Deep-Regex model can only generate regular expressions similar in shape to the training data.",
  "y": "background"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_2",
  "x": "---------------------------------- **EQ REG MODEL** Datasets: <cite>Locascio et al. (2016)</cite> created a set of NL-RX pair data by arbitrarily creating and combining data in a tree form.",
  "y": "background"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_3",
  "x": "Datasets: <cite>Locascio et al. (2016)</cite> created a set of NL-RX pair data by arbitrarily creating and combining data in a tree form. We define the depth of a regular expression in this dataset as the depth of the tree that generated the NL-RX pair (see Appendix A). Similar to <cite>Locascio et al. (2016)</cite> , we randomly generate regular expression pairs up to depth three and label the equivalence between each pair.",
  "y": "similarities"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_4",
  "x": "On the other hand, NL-RX-Synth is data generated automatically and NL-RX-Turk is made from ordinary people by paraphrasing NL descriptions in NL-RX-Synth using Mechanical Turk<cite> (Locascio et al., 2016)</cite> . Both datasets have 10,000 pairs of NL-RX data. We follow the previous work in splitting the data (train: 65%, dev: 10%, test: 25%).",
  "y": "background"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_5",
  "x": "Though the speedup described in experimental result may appear constant, our softened equivalence approximately decides a PSPACE-complete problem in linear time to the length of regular expressions, which would otherwise take exponential time. Zhong et al. (2018b) pointed out some problems in the NL-RX datasets. Specifically, there are some ambiguities since <cite>Locascio et al. (2016)</cite> tried to obtain data from machine-generated sentences.",
  "y": "background"
 },
 {
  "id": "4b65a59fc2331b9771ea09a12f32de_6",
  "x": "**A SUPPLEMENTAL MATERIAL** We now describe how <cite>Locascio et al. (2016)</cite> generated their synthetic regular expression data. To begin, they manually mapped primitive regular expression operations, such as union and concatenation, to natural language.",
  "y": "background"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_0",
  "x": "BERT <cite>(Devlin et al., 2018)</cite> , for example, trains on the BooksCorpus (Zhu et al., 2015) and English Wikipedia, for a combined 3,200M words. Other iterations increased the amount of knowledge used during pre-training, such as RoBERTa (Liu et al., 2019) . Training large-scale models on these massive datasets has drawbacks, such as significantly increased carbon pollution and harm to the environment (Schwartz et al., 2019; Strubell et al., 2019) .",
  "y": "background"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_1",
  "x": "For language modeling, we create training data similar to those in BERT <cite>(Devlin et al., 2018)</cite> . For knowledge graph use, we preprocess language to create commonsense object and relationship vocabulary and to match as many related commonsense objects as possible. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_2",
  "x": "We use this process for each training epoch, since it allows for the most dense pretraining framework. Commonly known as a cloze task,<cite> Devlin et al. (2018)</cite> introduced a framework that pretrained transformers (Vaswani et al., 2017) based on masked token prediction. First, we preprocess the tokens with WordPiece embeddings (Wu et al., 2016) .",
  "y": "background"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_3",
  "x": "Then, we randomly mask 15% of all WordPiece embeddings. Unlike<cite> Devlin et al. (2018)</cite> , we run the randomization script once per each training epoch. Otherwise, we follow the procedure in<cite> Devlin et al. (2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_4",
  "x": "Unlike<cite> Devlin et al. (2018)</cite> , we run the randomization script once per each training epoch. Otherwise, we follow the procedure in<cite> Devlin et al. (2018)</cite> . 80% of the time, we replace the word with the [M ASK] prediction, to be replaced through cloze task prediction.",
  "y": "uses"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_5",
  "x": "Combined with the above cloze task, we process the data for next sentence prediction. We do this process after the cloze task masking, similar to<cite> Devlin et al. (2018)</cite> . For each datum, we randomly pick either a sentence labeled correctly as the next sentence 50% of the time, or a random sentence 50% of the time.",
  "y": "similarities"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_6",
  "x": "Contrary to<cite> Devlin et al. (2018)</cite> , we do language model fine-tuning in addition to classification finetuning. We find that this generally provides better results, and allows for more stable accuracy since the shared task involves a small dataset. For each prompt, we use the previous preprocessed data to create tasks for our model to predict.",
  "y": "differences"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_7",
  "x": "For masked tokens, we predict that token through bidirectional context, the same as<cite> Devlin et al. (2018)</cite> . For next sentence prediction, we use the unbiased method previously introduced as well as in<cite> Devlin et al. (2018)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_8",
  "x": "For next sentence prediction, we use the unbiased method previously introduced as well as in<cite> Devlin et al. (2018)</cite> . ---------------------------------- **TOKEN REALIGNMENT**",
  "y": "uses"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_9",
  "x": "For BERT baselines, we use the process in<cite> Devlin et al. (2018)</cite> , and use the [CLS] token, without attention, for classification. ---------------------------------- **ANALYSIS**",
  "y": "uses"
 },
 {
  "id": "4bc12aca138835b5ed80b0cf69febf_10",
  "x": "Commonly known as a cloze task,<cite> Devlin et al. (2018)</cite> introduced a framework that pretrained transformers (Vaswani et al., 2017) based on masked token prediction. Unlike<cite> Devlin et al. (2018)</cite> , we run the randomization script once per each training epoch. Otherwise, we follow the procedure in<cite> Devlin et al. (2018)</cite> .",
  "y": "extends"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_0",
  "x": "Most of the previous coreference resolution methods have similar classification phases, implemented either as decision trees (Soon et al., 2001) or as maximum entropy classifiers<cite> (Luo et al., 2004)</cite> . Moreover, these methods employ similar feature sets. The clusterization phase is different across current approaches.",
  "y": "background"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_1",
  "x": "In contrast, globally optimized clustering decisions were reported in<cite> (Luo et al., 2004)</cite> and (DaumeIII and Marcu, 2005a) , where all clustering possibilities are considered by searching on a Bell tree representation or by using the Learning as Search Optimization (LaSO) framework (DaumeIII and Marcu, 2005b) respectively, but the first search is partial and driven by heuristics and the second one only looks back in text. We argue that a more adequate clusterization phase for coreference resolution can be obtained by using a graph representation. In this paper we describe a novel representation of the coreference space as an undirected edge-weighted graph in which the nodes represent all the mentions from a text, whereas the edges between nodes constitute the confidence values derived from the coreference classification phase.",
  "y": "background motivation"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_2",
  "x": "BESTCUT has a different way of computing the cut weight than Min-Cut and a different way of stopping the cut 2 . Moreover, we have slightly modified the Min-Cut procedures. BESTCUT replaces the bottom-up search in a tree representation (as it was performed in<cite> (Luo et al., 2004)</cite> ) with the top-down problem of obtaining the best partitioning of a graph.",
  "y": "extends differences background"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_4",
  "x": "Based on the data seen, a maximum entropy model (Berger et al., 1996) offers an expression (1) for the probability that there exists coreference C between a mention m i and a mention m j . where g k (m i , m j , C) is a feature and \u03bb k is its weight; Z(m i , m j ) is a normalizing factor. We created the training examples in the same way as<cite> (Luo et al., 2004)</cite> , by pairing all mentions of the same type, obtaining their feature vectors and taking the outcome (coreferent/noncoreferent) from the key files.",
  "y": "uses"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_5",
  "x": "We duplicated the statistical model used by<cite> (Luo et al., 2004)</cite> , with three differences. First, no feature combination was used, to prevent long running times on the large amount of ACE data. Second, through an analysis of the validation data, we implemented seven new features, presented in Table 1. Third, as opposed to<cite> (Luo et al., 2004)</cite> , who represented all numerical features quantized, we translated each numerical feature into a set of binary features that express whether the value is in certain intervals.",
  "y": "extends"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_7",
  "x": "**EXPERIMENTAL RESULTS** The clusterization algorithms that we implemented to evaluate in comparison with our method are<cite> (Luo et al., 2004)</cite> 's Belltree and Link-Best (best-first clusterization) from (Ng and Cardie, 2002) . The features used were described in section 2.2.",
  "y": "uses"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_8",
  "x": "Since we aimed to measure the performance of coreference, the metrics used for evaluation are the ECM-F<cite> (Luo et al., 2004)</cite> and the MUC P, R and F scores (Vilain et al., 1995) . In our first experiment, we tested the three coreference clusterization algorithms on the development-test set of the ACE Phase 2 corpus, first on true mentions (i.e. the mentions annotated in the key files), then on detected mentions (i.e. the mentions output by our mention detection system presented in section 3) and finally without any prior knowledge of the mention types. The results obtained are tabulated in Table 4 .",
  "y": "uses"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_9",
  "x": "It is apparent that the MUC score does not vary significantly between systems. This only shows that none of them is particularly poor, but it is not a relevant way of comparing methods-the MUC metric has been found too indulgent by researchers (<cite> (Luo et al., 2004)</cite> , (Baldwin et al., 1998) Table 4 : Comparison of results between three clusterization algorithms on ACE Phase 2. The learning algorithms are maxent for coreference and SVM for stopping the cut in BESTCUT.",
  "y": "background"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_10",
  "x": "Table 5 : Impact of feature categories on BEST-CUT on MUC6. Baseline system has the<cite> (Luo et al., 2004)</cite> features. The system was tested on key mentions.",
  "y": "uses"
 },
 {
  "id": "4cb16f436d910d82c3661052c1fa30_12",
  "x": "It is of interest to discuss why our implementation of the Belltree system<cite> (Luo et al., 2004</cite> ) is comparable in performance to Link-Best (Ng and Cardie, 2002) . (Luo et al., 2004) do the clusterization through a beam-search in the Bell tree using either a mention-pair or an entity-mention model, the first one performing better in their experiments. Despite the fact that the Bell tree is a complete representation of the search space, the search in it is optimized for size and time, while potentially losing optimal solutions-similarly to a Greedy search.",
  "y": "background motivation"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_0",
  "x": "This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art <cite>(Lee and Dernoncourt, 2016</cite>; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017) . The main contributions of the paper are:",
  "y": "similarities"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_1",
  "x": "The main contributions of the paper are: \u2022 Exhaustive experimental evaluation on dialog act datasets, outperforming state-of-theart deep CNN<cite> (Lee and Dernoncourt, 2016)</cite> and RNN variants (Khanpour et al., 2016; Ortega and Vu, 2017 ).",
  "y": "differences"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_2",
  "x": "Table 1 summarizes dataset statistics. We use the train, validation and test splits as defined in <cite>(Lee and Dernoncourt, 2016</cite>; Ortega and Vu, 2017) . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_3",
  "x": "Unlike prior approaches <cite>(Lee and Dernoncourt, 2016</cite>; Ortega and Vu, 2017 ) that rely on pre-trained word embeddings, we learn the projection weights on the fly during training, i.e word embeddings (or vocabularies) do not need to be stored. Instead, features are computed on the fly and are dynamically compressed via the projection matrices into projection vectors. These values were chosen via a grid search on development sets, we do not perform any other dataset-specific tuning.",
  "y": "differences"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_4",
  "x": "We compare our model against a majority class baseline and Naive Bayes classifier<cite> (Lee and Dernoncourt, 2016)</cite> . Our model significantly outperforms both baselines by 12 to 35% absolute. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_5",
  "x": "We compare our model against a majority class baseline and Naive Bayes classifier<cite> (Lee and Dernoncourt, 2016)</cite> . Our model significantly outperforms both baselines by 12 to 35% absolute. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_6",
  "x": "**COMPARISON AGAINST STATE-OF-ART METHODS** We also compare our performance against prior work using HMMs (Stolcke et al., 2000) and recent deep learning methods like CNN<cite> (Lee and Dernoncourt, 2016)</cite> , RNN (Khanpour et al., 2016) and RNN with gated attention (Tran et al., 2017) . To the best of our knowledge, <cite>(Lee and Dernoncourt, 2016</cite>; Ortega and Vu, 2017; Tran et al., 2017) are the latest approaches in dialog act classification, which also reported on the same data splits.",
  "y": "uses"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_7",
  "x": "We also compare our performance against prior work using HMMs (Stolcke et al., 2000) and recent deep learning methods like CNN<cite> (Lee and Dernoncourt, 2016)</cite> , RNN (Khanpour et al., 2016) and RNN with gated attention (Tran et al., 2017) . To the best of our knowledge, <cite>(Lee and Dernoncourt, 2016</cite>; Ortega and Vu, 2017; Tran et al., 2017) are the latest approaches in dialog act classification, which also reported on the same data splits. Therefore, we compare our research against these works.",
  "y": "background"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_8",
  "x": "This is very impressive given that we work with very small memory footprint and we do not rely on pre-trained word embeddings. Our study also shows that the proposed method is very effective for such natural language tasks compared to more complex neural network architectures such as deep CNN<cite> (Lee and Dernoncourt, 2016)</cite> and RNN variants (Khanpour et al., 2016; Ortega and Vu, 2017) . We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing lowdimensional semantic text representations that are useful for text classification applications.",
  "y": "differences"
 },
 {
  "id": "4cc18724e62db32e748838080cbfd0_10",
  "x": "We proposed Self-Governing Neural Networks for on-device short text classification. Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods <cite>(Lee and Dernoncourt, 2016</cite>; Khanpour et al., 2016; Ortega and Vu, 2017) . We introduced a compression technique that effectively captures low-dimensional semantic representation and produces compact models that significantly save on storage and computational cost.",
  "y": "differences"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_1",
  "x": "---------------------------------- **PRIOR WORK** Parisien and Stevenson (2011) and<cite> Kawahara et al. (2014)</cite> showed distinct ways of applying the Hierarchical Dirichlet Process (Teh et al., 2006) to uncover the latent clusters from cluster examples.",
  "y": "background"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_2",
  "x": "Parisien and Stevenson (2011) and<cite> Kawahara et al. (2014)</cite> showed distinct ways of applying the Hierarchical Dirichlet Process (Teh et al., 2006) to uncover the latent clusters from cluster examples. The latter used significantly larger corpora, and explicitly separated verb sense induction from the syntactic/semantic clustering, which allowed more fine-grained control of each step. In<cite> Kawahara et al. (2014)</cite> , two identical DPMM's were used.",
  "y": "background"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_3",
  "x": "The DPMM used in<cite> Kawahara et al. (2014)</cite> is shown in Figure 1 . The clusters are drawn from a Dirichlet Process with hyperparameter \u03b1 and base distribution G. The Dirichlet process prior creates a clustering effect described by the Chinese Restaurant Process. Each cluster is chosen proportionally to the number of elements it already contains, i.e.",
  "y": "uses"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_4",
  "x": "By separating the verb sense induction and the clustering of verb senses, the features can be optimized for the distinct tasks. According to <cite>(Kawahara et al., 2014)</cite> , the best features for inducing verb classes are joint slot:token pairs. For the verb clustering task, slot features which ignore the lexical items were the most effective.",
  "y": "background"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_5",
  "x": "When incorporating supervision, we flatten VerbNet, using only the top-level categories, simplifying the selection process for y. In<cite> Kawahara et al. (2014)</cite> , slot features were most effective features at producing a VerbNet-like structure; we follow suit. ---------------------------------- **RESULTS**",
  "y": "uses"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_6",
  "x": "For evaluation, we compare using the same dataset and metrics as<cite> Kawahara et al. (2014)</cite> . There, the authors use the polysemous verb classes of Korhonen et al. (2003) , a subset of frequent polysemous verbs. This makes the test set a sort of miniVerbNet, suitable for evaluation.",
  "y": "uses"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_7",
  "x": "This measures how well each gold standard cluster is recovered. We report each metric, and the F1 score combining them, to compare the clustering accuracy with respect to the gold standard G. We use the clustering from<cite> Kawahara et al. (2014)</cite> as a baseline for comparison. However, for evaluation, the authors only clustered senses of verbs in the evaluation set.",
  "y": "uses"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_8",
  "x": "Therefore, to compare apples-to-apples, we calculate the nPU, niPU, and F1 of the<cite> Kawahara et al. (2014)</cite> full clustering against the evaluation set. Our model also computes the full clustering, but with supervision for known verbs (other than the evaluation set). Parameters were selected using a grid search, and cross-validation.",
  "y": "uses"
 },
 {
  "id": "4d2488844c1f6f39f1f4b8f3487288_9",
  "x": "Swapping out the Dirichlet process for a Pitman-Yor process may bring finer control over the number of clusters. We have expanded the work in<cite> Kawahara et al. (2014)</cite> by explicitly modeling a VerbNet class for each verb sense, drawn from a product of experts based on the cluster and verb. This allowed us to leverage data from SemLink with VerbNet annotation, to produce a higher-quality clustering.",
  "y": "extends"
 },
 {
  "id": "4d8ae52583d41b4124800c419963df_0",
  "x": "We use the feature set introduced by<cite> Zwarts and Johnson (2011)</cite> , but instead of n-gram scores, we apply the LSTM language model probabilities. The features are so good that the reranker without any external language model is already a state-of-the-art system, providing a very strong baseline for our work. The reranker uses both model-based scores (including NCM scores and LM probabilities) and surface pattern features (which are boolean indicators) as described in Table 1 .",
  "y": "extends"
 },
 {
  "id": "4d8ae52583d41b4124800c419963df_1",
  "x": "The bigram LM within the NCM is too simple to capture more complicated language structure. In order to alleviate this problem, we follow<cite> Zwarts and Johnson (2011)</cite> by training LMs on different corpora, but we apply state-ofthe-art recurrent neural network (RNN) language models. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "4d8ae52583d41b4124800c419963df_2",
  "x": "The reranker uses both model-based scores (including NCM scores and LM probabilities) and surface pattern features (which are boolean indicators) as described in Table 1 . Our reranker optimizes the expected f-score approximation described in<cite> Zwarts and Johnson (2011)</cite> with L2 regularisation. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "4d8ae52583d41b4124800c419963df_3",
  "x": "Since the bigram language model of the NCM is trained on this corpus, we cannot directly use Switchboard to build LSTM LMs. The reason is that if the training data of Switchboard is used both for predicting language fluency and optimizing the loss function, the reranker will overestimate the model-based features 1-2. forward & backward LSTM LM scores 3-7. This is because the fluent sentence itself is part of the language model <cite>(Zwarts and Johnson, 2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_0",
  "x": "**ABSTRACT** We describe a machine learning based method to identify incorrect entries in translation memories. It extends previous work by<cite> Barbu (2015)</cite> through incorporating recall-based machine translation and part-of-speech-tagging features.",
  "y": "extends differences"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_1",
  "x": "In this paper, we describe our submitted system for distinguishing correct from incorrect TUs. Rather than tailoring it to individual languages, we aimed at a languageindependent solution to cover all of the language pairs in this shared task or, looking to the future, Autodesk's production environments. The system is based on previous work by<cite> Barbu (2015)</cite> and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification.",
  "y": "extends differences"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_2",
  "x": "In Section 3, we describe our method and, in Section 4, show how it compares to <cite>Barbu's (2015)</cite> approach as well as other submissions to this shared task. Lastly, we offer preliminary conclusions and outline future work in Section 5. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_3",
  "x": "The best classifier, a support vector machine with linear kernel, achieved 82% precision and 81% recall on a held-out test set of 309 TUs. To the best of our knowledge, Barbu provided the first and so far only research contribution on automatic TM cleaning, which the author himself described as \"a neglected research area\"<cite> (Barbu, 2015)</cite> . With our participation to this shared task, we seek to extend his work by examining new features based on statistical MT and POS tagging.",
  "y": "similarities"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_4",
  "x": "With our participation to this shared task, we seek to extend his work by examining new features based on statistical MT and POS tagging. As outlined above, comparing machine translated source segments to their actual target segments has proven effective in <cite>Barbu's (2015)</cite> experiments. We propose to complement or replace the similarity function used for this comparison (cosine similarity) by two automatic MT evaluation metrics, Bleu (Papineni et al., 2002) and characterbased Levenshtein distance, in order to reward higher-order n-gram (n > 1) and partial word overlaps, respectively.",
  "y": "background"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_5",
  "x": "Our feature extraction pipeline, including <cite>Barbu's (2015)</cite> as well as our own features (see Section 3.1), is implemented in Scala. This pipeline is used to transform translation units into feature vectors and train classifiers using the scikitlearn framework (Pedregosa et al., 2011) . From the various classification algorithms we tested, Random Forests performed best with our selection of features (see below).",
  "y": "similarities"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_6",
  "x": "We focused on gearing our classifiers to distinguish correct or almost correct (classes 1, 2) from incorrect TUs (class 3) -i.e., the Binary Classification (II) task -by optimising the weighted F 1 -score (F 1 ) on training data (see Tables 2a and 2b) . From the various feature combinations we tested, we found the following to be most successful: ratio_words, pos_sim_all, language_detection, mt_cfs, mt_bleu, ratio_chars (as described in Section 3.1), alongside cg_score, only_capletters_dif, and punctuation_similarity (from<cite> Barbu, 2015)</cite> . Evaluation results are given in the next section.",
  "y": "similarities"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_7",
  "x": "While eliminating almost correct TUs might decrease rather than increase MT quality, filtering out incorrect segments can have a positive impact (Vogel, 2003) . Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by<cite> Barbu (2015)</cite> (Baseline 2). More importantly, however, we compared our system to <cite>Barbu's (2015)</cite> approach, using the classification algorithms which reportedly worked best with the 17 features in his work.",
  "y": "similarities"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_8",
  "x": "Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by<cite> Barbu (2015)</cite> (Baseline 2). More importantly, however, we compared our system to <cite>Barbu's (2015)</cite> approach, using the classification algorithms which reportedly worked best with the 17 features in his work. Our system performed well in this comparison, surpassing Barbu's approach in all language pairs except en-de, where both systems were en par.",
  "y": "similarities uses"
 },
 {
  "id": "4da1c39dbbeaa2c9dac22118d0c698_9",
  "x": "Although geared to the Binary Classification (II) task (see above), we also assessed our system on the Fine-Grained Classification task. Here, the goal was to distinguish between all of the three classes, i.e., determine whether a TU is correct, almost correct, or incorrect. Again, we compared our system's performance to <cite>Barbu's (2015)</cite> method, using 2 /3-1 /3 splits of the training data (5-fold cross-validation).",
  "y": "similarities"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_0",
  "x": "Aletras and Stevenson (2013) showed that coherence can be measured by a classical distributional similarity approach. More recently, <cite>Lau et al. (2014)</cite> proposed a methodology to automate the word intrusion task directly. Their results also reveal the differences between these methodologies in their assessment of topic coherence.",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_1",
  "x": "A hyper-parameter in all these methodologies is the number of topic words, or its cardinality. These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for Chang et al. (2009) , N = 5, whereas for Newman et al. (2010) , Aletras and Stevenson (2013) and <cite>Lau et al. (2014)</cite> , N = 10. The germ of this paper came when using the automatic word intrusion methodology<cite> (Lau et al., 2014)</cite> , and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction.",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_2",
  "x": "These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for Chang et al. (2009) , N = 5, whereas for Newman et al. (2010) , Aletras and Stevenson (2013) and <cite>Lau et al. (2014)</cite> , N = 10. The germ of this paper came when using the automatic word intrusion methodology<cite> (Lau et al., 2014)</cite> , and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction. This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence.",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_3",
  "x": "The germ of this paper came when using the automatic word intrusion methodology<cite> (Lau et al., 2014)</cite> , and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction. This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence. To investigate this, we develop a new dataset with human-annotated coherence judgements for a range of cardinality settings (N = {5, 10, 15, 20}).",
  "y": "motivation background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_4",
  "x": "This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence. To investigate this, we develop a new dataset with human-annotated coherence judgements for a range of cardinality settings (N = {5, 10, 15, 20}). We experiment with the automatic word intrusion<cite> (Lau et al., 2014)</cite> and discover that correlation with human ratings decreases systematically as cardinality increases.",
  "y": "uses"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_5",
  "x": "Although there are existing datasets with human-annotated coherence scores (Newman et al., 2010; Aletras and Stevenson, 2013;<cite> Lau et al., 2014</cite>; Chang et al., 2009) , these topics were annotated using a fixed cardinality setting (e.g. 5 or 10). We thus develop a new dataset for this experiment. Following <cite>Lau et al. (2014)</cite> , we use two domains: (1) WIKI, a collection of 3.3 million English Wikipedia articles (retrieved November 28th 2009); and (2) NEWS, a collection of 1.2 million New York Times articles from 1994 to 2004 (English Gigaword).",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_6",
  "x": "Although there are existing datasets with human-annotated coherence scores (Newman et al., 2010; Aletras and Stevenson, 2013;<cite> Lau et al., 2014</cite>; Chang et al., 2009) , these topics were annotated using a fixed cardinality setting (e.g. 5 or 10). We thus develop a new dataset for this experiment. Following <cite>Lau et al. (2014)</cite> , we use two domains: (1) WIKI, a collection of 3.3 million English Wikipedia articles (retrieved November 28th 2009); and (2) NEWS, a collection of 1.2 million New York Times articles from 1994 to 2004 (English Gigaword).",
  "y": "uses"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_7",
  "x": "We sub-sample approximately 50M tokens (100K and 50K articles for WIKI and NEWS respectively) from both domains to create two smaller document collections. We then generate 300 LDA topics for each of the sub-sampled collection. 2 There are two primary approaches to assessing topic coherence: (1) via word intrusion (Chang et (2) by directly measuring observed coherence (Newman et al., 2010;<cite> Lau et al., 2014)</cite> .",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_8",
  "x": "2 There are two primary approaches to assessing topic coherence: (1) via word intrusion (Chang et (2) by directly measuring observed coherence (Newman et al., 2010;<cite> Lau et al., 2014)</cite> . As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities.",
  "y": "uses"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_9",
  "x": "<cite>Lau et al. (2014)</cite> proposed an automated approach to the word intrusion task. The methodology computes pairwise word association features for the top-N words, and trains a support vector regression model to rank the words. The top-ranked word is then selected as the predicted intruder word.",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_10",
  "x": "This is due to idiosyncratic words that are closely related in the collection, e.g. remnant Wikipedia markup tags. The topic model discovers them as topics and the word statistics derived from the same collection supports the association, but these topics are generally not coherent, as revealed by out-of-domain statistics. This result is consistent with previous studies<cite> (Lau et al., 2014)</cite> .",
  "y": "similarities"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_11",
  "x": "**AUTOMATED METHOD -NPMI** The other mainstream approach to evaluating topic coherence is to directly measure the average pairwise association between the top-N words. Newman et al. (2010) found PMI to be the best association measure, and later studies (Aletras and Stevenson, 2013;<cite> Lau et al., 2014)</cite> found that normalised PMI (NPMI: Bouma (2009)) improves PMI further.",
  "y": "background"
 },
 {
  "id": "4e1b01c1faebc447891bc0b847316d_12",
  "x": "Newman et al. (2010) found PMI to be the best association measure, and later studies (Aletras and Stevenson, 2013;<cite> Lau et al., 2014)</cite> found that normalised PMI (NPMI: Bouma (2009)) improves PMI further. To see if the benefit of aggregating coherence measures over several cardinalities transfers across to other methodologies, we test the NPMI methodology. We compute the topic coherence using the full collection of WIKI and NEWS, respectively, for varying N .",
  "y": "uses"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_0",
  "x": "KB's contains millions of facts usually in the form of triples (entity1, relation, entity2). However, KB's are woefully incomplete (Min et al., 2013) , missing important facts, and hence limiting their usefulness in downstream tasks. Figure 1: The two paths above consist of the same relations (locatedIn \u2192 locatedIn) and, hence, the model of <cite>Neelakantan (2015)</cite> will assign them the same score for the relation AirportServesPlace without considering the fact that Yankee Stadium is not an airport.",
  "y": "background"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_1",
  "x": "The performance of relation extraction methods have been greatly improved by incorporating selectional preferences, i.e., relations enforce constraints on the allowed entity types for the candidate entities, both in sentence level (Roth and Yih, 2007; Singh et al., 2013) and KB relation extraction (Chang et al., 2014) , and in learning entailment rules (Berant et al., 2011) . Another line of work in relation extraction performs reasoning on the paths (multi-hop reasoning on paths of length \u2265 1) connecting an entity pair (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014;<cite> Neelakantan et al., 2015</cite>; Guu et al., 2015) . For example, these models can infer the relation PlaysInLeague(Tom Brady, NFL) from the facts PlaysForTeam(Tom Brady, New England Patriots) and PartOf(New England Patriots, NFL).",
  "y": "background"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_2",
  "x": "In this work, we extend the method of <cite>Neelakantan (2015)</cite> by incorporating entity type information. Their method can generalize to paths unseen in training by composing embeddings of relations in the path non-linearly using a Recurrent Neural Network (RNN) (Werbos, 1990) . While entity type information has been successfully incorporated into relation extraction methods that perform single hop reasoning, here, we include them for multi-hop relation extraction.",
  "y": "extends"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_3",
  "x": "---------------------------------- **MODEL** This paper extends the Recurrent Neural Network model of <cite>Neelakantan (2015)</cite> by jointly reasoning over the relations and entity types occurring in the paths between an entity pair.",
  "y": "extends"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_4",
  "x": "Let \u03c0 be a path between the entity pair (e1, e2) containing the relation types \u03b4 1 , \u03b4 2 , . . . , \u03b4 N . In the following section, we first briefly describe the model proposed by <cite>Neelakantan (2015)</cite> (RNN model henceforth) followed by our extensions to it. ----------------------------------",
  "y": "extends background"
 },
 {
  "id": "4edb60770ebeafc56446aeca9a3b2e_5",
  "x": "We rank the entity pairs in the test set based on their scores and calculate the Mean Average Precision (MAP) score for the ranking following previous work<cite> Neelakantan et al., 2015)</cite> . Table 2 lists the MAP scores of both the models averaged over 12 freebase relation types. Incorporating selectional preferences by adding entity types gives a significant boost in scores (17.67 % over the baseline model.).",
  "y": "uses"
 },
 {
  "id": "4f0dec0ce2d7639c250be00d5efee4_0",
  "x": "**INTRODUCTION** Word collocation is one source of information that has been proposed as a useful tool to post-process word recognition results( <cite>[1,</cite> 4] ). It can be considered as a constraint on candidate selection so that the word candidate selection problem can be formalized as an instance of constraint satisfaction.",
  "y": "background"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_1",
  "x": "ceived helpfulness of a review depends not only on its review content, but also on social effects such as product qualities, and individual bias in the presence of mixed opinion distribution (Danescu-NiculescuMizil et al., 2009 ). Nonetheless, several properties distinguish our corpus of peer reviews from other types of reviews: 1) The helpfulness of our peer reviews is directly rated using a discrete scale from one to five instead of being defined as a function of binary votes (e.g. the percentage of \"helpful\" votes<cite> (Kim et al., 2006)</cite> ); 2) Peer reviews frequently refer to the related students' papers, thus review analysis needs to take into account paper topics; 3) Within the context of education, peer-review helpfulness often has a writing specific semantics, e.g. improving revision likelihood; 4) In general, peer-review corpora collected from classrooms are of a much smaller size compared to online product reviews. To tailor existing techniques to peer reviews, we will thus propose new specialized features to address these issues.",
  "y": "differences"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_2",
  "x": "An unhelpful peer review of average-rating 1: Your paper and its main points are easy to find and to follow. As shown in Table 1 , we first mine generic linguistic features from reviews and papers based on the results of syntactic analysis of the texts, aiming to replicate the feature sets used by<cite> Kim et al. (2006)</cite> .",
  "y": "uses"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_3",
  "x": "Following<cite> Kim et al. (2006)</cite> , we train our helpfulness model using SVM regression with a radial basis function kernel provided by SVM light (Joachims, 1999) . We first evaluate each feature type in isolation to investigate its predictive power of peerreview helpfulness; we then examine them together in various combinations to find the most useful feature set for modeling peer-review helpfulness. Performance is evaluated in 10-fold cross validation of our 267 peer reviews by predicting the absolute helpfulness scores (with Pearson correlation coefficient r) as well as by predicting helpfulness ranking (with Spearman rank correlation coefficient r s ).",
  "y": "uses"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_4",
  "x": "Structural features (bolded) achieve the highest Pearson (0.60) and Spearman correlation coefficients (0.59) (although within the significant correlations, the difference among coefficients are insignificant). Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews<cite> (Kim et al., 2006)</cite> where product scores are significantly correlated with product-review helpfulness. However, when combined with other features, MET does appear to add value (last row).",
  "y": "differences"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_5",
  "x": "Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews<cite> (Kim et al., 2006)</cite> where product scores are significantly correlated with product-review helpfulness. However, when combined with other features, MET does appear to add value (last row). When comparing the performance between predicting helpfulness ratings versus ranking, we observe r \u2248 r s consistently for our peer reviews, while<cite> Kim et al. (2006)</cite> reported r < r s for product reviews.",
  "y": "differences"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_6",
  "x": "However, when combined with other features, MET does appear to add value (last row). When comparing the performance between predicting helpfulness ratings versus ranking, we observe r \u2248 r s consistently for our peer reviews, while<cite> Kim et al. (2006)</cite> reported r < r s for product reviews. 4 Finally, we observed a similar feature redundancy effect as<cite> Kim et al. (2006)</cite> did, in that simply combining all features does not improve the model's performance.",
  "y": "similarities"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_7",
  "x": "Evaluation of the specialized features is shown in Table 3 , where all features examined are signifi- 4 The best performing single feature type reported<cite> (Kim et al., 2006)</cite> was review unigrams: r = 0.398 and rs = 0.593. cantly correlated with both helpfulness rating and ranking. When evaluated in isolation, although specialized features have weaker correlation coefficients ([0.43, 0.51] ) than the best generic features, these differences are not significant, and the specialized features have the potential advantage of being theory-based.",
  "y": "background"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_8",
  "x": "These specialized features not only introduce domain expertise, but also capture linguistic information at an abstracted level, which can help avoid the risk of over-fitting. Given only 267 peer reviews in our case compared to more than ten thousand product reviews<cite> (Kim et al., 2006)</cite> , this is an important consideration. Though our absolute quantitative results are not directly comparable to the results of<cite> Kim et al. (2006)</cite> , we indirectly compared them by analyzing the utility of features in isolation and combined.",
  "y": "motivation differences"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_9",
  "x": "Though our absolute quantitative results are not directly comparable to the results of<cite> Kim et al. (2006)</cite> , we indirectly compared them by analyzing the utility of features in isolation and combined. While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews). More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews<cite> (Kim et al., 2006</cite>; Danescu-Niculescu-Mizil et al., 2009) , have no predictive power for peer reviews.",
  "y": "uses"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_10",
  "x": "More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews<cite> (Kim et al., 2006</cite>; Danescu-Niculescu-Mizil et al., 2009) , have no predictive power for peer reviews. Perhaps because the paper grades and other helpfulness ratings are not visible to the reviewers, we have less of a social dimension for predicting the helpfulness of peer reviews. We also found that SVM regression does not favor ranking over predicting helpfulness as in<cite> (Kim et al., 2006)</cite> .",
  "y": "differences"
 },
 {
  "id": "4f1a48dc79b9a099783d7e63741883_11",
  "x": "Though our absolute quantitative results are not directly comparable to the results of<cite> Kim et al. (2006)</cite> , we indirectly compared them by analyzing the utility of features in isolation and combined. While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews). More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews<cite> (Kim et al., 2006</cite>; Danescu-Niculescu-Mizil et al., 2009) , have no predictive power for peer reviews.",
  "y": "differences"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_0",
  "x": "**INTRODUCTION** Springing forth from the pages of science fiction and capturing the daydreams of weary chore-doers everywhere, the promise and potential of general-purpose robotic assistants that follow natural language instructions has been long understood. Taking a small step towards this goal, recent work has begun developing artificial agents that follow natural language navigation instructions in perceptually-rich, simulated environments<cite> [4,</cite> 6] .",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_1",
  "x": "Many of these tasks have been developed from datasets of panoramic images captured in real scenes -e.g. Google StreetView images in Touchdown [6] or Matterport3D panoramas captured in homes in Vision-and-Language Navigation (VLN)<cite> [4]</cite> . This paradigm enables efficient data collection and high visual fidelity compared to 3D scanning or creating synthetic environments; however, scenes are only observed from a sparse set of points relative to the full 3D environment (\u223c117 viewpoints per environment in VLN).",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_2",
  "x": "Vision-and-Language Navigation in Continuous Environments. In this work, we focus in on the Vision-and-Language Navigation (VLN)<cite> [4]</cite> task and lift these implicit assumptions by instantiating it in continuous 3D environments rendered in a high-throughput simulator [19] . Consequently, we call this task Vision-and-Language Navigation in Continuous Environments (VLN-CE).",
  "y": "uses"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_5",
  "x": "Language-guided visual navigation tasks require agents to follow navigation directions in simulated environments. There have been a number of recent tasks proposed in this space<cite> [4,</cite> 6, 13, 20] . Chen et al. [6] introduce the Touchdown task which studies outdoor language-guided navigation in Google Street View panoramas.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_6",
  "x": "Most related to our work is the Vision-and-Language Navigation (VLN) task of Anderson et al.<cite> [4]</cite> . VLN provides nav-graph trajectories and crowdsourced instructions in Matterport3D [5] environments as the Room-to-Room (R2R) dataset. We build VLN-CE directly on these annotations -converting R2R panorama-based trajectories to fine-grained paths in continuous Matterport3D environments ( Fig. 1 (a) to Fig. 1(b) ).",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_7",
  "x": "In overview, we develop this setting by transferring nav-graph-based Room-to-Room (R2R)<cite> [4]</cite> trajectories to reconstructed continuous Matterport3D environments in the Habitat simulator [19] . We discuss the task specification and the details of this transfer process in this section. Continuous Matterport3D Environments in Habitat.",
  "y": "uses"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_8",
  "x": "In contrast to the simulator used in VLN<cite> [4]</cite> , Habitat allows agents to navigate freely in the continuous environments. Observations and Actions. We select observation and action spaces to emulate a ground-based, zero-turning radius robot with a single, forward-mounted RGBD camera, similar to a LoCoBot [1] .",
  "y": "differences"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_9",
  "x": "Agents perceive the world through egocentric RGBD images from the simulator with a resolution of 256\u00d7256 and a horizontal field-of-view of 90 degrees. Note that this is similar to the egocentric RGB perception in the original VLN task<cite> [4]</cite> but differs from the panoramic observation space adopted by nearly all follow-up work [9, 17, 26, 29] . While the simulator is quite flexible in terms of agent actions, we consider four simple, low-level actions for agents in VLN-CE -move forward 0.25m, turn-left or turn-right 15 degrees, or stop to declare that the goal position has been reached.",
  "y": "similarities"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_10",
  "x": "In contrast, actions to move between panoramas in<cite> [4]</cite> traverse 2.25m on average and can include avoiding obstacles. ---------------------------------- **TRANSFERRING NAV-GRAPH TRAJECTORIES**",
  "y": "differences"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_11",
  "x": "Matterport3D Simulator and the Room-to-Room Dataset. The original VLN task is based on panoramas from Matterport3D (MP3D) [5] . To enable agent interaction with these panoramas, Anderson et al.<cite> [4]</cite> developed the Matterport3D Simulator.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_12",
  "x": "Edges were manually added or removed based on judgement whether an agent could navigate between nodes -including by avoiding minor obstacles 4 . Agents act by teleporting between adjacent nodes in this graph. Based on this simulator, Anderson et al.<cite> [4]</cite> collect the Roomto-Room (R2R) dataset containing 7189 trajectories each with three humangenerated instructions on average.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_13",
  "x": "We develop two models for VLN-CE. A simple sequence-to-sequence baseline and a more powerful cross-modal attentional model. While there are many differences in the details, these models are conceptually similar to early<cite> [4]</cite> and more recent [29] work in the nav-graph based VLN task.",
  "y": "similarities"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_14",
  "x": "---------------------------------- **AUXILIARY LOSSES AND TRAINING REGIMES** Aside from modeling details, much of the remaining progress in VLN has come from adjusting the training regime -adding auxiliary losses / rewards [17, 29] , mitigating exposure bias during training<cite> [4,</cite> 29] , or reducing data sparsity by incorporating synthetically generated data augmentation [9, 26] .",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_15",
  "x": "Aside from modeling details, much of the remaining progress in VLN has come from adjusting the training regime -adding auxiliary losses / rewards [17, 29] , mitigating exposure bias during training<cite> [4,</cite> 29] , or reducing data sparsity by incorporating synthetically generated data augmentation [9, 26] . We explore some of these directions for VLN-CE, but note that this is not an exhaustive accounting of impactful techniques. Particularly, we suspect that methods addressing exposure bias and data sparsity in VLN will help in the VLN-CE setting where these problems may be amplified by lengthy action sequences.",
  "y": "motivation"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_16",
  "x": "Imitation learning in auto-regressive settings suffers from a disconnect between training and test -agents are not exposed to the consequences of their actions during training. Prior work has shown significant gains by addressing this issue for VLN through scheduled sampling<cite> [4]</cite> or reinforcement learning fine-tuning [26, 29] . In this work, we apply Dataset Aggregation (DAgger) [24] towards the same end.",
  "y": "background"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_17",
  "x": "Imitation learning in auto-regressive settings suffers from a disconnect between training and test -agents are not exposed to the consequences of their actions during training. Prior work has shown significant gains by addressing this issue for VLN through scheduled sampling<cite> [4]</cite> or reinforcement learning fine-tuning [26, 29] . In this work, we apply Dataset Aggregation (DAgger) [24] towards the same end.",
  "y": "similarities"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_18",
  "x": "As is common practice, we perform early stopping based on val-unseen performance. We report standard metrics for visual navigation tasks defined in [2, <cite>4,</cite> 18] -trajectory length in meters (TL), navigation error in meters from goal at termination (NE), oracle success rate (OS), success rate (SR), success weighted by inverse path length (SPL), and normalized dynamic-time warping (nDTW). For our discussion, we will examine success rate and SPL as the primary metrics for performance and use NDTW to describe how paths differ in shape from ground truth trajectories.",
  "y": "uses"
 },
 {
  "id": "4f646eceef2e5fc447a367488b6aaf_19",
  "x": "For our discussion, we will examine success rate and SPL as the primary metrics for performance and use NDTW to describe how paths differ in shape from ground truth trajectories. For full details on these metrics, see [2, <cite>4,</cite> 18] . Implementation Details.",
  "y": "uses"
 },
 {
  "id": "51575bb1ffb066d9570551f3347622_0",
  "x": "How to extract useful information from unannotated large scale corpus has been a research issue. Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks <cite>[7,</cite> 8, 9 ]. The word2vec [10] is among the most widely used word embedding models today.",
  "y": "background"
 },
 {
  "id": "51575bb1ffb066d9570551f3347622_1",
  "x": "Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks <cite>[7,</cite> 8, 9] . The word2vec [10] is among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora.",
  "y": "background"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_0",
  "x": "Lenci, 2018; Boleda, 2019 , for an overview) is motivated by the distributional hypothesis (Harris, 1954) . This framework holds that meaning can be inferred from the linguistic context of the word, usually seen as co-occurrence data. The context of usage is even more crucial for characterizing meanings of ambiguous or polysemous words: a definition that does not take disambiguating context into account will be of limited use <cite>(Gadetsky et al., 2018)</cite> .",
  "y": "motivation"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_1",
  "x": "On a more abstract level, definition modeling is related to research on the analysis and evaluation of word embeddings (Levy and Goldberg, 2014a,b; Arora et al., 2018; Batchkarov et al., 2016; Swinger et al., 2018, e.g.) . It also relates to other works associating definitions and embeddings, like the \"reverse dictionary task\" (Hill et al., 2016 )-retrieving the definiendum knowing its definition, which can be argued to be the opposite of definition modeling-or works that derive embeddings from definitions (Wang et al., 2015; Tissier et al., 2017; Bosc and Vincent, 2018) . 3 Definition modeling as a sequence-to-sequence task<cite> Gadetsky et al. (2018)</cite> remarked that words are often ambiguous or polysemous, and thus generating a correct definition requires that we either use sense-level representations, or that we disambiguate the word embedding of the definiendum.",
  "y": "background"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_2",
  "x": "The disambiguation that<cite> Gadetsky et al. (2018)</cite> proposed was based on a contextual cue-ie. a short text fragment. As notes, the cues in Gadetsky et al.'s (2018) dataset do not necessarily contain the definiendum or even an inflected variant thereof.",
  "y": "background"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_3",
  "x": "There is no guarantee that a single embedding, even if it be contextualized, would preserve this wealth of information-that is to say, that you can cram all the information pertaining to the syntactic context into a single vector. Despite some key differences, all of the previously proposed architectures we are aware of (Noraset et al., 2017;<cite> Gadetsky et al., 2018</cite>; followed a pattern similar to sequence-to-sequence models. They all implicitly or explicitly used distinct submodules to encode the definiendum and to generate the definientia.",
  "y": "background"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_4",
  "x": "It also seems to provide a strong and reasonable bias for training the definition generation system. Such an approach, however, is not guaranteed to excel: forcibly omitted context could contain important information that might not be easily incorporated in the definiendum embedding. Being simple and natural, the SELECT approach resembles architectures like that of<cite> Gadetsky et al. (2018)</cite> and : the full encoder is dedicated to altering the embedding of the definiendum on the basis of its context; in that, the encoder may be seen as a dedicated contextualization sub-module.",
  "y": "similarities"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_5",
  "x": "The dataset of Noraset et al. (2017) (henceforth D Nor ) maps definienda to their respective definientia, as well as additional information not used here. In the dataset of<cite> Gadetsky et al. (2018)</cite> (henceforth D Gad ), each example consists of a definiendum, the definientia for one of its meanings and a contextual cue sentence. D Nor contains on average shorter definitions than D Gad .",
  "y": "background"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_6",
  "x": "However consider for instance the word \"elation\": that it be defined either as \"mirth\" or \"joy\" should only influence our metric slightly, and not be discounted as a completely wrong prediction. , as they did not report the perplexity of their system and focused on a different dataset; likewise, consider only the Chinese variant of the task. Perplexity measures for Noraset et al. (2017) and<cite> Gadetsky et al. (2018)</cite> are taken from the authors' respective publications. All our models perform better than previous proposals, by a margin of 4 to 10 points, for a relative improvement of 11-23%.",
  "y": "uses"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_7",
  "x": "Like<cite> Gadetsky et al. (2018)</cite> , we conclude that disambiguating the definiendum, when done correctly, improves performances: our best performing contex-tual model outranks the non-contextual variant by 5 to 6 points. The marking of the definiendum out of its context (ADD vs. SELECT) also impacts results. Note also that we do not rely on taskspecific external resources (unlike Noraset et al., 2017; or on pre-training (unlike<cite> Gadetsky et al., 2018)</cite> .",
  "y": "similarities"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_8",
  "x": "Like<cite> Gadetsky et al. (2018)</cite> , we conclude that disambiguating the definiendum, when done correctly, improves performances: our best performing contex-tual model outranks the non-contextual variant by 5 to 6 points. The marking of the definiendum out of its context (ADD vs. SELECT) also impacts results. Note also that we do not rely on taskspecific external resources (unlike Noraset et al., 2017; or on pre-training (unlike<cite> Gadetsky et al., 2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_9",
  "x": "Adding such a sub-module to our proposed architecture might diminish the number of mistagged definienda. Another possibility would be to pre-train the model, as was done by<cite> Gadetsky et al. (2018)</cite> : in our case in particular, the encoder could be trained for POS-tagging or lemmatization. Lastly, one important kind of mistakes we observed is hallucinations.",
  "y": "future_work"
 },
 {
  "id": "5177188d88391f08325262dbdefabf_10",
  "x": "We showed that our approach is competitive against a more naive 'contextualize and select' pipeline. This was demonstrated by comparison both to the previous contextualized model by<cite> Gadetsky et al. (2018)</cite> and to the Transformerbased SELECT variation of our model, which differs from the proposed architecture only in the context encoding pipeline. While our results are encouraging, given the existing benchmarks we were limited to perplexity measurements in our quantitative evaluation.",
  "y": "similarities"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_0",
  "x": "There has also been some success incorporating selectional preferences<cite> (Sun and Korhonen, 2009)</cite> . Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004) . Exceptions to this include Merlo and Stevenson (2001) , Joanis et al. (2008) and Stevenson (2010, 2011) .",
  "y": "background"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_1",
  "x": "We experimented with two datasets that have been used in prior work on verb clustering: the test sets 7-11 (3-14 classes) in Joanis et al. (2008) , and the 17 classes set in Sun et al. (2008) . We used the spectral clustering (SPEC) method and settings as in<cite> Sun and Korhonen (2009)</cite> but adopted the Bhattacharyya kernel (Jebara and Kondor, 2003) to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space. The mean-filed bound of the Bhattacharyya kernel is very similar to the KL divergence kernel (Jebara et al., 2004) which is frequently used in verb clustering experiments (Korhonen et al., 2003;<cite> Sun and Korhonen, 2009)</cite> .",
  "y": "uses"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_2",
  "x": "We experimented with two datasets that have been used in prior work on verb clustering: the test sets 7-11 (3-14 classes) in Joanis et al. (2008) , and the 17 classes set in Sun et al. (2008) . We used the spectral clustering (SPEC) method and settings as in<cite> Sun and Korhonen (2009)</cite> but adopted the Bhattacharyya kernel (Jebara and Kondor, 2003) to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space. The mean-filed bound of the Bhattacharyya kernel is very similar to the KL divergence kernel (Jebara et al., 2004) which is frequently used in verb clustering experiments (Korhonen et al., 2003;<cite> Sun and Korhonen, 2009)</cite> .",
  "y": "background"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_3",
  "x": "In the next section, we will demonstrate that F3 outperforms F1 regardless of the feature number setting. The features are normalized to sum 1. The clustering results are evaluated using FMeasure as in<cite> Sun and Korhonen (2009)</cite> which provides the harmonic mean of precision (P ) and recall (R) P is calculated using modified purity -a global measure which evaluates the mean precision of clusters.",
  "y": "uses"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_4",
  "x": "This indicates that the frame independence assumption is a poor assumption. F3 yields substantially better result than F2 and F1. The result of F3 is 6.4% higher than the result (F=63.28) reported in<cite> Sun and Korhonen (2009)</cite> using the F1 feature.",
  "y": "differences"
 },
 {
  "id": "52b9efca757fe5a376ca6b548d77ce_5",
  "x": "Moreover, we plan to use Bayesian inference as in Vlachos et al. (2009); Stevenson (2010, 2011) to infer the actual parameter values and avoid the relaxation. Finally, we plan to supplement the DA feature with evidence from the slot fillers of the alternating slots, in the spirit of earlier work (McCarthy, 2000; Merlo and Stevenson, 2001; Joanis et al., 2008) . Unlike these previous works, we will use selectional preferences to generalize the argument heads but will do so using preferences from distributional data <cite>(Sun and Korhonen, 2009</cite> ) rather than WordNet, and use all argument head data in all frames.",
  "y": "uses"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_0",
  "x": "**INTRODUCTION** We contrasted two translation methods for the Workshop on Statistical Machine Translation (WMT2006) shared-task. One is a phrase-based translation in which a phrasal unit is employed for translation <cite>(Koehn et al., 2003)</cite> .",
  "y": "background"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_1",
  "x": "In a phrase-based statistical translation <cite>(Koehn et al., 2003)</cite> , a bilingual text is decomposed as K phrase translation pairs (\u0113 1 ,f\u0101 1 ), (\u0113 2 ,f\u0101 2 ), ...: The input foreign sentence is segmented into phrasesf K 1 , mapped into corresponding English\u0113 K 1 , then, reordered to form the output English sentence according to a phrase alignment index mapping\u0101. In a hierarchical phrase-based translation (Chiang, 2005) , translation is modeled after a weighted synchronous-CFG consisting of production rules whose right-hand side is paired (Aho and Ullman, 1969) : X \u2192 \u03b3, \u03b1, \u223c where X is a non-terminal, \u03b3 and \u03b1 are strings of terminals and non-terminals.",
  "y": "background"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_2",
  "x": "---------------------------------- **PHRASE/RULE EXTRACTION** The phrase extraction algorithm is based on those presented by<cite> Koehn et al. (2003)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_3",
  "x": "First, manyto-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003) , in both directions and by combining the results based on a heuristic (Och and Ney, 2004) . Second, phrase translation pairs are extracted from the word aligned corpus <cite>(Koehn et al., 2003)</cite> . The method exhaustively extracts phrase pairs ( f j+m j , e i+n i ) from a sentence pair ( f J 1 , e I 1 ) that do not violate the word alignment constraints a.",
  "y": "similarities uses"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_4",
  "x": "**DECODING** The decoder for the phrase-based model is a left-toright generation decoder with a beam search strategy synchronized with the cardinality of already translated foreign words. The decoding process is very similar to those described in <cite>(Koehn et al., 2003)</cite> : It starts from an initial empty hypothesis.",
  "y": "similarities uses"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_5",
  "x": "Our phrase-based model uses a standard pharaoh feature functions listed as follows <cite>(Koehn et al., 2003)</cite> : \u2022 Relative-count based phrase translation probabilities in both directions. \u2022 Lexically weighted feature functions in both directions.",
  "y": "similarities uses"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_6",
  "x": "\u2022 The number of words in English-side and the number of phrases that constitute translation. For details, please refer to<cite> Koehn et al. (2003)</cite> . In addition, we added three feature functions to restrict reorderings and to represent globalized insertion/deletion of words:",
  "y": "background"
 },
 {
  "id": "537ec54aac2c3e3c62070468dcd8a3_7",
  "x": "For each differently tokenized corpus, we computed word alignments by a HMM translation model (Och and Ney, 2003) and by a word alignment refinement heuristic of \"grow-diagfinal\" <cite>(Koehn et al., 2003)</cite> . Different preprocessing yields quite divergent alignment points as illustrated in Table 1 . The table also shows the numbers for the intersection and union of three alignment annotations.",
  "y": "similarities uses"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_0",
  "x": "Thus, whereas a complete dependency structure provides a fully disambiguated analysis of a sentence, this analysis is typically less complex than in frameworks based on constituent analysis and can therefore often be computed deterministically with reasonable accuracy. Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000) , English<cite> (Yamada and Matsumoto, 2003)</cite> , Turkish (Oflazer, 2003) , and Swedish (Nivre et al., 2004) . For English, the interest in dependency parsing has been weaker than for other languages.",
  "y": "background"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_1",
  "x": "Moreover, the deterministic dependency parser of<cite> Yamada and Matsumoto (2003)</cite> , when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of Collins (1997) and Charniak (2000) . The parser described in this paper is similar to that of<cite> Yamada and Matsumoto (2003)</cite> in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank. However, there are also important differences between the two approaches.",
  "y": "background"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_2",
  "x": "Moreover, the deterministic dependency parser of<cite> Yamada and Matsumoto (2003)</cite> , when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of Collins (1997) and Charniak (2000) . The parser described in this paper is similar to that of<cite> Yamada and Matsumoto (2003)</cite> in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank. However, there are also important differences between the two approaches.",
  "y": "uses similarities"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_3",
  "x": "The parser described in this paper is similar to that of<cite> Yamada and Matsumoto (2003)</cite> in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank. However, there are also important differences between the two approaches. First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (essentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithm proposed in Nivre (2003) , which combines bottomup and top-down processing in a single pass in order to achieve incrementality.",
  "y": "extends differences"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_4",
  "x": "As far as we know, this makes it different from all previous systems for dependency parsing applied to the Penn Treebank (Eisner, 1996; <cite>Yamada and Matsumoto, 2003)</cite> , although there are systems that extract labeled grammatical relations based on shallow parsing, e.g. Buchholz (2002) . The fact that we are working with labeled dependency graphs is also one of the motivations for choosing memory-based learning over support vector machines, since we require a multi-class classifier. Even though it is possible to use SVM for multi-class classification, this can get cumbersome when the number of classes is large.",
  "y": "differences"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_5",
  "x": "We are grateful to Yamada and Matsumoto for letting us use their rule set, which is a slight modification of the rules used by Collins (1999) . This permits us to make exact comparisons with the parser of<cite> Yamada and Matsumoto (2003)</cite> , but also the parsers of Collins (1997) and Charniak (2000) , which are evaluated on the same data set in<cite> Yamada and Matsumoto (2003)</cite> . One problem that we had to face is that the standard conversion of phrase structure trees to dependency trees gives unlabeled dependency trees, whereas our parser requires labeled trees.",
  "y": "uses"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_6",
  "x": "We use the following metrics for evaluation: The proportion of non-root words that are assigned the correct head<cite> (Yamada and Matsumoto, 2003)</cite> . The proportion of root words that are analyzed as such<cite> (Yamada and Matsumoto, 2003)</cite> . The proportion of sentences whose unlabeled dependency structure is completely correct<cite> (Yamada and Matsumoto, 2003)</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_7",
  "x": "Therefore, we have also included the labeled attachment score restricted to the G set for the parser using the B set (BG), and we see then that the attachment score improves, especially for Model 2. (All differences are significant beyond the .01 level; McNemar's test.) Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with Collins (1997) (Model 3) , Charniak (2000) , and<cite> Yamada and Matsumoto (2003)</cite> . 5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of<cite> Yamada and Matsumoto (2003)</cite> . We believe that there are mainly three reasons for this.",
  "y": "uses"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_8",
  "x": "On the other hand, the labeled attachment score drops, but it must be remembered that these scores are not really comparable, since the number of classes in the classification problem increases from 7 to 50 as we move from the G set to the B set. Therefore, we have also included the labeled attachment score restricted to the G set for the parser using the B set (BG), and we see then that the attachment score improves, especially for Model 2. (All differences are significant beyond the .01 level; McNemar's test.) Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with Collins (1997) (Model 3) , Charniak (2000) , and<cite> Yamada and Matsumoto (2003)</cite> . 5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of<cite> Yamada and Matsumoto (2003)</cite> .",
  "y": "differences"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_9",
  "x": "5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of<cite> Yamada and Matsumoto (2003)</cite> . We believe that there are mainly three reasons for this. First of all, the part-of-speech tagger used for preprocessing in our experiments has a lower accuracy than the one used by<cite> Yamada and Matsumoto (2003)</cite> (96.1% vs. 97.1%) .",
  "y": "differences"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_10",
  "x": "Although this is not a very interesting explanation, it undoubtedly accounts for part of the difference. Secondly, since 5 The information in the first three rows is taken directly from<cite> Yamada and Matsumoto (2003)</cite> . our parser makes crucial use of dependency type information in predicting the next action of the parser, it is very likely that it suffers from the lack of real dependency labels in the converted treebank.",
  "y": "uses"
 },
 {
  "id": "53ed85f4bfa634656062ad6ba342d2_11",
  "x": "The corresponding F-measures for our best parser (Model 2, BG) are 99.0% and 94.7%. For the larger B set, our best parser achieves an F-measure of 96.9% (DEP labels included), which can be compared with 97.0% for a similar (but larger) set of labels in Collins (1999) . 6 Although none of the previous results on labeling accuracy is strictly comparable to ours, it nevertheless seems fair to conclude that the<cite> (Yamada and Matsumoto, 2003)</cite> labeling accuracy of the present parser is close to the state of the art, even if its capacity to derive correct structures is not.",
  "y": "differences"
 },
 {
  "id": "5596207b89d917db38c04af49c08aa_0",
  "x": "**INTRODUCTION** Attention-based neural networks have demonstrated success in a wide range of NLP tasks ranging from neural machine translation , image captioning (Xu et al., 2015) , and speech recognition (Chorowski et al., 2015) . Benefiting from the availability of large-scale benchmark datasets such as SQuAD (Rajpurkar et al., 2016) , the attention-based neural networks has spread to machine comprehension and question answering tasks to allow the model to attend over past output vectors (Wang & Jiang, 2017; Seo et al., 2017; Xiong et al., 2017;<cite> Hu et al., 2017</cite>; Pan et al., 2017) .",
  "y": "background"
 },
 {
  "id": "5596207b89d917db38c04af49c08aa_2",
  "x": "The concatenation of raw features as inputs are processed in fusion layers followed by encoder layers to form more abstract representations. Here we choose a bi-directional Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997 ) to obtain more abstract representations for words in passages and questions. Different from the commonly used approaches that every single model has exactly one question and passage encoder (Seo et al., 2017;<cite> Hu et al., 2017)</cite> , our encoder layers simultaneously calculate multiple question and passage representations, for the purpose of serving different parts of attention functions of different phases.",
  "y": "differences"
 },
 {
  "id": "5596207b89d917db38c04af49c08aa_4",
  "x": "To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo et al., 2017) and RNET from their recently published papers instead of using the up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline is one implementation of MReader, re-named as Iterative Aligner which has very similar results as those of MReader <cite>(Hu et al., 2017)</cite> 71.1 / 79.5 71.3 / 79.7 75.6 / 82.8 75.9 / 82.9 MReader <cite>(Hu et al., 2017)</cite> N As shown in Table 3 , in the single model setting, our model PhaseCond is clearly more effective than all the single-layered models (BiDAF and RNET) and multi-layered models (MReader and Iterative Aligner). We draw the same conclusion for the ensemble model setting, despite that the RNET works better on the Dev EM measure.",
  "y": "similarities"
 },
 {
  "id": "55e429045af4434f9cb27ae8c6db66_0",
  "x": "AES is challenging since it relies not only on grammars, but also on semantics, discourse and pragmatics. Traditional approaches treat AES as a classification (Larkey, 1998; Rudner and Liang, 2002) , regression (Attali and Burstein, 2004;<cite> Phandi et al., 2015)</cite> , or ranking classification problem (Yannakoudakis et al., 2011; Chen and He, 2013) , addressing AES by supervised learning. Features are typically bag-of-words, spelling errors and lengths, such word length, sentence length and essay length, etc.",
  "y": "background"
 },
 {
  "id": "55e429045af4434f9cb27ae8c6db66_1",
  "x": "Feature templates follow <cite>(Phandi et al., 2015)</cite> , extracted by EASE 1 , which are briefly listed in Table 1 . \"Useful n-grams\" are determined using the Fisher test to separate the good scoring essays and bad scoring essays. Good essays are essays with a score greater than or equal to the average score, and the remainder are considered as bad scoring essays. The top 201 ngrams with the highest Fisher values are chosen as the bag of features and these top 201 n-grams constitute useful n-grams.",
  "y": "uses"
 },
 {
  "id": "55e429045af4434f9cb27ae8c6db66_2",
  "x": "However, for different prompts, features statistics vary significantly. This raises challenges for discrete feature patterns. ML-\u03c1 <cite>(Phandi et al., 2015)</cite> was proposed to address this issue.",
  "y": "uses"
 },
 {
  "id": "55e429045af4434f9cb27ae8c6db66_3",
  "x": "The essay scores are scaled into the range from 0 to 1. The settings of data preparation follow <cite>(Phandi et al., 2015)</cite> . We use quadratic weighted kappa (QWK) as the metric.",
  "y": "uses"
 },
 {
  "id": "55e429045af4434f9cb27ae8c6db66_4",
  "x": "For domainadaptation (cross-domain) experiments, we follow <cite>(Phandi et al., 2015)</cite> , picking four pairs of essay prompts, namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928, where 1\u21922 denotes prompt 1 as source domain and prompt Hyper-parameters We use Adagrad for optimization. Word embeddings are randomly initialized and the hyper-parameter settings are listed in Table 3 . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "57af9690eb41ff3f9217da6138425f_0",
  "x": "Some chapters suggest extensions of or amendments to the centering theory with a view to achieving a more comprehensive and successful model (e.g., the chapters by Kameyama, Roberts, and Walker). Ideally, in addition to papers such as Kameyama's and Walker's, this collection could perhaps also have featured extended versions of papers, such as those of Kehler (1997) and Hahn and Strube (1997) , that highlight certain weaknesses of the original centering model or suggest extensions or alternative solutions <cite>(Strube 1998)</cite> . It must be acknowledged here that the production schedule of this volume may have been a factor in not including some of this recent work, and also that space limits might not have allowed all possible areas of centering to be covered.",
  "y": "background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_0",
  "x": "Bias may reduce the accuracy of these models, and at worst, will mean that the models actively discriminate against the same groups they are designed to protect. Our study focuses on racial bias in hate speech and abusive language detection datasets<cite> (Waseem, 2016</cite>; Waseem and Hovy, 2016; Golbeck et al., 2017; Founta et al., 2018) , all of which use data collected from Twitter. We train classifiers using each of the datasets and use a corpus of tweets with demographic information to compare how each classifier performs on tweets written in African-American English (AAE) versus Standard American English (SAE) (Blodgett et al., 2016) .",
  "y": "background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_1",
  "x": "While not directly measuring bias, prior work has explored how annotation schemes and the identity of the annotators<cite> (Waseem, 2016</cite> ) might be manipulated to help to avoid bias. Dixon et al. (2018) directly measured biases in the Google Perspective API classifier, 1 trained on data from Wikipedia talk comments, finding that it tended to give high toxicity scores to innocuous statements like \"I am a gay man\". They called this \"false positive bias\", caused by the model overgeneralizing from the training data, in this case from examples where \"gay\" was used pejoratively.",
  "y": "background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_2",
  "x": "This dataset consists of 16,849 tweets labeled as either racism, sexism, or neither. Most of the tweets categorized as sexist relate to debates over an Australian TV show and most of those considered as racist are anti-Muslim. To account for potential bias in the previous dataset,<cite> Waseem (2016)</cite> relabeled 2876 tweets in the dataset, along with a new sample from the tweets originally collected.",
  "y": "background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_3",
  "x": "On the other hand, this classifier is 1.7 times more likely to classify tweets in the black-aligned corpus as sexist. For<cite> Waseem (2016)</cite> we see that there is no significant difference in the estimated rates at which tweets are classified as racist across groups, although the rates remain low. Tweets in the black-aligned corpus are classified as containing sexism almost twice as frequently and 1.1 times as frequently classified as containing racism and sexism compared to those in the white-aligned corpus.",
  "y": "similarities"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_4",
  "x": "In most cases the racial disparities persist, although they are generally smaller in magnitude and in some cases the direction even changes. Table 3 shows that for tweets containing the word \"n*gga\", classifiers trained on Waseem and Hovy (2016) and<cite> Waseem (2016)</cite> are both predict black-aligned tweets to be instances of sexism approximately 1.5 times as often as white-aligned tweets. The classifier trained on the data is significantly less likely to classify black-aligned tweets as hate speech, although it is more likely to classify them as offensive.",
  "y": "background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_5",
  "x": "We see similar results for Waseem and Hovy (2016) and<cite> Waseem (2016)</cite> . In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism. The Waseem and Hovy (2016) classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class.",
  "y": "similarities"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_6",
  "x": "Classifiers trained on data from Waseem and Hovy (2016) and<cite> Waseem (2016)</cite> only predicted a small fraction of the tweets to be racism. Looking at the sexism class on the other hand, we see that both models were consistently classifying tweets in the black-aligned corpus as sexism at a substantially higher rate than those in the white-aligned corpus. Given this result, and the gender biases identified in these data by Park et al. (2018), it not apparent that the purportedly expert annotators were any less biased than amateur annotators<cite> (Waseem, 2016)</cite> .",
  "y": "similarities background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_7",
  "x": "First, we expect that some biases emerge at the point of data collection. Some studies sampled tweets using small, ad hoc sets of keywords created by the authors (Waseem and Hovy, 2016;<cite> Waseem, 2016</cite>; Golbeck et al., 2017) , an approach demonstrated to produce poor results (King et al., 2017) . Others start with large crowdsourced dictionaries of keywords, which tend to include many irrelevant terms, resulting in high rates of false positives Founta et al., 2018) .",
  "y": "uses background"
 },
 {
  "id": "57e65909baf823ff00a9a10a64fffd_8",
  "x": "The datasets considered here relied upon a range of different annotators, from the authors (Golbeck et al., 2017; Waseem and Hovy, 2016) and crowdworkers Founta et al., 2018) to activists<cite> (Waseem, 2016)</cite> . Even the classifier trained on expert-labeled data<cite> (Waseem, 2016)</cite> flags black-aligned tweets as sexist at almost twice the rate of white-aligned tweets. While we agree that there is value in working with domain-experts to annotate data, these results suggest that activists may be prone to similar biases as academics and crowdworkers.",
  "y": "extends differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_0",
  "x": "In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; <cite>Miwa and Bansal, 2016</cite>; . End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task dependencies to be modeled explicitly for entity recognition. As a result, it gives better relation extraction accuracies compared to pipelines.",
  "y": "motivation"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_1",
  "x": "<cite>Miwa and Bansal (2016)</cite> were among the first to use neural networks for end-to-end relation extraction, showing highly promising results. In particular, <cite>they</cite> used bidirectional LSTM (Graves et al., 2013) to learn hidden word representations under a sentential context, and further leveraged treestructured LSTM (Tai et al., 2015) to encode syntactic information, given the output of a parser. The resulting representations are then used for making local decisions for entity and relation extraction incrementally, leading to much improved results compared with the best statistical model (Li and Ji, 2014) .",
  "y": "background"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_2",
  "x": "This demonstrates the strength of neural representation learning for end-to-end relation extraction. On the other hand, <cite>Miwa and Bansal (2016)</cite> 's model is trained locally, without considering structural correspondences between incremental decisions. This is unlike existing statistical methods, which utilize well-studied structured prediction methods to address the problem (Li and Ji, 2014; Miwa and Sasaki, 2014) .",
  "y": "motivation background"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_3",
  "x": "We take a different approach to representation learning, addressing two potential limitations of <cite>Miwa and Bansal (2016)</cite> . First, <cite>Miwa and Bansal (2016)</cite> rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008) . However, parsing errors can lead to encoding inaccuracies of tree-LSTMs, thereby hurting relation extraction potentially. We take an alternative approach to integrating syntactic information, by taking the hidden LSTM layers of a bi-affine attention parser (Dozat and Manning, 2016) to augment input representations.",
  "y": "motivation"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_4",
  "x": "This is different from the actionbased method of Li and Ji (2014) , yet has shown to be more flexible and accurate (Miwa and Sasaki, 2014) . We take a different approach to representation learning, addressing two potential limitations of <cite>Miwa and Bansal (2016)</cite> . First, <cite>Miwa and Bansal (2016)</cite> rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008) .",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_5",
  "x": "Our method is also free from a particular syntactic formalism, such as dependency grammar, constituent grammar or combinatory categorial grammar, requiring only hidden representations on word that contain syntactic information. In contrast, the method of <cite>Miwa and Bansal (2016)</cite> must consider tree LSTM formulations that are specific to grammar formalisms, which can be structurally different (Tai et al., 2015) . Second, <cite>Miwa and Bansal (2016)</cite> did not explicitly learn the representation of segments when predicting entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in several studies (Wang and Chang, 2016; .",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_6",
  "x": "Second, <cite>Miwa and Bansal (2016)</cite> did not explicitly learn the representation of segments when predicting entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in several studies (Wang and Chang, 2016; . We take the LSTM-Minus method of Wang and Chang (2016) , modelling a segment as the difference between its last and first LSTM hidden vectors. This method is highly efficient, yet gives as accurate results as compared to more complex neural network structures to model a span of words (Cross and Huang, 2016) .",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_7",
  "x": "We take the LSTM-Minus method of Wang and Chang (2016) , modelling a segment as the difference between its last and first LSTM hidden vectors. This method is highly efficient, yet gives as accurate results as compared to more complex neural network structures to model a span of words (Cross and Huang, 2016) . Evaluation on two benchmark datasets shows that our method outperforms previous methods of <cite>Miwa and Bansal (2016)</cite> , Li and Ji (2014) and Miwa and Sasaki (2014) , giving the best reported results on both benchmarks.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_8",
  "x": "---------------------------------- **METHOD** We follow Miwa and Sasaki (2014) and , treating relation extraction as a tablefilling problem, performing entity detection and relation classification using a single incremental model, which is similar in spirit to <cite>Miwa and Bansal (2016)</cite> by performing the task end-to-end.",
  "y": "similarities"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_9",
  "x": "When i = j, T (i, j) denotes an entity boundary label. We map entity words into labels under the BILOU (Begin, Inside, Last, Outside, Unit) scheme, assuming that there are no overlapping entities in one sentence (Li and Ji, 2014; Miwa and Sasaki, 2014; <cite>Miwa and Bansal, 2016</cite>) . Only the upper triangular table is necessary for indicating the relations.",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_10",
  "x": "Following <cite>Miwa and Bansal (2016)</cite> , we use a neural network to learn the vector representation of T i\u22121 , and then use Equation 1 to rank candidate next labels. There are two types of input features, including the word sequence w 1 w 2 \u00b7 \u00b7 \u00b7 w n , and the readily filled label sequence l 1 l 2 \u00b7 \u00b7 \u00b7 l i\u22121 . We build a neural network to represent T i\u22121 .",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_11",
  "x": "Two different forms of embeddings are used based on the word form, one being obtained by using a randomly initialized look-up table E w , 2 We remove the illegal table-filling labels during decoding for training and testing. The above two components have also been used by <cite>Miwa and Bansal (2016)</cite> . We further enhance the word representation by using its character sequence Lample et al., 2016) , taking a convolution neural network (CNN) to derive a character-based word representation h char , which has been demonstrated effective for several NLP tasks (dos Santos and Gatti, 2014) .",
  "y": "extends"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_12",
  "x": "We follow <cite>Miwa and Bansal (2016)</cite> , learning global context representations using LSTMs. Three basic LSTM structures are used: a leftto-right word LSTM ( and a left-to-right entity boundary label LSTM ( \u2212 \u2212\u2212\u2212 \u2192 LSTM e ).",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_13",
  "x": "Each LSTM derives a sequence of hidden vectors for inputs. For example, for Different from <cite>Miwa and Bansal (2016)</cite> , who use the output hidden vectors {h i } of LSTMs to represent words, we exploit segment representations as well.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_14",
  "x": "**SYNTACTIC FEATURES** Previous work has shown that syntactic features are useful for relation extraction (Zhou et al., 2005) . For example, the shortest dependency path has been used by several relation extraction models (Bunescu and Mooney, 2005; <cite>Miwa and Bansal, 2016</cite>) .",
  "y": "background"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_15",
  "x": "For example, the shortest dependency path has been used by several relation extraction models (Bunescu and Mooney, 2005; <cite>Miwa and Bansal, 2016</cite>) . Here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures. In particular, we take state-of-the-art syntactic parsers that use encoder-decoder neural models (Buys and Blunsom, 2015; Kiperwasser and Goldberg, 2016) , where the encoder represents the syntactic features of the input sentences.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_16",
  "x": "For example, we can use a constituent parser in the same way by dumping the implicit encoder features. Our exploration of syntactic features has two main advantages over the method of <cite>Miwa and Bansal (2016)</cite> , where dependency path LSTMs are used for relation classification. On the one hand, incorrect dependency paths between entity pairs can propagate to relation classification in <cite>Miwa and Bansal (2016)</cite> , because these paths rely on explicit discrete outputs from a syntactic parser.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_17",
  "x": "On the one hand, incorrect dependency paths between entity pairs can propagate to relation classification in <cite>Miwa and Bansal (2016)</cite> , because these paths rely on explicit discrete outputs from a syntactic parser. Our method can avoid the problem since we do not compute parser outputs. On the other hand, the computation complexity is largely reduced by using our method since sequential LSTMs are based on inputs only, while the dependency path LSTMs should be computed based on the dynamic entity detection outputs.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_18",
  "x": "**LOCAL OPTIMIZATION** Previous work (<cite>Miwa and Bansal, 2016</cite>; trains model parameters by modeling each step for labeling one input sentence separately. Given a partial table T , its neural representation h T is first obtained, and then compute the next label scores {l 1 , l 2 , \u00b7 \u00b7 \u00b7 , l s } using Equation 1.",
  "y": "background"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_19",
  "x": "For the ACE05 dataset, we follow Li and Ji (2014) and <cite>Miwa and Bansal (2016)</cite> , splitting and preprocessing the dataset into training, development and test sets. 5 For the CONLL04 dataset, we follow Miwa and Sasaki (2014) to split the data into training and test corpora, and then divide 10% of the training corpus for development. We use the micro F1-measure as the major metric to evaluate model performances, treating an entity as correct when its head region and type are both correct, 6 and regard a relation as correct when the argument entities and the relation category are all correct.",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_20",
  "x": "For the local model, we follow <cite>Miwa and Bansal (2016)</cite> , training parameters only for entity detection during the first 20 iterations. For the global model, we pretrain our model using local optimization for 40 iterations, before conducting beam global optimization. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_21",
  "x": "We consider the baseline system with no syntactic features using local training. Compared with <cite>Miwa and Bansal (2016)</cite> features for entity detection. Feature ablation experiments are conducted for the two types of features.",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_22",
  "x": "We study the influence of training strategies for relation extraction without using syntactic features. For the local model, we apply scheduled sampling (Bengio et al., 2015) , which has been shown to improve the performance of relation extraction by <cite>Miwa and Bansal (2016)</cite> . Table 4 shows the results.",
  "y": "background"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_23",
  "x": "Table 5 shows the development results using both local and global optimization. The proposed features improve the relation performances significantly under both settings (p < 10 \u22124 ), demonstrating that our use of syntactic features is highly effective. We also compare our feature integration method with the traditional methods based on syntactic outputs which <cite>Miwa and Bansal (2016)</cite> and all previous methods use.",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_24",
  "x": "<cite>Miwa and Bansal (2016)</cite> , who exploit end-to-end LSTM neural networks with local optimization, and L&J (2014) and M&S (2014) refer to Li and Ji (2014) and Miwa and Sasaki (2014) , respectively, which are both globally optimized models using discrete features, giving the top F-scores among statistical models. 7 Overall, neural models give better performances than statistical models, and global optimization can give improved performances as well. Our final model achieves the best performances on both datasets.",
  "y": "differences"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_25",
  "x": "To understand the effectiveness of the proposed syntactic features, we examine the relation Fscores with respect to entity distances. <cite>Miwa and Bansal (2016)</cite> exploit the shortest dependency path, which can make the distance between two entities closer compared with their sequential dis-tance, thus facilitating relation extraction. We verify whether the proposed syntactic features can benefit our model similarly.",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_26",
  "x": "Early work conducts joint inference for separate models (Ji and Grishman, 2005; Yih, 2004, 2007) . Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; <cite>Miwa and Bansal, 2016</cite>; , and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging Lample et al., 2016) , parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) , relation classification Vu et al., 2016; <cite>Miwa and Bansal, 2016</cite>) and sentiment analysis .",
  "y": "uses"
 },
 {
  "id": "580713b57ae47692af0d0c86a07fd1_27",
  "x": "Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; <cite>Miwa and Bansal, 2016</cite>; , and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging Lample et al., 2016) , parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) , relation classification Vu et al., 2016; <cite>Miwa and Bansal, 2016</cite>) and sentiment analysis . Based on the output of LSTM structures, Wang and Chang (2016) introduce segment features, and apply it to dependency parsing.",
  "y": "background"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_0",
  "x": "ED is a crucial component in the overall task of event extraction, which also involves event argument discovery. Recent systems for event extraction have employed either a pipeline architecture with separate classifiers for trigger and argument labeling (Ji and Grishman, 2008; Gupta and Ji, 2009 ; Patwardhan 1 https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ english-events-guidelines-v5. 4.3.pdf and Rilof, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a) or a joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b;<cite> Li et al., 2013b</cite>; Venugopal et al., 2014) .",
  "y": "background"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_1",
  "x": "Although this approach has achieved the top performance (Hong et al., 2011;<cite> Li et al., 2013b)</cite> , it suffers from at least two issues: (i) The choice of features is a manual process and requires linguistic intuition as well as domain expertise, implying additional studies for new application domains and limiting the capacity to quickly adapt to these new domains. (ii) The supervised NLP toolkits and resources for feature extraction might involve errors (either due to the imperfect nature or the performance loss of the toolkits on new domains (Blitzer et al., 2006; Daum\u00e9 III, 2007; McClosky et al., 2010) ), probably propagated to the final event detector.",
  "y": "motivation"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_2",
  "x": "We formalize the event detection problem as a multi-class classification problem. Given a sentence, for every token in that sentence, we want to predict if the current token is an event trigger: i.e, does it express some event in the pre-defined event set or not<cite> (Li et al., 2013b)</cite> ? The current token along with its context in the sentence constitute an event trigger candidate or an example in multiclass classification terms.",
  "y": "uses background"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_3",
  "x": "We evaluate the presented CNN over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaning 529 documents (14,849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010;<cite> Li et al., 2013b)</cite> . The ACE 2005 corpus has 33 event subtypes that, along with one class \"None\" for the non-trigger tokens, constitutes a 34-class classification problem.",
  "y": "uses"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_4",
  "x": "The state-of-the-art systems for event detection on the ACE 2005 dataset have followed the traditional feature-based approach with rich hand-designed feature sets, and statistical classifiers such as MaxEnt and perceptron for structured prediction in a joint architecture (Hong et al., 2011;<cite> Li et al., 2013b)</cite> . In this section, we compare the proposed CNNs with these state-of-the-art systems on the blind test set. Table 2 presents the overall performance of the systems with gold-standard entity mention and type information 4 .",
  "y": "background"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_5",
  "x": "In this section, we compare the proposed CNNs with these state-of-the-art systems on the blind test set. Table 2 presents the overall performance of the systems with gold-standard entity mention and type information 4 . As we can see from the table, considering the systems that only use sentence level information, CNN1 significantly outperforms the MaxEnt classifier as well as the joint beam search with local features from <cite>Li et al. (2013b)</cite> (an improvement of 1.6% in F1 score), and performs comparably with the joint beam search approach using both local and global features<cite> (Li et al., 2013b)</cite> .",
  "y": "differences"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_6",
  "x": "As we can see from the table, considering the systems that only use sentence level information, CNN1 significantly outperforms the MaxEnt classifier as well as the joint beam search with local features from <cite>Li et al. (2013b)</cite> (an improvement of 1.6% in F1 score), and performs comparably with the joint beam search approach using both local and global features<cite> (Li et al., 2013b)</cite> . This is remarkable since CNN1 does not require any external features 5 , in contrast to the other featurebased systems that extensively rely on such external features to perform well. More interestingly, when the entity type information is incorporated into CNN1, we obtain CNN2 that still only needs sentence level information but achieves the stateof-the-art performance for this task (an improvement of 1.5% over the best system with only sentence level information<cite> (Li et al., 2013b)</cite> ).",
  "y": "differences"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_7",
  "x": "Table 3 compares the performance of CNN1 and the feature-based systems in a more realistic setting, where entity mentions and types are acquired from an automatic high-performing name tagger and information extraction system<cite> (Li et al., 2013b)</cite> . Note that CNN1 is eligible for this comparison as it does not utilize any external features, thus avoiding usage of the name tagger and the information extraction system to identify entity mentions and types. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "586a0f40a9299ef2753d2b0575eff8_9",
  "x": "First, rather than relying on the symbolic and concrete forms (i.e words, types etc) to construct features as the traditional feature-based systems (Ji and Grishman, 2008;<cite> Li et al., 2013b)</cite> do, CNNs automatically induce their features from word embeddings, the general distributed representation of words that is shared across domains. This helps CNNs mitigate the lexical sparsity, learn more general and effective feature representation for trigger candidates, and thus bridge the gap between domains. Second, as CNNs minimize the reliance on the supervised pre-processing toolkits for features, they can alleviate the error Table 4 : In-domain (first column) and Out-of-domain Performance (columns two to four).",
  "y": "differences"
 },
 {
  "id": "59a7c1fffdd45f8e152d060a4b9f50_0",
  "x": "We present results using support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Sch\u00f6lkopf, 2004) for sentence and document translation prediction tasks and Global Linear Models (GLM) (Collins, 2002) with dynamic learning (GLMd) <cite>(Bi\u00e7ici, 2013</cite>; Bi\u00e7ici and Way, 2014) for word-level translation performance prediction. We also use these learning models after a feature subset selection (FS) with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009 ), or PLS after FS (FS+PLS). GLM relies on Viterbi decoding, perceptron learning, and flexible feature definitions.",
  "y": "uses"
 },
 {
  "id": "59a7c1fffdd45f8e152d060a4b9f50_1",
  "x": "We use mean absolute error (MAE), relative absolute error (RAE), root mean squared error (RMSE), and correlation (r) as well as relative MAE (MAER) and relative RAE (MRAER) to evaluate (Bi\u00e7ici, 2015;<cite> Bi\u00e7ici, 2013)</cite> . MAER is mean absolute error relative to the magnitude of the target and MRAER is mean absolute error relative to the absolute error of a predictor always predicting the target mean assuming that target mean is known (Bi\u00e7ici, 2015 2012) calculates the average quality difference between the top n\u22121 quartiles and the overall quality for the test set. Table 2 presents the training results for Task 1 and Task 3.",
  "y": "uses"
 },
 {
  "id": "59a7c1fffdd45f8e152d060a4b9f50_2",
  "x": "We develop individual RTM models for each subtask and use GLMd model <cite>(Bi\u00e7ici, 2013</cite>; Bi\u00e7ici and Way, 2014) , for predicting the quality at the word-level. The results on the test set are in Table 5 where the ranks are out of about 17 submissions. RTMs with GLMd becomes the second best system this task.",
  "y": "uses"
 },
 {
  "id": "59a7c1fffdd45f8e152d060a4b9f50_3",
  "x": "In Table 6 , we list the RTM test results for tasks and subtasks that predict HTER or METEOR from QET15, QET14 (Bi\u00e7ici and Way, 2014) , and QET13<cite> (Bi\u00e7ici, 2013)</cite> . The best results when predicting HTER are obtained this year. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "59a7c1fffdd45f8e152d060a4b9f50_4",
  "x": [
   "Referential translation machines achieve top performance in automatic, accurate, and language independent prediction of document-, sentence-, and word-level statistical machine translation (SMT) performance. RTMs remove the need to access any SMT system specific information or prior knowledge of the training data or models used when generating the translations. RTMs achieve top performance when predicting translation performance."
  ],
  "y": "uses"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_0",
  "x": "A record of earlier decisions could also help users to identify outstanding issues for discussion, and to therefore make better use of the remainder of the meeting. Our approach to decision detection uses an annotation scheme which distinguishes between different types of utterance based on the roles which they play in the decision-making process. Such a scheme facilitates the detection of decision discussions<cite> (Fern\u00e1ndez et al., 2008)</cite> , and by indicating which utterances contain particular types of information, it also aids their summarization.",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_1",
  "x": "Only very recent research has specifically investigated the automatic detection of decisions, namely (Hsueh and Moore, 2007) and<cite> (Fern\u00e1ndez et al., 2008)</cite> . Hsueh and Moore (2007) used the AMI Meeting Corpus, and attempted to automatically identify dialogue acts (DAs) in meeting transcripts which are \"decision-related\". Within any meeting, the authors decided which DAs were decision-related based on two different kinds of manually created summary: the first was an extractive summary of the whole meeting, and the second, an abstractive summary of the decisions which were made.",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_2",
  "x": "Unlike Hsueh and Moore (2007),<cite> Fern\u00e1ndez et al. (2008)</cite> made an attempt at modelling the structure of decision-making dialogue. They designed an annotation scheme that takes account of the different roles which different utterances play in the decision-making process -for example, their scheme distinguishes between decision DAs (DDAs) which initiate a discussion by raising a topic/issue, those which propose a resolution, and those which express agreement for a proposed resolution and cause it to be accepted as a decision. The authors applied the annotation scheme to a portion of the AMI corpus, and then took what they refer to as a hierarchical classification approach in order to automatically identify decision discussions and their component DAs. Here, one binary Support Vector Machine (SVM) per DDA class hypothesized occurrences of that DDA class, and then based on the hypotheses of these socalled sub-classifiers, a super-classifier, (a further SVM), determined which regions of dialogue represented decision discussions. This approach produced better results than the kind of \"flat classification\" approach pursued by Hsueh and Moore (2007) where a single classifier looks for examples of a single decision-related DA class. Using manual transcripts, and a variety of lexical, utterance, speaker, DA and prosodic features for the sub-classifiers, the super-classifier's F1-score was 0.58 according to a lenient match metric.",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_3",
  "x": "Using manual transcripts, and a variety of lexical, utterance, speaker, DA and prosodic features for the sub-classifiers, the super-classifier's F1-score was 0.58 according to a lenient match metric. Note that (Purver et al., 2007) had previously pursued the same basic approach as<cite> Fern\u00e1ndez et al. (2008)</cite> in order to detect action items. While both Hsueh and Moore (2007), and<cite> Fern\u00e1ndez et al. (2008)</cite> attempted off-line decision detection, in this paper, we attempt real-time decision detection.",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_4",
  "x": "While both Hsueh and Moore (2007), and<cite> Fern\u00e1ndez et al. (2008)</cite> attempted off-line decision detection, in this paper, we attempt real-time decision detection. We take the same basic approach as<cite> Fern\u00e1ndez et al. (2008)</cite> , and make changes to its implementation so that it can work effectively in real-time. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_5",
  "x": "Note that (Purver et al., 2007) had previously pursued the same basic approach as<cite> Fern\u00e1ndez et al. (2008)</cite> in order to detect action items. While both Hsueh and Moore (2007), and<cite> Fern\u00e1ndez et al. (2008)</cite> attempted off-line decision detection, in this paper, we attempt real-time decision detection. We take the same basic approach as<cite> Fern\u00e1ndez et al. (2008)</cite> , and make changes to its implementation so that it can work effectively in real-time.",
  "y": "differences"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_6",
  "x": "**MODELLING DECISION DISCUSSIONS** We use the same annotation scheme as<cite> (Fern\u00e1ndez et al., 2008</cite> ) in order to model decision-making dialogue. As stated in Section 2, this scheme distinguishes between a small number of dialogue act types based on the role which they perform in the formulation of a decision.",
  "y": "uses"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_7",
  "x": "(2007) are part of the AMI corpus, and are for the manual transcriptions. The reader can find a comparison between these annotations and our own manual transcript annotations in<cite> (Fern\u00e1ndez et al., 2008)</cite> . After obtaining the new off-line and real-time ASR transcripts, we transferred the DDA annotations from the manual transcripts.",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_9",
  "x": "For 1 and 2, we use the same lenient-match metric as<cite> (Fern\u00e1ndez et al., 2008</cite>; Hsueh and Moore, 2007) , which allows a margin of 20 seconds preceding and following a hypothesized DDA. Note that here we only give credit for hypotheses based on a 1-1 mapping with the gold-standard labels. For 3, we follow<cite> (Fern\u00e1ndez et al., 2008</cite>; Purver et al., 2007) and use a windowed metric that divides the dialogue into 30-second windows and evaluates on a per window basis.",
  "y": "uses"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_10",
  "x": "Note that here we only give credit for hypotheses based on a 1-1 mapping with the gold-standard labels. For 3, we follow<cite> (Fern\u00e1ndez et al., 2008</cite>; Purver et al., 2007) and use a windowed metric that divides the dialogue into 30-second windows and evaluates on a per window basis. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_11",
  "x": "Combining the output from each of the independent sub-classifiers might compensate somewhat for any decreases in their individual accuracy, as there was here for the I and RP sub-classifiers. The hierarchical real-time detector's F1-score is also 10 points higher than a flat classifier (.54 vs. .44). Hence, while<cite> Fern\u00e1ndez et al. (2008)</cite> demonstrated that the hierarchical classification approach could improve off-line decision detection, we have demonstrated here that it can also improve realtime decision detection.",
  "y": "differences"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_12",
  "x": "7 Conclusion<cite> (Fern\u00e1ndez et al., 2008)</cite> described an approach to decision detection in multi-party meetings and demonstrated how it could work relatively well in an off-line system. The approach has two defining characteristics. The first is its use of an annotation scheme which distinguishes between different utterance types based on the roles which they play in the decision-making process. The second is its use of hierarchical classification, whereby binary sub-classifiers detect instances of each of the decision DAs (DDAs), and then based on the sub-classifier hypotheses, a super-classifier determines which regions of dialogue are decision discussions.",
  "y": "background"
 },
 {
  "id": "5b17eb75600820a80b3573bf74c427_13",
  "x": "In this paper then, we have taken the same basic approach to decision detection as<cite> Fern\u00e1ndez et al. (2008)</cite> , but changed the way in which it is implemented so that it can work effectively in realtime. Our implementation changes include running the detector at regular and frequent intervals during the meeting, and reprocessing recent utterances in case a decision discussion straddles these and brand new utterances. The fact that the detector reprocesses utterances means that on consecutive runs, overlapping and duplicate hypothesized decision discussions are possible. We have therefore added facilities to merge overlapping hypotheses and to remove duplicates.",
  "y": "differences background"
 },
 {
  "id": "5c63296c36cbd95e07f05f2563a2a1_0",
  "x": "In the past several years, sequential neural models such as long-short term memory (LSTM) have been applied to NER. They have outperformed the conventional models (Huang et al., 2015) . Recently, Convolutional Neural Network (CNN) was introduced into many models for extracting sub-word information from a word (Santos and Guimaraes, 2015;<cite> Ma and Hovy, 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "5c63296c36cbd95e07f05f2563a2a1_1",
  "x": "Recently, new approaches introducing CNN or LSTM for extracting subword information from character inputs have been found to outperform other models (Lample et al., 2016) . Rei et al. (2016) proposed the model using an attention mechanism whose inputs are words and characters. Above all, BLSTM-CNNs-CRF<cite> (Ma and Hovy, 2016</cite> ) achieved state-of-theart performance on the standard English corpus: CoNLL2003 (Tjong Kim Sang and De Meulder, 2003) .",
  "y": "background"
 },
 {
  "id": "5c63296c36cbd95e07f05f2563a2a1_2",
  "x": "In this study, we specifically examine BLSTMCNNs-CRF<cite> (Ma and Hovy, 2016)</cite> because it achieves state-of-the-art performance in the CoNLL 2003 corpus. Figure 1 presents the architecture of this model. This word-based model combines CNN, BLSTM, and CRF layers.",
  "y": "motivation"
 },
 {
  "id": "5c63296c36cbd95e07f05f2563a2a1_3",
  "x": "In this study, we specifically examine BLSTMCNNs-CRF<cite> (Ma and Hovy, 2016)</cite> because it achieves state-of-the-art performance in the CoNLL 2003 corpus. Figure 1 presents the architecture of this model. This word-based model combines CNN, BLSTM, and CRF layers. We describe each layer of this model as the following. CNN Layer: This layer is aimed at extracting subword information. The inputs are character embeddings of a word. This layer consists of convolution and pooling layers. The convolution layer produces a matrix for a word with consideration of the sub-word. The pooling layer compresses the matrix for each dimension of character embedding. BLSTM Layer: BLSTM (Graves and Schmidhuber, 2005 ) is an approach to treat sequential data. The output of CNN and word embedding are concatenated as an input of BLSTM. CRF Layer: This layer was designed to select the best tag sequence from all possible tag sequences with consideration of outputs from BLSTM and correlations between adjacent tags. This layer introduces a transition score for each transition pattern between adjacent tags. The objective function is calculated using the sum of the outputs from BLSTM and the transition scores for a sequence.",
  "y": "background"
 },
 {
  "id": "5c63296c36cbd95e07f05f2563a2a1_4",
  "x": "Other conditions are the same as those reported for an earlier study<cite> (Ma and Hovy, 2016)</cite> . Table 2 : F1 score of each models. Average is a weighted average.",
  "y": "uses"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_0",
  "x": "Fine-grained opinion mining aims to detect structured user opinions in text, which has drawn much attention in the natural language processing (NLP) community <cite>(Kim and Hovy, 2006</cite>; Breck et al., 2007; Ruppenhofer et al., 2008; Wilson et al., 2009; Qiu et al., 2011; Cardie, 2013, 2014; Liu et al., 2015; Wiegand et al., 2016) . A structured opinion includes the key arguments of one opinion, such as expressions, holders and targets (Breck et al., 2007; Cardie, 2012, 2013; Katiyar and Cardie, 2016 ). Here we focus on opinion role labeling (ORL) (Marasovi\u0107 and Frank, 2018) , which identifies opinion holders and * Corresponding author.",
  "y": "background"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_1",
  "x": "Earlier work attempts to exploit a well-trained SRL model to recognize possible semantic roles for a given opinion expression, and then map the semantic roles into opinion roles <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008) . The heuristic approach is unable to obtain high performance for ORL because there are large mismatches between SRL and ORL. For example, opinion expressions are different from verb/noun predicates in SRL, and meanwhile, opinion holders and targets may not always correspond to semantic agents (ARG0) and patients (ARG1), respectively.",
  "y": "background"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_2",
  "x": "Earlier work attempts to exploit a well-trained SRL model to recognize possible semantic roles for a given opinion expression, and then map the semantic roles into opinion roles <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008) . The heuristic approach is unable to obtain high performance for ORL because there are large mismatches between SRL and ORL. We can exploit machine learning based method to solve the mismatching problem between ORL and SRL.",
  "y": "motivation"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_3",
  "x": "Experiments are conducted on the MPQA 2.0 dataset, which is a standard benchmark for opinion mining. Results show that SRL is highly effective for ORL, which is consistent with previous findings <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008; Marasovi\u0107 and Frank, 2018) . Meanwhile, our implicit SRL-SAWR method can achieve the best ORL performance, 2.23% higher F-scores than the second best method.",
  "y": "differences"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_4",
  "x": "The tendencies are similar by exploiting the binary and proportional matching methods. The results show that SRL information is very helpful for ORL, which is consistent with previous studies <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008; Marasovi\u0107 and Frank, 2018) . The implicit SRL-SAWR method is highly effective to integrate SRL information into the ORL model.",
  "y": "similarities"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_5",
  "x": "Experiments are conducted on the MPQA 2.0 dataset, which is a standard benchmark for opinion mining. Results show that SRL is highly effective for ORL, which is consistent with previous findings <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008; Marasovi\u0107 and Frank, 2018) . Meanwhile, our implicit SRL-SAWR method can achieve the best ORL performance, 2.23% higher F-scores than the second best method.",
  "y": "similarities uses"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_6",
  "x": "Several works even directly transfer semantic roles into opinion roles for ORL <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008) , treating opinion expressions as the major predicates. These systems can achieve good performances, indicating that SRL information can be greatly useful for ORL. Here we propose a novel method to encode the SRL information implicitly, enhancing ORL model with semantic-aware word representations from a neural SRL model (SRL-SAWR).",
  "y": "background"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_7",
  "x": "Several works even directly transfer semantic roles into opinion roles for ORL <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008) , treating opinion expressions as the major predicates. These systems can achieve good performances, indicating that SRL information can be greatly useful for ORL. Here we propose a novel method to encode the SRL information implicitly, enhancing ORL model with semantic-aware word representations from a neural SRL model (SRL-SAWR).",
  "y": "extends"
 },
 {
  "id": "5cf0215cd20c86f329c8debc0daeb8_8",
  "x": "According to the above findings, we design a simple system by mapping SRL outputs into ORL directly <cite>(Kim and Hovy, 2006</cite>; Ruppenhofer et al., 2008) . We simply convert the semantic role ARG0 into holder, and ARG1 into target. Table 2 shows the performance.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_0",
  "x": "The central aspects of our discussion are (a) three dependency formats: two 'classic' representations for dependency parsing, namely, Stanford Basic (SB) and CoNLL Syntactic Dependencies (CD), and bilexical dependencies from the HPSG English Resource Grammar (ERG), so-called DELPH-IN Syntactic Derivation Tree (DT), proposed recently by Ivanova et al. (2012) ; (b) three state-of-the art statistical parsers: Malt (Nivre et al., 2007) , MST (McDonald et al., 2005) and the parser of <cite>Bohnet and Nivre (2012)</cite> ; (c) two approaches to wordcategory disambiguation, e.g. exploiting common PTB tags and using supertags (i.e. specialized ERG lexical types). We parse the formats and compare accuracies in all configurations in order to determine how parsers, dependency representations and grammatical tagging methods interact with each other in application to automatic syntactic analysis. SB and CD are derived automatically from phrase structures of Penn Treebank to accommodate the needs of fast and accurate dependency parsing, whereas DT is rooted in the formal grammar theory HPSG and is independent from any specific treebank.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_2",
  "x": "In the experiments described in Section 4 we used parsers that adopt different approaches and implement various algorithms. <cite>Bohnet and Nivre (2012)</cite> parser: transitionbased dependency parser with joint tagger that implements global learning and beam search.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_3",
  "x": "---------------------------------- **EXPERIMENTS** In this section we give a detailed analysis of parsing into SB, CD and DT dependencies with Malt, MST and the <cite>Bohnet and Nivre (2012)</cite> parser.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_4",
  "x": "---------------------------------- **SETUP** For Malt and MST we perform the experiments on gold PoS tags, whereas the <cite>Bohnet and Nivre (2012)</cite> parser predicts PoS tags during testing.",
  "y": "differences"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_5",
  "x": "**SETUP** For Malt and MST we perform the experiments on gold PoS tags, whereas the <cite>Bohnet and Nivre (2012)</cite> parser predicts PoS tags during testing. Prior to each experiment with Malt, we used MaltOptimizer to obtain settings and a feature model; for MST we exploited default configuration; for the <cite>Bohnet and Nivre (2012)</cite> parser we set the beam parameter to 80 and otherwise employed the default setup.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_6",
  "x": "From the parser perspective Malt and MST are not very different in the traditional setup with gold PTB tags (Table 1, Gold PTB tags). The <cite>Bohnet and Nivre (2012)</cite> parser outperforms Malt on CD and DT and MST on SB, CD and DT with PTB tags even though it does not receive gold PTB tags during test phase but predicts them (Table 2 , Predicted PTB tags). This is explained by the fact that the <cite>Bohnet and Nivre (2012)</cite> parser implements a novel approach to parsing: beam-search algorithm with global structure learning.",
  "y": "background"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_7",
  "x": "From the parser perspective Malt and MST are not very different in the traditional setup with gold PTB tags (Table 1, Gold PTB tags). The <cite>Bohnet and Nivre (2012)</cite> parser outperforms Malt on CD and DT and MST on SB, CD and DT with PTB tags even though it does not receive gold PTB tags during test phase but predicts them (Table 2 , Predicted PTB tags). This is explained by the fact that the <cite>Bohnet and Nivre (2012)</cite> parser implements a novel approach to parsing: beam-search algorithm with global structure learning.",
  "y": "differences"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_8",
  "x": "For the <cite>Bohnet and Nivre (2012)</cite> parser the complexity of supertag prediction has significant negative influence on the attachment and labeling accuracies ( Table 2 , Predicted supertags). The addition of gold PTB tags as a feature lifts the performance of the <cite>Bohnet and Nivre (2012)</cite> parser to the level of performance of Malt and MST on CD with gold supertags and Malt on SB with gold supertags (compare Table 2 , Predicted supertags + gold PTB, and Table 1 , Gold supertags). Both Malt and MST benefit slightly from the combination of gold PTB tags and gold supertags (Table 1 , Gold PTB tags + gold supertags).",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_9",
  "x": "For the <cite>Bohnet and Nivre (2012)</cite> parser the complexity of supertag prediction has significant negative influence on the attachment and labeling accuracies ( Table 2 , Predicted supertags). The addition of gold PTB tags as a feature lifts the performance of the <cite>Bohnet and Nivre (2012)</cite> parser to the level of performance of Malt and MST on CD with gold supertags and Malt on SB with gold supertags (compare Table 2 , Predicted supertags + gold PTB, and Table 1 , Gold supertags). Both Malt and MST benefit slightly from the combination of gold PTB tags and gold supertags (Table 1 , Gold PTB tags + gold supertags).",
  "y": "extends"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_10",
  "x": "For the <cite>Bohnet and Nivre (2012)</cite> parser we also observe small rise of accuracy when gold supertags are provided as a feature for prediction of PTB tags (compare Predicted PTB tags and Predicted PTB tags + gold supertags sections of Table 2 ). The parsers have different running times: it takes minutes to run an experiment with Malt, about 2 hours with MST and up to a day with the <cite>Bohnet and Nivre (2012)</cite> parser. From the point of view of the dependency format, SB has the highest LACC and CD is first-rate on UAS for all three parsers in most of the configurations (Tables 1 and 2 ).",
  "y": "extends"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_11",
  "x": "For the <cite>Bohnet and Nivre (2012)</cite> parser we also observe small rise of accuracy when gold supertags are provided as a feature for prediction of PTB tags (compare Predicted PTB tags and Predicted PTB tags + gold supertags sections of Table 2 ). The parsers have different running times: it takes minutes to run an experiment with Malt, about 2 hours with MST and up to a day with the <cite>Bohnet and Nivre (2012)</cite> parser. From the point of view of the dependency format, SB has the highest LACC and CD is first-rate on UAS for all three parsers in most of the configurations (Tables 1 and 2 ).",
  "y": "differences"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_12",
  "x": "As discussed in section 3.4, they contain bits of information that are different. For this reason their combination results in slight increase of accuracy for all three parsers on all dependency formats (Table 1 , Gold PTB tags + gold supertags, and Table 2 , Predicted PTB + gold supertags and Predicted supertags + gold PTB). The <cite>Bohnet and Nivre (2012)</cite> parser predicts supertags with an average accuracy of 89.73% which is significantly lower than state-ofthe-art 95% (Ytrest\u00f8l, 2011) .",
  "y": "differences"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_13",
  "x": "VBP, VBZ and VBG. VBP (verb, non-3rd person singular present), VBZ (verb, 3rd person singular present) and VBG (verb, gerund or present participle) are the PTB tags that have error rates in 10 highest error rates list for each parser (Malt, MST and the <cite>Bohnet and Nivre (2012)</cite> parser) with each dependency format (SB, CD and DT) and with each PoS tag set (PTB PoS and supertags) when PTB tags are included as CPOSTAG feature. We automatically collected all sentences that contain 1) attachment errors, 2) label errors, 3) attachment and label errors for VBP, VBZ and VBG made by Malt parser on DT format with PTB PoS. For each of these three lexical categories we manually analyzed a random sample of sentences with errors and their corresponding gold-standard versions.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_14",
  "x": "Coordination. The error rate of Malt, MST and the <cite>Bohnet and Nivre (2012)</cite> parser for the coordination is not so high for SB and CD ( 1% and 2% correspondingly with MaltParser, PTB tags) whereas for DT the error rate on the CPOSTAGS is especially high (26% with MaltParser, PTB tags). It means that there are many errors on incoming dependency arcs for coordinating conjunctions when parsing DT.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_15",
  "x": "The <cite>Bohnet and Nivre (2012)</cite> parser succeeds in finding the correct conjucts (shown in bold font) on DT and makes mistakes on SB and CD in some difficult cases like the following ones: a) <. . . > investors hoard gold and help underpin its price <. . . > b) Then take the expected return and subtract one standard deviation.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_16",
  "x": "---------------------------------- **CONCLUSIONS AND FUTURE WORK** In this survey we gave a comparative experimental overview of (i) parsing three dependency schemes, viz., Stanford Basic (SB), CoNLL Syntactic Dependencies (CD) and DELPH-IN Syntactic Derivation Tree (DT), (ii) with three leading dependency parsers, viz., Malt, MST and the <cite>Bohnet and Nivre (2012)</cite> parser (iii) exploiting two different tagsets, viz., PTB tags and supertags.",
  "y": "uses"
 },
 {
  "id": "5d3c08596677a1f8ac48fa17766bb4_17",
  "x": "In this survey we gave a comparative experimental overview of (i) parsing three dependency schemes, viz., Stanford Basic (SB), CoNLL Syntactic Dependencies (CD) and DELPH-IN Syntactic Derivation Tree (DT), (ii) with three leading dependency parsers, viz., Malt, MST and the <cite>Bohnet and Nivre (2012)</cite> parser (iii) exploiting two different tagsets, viz., PTB tags and supertags. From the parser perspective, the <cite>Bohnet and Nivre (2012)</cite> parser performs better than Malt and MST not only on conventional formats but also on the new representation, although this parser solves a harder task than Malt and MST. From the dependency format perspective, DT appeares to be a more difficult target dependency representation than SB and CD.",
  "y": "differences"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_0",
  "x": "To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; <cite>Hoffmann et al., 2011</cite>; Surdeanu et al., 2012) exploited multi-instance learning models. Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it (Sun et al., 2011; Wang et al., 2011; Takamatsu et al., 2012) . However, their methods are handicapped by the built-in assumption that a sentence does not express a relation unless it mentions two entities which participate in the relation in the knowledge base, leading to false negatives.",
  "y": "background"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_1",
  "x": "To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; <cite>Hoffmann et al., 2011</cite>; Surdeanu et al., 2012) exploited multi-instance learning models. Only a few studies have directly examined the influence of the quality of the training data and attempted to enhance it (Sun et al., 2011; Wang et al., 2011; Takamatsu et al., 2012) . However, their methods are handicapped by the built-in assumption that a sentence does not express a relation unless it mentions two entities which participate in the relation in the knowledge base, leading to false negatives.",
  "y": "motivation"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_2",
  "x": "We further note that iterative bootstrapping over a single distant supervision system is difficult, because state-of-the-art systems (Surdeanu et al., 2012; <cite>Hoffmann et al., 2011</cite>; Riedel et al., 2010; Mintz et al., 2009) , detect only few false negatives in the training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009) . We present a reliable and novel way to address these issues and achieve significant improvement over the <cite>MULTIR</cite> system (<cite>Hoffmann et al., 2011</cite>) , increasing recall from 47.7% to 61.2% at comparable precision. The key to this success is the combination of two different views as in co-training (Blum and Mitchell, 1998) : an information extraction technique with fine features for high precision and an information retrieval technique with coarse features for high recall.",
  "y": "background motivation"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_3",
  "x": "Similar to iterative bootstrapping techniques (Yangarber, 2001) , this mechanism uses the outputs of the first trained model to expand training data for the second model, but unlike bootstrapping it does not require iteration and avoids the problem of semantic drift. We further note that iterative bootstrapping over a single distant supervision system is difficult, because state-of-the-art systems (Surdeanu et al., 2012; <cite>Hoffmann et al., 2011</cite>; Riedel et al., 2010; Mintz et al., 2009) , detect only few false negatives in the training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009) . We present a reliable and novel way to address these issues and achieve significant improvement over the <cite>MULTIR</cite> system (<cite>Hoffmann et al., 2011</cite>) , increasing recall from 47.7% to 61.2% at comparable precision.",
  "y": "extends"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_4",
  "x": "While relation extraction systems exploit rich and complex features that are necessary to extract the exact relation (Mintz et al., 2009; Riedel et al., 2010; <cite>Hoffmann et al., 2011</cite>) , passage retrieval components use coarse features in order to provide different and complementary feedback to information extraction models. We exploit two types of lexical features: BagOf-Words and Word-Position. The two types of simple binary features are shown in the following example:",
  "y": "background"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_5",
  "x": "We extend the learning-to-rank techniques (Liu, 2011) to distant supervision setting to create a robust passage retrieval system. While relation extraction systems exploit rich and complex features that are necessary to extract the exact relation (Mintz et al., 2009; Riedel et al., 2010; <cite>Hoffmann et al., 2011</cite>) , passage retrieval components use coarse features in order to provide different and complementary feedback to information extraction models. We exploit two types of lexical features: BagOf-Words and Word-Position.",
  "y": "extends"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_6",
  "x": "---------------------------------- **DISTANTLY SUPERVISED RELATION EXTRACTION** We use a state-of-the-art open-source system, <cite>MULTIR</cite> (<cite>Hoffmann et al., 2011</cite>) , as the relation extraction component.",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_7",
  "x": "We use a state-of-the-art open-source system, <cite>MULTIR</cite> (<cite>Hoffmann et al., 2011</cite>) , as the relation extraction component. <cite>MULTIR</cite> is based on multi-instance learning, which assumes that at least one sentence of those matching a given entity-pair contains the relation of interest (Riedel et al., 2010) in the given knowledge base to tolerate false positive noise in the training data and superior than previous models (Riedel et al., 2010; Mintz et al., 2009 ) by allowing overlapping relations. <cite>MULTIR</cite> uses features which are based on Mintz et al. (2009) and consist of conjunctions of named entity tags, syntactic dependency paths between arguments, and lexical information.",
  "y": "background"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_8",
  "x": "---------------------------------- **EXPERIMENTS** For evaluating extraction accuracy, we follow the experimental setup of <cite>Hoffmann et al. (2011)</cite> , and use <cite>their</cite> implementation of <cite>MULTIR 4</cite> with 50 training iterations as our baseline.",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_9",
  "x": "**EXPERIMENTS** For evaluating extraction accuracy, we follow the experimental setup of <cite>Hoffmann et al. (2011)</cite> , and use <cite>their</cite> implementation of <cite>MULTIR 4</cite> with 50 training iterations as our baseline. Our complete system, which we call IRMIE, combines our passage retrieval component with <cite>MULTIR</cite>.",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_10",
  "x": "We use the same datasets as in <cite>Hoffmann et al. (2011)</cite> and Riedel et al. (2010) , which include 3-years of New York Times articles aligned with Freebase. The sentential extraction evaluation is performed on a small amount of manually annotated sentences, sampled from the union of matched sentences and Table 1 : Overall sentential extraction performance evaluated on the original test set of <cite>Hoffmann et al. (2011)</cite> and our corrected test set: Our proposed relevance feedback technique yields a substantial increase in recall. system predictions.",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_11",
  "x": "We use the same datasets as in <cite>Hoffmann et al. (2011)</cite> and Riedel et al. (2010) , which include 3-years of New York Times articles aligned with Freebase. The sentential extraction evaluation is performed on a small amount of manually annotated sentences, sampled from the union of matched sentences and Table 1 : Overall sentential extraction performance evaluated on the original test set of <cite>Hoffmann et al. (2011)</cite> and our corrected test set: Our proposed relevance feedback technique yields a substantial increase in recall. system predictions.",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_12",
  "x": "We define S e as the sentences where some system extracted a relation and S F as the sentences that match the arguments of a fact in \u2206. The sentential precision and recall is computed on a randomly sampled set of sentences from S e \u222a S F , in which each sentence is manually labeled whether it expresses any relation in R. Figure 3 shows the precision/recall curves for <cite>MULTIR</cite> with and without pseudo-relevance feedback computed on the test dataset of 1000 sentence used by <cite>Hoffmann et al. (2011)</cite> . With the pseudo-relevance feedback from passage retrieval, IRMIE achieves significantly higher recall at a consistently high level of precision. At the highest recall point, IRMIE reaches 78.5% precision and 59.2% recall, for an F1 score of 68.9%.",
  "y": "uses"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_13",
  "x": "With the pseudo-relevance feedback from passage retrieval, IRMIE achieves significantly higher recall at a consistently high level of precision. At the highest recall point, IRMIE reaches 78.5% precision and 59.2% recall, for an F1 score of 68.9%. Because the two types of lexical features used in our passage retrieval models are not used in <cite>MUL-TIR</cite>, we created another baseline MULTIRLEX by adding these features into <cite>MULTIR</cite> in order to rule out the improvement from additional information.",
  "y": "extends"
 },
 {
  "id": "5e34591c2a7b1664e1275372c40b79_14",
  "x": "Because the two types of lexical features used in our passage retrieval models are not used in <cite>MUL-TIR</cite>, we created another baseline MULTIRLEX by adding these features into <cite>MULTIR</cite> in order to rule out the improvement from additional information. Note that the sentences are sampled from the union of Freebase matches and sentences from which some systems in <cite>Hoffmann et al. (2011)</cite> extracted a relation. It underestimates the improvements of the newly developed systems in this paper.",
  "y": "uses"
 },
 {
  "id": "5e6d5bb4fb5be2b18ce3256302bf28_0",
  "x": "---------------------------------- **INTRODUCTION** Generation of referring expression (GRE) is an important task in the field of Natural Language Generation (NLG) systems (Reiter and <cite>Dale, 1995)</cite> .",
  "y": "background"
 },
 {
  "id": "5e6d5bb4fb5be2b18ce3256302bf28_1",
  "x": "Grice, an eminent philosopher of language, has stressed on brevity of referential communication to avoid conversational implicature. Dale (1992) developed Full Brevity algorithm based on this observation. It always generates shortest possible referring description to identify an object. But Reiter and<cite> Dale (1995)</cite> later proved that Full Brevity requirement is an NP-Hard task, thus computationally intractable and offered an alternative polynomial time Incremental Algorithm.",
  "y": "background"
 },
 {
  "id": "5e6d5bb4fb5be2b18ce3256302bf28_2",
  "x": "**MODELING GRE USING TRIE STRUCTURE** In this section, it is shown how a scene can be represented using a trie data structure. The scheme is based on Incremental algorithm<cite> (Reiter and Dale 1995)</cite> and incorporates the attractive properties (e.g. speed, simplicity etc) of that algorithm.",
  "y": "uses"
 },
 {
  "id": "5e6d5bb4fb5be2b18ce3256302bf28_3",
  "x": "Reiter and<cite> Dale (1995)</cite> pointed out the notion of 'PreferredAttributes' (e.g. Type, Size, Color etc) which is a sequence of attributes of an object that human speakers generally use to identify that object from the contrast set. We assume that the initial description of an entity is following this sequence (e.g. \"The large black dog\") then the later references will be some subset of initial description (like \"The dog\" or \"The large dog\") which is defined as the prefix of the initial description. So, we have to search for a prefix of the initial full length description so that it is adequate to distinguish the target object.",
  "y": "uses"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_0",
  "x": "**INTRODUCTION** Fine-grained entity typing aims to assign types (e.g., \"person\", \"politician\", etc.) to entity mentions in the local context (a single sentence), and the type set constitutes a treestructured hierarchy (i.e., type hierarchy). Recent years witness the boost of neural models in this task, e.g.,<cite> (Shimaoka et al. 2016)</cite> employs an attention based LSTM to attain sentence representations and achieves state-of-the-art performance.",
  "y": "background"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_1",
  "x": "Fine-grained entity typing aims to assign types (e.g., \"person\", \"politician\", etc.) to entity mentions in the local context (a single sentence), and the type set constitutes a treestructured hierarchy (i.e., type hierarchy). Recent years witness the boost of neural models in this task, e.g.,<cite> (Shimaoka et al. 2016)</cite> employs an attention based LSTM to attain sentence representations and achieves state-of-the-art performance. However, it still suffers from noise in training data, which is a main challenge in this task.",
  "y": "motivation background"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_2",
  "x": "Firstly PAN employs LSTM to generate representations of sentences s i following<cite> (Shimaoka et al. 2016)</cite> , where s i \u2208 R d is the semantic representation of s i , i \u2208 {1, 2, ..., n}. Afterwards, we build path-based attention \u03b1 i,t over sentences s i for each type t \u2208 T e , which is expected to focus on relevant sentences to type t. Then, the representation of sentence set S e for type t, denoted by s e,t \u2208 R d , is calculated through weighted sum of vectors of sentences. Finally, we obtain predicted types through a classification layer. The architecture of PAN for given entity e, type t More precisely, given e, an attention \u03b1 i,t is learned to score how well sentence s i matches type t, i.e.,",
  "y": "uses"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_3",
  "x": "Experiments are carried on two widely used datasets OntoNotes and FIGER(GOLD), and the training dataset of OntoNotes is noisy compared to FIGER(GOLD)<cite> (Shimaoka et al. 2016)</cite> . The statistics of the datasets are listed in Table1. We employ Strict Accuracy (Acc), Loose Macro F1 (Ma-F1), and Loose Micro F1 (Mi-F1) as evaluation measures following <cite>(Shimaoka et al. 2016</cite> ).",
  "y": "differences"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_4",
  "x": "We employ Strict Accuracy (Acc), Loose Macro F1 (Ma-F1), and Loose Micro F1 (Mi-F1) as evaluation measures following <cite>(Shimaoka et al. 2016</cite> ). Specifically, \"Strict\" evaluates on the type set of each entity mention, while \"Loose\" on each type. \"Marco\" is the geometric average over all mentions, while \"Micro\" is the arithmetic average. The baselines are chosen from two aspects: (1) Predicting types in a unified process using raw noisy data, i.e., TLSTM<cite> (Shimaoka et al. 2016)</cite> , and other methods shown in Table2.",
  "y": "uses"
 },
 {
  "id": "5eb321d3c63642a4b148e1276eab20_5",
  "x": "We employ Strict Accuracy (Acc), Loose Macro F1 (Ma-F1), and Loose Micro F1 (Mi-F1) as evaluation measures following <cite>(Shimaoka et al. 2016</cite> ). Specifically, \"Strict\" evaluates on the type set of each entity mention, while \"Loose\" on each type. \"Marco\" is the geometric average over all mentions, while \"Micro\" is the arithmetic average. The baselines are chosen from two aspects: (1) Predicting types in a unified process using raw noisy data, i.e., TLSTM<cite> (Shimaoka et al. 2016)</cite> , and other methods shown in Table2.",
  "y": "uses"
 },
 {
  "id": "5f25b6a3bcaca2e4beb59ce0f3eb5f_0",
  "x": "SUDOKU's submissions to SemEval Task 13 treats Word Sense Disambiguation and Entity Linking as a deterministic problem that exploits two key attributes of open-class words as constraints -their degree of polysemy and their part of speech. This is an extension and further validation of the results achieved by <cite>Manion and Sainudiin (2014)</cite>. SUDOKU's three submissions are incremental in the use of the two aforementioned constraints.",
  "y": "extends"
 },
 {
  "id": "5f25b6a3bcaca2e4beb59ce0f3eb5f_2",
  "x": "For a more formal distinction between the conventional and iterative approach to WSD, please refer to this paper<cite> (Manion and Sainudiin, 2014</cite> Table 1 : Parts of Speech disambiguated (as percentages) for each SemEval Task (denoted by its year). In-Degree Centrality as implemented in<cite> (Manion and Sainudiin, 2014)</cite> observes F-Score improvement (F + \u2206F) by applying the iterative approach. The author found in the investigations of his thesis (Manion, 2014) that the iterative approach performed best on the SemEval 2013 Multilingual WSD Task (Navigli et al., 2013) , as opposed to earlier tasks such as SensEval 2004 English All Words WSD Task (Snyder and Palmer, 2004) and the SemEval 2010 All Words WSD task on a Specific Domain (Agirre et al., 2010) .",
  "y": "background"
 },
 {
  "id": "5f25b6a3bcaca2e4beb59ce0f3eb5f_3",
  "x": "Lastly the difference in F-Score between the conventional Run1 and the iterative Run2 and Run3 is listed beside each distribution. Firstly WSD tasks before 2013 generally relied on only a lexicon, such as WordNet (Fellbaum, 1998) or an alternative equivalent, whereas SemEval 2013 Task 12 WSD and this task (Moro and Navigli, 2015) included Entity Linking (EL) using the encyclopaedia Wikipedia via BabelNet (Navigli and Ponzetto, 2012) . Secondly, as shown by <cite>Manion and Sainudiin (2014)</cite> with a simple linear regression, the iterative approach increases WSD performance for documents that have a higher degree of document monosemy -the percentage of unique monosemous lemmas in a document.",
  "y": "background"
 },
 {
  "id": "5f25b6a3bcaca2e4beb59ce0f3eb5f_4",
  "x": "Formalised in<cite> (Manion and Sainudiin, 2014)</cite> , this run can act as a baseline to gauge any improvement for Run2 and Run3 that apply the iterative approach. Run2 (SUDOKU-2) has the constraint of words being disambiguated in order of increasing polysemy, leaving the most polysemous to last. Run3 (SUDOKU-3) is an untested and unpublished version of the iterative approach.",
  "y": "background"
 },
 {
  "id": "5f2f4087b80aa8dc3a5ccdb686983d_0",
  "x": "---------------------------------- **DATA SET** For our experiments we use the dataset described in<cite> (DeVault et al., 2011b)</cite> .",
  "y": "uses"
 },
 {
  "id": "5f2f4087b80aa8dc3a5ccdb686983d_2",
  "x": "We can use the weak accuracy of one referee, measured against all the others, to establish a performance ceiling for this metric. This score is .79; see <cite>DeVault et al. (2011b)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "5f5a59f8fbf999b9eecfe7c1897b2c_0",
  "x": "**INTRODUCTION** Several ensemble models have been proposed for the parsing of syntactic dependencies. These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008; Attardi and Dell'Orletta, 2009) , and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; <cite>Hall et al., 2007</cite>; Attardi and Dell'Orletta, 2009 ).",
  "y": "background"
 },
 {
  "id": "5f5a59f8fbf999b9eecfe7c1897b2c_1",
  "x": "parser variants are built by varying the parsing algorithm (we used three parsing models: Nivre's arceager (AE), Nivre's arc-standard (AS), and Covington's non-projective model (CN)), and the parsing direction (left to right (\u2192) or right to left (\u2190)), similar to<cite> (Hall et al., 2007)</cite> . The parameters of the Malt models were set to the values reported in<cite> (Hall et al., 2007)</cite> . The MST parser was used with the default configuration.",
  "y": "similarities"
 },
 {
  "id": "5f5a59f8fbf999b9eecfe7c1897b2c_2",
  "x": "The combined trees are assembled using a word-by-word voting scheme. parser variants are built by varying the parsing algorithm (we used three parsing models: Nivre's arceager (AE), Nivre's arc-standard (AS), and Covington's non-projective model (CN)), and the parsing direction (left to right (\u2192) or right to left (\u2190)), similar to<cite> (Hall et al., 2007)</cite> . The parameters of the Malt models were set to the values reported in<cite> (Hall et al., 2007)</cite> .",
  "y": "uses"
 },
 {
  "id": "5f5a59f8fbf999b9eecfe7c1897b2c_3",
  "x": "**ON RE-PARSING ALGORITHMS** To guarantee that the resulting dependency tree is well-formed, most previous work used the dynamic programming algorithm of Eisner (1996) for reparsing (Sagae and Lavie, 2006;<cite> Hall et al., 2007)</cite> . 6 However, it is not clear that this step is necessary.",
  "y": "background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_0",
  "x": "We show that this simple method can capture features of lexical humor effectively for continuous humor recognition. In particular, we achieve a distance of 0.887 on a global humor ranking task, comparable to the top performing systems from SemEval 2017 Task 6B (Potash et al., 2017) but without the need for any external training corpus. In addition, we further show that this approach is also beneficial for small sample humor recognition tasks through a semi-supervised label propagation procedure, which achieves about 0.7 accuracy on the 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun of the Day<cite> (Yang et al., 2015)</cite> humour classification datasets using only 10% of known labels.",
  "y": "motivation"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_1",
  "x": "**INTRODUCTION** Recognizing humor automatically is an important step for natural human-computer interaction (Shahaf et al., 2015) . While early works tend to frame humor recognition as a binary classification task (Mihalcea and Strapparava, 2005; <cite>Yang et al., 2015)</cite> , the last few years have seen the emergence of humor recognition as a pairwise relative ranking task (Cattle and Ma, 2016; Shahaf et al., 2015) .",
  "y": "background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_2",
  "x": "\u2020 E. Papalexakis was supported by a UCR-China collaboration grant by the Bourns College of Engineering at UCR, and by the National Science Foundation CDSE Grant no. OAC-1808591 global rankings using a series of pairwise comparisons (Potash et al., 2017) . Only Yan and Pedersen (2017) attempt to predict global rankings directly, ranking documents inversely to their probability according to an n-gram language model. State-of-the-art humor recognition algorithms usually require a considerable amount of training data with labels to learn effective features<cite> (Yang et al., 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_3",
  "x": "In addition, by applying a semi-supervised label propagation procedure (Zhou et al., 2003) , we can also use the tensor embedding method for small sample humor recognition, achieving about 0.7 accuracy with only 10% of known labels on the 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun of the Day<cite> (Yang et al., 2015)</cite> datasets. The contributions of this paper are: 1) we propose a tensor embedding method to model the lexical features of documents, which can capture lexical similarity effectively regardless of the size of the corpus, 2) we show that the lexical features can be used effectively for finegrained humor ranking and small sample humor recognition. Our implementation is open-sourced, and can be found at https://github.com/ zhaozj89/TensorEmbeddingNLP.",
  "y": "motivation background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_4",
  "x": "Semantic features range from attempts to measure incongruity (Cattle and Ma, 2018; Shahaf et al., 2015; <cite>Yang et al., 2015)</cite> to the use of word embeddings as inputs to neural models (Bertero and Fung, 2016; Donahue et al., 2017) . Content-based approaches include word frequency (Mihalcea and Strapparava, 2005) , n-gram probability (Yan and Pedersen, 2017) , and lexical centrality (Radev et al., 2015) . Centrality is based on the observation that humorous responses to common stimuli tend to cluster around a small number of core jokes (Radev et al., 2015; Shahaf et al., 2015) , with more central documents benefiting from \"wisdom of the crowd\".",
  "y": "background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_5",
  "x": "Once the humor features have been extracted, the next step is training a machine learning model to make predictions. Although learning-based methods have shown significant performance improvement recently<cite> (Yang et al., 2015)</cite> , one of their main bottlenecks is the lack of appropriate training corpora. While previous works have employed data crawled from websites (Mihalcea and Strapparava, 2005; <cite>Yang et al., 2015)</cite> , Twitter (Cattle and Ma, 2016; Reyes et al., 2012) , sitcom subtitles (Bertero and Fung, 2016; Purandare and Litman, 2006) , or the New Yorker Cartoon Caption Contest (Radev et al., 2015; Shahaf et al., 2015) , these datasets are generally not released publicly.",
  "y": "differences"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_6",
  "x": "The objective of tensor decomposition is to find an approximation\u0174 of W so<cite> Yang et al. (2015)</cite> that:\u0174 where v r \u2208 R V , d r \u2208 R D , R is the predefined rank parameter, and \u2297 is the outer product, namely, v r \u2297 v r \u2297 d r being a three-dimensional tensor, and With the tensor decomposition, we can find low-rank embeddings of sentences that capture the similarity of contextual patterns (Hosseinimotlagh and Papalexakis, 2018).",
  "y": "background"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_7",
  "x": "To show the effectiveness of label propagation of our tensor embedding method for small sample humor recognition, we conduct an experiment on two humor classification datasets 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun of the Day<cite> (Yang et al., 2015)</cite> . Similarly, we run a grid search procedure to find optimal parameters, and set the rank as 10, window size as 5, neighbor number as 50, \u03b1 as 0.2. F (0) is set as a zero matrix initially.",
  "y": "similarities uses"
 },
 {
  "id": "5fc7df69445712a50228d0bf80f30a_8",
  "x": "Our own implementation of<cite> Yang et al. (2015)</cite> is included as a baseline. While<cite> Yang et al. (2015)</cite> uses a large portion of data for training and combine different features, we find that at similar portion of training data (90%), the results of our method are comparable to it. In addition, with only a small portion of training data, our method still achieves good results.",
  "y": "similarities uses"
 },
 {
  "id": "604807137ee5d9a6775821496c6af5_0",
  "x": "The similarity module in SODA identifies the best adaptable source collection based on the similarity between the source and target collections. This is based on the observations from existing literature (Bhatt et al., 2015;<cite> Blitzer et al., 2007)</cite> which suggest that if the source and target collections are similar, the adaptation performance tends to be better than if the two collections are dissimilar. The similarity module in SODA is capable of computing different kinds of lexical, syntactic, and semantic similarities between unlabeled target and labeled source collections.",
  "y": "uses"
 },
 {
  "id": "604807137ee5d9a6775821496c6af5_1",
  "x": "During generalization, it learns shared common representation<cite> (Blitzer et al., 2007</cite>; Ji et al., 2011; Pan et al., 2010) which minimizes the divergence between two collections. We leverage one of the widely used structural correspondence learning (SCL) approach<cite> (Blitzer et al., 2007)</cite> to compute shared representations. The idea adhered here is that a model learned on the shared feature representation using labeled data from the source collection will also generalize well on the target collection.",
  "y": "uses"
 },
 {
  "id": "604807137ee5d9a6775821496c6af5_2",
  "x": "We leverage one of the widely used structural correspondence learning (SCL) approach<cite> (Blitzer et al., 2007)</cite> to compute shared representations. The idea adhered here is that a model learned on the shared feature representation using labeled data from the source collection will also generalize well on the target collection. Towards this, we learn a model (C S ) on the shared feature representation from the source collection, referred to as \"source classifier\".",
  "y": "uses"
 },
 {
  "id": "604807137ee5d9a6775821496c6af5_3",
  "x": "Moreover, SODA consistently performs better than the in-domain classifier with same amount of labeled data. We also evaluated the performance of domain adaptation (DA) module of SODA on the Amazon review dataset<cite> (Blitzer et al., 2007)</cite> which is a benchmark dataset for sentiment categorization. It has 4 domains, namely, books(B), dvds(D), electronics(E), and kitchen(K) each with 2000 reviews divided equally into positive and negative reviews.",
  "y": "uses"
 },
 {
  "id": "604807137ee5d9a6775821496c6af5_4",
  "x": "Table 2 shows that DA module of SODA outperforms 1) a widely used domain adaptation technique , namely, structural correspondence learning (SCL)<cite> (Blitzer et al., 2007</cite>; Blitzer et al., 2006) , 2) the baseline (BL) where a classifier trained on one domain is applied on another domain, and 3) the in-domain classifier. Note that in Table 2 , the performance of DA module of SODA is reported when it does not use any labeled instances from the target domain. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_0",
  "x": "We extend <cite>our</cite> previous work on constituency parsing <cite>(Kitaev and Klein, 2018)</cite> by incorporating pre-training for ten additional languages, and compare the benefits of no pre-training, ELMo , and BERT (Devlin et al., 2018). Pre-training is effective across all languages evaluated, and BERT outperforms ELMo in large part due to the benefits of increased model capacity. Our parser obtains new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1).",
  "y": "extends"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_1",
  "x": "There has recently been rapid progress in developing contextual word representations that improve accuracy across a range of natural language tasks Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018) . In <cite>our</cite> earlier work <cite>(Kitaev and Klein, 2018)</cite> , <cite>we</cite> showed that such representations are helpful for constituency parsing. However, <cite>these</cite> results only considered the LSTM-based ELMo representations , and only for the English language.",
  "y": "background"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_2",
  "x": "However, <cite>these</cite> results only considered the LSTM-based ELMo representations , and only for the English language. We now extend <cite>this</cite> work to show that using only self-attention also works by substituting BERT (Devlin et al., 2018) . We further demonstrate that pre-training and self-attention are effective across languages by applying our parsing architecture to ten additional languages.",
  "y": "motivation"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_3",
  "x": "We now extend <cite>this</cite> work to show that using only self-attention also works by substituting BERT (Devlin et al., 2018) . We further demonstrate that pre-training and self-attention are effective across languages by applying our parsing architecture to ten additional languages. Our parser code and trained models for 11 languages are publicly available.",
  "y": "extends"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_4",
  "x": "---------------------------------- **MODEL** Our parser as described in <cite>Kitaev and Klein (2018)</cite> accepts as input a sequence of vectors corresponding to words in a sentence, transforms these repre-1 https://github.com/nikitakit/self-attentive-parser sentations using one or more self-attention layers, and finally uses these representations to output a parse tree.",
  "y": "background"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_5",
  "x": "The fact that additional layers are applied to the output of BERT -which itself uses a selfattentive architecture -may at first seem redundant, but there are important differences between these two portions of the architecture. The extra layers on top of BERT use word-based tokenization instead of sub-words, apply the factored version of self-attention proposed in <cite>Kitaev and Klein (2018)</cite> , and are randomly-initialized instead of being pre-trained. We found that omitting these additional layers and using the BERT vectors directly hurt parsing accuracies.",
  "y": "uses"
 },
 {
  "id": "6092234b23f2620c356c2e417c2ce8_6",
  "x": "The tagging head is trained jointly with the parser by adding an auxiliary softmax crossentropy loss, averaged over all words present in a given batch. (2018) We train our parser with a learning rate of 5 \u00d7 10 \u22125 and batch size 32, where BERT parameters are fine-tuned as part of training. All other hyperparameters are unchanged from <cite>Kitaev and Klein (2018)</cite> and Devlin et al. (2018) .",
  "y": "similarities"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_0",
  "x": "In this paper, we propose contextbased methods to improve the automatic identification of revision purposes in student argumentative writing. Argumentation plays an important role in analyzing many types of writing such as persuasive essays , scientific papers (Teufel, 2000) and law documents (Palau and Moens, 2009 ). In student papers, identifying revision purposes with respect to argument structure has been used to predict the grade improvement in the paper after revision<cite> (Zhang and Litman, 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_1",
  "x": "In student papers, identifying revision purposes with respect to argument structure has been used to predict the grade improvement in the paper after revision<cite> (Zhang and Litman, 2015)</cite> . Existing works on the analysis of writing revisions (Adler et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013;<cite> Zhang and Litman, 2015)</cite> typically compare two versions of a text to extract revisions, then classify the purpose of each revision in isolation. That is, while limited contextual features such as revision location have been utilized in prior work, such features are computed from the revision being classified but typically not its neighbors.",
  "y": "background"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_2",
  "x": "---------------------------------- **RELATED WORK** There are multiple works on the classification of revisions (Adler et al., 2011; Javanmardi et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013;<cite> Zhang and Litman, 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_3",
  "x": "To label our data, we adapt the schema defined in<cite> (Zhang and Litman, 2015)</cite> as it can be reliably annotated and is argument- (Faigley and Witte, 1981) . As we focus on argumentative changes, we merge all the Surface subcategories into one Surface category. As <cite>Zhang and Litman (2015)</cite> reported that both Rebuttals and multiple labels for a single revision were rare, we merge Rebuttal and Warrant into one Warrant category 1 and allow only a single (primary) label per revision.",
  "y": "extends"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_4",
  "x": "As <cite>Zhang and Litman (2015)</cite> reported that both Rebuttals and multiple labels for a single revision were rare, we merge Rebuttal and Warrant into one Warrant category 1 and allow only a single (primary) label per revision. Corpora. Our experiments use two corpora consisting of Drafts 1 and 2 of papers written by high school students taking AP-English courses; papers were revised after receiving and generating peer feedback.",
  "y": "motivation"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_5",
  "x": "Our experiments use two corpora consisting of Drafts 1 and 2 of papers written by high school students taking AP-English courses; papers were revised after receiving and generating peer feedback. Corpus A was collected in our earlier pa-per<cite> (Zhang and Litman, 2015)</cite> , although the original annotations were modified as described above. It contains 47 paper draft pairs about placing contemporaries in Dante's Inferno.",
  "y": "uses"
 },
 {
  "id": "622bd6f16d55ab5853389286cdda56_6",
  "x": "Our previous work<cite> (Zhang and Litman, 2015)</cite> used three types of features primarily from prior work (Adler et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013) for argumentative revision classification. Location features encode the location of the sentence in the paragraph and the location of the sentence's paragraph in the essay. Textual features encode revision operation, sentence length, edit distance between aligned sentences and the difference in sentence length and punctuation numbers.",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_0",
  "x": "later works proposed partially-or purely-convolutional CTC models [8] [9] [10] [11] and convolution-heavy encoder-decoder models [16] for ASR. However, convolutional models must be significantly deeper to retrieve the same temporal receptive field [23] . Recently, the mechanism of self-attention<cite> [22,</cite> 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time.",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_1",
  "x": "Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer<cite> [22]</cite> ), sentiment analysis [25] , and other tasks. These successes have motivated preliminary work in self-attention for ASR. Time-restricted self-attention was used as a drop-in replacement for individual layers in the state-of-theart lattice-free MMI model [26] , an HMM-NN system.",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_2",
  "x": "**MOTIVATING THE SELF-ATTENTION LAYER** We now replace recurrent and convolutional layers for CTC with self-attention [24] . Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder<cite> [22]</cite> , previous explorations of self-attention in ASR [19, 27] , and defined in Section 2.3.",
  "y": "similarities uses"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_3",
  "x": "Maximum path length Table 1 : Operation complexity of each layer type, based on<cite> [22]</cite> . T is input length, d is no. of hidden units, and k is filter/context width. We also see inspiration from convolutional blocks: residual connections, layer normalization, and tied dense layers with ReLU for representation learning.",
  "y": "similarities uses"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_4",
  "x": "Let H \u2208 R T \u00d7d h denote a sublayer's input. The first sublayer performs multi-head, scaled dot-product, self-attention<cite> [22]</cite> . For each head i of nhds, we learn linear maps W",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_5",
  "x": "where \u03c3 is row-wise softmax. Heads are concatenated along the dh/nhds axis to give MltHdAtt = [HdAtt (1) , . . . , HdAtt (n hds ) ]. The second sublayer is a position-wise feed-forward network<cite> [22]</cite> FFN(H) = ReLU(HW1 + b1)W2 + b2 where parameters",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_6",
  "x": "**POSITION** Self-attention is inherently content-based<cite> [22]</cite> , and so one often encodes position into the post-embedding vectors. We use standard trigonometric embeddings, where for 0 \u2264 i \u2264 demb/2, we define",
  "y": "background"
 },
 {
  "id": "642aa9fe999d0b2b3793cb1603c04c_7",
  "x": "As self-attention architectures can be unstable in early training, we clip gradients to a global norm of 1 and use the standard linear warmup period before inverse square decay associated with these architectures [19, <cite>22]</cite> . Let n denote the global step number of the batch (across epochs); the learning rate is given by 1 Rescaling so that these differences also have var.",
  "y": "background"
 },
 {
  "id": "653327ecbc925624d509c679fbe0ba_0",
  "x": "Generally, there are two lines of this work. The first line focuses on designing task-specific model structures [1, 15] , which exploit the retrieved concepts from external knowledge base for enhancing the representation. Recently, the other line has studied to pre-train a language model over large corpus to learn the inherent word-level knowledge in an unsupervised way<cite> [4,</cite> 8] , which achieves very promising performance.",
  "y": "background"
 },
 {
  "id": "653327ecbc925624d509c679fbe0ba_1",
  "x": "These methods relied on task-specific model structures which are difficult to adapt to other tasks. Pre-trained language model such as BERT and GPT<cite> [4,</cite> 8] is also used as a kind of commonsense knowledge source. However, the LM method mainly captures the co-occurrence of words and phrases and cannot address some more complex problems which may require the reasoning ability.",
  "y": "background"
 },
 {
  "id": "653327ecbc925624d509c679fbe0ba_2",
  "x": "Here we utilize BERT <cite>[4]</cite> as the pretrained encoder for its superior performance in a range of natural language understanding tasks. Specially, we concatenate the given document, question (as sentence A) and each option (as sentence B) Next, on top of BERT encoder, we add a task-specific output layer and view the multi-choice MRC as a multi-class classification task. Specifically, we apply a linear head layer plus a softmax layer on the final contextualized word representation of [CLS] token h L 0 .",
  "y": "uses"
 },
 {
  "id": "653327ecbc925624d509c679fbe0ba_3",
  "x": "**INCORPORATING RELATION KNOWLEDGE** Task 1 is the relation-existence task. Following <cite>[4]</cite> , we first convert the concept to a set of BPE tokens tokens A and tokens B, with beginning index i and j in the input sequence respectively.",
  "y": "uses"
 },
 {
  "id": "653327ecbc925624d509c679fbe0ba_4",
  "x": "---------------------------------- **IMPLEMENTATION DETAILS** We use the uncased BERT(base) <cite>[4]</cite> as pre-trained language model.",
  "y": "uses"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_0",
  "x": "**ABSTRACT** Efficiency is a prime concern in syntactic MT decoding, yet significant developments in statistical parsing with respect to asymptotic efficiency haven't yet been explored in MT. Recently,<cite> McDonald et al. (2005b)</cite> formalized dependency parsing as a maximum spanning tree (MST) problem, which can be solved in quadratic time relative to the length of the sentence.",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_1",
  "x": "---------------------------------- **O(N 2 )-TIME DEPENDENCY PARSING FOR MT** We now formalize weighted non-projective dependency parsing similarly to<cite> (McDonald et al., 2005b)</cite> and then describe a modified and more efficient version that can be integrated into a phrasebased decoder.",
  "y": "uses"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_2",
  "x": "Extracting all these tags takes time O(n) for any arbitrary pair (i, j). Since i and j are both free variables, feature computation in<cite> (McDonald et al., 2005b)</cite> takes time O(n 3 ), even though parsing itself takes O(n 2 ) time. To make our parser genuinely O(n 2 ), we modified the set of in-between POS features in two ways.",
  "y": "motivation"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_3",
  "x": "Building upon the theoretical work of (Chu and Liu, 1965; Edmonds, 1967) ,<cite> McDonald et al. (2005b)</cite> present a quadratic-time dependency parsing algorithm that is just 0.7% less accurate than \"full-fledged\" chart parsing (which, in the case of dependency parsing, runs in time O(n 3 ) (Eisner, 1996) ). In this paper, we show how to exploit syntactic dependency structure for better machine translation, under the constraint that the depen-dency structure is built as a by-product of phrasebased decoding, without reliance on a dynamicprogramming or chart parsing algorithm such as CKY or Earley. Adapting the approach of<cite> McDonald et al. (2005b)</cite> for machine translation, we incrementally build dependency structure left-toright in time O(n 2 ) during decoding.",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_4",
  "x": "Building upon the theoretical work of (Chu and Liu, 1965; Edmonds, 1967) ,<cite> McDonald et al. (2005b)</cite> present a quadratic-time dependency parsing algorithm that is just 0.7% less accurate than \"full-fledged\" chart parsing (which, in the case of dependency parsing, runs in time O(n 3 ) (Eisner, 1996) ). In this paper, we show how to exploit syntactic dependency structure for better machine translation, under the constraint that the depen-dency structure is built as a by-product of phrasebased decoding, without reliance on a dynamicprogramming or chart parsing algorithm such as CKY or Earley. Adapting the approach of<cite> McDonald et al. (2005b)</cite> for machine translation, we incrementally build dependency structure left-toright in time O(n 2 ) during decoding.",
  "y": "extends"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_5",
  "x": "---------------------------------- **DEPENDENCY PARSING FOR MACHINE TRANSLATION** In this section, we review dependency parsing formulated as a maximum spanning tree problem<cite> (McDonald et al., 2005b)</cite> , which can be solved in quadratic time, and then present its adaptation and novel application to phrase-based decoding.",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_6",
  "x": "While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999) ,<cite> McDonald et al. (2005b)</cite> show O(n 2 )-time parsing is possible if trees are not required to be projective. This relaxation entails that dependencies may cross each other rather than being required to be nested, as shown in Fig. 1 . More formally, a non-projective tree is any tree that does not satisfy the following definition of a projective tree:",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_7",
  "x": "Non-projective dependency structures are sometimes even needed for languages like English, e.g., in the case of the wh-movement shown in Fig. 1 . For languages with relatively rigid word order such as English, there may be some concern that searching the space of non-projective dependency trees, which is considerably larger than the space of projective dependency trees, would yield poor performance. That is not the case: dependency accuracy for nonprojective parsing is 90.2% for English<cite> (McDonald et al., 2005b)</cite> , only 0.7% lower than a projective parser (McDonald et al., 2005a ) that uses the same set of features and learning algorithm.",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_8",
  "x": "That is not the case: dependency accuracy for nonprojective parsing is 90.2% for English<cite> (McDonald et al., 2005b)</cite> , only 0.7% lower than a projective parser (McDonald et al., 2005a ) that uses the same set of features and learning algorithm. In the case of dependency parsing for Czech,<cite> (McDonald et al., 2005b)</cite> even outperforms projective parsing, and was one of the top systems in the CoNLL-06 shared task in multilingual dependency parsing. ----------------------------------",
  "y": "background"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_9",
  "x": "While this change alone ensures that feature extraction is now O(1) for each word pair, this causes a fairly high drop of performance (dependency accuracy Table 4 : Dependency parsing experiments on test sentences of any length. The projective parsing algorithm is the one implemented as in (McDonald et al., 2005a) , which is known as one of the top performing dependency parsers for English. The O(n 3 ) non-projective parser of <cite>(McDonald et al., 2005b</cite> ) is slightly more accurate than our version, though ours runs in O(n 2 ) time. \"Local classifier\" refers to non-projective dependency parsing without removing loops as a post-processing step.",
  "y": "differences"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_10",
  "x": "Its performance is 95.02% on the WSJ, and 95.30% on the English CTB. Additional experiments reveal two main contributing factors to this drop on WSJ: tagging uncased texts reduces tagging accuracy by about 1%, and using only wordbased features further reduces it by 0.6%. Table 4 shows that the accuracy of our truly O(n 2 ) parser is only .25% to .34% worse than the O(n 3 ) implementation of<cite> (McDonald et al., 2005b)</cite> .",
  "y": "differences"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_11",
  "x": "The language pair for our experiments is Chinese-to-English. The training data consists of about 28 million English words and 23.3 million 5 Note that our results on WSJ are not exactly the same as those reported in <cite>(McDonald et al., 2005b</cite> ), since we used slightly different head finding rules. To extract dependencies from treebanks, we used the LTH Penn Converter (http:// nlp.cs.lth.se/pennconverter/), which extracts dependencies that are almost identical to those used for the CoNLL-2008 Shared Task.",
  "y": "differences"
 },
 {
  "id": "6580dc2f7316cea4e0933ff515a704_12",
  "x": "In this paper, we presented a non-projective dependency parser whose time-complexity of O(n 2 ) improves upon the cubic time implementation of<cite> (McDonald et al., 2005b)</cite> , and does so with little loss in dependency accuracy (.25% to .34%). Since this parser does not need to enforce projectivity constraints, it can easily be integrated into a phrase-based decoder during search (rather than during rescoring). We use dependency scores as an extra feature in our MT experiments, and found that our dependency model provides significant gains over a competitive baseline that incorporates a large 5-gram language model (0.92% TER and 0.45% BLEU absolute improvements).",
  "y": "uses"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_0",
  "x": "****EXTENDING SENSE COLLOCATIONS IN INTERPRETING NOUN COMPOUNDS**** **ABSTRACT** This paper investigates the task of noun compound interpretation, building on the sense collocation approach proposed by <cite>Moldovan et al. (2004)</cite> .",
  "y": "extends"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_1",
  "x": "Due to these challenges, current NC interpretation methods are too error-prone to employ directly in NLP applications without any human intervention or preprocessing. In this paper, we investigate the task of NC interpretation based on sense collocation. It has been shown that NCs with semantically similar compo-nents share the same SR ; this is encapsulated by the phrase coined as sense collocation in <cite>Moldovan et al. (2004)</cite> .",
  "y": "background"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_2",
  "x": "**RELATED WORK** A majority of research undertaken in interpreting NCs have been based on two statistical methods: SEMANTIC SIMILARITY (Barker and Szpakowicz, 1998; Rosario, 2001;<cite> Moldovan et al., 2004</cite>; Kim and Baldwin, 2005; Nastase, 2006; Girju, 2007; and SEMANTIC INTER-PRETABILITY (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006) . Our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity category.",
  "y": "background"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_3",
  "x": "**RELATED WORK** A majority of research undertaken in interpreting NCs have been based on two statistical methods: SEMANTIC SIMILARITY (Barker and Szpakowicz, 1998; Rosario, 2001;<cite> Moldovan et al., 2004</cite>; Kim and Baldwin, 2005; Nastase, 2006; Girju, 2007; and SEMANTIC INTER-PRETABILITY (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006) . Our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity category.",
  "y": "uses"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_4",
  "x": "A significant contribution to this area is by <cite>Moldovan et al. (2004)</cite> , who used the sense collocation (i.e. pair-of-word-senses) as their primary feature in disambiguating NCs. Many subsequent studies have been based on this sense collocation method, with the addition of other performance-improving features. For example, Girju (2007) added contextual information (e.g. the grammatical role and POS) and cross-lingual information from 5 European languages as features to her model.",
  "y": "background"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_5",
  "x": "**MOTIVATION** As mentioned above, <cite>Moldovan et al. (2004)</cite> showed that the sense collocation of NCs is a key feature when interpreting NCs. Further research in this area has shown that not only synonymous NCs share the same SR, but NCs whose components are replaced with similar words also have the same SR as the original NCs .",
  "y": "background"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_6",
  "x": "The original method described in <cite>Moldovan et al. (2004)</cite> only relies on observed sense collocations. The components of the NCs are represented as specific synsets in WordNet, and the model does not capture related words. Hence, in this paper, we aimed to develop a model that can take advantage of relatedness between WordNet synsets via hypernyms, hyponyms and sister words, without the risk of losing semantic granularity or introducing noisy training data.",
  "y": "motivation"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_7",
  "x": "At first, we describe the principal idea of sense collocation method on NC interpretation and the probability model proposed in<cite> (Moldovan et al., 2004)</cite> . Then we present our method using hypernyms, hyponyms and sister words in order to extend sense collocation method. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_8",
  "x": "---------------------------------- **SENSE COLLOCATION** The basic idea behind sense collocation method in <cite>Moldovan et al. (2004)</cite> was based on the \"pair-ofword-senses\" from the component nouns in noun compounds as features of the classifier.",
  "y": "background"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_9",
  "x": "We extend the approach of <cite>Moldovan et al. (2004)</cite> by adding similar words as features focusing on hypernyms, hyponyms and sister words of the modifier and head noun. We accumulate the features for semantic relations based on different taxonomic relation types, from which we construct a feature vector to build a classifier over. The features of each taxonomic relation types are listed below.",
  "y": "extends"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_10",
  "x": "The baseline was computed using a Zero-R classifier (i.e. majority vote). 2 The performance of the original method proposed in <cite>Moldovan et al. (2004)</cite> is considered as a benchmark. .217 .496 .544 .552 .573 .562 .588 .568 .557 .197 .142 .547 .547 533 .573 .600 .606 .586 .607 .630 .467 .453 IA .507 .581 .595 .608 .649 .671 .653 .629 .645 .500 .500 PP .655 .667 .679 .691 .679 .737 .700 .690 .687 .655 .655 OE .558 .636 .623 .610 .662 .645 .662 .625 .712 .558 .558 TT .636 .697 .727 .712 .742 .766 .732 .717 .650 .515 .394 PW .634 .620 .690 .690 .629 .657 .585 .731 .630 .633 .634 CC .514 .676 .703 .689 .689 .676 .667 .647 .698 .446 .514 All .579 .632 .649 .653 .662 .679 .654 .661 .667 .541 .534 Table 4 : Results for each of the 2-way classification tasks: B = baseline, M+ = <cite>Moldovan et al. (2004)</cite> method, H i = ith-order Hypernym, O = Hyponym and S = Sister word; the best performing system is indicated in boldface and that of extended sense collocation as proposed in this paper.",
  "y": "uses"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_12",
  "x": "The baseline was computed using a Zero-R classifier (i.e. majority vote). 2 The performance of the original method proposed in <cite>Moldovan et al. (2004)</cite> is considered as a benchmark. .217 .496 .544 .552 .573 .562 .588 .568 .557 .197 .142 .547 .547 533 .573 .600 .606 .586 .607 .630 .467 .453 IA .507 .581 .595 .608 .649 .671 .653 .629 .645 .500 .500 PP .655 .667 .679 .691 .679 .737 .700 .690 .687 .655 .655 OE .558 .636 .623 .610 .662 .645 .662 .625 .712 .558 .558 TT .636 .697 .727 .712 .742 .766 .732 .717 .650 .515 .394 PW .634 .620 .690 .690 .629 .657 .585 .731 .630 .633 .634 CC .514 .676 .703 .689 .689 .676 .667 .647 .698 .446 .514 All .579 .632 .649 .653 .662 .679 .654 .661 .667 .541 .534 Table 4 : Results for each of the 2-way classification tasks: B = baseline, M+ = <cite>Moldovan et al. (2004)</cite> method, H i = ith-order Hypernym, O = Hyponym and S = Sister word; the best performing system is indicated in boldface and that of extended sense collocation as proposed in this paper.",
  "y": "uses"
 },
 {
  "id": "66392c3b6fa3744de79f056f615a75_13",
  "x": "In the automatic interpretation of NCs, many claims have been made for the increase in performance, but these works make their own assumptions for interpretation (Barker and Szpakowicz, 1998;<cite> Moldovan et al., 2004</cite>; Kim and Baldwin, 2005; Girju, 2007; Seaghdha, 2007) . ---------------------------------- **CONCLUSION**",
  "y": "background"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_0",
  "x": "This paper attempts to use text information in financial reports as factors to rank the risk of stock returns. Considering such a problem is a text ranking problem, we attempt to use learning-to-rank techniques to deal with the problem. Unlike the previous study<cite> (Kogan et al., 2009)</cite> , in which a regression model is employed to predict stock return volatilities via text information, our work utilizes learning-to-rank methods to model the ranking of relative risk levels directly.",
  "y": "differences"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_1",
  "x": "The difficulty of predicting the values is partially because of the huge amount of noise within texts<cite> (Kogan et al., 2009</cite> ) and partially because of the weak connection between texts and the quantities. Regarding these issues, we turn to rank the relative risk levels of the companies (their stock returns). By means of learning-to-ranking techniques, we attempt to identify some key factors behind the text ranking problem.",
  "y": "background"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_2",
  "x": "Our experimental results show that in terms of two different ranking correlation metrics, our ranking approach significantly outperforms the regression-based method with a confidence level over 95%. In addition to the improvements, through the learned ranking models, we also discover meaningful words that are financially risk-related, some of which were not identified in<cite> (Kogan et al., 2009</cite> ). These words enable us to get more insight and understanding into financial reports.",
  "y": "differences"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_3",
  "x": "In addition to the conventional models, in recent years there have also been some attempts of using learning-based methods to solve the text ranking problem, such as (Freund et al., 2003; Burges et al., 2005; Joachims, 2006) , which subsequently brings about a new area of learning to rank in the fields of information retrieval and machine learning. Considering the prevalence of learning-to-rank techniques, this paper attempts to use such techniques to deal with the ranking problem of financial risk. In recent year, there have been some studies conducted on mining financial reports, such as (Lin et al., 2008; <cite>Kogan et al., 2009</cite>; Leidner and Schilder, 2010) .",
  "y": "background"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_4",
  "x": ". We now proceed to classify the volatilities of n stocks into 2\u2113 + 1 risk levels, where n, \u2113 \u2208 {1, 2, 3, \u00b7 \u00b7 \u00b7 }. Let m be the sample mean and s be the sample standard deviation of the logarithm of volatilities of n stocks (denoted as ln(v)). The distribution over ln(v) across companies tends to have a bell shape<cite> (Kogan et al., 2009)</cite> .",
  "y": "background"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_5",
  "x": "**EXPERIMENTS AND ANALYSIS** In this paper, the 10-K Corpus<cite> (Kogan et al., 2009</cite> ) is used to conduct the experiments; only Section 7 \"management's discussion and analysis of financial conditions and results of operations\" (MD&A) is included in the experiments since typically Section 7 contains the most important forward-looking statements. In the experiments, all documents were stemmed by the Porter stemmer, and the documents in each year are indexed separately.",
  "y": "similarities"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_6",
  "x": "In addition to the reports, the twelve months after the report volatility for each company can be calculated by Equation (1), where the price return series can be obtained from the Center for Research in Security Prices (CRSP) US Stocks Database. The company in each year is then classified into 5 risk levels (\u2113 = 2) via Equation (2). For regression, linear kernel is adopted with \u03b5 = 0.1 and the trade-off C is set to the default choice of SVM light , which are the similar settings of<cite> (Kogan et al., 2009</cite> ).",
  "y": "similarities"
 },
 {
  "id": "6678c19792be8d9ad66cf923d00c23_7",
  "x": "Almost all the terms found by our ranking approach are financially meaningful; in addition, some of highly risk-correlated terms are not even reported in<cite> (Kogan et al., 2009)</cite> . We now take the term defaut (only identified by our ranking approach) as an example. In finance, a company \"defaults\" when it cannot meet its legal obligations according to the debt contract; as a result, the term \"default\" is intuitively associated with a relative high risk level.",
  "y": "differences"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_0",
  "x": "Our method is based on the method described in (<cite>Hoshino et al., 2013</cite>) , and extends <cite>their</cite> rules to handle abbreviation and passivization frequently found in scientific papers. Experimental results show that our proposed method improves performance of both (<cite>Hoshino et al., 2013</cite>) 's system and our phrase-based SMT baseline without preordering. ----------------------------------",
  "y": "extends uses"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_1",
  "x": "Experimental results show that our proposed method improves performance of both (<cite>Hoshino et al., 2013</cite>) 's system and our phrase-based SMT baseline without preordering. ---------------------------------- **INTRODUCTION**",
  "y": "differences"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_2",
  "x": "Preordering methods employ various kinds of linguistic information to achieve better alignment between source and target languages. Specifically, previous work in the literature uses morphological analysis (Katz-Brown and Collins, 2008) , dependency structure (Katz-Brown and Collins, 2008) and predicate-argument structure (Komachi et al., 2006; <cite>Hoshino et al., 2013</cite>) for preordering in Japanese-English statistical machine translation. However, these preordering methods are tested on limited domains: travel (Komachi et al., 2006) and patent (Katz-Brown and Collins, 2008; <cite>Hoshino et al., 2013</cite>) corpora.",
  "y": "background"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_3",
  "x": "Preordering methods employ various kinds of linguistic information to achieve better alignment between source and target languages. Specifically, previous work in the literature uses morphological analysis (Katz-Brown and Collins, 2008) , dependency structure (Katz-Brown and Collins, 2008) and predicate-argument structure (Komachi et al., 2006; <cite>Hoshino et al., 2013</cite>) for preordering in Japanese-English statistical machine translation. However, these preordering methods are tested on limited domains: travel (Komachi et al., 2006) and patent (Katz-Brown and Collins, 2008; <cite>Hoshino et al., 2013</cite>) corpora.",
  "y": "motivation"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_4",
  "x": "Predicate-argument structure-based preordering is one of the promising approaches that can solve syntactic and stylistic difference between a language pair. Predicate-argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction. Following (<cite>Hoshino et al., 2013</cite>) , we perform predicate-argument structure analysis on the Japanese side to preorder Japanese sentences to form an SVO-like word order.",
  "y": "uses"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_5",
  "x": "We propose three modifications to the preordering rules to extend their model to better handle translation of scientific papers. The main contribution of this work is as follows: \u2022 We propose an extension to (<cite>Hoshino et al., 2013</cite>) in order to deal with abbreviation and passivization frequently found in scientific papers.",
  "y": "extends motivation"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_6",
  "x": "Dependency analysis-based method reorders segments into a head-initial sentence, and moves verbs to make an SVO-like structure. Unlike (Komachi et al., 2006) , they also reverse all words in each phrase. Third, <cite>Hoshino et al. (2013)</cite> proposed predicate-argument structure-based preordering rules in two-level for the Japanese-English patent translation task.",
  "y": "background"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_7",
  "x": "Furthermore, sentence-level preordering rules are divided into three parts. In total, sentences are reordered sequentially by four rules. Since <cite>this method</cite> is the one we re-implemented in this paper, we will describe <cite>their method</cite> in detail below.",
  "y": "uses"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_8",
  "x": "It will improve alignments because function words in Japanese (e.g. postposition) appear after content words while those in English (e.g. preposition) appear before content words. 3 Extension to (<cite>Hoshino et al., 2013</cite>) Our proposed preordering model is based on (<cite>Hoshino et al., 2013</cite>) with three extensions to better handle academic writing in scientific papers. ----------------------------------",
  "y": "extends uses"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_9",
  "x": "(explained . . .)\" can be either \". . . was explained\" or \"It was explained that . . .\".). <cite>Hoshino et al. (2013)</cite> proposed to move a predicate after the subject (inter-chunk preordering).",
  "y": "background"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_10",
  "x": "We compared translation performance using a standard phrase-based statistical machine translation technique with three kinds of data: \u2022 preordered data by our re-implementation of (<cite>Hoshino et al., 2013</cite>) , and",
  "y": "uses"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_11",
  "x": "We analyzed predicate-argument structure of only the last predicate for each sentence, regardless of the number of predicates in a sentence. Also, following (<cite>Hoshino et al., 2013</cite>) , we did not consider event nouns as predicates. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_12",
  "x": "Table 1 shows the experimental results. In terms of BLEU, our re-implementation of (<cite>Hoshino et al., 2013</cite>) is below the baseline method while our proposed methods better than the baseline. In terms of RIBES, all preordering methods outperform the baseline, and our proposed method archieve the highest score.",
  "y": "differences"
 },
 {
  "id": "668e8967d702d4538c85935de083f7_13",
  "x": [
   "**DISCUSSION** Some of the errors found in a translation result are due to the errors in predicate-argument structure analysis. We found that it is hard for predicateargument structure analyzer trained on a newswire Table 1 : Comparison of the preordering methods."
  ],
  "y": "uses"
 },
 {
  "id": "66b6283cf1f20977286f99ef21b3c7_0",
  "x": "The challenge is that these models must describe semantics and pragmatics, as well as syntax and behavior. My own slow progress (Cassell et al., 2000; <cite>Koller and Stone, 2007)</cite> shows that there's still lots of hard work needed to develop suitable techniques. I keep going because of the methodological payoffs I see on the horizon.",
  "y": "motivation future_work"
 },
 {
  "id": "672d4299e60752e866293d72f97905_0",
  "x": "Psycholinguistic properties have been used in various approaches, such as for Lexical Simplification<cite> [12]</cite> , for Text Simplification at the sentence level, with the aim of reducing the difficulty of informative text for language learners [18] , to predict the reading times (RTs) of each word in a sentence to assess sentence complexity [14] and also to create robust text level readability models [17] , which is also one of the purposes of this paper. Because of its inherent costs, the measurement of subjective psycholinguistic properties is usually used in the creation of datasets of limited size [2, 7, 8, 15] . For the English language, the most well known database of this kind is the MRC Psycholinguistic Database 4 , which contains 27 subjective psycholinguistic properties for 150,837 words.",
  "y": "background"
 },
 {
  "id": "672d4299e60752e866293d72f97905_1",
  "x": "In this work we aim to overcome this gap by automatically inferring the psycholinguistic properties of imageability, concreteness, AoA and subjective frequency (similar to familiarity) for a large database of 26,874 BP words using a resource-light regression approach. As for the automatic inference, this work is strongly based on the results of<cite> [12]</cite> which proposed an automatic bootstrapping method for regression to populate the MRC Database. We explore here 3 research questions: (1) is it possible to achieve high Pearson and Spearman correlations values and low MSE values with a regression method using only word embedding features to infer the psycholinguistic properties for BP? (2) which size a database with psycholinguistic properties should have to be used in regression models?",
  "y": "similarities"
 },
 {
  "id": "672d4299e60752e866293d72f97905_2",
  "x": "---------------------------------- **RELATED WORKS** To the best of our knowledge there are only two studies that propose regression methods to automatically estimate missing psycholinguistic properties in the MRC Database [4, <cite>12]</cite> .",
  "y": "background"
 },
 {
  "id": "672d4299e60752e866293d72f97905_3",
  "x": "[<cite>12]</cite> automatically estimate missing psycholinguistic properties in the MRC Database through a bootstrapping algorithm for regression. Their method exploits word embedding models and 15 lexical features, including the number of senses, synonyms, hyper-nyms and hyponyms for word in WordNet and also minimum, maximum and average distance between the word's senses in WordNet and the thesaurus' root sense. The Pearson correlation between the estimated score and the inferred score for familiarity was 0.846; 0.862 for AoA; 0.823 for imagenery and 0.869 for concretness, which is better than the results of [4] .",
  "y": "background"
 },
 {
  "id": "672d4299e60752e866293d72f97905_4",
  "x": "**A LIGHTWEIGHT REGRESSION METHOD TO INFER PSYCHOLINGUISTIC PROPERTIES OF WORDS** The fact that the methods developed by [4] and<cite> [12]</cite> are based on a large, scarce lexical resources as WordNet, led us to raise the question \"Could we have a similar performance with a simpler set of features which are easily obtainable for most languages?\". Therefore we decided to build our regressors using only word length, frequency lists, lexical databases composed of school dictionaries and word embeddings models.",
  "y": "motivation background"
 },
 {
  "id": "672d4299e60752e866293d72f97905_5",
  "x": "One critical difference between the strategy of<cite> [12]</cite> and ours is that they concatenate all features to train a regressor, while we take a different approach. Although simply combining all features is straightforward, it can lead to noise insertion, given that the features used greatly contrast among them (e.g. word embeddings and word length). Instead, we adopted a more elegant solution, called Multi-View Learning [19] .",
  "y": "motivation differences"
 },
 {
  "id": "672d4299e60752e866293d72f97905_6",
  "x": "We used a linear least squares regressor with L2 regularization, which is also known as Ridge Regression or Tikhonov regularization [6] . We choose this regression method due to the promising results reported by<cite> [12]</cite> . We trained three regressors in different feature spaces: lexical features, Skip-Gram embeddings, and GloVe embeddings.",
  "y": "uses"
 },
 {
  "id": "6869f08e826aa434471c51c010ef28_0",
  "x": "In a previous work, we proposed to discover the hierarchical structure of two-level acoustic patterns, including subword-like and word-like patterns. A similar two-level framework was also developed recently [18] . In a more recent attempt <cite>[19]</cite> , we further proposed a framework of discovering multi-level acoustic patterns with varying model granularity.",
  "y": "background"
 },
 {
  "id": "6869f08e826aa434471c51c010ef28_1",
  "x": "Note that in our previous work <cite>[19]</cite> , the effect of the third dimension, the acoustic granularity which is the number of Gaussians in each state, was shown to be negligible, thus here we simply set the number of Gaussians in each state to be 4 in all cases. Although the selection of the hyperparameters can be arbitrary in this two-dimensional space, here we only select M temporal granularities and N phonetic granularities, forming a two-dimensional array of M \u00d7 N hyperparameter sets in the granularity space. ----------------------------------",
  "y": "background"
 },
 {
  "id": "6869f08e826aa434471c51c010ef28_2",
  "x": "In this section we summarize the way to perform spoken term detection <cite>[19]</cite> . Let {pr, r = 1, 2, 3, .., n} denote the n acoustic patterns in the set of \u03c8=(m, n). We first construct a similarity matrix S of size n \u00d7 n off-line for every pattern set \u03c8=(m, n), for which the element S(i, j) is the similarity between any two pattern HMMs pi and pj in the set.",
  "y": "background"
 },
 {
  "id": "6869f08e826aa434471c51c010ef28_4",
  "x": "The maximum is selected to represent the relevance between document d and query q on the pattern set \u03c8=(m, n) as in (9) . It is also possible to consider dynamic time warping (DTW) on the matrix W as shown in Fig. 4(b) . However, previous experiments showed that the extra improvements brought in this way is almost negligible, probably because here we have jointly considered the M \u00d7 N different pattern sequences based on the M \u00d7 N different pattern sets (e.g. including longer /shorter patterns), so the different time-warped matching and insertion/deletion between d and q is already automatically included <cite>[19]</cite> .",
  "y": "background"
 },
 {
  "id": "6a693f9cbc6dbb3676d765eee97db7_0",
  "x": "Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000;<cite> Wang et al., 2005</cite>; McDonald et al., 2005) . Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997) . Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004) .",
  "y": "background"
 },
 {
  "id": "6a693f9cbc6dbb3676d765eee97db7_1",
  "x": "Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al., 2005) , a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches. For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997;<cite> Wang et al., 2005)</cite> , and yet they are not being used in current large margin training algorithms. Another unexploited connection is that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach-the \"structured margin loss\" (McDonald et al., 2005) -is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component.",
  "y": "background"
 },
 {
  "id": "6a693f9cbc6dbb3676d765eee97db7_2",
  "x": "For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997;<cite> Wang et al., 2005)</cite> , and yet they are not being used in current large margin training algorithms. I have addressed both of these issues, as well as others in my work.",
  "y": "background motivation"
 },
 {
  "id": "6a693f9cbc6dbb3676d765eee97db7_3",
  "x": "This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models<cite> (Wang et al., 2005</cite>; Eisner, 1996) as well as non-probabilistic models (McDonald et al., 2005; Wang et al., 2006) . For the purpose of learning, the score of each link can be expressed as a weighted linear combination of features where i are the weight parameters to be estimated during training.",
  "y": "uses"
 },
 {
  "id": "6a693f9cbc6dbb3676d765eee97db7_4",
  "x": "To learn an accurate dependency parser from data, the first approach I investigated is based on a strictly lexical parsing model where all the parameters are based on words<cite> (Wang et al., 2005)</cite> . The advantage of this approach is that it does not rely on part-ofspeech tags nor grammatical categories. Furthermore, I based training on maximizing the conditional probability of a parse tree given a sentence, unlike most previous generative models (Magerman, 1995; Collins, 1997; Charniak, 1997) , which focus on maximizing the joint probability of the parse tree and the sentence.",
  "y": "uses"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_0",
  "x": "Owing to notable advances in deep learning and representation learning, important progress has been achieved on text classification, reading comprehension, and other NLP tasks. Recently, pretrained language representations with self-supervised objectives (Peters et al., 2018; <cite>Devlin et al., 2018</cite>; Radford et al., 2018) have further pushed forward the state-of-the-art on many English tasks. While these sorts of deep models can be trained on different languages, deep models typically require substantial amounts of labeled data for the specific domain of data.",
  "y": "background"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_1",
  "x": "Cross-lingual systems rely on training data from one language to train a model that can be applied to other languages (de Melo and Siersdorfer, 2007) , alleviating the training bottleneck issues for low-resource languages. This is facilitated by recent advances in learning joint multilingual representations (Lample and Conneau, 2019; Artetxe and Schwenk, 2018;<cite> Devlin et al., 2018)</cite> . In our work, we propose a self-learning framework to incorporate the predictions of the multilingual BERT model<cite> (Devlin et al., 2018)</cite> on non-English data into an English training procedure.",
  "y": "background"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_2",
  "x": "This is facilitated by recent advances in learning joint multilingual representations (Lample and Conneau, 2019; Artetxe and Schwenk, 2018;<cite> Devlin et al., 2018)</cite> . In our work, we propose a self-learning framework to incorporate the predictions of the multilingual BERT model<cite> (Devlin et al., 2018)</cite> on non-English data into an English training procedure. The initial multilingual BERT model was simultaneously pretrained on 104 languages, and has shown to perform well for cross-lingual transfer of natural language tasks (Wu and Dredze, 2019) .",
  "y": "extends"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_3",
  "x": "For the encoder, we invoke the multilingual BERT model<cite> (Devlin et al., 2018)</cite> , which supports 104 languages 1 . It relies on a shared 110k WordPiece vocabulary across all languages and yields sentence representations in a common multilingual space. Most model hyperparameters are the same as in pretraining, with the exception of the batch size, max.",
  "y": "uses"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_4",
  "x": "With models such as ELMo (Peters et al., 2018) , GPT-2 (Radford et al., 2018) , and BERT<cite> (Devlin et al., 2018)</cite> , important progress has been made in learning improved sentence representations with context-specific encodings of words via a language modeling objective. The latter two approaches both rely on Transformer encoders, but BERT is trained using masked language modeling instead of right-to-left or left-to-right language modeling. Additionally, BERT also optimizes a next sentence classification objective.",
  "y": "background"
 },
 {
  "id": "6da7dcbcb7f52f31ec23c8131d438d_5",
  "x": [
   "Recent work has also investigated cross-lingual extensions. Devlin et al. (2018) themselves published a multilingual version of BERT, following the same model architecture and training procedure, except that the union of 104 different language editions of Wikipedia serves as the training input. Lample and Conneau (2019) incorporate parallel text into BERT's architecture by training on a new supervised learning objective."
  ],
  "y": "background"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_0",
  "x": "A good DID system used as a front-end to an automatic speech recognition system, can help improve the recognition performance by providing dialectal data for acoustic and language model adaptation to the specific dialect being spoken [1] . In this work, we focus on Arabic DID which can can be posed as a five class classification problem, given that the Arabic language can be divided into five major dialects; Egyptian (EGY), Gulf (GLF), Lavantine (LAV), Modern Standard Arabic (MSA) and North African (NOR) <cite>[2]</cite> . Over the past decade, great advances have been made in the field of automatic language identification (LID).",
  "y": "similarities"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_1",
  "x": "On the other hand, Acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, prosody, shifted delta ceptral coefficients, bottleneck features [9, 10] . One of the most successful acoustic approaches is, the use of i-Vector framework for LID, where i-Vectors are extracted for each speech utterance, using an i-Vector extractor that consists of a GMM-UBM trained on top of BNF, followed by a Total Variability Subspace Model<cite> [2,</cite> 11] . The extracted i-Vectors give an Acoustic VSM (Section 2.2).",
  "y": "extends differences"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_2",
  "x": "At prediction time, output scores from the two DID systems are combined to give a final score, on the basis of which classification decision is made. This model combination approach has been shown to give performace improvements on the DID task <cite>[2]</cite> . This also shows that the two systems are complementary to each other, which leads us to investigate a feature space combination approach i.e. to construct a single VSM by combining Phonotactic and Acoustic VSMs, in an attempt to encode useful discriminative information in that single VSM.",
  "y": "extends differences"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_3",
  "x": "**PHONOTACTIC VSM; X P** Phonotactic VSM is constructed by modeling the n-gram phone statistics of the phone sequences that are extracted using an Arabic phone recognizer. Details about the phone recognizer can be found in <cite>[2]</cite> .",
  "y": "background"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_4",
  "x": "**ACOUSTIC VSM; X A** Acoustic VSM is constructed in two steps; 1) Extracting the bottleneck features (BNF) from speech and 2) Modeling BNF using the i-Vector extraction framework. We use the same Deep Neural Network (DNN) based ASR system to extract the BNF as in our previous works<cite> [2,</cite> 13] .",
  "y": "similarities uses"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_5",
  "x": "The parameters of the model are estimated using Maximum Likelihood training criterion. For a detailed explanation of i-Vector modeling framework, reader is directed to excellent work in [15, 11] . In this work, GMM-UBM model has 2048 gaussian components, MFCC features are extracted using a 25 ms window and the i-Vectors are 400 dimensional <cite>[2]</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_6",
  "x": "We also perform Linear Discriminant Analysis (LDA) and Within Class Co-variance Normalization (WCCN) on the Acoustic Vector Space, to increase the discriminative strength of the VSM. This method has been shown to improve DID (LID) performance<cite> [2,</cite> 11] . Here, we give a brief overview of the mathematical foundations of the CCA .",
  "y": "background"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_7",
  "x": "Training and test data used in this work is the same as used in <cite>[2]</cite> . Table 1 gives the number of hours of data available for each dialect for training and testing. Train 13  10  11  9  10  Test  2  2  2  2  2   Table 1 . Number of hours of training and testing data for each dialect Table 2 shows the number of speech utterances that are available for training and testing the DID system.",
  "y": "similarities uses"
 },
 {
  "id": "6e92b1fa4f3b78a099cb222b3eb9a9_8",
  "x": "More details about the train and test data can be found in<cite> [2,</cite> 18] . Fig 3 gives an overview of our DID system, which can be seen as a combination of two broad components; 1) Vector Space Modeling Component and 2) Back-end classifier. ----------------------------------",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_0",
  "x": "For this reason, DCS trees are detached from any specific relational database, in a way that each node of a DCS tree indicates a content word in a sentence (thus no fixed set of possible word labels for a DCS tree node), and each edge indicates a semantic relation between two words. Labels on the two ends of an edge, initially indicating fields of tables in a database, are considered as semantic roles of the corresponding words. Abstract denotation is proposed to capture the meaning of this abstract version of DCS tree, and a textual inference system based on abstract denotation is built (<cite>Tian et al., 2014</cite>) .",
  "y": "uses"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_1",
  "x": "To elaborate the above discussion and to provide more topics to the literature, in this paper we discuss the following four questions: ( \u00a72) How well can tree transformation approximate logical inference? ( \u00a73) With rigorous inference on DCS trees, where does logic contribute in the system of <cite>Tian et al. (2014)</cite> In the tree transformation based approach to RTE, it has been realized that some gaps between T and H cannot be filled even by a large number of tree transformation rules extracted from corpus (BarHaim et al., 2007a) . For example in Figure 1 , it is possible to extract the rule blamed for death \u2192 cause loss of life, but not easy to extract tropical storm Debby \u2192 storm, because \"Debby\" could be an arbitrary name which may not even appear in the corpus. This kind of gaps was typically addressed by approximate matching methods, for example by counting common sub-graphs of T and H, or by computing a cost of tree edits that convert T to H. In the example of Figure 1 , we would expect that T is \"similar enough\" (i.e. has many common sub-graphs) with H, or the cost to convert T into H (e.g. by deleting the node Debby and then add the node storm) is low.",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_2",
  "x": "3 Alignment with logical clues <cite>Tian et al. (2014)</cite> proposed a way to generate onthe-fly knowledge to fill knowledge gaps: if H is not proven, compare DCS trees of T and H to generate path alignments (e.g. blamed for death \u223c cause loss of life, as underscored in Figure 1) ; evaluate the path alignments by a similarity score function; and path alignments with a score greater than a threshold (0.4) are accepted and converted to inference rules. The word vectors <cite>Tian et al. (2014)</cite> use to calculate similarities are reported able to capture semantic compositions by simple additions and subtractions (Mikolov et al., 2013) . This is also the case when used as knowledge resource for RTE, for example the similarities between blamed+death and cause+loss+life, or between found+shot+dead and killed, are computed > 0.4.",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_3",
  "x": "The issue is indeed very logical: from \"Hurricane Isabel = she\", \"Hurricane Isabel = storm\", \"she = subject of enter\" and \"Hurricane Isabel = subject of cause\", we can imply that \"storm = subject of enter = subject of cause\". 3 Alignment with logical clues <cite>Tian et al. (2014)</cite> proposed a way to generate onthe-fly knowledge to fill knowledge gaps: if H is not proven, compare DCS trees of T and H to generate path alignments (e.g. blamed for death \u223c cause loss of life, as underscored in Figure 1) ; evaluate the path alignments by a similarity score function; and path alignments with a score greater than a threshold (0.4) are accepted and converted to inference rules. The word vectors <cite>Tian et al. (2014)</cite> use to calculate similarities are reported able to capture semantic compositions by simple additions and subtractions (Mikolov et al., 2013) .",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_4",
  "x": "However, generally such kind of similarity is very noisy. <cite>Tian et al. (2014)</cite> used some logical clues to filter out irrelevant path alignments, which helps to keep a high precision. To evaluate the effect of such logical filters, we compare it with some other alignment strategies, the performance of which on RTE5-test data is shown in Table 1 .",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_5",
  "x": "---------------------------------- **HOW CAN LOGICAL INFERENCE HELP RTE?** Logical inference is shown to be useful for RTE, as <cite>Tian et al. (2014)</cite> demonstrates a system with competitive results.",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_6",
  "x": "Logical inference is shown to be useful for RTE, as <cite>Tian et al. (2014)</cite> demonstrates a system with competitive results. However, despite the expectation that all entailment matters can be explained logically, our observation is that currently logical inference only fills very limited short gaps from T to H. The logical phenomena easily addressed by <cite>Tian et al. (2014)</cite> Table 2 : Proportion (%) of exit status of Prover9 The system of <cite>Tian et al. (2014)</cite> generated onthe-fly knowledge to join several fragments in T and wrongly proved H. In examples of such complexity, distributional similarity is no longer reliable.",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_7",
  "x": "Logical inference is shown to be useful for RTE, as <cite>Tian et al. (2014)</cite> demonstrates a system with competitive results. However, despite the expectation that all entailment matters can be explained logically, our observation is that currently logical inference only fills very limited short gaps from T to H. The logical phenomena easily addressed by <cite>Tian et al. (2014)</cite> Table 2 : Proportion (%) of exit status of Prover9 The system of <cite>Tian et al. (2014)</cite> generated onthe-fly knowledge to join several fragments in T and wrongly proved H. In examples of such complexity, distributional similarity is no longer reliable.",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_8",
  "x": "We plot the running time of <cite>Tian et al. (2014)</cite> 's inference engine (single-threaded) on a 2.27GHz Xeon CPU, with respect to the weighted sum of all statements 2 , as shown in Figure 3 . The graph shows all pairs can be proven in 6 seconds, and proof time scales logarithmically on weight of statements. On the other hand, we converted statements on abstract denotations into FOL formulas, and tried to prove the same pairs using Prover9, 3 a popu-lar FOL theorem prover.",
  "y": "uses"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_9",
  "x": "3 Sec.\" column), and only 16% pairs can be proven in 5 minutes (the \"Orig. 5 Min.\" column), showing severe difficulties for an FOL prover to handle textual inferences with many (usually hundreds of) on-the-fly rules. As such, we use <cite>Tian et al. (2014)</cite> 's inference engine to pin down statements that are actually needed for proving H (usually just 2 or 3 statements), and try to prove H by Prover9 again, using only necessary statements.",
  "y": "uses"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_10",
  "x": "Improvement of similarity score To calculate phrase similarities, <cite>Tian et al. (2014)</cite> use the cosine similarity of sums of word vectors, which ignores syntactic information. We plan to add syntactic information to words by some supertags, and learn a vector space embedding for this structure. Integration of FreeBase to RTE It would be exciting if we can utilize the huge amount of FreeBase data in RTE task.",
  "y": "background"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_11",
  "x": "Directions of our future work are described below. Improvement of similarity score To calculate phrase similarities, <cite>Tian et al. (2014)</cite> use the cosine similarity of sums of word vectors, which ignores syntactic information. We plan to add syntactic information to words by some supertags, and learn a vector space embedding for this structure.",
  "y": "extends"
 },
 {
  "id": "6ed955baf28ad1c7fd6d590e660c20_12",
  "x": "To evaluate the efficiency of logical inference on abstract denotations, we took 110 true entailment pairs from RTE5 development set, which are also pairs that can be proven with on-the-fly knowledge. We plot the running time of <cite>Tian et al. (2014)</cite> 's inference engine (single-threaded) on a 2.27GHz Xeon CPU, with respect to the weighted sum of all statements 2 , as shown in Figure 3 . The graph shows all pairs can be proven in 6 seconds, and proof time scales logarithmically on weight of statements.",
  "y": "similarities"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_0",
  "x": "The NLP community is, however, witnessing a dramatic paradigm shift toward the pretrained deep language representation model, which achieves state of the art in question answering, sentiment classification, and similarity modeling, to name a few. Bidirectional Encoder Representations from Transformers (BERT;<cite> Devlin et al., 2018)</cite> represents one of the latest developments in this line of work. It outperforms its predecessors, ELMo (Peters et al., 2018) and GPT (Radford et al.) , staggeringly exceeding state of the art by a wide margin on multiple natural language understanding tasks.",
  "y": "background"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_1",
  "x": "The resulting models achieve state of the art in question answering, named-entity recognition, and natural language inference, to name a few. Bidirectional Encoder Representations from Transformers (BERT;<cite> Devlin et al., 2018)</cite> currently represents state of the art, vastly outperforming previous models, such as the Generative Pretrained Transformer (GPT; Radford et al.) and Embeddings from Language Models (ELMo; Peters et al., 2018) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_2",
  "x": "---------------------------------- **MODEL** We begin with the pre-trained BERT base and BERT large models, which respectively represent the normal and large model variants <cite>(Devlin et al., 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_3",
  "x": "We begin with the pre-trained BERT base and BERT large models, which respectively represent the normal and large model variants <cite>(Devlin et al., 2018)</cite> . To adapt BERT for document classifica- Figure 1 . During fine-tuning, we optimize the entire model end-to-end, with the additional softmax classifier parameters W \u2208 IR K\u00d7H , where H is the dimension of the hidden state vectors and K is the number of classes.",
  "y": "extends"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_4",
  "x": "Due to resource constraints, we fine-tune on Yelp for only one epoch. As is the case with<cite> Devlin et al. (2018)</cite> , we find that choosing a batch size of 16, learning rate of 2\u00d710 \u22125 , and MSL of 512 tokens yields optimal performance on the validation sets of all datasets. Hyperparameter study.",
  "y": "similarities"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_5",
  "x": "To gauge the improvement over the default hyperparameters, as well as to highlight the differences in fine-tuning BERT for document classification, we explore varying several key hyperparameters: namely, the number of epochs and the MSL. Originally,<cite> Devlin et al. (2018)</cite> find that fine-tuning for three or four epochs works well for both small and large datasets alike. They also apply a generous MSL of 512, which may be unnecessary for document classification, where fewer tokens may suffice in determining the topic.",
  "y": "background"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_6",
  "x": "Originally,<cite> Devlin et al. (2018)</cite> find that fine-tuning for three or four epochs works well for both small and large datasets alike. They also apply a generous MSL of 512, which may be unnecessary for document classification, where fewer tokens may suffice in determining the topic. Furthermore, while conducting our experiments, we find that even fine-tuning BERT is a computationally intensive task. We argue that it is important to study these two hyperparameters, as they are major determinants of the computational resources required to fine-tune BERT.",
  "y": "motivation"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_7",
  "x": "Trending with<cite> Devlin et al. (2018)</cite> , BERT large achieves state-of-the-art results on all four datasets, followed by BERT base (see Table 2 , rows 11 and 12). The considerably simpler LSTM reg model (row 10) achieves a high F 1 and accuracy of 87.0 and 52.8, respectively, coming close to the quality of BERT base . Surprisingly, the LR and SVM baselines yield competitive results for the multi-label datasets.",
  "y": "similarities background"
 },
 {
  "id": "70d41cad40091bcc30a1fd544c277d_8",
  "x": "The rightmost two subplots in Figure 2 illustrate the F 1 score of BERT fine-tuned using a various number of epochs for AAPD and Reuters. Contrary to<cite> Devlin et al. (2018)</cite> , who achieve state of the art on small datasets with only a few epochs of fine-tuning, we find that smaller datasets require many more epochs to converge. On both the datasets (see Figure 2) , we see a significant drop in model quality when the BERT models are fine-tuned on only four epochs, as suggested in the original paper.",
  "y": "differences"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_0",
  "x": "The dataset for the shared task was introduced by <cite>Thorne et al. (2018)</cite> and consists of 185,445 claims. Table 1 shows three instances from the data set with the claim, the evidence and the verdict. Table 1 : Examples of claims, the extracted evidence from Wikipedia and the verdicts from the shared task dataset<cite> (Thorne et al., 2018)</cite> The baseline system described by <cite>Thorne et al. (2018)</cite> uses 3 major components:",
  "y": "background"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_1",
  "x": "The dataset for the shared task was introduced by <cite>Thorne et al. (2018)</cite> and consists of 185,445 claims. Table 1 shows three instances from the data set with the claim, the evidence and the verdict. Table 1 : Examples of claims, the extracted evidence from Wikipedia and the verdicts from the shared task dataset<cite> (Thorne et al., 2018)</cite> The baseline system described by <cite>Thorne et al. (2018)</cite> uses 3 major components:",
  "y": "background"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_2",
  "x": "Table 1 : Examples of claims, the extracted evidence from Wikipedia and the verdicts from the shared task dataset<cite> (Thorne et al., 2018)</cite> The baseline system described by <cite>Thorne et al. (2018)</cite> uses 3 major components: \u2022 Document Retrieval: Given a claim, identify relevant documents from Wikipedia which contain the evidence to verify the claim. <cite>Thorne et al. (2018)</cite> used the document retrieval component from the DrQA system (Chen et al., 2017) , which returns the k nearest documents for a query using cosine similarity between binned unigram and bigram TF-IDF vectors.",
  "y": "background"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_3",
  "x": "<cite>Thorne et al. (2018)</cite> used the document retrieval component from the DrQA system (Chen et al., 2017) , which returns the k nearest documents for a query using cosine similarity between binned unigram and bigram TF-IDF vectors. \u2022 Sentence Selection: Given the set of retrieved document, identify the candidate evidence sentences. <cite>Thorne et al. (2018)</cite> used a modified document retrieval component of DrQA (Chen et al., 2017) to select the top most similar sentences w.r.t the claim, using bigram TF-IDF with binning.",
  "y": "background"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_4",
  "x": "<cite>Thorne et al. (2018)</cite> used a modified document retrieval component of DrQA (Chen et al., 2017) to select the top most similar sentences w.r.t the claim, using bigram TF-IDF with binning. \u2022 Textual Entailment: For the entailment task, training is done using labeled claims paired with evidence (labels are SUPPORTS, REFUTES, NOT ENOUGH INFO). <cite>Thorne et al. (2018)</cite> used the decomposable attention model (Parikh et al., 2016) for this task.",
  "y": "background"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_5",
  "x": "We see that our best approach (combined) achieved a high coverage 94.4% compared to the baseline<cite> (Thorne et al., 2018)</cite> of 55.3%. Because we do not have the gold evidences for the blind test set we cannot report the claim coverage using our pipeline . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_6",
  "x": "**SENTENCE SELECTION** For sentence selection, we used the modified document retrieval component of DrQA (Chen et al., 2017) to select sentences using bigram TF-IDF with binning as proposed by<cite> (Thorne et al., 2018)</cite> . We extract the top 5 most similar sentences from the k-most relevant documents using the TF-IDF vector similarity.",
  "y": "similarities uses"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_7",
  "x": "Our evidence recall is 78.4 as compared to 45.05 in the development set of FEVER<cite> (Thorne et al., 2018)</cite> , which demonstrates the importance of document retrieval in fact extraction and verification. On the blind test set our sentence selection approach achieves an evidence recall of 75.89. However, even though TF-IDF proves to be a strong baseline for sentence selection we noticed on the dev set that using all 5 evidences together introduced additional noise to the entailment model.",
  "y": "differences"
 },
 {
  "id": "70dc108166d6b5fb9da39c451c3229_8",
  "x": "**TEXTUAL ENTAILMENT** The final stage of our pipeline is recognizing textual entailment. Unlike <cite>Thorne et al. (2018)</cite> , we did not concatenate evidences, but trained our model for each claim-evidence pair.",
  "y": "differences"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_0",
  "x": "In addition to reducing search errors compared to greedy search, it also enables the use of global models that accommodate richer non-local features without overfitting, leading to recent state-of-the-art accuracies of transition-based dependency parsing<cite> (Zhang and Nivre, 2011</cite>; Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012) that are competitive with the best graph-based dependency parsers. It has been known that a transition-based parser using global learning, beam-search and rich features gives significantly higher accuracies than one with local learning and greedy search. However, the effects of global learning, beam-search and rich features have not been separately studied.",
  "y": "background"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_1",
  "x": "It has not been shown how global learning affects the accuracies, or whether it is important at all. For another example, it would be interesting to know whether a local, greedy, transition-based parser can be equipped with the rich features of<cite> Zhang and Nivre (2011)</cite> to improve its accuracy, and in particular whether MaltParser (Nivre et al., 2006) can achieve the same level of accuracies as ZPar<cite> (Zhang and Nivre, 2011)</cite> by using the same range of rich feature definitions. In this paper, we answer the above questions empirically.",
  "y": "motivation"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_2",
  "x": "Our results show that significant improvements are achieved only when the two are jointly applied. Second, we show that the accuracies of a local, greedy transition-based parser cannot be improved by adding the rich features of<cite> Zhang and Nivre (2011)</cite> . Our result suggests that global learning with beam-search accommodates more complex models with richer features than a local model with greedy search and therefore enables higher accuracies.",
  "y": "uses"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_3",
  "x": "While MaltParser is more accurate on frequently occurring short sentences and dependencies, it performs worse on long sentences and dependencies due to search errors. We present empirical studies of the error distribution of global, beam-search transition-based dependency parsing, using ZPar<cite> (Zhang and Nivre, 2011)</cite> as a representative system. We follow McDonald and Nivre (2007) and perform a comparative error analysis of ZPar, MSTParser and MaltParser using the CoNLL-X shared task data.",
  "y": "uses"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_4",
  "x": "Global learning is implemented in the same way as<cite> Zhang and Nivre (2011)</cite> , using the averaged perceptron algorithm (Collins, 2002) and early update (Collins and Roark, 2004) . This is a global learning method in the sense that it tries to maximize accuracy over the entire sentence and not on isolated local transitions. Unless explicitly specified, the same beam size is applied for training and testing when beam-search is applied.",
  "y": "uses"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_5",
  "x": "Figure 1 shows the UAS of ZPar under different settings, where 'global' refers to a global model trained using the same method as<cite> Zhang and Nivre (2011)</cite> , 'local' refers to a local classifier trained using the averaged perceptron, 'base features' refers to the set of base feature templates in<cite> Zhang and Nivre (2011)</cite> , and 'all features' refers to the set of base and all extended feature templates in<cite> Zhang and Nivre (2011)</cite> . When the size of the beam is 1, the decoding algorithm is greedy local search. Using base features, a locally trained model gives a UAS of 89.15%, higher than that of a globally trained model (89.04%).",
  "y": "uses"
 },
 {
  "id": "730738d63cabcd4e63ec4300a8091b_6",
  "x": "Beam-search does not bring additional improvements. For further evidence, we add rich non-local features in the same increments as<cite> Zhang and Nivre (2011)</cite> to both ZPar and MaltParser, and evaluate UAS on the same development data set. Original settings are applied to both parsers, with ZPar using global learning and beam-search, and MaltParser using local learning and greedy search.",
  "y": "uses"
 },
 {
  "id": "7404a4e4e23eea9663b580f9959689_0",
  "x": "String ED is defined here as the total edit cost incurred in transforming a source language string (S) to a target language string (T) through a sequence of edit operations. The edit operations include: (M)atching an element in S with an element in T; (I)nserting an element into T, and (D)eleting an element in S. 1 The generation task is part of the NEWS 2009 machine transliteration shared task<cite> (Li et al., 2009)</cite> Based on all representative symbols used for each of the two languages, emission costs for each of the edit operations and transition parameters can be estimated and used in measuring the similarity between two strings. To generate transliterations using pair HMM parameters, WFST (Graehl, 1997) techniques are adopted.",
  "y": "uses background"
 },
 {
  "id": "7404a4e4e23eea9663b580f9959689_1",
  "x": "---------------------------------- **DATA SETUP** The data used is divided according to the experimental runs that were specified for the NEWS 2009 shared transliteration task<cite> (Li et al., 2009</cite> ): a standard run and non-standard runs.",
  "y": "uses"
 },
 {
  "id": "7404a4e4e23eea9663b580f9959689_3",
  "x": "Six measures were used for evaluating system transliteration quality. These include<cite> (Li et al., 2009)</cite> : Accuracy (ACC), Fuzziness in Top-1 (Mean F Score), Mean Reciprocal Rank (MRR), Mean Average Precision for reference transliterations (MAP_R), Mean Average Precision in 10 best candidate transliterations (MAP_10), Mean Average Precision for the system (MAP_sys). Table 1 shows the results obtained using only the data sets provided for the shared transliteration task.",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_0",
  "x": "Given enough training data, data-driven models can learn to map word problem texts to arbitrarily complex equations or systems of equations. These models have the additional advantage of being more language-independent than semantic methods, which often rely on parsers and other NLP tools. To train these fully data driven models, large-scale datasets for both English and Chinese were recently introduced<cite> (Wang et al., 2017</cite>; Koncel-Kedziorski et al., 2016) .",
  "y": "background"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_1",
  "x": "These models have the additional advantage of being more language-independent than semantic methods, which often rely on parsers and other NLP tools. To train these fully data driven models, large-scale datasets for both English and Chinese were recently introduced<cite> (Wang et al., 2017</cite>; Koncel-Kedziorski et al., 2016) . In response to the success of representation learning elsewhere in NLP, sequence to sequence (seq2seq) models have been applied to algebra problem solving<cite> (Wang et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_2",
  "x": "Downstream applications such as question answering or automated tutoring systems may never have to deal with arbitrarily complex or even unseen equation types, obviating the need for a sequence prediction model. These considerations beg the questions: how do data-driven approaches to math word problem solving compare to each other? How can datadriven approaches benefit from recent advances in neural representation learning? What are the limits of data-driven solvers? In this paper, we thoroughly examine datadriven techniques on three larger algebra word problem datasets (Huang et al., 2016; Koncel-Kedziorski et al., 2016;<cite> Wang et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_3",
  "x": "Following <cite>Wang et al. (2017)</cite> , we use Jaccard distance in this model. For test problem S and training problem T , the Jaccard similarity is computed as: jacc(S, T ) = S\u2229T S\u222aT . We also evaluate the use of a cosine similarity metric.",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_4",
  "x": "These models condition the token-by-token generation of the equation template on encodings of the word problem text. Following <cite>Wang et al. (2017)</cite> we evaluate a seq2seq with LSTMs as the encoder and decoder. We also evaluate the use of Convolutional Neural Networks (CNNs) in the encoder and decoder.",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_5",
  "x": "Datasets For comparison, we report solution accuracy on the Chinese language Math23K dataset<cite> (Wang et al., 2017)</cite> , and the English language DRAW (Upadhyay and Chang, 2015) and MAWPS (Koncel-Kedziorski et al., 2016) datasets. Math23K and MAWPS consist of single equation problems, and DRAW contains both single and simultaneous equation problems. Details on the datasets are shown in Table 1 .",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_6",
  "x": "Details on the datasets are shown in Table 1 . The Math23K dataset contains problems with possibly irrelevant quantities. To prune these quantities, we implement a significant number identifier (SNI) as discussed in <cite>Wang et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_7",
  "x": "It appears that the ELMo technique may require more training examples before it can improve solution accuracy. The previous state of the art model for the DRAW dataset is described in Upadhyay and Chang (2015) . The state of the art for Math23K, described in <cite>Wang et al. (2017)</cite> , uses a hybrid Jaccard retrieval and seq2seq model.",
  "y": "background"
 },
 {
  "id": "742d9ca22bf801b0ade5fd1671473c_8",
  "x": "More recently, <cite>Wang et al. (2017)</cite> provide a large dataset of Chinese algebra word problems and learn a hybrid model consisting of both retrieval and seq2seq components. The current work extends these approaches by exploring advanced techniques in data-driven solving. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_0",
  "x": "In all of these cases, conversations are entangled: all messages appear together, with no indication of separate conversations. Automatic disentanglement could be used to provide more interpretable results when searching over chat logs, and to help users understand what is happening when they join a channel. Over a decade of research has considered conversation disentanglement (Shen et al., 2006) , but using datasets that are either small (2,500 messages,<cite> Elsner and Charniak, 2008)</cite> or not released (Adams and Martell, 2008) .",
  "y": "motivation background"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_1",
  "x": "3 Related Work IRC Disentanglement Data: The most significant work on conversation disentanglement is a line of papers developing data and models for the #Linux IRC channel<cite> (Elsner and Charniak, 2008</cite>; Elsner and Schudy, 2009; Charniak, 2010, 2011) . Until now, their dataset was the only publicly available set of messages with annotated conversations (partially re-annotated by Mehri and Carenini (2017) with reply-structure graphs), and has been used for training and evaluation in subsequent work (Wang and Oard, 2009; Mehri and Carenini, 2017; Jiang et al., 2018) . We are aware of three other IRC disentanglement datasets.",
  "y": "background"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_2",
  "x": "3 Related Work IRC Disentanglement Data: The most significant work on conversation disentanglement is a line of papers developing data and models for the #Linux IRC channel<cite> (Elsner and Charniak, 2008</cite>; Elsner and Schudy, 2009; Charniak, 2010, 2011) . Using our data we provide the first empirical evaluation of their method.",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_3",
  "x": "Models: <cite>Elsner and Charniak (2008)</cite> explored various message-pair feature sets and linear classifiers, combined with local and global inference methods. Their system is the only publicly released statistical model for disentanglement of chat conversation, but most of the other work cited above applied similar models. We evaluate their model on both our data and our re-annotated version of their data.",
  "y": "background"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_4",
  "x": "Models: <cite>Elsner and Charniak (2008)</cite> explored various message-pair feature sets and linear classifiers, combined with local and global inference methods. We evaluate their model on both our data and our re-annotated version of their data.",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_5",
  "x": "**DATA** We introduce a manually annotated dataset of 77,563 messages: 74,963 from the #Ubuntu IRC channel, 3 and 2,600 messages from the #Linux IRC channel. 4 Annotating the #Linux data enables comparison with <cite>Elsner and Charniak (2008)</cite> , while the #Ubuntu channel has over 34 million messages, making it an interesting largescale resource for dialogue research.",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_6",
  "x": "In both examples the disagreement involves one link, but the conversation structure in the second case is substantially changed. Some disagreements in our data are mistakes, where one annotation is clearly incorrect, and some are ambiguous cases, such as these. In Channel Two, we also see mistakes and ambiguous cases, including a particularly long discussion about a user's financial difficulties that could be divided in multiple ways (also noted by <cite>Elsner and Charniak (2008)</cite> ).",
  "y": "similarities"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_9",
  "x": "We consider three metrics: 6 (1) Variation of Information (VI, Meila, 2007) . (2) One-to-One Overlap (1-1,<cite> Elsner and Charniak, 2008)</cite> .",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_10",
  "x": "First, the baseline has consistently low scores since it forms a single conversation containing all messages. Second, <cite>Elsner and Charniak (2008)</cite> and Lowe et al. (2017) per-form similarly, with one doing better on VI and the other on 1-1, though <cite>Elsner and Charniak (2008)</cite> do consistently better across the exact conversation extraction metrics. Third, our methods do best, with x10 vote best in all cases except precision, where the intersect approach is much better.",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_11",
  "x": "Decreasing the data size to match <cite>Elsner and Charniak (2008)</cite> 's training set leads to worse results, both if the sentences are from diverse contexts (3rd row), and if they are from just two contexts (bottom row). We also see a substantial increase in the standard deviation when only two samples are used, indicating that performance is not robust when the data is not widely sampled. ----------------------------------",
  "y": "differences uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_12",
  "x": "For channel Two, we consider two annotations of the same underlying text: ours and <cite>Elsner and Charniak (2008)</cite>'s. To compare with prior work, we use the metrics defined by Shen et al. (2006, Shen) and Elsner and Charniak (2008, Loc) . 8 We do not use these for our data as they have been superseded by more rigorously studied metrics (VI for Shen) or make strong assumptions about the data (Loc).",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_13",
  "x": "To compare with prior work, we use the metrics defined by Shen et al. (2006, Shen) and Elsner and Charniak (2008, Loc) . 8 We do not use these for our data as they have been superseded by more rigorously studied metrics (VI for Shen) or make strong assumptions about the data (Loc). We do not evaluate on graphs because <cite>Elsner and Charniak (2008)</cite> 's annotations do not include them.",
  "y": "uses"
 },
 {
  "id": "74623c8d812e3c84e7bc6b46e982f5_14",
  "x": "How far apart consecutive messages in a conversation are: <cite>Elsner and Charniak (2008)</cite> and Mehri and Carenini (2017) use a limit of 129 seconds, Jiang et al. (2018) limit to within 1 hour, Guo et al. (2017) limit to within 8 messages, and we limit to within 100 messages. Figure 4 shows the distribution of time differences in our conversations. 94.9% are within 2 minutes, and almost all are within an hour.",
  "y": "differences"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_0",
  "x": "Previous experiments with tasks like language modelling<cite> [Bengio et al., 2009]</cite> , Dependency Parsing, and entailment <cite>[Hashimoto et al., 2016]</cite> have shown faster convergence and performance gains by following a curriculum training regimen in the order of increasingly complicated syntactic and semantic tasks. [Weinshall and Cohen, 2018 ] also find theoretical and experimental evidence for curriculum learning by pretraining on another task leading to faster convergence. With this purview, we propose a syntactico-semantic curriculum training strategy for Hi-En codemixed twitter sentiment analysis.",
  "y": "background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_1",
  "x": "<cite>[Hashimoto et al., 2016]</cite> propose a hierarchical multitask neural architecture with the lower layers performing syntactic tasks, and the higher layers performing the more involved semantic tasks while using the lower layer predictions. [Swayamdipta et al., 2018] also propose a syntactico semantic curriculum with chunking, semantic role labelling and coreference resolution, and show performance gains over strong baselines. Like <cite>[Hashimoto et al., 2016]</cite> , they hypothesize the incorporation of simpler syntactic information into semantic tasks, and provide empirical evidence for the same.",
  "y": "background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_2",
  "x": "Like <cite>[Hashimoto et al., 2016]</cite> , they hypothesize the incorporation of simpler syntactic information into semantic tasks, and provide empirical evidence for the same. 3 Datasets [Prabhu et al., 2016] released a Hi-En codemixed dataset for Sentiment Analysis, comprising 3879 Facebook comments on public pages of Salman Khan and Narendra Modi. Comments are annotated as positive, negative and neutral based on their sentiment polarity, and are distributed across the 3 classes as 15% negative, 50% neutral and 35% positive comments.",
  "y": "background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_3",
  "x": "Curriculum learning can be seen as a sequence of training criteria<cite> [Bengio et al., 2009]</cite> , with increasing task or sample difficulty as the training progresses. It is also closely related with transfer learning by pretraining, especially in the case when the tasks form a logical hierarchy and contribute to the downstream tasks. With this purview, we propose a linguistic hierarchy of training tasks for codemixed languages, with further layers abstracting over the previous ones to achieve increasingly complicated tasks.",
  "y": "background motivation"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_4",
  "x": "Considering the codemixed nature of texts and linguistic hierarchy of information, we propose the tasks in the order of : Language Identification, Part of Speech Tagging, Language Modelling and further semantic tasks like sentiment analysis. Since tokens in codemixed texts have distinct semantic spaces based on their source language, Language Identification can incorporate this disparity among the learnt trigram representations. Following this, the Part of Speech Tagging groups the words based on their logical semantic categories, and encodes simpler word category information in a sequence. Also, as in [Singh et al., 2018a; Sharma et al., 2016] , Language Tag and Part of Speech Tag have previously been provided as manual handcrafted features for a range of downstream syntactic and semantic tasks. In addition to the above tasks, Language Model pretraining has shown significant performance gains as reported by<cite> [Howard and Ruder, 2018]</cite> .",
  "y": "background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_5",
  "x": "**TRANSFER LEARNING** As noted in earlier efforts<cite> [Howard and Ruder, 2018</cite> ] towards finetuning pretrained models for NLP tasks, aggressive finetuning can cause catastrophic forgetting, thus causing the model to simply fit over the target task and forget any capabilities gained during the pretraining stage. On the other hand, too cautious finetuning can cause slow convergence and overfitting.",
  "y": "background motivation"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_6",
  "x": "Gradual Unfreezing: Similar to<cite> [Howard and Ruder, 2018]</cite> , rather than updating all the layers together for finetuning, we explore gradual ordered unfreezing of layers. Thus, initially we freeze all the layers. Then starting from the last layer, we train the model for a certain number of epochs before unfreezing the layer below it. Thus for Sentiment Analysis finetuning, for the first epoch, only \u03b8 sentiment receives the gradient updates, after which we unfreeze the \u03b8 lstm2 , and subsequently unfreeze the lower layers in a similar manner.",
  "y": "similarities uses"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_7",
  "x": "With this purview, similar to<cite> [Howard and Ruder, 2018]</cite> , we propose optimizing different layers in our model to different extents, and keep lower step sizes for the deeper pretrained layers while finetuning on a downstream task. We thus split the parameters as {\u03b8 1 , ..., \u03b8 l } , where \u03b8 i corresponds to the parameters of layer i, and optimize them with separate learning rates {\u03b7 1 , ...., \u03b7 l } . Also, when finetuning a pretrained layer for a downstream task, we keep \u03b7 i < \u03b7 j ; \u2200i < j. Thus, while finetuning the POS + Lang Id pretrained model for Language Modeling, we propose to keep the learning rates for Embedding Layer and LSTM Layer 1 lower than the LSTM Layer 2 weights. Similarly, when finetuning the Language Model for Sentiment Analysis, we keep the learning rates of the deeper layers lower than that of the shallower ones.",
  "y": "similarities uses"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_8",
  "x": "We add a dropout layer with the dropout rate set to 0.2 between the LSTM layers to prevent overfitting. We experiment with average pooling and max pooling concatenation over hidden states for semantic prediction, similar to<cite> [Howard and Ruder, 2018]</cite> , and observe increase in model accuracy by 2.2% on sentiment analysis. To evaluate our baseline for curriculum training experiments, we initially train the model from scratch on the single target task (Sentiment Analysis) for 25 epochs.",
  "y": "similarities background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_9",
  "x": "This experiement highlights the importance of inclusion of language model pretraining for better token level representation learning, and provides a better model prior for sequence representation (LSTM Layer 2 output). We experiment with only Language Modelling as pretraining task, and observe significant gains over no curriculum strategy. We note the convergence of our model with and without curriculum training, and observe that the curriculum training regimen causes faster convergence, as has been observed in previous works [Bengio et al., 2009;<cite> Howard and Ruder, 2018]</cite> .",
  "y": "similarities background"
 },
 {
  "id": "74cd12a801d1f8a95f8898a8cef9c0_10",
  "x": "This is expected as the model is pretrained on prior tasks already have a general purpose representation learning, and only needs to adapt to the idiosyncrasies of the target task, i.e. Sentiment Analysis in this case. As discussed in Section 4.3, for our transfer learning optimization experiments, we segment the optimization of different parameters of our model with different learning rates, in order to limit catastrophic forgetting and interference among the tasks, as proposed by<cite> [Howard and Ruder, 2018]</cite> . We segregate our model parameters in the following 4 groups:",
  "y": "similarities background"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_0",
  "x": "**INTRODUCTION** Attention has emerged as a prominent neural module extensively adopted in a wide range of deep learning research problems (Das et al., 2017; Rockt\u00e4schel et al., 2015; Santos et al., 2016; Xu and Saenko, 2016; Yang et al., 2016; Yin et al., 2016; Zhu et al., 2016; Xu et al., 2015; Chorowski et al., 2015) such as VQA, reading comprehension, textual entailment, image captioning, speech recognition and so forth. It's remarkable success is also embodied in machine translation tasks (Bahdanau et al., 2014;<cite> Vaswani et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_1",
  "x": "We customize the transformer<cite> (Vaswani et al., 2017)</cite> featured by non-local operations (Wang et al., 2018) with two * The work was done when Yaoyiran was working at Living Analytics Research Centre, Singapore Management University who is now a PhD student at University of Cambridge. input branches and tailor the transformer's multihead attention mechanism to the needs of information exchange between these two parallel branches. A higher-level and more abstract paradigm generalized from CCNs is denoted as \"Two-Headed Monster\" (THM), representing a broader class of neural structure benefiting from two parallel neural channels that would be intertwined with each other through, for example, co-attention mechanism as illustrated in Fig. 1 .",
  "y": "extends"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_2",
  "x": "We basically follow the definition of no-local operation in (Wang et al., 2018) where f : is a unary function and C : R d \u00d7 R n\u00d7d \u2192 R calculates a normalizer, but dispense with the assumption that then the non-local operation degrades to the multihead self-attention as is described in<cite> (Vaswani et al., 2017)</cite> (formula 2 describes only one attention head):",
  "y": "uses"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_3",
  "x": "Based on the transformer model<cite> (Vaswani et al., 2017)</cite> , we design a novel co-attention mechanism. Our proposed mechanism consists of two symmetrical branches that work in parallel to assimilate information from two input channels respectively. Different from previously known coattention mechanisms such as (Xiong et al., 2017; Lu et al., 2016a) , our co-attention is built through connecting two multiplicative attention modules<cite> (Vaswani et al., 2017</cite> ) each containing three gates, i.e., Value, Key and Query.",
  "y": "uses"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_4",
  "x": "Different from previously known coattention mechanisms such as (Xiong et al., 2017; Lu et al., 2016a) , our co-attention is built through connecting two multiplicative attention modules<cite> (Vaswani et al., 2017</cite> ) each containing three gates, i.e., Value, Key and Query. The information flows from two input channels then interact with and benefit from each other via crossed connections. Suppose the input fed into the left branch is X Lef t , and the right branch X right .",
  "y": "extends uses"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_5",
  "x": "Based on the transformer model<cite> (Vaswani et al., 2017)</cite> , we design a novel co-attention mechanism. Our proposed mechanism consists of two symmetrical branches that work in parallel to assimilate information from two input channels respectively. Different from previously known coattention mechanisms such as (Xiong et al., 2017; Lu et al., 2016a) , our co-attention is built through connecting two multiplicative attention modules<cite> (Vaswani et al., 2017</cite> ) each containing three gates, i.e., Value, Key and Query.",
  "y": "extends"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_6",
  "x": "Based on the transformer model<cite> (Vaswani et al., 2017)</cite> , we design a novel co-attention mechanism. Our proposed mechanism consists of two symmetrical branches that work in parallel to assimilate information from two input channels respectively. Different from previously known coattention mechanisms such as (Xiong et al., 2017; Lu et al., 2016a) , our co-attention is built through connecting two multiplicative attention modules<cite> (Vaswani et al., 2017</cite> ) each containing three gates, i.e., Value, Key and Query.",
  "y": "extends"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_7",
  "x": "Attention: Multi-head self-attention has demonstrated its capacity in neural transduction models<cite> (Vaswani et al., 2017)</cite> , language model pre-training (Devlin et al., 2018; Radford et al., 2018) and speech synthesis (Yang et al., 2019c) . While the novel attention mechanism, eschewing recurrence, is famous for modeling global dependencies and considered faster than recurrent layers<cite> (Vaswani et al., 2017)</cite> , recent work points out that it may tend to overlook neighboring information (Yang et al., 2019a; Xu et al., 2019) . It is found that applying an adaptive attention span could be conducive to character level language modeling tasks (Sukhbaatar et al., 2019) .",
  "y": "background"
 },
 {
  "id": "74db2b52e81969742f8f7e5681bd2b_8",
  "x": "**RELATED WORK** Attention: Multi-head self-attention has demonstrated its capacity in neural transduction models<cite> (Vaswani et al., 2017)</cite> , language model pre-training (Devlin et al., 2018; Radford et al., 2018) and speech synthesis (Yang et al., 2019c) . While the novel attention mechanism, eschewing recurrence, is famous for modeling global dependencies and considered faster than recurrent layers<cite> (Vaswani et al., 2017)</cite> , recent work points out that it may tend to overlook neighboring information (Yang et al., 2019a; Xu et al., 2019) .",
  "y": "background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_0",
  "x": "---------------------------------- **INTRODUCTION** Natural language to code generation, a subtask of semantic parsing, is the problem of converting natural language (NL) descriptions to code (Ling et al., 2016;<cite> Yin and Neubig, 2017</cite>; Rabinovich et al., 2017) .",
  "y": "background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_1",
  "x": "A number of neural network approaches have been proposed to solve this task. Sequential approaches (Ling et al., 2016; Jia and Liang, 2016; Locascio et al., 2016) convert the target code into a sequence of symbols and apply a sequence-tosequence model, but this approach does not ensure that the output will be syntactically correct. 1 Code available at https://github.com/ sweetpeach/ReCode Tree-based approaches<cite> (Yin and Neubig, 2017</cite>; Rabinovich et al., 2017) represent code as Abstract Syntax Trees (ASTs), which has proven effective in improving accuracy as it enforces the well-formedness of the output code.",
  "y": "background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_2",
  "x": "1 Code available at https://github.com/ sweetpeach/ReCode Tree-based approaches<cite> (Yin and Neubig, 2017</cite>; Rabinovich et al., 2017) represent code as Abstract Syntax Trees (ASTs), which has proven effective in improving accuracy as it enforces the well-formedness of the output code. However, representing code as a tree is not a trivial task, as the number of nodes in the tree often greatly exceeds the length of the NL description. As a result, tree-based approaches are often incapable of generating correct code for phrases in the corresponding NL description that have low frequency in the training data.",
  "y": "motivation background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_3",
  "x": "---------------------------------- **SYNTACTIC CODE GENERATION** Given an NL description q, our purpose is to generate code (e.g. Python) represented as an AST a. In this work, we start with the syntactic code gen-eration model by<cite> Yin and Neubig (2017)</cite> , which uses sequences of actions to generate the AST before converting it to surface code.",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_4",
  "x": "The generation process follows a preorder traversal starting with the root node. Figure 1 shows an action tree for the example code: the nodes correspond to actions per time step in the construction of the AST. Interested readers can reference<cite> Yin and Neubig (2017)</cite> for more detail of the neural model, which consists of a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) encoder-decoder with action embeddings, context vectors, parent feeding, and a copy mechanism using pointer networks.",
  "y": "background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_5",
  "x": "N -gram subtrees from all retrieved sentences are assigned a score, based on the best similarity score<cite> Yin and Neubig (2017)</cite> of all instances where they appeared. We normalize the scores for each input sentence by subtracting the average over the training dataset. At decoding time, incorporate these retrievalderived scores into beam search: for a given time step, all actions that would result in one of the retrieved n-grams u to be in the prediction tree has its log probability log(p(y t | y t\u22121 1 )) increased by \u03bb * score(u) where \u03bb is a hyperparameter, and score(u) is the maximal sim(q, q m ) from which u is extracted.",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_6",
  "x": "---------------------------------- **DATASETS AND EVALUATION METRICS** We evaluate RECODE with the Hearthstone (HS) (Ling et al., 2016) and Django (Oda et al., 2015) datasets, as preprocessed by<cite> Yin and Neubig (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_7",
  "x": "Table  1 summarizes dataset statistics. For evaluation metrics, we use accuracy of exact match and the BLEU score following<cite> Yin and Neubig (2017)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_8",
  "x": "---------------------------------- **EXPERIMENTS** For the neural code generation model, we use the settings explained in<cite> Yin and Neubig (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_9",
  "x": "For the retrieval method, we tuned hyperparameters and achieved best result when we set n max = 4 and \u03bb = 3 for both datasets 3 . For HS, we set M = 3 and M = 10 for Django. We compare our model with<cite> Yin and Neubig (2017)</cite>'s model that we call YN17 for brevity, and a sequence-to-sequence (SEQ2SEQ) model that we implemented.",
  "y": "uses"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_10",
  "x": "**DISCUSSION AND ANALYSIS** From our observation and as mentioned in Rabinovich et al. (2017) , HS contains classes with similar structure, so the code generation task could be simply matching the tree structure and filling the terminal tokens with correct variables and values. However, when the code consists of complex logic, partial implementation errors occur, leading to low exact match accuracy<cite> (Yin and Neubig, 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_11",
  "x": "However, when the code consists of complex logic, partial implementation errors occur, leading to low exact match accuracy<cite> (Yin and Neubig, 2017)</cite> . Analyzing our result, we find this intuition to be true not only for HS but also for Django. Examining the generated output for the Django dataset in Table 3 , we can see that in the first example, our retrieval model can successfully generate the correct code when YN17 fails.",
  "y": "similarities"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_12",
  "x": "The third example shows where our method fails to apply the correct action as it cannot cast s to str type while YN17 can at least cast s into a type (bool). Another common type of error that we found RECODE's generated outputs is incorrect variable copying, similarly to what is discussed in<cite> Yin and Neubig (2017)</cite> and Rabinovich et al. (2017) . Table 4 presents a result on the HS dataset 4 .",
  "y": "similarities"
 },
 {
  "id": "76476d80e1d3f65818592ec4caab0e_13",
  "x": "Some attempts using neural networks have used sequence-to-sequence models (Ling et al., 2016) or tree-based architectures (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017) . Ling et al. (2016) ; Jia and Liang (2016) ; Locascio et al. (2016) treat semantic parsing as a sequence generation task by linearizing trees. The closest work to ours are<cite> Yin and Neubig (2017)</cite> and Rabinovich et al. (2017) which represent code as an AST.",
  "y": "similarities"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_0",
  "x": "Traditional methods of manually constructing taxonomies by experts (e.g. WordNet) and interest communities (e.g. Wikipedia) are either knowledge or time intensive, and the results have limited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014;<cite> Bansal et al., 2014</cite>; Tuan et al., 2015) .",
  "y": "background"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_1",
  "x": "On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014;<cite> Bansal et al., 2014</cite>; Tuan et al., 2015) . These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; . On the other hand, researchers have built visual hierarchies by utilizing only visual features (Griffin and Perona, 2008; Yan et al., 2015; Sivic et al., 2008) . The resulting hierarchies are limited in interpretability and usability for knowledge transfer. Hence, we propose to combine both visual and textual knowledge to automatically build taxonomies.",
  "y": "motivation extends"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_2",
  "x": "Fig 1 il lustrates this via an example. The parent category seafish and its two child categories shark and ray are closely related as: (1) there is a hypernym-hyponym (is-a) relation between the words \"seafish\" and \"shark\"/\"ray\" through text descriptions like \"...seafish, such as shark and ray...\", \"...shark and ray are a group of seafish...\"; (2) images of the close neighbors, e.g., shark and ray are usually visually similar and images of the child, e.g. shark/ray are similar to a subset of images of seafish. To effectively capture these patterns, in contrast to previous works that rely on various hand-crafted features<cite> Bansal et al., 2014)</cite> , we extract features by leveraging the distributed representations that embed images (Simonyan and Zisserman, 2014) and words as compact vectors, based on which the semantic closeness is directly measured in vector space.",
  "y": "differences"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_3",
  "x": "---------------------------------- **RELATED WORK** Many approaches have been recently developed that build hierarchies purely by identifying either lexical patterns or statistical features in text corpora (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Zhu et al., 2013; Fu et al., 2014;<cite> Bansal et al., 2014</cite>; Tuan et al., 2014; Tuan et al., 2015; Kiela et al., 2015) .",
  "y": "background"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_4",
  "x": "Different from them, our model is discriminatively trained with multi-modal data. The works of Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> use similar language-based features as ours. Specifically, in (Fu et al., 2014) , linguistic regularities between pretrained word vectors are modeled as projection mappings.",
  "y": "similarities"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_5",
  "x": "In<cite> (Bansal et al., 2014)</cite> , a structural learning model is developed to induce a globally optimal hierarchy. Compared with this work, we exploit much richer features from both text and images, and leverage distributed representations instead of hand-crafted features. Several approaches (Griffin and Perona, 2008; Bart et al., 2008; Marsza\u0142ek and Schmid, 2008) have also been proposed to construct visual hierarchies from image collections.",
  "y": "background"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_6",
  "x": "In<cite> (Bansal et al., 2014)</cite> , a structural learning model is developed to induce a globally optimal hierarchy. Compared with this work, we exploit much richer features from both text and images, and leverage distributed representations instead of hand-crafted features. Several approaches (Griffin and Perona, 2008; Bart et al., 2008; Marsza\u0142ek and Schmid, 2008) have also been proposed to construct visual hierarchies from image collections.",
  "y": "differences"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_7",
  "x": "To apply the model to discover the underlying taxonomy from a given set of categories, we first obtain the marginals of z by averaging over the samples generated through eq 3, then output the optimal taxonomy z * by finding the maximum spanning tree (MST) using the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965;<cite> Bansal et al., 2014)</cite> . Training. We need to learn the model parameters w l of each layer l, which capture the relative importance of different features.",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_8",
  "x": "Word surface features. In addition to the embedding-based features, we further leverage lexical features based on the surface forms of child/parent category names. Specifically, we employ the Capitalization, Ends with, Contains, Suffix match, LCS and Length different features, which are commonly used in previous works in taxonomy induction (Yang and Callan, 2009;<cite> Bansal et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_9",
  "x": "We then compare our model with previous state-of-the-art methods (Fu et al., 2014;<cite> Bansal et al., 2014)</cite> with two taxonomy induction tasks. Finally, we provide analysis on the weights and taxonomies induced. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_10",
  "x": "(2) Different from the previous one, the hierarchy construction task is designed to test the generalization ability of our model, i.e. whether our model can learn statistical patterns from one hierarchy and transfer the knowledge to build a taxonomy for another collection of out-of-domain labels. Specifically, we select two trees as the training set to learn w. In the test phase, the model is required to build the full taxonomy from scratch for the third tree. We use Ancestor F 1 as our evaluation metric (Kozareva and Hovy, 2010; Navigli et al., 2011;<cite> Bansal et al., 2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_11",
  "x": "Specifically, we measure F 1 = 2P R/(P + R) values of predicted \"is-a\" relations where the precision (P) and recall (R) are: We compare our method to two previously state-of-the-art models by Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> , which are closest to ours. Table 2 : Comparisons among different variants of our model, Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> on two tasks.",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_12",
  "x": "We compare our method to two previously state-of-the-art models by Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> , which are closest to ours. Table 2 : Comparisons among different variants of our model, Fu et al. (2014) and <cite>Bansal et al. (2014)</cite> on two tasks. The ancestor-F 1 scores are reported.",
  "y": "uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_13",
  "x": "We compare the following methods: (1) Fu2014, (2) Ours (L), and (3) Ours (LV), as described above; (4) Bansal2014: The model by <cite>Bansal et al. (2014)</cite> retrained using our dataset; (5) Ours (LB): By excluding visual features, but including other language features from <cite>Bansal et al. (2014)</cite> ; (6) Ours (LVB): Our full model further enhanced with all semantic features from <cite>Bansal et al. (2014)</cite> ; (7) Ours (LVB -E): By excluding word embeddingbased language features from Ours (LVB). As shown, on the hierarchy construction task, our model with only language features still outperforms Fu2014 with a large gap (0.30 compared to 0.18 when h = 7), which uses similar embeddingbased features. The potential reasons are two-fold.",
  "y": "differences uses"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_14",
  "x": "However, when introducing visual features, our performance is comparable (pvalue = 0.058).Furthermore, if we discard visual features but add semantic features from <cite>Bansal et al. (2014)</cite> , we achieve a slight improvement of 0.02 over Bansal2014 (p-value = 0.016), which is largely attributed to the incorporation of word embedding-based features that encode high-level linguistic regularity. Finally, if we enhance our full model with all semantic features from <cite>Bansal et al. (2014)</cite> , our model outperforms theirs by a gap of 0.04 (p-value < 0.01), which justifies our intuition that perceptual semantics underneath visual contents are quite helpful. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "7700b6c3c096d5cd7999c34e7614f7_15",
  "x": "Relevance of textual and visual features v.s. depth of tree. Compared to <cite>Bansal et al. (2014)</cite> , a major difference of our model is that different layers of the taxonomy correspond to different weights w l , while in<cite> (Bansal et al., 2014)</cite> all layers share the same weights. Intuitively, introducing layer-wise w not only extends the model capacity, but also differentiates the importance of each feature at different layers.",
  "y": "differences"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_0",
  "x": "**PREVIOUS RESEARCH** Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies (Mihalcea and Strapparava, 2005; Purandare and Litman, 2006; <cite>Yang et al., 2015</cite>) , humor recognition was modeled as a binary classification task.",
  "y": "background"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_1",
  "x": "Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work (<cite>Yang et al., 2015</cite>) , a new corpus was constructed from the Pun of the Day website. <cite>Yang et al. (2015)</cite> explained and computed stylistic features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style.",
  "y": "background"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_2",
  "x": "From the brief review, it is clear that there is a great need for an open corpus that can support investigating humor in presentations. 1 CNNbased text categorization methods have been applied to humor recognition (e.g., in (Bertero and Fung, 2016b) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in <cite>Yang et al. (2015)</cite> is missing; (b) CNN's performance in the previous research is not quite clear; and (c) some important techniques that can improve CNN performance (e.g., using varied-sized filters and dropout regularization (Hinton et al., 2012)) were not applied. Therefore, the present study is meant to address these limitations.",
  "y": "background"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_3",
  "x": "1 CNNbased text categorization methods have been applied to humor recognition (e.g., in (Bertero and Fung, 2016b) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in <cite>Yang et al. (2015)</cite> is missing; (b) CNN's performance in the previous research is not quite clear; and (c) some important techniques that can improve CNN performance (e.g., using varied-sized filters and dropout regularization (Hinton et al., 2012)) were not applied. Therefore, the present study is meant to address these limitations. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_4",
  "x": "Following Mihalcea and Strapparava (2005) and <cite>Yang et al. (2015)</cite> , we selected the same numbers (n = 4726) of 'Laughter' and 'NoLaughter' sentences. To minimize possible topic shifts between positive and negative instances, for each positive instance, we randomly picked one negative instance nearby (the context window was 7 sentences in this study). For example, in Figure 1 , a negative instance (corresponding to 'sent-2') was selected from the nearby sentences ranging from 'sent-7' to 'sent+7'.",
  "y": "uses"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_5",
  "x": "**CONVENTIONAL MODEL** Following <cite>Yang et al. (2015)</cite> , we applied Random Forest (Breiman, 2001 ) to perform humor recognition by using the following two groups of features. The first group are humor-specific stylistic features covering the following 4 categories 4 : Incongruity (2), Ambiguity (6), Interpersonal Effect (4), and Phonetic Pattern (4).",
  "y": "uses"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_6",
  "x": "Note that we normalized words in the Pun data to lowercase to avoid a possibly elevated result caused by a special pattern: in the original format, all negative instances started with capital letters. The Pun data allows us to verify that our implementation of the conventional model is consistent with the work reported in <cite>Yang et al. (2015)</cite> . In our experiment, we firstly divided each corpus into two parts.",
  "y": "similarities"
 },
 {
  "id": "780b96afa8d417aa241e01ad594ce9_7",
  "x": "After running 200 iterations of tweaking, we ended up with the following selection: f w is 6 (entailing that the various filter sizes are (5, 6, 7)), n f is 100, dropout 1 is 0.7 and dropout 2 is 0.35, optimization uses Adam (Kingma and Ba, 2014) . When training the CNN model, we randomly selected 10% of the training data as the validation set for using early stopping to avoid over-fitting. On the Pun data, the CNN model shows consistent improved performance over the conventional model, as suggested in <cite>Yang et al. (2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_1",
  "x": "Complex predicates (CPs), 'predicates which are multi-headed: they are composed of more than one grammatical element' (Ramisch, 2012) , are most relevant in the context of SRL. Light verb constructions (LVCs), e.g. take care, and verb particle constructions (VPCs), e.g. watch out, are the most frequently occurring types of CPs. <cite>As Bonial et al. (2014)</cite> stated 'PB has previously treated language as if it were purely compositional, and has therefore lumped the majority of MWEs in with lexical verb usages'.",
  "y": "background"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_2",
  "x": "Recently, <cite>Bonial et al. (2014)</cite> have introduced an approach to improve the handling of MWEs in PB while keeping annotation costs low. The process is called <cite>aliasing</cite>. Instead of creating new frames for CPs, human annotators map them to existing PB rolesets which encompass the same semantic and argument structure.",
  "y": "background"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_4",
  "x": "In addition, we set up an annotation effort to gather a frequency-balanced, data-driven evaluation set that is larger and more diverse than the annotated set provided by <cite>Bonial et al. (2014)</cite> . ---------------------------------- **REPRESENTING CPS FOR SRL**",
  "y": "differences"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_5",
  "x": "Atkins et al. (2003) describe a way in which LVCs can be annotated in FrameNet (Baker et al., 1998) , a framework that describes the semantic argument structure of predicates with semantic roles specific to the meaning of the predicate. In contrast to the proposals for PB by Hwang et al. (2010) and Duran et al. (2011) , they suggest to annotate the light verb and its counterpart separately. The <cite>aliasing</cite> process introduced by <cite>Bonial et al. (2014)</cite> tries to extend the coverage of PB for CPs while keeping the number of rolesets that should be newly created to a minimum.",
  "y": "background"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_6",
  "x": "In contrast to the proposals for PB by Hwang et al. (2010) and Duran et al. (2011) , they suggest to annotate the light verb and its counterpart separately. The <cite>aliasing</cite> process introduced by <cite>Bonial et al. (2014)</cite> tries to extend the coverage of PB for CPs while keeping the number of rolesets that should be newly created to a minimum. <cite>Bonial et al. (2014)</cite> conducted a pilot study re-annotating 138 CPs involving the verb take.",
  "y": "background"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_7",
  "x": "2 According to this process, take care could be aliased to the existing PB roleset care.01 whose entry is shown in Figure 3 . This alias replaces (take+care).01 shown in Figure 2 and thus avoids the creation of a new roleset. Roleset id: care.01, to be concerned Arg0: carer, agent Arg1: thing cared for/about Encouraged by the high proportion of CPs that could successfully be aliased in the pilot study by <cite>Bonial et al. (2014)</cite> , we created a method to automatically find aliases for CPs in order to decrease the amount of human intervention, thereby scaling up the coverage of CPs in PB.",
  "y": "motivation"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_8",
  "x": "Human Annotation. In order to evaluate our system, we set up an annotation effort loosely following the guidelines provided by <cite>Bonial et al. (2014)</cite> . We selected 50 LVCs and 50 VPCs from the Wiki50 corpus (Vincze et al., 2011) divided equally over two frequency groups: Half of the expressions occur only once in the Wiki50 corpus (low-frequency subgroup) and the other half occur at least twice (high-frequency subgroup).",
  "y": "extends"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_9",
  "x": "The annotators were requested to divide these cases into semantically compositional CPs (e.g. obtain permission with the roleset obtain.01) and uncompositional CPs for which PB already provides a multi-word predicate (e.g. open.03 for open up). For the remaining CPs, they were asked to suggest PB rolesets (aliases) that share the same semantics and argument structure as the CP. The simple inter-annotator agreement 5 was 67% for annotator A%B, 51% for A&C and 44% for A&D. These agreement figures are higher than the figures in <cite>Bonial et al. (2014)</cite> , and actual agreement is probably even higher, because synonymous rolesets are regarded as disagreements.",
  "y": "differences"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_10",
  "x": "Annotator A discussed the annotations with the other annotators and they were able to reach a consensus that resulted in a final agreed-upon test set. Table 1 shows the final decisions with respect to the complete set of 197 expressions. In line with the results from <cite>Bonial et al. (2014)</cite> who aliased 100 out of 138 uncompositional take MWEs, we were also able to alias most of the CPs in our annotation set.",
  "y": "similarities"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_11",
  "x": "In addition, we evaluated our system on the dataset from <cite>Bonial et al. (2014)</cite> , restricted to the type of CP our system handles (LVCs and VPCs) and verb aliases (as opposed to aliases being a noun or adjective roleset). We used 70 of the 100 MWEs from their annotations. Evaluation Measures and Baseline.",
  "y": "uses"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_12",
  "x": "---------------------------------- **RESULTS AND DISCUSSION** We evaluated our approach on the 160 CPs annotated in the course of this work (Wiki50 set), as well as on the 70 take CPs from <cite>Bonial et al. (2014)</cite> (take set) and compare our results to the baseline.",
  "y": "uses"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_13",
  "x": "---------------------------------- **CONCLUSIONS** We have presented an approach to handle CPs in SRL that extends on work from <cite>Bonial et al. (2014)</cite> .",
  "y": "extends"
 },
 {
  "id": "78afdf391c70d7992200b4071e4ac2_14",
  "x": "We automatically link VPCs and LVCs to the PB roleset that best describes their meaning, by relying on word alignments in parallel corpora and distributional methods. We set up an annotation effort to gather a frequency-balanced, contextualized evaluation set that is more natural, varied and larger than the pilot annotations provided by <cite>Bonial et al. (2014)</cite> . Our method can be used to alleviate the manual annotation effort by providing a correct alias in 44% of the cases (up to 72% for the more frequent test items when taking synonymous rolesets into account).",
  "y": "extends"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_0",
  "x": "Recently, the augmentation of neural networks with external memory components has been shown to be a powerful means of capturing context of different types (Graves et al., 2014 Rae et al., 2016) . Of particular interest to this work is the work by<cite> Sukhbaatar et al. (2015)</cite> , on end-toend memory networks (N2Ns), which exhibit remarkable reasoning capabilities, e.g. for reasoning and goal-oriented dialogue tasks . Typically, such tasks consist of three key components: a sequence of supporting facts (the story), a question, and its answer.",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_1",
  "x": "Recently, the augmentation of neural networks with external memory components has been shown to be a powerful means of capturing context of different types (Graves et al., 2014 Rae et al., 2016) . Of particular interest to this work is the work by<cite> Sukhbaatar et al. (2015)</cite> , on end-toend memory networks (N2Ns), which exhibit remarkable reasoning capabilities, e.g. for reasoning and goal-oriented dialogue tasks . Typically, such tasks consist of three key components: a sequence of supporting facts (the story), a question, and its answer.",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_2",
  "x": "One drawback of N2Ns is the problem of choosing between two types of weight tying (adjacent and layer-wise; see Section 2 for a technical description). While N2Ns generally work well with either weight tying approach, as reported in<cite> Sukhbaatar et al. (2015)</cite> , the performance is uneven on some difficult tasks. That is, for some tasks, one weight tying approach attains nearperfect accuracy and the other performs poorly, but for other tasks, this trend is reversed.",
  "y": "motivation"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_3",
  "x": "**RELATED WORK** End-to-End Memory Networks: Building on top of memory networks ,<cite> Sukhbaatar et al. (2015)</cite> ing the memory position supervision and making the model trainable in an end-to-end fashion, through the advent of supporting memories and a memory access controller. Representations of the context sentences x 1 , . . . , x n in the story are encoded using two sets of embedding matrices A and C (both of size d \u21e5 |V | where d is the embedding size and |V | the vocabulary size), and stored in the input and output memory cells m 1 , . . . , m n and c 1 , . . . , c n , each of which is obtained via",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_4",
  "x": "Once the attention weights have been computed, the memory access controller receives the response o in the form of a weighted sum over the output memory representations: To enhance the model's ability to cope with more challenging tasks requiring multiple supporting facts from the memory,<cite> Sukhbaatar et al. (2015)</cite> further extended the model by stacking multiple memory layers (also known as \"hops\"), in which case the output of the k th hop is taken as input to the (k + 1) th hop: Lastly, N2N predicts the answer to question q using a softmax function: where\u0177 is the predicted answer distribution, W 2 R |V |\u21e5d is a parameter matrix for the model to learn (note that in the context of bAbI tasks, answers are single words), and K is the total number of hops.",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_5",
  "x": "**CURRENT ISSUES AND MOTIVATION:** In<cite> Sukhbaatar et al. (2015)</cite> , two types of weight tying were explored for N2N, namely adjacent (\"ADJ\") and layer-wise (\"LW\"). With LW, the input and output embedding matrices are shared across different hops (i.e., A 1 = A 2 = . . . = A K and C 1 = C 2 = . . . = C K ), resembling RNNs.",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_6",
  "x": "Based on this observation, we propose a unified weight tying mechanism exploiting the benefits of both ADJ and LW, and capable of dynamically determining the best weight tying approach for a given task. Table 1 : Accuracy (%) reported in<cite> (Sukhbaatar et al., 2015)</cite> on a selected subset of the 20 bAbI 10k tasks. Note that performance in the LW column is obtained with a larger embedding size d = 100 and ReLU non-linearity applied to the internal state after each hop.",
  "y": "background"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_7",
  "x": "Moreover, following<cite> Sukhbaatar et al. (2015)</cite> , we add a linear mapping H 2 R d\u21e5d to the update connection between memory hops, but in our case, down-weight it by 1 z, resulting in: Regularisation: In order to prevent the input and output embedding matrices A k and C k from being dominated by the unconstrained embedding matrices, it is necessary to restrain the magnitude of the values in\u00c3 1 andC k . Therefore, in addition to the cross entropy loss over N training instances:",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_8",
  "x": "Training Details: Following<cite> Sukhbaatar et al. (2015)</cite>, we hold out 10% of the bAbI training set to form a development set. Position encoding and temporal encoding (with 10% random noise) are also incorporated into the model. Training is performed over 100 epochs with a batch size of 32 using the Adam optimiser (Kingma and Ba, 2015) with a learning rate of 0.005.",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_9",
  "x": "Training is performed over 100 epochs with a batch size of 32 using the Adam optimiser (Kingma and Ba, 2015) with a learning rate of 0.005. Following<cite> Sukhbaatar et al. (2015)</cite> , linear start is employed in all our experiments for the first 20 epochs. All weight parameters are initialised based on a Gaussian distribution with zero mean and = 0.1.",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_10",
  "x": "All weight parameters are initialised based on a Gaussian distribution with zero mean and = 0.1. Gradients with an`2 norm of 40 are divided by a scalar to have norm 40. Also following<cite> Sukhbaatar et al. (2015)</cite> , we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and to",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_11",
  "x": "---------------------------------- **0.001.** Consistent with other published results over bAbI<cite> (Sukhbaatar et al., 2015</cite>; Seo et al., 2017) , we repeat training 30 times for each task, and select the model which performs best on the development set.",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_12",
  "x": "**RESULTS** The results on the 20 bAbI QA tasks are presented in Table 3 . We benchmark against other memory network based models: (1) N2N with ADJ and LW<cite> (Sukhbaatar et al., 2015)</cite> ; (2) DMN (Kumar et al., 2016) and its improved version DMN+ (Xiong et al., 2016) ; and (3) GN2N (Liu and Perez, 2017) .",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_13",
  "x": "We refer to these 7 features as the match features. In terms of the training procedure, experiments are carried out with the same configuration as described in Section 4.1. As a large variance can be observed due to how sensitive memory-based models are to parameter initialisation, following<cite> (Sukhbaatar et al., 2015)</cite> and (Liu and Perez, 2017) , we repeat each training 10 times using the Table 4 : Per-response accuracy on the Dialog bAbI tasks.",
  "y": "uses"
 },
 {
  "id": "7a2f56cb4bbcd09ba35934ca76c9a9_14",
  "x": "The results on the Dialog bAbI tasks are shown in Table 4 . In terms of baselines, we benchmark against other memory network-based models: 4 (1) N2N<cite> (Sukhbaatar et al., 2015)</cite> ; and (2) GN2N<cite> (Sukhbaatar et al., 2015)</cite> . While the results of GN2N is achieved with ADJ, the type of weight tying for N2N is not reported in .",
  "y": "uses"
 },
 {
  "id": "7b4a976ba6a43b5ba42cc350b4d132_0",
  "x": "Recent work has been inspired by efforts in improving model's interpretability in image processing tasks, in particular by the Layerwise Relevance Propagation (LRP) [3] . In LRP, the classification decision of a deep neural network is decomposed backward across the network layers and evidence about the contribution to the final decision brought by individual input fragments (i.e., pixels of the input image) is gathered. We propose here to extend the LRP application to a linguistically motivated network architecture, known as <cite>Kernel-Based Deep Architecture</cite> (<cite>KDA</cite>) <cite>[5]</cite> , which frames semantic information captured by linguistic Tree Kernel [2] methods within the neural-based learning paradigm.",
  "y": "uses"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_0",
  "x": "We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006;<cite> Galley et al., 2006</cite>; May and Knight, 2007) , as opposed to formally syntactic systems such as Hiero (Chiang, 2007) . The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given. Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs.",
  "y": "uses"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_1",
  "x": "The first is synchronous parsing <cite>(Galley et al., 2006</cite>; May and Knight, 2007) , where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up. The other way is the TTS transducer (Liu et al., 2006; Huang et al., 2006) , where TTS templates are used just as their name indicates: to transform a source parse tree (or forest) into the proper target string. Since synchronous parsing considers all possible synchronous parse trees of the source sentence, it is less constrained than TTS transducers and hence requires more computational power.",
  "y": "background"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_2",
  "x": "The first is synchronous parsing <cite>(Galley et al., 2006</cite>; May and Knight, 2007) , where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up. In this paper, we use a TTS transducer to test the performance of different TTS templates, but our techniques could also be applied to SSMT systems based on synchronous parsing.",
  "y": "similarities background"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_3",
  "x": "To keep the number of TTS templates to a manageable scale, only the non-decomposable TTS templates are generated. This algorithm is referred to as GHKM (Galley et al., 2004) and is widely used in SSMT systems <cite>(Galley et al., 2006</cite>; Liu et al., 2006; Huang et al., 2006) . The word alignment used in GHKM is usually computed independent of the syntactic structure, and as DeNero and Klein (2007) and May and Knight (2007) have noted,",
  "y": "similarities"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_4",
  "x": "The other one is normalization based on the root of the LHS (ROOTN)<cite> (Galley et al., 2006)</cite> , corresponding to the generative process where, given the root of the syntax subtree, the LHS syntax subtree and the RHS string are generated simultaneously. By omitting the decomposition probability in the LHS-based generative model, the two generative models share the same formula for computing the probability of a training instance: where T and S denote the source syntax tree and target string respectively, R denotes the decomposition of (T, S), and t denotes the TTS template.",
  "y": "background"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_5",
  "x": "The expected counts of the TTS templates can then be efficiently computed using an inside-outsidelike dynamic programming algorithm (May and Knight, 2007) . LHSN, as shown by<cite> Galley et al. (2006)</cite> , cannot accurately restore the true conditional probabilities of the target sentences given the source sentences in the training corpus. This indicates that LHSN is not good at predicting unseen sentences or at translating new sentences.",
  "y": "background"
 },
 {
  "id": "7b69c68a602e90c7ee7b9fa8a8facf_6",
  "x": "LHSN, as shown by<cite> Galley et al. (2006)</cite> , cannot accurately restore the true conditional probabilities of the target sentences given the source sentences in the training corpus. 4 Because the two normalization methods have their 4 Based on LHSN, the difference between the probability of a big Template and the product of the probabilities of E-step: for all pair of syntax tree T and target string S do for all TTS Template t do EC(t)+ = (Rose et al., 1992 ) is is used in our system to speed up the training process, similar to Goldwater et al. (2006) .",
  "y": "uses"
 },
 {
  "id": "7c3f94a231c83c94b5d93c33ab8bfa_0",
  "x": "---------------------------------- **INTRODUCTION** This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b) , which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation <cite>(Zhang and Clark, 2007)</cite> , dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012) , context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013) , combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013) , achieving stateof-the-art accuracies and efficiencies.",
  "y": "background"
 },
 {
  "id": "7c3f94a231c83c94b5d93c33ab8bfa_1",
  "x": "We start with a detailed introduction of the framework, describing the averaged perceptron algorithm (Collins, 2002) and its efficient implementation issues <cite>(Zhang and Clark, 2007)</cite> , as well as beam-search and the early-update strategy (Collins and Roark, 2004) . We then illustrate how the framework can be applied to NLP tasks, including word segmentation, joint segmentation & POS-tagging, labeled and unlabeled dependency parsing, joint POS-tagging and dependency parsing, CFG parsing, CCG parsing, and joint segmentation, POS-tagging and parsing. In each case, we illustrate how the task is turned into an incremental left-to-right output-building process, and how rich features are defined to give competitive accuracies.",
  "y": "motivation"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_0",
  "x": "Sentence embeddings are distributed representations of natural language sentences with the intention to encode the meaning of the sentences in a neural network representation. Sentence embeddings have been generated using unsupervised learning approaches (e.g. Hill et al., 2016) , and supervised learning (e.g. Bowman et al., 2016;<cite> Conneau et al., 2017)</cite> . Sentence-level representations have shown promise in multiple different NLP tasks.",
  "y": "background"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_1",
  "x": "Some approaches focus on building sentence embeddings for the premises and the hypothesis separately and then combine those using a classifier (e.g. Bowman et al., 2015 Bowman et al., , 2016<cite> Conneau et al., 2017)</cite> . Other approaches do not treat the two sentences separately but utilize e.g. crosssentence attention (Tay et al., 2018; Chen et al., 2017a) . In this paper we focus on the sentence embedding approach.",
  "y": "background"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_2",
  "x": "Motivated by the success of the architecture of InferSent<cite> (Conneau et al., 2017)</cite> , we build a hierarchical architecture utilizing bidirectional LSTM (BiLSTM) layers and max pooling. All in all, our model in on par with the state of the art for Stanford Natural Language Inference corpus (SNLI) (Bowman et al., 2015) sentence encoding-based models and improves the previous state of the art for SciTail (Khot et al., 2018) . We also achieve strong results for the Multi-Genre Natural Language Inference corpus (MultiNLI) (Williams et al., 2018) .",
  "y": "motivation"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_3",
  "x": "All in all, our model in on par with the state of the art for Stanford Natural Language Inference corpus (SNLI) (Bowman et al., 2015) sentence encoding-based models and improves the previous state of the art for SciTail (Khot et al., 2018) . We also achieve strong results for the Multi-Genre Natural Language Inference corpus (MultiNLI) (Williams et al., 2018) . We also test our model on a number of transfer learning tasks using the SentEval testing library<cite> (Conneau et al., 2017)</cite> , and show that our model outperforms the InferSent model on 7 out of 10 and SkipThought on 8 out of 9 tasks, comparing to the scores reported by <cite>Conneau et al. (2017)</cite> .",
  "y": "differences uses"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_4",
  "x": "The basic idea behind these approaches is to encode the premise and hypothesis sentences separately and then combine those using a neural network classifier. <cite>Conneau et al. (2017)</cite> explore multiple different sentence embedding architectures ranging from LSTM, BiLSTM and intra-attention to convolution neural networks and the performance of these architectures on NLI tasks. They show that out of these models BiLSTM with max pooling achieves the strongest results in NLI.",
  "y": "background"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_5",
  "x": "Motivated by the strong results of the BiLSTM max pooling network by <cite>Conneau et al. (2017)</cite> , we experimented with combining BiLSTM max pooling networks as a hierarchical structure. 1 To improve the BiLSTM layers' ability to remember the input words, we let each layer of the stack re-read the input sentence. In our baseline model we stack three BiLSTM max pooling networks as a hierarchical structure, where each BiLSTM reads the input sentence as the input.",
  "y": "motivation extends"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_6",
  "x": "We report the results below. 3 We also conducted a linguistic error analysis and compared our results to the results obtained with the InferSent BiLSTM max pooling model of <cite>Conneau et al. (2017)</cite> (our implementation). 4 3 For more detailed error statistics, see the appendix.",
  "y": "uses"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_7",
  "x": "4 3 For more detailed error statistics, see the appendix. 4 The scores for our implementation of InferSent are on par or slightly higher than the scores reported by <cite>Conneau et al. (2017)</cite> using their training setup. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_8",
  "x": "---------------------------------- **TRANSFER LEARNING** To better understand how well our model generalizes to different tasks, we conducted additional transfer learning tests using the SentEval sentence embedding evaluation library 5<cite> (Conneau et al., 2017)</cite> and compared our results to the results published for InferSent and SkipThought .",
  "y": "uses"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_9",
  "x": "This allows us to compare our results to the InferSent results which were obtained using a model trained on the same data<cite> (Conneau et al., 2017)</cite> . <cite>Conneau et al. (2017)</cite> have shown that including all the training data from SNLI and MultiNLI improves significantly the model performance on transfer learning tasks, compared to training the model only on SNLI data. For training the model we used the same setup as described above in Section 4.",
  "y": "similarities"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_10",
  "x": "<cite>Conneau et al. (2017)</cite> have shown that including all the training data from SNLI and MultiNLI improves significantly the model performance on transfer learning tasks, compared to training the model only on SNLI data. For training the model we used the same setup as described above in Section 4. We used the SentEval sentence embedding evaluation library using the default settings 6 recommended on the SentEval website, with a logistic regression classifier, Adam optimizer with learning rate of 0.001, batch size of 64 and epoch size of 4.",
  "y": "background"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_13",
  "x": "Our model is still outperformed by and ESIM (Chen et al., 2017a) and KIM, an ESIM model incorporating external knowledge (Chen et al., 2018) . The results of the comparison are summarized in Glockner et al. (2018) . InferSent results obtained with our implementation using the architecture and training set-up described in<cite> (Conneau et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "7c8f54479ce1f9d81b49839425f58e_14",
  "x": "In all our experiments with the three datasets we used only the training data provided in the respective corpus. For the transfer learning tasks, described in Section 7, we used training data from both the SNLI and the MultiNLI datasets in order to compare to the results by <cite>Conneau et al. (2017)</cite> . ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_0",
  "x": "In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation -namely WordSim-353 (Finkelstein et al., 2001) , MEN (Bruni et al., 2014) and <cite>SimLex-999</cite> (<cite>Hill et al., 2015</cite>) . For comparison, Vector Cosine is also calculated on several countbased DSMs. We implement a total of twenty-eight models with different parameters settings, each of which differs according to corpus size, context window width, weighting scheme and SVD application.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_1",
  "x": "The new metric is shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (Bullinaria and Levy, 2012) , against which APSyn still obtains competitive performances. The results are also discussed in relation to the state-of-the-art DSMs, as reported in <cite>Hill et al. (2015)</cite> . In such comparison, the best settings of our models outperform the word embeddings in almost all datasets.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_2",
  "x": "Results prove its high performance also when calculated on large corpora, such as those used by . On top of the performance, APSyn seems not to be subject to some of the biases that affect Vector Cosine. Finally, considering the debate about the ability of DSMs to calculate genuine similarity as opposed to word relatedness (Turney, 2001; Agirre et al., 2009; <cite>Hill et al., 2015</cite>) , we test the ability of the models to quantify genuine semantic similarity.",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_3",
  "x": "---------------------------------- **DATASETS** For our evaluation, we used three widely popular datasets: WordSim-353 (Finkelstein et al., 2001) , MEN (Bruni et al., 2014) , <cite>SimLex-999</cite> (<cite>Hill et al., 2015</cite>) .",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_4",
  "x": "WordSim-353 (Finkelstein et al., 2001 ) was proposed as a word similarity dataset containing 353 pairs annotated with scores between 0 and 10. However, <cite>Hill et al. (2015)</cite> claimed that the instructions to the annotators were ambiguous with respect to similarity and association, so that the subjects assigned high similarity scores to entities that are only related by virtue of frequent association (e.g. coffee and cup; movie and theater). On top of it, WordSim-353 does not provide the POS-tags for the 439 words that it contains, forcing the users to decide which POS to assign to the ambiguous words (e.g. [white, rabbit] and [run, marathon] ).",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_5",
  "x": "Even though such a classification made a clear distinction between the two types of relations (i.e. similarity and association), <cite>Hill et al. (2015)</cite> argue that these gold standards still carry the scores they had in WordSim-353, which are known to be ambiguous in this regard. The MEN Test Collection (Bruni et al., 2014) includes 3,000 word pairs divided in two sets (one for training and one for testing) together with human judgments, obtained through Amazon Mechanical Turk. The construction was performed by asking subjects to rate which pair -among two of themwas the more related one (i.e. the most associated).",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_6",
  "x": "Every pairs-couple was proposed only once, and a final score out of 50 was attributed to each pair, according to how many times it was rated as the most related. According to <cite>Hill et al. (2015)</cite> , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association. <cite>SimLex-999</cite> is the dataset introduced by <cite>Hill et al. (2015)</cite> to address the above mentioned criticisms of confusion between similarity and association.",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_7",
  "x": "<cite>SimLex-999</cite> is the dataset introduced by <cite>Hill et al. (2015)</cite> to address the above mentioned criticisms of confusion between similarity and association. The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness. The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association.",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_8",
  "x": "<cite>Hill et al. (2015)</cite> claim that differently from other datasets, <cite>SimLex-999</cite> interannotator agreement has not been surpassed by any automatic approach. ---------------------------------- **STATE OF THE ART VECTOR SPACE MODELS**",
  "y": "background"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_9",
  "x": "---------------------------------- **STATE OF THE ART VECTOR SPACE MODELS** In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by <cite>Hill et al. (2015)</cite> , who used the code (or directly the embeddings) shared by the original authors.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_10",
  "x": "In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by <cite>Hill et al. (2015)</cite> , who used the code (or directly the embeddings) shared by the original authors. As we trained our models on almost the same corpora used by <cite>Hill and colleagues</cite>, the results are perfectly comparable. The three models we compare our results to are: i) the convolutional neural network of Collobert and Weston (2008) , which was trained on 852 million words of Wikipedia; ii) the neural network of Huang et al. (2012) , which was trained on 990 million words of Wikipedia; and iii) the word2vec of Mikolov et al. (2013) , which was trained on 1000 million words of Wikipedia and on the RCV Vol.",
  "y": "similarities"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_12",
  "x": "**DSMS** For our experiments, we implemented twenty-eight DSMs, but for reasons of space only sixteen of them are reported in the tables. All of them include the pos-tagged target words used in the three datasets (i.e. MEN, WordSim-353 and <cite>SimLex-999</cite>) and the pos-tagged contexts having frequency above 100 in the two corpora.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_13",
  "x": "Given the twenty-eight DSMs, for each dataset we have measured the Vector Cosine and APSyn between the words in the test pairs. Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets <cite>Simlex-999</cite>, WordSim-353 and MEN. In the bottom the performance of the state-of-the-art models of Collobert and Weston (2008) , Huang et al. (2012), Mikolov et al. (2013) , as reported in <cite>Hill et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_14",
  "x": "Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets <cite>Simlex-999</cite>, WordSim-353 and MEN. In the bottom the performance of the state-of-the-art models of Collobert and Weston (2008) , Huang et al. (2012), Mikolov et al. (2013) , as reported in <cite>Hill et al. (2015)</cite> . The Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2 .",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_15",
  "x": "In particular, Table 1 describes the performances on <cite>SimLex-999</cite>, WordSim-353 and MEN for the measures applied on RCV Vol. 1 models. Table 2 , instead, describes the performances of the measures on the three datasets for the Wikipedia models.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_16",
  "x": "1 and Wikipedia models, tested on the subsets of WordSim-353 extracted by Agirre et al. (2009) . Table 1 shows the Spearman correlation scores for Vector Cosine and APSyn on the three datasets for the eight most representative DSMs built using RCV Vol. 1. Table 2 does the same for the DSMs built using Wikipedia. For the sake of comparison, we also report the results of the state-of-the-art DSMs mentioned in <cite>Hill et al. (2015)</cite> (see Section 2.5).",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_17",
  "x": "As a note about iii), the results of using SVD jointly with LMI spaces are less predictable than when combining it with PPMI. Also, we can notice that the smaller window (i.e. 2) does not always perform better than the larger one (i.e. 3). The former appears to perform better on <cite>SimLex-999</cite>, while the latter seems to have some advantages on the other datasets.",
  "y": "differences"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_18",
  "x": "Also, we can notice that the smaller window (i.e. 2) does not always perform better than the larger one (i.e. 3). The former appears to perform better on <cite>SimLex-999</cite>, while the latter seems to have some advantages on the other datasets. This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in <cite>SimLex-999</cite> (i.e. genuine similarity).",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_19",
  "x": "This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in <cite>SimLex-999</cite> (i.e. genuine similarity). On top of it, despite <cite>Hill et al. (2015)</cite> 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (Agirre et al., 2009; Kiela and Clark, 2014) , we need to mention that window 5 was abandoned because of its low performance. With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by Schnabel et al. (2015) , using the words of the <cite>SimLex-999</cite> dataset as query words and collecting for each of them the top 1000 nearest neighbors.",
  "y": "differences"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_20",
  "x": "This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in <cite>SimLex-999</cite> (i.e. genuine similarity). On top of it, despite <cite>Hill et al. (2015)</cite> 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (Agirre et al., 2009; Kiela and Clark, 2014) , we need to mention that window 5 was abandoned because of its low performance. With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by Schnabel et al. (2015) , using the words of the <cite>SimLex-999</cite> dataset as query words and collecting for each of them the top 1000 nearest neighbors.",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_21",
  "x": "APSyn does not seem to be able to overcome such bias, which seems to be in fact an inherent property of the DSMs (Radovanovic et al., 2010) . Further investigation is needed to see whether variations of APSyn can tackle this problem. Finally, few words need to be spent with regard to the ability of calculating genuine similarity, as distinguished from word relatedness (Turney, 2001; Agirre et al., 2009; <cite>Hill et al., 2015</cite>) .",
  "y": "uses"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_22",
  "x": "Interestingly, our best models achieve results that are comparable to -or even better than -those reported by <cite>Hill et al. (2015)</cite> for the stateof-the-art word embeddings models. In Section 3.5 we show that APSyn is scalable, outperforming the state-of-the-art count-based models reported in . On top of it, APSyn does not suffer from some of the problems reported for the Vector Cosine, such as the inability of identifying the number of shared features.",
  "y": "similarities"
 },
 {
  "id": "7ce85e3c3f58cee33015409b74f99e_23",
  "x": "It still however seems to be affected by the hubness issue, and more research should be carried out to tackle it. Concerning the discrimination between similarity and association, the good performance of APSyn on <cite>SimLex-999</cite> (which was built with a specific attention to genuine similarity) and the large difference in performance between the two subsets of WordSim-353 described in Table  3 and Table 4 make us conclude that APSyn is indeed efficient in quantifying genuine similarity. To conclude, being a linguistically and cognitively grounded metric, APSyn offers the possibility for further improvements, by simply combining it to other properties that were not yet considered in its definition.",
  "y": "uses"
 },
 {
  "id": "7e52a90a9a0a703250d5c3c1890058_0",
  "x": "As for language, we are interested in Runyankore, a Bantu language indigenous to south western Uganda. The highly agglutinative structure and complex verbal morphology of Runyankore make existing NLG systems based on templates inapplicable (Keet and Khumalo, 2017) . There have been efforts undertaken to apply the grammar engine technique instead (Byamugisha et al., 2016a;<cite> Byamugisha et al., 2016b</cite>; Byamugisha et al., 2016c) , which resulted in theoretical advances in verbalization rules for ontologies, pluralization of nouns, and verb conjugation that address the text generation needs for Runyankore.",
  "y": "background"
 },
 {
  "id": "7e52a90a9a0a703250d5c3c1890058_1",
  "x": "The highly agglutinative structure and complex verbal morphology of Runyankore make existing NLG systems based on templates inapplicable (Keet and Khumalo, 2017) . There have been efforts undertaken to apply the grammar engine technique instead (Byamugisha et al., 2016a;<cite> Byamugisha et al., 2016b</cite>; Byamugisha et al., 2016c) , which resulted in theoretical advances in verbalization rules for ontologies, pluralization of nouns, and verb conjugation that address the text generation needs for Runyankore. We present our implementation of these algorithms and required linguistic annotations as a Prot\u00e9g\u00e9 5.x plugin.",
  "y": "uses"
 },
 {
  "id": "7e52a90a9a0a703250d5c3c1890058_2",
  "x": "**IMPLEMENTATION OF THE GRAMMAR ENGINE** We implemented the algorithms for verbalization and pluralization presented in (Byamugisha et al., 2016a; Byamugisha et al., 2016c ) as a Java application. The CFG specified in <cite>(Byamugisha et al., 2016b)</cite> was implemented using the CFG Java tool (Xu et al., 2011) .",
  "y": "background"
 },
 {
  "id": "7e52a90a9a0a703250d5c3c1890058_3",
  "x": "We implemented the algorithms for verbalization and pluralization presented in (Byamugisha et al., 2016a; Byamugisha et al., 2016c ) as a Java application. The CFG specified in <cite>(Byamugisha et al., 2016b)</cite> was implemented using the CFG Java tool (Xu et al., 2011) . We used this tool for three main reasons: our grammar engine implementation was done in Java, so we wanted a Java tool as well; we wanted a small CFG implementation for reasonable performance; and their tool extended Purdom's algorithm to fulfill Context-Dependent Rule Coverage (CDRC), which generates more and simpler sentences.",
  "y": "uses"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_0",
  "x": "For example, given a question like Who was responsible for coordinating disaster relief for victims of Hurricane Katrina?, ATD components enable Q/A systems to retrieve the sets of candidate answers which include the INDIVIDUALs and/or ORGANIZATIONs who provided aid to the victims of the hurricane. Early work in ATD (Harabagiu et al., 2000; Harabagiu et al., 2001 ) leveraged sets of heuristics in order to identify the expected answer types (EATs) of questions submitted to a Q/A system. Most modern Q/A systems, however, follow work done by<cite> (Li and Roth, 2002)</cite> in using machine learning classifiers in order to select the one (or more) EATs from a fixed hierarchy of answer types which are most appropriate for a particular question.",
  "y": "background"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_1",
  "x": "Most modern Q/A systems, however, follow work done by<cite> (Li and Roth, 2002)</cite> in using machine learning classifiers in order to select the one (or more) EATs from a fixed hierarchy of answer types which are most appropriate for a particular question. While learning-based approaches have dramatically increased the precision of open-domain ATD systems, most current ATD components have only been tasked with distinguishing amongst a limited set of EATs. For example, the most commonly-used answer type hierarchy (ATH), the University of Illinois (UIUC) answer type hierarchy created by<cite> (Li and Roth, 2002)</cite> , includes only a total of 50 unique expected answer types (generally referred to as \"fine\" answer types), organized into 6 different categories (referred to as \"coarse\" answer types).",
  "y": "background"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_2",
  "x": "in contrast, the<cite> (Li and Roth, 2002)</cite> UIUC ATH is designed especially for questions, but lacks the ability to extend the depth of the hierarchy when Q/A systems are capable of handling more detailed answer types. (See Section 3 for a more detailed discussion of the UIUC hierarchy.) In order to create a new ATH capable of addressing the needs of today's open-domain factoid Q/A systems, we wanted to develop an ATH which was capable of scaling to any given set of named entity types. Our work in this paper sought to address the following requirements: Requirement 1.",
  "y": "differences"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_3",
  "x": "(A graphical representation of a portion of the LCC ATH is presented in Figure 1 .) The UIUC answer type hierarchy<cite> (Li and Roth, 2002)</cite> We feel that the time is right for work in ATD to move beyond the UIUC ATH and to begin to tackle problems of organizing and learning answer type hierarchies that encompass several hundreds of diverse expected answer types. We believe that this effort would be in line with recent work looking at a similar type of semantic categorization problem -named entity recognition -in which researchers have moved from using simple heuristics and classifiers to unsupervised or semi-supervised methods capable of inducing hundreds (if not thousands) of entity types from large collections of texts. In our work, we used output from LCC's own, widecoverage named entity recognition system, CICEROLITE in order to construct a novel ATH which included more than 200 different EATs.",
  "y": "background"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_4",
  "x": "**ANNOTATING THE QUESTION CORPUS** In this section, we describe how we used the large ATH introduced in Section 3 in order to annotate a corpus drawn from more than 10,000 questions compiled from (1) existing annotated question corpora<cite> (Li and Roth, 2002)</cite> , (2) collections of questions mined from the web, and (3) questions submitted to LCC's FERRET question-answering system (Hickl et al., 2006a) . (A breakdown of the number of questions obtained from each of these three strategies is provided in Table 3 Table 3 : Distribution of 10,000 annotated questions by originating data set.",
  "y": "uses similarities"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_5",
  "x": "A hierarchical structure (i.e., only the children of the current type are considered as outcomes) to make a classification at every branching point in the hierarchy. 3 classifiers are machine-learning based while the remainder are heuristic classifiers. In a departure from previous machine-learning based approaches<cite> (Li and Roth, 2002</cite>; Krishnan et al., 2005) , we used a maximum entropy classifier to learn our ATH.",
  "y": "similarities uses"
 },
 {
  "id": "7e73137c97a84fe7fa4941ecd06a91_6",
  "x": "**CONCLUSIONS** This paper described the creation of a new answer type detection system capable of recognizing more than 200 different expected answer types with greater 85% precision. In a departure from previous work in answer type detection (Krishnan et al., 2005;<cite> Li and Roth, 2002)</cite> , we have demonstrated how a large, multi-tiered answer type hierarchy can be created which incorporates many of the entity types included in LCC's wide coverage named entity recognition system, CICEROLITE; this hierarchy was then used in order to create a new corpus of more than 10,000 questions which could be used to train an ATD system.",
  "y": "similarities"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_0",
  "x": "Unsupervised speech representation learning [2, 3, 4, 5, 6,<cite> 7,</cite> 8, 9, 10] is effective in extracting high-level properties from speech. SLP downstream tasks can be improved through speech representations because surface features such as log Mel-spectrograms or waveform can poorly reveal the abundant information within speech. Contrastive Predictive Coding (CPC) [5] and wav2vec <cite>[7]</cite> use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task.",
  "y": "background"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_1",
  "x": "Contrastive Predictive Coding (CPC) [5] and wav2vec <cite>[7]</cite> use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task. Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss. Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6,<cite> 7]</cite> .",
  "y": "background"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_2",
  "x": "Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6,<cite> 7]</cite> . However, this constraint on model architectures limits the potential of speech representation learning. The recently proposed vq-wav2vec [8] approach attempts to apply the well-performing Natural Language Processing (NLP) algorithm BERT [12] on continuous speech.",
  "y": "background"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_3",
  "x": "Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6,<cite> 7]</cite> . However, this constraint on model architectures limits the potential of speech representation learning. The recently proposed vq-wav2vec [8] approach attempts to apply the well-performing Natural Language Processing (NLP) algorithm BERT [12] on continuous speech.",
  "y": "motivation background"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_4",
  "x": "Moreover, as previous approaches restrict the power of the pre-trained models to representation extraction only [5, 6,<cite> 7,</cite> 8] , the proposed method is robust and can be fine-tuned easily on downstream tasks. We show that finetuning for 2 epochs easily acquires significant improvement. The proposed approach outperforms other representations and features.",
  "y": "differences"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_5",
  "x": "**EXPERIMENT** Following previous works [2, 3, 4, 5, 6,<cite> 7,</cite> 8] , we evaluate different features and representations on downstream tasks, including: phoneme classification, speaker recognition, and sentiment classification on spoken content. For a fair comparison, each downstream task uses an identical model architecture and hyperparameters despite different input features.",
  "y": "uses"
 },
 {
  "id": "7f1723c42fc577fdfd7144b7991db1_6",
  "x": "**COMPARING WITH OTHER REPRESENTATIONS** The proposed approaches are mainly compared with APC [6] representations, as they also experiment on phone classification and speaker verification. As reported in [6] , the APC approach outperformed CPC representations [5,<cite> 7,</cite> 9] in both two tasks, which makes APC suitable as a strong baseline.",
  "y": "differences"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_0",
  "x": "The challenge's baseline system poor performance (about 80 % WER) is an indication that ASR training did not work well. Recently, Guided Source Separation (GSS) enhancement on the test data was shown to significantly improve the performance of an acoustic model, which had been trained with a large amount of unprocessed and simulated noisy data <cite>[13]</cite> . GSS is a spatial mixture model based blind source separation approach which exploits the annotation given in the CHiME-5 database for initialization and, in this way, avoids the frequency permutation problem [14] .",
  "y": "background"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_1",
  "x": "Using a single acoustic model trained with 308 hrs of training data, which resulted after applying multi-array GSS data cleaning and a three-fold speed perturbation, we achieved a WER of 41.6 % on the development (DEV) and 43.2 % on the evaluation (EVAL) test set of CHiME-5, if the test data is also enhanced with multi-array GSS. This compares very favorably with the recently published topline in <cite>[13]</cite> , where the single-system best result, i.e., the WER without system combination, was 45.1 % and 47.3 % on DEV and EVAL, respectively, using an augmented training data set of 4500 hrs total. The rest of this paper is structured as follows.",
  "y": "differences"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_2",
  "x": "An Expectation Maximization (EM) algorithm estimates the parameters of a spatial mixture model and the posterior probabilities of each speaker being active are used for mask based beamforming. An overview block diagram of this enhancement by source separation is depicted in Fig. 1 . It follows the approach presented in <cite>[13]</cite> , which was shown to outperform the baseline version.",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_3",
  "x": "However, having such a large temporal context may become problematic when the speakers are moving, because the estimated spatial covariance matrix can become outdated due to the movement <cite>[13]</cite> . Alternatively, one can run the EM first with a larger temporal context until convergence, then drop the context and re-run it for some more iterations. As shown later in the paper, this approach did not improve ASR performance.",
  "y": "motivation"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_4",
  "x": "Alternatively, one can run the EM first with a larger temporal context until convergence, then drop the context and re-run it for some more iterations. As shown later in the paper, this approach did not improve ASR performance. Therefore, the temporal context was only used for dereverberation and the mixture model parameter estimation, while for the estimation of covariance matrices for beamforming the context was dropped and only the original segment length was considered <cite>[13]</cite> .",
  "y": "extends uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_5",
  "x": "---------------------------------- **STATE-OF-THE-ART SINGLE-SYSTEM FOR CHIME-5** To facilitate comparison with the recently published top-line in <cite>[13]</cite> (H/UPB), we have conducted a more focused set of experiments whose results are depicted in Table 4 .",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_6",
  "x": "**STATE-OF-THE-ART SINGLE-SYSTEM FOR CHIME-5** To facilitate comparison with the recently published top-line in <cite>[13]</cite> (H/UPB), we have conducted a more focused set of experiments whose results are depicted in Table 4 . As explained in Section 5.1, we opted for <cite>[13]</cite> instead of [14] as baseline because the former system is stronger.",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_7",
  "x": "To have a fair comparison, the results are compared with the single-system performance reported in <cite>[13]</cite> . For the single array track, the proposed system without RNN LM rescoring achieves 16 % (11 %) relative WER reduction on the DEV (EVAL) set when compared with System8 in <cite>[13]</cite> (row one in Table 4 ). RNN LM rescoring further helps improve the proposed system performance.",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_8",
  "x": "We report performance of our system on both single and multiple array tracks. To have a fair comparison, the results are compared with the single-system performance reported in <cite>[13]</cite> . For the single array track, the proposed system without RNN LM rescoring achieves 16 % (11 %) relative WER reduction on the DEV (EVAL) set when compared with System8 in <cite>[13]</cite> (row one in Table 4 ).",
  "y": "differences"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_9",
  "x": "For the single array track, the proposed system without RNN LM rescoring achieves 16 % (11 %) relative WER reduction on the DEV (EVAL) set when compared with System8 in <cite>[13]</cite> (row one in Table 4 ). RNN LM rescoring further helps improve the proposed system performance. For the multi array track, the proposed system without RNN LM rescoring achieved 6 % (7 %) relative WER reduction on the DEV (EVAL) set when compared with System16 in <cite>[13]</cite> (row six in Table 4 ).",
  "y": "differences"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_10",
  "x": "Finally, cleaning up the training set not only boosted the recognition performance, but managed to do so using a fraction of the training data in <cite>[13]</cite> , as shown in Table 5 . This translates to significantly faster and cheaper training of acoustic models, which is a major advantage in practice. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_11",
  "x": "Results are depicted in Table 6 . The first row corresponds to the GSS configuration in [14] while the second one corresponds to the GSS configuration in <cite>[13]</cite> . First two rows show that dropping the temporal context for estimating statistics for beamforming improves ASR accuracy.",
  "y": "uses"
 },
 {
  "id": "7f2622701e1f6c8492ec627b6ac32b_13",
  "x": "2 (52.5) creased, we concluded that the best configuration for the GSS enhancement in CHiME-5 scenario is using full temporal context for the EM stage and dropping it for the beamforming stage. Consequently, we have chosen system <cite>[13]</cite> as baseline in this study since is using the stronger GSS configuration. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_0",
  "x": "1 1. Hereford United were seeing stars at Gillingham after letting in 2 early goals 2. Look into the night sky to see the stars MWE identification is the task of automatically determining which word combinations at the token-level form MWEs (Baldwin and Kim, 2010) , and must be able to make such distinctions. This is particularly important for applications such as machine translation (Sag et al., 2002) , where the appropriate meaning of word combinations in context must be preserved for accurate translation. In this paper, following prior work (e.g., <cite>Salton et al., 2016</cite> ), we frame token-level identification of VNCs as a supervised binary classification problem, i.e., idiomatic vs. literal.",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_1",
  "x": "Surprisingly, we find that an approach based on representing sentences as the average of their word embeddings performs comparably to, or better than, the skip-thoughts based approach previously proposed by <cite>Salton et al. (2016)</cite> . VNCs exhibit lexico-syntactic fixedness. For example, the idiomatic interpretation in example 1 above is typically only accessible when the verb see has active voice, the determiner is null, and the noun star is in plural form, as in see stars or seeing stars.",
  "y": "differences"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_2",
  "x": "**RELATED WORK** Much research on MWE identification has focused on specific kinds of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005) , including English VNCs (e.g., Fazly et al., 2009; <cite>Salton et al., 2016</cite>) , although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017) . Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., Fazly et al., 2009; Fothergill and Baldwin, 2012) , treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008) , incorporated topic models (e.g., Li et al., 2010) , and made use of distributed representations of words (Gharbieh et al., 2016) .",
  "y": "background"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_3",
  "x": "Much research on MWE identification has focused on specific kinds of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005) , including English VNCs (e.g., Fazly et al., 2009; <cite>Salton et al., 2016</cite>) , although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017) . Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., Fazly et al., 2009; Fothergill and Baldwin, 2012) , treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008) , incorporated topic models (e.g., Li et al., 2010) , and made use of distributed representations of words (Gharbieh et al., 2016) . In the most closely related work to ours, <cite>Salton et al. (2016)</cite> represent token instances of VNCs by embedding the sentence that they occur in using skip-thoughts (Kiros et al., 2015) -an encoderdecoder model that can be viewed as a sentencelevel counterpart to the word2vec (Mikolov et al., 2013 ) skip-gram model.",
  "y": "background"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_4",
  "x": "During training the target sentence is encoded using a recurrent neural network, and is used to predict the previous and next sentences. <cite>Salton et al.</cite> then use these sentence embeddings, representing VNC token instances, as features in a supervised classifier. We treat this skip-thoughts based approach as a strong baseline to compare against.",
  "y": "background"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_5",
  "x": "<cite>Salton et al.</cite> then use these sentence embeddings, representing VNC token instances, as features in a supervised classifier. We treat this skip-thoughts based approach as a strong baseline to compare against. Fazly et al. (2009) formed a set of eleven lexicosyntactic patterns for VNC instances capturing the voice of the verb (active or passive), determiner (e.g., a, the), and number of the noun (singular or plural).",
  "y": "motivation"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_6",
  "x": "6 We represent a given sentence containing a VNC instance using the skip-thoughts encoder. Note that this approach is our re-implementation of the skipthoughts based method of <cite>Salton et al. (2016)</cite> , and we use it as a strong baseline for comparison. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_7",
  "x": "**DATASET** We use the VNC-Tokens dataset (Cook et al., 2008) -the same dataset used by Fazly et al. (2009) and <cite>Salton et al. (2016)</cite> -to train and evaluate our models. This dataset consists of sentences containing VNC usages drawn from the British National Corpus (Burnard, 2000) , 7 along with a label indicating whether the VNC is an idiomatic or literal usage (or whether this cannot be determined, in which case it is labelled \"unknown\").",
  "y": "uses similarities"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_8",
  "x": "VNC-Tokens is divided into DEV and TEST sets that each include fourteen VNC types and a total of roughly six hundred instances of these types annotated as literal or idiomatic. Following <cite>Salton et al. (2016)</cite> , we use DEV and TEST, and ignore all token instances annotated as \"unknown\". Fazly et al. (2009) and <cite>Salton et al. (2016)</cite> structured their experiments differently.",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_9",
  "x": "Fazly et al. (2009) and <cite>Salton et al. (2016)</cite> structured their experiments differently. Fazly et al. report results over DEV and TEST separately. In this setup TEST consists of expressions that were not seen during model development (done on DEV). <cite>Salton et al.</cite>, on the other hand, merge DEV and TEST, and create new training and testing sets, such that each expression is present in the training and testing data, and the ratio of idiomatic to literal usages of each expression in the training data is roughly equal to that in the testing data. We borrowed ideas from both of these approaches in structuring our experiments.",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_10",
  "x": "We borrowed ideas from both of these approaches in structuring our experiments. We retain We then divide each of these into training and testing sets, using the same ratios of idiomatic to literal usages for each expression as <cite>Salton et al. (2016)</cite> . This allows us to develop and tune a model on DEV, and then determine whether, when retrained on instances of unseen VNCs in (the training portion of) TEST, that model is able to generalize to new VNCs without further tuning to the specific expressions in TEST.",
  "y": "similarities uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_11",
  "x": "The proportion of idiomatic usages in the testing portions of both DEV and TEST is 63%. We therefore use accuracy to evaluate our models following Fazly et al. (2009) because the classes are roughly balanced. We randomly divide both DEV and TEST into training and testing portions ten times, following <cite>Salton et al. (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_12",
  "x": "Nevertheless, it is remarkable that the relatively simple approach to averaging word embeddings used by word2vec performs as well as, or better than, the much more complex skipthoughts model used by <cite>Salton et al. (2016)</cite> . 8 8 The word2vec and skip-thoughts models were trained on different corpora, which could contribute to the differences in results for these models. We therefore carried out an additional experiment in which we trained word2vec on BookCorpus, the corpus on which skip-thoughts was trained.",
  "y": "differences"
 },
 {
  "id": "7f78697390e28cc7798f8cb183cb59_13",
  "x": "In this paper we proposed two approaches to the task of classifying VNC token instances as idiomatic or literal based on word2vec embeddings and Siamese CBOW. We compared these approaches against a linguistically-informed unsupervised baseline, and a model based on skip-thoughts previously applied to this task (<cite>Salton et al., 2016</cite>) . Our experimental results show that a comparatively simple approach based on averaging word embeddings performs at least as well as, or better than, the approach based on skip-thoughts.",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_0",
  "x": "While the problem of cue phrase classi cation has often been noted (Grosz & Sidner 1986; Halliday & Hassan 1976; Reichman 1985; Schi rin 1987; Zuckerman & Pearl 1986) , it has generally not received careful study. Recently, however, <cite>Hirschberg and Litman (1993)</cite> have presented rules for classifying cue phrases in both text and speech. <cite>Hirschberg and Litman</cite> pre-classi ed a set of naturally occurring cue phrases, described each cue phrase in terms of prosodic and textual features, then manually examined the data to construct rules that best predicted the classi cations from the features.",
  "y": "background motivation"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_1",
  "x": "The accuracy of the learned rulesets is often higher than the accuracy of the rules in <cite>(Hirschberg & Litman 1993)</cite> , while the linguistic implications are more precise. ---------------------------------- **CUE PHRASE CLASSI CATION**",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_3",
  "x": "**CUE PHRASE CLASSI CATION** This section summarizes <cite>Hirschberg and Litman's study</cite> of the classi cation of multiple cue phrases in text and speech <cite>(Hirschberg & Litman 1993)</cite> . The data from <cite>this study</cite> is used to create the input for the machine learning experiments, while the results are used as a benchmark for evaluating performance.",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_4",
  "x": "The corpus examined was a technical address by a single speaker, lasting 75 minutes and consisting of approximately 12,500 words. The corpus yielded 953 instances of 34 di erent single word cue phrases. <cite>Hirschberg and Litman</cite> each classi ed the 953 tokens (as discourse, sentential or ambiguous) while listening to a recording and reading a transcription.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_5",
  "x": "In <cite>(Hirschberg & Litman 1993)</cite> , every cue phrase was described using the following prosodic features. Accent corresponded to the pitch accent (if any) that was associated with the token. For both the intonational and intermediate phrases containing each token, the feature composition of phrase represented whether or not the token was alone in the phrase (the phrase contained only the token, or only cue phrases).",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_6",
  "x": "In a procedure similar to that described above, <cite>Hirschberg and Litman</cite> rst classi ed and described each of the 48 tokens. <cite>They</cite> then examined their data manually to develop the prosodic model, which correctly classi ed all of the 48 tokens. (When later tested on 52 new examples of \\now\" from the radio corpus, the model also performed nearly perfectly).",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_7",
  "x": "<cite>They</cite> then examined their data manually to develop the prosodic model, which correctly classi ed all of the 48 tokens. (When later tested on 52 new examples of \\now\" from the radio corpus, the model also performed nearly perfectly). The model uniquely classi es any cue phrase using the features composition of Figure 1 : Decision tree representation of the classi cation models of (<cite>Hirschberg and Litman 1993</cite> Figure 1 ), or in a larger intermediate phrase with an initial position (possibly preceded by other cue phrases) and a L* accent or deaccented, it is classi ed as discourse.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_8",
  "x": "The model uniquely classi es any cue phrase using the features composition of Figure 1 : Decision tree representation of the classi cation models of (<cite>Hirschberg and Litman 1993</cite> Figure 1 ), or in a larger intermediate phrase with an initial position (possibly preceded by other cue phrases) and a L* accent or deaccented, it is classi ed as discourse. When part of a larger intermediate phrase and either in initial position with a H* or complex accent, or in a non-initial position, it is sentential. The textual model was also manually developed, and was based on an examination of the rst 17 minutes of the single speaker technical address (Litman & Hirschberg 1990) ; the model correctly classi ed 89.4% of these 133 tokens.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_9",
  "x": "When a cue phrase is preceded by any type of orthography it is classi ed as discourse, otherwise as sentential. The models were evaluated by quantifying their performance in correctly classifying two subsets of the 953 tokens from the corpus. The rst subset (878 examples) consisted of only the classi able tokens, i.e., the tokens that both <cite>Hirschberg and Litman</cite> classi ed as discourse or that both classi ed as sentential.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_10",
  "x": "---------------------------------- **EXPERIMENTS USING MACHINE INDUCTION** This section describes experiments that use the machine learning programs C4.5 (Quinlan 1986; and cgrendel (Cohen 1992; 1993) to automatically induce cue phrase classi cation rules from both the data of <cite>(Hirschberg & Litman 1993)</cite> and an extension of this data.",
  "y": "extends uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_11",
  "x": "The rst set of experiments does not distinguish among the 34 cue phrases. In each experiment, a different subset of the features coded in <cite>(Hirschberg & Litman 1993 )</cite> is examined. The experiments consider every feature in isolation (to comparatively evaluate the utility of each individual knowledge source for classi cation), as well as linguistically motivated sets of features (to gain insight into the interactions between the knowledge sources).",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_12",
  "x": "The second set of experiments treats cue phrases individually. This is done by adding a lexical feature representing the cue phrase to each feature set from the rst set of experiments. The potential use of such a lexical feature was noted but not used in <cite>(Hirschberg & Litman 1993)</cite> .",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_13",
  "x": "These experiments evaluate the utility of developing classi cation models specialized for particular cue phrases, and also provide qualitatively new linguistic insights into the data. The rst input to each learning program de nes the classes and features. The classi cations produced <cite>by Hirschberg and by Litman</cite> (discourse, sentential, and ambiguous) are combined into a single classi cation for each cue phrase.",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_14",
  "x": "A cue phrase is classi ed as discourse (or as sentential) if both <cite>Hirschberg and Litman</cite> agreed upon the classi cation discourse (or upon sentential). A cue phrase is non-classi able if at least one of <cite>Hirschberg and/or Litman</cite> classi ed the token as ambiguous, or one classi ed it as discourse while the other classi ed it as sentential. The features considered in the learning experiments are shown in Figure 2 .",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_15",
  "x": "A cue phrase is classi ed as discourse (or as sentential) if both <cite>Hirschberg and Litman</cite> agreed upon the classi cation discourse (or upon sentential). A cue phrase is non-classi able if at least one of <cite>Hirschberg and/or Litman</cite> classi ed the token as ambiguous, or one classi ed it as discourse while the other classi ed it as sentential. The features considered in the learning experiments are shown in Figure 2 .",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_16",
  "x": "The feature representation shown here follows the representation of <cite>(Hirschberg & Litman 1993 )</cite> except as noted. Length of phrase (P-L and I-L) represents the number of words in the phrase. This feature was not coded in the data from which the prosodic model was developed, but was coded (although not used) in the later data of <cite>(Hirschberg & Litman 1993)</cite> .",
  "y": "similarities"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_17",
  "x": "The feature representation shown here follows the representation of <cite>(Hirschberg & Litman 1993 )</cite> except as noted. Length of phrase (P-L and I-L) represents the number of words in the phrase. This feature was not coded in the data from which the prosodic model was developed, but was coded (although not used) in the later data of <cite>(Hirschberg & Litman 1993)</cite> .",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_18",
  "x": "Position in phrase (P-P and I-P) uses numeric rather than symbolic values. The conjunction of the rst two values for I-C is equivalent to alone in Figure 1 . Ambiguous, the last value of A, is assigned when the prosodic anal- ysis of <cite>(Hirschberg & Litman 1993 )</cite> is a disjunction (e.g., \\H*+L or H*\").",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_19",
  "x": "The conjunction of the rst two values for I-C is equivalent to alone in Figure 1 . Ambiguous, the last value of A, is assigned when the prosodic anal- ysis of <cite>(Hirschberg & Litman 1993 )</cite> is a disjunction (e.g., \\H*+L or H*\"). NA (not applicable) in the textual features re ects the fact that 39 recorded examples were not included in the transcription, which was done independently of <cite>(Hirschberg & Litman 1993)</cite> .",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_20",
  "x": "Finally, the lexical feature token is new to this study, and represents the actual cue phrase being described. The second input to each learning program is training data, i.e., a set of examples for which the class and feature values are speci ed. Consider the following utterance, taken from the corpus of <cite>(Hirschberg & Litman 1993)</cite>: Example 1 (Now) (now that we have all been welcomed here)] it's time to get on with the business of the conference. This utterance contains two cue phrases, corresponding to the two instances of \\now\".",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_21",
  "x": "These sets correspond to the two subsets of the corpus examined in <cite>(Hirschberg & Litman 1993 )</cite> { the classi able tokens, and the classi able non-conjuncts. ---------------------------------- **RESULTS**",
  "y": "similarities"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_22",
  "x": "Note that for each run, the training and testing examples are disjoint subsets of the same set of examples, and the training set is much larger than the test set. In contrast (as discussed above), the \\training\" and test sets for the intonational model of <cite>(Hirschberg & Litman 1993)</cite> were taken from di erent corpora, while for the textual model of <cite>(Hirschberg & Litman 1993 )</cite> the test set was a superset of the training set. Furthermore, more data was used for testing than for training, and the computation of the error rate did not use cross-validation.",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_23",
  "x": "The left half of the table considers the single feature and single feature plus token sets, while the right half considers the multiple features with and without token. The top of the table considers prosodic features, the bottom textual features, and the bottom right prosodic/textual combinations. The error rates in italics indicate that the performance of the learned ruleset exceeds the performance reported in <cite>(Hirschberg & Litman 1993)</cite> , where the rules of Figure 1 were tested using 100% of the 878 classi able tokens.",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_24",
  "x": "(Recall that length was coded by <cite>Hirschberg and Litman</cite> only in <cite>their</cite> test data. Length was thus never used to generate or revise their prosodic model.) Both of the learned rulesets perform similarly to each other, and outperform the prosodic model of Figure 1 . Examination of the learned textual rulesets yields similar ndings.",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_26",
  "x": "When the feature token is taken into account, however, the learned rulesets outperform the models of <cite>(Hirschberg & Litman 1993</cite> sider this feature), and also provide new insights into cue phrase classi cation. Figure 5 shows the cgrendel ruleset learned from A+, which reduces the 30% error rate of A to 12%. The rst rule corresponds to line (5) of Figure 1 .",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_27",
  "x": "Siegel (in press) was the rst to apply machine learning to cue phrases. He developed a genetic learning algorithm to induce decision trees using the non-ambiguous examples of <cite>(Hirschberg & Litman 1993 )</cite> (using the classi cations of only one judge) as well as additional examples. Each example was described using a feature corresponding to token, as well as textual features containing the lexical or orthographic item immediately to the left of and in the 4 positions to the right of the example.",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_28",
  "x": "An examination of Table 2 shows that the error of the best C4.5 and cgrendel rulesets was often lower than 21% (even for theories which did not consider the token), as was the 19.1% error of the textual model of <cite>(Hirschberg & Litman 1993)</cite> . Siegel and McKeown (1994) have also proposed a method for developing linguistically viable rulesets, based on the partitioning of the training data produced during induction. ----------------------------------",
  "y": "background"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_29",
  "x": "**CONCLUSION** This paper has demonstrated the utility of machine learning techniques for cue phrase classi cation. A rst set of experiments were presented that used the programs cgrendel (Cohen 1992; 1993) and C4.5 (Quinlan 1986; to induce classi cation rules from the preclassi ed cue phrases and their features that were used as test data in <cite>(Hirschberg & Litman 1993)</cite> .",
  "y": "uses"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_30",
  "x": "The results of these experiments suggest that machine learning is an e ective technique for not only automating the generation of linguistically plausible classi cation rules, but also for improving accuracy. In particular, a large number of learned rulesets (including P-P, an extremely simple one feature model) had signicantly lower error rates than the rulesets of <cite>(Hirschberg & Litman 1993)</cite> . One possible explanation is that the hand-built classi cation models were derived using very small \\training\" sets; as new data became available, this data was used for testing but not for updating the original models.",
  "y": "differences"
 },
 {
  "id": "8075c3716c0be601a769ccd53b0c5e_31",
  "x": "For example, in a second set of experiments, new classi cation rules were induced using the feature token, which was not considered in <cite>(Hirschberg & Litman 1993)</cite> . Allowing the learning programs to treat cue phrases individually further improved the accuracy of the resulting rulesets, and added to the body of linguistic knowledge regarding cue phrases. Another advantage of the machine learning approach is that the ease of inducing rulesets from many different sets of features supports an exploration of the comparative utility of di erent knowledge sources.",
  "y": "extends"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_0",
  "x": "**INTRODUCTION** Current state-of-the-art parsers score over 90% on the standard newswire evaluation, but the remaining errors are difficult to overcome using only the training corpus. Features from n-gram counts over resources like Web1T (Brants and Franz, 2006 ) have proven to be useful proxies for syntax<cite> (Bansal and Klein, 2011</cite>; Pitler, 2012) , but they enforce linear word order, and are unable to distinguish between syntactic and non-syntactic co-occurrences.",
  "y": "motivation background"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_1",
  "x": "We compare the performance of our syntactic n-gram features against the surface n-gram features of<cite> Bansal and Klein (2011)</cite> in-domain on newswire and out-of-domain on the English Web Treebank (Petrov and McDonald, 2012) across CoNLL-style (LTH) dependencies. We also extend the first-order surface n-gram features to second-order, and compare the utility of Web1T and the Google Books Ngram corpus (Lin et al., 2012) as surface n-gram sources. We find that surface and syntactic n-grams provide statistically significant and complementary accuracy improvements in-and out-of-domain.",
  "y": "uses"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_2",
  "x": "Additional features for each bucket value up to the maximum are also encoded. We also develop paraphrase-style features like those of<cite> Bansal and Klein (2011)</cite> based on the most frequently occurring words and POS tags before, in between, and after each head-argument ambiguity (see Section 3.2). Figure 1 depicts the potential context words available the hold \u2192 hearing dependency.",
  "y": "uses similarities"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_3",
  "x": "Similar to the surface n-gram features (Section 3), counts for our syntactic n-gram features are precomputed to improve the run-time efficiency of the parser. Experiments on the development set led to a minimum cutoff frequency of 10,000 for each feature to avoid noise from parser and OCR errors. 3 Surface n-gram Features<cite> Bansal and Klein (2011)</cite> demonstrate that features generated from bucketing simple surface n-gram counts and collecting the top paraphrase-based contextual words over Web1T are useful for almost all attachment decisions, boosting dependency parsing accuracy by up to 0.6%.",
  "y": "background"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_4",
  "x": "3 Surface n-gram Features<cite> Bansal and Klein (2011)</cite> demonstrate that features generated from bucketing simple surface n-gram counts and collecting the top paraphrase-based contextual words over Web1T are useful for almost all attachment decisions, boosting dependency parsing accuracy by up to 0.6%. However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts. Bansal and Klein (2011) also tested only on in-domain text, though these external count features should be useful out of domain.",
  "y": "motivation"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_5",
  "x": "Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts. We also extend<cite> Bansal and</cite> Klein's affinity and paraphrase features to second-order. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_6",
  "x": "Affinity features rely on the intuition that frequently co-occurring words in large unlabeled text collections are likely to be in a syntactic relationship (Nakov and Hearst, 2005;<cite> Bansal and Klein, 2011)</cite> . N-gram resources such as Web1T and Google Books provide large offline collections from which these co-occurrence statistics can be harvested; given each head and argument ambiguity in a training and test corpus, the corpora can be linearly scanned ahead of parsing time to reduce the impact of querying in the parser. When scanning, the head and argument word may appear immediately adjacent to one another in linear order (CONTIG), or with up to three intervening words (GAP1, GAP2, and GAP3) as the maximum n-gram length is five.",
  "y": "background"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_7",
  "x": "In<cite> Bansal and Klein (2011)</cite> , paraphrase features are generated for all full-parse attachment ambiguities from the surface n-gram corpus. For each attachment ambiguity, 3-grams of the form ( q 1 q 2 ), (q 1 q 2 ), and (q 1 q 2 ) are extracted, where q 1 and q 2 are the head and argument in their linear order of appearance in the original sentence, and is any single context word appearing before, in between, or after the query words. Then the most frequent words appearing in each of these configurations for each head-argument ambiguity is encoded as a feature with the POS tags of the head and argument 2 .",
  "y": "background"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_8",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** As with<cite> Bansal and Klein (2011) and</cite> Pitler (2012) , we convert the Penn Treebank to dependencies using pennconverter 3 (Johansson and Nugues, 2007) (henceforth LTH) and generate POS tags with MX-POST (Ratnaparkhi, 1996) .",
  "y": "uses"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_9",
  "x": "---------------------------------- **RELATED WORK** Surface n-gram counts from large web corpora have been used to address NP and PP attachment errors (Volk, 2001; Nakov and Hearst, 2005) Aside from<cite> Bansal and Klein (2011)</cite> , other feature-based approaches to improving dependency parsing include Pitler (2012) , who exploits Brown clusters and point-wise mutual information of surface n-gram counts to specifically address PP and coordination errors.",
  "y": "background"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_10",
  "x": "We extract<cite> Bansal and Klein (2011)</cite> 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts. We also extend<cite> Bansal and</cite> Klein's affinity and paraphrase features to second-order.",
  "y": "extends"
 },
 {
  "id": "808e0a94b877182dc06447c8682a63_11",
  "x": "Bansal and Klein (2011) also tested only on in-domain text, though these external count features should be useful out of domain. We extract<cite> Bansal and Klein (2011)</cite> 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts. Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts.",
  "y": "extends"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_0",
  "x": "<cite>[21]</cite> further showed that use of synthetic training data can work better than multitask training. In this work we take advantage of both synthetic training targets and multitask training. The proposed model resembles recent sequence-to-sequence models for voice conversion, the task of recreating an utterance in another person's voice [22] [23] [24] .",
  "y": "background"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_1",
  "x": "Following recent speech translation <cite>[21]</cite> and recognition [28] models, the encoder is composed of a stack of 8 bidirectional LSTM layers. As shown in Fig. 1 , the final layer output is passed to the primary decoder, whereas intermediate activations are passed to auxiliary decoders predicting phoneme sequences. We hypothesize that early layers of the encoder are more likely to represent the source content well, while deeper layers might learn to encode more information about the target content.",
  "y": "similarities uses"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_2",
  "x": "---------------------------------- **EXPERIMENTS** We study two Spanish-to-English translation datasets: the large scale \"conversational\" corpus of parallel text and read speech pairs from <cite>[21]</cite> , and the Spanish Fisher corpus of telephone conversations and corresponding English translations [38] , which is smaller and more challenging due to the spontaneous and informal speaking style.",
  "y": "similarities uses"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_3",
  "x": "**CONVERSATIONAL SPANISH-TO-ENGLISH** This proprietary dataset described in <cite>[21]</cite> was obtained by crowdsourcing humans to read the both sides of a conversational Spanish-English MT dataset. In this section, instead of using the human target speech, we use a TTS model to synthesize target In addition, we augment the input source speech by adding background noise and reverberation in the same manner as <cite>[21]</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_4",
  "x": "**CONVERSATIONAL SPANISH-TO-ENGLISH** This proprietary dataset described in <cite>[21]</cite> was obtained by crowdsourcing humans to read the both sides of a conversational Spanish-English MT dataset. In this section, instead of using the human target speech, we use a TTS model to synthesize target In addition, we augment the input source speech by adding background noise and reverberation in the same manner as <cite>[21]</cite> .",
  "y": "extends differences"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_5",
  "x": "The total target speech duration is much smaller because the TTS output is better endpointed, and contains fewer pauses. 9.6k pairs are held out for testing. Input feature frames are created by stacking 3 adjacent frames of an 80-channel log-mel spectrogram as in <cite>[21]</cite> .",
  "y": "similarities"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_6",
  "x": "The speaker encoder was not used in these experiments since the target speech always came from the same speaker. Table 2 shows performance of the model trained using different combinations of auxiliary losses, compared to a baseline ST \u2192 TTS cascade model using a speech-to-text translation model <cite>[21]</cite> trained on the same data, and the same Tacotron 2 TTS model used to synthesize training targets. Note that the ground truth BLEU score is below 100 due to ASR errors during evaluation, or TTS failure when synthesizing the ground truth.",
  "y": "similarities uses"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_7",
  "x": "Performance using only the source recognition loss is very poor, indicating that learning alignment on this task is especially difficult without strong supervision on the translation task. We found that 4-head attention works better than one head, unlike the conversational task, where both attention mechanisms had similar performance. Finally, as in <cite>[21]</cite> , we find that pretraining the bottom 6 encoder layers on an ST task improves BLEU scores by over 5 points.",
  "y": "similarities"
 },
 {
  "id": "831342435ca0a4695e2a7f149891e4_8",
  "x": "Potential strategies to improve voice transfer performance include improving the speaker encoder by adding a language adversarial loss, or by incorporating a cycle-consistency term [13] into the S2ST loss. Other future work includes utilizing weakly supervision to scale up training with synthetic data <cite>[21]</cite> or multitask learning [19, 20] , and transferring prosody and other acoustic factors from the source speech to the translated speech following [45] [46] [47] . ----------------------------------",
  "y": "future_work"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_0",
  "x": "AWEs are representations that can be learned from data, ideally such that the embeddings of two segments corresponding to the same word are close, while embeddings of segments corresponding to different words are far apart. Once word segments are represented via fixed-dimensional embeddings, computing distances is as simple as measuring a cosine or Euclidean distance between two vectors. There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13,<cite> 14,</cite> 15, 16, 17] .",
  "y": "background"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_1",
  "x": "Kamper et al.<cite> [14]</cite> compared several types of acoustic word embeddings for a word discrimination task related to query-by-example search, finding that embeddings based on convolutional neural networks (CNNs) trained with a contrastive loss outperformed the reference vector approach of Levin et al. [12] as well as several other CNN and DNN embeddings and DTW using several feature types. There have now been a number of approaches compared on this same task and data [12, 21, 22, 23] . For a direct comparison with this prior work, in this paper we use the same task and some of the same training losses as Kamper et al., but develop new embedding models based on RNNs.",
  "y": "background"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_2",
  "x": "We use two main training approaches, inspired by prior work but with some differences in the details. As in <cite>[14,</cite> 11] , our first approach is to use the word labels of the training segments and train the networks to classify the word. In this case, the final layer of g(X) is a log-softmax layer.",
  "y": "similarities uses"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_4",
  "x": "In the experiments below, we examine this assumption by analyzing performance on words that appear in the training data compared to those that do not. The second training approach, based on earlier work of Kamper et al.<cite> [14]</cite> , is to train \"Siamese\" networks [31] . In this approach, full supervision is not needed; rather, we use weak supervision in the form of pairs of segments labeled as same or different.",
  "y": "uses"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_5",
  "x": "The data used for this task is drawn from the Switchboard conversational English corpus [32] . The word segments range from 50 to 200 frames in length. The acoustic features in each frame (the input to the word embedding models x t ) are 39-dimensional MFCCs+\u2206+\u2206\u2206. We use the same train, development, and test partitions as in prior work <cite>[14,</cite> 12] , and the same acoustic features as in<cite> [14]</cite> , for as direct a comparison as possible.",
  "y": "uses"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_6",
  "x": "As in<cite> [14]</cite> , when training the classificationbased embeddings, we use a subset of the training set containing all word types with a minimum of 3 occurrences, reducing the training set size to approximately 9k segments. 1 When training the Siamese networks, the training data consists of all of the same-word pairs in the full training set (approximately 100k pairs). For each such training pair, we randomly sample a third example belonging to a different word type, as required for the l cos hinge loss.",
  "y": "similarities uses"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_7",
  "x": "B triplets are of the form (x a , x s , x d ) where x a and x s are examples of the same class (a pair from the 100k same-word pair set) and x d is a randomly sampled example from a different class. Then, for each of these B triplets (x a , x s , x d ) , an additional triplet (x s , x a , x d ) is added to the mini-batch to allow all segments to serve as anchors. This is a slight departure from earlier work<cite> [14]</cite> , which we found to improve stability in training and performance on the development set.",
  "y": "differences"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_10",
  "x": "There is clear utility in stacking additional layers; however, even with 4 stacked layers the RNNs still underperform the CNN-based embeddings of<cite> [14]</cite> until we begin adding fully connected layers. ---------------------------------- **EFFECT OF MODEL STRUCTURE**",
  "y": "differences"
 },
 {
  "id": "845c66e6dfafc21ab90e5aa5cbf947_12",
  "x": "**EFFECT OF EMBEDDING DIMENSIONALITY** For the Siamese networks, we varied the output embedding dimensionality, as shown in Fig. 2 . This analysis shows that the embeddings learned by the Siamese RNN network are quite robust to reduced dimensionality, outperforming the classifier model for all dimensionalities 32 or higher and outperforming previously reported dev set performance with CNN-based embeddings<cite> [14]</cite> for all dimensionalities \u2265 16.",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_0",
  "x": "We show that such representation together with nCCA, a successful multimodal embedding technique, achieves state-of-the-art performance on the <cite>Visual Madlibs task</cite>. Moreover, inspired by the nCCA's objective function, we extend classical CNN+LSTM approach to train the network by directly maximizing the similarity between the internal representation of the deep learning architecture and candidate answers. Again, such approach achieves a significant improvement over the prior work that also uses CNN+LSTM approach on <cite>Visual Madlibs</cite>.",
  "y": "uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_1",
  "x": "Again, such approach achieves a significant improvement over the prior work that also uses CNN+LSTM approach on <cite>Visual Madlibs</cite>. ---------------------------------- **INTRODUCTION**",
  "y": "differences motivation"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_2",
  "x": "Arguably, it is also less prone to over-interpretations compared with the classical Turing Test [16, 25] . To foster progress on this task, a few metrics and datasets have been proposed [2, 4, 14, 20] . The recently introduced <cite>Visual Madlibs task</cite> <cite>[32]</cite> removes ambiguities in question or scene interpretations by introducing a multiple choice \"filling the blank\" task, where a c 2016.",
  "y": "background"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_4",
  "x": "Although related ideas have been explored for visual question answering [22] , and even have been used in <cite>Visual Madlibs</cite> <cite>[32]</cite> , we are first to show a significant improvement of such representation by using object proposals. More precisely, we argue for an approach that pools over a large number, highly overlapping object proposals. This, arguably, increases the recall of extracting bounding boxes that describe an object, but also allows for multi-scale and multi-parts object representation.",
  "y": "differences motivation"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_5",
  "x": "More precisely, we argue for an approach that pools over a large number, highly overlapping object proposals. This, arguably, increases the recall of extracting bounding boxes that describe an object, but also allows for multi-scale and multi-parts object representation. Our approach in the combination with the Normalized Correlation Analysis embedding technique improves on the state-of-the-art of the <cite>Visual Madlibs task</cite>.",
  "y": "extends"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_6",
  "x": "Our approach in the combination with the Normalized Correlation Analysis embedding technique improves on the state-of-the-art of the <cite>Visual Madlibs task</cite>. Text-Embedding Loss: Motivated by the popularity of deep architectures for visual question answering, that combine a global CNN image representation with an LSTM [7] question representation [4, 13, 17, 20, 29, 30, 31] , as well as the leading performance of nCCA on the multi-choice <cite>Visual Madlibs task</cite> <cite>[32]</cite> , we propose a novel extension of the CNN+LSTM architecture that chooses a prompt completion out of four candidates (see Figure 4 ) by measuring similarities directly in the embedding space. This contrasts with the prior approach of <cite>[32]</cite> that uses a post-hoc comparison between the discrete output of the CNN+LSTM method and all four candidates.",
  "y": "extends"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_7",
  "x": "Text-Embedding Loss: Motivated by the popularity of deep architectures for visual question answering, that combine a global CNN image representation with an LSTM [7] question representation [4, 13, 17, 20, 29, 30, 31] , as well as the leading performance of nCCA on the multi-choice <cite>Visual Madlibs task</cite> <cite>[32]</cite> , we propose a novel extension of the CNN+LSTM architecture that chooses a prompt completion out of four candidates (see Figure 4 ) by measuring similarities directly in the embedding space. This contrasts with the prior approach of <cite>[32]</cite> that uses a post-hoc comparison between the discrete output of the CNN+LSTM method and all four candidates. To achieve this, we directly train an LSTM with a cosine similarity loss between the output embedding of the network and language representation of the ground truth completion.",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_8",
  "x": "This contrasts with the prior approach of <cite>[32]</cite> that uses a post-hoc comparison between the discrete output of the CNN+LSTM method and all four candidates. To achieve this, we directly train an LSTM with a cosine similarity loss between the output embedding of the network and language representation of the ground truth completion. Such an approach integrates more tightly with the multi-choice filling the blanks task, and significantly outperforms the prior CNN+LSTM method <cite>[32]</cite> .",
  "y": "extends"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_9",
  "x": "Although answering questions on images is, arguably, more susceptible to automatic evaluation than the image description task [3, 8, 27] , ambiguities in the output space still remain. While such ambiguities can be handled using appropriate metrics [14, 15, 17, 26] , <cite>Visual Madlibs</cite> <cite>[32]</cite> has taken another direction, and handles them directly within the task. <cite>It</cite> asks machines to fill the blank prompted with a natural language description with a phrase chosen from four candidate completions (Figure 4 ).",
  "y": "background"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_10",
  "x": "However, in contrast to [22] , our experiments indicate a strong improvement by using object proposals. While a majority of the most recent work on visual question answering combine LSTM [7] with CNN [11, 23, 24] by concatenation or summation or piece-wise multiplication, Canonical Correlation Analysis (CCA and nCCA) [6] have also been shown to be a very effective multimodal embedding technique <cite>[32]</cite> . Our work further investigates this embedding method as well as brings ideas from CCA over to an CNN+LSTM formulation.",
  "y": "background"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_11",
  "x": "Instead of generating a prompt completion that is next compared against candidate completions in a post-hoc process, we propose to choose a candidate completion by directly comparing candidates in the embedding space. This puts CNN+LSTM approach closer to nCCA with a tighter integration with the multi-choice <cite>Visual Madlibs task</cite>. This approach is depicted in Figure 3 .",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_13",
  "x": "This embedding method has shown outstanding performance on the <cite>Visual Madlibs task</cite> <cite>[32]</cite> . At the test time, given the encoded image, we choose an answer (encoded by the mean pooling over word2vec words representations) from the set of four candidate answers that is the most similar to the encoded image in the multimodal embedding space. Formally, the Canonical Correlation Analysis (CCA) maximizes the cosine similarity between two modalities (also called views) in the embedding space, that is:",
  "y": "uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_14",
  "x": "Normalized Canonical Correlation Analysis (nCCA) [6] has been reported to work significantly better than the plain CCA. Here, columns of the projection matrices W 1 and W 2 are scaled by the p-th power (p=4) of the corresponding eigen values. The improvement is consistent with the findings of <cite>[32]</cite> , where nCCA performs better than CCA by about five percentage points in average on the hard task.",
  "y": "similarities"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_15",
  "x": "We use a special '<BLANK>' token to denote the empty blank space in the image description. On the other side, for each completion candidate s we compute its representation by averaging over word2vec [18] representations of the words contributing to s. However, in contrast to the prior work <cite>[32]</cite> , instead of comparing the discrete output of the network with the representation of s, we directly optimize an objective in the embedding space. During training we maximize the similarity measure between the output embedding and the representation of \u03c3 by optimizing the following objective:",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_17",
  "x": "Although the CNN+LSTM models we trained on <cite>Madlibs</cite> were not quite as accurate as nCCA for selecting the correct multiple-choice answer, they did result in better, sometimes much better, accuracy (as measured by BLEU scores) for targeted generation. ---------------------------------- **CONCLUSIONS**",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_19",
  "x": "**EXPERIMENTAL RESULTS** We evaluate our method on the multiple choice task of the <cite>Visual Madlibs dataset</cite>. <cite>The dataset</cite> consists of about 360k descriptions, spanning 12 different categories specified by different types of templates, of about 10k images.",
  "y": "uses background"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_20",
  "x": "Every category represents a different type of question including scenes, affordances, emotions, or activities (the full list is shown in the first column of Table 1 ). Since each category has fixed prompt, there is no need to include the prompt in the modeling given the training is done per each category. Finally, <cite>Visual Madlibs</cite> considers an easy and difficult tasks that differ in how the negative 3 candidate completions (distractors) are chosen.",
  "y": "background"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_22",
  "x": "Comparison to the state-of-the-art. Guided by the results of the previous experiments, we compare nCCA that uses Edge Boxes object proposals (nCCA (ours)) with the state-ofthe-arts on <cite>Visual Madlibs</cite> (nCCA <cite>[32]</cite> ). Both models use the same VGG Convolutional Neural Network [23] to encode images (or theirs crops), and word2vec to encode words.",
  "y": "uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_23",
  "x": "Both models use the same VGG Convolutional Neural Network [23] to encode images (or theirs crops), and word2vec to encode words. The models are trained per category (a model trained over all the categories performs inferior on the hard task <cite>[32]</cite> ). As Table 3 shows using a large number of object proposals Table 4 : Accuracies computed for different approaches on the easy and hard task.",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_24",
  "x": "This naturally leads to the following question if better localization helps. To see the limits, we compare nCCA (ours) against nCCA (bbox) <cite>[32]</cite> that crops over ground truth bounding boxes from MS COCO segmentations and next averages over theirs representations (Table 3 in <cite>[32]</cite> shows that ground truth bounding boxes outperforms automatically detected bounding boxes, and hence they can be seen as an upper bound for a detection method trained to detect objects on MS COCO). Surprisingly, nCCA (ours) outperforms nCCA (bbox) by a large margin as Table 4 shows.",
  "y": "differences uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_25",
  "x": "Arguably, object proposals have better recall and captures multi-scale, multi-parts phenomena. CNN+LSTM with comparison in the output embedding space. On one hand nCCA tops the leaderboard on the <cite>Visual Madlibs task</cite> <cite>[32]</cite> .",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_26",
  "x": "On one hand nCCA tops the leaderboard on the <cite>Visual Madlibs task</cite> <cite>[32]</cite> . Table 5 : Comparison between our Embedded CNN+LSTM approach that computes the similarity between input and candidate answers in the embedding space, and the plain CNN+LSTM original approach from <cite>[32]</cite> . Since the accuracies of CNN+LSTM <cite>[32]</cite> are unavailable for two categories, we report average over 10 categories in this case.",
  "y": "uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_27",
  "x": "On one hand nCCA tops the leaderboard on the <cite>Visual Madlibs task</cite> <cite>[32]</cite> . Table 5 : Comparison between our Embedded CNN+LSTM approach that computes the similarity between input and candidate answers in the embedding space, and the plain CNN+LSTM original approach from <cite>[32]</cite> . Since the accuracies of CNN+LSTM <cite>[32]</cite> are unavailable for two categories, we report average over 10 categories in this case.",
  "y": "uses"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_28",
  "x": "Results in %. completion of the prompt sentence out of four candidates, the comparison between the candidate completions should be directly done in the output embedding space. This contrasts to a post-hoc process used in <cite>[32]</cite> where an image description architecture (CNN+LSTM) first generates a completion that is next compared against the candidates in the word2vec space (see section 3 for more details).",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_29",
  "x": "This contrasts to a post-hoc process used in <cite>[32]</cite> where an image description architecture (CNN+LSTM) first generates a completion that is next compared against the candidates in the word2vec space (see section 3 for more details). Moreover, since the \"Ask Your Neurons\" architecture [17] is more suitable for the question answering task, we extend that method to do comparisons directly in the embedding space (\"Embedded CNN+LSTM\" in Table 5 ). Note that, here we feed the sentence prompt to LSTM even though it is fixed per category.",
  "y": "extends"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_30",
  "x": "Note that, here we feed the sentence prompt to LSTM even though it is fixed per category. Table 5 shows the performance of different methods. Our \"Embedded CNN+LSTM\" outperforms other methods on both tasks confirming our hypothesis. \"Ask Your Neurons\" [17] is also slightly better than the original CNN+LSTM <cite>[32]</cite> (on the 10 categories that the results for CNN+LSTM are available it achieves 49.8% accuracy on the easy task, which is 2.1 percentage points higher than CNN+LSTM).",
  "y": "differences"
 },
 {
  "id": "866cc7036c626f07fba10ab2a839d8_31",
  "x": "**CONCLUSION** We study an image representation formed by averaging over representations of object proposals, and show its effectiveness through experimental evaluation on the <cite>Visual Madlibs dataset</cite> <cite>[32]</cite> . We achieve state of the art performance on the multi-choice \"filling the blank\" task.",
  "y": "uses"
 },
 {
  "id": "87a190b1df5a7a941ba7b9a98064a3_0",
  "x": "Similar to<cite> Zhang et al. (2006)</cite> , we employ a convolution parse tree kernel in order to model syntactic structures. Different from their method, we use the convolution parse tree kernel expanded with entity information other than a composite kernel. One of our motivations is to capture syntactic and semantic information in a single parse tree for further graceful refinement, the other is that we can avoid the difficulty with tuning parameters in composite kernels.",
  "y": "similarities uses"
 },
 {
  "id": "87a190b1df5a7a941ba7b9a98064a3_1",
  "x": "Figure 1: Different representations of a relation instance in the example sentence \"in many cities, angry crowds roam the streets.\", which is excerpted from the ACE2004 corpus, where a relation \"PHSY.Located\" holds between the first entity \"crowds\"(PER) and the second entity \"streets\" (FAC). We employ the same convolution tree kernel used by Collins and Duffy (2001) , Moschitti (2004) and<cite> Zhang et al. (2006)</cite> . This convolution tree kernel counts the number of subtrees that have similar productions on every node between two parse trees.",
  "y": "similarities uses"
 },
 {
  "id": "87a190b1df5a7a941ba7b9a98064a3_2",
  "x": "The four cases is listed as follows: (1) Compressed Path-enclosed Tree (CPT, T1 in Fig.1 ): Originated from PT in<cite> Zhang et al. (2006)</cite> , we further make two kinds of compression. One is to prune out the children nodes right before the second entity under the same parent node of NP.",
  "y": "uses"
 },
 {
  "id": "87a190b1df5a7a941ba7b9a98064a3_3",
  "x": "(2) Bottom-attached CPT (B-CPT, T2 in Fig.1 ): the entity type information is attached to the bottom of the entity node, i.e., two more nodes whose tags are \"TP\" are added under the first and the second entity nodes respectively. (3) Entity-attached CPT (E-CPT, T3 in Fig.1 ): the entity type name is combined with entity order name, e.g. \"E1-PER\" denotes the first entity whose type is \"PER\". This case is also explored by<cite> Zhang et al. (2006)</cite> , and we include it here just for the purpose of comparison.",
  "y": "similarities"
 },
 {
  "id": "87a190b1df5a7a941ba7b9a98064a3_4",
  "x": "It shows that our system slightly outperforms recently best-reported systems. Compared with the composite kernel<cite> (Zhang et al, 2006)</cite> , our system further prunes the parse tree and incorporates entity features into the convolution parse tree kernel. It shows that our system achieves higher precision, lower recall and slightly better F-measure than their method.",
  "y": "differences"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_0",
  "x": "However, the authors focus on the alignment between source speech utterances and their text translation without proposing a complete end-to-end translation system. The first attempt to build an end-to-end speech-to-text translation system (which does not use source language) is <cite>our own work</cite> <cite>[2]</cite> but it was applied to a synthetic (TTS) speech corpus. A similar approach was then proposed and evaluated on a real speech corpus by [3] .",
  "y": "background"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_1",
  "x": "The first attempt to build an end-to-end speech-to-text translation system (which does not use source language) is <cite>our own work</cite> <cite>[2]</cite> but it was applied to a synthetic (TTS) speech corpus. A similar approach was then proposed and evaluated on a real speech corpus by [3] . This paper is a follow-up of <cite>our previous work</cite> <cite>[2]</cite> .",
  "y": "extends"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_2",
  "x": "We now investigate end-to-end speech-to-text translation on a corpus of audiobooks -LibriSpeech [4] -specifically augmented to perform end-to-end speech translation [5] . While previous works <cite>[2,</cite> 3] investigated the extreme case where source language transcription is not available during learning nor decoding (unwritten language scenario defined in [6, 7] ), we also investigate, in this paper, a midway case where a certain amount of source language transcription is available during training. In this intermediate scenario, a unique (endto-end) model is trained to decode source speech into target text through a single pass (which can be interesting if compact speech translation models are needed).",
  "y": "extends"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_3",
  "x": "This paper is organized as follows: after presenting our corpus in section 2, we present our end-to-end models in section 3. Section 4 describes our evaluation on two datasets: the synthetic dataset used in <cite>[2]</cite> and the audiobook dataset described in section 2. Finally, section 5 concludes this work.",
  "y": "uses"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_4",
  "x": "We also mirror our experiments on the BTEC synthetic speech corpus, as a follow-up to <cite>[2]</cite> . ---------------------------------- **END-TO-END MODELS**",
  "y": "uses"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_5",
  "x": "---------------------------------- **END-TO-END MODELS** For the three tasks, we use encoder-decoder models with attention [9, 10, 11, <cite>2,</cite> 3] .",
  "y": "uses"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_6",
  "x": "The speech encoder is a mix between the convolutional encoder presented in [3] and our previously proposed encoder <cite>[2]</cite> . It takes as input a sequence of audio features: x = (x 1 , . . . , x Tx ) \u2208 R Tx\u00d7n . Like <cite>[2]</cite> , these features are given as input to two non-linear (tanh) layers, which output new features of size n .",
  "y": "uses"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_7",
  "x": "The speech encoder is a mix between the convolutional encoder presented in [3] and our previously proposed encoder <cite>[2]</cite> . It takes as input a sequence of audio features: x = (x 1 , . . . , x Tx ) \u2208 R Tx\u00d7n . Like <cite>[2]</cite> , these features are given as input to two non-linear (tanh) layers, which output new features of size n .",
  "y": "similarities"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_8",
  "x": "**MODEL SETTINGS** Speech files were preprocessed using Yaafe [13] , to extract 40 MFCC features and frame energy for each frame with a step size of 10 ms and window size of 40 ms, following [14, <cite>2]</cite> . We tokenize and lowercase all the text, and normalize the punctuation, with the Moses scripts 3 for LibriSpeech are of size 46 for English (transcriptions) and 167 for French (translation).",
  "y": "uses"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_9",
  "x": "Contrary to <cite>[2]</cite> , we only present mono-reference results. ASR mono ASR multi MT mono MT multi Fig. 1 : Augmented LibriSpeech Dev BLEU scores for the MT task, and WER scores for the ASR task, with the initial (mono-task) models, and when multi-task training picks up. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_10",
  "x": "The large improvements on MT and AST on the BTEC corpus, compared to <cite>[2]</cite> are mostly due to our use of a better decoder, which outputs characters instead of words. 6 BLEU End-to-End Pre-train Multi-task Fig. 2 : Dev BLEU scores on 3 models for end-to-end AST of audiobooks. Best scores on the dev set for the end-to-end (mono-task), pre-train and multi-task models were achieved at steps 369k, 129k and 95k.",
  "y": "differences"
 },
 {
  "id": "87a87855d67d90c691ab5bedb4d460_11",
  "x": "---------------------------------- **CONCLUSION** We present baseline results on End-to-End Automatic Speech Translation on a new speech translation corpus of audiobooks, and on a synthetic corpus extracted from BTEC (follow-up to <cite>[2]</cite> ).",
  "y": "extends"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_0",
  "x": "Very recently, <cite>[8]</cite> proposed a multi-modal <cite>encoder-decoder framework</cite> that, given an image caption, jointly predicts another caption and the features of associated image. <cite>The work</cite> showed promising results for further improving general sentence representations by grounding them visually. However, according to <cite>the model</cite>, visual association only occurs at the final hidden state of the encoder, potentially limiting the effect of visual grounding.",
  "y": "background"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_1",
  "x": "However, how general sentence representations can be benefited from visual grounding has not been fully explored yet. Very recently, <cite>[8]</cite> proposed a multi-modal <cite>encoder-decoder framework</cite> that, given an image caption, jointly predicts another caption and the features of associated image. <cite>The work</cite> showed promising results for further improving general sentence representations by grounding them visually. However, according to <cite>the model</cite>, visual association only occurs at the final hidden state of the encoder, potentially limiting the effect of visual grounding.",
  "y": "motivation"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_2",
  "x": "Very recently, <cite>[8]</cite> proposed a multi-modal <cite>encoder-decoder framework</cite> that, given an image caption, jointly predicts another caption and the features of associated image. <cite>The work</cite> showed promising results for further improving general sentence representations by grounding them visually. However, according to <cite>the model</cite>, visual association only occurs at the final hidden state of the encoder, potentially limiting the effect of visual grounding.",
  "y": "background"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_3",
  "x": "Very recently, <cite>[8]</cite> proposed a multi-modal <cite>encoder-decoder framework</cite> that, given an image caption, jointly predicts another caption and the features of associated image. <cite>The work</cite> showed promising results for further improving general sentence representations by grounding them visually. However, according to <cite>the model</cite>, visual association only occurs at the final hidden state of the encoder, potentially limiting the effect of visual grounding.",
  "y": "motivation"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_4",
  "x": "There have been significant studies focusing on improving word embeddings [16, 17] , phrase embeddings [18] , sentence embeddings <cite>[8</cite>, 19] , language models [20] through multi-modal learning of vision and language. Among all studies, <cite>[8]</cite> is the first to apply skip-gram-like intuition (predicting multiple modalities from langauge) to joint learning of language and vision in the perspective of general sentence representations. Attention Mechanism in Multi-Modal Semantics.",
  "y": "background"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_5",
  "x": "Image captioning [11] [12] [13] [14] and image synthesis [15] are two common tasks. There have been significant studies focusing on improving word embeddings [16, 17] , phrase embeddings [18] , sentence embeddings <cite>[8</cite>, 19] , language models [20] through multi-modal learning of vision and language. Among all studies, <cite>[8]</cite> is the first to apply skip-gram-like intuition (predicting multiple modalities from langauge) to joint learning of language and vision in the perspective of general sentence representations.",
  "y": "background"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_6",
  "x": "There have been significant studies focusing on improving word embeddings [16, 17] , phrase embeddings [18] , sentence embeddings <cite>[8</cite>, 19] , language models [20] through multi-modal learning of vision and language. Among all studies, <cite>[8]</cite> is the first to apply skip-gram-like intuition (predicting multiple modalities from langauge) to joint learning of language and vision in the perspective of general sentence representations. Attention Mechanism in Multi-Modal Semantics.",
  "y": "background"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_7",
  "x": "---------------------------------- **VISUALLY GROUNDED ENCODER-DECODER FRAMEWORK** We base our model on the <cite>encoder-decoder framework</cite> introduced in <cite>[8]</cite> .",
  "y": "uses"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_8",
  "x": "Although margin ranking loss has been the dominant choice for training cross-modal feature matching <cite>[8</cite>, 20, 25] , we find that log-exp-sum pairwise ranking [26] yields better results in terms of evaluation performance and efficiency. Thus, the objective for ranking where N is the set of negative examples and sim is cosine similarity.",
  "y": "differences"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_9",
  "x": "Following the experimental design of <cite>[8]</cite> , we conduct experiments on three different learning objectives: CAP2ALL, CAP2CAP, CAP2IMG. Under CAP2ALL, the model is trained to predict both the target caption and the associated image: L = L C + L V G . Under CAP2CAP, the model is trained to predict only the target caption (L = L C ) and, under CAP2IMG, only the associated image (L = L V G ).",
  "y": "uses"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_10",
  "x": "We evaluate sentence representation quality using SentEval 2 <cite>[8</cite>, 10] scripts. Mini-batch size is 128 and negative samples are prepared from remaining data samples in the same mini-batch. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_11",
  "x": "**EVALUATION** Adhering to the experimental settings of <cite>[8]</cite> , we concatenate sentence representations produced from our model with those obtained from the state-of-the-art unsupervised learning model (Layer Normalized Skip-Thoughts, ST-LN) [33] . We evaluate the quality of sentence representations produced from different variants of our encoders on well-known transfer tasks: movie review sentiment (MR) [34] , customer reviews (CR) [35] , subjectivity (SUBJ) [36] , opinion polarity (MPQA) [37] , paraphrase identification (MSRP) [38] , binary sentiment classification (SST) [39] , SICK entailment and SICK relatedness [40] .",
  "y": "uses"
 },
 {
  "id": "87af486eb2e968d2055eeab094b3f9_12",
  "x": "In order to study the effects of incorporating self-attention mechanism in joint prediction of image and language features, we examine attention vectors for selected samples from MS-COCO dataset and compare them to associated images ( Figure 1 ). These findings show that visually grounding self-attended sentence representations helps to expose word-level visual features onto sentence representations <cite>[8]</cite> .",
  "y": "extends differences"
 },
 {
  "id": "88900d3533701056f6a26bf7c68670_0",
  "x": "These MRs are expressed in domain-specific unambiguous formal meaning representation languages (MRLs). Given a training corpus of NL sentences annotated with their correct MRs, the goal of a learning system for semantic parsing is to induce an efficient and accurate semantic parser that can map novel sentences into their correct MRs. Several learning systems have been developed for semantic parsing, many of them recently (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005;<cite> Kate and Mooney, 2006)</cite> .",
  "y": "background"
 },
 {
  "id": "88900d3533701056f6a26bf7c68670_1",
  "x": "Several learning systems have been developed for semantic parsing, many of them recently (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005;<cite> Kate and Mooney, 2006)</cite> . These systems use supervised learning methods which only utilize annotated NL sentences. In this paper we present, to our knowledge, the first semi-supervised learning system for semantic parsing.",
  "y": "differences background"
 },
 {
  "id": "88900d3533701056f6a26bf7c68670_2",
  "x": "We modify KRISP, a supervised learning system for semantic parsing presented in<cite> (Kate and Mooney, 2006)</cite> , to make a semi-supervised system we call SEMISUP-KRISP. Experiments on a realworld dataset show the improvements SEMISUP-KRISP obtains over KRISP by utilizing unannotated sentences. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "88900d3533701056f6a26bf7c68670_3",
  "x": "---------------------------------- **KRISP: THE SUPERVISED SEMANTIC PARSING** Learning System KRISP (Kernel-based Robust Interpretation for Semantic Parsing) <cite>(Kate and Mooney, 2006</cite> ) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data.",
  "y": "background"
 },
 {
  "id": "88900d3533701056f6a26bf7c68670_4",
  "x": "Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data<cite> (Kate and Mooney, 2006)</cite> . ---------------------------------- **TRANSDUCTIVE SVMS**",
  "y": "background"
 },
 {
  "id": "891d0e17bf2fb79a378c2d77dda768_0",
  "x": "While application of attention [3, 4] and advanced decoding mechanisms like beam search and variation sampling <cite>[5]</cite> have shown improvements, it does not solve the underlying problem. In creative text generation, the objective is not strongly bound to the ground truth-instead the objective is to generate diverse, unique or original samples. We attempt to do this through a discriminator which can give feedback to the generative model through a cost function that encourages sampling of creative tokens.",
  "y": "motivation"
 },
 {
  "id": "891d0e17bf2fb79a378c2d77dda768_1",
  "x": "We follow a similar training procedure for GumbelGAN. Outputs are generated through sampling over a multinomial distribution for all methods, instead of argmax on the log-likelihood probabilities, as sampling has shown to produce better output quality <cite>[5]</cite> . Please refer to Supplementary Section Table 3 for training parameters of each dataset and Table 2 for hyperparameters of each encoder. We pick these values after experimentation with our validation set.",
  "y": "motivation"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_0",
  "x": "For instance, (Yin et al., 2009 ) use features derived from the sentences neighboring a given message to detect harassment on the Web. (Balci and Salah, 2015) take advantage of user features such as the gender, the number of in-game friends or the number of daily logins to detect abuse in the community of an online game. In our previous work<cite> (Papegnies et al., 2019)</cite> , we proposed a radically different method that completely ignores the textual content of the messages, and relies only on a graph-based modeling of the conversation. This is the only graph-based approach ignoring the linguistic content proposed in the context of abusive messages detection.",
  "y": "background"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_1",
  "x": "This is the only graph-based approach ignoring the linguistic content proposed in the context of abusive messages detection. Our conversational network extraction process is inspired from other works leveraging such graphs for other purposes: chat logs (Mutton, 2004) or online forums (Forestier et al., 2011) interaction modeling, user group detection (Camtepe et al., 2004) . Additional references on abusive message detection and conversational network modeling can be found in<cite> (Papegnies et al., 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_2",
  "x": "In this paper, based on the assumption that the interactions between users and the content of the exchanged messages convey different information, we propose a new method to perform abuse detection while leveraging both sources. For this purpose, we take advantage of the content- (Papegnies et al., 2017b) and graph-based<cite> (Papegnies et al., 2019</cite> ) methods that we previously developed. We propose three different ways to combine them, and compare their performance on a corpus of chat logs originating from the community of a French multiplayer online game.",
  "y": "extends"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_3",
  "x": "In this paper, based on the assumption that the interactions between users and the content of the exchanged messages convey different information, we propose a new method to perform abuse detection while leveraging both sources. For this purpose, we take advantage of the content- (Papegnies et al., 2017b) and graph-based<cite> (Papegnies et al., 2019</cite> ) methods that we previously developed. We propose three different ways to combine them, and compare their performance on a corpus of chat logs originating from the community of a French multiplayer online game.",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_4",
  "x": "In this section, we summarize the content-based method from (Papegnies et al., 2017b ) (Section 2.1) and the graph-based method from<cite> (Papegnies et al., 2019</cite> ) (Section 2.2). We then present the fusion method proposed in this paper, aiming at taking advantage of both sources of information (Section 2.3). Figure 1 shows the whole process, and is discussed through this section.",
  "y": "background"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_5",
  "x": "Representation of our processing pipeline. Existing methods refers to our previous work described in (Papegnies et al., 2017b ) (content-based method) and<cite> (Papegnies et al., 2019)</cite> (graph-based method), whereas the contribution presented in this article appears on the right side (fusion strategies). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_6",
  "x": "**GRAPH-BASED METHOD** This method corresponds to the top-left part of Figure 1 (in red). It completely ignores the content of the messages, and only focuses on the dynamics of the conversation, based on the interactions between its participants<cite> (Papegnies et al., 2019)</cite> .",
  "y": "motivation"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_7",
  "x": "All these measures are computed for each graph, and allow describing the conversation surrounding the message of interest. The SVM is then trained using these values as features. In this work, we use exactly the same measures as in<cite> (Papegnies et al., 2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_8",
  "x": "---------------------------------- **EXPERIMENTAL PROTOCOL** The dataset is the same as in our previous publications (Papegnies et al., 2017b <cite>(Papegnies et al., , 2019</cite> .",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_9",
  "x": "We use the values matching the best performance, obtained during the greedy search of the parameter space performed in<cite> (Papegnies et al., 2019)</cite> . In particular, regarding the two most important parameters (see Section 2.2), we fix the context period size to 1,350 messages and the sliding window length to 10 messages. Implementation-wise, we use the iGraph library (Csardi and Nepusz, 2006) to extract the conversational networks and process the corresponding features.",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_10",
  "x": "Because of the relatively small dataset, we set-up our experiments using a 10-fold cross-validation. Each fold is balanced between the Abuse and Non-abuse classes, 70% of the dataset being used for training and 30% for testing. Table 1 presents the Precision, Recall and F -measure scores obtained on the Abuse class, for both baselines (Content-based (Papegnies et al., 2017b) and Graph-based<cite> (Papegnies et al., 2019)</cite> ) and all three proposed fusion strategies (Early Fusion, Late Fusion and Hybrid Fusion).",
  "y": "uses"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_11",
  "x": "The second is the tf -idf score computed over the Abuse class, which shows that considering term frequencies indeed improve the classification performance. The third is the Capital Ratio (proportion of capital letters in the comment), which is likely to be caused by abusive message tending to be shouted, and therefore written in capitals. The Graph-Based TF are discussed in depth in our previous article<cite> (Papegnies et al., 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "8abb7b77fd6996a905395de9693d42_12",
  "x": "We take advantage of the methods that we previously developed to leverage message content (Papegnies et al., 2017a) and interactions between users<cite> (Papegnies et al., 2019)</cite> , and create a new method using both types of information simultaneously. We show that the features extracted from our content-and graph-based approaches are complementary, and that combining them allows to sensibly improve the results up to 93.26 (F -measure). One limitation of our method is the computational time required to extract certain features.",
  "y": "extends"
 },
 {
  "id": "8abffa3f807bad5ae2073aa7db215d_0",
  "x": "Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding <cite>[11,</cite> 12, 13] , which considers syntactic contexts rather",
  "y": "background"
 },
 {
  "id": "8abffa3f807bad5ae2073aa7db215d_1",
  "x": "Another different context type is dependency-based word embedding <cite>[11,</cite> 12, 13] , which considers syntactic contexts rather The 2016 Conference on Computational Linguistics and Speech Processing ROCLING 2016, pp. 100-102 \uf0d3 The Association for Computational Linguistics and Chinese Language Processing 100 than window contexts in word2vec.",
  "y": "background"
 },
 {
  "id": "8abffa3f807bad5ae2073aa7db215d_2",
  "x": "100-102 \uf0d3 The Association for Computational Linguistics and Chinese Language Processing 100 than window contexts in word2vec. Bansal et al. [8] and <cite>Melamud et al. [11]</cite> show the benefits of such modified-context embeddings in dependency parsing task. The dependency-based word embedding can relieve the problem of data sparseness, since even without occurrence of dependency word pairs in a corpus, dependency scores can be still calculated by word embeddings [12] .",
  "y": "background"
 },
 {
  "id": "8b223f35a4685d6627d29c907e4742_0",
  "x": "However, such an approach of manual analysis cannot scale to millions of songs. Caliskan et al. proposed Word Embedding Association Test (WEAT) to computationally measure biases in any text repository <cite>[5]</cite> . Their test quantifies biases by computing similarity scores between various sets of words.",
  "y": "background"
 },
 {
  "id": "8b223f35a4685d6627d29c907e4742_1",
  "x": "Caliskan et al. proposed Word Embedding Association Test (WEAT) to computationally measure biases in any text repository <cite>[5]</cite> . We apply the WEAT test on song lyrics and discuss its implications.",
  "y": "uses background"
 },
 {
  "id": "8b223f35a4685d6627d29c907e4742_2",
  "x": "Caliskan et al. designed the Word Embedding Association Test (WEAT) by tweaking the IAT test <cite>[5]</cite> . Similar to the IAT test, this test can measure bias given the sets of attribute and target words. However, the IAT test requires human subjects to compute the bias value.",
  "y": "background"
 },
 {
  "id": "8b223f35a4685d6627d29c907e4742_3",
  "x": "Due to the small size of popular songs dataset, we cannot apply the WEAT test separately on popular songs lyrics. Please refer to Table 1 . Corresponding to eight rows of the table, we have measured eight biases. We borrowed these attribute and target word sets from Caliskan et al. <cite>[5]</cite> . First two columns (w2v and FT) correspond to measurements on the song lyrics dataset with word2vec and Out of all tests, we can see that the effect size of both FT and CA column is highest for test 4.",
  "y": "uses"
 },
 {
  "id": "8b5e14bdf3f415725333de672be114_0",
  "x": "The history of assessing readability using simple arithmetic metrics dates back to the 1920s when Thorndike (1921) has measured difficulty of texts by tabulating words according to the frequency of their use in general literature. Most of the traditional readability formulas were also based on countable features of text, such as syllable counts (Flesch, 1948) . More advanced machine learning techniques such as classification and regression have been applied to the task of reading level prediction (Collins- Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009; <cite>Feng et al., 2010)</cite> ; such works are described in further detail in the next Section 2.",
  "y": "background"
 },
 {
  "id": "8b5e14bdf3f415725333de672be114_1",
  "x": "We are interested in exploring how much performance will change by completely eliminating manual intervention. At the same time, we have also extended our previous feature set by introducing a richer set of automatically derived textbased features, proposed by<cite> Feng et al. (2010)</cite> , which capture deeper syntactic complexities of the text. Unlike our previous work, the major goal of this paper is not trying to compare different machine learning techniques used in readability assessment task, but rather to compare the performance differences between with and without human labor involved within our previous proposed system framework.",
  "y": "uses"
 },
 {
  "id": "8b5e14bdf3f415725333de672be114_2",
  "x": "They combined traditional methods of readability assessment and the features from language models and parsers. Aluisio et al. (2010) have developed a tool for text simplification for the authoring process which addresses lexical and syntactic phenomena to make text readable but their assessment takes place at more coarse levels of literacy instead of finer-grained levels used for children's books. A detailed analysis of various features for automatic readability assessment has been done by<cite> Feng et al. (2010)</cite> .",
  "y": "background"
 },
 {
  "id": "8b5e14bdf3f415725333de672be114_3",
  "x": "Feng et al. (2010) found several parsing-based features and part-of-speech based features to be useful. We utilize the Stanford Parser (Klein and Manning, 2003) to extract the following features from the XML files based on those used in<cite> (Feng et al., 2010)</cite> : ----------------------------------",
  "y": "uses"
 },
 {
  "id": "8b5e14bdf3f415725333de672be114_4",
  "x": "**INCORPORATING STRUCTURAL FEATURES** Our previous study demonstrated that combining surface features with visual features produces promising results. As mentioned above, the second aim of this study is to see how much benefit we can get from incorporating high-level structural features, such as those used in<cite> (Feng et al., 2010)</cite> (described in Section 4.2), with the features in our previous study.",
  "y": "uses"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_0",
  "x": "However, informal arguments in online argumentative discourses can exhibit different styles. Recent work has begun to model different aspects of these naturally occurring lay arguments, with tasks including stance classification (Somasundaran and Wiebe, 2009; Walker et al., 2012) , argument summarization (Misra et al., 2015) , sarcasm detection (Justo et al., 2014) and classification of propositions and arguments <cite>(Park and Cardie, 2014</cite>; Park et al., 2015; Oraby et al., 2015) . Of particular interest is the fact that arguments in online user comments, unlike those written by professionals, often have inappropriate or missing justifications.",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_1",
  "x": "Recognizing such propositions and determining the appropriate types of support can be useful for assessing the strength of the supporting information and, in turn, the strength of the whole argument. To this end, two previous studies have produced data sets and methods for classifying propositions in online argumentative discourse. The first of these studies<cite> (Park and Cardie, 2014)</cite> compiled online user comments from a discussion website and developed a framework for automatically classifying each proposition as either \"unverifiable\", \"verifiable non-experiential\", or \"verifiable experiential\", where the appropriate types of support are reason, evidence, and optional evidence, respectively.",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_2",
  "x": "In classifying propositions,<cite> Park and Cardie (2014)</cite> followed previous work such as Reed et al. (2008) and Palau and Moens (2009) , employing supervised learning methods. Despite using a rich set of linguistic features, these approaches suffer from low accuracy. Moreover, generating these features can be a tedious and complex process.",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_3",
  "x": "In classifying propositions,<cite> Park and Cardie (2014)</cite> followed previous work such as Reed et al. (2008) and Palau and Moens (2009) , employing supervised learning methods. Despite using a rich set of linguistic features, these approaches suffer from low accuracy. In this paper, we show that state-of-the-art performance in claim classification for online user comments can be achieved without the need for expensive features.",
  "y": "motivation differences"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_4",
  "x": "(Merely using clause tags without capturing dependencies for important clauses may not help much in distinguishing objective verifiable claims from unverifiable subjective ones.)<cite> Park and Cardie (2014)</cite> also used tense and person counts for distinguishing verifiable claims from unverifiable claims. We hypothesize that word2vec and dependency context-based embeddings can inherently capture these linguistic characteristics and can replace these features. Dependency context based embeddings capture functional similarities across the words using different contexts (Levy and Goldberg, 2014) .",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_5",
  "x": "Park and Cardie (2014) extracted clause-specific features using the Stanford syntactic parser and the Penn Treebank. (Merely using clause tags without capturing dependencies for important clauses may not help much in distinguishing objective verifiable claims from unverifiable subjective ones.)<cite> Park and Cardie (2014)</cite> also used tense and person counts for distinguishing verifiable claims from unverifiable claims. We hypothesize that word2vec and dependency context-based embeddings can inherently capture these linguistic characteristics and can replace these features.",
  "y": "extends"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_6",
  "x": "In online argumentative discourse, claims often serve as implicit arguments with inappropriate or missing justification<cite> (Park and Cardie, 2014)</cite> . The certainty and factuality signals present in such claims may be appropriate for determining its factuality or verifiability. As the claims in our data set are objective, subjective and factual types, predicates, adverbs and other modals (related to certainty and factuality) present in FactBank 1.0 may help in better distinguishing various types of claims.",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_7",
  "x": "This corpus consists of 9476 manually annotated sentences and independent clauses from 1047 user comments extracted from the Regulation Room website. 2<cite> Park and Cardie (2014)</cite> and Park et al. (2015) used this corpus for examining each proposition with respect to its verifiability to determine the desirable types of support for the analysis of arguments. The propositions are manually annotated with three classes-\"verifiable experiential\", \"verifiable non-experiential\", and \"unverifiable\"-where the support types are evidence, optional evidence, and reason, respectively.",
  "y": "background"
 },
 {
  "id": "8c202e3610599c9eee23724ef213de_8",
  "x": "We also use a development set to tune the hyper-parameters of the model. This corpus consists of 9476 manually annotated sentences and independent clauses from 1047 user comments extracted from the Regulation Room website. 2<cite> Park and Cardie (2014)</cite> and Park et al. (2015) used this corpus for examining each proposition with respect to its verifiability to determine the desirable types of support for the analysis of arguments.",
  "y": "similarities"
 },
 {
  "id": "8c530e0c9f7256ac44b1a2adfaf6a9_0",
  "x": "The shared task focuses on identifying emotions namely Angry, Happy, Sad and Others from conversation with three turns. Since, emotion detection is a classification problem, research works have been carried out by using machine learning with lexical features (Sharma et al., 2017) and deep learning with deep neural network (Phan et al., 2016) and convolutional neural network<cite> (Zahiri and Choi, 2018)</cite> to detect the emotions from text. However, we have adopted Seq2Seq deep neural network for detecting the emotions from textual conversations which include sequence of phrases.",
  "y": "background"
 },
 {
  "id": "8c530e0c9f7256ac44b1a2adfaf6a9_1",
  "x": "**RELATED WORK** This section reviews the research work reported for emotion detection from text / tweets (Perikos and Hatzilygeroudis, 2013; Rao, 2016; AbdulMageed and Ungar, 2017; Samy et al., 2018; AlBalooshi et al., 2018; Gaind et al., 2019 ) and text conversations (Phan et al., 2016; Sharma et al., 2017;<cite> Zahiri and Choi, 2018)</cite> . Sharma et al. (2017) proposed a methodology to create a lexicon -a vocabulary consisting of positive and negative expressions.",
  "y": "uses"
 },
 {
  "id": "8c530e0c9f7256ac44b1a2adfaf6a9_2",
  "x": "The topic vectors resembling an image are fed to CNN to learn the contextual information. Abdul-Mageed and Ungar (2017) built a very large dataset with 24 fine-grained types of emotions and classified the emotions using gated RNN. Instead of using basic CNN, a new recurrent sequential CNN is used by <cite>Zahiri and Choi (2018)</cite> .",
  "y": "background"
 },
 {
  "id": "8c530e0c9f7256ac44b1a2adfaf6a9_3",
  "x": "Instead of using basic CNN, a new recurrent sequential CNN is used by <cite>Zahiri and Choi (2018)</cite> . All the models discussed above show that the emotion prediction can be handled using variants of deep neural network such as C-GRU, G-RNN and Sequential-CNN. The commonality between the above models are the variations of RNN or LSTM. This motivated us to use the Sequenceto-Sequence (Seq2Seq) model which consists of stacked LSTMs to predic the emotion labels conditioned on the given utterance sequences.",
  "y": "motivation background"
 },
 {
  "id": "8dbc779d455ad72def6654564f9e13_0",
  "x": "Here we investigate the unsupervised word discovery and segmentation task, using the bilingual-rooted approach from <cite>Godard et al. (2018)</cite> . There, words in the well-resourced language are aligned to unsegmented phonemes in the endangered language in order to identify group of phonemes, and to cluster them into word-like units. We experiment with the Mboshi-French parallel corpus, translating the French text into four other well-resourced languages in order to investigate language impact in this CLD approach.",
  "y": "uses"
 },
 {
  "id": "8dbc779d455ad72def6654564f9e13_1",
  "x": "1 The languages added to the dataset are: English, German, Portuguese and Spanish. Table 1 shows some statistics for the produced Multilingual Mboshi parallel corpus. 2 Bilingual Unsupervised Word Segmentation/Discovery Approach: We use the bilingual neuralbased Unsupervised Word Segmentation (UWS) approach from <cite>Godard et al. (2018)</cite> to discover words in Mboshi.",
  "y": "uses"
 },
 {
  "id": "8dbc779d455ad72def6654564f9e13_2",
  "x": "Multilingual Leveraging: In this work we apply two simple methods for including multilingual information into the bilingual models from <cite>Godard et al. (2018)</cite> . The first one, Multilingual Voting, consists of merging the information learned by models trained with different language pairs by performing a voting over the final discovered boundaries. The voting is performed by applying an agreement threshold T over the output boundaries.",
  "y": "uses"
 },
 {
  "id": "8e0dcaec15a3b9c4947946a4e885c8_0",
  "x": "Similarly, Bollegala et al. (Bollegala et al., 2017) has focused on finding a linear transformation between count-based and prediction-based embeddings, showing that linearly transformed count-based embeddings can be used for predictions in the localized neighborhoods in the target space. Most recent work<cite> (Bao and Bollegala, 2018</cite> ) has focused on the use of an autoencoder (AE) to encode a set of N pretrained embeddings using 3 different variants: (1) Decoupled Autoencoded Meta Embeddings (DAEME) that keep activations separated for each respective embedding input during encoding and uses a reconstruction loss for both predicted embeddings while minimizing the loss for each respective decoded output, (2) Coupled Autoencoded Meta Embeddings (CAEME) which instead learn to predict from a shared encoding and (3) Averaged Autoencoded Meta-Embedding (AAME) is simply an averaging of the embedding set as input instead of using a concatenation. This is the most relevant work to our paper, hence, we include these 3 autoencoding schemes along with aforementioned methods for experiments, described in Section 3.",
  "y": "background"
 },
 {
  "id": "8e0dcaec15a3b9c4947946a4e885c8_1",
  "x": "Before describing the loss functions used, we explain the aforementioned variation on the autoencoding method and how it slightly differs from 1TON/1TON + (Yin and Sch\u00fctze, 2015) and standard AEs<cite> (Bao and Bollegala, 2018)</cite> presented in previous work. Target Autoencoders (TAE) are defined as learning an ensemble of nonlinear transformations between sets of bases X s in sets of vector spaces X S = {X 1 , .., X s , .., X N } s.t X s \u2208 R |vs|\u00d7ds to a target space X t \u2208 R |vt|\u00d7dt , where f \u2192 X t \u2200i is the nonlinear transformation function used to make the mapping.",
  "y": "differences uses"
 },
 {
  "id": "8e0dcaec15a3b9c4947946a4e885c8_2",
  "x": "This means thats all vector spaces have been mapped to a target space and there hidden meta-word representations have been averaged, as illustrated in Figure 1 . Figure 2 shows a comparison of the previous autoencoder approaches<cite> (Bao and Bollegala, 2018)</cite> (left) and the alternative AE (right), where dashed lines indicate connections during training and bold lines indicate prediction. The ConcatAutoEncoder (CAEME) simply concatenates the embedding set into a single vector and trains the autoencoder so to produce a lower-dimensional representation (shown in red), while the decoupled autoencoder (DAEME) keeps the embedding vectors separate in the encoding.",
  "y": "uses background"
 },
 {
  "id": "8e0dcaec15a3b9c4947946a4e885c8_3",
  "x": "The above parameters were chosen (h d ,x and number of epochs) over a small grid search. As stated, we compare against previous methods (Yin and Sch\u00fctze, 2015;<cite> Bao and Bollegala, 2018</cite> ) that use 2 distance, as shown in Equation 1). Similarly, the Mean Absolute Error ( 1 norm of difference) loss 1 N N i=1 |y \u2212\u0177| is tested.",
  "y": "uses"
 },
 {
  "id": "8e0dcaec15a3b9c4947946a4e885c8_4",
  "x": "The word vectors considered in the embeddings set are skipgram and cbow (Mikolov et al., 2013) , FastText (Bojanowski et al., 2016) , LexVec (Salle et al., 2016) , Hellinger PCA (HPCA) (Lebret and Collobert, 2013) and Hierarchical Document Context (HDC) (Sun et al., 2015) . We now report results on the performance of meta-embedding autoencodings with various loss functions, while also presenting target autoencoders for combinations of word embeddings and compare against existing current SoTA meta-embeddings. Table 1 shows the scaled Spearman correlation test scores, where (1) shows the original single embeddings, (2) results for standard metaembedding approaches that either apply a single mathematical operation or employ a linear projection as an encoding, (3) presents the results using autoencoder schemes by<cite> (Bao and Bollegala, 2018</cite> ) that we have used to test the various losses, (4) introduces TAE without concatenating the target Y embedding post-training with MSE loss and (5) shows the results of concatenating Y with the lower-dimensional (200-dimensions) vector that encodes all embeddings apart from the target vector.",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_0",
  "x": "Earlier works on conversation summarization have mainly focused on extractive techniques. However, as pointed out in (Murray et al., 2010) and<cite> (Oya et al., 2014)</cite> , abstractive summaries are preferred to extractive ones by human judges. The possible reason for this is that extractive techniques are not well suited for the conversation summarization, since there are style differences between spoken conversations and humanauthored summaries.",
  "y": "motivation"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_1",
  "x": "The possible reason for this is that extractive techniques are not well suited for the conversation summarization, since there are style differences between spoken conversations and humanauthored summaries. Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (Mehdad et al., 2013; <cite>Oya et al., 2014)</cite> . The authors cluster conversation sentences/utterances into communities to identify most relevant ones and aggregate them using word-graph models.",
  "y": "background"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_2",
  "x": "In this paper we evaluate a set of heuristics for automatic linking of summary and conversations sentences, i.e. 'community' creation. The heuristics rely on the similarity between the two, and we experiment with the cosine similarity computation on different levels of representation -raw text, text after replacing the verbs with their WordNet SynSet IDs, and the similarity computed using distributed word embeddings. The heuristics are evaluated within the template-based abstractive summarization system of<cite> Oya et al. (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_3",
  "x": "---------------------------------- **TEMPLATE GENERATION** Template Generation follows the approach of<cite> (Oya et al., 2014)</cite> and, starting from human-authored summaries, produces abstract templates applying slot labeling, summary clustering and template fusion steps.",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_4",
  "x": "The clustered templates are further generalized using a word graph algorithm extended to templates in<cite> (Oya et al., 2014)</cite> . The paths in the word graph are ranked using language models trained on the abstract templates and the top 10 are selected as a template for the cluster. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_5",
  "x": "**SENTENCE RANKING** Since the system produces many sentences that might repeat the same information, the final set of automatic sentences is selected from these filled templates with respect to the ranking using the token and part-of-speech tag 3-gram language models. In this paper, different from<cite> (Oya et al., 2014)</cite> , the sentence ranking is based solely on the n-gram language models trained on the tokens and part-ofspeech tags from the human-authored summaries.",
  "y": "differences"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_6",
  "x": "The AMI meeting corpus (Carletta et al., 2006 ) is a collection of 139 meeting records where groups of people are engaged in a 'roleplay' as a team and each speaker assumes a certain role in a team (e.g. project manager (PM)). Following<cite> (Oya et al., 2014)</cite> , we removed 20 dialogs used by the authors for development, and use the remaining dialogs for the threefold cross-validation. The LUNA Human-Human corpus (Dinarelli et al., 2009 ) consists of 572 call-center dialogs where a client and an agent are engaged in a problem solving task over the phone.",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_7",
  "x": "For AMI corpus, following<cite> (Oya et al., 2014)</cite> , we report ROUGE-2 F-measures on 3-fold cross-validation. For LUNA Corpus, on the other hand, we have used the modified version of ROUGE 1.5.5 toolkit from the CCCS Shared Task , which was adapted to deal with a conversation-dependent length limit of 7%. Unlike the AMI Corpus, the official reported results for the CCCS Shared Task were recall; thus, for LUNA Corpus the reported values are ROUGE-2 recall.",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_8",
  "x": "Following the Call-Center Conversation Summarization Shared Task at MultiLing 2015 , for LUNA Corpus (Dinarelli et al., 2009) we compare performances to three extractive baselines: (1) the longest turn in the conversation up to the length limit (7% of a conversation) (Baseline-L), (2) the longest turn in the first 25% of the conversation up to the length limit (Baseline-LB) (Trione, 2014) , and (3) Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998 ) with \u03bb = 0.7. For AMI corpus, on the other hand, we compare performances to the abstractive systems reported in<cite> (Oya et al., 2014)</cite> . The performances of the heuristics on AMI corpus are given in Table 1 .",
  "y": "uses"
 },
 {
  "id": "8ef47e16cd41aa3a606cf21c41adb7_9",
  "x": "In the table we also report the performances of the previously published summarization systems that make use of the manual communities -<cite> (Oya et al., 2014)</cite> and (Mehdad et al., 2013) ; and our run of the system of<cite> (Oya et al., 2014)</cite> . With manual communities we have Table 2 : ROUGE-2 recall with 7% summary length limit for the extractive baselines and abstractive summarization systems with the community creation heuristics on LUNA corpus. obtained average F-measure of 0.072.",
  "y": "uses"
 },
 {
  "id": "8f0aab7fd30ffc56cc477b25e6bb16_0",
  "x": "**** \u2022 Introduction Michael Ellsworth (International Computer Science Institute, infinity@icsi.berkeley.edu) has been involved with FrameNet for well over a decade. His chief focus is on semantic relations in FrameNet (Ruppenhofer et al. 2006) , how they can be used for paraphrase <cite>(Ellsworth & Janin 2007)</cite> , and mapping to other resources (Sche\u21b5czyk & Ellsworth 2006; Ferr\u00e1ndez et al. 2010b) .",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_0",
  "x": "Top row: baseline approach by Zellers et al. <cite>[18]</cite> . Bottom row: our approach. Q:Question, Ac:Correct Answer, Ap: Predicted Answer, R: Predicted Rationale, Q\u2212 > AR: Both answer and rationale prediction given question. well beyond such trivial recognition tasks.",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_1",
  "x": "As the task is new, Zellers et al. <cite>[18]</cite> provided a new baseline for it which seeks to tackle the task of predicting answers and predicting rationales separately. At first, Figure 2 . The Visual Commonsense Reasoning task, Zellers et al. <cite>[18]</cite> answers are predicted given the question and image and then, rationales are predicted given the image and question with the correct answer (see figure 1 and 2).",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_2",
  "x": "As the task is new, Zellers et al. <cite>[18]</cite> provided a new baseline for it which seeks to tackle the task of predicting answers and predicting rationales separately. At first, Figure 2 . The Visual Commonsense Reasoning task, Zellers et al. <cite>[18]</cite> answers are predicted given the question and image and then, rationales are predicted given the image and question with the correct answer (see figure 1 and 2).",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_3",
  "x": "Precisely, the task is introduced and formulated in <cite>[18]</cite> as follows: given an image and a question related to the image, the model has to predict the correct answer from four possible choices and at the same time, it has to pick the right rationale, again from four options. As the task is new, Zellers et al. <cite>[18]</cite> provided a new baseline for it which seeks to tackle the task of predicting answers and predicting rationales separately. At first, Figure 2 .",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_4",
  "x": "By adopting these methods, we can get rid of the assumption that answer prediction network has to be 100% accurate. Concurrently, it makes our approaches incomparable to the baselines provided by Zellers et al. <cite>[18]</cite> . It is so because they always condition their rationale prediction module on correct answer and as such it is bound to be better than a model which conditions on predicted answer.",
  "y": "differences"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_5",
  "x": "The task in Zellers et al. <cite>[18]</cite> is essentially posed as a question-answering task. Although they have enforced reasoning for the network, the reasoning is still in the questionanswer format. As such, it makes sense to explore current work in visual question answer domain.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_6",
  "x": "Anderson et al. [1] propose an orthogonal work to <cite>[18]</cite> , in which Faster-RCNN [15] is used to predict the image regions the model should attend to. We note the difference from our current proposed work -annotations are provided in the VCR 1.0 dataset <cite>[18]</cite> in form of bounding box and segmentation maps. Akira et al. [6] propose a more sophisticated method to combine the feature vectors from questions and images.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_7",
  "x": "We note the difference from our current proposed work -annotations are provided in the VCR 1.0 dataset <cite>[18]</cite> in form of bounding box and segmentation maps. Akira et al. [6] propose a more sophisticated method to combine the feature vectors from questions and images. After extracting features from questions and images using LSTM and CNN respectively, they use bi-linear pooling (outer vector product) to encode the interplay between image and question representations.",
  "y": "differences"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_8",
  "x": "Zellers et al. <cite>[18]</cite> propose to jointly learn language and image representation using Bi-LSTM by feeding in image features from CNN for all annotated words. They call this step grounding. Further, query and responses is contextualized using attention mechanism.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_9",
  "x": "Finally, the attended query, attended image and response is passed through a Bi-LSTM to make final predictions. One major drawback of the work is that separate networks are trained to predict answers and to reason. We seek to build on <cite>[18]</cite> by proposing a method to jointly train prediction and reasoning networks.",
  "y": "extends"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_10",
  "x": "The network is made differentiable using an expected loss as defined in [16] . We use the same backbone architecture to predict answers and rationales as in <cite>[18]</cite> . The questions and response (answers and rationales) are provided as a combination of natural language words and tags for annotated objects in the image.",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_11",
  "x": "<cite>[18]</cite> call this step Grounding. Next, the response vector is contextualized against the question vector using attention mechanism. In this step an attended question representation is found for every token in the response.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_12",
  "x": "The dataset used in all experiments in this work is VCR 1.0 <cite>[18]</cite> . VCR dataset contains 290k multiple choice questions which has been collected from 110k movie scenes. The dataset provides object annotations, labels and classes for all objects in the image.",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_13",
  "x": "We train the model using the whole VCR dataset for 20 epochs as was done in <cite>[18]</cite> to align with the baselines. We also use gradient clipping while training. We use two losses for all our methods -answer prediction loss and rationale prediction loss, corresponding to each module.",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_14",
  "x": "As mentioned earlier, our approach is not directly comparable to the baseline provided by Zellers et al. <cite>[18]</cite> since they feed the correct answer to the rationale module while we feed in the predicted answer. As such, we generate new baseline model. State-of-the-art VQA model have at max 75% accuracy, Kim et al. [11] .",
  "y": "differences"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_15",
  "x": "Keeping this in mind we make our baseline wherein we feed to the rationale module original question appended by the correct answer 75% time and random answer 25% time. We leave the answer prediction module as is. Finally, we train the two networks separately and combine the results of answer prediction module and rationale prediction module using \"AND\" operation as was done by Zellers et al. <cite>[18]</cite> .",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_16",
  "x": "We leave the answer prediction module as is. Finally, we train the two networks separately and combine the results of answer prediction module and rationale prediction module using \"AND\" operation as was done by Zellers et al. <cite>[18]</cite> . For completeness, we also mention the results of four other baselines from <cite>[18]</cite> .",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_17",
  "x": "For completeness, we also mention the results of four other baselines from <cite>[18]</cite> . These baseline methods use the ResNet-50 (same as <cite>[18]</cite> ) visual architecture and Glove as text representations. These baselines are as follows:",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_18",
  "x": "\u2022 RevisitedVQA [9] : This is a version of VQA model which is mainly optimized for response like 'yes' and 'no'. Basically, it takes a query, response, and image features as inputs and trains by passing the result through MLP layer. \u2022 Bottom-up and Top-down attention (BottomUpTop-Down) [1] : <cite>[18]</cite> adopted this model as another baseline by passing object regions referenced by the query and response.",
  "y": "background"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_19",
  "x": "Rather, a weighted average of answers (according to probabilities predicted by Q->A module) is provided to the rationale prediction module, unlike the baseline <cite>[18]</cite> , which gives the correct answer as input. Gumbel-Softmax: For gumbel-softmax, we anneal the temperature \u03c4 from 5 to 1 for 10 epochs and then keep it constant at 1. As can be seen from 1, this model gives the best result among all the approaches we used.",
  "y": "differences"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_20",
  "x": "We conclude from this that gradient flow between the two Q->A and QA->R modules enabled by our end-to-end joint learning scheme, helps the network learn better answers for better/correct reasons. Comparison with State-of-the-art: We provide comparison of our best model against state-of-the-art for visual common sense reasoning task. We also summarize results of other baselines reported in <cite>[18]</cite> .",
  "y": "uses"
 },
 {
  "id": "90604f1ab33aafff682df0556aaa4e_21",
  "x": "As can be seen from table 3, our Gumbel-softmax method performs better than the baseline <cite>[18]</cite> in Q->A task. For the QA->R task, it is to be expected that our method should perform worse than the baseline as we are providing predicted answers, rather than the correct answer to the QA->R module. Still, our approach performs comparably against the baseline on the task.",
  "y": "differences"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_0",
  "x": "We develop a grammatical error correction (GEC) system for German using a small gold GEC corpus augmented with edits extracted from Wikipedia revision history. We extend the automatic error annotation tool ERRANT (Bryant et al., 2017) for German and use it to analyze both gold GEC corrections and Wikipedia edits (Grundkiewicz and JunczysDowmunt, 2014) in order to select as additional training data Wikipedia edits containing grammatical corrections similar to those in the gold corpus. Using a multilayer convolutional encoder-decoder neural network GEC approach<cite> (Chollampatt and Ng, 2018)</cite>, we evaluate the contribution of Wikipedia edits and find that carefully selected Wikipedia edits increase performance by over 5%.",
  "y": "uses"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_1",
  "x": "On the basis of these resources along with advances in machine translation, the current state-of-the-art English GEC systems use ensembles of neural MT models<cite> (Chollampatt and Ng, 2018)</cite> and hybrid systems with both statistical and neural MT models (Grundkiewicz and Junczys-Dowmunt, 2018) . In addition to using gold GEC corpora, which are typically fairly small in the context of MTbased approaches, research in GEC has taken a number of alternate data sources into consideration such as artificially generated errors (e.g., Wagner et al., 2007; Foster and Andersen, 2009; Yuan and Felice, 2013) , crowd-sourced corrections (e.g., Mizumoto et al., 2012) , or errors from native language resources (e.g., Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014) . For English, Grundkiewicz and JunczysDowmunt (2014) extracted pairs of edited sentences from the Wikipedia revision history and filtered them based on a profile of gold GEC data in order to extend the training data for a statistical MT GEC system and found that the addition of filtered edits improved the system's F 0.5 score b\u1ef9 2%.",
  "y": "background"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_2",
  "x": "**DATA AND RESOURCES** The following sections describe the data and resources used in our experiments on GEC for German. We create a new GEC corpus for German along with the models needed for the neural GEC approach presented in <cite>Chollampatt and Ng (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_3",
  "x": "We learn a byte pair encoding (BPE) (Sennrich et al., 2016) with 30K symbols using the corrections from the Falko-MERLIN training data plus the complete plain Wikipedia article text. As suggested by <cite>Chollampatt and Ng (2018)</cite> , we encode the Wikipedia article text using the BPE model and learn fastText embeddings (Bojanowski et al., 2017) with 500 dimensions. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_4",
  "x": "**RESULTS AND DISCUSSION** We evaluate the effect of extending the Falko-MERLIN GEC Corpus with Wikipedia edits for a German GEC system using the multilayer convolutional encoder-decoder neural network approach from <cite>Chollampatt and Ng (2018)</cite> , using the same parameters as for English. 8 We train a single model for each condition and evaluate on the Falko-MERLIN test set using M 2 scorer (Dahlmeier and Ng, 2012 The results, presented in Table 3 , show that the addition of both unfiltered and filtered Wikipedia edits to the Falko-MERLIN GEC training data lead to improvements in performance, however larger numbers of unfiltered edits (>250K) do not consistently lead to improvements, similar to the results for English in Grundkiewicz and JunczysDowmunt (2014) .",
  "y": "uses"
 },
 {
  "id": "91685660d3d689c50e7436be46f37e_6",
  "x": "We provide initial results for grammatical error correction for German using data from the Falko and MERLIN corpora augmented with Wikipedia edits that have been filtered using a new German extension of the automatic error annotation tool ERRANT (Bryant et al., 2017) . Wikipedia edits are extracted using Wiki Edits (Grundkiewicz and Junczys-Dowmunt, 2014) , profiled with ER-RANT, and filtered with reference to the gold GEC data. We evaluate our method using the multilayer convolutional encoder-decoder neural network GEC approach from <cite>Chollampatt and Ng (2018)</cite> and find that augmenting a small gold German GEC corpus with one million filtered Wikipedia edits improves the performance from 39.22 to 44.47 F 0.5 and additional language model reranking increases performance to 45.22.",
  "y": "uses"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_0",
  "x": "Testing data is TREC-10. In the experiment, questions are represented by 13 features, 9 of which are semantic features based on WordNet. <cite>Li and Roth (2002)</cite> use a Sparse Network of Winnows (SNoW) to classify questions with respect to their expected answer type.",
  "y": "background"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_1",
  "x": "Zhang and Lee (2003) used the same taxonomy as <cite>Li and Roth (2002)</cite> , as well as the same training and testing data. In an initial experiment they compared different machine learning approaches with regards to the question classification problem: Nearest Neighbors (NN), Na\u00efve Bayes (NB), Decision Trees (DT), SNoW, and Support Vector Machines. The feature extracted and used as input to the machine learning algorithms in the initial experiment was bag-of-words and bag-of-ngrams (all continuous word sequences in the question).",
  "y": "background"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_2",
  "x": "The SVM:s also used linear kernels. The same taxonomy, training and testing data was used as in <cite>Li and Roth (2002)</cite> ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_3",
  "x": "For present purposes the micro and macro sign tests established by Yang and Liu (1999) have been used. Thses were originally developed for the text categorization task, but as question classification bears many resemblances and can be seen as a special case of text categorization. The taxonomy used is the taxonomy proposed by <cite>Li and Roth (2002)</cite> .",
  "y": "uses"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_4",
  "x": "This taxonomy has been chosen since it is the most frequently used one in earlier work in the field<cite> (Li and Roth, 2002</cite>; Zhang and Lee, 2003; Hacioglu and Ward, 2003) . The corpora used is both the corpus constructed and tagged by <cite>Li and Roth (2002)</cite> , as well as a newly tagged corpus extracted from the AnswerBus logs. AnswerBus is a question answering system that has been online and logged real users questions.",
  "y": "background"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_5",
  "x": "This taxonomy has been chosen since it is the most frequently used one in earlier work in the field<cite> (Li and Roth, 2002</cite>; Zhang and Lee, 2003; Hacioglu and Ward, 2003) . The corpora used is both the corpus constructed and tagged by <cite>Li and Roth (2002)</cite> , as well as a newly tagged corpus extracted from the AnswerBus logs. AnswerBus is a question answering system that has been online and logged real users questions.",
  "y": "similarities uses"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_6",
  "x": "This experiment has been done under two different settings. First, we have used the corpus originally developed by<cite> (Li and Roth, 2002)</cite> , but since the test corpus used consists of questions solely from TREC-10 and the TREC conferences have a specific agenda the test corpus might be slightly different from the training data. Therefore, a second setting was used where the questions from the training and test corpora were pooled together and a randomized test corpus was extracted.",
  "y": "extends differences"
 },
 {
  "id": "918caabbc0bcad04cd07761b29e767_7",
  "x": "---------------------------------- **CONCLUSIONS** The results in this paper indicate that some of the results found in previous work<cite> (Li and Roth, 2002</cite>; Zhang and Lee, 2003; Hacioglu and Ward, 2003) on question classification might be incorrect due to an unbiased training and test corpus.",
  "y": "background"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_0",
  "x": "Evaluated by human graders on 500 random-selected triples from Freebase, questions generated by our system are judged to be more fluent than those of <cite>Serban et al. (2016)</cite> by human graders. ---------------------------------- **INTRODUCTION**",
  "y": "differences"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_1",
  "x": "Recently people are becoming interested in question generation from KB, since large-scale KBs, such as Freebase (Bollacker et al., 2008) and DBPedia (Auer et al., 2007) , are freely available, and entities and their relations are already present in KBs but not for texts. Question generation from KB is challenging as function words and morphological forms for entities are abstracted away when a KB is created. To tackle this challenge, previous work (Seyler et al., 2015; <cite>Serban et al., 2016</cite> ) relies on massive human-labeled data.",
  "y": "background"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_2",
  "x": "Treating question generation as a machine translation problem, <cite>Serban et al. (2016)</cite> train a neural machine translation (NMT) system with 10,000 triple 1 , question pairs. At test time, input triples are \"translated\" into questions with the NMT system. On the other hand, the question part of the 10,000 pairs are human generated, which requires a large amount of human effort.",
  "y": "background"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_3",
  "x": "Our system does not require a large number of templates because: (1) the iterative question expansion can produce a large number of questions even with a relatively small number of seed questions, as we see in the experiments, (2) multiple entities in the KB share the same predicates. Another advantage is that our system can easily generate updated questions as web is self-updating consistently. In our experiment, we compare with <cite>Serban et al. (2016)</cite> on 500 random selected triples from Freebase (Bollacker et al., 2008) .",
  "y": "uses"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_4",
  "x": "Another advantage is that our system can easily generate updated questions as web is self-updating consistently. In our experiment, we compare with <cite>Serban et al. (2016)</cite> on 500 random selected triples from Freebase (Bollacker et al., 2008) . Evaluated by 3 human graders, questions generated by our system are significantly better then <cite>Serban et al. (2016)</cite> on grammaticality and naturalness.",
  "y": "differences"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_5",
  "x": "For System grammatical naturalness <cite>Serban et al. (2016)</cite> 3.36 3.14 Ours 3.53 3.31 Table 1 : Comparing generated questions domain relevance, we take the seed question set as the in-domain data D in , the domain relevance of expanded question q is defined as: where v(\u00b7) is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as:",
  "y": "differences"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_6",
  "x": "In the first experiment, we compare our end-to-end system with the previous state-of-the-art method <cite>(Serban et al., 2016)</cite> on Freebase (Bollacker et al., 2008) , a domain-general KB. In the second experiment, we validate our domain relevance evaluation method on a standard dataset about short document classification. In the final experiment, we run our endto-end system on a highly specialized in-house KB and present sample results, showing that our system is capable of generating questions from domain specific KBs.",
  "y": "uses"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_7",
  "x": "---------------------------------- **EVALUATION ON FREEBASE** We first compare our system with <cite>Serban et al. (2016)</cite> on 500 randomly selected triples from Freebase (Bollacker et al., 2008) 2 .",
  "y": "uses"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_8",
  "x": "To evaluate the fluency of the candidate questions, we train a 4-gram language model (LM) on gigaword (LDC2011T07) with Kneser Ney smoothing. Using the averaged language model score as index, the top 500 questions are selected to compare with the results from <cite>Serban et al. (2016)</cite> . We ask three native English speakers to evaluate the fluency and the naturalness 3 of both results based on a 4-point scheme where 4 is the best.",
  "y": "uses"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_9",
  "x": "We show the averaged human rate in Table 2 , where we can see that our questions are more grammatical and natural than <cite>Serban et al. (2016)</cite> . The naturalness score is less than the grammatical score for both methods. It is because naturalness is a more strict metric since a natural question should also be grammatical.",
  "y": "differences"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_10",
  "x": "The naturalness score is less than the grammatical score for both methods. It is because naturalness is a more strict metric since a natural question should also be grammatical. Shown in Table 1 , we compare our questions with <cite>Serban et al. (2016)</cite> where questions in the same line describe the same entity.",
  "y": "uses"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_11",
  "x": "We can see that our questions are grammatical and natural as these questions are what people usually ask on the web. On the other hand, questions from <cite>Serban et al. (2016)</cite> Ma et al. (2015) 85.48 Ours 85.65 Table 3 : Precision on the web snippet dataset was someone who was involved in the leukemia ?\" and \"whats the title of a book of the subject of the bible ?\"), unnatural (\"what 's one of the mountain where can you found in argentina in netflix ?\") or confusing (\"who was someone who was involved in the leukemia ?\"). ----------------------------------",
  "y": "differences"
 },
 {
  "id": "91e4fd2556d4a04e477ea97208b218_12",
  "x": "By leveraging rich web information, our system is able to generate domain-relevant questions in wide scope, while human effort is significantly reduced. Evaluated by human graders, questions generated by our system are significantly better than these from <cite>Serban et al. (2016)</cite> on 500 random-selected triples from Freebase. We also demonstrated generated questions from our in-house KB of power tool domain, which are fluent and domain-relevant in general.",
  "y": "differences"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_0",
  "x": "---------------------------------- **LEXICAL DISTRIBUTION IN SPOKEN MANDARIN** Results presented by <cite>Tseng (2001)</cite> show that speakers of Mandarin adopt some 30 words for building core structures of utterances in conversation, independently of individual speakers.",
  "y": "background"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_1",
  "x": "Interestingly but also expected in conversational dialogues, the distribution of token frequency across all subjects is highly symmetric<cite> (Tseng 2001)</cite> . For instance, verbs \"is located\", \"is\", \"that is\", \"say\", \"want\" and \"have\" were frequently used, so were pronouns \"s/he\", \"you\" and \"I\". The negation \"don't have\" was a high-frequency word, so were words \"right\", \"this/these\" and \"that/those\".",
  "y": "background"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_2",
  "x": "They are very often observed in Mandarin spoken conversations as mentioned in <cite>Tseng (2001)</cite> and Clancy et al. (1996) . In <cite>Tseng (2001)</cite> , each subject used on average 1.6 discourse particles per turn. This result leads to the consideration, if there is a need to add special categories for discourse particles or particle-like words for spoken Mandarin.",
  "y": "background"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_3",
  "x": "In <cite>Tseng (2001)</cite> , each subject used on average 1.6 discourse particles per turn. This result leads to the consideration, if there is a need to add special categories for discourse particles or particle-like words for spoken Mandarin. Discourse particles were found to have different and specific discourse use in conversation.",
  "y": "background"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_4",
  "x": "They are very often observed in Mandarin spoken conversations as mentioned in <cite>Tseng (2001)</cite> and Clancy et al. (1996) . In <cite>Tseng (2001)</cite> , each subject used on average 1.6 discourse particles per turn. This result leads to the consideration, if there is a need to add special categories for discourse particles or particle-like words for spoken Mandarin.",
  "y": "motivation"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_5",
  "x": "Namely, there exist discourse particles appearing preferably in turn-beginning position and some other discourse particles may exclusively mark the location of repairs. Regarding the small size of data used in <cite>Tseng (2001)</cite> , it is one of the reasons why the ongoing project is necessary for research of Mandarin spontaneous conversations. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_6",
  "x": "Thus, how to annotate overlapping sequences is one of the essential tasks in developing annotation systems. In Mandarin conversation, there are words preferably used in turn-initial position<cite> (Tseng 2001</cite> , Chui 2000 . They normally have their own discourse-related pragmatic function associated with their positioning in utterances.",
  "y": "background"
 },
 {
  "id": "9227b5afd1ef18ecf83400dc402459_7",
  "x": "Prosody in Spoken Mandarin Lexical tones are typically characteristic of spoken Mandarin. The interaction of lexical tones and the other prosodic means such as stress and intonation are related to a number of research issues, particularly in conversation. Falling tones may not show falling tendency anymore, when the associated words are used for specific discourse functions such as for indicating hesitation or the beginning of a turn<cite> (Tseng 2001)</cite> .",
  "y": "background"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_0",
  "x": "**INTRODUCTION** With the growth of the Internet and rapid production of a vast amount of information, question answering systems, designed to find a relevant proper answer by searching throughout a data source, are of great importance. The production of knowledge bases and the need to answer questions over such resources received researchers attentions to propose different models to find the answer of questions from the knowledge bases, known as KBQA 1 . Answering factoid questions with one relation, also known as simple question answering, has been widely studied in recent years (Dai et al., 2016; Yin et al., 2016; He and Golub, 2016;<cite> Yu et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_1",
  "x": "Having a large number of relations in a knowledge base, however, this simple question relation extraction is not a solved problem yet. Classifying questions to predefined set of relations is one of the main approaches for this task (Mohammed et al., 2018) . Moreover, matching question content with relations has also been proposed and shown promising results (Yin et al., 2016;<cite> Yu et al., 2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_2",
  "x": "However, most of the recent approaches (Mohammed et al., 2018; Bordes et al., 2015; Dai et al., 2016; He and Golub, 2016;<cite> Yu et al., 2017)</cite> are based on automatically extracted features of terms; thanks to the prominent performance of neural network on representation learning (Mikolov et al., 2013a,b) . From another point of view, two mainstreams for extracting relations in KBQA are studied: (a) using a classifier which chooses the most probable relation among all (Mohammed et al., 2018) ; (b) matching questions and relations through learning of an embedding space for representing all relations and question words (Bordes et al., 2015; Dai et al., 2016; Yin et al., 2016; He and Golub, 2016;<cite> Yu et al., 2017)</cite> , in which each relation is considered either as a meaningful sequence of words or as a unique entity. Dai et al. (2016) considered the relation prediction, as well as the whole KBQA problem, as a conditional probability task in which the goal is finding the most probable relation given the question mention.",
  "y": "background"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_3",
  "x": "However, most of the recent approaches (Mohammed et al., 2018; Bordes et al., 2015; Dai et al., 2016; He and Golub, 2016;<cite> Yu et al., 2017)</cite> are based on automatically extracted features of terms; thanks to the prominent performance of neural network on representation learning (Mikolov et al., 2013a,b) . From another point of view, two mainstreams for extracting relations in KBQA are studied: (a) using a classifier which chooses the most probable relation among all (Mohammed et al., 2018) ; (b) matching questions and relations through learning of an embedding space for representing all relations and question words (Bordes et al., 2015; Dai et al., 2016; Yin et al., 2016; He and Golub, 2016;<cite> Yu et al., 2017)</cite> , in which each relation is considered either as a meaningful sequence of words or as a unique entity. Dai et al. (2016) considered the relation prediction, as well as the whole KBQA problem, as a conditional probability task in which the goal is finding the most probable relation given the question mention.",
  "y": "background"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_4",
  "x": "As can be seen, represents the similarity of two questions (Q' with Q). Additionally, following <cite>Yu et al. (2017)</cite> , we add another neural network (Q'-R), the right part of the architecture, to compute the matching score of Q' with the relation of Q (R). By doing so, we are enhancing the matching signals between Q' and Q to estimate the overall score.",
  "y": "extends differences"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_5",
  "x": "**DATASET** Following the previous works by Yin et al. (2016) and <cite>Yu et al. (2017)</cite> , we use the common benchmark dataset of the simple question answering, namely SimpleQuestions, which was originally introduced by Bordes et al. (2015) . This dataset contains 108442 questions gathered with the help of English-speaking annotators.",
  "y": "similarities uses"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_7",
  "x": "Proposed Q'-Q + Q'-R model 93.41 are not originally evaluated on relation prediction of simple questions. In fact, the authors of AMPCNN (Yin et al., 2016) , conducted the corresponding experiments on a one-way-attention adaptation of these two models to compare them with the available methods in this task. Hier-Res-BiLSTM<cite> (Yu et al., 2017)</cite> uses hierarchical residual connections to ease the training procedure of BiL-STM.",
  "y": "background"
 },
 {
  "id": "92d9291093f4ff9f10cca1b8ad2a27_8",
  "x": "Hier-Res-BiLSTM<cite> (Yu et al., 2017)</cite> uses hierarchical residual connections to ease the training procedure of BiL-STM. BiCNN (Yih et al., 2015) uses convolutional neural networks for matching a question with relations. The model is reimplemented for SimpleQuestions by <cite>Yu et al. (2017)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "9340338e7cf8ff8de4db84b462dfe5_0",
  "x": "However, neither of these are suitable for fact checking a claim made in natural language against a database. Previous works appropriate for this task operate on a limited domain and are not able to incorporate temporal information when checking time-dependent claims<cite> (Vlachos and Riedel, 2015)</cite> . In this paper we introduce our fact checking tool, describe its architecture and design decisions, evaluate its accuracy and discuss future work.",
  "y": "background"
 },
 {
  "id": "9340338e7cf8ff8de4db84b462dfe5_1",
  "x": "We highlight the ease of incorporating new information sources to fact check, which may be unavailable during training. To validate the extensibility of the system, we complete an additional evaluation of the system using claims taken from <cite>Vlachos and Riedel (2015)</cite> . We make the source code publicly available to the community.",
  "y": "uses"
 },
 {
  "id": "9340338e7cf8ff8de4db84b462dfe5_2",
  "x": "The types of claims the system presented can fact check was restricted to those which require looking up a value in a KB, similar to the one in Figure 1 . To learn a model to perform the KB look up (essentially a semantic parsing task), we extend the work of <cite>Vlachos and Riedel (2015)</cite> who used distant supervision (Mintz et al., 2009 ) to generate training data, obviating the need for manual labeling. In particular, we extend it to handle simple temporal expressions in order to fact check time-dependent claims appropriately, i. e. population in 2015.",
  "y": "extends"
 },
 {
  "id": "9340338e7cf8ff8de4db84b462dfe5_3",
  "x": "Analysis of our entry to this competition showed that two errors were caused by incorrect initial source data and one partial error caused by recalling a correct property but making an incorrect deduction. Of numerical claims that we did not attempt, we observed that many required looking up multiple entries and performing a more complex deduction step which was beyond the scope of this project. We further validate the system by evaluating the ability of this fact checking system to make veracity assessments on simple numerical claims from the data set collected by<cite> (Vlachos and Riedel, 2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "9340338e7cf8ff8de4db84b462dfe5_4",
  "x": "**CONCLUSIONS AND FUTURE WORK** The core capability of the system demonstration we presented is to fact check natural language claims against relations stored in a KB. Although the range of claims is limited, the system is a fieldtested prototype and has been evaluated on a published data set<cite> (Vlachos and Riedel, 2015)</cite> and on real-world claims presented as part of the HeroX fact checking challenge.",
  "y": "uses"
 },
 {
  "id": "93a1f611592ce6aa5cde7538486f97_0",
  "x": "The lexicon can be considered the most dynamic part of all linguistic knowledge sources over time. There are two innovative change strategies typical for lexical systems: the creation of entirely new lexical items, commonly reflecting the emergence of novel ideas, technologies or artifacts, on the one hand, and, on the other hand, shifts in the meaning of already existing lexical items, a process which usually takes place over larger periods of time. Tracing semantic changes of the latter type is the main focus of our research. Meaning shift has recently been investigated with emphasis on neural language models<cite> (Kim et al., 2014</cite>; Kulkarni et al., 2015) .",
  "y": "background"
 },
 {
  "id": "93a1f611592ce6aa5cde7538486f97_1",
  "x": "Neural language models, originating from the word2vec algorithm (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c) , are currently considered as state-of-the-art solutions for implementing this assumption (Schnabel et al., 2015) . Within this approach, changes in similarity relations between lexical items at two different points of time are interpreted as a signal for meaning shift. Accordingly, lexical items which are very similar to the lexical item under scrutiny can be considered as approximating its meaning at a given point in time. Both techniques were already combined in prior work to show, e.g., the increasing association of the lexical item \"gay\" with the meaning dimension of \"homosexuality\"<cite> (Kim et al., 2014</cite>; Kulkarni et al., 2015) .",
  "y": "background motivation"
 },
 {
  "id": "93a1f611592ce6aa5cde7538486f97_2",
  "x": "Neural language models for tracking semantic changes over time typically distinguish between two different training protocols-continuous training of models<cite> (Kim et al., 2014)</cite> where the model for each time span is initialized with the embeddings of its predecessor, and, alternatively, independent training with a mapping between models for different points in time (Kulkarni et al., 2015) . A comparison between these two protocols, such as the one proposed in this paper, has not been carried out before. Also, the application of such protocols to non-English corpora is lacking, with the exception of our own work relating to German data (Hellrich and Hahn, 2016b; Hellrich and Hahn, 2016a) .",
  "y": "background motivation"
 },
 {
  "id": "93a1f611592ce6aa5cde7538486f97_3",
  "x": "---------------------------------- **EXPERIMENTAL SET-UP** For comparability with earlier studies<cite> (Kim et al., 2014</cite>; Kulkarni et al., 2015) , we use the fiction part of the GOOGLE BOOKS NGRAM corpus (Michel et al., 2011; Lin et al., 2012) .",
  "y": "uses"
 },
 {
  "id": "93a1f611592ce6aa5cde7538486f97_4",
  "x": "We thus concentrate on two experimental protocols-the one described by <cite>Kim et al. (2014)</cite> (referred to as Kim protocol) and the one from Kulkarni et al. (2015) (referred to as Kulkarni protocol), including close variations thereof. Kim's protocol operates on uniformly sized samples of 10M 5-grams for each year from 1850 onwards in a continuous fashion (years before 1900 are used for initialization only). Its constant sampling sizes result in both oversampling and undersampling as is evident from Figure 1 .",
  "y": "uses background"
 },
 {
  "id": "966106c9e00f0333bda45d977a9f35_0",
  "x": "**SYSTEM DESCRIPTION** We trained Neural Machine Transaltion systems using the encoder-decoder architecture with attention (Bahdanau et al., 2014) for English-Hindi as well Hindi-English translation. We compared convolutional neural network (ConvS2S)<cite> (Gehring et al., 2017)</cite> and recurrent neural network (RNNS2S) (Bahdanau et al., 2014) based sequence to sequence learning architectures.",
  "y": "uses"
 },
 {
  "id": "966106c9e00f0333bda45d977a9f35_1",
  "x": "Hence, it is appealing to explore ConvS2S as the basis of an architecture to speed up training and decoding. Recent work<cite> (Gehring et al., 2017)</cite> has shown that a purely CNN based encoder-decoder network is competitive with a RNN based network. ----------------------------------",
  "y": "background motivation"
 },
 {
  "id": "966106c9e00f0333bda45d977a9f35_2",
  "x": "**CONVOLUTIONAL SEQUENCE TO SEQUENCE MODEL (CONVS2S)** In convolutional sequence to sequence model<cite> (Gehring et al., 2017)</cite> , the input sequence is encoded into distributional vector space using a CNN and decoded back to output sequence again using CNN instead of RNN (Sutskever et al., 2014) . Each input element embedding is combined with its positional embedding (signifies the position of the input element).",
  "y": "background"
 },
 {
  "id": "966106c9e00f0333bda45d977a9f35_3",
  "x": "For handling the rare words, the source side and target side corpora were segmented using byte pair encoding (BPE) (Shibata et al., 1999) . The baseline model with 4 encoder layers and 3 decoder layers was trained using nag optimizer<cite> (Gehring et al., 2017</cite> ) with a learning rate of 0.25 with 0.2 as its dropout value and gradient clipping was also applied. Table 6 : Hindi to English Translation Systems at WAT2017",
  "y": "uses"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_0",
  "x": "Our supervised base model will be similar to (Zhou et al., 2016) . Our initial experiments did not use syntactic features <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017 ) that require additional parsers. In order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation.",
  "y": "differences"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_1",
  "x": "**SUPERVISED NEURAL RELATION EXTRACTION MODEL** The supervised neural model on a single dataset was introduced by Zeng et al. (2014) and followed by many others (Nguyen and Grishman, 2015; Zhou et al., 2016; Miwa and Bansal, 2016;<cite> Nguyen and Grishman, 2016</cite>; Fu et al., 2017) . We use a similar model as our base model.",
  "y": "background"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_2",
  "x": "Some work <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017) used extra syntax features as input. However, the parsers that produce syntax features could have errors and vary depending on the domain of text. The syntax features learned could also be too specific for a single dataset.",
  "y": "background"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_3",
  "x": "It takes word tokens, position of arguments and their entity types as input. Some work <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017) used extra syntax features as input. However, the parsers that produce syntax features could have errors and vary depending on the domain of text.",
  "y": "motivation"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_4",
  "x": "We also convert the entity types of the arguments to entity embeddings. The entity embedding <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017 ) is included for arguments that are entities rather than common nouns.",
  "y": "uses"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_5",
  "x": "It usually contains one hidden layer (Zeng et al., 2014;<cite> Nguyen and Grishman, 2016</cite>; Fu et al., 2017 ) and a softmax output layer. We use the same structure which can be formalized as the following: where W h and b h are the weights for the hidden",
  "y": "background"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_6",
  "x": "It usually contains one hidden layer (Zeng et al., 2014;<cite> Nguyen and Grishman, 2016</cite>; Fu et al., 2017 ) and a softmax output layer. We use the same structure which can be formalized as the following: where W h and b h are the weights for the hidden",
  "y": "uses similarities"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_8",
  "x": "Previous work (Gormley et al., 2015;<cite> Nguyen and Grishman, 2016</cite>; Fu et al., 2017) set, and the other half of bc, cts and wl as the test sets. We followed their split of documents and their split of the relation types for asymmetric relations. The ERE dataset has a similar relation schema to ACE05, but is different in some annotation guidelines (Aguilar et al., 2014) .",
  "y": "uses"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_9",
  "x": "We fix the word embeddings during the training. We follow<cite> Nguyen and Grishman (2016)</cite> to set the position and entity type embedding size to be 50. We use 150 dimensions for the GRU state, 100 dimensions for the word context vector and use 300 dimensions for the hidden layer in the decoders.",
  "y": "uses"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_10",
  "x": "Training separately on the two corpora (row \"Supervised\" in Table 1 ), we obtain results on ACE05 comparable to previous work (Gormley et al., 2015) with substantially fewer features. With syntactic features as <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017) did, it could be further improved. In this paper, however, we want to focus on representation learning from scratch first.",
  "y": "future_work"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_11",
  "x": "**AUGMENTATION BETWEEN ACE05 AND ERE** Training separately on the two corpora (row \"Supervised\" in Table 1 ), we obtain results on ACE05 comparable to previous work (Gormley et al., 2015) with substantially fewer features. With syntactic features as <cite>(Nguyen and Grishman, 2016</cite>; Fu et al., 2017) did, it could be further improved.",
  "y": "motivation"
 },
 {
  "id": "967c78ccf905c69d732a4c1ef00289_12",
  "x": "Although representation learning from scratch could be more general across multiple datasets, we compare the effect of multi-task learning with extra features on this specific dataset. We add chunk embedding and on dep path embedding<cite> (Nguyen and Grishman, 2016)</cite> . Similar to entity type embedding, chunk embedding is created according to each token's chunk type, we set the embedding size to 50.",
  "y": "uses"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_0",
  "x": "Studies on automatic humor recognition (Mihalcea and Strapparava, 2005;<cite> Yang et al., 2015</cite>; Zhang and Liu, 2014; Purandare and Litman, 2006 ) have defined the recognition task as a binary classification task. So, their classification models categorized a given sentence as a humorous or non-humorous sentence. Among the studies on humor classification, Mihalcea and Strapparava (2005) and <cite>Yang et al. (2015)</cite> reported high performance on the task.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_1",
  "x": "Among the studies on humor classification, Mihalcea and Strapparava (2005) and <cite>Yang et al. (2015)</cite> reported high performance on the task. Considering the performance of their systems, it is reasonable to test the applicability of their models to a real application. In this study, we specifically applied a state-of-the-art automatic humor recognition model to talks and investigated if the model could be used to provide simulated laughters.",
  "y": "background motivation"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_2",
  "x": "In our application of the state-of-art system to talks, we could not achieve a comparable performance to the reported performance of the system. We investigated the potential reasons for the performance difference through further analysis. Some humor classification studies (Mihalcea and Strapparava, 2005;<cite> Yang et al., 2015</cite>; Barbieri and Saggion, 2014 ) have used negative instances from different domains or topics, because non-humorous sentences could not be found or are very challenging to collect in target domains or topics. Their studies showed that it was possible to achieve promising performance using data from heterogeneous domains. However, our study showed that humorous sentences which were semantically close to non-humorous sentences were very challenging to distinguish.",
  "y": "differences background motivation"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_3",
  "x": "Previous studies (Mihalcea and Strapparava, 2005;<cite> Yang et al., 2015</cite>; Zhang and Liu, 2014; Purandare and Litman, 2006; Bertero and Fung, 2016) dealt with the humor recognition task as a binary classification task, which was to categorize a given text as humorous or non-humorous. These studies collected textual data which consisted of humorous texts and non-humorous texts and built a classification model using textual features. Humorous and non-humorous texts were from different domains across the studies. Pun websites, daily joke websites, or tweets were used as sources of humorous texts. Resources such as news websites, proverb websites, etc. were used as sources of non-humorous texts.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_4",
  "x": "Pun websites, daily joke websites, or tweets were used as sources of humorous texts. Resources such as news websites, proverb websites, etc. were used as sources of non-humorous texts. <cite>Yang et al. (2015)</cite> tried to minimize genre differences between humorous and non-humorous texts in order to avoid a chance that a trained model was optimized to distinguish genre differences.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_5",
  "x": "We only examined textual features. Compared to previous studies, one innovation of this study was that a trained model was evaluated using humorous and non-humorous sentences from the same genre and same topic. Mihalcea and Strapparava (2005) and <cite>Yang et al. (2015)</cite> borrowed negative instances from different genres such as news websites or proverbs.",
  "y": "differences"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_6",
  "x": "<cite>Yang et al. (2015)</cite> collected a corpus of Pun of Day data 1 . The data consisted of 2,423 humorous (positive) texts and 2,403 non-humorous (negative) texts. The humorous texts were from the Pun of the Day website, and the negative texts from AP News2, New York Times, Yahoo! Answers and Proverb websites. Examples of humorous and non-humorous sentences are given below. Humorous The one who invented the door knocker got a No-bell prize. Non-Humorous The one who discovered/invented it had the last name of fahrenheit.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_7",
  "x": "In order to reduce the differences between positive and negative instances in the data, <cite>Yang et al. (2015)</cite> used two constraints when collecting negative instances. Non-humorous texts were required to have lengths between the minimum and maximum lengths of positive instances, in order to be selected as negative instances. In addition, only non-humorous texts which consisted of words found in positive instances were collected.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_8",
  "x": "Utilizing the same experimental setup as Mihalcea and Strapparava (2005) and <cite>Yang et al. (2015)</cite> (50% positive and 50% negative instances), we selected 4,726 sentences from among all collected nonhumorous sentences as negative instances. During selection, we minimized differences between positive and negative instances. A negative instance was selected from among sentences located close to a positive instance in a talk.",
  "y": "uses"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_9",
  "x": "**IMPLEMENTATION OF FEATURES** Features from <cite>Yang et al. (2015)</cite> , which we implemented, consisted of (1) two incongruity features, (2) six ambiguity features, (3) four interpersonal effect features, (4) four phonetic features, (5) five k-Nearest Neighbor features, and (6) 300 Word2Vec features. The total number of features used in this study was 321.",
  "y": "uses"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_10",
  "x": "**APPLICATION OF STATE-OF-ART TECHNOLOGY TO TALK DATA** In this section, we present expeirments that we ran to determine 1) how effective a model trained using 'Pun of Day' data (Pun) is when applied to TED Talk data (Talk), and 2) whether the performance of a model trained using Talk data would be similar to the performance reported in <cite>Yang et al. (2015)</cite> . We reimplemented features developed by <cite>Yang et al. (2015)</cite> and evaluated those features on Talk data.",
  "y": "background"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_11",
  "x": "We reimplemented features developed by <cite>Yang et al. (2015)</cite> and evaluated those features on Talk data. Considering the different characteristics of Talk data versus Pun data, we sought to investigate whether Yang's model could achieve the reported performance (over 85% accuracy) on our Talk data. The differences were 1) humorous sentences in Talk data were sentences which induced audience laughters, compared to Pun data which used canned textual humor, 2) all non-humorous sentences in Talk data were also from TED talks, and 3) each pair of humorous and non-humorous sentences were semantically close because they were closely placed. These differences made the humor classification task more challenging.",
  "y": "extends differences"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_12",
  "x": "We followed the experimental setup of <cite>Yang et al. (2015)</cite> in order to see if the performance of our duplicated features was comparable to their reported performance. Their best performance was 85.4% accuracy (Yang in Table 1 ) when they used Random Forest as a classifier and 10-fold cross validation (CV) as an evaluation method. Replicating this experiment setup, we were able to achieve 86.0% accuracy (Pun-to-Pun in Table 1 ), which is slightly better than the performance reported in their paper. The performance difference could be due to the difference in partitions in CV.",
  "y": "differences uses"
 },
 {
  "id": "9684063f991f9a4688d6530fe5a16c_13",
  "x": "A major difference in the two data sets was the source of negative instances. <cite>Yang et al. (2015)</cite> borrowed negative instances from different genres such as news websites and proverbs. But, in Talk-to-Talk, both positive and negative instances were from the same genre. Furthermore, each humorous instance had a corresponding non-humorous instance from the same talk. In this section, we investigate the impact of genre differences in the humor classification task, using Pun and Talk data.",
  "y": "differences"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_0",
  "x": "To be an effective teammate, an AI must overcome the challenges involved with adapting to humans; however, progress in AI is routinely measured in isolation, without a human in the loop. In this work, we focus specifically on the evaluation of visual conversational agents and develop a human computation game to benchmark their performance as members of human-AI teams. Visual conversational agents (Das et al. 2017a;<cite> Das et al. 2017b</cite>; ) are AI agents Figure 1 : A human and an AI (a visual conversation agent called ALICE) play the proposed GuessWhich game.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_1",
  "x": "Recent work has evaluated these models more pragmatically by evaluating how well pairs of visual conversational agents perform on goal-based conversational tasks rather than response retrieval from fixed dialogs. Specifically, <cite>(Das et al. 2017b</cite> ) train two visual conversational agents -a questioning bot QBOT, and an answering bot ABOT -for an image-guessing task. Starting from a description of the scene, QBOT and ABOT converse over multiple rounds of questions (QBOT) and answers (ABOT) in order to improve QBOT's understanding of a secret image known only to ABOT.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_2",
  "x": "After a fixed number of rounds, QBOT must guess the secret image from a large pool and both QBOT and ABOT are evaluated based on this guess. <cite>(Das et al. 2017b</cite> ) compare supervised baseline models with QBOT-ABOT teams trained through reinforcement learning based self-talk on this image-guessing task. They find that the AI-AI teams improve significantly at guessing the correct image after self-talk updates compared to the supervised pretraining.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_3",
  "x": "In this work, we propose to evaluate if and how this progress in AI-AI evaluation translates to the performance of human-AI teams. Inspired by the popular GuessWhat or 20-Questions game, we design a human computation game -GuessWhich -which requires collaboration between human and visual conversational AI agents. Mirroring the setting of<cite> (Das et al. 2017b)</cite> , GuessWhich is an image-guessing game that consists of 2 participants -questioner and answerer.",
  "y": "similarities uses"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_4",
  "x": "We evaluate human-AI team performance in GuessWhich, for the setting where the questioner is a human and the answerer is an AI (that we denote ALICE). Specifically, we evaluate two versions of ALICE for GuessWhich: 1. ALICE SL which is trained in a supervised manner on the Visual Dialog dataset (Das et al. 2017a ) to mimic the answers given by humans when engaged in a conversation with other humans about an image, and 2. ALICE RL which is pre-trained with supervised learning and fine-tuned via reinforcement learning for an imageguessing task as in<cite> (Das et al. 2017b)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_5",
  "x": "Our main experimental finding is that despite significant differences between SL and RL agents reported in previous work<cite> (Das et al. 2017b)</cite> , we find no significant difference in performance between ALICE SL or ALICE RL when paired with human partners (Sec. 6.1). This suggests that while self-talk and RL are interesting directions to pursue for building better visual conversational agents, there appears to be a disconnect between AI-AI and human-AI evaluations -progress on former does not seem predictive of progress on latter. This is an important finding to guide future research.",
  "y": "differences"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_6",
  "x": "Visual Conversational Agents. Our AI agents are visual conversational models, which have recently emerged as a popular research area in visually-grounded language modeling (Das et al. 2017a;<cite> Das et al. 2017b</cite>; . (Das et al. 2017a ) introduced the task of Visual Dialog and collected the VisDial dataset by pairing subjects on Amazon Mechanical Turk (AMT) to chat about an image (with assigned roles of questioner and answerer). <cite>(Das et al. 2017b</cite> ) pre-trained questioner and answerer agents on this VisDial dataset via supervised learning and fine-tuned them via self-talk (reinforcement learning), observing that RL-fine-tuned QBOT-ABOT are better at image-guessing after interacting with each other.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_7",
  "x": "<cite>(Das et al. 2017b</cite> ) pre-trained questioner and answerer agents on this VisDial dataset via supervised learning and fine-tuned them via self-talk (reinforcement learning), observing that RL-fine-tuned QBOT-ABOT are better at image-guessing after interacting with each other. However, (Aras et al. 2010; Chamberlain, Poesio, and Kruschwitz 2008) , movies (Michelucci 2013) etc. While such games have traditionally focused on human-human collaboration, we extend these ideas to human-AI teams.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_8",
  "x": "Human evaluation of conversations is typically in the format where humans rate the quality of machine utterances given context, without actually taking part in the conversation, as in <cite>(Das et al. 2017b</cite> ) and (Li et al. 2016) . To the best of our knowledge, we are the first to evaluate conversational models via team performance where humans are continuously interacting with agents to succeed at a downstream task. Turing Test.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_10",
  "x": "<cite>(Das et al. 2017b</cite> ) formulate a self-supervised imageguessing task between a questioner bot (QBOT) and an answerer bot (ABOT) which plays out over multiple rounds of dialog. At the start of the task, QBOT and ABOT are shown a one sentence description (i.e. a caption) of an image (unknown to QBOT). The pair can then engage in question and answer based dialog for a fixed number of iterations after which QBOT must try to select the secret image from a pool.",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_12",
  "x": "We compare the performance of the two agents ALICE SL and ALICE RL in the GuessWhich game. These bots are state-ofthe-art visual dialog agents with respect to emulating human responses and generating visually discriminative responses in AI-AI dialog. <cite>(Das et al. 2017b</cite> ) evaluate these agents against strong baselines and report AI-AI team results that are significantly better than chance on a pool of \u223c10k images (rank \u223c1000 for SL, rank \u223c500 for RL).",
  "y": "background"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_13",
  "x": "Unlike<cite> (Das et al., 2017b)</cite> , we find no significant difference between ALICE SL and ALICE RL . cally significant. As we collected additional data, the error margins became smaller but the means also became closer.",
  "y": "differences"
 },
 {
  "id": "9795a839cb79ed971de4c325e01e74_14",
  "x": "As we collected additional data, the error margins became smaller but the means also became closer. This interesting finding stands in stark contrast to the results reported by<cite> (Das et al. 2017b)</cite> , where ALICE RL was found to be significantly more accurate than ALICE SL when evaluated in an AI-AI team. Our results suggest that the improvements of RL over SL (in AI-AI teams) do not seem to translate to when the agents are paired with a human in a similar setting.",
  "y": "differences"
 },
 {
  "id": "97fd0f1ce3d4f510c1566d642e9d2c_0",
  "x": "Neural Machine Translation <cite>(Luong et al., 2015</cite>; Bahdanau et al., 2014; Johnson et al., 2017; Vaswani et al., 2017) has been receiving considerable attention in the recent years, given its superior performance without the demand of heavily hand crafted engineering efforts. NMT often outperforms Statistical Machine Translation (SMT) techniques but it still struggles if the parallel data is insufficient like in the case of Indian languages. The bulk of research on low resource NMT has focused on exploiting monolingual data or parallel data from other language pairs.",
  "y": "background"
 },
 {
  "id": "97fd0f1ce3d4f510c1566d642e9d2c_1",
  "x": "Our NMT model consists of an encoder and a decoder, each of which is a Recurrent Neural Network (RNN) as described in<cite> (Luong et al., 2015)</cite> . The model directly estimates the posterior distribution P \u03b8 (y|x) of translating a source sentence x = (x 1 , .., x n ) to a target sentence y = (y 1 , .., y m ) as: Each of the local posterior distribution P (y t |y 1 , 2 , .., y t\u22121 , x) is modeled as a multinomial distribution over the target language vocabulary which is represented as a linear transformation followed by a softmax function on the decoder's output vectorh dec t :",
  "y": "uses"
 },
 {
  "id": "97fd0f1ce3d4f510c1566d642e9d2c_2",
  "x": "Each of the local posterior distribution P (y t |y 1 , 2 , .., y t\u22121 , x) is modeled as a multinomial distribution over the target language vocabulary which is represented as a linear transformation followed by a softmax function on the decoder's output vectorh dec t : where c t is the context vector, h enc and h dec are the hidden vectors generated by the encoder and decoder respectively, AttentionFunction(. , .) is the attention mechanism as shown in<cite> (Luong et al., 2015)</cite> and [. ; .] is the concatenation of two vectors. An RNN encoder first encodes x to a continuous vector, which serves as the initial hidden vector for the decoder and then the decoder performs recursive updates to produce a sequence of hidden vectors by applying the transition function f as:",
  "y": "uses"
 },
 {
  "id": "97fd0f1ce3d4f510c1566d642e9d2c_3",
  "x": "where D is the training set. Our NMT model uses a bi-directional RNN as an encoder and a unidirectional RNN as a decoder with global attention<cite> (Luong et al., 2015)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "97fd0f1ce3d4f510c1566d642e9d2c_4",
  "x": "The structure of our NMT model is same as in<cite> Luong et al. (2015)</cite> , an RNN based encoder-decoder model with Global Attention mechanism. We used an LSTM based Bi-directional encoder and a unidirectional decoder. We kept 4 layers in both the encoder & decoder with embedding size set to 512.",
  "y": "uses"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_0",
  "x": "These all start with ===, but not all messages starting with === are system messages, as shown by the second message in Figure 1 . 3 Related Work IRC Disentanglement Data: The most significant work on conversation disentanglement is a line of papers developing data and models for the #Linux IRC channel (Elsner and Charniak, 2008; Elsner and Schudy, 2009; Charniak, 2010, 2011) . Until now, their dataset was the only publicly available set of messages with annotated conversations (partially re-annotated by<cite> Mehri and Carenini (2017)</cite> with reply-structure graphs), and has been used for training and evaluation in subsequent work (Wang and Oard, 2009;<cite> Mehri and Carenini, 2017</cite>; Jiang et al., 2018) .",
  "y": "background"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_1",
  "x": "We evaluate their model on both our data and our re-annotated version of their data. Recent work has applied neural networks <cite>(Mehri and Carenini, 2017</cite>; Guo et al. (2017) 1,500 1 48 hr 5 n/a 2 Table 1 : Annotated disentanglement dataset comparison. Our data is much larger than prior work, one of the only released sets, and the only one with context and adjudication. '+a' indicates there was an adjudication step to resolve disagreements. '?' indicates the value is not in the paper and the authors no longer have access to the data.",
  "y": "background"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_2",
  "x": "Studies that do consider graphs for disentanglement have used small datasets (Dulceanu, 2016;<cite> Mehri and Carenini, 2017)</cite> that are not always released (Wang et al., 2008; Guo et al., 2017) . ---------------------------------- **DATA**",
  "y": "background"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_3",
  "x": "Values are in the good agreement range proposed by Altman (1990) , and slightly higher than for<cite> Mehri and Carenini (2017)</cite>'s annotations. Results are not shown for Elsner and Charniak (2008) because they did not annotate graphs. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_4",
  "x": "Percentage overlap when conversations from two annotations are optimally paired up using the max-flow algorithm. We follow<cite> Mehri and Carenini (2017)</cite> and keep system messages. (3) Exact Match F 1 .",
  "y": "similarities uses"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_5",
  "x": "Our scores are higher in 4 cases and lower in 5. Interestingly, while \u03ba was higher for us than<cite> Mehri and Carenini (2017)</cite> , our scores for conversations are lower. This is possible because a single link can merge two conversations, meaning a single disagreement in links can cause a major difference in conversations.",
  "y": "differences"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_6",
  "x": "For Channel Two we also compare to Wang and Oard (2009) and<cite> Mehri and Carenini (2017)</cite> , but their code was unavailable, preventing evaluation on our data. We exclude Jiang et al. (2018) as they substantially modified the dataset. For details of models, including hyperparameters tuned on the development set, see the supplementary material.",
  "y": "similarities"
 },
 {
  "id": "982991efdb6b14f187702e0a577bac_7",
  "x": "How far apart consecutive messages in a conversation are: Elsner and Charniak (2008) and<cite> Mehri and Carenini (2017)</cite> use a limit of 129 seconds, Jiang et al. (2018) limit to within 1 hour, Guo et al. (2017) limit to within 8 messages, and we limit to within 100 messages. Figure 4 shows the distribution of time differences in our conversations. 94.9% are within 2 minutes, and almost all are within an hour.",
  "y": "differences"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_0",
  "x": "Most previous research on user geolocation has focused either on text-based classification approaches (Eisenstein et al., 2010; Wing and Baldridge, 2011;<cite> Roller et al., 2012</cite>; Han et al., 2014) or, to a lesser extent, network-based regression approaches (Jurgens, 2013; Compton et al., 2014;<cite> Rahimi et al., 2015)</cite> . Methods which combine the two, however, are rare. In this paper, we present work on Twitter user geolocation using both text and network information.",
  "y": "background"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_1",
  "x": "Most previous research on user geolocation has focused either on text-based classification approaches (Eisenstein et al., 2010; Wing and Baldridge, 2011;<cite> Roller et al., 2012</cite>; Han et al., 2014) or, to a lesser extent, network-based regression approaches (Jurgens, 2013; Compton et al., 2014;<cite> Rahimi et al., 2015)</cite> . Methods which combine the two, however, are rare. In this paper, we present work on Twitter user geolocation using both text and network information.",
  "y": "motivation"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_2",
  "x": "In this paper, we present work on Twitter user geolocation using both text and network information. Our contributions are as follows: (1) we propose the use of Modified Adsorption (Talukdar and Crammer, 2009) as a baseline networkbased geolocation model, and show that it outperforms previous network-based approaches (Jurgens, 2013;<cite> Rahimi et al., 2015)</cite> ; (2) we demonstrate that removing \"celebrity\" nodes (nodes with high in-degrees) from the network increases geolocation accuracy and dramatically decreases network edge size; and (3) we integrate textbased geolocation priors into Modified Adsorption, and show that our unified geolocation model outperforms both text-only and network-only approaches, and achieves state-of-the-art results over three standard datasets. ----------------------------------",
  "y": "uses differences"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_3",
  "x": "The greatest shortcoming of networkbased models is that they completely fail to geolocate users who are not connected to geolocated components of the graph. As shown by<cite> Rahimi et al. (2015)</cite> , geolocation predictions from text can be used as a backoff for disconnected users, but there has been little work that has investigated a more integrated text-and network-based approach to user geolocation. ----------------------------------",
  "y": "background"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_4",
  "x": "---------------------------------- **DATA** We evaluate our models over three pre-existing geotagged Twitter datasets: (1) GEOTEXT (Eisen-stein et al., 2010), (2) TWITTER-US <cite>(Roller et al., 2012)</cite> , and (3) TWITTER-WORLD (Han et al., 2012) .",
  "y": "uses"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_5",
  "x": "A Unified Geolocation Model To address the issue of disconnected test users, we incorporate text information into the model by attaching a labelled dongle node to every test node (Zhu and Ghahramani, 2002; Goldberg and Zhu, 2006) . The label for the dongle node is based on a textbased l 1 regularised logistic regression model, using the method of<cite> Rahimi et al. (2015)</cite> . The dongle nodes with their corresponding label confidences are added to the seed set, and are treated in the same way as other labelled nodes (i.e. the training nodes).",
  "y": "uses"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_6",
  "x": "Table 1 shows the performance of MAD-B, MADCEL-B, MADCEL-W, MADCEL-B-LR and MADCEL-W-LR over the GEOTEXT, TWITTER-US and TWITTER-WORLD datasets. The results are also compared with prior work on network-based geolocation using label propagation (LP)<cite> (Rahimi et al., 2015)</cite> , text-based classification models (Han et al., 2012; Wing and Baldridge, 2011;<cite> Rahimi et al., 2015</cite>; Cha et al., 2015) , textbased graphical models (Ahmed et al., 2013) , and network-text hybrid models (LP-LR)<cite> (Rahimi et al., 2015)</cite> . ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_7",
  "x": "Our baseline network-based model of MAD-B outperforms the text-based models and also previous network-based models (Jurgens, 2013; Compton et al., 2014;<cite> Rahimi et al., 2015)</cite> . The inference, however, is intractable for TWITTER-US and TWITTER-WORLD due to the size of the network. Celebrity removal in MADCEL-B and MADCEL-W has a positive effect on geolocation accuracy, and results in a 47% reduction in Median over GEOTEXT.",
  "y": "differences"
 },
 {
  "id": "9885b924f6b0806844d4e70d857a35_8",
  "x": "The inference, however, is intractable for TWITTER-US and TWITTER-WORLD due to the size of the network. Celebrity removal in MADCEL-B and MADCEL-W has a positive effect on geolocation accuracy, and results in a 47% reduction in Median over GEOTEXT. It also makes graph inference over TWITTER-US and TWITTER-WORLD tractable, and results in superior Acc@161 and Median, but slightly inferior Mean, compared to the state-of-the-art results of LR, based on text-based classification<cite> (Rahimi et al., 2015)</cite> .",
  "y": "differences"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_0",
  "x": "Word2vec <cite>[23]</cite> is a recently proposed family of algorithms for training such vector representations from unstructured text data via shal- * Work done while with Yahoo, Inc. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.",
  "y": "background"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_1",
  "x": "low neural networks. The geometry of the resulting vectors was shown in <cite>[23]</cite> to capture word semantic similarity through the cosine similarity of the corresponding vectors as well as more complex semantic relationships through vector differences, such as vec(\"Madrid\") -vec(\"Spain\") + vec(\"France\") \u2248 vec(\"Paris\"). More recently, novel applications of word2vec involving unconventional generalized \"words\" and training corpuses have been proposed.",
  "y": "background"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_2",
  "x": "The training of vectors for such large vocabularies presents several challenges. In word2vec, each vocabulary word has two associated d-dimensional vectors which must be trained, respectively referred to as input and output vectors, each of which is represented as an array of d single precision floating point numbers <cite>[23]</cite> . To achieve acceptable training latency, all vectors need to be kept in physical memory during training, and, as a result, word2vec requires 2 \u00b7 d \u00b7 4 \u00b7 |V| bytes of RAM to train a vocabulary V. For example, in Section 2, we discuss the search advertisement use case with 200 million generalized words and d = 300 which would thus require 2 \u00b7 300 \u00b7 4 \u00b7 200M = 480GB memory which is well beyond the capacity of typical commodity servers today.",
  "y": "background"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_3",
  "x": "---------------------------------- **THE WORD2VEC TRAINING PROBLEM** In this paper we focus on the skipgram approach with random negative examples proposed in <cite>[23]</cite> .",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_4",
  "x": "In this paper we focus on the skipgram approach with random negative examples proposed in <cite>[23]</cite> . This has been found to yield the best results among the proposed variants on a variety of semantic tests of the resulting vectors [19,<cite> 23]</cite> . Given a corpus consisting of a sequence of sentences s1, s2, . . . , sn each comprising a sequence of words si = wi,1, wi,2, . .",
  "y": "background"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_5",
  "x": "\u2022 window sizes bi,j are randomly selected so that each inner sum includes between 1 and a maximum B terms, as in <cite>[23]</cite> and its open-source implementation; 2 \u2022 negative examples N i,j,k associated with positive output word w i,k are selected randomly according to a probability distribution suggested in <cite>[23]</cite> ; \u2022 and the vocabulary V consists of the set of words for which vectors are to be trained.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_6",
  "x": "\u2022 window sizes bi,j are randomly selected so that each inner sum includes between 1 and a maximum B terms, as in <cite>[23]</cite> and its open-source implementation; 2 \u2022 negative examples N i,j,k associated with positive output word w i,k are selected randomly according to a probability distribution suggested in <cite>[23]</cite> ; \u2022 and the vocabulary V consists of the set of words for which vectors are to be trained.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_7",
  "x": "We follow <cite>[23]</cite> for setting V and select words occurring in the corpus a sufficient number of times (e.g., at least 5 times), or, if this results in too many words, as the most frequently occurring N words, where N is the largest number words that can be handled by available computational resources. We further also assume a randomized version of (1) according to the subsampling technique of <cite>[23]</cite> , which removes some occurrences of frequent words. The algorithm for maximizing (1) advocated in <cite>[23]</cite> , and implemented in its open-source counterpart, is a minibatch stochastic gradient descent (SGD).",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_8",
  "x": "\u2022 and the vocabulary V consists of the set of words for which vectors are to be trained. We follow <cite>[23]</cite> for setting V and select words occurring in the corpus a sufficient number of times (e.g., at least 5 times), or, if this results in too many words, as the most frequently occurring N words, where N is the largest number words that can be handled by available computational resources. We further also assume a randomized version of (1) according to the subsampling technique of <cite>[23]</cite> , which removes some occurrences of frequent words.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_9",
  "x": "The algorithm for maximizing (1) advocated in <cite>[23]</cite> , and implemented in its open-source counterpart, is a minibatch stochastic gradient descent (SGD). Our training system is also based on minibatch SGD optimization of (1), however, as described in Section 5, it is carried out in a distributed fashion in a manner quite different from the implementation of <cite>[23]</cite> . Any form of minibatch SGD optimization of (1) involves the computation of dot products and linear combinations between input and output word vectors for all pairs of words occurring within the same window (with indices in {k = j : |k \u2212 j| \u2264 bi,j}).",
  "y": "uses background"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_10",
  "x": "Our training system is also based on minibatch SGD optimization of (1), however, as described in Section 5, it is carried out in a distributed fashion in a manner quite different from the implementation of <cite>[23]</cite> . Any form of minibatch SGD optimization of (1) involves the computation of dot products and linear combinations between input and output word vectors for all pairs of words occurring within the same window (with indices in {k = j : |k \u2212 j| \u2264 bi,j}). This is a massive computational task when carried out for multiple iterations over data sets with tens of billions of words, as encountered in applications described in the previous section.",
  "y": "differences"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_11",
  "x": "These include the original open source implementation of word2vec <cite>[23]</cite> , as well as those of Medallia [22] , and Rehurek [28] . As mentioned in the introduction, these systems would require far larger memory configurations than available on typical commodity-scale servers. ----------------------------------",
  "y": "background"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_12",
  "x": "3 The formula arises from the fact that the input and output vectors for each term in the minibatch must be sent (this the '2' in the first factor in (2)), as must the output vectors for each random negative example. There are on average w \u00b7 n of these per minibatch word. For w = 10, n = 10, d = 500, values within the ranges recommended in <cite>[23]</cite> , this works out to r(10, 10, 500) \u2248 200, 000 bytes transferred per word with each get and put.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_13",
  "x": "For S shards, the vocabulary size can thus be scaled up by as much as a factor of S relative to a single machine. The vectors are initialized in the parameter server shards as in <cite>[23]</cite> . Multiple clients running on cluster nodes then read in different portions of the corpus and interact with the parameter server shards to carry out minibatch stochastic gradient descent (SGD) optimization of (1) over the word vectors, following the algorithm in Figure 7 (in the appendix).",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_14",
  "x": "The RPC calls are detailed in Figures 5 and 6 (in the Appendix), and, at a higher level, entail the following server/shard side operations: \u2022 dotprod: Select negative examplesw in (4) according to a probability distribution derived from the vocabulary histogram proposed in <cite>[23]</cite> , but with the client thread supplied seed initializing the random number generation, and then return all partial dot products required to evaluate the gradient (4) for all positive output, negative output, and input word vectors associated with the minibatch, wherein the partial dot products involve those vector components stored on the designated shard: usv T s . \u2022 adjust: Regenerate negative examples used in preceding dotprod call using the same seed that is again supplied by the client thread.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_15",
  "x": "The data set is iterated over multiple times and after each iteration, the learning rate \u03b1 is reduced in a manner similar to the open source implementation of <cite>[23]</cite> . Note that there is no locking or synchronization of the word vector state within or across shards or across client threads during any part of the computation. The only synchronization in effect is that the RPC broadcast ensures that all shards operate on the same set of word vector indices for computing their portion of the corresponding calls.",
  "y": "similarities"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_16",
  "x": "To compare the proposed distributed system we trained vectors on a publicly available data set collected and processed by the script 'demo-train-big-model-v1-compute-only.sh' from the open-source package of <cite>[23]</cite> . This script collects a variety of publicly available text corpuses and processes them using the algorithm described in <cite>[23]</cite> to coalesce sufficiently co-occurring words into phrases. We then randomly shuffled the order of sentences (delimited by new line) in the data set, retaining order of words within each sentence.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_17",
  "x": "To compare the proposed distributed system we trained vectors on a publicly available data set collected and processed by the script 'demo-train-big-model-v1-compute-only.sh' from the open-source package of <cite>[23]</cite> . This script collects a variety of publicly available text corpuses and processes them using the algorithm described in <cite>[23]</cite> to coalesce sufficiently co-occurring words into phrases. We then randomly shuffled the order of sentences (delimited by new line) in the data set, retaining order of words within each sentence.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_18",
  "x": "We evaluated accuracy on the phrase analogies in the 'question-phrases.txt' file and also evaluated Spearman's rank correlation with respect to the editorial evaluation of semantic relatedness of pairs of words in the well known wordsim-353 collection [14] . The results are shown in Table 1 . The first column shows results for the single machine implementation of <cite>[23]</cite> , the second for a 'low parallelism' configuration of our system using 50 Spark executors, minibatch size of 1, and 1 thread per executor, and the third column for a 'high parallelism' configuration again with 50 executors, but with minibatch size increased to 50 and 8 threads per executor.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_19",
  "x": "The various systems were run using the skipgram variant with 500 dimensional vectors, maximum window size of 20 (10 in each direction), 5 negative examples, subsample ratio of 1e-6 (see <cite>[23]</cite> ), initial learning rate of 0.01875, and 3 iterations over the data set. It can be seen that the vectors trained by the 'high parallelism' configuration of the proposed system, which is the closest to the configurations required for acceptable training latency in the large-scale sponsored search application, suffers only a modest loss in quality as measured by these tests. Note that this data set is more challenging for our system than the sponsored search data set, as it is less sparse and there is on average more overlap between words in different minibatches.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_20",
  "x": "In fact, if we attempt to increase the parallelism to 200 executors as was used for the training of the vectors described in the next subsection, training fails to converge altogether. We are unsure why our system yields better results than the implementation of <cite>[23]</cite> on the wordsim test, yet worse scores on the analogies test. We also note that the analogies test scores reported here involve computing the closest vector for each analogy \"question\" over the entire vocabulary and not just over the 1M most frequent words, as in the script 'demo-train-big-model-v1-computeonly.sh' of <cite>[23]</cite> .",
  "y": "differences"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_21",
  "x": "We also note that the analogies test scores reported here involve computing the closest vector for each analogy \"question\" over the entire vocabulary and not just over the 1M most frequent words, as in the script 'demo-train-big-model-v1-computeonly.sh' of <cite>[23]</cite> . ---------------------------------- **SPONSORED SEARCH DATA SET**",
  "y": "differences"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_22",
  "x": "We also compared the cosine similarities for pairs of vectors trained using the proposed distributed system and for corresponding vector pairs trained using the open-source implementation of <cite>[23]</cite> , again on a large search session data set. The former was trained using a vocabulary of 200 million generalized words while the latter was trained using about 90 million words which is the most that could fit onto a specialized large memory machine. For a set of 7,560 generalized word pairs with words common to the vocabularies trained by the respective systems we found very good agreement in cosine similarities between the corresponding vectors from the two systems, with over 50% of word pairs having cosine similarity differences less than 0.06, and 91% of word pairs having differences less than 0.1.",
  "y": "uses"
 },
 {
  "id": "98eef9a1dbea3ddd0a8fd1b9c9376c_23",
  "x": "One model was trained using implementation from <cite>[23]</cite> and the other was trained using the proposed distributed system. Both buckets were compared against control bucket, which employed a collection of different broad match techniques used in production at the time of the test. Each of the online tests were run for 10 days, one after another, more than a month apart.",
  "y": "uses"
 },
 {
  "id": "99aebc86f34ace0133c9f0922373fe_0",
  "x": "A new paradigm called the symbolic neural learning is introduced to extend how data analysis is performed from language processing to semantic learning and memory networking. Secondly, we address a number of Bayesian models ranging from latent variable model to VB inference (Chien and Chang, 2014; Chien and Chueh, 2011; Chien, 2015b) , MCMC sampling (Watanabe and Chien, 2015) and BNP learning (Chien, 2016; Chien, 2015a; Chien, 2018) for hierarchical, thematic and sparse topics from natural language. In the third part, a series of deep models including deep unfolding (Chien and Lee, 2018) , Bayesian RNN (Gal and Ghahramani, 2016; Chien and Ku, 2016) , sequence-to-sequence learning (Graves et al., 2006; <cite>Gehring et al., 2017)</cite> , CNN (Kalchbrenner et al., 2014; Xingjian et al., 2015; , GAN (Tsai and Chien, 2017) and VAE are introduced.",
  "y": "background"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_0",
  "x": "**ABSTRACT** In this paper, we propose a novel deep neural network architecture, Speech2Vec, for learning fixed-length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the underlying spoken words, and are close to other vectors in the embedding space if their corresponding underlying spoken words are semantically similar. The proposed model can be viewed as a speech version of Word2Vec <cite>[1]</cite>.",
  "y": "extends"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_1",
  "x": "**INTRODUCTION** Natural language processing (NLP) techniques such as Word2Vec<cite> [1,</cite> 2] and GloVe [3] transform words into fixed dimensional vectors, or word embeddings. The embeddings are obtained via unsupervised learning from co-occurrence information in text, and contain semantic information about the word which are useful for many NLP tasks [4, 5, 6, 7, 8] .",
  "y": "background"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_2",
  "x": "Researchers have also explored the concept of learning vector representations from speech [9,<cite> 1</cite>0,<cite> 1</cite>1,<cite> 1</cite>2,<cite> 1</cite>3,<cite> 1</cite>4] . These approaches are based on notions of acoustic-phonetic (rather than semantic) similarity, so that different instances of the same underlying word would map to the same point in a latent embedding space. Our work, highly inspired by Word2Vec <cite>[1]</cite> , uses a skipgrams or continuous bag-of-words formulation to focus on neighboring acoustic regions, rather than the acoustic segment associated with the word itself.",
  "y": "motivation"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_9",
  "x": "Existing schemes for evaluating methods for word embeddings fall into two major categories: extrinsic and intrinsic [29] . With the extrinsic method, the learned word embeddings are used as input features to a downstream task [4, 5, 6, 7, 8] , and the performance metric varies from task to task. The intrinsic method directly tests for semantic or syntactic relationships between words, and includes the tasks of word similarity and word analogy <cite>[1]</cite> .",
  "y": "uses"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_21",
  "x": "This result aligns with the empirical fact that skipgrams Word2Vec is likely to work better than cbow Word2Vec with small training corpus size <cite>[1]</cite> . Training size <cite> 1</cite>0%  40%  70% <cite> 1</cite>00% <cite> 1</cite>0%  40%  70% <cite> 1</cite>00% <cite> 1</cite>0%  40%  70% <cite> 1</cite>00% <cite> 1</cite>0%  40%  70% Impact of training corpus size. From Table 2 we observe that when<cite> 1</cite>0% of the corpus was used for training, the resulting word embeddings perform poorly.",
  "y": "similarities"
 },
 {
  "id": "99b26d9151c7c0a1df1df1300fc764_26",
  "x": "**CONCLUSIONS AND FUTURE WORK** Speech2Vec, which integrates a RNN Encoder-Decoder framework with skipgrams or cbow for training, extends the textbased Word2Vec <cite>[1]</cite> model to learn word embeddings directly from speech. Speech2Vec has access to richer information in the speech signal that does not exist in plain text.",
  "y": "extends"
 },
 {
  "id": "9a52e0ea1f12e3455fca48ac8f8936_1",
  "x": "The current state of the art on modeling both PTB and WikiText 2 [8] datasets as reported in <cite>[4]</cite> shows little sensitivity to hyper parameters; sharing almost all hyper parameters values between both datasets. In [9] , its is also shown that deep learning model can jointly learn a number of large-scale tasks from multiple domains by designing a multi-modal architecture in which as many parameters as possible are shared. Training and evaluating a neural network involves mapping the hyper parameter configuration (set of values for each hyper parameter) used in training the network to the validation error obtained at the end.",
  "y": "background"
 },
 {
  "id": "9a52e0ea1f12e3455fca48ac8f8936_2",
  "x": "Following a meta-learning approach, we apply a genetic algorithm and a sequential search algorithm, described in the next section, initialized using the best configuration reported in <cite>[4]</cite> to search the space around optimal hyper parameters for the AWD-LSTM model. Twitter tweets collected using a geolocation filter for Nigeria and Kenya with the goal of acquiring a code-mixed text corpus serve as our evaluation datasets. We report the test perplexity distributions of the various evaluated configurations and draw inferences on the sensitivity of each hyper parameter to our unique dataset.",
  "y": "uses"
 },
 {
  "id": "9a52e0ea1f12e3455fca48ac8f8936_3",
  "x": "We begin our work by establishing what the baseline and current state of the art model is for a language modeling task <cite>[4]</cite> . Applying the AWD-LSTM model, based on the open sourced code and trained on code-mixed Twitter data, we sample 84 different hyper parameter configurations for each dataset, and evaluate the resulting test perplexity distributions while varying individual hyperparameter values to understand the effect of the set of hyper parameter values selected on the model perplexity. ----------------------------------",
  "y": "background"
 },
 {
  "id": "9a52e0ea1f12e3455fca48ac8f8936_4",
  "x": "**E. META-LEARNING INITIALIZATION** Both the population based and sequential search space were manually initialized with four (4) values of each hyperparameter in the neighbourhood of the best values reported in <cite>[4]</cite> as shown in Table I . It is important to note that the sampled configuration space is very small compared to the overall space which is of size 4",
  "y": "similarities"
 },
 {
  "id": "9a52e0ea1f12e3455fca48ac8f8936_6",
  "x": "In this work we assess the performance of the AWD-LSTM model <cite>[4]</cite> for language modeling to better understand how relevant the published hyper parameters may be for a codemixed corpus and to isolate which hyper parameters could be further tuned to improve performance. Our results show that as a whole, the set of hyperparameters considered the best <cite>[4]</cite> are reasonably good, however ther are better sets hyperparamers for the code-mixed corpora. Moreover, even with the best set of hyper parameters, the perplexity observed for our data are significantly higher (i.e. performance is worse at the task of language modeling) than the performance demonstrated in the literature.",
  "y": "uses"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_0",
  "x": "Examples of supervised learning methods for WSD appear in [2] [3] [4] , [7] [8] . The learning algorithms applied including: decision tree, decisionlist [15] , neural networks [7] , na\u00efve Bayesian learning ( [5] , [11] ) and maximum entropy <cite>[10]</cite> . Among these leaning methods, the most important issue is what features will be used to construct the classifier.",
  "y": "background"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_1",
  "x": "An interesting study on feature selection for Chinese <cite>[10]</cite> has considered topical features as well as local collocational, syntactic, and semantic features using the maximum entropy model. In Dang's <cite>[10]</cite> work, collocational features refer to the local PoS information and bi-gram co-occurrences of words within 2 positions of the ambiguous word. A useful result from this work based on (about one million words) the tagged People's Daily News shows that adding more features from richer levels of linguistic information such as PoS tagging yielded no significant improvement (less than 1%) over using only the bi-gram co-occurrences information.",
  "y": "background"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_2",
  "x": "Most of the recent works for English corpus including [7] and [8] , which combine both local and topical information in order to improve their performance. An interesting study on feature selection for Chinese <cite>[10]</cite> has considered topical features as well as local collocational, syntactic, and semantic features using the maximum entropy model. In Dang's <cite>[10]</cite> work, collocational features refer to the local PoS information and bi-gram co-occurrences of words within 2 positions of the ambiguous word.",
  "y": "background"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_3",
  "x": "To avoid this problem, we combine the contextual features in the target context with the pre-prepared collocations list to build our classifier. As stated early, an important issue is what features will be used to construct the classifier in WSD. Early researches have proven that using lexical statistical information, such as bi-gram co-occurrences was sufficient to produce close to the best results <cite>[10]</cite> for Chinese WSD.",
  "y": "background"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_4",
  "x": "As stated early, an important issue is what features will be used to construct the classifier in WSD. Early researches have proven that using lexical statistical information, such as bi-gram co-occurrences was sufficient to produce close to the best results <cite>[10]</cite> for Chinese WSD. Instead of including bi-gram features as part of discrimination features, in our system, we consider both topical contextual features as well as local collocation features.",
  "y": "differences"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_5",
  "x": "The sense of ambiguous words can be uniquely determined in these two types of collocations, therefore are the collocations applied in our system. The sources of the collocations will be explained in Section 4.1. In both Niu [11] and Dang's <cite>[10]</cite> work, topical features as well as the so called collocational features were used.",
  "y": "background"
 },
 {
  "id": "9bab4741e6b9f132c2851bae3a3cf4_6",
  "x": "In both Niu [11] and Dang's <cite>[10]</cite> work, topical features as well as the so called collocational features were used. However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features. Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD.",
  "y": "differences"
 },
 {
  "id": "9ea99bf57e9113b2f03f2285741397_0",
  "x": "A code-switching language model learned in this way is able to capture the patterns and constraints of the switches and mitigate the out-of-vocabulary (OOV) issue during sequence generation. By adding the generated sentences and incorporating syntactic information to the training data, we achieve better performance by 10% compared to an LSTM baseline <cite>[10]</cite> and 5% to the equivalent constraint. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "9ea99bf57e9113b2f03f2285741397_1",
  "x": "A further investigation of Equivalence Constraint and Curriculum Learning showed an improvement in language modeling [7] . A multi-task learning approach was introduced to train the syntax representation of languages by constraining the language generator <cite>[10]</cite> . A copy mechanism was proposed to copy words directly from the input to the output using an attention mechanism [16] .",
  "y": "background"
 },
 {
  "id": "9ea99bf57e9113b2f03f2285741397_2",
  "x": "The POS tags are generated phrase-wise using pretrained English and Chinese Stanford POS Tagger [21] by adding a word at a time in a unidirectional way to avoid any intervention from future information. The word and syntax unit are represented as a vector x w and x p respectively. Next, we concatenate both vectors and use it as an input [x w |x p ] to an LSTM layer similar to <cite>[10]</cite> . 4.",
  "y": "similarities"
 },
 {
  "id": "9ea99bf57e9113b2f03f2285741397_3",
  "x": "As the data preprocessing, words are tokenized using Stanford NLP toolkit [23] and all hesitations and punctuations were removed except apostrophe. The split of the dataset is identical to <cite>[10]</cite> and it is showed in Table 1 . ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "9f1d2be80dbfd726a24fb2a05e130b_0",
  "x": "Machine Transliteration is defined as phonetic transformation of names across languages Karimi et al., 2011) . Transliteration of named entities is the essential part of many multilingual applications, such as machine translation (Koehn, 2010) and cross-language information retrieval (Jadidinejad and Mahmoudi, 2010) . Recent studies pay a great attention to the task of Neural Machine Translation (Cho et al., 2014a; <cite>Sutskever et al., 2014</cite>) .",
  "y": "background"
 },
 {
  "id": "9f1d2be80dbfd726a24fb2a05e130b_1",
  "x": "---------------------------------- **EXPERIMENTS** We conducted a set of experiments to show the effectiveness of <cite>RNN Encoder-Decoder model</cite> (Cho et al., 2014b; <cite>Sutskever et al., 2014</cite>) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task .",
  "y": "uses"
 },
 {
  "id": "9f1d2be80dbfd726a24fb2a05e130b_2",
  "x": "**CONCLUSION** In this paper we proposed Neural Machine Transliteration based on successful studies in sequence to sequence learning (<cite>Sutskever et al., 2014</cite>) and Neural Machine Translation (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Bahdanau et al., 2015; Cho et al., 2014a) . Neural Machine Transliteration typically consists of two components, the first of which encodes a source name sequence x and the second decodes to a target name sequence y. Different parts of the proposed model jointly trained using stochastic gradient descent to minimize the log-likelihood.",
  "y": "uses"
 },
 {
  "id": "9f1d2be80dbfd726a24fb2a05e130b_3",
  "x": "Transforming a name from spelling to phonetic and then use the constructed phonetic to generate the spelling on the target language is a very complex task (Oh et al., 2006; Finch et al., 2015) . Based on successful studies on Neural Machine Translation (Cho et al., 2014a; <cite>Sutskever et al., 2014</cite>; Hirschberg and Manning, 2015) , in this paper, we proposed a character-based encoderdecoder model which learn to transliterate endto-end. In the opposite side of classical models which contains different components, the proposed model is trained end-to-end, so it able to apply to any language pairs without tuning for a spacific one.",
  "y": "uses background"
 },
 {
  "id": "9f1d2be80dbfd726a24fb2a05e130b_4",
  "x": "**PROPOSED MODEL** Here, we describe briefly the underlying framework, called <cite>RNN Encoder-Decoder</cite>, proposed by (Cho et al., 2014b) and <cite>(Sutskever et al., 2014)</cite> upon which we build a machine transliteration model that learns to transliterate end-to-end. The enoder is a character-based recurrent neural network that learns a highly nonlinear mapping from a spelling to the phonetic of the input sequence.",
  "y": "uses background"
 },
 {
  "id": "9fdeb20207af1e8ee0c6e5374e3731_0",
  "x": "**RESEARCH** The SINNET system is the result of several years of research<cite> Agarwal et al., 2012</cite>; Agarwal et al., 2013) . In , we introduced the notion of social events.",
  "y": "background"
 },
 {
  "id": "9fdeb20207af1e8ee0c6e5374e3731_1",
  "x": "In , we presented a preliminary system that uses tree kernels and Support Vector Machines (SVMs) to extract social events from news articles. In<cite> Agarwal et al. (2012)</cite> , we presented a case study on a manually extracted network from Alice in Wonderland, showing that analyzing networks based on these social events gives us insight into the roles of characters in the story. Also, static network analysis has limitations which become apparent from our analysis.",
  "y": "background"
 },
 {
  "id": "9fdeb20207af1e8ee0c6e5374e3731_2",
  "x": "Figure 2 shows the network extracted from an abridged version of Alice in Wonderland <cite>(Agarwal et al., 2012)</cite> . Figure 3 shows the output of running the Hubs and Authority algorithm (Kleinberg, 1998 ) on the network. In information retrieval, an authority is a webpage that many hubs point to and a hub is a webpage that points to many authorities.",
  "y": "background"
 },
 {
  "id": "9fdeb20207af1e8ee0c6e5374e3731_3",
  "x": "Figure 3b shows the authorities in decreasing order of authority weights. We see that the main character of the story, Alice, is the main authority but not the main hub. This network may be used for other In<cite> Agarwal et al. (2012)</cite> , we argued that a static network does not bring out the true nature of a network.",
  "y": "background"
 },
 {
  "id": "a0614f13b4ed0c6370deb26032f62b_0",
  "x": "Beyond language modeling, methods for PCFG induction from treebanks have been a popular topic in the field over the past decade, and some understanding of the impact of flattening trees can be had in <cite>Johnson (1998)</cite> , where the beneficial impact of various tree transformations for probabilistic grammars is presented. None of this work is discussed or cited, and the naive reader might be left with the impression that data-driven approaches have been demonstrated to underperform relative to knowledge-based approaches, when in fact the authors simply demonstrate that their hybrid grammar-based/data-driven approach outperforms class-based language models. Perhaps this is worth demonstrating, but the chapter couches the results within the context of a clash between paradigms, which simply does not ring true.",
  "y": "background"
 },
 {
  "id": "a0614f13b4ed0c6370deb26032f62b_1",
  "x": "Beyond language modeling, methods for PCFG induction from treebanks have been a popular topic in the field over the past decade, and some understanding of the impact of flattening trees can be had in <cite>Johnson (1998)</cite> , where the beneficial impact of various tree transformations for probabilistic grammars is presented. None of this work is discussed or cited, and the naive reader might be left with the impression that data-driven approaches have been demonstrated to underperform relative to knowledge-based approaches, when in fact the authors simply demonstrate that their hybrid grammar-based/data-driven approach outperforms class-based language models. Perhaps this is worth demonstrating, but the chapter couches the results within the context of a clash between paradigms, which simply does not ring true. This one misstep, however, does not detract from the quality of the authors' system, nor from the interesting presentation of too-often-ignored aspects of spoken language systems engineering. The book and the toolkit it describes can serve a very useful pedagogical purpose by providing a foundation upon which students and researchers",
  "y": "motivation"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_0",
  "x": "**TREE-BASED METHODS** The arithmetic expression can be naturally represented as a binary tree structure such that the operators with higher priority are placed in the lower level and the root of the tree contains the operator with the lowest priority. The idea of tree-based approaches <cite>[26]</cite> , [27] , [28] , [15] is to transform the derivation of the arithmetic expression to constructing an equivalent tree structure step by step in a bottom-up manner.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_1",
  "x": "Figure 3 shows two tree examples derived from the math word problem in Figure 2 . One is called expression tree that is used in <cite>[26]</cite> , [28] , [15] and the other is called equation tree in [27] . These two types of trees are essentially equivalent and result in the same solution, except that equation tree contains a node for the unknown variable x. The overall algorithmic framework among the tree-based approaches consists of two processing stages.",
  "y": "uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_2",
  "x": "Roy et al. <cite>[26]</cite> proposed the first algorithmic approach that leverages the concept of expression tree to solve arithmetic word problems. Its first strategy to reduce the search space is training a binary classifier to determine whether an extracted quantity is relevant or not. Only the relevant ones are used for tree construction and placed in the bottom level.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_3",
  "x": "With these two factors, Score(E) is formally defined as where I (E) is the group of irrelevant quantities that are not included in expression E, and N refers to the set of internal tree nodes. To further reduce the tree enumeration space, beam search is applied in <cite>[26]</cite> .",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_4",
  "x": "Experimental results with k = 200 show that the strategy achieves a good balance between accuracy and running time. In [29] , the authors publish the service as a web tool and it can respond promptly to a math word problem. The solution in [27] , named ALGES, differs from <cite>[26]</cite> in two major ways.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_5",
  "x": "It is worth noting that to reduce the search space and simplify the tree construction, only adjacent nodes are combined to generate their parent node. UnitDep [28] can be viewed as an extension work of <cite>[26]</cite> by the same authors. An important concept, named Unit Dependency Graph (UDG), is proposed to enhance the scoring function.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_7",
  "x": "We present their descriptions in the following and summarize the statistics of the datasets in Table 1. 1) AI2 [21] . 2) IL <cite>[26]</cite> .",
  "y": "uses background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_9",
  "x": "We present their descriptions in the following and summarize the statistics of the datasets in Table 1. 1) AI2 [21] . 3) CC <cite>[26]</cite> .",
  "y": "uses background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_10",
  "x": "It is interesting to observe that ALGES [27] , ExpressionTree <cite>[26]</cite> and UNITDEP [28] cannot perform equally well on the three datasets. ALGES works poorly in AI2 because irrelevant quantities exist in its math problems and ALGES is not trained with a classifier to get rid of them. However, it outperforms ExpressionTree and UNITDEP by a wide As to the datasets of SingleEQ and AllArith, UNITDEP is a winner in both datasets, owning to the effectiveness of the proposed unit dependency graph (UDG).",
  "y": "uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_11",
  "x": "This is similar to the relevance model trained in ExpressionTree <cite>[26]</cite> and UNITDEP [28] . The difference is that DNS uses LSTM as the classifier with unsupervised word-embedding features whereas ExpressionTree and UNITDEP use SVM with handcrafted features. Second, the seq2seq model is integrated with a similarity-based method [12] introduced in Section 3.2.",
  "y": "similarities"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_12",
  "x": "We observed its occurrence in Formula [23] and ARIS [21] to extract attributes of entities (the subject, verb, object, preposition and temporal information), in KAZB [39] to generate part-of-speech tags, lematizations, and dependency parses to compute features, and in ALGES [27] to obtain syntactic information used for grounding and feature computation. ExpressionTree <cite>[26]</cite> is an exceptional case without using Stanford Parser. Instead, it uses the easy-fist parsing algorithm [69] to detect the verb associated with each quantity.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_13",
  "x": "Hence, a natural idea is to extract quantity-related features to help identify the relevant operands and their associated operators. As shown in Table 5 , a binary indicator to determine whether a quantity refers to a rate is adopted in many solvers <cite>[26]</cite> [28] [15] [40] [45] . It signals a strong connection between the quantity and operators of {\u00d7, \u00f7}. The value of the quantity is also useful for operator classifier or quantity relevance classifier.",
  "y": "uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_14",
  "x": "In this manner, quantities associated with the same operators would to likely to share similar context information. A trivial trick used in <cite>[26]</cite> [28] [15] is to examine whether there exists comparative adverbs. For example, terms \"more\", \"less\" and \"than\" indicate operators of {+, \u2212}.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_15",
  "x": "The relationship between two quantities is helpful to determine their associated operator. A straightforward example is that if two quantities are associated with the same unit, they can be applied with addition and subtraction <cite>[26]</cite> [28] [15] [40] . If one quantity is related to a rate and the other is associated with a unit that is part of the rate, their operator is likely to be multiplication or division <cite>[26]</cite> [27] [28] [15] .",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_16",
  "x": "If one quantity is related to a rate and the other is associated with a unit that is part of the rate, their operator is likely to be multiplication or division <cite>[26]</cite> [27] [28] [15] . Numeric relation and context similarity are two types of quantity-pair features proposed in [40] [45] . The former obtains two sets of nouns located within the same sentence as the two quantities and sorts them by the distance in the dependency tree.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_17",
  "x": "Finally, a popular quantity-pair feature used in <cite>[26]</cite> [28] [15] [39] [40] [45] examines whether the value of one quantity is greater than the other, which is helpful to determine the correct operands for subtraction operator. ---------------------------------- **QUESTION-RELATED FEATURES**",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_18",
  "x": "Two types of quantity-pair features were both adopted in the template-based solutions to equation set problems [39] [40] . Finally, a popular quantity-pair feature used in <cite>[26]</cite> [28] [15] [39] [40] [45] examines whether the value of one quantity is greater than the other, which is helpful to determine the correct operands for subtraction operator.",
  "y": "uses background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_19",
  "x": "Distinguishing features can also be derived from questions. It is straightforward to figure out that the unknown variable can be inferred from the question and if a quantity whose unit appears in the question, this quantity is likely to be relevant. The remain question-related features presented in Table 5 were proposed by Roy et al. <cite>[26]</cite> , [28] and followed by MathDQN [15] .",
  "y": "uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_20",
  "x": "For example, \"lose\" is a verb indicating quantity loss for an entity and related to the subtraction operator. Given a quantity, we call the verb closest to it in the dependency tree as its dependent verb. <cite>[26]</cite> [27] [28] [15] directly use dependent verb as one of the features.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_21",
  "x": "Given a quantity, we call the verb closest to it in the dependency tree as its dependent verb. <cite>[26]</cite> [27] [28] [15] directly use dependent verb as one of the features. Another widely-adopted verb-related feature is a vector capturing the distance between the dependent verb and a small pre-defined collection of verbs that are found to be useful in categorizing arithmetic operations.",
  "y": "similarities"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_22",
  "x": "<cite>[26]</cite> [27] [28] [15] directly use dependent verb as one of the features. Another widely-adopted verb-related feature is a vector capturing the distance between the dependent verb and a small pre-defined collection of verbs that are found to be useful in categorizing arithmetic operations. Again, the remaining features come from the works <cite>[26]</cite> , [28] , [15] .",
  "y": "uses"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_23",
  "x": "**GLOBAL FEATURES** There are certain types of global features in the document-level proposed by existing solvers. <cite>[26]</cite> , [28] , [15] use the number of quantities in the problem text as part of feature space.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_24",
  "x": "The dependence root is \"bought\". Comparative adverbs <cite>[26]</cite> [28] [15] For \"If she drank 25 of them and then bought 30 more.\", \"more\" is a comparative term in the window of quantity \"30\". ----------------------------------",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_25",
  "x": "Whether both quantities have the same unit <cite>[26]</cite> [28] [15] [40] For \"Student tickets cost 4 dollars and general admission tickets cost 6 dollars\", quantities \"4\" and \"6\" have the same unit. If one quantity is related to a rate and the other is associated with a unit that is part of the rate <cite>[26]</cite> [27] [28] [15] For \"each box has 9 pieces\" and \"Paul bought 6 boxes of chocolate candy\", \"9\" is related to a rate ( i.e., pieces/box) and \"6\" is associated to the unit \"box\". Numeric relation of two quantities [40] [45] For each quantity, the nouns around it are extracted and sorted by the distance in the dependency tree.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_26",
  "x": "Whether both quantities have the same unit <cite>[26]</cite> [28] [15] [40] For \"Student tickets cost 4 dollars and general admission tickets cost 6 dollars\", quantities \"4\" and \"6\" have the same unit. If one quantity is related to a rate and the other is associated with a unit that is part of the rate <cite>[26]</cite> [27] [28] [15] For \"each box has 9 pieces\" and \"Paul bought 6 boxes of chocolate candy\", \"9\" is related to a rate ( i.e., pieces/box) and \"6\" is associated to the unit \"box\". Numeric relation of two quantities [40] [45] For each quantity, the nouns around it are extracted and sorted by the distance in the dependency tree.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_27",
  "x": "[39] [40] For \"2 footballs and 3 soccer balls cost 220 dollars\", the dependency path for the quantity pair (2, 3) is num(footballs,2) conj(footballs, balls)num(balls, 3). Whether both quantities appear in the same sentence [39] [40] Whether the value of the first quantity is greater than the other [ [15] For the question \"How many apples are left in the box?\" and a quantity 77 that appears in \"77 apples in a box\", there are two matching tokens (\"apples\" and \"box\"). Number of quantities which happen to have the maximum number of matching tokens with the question <cite>[26]</cite> [28] [15] For \"Rose have 9 apples and 12 erasers. ... 3 friends.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_28",
  "x": "Whether any component of the rate is present in the question <cite>[26]</cite> [28] [15] Given a question \"How many blocks does George have?\" and a quantity 6 associated with rate \"blocks/box\", the feature indicator is set to 1 since block appears in the question. Whether the question contains terms like \"each\" or \"per\" <cite>[26]</cite> [28] [15] Whether the question contains comparison-related terms like \"more\" or \"less\" <cite>[26]</cite> [28] [15] Whether the question contains terms like \"how many\" [39] [40] [45] [13] It implies that the solution is positive. ----------------------------------",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_29",
  "x": "Whether any component of the rate is present in the question <cite>[26]</cite> [28] [15] Given a question \"How many blocks does George have?\" and a quantity 6 associated with rate \"blocks/box\", the feature indicator is set to 1 since block appears in the question. Whether the question contains terms like \"each\" or \"per\" <cite>[26]</cite> [28] [15] Whether the question contains comparison-related terms like \"more\" or \"less\" <cite>[26]</cite> [28] [15] Whether the question contains terms like \"how many\" [39] [40] [45] [13] It implies that the solution is positive. ----------------------------------",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_30",
  "x": "---------------------------------- **VERB-RELATED FEATURES** Dependent verb of a quantity <cite>[26]</cite> [27] [28] [15] the verb closest to the quantity in the dependency tree Distance vector between the dependent verb and a small collection of predefined verbs that are useful for arithmetic operator classification [21] [24] [27] Whether two quantities have the same dependent verbs <cite>[26]</cite> [28] [15] For \"In the first round she scored 40 points and in the second round she scored 50 points\", the quantities \"40\" and \"50\" both have the same verb \"scored\". Note that \"scored\" appeared twice in the sentence.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_31",
  "x": "**VERB-RELATED FEATURES** Dependent verb of a quantity <cite>[26]</cite> [27] [28] [15] the verb closest to the quantity in the dependency tree Distance vector between the dependent verb and a small collection of predefined verbs that are useful for arithmetic operator classification [21] [24] [27] Whether two quantities have the same dependent verbs <cite>[26]</cite> [28] [15] For \"In the first round she scored 40 points and in the second round she scored 50 points\", the quantities \"40\" and \"50\" both have the same verb \"scored\". Note that \"scored\" appeared twice in the sentence. Whether both dependent verbs refer to the same verb mention <cite>[26]</cite> [28] [15] For \"She baked 4 cupcakes and 29 cookies.\", the quantities \"4\" and \"29\" both shared the verb \"baked\". Note that \"baked\" appeared only once in the sentence.",
  "y": "background"
 },
 {
  "id": "a0db9c3a74487d3fcd11e79d44e163_32",
  "x": "Number of quantities mentioned in text <cite>[26]</cite> [28] [15] Unigrams and bigrams of sentences in the problem text [20] [39] presented an efficient characteristic pattern detection method by scanning the distribution of black pixels and generating feature points graph. In [78] , a structure mapping engine named GeoRep was proposed to generate qualitative spatial descriptions from line diagrams. After that, the visual elements can be formulated through a two-level representation architecture.",
  "y": "background"
 },
 {
  "id": "a127218cca5653f1700c0de6c8318a_0",
  "x": "Therefore, a more sophisticated method to summarize many word usages in a large corpus for concordancers is desirable. Recently, contextualized word embeddings such as (<cite>Devlin et al., 2019</cite>) were proposed in NLP to capture the context of each word usage in vectors and to model the semantic distances between the usages using contexts as a clue. Unlike previous studies (Liu et al., 2017; Smilkov et al., 2016) that visualized different words using word embeddings, in this paper, we introduce a novel system intuitively helpful for concordancer users to visualize different usages of a word of interest.",
  "y": "background"
 },
 {
  "id": "a127218cca5653f1700c0de6c8318a_1",
  "x": "Unlike concordancers, our system has a database that stores contextualized word embeddings for each usage or occurrence of each word in the corpus. We used half a million sentences from the British National Corpus (BNC Consortium, 2007) as the raw corpus. We built the database by applying the bert-base-uncased model of the PyTorch Pretrained the BERT project 1 (<cite>Devlin et al., 2019</cite>) to the corpus.",
  "y": "uses"
 },
 {
  "id": "a127218cca5653f1700c0de6c8318a_2",
  "x": "The red lightly-colored point is the probe point: the usages are listed in the nearest order of the probe point. No usage is linked to the probe point. Users can 1 <cite>https://github.com/huggingface/ pytorch-pretrained-BERT</cite> 2 Fig. 2 and Fig. 3 shows use cases on a 10, 000-sentence experpt of the BNC corpus to avoid having too many hits hinder the reading of the paper.",
  "y": "uses"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_0",
  "x": "Whilst gender bias in the form of concepts of masculinity and femininity has been found inscribed in implicit ways in AI systems more broadly (Adam, 2006) , this paper focuses on gender bias on word embeddings. Word embeddings are one of the most common techniques for giving semantic meaning to words in text and are used as input in virtually every neural NLP system (Goldberg, 2017) . It has been shown that word embeddings capture human biases (such as gender bias) present in these corpora in how they relate words to each other (Bolukbasi et al., 2016;<cite> Caliskan et al., 2017</cite>; Garg et al., 2018) .",
  "y": "background"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_1",
  "x": "Several methods have been proposed to test for the presence of gender bias in word embeddings; an example being the Word Embedding Association Test (WEAT) <cite>(Caliskan et al., 2017)</cite> . WEAT is a statistical test that detects bias in word embeddings using cosine similarity and averaging methods, paired with hypothesis testing. WEAT's authors applied these tests to the publicly-available GloVe embeddings trained on the English-language \"Common Crawl\" corpus (Pennington et al., 2014) as well as the Skip-Gram (word2vec) embeddings trained on the Google News corpus (Mikolov et al., 2013) .",
  "y": "background"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_2",
  "x": "To address this, we applied the WEAT test on four sets of word embeddings trained on corpora from four domains: social media (Twit-ter), a Wikipedia-based gender-balanced corpus (GAP) and a biomedical corpus (PubMed) and news (Google News, in order to reproduce and validate our results against those of<cite> Caliskan et al. (2017)</cite> ) (see Section 3). Caliskan et al. (2017) confirmed the presence of gender bias using three categories of words wellknown to be prone to exhibit gender bias: (B1) career vs. family activities, (B2) Maths vs. Arts and (B3) Science vs. Arts. Garg et al. (2018) expanded on this work and tested additional gender bias word categories: (B4) differences on personal descriptions based on intelligence vs. appearance and on (B5) physical or emotional strength vs. weakness.",
  "y": "similarities uses"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_3",
  "x": "**WEAT HYPOTHESIS TESTING 4.1 EXPERIMENTAL PROTOCOL** We largely follow the WEAT Hypothesis testing protocol introduced by<cite> Caliskan et al. (2017)</cite> . The input is a suspected gender bias word category represented by two lists, X and Y , of target words, i.e. words which are suspected to be biased to one or another gender.",
  "y": "similarities uses"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_4",
  "x": "Assuming that there is a word embedding vector w (trained on some corpus from some domain) for each word w in X, Y , M and F , we compute the following test statistic: (1) where s(w, M, F ) is the measure of association between target word w and the attribute words in M and F : In<cite> Caliskan et al. (2017)</cite> H o is tested through a permutation test, in which X \u222a Y is partitioned into alternative target listsX and\u0176 exhaustively and computing the one-sided p-value p[s(X,\u0176 , M, F ) > s(X, Y, M, F )], i.e. the proportion of partition permutationsX,\u0176 in which the test statistic s(X,\u0176 , M, F ) is greater than the observed test statistic s(X, Y, M, F ).",
  "y": "background"
 },
 {
  "id": "a334cda78f8ba6dea709809f0999b6_5",
  "x": "Although one would hope to find little gender bias in a news corpus, given that its authors are professional journalists, bias had already been detected by<cite> Caliskan et al. (2017)</cite> and Garg et al. (2018) using methods similar to ours. This is not surprising given that women represent only a third (33.3%) of the full-time journalism workforce (Byerly, 2011) . In addition, it has been found that news coverage of female personalities more frequently mentions family situations and is more likely to invoke matters of superficial nature, such as personality, appearance and fashion decisions, whereas the focus on men in news coverage tends to be be given to their experience and accomplishments (Armstrong et al., 2006) .",
  "y": "similarities"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_0",
  "x": "Hence, an important question arises for this scenario: how can we do unsupervised adaptation for acoustic models by utilizing labeled out-of-domain data and unlabeled in-domain data, in order to achieve good performance on in-domain data? Research on unsupervised adaptation for acoustic models can be roughly divided into three categories: (1) constrained model adaptation [5, 6, 7] , (2) domain-invariant feature extraction [8, 9, 10] , and (3) labeled in-domain data augmentation by synthesis [11, 12, <cite>13]</cite> . Among these approaches, data augmentation-based adaptation is favorable, because it does not require extra hyperparameter tuning for acoustic model training, and can utilize full model capacity by training a model with as much and as diverse a dataset as possible.",
  "y": "background"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_1",
  "x": "In other words, it is easier for us to inspect and manipulate the data. Furthermore, with the recent progress on domain translation<cite> [13,</cite> 14, 15] , conditional synthesis of indomain data without parallel data has become achievable, which makes data augmentation-based adaptation a more promising direction to investigate. Variational autoencoder-based data augmentation (VAE-DA) is a domain adaptation method proposed in<cite> [13]</cite> , which pools in-domain and out-domain to train a VAE that learns factorized latent representations of speech segments.",
  "y": "background"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_2",
  "x": "In other words, it is easier for us to inspect and manipulate the data. Furthermore, with the recent progress on domain translation<cite> [13,</cite> 14, 15] , conditional synthesis of indomain data without parallel data has become achievable, which makes data augmentation-based adaptation a more promising direction to investigate. Variational autoencoder-based data augmentation (VAE-DA) is a domain adaptation method proposed in<cite> [13]</cite> , which pools in-domain and out-domain to train a VAE that learns factorized latent representations of speech segments.",
  "y": "background"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_3",
  "x": "Variational autoencoder-based data augmentation (VAE-DA) is a domain adaptation method proposed in<cite> [13]</cite> , which pools in-domain and out-domain to train a VAE that learns factorized latent representations of speech segments. In this paper, we extend VAE-DA and address the issue by learning interpretable and disentangled representations using a variant of VAEs that is designed for sequential data, named factorized hierarchical variational autoencoders (FHVAEs) [15] .",
  "y": "extends"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_4",
  "x": "---------------------------------- **ESTIMATING LATENT NUISANCE VECTORS** A key observation made in<cite> [13]</cite> is that nuisance factors, such as speaker identity and room acoustics, are generally constant over segments within an utterance, while linguistic content changes from segment to segment.",
  "y": "background"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_5",
  "x": "We now derive two data augmentation methods similar to those proposed in<cite> [13]</cite> , named nuisance factor replacement and nuisance factor perturbation. ---------------------------------- **NUISANCE FACTOR REPLACEMENT**",
  "y": "similarities"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_6",
  "x": "However, in practice, VAE-type of models suffer from an over-pruning issue [21] in that some latent variables become inactive, which we do not want to perturb. Instead, we only want to perturb the linear subspace which models the variation of nuisance factors between utterances. Therefore, we adopt a similar soft perturbation scheme as in<cite> [13]</cite> .",
  "y": "similarities"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_8",
  "x": "The third row reports the results of training with domain invariant feature, z1, extracted with a FHVAE as is done in [10] . It improves over the baseline by 6% absolute. VAE-DA<cite> [13]</cite> results with nuisance factor replacement (repl) and latent nuisance perturbation (p) are shown in the last three rows.",
  "y": "uses"
 },
 {
  "id": "a3ad95d75b7750b8a879fa183e30f6_9",
  "x": "Both augmentation methods outperform their VAE counterparts and the domain invariant feature baseline using the same FHVAE model. We attribute the improvement to the better quality of the transformed IHM data, which covers the nuisance factors of the SDM data, without altering the original linguistic content. To verify the superiority of the proposed method of drawing random perturbation vectors, we compare two alternative sampling methods: rev-p and uni-p, similar to<cite> [13]</cite> , with the same expected squared Euclidean norm as the proposed method.",
  "y": "similarities"
 },
 {
  "id": "a62f376adefad10c5fb8b6c08ebb63_0",
  "x": "In string-based statistical machine translation, the alignment space is typically restricted by the n-grams considered in the underlying language model, but in syntax-based machine translation the alignment space is restricted by very different and less transparent structural contraints. While it is easy to estimate the consequences of restrictions to n-grams of limited size, it is less trivial to estimate the consequences of the structural constraints imposed by syntax-based machine translation formalisms. Consequently, much work has been devoted to this task (Wu, 1997; Zens and Ney, 2003;<cite> Wellington et al., 2006</cite>; Macken, 2007; S\u00f8gaard and Kuhn, 2009) .",
  "y": "background"
 },
 {
  "id": "a62f376adefad10c5fb8b6c08ebb63_1",
  "x": "The task of estimating the consequences of the structural constraints imposed by a particular syntax-based formalism consists in finding what is often called \"empirical lower bounds\" on the coverage of the formalism <cite>(Wellington et al., 2006</cite>; S\u00f8gaard and Kuhn, 2009 ). Gold standard alignments are constructed and queried in some way as to identify complex alignment configurations, or they are parsed by an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism. The assumption in this and related work that enables us to introduce a meaningful notion of alignment capacity is that simultaneously recognized words are aligned (Wu, 1997; Zhang and Gildea, 2004;<cite> Wellington et al., 2006</cite>; S\u00f8gaard and Kuhn, 2009) .",
  "y": "background"
 },
 {
  "id": "a62f376adefad10c5fb8b6c08ebb63_2",
  "x": "The task of estimating the consequences of the structural constraints imposed by a particular syntax-based formalism consists in finding what is often called \"empirical lower bounds\" on the coverage of the formalism <cite>(Wellington et al., 2006</cite>; S\u00f8gaard and Kuhn, 2009 ). Gold standard alignments are constructed and queried in some way as to identify complex alignment configurations, or they are parsed by an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism. The assumption in this and related work that enables us to introduce a meaningful notion of alignment capacity is that simultaneously recognized words are aligned (Wu, 1997; Zhang and Gildea, 2004;<cite> Wellington et al., 2006</cite>; S\u00f8gaard and Kuhn, 2009) .",
  "y": "background motivation"
 },
 {
  "id": "a62f376adefad10c5fb8b6c08ebb63_3",
  "x": "---------------------------------- **NON-INDUCED CONFIGURATIONS** Inside-out alignments were first described by Wu (1997) , and their frequency has been a matter of some debate (Lepage and Denoual, 2005;<cite> Wellington et al., 2006</cite>; S\u00f8gaard and Kuhn, 2009) .",
  "y": "background"
 },
 {
  "id": "a7559a8775941622d269433937633a_0",
  "x": "The primary shortcoming of this presentation lies in perpetuating the false dichotomy between \"grammar-based\" and \"data-driven\" approaches to language modeling for speech recognition, which motivates the final chapter of the book. In fact, the authors' approach is both grammar-based and data-driven, given the corpus-based grammar specialization and PCFG estimation, which the authors themselves demonstrate to be indispensable. Robust grammar-based language modeling is a topic that has received a fair bit of attention over the past decade (Chelba and Jelinek 2000; <cite>Charniak 2001</cite>; Roark 2001; Wang, Stolcke, and Harper 2004, among others) , and while this line of research has not focused on the use of manually built, narrow-domain feature grammars, there is enough similarity between the approach described in this book and the cited papers that the papers would seem to be better comparison points than the class-based language models that are chosen to represent robust approaches in the comparison.",
  "y": "similarities background"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_0",
  "x": "It does so with high accuracy on informal event mentions in social media by learning to integrate the likelihood of multiple candidate dates extracted from event mentions in timerich sentences with temporal constraints extracted from event-related sentences. Despite considerable prior work in temporal information extraction, to date state-of-the-art resources are designed for extracting temporally scoped facts about public figures/organizations from newswire or Wikipedia articles<cite> McClosky and Manning, 2012</cite>; Garrido et [11/15/2008] I have noticed some pulling recently and I won't start rads until March. [11/20/2008] It is sloowwwly healing, so slowly, in fact, that she said she HOPES it will be healed by March, when I am supposed to start rads.",
  "y": "background"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_1",
  "x": "Besides a standard event-time classifier for within-sentence event-time anchoring, we leverage a new source of temporal information to train a constraint-based event-time classifier. Previous work only retrieves time-rich sentences that include both the query and some TEs<cite> McClosky and Manning, 2012</cite>; Garrido et al., 2012) . However, sentences that contain only the event mention but no explicit TE can also be informative.",
  "y": "background motivation"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_2",
  "x": "Previous work on TE extraction has focused mainly on newswire text (Strotgen and Gertz, 2010; Chang and Manning, 2012) . This paper presents a rule-based TE extractor that identifies and resolves a higher percentage of nonstandard TEs than earlier state-of-art temporal taggers. Our task is closest to the temporal slot filling track in the TAC-KBP 2011 shared task and timelining task<cite> (McClosky and Manning, 2012)</cite> .",
  "y": "similarities"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_3",
  "x": "We train a MaxEnt classifier to predict the temporal relationship between the retrieved TE and the event date as overlap or no-overlap, similar to the within-sentence event-time anchoring task in TempEval-2 (UzZaman and Allen, 2010). Features for the classifier include many of those in <cite>(McClosky and Manning, 2012</cite>; Yoshikawa et al., 2009 ): namely, event keyword and its dominant verb, verb and preposition that dominate TE, dependency path between TE and keyword and its length, unigram and bigram word and POS features. New features include the Event-Subject, Negative and Modality features.",
  "y": "similarities"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_4",
  "x": "Previous work only retrieves time-rich sentences (i.e., date sentences) (Ling and Weld, 2010;<cite> McClosky and Manning, 2012</cite>; Garrido et al., 2012) . However, keyword sentences can inform temporal constraints for events and therefore should not be ignored. For example, \"Well, I'm officially a Radiation grad!\" indicates the user has done radiation by the time of the post (DCT).",
  "y": "motivation background"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_5",
  "x": "For less than 4% of users, we have multiple dates for the same event (e.g., a user had a mastectomy twice). Similar to the evaluation metric in a previous study , in these cases, we give the system the benefit of the doubt and the extracted date is considered correct if it matches one of the gold dates. In previous work <cite>(McClosky and Manning, 2012</cite>; , the evaluation metric score is defined as 1/((1 + |d|)) where d is the difference between the values in years.",
  "y": "background"
 },
 {
  "id": "a789aea59eebfefb990dfc6367d323_6",
  "x": "Similar to the evaluation metric in a previous study , in these cases, we give the system the benefit of the doubt and the extracted date is considered correct if it matches one of the gold dates. In previous work <cite>(McClosky and Manning, 2012</cite>; , the evaluation metric score is defined as 1/((1 + |d|)) where d is the difference between the values in years. We choose a much stricter evaluation metric because we need a precise event date to study user behavior changes.",
  "y": "differences background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_0",
  "x": "**INTRODUCTION** Recent work has highlighted the challenges of identifying the STANCE that a speaker holds towards a particular political, social or technical topic. Classifying stance involves identifying a holistic subjective disposition, beyond the word or sentence (Lin et al., 2006; Malouf and Mullen, 2008; Greene and Resnik, 2009; Somasundaran and Wiebe, 2009;<cite> Somasundaran and Wiebe, 2010)</cite> .",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_1",
  "x": "Our long term goal is to understand the discourse and dialogic structure of such conversations. This could be useful for: (1) creating automatic summaries of each position on an issue (SparckJones, 1999); (2) gaining a deeper understanding of what makes an argument persuasive (Marwell and Schmitt, 1967) ; and (3) identifying the linguistic reflexes of perlocutionary acts such as persuasion and disagreement (Walker, 1996; Greene and Resnik, 2009;<cite> Somasundaran and Wiebe, 2010</cite>; Marcu, 2000) . As a first step, in this paper we aim to automatically identify rebuttals, and identify the speaker's stance towards a particular topic.",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_2",
  "x": "Thus, this work treats each post as a monologic text to be classified in terms of stance, for a particular topic. They show that discourse relations such as concessions and the identification of argumentation triggers improves performance over sentiment features alone (Somasundaran and Wiebe, 2009;<cite> Somasundaran and Wiebe, 2010)</cite> . This work, along with others, indicates that for such tasks it is difficult to beat a unigram baseline (Pang and Lee, 2008) .",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_3",
  "x": "For example, Table 1 shows that discussions about Cats vs. Dogs consist of short simple words in short sentences with relatively high usage of positive emotion words and pronouns, whereas 2nd amendment debates use relatively longer sentences, and death penalty debates (unsurprisingly) use a lot of negative emotion words. Human Topline. The best performance for siding ideological debates in previous work is approximately 64% accuracy over all topics, for a collection of 2nd Amendment, Abortion, Evolution, and Gay Rights debate posts<cite> (Somasundaran and Wiebe, 2010)</cite> .",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_4",
  "x": "Each of our 12 topics consists of more than one debate: each debate was mapped by hand to the topic and topic-siding (as in<cite> (Somasundaran and Wiebe, 2010)</cite>). We selected equal numbers of posts for each topic for each side, and created 132 tasks (Mechanical Turk HITs). Each HIT consisted of choosing the correct side for 10 posts divided evenly, and selected randomly without replacement, from two debates.",
  "y": "similarities"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_5",
  "x": "Previous work suggests that the unigram baseline can be difficult to beat for certain types of debates <cite>(Somasundaran and Wiebe, 2010</cite> ). Thus we derived both unigrams and bigrams as features. We also include basic counts such as post length.",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_6",
  "x": "See Table 1 . Syntactic Dependency. Previous research in this area suggests the utility of dependency structure to determine the TARGET of an opinion word (Joshi and Penstein-Ros\u00e9, 2009; Somasundaran and Wiebe, 2009;<cite> Somasundaran and Wiebe, 2010)</cite> .",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_7",
  "x": "The overal lack of impact for either the POS generalized dependency features (GDepP) or the Opinion generalized dependency features (GDep0) is surprising given that they improve accuracy for other similar tasks (Joshi and Penstein-Ros\u00e9, 2009;<cite> Somasundaran and Wiebe, 2010)</cite> . While our method of extracting the GDepP features is identical to (Joshi and Penstein-Ros\u00e9, 2009 ), our method for extracting GDepO is an approximation of the method of<cite> (Somasundaran and Wiebe, 2010)</cite> , that does not rely on selecting particular patterns indicating the topics of arguing by using a development set. The LIWC feature set, which is based on a lexical hierarchy that includes social features, negative and positive emotion, and psychological processes, is the only feature set that appears to have the potential to systematically show improvement over a good range of topics.",
  "y": "background"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_8",
  "x": "This suggests a difference between the debate posts in their corpus and the Convinceme data we used which may be related to the proportion of rebuttals. The overal lack of impact for either the POS generalized dependency features (GDepP) or the Opinion generalized dependency features (GDep0) is surprising given that they improve accuracy for other similar tasks (Joshi and Penstein-Ros\u00e9, 2009;<cite> Somasundaran and Wiebe, 2010)</cite> . While our method of extracting the GDepP features is identical to (Joshi and Penstein-Ros\u00e9, 2009 ), our method for extracting GDepO is an approximation of the method of<cite> (Somasundaran and Wiebe, 2010)</cite> , that does not rely on selecting particular patterns indicating the topics of arguing by using a development set.",
  "y": "extends differences"
 },
 {
  "id": "a82bdc55c15bb2bcee77c57641b1b5_9",
  "x": "Interesting, the use of contextual features provided surprisingly greater performance for particular topics. For example for 2nd Amendment, unigrams with context yield a performance of 69.23% as opposed to the best performing without context features using LIWC of 64.10%. The best performance of<cite> (Somasundaran and Wiebe, 2010)</cite> below the majority class baseline for all of the features without context.",
  "y": "background"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_0",
  "x": "We experimented with four datasets widely used in literature: BLESS (Baroni and Lenci, 2011) , EVALution (Santus et al., 2015) , Lenci/Benotto (Benotto, 2015) , and Weeds (Weeds et al., 2014) taken from the repository provided by <cite>(Shwartz et al., 2017)</cite> . The corpus of articles we use is a complete xml dump of the English Wikipedia dated 3 Nov 2017. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_1",
  "x": "For BLESS, as seen in (Santus et al., 2014) , the performance of SLQS is 87%. Similar to SLQS, our depth measure is motivated by distributional informativeness hypothesis <cite>(Shwartz et al., 2017)</cite> . However, without using the extensive computation of context vectors and entropy, we are able to demonstrate good performance.",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_2",
  "x": "We compared our numbers with those given in <cite>(Shwartz et al., 2017)</cite> . In that paper, multiple measures are used, and the best performing measure for every row of the table is presented. We conducted the experiments for both, Star as well as the Linear topology.",
  "y": "uses similarities"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_3",
  "x": "We conducted the experiments for both, Star as well as the Linear topology. However, the results for Star topology were slightly better, hence we present these in Table ( 2). For the case of hypernym vs all other relations, except for EVALution, in all other data sets, our average precision (AP ) using both Jaccard and word2vec ( <cite>(Shwartz et al., 2017)</cite> call this as AP @all) is better than the best unsu- Table 2 : AP = average precision.",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_4",
  "x": "For the case of hypernym vs all other relations, except for EVALution, in all other data sets, our average precision (AP ) using both Jaccard and word2vec ( <cite>(Shwartz et al., 2017)</cite> call this as AP @all) is better than the best unsu- Table 2 : AP = average precision. The Best AP and Best Measure is taken from <cite>(Shwartz et al., 2017)</cite> . pervised measure as reported in <cite>(Shwartz et al., 2017)</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_5",
  "x": "pervised measure as reported in <cite>(Shwartz et al., 2017)</cite> . For comparing hypernyms against individual relations, we find that with Jaccard similarity, it performs better than the best measures on meronyms in EVALution, and coordinates in Weeds. However, it performs worse for both the relations in BLESS.",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_6",
  "x": "For comparing hypernyms against individual relations, we find that with Jaccard similarity, it performs better than the best measures on meronyms in EVALution, and coordinates in Weeds. However, it performs worse for both the relations in BLESS. Our systems performs worse than the best measure whenever an Informativeness Measure <cite>(Shwartz et al., 2017)</cite> , like SLQS and its variants perform well.",
  "y": "similarities uses"
 },
 {
  "id": "aa496bd71f380e02dd392cda969999_7",
  "x": "However, having common headings is an indication of shared features, implying similarity, which is also indicated by inclusion measures. However, it should be noted, that we are comparing our single system against the best performing one in each case. For finding the best measure, <cite>(Shwartz et al., 2017)</cite> finds the best by varying the measures as well as the features, whereas we have a fixed system.",
  "y": "extends differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_0",
  "x": "There are several works for WSD that do not depend on a sense tagged corpus, and they can be classified into three approaches according to main resources used: raw corpus based approach [2] , dictionary based approach [3, 4] and hierarchical lexical database approach. The hierarchical lexical database approach can be reclassified into three groups according to usages of the database: gloss based method [5] , conceptual density based method [6, 7] and relative based method [8,<cite> 9,</cite> 10] . Since our method is a kind of the relative based method, this section describes the related works of the relative based method.",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_1",
  "x": "His method has another problem in disambiguating senses of a large number of target words because it requires a great amount of time and storage space to collect example sentences of relatives of the target words. <cite>[9]</cite> followed the method of [8] , but tried to resolve the ambiguous relative problem by using just unambiguous relatives. That is, the ambiguous relative rail is not utilized to build a training data of the word crane because the word rail is ambiguous.",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_2",
  "x": "Since WordNet is freely available for research, various kinds of WSD studies based on WordNet can be compared with the method of <cite>[9]</cite> . They evaluated their method on 14 ambiguous nouns and achieved a good performance comparable to the methods based on the sense tagged corpus. However, the evaluation was conducted on a small part of senses of the target words like [8] .",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_3",
  "x": "[10] reimplemented the method of <cite>[9]</cite> using a web, which may be a very large corpus, in order to collect example sentences. They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as [8] and <cite>[9]</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_4",
  "x": "Like [8] , the method also has a difficulty in disambiguating senses of many words because the method collects the example sentences of relatives of many words. [10] reimplemented the method of <cite>[9]</cite> using a web, which may be a very large corpus, in order to collect example sentences. They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as [8] and <cite>[9]</cite> .",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_5",
  "x": "3) Finally, a sense of the target word is determined as the sense that is related to the selected relative. In this example, the relative stork is selected with the highest probability and the proper sense is determined as crane#1, which is related to the selected relative stork. Our method makes use of ambiguous relatives as well as unambiguous relatives unlike <cite>[9]</cite> and hence overcomes the shortage problem of relatives and also reduces the problem of ambiguous relatives in [8] by handling relatives separately instead of putting example sentences of the relatives together into a pool.",
  "y": "differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_6",
  "x": "Comparison with Other Relative Based Methods. We tried to compare our proposed method with the previous relative based methods. However, both of [8] and <cite>[9]</cite> did not evaluate their methods on a publicly available data.",
  "y": "differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_7",
  "x": "We tried to compare our proposed method with the previous relative based methods. However, both of [8] and <cite>[9]</cite> did not evaluate their methods on a publicly available data. We implemented their methods and compared our method with them on the same evaluation data.",
  "y": "uses"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_8",
  "x": "WordNet was utilized as a lexical database to acquire relatives of target words and the sense disambiguation modules were implemented by using on Na\u00efve Bayesian classifier, which <cite>[9]</cite> adopted though [8] utilized International Roget's Thesaurus and other classifier similar to decision lists. Also the bias of word senses, which is presented at WordNet, is reflected on the implementation in order to be in a same condition with our method. Hence, the reimplemented methods in this paper are not exactly same with the previous methods, but the main ideas of the methods are not corrupted.",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_9",
  "x": "where r l is a relative related to the sense s ij . f req(r l , w k ) and f req(r l ) are the co-occurrence frequency between r l and w k and the frequency of r l , respectively, and both frequencies can be obtained by looking up the matrix since the matrix contains the frequencies of words and word pairs. The main difference between [8] and <cite>[9]</cite> is whether ambiguous relatives are utilized or not.",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_10",
  "x": "The main difference between [8] and <cite>[9]</cite> is whether ambiguous relatives are utilized or not. Considering the difference, we implemented the method of [8] to include the ambiguous relatives into relatives, but the method of <cite>[9]</cite> to exclude the ambiguous relatives. Table 3 .",
  "y": "uses"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_11",
  "x": "[5] 24.4% 32.8% . [17] . . 58.3% [18] . . 54.8% [19] . . 48.1% Our method 40.94% 45.12% 51.35% Table 2 shows the comparison results. 7 In the table, All Relatives and Unambiguous Relatives represent the results of the reimplemented methods of [8] and <cite>[9]</cite> , respectively.",
  "y": "uses"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_12",
  "x": "7 In the table, All Relatives and Unambiguous Relatives represent the results of the reimplemented methods of [8] and <cite>[9]</cite> , respectively. It is observed in the table that our proposed method achieves better performance on all evaluation data than the previous methods though the improvement is not large. Hence, we may have an idea that our method handles relatives and in particular ambiguous relatives more effectively than [8] and <cite>[9]</cite> .",
  "y": "differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_13",
  "x": "Compared with <cite>[9]</cite> , [8] obtains a better performance, and the difference between the performance of them are totally more than 15 % on all of the evaluation data. From the comparison results, it is desirable to utilize ambiguous relatives as well as unambiguous relatives. [10] evaluated their method on nouns of lexical sample task of SENSEVAL-2.",
  "y": "background"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_14",
  "x": "When evaluated on the same nouns of the lexical sample task, our proposed method achieved 47.26%, and the method of [8] 45.61%, and the method of <cite>[9]</cite> 38.03%. Compared with our implementations, [10] utilized a web as a raw corpus that is much larger than our raw corpus, and employed various kinds of features such as bigram, trigram, part-of-speeches, etc. 8 Therefore, it can be conjectured that a size of a raw corpus and features play an important role in the performance.",
  "y": "differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_15",
  "x": "8 Therefore, it can be conjectured that a size of a raw corpus and features play an important role in the performance. We can observe that in our implementation of the method of <cite>[9]</cite> , the data sparseness problem is very serious since unambiguous relatives are usually not frequent in the raw corpus. In the web, the problem seems to be alleviated.",
  "y": "uses"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_16",
  "x": "The experimental results show that the proposed method effectively disambiguates many ambiguous words in SemCor and in test data for SENSEVAL all words task, as well as a small number of ambiguous words in test data for SENSEVAL lexical sample task. Also our method more correctly disambiguates senses than [8] and <cite>[9]</cite> . Furthermore, the proposed method achieved comparable performance with the top 3 ranked systems at SENSEVAL-2 & 3.",
  "y": "differences"
 },
 {
  "id": "abc19723df6670960705eadbaa6c13_17",
  "x": "Furthermore, the proposed method achieved comparable performance with the top 3 ranked systems at SENSEVAL-2 & 3. In consequence, our method has two advantages over the previous methods ( [8] and <cite>[9]</cite> ): our method 1) handles the ambiguous relatives and unambiguous relatives more effectively, and 2) utilizes only one co-occurrence matrix for disambiguating all contents words instead of collecting training data of the content words. However, our method did not achieve good performances.",
  "y": "differences"
 },
 {
  "id": "ad62ec914bb7b002952f22afdca15f_0",
  "x": "Their success is largely due to an efficient and user-friendly implementation that learns high quality word embeddings from very large corpora. The word2vec learns low dimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target words. Another different context type is dependency-based word embedding [11, 12, <cite>13]</cite> , which considers syntactic contexts rather",
  "y": "background"
 },
 {
  "id": "ad62ec914bb7b002952f22afdca15f_1",
  "x": "Another different context type is dependency-based word embedding [11, 12, <cite>13]</cite> , which considers syntactic contexts rather The 2016 Conference on Computational Linguistics and Speech Processing ROCLING 2016, pp. 100-102 \uf0d3 The Association for Computational Linguistics and Chinese Language Processing 100 than window contexts in word2vec.",
  "y": "background"
 },
 {
  "id": "ad62ec914bb7b002952f22afdca15f_2",
  "x": "The first step is to have the parser to produce n-best parse trees with their structural scores. For each parsed tree including words, part-of-speech (PoS) and semantic role labels. Second, we extract word-to-word associations (or called word dependency, a dependency implies its close association with other words in either syntactic or semantic perspective) from large amounts of auto-parsed data and adopt word2vecf <cite>[13]</cite> to train dependency-based word embeddings.",
  "y": "uses"
 },
 {
  "id": "aeb6a815732b36d7602a9c43c47cfa_0",
  "x": "Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (Jurgens et al., 2015) or some combination of these<cite> (Rahimi et al., 2015b</cite>,a) . The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (Eisenstein et al., 2010; Roller et al., 2012; Rout et al., 2013; Wing and Baldridge, 2014) or dialectology Eisenstein, 2015) . In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (Pavalanathan and Eisenstein, 2015) .",
  "y": "background"
 },
 {
  "id": "aeb6a815732b36d7602a9c43c47cfa_1",
  "x": "4 The results reported in <cite>Rahimi et al. (2015b</cite>; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset. While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user (Rahimi et al., 2015a) . Note that it would, of course, be possible to combine text and network information in a joint deep learning model (Yang et al., 2016; Kipf and Welling, 2016) , which we leave to future work (noting that scalability will potentially be a major issue for the larger datasets).",
  "y": "differences"
 },
 {
  "id": "aeb6a815732b36d7602a9c43c47cfa_2",
  "x": [
   "**DIALECTOLOGY** We quantitatively tested the quality of the geographical embeddings by calculating the micro-average recall of the k-nearest dialect terms (in terms of the proportion of retrieved dialect terms) given a dialect region, as shown in Figure 4 . Recall at 0.5% is about 3.6%, meaning that we were able to retrieve 3.6% of the dialect terms given the dialect region name in the geographical embedding space."
  ],
  "y": "differences"
 },
 {
  "id": "aeb6a815732b36d7602a9c43c47cfa_3",
  "x": "The performance of the text-based MLP model with k-d tree and k-means discretisation over the three datasets is shown in Table 1 . The results are also compared with state-of-the-art text-based methods based on a flat<cite> (Rahimi et al., 2015b</cite>; Cha et al., 2015) or hierarchical (Wing and Baldridge, 2014; Melo and Martins, 2015; Liu and Inkpen, 2015) geospatial representation. Our method outperforms both the flat and hierarchical text-based models by a large margin.",
  "y": "differences"
 },
 {
  "id": "af39041414dec545df878404328aab_0",
  "x": "In this paper, we propose to scale up discriminative training of<cite> (He and Deng, 2012)</cite> to train features with 150 million parameters, which is one order of magnitude higher than previously published effort, and to apply discriminative training to redistribute probability mass that is lost due to model pruning. The experimental results confirm the effectiveness of our proposals on NIST MT06 set over a strong baseline. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "af39041414dec545df878404328aab_1",
  "x": "The maximum expected BLEU training of<cite> (He and Deng, 2012</cite> ) is a recent effort towards this direction, and in this paper, we extend their work to a scaled-up task of discriminative training of the features of a strong hierarchical phrase-based model and confirm its effectiveness empirically. In this work, we further consider the application of discriminative training to pruned model. Various pruning techniques (Johnson et al., 2007; Zens et al., 2012; Eck et al., 2007; Lee et al., 2012; Tomeh et al., 2011) have been proposed recently to filter translation rules.",
  "y": "extends background"
 },
 {
  "id": "af39041414dec545df878404328aab_2",
  "x": "This shows that discriminative training makes it possible to achieve smaller models that perform comparably or even better than the baseline model. Our contributions in this paper are two-folded: First of all, we scale up the maximum expected BLEU training proposed in<cite> (He and Deng, 2012)</cite> in a number of ways including using 1) a hierarchical phrase-based model, 2) a richer feature set, and 3) a larger training set with a much larger parameter set, resulting in more than 150 million parameters in the model being updated, which is one order magnitude higher than the phrase-based model reported in<cite> (He and Deng, 2012)</cite> . We are able to show a reasonable improvement over this strong baseline.",
  "y": "extends differences"
 },
 {
  "id": "af39041414dec545df878404328aab_3",
  "x": "Given the entire training data {F n , E n } N n=1 , and current parameterization {\u03bb, \u03a6}, we decode the source side of training data F n to produce hypothesis {\u00ca n } N n=1 . Our goal is to update \u03a6 towards \u03a6 that maximizes the expected BLEU scores of the entire training data given the current \u03bb: where B(\u00ca 1 ...\u00ca N ) is the BLEU score of the concatenated hypothesis of the entire training data, following<cite> (He and Deng, 2012)</cite> .",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_4",
  "x": "To further simplify the problem and relate it with model pruning, we consider to update a subset of \u03b8 \u2282 \u03a6 while keeping other parameterization of \u03a6 unchanged, where \u03b8 = {\u03b8 ij = p(e j |f i )} denotes our parameter set that satisfies j \u03b8 ij = 1 and \u03b8 ij \u2265 0. In experiments, we also consider {\u03b8 ji = p(f i |e j )}. To alleviate overfitting, we introduce KL-distance based reguralization as in<cite> (He and Deng, 2012)</cite> .",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_5",
  "x": "The optimization algorithm is based on the Extended Baum Welch (EBW) (Gopalakrishnan et al., 1991) as derived by<cite> (He and Deng, 2012)</cite> . The final update rule is as follow: where \u03b8 ij is the updated parameter,",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_6",
  "x": "Following<cite> (He and Deng, 2012)</cite> , we focus on discriminative training of p(f |e) and p(e|f ), which in practice affects around 150 million of parameters; hence the title. For the tuning and development sets, we set aside 1275 and 1239 sentences respectively from LDC2010E30 corpus. The tune set is used by PRO for tuning \u03bb while the dev set is used to decide the best DT model.",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_7",
  "x": "Table 1 compares the key components of our baseline system with that of<cite> (He and Deng, 2012)</cite> . As shown, we are working with a stronger system than<cite> (He and Deng, 2012)</cite> , especially in terms of the number of parameters under consideration |\u03b8|. He&Deng (2012)",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_8",
  "x": "As shown, we are working with a stronger system than<cite> (He and Deng, 2012)</cite> , especially in terms of the number of parameters under consideration |\u03b8|. He&Deng (2012) ----------------------------------",
  "y": "differences"
 },
 {
  "id": "af39041414dec545df878404328aab_9",
  "x": "We then explore the optimal setting for \u03c4 which controls the contribution of the regularization term. Specifically, we perform grid search, exploring values of \u03c4 from 0.1 to 0.75. For each \u03c4 , we run several iterations of discriminative training where each iteration involves one simultaneous update of p(f |e) and p(e|f ) according to Eq. 4, followed by one update of \u03bb via PRO (as in<cite> (He and Deng, 2012)</cite> ).",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_10",
  "x": "This improvement is in the same ballpark as in<cite> (He and Deng, 2012</cite> ) though on a scaledup task. We next decode the MT06 using the best model (i.e. \u03c4 = 0.10 at 6-th iteration) observed on the dev set, and obtained 40.33 BLEU with an improvement of around 0.4 BLEU point. We see this result as confirming the effectiveness of discriminative training but on a larger-scale task, adding to what was reported by<cite> (He and Deng, 2012)</cite> .",
  "y": "similarities"
 },
 {
  "id": "af39041414dec545df878404328aab_11",
  "x": "We next decode the MT06 using the best model (i.e. \u03c4 = 0.10 at 6-th iteration) observed on the dev set, and obtained 40.33 BLEU with an improvement of around 0.4 BLEU point. We see this result as confirming the effectiveness of discriminative training but on a larger-scale task, adding to what was reported by<cite> (He and Deng, 2012)</cite> . ----------------------------------",
  "y": "extends"
 },
 {
  "id": "af39041414dec545df878404328aab_12",
  "x": "**CONCLUSION** In this paper, we first extend the maximum expected BLEU training of<cite> (He and Deng, 2012)</cite> to train two features of a state-of-the-art hierarchical phrasebased system, namely: p(f |e) and p(e|f ). Compared to<cite> (He and Deng, 2012)</cite> , we apply the algorithm to a strong baseline that is trained on a bigger parallel corpora and comes with a richer feature set.",
  "y": "extends"
 },
 {
  "id": "af39041414dec545df878404328aab_13",
  "x": "In this paper, we first extend the maximum expected BLEU training of<cite> (He and Deng, 2012)</cite> to train two features of a state-of-the-art hierarchical phrasebased system, namely: p(f |e) and p(e|f ). Compared to<cite> (He and Deng, 2012)</cite> , we apply the algorithm to a strong baseline that is trained on a bigger parallel corpora and comes with a richer feature set. The number of parameters under consideration amounts to 150 million.",
  "y": "uses"
 },
 {
  "id": "af39041414dec545df878404328aab_14",
  "x": "Compared to<cite> (He and Deng, 2012)</cite> , we apply the algorithm to a strong baseline that is trained on a bigger parallel corpora and comes with a richer feature set. The number of parameters under consideration amounts to 150 million. Our experiments show that discriminative training these two features (out of 50) gives around 0.40 BLEU point improvement, which is consistent with the conclusion of<cite> (He and Deng, 2012)</cite> but in a much larger-scale system.",
  "y": "similarities"
 },
 {
  "id": "af39041414dec545df878404328aab_15",
  "x": "**APPENDIX** We describe the process to simplify Eq. 1 to Eq. 2, which is omitted in<cite> (He and Deng, 2012)</cite> . For conciseness, we drop the conditions and write P (\u00ca i |F i ) as P (\u00ca i ).",
  "y": "differences"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_0",
  "x": "Some (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) targeted specific relations like BornInYear, CorporationAcquired, others (Wu and Weld, 2010; <cite>Fader et al., 2011</cite> ) extracted any phrase denoting a relation in an English sentence. (Banko, 2009) used labeled data to learn relations, (Suchanek et al., 2007) used information encoded in the structured Wikipedia documents, (Riloff and Jones, 1999) bootstrapped patterns. As a result various knowledge bases have been produced like TopicSignatures (Agirre and Lacalle, 2004) , ConceptNet (Liu and Singh, 2004) , Yago (Suchanek et al., 2007) , NELL (Carlson et al., 2009) and ReVerb<cite> (Fader et al., 2011)</cite> .",
  "y": "background"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_1",
  "x": "(Banko, 2009) used labeled data to learn relations, (Suchanek et al., 2007) used information encoded in the structured Wikipedia documents, (Riloff and Jones, 1999) bootstrapped patterns. As a result various knowledge bases have been produced like TopicSignatures (Agirre and Lacalle, 2004) , ConceptNet (Liu and Singh, 2004) , Yago (Suchanek et al., 2007) , NELL (Carlson et al., 2009) and ReVerb<cite> (Fader et al., 2011)</cite> . Despite the many efforts to date, yet there is no universal repository (or even a system), which for a given term it can immediately return all verb relations related to the term.",
  "y": "background"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_2",
  "x": "\u2022 We establish the effectiveness of our approach through human-based evaluation. \u2022 We conduct a comparative study with the verb-based relation extraction system ReVerb<cite> (Fader et al., 2011)</cite> and show that our approach accurately extracts more verb-based relations. \u2022 We also compare the verb relations produced by our system with those available in existing knowledge bases, and observe that despite their completeness these repositories lack many verb-based relations.",
  "y": "differences"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_3",
  "x": "Others (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) have focused on learning specific relations like BornInYear, EmployedBy and CorporationAcquired. However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (Kim and Moldovan, 1993; Soderland et al., 1999) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance<cite> (Fader et al., 2011)</cite> . However, recently developed OpenIE systems like TextRunner (Banko et al., 2007; Banko, 2009) and ReVerb<cite> (Fader et al., 2011)</cite> surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in English sentences.",
  "y": "background"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_4",
  "x": "However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (Kim and Moldovan, 1993; Soderland et al., 1999) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance<cite> (Fader et al., 2011)</cite> . However, recently developed OpenIE systems like TextRunner (Banko et al., 2007; Banko, 2009) and ReVerb<cite> (Fader et al., 2011)</cite> surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in English sentences. (Banko et al., 2007; Banko, 2009 ) define relation to be any verb-prep, adj-noun construction.",
  "y": "background"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_5",
  "x": "For our comparative study with existing systems, we used ReVerb 4<cite> (Fader et al., 2011)</cite> , which similarly to our approach was specifically designed to learn verb-based relations from unstructured texts. Currently, ReVerb has extracted relations from ClueWeb09 5 and Wikipedia, which have been freely distributed to the public. ReVerb learns relations by taking as input any document and applies POS-tagging, NP-chunking and a set of rules over all sentences in the document to generate triples containing the verbs and the arguments associated with them.",
  "y": "similarities"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_6",
  "x": "ReVerb learns relations by taking as input any document and applies POS-tagging, NP-chunking and a set of rules over all sentences in the document to generate triples containing the verbs and the arguments associated with them. According to<cite> (Fader et al., 2011)</cite> ReVerb outperforms TextRunner (Banko et al., 2007) and the open Wikipedia extractor WOE (Wu and Weld, 2010) in terms of the quantity and quality of the learned relations. For comparison, we took five terms from our experiment: ant, bomb, president, terrorists, virus and collected all verbs found by ReVerb in the ClueWeb09 and Wikipedia triples.",
  "y": "background"
 },
 {
  "id": "b1c06a67b03d81b249b320413a6e7e_7",
  "x": "Our key contribution is the development of a semi-supervised procedure, which starts with a term and a verb to learn from Web documents a large and diverse set of verb relations. We have conducted an experimental evaluation with 36 terms and have collected 26, 678 unique candidate verbs and 1, 040, 651 candidate argument fillers. We have evaluated the accuracy of our approach using human based evaluation and have compared results against the ReVerb<cite> (Fader et al., 2011)</cite> system and existing knowledge bases like NELL (Carlson et al., 2009) , Yago (Suchanek et al., 2007) and ConceptNet (Liu and Singh, 2004) .",
  "y": "similarities"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_0",
  "x": "Zero-shot NMT was first demonstrated by<cite> Johnson et al. (2017)</cite> . However, this zero-shot translation method is inferior to pivoting. They showed that the context vectors (from attention) for unseen language pairs differ from the seen language pairs, possibly explaining the degradation in translation quality.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_1",
  "x": "While initial research on NMT started with building translation systems between two languages, researchers discovered that the NMT framework can naturally incorporate multiple languages. Hence, there has been a massive increase in work on MT systems that involve more than two languages (Dong et al., 2015; Firat et al., 2016a; Cheng et al., 2017; <cite>Johnson et al., 2017</cite>;<cite> Chen et al., 2017</cite> Neubig and Hu, 2018) etc. We refer to NMT systems handling translation between more than one language pair as multilingual NMT (MNMT) systems.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_2",
  "x": "Knowledge transfer has been strongly observed for translation between low-resource languages, which have scarce parallel corpora or other linguistic resources but have benefited from data in other languages . In addition, MNMT systems will be compact, because a single model handles translations for multiple languages<cite> (Johnson et al., 2017)</cite> . This can reduce the deployment footprint, which is crucial for con- There are multiple MNMT scenarios based on available resources and studies have been conducted for the following scenarios ( Figure 1 1 ) : Multiway Translation.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_3",
  "x": "Zeroshot translation: Translating between language pairs without parallel corpora<cite> (Johnson et al., 2017)</cite> . Multi-Source Translation. Documents that have been translated into more than one language might, in the future, be required to be translated 1 Please see the supplementary material for papers related to each category.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_4",
  "x": "In fact, the number of parameters is only a small multiple of the compact model (the multiplication factor accounts for the language embedding size)<cite> (Johnson et al., 2017)</cite> , but the language embeddings can directly impact the model parameters instead of the weak influence that language tags have. Universal Encoder Representation. Ideally, multiway systems should generate encoder representations that are language agnostic.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_5",
  "x": "Blackwood et al. (2018) added the language tag to the beginning as well as end of sequence to avoid its attenuation in a leftto-right encoder. explored multiple methods for supporting target languages: (a) target language tag at beginning of the decoder, (b) target language dependent positional embeddings, and (c) divide hidden units of each decoder layer into shared and language-dependent ones. Each of these methods provide gains over<cite> Johnson et al. (2017)</cite> , and combining all gave the best results.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_6",
  "x": "To avoid this, sentence pairs from different language pairs are sampled to maintain a healthy balance. Mini-batches can be comprised of a mix of samples from different language pairs<cite> (Johnson et al., 2017)</cite> or the training schedule can cycle through mini-batches consisting of a language pair only (Firat et al., 2016a) . For architectures with language specific layers, the latter approach is convenient to implement.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_7",
  "x": "There are also many differences between MNMT and domain adaptation for NMT. While pivoting is a popular approach for MNMT (Cheng et al., 2017) , it is unsuitable for domain adaptation. As there are always vocabulary overlaps between different domains, there are no zero-shot translation<cite> (Johnson et al., 2017)</cite> settings in domain adaptation.",
  "y": "background"
 },
 {
  "id": "b1c9b8e24916b136948610383f8ea2_8",
  "x": "Addressing intrasentence multilingualism i.e. code mixed input and output, creoles and pidgins is an interesting research direction. The compact MNMT models can handle code-mixed input, but code-mixed output remains an open problem<cite> (Johnson et al., 2017)</cite> . Multilingual and Multi-Domain NMT.",
  "y": "future_work"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_0",
  "x": "Recent work has argued instead for image paragraph captioning with the aim of generating a (usually 5-8 sentence) paragraph describing an image. Compared with single-sentence captioning, paragraph captioning is a relatively new task. The main paragraph captioning dataset is the Visual Genome corpus, introduced by <cite>Krause et al. (2016)</cite> .",
  "y": "background"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_1",
  "x": "<cite>Krause et al. (2016)</cite> introduced the first large-scale paragraph captioning dataset, a subset of the Visual Genome dataset, along with a number of models for paragraph captioning. Empirically, they showed that paragraphs contain significantly more pronouns, verbs, coreferences, and greater overall \"diversity\" than singlesentence captions. Whereas most single-sentence captions in the MSCOCO dataset describe only the most important object or action in an image, paragraph captions usually touch on multiple objects and actions.",
  "y": "background"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_2",
  "x": "The paragraph captioning models proposed by <cite>Krause et al. (2016)</cite> included template-based (nonneural) approaches and two encoder-decoder models. In both neural models, the encoder is an object detector pre-trained for dense captioning. In the first model, called the flat model, the decoder is a single LSTM which outputs an entire paragraph word-by-word.",
  "y": "background"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_3",
  "x": "For our experiments, we use the top-down single-sentence captioning model in Anderson et al. (2017) . This model is similar to the \"flat\" model in <cite>Krause et al. (2016)</cite> , except that it incorporates attention with a top-down mechanism. ----------------------------------",
  "y": "similarities background"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_4",
  "x": "For our paragraph captioning model we use the top-down model from Anderson et al. (2017) . Our encoder is a convolutional network pretrained for object detection (as opposed to dense captioning, as in <cite>Krause et al. (2016)</cite> and Liang et al. (2017) ). METEOR CIDEr BLEU-1 BLEU-2 BLEU-3 BLEU-4 Krause et al. (Template) The encoder extracts between 10 and 100 objects per image and applies spatial max-pooling to yield a single feature vector of dimension 2048 per object.",
  "y": "differences"
 },
 {
  "id": "b208c7180bc3f973b8616937b2801c_5",
  "x": "METEOR CIDEr BLEU-1 BLEU-2 BLEU-3 BLEU-4 Krause et al. (Template) The encoder extracts between 10 and 100 objects per image and applies spatial max-pooling to yield a single feature vector of dimension 2048 per object. The decoder is a 1-layer LSTM with hidden dimension 512 and top-down attention. Evaluation is done on the Visual Genome dataset with the splits provided by <cite>Krause et al. (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "b2cb08afadadeddc0f8e7267163c0e_0",
  "x": "For example, in languages such as Chinese, researchers have been looking at the ability of characters to carry sentiment information (Ku et al., 2005; Xiang, 2011) . In Romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve (<cite>Banea et al., 2008</cite>) . These additional sources of information may not be available across all languages, yet, various articles have pointed out that by investigating a synergistic approach for detecting subjectivity and sentiment in multiple languages at the same time, improvements can be achieved not only in other languages, but in English as well.",
  "y": "background"
 },
 {
  "id": "b2cb08afadadeddc0f8e7267163c0e_1",
  "x": "While most of the researchers in the field are familiar with the methods applied on English, few of them have closely looked at the original research carried out in other languages. For example, in languages such as Chinese, researchers have been looking at the ability of characters to carry sentiment information (Ku et al., 2005; Xiang, 2011) . In Romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve (<cite>Banea et al., 2008</cite>) . These additional sources of information may not be available across all languages, yet, various articles have pointed out that by investigating a synergistic approach for detecting subjectivity and sentiment in multiple languages at the same time, improvements can be achieved not only in other languages, but in English as well.",
  "y": "motivation"
 },
 {
  "id": "b2cb08afadadeddc0f8e7267163c0e_2",
  "x": "While most of the researchers in the field are familiar with the methods applied on English, few of them have closely looked at the original research carried out in other languages. For example, in languages such as Chinese, researchers have been looking at the ability of characters to carry sentiment information (Ku et al., 2005; Xiang, 2011) . In Romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve (<cite>Banea et al., 2008</cite>) .",
  "y": "motivation"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_0",
  "x": "Ranging from count-based to predictive or task-based methods, in the past years, many approaches were developed to produce word embeddings, such as Neural Probabilistic Language Model [3] , Word2Vec [28] , GloVe [32] , and more recently ELMo <cite>[33]</cite> , to name a few. Although most of the recent word embedding techniques rely on the distributional linguistic hypothesis, they differ on the assumptions of how meaning or context are modeled to produce the word embeddings. These differences between word embedding techniques can have unsuspected implications regarding their performance in downstream tasks as well as in their capacity to capture linguistic properties.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_1",
  "x": "Lately, the Universal Sentence Encoder (USE) [8] mixed an unsupervised task using a large corpus together with the supervised SNLI task and showed a significant improvement by leveraging the Transformer architecture [37] , which is solely based on attention mechanisms, although without providing an evaluation with other baselines and previous works such as InferSent [10] . Neural Language Models can be tracked back to [3] , and more recently deep bi-directional language models (biLM) <cite>[33]</cite> have successfully been applied to word embeddings in order to incorporate contextual information. Very recently, [34] used unsupervised generative pre-training of language models followed by discriminative fine-tunning to achieve state-of-the-art results in several NLP downstream tasks (improving 9 out of 12 tasks).",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_2",
  "x": "In [28] , they presented two different methods to compute Two main challenges exist when learning high-quality representations: they should capture semantic and syntax and the different meanings the word can represent in different contexts (polysemy). To solve these two issues, Embedding from Language Models (ELMo) <cite>[33]</cite> was recently introduced. It uses representations from a bi-directional LSTM that is trained with a language model (LM) objective on a large text dataset.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_3",
  "x": "To solve these two issues, Embedding from Language Models (ELMo) <cite>[33]</cite> was recently introduced. It uses representations from a bi-directional LSTM that is trained with a language model (LM) objective on a large text dataset. ELMo <cite>[33]</cite> representations are a function of the internal layers of the bi-directional Language Model (biLM), which provides a very rich representation about the tokens.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_4",
  "x": "ELMo <cite>[33]</cite> representations are a function of the internal layers of the bi-directional Language Model (biLM), which provides a very rich representation about the tokens. Like in fastText [4] , ELMo <cite>[33]</cite> breaks the tradition of word embeddings by incorporating sub-word units, but ELMo <cite>[33]</cite> has also some fundamental differences with previous shallow representations such as fastText or Word2Vec. In ELMo <cite>[33]</cite> , they use a deep representation by incorporating internal representations of the LSTM network, therefore capturing the meaning and syntactical aspects of words.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_5",
  "x": "Like in fastText [4] , ELMo <cite>[33]</cite> breaks the tradition of word embeddings by incorporating sub-word units, but ELMo <cite>[33]</cite> has also some fundamental differences with previous shallow representations such as fastText or Word2Vec. In ELMo <cite>[33]</cite> , they use a deep representation by incorporating internal representations of the LSTM network, therefore capturing the meaning and syntactical aspects of words. Since ELMo <cite>[33]</cite> is based on a language model, each token representation is a function of the entire input sentence, which can overcome the limitations of previous word embeddings where each word is usually modeled as an average of their multiple contexts.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_6",
  "x": "Since ELMo <cite>[33]</cite> is based on a language model, each token representation is a function of the entire input sentence, which can overcome the limitations of previous word embeddings where each word is usually modeled as an average of their multiple contexts. Through the lens of the Ludwig Wittgenstein philosophy of language [40] , it is clear that the ELMo <cite>[33]</cite> embeddings are a better approximation to the idea of \"meaning is use\" [40] , where a word can contain a wide spectrum of different meanings depending on context, as opposed to traditional word embeddings that are not only context-independent but have a very limited definition of context. Although bag-of-words of word embeddings showed good performance for some tasks, it is still unclear how to properly represent the full sentence meaning.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_7",
  "x": "In ELMo <cite>[33]</cite> , they use a deep representation by incorporating internal representations of the LSTM network, therefore capturing the meaning and syntactical aspects of words. Since ELMo <cite>[33]</cite> is based on a language model, each token representation is a function of the entire input sentence, which can overcome the limitations of previous word embeddings where each word is usually modeled as an average of their multiple contexts. Through the lens of the Ludwig Wittgenstein philosophy of language [40] , it is clear that the ELMo <cite>[33]</cite> embeddings are a better approximation to the idea of \"meaning is use\" [40] , where a word can contain a wide spectrum of different meanings depending on context, as opposed to traditional word embeddings that are not only context-independent but have a very limited definition of context.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_8",
  "x": "ELMo (BoW, all layers, 5.5B) <cite>[33]</cite> : this model was obtained from the authors' website at https: //allennlp.org/elmo. According to the authors, the model was trained on a dataset with 5.5B tokens consisting of Wikipedia (1.9B) and all of the monolingual news crawl data from WMT 2008-2012 (3.6B). To evaluate this model, we used the AllenNLP framework [13] .",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_9",
  "x": "To evaluate this model, we used the AllenNLP framework [13] . An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> .",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_10",
  "x": "We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . ELMo (BoW, all layers, original) <cite>[33]</cite> : this model was obtained from the authors website at https://allennlp.org/elmo. According to the authors, the model was trained on the 1 Billion Word Benchmark, approximately 800M tokens of news crawl data from WMT 2011.",
  "y": "differences"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_11",
  "x": "An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . ELMo (BoW, all layers, original) <cite>[33]</cite> : this model was obtained from the authors website at https://allennlp.org/elmo.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_12",
  "x": "To evaluate this model, we used the AllenNLP framework [13] . An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model and averaging along the word dimension. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> .",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_13",
  "x": "To evaluate this model, we used the AllenNLP framework [13] . An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo <cite>[33]</cite> model and averaging along the word dimension. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> .",
  "y": "differences"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_14",
  "x": "We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . ELMo (BoW, top layer, original) <cite>[33]</cite> : the same model and procedure as in ELMo (BoW, all layers, original) was employed, except that in this experiment, we used only the top layer representation from the ELMo <cite>[33]</cite> model. As shown in <cite>[33]</cite> , the higher-level LSTM representations capture context-dependent aspects of meaning, while the lower level representations capture aspects of syntax.",
  "y": "extends"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_15",
  "x": "ELMo (BoW, top layer, original) <cite>[33]</cite> : the same model and procedure as in ELMo (BoW, all layers, original) was employed, except that in this experiment, we used only the top layer representation from the ELMo <cite>[33]</cite> model. As shown in <cite>[33]</cite> , the higher-level LSTM representations capture context-dependent aspects of meaning, while the lower level representations capture aspects of syntax. Therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_16",
  "x": "Therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment. We did not employ the trainable task-specific weighting scheme described in <cite>[33]</cite> . [7] To measure the semantic similarity between two sentences from 0 (not similar",
  "y": "differences"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_17",
  "x": "**DOWNSTREAM CLASSIFICATION TASKS** In Table 6 we show the tabular results for the downstream classification tasks, and in Figure 1 we show a graphical comparison between the different methods. As seen in Table 6 , although no method had a consistent performance among all tasks, ELMo <cite>[33]</cite> achieved best results in 5 out of 9 tasks.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_18",
  "x": "Even though ELMo <cite>[33]</cite> was trained on a language model objective, it is important to note that in this experiment a bag-of-words approach was employed. Therefore, these results are quite impressive, which lead us to believe that excellent results can be obtained by integrating ELMo <cite>[33]</cite> and the trainable task-specific weighting scheme described in <cite>[33]</cite> into InferSent [10] . InferSent [10] achieved very good results in the paraphrase detection as well as in the SICK-E (entailment).",
  "y": "differences"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_19",
  "x": "As seen in Table 6 , although no method had a consistent performance among all tasks, ELMo <cite>[33]</cite> achieved best results in 5 out of 9 tasks. Even though ELMo <cite>[33]</cite> was trained on a language model objective, it is important to note that in this experiment a bag-of-words approach was employed. Therefore, these results are quite impressive, which lead us to believe that excellent results can be obtained by integrating ELMo <cite>[33]</cite> and the trainable task-specific weighting scheme described in <cite>[33]</cite> into InferSent [10] .",
  "y": "future_work"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_20",
  "x": "As we can see, sentence embedding methods are still far away from the idea of a universal sentence encoder that can have a broad transfer quality. Given that ELMo <cite>[33]</cite> demonstrated excellent results on a broad set of tasks, it is clear that a proper integration of deep representation from language models can potentially improve sentence embedding methods by a significant margin and it is a promising research line. For completeness, we also provide an evaluation using Logistic Regression instead of a MLP in Table  11 of the appendix.",
  "y": "future_work"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_21",
  "x": "**LINGUISTIC PROBING TASKS** In Table 8 we report the results for the linguistic probing tasks and in Figure 3 we show a graphical comparison as well. As we can see in Table 8 , ELMo <cite>[33]</cite> was one of the methods that were able to achieve high performance on a broad set of different tasks.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_22",
  "x": "In Table 8 we report the results for the linguistic probing tasks and in Figure 3 we show a graphical comparison as well. As we can see in Table 8 , ELMo <cite>[33]</cite> was one of the methods that were able to achieve high performance on a broad set of different tasks. Interestingly, in the BShift (bi-gram shift) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, ELMo <cite>[33]</cite> achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as \"This is my Eve Christmas\", a sample from the BShift dataset.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_23",
  "x": "In [11] , they found that the binned sentence length task (SentLen) was negatively correlated with the performance in downstream tasks. This hypothesis was also supported by the model learning dynamics, since it seems that as model starts to capture deeper linguistic properties, it will tend to forget about this superficial feature [11] . However, the <cite>[33]</cite> bag-of-words not only achieved the best result in the SentLent task but also in many downstream tasks.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_24",
  "x": "However, the <cite>[33]</cite> bag-of-words not only achieved the best result in the SentLent task but also in many downstream tasks. Our hypothesis is that this is due to the fact that ELMo <cite>[33]</cite> is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging SOMO task. ELMo <cite>[33]</cite> word embeddings can be seen as analogous to the hypercolumns [15] approach in Computer Vision, where multiple feature levels are aggregated to form a single pixelwise representation.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_25",
  "x": "However, the <cite>[33]</cite> bag-of-words not only achieved the best result in the SentLent task but also in many downstream tasks. Our hypothesis is that this is due to the fact that ELMo <cite>[33]</cite> is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging SOMO task. ELMo <cite>[33]</cite> word embeddings can be seen as analogous to the hypercolumns [15] approach in Computer Vision, where multiple feature levels are aggregated to form a single pixelwise representation.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_26",
  "x": "Our hypothesis is that this is due to the fact that ELMo <cite>[33]</cite> is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging SOMO task. ELMo <cite>[33]</cite> word embeddings can be seen as analogous to the hypercolumns [15] approach in Computer Vision, where multiple feature levels are aggregated to form a single pixelwise representation. We leave the exploration of probing tasks for each ELMo <cite>[33]</cite> layer representation to future research, given that it could provide a framework to expose the linguistic properties capture by each representation level of the LSTM.",
  "y": "future_work"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_27",
  "x": "However, in our evaluation, the p-mean [35] approach, which has achieved better results in the WC task did not exceed other techniques such as ELMo <cite>[33]</cite> bag-of-words or InferSent [10] and USE in the downstream classification tasks. We believe that the high performance of the p-mean [35] in the WC task is due to the concatenative approach employed to aggregate the different power means. For completeness, we also provide an evaluation using Logistic Regression instead of a MLP in Table  10 of the appendix.",
  "y": "uses"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_28",
  "x": "Especially for evaluating different levels of word representations, where it can be a very useful tool to provide insights on what kind of relationships and linguistic properties each representation level (in the case of deep representations such as ELMo <cite>[33]</cite> ) is capturing. We also showed that no method had a consistent performance across all tasks, with performance being linked mostly with the downstream task similarity to the trained task of these techniques. Given that we are still far from a universal sentence encoder, we believe that this evaluation can provide an important basis for choosing which technique can potentially perform well in particular tasks.",
  "y": "background"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_29",
  "x": "We also showed that no method had a consistent performance across all tasks, with performance being linked mostly with the downstream task similarity to the trained task of these techniques. Given that we are still far from a universal sentence encoder, we believe that this evaluation can provide an important basis for choosing which technique can potentially perform well in particular tasks. Finally, we believe that new embedding training techniques that include language models as a way to capture context and meaning, such as ELMo <cite>[33]</cite> , combined with clever techniques of encoding sentences such as in InferSent [10] , can improve the performance of these encoders by a significant margin.",
  "y": "future_work"
 },
 {
  "id": "b3952c840ce970f0e66460ea6e145a_30",
  "x": "However, as we saw in the experiments, the performance of these encoders trained on particular datasets such as entailment did not perform well on a broad set of downstream tasks. Therefore, one hypothesis is that these encoders are too narrow at modeling what these embeddings can carry. We believe that the research direction of incorporating language models and multiple levels of representations can help to provide a wide set of rich features that can capture context-dependent semantics as well as linguistic features, such as seen on ELMo <cite>[33]</cite> downstream and linguistic probing task experiments, but for sentence embeddings.",
  "y": "differences future_work"
 },
 {
  "id": "b4093db328fd6839777a6d34507b34_0",
  "x": "**INTRODUCTION** Readers fixate more and longer on open syntactic categories (verbs, nouns, adjectives) than on closed class items like prepositions and conjunctions (Rayner and Duffy, 1988; Nilsson and Nivre, 2009) . Recently,<cite> Barrett and S\u00f8gaard (2015)</cite> presented evidence that gaze features can be used to discriminate between most pairs of parts of speech (POS) .",
  "y": "background"
 },
 {
  "id": "b4093db328fd6839777a6d34507b34_1",
  "x": "**EYE TRACKING DATA** The data comes from <cite>(Barrett and S\u00f8gaard, 2015)</cite> and is publicly available 1 . In this experiment 10 native English speakers read 250 syntactically annotated sentences in English (min.",
  "y": "uses"
 },
 {
  "id": "b4093db328fd6839777a6d34507b34_2",
  "x": "The features predictive of grammatical functions are similar to the features that were found to be predictive of POS <cite>(Barrett and S\u00f8gaard, 2015)</cite> , however, the probability that a word gets first and second fixation were not important features for POS classification, whereas they are contributing to dependency classification. This could suggest that words with certain grammatical functions are consistently more likely or less likely to get first and second fixation, but could also be due to a frequent syntactic order in the sample. Binary discrimination Error reduction over the baseline can be seen in Figure 2 .",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_0",
  "x": "Experiments on Automatic Content Extraction (ACE) corpora show that our model significantly outperforms featurebased joint model by Li and Ji (2014) . We also compare our model with an end-toend tree-based LSTM model (SPTree) by <cite>Miwa and Bansal (2016)</cite> and show that our model performs within 1% on entity mentions and 2% on relations. Our finegrained analysis also shows that our model performs significantly better on AGENT-ARTIFACT relations, while SPTree performs better on PHYSICAL and PART-WHOLE relations.",
  "y": "similarities uses"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_1",
  "x": "For these tasks, RNNs that make use of tree structures have been deemed more suitable. <cite>Miwa and Bansal (2016)</cite> , for example, propose an RNN comprised of a sequencebased long short term memory (LSTM) for entity identification and a separate tree-based dependency LSTM layer for relation classification using shared parameters between the two components. As a result, their model depends critically on access to dependency trees, restricting it to sentencelevel extraction and to languages for which (good) dependency parsers exist.",
  "y": "background"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_2",
  "x": "We also add an additional layer to our network to encode the output sequence from right-to-left and find significant improvement on the performance of relation identification using bi-directional encoding. Our model significantly outperforms the feature-based structured perceptron model of Li and Ji (2014) , showing improvements on both entity and relation extraction on the ACE05 dataset. In comparison to the dependency treebased LSTM model of <cite>Miwa and Bansal (2016)</cite> , our model performs within 1% on entities and 2% on relations on ACE05 dataset.",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_3",
  "x": "For joint-extraction of entities and relations, feature-based structured prediction models (Li and Ji, 2014; Miwa and Sasaki, 2014) , joint inference integer linear programming models (Yih and Roth, 2007; Yang and Cardie, 2013) , card-pyramid parsing (Kate and Mooney, 2010) and probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013) have been proposed. In contrast, we propose a neural network model which does not depend on the availability of any features such as part of speech (POS) tags, dependency trees, etc. Recently, <cite>Miwa and Bansal (2016)</cite> proposed an end-to-end LSTM based sequence and treestructured model.",
  "y": "background"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_4",
  "x": "Recently, <cite>Miwa and Bansal (2016)</cite> proposed an end-to-end LSTM based sequence and treestructured model. They extract entities via a sequence layer and relations between the entities via the shortest path dependency tree network. In this paper, we try to investigate recurrent neural networks with attention for extracting semantic relations between entity mentions without using any dependency parse tree features.",
  "y": "motivation"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_5",
  "x": "We formulate entity detection as a sequence labeling task using BILOU scheme similar to Li and Ji (2014) and <cite>Miwa and Bansal (2016)</cite> . We assign each token in the entity with the tag B appended with the entity type if it is the beginning of the entity, I for inside of an entity, L for the end of the entity or U if there is only one token in the entity. Figure 1 shows an example of the entity tag sequence assigned to the sentence.",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_6",
  "x": "We select the positive and more confident label similar to <cite>Miwa and Bansal (2016)</cite> . Multiple Relations Our approach to relation extraction is different from <cite>Miwa and Bansal (2016)</cite> . <cite>Miwa and Bansal (2016)</cite> present each pair of entities to their model for relation classification.",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_7",
  "x": "Multiple Relations Our approach to relation extraction is different from <cite>Miwa and Bansal (2016)</cite> . <cite>Miwa and Bansal (2016)</cite> present each pair of entities to their model for relation classification. In our approach, we use pointer networks to identify the related entities.",
  "y": "differences"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_8",
  "x": "For each entity, both entity mentions and its head phrase are annotated. For the scope of this paper, we only use the entity head phrase similar to Li and Ji (2014) and <cite>Miwa and Bansal (2016)</cite> . Also, there are relation types namely Physical (PHYS), Person-Social (PER-SOC), Organization-Affiliation (ORG-AFF), Agent-Artifact (ART), GPE-Affiliation (GPE-AFF).",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_9",
  "x": "We use the same data splits as Li and Ji (2014) and <cite>Miwa and Bansal (2016)</cite> such that there are 351 documents for training, 80 for development and the remaining 80 documents for the test set. ACE04 has 7 relation types with an additional Discourse (DISC) type and split ORG-AFF relation type into ORG-AFF and OTHER-AFF. We perform 5-fold cross validation similar to Chan and Roth (2011) for fair comparison with the state-of-theart.",
  "y": "uses"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_10",
  "x": "---------------------------------- **EVALUATION METRICS** In order to compare our system with the previous systems, we report micro F1-scores, Precision and Recall on both entities and relations similar to Li and Ji (2014) and <cite>Miwa and Bansal (2016)</cite> .",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_12",
  "x": "We initialized our word embeddings Table 1 : Performance on ACE05 test dataset. The dashed (\"-\") performance numbers were missing in the original paper<cite> (Miwa and Bansal, 2016)</cite> . 1 We ran the system made publicly available by <cite>Miwa and Bansal (2016)</cite> , on ACE05 dataset for filling in the missing values and comparing our system with theirs at fine-grained level.",
  "y": "uses"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_13",
  "x": "In this section, we perform a fine-grained comparison of our model with respect to the SPTree<cite> (Miwa and Bansal, 2016)</cite> model. We compare the performance of the two models with respect to entities, relation types and the distance between the relation arguments and provide examples from the test set in Table 6 . Table 3 : Performance on ACE04 test dataset.",
  "y": "uses"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_15",
  "x": "We find that our model has lower recall on entity extraction than SPTree as shown in Table 1 . <cite>Miwa and Bansal (2016)</cite> , in one of the ablation tests on ACE05 development set, show that their model can gain upto 2% improvement in recall by entity pretraining. Since we propose a jointmodel, we cannot directly apply their pretraining trick on entities separately.",
  "y": "background"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_16",
  "x": "<cite>Miwa and Bansal (2016)</cite> , in one of the ablation tests on ACE05 development set, show that their model can gain upto 2% improvement in recall by entity pretraining. Since we propose a jointmodel, we cannot directly apply their pretraining trick on entities separately. We leave it for future work.",
  "y": "future_work"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_17",
  "x": "We found considerable improvements on entity recall when using pretrained word embeddings, if available, for these \"UNK\" tokens. <cite>Miwa and Bansal (2016)</cite> also use additional features such as POS tags in addition to pretrained word embeddings at the input layer. Table 5 : Performance based on the distance between entity arguments in relations for ACE05 test dataset.",
  "y": "background"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_18",
  "x": "We also compare our model to an endto-end LSTM model by <cite>Miwa and Bansal (2016)</cite> which comprises of a sequence layer for entity extraction and a tree-based dependency layer for relation classification. We find that our model, without access to dependency trees, POS tags, etc performs within 1% on entities and 2% on relations on ACE05 dataset. We also find that our model performs significantly better than their treebased model on the ART relation, while their treebased model performs better on PHYS and PART-WHOLE relations; the two models perform comparably on all other relation types.",
  "y": "uses"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_19",
  "x": "We also compare our model to an endto-end LSTM model by <cite>Miwa and Bansal (2016)</cite> which comprises of a sequence layer for entity extraction and a tree-based dependency layer for relation classification. We find that our model, without access to dependency trees, POS tags, etc performs within 1% on entities and 2% on relations on ACE05 dataset. We also find that our model performs significantly better than their treebased model on the ART relation, while their treebased model performs better on PHYS and PART-WHOLE relations; the two models perform comparably on all other relation types.",
  "y": "similarities"
 },
 {
  "id": "b56e408c53636ac5fbf5149226319f_20",
  "x": "We find that our model, without access to dependency trees, POS tags, etc performs within 1% on entities and 2% on relations on ACE05 dataset. We also find that our model performs significantly better than their treebased model on the ART relation, while their treebased model performs better on PHYS and PART-WHOLE relations; the two models perform comparably on all other relation types. In future, we plan to explore pretraining methods for our model which were shown to improve recall on entity and relation performance by <cite>Miwa and Bansal (2016)</cite> .",
  "y": "future_work"
 },
 {
  "id": "b722b98f50669bf3b22208a25f6854_0",
  "x": "We computed multiple random subsets of sentences from the UMBC WEBBASE CORPUS (\u223c 17.13GB) via a custom implementation using the SPARK distributed framework. We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC [4]), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) <cite>[5]</cite> .",
  "y": "background"
 },
 {
  "id": "b722b98f50669bf3b22208a25f6854_1",
  "x": "Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) <cite>[5]</cite> . In our semantic tests, on average 85% of the quality can be obtained by training on a random \u223c 4% subset of the original corpus (e.g. as in Fig. 1 , 5 random million lines yield 64.14% instead of 75.14%). Our claims are that i) such evaluation posteriors are Normally distributed (Tab. I), and that ii) the variance is inversely proportional to the subset size (Tab. II).",
  "y": "extends"
 },
 {
  "id": "b722b98f50669bf3b22208a25f6854_2",
  "x": "Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) <cite>[5]</cite> . In our semantic tests, on average 85% of the quality can be obtained by training on a random \u223c 4% subset of the original corpus (e.g. as in Fig. 1 , 5 random million lines yield 64.14% instead of 75.14%). Our claims are that i) such evaluation posteriors are Normally distributed (Tab. I), and that ii) the variance is inversely proportional to the subset size (Tab. II).",
  "y": "background"
 },
 {
  "id": "b722b98f50669bf3b22208a25f6854_3",
  "x": "Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) <cite>[5]</cite> . In our semantic tests, on average 85% of the quality can be obtained by training on a random \u223c 4% subset of the original corpus (e.g. as in Fig. 1 , 5 random million lines yield 64.14% instead of 75.14%). Our claims are that i) such evaluation posteriors are Normally distributed (Tab. I), and that ii) the variance is inversely proportional to the subset size (Tab. II).",
  "y": "extends"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_0",
  "x": "(3) For entity recognition, we integrate the gazetteer with a simple, but effective machine learning classifier, and experimentally show that the extended gazetteers improve the F 1 score between 7% and 12% over our baseline approach and outperform <cite>(Zhang and Iria, 2009 )</cite> on all learned concepts (subject, location, temporal). ---------------------------------- **RELATED WORK**",
  "y": "differences"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_1",
  "x": "The approach presented in (Kazama and Torisawa, 2007; Kazama and Torisawa, 2008 ) relies solely on WIKIPEDIA, producing gazetteers without explicitly named concepts, arguing that consistent but anonymous labels are still useful. Most closely related to our own work, the authors of <cite>(Zhang and Iria, 2009 )</cite> build an approach solely on WIKIPEDIA which does not only exploit the article text but also analyzes the structural elements of WIKIPEDIA: 3 Automatically Extending Gazetteer Lists 3.1 Extraction Algorithm: Overview Algorithm 1 shows an outline of the gazetteer expansion algorithm used in EAGER.",
  "y": "similarities"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_2",
  "x": "As <cite>(Zhang and Iria, 2009 )</cite>, we reject redirection entries in this step as ambiguous. With the articles identified, we can proceed to extract category information from the abstracts and new entities from the redirect information. In the dependency analysis of article abstracts (Lines 6-9), we aim to extract category (or, more generally, hypernym) information from the abstracts of articles on the ssed list.",
  "y": "similarities"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_3",
  "x": "This analysis is inspired by <cite>(Zhang and Iria, 2009 )</cite>, but performed on the entire abstract which is clearly dis- <cite>(Zhang and Iria, 2009)</cite> , where this is applied only to the first sentence (as WIKIPEDIA does not directly provide a concept of \"abstract\"). All categories thus obtained are added to P and will be used in Section 3.4 to generate additional entities. Finally, we are interested in redirection information (Lines 10-11) about an article for a seed entity as that provides such with synonyms, plural forms, different spellings, etc.",
  "y": "motivation differences"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_4",
  "x": "In addition to categories from the abstract analysis, we also use the category graph of DBPEDIA. It has been previously observed, <cite>(Zhang and Iria, 2009 )</cite> and (Strube and Ponzetto, 2006) , that the category graph of poor quality. DBPEDIA improves little on that fact.",
  "y": "background"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_5",
  "x": "In addition to categories from the abstract analysis, we also use the category graph of DBPEDIA. It has been previously observed, <cite>(Zhang and Iria, 2009 )</cite> and (Strube and Ponzetto, 2006) , that the category graph of poor quality. DBPEDIA improves little on that fact.",
  "y": "similarities"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_6",
  "x": "To evaluate the impact of EAGER on entity recognition, we performed a large set of experiments (on the archeology domain). The experiment domains and <cite>(Zhang and Iria, 2009 )</cite>, which we outperform for all entity types, in some cases up to 5% in F 1 score. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_7",
  "x": "In this evaluation, we use the same setup as in <cite>(Zhang and Iria, 2009 )</cite>: A corpus of 30 full length UK archaeological reports archived by the Arts and Humanities Data Service (AHDS). The length of the documents varies from 4 to 120 pages. The corpus is inter-annotated by three archaeologists.",
  "y": "uses"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_8",
  "x": "The evaluate the performance (in terms of precision, recall and F 1 score) for entity recognition of the baseline system as well as the baseline system extended with a gazetteer feature. For the latter, we consider full EAGER as described in Section 3 as well as only the entities derived from dependency analysis of abstracts, from the category graph, and from redirection information. Finally, we also include the performance numbers report in <cite>(Zhang and Iria, 2009 )</cite> for comparison (since we share <cite>their</cite> evaluation settings).",
  "y": "uses"
 },
 {
  "id": "b7278824bdae498021b899fbc6c638_9",
  "x": "For the latter, we consider full EAGER as described in Section 3 as well as only the entities derived from dependency analysis of abstracts, from the category graph, and from redirection information. Finally, we also include the performance numbers report in <cite>(Zhang and Iria, 2009 )</cite> for comparison (since we share <cite>their</cite> evaluation settings). Table 1 show the results of the comparison: EA-GER significantly improves precision and recall over the baseline system and outperforms <cite>(Zhang and Iria, 2009 )</cite> in all cases.",
  "y": "differences"
 },
 {
  "id": "b7e0879c4cac85054870146e61aa6f_0",
  "x": "Subsequent to the introduction of the task, several methods have been introduced to solve the EmbodiedQA task [5, <cite>6]</cite> , using some combination of reinforcement learning, behavior cloning and hierarchical control. Apart from using the question and images from the environment, these methods also rely on varying degrees of expert supervision such as shortest path demonstrations and subgoal policy sketches. In this work, we evaluate simple question-only baselines that never see the environment and receive no form of expert supervision.",
  "y": "background"
 },
 {
  "id": "b7e0879c4cac85054870146e61aa6f_1",
  "x": "In a later work, Das et al.<cite> [6]</cite> introduce Neural Modular Control (NMC) which is a hierarchical policy network that operates over expert sub-policy sketches. The master and sub-policies are initialized with Behavior Cloning (BC), and later fine-tuned with Asynchronous Advantage Actor-Critic (A3C) [19] . Dataset Biases and Trivial Baselines: Many recent studies in language and vision show how biases in a dataset allow models to perform well on a task without leveraging the meaning of the text or image in the underlying dataset.",
  "y": "background"
 },
 {
  "id": "b7e0879c4cac85054870146e61aa6f_2",
  "x": "Specifically we train the VQA model described in<cite> [6]</cite> on the last 5 frames of oracle navigation for 50 epochs with ADAM and a learning rate of 3e \u2212 4 using batch size 20. We observe the accuracy is improved over text baselines in this unrealistic setting, but the use of this model with navigation in PACMAN reduces performance to below the text baselines. For completeness we benchmark an oracle with our BoW embedding model in place of the LSTM with all other settings kept constant.",
  "y": "uses"
 },
 {
  "id": "b7e0879c4cac85054870146e61aa6f_3",
  "x": "---------------------------------- **T 10** T 20 T 50 T any Navigation + VQA PACMAN (BC) [5] 48 BOW-CNN VQA-Only 56.5 Table 1 : We compare to the published results from<cite> [6]</cite> for agent spawned at various steps away from the target: 10, 30, 50, and anywhere in the environment.",
  "y": "uses"
 },
 {
  "id": "b7e0879c4cac85054870146e61aa6f_4",
  "x": "Results Detailed results are reported in Table 4 . Following Das et al.<cite> [6]</cite> , we report the agent's top-1 accuracy on the test set when spawned 10, 20 and 50 steps away from the goal, denoted as T 10 , T 20 and T 50 respectively. Since the performance of blindfold baselines are not affected based on where the agent is spawned, their accuracy is same across T 10 , T 20 and T 50 .",
  "y": "uses"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_0",
  "x": "---------------------------------- **INTRODUCTION** The Ubuntu Dialogue Corpus is the largest freely available multi-turn based dialog corpus <cite>[1]</cite> 1 .",
  "y": "background"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_1",
  "x": "**DATA** In this section we briefly describe the data and evaluation metrics used in <cite>[1]</cite> . First, all the collected data was preprocessed by replacing named entities with corresponding tags (name, location, organization, url, path).",
  "y": "background"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_2",
  "x": "Note that pointwise method was also used in the original baselines <cite>[1]</cite> . ---------------------------------- **POINTWISE RANKING**",
  "y": "background"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_3",
  "x": "---------------------------------- **PREVIOUS WORK** The pointwise architectures reported in <cite>[1]</cite> included (i) TF-IDF, (ii) RNN and (iii) LSTM.",
  "y": "background"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_4",
  "x": "---------------------------------- **METHOD** To match the original setup of <cite>[1]</cite> we use the same training data 3 .",
  "y": "similarities uses"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_5",
  "x": "**RESULTS** Baselines from <cite>[1]</cite> Our Table 1 : Results of our experiments compared to the results reported in <cite>[1]</cite> . Meta-parameters of our architectures are the following: our CNN had 400 filters of length 1, 100 filters of length 2 and 100 filters of length 3; our LSTM had 200 hidden units and our bidirectional LSTM had 250 hidden units in each network.",
  "y": "uses background"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_6",
  "x": "**CONCLUSION** In this work we achieved a new state-of-the-art results on the next utterance ranking problem recently introduced in <cite>[1]</cite> . The best performing system is an ensemble of multiple diverse neural networks.",
  "y": "extends differences"
 },
 {
  "id": "bb2609c568540390a560757dd40b32_7",
  "x": "On the other hand, the simplicity of the operation does not allow the model to properly handle more complicated dependencies (such as the order in which the n-grams occur in the text), thus recurrent models perform better given enough data; (ii) the recurrent models have not made its peak yet, suggesting that adding more training data would improve the model's accuracy. This agrees with Figure 3 of the previous evaluation <cite>[1]</cite> . Figure 3 : Training data size ranging from 100, 000 to the full 1, 000, 000 examples (X axis) and the resulting Recall@1 (Y axis).",
  "y": "background"
 },
 {
  "id": "bb45a61408a0ade8ce0aba2b8f9ce7_0",
  "x": "We will comprehensively review existing stateof-the-art approaches to selected tasks such as image captioning (Chen et al., 2015) , visual question answering (VQA) (Antol et al., 2015; Goyal et al., 2017) and visual dialog (Das et al., 2017a,b) , presenting the key architectural building blocks (such as co-attention (Lu et al., 2016) ) and novel algorithms (such as cooperative/adversarial games (Das et al., 2017b) ) used to train models for these tasks. We will then discuss some of the current and upcoming challenges of combining language, vision and actions, and introduce some recently-released interactive 3D simulation environments designed for this purpose (Anderson et al., 2018b; <cite>Das et al., 2018)</cite> . The goal of this tutorial is to provide a comprehensive yet accessible overview of existing work and to reduce the entry barrier for new researchers.",
  "y": "background"
 },
 {
  "id": "bb45a61408a0ade8ce0aba2b8f9ce7_1",
  "x": "Although models that link vision, language and actions have a rich history (Tellex et al., 2011; Paul et al., 2016; Misra et al., 2017) , we will focus primarily on embodied 3D environments (Anderson et al., 2018b; , considering tasks such as visual navigation from natural language instructions (Anderson et al., 2018b) , and question answering<cite> (Das et al., 2018</cite>; Gordon et al., 2018) . We will position this work in context of related simulators that also offer significant potential for grounded language learning (Beattie et al., 2016; Zhu et al., 2017) . To finish, we will discuss some of the challenges in developing agents for these tasks, as they need to be able to combine active perception, language grounding, commonsense reasoning and appropriate long-term credit assignment to succeed.",
  "y": "background"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_0",
  "x": "A popular approach for RNN compression is sparsification (setting a lot of weights to zero), it may compress RNN orders of times with only a slight quality drop or even with quality improvement due to the regularization effect [11] . Sparsification of the RNN is usually performed either at the level of individual weights (unstructured sparsification) [13, 11,<cite> 1]</cite> or at the level of neurons [14] (structured sparsification -removing weights by groups corresponding to neurons). The latter additionally accelerates the testing stage.",
  "y": "background"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_1",
  "x": "However, most of the modern recurrent architectures (e. g. LSTM [3] or GRU [2] ) have a gated structure. We propose to add an intermediate level of sparsification between individual weights <cite>[1]</cite> and neurons [14] -gates (see fig. 1 , left). Precisely, we remove weights by groups corresponding to gates, which makes some gates constant, independent of the inputs, and equal to the activation function of the bias.",
  "y": "extends uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_2",
  "x": "We implement the idea for LSTM in two frameworks: pruning [14] and Bayesian sparsification <cite>[1]</cite> and observe that resulting gate structures (which gates are constant and which are not) vary for different NLP tasks. We analyze these gate structures and connect them to the specifics of the particular tasks. The proposed method also improves neuron-wise compression of the RNN in most cases.",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_3",
  "x": "In contrast to our approach, they do not sparsify gates, and remove a neuron if all its ingoing and outgoing connections are set to zero. Bayesian sparsification. We rely on Sparse Variational Dropout [10,<cite> 1]</cite> to sparsify individual weights.",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_5",
  "x": "In the Bayesian framework, we perform an evaluation on the text classification (datasets IMDb [6] and AGNews [17]) and language modeling (dataset PTB, character and word level tasks) following <cite>[1]</cite> . The architecture for the character-level LM is LSTM + FC, for the text classification is Emb + LSTM + FC on the last hidden state, for the word level LM is the same as in pruning. Here we regularize and sparsify all layers following <cite>[1]</cite> .",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_6",
  "x": "In the Bayesian framework, we perform an evaluation on the text classification (datasets IMDb [6] and AGNews [17]) and language modeling (dataset PTB, character and word level tasks) following <cite>[1]</cite> . The architecture for the character-level LM is LSTM + FC, for the text classification is Emb + LSTM + FC on the last hidden state, for the word level LM is the same as in pruning. Here we regularize and sparsify all layers following <cite>[1]</cite> .",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_7",
  "x": "To find the parameters of the approximate posterior distribution and biases, the evidence lower bound (ELBO) is optimized: Because of the log-uniform prior, for the majority of weights, the signal-to-noise ratio m 2 ij /\u03c3 2 ij \u2192 0 and these weights do not affect the network's output. In <cite>[1]</cite> , SparseVD is adapted to the RNNs.",
  "y": "background"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_8",
  "x": "We work with the group weights z in the same way as with the weights W : we approximate the posterior with the fully factorized normal distribution given the fully factorized log-uniform prior distribution. To estimate the expectation in (1), we sample weights from the approximate posterior distribution in the same way as in <cite>[1]</cite> . With the integral estimated with one Monte-Carlo sample, the first term in (1) becomes the usual loss function (for example, cross-entropy in language modeling).",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_9",
  "x": "In all the Bayesian models, we sparsify the weight matrices of all layers. Since in text classification tasks, usually only a small number of input words are important, we use additional multiplicative weights to sparsify the input vocabulary following Chirkova et al. <cite>[1]</cite> . For the networks with the embedding layer, in configurations W+N and W+G+N, we also sparsify the embedding components (by introducing group weights z x multiplied by x t .)",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_10",
  "x": "Baseline networks overfit for all our tasks, therefore, we present results for them with early stopping. Models for the text classification and the character-level LM are trained in the same setting as in <cite>[1]</cite> (we used the code provided by the authors). For the text classification tasks, we use a learning rate equal to 0.0005 and train Bayesian models for 800 / 150 epochs on IMDb / AGNews.",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_12",
  "x": "In this section, we present experimental results for the unstructured Bayesian sparsification (configuration Bayes W). This configuration corresponds to a model of Chirkova et al. <cite>[1]</cite> . Table 2 shows quantitative results, and figure 5 shows the resulting gate structures for the IMDB classification task and the second LSTM layer of the word-level language modeling task.",
  "y": "uses"
 },
 {
  "id": "bb5e6e32d7e507bc6d943719c02902_13",
  "x": "This configuration corresponds to a model of Chirkova et al. <cite>[1]</cite> . Table 2 shows quantitative results, and figure 5 shows the resulting gate structures for the IMDB classification task and the second LSTM layer of the word-level language modeling task. Since Bayes W model does not comprise any group weights, the overall compression of the RNN is lower than for Bayes W+G+N (tab. 1), so there are more non-constant gates.",
  "y": "uses background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_0",
  "x": "If we can achieve this learning goal, the algorithms are able to solve new tasks without teaching. Zhiyuan Chen and etc. <cite>[2]</cite> ever proposed a approach to close the goal.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_1",
  "x": "Guangyi Lv and etc. [4] extend the work of <cite>[2]</cite> with a neural network based approach. However, the supervised learning still is necessary under their setting and huge volume of labeled data are required.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_2",
  "x": "Zhiyuan and etc. <cite>[2]</cite> improved the sentiment classification by involving knowledge. The object function was modified with two penalty terms which corresponding with previous tasks.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_3",
  "x": "---------------------------------- **COMPONENTS OF LML** \u2022 Knowledge Base (KB): The knowledge Base <cite>[2]</cite> mainly used to maintain the previous knowledge.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_4",
  "x": "\u2022 Knowledge Reasoner (KR): The knowledge reasoner is designed to generate new knowledge upon the archived knowledge by logic inference. A strict logic design is required so the most of the LML algorithms lack of the component. \u2022 Knowledge-Base Learner (KBL): The Knowledge-Based Learner <cite>[2]</cite> aims to retrieve and transfer previous knowledge to the current task.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_5",
  "x": "Previous classical paper <cite>[2]</cite> chose the sentiment classification as the learning target because it could be regarded as a large task as well as a group of related sub-tasks in the different domains. Although these sub-tasks are related to each other but a model only trained on a single subtasks is unable to perform well in the rest sub-tasks. This requires the algorithms could know when the knowledge can be used and when can not due to the distribution of each sub-tasks is different.",
  "y": "motivation"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_6",
  "x": "Previous classical paper <cite>[2]</cite> chose the sentiment classification as the learning target because it could be regarded as a large task as well as a group of related sub-tasks in the different domains. Although these sub-tasks are related to each other but a model only trained on a single subtasks is unable to perform well in the rest sub-tasks. This requires the algorithms could know when the knowledge can be used and when can not due to the distribution of each sub-tasks is different.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_7",
  "x": "Although deep learning already is applied in sentiment classification, it still could not leverage past knowledge well. This because the complexity of neural network limits the researches to define and extract knowledge from the data. As the previous work <cite>[2]</cite> , this paper also uses Na\u00efve Bayes as the knowledge can be presented by the probability.",
  "y": "uses"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_8",
  "x": "\"Lifelong Sentiment Classification\" (\"LSC\" for simple below) <cite>[2]</cite> records that which domain does a word have the sentiment orientation. If a word always has sentiment polarity or has significant polarity in current domain, a higher weight will sign to it more than other words. This approach contains a knowledge transfer operation and a knowledge validation operation.",
  "y": "background"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_9",
  "x": "**CONTRIBUTION OF THIS PAPER** Although LSC <cite>[2]</cite> already raised a lifelong approach, it only aims to improve the classification accuracy. It still is under the setting of the supervised learning and also is unable to deliver an explicit knowledge to guild further learning.",
  "y": "motivation"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_10",
  "x": "Although LSC <cite>[2]</cite> already raised a lifelong approach, it only aims to improve the classification accuracy. Based on the LSC, this paper advances the lifelong learning in sentiment classification and have two main contributions: \u2022 A improved lifelong learning paradigm is proposed to solve the sentiment classification problem under unsupervised learning setting with previous knowledge. \u2022 We introduce a novel approach to discover and store the words with sentiment polarity for reuse.",
  "y": "extends"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_11",
  "x": "Hence, to determine the words with polarity is the key to predict the sentiment. Na\u00efve Bayesian (NB) classifier [5] calculates the probability of each word w in a document d and then to predict the sentiment polarity (positive or negative). We use the same formula below as in the LSC <cite>[2]</cite> .",
  "y": "uses"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_12",
  "x": [
   "As we known, not all words have sentimental polarity like \"a\", \"one\" and etc. while some words always have polarity like \"good\", \"hate\", \"excellent\" and so on. Hence, in order to achieve the goal of the lifelong learning. We need to find the words always have sentiment polarity and be careful for those words only shows polarity in specific domains."
  ],
  "y": "motivation"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_13",
  "x": "**LIFELONG SEMI-SUPERVISED LEARNING FOR SENTIMENT CLASSIFICATION** Although LSC <cite>[2]</cite> considered the difference among domains, it still is a typical supervised learning approach. In this paper, we proposed to learn as two stages:",
  "y": "motivation"
 },
 {
  "id": "bb74dd634a8fc5cdb2f4f3294b6bc5_14",
  "x": "**EXPERIMENT 6.1 DATASETS** In the experiment, we use the same datasets as LSC <cite>[2]</cite> used. It contains the reviews from 20 domains crawled from the Amazon.com and each domain has 1,000 reviews (the distribution of positive and negative reviews is imbalanced).",
  "y": "uses"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_0",
  "x": "Approaches to lexical simplification generally follow a standard pipeline consisting of two main steps: generation and ranking. In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (Devlin and Tait, 1998) , learning substitution rules from sentence-aligned parallel corpora of complex-simple texts<cite> (Horn et al., 2014</cite>; Paetzold and Specia, 2017) , and learning word embeddings from a large corpora to obtain similar words of the complex word (Glava\u0161 and\u0160tajner, 2015; Kim et al., 2016; Specia, 2016a, 2017) . In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness.",
  "y": "background"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_1",
  "x": "* This research was conducted while the first author was a Post Doctoral Fellow at the City University of Hong Kong. The ranking step is challenging because the substitution candidates usually have similar meaning to the target word, and thus share similar context features. State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model<cite> (Horn et al., 2014</cite>; Bingel and S\u00f8gaard, 2016; Specia, 2016a, 2017) .",
  "y": "background"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_2",
  "x": "State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model<cite> (Horn et al., 2014</cite>; Bingel and S\u00f8gaard, 2016; Specia, 2016a, 2017) . Moreover, deep architectures are not explored in these models. In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to rank substitution candidates.",
  "y": "motivation"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_3",
  "x": "Following previous works that used supervised machine learning for ranking in lexical simplification<cite> (Horn et al., 2014</cite>; Paetzold and Specia, 2017) , we train the DSSM using the LexMTurk dataset<cite> (Horn et al., 2014)</cite> , which contains 500 instances composed of a sentence, a target word and substitution candidates ranked by simplicity (Paetzold and Specia, 2017) . In order to learn the parameters W t and W s (Figure 1 ) of the DSSM, we use the standard backpropagation algorithm (Rumelhart et al., 1988) . The objective used in this paper follows the pair-wise learning-to-rank paradigm outlined in (Burges et al., 2005) .",
  "y": "uses"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_4",
  "x": "Since both datasets contain instances from the LexMturk dataset<cite> (Horn et al., 2014)</cite> , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 . We finally obtain 429 remaining instances in the BenchLS dataset, and 78 instances in the NNEval dataset, which are used in our evaluation. We adopt the same evaluation metrics featured in Glava\u0161 and\u0160tajner (2015) and<cite> Horn et al. (2014)</cite> : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system.",
  "y": "uses"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_5",
  "x": "We adopt the same evaluation metrics featured in Glava\u0161 and\u0160tajner (2015) and<cite> Horn et al. (2014)</cite> : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system. ---------------------------------- **BASELINES**",
  "y": "uses"
 },
 {
  "id": "bd3663405d2d68f943acc73720b42d_6",
  "x": "with default parameters) for ranking substitution candidates, similar to the method described in<cite> (Horn et al., 2014)</cite> . All the three models employ the n-gram probability features extracted from the SubIMDB corpus (Paetzold and Specia, 2015) , as described in (Paetzold and Specia, 2017) , and are trained using the LexMTurk dataset. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_0",
  "x": "In this paper, we investigate a state-of-the-art paragraph embedding method proposed by<cite> Zhang et al. (2017)</cite> and discover that it cannot reliably tell whether a given sentence occurs in the input paragraph or not. We formulate a sentence content task to probe for this basic linguistic property and find that even a much simpler bag-of-words method has no trouble solving it. This result motivates us to replace the reconstructionbased objective of<cite> Zhang et al. (2017)</cite> with our sentence content probe objective in a semisupervised setting.",
  "y": "motivation"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_1",
  "x": "Methods that embed a paragraph into a single vector have been successfully integrated into many NLP applications, including text classification<cite> (Zhang et al., 2017)</cite> , document retrieval (Le and Mikolov, 2014) , and semantic similarity and relatedness (Dai et al., 2015; Chen, 2017) . However, downstream performance provides little insight into the kinds of linguistic properties that are encoded by these embeddings. Inspired by the growing body of work on sentence-level linguistic probe tasks (Adi et al., 2017; Conneau et al., 2018) , we set out to evaluate a state-of-the-art paragraph embedding method using a probe task to measure how well it encodes the identity of the sentences within a paragraph.",
  "y": "background"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_2",
  "x": "We specifically investigate the paragraph embedding method of<cite> Zhang et al. (2017)</cite> , which consists of a CNN-based encoder-decoder model paired with a reconstruction objective to learn powerful paragraph embeddings that are capable of accurately reconstructing long paragraphs. This model significantly improves downstream classification accuracies, outperforming LSTM-based alternatives (Li et al., 2015) . How well do these embeddings encode whether or not a given sentence appears in the paragraph?",
  "y": "uses"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_4",
  "x": "**PROBING PARAGRAPH EMBEDDINGS FOR SENTENCE CONTENT** In this section, we first fully specify our probe task before comparing the model of<cite> Zhang et al. (2017)</cite> to a simple bag-of-words model. Somewhat surprisingly, the latter substantially outperforms the former despite its relative simplicity.",
  "y": "differences"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_5",
  "x": "---------------------------------- **PROBE EXPERIMENTAL DETAILS** Paragraphs to train our classifiers are extracted from the Hotel Reviews corpus (Li et al., 2015) , which has previously been used for evaluating the quality of paragraph embeddings (Li et al., 2015; <cite>Zhang et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_6",
  "x": "For reconstruction, there is no apparent correlation between BLEU and downstream accuracy; while BLEU increases with the number of epochs, the downstream performance quickly begins to decrease. This result indicates that early stopping based on BLEU is not feasible with reconstruction-based pre-training objectives. With fine-tuning, CNN-SC substantially boosts accuracy and generalization We switch gears Table 4 : CNN-SC outperforms other baseline models that do not use external data, including CNN-R. All baseline models are taken from<cite> Zhang et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_8",
  "x": "---------------------------------- **RELATED WORK** Text embeddings and probe tasks A variety of methods exist for obtaining fixed-length dense vector representations of words (e.g., Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018) , sentences (e.g., Kiros et al., 2015; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018) , and larger bodies of text (e.g., Le and Mikolov, 2014; Dai et al., 2015; Iyyer et al., 2015; Li et al., 2015; Chen, 2017; <cite>Zhang et al., 2017)</cite> To analyze word and sentence embeddings, recent work has studied classification tasks that probe them for various linguistic properties (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017a,b; Conneau et al., 2018; Tenney et al., 2019) .",
  "y": "background"
 },
 {
  "id": "be39cfec0479ace0a7e08508239cb0_9",
  "x": "**ABSTRACT** While paragraph embedding models are remarkably effective for downstream classification tasks, what they learn and encode into a single vector remains opaque. In this paper, we investigate a state-of-the-art paragraph embedding method proposed by<cite> Zhang et al. (2017)</cite> and discover that it cannot reliably tell whether a given sentence occurs in the input paragraph or not.",
  "y": "motivation"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_0",
  "x": "Based on this, we propose the first end-to-end incremental parser that jointly parses at both constituency and discourse levels. Our algorithm builds up on the span-based parser <cite>(Cross and Huang, 2016)</cite> ; it employs the strong generalization power of bi-directional LSTMs, and parses efficiently and robustly with an extremely simple span-based feature set that does not use any tree structure information. We make the following contributions:",
  "y": "extends differences"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_1",
  "x": "Our parser performs discourse parsing in an endto-end manner, which greatly reduces the efforts required in preprocessing the text for segmentation and feature extraction, and, to our best knowledge, is the first end-to-end discourse parser in literature (Section 3). 3. Even though it simultaneously performs constituency parsing, our parser does not use any explicit syntactic feature, nor does it need any binarization of discourse trees, thanks to the powerful span-based framework of<cite> Cross and Huang (2016)</cite> (Section 3). 4. Empirically, our end-to-end parser outperforms the existing pipelined discourse parsing efforts.",
  "y": "extends differences"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_2",
  "x": "---------------------------------- **EXTENDING SPAN-BASED PARSING** As mentioned above, the input sequences are substantially longer than PTB parsing, so we choose linear-time parsing, by adapting a popular greedy constituency parser, the span-based constituency parser of<cite> Cross and Huang (2016)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_3",
  "x": "Notice that in conventional incremental parsing, the stack stores the subtrees Similar to span-based constituency parsing, we alternate between structural (either shift or combine) and label (label X or nolabel) actions in an odd-even fashion. But different from<cite> Cross and Huang (2016)</cite> , after a structural action, we choose to keep the last branching point k, i.e., i Some text and the symbol or scaled k j (mostly for combine, but also trivially for shift). This is because in our parsing mechanism, the discourse relation between two EDUs is actually determined after the previous combine action.",
  "y": "extends differences"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_4",
  "x": "This is because in our parsing mechanism, the discourse relation between two EDUs is actually determined after the previous combine action. We need to keep the splitting point to clearly find the spans of the two EDUs to determine their relations. This midpoint k disappears after a label action; therefore we can use the shape of the last span on the stack (whether it contains the split point, i.e., i xt and the symbol or scaled k j or i Some text and the symbol or scaled j ) to determine the parity of the step and thus no longer need to carry the step z in the state as in<cite> Cross and Huang (2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_5",
  "x": "---------------------------------- **RECURRENT NEURAL MODELS AND TRAINING** The scoring functions in the deductive system (Figure 4 ) are calculated by an underlying neural model, which is similar to the bi-directional LSTM model in<cite> Cross and Huang (2016)</cite> that evaluates based on span boundary features.",
  "y": "similarities"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_6",
  "x": "The span features are then passed into fully connected networks with softmax to calculate the likelihood of performing the corresponding action or marking the corresponding label. We use the \"training with exploration\" strategy (Goldberg and Nivre, 2013) and the dynamic oracle mechanism described in<cite> Cross and Huang (2016)</cite> to make sure the model can handle unseen parsing configurations properly. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_7",
  "x": "We tune the hyperparameters of the neural model on the development set. For most of the hyperparameters we settle with the same values suggested by<cite> Cross and Huang (2016)</cite> . To alleviate the overfitting problem for training on the relative small RST Treebank, we use a dropout of 0.5.",
  "y": "uses similarities"
 },
 {
  "id": "be77eed8430b6492c81ae6535f1dd5_8",
  "x": "We also evaluate the performances on sentence constituency level and discourse level separately. The result is shown in Table 1 . Note that in constituency level, the accuracy is not directly comparable with the accuracy reported in<cite> Cross and Huang (2016)</cite> , since: a) our parser is trained on a much smaller dataset (RST Treebank is about 1/6 of Penn Treebank); b) the parser is trained to optimize the discourse-level accuracy. Table 2 shows that, in the perspective of endto-end discourse parsing, our parser first outperforms the state-of-the-art segmentator of Bach et al. (2012) , and furthermore, in end-to-end parsing, the superiority of our parser is more pronounced comparing to the previously best parser of Hernault et al. (2010) .",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_1",
  "x": "However, methods for measuring and removing such biases remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection <cite>(Bolukbasi et al., 2016)</cite> is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias.",
  "y": "uses"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_2",
  "x": "A common criticism of word embeddings is that they capture undesirable associations in vector space. In addition to gender-appropriate analogies such as king:queen::man:woman, stereotypical analogies such as doctor:nurse::man:woman also hold in SGNS embedding spaces <cite>(Bolukbasi et al., 2016)</cite> . Caliskan et al. (2017) created an association test for word vectors called WEAT, which uses cosine similarity to measure how associated words are with respect to two sets of attribute words (e.g., 'male' vs. 'female').",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_3",
  "x": "We find that contrary to what<cite> Bolukbasi et al. (2016)</cite> suggested, word embeddings should not be normalized before debiasing, as vector length can contain important information (Ethayarajh et al., 2018) . To guarantee unbiasedness, the bias subspace should also be the span -rather than a principal component -of the vectors used to define it. If applied this way, the subspace projection method can be used to provably debias SGNS and GloVe embeddings with respect to the word pairs that define the bias subspace.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_4",
  "x": "In contrast to the supervised method proposed by<cite> Bolukbasi et al. (2016)</cite> for identifying these gender-specific words, we introduce an unsupervised method. Ours is much more effective at preserving gender-appropriate analogies and precluding gender-biased ones. To allow a fair comparison with prior work, our experiments in this paper focus on gender association.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_5",
  "x": "This subspace was defined by the first m principal components for ten gender relation vectors (e.g., man \u2212 woman). Each debiased word vector was thus orthogonal to the gender bias subspace and its projection on the subspace was zero. While this subspace projection method precluded gender-biased analogies from holding in the embedding space,<cite> Bolukbasi et al. (2016)</cite> did not provide any theoretical guarantee that the vectors were unbiased (i.e., equivalent to vectors that would be obtained from training on a gender-agnostic corpus with no reconstruction error).",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_6",
  "x": "---------------------------------- **PROVABLY DEBIASING EMBEDDINGS** Experiments by<cite> Bolukbasi et al. (2016)</cite> found that debiasing word embeddings using the subspace projection method precludes gender-biased analogies from holding.",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_7",
  "x": "Experiments by<cite> Bolukbasi et al. (2016)</cite> found that debiasing word embeddings using the subspace projection method precludes gender-biased analogies from holding. However, as we noted earlier, despite this method being intuitive, there is no theoretical guarantee that the debiased vectors are perfectly unbiased or that the debiasing method works for embedding models other than SGNS. In this section, we prove that for any embedding model that does implicit matrix factorization (e.g., GloVe, SGNS), debiasing embeddings post hoc using the subspace projection method is, under certain conditions, equivalent to training on a perfectly unbiased corpus without reconstruction error.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_8",
  "x": "In conjunction with (2), this implies that This means that if a debiased word w is represented with vector w d instead of w, it is unbiased with respect to S by Definition 1. This implies that the co-occurrence matrix M d that is reconstructed using the debiased word matrix W d is also unbiased with respect to S. The subspace projection method is therefore far more powerful than initially stated in<cite> Bolukbasi et al. (2016)</cite> : not only can it be applied to any embedding model that implicitly does matrix factorization (e.g., GloVe, SGNS), but debiasing word vectors in this way is equivalent to training on a perfectly unbiased corpus when there is no reconstruction error.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_9",
  "x": "However, as shown in Propositions 1 and 2, there are two key theoretical flaws to WEAT that cause it to overestimate the degree of association and ultimately make it an inappropriate metric for word embeddings. The only other metric of note quantifies association as |cos( w, b)| c , where b is the bias subspace and c \u2208 R the \"strictness\" of the measurement <cite>(Bolukbasi et al., 2016)</cite> . For the same reason discussed in Proposition 1, this measure can also overestimate the degree of association.",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_10",
  "x": "Where S is a non-empty set of ordered word pairs (x, y) that define the association, b is the first principal component of { x \u2212 y | (x, y) \u2208 S}. Our metric, the relational inner product association (RIPA), is simply the inner product of a relation vector describing the association and a given word vector in the same embedding space. To use the terminology in<cite> Bolukbasi et al. (2016)</cite> , RIPA is the scalar projection of a word vector onto a onedimensional bias subspace defined by the unit vector b. In their experiments,<cite> Bolukbasi et al. (2016)</cite> defined b as the first principal component for a set of gender difference vectors (e.g., man \u2212 woman).",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_11",
  "x": "This means that for any given word in the vocabulary, we can compare its gender association in the training corpus to its gender association in the embedding space, which should be equal under perfect reconstruction. Words are grouped into three categories with respect to gender: biased, appropriate, and neutral. We create lists of biased and appropriate words using the<cite> Bolukbasi et al. (2016)</cite> lists of gender-biased and gender-appropriate analogies.",
  "y": "uses"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_12",
  "x": "Let g(w; x, y) denote the RIPA of a word w with respect to the gender relation vector defined by word pair (x, y), let\u011d(w; x, y) denote what g(w; x, y) would be under perfect reconstruction for an SGNS embedding model, and let \u2206 g denote the change in absolute gender association from corpus to embedding space. Where S is a set of gender-defining word pairs 1 from<cite> Bolukbasi et al. (2016)</cite> and \u03bb , \u03b1 are the model-specific constants defined in section 5.1, We take the absolute value of each term because the embedding model may make a word more gendered, but in the direction opposite of what is implied in the corpus.",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_13",
  "x": "Where S is a set of gender-defining word pairs 1 from<cite> Bolukbasi et al. (2016)</cite> and \u03bb , \u03b1 are the model-specific constants defined in section 5.1, We take the absolute value of each term because the embedding model may make a word more gendered, but in the direction opposite of what is implied in the corpus. \u03bb \u2190 1 because we expect Figure 1 : Before debiasing words using subspace projection, one needs to identify which words are genderappropriate -to avoid debiasing them.",
  "y": "uses"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_14",
  "x": "\u03bb \u2190 1 because we expect Figure 1 : Before debiasing words using subspace projection, one needs to identify which words are genderappropriate -to avoid debiasing them. The<cite> Bolukbasi et al. (2016)</cite> method of identifying these words is ineffective: it ends up precluding most gender-appropriate analogies (dotted line, left) while preserving most gender-biased analogies (dotted line, right). Our unsupervised method (dashed line) does much better in both respects.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_15",
  "x": "---------------------------------- **DEBIASING WITHOUT SUPERVISION** To use the subspace projection method <cite>(Bolukbasi et al., 2016)</cite> , one must have prior knowledge of which words are gender-appropriate, so that they are not debiased.",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_16",
  "x": "To create an exhaustive list of gender-appropriate words,<cite> Bolukbasi et al. (2016)</cite> started with a small, human-labelled set of words and then trained an SVM to predict more genderappropriate terms in the vocabulary. This bootstrapped list of gender-appropriate words was then left out during debiasing. The way in which<cite> Bolukbasi et al. (2016)</cite> evaluated their method is unorthodox: they tested the ability of their debiased embedding space to generate new analogies.",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_17",
  "x": "To create an exhaustive list of gender-appropriate words,<cite> Bolukbasi et al. (2016)</cite> started with a small, human-labelled set of words and then trained an SVM to predict more genderappropriate terms in the vocabulary. This bootstrapped list of gender-appropriate words was then left out during debiasing. The way in which<cite> Bolukbasi et al. (2016)</cite> evaluated their method is unorthodox: they tested the ability of their debiased embedding space to generate new analogies.",
  "y": "background"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_18",
  "x": "In our debiased embedding space, 94.9% of gender-appropriate analogies with a strength of at least 0.5 are preserved in the embedding space while only 36.7% of gender-biased analogies are. In contrast, the<cite> Bolukbasi et al. (2016)</cite> approach 2 Available at https://github.com/tolga-b/debiaswe preserves only 16.5% of appropriate analogies with a strength of at least 0.5 while preserving 80.0% of biased ones. Recall that we use the same debiasing method as<cite> Bolukbasi et al. (2016)</cite> ; the difference in performance can only be ascribed to how we choose the gender-appropriate words.",
  "y": "differences"
 },
 {
  "id": "bead17ef9512f960461b681a78be4c_19",
  "x": "In our debiased embedding space, 94.9% of gender-appropriate analogies with a strength of at least 0.5 are preserved in the embedding space while only 36.7% of gender-biased analogies are. In contrast, the<cite> Bolukbasi et al. (2016)</cite> approach 2 Available at https://github.com/tolga-b/debiaswe preserves only 16.5% of appropriate analogies with a strength of at least 0.5 while preserving 80.0% of biased ones. Recall that we use the same debiasing method as<cite> Bolukbasi et al. (2016)</cite> ; the difference in performance can only be ascribed to how we choose the gender-appropriate words.",
  "y": "differences uses"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_0",
  "x": "**ABSTRACT** I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) <cite>\"coloreless green ideas\"</cite> subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.",
  "y": "motivation"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_1",
  "x": "In particular, in (Linzen et al., 2016) we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences. <cite>(Gulordava et al., 2018 )</cite> also consider subject-verb agreement, but in a <cite>\"colorless green ideas\"</cite> setting in which content words in naturally occurring sentences are replaced with random words with the same partof-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues. Marvin and Linzen (2018) consider a wider range of syntactic phenomena (subjectverb agreement, reflexive anaphora, negative polarity items) using manually constructed stimuli, allowing for greater coverage and control than in the naturally occurring setting.",
  "y": "differences"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_2",
  "x": "In particular, in (Linzen et al., 2016) we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences. <cite>(Gulordava et al., 2018 )</cite> also consider subject-verb agreement, but in a <cite>\"colorless green ideas\"</cite> setting in which content words in naturally occurring sentences are replaced with random words with the same partof-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues. Marvin and Linzen (2018) consider a wider range of syntactic phenomena (subjectverb agreement, reflexive anaphora, negative polarity items) using manually constructed stimuli, allowing for greater coverage and control than in the naturally occurring setting.",
  "y": "background"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_3",
  "x": "In contrast, (Tang et al., 2018) find that self-attention performs on par with LSTM for syntax sensitive dependencies in the context of machine-translation, and performance on syntactic tasks is correlated with the number of attention heads in multi-head attention. I adapt the evaluation protocol and stimuli of Linzen et al. (2016) , <cite>Gulordava et al. (2018)</cite> and Marvin and Linzen (2018) to the bidirectional setting required by BERT, and evaluate the pretrained BERT models (both the LARGE and the BASE models). Surprisingly (at least to me), the out-of-the-box models (without any task-specific fine-tuning) perform very well on all the syntactic tasks.",
  "y": "extends"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_4",
  "x": "I use the stimuli provided by (Linzen et al., 2016; <cite>Gulordava et al., 2018</cite>; Marvin and Linzen, 2018) , but change the experimental protocol to adapt it to the bidirectional nature of the BERT model. This requires discarding some of the stimuli, as described below. Thus, the numbers are not strictly comparable to those reported in previous work.",
  "y": "extends"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_5",
  "x": "<cite>Gulordava et al. (2018)</cite> also start with existing sentences. However, in order to control for the possibillity of the model learning to rely on \"semantic\" selectional-preferences cues rather than syntactic ones, <cite>they</cite> replace each content word with random words from the same part-ofspeech and inflection. This results in \"<cite>coloreless green ideas</cite>\" nonce sentences.",
  "y": "background"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_6",
  "x": "This differs from Linzen et al. (2016) and <cite>Gulordava et al. (2018)</cite> by considering the entire sentence (excluding the verb) and not just its prefix leading to the verb, and differs from Marvin and Linzen (2018) by conditioning the focus verb on bidirectional context. I use the PyTorch implementation of BERT, with the pre-trained models supplied by Google. 4 I experiment with the bert-large-uncased and bert-base-uncased models.",
  "y": "motivation extends"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_7",
  "x": "4 I experiment with the bert-large-uncased and bert-base-uncased models. Discarded Material The bi-directional setup precludes using using the NPI stimuli of Marvin and Linzen (2018) , in which the minimal pair differs in two words position, which I discard from the evaluation. I also discard the agreement cases involving the verbs is or are in Linzen et al. (2016) and in <cite>Gulordava et al. (2018)</cite> , because some of them are copular construction, in which strong agreement hints can be found also on the object following the verb.",
  "y": "extends differences"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_8",
  "x": "I similarly discard 680 sentences from (Linzen et al., 2016) where the focus verb or its inflection were one of 108 out-ofvocabulary tokens, 6 and 28 sentence-pairs (8 tokens 7 ) from <cite>(Gulordava et al., 2018)</cite> . Limitations The BERT results are not directly comparable to the numbers reported in previous work. Beyond the differences due to bidirectionality and the discarded stimuli, the BERT models are also trained on a different and larger corpus (covering both wikipedia and books).",
  "y": "extends"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_10",
  "x": "6 blames, dislike, inhabit, exclude, revolves, governs, delete, composes, overlap, edits, embrace, compose, undertakes, disagrees, redirect, persist, recognise, rotates, accompanies, attach, undertake, earn, communicates, imagine, contradicts, specialize, accuses, obtain, caters, welcomes, interprets, await, communicate, templates, qualify, reverts, achieve, achieves, govern, restricts, violate, behave, emit, contend, adopt, overlaps, reproduces, rotate, defends, submit, revolve, lend, pertain, disagree, concentrate, detects, endorses, detect, predate, persists, consume, locates, earns, predict, interact, merge, consumes, behaves, locate, predates, enhances, predicts, integrates, inhabits, satisfy, contradict, swear, activate, restrict, satisfies, redirects, excludes, violates, interacts, admires, speculate, blame, drag, qualifies, activates, criticize, assures, welcome, depart, characterizes, defend, obtains, lends, strives, accuse, recognises, characterize, contends, perceive, complain, awaits 7 toss, spills, tosses, affirms, spill, melt, approves, affirm Table 2 : Results on the EN NONCE <cite>(Gulordava et al., 2018)</cite> stimuli. While not strictly comparable, the numbers reported by <cite>Gulordava et al. (2018)</cite> for the LSTM in this condition (on All) is 74.1 \u00b1 1.6. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "bebcad79900e9a4a25020ed0d886b5_11",
  "x": "The BERT models perform remarkably well on all the syntactic test cases. I expected the attentionbased mechanism to fail on these (compared to the LSTM-based models), and am surprised by these results. The <cite>Gulordava et al. (2018)</cite> and Marvin and Linzen (2018) conditions rule out the possibility of overly relying on selectional preference cues or memorizing the wikipedia training data, and suggest real syntactic generalization is taking place.",
  "y": "uses background"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_0",
  "x": "Parses are weighted by their probabilities and combined using an adapted version of<cite> Sagae and Lavie (2006)</cite> . These accuracy gains come with marginal computational costs and are obtained on top of existing parsing techniques such as discriminative reranking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23. On out-of-domain corpora, accuracy is improved by 0.4% on average.",
  "y": "extends"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_1",
  "x": "---------------------------------- **INTRODUCTION** Researchers have proposed many algorithms to combine parses from multiple parsers into one final parse (Henderson and Brill, 1999; Zeman an\u010f Zabokrtsk\u1ef3, 2005;<cite> Sagae and Lavie, 2006</cite>; Nowson and Dale, 2007; Fossum and Knight, 2009; Petrov, 2010; Johnson and Ural, 2010; Huang et al., 2010; McDonald and Nivre, 2011; Shindo et al., 2012; Narayan and Cohen, 2015) .",
  "y": "background"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_2",
  "x": "Our method treats each parse in a single parser's n-best list as a parse from n separate parsers. We then adapt parse combination methods by Henderson and Brill (1999) ,<cite> Sagae and Lavie (2006)</cite> , and Fossum and Knight (2009) to fuse the constituents from the n parses into a single tree. We empirically show that six n-best parsers benefit from parse fusion across six domains, obtaining stateof-the-art results.",
  "y": "extends"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_3",
  "x": "Surprisingly, exploiting n-best trees does not lead to large improvement over combining 1-best trees in their experiments. Our extension takes the n-best trees from a parser as if they are 1-best parses from n parsers, then follows<cite> Sagae and Lavie (2006)</cite> . Parses are weighted by the estimated probabilities from the parser.",
  "y": "uses"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_4",
  "x": "Since score values are normalized, this means that constituents need roughly half the \"score mass\" in order to be included in the chart. Varying the threshold changes the precision/recall balance since a high threshold adds only the most confident constituents to the chart <cite>(Sagae and Lavie, 2006)</cite> . Baselines: Table 2 gives the accuracy of fusion and baselines for BLLIP on the development corpora.",
  "y": "background"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_5",
  "x": "We also compare against model combination using our reimplementation of<cite> Sagae and Lavie (2006)</cite> . For these results, all six parsers were given equal weight. The threshold was set to 0.42 to optimize model combination F 1 on development data (similar to Setting 2 for constituency parsing in<cite> Sagae and Lavie (2006)</cite> ).",
  "y": "uses"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_6",
  "x": "We also compare against model combination using our reimplementation of<cite> Sagae and Lavie (2006)</cite> . For these results, all six parsers were given equal weight. The threshold was set to 0.42 to optimize model combination F 1 on development data (similar to Setting 2 for constituency parsing in<cite> Sagae and Lavie (2006)</cite> ).",
  "y": "similarities"
 },
 {
  "id": "c022b7cf4568e26c7408a835eaafb7_7",
  "x": "We explored two ways to apply fusion when starting from constituency parses: (1) fuse constituents and then convert them to dependencies and (2) convert to dependencies then fuse the dependencies as in<cite> Sagae and Lavie (2006)</cite> . Approach (1) does not provide any benefit (LAS drops between 0.5% and 2.4%). This may result from fusion's artifacts including unusual unary chains or nodes with a large number of children -it is possible that adjusting unary handling and the precision/recall tradeoff may reduce these issues.",
  "y": "similarities uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_0",
  "x": "It also contains 67,965 silver (partially manually annotated) and 120,662 bronze (no manual annotations) instances. Most sentences are between 5 and 15 tokens in length. Since we will compare our results mainly to<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> , we will only employ the gold and silver data.",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_1",
  "x": "In our experiments, we only use the English texts and corresponding DRSs. We use PMB release 2.2.0, which contains gold standard (fully manually annotated) data of which we use 4,597 as train, 682 as dev and 650 as test instances. It also contains 67,965 silver (partially manually annotated) and 120,662 bronze (no manual annotations) instances. Since we will compare our results mainly to<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> , we will only employ the gold and silver data.",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_2",
  "x": "We represent the source and target data in the same way as<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> , who represent the source sentence as a sequence of characters, with a special character indicating uppercase characters. The target DRS is also represented as a sequence of characters, with the exception of DRS operators, thematic roles and DRS variables, which are represented as super characters (Van Noord and Bos, 2017b), i.e. individual tokens. Since the variable names itself are meaningless, the DRS variables are rewritten to a more general representation, using the De Bruijn index (de Bruijn, 1972) .",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_3",
  "x": "---------------------------------- **NEURAL ARCHITECTURE** We employ a recurrent sequence-to-sequence neural network with attention (Bahdanau et al., 2014) and two bi-LSTM layers, similar to the one used by<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_4",
  "x": "All clauses have the same weight in matching, except for REF clauses, which are ignored. An example of the matching procedure is shown in Figure 1 . The produced DRSs go through a strict syntactic and semantic validation process, as described in<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_5",
  "x": "We now compare our best models to previous parsers 4 (Bos, 2015;<cite> Van Noord, Abzianidze, Toral, and Bos, 2018)</cite> and two baseline systems, SPAR and SIM-SPAR. As previously indicated,<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> used a similar sequence-to-sequence model as our current approach, but implemented in OpenNMT and without the linguistic features. Boxer (Bos, 2008 (Bos, , 2015 ) is a DRS parser that uses a statistical CCG parser for syntactic analysis and a compositional semantics based on \u03bb-calculus, followed by pronoun and presupposition resolution.",
  "y": "uses"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_6",
  "x": "Interestingly, when stacking the linguistic features, there is no improvement over only using the lemma of the source words. We now compare our best models to previous parsers 4 (Bos, 2015;<cite> Van Noord, Abzianidze, Toral, and Bos, 2018)</cite> and two baseline systems, SPAR and SIM-SPAR. As previously indicated,<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> used a similar sequence-to-sequence model as our current approach, but implemented in OpenNMT and without the linguistic features.",
  "y": "uses similarities differences"
 },
 {
  "id": "c067711a58722737ef8b7ea987bcf3_7",
  "x": "Our model clearly outperforms the previous systems, even when only using gold standard data. When compared to<cite> Van Noord, Abzianidze, Toral, and Bos (2018)</cite> , retrained with the same data used in our systems, the largest improvement (3.6 and 3.5 for dev and test) comes from switching framework and changing certain parameters such as the optimizer and learning rate. However, the linguistic features are clearly still beneficial when using only gold data (increase of 2.7 and 1.9 for dev and test), and also still help when employing additional silver data (1.1 and 0.3 increase for dev and test, both significant).",
  "y": "differences"
 },
 {
  "id": "c293e9fb8d6382f185a3efeaf0dbf7_0",
  "x": "Different from Ma and Hovy (2016) and<cite> Liu et al. (2018)</cite> , choose a different data split on the POS dataset. Liu et al. (2018) and Hashimoto et al. (2017) use different development sets for chunking. \u2022 Preprocessing.",
  "y": "extends differences"
 },
 {
  "id": "c293e9fb8d6382f185a3efeaf0dbf7_1",
  "x": "However, existing models use different parameter settings, which affects the fair comparison. \u2022 Evaluation. Some literature reports results using mean and standard deviation under different random seeds (Chiu and Nichols, 2016; Peters et al., 2017; <cite>Liu et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "c293e9fb8d6382f185a3efeaf0dbf7_2",
  "x": "Hammerton (2003) was the first to exploit LSTM for sequence labeling. built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (Lample et al., 2016; <cite>Liu et al., 2018)</cite> , GRU (Yang et al., 2016) , and CNN (Chiu and Nichols, 2016; Ma and Hovy, 2016) features. Yang et al. (2017a) proposed a neural reranking model to improve NER models.",
  "y": "background"
 },
 {
  "id": "c293e9fb8d6382f185a3efeaf0dbf7_3",
  "x": "**WORD SEQUENCE REPRESENTATIONS** Similar to character sequences in words, we can model word sequence information through LSTM or CNN structures. LSTM has been widely used in sequence labeling (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; <cite>Liu et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "c293e9fb8d6382f185a3efeaf0dbf7_4",
  "x": "Data. The NER dataset has been standardly split in Tjong Kim Sang and De Meulder (2003 (Toutanova et al., 2003; Santos and Zadrozny, 2014; Ma and Hovy, 2016; <cite>Liu et al., 2018)</cite> , we adopt the standard splits by using sections 0-18 as training set, sections 19-21 as development set and sections 22-24 as test set. No preprocessing is performed on either dataset except for normalizing digits.",
  "y": "differences"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_0",
  "x": "Garg et al. (2018) show that temporal changes of the embeddings can be used to quantify gender and ethnic stereotypes over time, and Zhao et al. (2017) suggest that biases might in fact be amplified by embedding models. Several researchers have also investigated ways to counter stereotypes and biases in learned language models. While the seminal work by Bolukbasi et al. (2016a <cite>Bolukbasi et al. ( , 2016b</cite> concerns the identification and mitigation of gender bias in pretrained word embeddings, Zhao et al. (2018) provide insights into the possibilities of learning embeddings that are gender neutral.",
  "y": "background"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_1",
  "x": "While the seminal work by Bolukbasi et al. (2016a <cite>Bolukbasi et al. ( , 2016b</cite> concerns the identification and mitigation of gender bias in pretrained word embeddings, Zhao et al. (2018) provide insights into the possibilities of learning embeddings that are gender neutral. Bordia and Bowman (2019) outline a way of training a recurrent neural network for word-based language modelling such that the model is gender neutral. Park et al. (2018) discuss different ways of mitigating gender bias, in the context of abusive language detection, ranging from debiasing a model by using the hard debiased word embeddings produced by<cite> Bolukbasi et al. (2016b)</cite> , to manipulating the data prior to training a model by swapping masculine and feminine mentions, and employing transfer learning from a model learned from less biased text.",
  "y": "background"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_2",
  "x": "Gonen and Goldberg (2019) contest the approaches to debiasing word embeddings presented by<cite> Bolukbasi et al. (2016b)</cite> and Zhao et al. (2018) , arguing that while the bias is reduced when measured according to its definition, i.e., dampening the impact of the general gender direction in the vector space, \"the actual effect is mostly hiding the bias, not removing it\". Further, Gonen and Gold-berg (2019) claim that a lot of the supposedly removed bias can be recovered due to the geometry of the vector representation of the gender neutralized words. Our contribution consists of an investigation of the presence of gender bias in pretrained embeddings for Swedish.",
  "y": "background"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_3",
  "x": "Our experiments are therefore tightly tied to a real-world use case where gender bias would have potentially serious ramifications. We also provide further evidence of the inability of the debiasing method proposed by<cite> Bolukbasi et al. (2016b)</cite> to handle the type of bias we are concerned with. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_4",
  "x": "---------------------------------- **DEBIASING EMBEDDINGS** We apply the debiasing methodology in<cite> (Bolukbasi et al., 2016b)</cite> to the pretrained embedddings.",
  "y": "uses similarities"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_5",
  "x": "We use the same methodology for growing a seed set of gender specific words into a larger set as described in<cite> (Bolukbasi et al., 2016b)</cite> , and end up with 486 manually curated gender specific words, including e.g., farfar (paternal grandfather), tvillingsystrar (twin sisters), and matriark (matriarch). The definitional pairs are used to find a gender direction in the embedding space, which is done by taking the difference vector of each of the definitional pairs (i.e. w 1 \u2212 w 2 ), and then factorizing the mean-centered difference vectors using PCA, retaining only the first principal component, which will act as the gender direction. The vector space is then hard debiased 1 in the sense that the gen- der direction b is removed from the embeddings of all non-gender specific words w using orthogonal projection: w = w \u2212 b \u00d7 w\u00b7b b\u00b7b .",
  "y": "similarities uses"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_6",
  "x": "The definitional pairs are used to find a gender direction in the embedding space, which is done by taking the difference vector of each of the definitional pairs (i.e. w 1 \u2212 w 2 ), and then factorizing the mean-centered difference vectors using PCA, retaining only the first principal component, which will act as the gender direction. The vector space is then hard debiased 1 in the sense that the gen- der direction b is removed from the embeddings of all non-gender specific words w using orthogonal projection: w = w \u2212 b \u00d7 w\u00b7b b\u00b7b . The approach described by<cite> (Bolukbasi et al., 2016b)</cite> includes an equalize step to make all gender neutral words equidistant to each of the members of a given equality set of word pairs.",
  "y": "background"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_7",
  "x": "We now turn to investigate the effect the hard debiasing operation has on the embedding spaces, using the intrinsic evaluation methodology of<cite> Bolukbasi et al. (2016b)</cite> . In this setting, a number of analogy pairs are extracted for the original and debiased embeddings, and human evaluators are used to asses the number of appropriate and stereotypical pairs in the respective representations. Bolukbasi et al. (2016b) used 10 crowdworkers to classify the analogy pairs as being appropriate or stereotypical.",
  "y": "uses"
 },
 {
  "id": "c2952b2da147d5f128cdbd5d8074a5_8",
  "x": "The number of stereotypical analogy pairs output by the Swedish models is small compared to the numbers reported by<cite> Bolukbasi et al. (2016b)</cite> . Further, the number of stereotypical pairs is larger in the debiased word2vec model than in the original model (we anticipated that it should be lower). It thus seems as if the debiasing operation makes the word2vec embedding space more biased.",
  "y": "differences"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_0",
  "x": "It builds better classifiers or regressors using both labeled and unlabeled data, under appropriate assumptions (Zhu, 2005; Seeger, 2001 ). This paper contains three contributions: \u2022 We present a novel adaptation of graph-based semi-supervised learning (Zhu et al., 2003) to the sentiment analysis domain, extending past supervised learning work by<cite> Pang and Lee (2005)</cite> ;",
  "y": "extends"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_1",
  "x": "A large w ij implies that the two documents tend to express the same sentiment (i.e., rating). We experiment with positive-sentence percentage (PSP) based similarity which is proposed in <cite>(Pang and Lee, 2005)</cite> , and mutual-information modulated word-vector cosine similarity. Details can be found in section 4.",
  "y": "uses"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_2",
  "x": "Details can be found in section 4. 2. Optionally, we are given numerical rating predictions\u0177 l+1 , . . . ,\u0177 n on the unlabeled documents from a separate learner, for instance -insensitive support vector regression (Joachims, 1999; Smola and Sch\u00f6lkopf, 2004) used by <cite>(Pang and Lee, 2005)</cite> . This acts as an extra knowledge source for our semisupervised learning framework to improve upon.",
  "y": "uses"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_3",
  "x": "2. Optionally, we are given numerical rating predictions\u0177 l+1 , . . . ,\u0177 n on the unlabeled documents from a separate learner, for instance -insensitive support vector regression (Joachims, 1999; Smola and Sch\u00f6lkopf, 2004) used by <cite>(Pang and Lee, 2005)</cite> . This acts as an extra knowledge source for our semisupervised learning framework to improve upon. We note our framework is general and works without the separate learner, too.",
  "y": "motivation"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_4",
  "x": "Before moving on to experiments, we note an interesting connection to the supervised learning method in <cite>(Pang and Lee, 2005)</cite> , which formulates rating inference as a metric labeling problem (Kleinberg and Tardos, 2002) . Consider a special case of our loss function (1) when b = 0 and M \u2192 \u221e. It is easy to show for labeled nodes j \u2208 L, the optimal value is the given label: f (x j ) = y j . Then the optimization problem decouples into a set of onedimensional problems, one for each unlabeled node",
  "y": "similarities"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_5",
  "x": "The above problem is easy to solve. It corresponds exactly to the supervised, non-transductive version of metric labeling, except we use squared difference while <cite>(Pang and Lee, 2005)</cite> used absolute difference. Indeed in experiments comparing the two (not reported here), their differences are not statistically significant.",
  "y": "differences"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_6",
  "x": "We performed experiments using the movie review documents and accompanying 4-class (C = {0, 1, 2, 3}) labels found in the \"scale dataset v1.0\" available at http://www.cs.cornell.edu/people/pabo/ movie-review-data/ and first used in <cite>(Pang and Lee, 2005)</cite> . We chose 4-class instead of 3-class labeling because it is harder. The dataset is divided into four author-specific corpora, containing 1770, 902, 1307, and 1027 documents.",
  "y": "uses"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_8",
  "x": "All reported results are average test set accuracy. We compare our graph-based semi-supervised method with two previously studied methods: regression and metric labeling as in <cite>(Pang and Lee, 2005)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_9",
  "x": "Among others, we experimented with PSP-based similarity. For consistency with <cite>(Pang and Lee, 2005)</cite> , supervised metric labeling results with this measure are reported under 'reg+PSP. ' Note this method does not use unlabeled data for training either.",
  "y": "similarities"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_10",
  "x": "' Note this method does not use unlabeled data for training either. PSP i is defined in <cite>(Pang and Lee, 2005)</cite> as the percentage of positive sentences in review x i . The similarity between reviews x i , x j is the cosine angle Figure 2 : PSP for reviews expressing each fine-grain rating.",
  "y": "uses"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_11",
  "x": "We identified positive sentences using SVM instead of Na\u00efve Bayes, but the trend is qualitatively the same as in <cite>(Pang and Lee, 2005)</cite> . between the vectors (PSP i , 1\u2212PSP i ) and (PSP j , 1\u2212 PSP j ). Positive sentences are identified using a binary classifier trained on a separate \"snippet data set\" located at the same URL as above.",
  "y": "similarities"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_12",
  "x": "Because c is fixed, k varies directly with |L| (i.e., when less labeled data is available, our algorithm considers fewer nearby labeled examples). In an attempt to reproduce the findings in <cite>(Pang and Lee, 2005)</cite> , we tuned c, \u03b1 with cross validation. Tuning ranges are c \u2208 {0.05, 0.1, 0.15, 0.2, 0.25, 0.3} and \u03b1 \u2208 {0.01, 0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0}. The optimal parameters we found are c = 0.2 and \u03b1 = 1.5. (In section 4.4, we discuss an alternative similarity measure, for which we re-tuned these parameters.)",
  "y": "similarities"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_13",
  "x": "Tuning ranges are c \u2208 {0.05, 0.1, 0.15, 0.2, 0.25, 0.3} and \u03b1 \u2208 {0.01, 0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0}. The optimal parameters we found are c = 0.2 and \u03b1 = 1.5. (In section 4.4, we discuss an alternative similarity measure, for which we re-tuned these parameters.) Note that we learned a single set of shared parameters for all authors, whereas <cite>(Pang and Lee, 2005)</cite> tuned k and \u03b1 on a per-author basis. To demonstrate that our implementation of metric labeling produces comparable results, we also determined the optimal author-specific parameters.",
  "y": "differences"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_14",
  "x": "<cite>(Pang and Lee, 2005)</cite> found that their metric labeling method, when applied to the 4-class data we are using, was not statistically better than regression, though they observed some improvement for authors (c) and (d). Using author-specific parameters, we obtained the same qualitative result, but the improvement for (c) and (d) appears even less significant in our results. Possible explanations for this difference are the fact that we derived our PSP measurements using an SVM classifier instead of an NB classifier, and that we did not use the same range of parameters for tuning.",
  "y": "background"
 },
 {
  "id": "c384f48d5f04ea8d63bbbb94a3b24b_15",
  "x": "There are several directions to improve the work: 1. We will investigate better document representations and similarity measures based on parsing and other linguistic knowledge, as well as reviews' sentiment patterns. For example, several positive sentences followed by a few concluding negative sentences could indicate an overall negative review, as observed in prior work <cite>(Pang and Lee, 2005)</cite> . 2.",
  "y": "background"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_0",
  "x": "On the proverb data, the novel features result in compact models that significantly outperform existing features designed for word-level metaphor detection in other genres <cite>(Klebanov et al., 2014)</cite> , such as news and essays. By also testing the new features on these other genres, we show that their generalization power is not limited to proverbs. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_1",
  "x": "Similarly to<cite> Klebanov et al. (2014)</cite> , we classify each content word (i.e., adjective, noun, verb or adverb) appearing in a proverb as being used metaphorically or not. As a baseline, we use a set of features very similar to the one proposed by<cite> Klebanov et al. (2014)</cite> . To obtain results more easily comparable with<cite> Klebanov et al. (2014)</cite>, we use the same classifier, i.e., logistic regression, in the implementation bundled with the scikit-learn package (Pedregosa et al., 2011) .",
  "y": "similarities uses"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_2",
  "x": "**BASELINE FEATURES (B)** Unigrams (u B ):<cite> Klebanov et al. (2014)</cite> use all content word forms as features without stemming or lemmatization. To reduce sparsity, we consider lemmas along with their POS tag.",
  "y": "differences"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_3",
  "x": "Similarly to<cite> Klebanov et al. (2014)</cite> , the mean concreteness ratings, ranging from 1 to 5, are binned in 0.25 increments. We also add a binary feature which encodes the information about whether the lemma is found in the resource. Topic models (t B ): We use Latent Dirichlet Allocation (LDA) (Blei et al., 2003) using Gibbs sampling for parameter estimation and inference (Griffiths, 2002) .",
  "y": "similarities"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_4",
  "x": "This finding can be further confirmed by inspecting the cases where B misclassifies metaphors that are correctly detected by N . Among these, we can find several examples including words that belong to domains often used as a metaphor source, such as \"grist\" (domain: \"gastronomy\") in \"All is grist that comes to the mill\", or \"horse\" (domain: \"animals\") in \"You can take a horse to the water , but you can't make him drink\". Finally, Table 3 shows the effect of the different feature sets on VUAMC used by<cite> Klebanov et al. (2014)</cite> .",
  "y": "background"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_5",
  "x": "Among these, we can find several examples including words that belong to domains often used as a metaphor source, such as \"grist\" (domain: \"gastronomy\") in \"All is grist that comes to the mill\", or \"horse\" (domain: \"animals\") in \"You can take a horse to the water , but you can't make him drink\". Finally, Table 3 shows the effect of the different feature sets on VUAMC used by<cite> Klebanov et al. (2014)</cite> . We use the same 12-fold data split as<cite> Klebanov et al. (2014)</cite> , and also in this case we perform a grid-search to optimize the meta-parameter C of the logistic regression classifier.",
  "y": "similarities uses"
 },
 {
  "id": "c3f6140bd69d1eef0124665e651c0c_6",
  "x": "Besides, the addition of N always leads to more balanced models, by compensating for the relatively lower precision of B. Due to the lack of a separate test set, as in the original setup by<cite> Klebanov et al. (2014)</cite> , and to the high dimensionality of B's lexicalized features, we cannot rule out over-fitting as an explanation for the relatively good performance of B on this benchmark. In particular, our implementation of the B features performs better than reported by<cite> Klebanov et al. (2014)</cite> on all four genres, namely: 0.52 vs. 0.51 for \"news\", 0.51 vs. 0.28 for \"academic\", 0.39 vs. 0.28 for \"conversation\" and 0.42 vs. 0.33 for \"fiction\".",
  "y": "extends differences"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_0",
  "x": "The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald and Satta, 2007) . Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008) , dual decomposition , or multi-commodity flows (Martins et al., 2009<cite> (Martins et al., , 2011</cite> . These are all instances of turbo parsers, as shown by Martins et al. (2010) : the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects.",
  "y": "background"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_1",
  "x": "\u2022 This extension is non-trivial since exact dynamic programming is not applicable. Instead, we adapt AD 3 , the dual decomposition algorithm proposed by<cite> Martins et al. (2011)</cite> , to handle third-order features, by introducing specialized head automata. \u2022 We make our parser substantially faster than the many-components approach of<cite> Martins et al. (2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_2",
  "x": "\u2022 This extension is non-trivial since exact dynamic programming is not applicable. Instead, we adapt AD 3 , the dual decomposition algorithm proposed by<cite> Martins et al. (2011)</cite> , to handle third-order features, by introducing specialized head automata. \u2022 We make our parser substantially faster than the many-components approach of<cite> Martins et al. (2011)</cite> .",
  "y": "differences"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_3",
  "x": "Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in<cite> Martins et al. (2011)</cite> , in addition to third-order features for grand-and tri-siblings . problems in a modular and extensible manner (Komodakis et al., 2007; . In this paper, we employ alternating directions dual decomposition (AD 3 ;<cite> Martins et al., 2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_4",
  "x": "Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in<cite> Martins et al. (2011)</cite> , in addition to third-order features for grand-and tri-siblings . problems in a modular and extensible manner (Komodakis et al., 2007; . In this paper, we employ alternating directions dual decomposition (AD 3 ;<cite> Martins et al., 2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_5",
  "x": "The difference is that the AD 3 subproblems have an additional quadratic term to accelerate consensus. Recent analysis (Martins et al., 2012 ) has shown that: (i) AD 3 converges at a faster rate, 2 and (ii) the quadratic subproblems can be solved using the same combinatorial machinery that is used in the subgradient algorithm. This opens the door for larger subproblems (such as the combination of trees and head automata in instead of a many-components approach <cite>(Martins et al., 2011)</cite> , while still enjoying faster convergence.",
  "y": "differences background motivation"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_6",
  "x": "Let {A s } S s=1 be a cover of A, where each A s \u2286 A. We assume that the score of a parse tree u \u2208 Y decomposes as f (u) := S s=1 f s (z s ), where each z s := z s,a a\u2208As is a \"partial view\" of u, and each local score function f s comes from a feature-based linear model. Past work in dependency parsing considered either (i) a few \"large\" components, such as trees and head automata (Smith and Eisner, 2008; , or (ii) many \"small\" components, coming from a multi-commodity flow formulation (Martins et al., 2009<cite> (Martins et al., , 2011</cite> ). Let Y s \u2286 R |As| denote the set of feasible realizations of z s , i.e., those that are partial views of an actual parse tree.",
  "y": "background"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_7",
  "x": "We assume each parse u \u2208 Y corresponds uniquely to a globally consistent tuple of views, and vice-versa. Following<cite> Martins et al. (2011)</cite> , the problem of obtaining the best-scored tree can be written as follows: where the equality constraint ensures that the partial views \"glue\" together to form a coherent parse tree.",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_8",
  "x": "Its members represent marginal probabilities over the arcs in As. The AD 3 algorithm <cite>(Martins et al., 2011)</cite> alternates among the following iterative updates: \u2022 z-updates, which decouple over s = 1, . . . , S, and solve a penalized version of Eq. 2:",
  "y": "background"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_9",
  "x": "\u2022 \u03bb-updates, where the Lagrange multipliers are adjusted to penalize disagreements: In sum, the only difference between AD 3 and the subgradient method is in the z-updates, which in AD 3 require solving a quadratic problem. While closed-form solutions have been developed for some specialized components <cite>(Martins et al., 2011)</cite> , this problem is in general more difficult than the one arising in the subgradient algorithm.",
  "y": "background motivation"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_10",
  "x": "While closed-form solutions have been developed for some specialized components <cite>(Martins et al., 2011)</cite> , this problem is in general more difficult than the one arising in the subgradient algorithm. However, the following result, proved in Martins et al. (2012) , allows to expand the scope of AD 3 to any problem which satisfies Assumption 1. Proposition 2. The problem in Eq. 3 admits a solution z * s which is spanned by a sparse basis W \u2286 Y s with cardinality at most |W| \u2264 O(|A s |).",
  "y": "extends"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_11",
  "x": "Sequential head bigram model. Head bigrams can be captured with a simple sequence model: Each score \u03c3 HB (m, h, h ) is obtained via features that look at the heads of consecutive words (as in<cite> Martins et al. (2011)</cite> ).",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_12",
  "x": "We handle arbitrary siblings as in<cite> Martins et al. (2011)</cite> , defining O(L 3 ) component functions of the form f ASIB h,m,s (z h,m , z h,s ) = \u03c3 ASIB (h, m, s). In this case, the quadratic problem in Eq. 3 can be solved directly in constant time. Tab.",
  "y": "uses"
 },
 {
  "id": "c4a9b122e8f1b9e98197743c94fea2_13",
  "x": "By looking at the two bottom blocks, we observe that our parser has slightly better accuracies than recent projective parsers, with comparable speed levels (with the exception of the highly optimized vine cascade approach of Rush and Petrov, 2012 Martins et al. (2010<cite> Martins et al. ( , 2011</cite> , , Rush and Petrov (2012) , Zhang and McDonald (2012) . The last two are shown separately in the rightmost columns. In our second experiment (Tab. 3), we used 14 datasets, most of which are non-projective, from the CoNLL 2006 and 2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008) .",
  "y": "similarities"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_0",
  "x": "One prominent line of recent neural dialogue work has continued to build systems with modularly-connected representation, belief state, and generation components (Wen et al., 2016b) . These models must learn to explicitly represent user intent through intermediate supervision, and hence suffer from not being truly end-to-end trainable. Other work stores dialogue context in a memory module and repeatedly queries and reasons about this context to select an adequate system response<cite> (Bordes and Weston, 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_1",
  "x": "However, the present literature lacks results for now standard sequence-to-sequence architectures, and we aim to fill this gap by building increasingly complex models of text generation, starting with a vanilla sequence-to-sequence recurrent architecture. The result is a simple, intuitive, and highly competitive model, which outperforms the more complex model of<cite> Bordes and Weston (2016)</cite> by 6.9%. Our contributions are as follows: 1) We perform a systematic, empirical analysis of increasingly complex sequence-to-sequence models for task-oriented dialogue, and 2) we develop a recurrent neural dialogue architecture augmented with an attention-based copy mechanism that is able to significantly outperform more complex models on a variety of metrics on realistic data.",
  "y": "differences"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_2",
  "x": "These type features improve generalization to novel entities by allowing the model to hone in on positions with particularly relevant bits of dialogue context during its soft attention and copying. Other cited work using the DSTC2 dataset <cite>(Bordes and Weston, 2016</cite>; Liu and Perez, 2016; Seo et al., 2016) implement similar mechanisms whereby they expand the feature representations of candidate system responses based on whether there is lexical entity class matching with provided dialogue context. In these works, such features are referred to as match features.",
  "y": "background"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_3",
  "x": "For our experiments, we used dialogues extracted from the Dialogue State Tracking Challenge 2 (DSTC2) (Henderson et al., 2014) , a restaurant reservation system dataset. While the goal of the original challenge was building a system for inferring dialogue state, for our study, we use the version of the data from<cite> Bordes and Weston (2016)</cite> , which ignores the dialogue state annotations, using only the raw text of the dialogues. The raw text includes user and system utterances as well as the API calls the system would make to the underlying KB in response to the user's queries.",
  "y": "uses"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_4",
  "x": "We employ several metrics for assessing specific aspects of our model, drawn from previous work: \u2022 Per-Response Accuracy:<cite> Bordes and Weston (2016)</cite> report a per-turn response accuracy, which tests their model's ability to select the system response at a certain timestep. Their system does a multiclass classification over a predefined candidate set of responses, which was created by aggregating all system responses seen in the training, validation, and test sets. Our model actually generates each individual token of the response, and we consider a prediction to be correct only if every token of the model output matches the corresponding token in the gold response.",
  "y": "uses"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_5",
  "x": "Their system does a multiclass classification over a predefined candidate set of responses, which was created by aggregating all system responses seen in the training, validation, and test sets. Our model actually generates each individual token of the response, and we consider a prediction to be correct only if every token of the model output matches the corresponding token in the gold response. Evaluating using this metric on our model is therefore significantly more stringent a test than for the model of<cite> Bordes and Weston (2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_6",
  "x": "Evaluating using this metric on our model is therefore significantly more stringent a test than for the model of<cite> Bordes and Weston (2016)</cite> . \u2022 Per-Dialogue Accuracy:<cite> Bordes and Weston (2016)</cite> also report a per-dialogue accuracy, which assesses their model's ability to produce every system response of the dialogue correctly. We calculate a similar value of dialogue accuracy, though again our model generates every token of every response.",
  "y": "similarities differences"
 },
 {
  "id": "c54a1aba5845a52f468cde916c970b_7",
  "x": "**RESULTS** In Table 2 , we present the results of our models compared to the reported performance of the best performing model of<cite> (Bordes and Weston, 2016)</cite> , which is a variant of an end-to-end memory network (Sukhbaatar et al., 2015) . Their model is referred to as MemNN.",
  "y": "motivation"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_0",
  "x": "To address this shortcoming, (Lao et al., 2012) augmented the knowledge graph with paths obtained from an external corpus. The added paths consisted of unlexicalized dependency labels obtained from a dependency parsed external corpus. To improve the expressivity of the added paths, instead of the unlexicalized labels, <cite>(Gardner et al., 2013)</cite> augmented the KB graph with verbs (surface relations) from a corpus containing over 600 million Subject-Verb-Object (SVO) triples.",
  "y": "background"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_1",
  "x": "This reduces feature sparsity and has been shown to improve PRA inference <cite>(Gardner et al., 2013)</cite> , (Gardner et al., 2014) . In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path.",
  "y": "background"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_2",
  "x": "We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in <cite>(Gardner et al., 2013)</cite> and (Gardner et al., 2014) , which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion.",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_3",
  "x": "We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this procedure as On-Demand Augmentation (ODA), because the search can be performed during test time in an on-demand manner. In contrast, the previous approaches of adding edges or embeddings to the KB <cite>(Gardner et al., 2013)</cite> , and vector space random walk PRA (Gardner et al., 2014) are batch procedures.",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_4",
  "x": "As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in <cite>(Gardner et al., 2013</cite>; Gardner et al., 2014) . Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than <cite>(Gardner et al., 2013)</cite> and nearly the same prediction performance as provided by (Gardner et al., 2014) , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature.",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_5",
  "x": "Our experiments suggest that ODA provides better performance than <cite>(Gardner et al., 2013)</cite> and nearly the same prediction performance as provided by (Gardner et al., 2014) , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_6",
  "x": "The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011) . It was extended by (Lao et al., 2012) , to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in <cite>(Gardner et al., 2013)</cite> .",
  "y": "background"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_7",
  "x": "Although, like others, we too use an external corpus to augment the KB, the crucial difference in our approach is that apart from adding surface relations, we also add bridging entities that enable us to create new paths in the KB. Furthermore, the procedure is targeted so that only paths that play a part in inferring the relations that are of interest are added. Thus, the number of paths added in this manner is much lower than the number of surface relations added using the procedure in <cite>(Gardner et al., 2013)</cite> .",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_9",
  "x": "<cite>PRA-SVO</cite> and PRA-VS are the systems proposed in <cite>(Gardner et al., 2013)</cite> and (Gardner et al., 2014) respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner.",
  "y": "background"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_10",
  "x": "<cite>PRA-SVO</cite> and PRA-VS are the systems proposed in <cite>(Gardner et al., 2013)</cite> and (Gardner et al., 2014) respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner.",
  "y": "differences motivation"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_12",
  "x": "Improvements in PRA-ODA over <cite>PRA-SVO</cite> is statistically significant with p < 0.007, with <cite>PRA-SVO</cite> as null hypothesis. classifier. Query Time: The set of target entities corresponding to a source entity and the relation being predicted is not available during query (test) time.",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_14",
  "x": "For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in (<cite>Gardner et al., 2013</cite>; Gardner et al., 2014) , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented. We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (<cite>PRA-SVO</cite>) (<cite>Gardner et al., 2013</cite>) and vector space random walk PRA (PRA-VS) (Gardner et al., 2014) .",
  "y": "uses similarities"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_16",
  "x": "As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8). In addition to the time taken for the full SVO augmentation, PRA-VS takes additional time to generate embeddings (13 minutes) from the added verbs. We note that the batch augmentation in case of <cite>PRA-SVO</cite> and PRA-VS, and embedding computation in case of PRA-VS are all specific to the relations in the evaluation set, and hence can't be ignored as a one-time offline cost.",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_17",
  "x": "We compare the results (PRA-ODA) with the PRA algorithm executed on the NELL KB, NELL KB augmented with surface relations (<cite>PRA-SVO</cite>) (<cite>Gardner et al., 2013</cite>) and vector space random walk PRA (PRA-VS) (Gardner et al., 2014) . The run times, i.e, the time taken to perform an entire experiment for <cite>PRA-SVO</cite> and PRA-VS includes the time taken to augment NELL KB with SVO edges. The PRA-VS runtime also includes the time taken for generating embeddings to perform the vector space random walk. As can be seen from Table 2 and Table 3 , our scheme, PRA-ODA, provides performance equivalent to PRA-VS with faster running time (speed up of 1.8).",
  "y": "differences"
 },
 {
  "id": "c60a1131c6b1639b772b0e5c59588e_18",
  "x": "An additional advantage of the proposed algorithm is that it can also be run on the top of any PRA based algorithm such as the <cite>PRA-SVO</cite> and PRA-VS. ---------------------------------- **CONCLUSION**",
  "y": "differences"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_0",
  "x": "We also compare to the recent biLSTM-Max Encoder of <cite>Conneau et al. (2017)</cite> , which served as our model's 1-layer starting point. 1 The results indicate that 'Our Shortcut-Stacked Encoder' sur-passes all the previous state-of-the-art encoders, and achieves the new best encoding-based result on SNLI, suggesting the general effectiveness of simple shortcut-connected stacked layers in sentence encoders. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_1",
  "x": "We also compare to the recent biLSTM-Max Encoder of <cite>Conneau et al. (2017)</cite> , which served as our model's 1-layer starting point. 1 The results indicate that 'Our Shortcut-Stacked Encoder' sur-passes all the previous state-of-the-art encoders, and achieves the new best encoding-based result on SNLI, suggesting the general effectiveness of simple shortcut-connected stacked layers in sentence encoders. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_2",
  "x": "The overall supervised model uses these shortcutstacked encoders to encode two input sentences into two vectors, and then we use a classifier over the vector combination to label the relationship between these two sentences as that of entailment, contradiction, or neural (similar to the classifier setup of Bowman et al. (2015) and <cite>Conneau et al. (2017)</cite> ). Our simple shortcut-stacked encoders achieve strong improvements over existing encoders due to its multi-layered and shortcutconnected properties, on both matched and mis- matched evaluation settings for multi-domain natural language inference, as well as on the original SNLI dataset. It is the top single-model (nonensemble) result in the EMNLP RepEval 2017 Multi-NLI Shared Task , and the new state-of-the-art for encoding-based results on the SNLI dataset (Bowman et al., 2015) .",
  "y": "similarities uses"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_3",
  "x": "Figure 1 shows the overview of our encoding model (the standard classifier setup is not shown here; see Bowman et al. (2015) and <cite>Conneau et al. (2017)</cite> for that). ---------------------------------- **SENTENCE ENCODER**",
  "y": "background"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_4",
  "x": "We assume w i \u2208 R d is a word embedding vector which are initialized using some pre-trained vector embeddings (and is then fine-tuned end-to-end via the NLI supervision). Then, the input of ith biLSTM layer at time t is defined as: Then, assuming we have m layers of biLSTM, the final vector representation will be obtained by applying row-max-pool over the output of the last biLSTM layer, similar to <cite>Conneau et al. (2017)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_5",
  "x": "The final layer is defined as: , d m is the dimension of the hidden state of the last forward and backward LSTM layers, and v is the final vector representation for the source sentence (which is later fed to the NLI classifier). The closest encoder architecture to ours is that of <cite>Conneau et al. (2017)</cite> , whose model consists of a single-layer biLSTM with a max-pooling layer, which we treat as our starting point.",
  "y": "similarities extends"
 },
 {
  "id": "c6bae8dbdb66092865945e776148e6_7",
  "x": "The dimension size of a biLSTM layer is referring to the dimension of the hidden state for both the forward and backward LSTM-RNNs. As shown, each added layer model improves the accuracy and we achieve a substantial improvement in accuracy (around 2%) on both matched and mismatched settings, compared to the single-layer biLSTM in <cite>Conneau et al. (2017)</cite> . We only experimented with up to 3 layers with 512, 1024, 2048 dimensions each, so the model still has potential to improve the result further with a larger dimension and more layers.",
  "y": "differences"
 },
 {
  "id": "c78464cfe0fc44f5fd0da2e4f9d90e_0",
  "x": "The most immediate answer is the infamously free-form nature of language in social media, encompassing spelling inconsistencies, the free-form adoption of new terms, and regular violations of English grammar norms. Unsurprisingly, when NLP tools are applied directly to social media data, the results tend to be miserable when compared to data sets such as the Wall Street Journal component of the Penn Treebank. However, there have been recent successes in adapting parsers and POS taggers to social media data (Foster et al., 2011; <cite>Gimpel et al., 2011)</cite> .",
  "y": "background"
 },
 {
  "id": "c7e304499654516cce43c550256eae_0",
  "x": "For example, POS tags can be projected via word alignments, and the projected POS is then used to train a model in the lowresource language Zhang et al., 2016;<cite> Fang and Cohn, 2016)</cite> . These methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages. They also assume a large parallel corpus, which may not be available for many low-resource languages.",
  "y": "background"
 },
 {
  "id": "c7e304499654516cce43c550256eae_1",
  "x": "For example, POS tags can be projected via word alignments, and the projected POS is then used to train a model in the lowresource language Zhang et al., 2016;<cite> Fang and Cohn, 2016)</cite> . These methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages. They also assume a large parallel corpus, which may not be available for many low-resource languages.",
  "y": "motivation background"
 },
 {
  "id": "c7e304499654516cce43c550256eae_2",
  "x": "For example, POS tags can be projected via word alignments, and the projected POS is then used to train a model in the lowresource language Zhang et al., 2016;<cite> Fang and Cohn, 2016)</cite> . These methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages. They also assume a large parallel corpus, which may not be available for many low-resource languages. To address these limitations, we propose a new technique for low resource tagging, with more modest resource requirements: 1) a bilingual dictionary; 2) monolingual corpora in the high and low resource languages; and 3) a small annotated corpus of around 1, 000 tokens in the low-resource language.",
  "y": "differences background motivation"
 },
 {
  "id": "c7e304499654516cce43c550256eae_3",
  "x": "Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (Yarowsky and Ngai, 2001; T\u00e4ckstr\u00f6m et al., 2013; <cite>Fang and Cohn, 2016</cite>; Zhang et al., 2016) . Yarowsky and Ngai (2001) pioneered the use of parallel data for projecting POS tag information from one language to another language. used parallel data and exploited graph-based label propagation to expand the coverage of labelled tokens.",
  "y": "background"
 },
 {
  "id": "c7e304499654516cce43c550256eae_4",
  "x": "Our approach extends the work of<cite> Fang and Cohn (2016)</cite> , who present a model based on distant supervision in the form of cross-lingual projection and use projected tags generated from parallel corpora as distant annotations. There are three main differences between their work and ours: 1) We do not use parallel corpora, but instead use a bilingual dictionary for knowledge transfer. 2) Our model uses a more expressive multi-layer perceptron when generating the gold standard tags.",
  "y": "extends"
 },
 {
  "id": "c7e304499654516cce43c550256eae_5",
  "x": "To model this data we employ the same model structure as above but augmented with a second perceptron output layer, as illustrated in Figure 1 (right) . Formally,\u1ef9 t \u223c Categorial(\u00f5 t ) where\u00f5 t = MLP(o t ) is a single hidden layer perceptron with tanh activation and softmax output transformation. This component allows for a more expressive label mapping than<cite> Fang and Cohn (2016)</cite>'s linear matrix translation.",
  "y": "differences"
 },
 {
  "id": "c7e304499654516cce43c550256eae_6",
  "x": "For a more direct comparison, we include BILSTM-DEBIAS<cite> (Fang and Cohn, 2016)</cite> , applied using our proposed cross-lingual supervision based on dictionaries, instead of parallel corpora; accordingly the key difference is their linear transformation for the distant data, versus our non-linear transformation to the gold data. Results Table 1 reports the tagging accuracy, showing that our models consistently outperform the baseline techniques. The poor performance of the supervised methods suggests they are overfitting the small training set, however this is much less of a problem for our approach (labelled Joint).",
  "y": "differences uses"
 },
 {
  "id": "c7e304499654516cce43c550256eae_7",
  "x": "The poor performance of the supervised methods suggests they are overfitting the small training set, however this is much less of a problem for our approach (labelled Joint). Note that distant supervision alone gives reasonable performance (labelled DISTANT) however the joint modelling of the ground truth and distant data yields significant improvements in almost all cases. BILSTM-DEBIAS<cite> (Fang and Cohn, 2016)</cite> performs worse than our proposed method, indicating that a linear transformation is insufficient for modelling distant supervision.",
  "y": "differences"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_0",
  "x": "For the last ten years, many methods have been proposed for the segmentation of texts in topically related units on the basis of lexical cohesion. The major distinction between these methods is in the contrast between the approaches based exclusively on the information contained in the text to be segmented, such as lexical repetition (e.g., <cite>Choi 2000</cite>; Hearst 1997; Heinonen 1998; Kehagias, Pavlina, and Petridis 2003; Utiyama and Isahara 2001) , and those approaches that rest on complementary semantic knowledge extracted from dictionaries and thesauruses (e.g., Kozima 1993; Lin et al. 2004; Morris and Hirst 1991) , or from collocations collected in large corpora (Bolshakov and Gelbukh 2001; Brants, Chen, and Tsochantaridis 2002; Choi et al. 2001; Ferret 2002; Kaufmann 1999; Ponte and Croft 1997) . According to their authors, methods that use additional knowledge allow for a solution to problems encountered when sentences belonging to a unique topic do not share common words due to the use of hyperonyms or synonyms and allow words that are semantically related to be taken as positive evidence for topic continuity.",
  "y": "differences background"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_1",
  "x": "Before reporting these experiments, <cite>Choi's algorithm</cite> and the use of LSA within this framework are described. ---------------------------------- **THE TWO VERSIONS OF <cite>CHOI'S ALGORITHM</cite>**",
  "y": "background"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_3",
  "x": "The segmentation algorithm proposed by <cite>Choi (2000)</cite> is made up of the three steps usually found in any segmentation procedure based on lexical cohesion. Firstly, the document to be segmented is divided into minimal textual units, usually sentences. Then, a similarity index between every pair of adjacent units is calculated.",
  "y": "background"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_4",
  "x": "The procedure initially proposed by <cite>Choi (2000)</cite> , C99, rests exclusively on the information contained in the text to be segmented. According to the vector space model, each sentence is represented by a vector of word frequency count, and the similarity between two sentences is calculated by means of the cosine measure between the corresponding vectors. In a first evaluation based on the procedure described below, <cite>Choi</cite> showed that its algorithm outperforms several other approaches such as TextTiling (Hearst 1997) and Segmenter (Kan, Klavans, and McKeown 1998) .",
  "y": "background"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_5",
  "x": "In a first evaluation based on the procedure described below, <cite>Choi</cite> showed that its algorithm outperforms several other approaches such as TextTiling (Hearst 1997) and Segmenter (Kan, Klavans, and McKeown 1998) . Choi et al. (2001) claimed that it was possible to improve the inter-sentence similarities index by taking into account the semantic proximities between words estimated on the basis of Latent Semantic Analysis (LSA). Briefly stated, LSA rests on the thesis that analyzing the contexts in which words occur permits an estimation of their similarity in meaning (Deerwester et al. 1990; Landauer and Dumais 1997) .",
  "y": "background"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_6",
  "x": "---------------------------------- **METHOD** This experiment was based on the procedure and test materials designed by <cite>Choi (2000)</cite> , which was also used by several authors as a benchmark for comparing segmentation systems (Brants et al. 2002; Ferret 2002; Kehagias et al. 2003; Utiyama and Isahara 2001) .",
  "y": "uses"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_7",
  "x": "Each segment consisted in the first n sentences of a randomly selected text from two sub-sections of the Brown corpus. For the present experiment, I used the most general test materials built by <cite>Choi (2000)</cite> , in which the size of the segments within each sample varies randomly from 3 to 11 sentences. It is composed of 400 samples.",
  "y": "uses"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_8",
  "x": "Concerning the segmentation algorithm, I used the version in which the number of boundaries to be found is imposed, and thus fixed at nine. An 11 \u00d7 11 rank mask was used for the ordinal transformation, as recommended by <cite>Choi (2000)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c856f5ce5d2cdcfc71027d6fa4c6b3_9",
  "x": "The test materials were extracted from the 1997-1998 corpus following the guidelines given in <cite>Choi (2000)</cite> . It is composed of 400 samples of ten segments, of which the length varies randomly from 3 to 11 sentences. Three types of LSA space were composed.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_0",
  "x": "Learning representations (Mikolov et al., 2013) of natural language and language model pre-training <cite>(Devlin et al., 2018</cite>; Radford et al., 2019) has shown promising results recently. These pretrained models serve as generic up-stream models and they can be used to improve down-stream applications such as natural language inference, paraphrasing, named entity recognition, and question answering. The innovation of BERT <cite>(Devlin et al., 2018)</cite> comes from the \"masked language model\" with a pre-training objective, inspired by the Cloze task (Taylor, 1953) .",
  "y": "background"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_1",
  "x": "The innovation of BERT <cite>(Devlin et al., 2018)</cite> comes from the \"masked language model\" with a pre-training objective, inspired by the Cloze task (Taylor, 1953) . The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original token based only on its context. Follow-up work including RoBERTa (Liu et al., 2019b) investigated hyper-parameter design choices and suggested longer model training time.",
  "y": "background"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_2",
  "x": "Nearly all existing work suggests that a large network is crucial to achieve the state-of-the-art performance. For example, <cite>(Devlin et al., 2018)</cite> has shown that across natural language understanding tasks, using larger hidden layer size, more hidden layers, and more attention heads always leads to better performance. However, <cite>they</cite> stop at a hidden layer size of 1024.",
  "y": "background"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_3",
  "x": "Nearly all existing work suggests that a large network is crucial to achieve the state-of-the-art performance. For example, <cite>(Devlin et al., 2018)</cite> has shown that across natural language understanding tasks, using larger hidden layer size, more hidden layers, and more attention heads always leads to better performance. However, <cite>they</cite> stop at a hidden layer size of 1024.",
  "y": "motivation"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_4",
  "x": "On the other hand, model distillation (Hinton et al., 2015; Tang et al., 2019; Sun et al., 2019; Sanh et al., 2019) has been proposed to reduce the BERT model size while maintaining high performance. In this paper, we attempt to improve the performance of BERT via architecture enhancement. BERT is based on the encoder of the transformer model (Vaswani et al., 2017) , which has been proven to obtain state-of-the-art accuracy across a broad range of NLP applications <cite>(Devlin et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_5",
  "x": "2 Related work 2.1 BERT Our work focuses on improving the transformer architecture (Vaswani et al., 2017) , which motivated the recent breakthrough in language representation, BERT <cite>(Devlin et al., 2018)</cite> . Our work builds on top of the transformer architecture, integrating each transformer block with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) . Related to our work, XLNet (Yang et al., 2019) proposes twostream self-attention as opposed to single-stream self-attention used in classic transformers.",
  "y": "motivation"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_6",
  "x": "We used the same multi-head selfattention from the original paper (Vaswani et al., 2017) . We used the same input and output representations, i.e., the embedding and positional encoding, and the same loss objective, i.e., masked LM prediction and next sentence prediction, from the BERT paper <cite>(Devlin et al., 2018)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_7",
  "x": "---------------------------------- **OBJECTIVE FUNCTIONS** Following the BERT <cite>(Devlin et al., 2018)</cite> , we use masked language model loss and next sentence prediction (NSP) loss to train the models.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_8",
  "x": "In our experiments, we randomly mask 15% of all whole word wordpiece tokens in each sequence (Wu et al., 2016) . We also use the next sentence prediction loss as introduced in <cite>(Devlin et al., 2018)</cite> to train our models. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A, and 50% of the time it is a random sentence from the corpus.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_9",
  "x": "**EXPERIMENTAL SETUP** We use the same large-scale data which has been used for BERT model pre-training, the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2.5B words) (Wikipedia contributors, 2004; <cite>Devlin et al., 2018)</cite> . The two corpora consist of about 16GB of text.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_10",
  "x": "We use the same large-scale data which has been used for BERT model pre-training, the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2.5B words) (Wikipedia contributors, 2004; <cite>Devlin et al., 2018)</cite> . The two corpora consist of about 16GB of text. Following the original BERT setup <cite>(Devlin et al., 2018)</cite> , we format the inputs as \"[CLS] TRANS/BERT  108M  12  768  768  12  6.0X  Base  TRANS-BLSTM-SMALL  152M  12  768  768  12  3.3X  TRANS-BLSTM  237M  12  768  768  12  2.5X  Large TRANS/BERT  334M  24  1024  1024  16  2.8X  TRANS-BLSTM-SMALL  487M  24  1024  1024  16  1.4X  TRANS-BLSTM  789M  24  1024  1024  16  1   Table 1 : Parameter size and training speed for TRANS/BERT, TRANS-BLSTM-SMALL, and TRANS-BLSTM on base and large settings respectively.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_11",
  "x": "Similar to <cite>(Devlin et al., 2018)</cite> , the training data generator chooses 15% of the token positions at random for making. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. The model updates use a batch size of 256 and Adam optimizer with learning rate starting from 1e-4.",
  "y": "similarities"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_12",
  "x": "Our TRANS-BLSTM is implemented on top of Pytorch transformer repository 2 . 1 Nevertheless, our implementation of baseline BERT model obtained higher accuracy than that reported by the original BERT paper <cite>(Devlin et al., 2018)</cite> . 2 https://github.com/huggingface/pytorch-transformers.",
  "y": "differences"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_13",
  "x": "**DOWNSTREAM EVALUATION DATASETS** Following the previous work <cite>(Devlin et al., 2018</cite>; Yang et al., 2019; Liu et al., 2019a; Lan et al., 2019) , we evaluate our models on the General Language Understanding Evaluation (GLUE) benchmark and the Stanford Question Answering Dataset (SQuAD 1.1) (Rajpurkar et al., 2016) . GLUE is the General Language Understanding Evaluation benchmark consisting of a diverse collection of natural language understanding tasks.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_14",
  "x": "Table 2 shows the BERT base models, including the original BERT-base model in <cite>(Devlin et al., 2018)</cite> and our implementation, and the bidirectional LSTM model accuracy over SQuAD 1.1 development dataset. Our implementation results in a higher F1 score (90.05%) compared to the original BERT-base one (88.50%). This may be due to the fact that we use the whole word masking while BERT-base used partial word masking (an easier task, which may prevent from learning a better model).",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_16",
  "x": "**MODEL** EM F1 BERT-base <cite>(Devlin et al., 2018)</cite> ----------------------------------",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_17",
  "x": "---------------------------------- **MODEL EVALUATION ON GLUE DATASETS** Following <cite>(Devlin et al., 2018)</cite> , we use a batch size of 32 and 3-epoch fine-tuning over the data for all GLUE tasks.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_18",
  "x": "Additionally similar to <cite>(Devlin et al., 2018)</cite> , for large BERT and TRANS-BLSTM models, we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the development set. Table 5 shows the results of GLUE datasets for original BERT <cite>(Devlin et al., 2018)</cite> , ours TRANS/BERT, TRANS-BLSTM-SMALL and TRANS-BLSTM on base and large settings respectively. Following the BERT setting <cite>(Devlin et al., 2018)</cite> , we exclude the problematic WNLI set.",
  "y": "similarities"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_19",
  "x": "For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the development set. Additionally similar to <cite>(Devlin et al., 2018)</cite> , for large BERT and TRANS-BLSTM models, we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the development set. Table 5 shows the results of GLUE datasets for original BERT <cite>(Devlin et al., 2018)</cite> , ours TRANS/BERT, TRANS-BLSTM-SMALL and TRANS-BLSTM on base and large settings respectively.",
  "y": "uses"
 },
 {
  "id": "caa0ffb1d4e3e5310a28b921333d1e_20",
  "x": "Additionally similar to <cite>(Devlin et al., 2018)</cite> , for large BERT and TRANS-BLSTM models, we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the development set. Table 5 shows the results of GLUE datasets for original BERT <cite>(Devlin et al., 2018)</cite> , ours TRANS/BERT, TRANS-BLSTM-SMALL and TRANS-BLSTM on base and large settings respectively. Following the BERT setting <cite>(Devlin et al., 2018)</cite> , we exclude the problematic WNLI set.",
  "y": "uses"
 },
 {
  "id": "cb57b8886be9ea4f0c50fd2c3a178a_0",
  "x": "**INTRODUCTION** Recently, there has been much work designing ranking architectures to effectively score query-document pairs, with encouraging results [5,<cite> 6,</cite> 20] . Meanwhile, pretrained contextualized language models (such as ELMo [1<cite>6</cite>] and BERT [4] ) have shown great promise on various natural language processing tasks [4, 1<cite>6</cite>] .",
  "y": "background"
 },
 {
  "id": "cb57b8886be9ea4f0c50fd2c3a178a_4",
  "x": "We evaluate our methods on three neural relevance matching methods: PACRR<cite> [6]</cite> , KNRM [20] , and DRMM [5] . Relevance matching models have generally shown to be more effective than semantic matching models, while not requiring massive amounts of behavioral data (e.g., query logs). For PACRR, we increase k max = 30 to allow for more term matches and better back-propagation to the language model.",
  "y": "uses"
 },
 {
  "id": "cb57b8886be9ea4f0c50fd2c3a178a_7",
  "x": "5 Following prior work<cite> [6]</cite> , documents are truncated to 800 tokens. Baselines. We compare contextualized language model performance to the following strong baselines:",
  "y": "uses"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_0",
  "x": "This paper has the intention of fostering synergy between the different research strands. It discusses the different research strands, details the crucial differences, and explores under which circumstances systems can be compared given publicly available data. To that end, we present results with the CoMiC-EN Content Assessment system (Meurers et al., 2011a) on the dataset published by<cite> Mohler et al. (2011)</cite> and outline what was necessary to perform this comparison.",
  "y": "uses background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_1",
  "x": "We will provide an overview of 12 systems and sketch their attributes. Subsequently, we will zoom into the comparison of two of them, namely CoMiC-EN (Meurers et al., 2011a ) and the one which we call the Texas system <cite>(Mohler et al., 2011)</cite> and discuss the issues that arise with this endeavor. Returning to the bigger picture, we will explore how such systems could be compared in general, in the belief that meaningful comparison of approaches across research strands will be an important ingredient in advancing this relatively new research field. 2 The short answer assessment landscape",
  "y": "background motivation"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_2",
  "x": "Another recent approach is described by<cite> Mohler et al. (2011)</cite> , hereafter referred to as the Texas system. Student responses and target responses are annotated using a dependency parser. Thereupon, subgraphs of the dependency structures are constructed in order to map one response to the other. These alignments are generated using machine learning. Dealing with subgraphs allows for variation in word order between the two responses that are to be compared. In order to account for meaning, they combine lexical semantic similarity with the aforementioned alignment. They make use of several WordNet-based measures and two corpus-based measures, namely Latent Semantic Analysis and Explicit Semantic Analysis (ESA, Gabrilovich and Markovitch 2007) .",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_3",
  "x": "For evaluating their system,<cite> Mohler et al. (2011)</cite> collected student responses from an online learning environment. 80 questions from ten introductory computer science assignments spread across two exams were gathered together with 2,273 student responses. These responses were graded by two human judges on a scale from zero to five. The judges fully agreed in 57% of all cases, their Pearson correlation computes to r = 0.586. The gold standard has been created by computing the arithmetic mean of the two judgments for each response. The Texas system achieves r = 0.518 and a Root Mean Square Error of 0.978 as its best result.",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_4",
  "x": [
   "Mohler et al. (2011) mention that \"[t]he dataset is biased towards correct answers\". Data are publicly available. We used these in an evaluation experiment with the CoMiC-EN system, discussed in Section 3."
  ],
  "y": "uses background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_5",
  "x": "After discussing the broad landscape of Short Answer Evaluation systems, the main characteristics and differences, we now turn to a comparison of two concrete systems, namely CoMiC-EN (Meurers et al., 2011a ) and the Texas system<cite> Mohler et al. (2011)</cite> , to explore what is involved in such a concrete comparison of two systems from different contexts. While CoMiC-EN was developed with meaning comparison in mind, the purpose of the Texas system is answer grading. We pick these two systems because they constitute recent and interesting instances of their respective fields and the corresponding data are freely available.",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_6",
  "x": "In evaluating the Texas system,<cite> Mohler et al. (2011)</cite> used a corpus of ten assignments and two exams from an introductory computer science class. In total, the Texas corpus consists of 2,442 responses, which were collected using an online learning platform. Each response is rated by two annotators with a numerical grade on a 0-5 scale. Annotators were not given any specific instructions besides the scale itself, which resulted in an exact agreement of 57.7%. In order to arrive at a gold standard rating, the numerical average of the two ratings was computed. The data exist in raw, sentence-segmented and parsed versions and are freely available for research use. Table 2 presents a breakdown of the score counts and distribution statistics of the Texas corpus.",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_7",
  "x": "A bias towards correct answers can be observed, which is also mentioned by<cite> Mohler et al. (2011)</cite> . Table 2 : Details on the gold standard scores in the Texas corpus. Non-integer scores result from averaging between raters and normalization onto the 0-5 scale.",
  "y": "background motivation"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_8",
  "x": [
   "The dependency graph alignment approach builds on a node-to-node matching stage which computes a score for each possible match between nodes of the student and target response. In the next stage, the optimal graph alignment is computed based on the node-to-node scores using the Hungarian algorithm. Mohler et al. (2011) also employ a technique they call \"question demoting\", which refers to the exclusion of words from the alignment process if they already appeared in the question string."
  ],
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_9",
  "x": "Therefore,<cite> Mohler et al. (2011)</cite> employ isotonic regression to map the ranking to the 0-5 scale. In terms of performance,<cite> Mohler et al. (2011)</cite> report that the SVMRank system produces a better correlation measure (r = 0.518) while the SVR system yields a better RMSE (0.978). ----------------------------------",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_10",
  "x": "For these reasons, to obtain a more interesting comparison, we modified CoMiC-EN to perform scoring instead of meaning comparison. This means that the memory-based learning approach CoMiC-EN had employed so far was no longer applicable and had to be replaced with a regression-capable learning strategy. We chose Support Vector Regression (SVR) using libSVM 4 since that is one of the methods employed by<cite> Mohler et al. (2011)</cite> .",
  "y": "extends"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_11",
  "x": [
   "Having compared the two systems using Pearson correlation and RMSE, it also makes sense to consider the relevance of these evaluation metrics. For example, it is the case that pairwise correlation assumes a normal distribution whereas datasets like the Texas corpus are heavily skewed towards correct answers (see Table 2 ). Mohler et al. (2011) also note that in distributions with zero variance, correlation is undefined, which is not a problem as such but limits the use of correlation as evaluation metric."
  ],
  "y": "background motivation"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_12",
  "x": [
   "Mohler et al. (2011) propose that RMSE is better suited to the task since it captures the relative error a system makes when trying to predict scores. However, RMSE is scale-dependent and thus RMSE values across different studies cannot be compared. We can only suggest that in order to sufficiently describe a system's performance, several metrics need to be reported."
  ],
  "y": "background motivation"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_14",
  "x": "To that end, we gave an overview of the existing systems and picked two for a concrete comparison on the same data, the CoMiC-EN system (Meurers et al., 2011a ) and the Texas system <cite>(Mohler et al., 2011)</cite> . In comparing the two, it was necessary to turn CoMiC-EN into a scoring system because the Texas corpus as the chosen gold standard contains numeric scores assigned by humans. Taking a step back from the concrete comparison, we gave a more general description of what is necessary to compare short answer evaluation systems.",
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_15",
  "x": [
   "To produce the final system score, the Texas system uses two machine learning techniques based on Support Vector Machines (SVMs), SVMRank and Support Vector Regression (SVR). Both techniques are trained with several combinations of the dependency alignment and BOW features. While with SVR one trains a function to produce a score on the 0-5 scale itself, SVMRank produces a ranking of student answers which does not produce a 0-5 grade."
  ],
  "y": "background"
 },
 {
  "id": "cb64ba694c37df9ebc1065a1deac0f_16",
  "x": [
   "As mentioned before, CoMiC-EN performs meaning comparison based on a system of categories while the Texas system is a scoring approach, trying to predict a grade. While the former is a classification task, the latter is better characterized as a regression problem because of the desired numerical outcome. Of course, one could simply pretend that individual grades are classes and treat scoring as a classification task. However, a classification approach has no knowledge of numerical relationships, i.e., it does not 'know' that 4 is a higher grade than 3 and a much higher grade than 1 (assuming a 0-5 scale). As a result, if an evaluation metric such as Pearson correlation is used, classification systems are at a disadvantage because some misclassifications are punished more than others."
  ],
  "y": "background motivation"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_0",
  "x": "Learning to perform these tasks jointly is challenging due to the ambiguity in deciding which records are relevant, the complex dependencies between selected records, and the multiple ways in which these records can be described. Previous work has made significant progress on this task (Chen and Mooney, 2008;<cite> Angeli et al., 2010</cite>; Konstas and Lapata, 2012) . However, most approaches solve the two content selection and surface realization subtasks separately, use manual domain-dependent resources (e.g., semantic parsers) and features, or employ template-based generation.",
  "y": "background"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_1",
  "x": "Learning to perform these tasks jointly is challenging due to the ambiguity in deciding which records are relevant, the complex dependencies between selected records, and the multiple ways in which these records can be described. Previous work has made significant progress on this task (Chen and Mooney, 2008;<cite> Angeli et al., 2010</cite>; Konstas and Lapata, 2012) . However, most approaches solve the two content selection and surface realization subtasks separately, use manual domain-dependent resources (e.g., semantic parsers) and features, or employ template-based generation.",
  "y": "motivation"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_2",
  "x": "This enables our approach to generalize to new domains. Further, our memorybased model captures the long-range contextual dependencies among records and descriptions, which are integral to this task<cite> (Angeli et al., 2010)</cite> . We formulate our model as an encoder-alignerdecoder framework that uses recurrent neural networks with long short-term memory units (LSTMRNNs) (Hochreiter and Schmidhuber, 1997) together with a coarse-to-fine aligner to select and \"translate\" the rich world state into a natural language description.",
  "y": "motivation"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_3",
  "x": "Wong and Mooney (2007) and Lu and Ng (2011) use synchronous context-free grammars to generate natural language sentences from formal meaning representations. Similarly, Belz (2008) employs probabilistic context-free grammars to perform surface realization. Other effective approaches include the use of tree conditional random fields (Lu et al., 2009) and template extraction within a log-linear framework<cite> (Angeli et al., 2010)</cite> .",
  "y": "background"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_4",
  "x": "Chen and Mooney (2008) and Chen et al. (2010) learn alignments between comments and their corresponding event records using a translation model for parsing and generation. implement a two-stage framework that decides what to discuss using a combination of the methods of Lu et al. (2008) and Liang et al. (2009) , and then produces the text based on the generation system of Wong and Mooney (2007) . <cite>Angeli et al. (2010)</cite> propose a unified conceptto-text model that treats joint content selection and surface realization as a sequence of local decisions represented by a log-linear model.",
  "y": "background"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_5",
  "x": "Having trained the model, we generate the natural language description by finding the maximum a posteriori words under the learned model (Eqn. 1). For inference, we perform greedy search starting with the first word x 1 . Beam search offers a way to perform approximate joint inference -however, we empirically found that beam search does not perform any better than greedy search on the datasets that we consider, an observation that is shared with previous work<cite> (Angeli et al., 2010)</cite> .",
  "y": "similarities"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_6",
  "x": "**EXPERIMENTAL SETUP** Datasets We analyze our model on the benchmark WEATHERGOV dataset, and use the data-starved ROBOCUP dataset to demonstrate the model's generalizability. Following <cite>Angeli et al. (2010)</cite> , we use WEATHERGOV training, development, and test splits of size 25000, 1000, and 3528, respectively.",
  "y": "uses"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_7",
  "x": "We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of <cite>Angeli et al. (2010)</cite>), respectively (Sec. 5). Table 1 compares our test results against previous methods that include KL12 (Konstas and Lapata, 2012) , KL13 (Konstas and Lapata, 2013) , and ALK10<cite> (Angeli et al., 2010)</cite> . Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11.94% (F-1), 58.88% (sBLEU), and 36.68% (cBLEU) over the previous state-of-the-art.",
  "y": "uses"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_8",
  "x": "Table 1 compares our test results against previous methods that include KL12 (Konstas and Lapata, 2012) , KL13 (Konstas and Lapata, 2013) , and ALK10<cite> (Angeli et al., 2010)</cite> . Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11.94% (F-1), 58.88% (sBLEU), and 36.68% (cBLEU) over the previous state-of-the-art. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_9",
  "x": "---------------------------------- **BEAM FILTER WITH K-NEAREST NEIGHBORS** We considered beam search as an alternative to greedy search in our primary setup (Eqn. 1), but this performs worse, similar to what previous work found on this dataset<cite> (Angeli et al., 2010)</cite> .",
  "y": "differences"
 },
 {
  "id": "cc66b46b34a0d716414e8b845707f9_10",
  "x": "Table 4 compares the results with and without the encoder on the development set, and demonstrates that there is a significant gain from encoding the event records using the LSTM-RNN. We attribute this improvement to the LSTM-RNN's ability to capture the relationships that exist among the records, which is known to be essential to selective generation (Barzilay and Lapata, 2005;<cite> Angeli et al., 2010)</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_0",
  "x": "RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain specific information or resource. RTMs become the 2nd system out of 13 systems participating in Paraphrase and Semantic Similarity in Twitter, 6th out of 16 submissions in Semantic Textual Similarity Spanish, and 50th out of 73 submissions in Semantic Textual Similarity English. We present positive results from a fully automated judge for semantic similarity based on Referential Translation Machines<cite> (Bi\u00e7ici and Way, 2014b)</cite> in two semantic similarity tasks at SemEval-2015, Semantic Evaluation Exercises -International Workshop on Semantic Evaluation (Nakov et al., 2015).",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_1",
  "x": "RTMs achieve (i) top performance when predicting the quality of translations (Bi\u00e7ici, 2013; Bi\u00e7ici and Way, 2014a) ; (ii) top performance when predicting monolingual cross-level semantic similarity; (iii) second performance when predicting paraphrase and semantic similarity in Twitter (iv) good performance when judging the semantic similarity of sentences; (iv) good performance when evaluating the semantic relatedness of sentences and their entailment<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation. MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation.",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_2",
  "x": "RTMs pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bi\u00e7ici and Yuret, 2015) as interpretants for reaching shared semantics. RTMs achieve (i) top performance when predicting the quality of translations (Bi\u00e7ici, 2013; Bi\u00e7ici and Way, 2014a) ; (ii) top performance when predicting monolingual cross-level semantic similarity; (iii) second performance when predicting paraphrase and semantic similarity in Twitter (iv) good performance when judging the semantic similarity of sentences; (iv) good performance when evaluating the semantic relatedness of sentences and their entailment<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation.",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_3",
  "x": "RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation. MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. MTPP features for translation acts are provided in<cite> (Bi\u00e7ici and Way, 2014b)</cite> .",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_4",
  "x": "**REFERENTIAL TRANSLATION MACHINE (RTM)** We present positive results from a fully automated judge for semantic similarity based on Referential Translation Machines<cite> (Bi\u00e7ici and Way, 2014b)</cite> in two semantic similarity tasks at SemEval-2015, Semantic Evaluation Exercises -International Workshop on Semantic Evaluation (Nakov et al., 2015) . Referential translation machine (RTM) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain.",
  "y": "uses similarities"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_5",
  "x": "RTMs achieve (i) top performance when predicting the quality of translations (Bi\u00e7ici, 2013; Bi\u00e7ici and Way, 2014a) ; (ii) top performance when predicting monolingual cross-level semantic similarity; (iii) second performance when predicting paraphrase and semantic similarity in Twitter (iv) good performance when judging the semantic similarity of sentences; (iv) good performance when evaluating the semantic relatedness of sentences and their entailment<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation. MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation.",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_6",
  "x": "RTMs use Machine Translation Performance Prediction (MTPP) System<cite> Bi\u00e7ici and Way, 2014b)</cite> , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation. MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. MTPP features for translation acts are provided in<cite> (Bi\u00e7ici and Way, 2014b)</cite> .",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_7",
  "x": "MTPP system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. MTPP features for translation acts are provided in<cite> (Bi\u00e7ici and Way, 2014b)</cite> . RTMs become the 2nd system out of 13 systems participating in Paraphrase and Semantic Similarity in Twitter (Task 1) (Xu et al., 2015) and achieve good results in Semantic Tex-",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_8",
  "x": "We optimize the learning parameters, the number of dimensions used for PLS, and the parameters for parallel FDA5. More details about the optimization processes are in <cite>(Bi\u00e7ici and Way, 2014b</cite>; Bi\u00e7ici et al., 2014) . We optimize the learning parameters by selecting \u03b5 close to the standard deviation of the noise in the training set (Bi\u00e7ici, 2013) since the optimal value for \u03b5 is shown to have linear dependence to the noise level for different noise models (Smola et al., 1998) .",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_9",
  "x": "We build separate RTM models for headlines and images domains for STS English. Domain specific RTM models obtain improved performance in those domains<cite> (Bi\u00e7ici and Way, 2014b)</cite> . STS English test set contains 2000, 1500, 2000, 1500, and 1500 sentences respectively from the specified domains however for evaluation, STS use a subset of the test set, 375, 750, 375, 750, and 750 instances respectively from the corresponding domains.",
  "y": "background"
 },
 {
  "id": "cc992a7a918858f9e04b9bb5c15c3f_10",
  "x": "**RTMS ACROSS TASKS AND YEARS** We compare the difficulty of tasks according to MRAER where the correlation of RAE and MRAER is 0.89. In Table 8 , we list the RAE, MAER, and MRAER obtained for different tasks and subtasks, also listing RTM results from SemEval-2013 , from SemEval-2014<cite> (Bi\u00e7ici and Way, 2014b)</cite> , and and from quality estimation task (QET) (Bi\u00e7ici and Way, 2014a ) of machine translation (Bojar et al., 2014) .",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_0",
  "x": "According to (Becker, 1985; Huang, 1985; Gu et al., 1991; Chung, 1993; Kuo, 1995; Fu et al., 1996; Lee et al., 1997; Hsu et al., 1999; Chen et al., 2000; Tsai and Hsu, 2002; Gao et al., 2002; Lee, 2003;<cite> Tsai, 2005)</cite> , the approaches of Chinese input methods (i.e. Chinese input systems) can be classified into two types: (1) keyboard based approach: including phonetic and pinyin based (Chang et al., 1991; Hsu et al., 1993; Hsu, 1994; Hsu et al., 1999; Kuo, 1995; Lua and Gan, 1992) , arbitrary codes based (Fan et al., 1988) and structure scheme based (Huang, 1985) ; and (2) non-keyboard based approach: including optical character recognition (OCR) (Chung, 1993) , online handwriting and speech recognition (Fu et al., 1996; Chen et al., 2000) . Currently, the most popular Chinese input system is phonetic and pinyin based approach, because Chinese people are taught to write phonetic and pinyin syllables of each Chinese character in primary school. In Chinese, each Chinese word can be a mono-syllabic word, such as \"\u9f20(mouse)\", a bisyllabic word, such as \"\u888b\u9f20(kangaroo)\", or a multi-syllabic word, such as \"\u7c73\uf934\u9f20(Mickey mouse).\" The corresponding phonetic and pinyin syllables of each Chinese word is called syllable-words, such as \"dai4 shu3\" is the pinyin syllable-word of \"\u888b\u9f20(kangaroo).\" According to our computation, the {minimum, maximum, average} words per each distinct mono-syllableword and poly-syllable-word (including bisyllable-word and multi-syllable-word) in the CKIP dictionary (Chinese Knowledge Information Processing Group, 1995) are {1, 28, 2.8} and {1, 7, 1.1}, respectively.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_1",
  "x": "The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP). Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (Hsu, 1994; Hsu et al., 1999; Tsai and Hsu, 2002; Gao et al., 2002; Microsoft Research Center in Beijing;<cite> Tsai, 2005)</cite> are addressed on STW conversion. On the other hand, STW conversion is the main task of Chinese Language Processing in typical Chinese speech recognition systems (Fu et al., 1996; Lee et al., 1993; Chien et al., 1993; Su et al., 1992) .",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_2",
  "x": "On the other hand, STW conversion is the main task of Chinese Language Processing in typical Chinese speech recognition systems (Fu et al., 1996; Lee et al., 1993; Chien et al., 1993; Su et al., 1992) . As per (Chung, 1993; Fong and Chung, 1994; Tsai and Hsu, 2002; Gao et al., 2002; Lee, 2003;<cite> Tsai, 2005)</cite> , homophone selection and syllableword segmentation are two critical problems in developing a Chinese input system. Incorrect homophone selection and syllable-word seg-mentation will directly influence the STW conversion accuracy.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_3",
  "x": "From the studies (Hsu 1994; Tsai and Hsu, 2002; Gao et al., 2002; Kee, 2003;<cite> Tsai, 2005)</cite> , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake. The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems. In our previous work<cite> (Tsai, 2005)</cite> , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_4",
  "x": "From the studies (Hsu 1994; Tsai and Hsu, 2002; Gao et al., 2002; Kee, 2003;<cite> Tsai, 2005)</cite> , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake. The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems. In our previous work<cite> (Tsai, 2005)</cite> , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_5",
  "x": "The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems. In our previous work<cite> (Tsai, 2005)</cite> , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively. In<cite> (Tsai, 2005)</cite> , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_6",
  "x": "Since the identified character ratio of the WP identifier <cite>(Tsai, 2005</cite> ) is about 55%, there are still about 15% improving room left. The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database. We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method Editor 2003, MSIME) , and an optimized bigram model, BiGram<cite> (Tsai, 2005)</cite> , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier.",
  "y": "motivation"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_7",
  "x": "Since the identified character ratio of the WP identifier <cite>(Tsai, 2005</cite> ) is about 55%, there are still about 15% improving room left. The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database. We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method Editor 2003, MSIME) , and an optimized bigram model, BiGram<cite> (Tsai, 2005)</cite> , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier.",
  "y": "uses"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_8",
  "x": "---------------------------------- **AUTO-GENERATION OF WP DATABASE** Following<cite> (Tsai, 2005)</cite> , the three steps of autogenerating word-pairs (AUTO-WP) for a given Chinese sentence are as below: (the details of AUTO-WP can be found in<cite> (Tsai, 2005))</cite> Step 1. Get forward and backward word segmentations: Generate two types of word segmentations for a given Chinese sentence by forward maximum matching (FMM) and backward maximum matching (BMM) techniques (Chen et al., 1986; Tsai et al., 2004) with the system dictionary.",
  "y": "uses"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_9",
  "x": "The purpose of this experiment is to demonstrate the tonal and toneless STW accuracies among the identified words by using the WSM with the system WP database. The comparative system is the WP identifier<cite> (Tsai, 2005)</cite> . Table  2 is the experimental results.",
  "y": "uses"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_10",
  "x": "In addition, following<cite> (Tsai, 2005)</cite> , an optimized bigram model called BiGram was developed. The BiGram STW system is a bigrambased model developing by SRILM (Stolcke, 2002) with Good-Turing back-off smoothing (Manning and Schuetze, 1999) , as well as forward and backward longest syllable-word first strategies (Chen et al., 1986; Tsai et al., 2004) . The system dictionary of the BiGram is same with that of the WP identifier and the WSM.",
  "y": "uses"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_11",
  "x": "From Table 3b , the tonal and toneless STW improvements of the BiGram by using the WP identifier and the WSM are (8.6%, 11.9%) and (17.1%, 22.0%), respectively. (Note that, as per<cite> (Tsai, 2005)</cite> , the differences between the tonal and toneless STW accuracies of the BiGram and the TriGram are less than 0.3%). Table 3c is the results of the MSIME and the BiGram by using the WSM as an adaptation processing with both system and user WP database.",
  "y": "background"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_12",
  "x": "This observation is similarly with that of our previous work<cite> (Tsai, 2005)</cite> . (2) The major problem of error conversions in tonal and toneless STW systems is different. This observation is similarly with that of<cite> (Tsai, 2005)</cite> .",
  "y": "similarities"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_13",
  "x": "This observation is similarly with that of our previous work<cite> (Tsai, 2005)</cite> . (2) The major problem of error conversions in tonal and toneless STW systems is different. This observation is similarly with that of<cite> (Tsai, 2005)</cite> .",
  "y": "similarities"
 },
 {
  "id": "cce566b9111abdc7ab7576662922dd_14",
  "x": "In this paper, we present a word support model (WSM) to improve the WP identifier<cite> (Tsai, 2005)</cite> and support the Chinese Language Processing on the STW conversion problem. All of the WP data can be generated fully automatically by applying the AUTO-WP on the given corpus. We are encouraged by the fact that the WSM with WP knowledge is able to achieve state-of-the-art tonal and toneless STW accuracies of 99% and 92%, respectively, for the identified poly-syllabic words.",
  "y": "extends"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_0",
  "x": "With the phoneme mapping from a foreign language phoneme set to the recognizer's phoneme set, foreign words are modeled as a phoneme-level contextual FST for biasing. It is unclear whether such an approach can be directly applied to E2E models. Phoneme-only E2E systems have been shown to have inferior performance compared to grapheme or wordpiece models (WPM) in general <cite>[16,</cite> 17] , but shows better recognition of rare words and proper nouns.",
  "y": "background"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_1",
  "x": "In this work we propose to incorporate phonemes to a wordpiece E2E model as modeling units and use phoneme-level FST for contextual biasing. We propose a word-frequency based sampling strategy to randomly tokenize rare words into phonemes in the target sequence using a lexicon. This approach also mitigates accuracy regressions that have been observed when using phoneme-only E2E models <cite>[16,</cite> 17] .",
  "y": "motivation"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_2",
  "x": "The output of the model is a single softmax whose symbol set is the union of wordpiece and phoneme symbols. We use a pronunciation lexicon to obtain phoneme sequences of words. Since phonemes show strength in recognizing rare words<cite> [16]</cite> , we want to present these words as phonemes more often.",
  "y": "motivation"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_3",
  "x": "For words that appear more than T times, the more frequent they are, the less likely they are presented as phonemes 2 . Note that the decision of whether to use wordpieces or phonemes is made randomly at each gradient iteration, and thus a given sentence could have different target sequences at different epochs. We use context-independent phonemes as in<cite> [16]</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_4",
  "x": "---------------------------------- **DECODING GRAPH** To generate words as outputs, we search through a decoding graph similar to<cite> [16]</cite> but accept both phonemes and wordpieces.",
  "y": "similarities differences"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_5",
  "x": "Based on<cite> [16]</cite> , we add two improvements to the decoding strategy. First, during decoding we consume as many input epsilon arcs as possible thus guaranteeing that all wordpieces in word are produced when all corresponding phonemes are seen in the input. Second, we merge paths that have the same output symbols.",
  "y": "extends"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_6",
  "x": "The WER reductions range from 9%-23% relatively for different models when compared to the no-bias case. Comparing different biasing strategies, we find that the wordpiece-phoneme model performs the best: 16% relatively better than the grapheme model, and 8.3% better than the wordpiece model. We attribute the superior per- formance of the wordpiece-phoneme model to the robustness of phonemes to OOV words, as observed in<cite> [16]</cite> .",
  "y": "similarities"
 },
 {
  "id": "cd10d509dacd8f55993396258eb92a_7",
  "x": "However, we note that the regression is significantly smaller than the all-phoneme model in<cite> [16]</cite> . ---------------------------------- **EFFECT OF NUMBER OF BIASING WORDS**",
  "y": "differences"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_0",
  "x": "Semantic parsing is a task of transducing natural language to meaning representations, which in turn can be expressed through many different semantic formalisms including lambda calculus (Zettlemoyer and Collins, 2012) , DCS (Liang et al., 2013) , Discourse Representation Theory (DRT) (Kamp and Reyle, 2013) , AMR (Banarescu et al., 2013) and so on. This availability of annotated data in English has translated into the development of a plethora of models, including encoder-decoders (Dong and Lapata, 2016; Jia and Liang, 2016) as well as tree or graph-structured decoders Lapata, 2016, 2018;<cite> Liu et al., 2018</cite>; Yin and Neubig, 2017) . Whereas the majority of semantic banks focus on English, recent effort has focussed on *Work done when Jingfeng Yang was an intern and Federico Fancellu a post-doc at the University of Edinburgh building multilingual representations, e.g. PMB (Abzianidze et al., 2017) , MRS (Copestake et al., 1995) and FrameNet (Pad\u00f3 and Lapata, 2005) .",
  "y": "background"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_1",
  "x": "DRS can be linked to each other via logic operator (e.g. \u00ac, \u2192, \u22c4) or, as in this case, discourse relations (e.g. CONTINUATION, RESULT, ELABORA-TION, etc.). To test our approach we leverage the DRT parser of <cite>Liu et al. (2018)</cite> , an encoder-decoder architecture where the meaning representation is reconstructed in three stages, coarse-to-fine, by first building the DRS skeleton (i.e. the 'box' structures) and then fill each DRS with predicates and variables. Whereas the original parser utilizes a sequential Bi-LSTM encoder with monolingual lexical features, we experiment with languageindependent features in the form of cross-lingual word-embeddings, universal PoS tags and universal dependencies.",
  "y": "extends"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_2",
  "x": "**MODEL** In this section, we describe the modifications to the coarse-to-fine encoder-decoder architecture of <cite>Liu et al. (2018)</cite> ; for more detail, we refer the reader to the original paper. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_3",
  "x": "**ENCODER** BiLSTM. We use <cite>Liu et al. (2018)</cite> 's Bi-LSTM as baseline.",
  "y": "uses"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_4",
  "x": "---------------------------------- **DECODER** The decoder of <cite>Liu et al. (2018)</cite> reconstructs the DRS in three steps, by first predicting the overall structure (the 'boxes'), then the predicates and finally the referents, with each subsequent step being conditioned on the output of the previous.",
  "y": "background"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_5",
  "x": "**DATA** We use the PMB v. In order to be used as input to the parser, <cite>Liu et al. (2018)</cite> first convert the DRS into treebased representations, which are subsequently linearized into PTB-style bracketed sequences. This transformation is lossless in that re-entrancies are duplicated to fit in the tree structure.",
  "y": "background"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_6",
  "x": "For further details about Counter, the reader is referred to Van Noord et al. (2018) . It is worth reminding that unlike other work on the PMB (e.g. van Noord et al., 2018), <cite>Liu et al. (2018)</cite> does not deal with presupposition. In the PMB, presupposed variables are extracted from a main box and included in a separate one.",
  "y": "background"
 },
 {
  "id": "cd56849805cdb43bba567f74b31b87_7",
  "x": "Amongst these, tree or graph-structured decoders have recently shown to be state-of-the-art Lapata, 2016, 2018;<cite> Liu et al., 2018</cite>; Cheng et al., 2017; Yin and Neubig, 2017) . ---------------------------------- **CONCLUSIONS**",
  "y": "background"
 },
 {
  "id": "ce0441b3ae7b957520d329799f8b9f_0",
  "x": "Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; <cite>Meyers et al., 2004)</cite> . It is my view that these advantages outweigh the disadvantages.",
  "y": "background"
 },
 {
  "id": "ce0441b3ae7b957520d329799f8b9f_1",
  "x": "It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004), English (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; <cite>Meyers et al., 2004)</cite> , Czech (Sgall et al., 2004) and German (Baumann et al., 2004).",
  "y": "background"
 },
 {
  "id": "ce0441b3ae7b957520d329799f8b9f_2",
  "x": "Thus a system that uses one PennTreebank-based parser as a component can easily be adapted to use another better performing PennTreebank-based parser. Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; <cite>Meyers et al., 2004)</cite> .",
  "y": "background"
 },
 {
  "id": "ce0441b3ae7b957520d329799f8b9f_3",
  "x": "The first six papers describe linguistic annotation in four languages: Spanish (Alc\u00e1ntara and Moreno, 2004) , English (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; <cite>Meyers et al., 2004)</cite> , Czech (Sgall et al., 2004) and German (Baumann et al., 2004) . The sixth, seventh and eighth papers (Baumann et al., 2004; \u00c7 mejrek et al., 2004; Helmreich et al., 2004) explore questions of multilingual annotation of syntax and semantics, beginning to answer the question of how annotation systems can be made compatible across languages. Indeed (Helmreich et al., 2004) explores the question of integration across languages, as well as levels of annotation.",
  "y": "background"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_0",
  "x": "Recent work by <cite>Lau et al. (2018)</cite> proposes a quatrain generation method that relies on specific domain knowledge about the dataset to train a classifier for learning the notion of rhyming: that a line ending word always rhymes with exactly one more ending word in the poem. This limits the applicability of their method to other forms of poetry with different rhyming patterns. They train the classifier along with a language model in a multi-task setup.",
  "y": "background"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_1",
  "x": "This final convolution is naturally biased to identify spatial patterns across word comparisons, which, in turn, biases learned word comparisons to pick up rhyming since rhymes are typically the most salient spatial patterns. Recent work by <cite>Lau et al. (2018)</cite> proposes a quatrain generation method that relies on specific domain knowledge about the dataset to train a classifier for learning the notion of rhyming: that a line ending word always rhymes with exactly one more ending word in the poem. This limits the applicability of their method to other forms of poetry with different rhyming patterns.",
  "y": "motivation"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_2",
  "x": "Recent work by <cite>Lau et al. (2018)</cite> proposes a quatrain generation method that relies on specific domain knowledge about the dataset to train a classifier for learning the notion of rhyming: that a line ending word always rhymes with exactly one more ending word in the poem. They train the classifier along with a language model in a multi-task setup. In contrast, we find that generators trained using our structured adversary produce samples that satisfy rhyming constraints with much higher frequency.",
  "y": "differences"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_3",
  "x": "Following prior work<cite> (Lau et al., 2018)</cite>, we generate words in each line in reverse order (i.e. right to left), and begin generation with the last line first. Letx represent a sample from the current generator distribution, denoted by p \u03b8 , where \u03b8 represents the generator parameters. We initialize the word embeddings in the generator with pre-trained word embeddings<cite> (Lau et al., 2018)</cite> trained on a separate non-sonnet corpus.",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_4",
  "x": "We initialize the word embeddings in the generator with pre-trained word embeddings<cite> (Lau et al., 2018)</cite> trained on a separate non-sonnet corpus. ---------------------------------- **STRUCTURED DISCRIMINATOR**",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_5",
  "x": "**DATASETS** We work with the Shakespeare SONNET dataset<cite> (Lau et al., 2018</cite> ) and a new LIMERICK corpus. Each sonnet in the Sonnet dataset is made up of 3 quatrains of 4 lines each, and a couplet.",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_6",
  "x": "Fol-lowing prior work<cite> (Lau et al., 2018)</cite> , words are sampled with a temperature value between 0.6 and 0.8. We use CMU dictionary (Weide, 1998) to look up the phonetic representation of a word, and extract the sequence of phonemes from the last stressed syllable onward. Two words are considered to be rhyming if their extracted sequences match (Parrish, 2015) .",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_7",
  "x": "The NLL for RHYME-LM and RHYME-GAN are very similar, though RHYME-GAN gets much better sampling efficiency scores than RHYME-LM. Our generator implementation is largely based on that of <cite>Lau et al. (2018)</cite> . The main difference is that we first generate all the line-ending words and then condition on them to generate the remaining words.",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_8",
  "x": "Our generator implementation is largely based on that of <cite>Lau et al. (2018)</cite> . The main difference is that we first generate all the line-ending words and then condition on them to generate the remaining words. The change was made to make it more amenable to our proposed discriminator.",
  "y": "extends"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_9",
  "x": "Following prior work<cite> (Lau et al., 2018)</cite> , we requested human annotators to identify the humanwritten poem when presented with two samples at a time -a quatrain from the Sonnet corpus and a machine-generated quatrain, and report the annotator accuracy on this task. Note that a lower accuracy value is favorable as it signifies higher quality of machine-generated samples. Using 150 valid samples (i.e. samples belonging to one of the allowed rhyming patterns), we observe 56.0% annotator accuracy for RHYME-GAN, and 53.3% for DEEP-SPEARE -indicating that the post-rejection sampling outputs from the two methods are of comparable quality (the difference in annotator accuracy is not statistically significant as per McNemar's test under p < 0.05).",
  "y": "uses"
 },
 {
  "id": "ce8997b630e9544b0f5812be319a59_10",
  "x": "More recently, neural models for poetry generation have been proposed (Zhang and Lapata, 2014; Ghazvininejad et al., 2016 Ghazvininejad et al., , 2017 Hopkins and Kiela, 2017;<cite> Lau et al., 2018</cite>; Liu et al., 2019) . Yan et al. (2013) retrieve high ranking sentences for a given user query, and repeatedly swap words to satisfy poetry constraints. Ghazvininejad et al. (2018) worked on poetry translation using an unconstrained machine translation model and separately learned Finite State Automata for enforcing rhythm and rhyme.",
  "y": "background"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_0",
  "x": "Our paper expands the existing FOIL dataset <cite>(Shekhar et al., 2017)</cite> . FOIL consists of a set of images matched with captions containing one single mistake. The mistakes are always nouns referring to objects not actually present in the image.",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_1",
  "x": "---------------------------------- **THE FOIL METHODOLOGY** We follow the methodology highlighted in<cite> Shekhar et al. (2017)</cite> , which consists of replacing a single word in a human-generated caption with a 'foil' item, making the caption unsuitable to describe the original image.",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_2",
  "x": "For T3, we regress over all<cite> Shekhar et al. (2017)</cite> . the target words on the position of the foil word and select the one which generates the caption with the highest probability to be \"good\". Due to the generative nature of IC models, adapting IC-Wang for the classification purpose is less straightforward.",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_3",
  "x": "**DATASET CREATION** Following<cite> Shekhar et al. (2017)</cite> , we aim at creating a dataset of images associated with both correct and foil captions, where the latter are obtained by replacing one word in the original text. Expanding on the original paper, our target/foil pairs do not merely consist of nouns.",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_4",
  "x": "Following<cite> Shekhar et al. (2017)</cite> , we aim at creating a dataset of images associated with both correct and foil captions, where the latter are obtained by replacing one word in the original text. Expanding on the original paper, our target/foil pairs do not merely consist of nouns. The introduced error can also be an adjective (an object's attribute), a verb (an action), a preposition (a relation between objects) or an adverb (a manner of action).",
  "y": "extends"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_5",
  "x": "In order to obtain a balanced dataset across the various PoS, we only use a subset of the FOIL-COCO dataset of<cite> Shekhar et al. (2017)</cite> . From the FOIL dataset, we retain the 37,536 images for which foil captions could be generated, using the target/foil pairs extracted from the resources mentioned above. Of the FOIL datapoints generated for the noun pairs, only those containing images used for the other PoS are selected.",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_6",
  "x": "**FOIL CAPTION GENERATION** From the word pair lists above, foil captions are generated from MS-COCO original captions. The foil captions are generated by replacing nouns are directly extracted from the FOIL dataset by<cite> Shekhar et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_7",
  "x": "These numbers, however, do not show to which extent the models are able to avoid the trap of the dataset:<cite> Shekhar et al. (2017)</cite> showed that on the FOIL data, models tend to detect correct captions with reasonable accuracy but fail to identify the incorrect ones, leading to a large bias in classification. Taking this insight into account, for the rest of this paper, we focus on the accuracy of the systems in dealing with foil captions, across all three tasks. As shown in Table 3 , the blind model's accuracy is still reasonable on T1, but lower than chance for nouns and adverbs.",
  "y": "uses"
 },
 {
  "id": "cee22bd0384d3d3fd4e45833341e77_8",
  "x": "HieCoAtt is the overall best performing model, but we note that it only outperforms the blind model by a few points. These numbers, however, do not show to which extent the models are able to avoid the trap of the dataset:<cite> Shekhar et al. (2017)</cite> showed that on the FOIL data, models tend to detect correct captions with reasonable accuracy but fail to identify the incorrect ones, leading to a large bias in classification. Taking this insight into account, for the rest of this paper, we focus on the accuracy of the systems in dealing with foil captions, across all three tasks.",
  "y": "background"
 },
 {
  "id": "cf7d01faf555f09973e44be400e768_0",
  "x": "To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017) . We then focus on a new case study of hierarchical deep reinforcement learning for video captioning<cite> (Wang et al., 2018b)</cite> , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems. This tutorial aims at introducing deep reinforcement learning methods to researchers in the NLP community.",
  "y": "uses"
 },
 {
  "id": "cf7d01faf555f09973e44be400e768_1",
  "x": "To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017) . We then focus on a new case study of hierarchical deep reinforcement learning for video captioning<cite> (Wang et al., 2018b)</cite> , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems. This tutorial aims at introducing deep reinforcement learning methods to researchers in the NLP community.",
  "y": "uses"
 },
 {
  "id": "cf7d01faf555f09973e44be400e768_2",
  "x": "\u2022 Lessons Learned, Future Directions, and Practical Advices for DRL in NLP Third, we switch from the theoretical presentations to an interactive demonstration and discussion session: we aim at providing an interactive session to transfer the theories of DRL into practical insights. More specifically, we will discuss three important issues, including problem formulation/model design, exploration vs. exploitation, and the integration of linguistic structures in DRL. We will show case a recent study<cite> (Wang et al., 2018b</cite> ) that leverages hierarchical deep reinforcement learning for language and vision, and extend the discussion.",
  "y": "uses"
 },
 {
  "id": "cffa735deb802118640005a1d527ee_0",
  "x": "Active learning (AL) copes with this problem as it intelligently selects the data to be labeled. It is a sampling strategy where the learner has control over the training material to be manually annotated by selecting those examples which are of high utility for the learning process. AL has been successfully applied to speed up the annotation process for many NLP tasks without sacrificing annotation quality<cite> (Engelson and Dagan, 1996</cite>; Ngai and Yarowsky, 2000; Hwa, 2001; Tomanek et al., 2007a) .",
  "y": "background"
 },
 {
  "id": "cffa735deb802118640005a1d527ee_1",
  "x": "To calculate the disagreement among the committee members several metrics have been proposed including the vote entropy<cite> (Engelson and Dagan, 1996)</cite> as possibly the most well-known one. Our approach to approximating the learning curve is based on the disagreement within a committee. However, it is independent of the actual metric used to calculate the disagreement.",
  "y": "background"
 },
 {
  "id": "cffa735deb802118640005a1d527ee_2",
  "x": "Each committee member then makes its predictions on the pool of unlabeled examples, and those examples on which the committee members express the highest disagreement are considered most informative for learning and are thus selected for manual annotation. To calculate the disagreement among the committee members several metrics have been proposed including the vote entropy<cite> (Engelson and Dagan, 1996)</cite> as possibly the most well-known one. Our approach to approximating the learning curve is based on the disagreement within a committee.",
  "y": "similarities background"
 },
 {
  "id": "cffa735deb802118640005a1d527ee_3",
  "x": "Disagreement is measured by vote entropy<cite> (Engelson and Dagan, 1996)</cite> . In our NER scenario, complete sentences are selected by AL. While we made use of ME classifiers during the selection, we employed a NE tagger based on Conditional Random Fields (CRFs) (Lafferty et al., 2001 ) during evaluation time to determine the learning curves.",
  "y": "uses"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_0",
  "x": "**INTRODUCTION** In the relatively short time since its introduction, neural machine translation has risen to prominence in both academia and industry. Neural models have consistently shown top performance in shared evaluation tasks (Bojar et al., 2016; Cettolo et al., 2016) and are becoming the technology of choice for commercial MT service providers<cite> (Wu et al., 2016</cite>; Crego et al., 2016) .",
  "y": "background"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_1",
  "x": "Our starting point for experimentation is a standard baseline neural machine translation system implemented using the Lamtram 1 and DyNet 2 toolkits (Neubig, 2015; Neubig et al., 2017) . Translation models are trained until perplexity convergence on held-out data using the Adam algorithm with a maximum step size of 0.0002 (Kingma and Ba, 2015; <cite>Wu et al., 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_2",
  "x": "This system uses the attentional encoder-decoder architecture described by Bahdanau et al. (2015) , building on work by Sutskever et al. (2014) . The translation model uses a bi-directional encoder with a single LSTM layer of size 1024, multilayer perceptron attention with a layer size of 1024, and word representations of size 512. Translation models are trained until perplexity convergence on held-out data using the Adam algorithm with a maximum step size of 0.0002 (Kingma and Ba, 2015; <cite>Wu et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_3",
  "x": "While this can lead to much faster convergence, the resulting models are shown to slightly underperform compared to annealing SGD<cite> (Wu et al., 2016)</cite> . However, Adam's speed and reputation of generally being \"good enough\" have made it a popular choice for researchers and NMT toolkit authors 6 (Arthur et al., 2016; Lee et al., 2016; Britz et al., 2017; Sennrich et al., 2017) . While differences in automatic metric scores between SGD and Adam-trained systems may be relatively small, they raise the more general question of training effectiveness.",
  "y": "differences background motivation"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_4",
  "x": "More recent work seeks to accelerate training with the Adam algorithm, which applies momentum on a per-parameter basis and automatically adapts step size subject to a user-specified maximum (Kingma and Ba, 2015) . While this can lead to much faster convergence, the resulting models are shown to slightly underperform compared to annealing SGD<cite> (Wu et al., 2016)</cite> . However, Adam's speed and reputation of generally being \"good enough\" have made it a popular choice for researchers and NMT toolkit authors 6 (Arthur et al., 2016; Lee et al., 2016; Britz et al., 2017; Sennrich et al., 2017) .",
  "y": "background motivation"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_5",
  "x": "More recent work seeks to accelerate training with the Adam algorithm, which applies momentum on a per-parameter basis and automatically adapts step size subject to a user-specified maximum (Kingma and Ba, 2015) . While this can lead to much faster convergence, the resulting models are shown to slightly underperform compared to annealing SGD<cite> (Wu et al., 2016)</cite> . However, Adam's speed and reputation of generally being \"good enough\" have made it a popular choice for researchers and NMT toolkit authors 6 (Arthur et al., 2016; Lee et al., 2016; Britz et al., 2017; Sennrich et al., 2017) .",
  "y": "background"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_6",
  "x": "7 Learning rates of 0.5 for SGD and 0.0002 for Adam or very similar are shown to work well in NMT implementations including GNMT<cite> (Wu et al., 2016)</cite> , Nematus, Marian, and OpenNMT (http://opennmt.net). 8 For each mini-batch, sentences are added until the word set perplexity every 50K training sentences for the first training run and every 25K sentences for subsequent runs. For IWSLT systems, we evaluate every 25K sentences and then every 6,250 sentences.",
  "y": "uses"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_8",
  "x": "As<cite> Wu et al. (2016)</cite> show different levels of effectiveness for different sub-word vocabulary sizes, we evaluate running BPE with 16K and 32K merge operations. As shown in Table 2 , sub-word systems outperform full-word systems across the board, despite having fewer total parameters. Systems built on larger data generally benefit from larger vocabularies while smaller systems perform better with smaller vocabularies.",
  "y": "uses similarities"
 },
 {
  "id": "d0fa481abaf6d1b5529e40ff73f00a_9",
  "x": "This trend follows previous work showing that dropout combats overfitting of small data, though the point of inflection is worth noting (Sennrich et al., 2016a; <cite>Wu et al., 2016)</cite> . Even though the English-French data is still relatively small (220K sentences), BPE leads to a smaller vocabulary of more general translation units, effectively reducing sparsity, while annealing Adam can avoid getting stuck in poor local optima. These techniques already lead to better generalization without the need for dropout.",
  "y": "similarities"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_1",
  "x": "However, most of this research has focused on supervised methods requiring large amounts of labeled data. The supervision was either given in the form of meaning representations aligned with sentences (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007) or in a somewhat more relaxed form, such as lists of candidate meanings for each sentence (Kate and Mooney, 2007; Chen and Mooney, 2008) or formal representations of the described world state for each text<cite> (Liang et al., 2009)</cite> . Such annotated resources are scarce and expensive to create, motivating the need for unsupervised or semi-supervised techniques (Poon and Domingos, 2009 ).",
  "y": "background"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_3",
  "x": "We study our set-up on the weather forecast data<cite> (Liang et al., 2009)</cite> where the original textual weather forecasts were complemented by additional forecasts describing the same weather states (see figure 1 for an example). The average overlap between the verbalized fields in each group of noncontradictory forecasts was below 35%, and more than 60% of fields are mentioned only in a single forecast from a group. Our model, learned from 100 labeled forecasts and 259 groups of unannotated non-contradictory forecasts (750 texts in total), achieved 73.9% F 1 .",
  "y": "uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_4",
  "x": "Section 3 redescribes the semantics-text correspondence model<cite> (Liang et al., 2009)</cite> in the context of our learning scenario. In section 4 we provide an empirical evaluation of the proposed method. We conclude in section 5 with an examination of additional related work.",
  "y": "background"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_5",
  "x": "An instantiation of the algorithm for the semantics-text correspondence model is given in section 3.2. Statistical models of parsing can often be regarded as defining the probability distribution of meaning m and its alignment a with the given text w, P (m, a, w) = P (a, w|m)P (m). The semantics m can be represented either as a logical formula (see, e.g., (Poon and Domingos, 2009 )) or as a set of field values if database records are used as a meaning representation<cite> (Liang et al., 2009</cite> ).",
  "y": "background"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_6",
  "x": "It holds because on each iteration we add a single meaningm n i to m (line 7), and m n i is guaranteed to be consistent with m , as the semanticsm n i was conditioned on the meaning m during inference (line 4). An important aspect of this algorithm is that unlike usual greedy inference, the remaining ('future') texts do affect the choice of meaning representations made on the earlier stages. As soon as semantics m k are inferred for every k, we find ourselves in the set-up of learning with unaligned semantic states considered in<cite> (Liang et al., 2009)</cite> .",
  "y": "uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_7",
  "x": "In this section we redescribe the semantics-text correspondence model<cite> (Liang et al., 2009</cite> ) with an extension needed to model examples with latent states, and also explain how the inference algorithm defined in section 2 can be applied to this model. ---------------------------------- **MODEL DEFINITION**",
  "y": "extends"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_9",
  "x": "To estimate the parameters, we use the Expectation-Maximization algorithm (Dempster et al., 1977) . When the world state is observable, learning does not require any approximations, as dynamic programming (a form of the forward-backward algorithm) can be used to infer the posterior distribution on the E-step<cite> (Liang et al., 2009)</cite> . However, when the state is latent, dependencies are not local anymore, and approximate inference is required.",
  "y": "background"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_10",
  "x": "As soon as the meaning representations m are inferred, we find ourselves in the set-up studied in<cite> (Liang et al., 2009</cite> ): the state s is no longer latent and we can run efficient inference on the E-step. Though some fields of the state s may still not be specified by m , we prohibit utterances from aligning to these non-specified fields. On the M-step of EM the parameters are estimated as proportional to the expected marginal counts computed on the E-step.",
  "y": "uses similarities"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_11",
  "x": "**EXPERIMENTS** To perform the experiments we used a subset of the weather dataset introduced in<cite> (Liang et al., 2009</cite> ). The original dataset contains 22,146 texts of 28.7 words on average, there are 12 types of records (predicates) and 36.0 records per forecast on average.",
  "y": "uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_12",
  "x": "We aimed to preserve approximately the same proportion of new and original examples as we had in the training set, therefore, we combined 50 texts originally present in the weather dataset with additional 100 newly-produced texts. We annotated these 100 texts by aligning each line to one or more records, 7 whereas for the original texts the alignments were already present. Following<cite> Liang et al. (2009)</cite> we evaluate the models on how well they predict these alignments.",
  "y": "uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_13",
  "x": "When estimating the model parameters, we followed the training regime prescribed in<cite> (Liang et al., 2009)</cite> . Namely, 5 iterations of EM with a basic model (with no segmentation or coherence modeling), followed by 5 iterations of EM with the model which generates fields independently and, at last, 5 iterations with the full model. Only then, in the semi-supervised learning scenarios, we added unlabeled data and ran 5 additional iterations of EM.",
  "y": "uses"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_14",
  "x": "Only then, in the semi-supervised learning scenarios, we added unlabeled data and ran 5 additional iterations of EM. Instead of prohibiting records from crossing punctuation, as suggested by<cite> Liang et al. (2009)</cite> , in our implementation we disregard the words not attached to specific fields (attached to the nullfield, see section 3.1) when computing spans of records. To speed-up training, only a single record of each type is allowed to be generated when running inference for unlabeled examples on the E- Table 1 : Results (precision, recall and F 1 ) on the weather forecast dataset.",
  "y": "differences"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_15",
  "x": "Note that the words \"sun\", \"cloudiness\" or \"gaps\" were not appearing in the labeled part of the data, but seem to be assigned to correct categories. However, correlation between rain and overcast, as also noted in<cite> (Liang et al., 2009)</cite> , results in the wrong assignment of the rain-related words to the field value corresponding to very cloudy weather. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "d1decbc03929cbf67a412d0a3a2a66_16",
  "x": "We showed how it can be instantiated for the semantics-text correspondence model<cite> (Liang et al., 2009</cite> ) and evaluated it on a dataset of weather forecasts. Our approach resulted in an improvement over the scores of both the supervised baseline and of the traditional semi-supervised learning. There are many directions we plan on investigating in the future for the problem of learning semantics with non-contradictory relations.",
  "y": "uses"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_0",
  "x": "---------------------------------- **INTRODUCTION** Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., Dyer et al. (2016) with 92.4 F 1 on Penn Treebank constituency parsing and<cite> Vinyals et al. (2015)</cite> with 92.8 F 1 .",
  "y": "background"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_1",
  "x": "**INTRODUCTION** Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., Dyer et al. (2016) with 92.4 F 1 on Penn Treebank constituency parsing and<cite> Vinyals et al. (2015)</cite> with 92.8 F 1 . In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture.",
  "y": "uses"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_3",
  "x": "We look here at three neural net (NN) models closest to our research along various dimensions. The first (Zaremba et al., 2014) gives the basic language modeling architecture that we have adopted, while the other two<cite> (Vinyals et al., 2015</cite>; Dyer et al., 2016) are parsing models that have the current best results in NN parsing. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_4",
  "x": "We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing (23) and millions of auto-parsed \"silver\" trees (McClosky et al., 2006; Huang et al., 2010; <cite>Vinyals et al., 2015)</cite> for tritraining. To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (Parker et al., 2011 ) with a product of eight Berkeley parsers (Petrov, 2010) 2 and ZPar (Zhu et al., 2013) and select 24 million trees on which both parsers agree (Li et al., 2014) . We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et 1 The code and trained models used for experiments are available at github.com/cdg720/emnlp2016.",
  "y": "uses"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_6",
  "x": "---------------------------------- **SUPERVISION** As shown in Table 2 , with 92.6 F 1 LSTM-LM (G) outperforms an ensemble of five MTPs<cite> (Vinyals et al., 2015)</cite> and RNNG (Dyer et al., 2016) , both of which are trained on the WSJ only.",
  "y": "differences"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_7",
  "x": "**SEMI-SUPERVISION** We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus 4 (HC)<cite> (Vinyals et al., 2015)</cite> ; and an ensemble of six one-to-many sequence models trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (Luong et al., 2016) . We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature.",
  "y": "uses"
 },
 {
  "id": "d2b9c678a3d4920919f59c3b5903d3_8",
  "x": "Table 3 : Evaluation of models trained on the WSJ and additional resources. Note that the numbers of<cite> Vinyals et al. (2015)</cite> and Luong et al. (2016) are not directly comparable as their models are evaluated on OntoNotesstyle trees instead of PTB-style trees. E(LSTM-LMs (GS)) is an ensemble of eight LSTM-LMs (GS).",
  "y": "differences"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_0",
  "x": "**PREVIOUS RESEARCH** Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies (Mihalcea and Strapparava, 2005; Purandare and Litman, 2006; <cite>Yang et al., 2015)</cite> , humor recognition was modeled as a binary classification task",
  "y": "background"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_1",
  "x": "In the seminal work (Mihalcea and Strapparava, 2005) , a corpus of 16,000 \"one-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work<cite> (Yang et al., 2015)</cite> , a new corpus was constructed from a Pun of the Day website.",
  "y": "background"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_2",
  "x": "There is a great need for an open corpus that can support investigating humor in presentations. 1 CNN-based text categorization methods have been applied for humor recognition (e.g., in (Bertero and Fung, 2016b) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in<cite> Yang et al. (2015)</cite> is missing; (b) CNN's performance in the previous research is not quite clear 2 ; and (c) some important techniques that can improve CNN performance (e.g., using variedsized filters and dropout regularization (Hinton et al., 2012) ) were missing. Therefore, the present study is meant to address these limitations.",
  "y": "motivation"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_3",
  "x": "Following (Mihalcea and Strapparava, 2005; <cite>Yang et al., 2015)</cite> , we selected the same sizes (n = 4726) of humorous and non-humorous sentences. To minimize possible topic shifts between positive and negative instances, for each positive instance, we picked up one negative instance nearby (the context window was 7 sentences in this study). For example, in Figure 1 , a negative instance (corresponding to 'sent-2') was selected from the nearby sentences ranging from 'sent-7' and 'sent+7'.",
  "y": "uses background"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_4",
  "x": "There is a great need for an open corpus that can support investigating humor in presentations. 1 CNN-based text categorization methods have been applied for humor recognition (e.g., in (Bertero and Fung, 2016b) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in<cite> Yang et al. (2015)</cite> is missing; (b) CNN's performance in the previous research is not quite clear 2 ; and (c) some important techniques that can improve CNN performance (e.g., using variedsized filters and dropout regularization (Hinton et al., 2012) ) were missing. Therefore, the present study is meant to address these limitations.",
  "y": "motivation"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_5",
  "x": "Following<cite> Yang et al. (2015)</cite> , we applied Random Forest (Breiman, 2001 ) to do humor recognition by using the following two groups of features. The first group are latent semantic structural features covering the following 4 categories 5 : Incongruity (2), Ambiguity (6), Interpersonal Effect (4), and Phonetic Pattern (4). The second group are semantic distance features, including the humor label classes from 5 sentences in the training set that are closest to this sentence (found by using a k-Nearest Neighbors (kNN) method), and each sentence's averaged Word2Vec representations (n = 300).",
  "y": "uses"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_6",
  "x": "The Pun data allows us to verify that our implementation is consistent with the work reported in<cite> Yang et al. (2015)</cite> . In our experiment, we firstly divided each corpus into two parts. The smaller part (the Held-Out Partition) was used for tweaking various hyper- Table 1 : Humor recognition on both Pun and TED data sets by using (a) random prediction (Chance), conventional method (Base) and CNN method; the sizes of the dev and CV partitions are provided for each data set.",
  "y": "similarities"
 },
 {
  "id": "d42e2a9175e024e3ae44118e12fb58_7",
  "x": "When training the CNN model, we randomly selected 10% of the training data as the validation set for using early stopping to avoid overfitting. 7 https://github.com/ EducationalTestingService/skll 8 https://github.com/fchollet/keras 9 The implementation will be released with the paper On the Pun data, the CNN model shows consistent improved performance over the conventional model, as suggested in<cite> Yang et al. (2015)</cite> . In particular, precision has been greatly increased from 0.762 to 0.864.",
  "y": "similarities background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_0",
  "x": "Some studies extract parallel fragments relying on a probabilistic translation lexicon estimated on an external parallel corpus. They locate the source and target fragments independently, making the extracted fragments unreliable<cite> (Munteanu and Marcu, 2006)</cite> . Some studies develop alignment models for comparable sentences to extract parallel fragments (Quirk et al., 2007) .",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_1",
  "x": "Experimental results on Chinese-Japanese corpora show that our proposed method significantly outperforms a state-of-theart approach, which indicate the effectiveness of our parallel fragment extraction system. Moreover, we investigate the factors that may affect the performance of our system in detail. 2 Related Work<cite> (Munteanu and Marcu, 2006)</cite> is the first attempt to extract parallel fragments from comparable sentences.",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_2",
  "x": "The first lexicon we use is the IBM Model 1 lexicon, which is obtained by running GIZA++ 3 that implements sequential word-based statistical alignment model of IBM models. The second lexicon we use is the LLR lexicon. <cite>Munteanu and Marcu (2006)</cite> show that the LLR lexicon performs better than the IBM Model 1 lexicon for parallel fragment extraction.",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_3",
  "x": "One advantage of the LLR lexicon is that it can produce both positive and negative associations. <cite>Munteanu and Marcu (2006)</cite> develop a smoothing filter applying this advantage. We extract the LLR lexicon from a word-aligned parallel corpus using the same method as<cite> (Munteanu and Marcu, 2006)</cite> .",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_4",
  "x": "We extract the LLR lexicon from a word-aligned parallel corpus using the same method as<cite> (Munteanu and Marcu, 2006)</cite> . The last lexicon we use is the SampLEX lexicon. Vuli\u0107 and Moens (2012) propose an associative approach for lexicon extraction from par-allel corpora that relies on the paradigm of data reduction.",
  "y": "similarities uses"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_5",
  "x": "They show that their method outperforms IBM Model 1 and other associative methods such as LLR in terms of precision and F-measure. We extract SampLEX lexicon from a parallel corpus using the same method as (Vuli\u0107 and Moens, 2012) . Aiming to gain new knowledge that does not exist in the lexicon, we apply a smoothing filter similar to<cite> (Munteanu and Marcu, 2006)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_6",
  "x": "The averaging filter sets the score of one word to the average score of several words around it. We think the words with initial positive scores are reliable, because they satisfy two strong constraints, namely alignment by IBM models and existence in the lexicon. Therefore, unlike<cite> (Munteanu and Marcu, 2006)</cite>, we only apply the averaging filter to the words with negative scores.",
  "y": "extends differences"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_7",
  "x": "**EXPERIMENTS** In our experiments, we compared our proposed fragment extraction method with<cite> (Munteanu and Marcu, 2006)</cite> . We manually evaluated the accuracy of the extracted fragments.",
  "y": "similarities"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_8",
  "x": "We then applied fragment extraction on the extracted comparable sentences. We compared our proposed method with<cite> (Munteanu and Marcu, 2006)</cite>. We applied word alignment using GIZA++.",
  "y": "similarities"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_9",
  "x": "We can see that the average size of fragments extracted by<cite> (Munteanu and Marcu, 2006</cite> ) is unusually long, which is also reported in (Quirk et al., 2007) . Our proposed method extracts shorter fragments. The number of extracted fragments and the average size are similar among the three lexicons when using the same alignment setting.",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_10",
  "x": "We manually evaluated the accuracy based on the number of exact match. Note that exact match criteria has a bias against<cite> (Munteanu and Marcu, 2006)</cite> , because their method extacts subsentential fragments which are quite long. We found that only one of the fragments extracted by \"Munteanu+, 2006\" is exact match, while for the remainder only partial matches are contained in long fragments.",
  "y": "background"
 },
 {
  "id": "d44648766e68cb914c5489e385f42e_11",
  "x": "Translation results evaluated on BLEU-4, are shown in Table 2 . We can see that appending the extracted comparable sentences have a positive effect on translation quality. Adding the fragments extracted by<cite> (Munteanu and Marcu, 2006)</cite> has a negative impact, compared to appending the sentences.",
  "y": "differences"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_0",
  "x": "**ABSTRACT** The automatic induction of scripts (Schank and Abelson, 1977) has been the focus of many recent works. In this paper, we employ a variety of these methods to learn Schank and Abelson's canonical restaurant script, using a novel dataset of restaurant narratives we have compiled from a website called \"Dinners from Hell.\" Our models learn narrative chains, script-like structures that we evaluate with the \"narrative cloze\" task<cite> (Chambers and Jurafsky, 2008)</cite> .",
  "y": "uses"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_1",
  "x": "In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora<cite> (Chambers and Jurafsky, 2008</cite>; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014) . These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed. For many specialized applications, however, knowledge of a few relevant scripts may be more useful than knowledge of many irrelevant scripts.",
  "y": "background"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_2",
  "x": "Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001) ,<cite> Chambers and Jurafsky (2008)</cite> propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014) . It is from this body of work that we borrow techniques to apply to the Dinners from Hell dataset.",
  "y": "background"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_3",
  "x": "As defined by<cite> Chambers and Jurafsky (2008)</cite> , a narrative chain is \"a partially ordered set of narrative events that share a common actor,\" where a narrative event is \"a tuple of an event (most simply a verb) and its participants, represented as typed dependencies.\" To learn narrative chains from text, Chambers and Jurafsky extract chains of narrative events linked by a common coreferent within a document. For example, the sentence \"John drove to the store where he bought some ice cream.\" would generate two narrative events corresponding to the protagonist John: (DRIVE, nsubj) followed by (BUY, nsubj). Over these extracted chains of narrative events, pointwise mutual information (PMI) is computed between all pairs of events.",
  "y": "background"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_4",
  "x": "They also introduce a competitive \"unigram model\" as a baseline for the narrative cloze task. To learn the restaurant script from our dataset, we implement the models of<cite> Chambers and Jurafsky (2008)</cite> and Jans et al. (2012) , as well as the unigram baseline of Pichotta and Mooney (2014) . To evaluate our success in learning the restaurant script, we perform a modified version of the narrative cloze task, predicting only verbs that we annotate as \"restaurant script-relevant\" and comparing the performance of each model.",
  "y": "uses similarities"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_5",
  "x": "**METHODS** This section provides an overview of each of the different methods and parameter settings we employ to learn narrative chains from the Dinners from Hell corpus, starting with the original model<cite> (Chambers and Jurafsky, 2008)</cite> and extending to the modifications of Jans et al. (2012) . As part of this work, we are releasing a program called NaChos, our integrated Python implementation of each of the methods for learning narrative chains described in this section.",
  "y": "extends differences"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_6",
  "x": "e n , at insertion point k. The original model, proposed by<cite> Chambers and Jurafsky (2008)</cite> , predicts the event that maximizes unordered pmi, where V is the set of all observed events (the vocabulary) and C(e 1 , e 2 ) is symmetric. Two additional models are introduced by Jans et al. (2012) and we use them here, as well.",
  "y": "background"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_7",
  "x": "C(e 1 , * ) and C(e 1 , e 2 ) is asymmetric. Discounting For each model, we add an option for discounting the computed scores. In the case of the two PMI-based models, we use the discount score described in Pantel and Ravichandran (2004) and used by<cite> Chambers and Jurafsky (2008)</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "d4563562cd0dfd8ef6cdb57117fb22_8",
  "x": "Chains with fewer than two events are excluded. In this way, we generate a total of 2,273 cloze tests. Scoring We employ three different scoring metrics: average rank<cite> (Chambers and Jurafsky, 2008)</cite> , mean reciprocal rank, and recall at 50 (Jans et al., 2012) .",
  "y": "similarities uses"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_0",
  "x": "Given its all-neural nature, an E2E model can be reasonably downsized to fit on mobile devices [6] . Despite the rapid progress made by E2E models, they still face challenges compared to state-of-the-art conventional models [8, 9] . To bridge the quality gap between a streaming recurrent neural network transducer (RNN-T) [6] and a large conventional model [8] , a two-pass framework has been proposed in<cite> [10]</cite> , which uses a non-streaming LAS decoder to rescore the RNN-T hypotheses.",
  "y": "background"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_1",
  "x": "We first analyze the behavior of the deliberation model, including performance when attending to multiple RNN-T hypotheses, contribution of different attention, and rescoring vs. beam search. We apply additional encoder (AE) layers and minimum WER (MWER) training [22] to further improve quality. The results show that our MWER trained 8-hypothesis deliberation model performs 11% relatively better than LAS rescoring<cite> [10]</cite> in VS WER, and up to 15% for proper noun recognition.",
  "y": "differences"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_2",
  "x": "**MODEL ARCHITECTURE** As shown in Fig. 1 , our deliberation network consists of three major components: A shared encoder, an RNN-T decoder [1] , and a deliberation decoder, similar to<cite> [10,</cite> 16] . The shared encoder takes log-mel filterbank energies, x = (x1, ..., xT ), where T denotes the number of frames, and generates an encoding e. The encoder output e is then fed to an RNN-T decoder to produce first-pass decoding results yr in a streaming fashion.",
  "y": "similarities"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_3",
  "x": "There are two major differences between our model and the LAS rescoring<cite> [10]</cite> . First, the deliberation model attends to both e and yr, while<cite> [10]</cite> only attends to the acoustic embedding, e. Second, our deliberation model encodes yr bidirectionally, while<cite> [10]</cite> only relies on unidirectional encoding e for decoding. [10] shows that the incompatibility between an RNN-T encoder and a LAS decoder leads to a gap between the rescoring model and LASonly model.",
  "y": "differences"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_4",
  "x": "**TRAINING** A deliberation model is typically trained from scratch by jointly optimizing all components [16] . However, we find training a two-pass model from scratch tends to be unstable in practice<cite> [10]</cite> , and thus use a two-step training process: Train the RNN-T as in [6] , and then fix the RNN-T parameters and only train the deliberation decoder and additional encoder layers as in [7, <cite>10]</cite> .",
  "y": "uses"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_5",
  "x": "\u03b8e, \u03b81, and \u03b82 denote the parameters of shared encoder, RNN-T decoder, and deliberation decoder, respectively. Note that a jointly trained model can be further trained with MWER loss. The joint training is similar to \"deep finetuning\" in<cite> [10]</cite> but without a pre-trained decoder.",
  "y": "similarities"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_6",
  "x": "We are also curious how rescoring performs given bidirectional encoding from yr. In rescoring, we run the deliberation decoder on yr in a teacher-forcing mode<cite> [10]</cite> . Note the difference from<cite> [10]</cite> when rescoring a hypothesis is that the deliberation network sees all candidate hypotheses.",
  "y": "uses"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_7",
  "x": "Note the difference from<cite> [10]</cite> when rescoring a hypothesis is that the deliberation network sees all candidate hypotheses. We compare rescoring and beam search in Sect. 4.",
  "y": "differences"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_8",
  "x": "The SxS set contains utterances where the LAS rescoring model<cite> [10]</cite> performs inferior to a state-of-the-art conventional model [8] , and one reason is due to proper nouns. The voice command test sets include 3 TTS test sets created using parallel-wavenet [25] : Songs, Contacts-TTS, and Apps, where the commands include song, contact, and app names, respectively. The Contacts-Real set contains anonymized and hand-transcribed utterances from Google traffic to communicate with a contact, for example, \"Call Jon Snow\".",
  "y": "differences"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_9",
  "x": "We propose to use the deliberation decoder to rescore first-pass RNN-T results, and expect bidirectional encoding to help compared to LAS rescoring<cite> [10]</cite> . Table 5 shows that the deliberation rescoring (E8) performs 5% relatively better than LAS rescoring (B3). AE layers are added to both models.",
  "y": "differences"
 },
 {
  "id": "d53d1b53168041baea5b5002b46627_10",
  "x": "**COMPARISONS** From the above analysis, an MWER trained 8-hypothesis deliberation model with AE layers performs the best, and thus we use that for comparison below. In Table 4 , we compare deliberation models with an RNN-T [6] and LAS rescoring model<cite> [10]</cite> To understand where the improvement comes from, in Fig. 2 we show an example of deliberation attention distribution on the RNN-T hypotheses (x-axis) at every step of the second-pass decoding (yaxis).",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_0",
  "x": "Table 1 lists the number of sentences in the training and test sets for each task and the number of instances used as interpretants in the RTM models (M for million). We use referential translation machine (RTM) <cite>(Bi\u00e7ici, 2018</cite>; Bi\u00e7ici and Way, 2015) models for building our prediction models. RTMs predict data translation between the instances in the training set and the test set using interpretants, data close to the task instances.",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_1",
  "x": "We use Global Linear Models (GLM) (Collins, 2002) with dynamic learning (GLMd)<cite> (Bi\u00e7ici, 2018)</cite> for word-and phrase-level translation performance prediction. GLMd uses weights in a range [a, b] to update the learning rate dynamically according to the error rate. Evaluation metrics listed are Pearson's correlation (r), mean absolute error (MAE), and root mean squared error (RMSE).",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_2",
  "x": "We use prediction averaging<cite> (Bi\u00e7ici, 2018)</cite> to obtain a combined prediction from various prediction outputs better than the components, where the performance on the training set is used to obtain weighted average of the top k predictions,\u0177 with evaluation metrics indexed by j \u2208 J and weights with w: We assume independent predictions and use p i /(1 \u2212 p i ) for weights where p i represents the accuracy of the independent classifier i in a weighted majority ensemble (Kuncheva and Rodr\u00edguez, 2014) . We only use the MIX prediction if we obtain better results on the training set.",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_3",
  "x": "This conversion decreases the number of features and obtains close results<cite> (Bi\u00e7ici, 2018)</cite> . Before model combination, we further filter prediction results from different machine learn- ing models based on the results on the training set to decrease the number of models combined and improve the results. A criteria that we use is to include results that are better than the best RR model's results.",
  "y": "similarities"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_4",
  "x": "We use referential translation machine (RTM) <cite>(Bi\u00e7ici, 2018</cite>; Bi\u00e7ici and Way, 2015) models for building our prediction models. RTMs predict data translation between the instances in the training set and the test set using interpretants, data close to the task instances. Interpretants provide context for the prediction task and are used during the derivation of the features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and to identify translation acts between any two data sets for building prediction models.",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_5",
  "x": "Martins et al. (2017) used a hybrid stacking model to combine the word-level predictions from 15 predictors using neural networks with different initializations together with the previous features from a linear model. The neural network architecture they used is also hybrid with different types of layers: input word embedding use 64 dimensional vectors, the next three layers are two feedforward layers with 400 nodes and a bidirectional gated recurrent units layer with 200 units, followed by similar three layers with half nodes, followed by a feedforward layer with 50 nodes and a softmax layer. We use Global Linear Models (GLM) (Collins, 2002) with dynamic learning (GLMd)<cite> (Bi\u00e7ici, 2018)</cite> for word-and phrase-level translation performance prediction.",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_6",
  "x": "---------------------------------- **MIXTURE OF EXPERTS MODELS** We use prediction averaging<cite> (Bi\u00e7ici, 2018)</cite> to obtain a combined prediction from various prediction outputs better than the components, where the performance on the training set is used to obtain weighted average of the top k predictions,\u0177 with evaluation metrics indexed by j \u2208 J and weights with w:",
  "y": "uses"
 },
 {
  "id": "d63acda66b0c17c5c6725c0e20b2d9_7",
  "x": "The stacking models use the predictions from predictors as features and build second level predictors. For the document-level RTM model, instead of running separate MTPPS instances for each training or test document to obtain specific features for each document, we concatenate the sentences from each document to obtain a single sentence representing each and then run an RTM model. This conversion decreases the number of features and obtains close results<cite> (Bi\u00e7ici, 2018)</cite> .",
  "y": "similarities"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_0",
  "x": "During the selection of linguistic indicators, we have taken into consideration previously studied features of readability (Fran\u00e7ois and Fairon, 2012; <cite>Heimann M\u00fchlenbock, 2013</cite>; Vajjala and Meurers, 2012) , L2 Swedish curricula (Levy Scherrer and Lindemalm, 2009; Folkuniversitet, 2013) and aspects of Good Dictionary Examples (GDEX) (Hus\u00e1k, 2010; Kilgarriff et al., 2008) , being that we believe they have some properties in common with exercise items. The current version of the machine learning model distinguishes sentences readable by students at an intermediate level of proficiency from sentences of a higher readability level. The approaches have been implemented and integrated into an online Intelligent ComputerAssisted Language Learning (ICALL) platform, L\u00e4rka .",
  "y": "similarities uses"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_1",
  "x": "It measures not only linguistic difficulty, but also cohesion in texts. Research on L1 readability for Swedish, using machine learning, is described in Heimann M\u00fchlenbock (2013) and<cite> Falkenjack et al. (2013)</cite> . Heimann M\u00fchlenbock (2013) examined readability along five dimensions: surface features, word usage, sentence structure, idea density and human interest.",
  "y": "background"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_2",
  "x": "These required less sophisticated text processing and had previously been used in several studies with success (Beinborn et al., 2012; Dell'Orletta et al., 2011; Fran\u00e7ois and Fairon, 2012; <cite>Heimann M\u00fchlenbock, 2013</cite>; Vajjala and Meurers, 2012) . We computed sentence length as the number of tokens including punctuation, and token length as the number of characters per token. Part of the syntactic features was based on the depth (length) and direction of dependency arcs (features 5-8).",
  "y": "similarities uses"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_3",
  "x": "These lexical categories comprised nouns, verbs, adverbs and adjectives. Subordinates (11) were detected on the basis of the \"UA\" (subordinate clause minus subordinating conjunction) dependency relation tag<cite> (Heimann M\u00fchlenbock, 2013)</cite> . Features DepDepth, Mod, Sub and RightDep, PrepComp have previously been empoyed for Swedish L1 readability at the text level in Heimann M\u00fchlenbock (2013) and<cite> Falkenjack et al. (2013)</cite> respectively.",
  "y": "similarities"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_4",
  "x": "Features DepDepth, Mod, Sub and RightDep, PrepComp have previously been empoyed for Swedish L1 readability at the text level in Heimann M\u00fchlenbock (2013) and<cite> Falkenjack et al. (2013)</cite> respectively. The lexical-morphological features (features 13-25) constituted the largest group. Difficulty at the lexical level was determined based on both the TTR feature mentioned above, expressing vocabulary diversity, and on the basis of the rarity of words (features 13-17) according to the Kelly list and the Wikipedia word list.",
  "y": "similarities uses"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_5",
  "x": "The NN/VB ratio feature, which has a higher value in written text, can also indicate a more complex sentence (Biber et al., 2004;<cite> Heimann M\u00fchlenbock, 2013)</cite> . Features 21-25 are based on evidence from the content of L2 Swedish course syllabuses (Folkuniversitet, 2013) and course books (Levy Scherrer and Lindemalm, 2009), part of them being language-dependent, namely S-VB/VB and S-VB%. These two features cover different types of Swedish verbs ending in -s which can indicate either a reciprocal verb, a passive construction or a deponent verb, active in meaning but passive in form (Fasth and Kannermark, 1989) .",
  "y": "background"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_6",
  "x": "As pronouns indicate a potentially more difficult text (Graesser et al., 2011), we included PN/NN in our set. Both NomR and PN/NN capture idea density, i.e. how complex the relation between the ideas expressed are<cite> (Heimann M\u00fchlenbock, 2013)</cite>. ----------------------------------",
  "y": "background"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_7",
  "x": "The precision and recall values for the identification of B1 sentences was 73% and 68%. Previous classification results for a similar task obtained an average of 77.25% of precision for the classification of easy-to-read texts within an L1 Swedish text-level readability study<cite> (Heimann M\u00fchlenbock, 2013)</cite> . Another classification at the sentence level, but for Italian and from an L1 perspective achieved an accuracy of 78.2%, thus 7% higher compared to our results (Dell'Orletta et al., 2011) .",
  "y": "differences"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_8",
  "x": "An informative traditional measure was sentence length, similarly to the results of previous studies (Beinborn et al., 2012; Dell'Orletta et al., 2011; Fran\u00e7ois and Fairon, 2012; <cite>Heimann M\u00fchlenbock, 2013</cite>; Vajjala and Meurers, 2012) . Lexical-morphological features based on information about the frequency and the CEFR level of items in the Kelly list (DiffW%, DiffWs and KellyFr) also proved to be influential for the classification, as well as AdvVar. Two out of our three semantic features, namely NomR and, in particular, Sense/W, were also highly predictive.",
  "y": "similarities"
 },
 {
  "id": "d66ca5ff22e508da239fc7fdf5ac29_9",
  "x": "Moreover, in the case of Swedish L1 text readability the noun/pronoun ratio and modifiers proved to be indicative of textlevel difficulty<cite> (Heimann M\u00fchlenbock, 2013</cite> ), but at the sentence level from the L2 perspective only the latter seemed influential in our experiments. The data used for the experiments was labeled for CEFR levels at the text level, not at the sentence level. This introduced some noise in the data and made the classification task somewhat harder.",
  "y": "differences"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_0",
  "x": "English has evolved continually over the centuries, in the branching off from antecedent languages in Indo-European prehistory [34, 39] , in the rates of regularisation of verbs [34] and in the waxing and waning in the popularity of individual words [3, 13, 37] . At a much finer scale of time and population, languages change through modifications and errors in the learning process [14, 27] . This continual change and diversity contrasts with the simplicity and consistency of Zipf's law, by which the frequency a word, f , is inversely proportional to its rank k, as f \u223c k \u2212\u03b3 and Heaps law, by which vocabulary size scales sub-linearly with total number of words, across diverse textual and spoken samples [32, 41, 46, 49, 15, 21, 48,<cite> 42]</cite> .",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_1",
  "x": "The Google Ngram corpus [37] provides new support for these statistical regularities in word frequency dynamics at timescales from decades to centuries [22, 41,<cite> 42,</cite> 1, 28] of n-grams -an n-gram being n consecutive character strings, separated by spaces -derived from millions of books over multiple centuries [35] , the n-gram data now covers English books from the year 1500 to year 2008. In English, the Zipf's law in the n-gram data [41] exhibits two regimes: one among words with frequencies above about 0.01% (Zipf's exponent \u03b3 \u2248 1) and another (\u03b3 \u2248 1.4) among words with frequency below 0.0001% <cite>[42]</cite> . The latter Zipf's law exponent \u03b3 of 1.4 is equivalent to a probability distribution function (PDF) exponent, \u03b1, of about 1.7 (\u03b1 = 1 + 1/\u03b3).",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_2",
  "x": "This continual change and diversity contrasts with the simplicity and consistency of Zipf's law, by which the frequency a word, f , is inversely proportional to its rank k, as f \u223c k \u2212\u03b3 and Heaps law, by which vocabulary size scales sub-linearly with total number of words, across diverse textual and spoken samples [32, 41, 46, 49, 15, 21, 48,<cite> 42]</cite> . The Google Ngram corpus [37] provides new support for these statistical regularities in word frequency dynamics at timescales from decades to centuries [22, 41,<cite> 42,</cite> 1, 28] of n-grams -an n-gram being n consecutive character strings, separated by spaces -derived from millions of books over multiple centuries [35] , the n-gram data now covers English books from the year 1500 to year 2008. In English, the Zipf's law in the n-gram data [41] exhibits two regimes: one among words with frequencies above about 0.01% (Zipf's exponent \u03b3 \u2248 1) and another (\u03b3 \u2248 1.4) among words with frequency below 0.0001% <cite>[42]</cite> .",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_3",
  "x": "The n-gram data show Heaps law in that, if N t is corpus size and v t is vocabulary size at time t, then v t \u2248 N \u03b2 t , with \u03b2 \u2248 0.5, for all English words in the corpus <cite>[42]</cite> . If the n-gram corpus is truncated by a minimum word count, then as that minimum is raised the Heaps scaling exponent increases from \u03b2 < 0.5, approaching \u03b2 < 1 <cite>[42]</cite> . The other statistical property is dynamic turnover in the ranked list of most commonly used words.",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_4",
  "x": "If the n-gram corpus is truncated by a minimum word count, then as that minimum is raised the Heaps scaling exponent increases from \u03b2 < 0.5, approaching \u03b2 < 1 <cite>[42]</cite> . The other statistical property is dynamic turnover in the ranked list of most commonly used words. This can be measured in terms of how many words are replaced through time on \"Top y\" ranked lists of different sizes y of most frequently-used words [12, 17, 19, 23] .",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_5",
  "x": "As the \"bar\" is raised, words are more likely to 'die' before they ever reach the bar by stochastic walk [43] . As a result, turnover in the Top y can slow down over time and growth of N t . The FNM does not, however, readily yield Heaps law (v t = N \u03b2 t , where \u03b2 < 1), for which \u03b2 \u2248 0.5 among the 1-gram data for English <cite>[42]</cite> .",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_6",
  "x": "Our PNM, which takes exponentially increasing sample sizes from a neutrally evolved latent population, replicated the Zipf's law, Heaps law, and turnover patterns in the 1-gram data. Although it did not replicate exactly the particular 1-gram corpus we used here, the Heaps law exponent yielded by the PNM does fall within the range-from 0.44 to 0.54-observed in different English 1-gram corpora <cite>[42]</cite> . Among all features we attempted to replicate, the one mismatch between PNM and the 1-gram data is that the PNM yielded an order of magnitude fewer vocabulary words for a given corpus size, while increasing with corpus size according to the same Heaps law exponent.",
  "y": "similarities"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_7",
  "x": "Our canonical model of the PNM differs somewhat from the explanation by <cite>[42]</cite> , in which a \"decreasing marginal need for additional words\" as the corpus grows is underlain by the \"dependency network between the common words ... and their more esoteric counterparts. \" In our PNM representation, there is no network structure between words at all, such as \"inter-word statistical dependencies\" [44] or grammar as a hierarchical network structure between words [20] . Since the PNM performed quite well in replicating multiple static and dynamic statistical properties of 1-grams simultaneously, which the FNM could not do, we find two insights.",
  "y": "differences"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_8",
  "x": "Fig 3b shows how 100 runs of the PNM yields a Heaps law exponent within the range derived by <cite>[42]</cite> for several different n-grams corpora (all English, English fiction, English GB, English US and English 1M). We also The PNM yields Heaps law exponent \u03b2 \u2248 0.52 \u00b1 0.006, within the range of English corpora, whereas the FNM yields a mismatch with the data of \u03b2 \u2248 1 \u00b1 0.002 (Fig 3b) . In Fig 3a, there is a constant offset on the y-axis between vocabulary size in the PNM (\u03b1 = 0.02, N = 10000) versus the 1-gram data.",
  "y": "similarities background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_9",
  "x": "Although we track 1-grams from the year 1700, for turnover statistics we follow other studies <cite>[42]</cite> in being cautious about the n-grams record before the year 1800, due to misspelled words before 1800 that were surely digital scanning errors related to antique printing styles of that may conflate letters such as 's' and 'f' (e.g., myfelf, yourfelf, provifions, increafe, afked etc). The code used for modeling is available at: https://github.com/dr2g08/Neutral-evolution-and-turnover-over-centuries-of-English-word-popularity. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_10",
  "x": "English has evolved continually over the centuries, in the branching off from antecedent languages in Indo-European prehistory [34, 39] , in the rates of regularisation of verbs [34] and in the waxing and waning in the popularity of individual words [3, 13, 37] . At a much finer scale of time and population, languages change through modifications and errors in the learning process [14, 27] . This continual change and diversity contrasts with the simplicity and consistency of Zipf's law, by which the frequency a word, f , is inversely proportional to its rank k, as f \u223c k \u2212\u03b3 and Heaps law, by which vocabulary size scales sub-linearly with total number of words, across diverse textual and spoken samples [32, 41, 46, 49, 15, 21, 48,<cite> 42]</cite> .",
  "y": "differences background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_11",
  "x": "This continual change and diversity contrasts with the simplicity and consistency of Zipf's law, by which the frequency a word, f , is inversely proportional to its rank k, as f \u223c k \u2212\u03b3 and Heaps law, by which vocabulary size scales sub-linearly with total number of words, across diverse textual and spoken samples [32, 41, 46, 49, 15, 21, 48,<cite> 42]</cite> . The Google Ngram corpus [37] provides new support for these statistical regularities in word frequency dynamics at timescales from decades to centuries [22, 41,<cite> 42,</cite> 1, 28] . With annual counts of n-grams -an n-gram being n consecutive character strings, separated by spaces -derived from millions of books over multiple centuries [35] , the n-gram data now covers English books from the year 1500 to year 2008.",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_12",
  "x": "The Google Ngram corpus [37] provides new support for these statistical regularities in word frequency dynamics at timescales from decades to centuries [22, 41,<cite> 42,</cite> 1, 28] . With annual counts of n-grams -an n-gram being n consecutive character strings, separated by spaces -derived from millions of books over multiple centuries [35] , the n-gram data now covers English books from the year 1500 to year 2008. In English, the Zipf's law in the n-gram data [41] exhibits two regimes: one among words with frequencies above about 0.01% (Zipf's exponent \u03b3 \u2248 1) and another (\u03b3 \u2248 1.4) among words with frequency below 0.0001% <cite>[42]</cite> .",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_13",
  "x": "One, known as Heaps law, refers to the way that vocabulary size scales sub-linearly with corpus size (raw word count). The n-gram data show Heaps law in that, if N t is corpus size and v t is vocabulary size at time t, then v t \u2248 N \u03b2 t , with \u03b2 \u2248 0.5, for all English words in the corpus <cite>[42]</cite> . If the n-gram corpus is truncated by a minimum word count, then as that minimum is raised the Heaps scaling exponent increases from \u03b2 < 0.5, approaching \u03b2 < 1 <cite>[42]</cite> .",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_14",
  "x": "The n-gram data show Heaps law in that, if N t is corpus size and v t is vocabulary size at time t, then v t \u2248 N \u03b2 t , with \u03b2 \u2248 0.5, for all English words in the corpus <cite>[42]</cite> . If the n-gram corpus is truncated by a minimum word count, then as that minimum is raised the Heaps scaling exponent increases from \u03b2 < 0.5, approaching \u03b2 < 1 <cite>[42]</cite> . The other statistical property is dynamic turnover in the ranked list of most commonly used words.",
  "y": "background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_15",
  "x": "The FNM does not, however, readily yield Heaps law (v t = N \u03b2 t , where \u03b2 < 1), for which \u03b2 \u2248 0.5 among the 1-gram data for English <cite>[42]</cite> . In the FNM, the expected exponent \u03b2 is 1.0, as the number of different variants (vocabulary) normally scales linearly with \u00b5N t [11] . While the FNM has been a powerful null model, in the case of books, we can make a notable improvement to account for the fact that most published material goes unnoticed while a relatively small portion of the corpus is highly visible.",
  "y": "differences"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_16",
  "x": "On the double-logarithmic plot in Fig 3a, the Heaps law exponent is equivalent to the slope of the data series. The PNM matches the 1-gram data with Heaps exponent (slope) of about 0.5, whereas the FNM, with exponent about 1.0, does not. Fig 3b shows how 100 runs of the PNM yields a Heaps law exponent within the range derived by <cite>[42]</cite> for several different n-grams corpora (all English, English fiction, English GB, English US and English 1M).",
  "y": "similarities background"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_19",
  "x": "What did ultimately work very well was our partial-sampling Neutral model, or PNM (Fig 1b) , which models a growing sample from a fixed-sized FNM. Our PNM, which takes exponentially increasing sample sizes from a neutrally evolved latent population, replicated the Zipf's law, Heaps law, and turnover patterns in the 1-gram data. Although it did not replicate exactly the particular 1-gram corpus we used here, the Heaps law exponent yielded by the PNM does fall within the range-from 0.44 to 0.54-observed in different English 1-gram corpora <cite>[42]</cite> .",
  "y": "similarities"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_20",
  "x": "As a heuristic device, we consider the fixed-size FNM to represent a canonical literature, while the growing sample represents the real world of exponentially growing numbers of books published ever year in English. Of course, the world is not as simple as our model; there is no official fixed canon, that canon does not strictly copy words from the previous year only and there are plenty of words being invented that occur outside this canon. Our canonical model of the PNM differs somewhat from the explanation by <cite>[42]</cite> , in which a \"decreasing marginal need for additional words\" as the corpus grows is underlain by the \"dependency network between the common words ... and their more esoteric counterparts.",
  "y": "differences"
 },
 {
  "id": "d68bb5264d157cc4c2d9fa9c8f82b6_21",
  "x": "As in a previous study [1] , we removed 1-grams that are common symbols or numbers, and 1-grams containing the same consonant three or more times consecutively. As in our other studies [1, 8, 6] , we normalized the count of 1-grams using the yearly occurrences of the most common English word, the. Although we track 1-grams from the year 1700, for turnover statistics we follow other studies <cite>[42]</cite> in being cautious about the n-grams record before the year 1800, due to misspelled words before 1800 that were surely digital scanning errors related to antique printing styles of that may conflate letters such as 's' and 'f' (e.g., myfelf, yourfelf, provifions, increafe, afked etc).",
  "y": "uses"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_0",
  "x": "Later work by <cite>Wang et al. (2015b)</cite> adopted a different strategy based on the similarity between the dependency parse of a sentence and the semantic AMR graph. To learn the parser, <cite>Wang et al. (2015b)</cite> define an algorithm that for each instance in the training data infers the action sequence that convert the input dependency tree into the corresponding AMR graph and train a classifier to predict the actions to be taken during testing.",
  "y": "background"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_1",
  "x": "This strategy is also referred to as exact imitation learning, while the algorithm that infers the action sequence in the training instances is commonly referred to as the expert policy. In our submission to SemEval Task 8 on AMR parsing, we follow the transition-based paradigm of <cite>Wang et al. (2015b)</cite> with modifications to the parsing algorithm, and also use the DAGGER imitation learning algorithm (Ross et al., 2011) to generalise better to unseen data. The central idea of DAGGER is that the distribution of states encountered by the expert policy during training may not be a good approximation to those seen in testing by the trained policy.",
  "y": "extends differences"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_2",
  "x": "**SYSTEM DESCRIPTION** In the following subsections we focus on the differences from previous work and in particular that of <cite>Wang et al. (2015b)</cite> who introduced the transitionbased dependency-to-AMR paradigm we follow. We initialise the main algorithm with a stack of the nodes in the dependency tree, root node first.",
  "y": "differences"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_3",
  "x": "**ACTION SPACE** Flanigan et al. (2014) and <cite>Wang et al. (2015b)</cite> , both use AMR fragments as their smallest unit, which may consist of more than one AMR concept. Instead, we always work with the individual AMR nodes, and rely on Insert actions to learn how to build common fragments, such as country names.",
  "y": "differences"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_4",
  "x": "ReplaceHead covers two distinct actions in <cite>Wang et al. (2015b)</cite> ; ReplaceHead and Merge. Their Merge action merges \u03c3 0 and \u03b2 0 into a composite node; this is not required without composite nodes and retention of a 1:1 mapping between nodes and AMR concept. Unlike <cite>Wang et al. (2015b)</cite> we do not parameterise Swap or Reattach actions with a label.",
  "y": "background"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_5",
  "x": "ReplaceHead covers two distinct actions in <cite>Wang et al. (2015b)</cite> ; ReplaceHead and Merge. Their Merge action merges \u03c3 0 and \u03b2 0 into a composite node; this is not required without composite nodes and retention of a 1:1 mapping between nodes and AMR concept. Unlike <cite>Wang et al. (2015b)</cite> we do not parameterise Swap or Reattach actions with a label.",
  "y": "differences"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_6",
  "x": "<cite>Wang et al. (2015b)</cite> use all AMR concepts and relations that appear in the training set as possible parameters (l c and l r ) if they appear in any sentence containing the same lemma as \u03c3 0 and \u03b2. We reduce this to just concepts that have been aligned to the current lemma. We initially run the expert policy over the training set, and track the AMR concept assigned for each lemma.",
  "y": "differences"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_7",
  "x": "All features used are detailed in Table 2 , largely based on <cite>Wang et al. (2015b)</cite> . All are 0-1 indicator functions. inserted is 1 if the node was inserted by the parser; dl is the dependency label in the original dependency tree; ner the named entity tag; POS the part-of-speech tag; prefix is the string before the hyphen if word is hyphenated; suffix is the string after the hyphen; brown is the 100-class Brown cluster id with cuts at 4, 6, 10 and 20 2 ; deleted is the lemma of any child node previously deleted by the parser; merged is the lemma of any node merged into this node by a ReplaceHead action; distance is the distance between the tokens in the sentence; path concatenates lemmas and dls between the tokens in the dependency tree; POSpath concatenates POS tags between the tokens; NERpath concatenates NER tags between the tokens.",
  "y": "similarities uses"
 },
 {
  "id": "d8a250a1a0495ee824837839b74f26_8",
  "x": "The key differences to <cite>Wang et al. (2015b)</cite> are the inclusion of the brown, POSpath, NERpath, prefix and suffix feature types. ---------------------------------- **PRE-PROCESSING**",
  "y": "extends differences"
 },
 {
  "id": "d9567072d2df6c0010b32e1d1eb676_0",
  "x": "This has led to several attempts to use GANs for text generation, with a generator using either a recurrent neural network (RNN) Guo et al., 2017;<cite> Press et al., 2017</cite>; Rajeswar et al., 2017) , or a Convolutional Neural Network (CNN) (Gulrajani et al., 2017; Rajeswar et al., 2017) . However, evaluating GANs is more difficult than evaluating LMs. While in language modeling, evaluation is based on the log-probability of a model on held-out text, this cannot be straightforwardly extended to GAN-based text generation, because the generator outputs discrete tokens, rather than a probability distribution.",
  "y": "background"
 },
 {
  "id": "d9567072d2df6c0010b32e1d1eb676_1",
  "x": "We leverage this characteristic in our approximation model ( \u00a74.1). A main challenge in applying GANs for text is that generating discrete symbols is a nondifferentiable operation. One solution is to perform a continuous relaxation of the GAN output, which leads to generators that emit a nearly discrete continuous distribution<cite> (Press et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "d9567072d2df6c0010b32e1d1eb676_2",
  "x": "Consequently, other metrics have been proposed: \u2022 N-gram overlap:<cite> Press et al., 2017)</cite> : Inspired by BLEU (Papineni et al., 2002) , this measures whether n-grams generated by the model appear in a held-out corpus. A major drawback is that this metric favors conservative models that always generate very common text (e.g., \"it is\").",
  "y": "background"
 },
 {
  "id": "d9567072d2df6c0010b32e1d1eb676_3",
  "x": "The inputs to an RNN at time step t, are the state vector h t and the current input token x t . The output token (one-hot) is denoted by o t . In RNNbased GANs, the previous output token is used at inference time as the input x t Guo et al., 2017;<cite> Press et al., 2017</cite>; Rajeswar et al., 2017) .",
  "y": "background"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_0",
  "x": "This then allows the device to record and transmit a limited segment of relevant speech only, obviating the need to be always-listening. Specifically, the task of keyword spotting (KWS) is to detect the presence of pre-specified phrases in a stream of audio, often with the end goal of wake-word detection or simple command recognition on device. Currently, state of the art uses lightweight neural networks [1, 2,<cite> 3,</cite> 4] , which can perform inference in real-time even on low-end devices [4, 5] .",
  "y": "background"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_1",
  "x": "Fortunately, in recent years, the art of compressing neural networks has made significant advances in both general [6, 7, 8] and keyword spotting literature [4, 9] . On our task, we demonstrate that network slimming [6] is a simple yet highly effective method to achieve low latency with minimal impact on accuracy. Thus, our main contributions are as follows: first, we develop a novel web application with an in-browser KWS system based on previous state-of-the-art<cite> [3]</cite> models.",
  "y": "background"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_2",
  "x": "Keyword spotting. KWS is the task of detecting a spoken phrase in audio, applicable to simple command recognition <cite>[3,</cite> 10] and wake-word detection [2, 1] . A typical requirement is that such a KWS system must be small-footprint at inference time, since the target platforms are mobile phones, Internet-of-things (IoT) devices, and other portable electronics.",
  "y": "background"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_3",
  "x": "A typical requirement is that such a KWS system must be small-footprint at inference time, since the target platforms are mobile phones, Internet-of-things (IoT) devices, and other portable electronics. To achieve this goal, resource-efficient architectures using convolutional neural networks (CNNs) <cite>[3,</cite> 1] and recurrent neural networks (RNNs) [2] have been proposed, while other works make use of low-bitwidth weights [4, 9] . However, despite the pervasiveness of modern web browsers in devices from smartphones to desktops, and in spite of the availability of JavaScript-based deep learning toolkits, implementing on-device KWS systems in web applications has never been done before.",
  "y": "background"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_4",
  "x": "To compare with past work<cite> [3]</cite> , we pick the following twelve classes: \"yes,\" \"no,\" \"stop,\" \"go,\" \"left,\" \"right,\" \"on,\" \"off,\" unknown, and silence. It contains roughly 2,000 examples per class, including a few background noise samples of both man-made and artificial noise, e.g., washing dishes and white noise. As is standard in speech processing literature, all audio is in 16-bit PCM, 16kHz mono-channel WAV format.",
  "y": "similarities uses"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_5",
  "x": "---------------------------------- **DATA AND IMPLEMENTATION** For consistency with past results <cite>[3,</cite> 5] , we train our models on the first version of the Google Speech Commands dataset [10] , which comprises a total of 65,000 spoken utterances for 30 short, one-second phrases.",
  "y": "similarities uses"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_6",
  "x": "As is standard in speech processing literature, all audio is in 16-bit PCM, 16kHz mono-channel WAV format. We use the standard 80%, 10%, and 10% splits for the training, validation, and test sets, respectively <cite>[3,</cite> 10] . ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "d9aa77a03ff98cae29701eddb414d3_7",
  "x": "---------------------------------- **MODEL ARCHITECTURE** We use the res8 and res8-narrow architectures from Tang and Lin<cite> [3]</cite> as a starting point, which represent prior state of the art in residual CNNs [13] for KWS.",
  "y": "similarities uses"
 },
 {
  "id": "d9d5fce2b33c15bf073a5840930be1_1",
  "x": "One of the features of BioSimplify is avoidance of domainspecific rules. For example, we don't replace entity names (like genes) with shorter alternatives as is done with the noun phrases in the present version and with the gene names in our earlier version <cite>11</cite> . We also avoided hard-coding the words in the rules created to split sentences with relative clauses.",
  "y": "differences"
 },
 {
  "id": "d9d5fce2b33c15bf073a5840930be1_2",
  "x": "We also compare the present version of BioSimplify with the older version <cite>11</cite> which is limited in its functionality because it only implements the rules described by Siddharthan 4 . The present version which has an average time complexity of O(nlog(n)*R) is faster than the older version which has a time complexity of O(n 3 *R), where n is the number of tokens in the sentence and R is the number of rules. The older version has domain specific optimizations (like replacing the gene names with single-word identifiers), which were not used in the newer version for portability.",
  "y": "uses"
 },
 {
  "id": "d9d5fce2b33c15bf073a5840930be1_3",
  "x": "We also compare the present version of BioSimplify with the older version <cite>11</cite> which is limited in its functionality because it only implements the rules described by Siddharthan 4 . The present version which has an average time complexity of O(nlog(n)*R) is faster than the older version which has a time complexity of O(n 3 *R), where n is the number of tokens in the sentence and R is the number of rules. The older version has domain specific optimizations (like replacing the gene names with single-word identifiers), which were not used in the newer version for portability.",
  "y": "differences"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_0",
  "x": "After training the network, the attention mechanism enables the system to perform translations that can handle issues such as the movement of words and phrases, and fertility. However, even with these attention mechanisms, NMT models have their drawbacks, which include long training time and high computational requirements. Recent papers <cite>[3]</cite> , [4] in neural machine translation have proposed the strict use of attention mechanisms in networks such as the Transformer over previous approaches such as recurrent neural networks (RNNs) [5] and convolutional neural networks (CNNs) [6] .",
  "y": "background"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_1",
  "x": "Facebook's end-to-end translation approach [10] depends entirely on CNNs with attention mechanism. Our work reported in this paper is based on another translation work by Google. Google's Vaswani et al. <cite>[3]</cite> proposed the reduction in the sequential steps seen in CNNs and RNNs.",
  "y": "background"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_2",
  "x": "The Transformer architectures proposed by Vaswani et al. <cite>[3]</cite> , seen in Figure 1 , inspires this paper's work. We have made modifications to this architecture, to make it more efficient. However, our modifications can be applied to any encoder-decoder based model and is architecture-agnostic.",
  "y": "similarities"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_3",
  "x": "Around each of the main sub-layers, a skip or residual connection [13] is also used. This same structure is used in the decoder with an attention mask to avoid attending to subsequent positions. The attention mechanism used by Vaswani et al. <cite>[3]</cite> can be thought of as a function that maps a query and set of keyvalue pairs to an output.",
  "y": "background"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_4",
  "x": "---------------------------------- **A. PARALLEL ENCODING BRANCHES** A motivation for creating the Transformer model was the sluggish training and generation times of other common sequence-to-sequence models such as RNNs and CNNs <cite>[3]</cite> .",
  "y": "background"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_5",
  "x": "All proposed architectures including the base Transformer model <cite>[3]</cite> are trained over the International Workshop on Spoken Language Translation (IWSLT) 2016 corpus and tested similarly over the IWSLT 2014 test corpus [15] . The training corpus includes over 200,000 parallel sentence pairs, and 4 million tokens for each language. The testing set contains 1,250 sentences, and 20-30 thousand tokens for French and German.",
  "y": "background"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_6",
  "x": "---------------------------------- **B. MACHINE TRANSLATION** On the much larger WMT English-German test set, all our models achieve better results then Vaswani et al. <cite>[3]</cite> .",
  "y": "differences"
 },
 {
  "id": "db42e01dbc86b77335a0e488ff85e2_7",
  "x": "Our model with five parallel encoding branches has a BLEU score of 62.69 compared to 60.95 and 61.00 for the two Transformers shown in Table III . Our approach also takes considerably less time than the large Transformer model with a stack of eight encoder attention heads, although it is a little slower than the smaller Transformer model reported by Vaswani et al. <cite>[3]</cite> . In terms of the BLEU metric, we establish state-of-the-art performance for both EN-DE and EN-FR translation considering the IWSLT 2014, and comparable results for the WMT data sets.",
  "y": "differences"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_0",
  "x": "However, the silver lining is that the peer review system is evolving with the likes of OpenReviews 2 , author response periods/rebuttals, increased effective communications between authors and reviewers, open access initiatives, peer review workshops, review forms with objective questionnaires, etc. gaining momentum. The PeerRead dataset<cite> (Kang et al., 2018)</cite> is an excellent resource towards research and study on this very impactful and crucial problem.",
  "y": "background"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_1",
  "x": "Wang and Wan (2018) explored a multi-instance learning framework for sentiment analysis from the peer review texts. We carry our current investigations on a portion of the recently released PeerRead dataset<cite> (Kang et al., 2018)</cite> . Study towards automated support for peer review was otherwise not possible due to the lack of rejected paper instances and corresponding reviews.",
  "y": "uses"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_2",
  "x": "We carry our current investigations on a portion of the recently released PeerRead dataset<cite> (Kang et al., 2018)</cite> . Study towards automated support for peer review was otherwise not possible due to the lack of rejected paper instances and corresponding reviews. Our approach achieves significant performance improvement over the two tasks defined in<cite> Kang et al. (2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_3",
  "x": "We could not consider NIPS and arXiv portions of PeerRead due to the lack of aspect scores and reviews, respectively. For more details on the dataset creation and the task, we request the readers to refer to<cite> Kang et al. (2018)</cite> . We further use the submissions of ICLR 2018, corresponding reviews and aspect scores to boost our training set for the decision prediction task.",
  "y": "background"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_4",
  "x": "We further use the submissions of ICLR 2018, corresponding reviews and aspect scores to boost our training set for the decision prediction task. One motivation of our work stems from the finding that aspect scores for certain factors like Impact, Originality, Soundness/Correctness which are seemingly central to the merit of the paper, often have very low correlation with the final recommendation made by the reviewers as is made evident in<cite> Kang et al. (2018)</cite> . However, from the heatmap in Figure 1 we can see that the reviewer's sentiments (compound/positive) embedded within the review texts have visible correlations with the aspects like Recommendation, Appropriateness and Overall Decision.",
  "y": "motivation"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_5",
  "x": "---------------------------------- **MULTI-LAYER PERCEPTRON** We employ a Multi-Layer Perceptron (MLP Predict) to take the joint paper+review representations x pr as input to get the final<cite> (Kang et al., 2018)</cite> , RMSE\u2192Root Mean Squared Error.",
  "y": "uses"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_6",
  "x": "**MULTI-LAYER PERCEPTRON** We employ a Multi-Layer Perceptron (MLP Predict) to take the joint paper+review representations x pr as input to get the final<cite> (Kang et al., 2018)</cite> , RMSE\u2192Root Mean Squared Error. CNN variant as in<cite> (Kang et al., 2018</cite> ) is used as the comparing system.",
  "y": "uses"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_7",
  "x": "As we mention earlier, we undertake two tasks: Task 1: Predicting the overall recommendation score (Regression) and Task 2: Predicting the Accept/Reject Decision (Classification). To compare with<cite> Kang et al. (2018)</cite> , we keep the experimental setup (train vs test ratio) identical and re-implement their codes to generate the comparing figures. However,<cite> Kang et al. (2018)</cite> performed Task 2 on ICLR 2017 dataset with handcrafted features, and Task 1 in a deep learning setting.",
  "y": "uses"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_8",
  "x": "However,<cite> Kang et al. (2018)</cite> performed Task 2 on ICLR 2017 dataset with handcrafted features, and Task 1 in a deep learning setting. Since our approach is a deep neural network based, we crawl additional paper+reviews from ICLR 2018 to boost the training set. For Task 1, n 1 is 666 and n 2 is 98 while for Task 2, n 1 is 1494 and n 2 is 525.",
  "y": "extends differences"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_9",
  "x": "For Task 1, F is 256, l is 5. ReLU is the non-linear function g(), learning rate is 0.007. We train the model with SGD optimizer, set momentum as 0.9<cite> (Kang et al., 2018</cite> ) is feature-based and considers only paper, and not the reviews. and batch size as 32.",
  "y": "uses"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_10",
  "x": "For Task 1, we can see that our review sentiment augmented approach outperforms the baselines and the comparing systems by a wide margin (\u223c 29% reduction in error) on the ICLR 2017 dataset. With only using review+sentiment information, we are still able to outperform<cite> Kang et al. (2018)</cite> by a margin of 11% in terms of RMSE. A further relative error reduction of 19% with the addition of paper features strongly suggests that only review is not sufficient for the final recommendation.",
  "y": "differences"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_11",
  "x": "**RESULTS AND ANALYSIS** For Task 2, we observe that the handcrafted feature-based system by<cite> Kang et al. (2018)</cite> performs inferior compared to the baselines. This is because the features were very naive and did not 4 https://github.com/aritzzz/DeepSentiPeer address the complexity involved in such a task.",
  "y": "differences"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_12",
  "x": "Here we observe a relative error reduction of 4.8% and 14.5% over the comparing system for ACL 2017 and CoNLL 2016, respectively (Table 2 ). For the decision prediction task, the comparing system performs even worse, and we outperform them by a considerable margin of 28% (ACL 2017) and 26% (CoNLL 2017), respectively ( Table 3 ). The reason is that the work reported in<cite> Kang et al. (2018)</cite> relies on elementary handcrafted features extracted only from the paper; does not consider the review features whereas we include the review features along with the sentiment information in our deep neural architecture.",
  "y": "differences"
 },
 {
  "id": "dbb0178b572c2a451853737910ac86_13",
  "x": "For the decision prediction task, the comparing system performs even worse, and we outperform them by a considerable margin of 28% (ACL 2017) and 26% (CoNLL 2017), respectively ( Table 3 ). The reason is that the work reported in<cite> Kang et al. (2018)</cite> relies on elementary handcrafted features extracted only from the paper; does not consider the review features whereas we include the review features along with the sentiment information in our deep neural architecture. However, we also find that our approach with only Review+Sentiment performs inferior to the Paper+Review method in<cite> Kang et al. (2018)</cite> for ACL 2017.",
  "y": "differences"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_0",
  "x": "Moreover, it is not straightforward to construct a single vector that properly represents the \"semantics\" of the ar-guments. As a result, neural network models that use dense vectors have been shown to have inferior performance against traditional systems that use manually crafted features, unless the dense vectors are combined with the hand-crafted surface features <cite>(Ji and Eisenstein, 2015)</cite> . In this work, we explore multiple neural architectures in an attempt to find the best distributed representation and neural network architecture suitable for this task in both English and Chinese.",
  "y": "background"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_1",
  "x": "The prevailing approach for this task is to use surface features derived from various semantic lexicons (Pitler et al., 2009) , reducing the number of parameters by mapping raw word tokens in the arguments of discourse relations to a limited number of entries in a semantic lexicon such as polarity and verb classes. Along the same vein, Brown cluster assignments have also been used as a general purpose lexicon that requires no human manual annotation (Rutherford and Xue, 2014) . However, these solutions still suffer from the data sparsity problem and almost always require extensive feature selection to work well (Park and Cardie, 2012; Lin et al., 2009;<cite> Ji and Eisenstein, 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_2",
  "x": "For a fair comparison with the sequential model, we apply the same formulation of LSTM on the binarized constituent parse tree. The hidden state vector now corresponds to a constituent in the tree. These hidden state vectors are then used in the same fashion as the sequential LSTM. The mathematical formulation is the same as Tai et al. (2015) . This model is similar to the recursive neural networks proposed by<cite> Ji and Eisenstein (2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_4",
  "x": "Each relation consists of two spans of text that are minimally required to infer the relation, and the sense is organized hierarchically. The classification problem can be formulated in various ways based on the hierarchy. Previous work in this task has been done over three schemes of evaluation: top-level 4-way classification (Pitler et al., 2009 ), second-level 11-way classification (Lin et al., 2009;<cite> Ji and Eisenstein, 2015)</cite> , and modified second-level classification introduced in the CoNLL 2015 Shared Task .",
  "y": "background"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_5",
  "x": "The data split and the label set are exactly the same as previous works that use this label set (Lin et al., 2009;<cite> Ji and Eisenstein, 2015)</cite> . Preprocessing All tokenization is taken from the gold standard tokenization in the PTB (Marcus et al., 1993) . We use the Berkeley parser to parse all of the data (Petrov et al., 2006 too little data, 50-dimensional WSJ-trained word vectors have previously been shown to be the most effective in this task <cite>(Ji and Eisenstein, 2015)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_6",
  "x": "We use the Berkeley parser to parse all of the data (Petrov et al., 2006 too little data, 50-dimensional WSJ-trained word vectors have previously been shown to be the most effective in this task <cite>(Ji and Eisenstein, 2015)</cite> . Additionally, we also test the off-the-shelf word vectors trained on billions of tokens from Google News data freely available with the word2vec tool. All word vectors are trained on the Skipgram architecture (Mikolov et al., 2013b; Mikolov et al., 2013a) .",
  "y": "background"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_7",
  "x": "The feedforward model performs best overall among all of the neural architectures we explore (Table 2) . It outperforms the recursive neural network with bilinear output layer introduced by<cite> Ji and Eisenstein (2015)</cite> (p < 0.05; bootstrap test) and performs comparably with the surface feature baseline (Lin et al., 2009) , which uses various lexical and syntactic features and extensive feature selection. Tree LSTM achieves inferior accuracy than our best feedforward model.",
  "y": "differences"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_9",
  "x": "Tree LSTM seems to show improvement when there is a need to model longdistance dependency in the data (Tai et al., 2015; Li et al., 2015) . Furthermore, the benefits of tree LSTM are not readily apparent for a model that discards the syntactic categories in the intermediate nodes and makes no distinction between heads and their dependents, which are at the core of syntactic representations. Another point of contrast between our work and<cite> Ji and Eisenstein's (2015)</cite> is the modeling choice for inter-argument interaction.",
  "y": "differences"
 },
 {
  "id": "dbe1f1bdf7d94824f6f7cd176a4f6d_10",
  "x": "Linear interaction allows us to stack up hidden layers without the exponential growth in the number of parameters. Secondly, using linear interaction allows us to use high dimensional word vectors, which we found to be another important component for the performance. The recursive model by<cite> Ji and Eisenstein (2015)</cite> is limited to 50 units due to the bilinear layer.",
  "y": "differences"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_0",
  "x": "The research community in artificial intelligence (AI) has witnessed a series of dramatic advances in the AI tasks concerning language and vision in recent years, thanks to the successful applications of deep learning techniques, particularly convolutional neural networks (CNN) and recurrent neural networks (RNN). AI has moved on from naming the entities in the image (Mei et al. 2008; Wang et al. 2009) , to describing the image with a natural sentence (Vinyals et al. 2015; Xu et al. 2015; Karpathy and Li 2015) and then to answering specific questions about the image with the advent of visual question answering (VQA) task <cite>(Antol et al. 2015)</cite> . However, current VQA task is focused on generating a short answer, mostly single words, which does not fully take advantage of the wide range of expressibility inherent in human natural language.",
  "y": "background"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_1",
  "x": "A number of datasets on visual question answering have been introduced in recent years (Malinowski and Fritz 2014; Ren, Kiros, and Zemel 2015) , among which <cite>(Antol et al. 2015)</cite> in particular has gained the most attention and helped popularize the task. However, these datasets mostly consist of a small set of answers covering most of the questions, and most of the answers being single word. Our FSVQA dataset, derived from <cite>(Antol et al. 2015)</cite> , minimizes such limitation by converting the answers to full-sentences, thus widely expanding the set of answers.",
  "y": "background"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_2",
  "x": "A number of datasets on visual question answering have been introduced in recent years (Malinowski and Fritz 2014; Ren, Kiros, and Zemel 2015) , among which <cite>(Antol et al. 2015)</cite> in particular has gained the most attention and helped popularize the task. However, these datasets mostly consist of a small set of answers covering most of the questions, and most of the answers being single word. Our FSVQA dataset, derived from <cite>(Antol et al. 2015)</cite> , minimizes such limitation by converting the answers to full-sentences, thus widely expanding the set of answers.",
  "y": "motivation background"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_3",
  "x": "Our FSVQA dataset, derived from <cite>(Antol et al. 2015)</cite> , minimizes such limitation by converting the answers to full-sentences, thus widely expanding the set of answers. (Fukui et al. 2016) proposed multimodal compact bilinear pooling (MCB) to combine multimodal features of visual and text representations. This approach won the 1st place in 2016 VQA Challenge in real images category.",
  "y": "extends"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_4",
  "x": "We circumvent this financial cost by converting the answers in the original VQA dataset <cite>(Antol et al. 2015)</cite> to full-sentence answers by applying a number of linguistic rules using natural language processing techniques. Furthermore, we also provide an augmented version of dataset by converting the human-written captions provided in the MS COCO (Lin et al. 2014) . We generated questions with a set of rules, for which the caption itself becomes the answer.",
  "y": "extends"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_5",
  "x": "Following <cite>(Antol et al. 2015)</cite> , we examined the effect of Conversely, we also examined an approach where only image features are concerned. This requires a slightly different training procedure, as it does not involve a series of onehot vector inputs. We followed the conventional approach used in image captioning task (Vinyals et al. 2015) , where the image features are fixed, and a stack of LSTM units learns to generate the ground truth captions.",
  "y": "uses"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_6",
  "x": "This tendency is consistent with the results reported in <cite>(Antol et al. 2015)</cite> . It must nevertheless be reminded that the best performances in both <cite>(Antol et al. 2015)</cite> and our experiment were achieved with the presence of both visual and textual clues. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "dcfd8cb0179ab156a6ffcab3358a45_7",
  "x": "This tendency is consistent with the results reported in <cite>(Antol et al. 2015)</cite> . It must nevertheless be reminded that the best performances in both <cite>(Antol et al. 2015)</cite> and our experiment were achieved with the presence of both visual and textual clues. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_0",
  "x": "However human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of output translation. In this paper we leverage the Machine Translation of Noisy Text (MTNT) dataset<cite> (Michel and Neubig, 2018)</cite> to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system resilient to naturally occurring noise and partially mitigate loss in accuracy resulting therefrom.",
  "y": "uses similarities"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_1",
  "x": "Machine Translation (MT) systems have been shown to exhibit severely degraded performance when presented with translation of out-of-domain or noisy data (Luong and Manning, 2015; Sakaguchi et al., 2016; Belinkov and Bisk, 2017) . This is particularly pronounced in systems trained on clean, formalized parallel data such as Europarl (Koehn, 2005) , are tasked with translation of unedited, human generated text such as is common in domains such as social media, where accurate translation is becoming of widespread relevance<cite> (Michel and Neubig, 2018)</cite> . Improving the robustness of MT systems to naturally occurring noise presents an important and interesting task.",
  "y": "background"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_2",
  "x": "Can we artificially synthesize the types of noise common to social media text in otherwise clean data? 2. Are we able to improve the performance of vanilla MT systems on noisy data by leveraging artificially generated noise? In this work we present two primary methods of synthesizing natural noise in accordance with the types of noise identified in prior work (Eisenstein, 2013;<cite> Michel and Neubig, 2018)</cite> as naturally occurring in internet and social media based text.",
  "y": "background"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_3",
  "x": "In this work we present two primary methods of synthesizing natural noise in accordance with the types of noise identified in prior work (Eisenstein, 2013;<cite> Michel and Neubig, 2018)</cite> as naturally occurring in internet and social media based text. We present a series of experiments based on the Machine Translation of Noisy Text (MTNT) data set <cite>(Michel and Neubig, 2018</cite> ) through which we demonstrate improved resilience of a vanilla MT system by adaptation using artificially noised data. The primary contributions of this work are our Synthetic Noise Induction model which specifically introduces types of noise unique to social media text and the introduction of back translation (Sennrich et al., 2015a) as a means of emulating target noise.",
  "y": "uses"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_4",
  "x": "**RELATED WORK** Human generated text on the internet and social media are a particularly rich source of natural noise (Eisenstein, 2013; Baldwin et al., 2015) which causes pronounced problems for MT<cite> (Michel and Neubig, 2018)</cite> . Robustness to noise in MT can be treated as a domain adaptation problem (Koehn and Knowles, 2017) and several attempts have been made to handle noise from this perspective.",
  "y": "background"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_5",
  "x": "The hidden and embedding sizes are set to 256 and 512, respectively. We also employ weighttying (Press and Wolf, 2016) between the embedding layer and projection layer of the decoder. For expediency and convenience of experimentation we have chosen to deploy a smaller, faster variant of the model used in<cite> Michel and Neubig (2018)</cite> , which allows us to provide comparative results across a variety of settings.",
  "y": "extends differences"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_6",
  "x": "Other model parameters reflect the implementation outlined in<cite> Michel and Neubig (2018)</cite> . In all experimental settings we employ Byte-Pair Encoding (BPE) (Sennrich et al., 2015b) using Google's SentencePiece 2 . ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "dd603c79f87e98d23f6f8e13028ae9_7",
  "x": "For this method, we inject artificial noise in the clean data according to the distribution of types of noise in MTNT specified in<cite> Michel and Neubig (2018)</cite> . For every token we choose to introduce the different types of noise with some probability on both French and English sides in 100k sentences of EP. Specifically, we fix the probabilities of error types as follows: spelling (0.04), profanity (0.007), grammar (0.015) and emoticons (0.002).",
  "y": "similarities uses"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_0",
  "x": "Qualia Structures have been originally introduced by <cite>(Pustejovsky, 1991)</cite> and are used for a variety of purposes in Natural Language processing such as the analysis of compounds (Johnston and Busa, 1996) , co-composition and coercion <cite>(Pustejovsky, 1991)</cite> as well as for bridging reference resolution (Bos et al., 1995) . Further, it has also been argued that qualia structures and lexical semantic relations in general have applications in information retrieval (Voorhees, 1994; Pustejovsky et al., 1993) . One major bottleneck however is that currently Qualia Structures need to be created by hand, which is probably also the reason why there are no practical system using qualia structures, but a lot of systems using globally available resources such as WordNet (Fellbaum, 1998) or FrameNet 1 1 http://framenet.icsi.berkeley.edu/ as source of lexical/world knowledge.",
  "y": "background"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_1",
  "x": "**QUALIA STRUCTURES** According to Aristotle, there are four basic factors or causes by which the nature of an object can be described (cf. (Kronlid, 2003) ): the material cause, i.e. the material an object is made of the agentive cause, i.e. the source of movement, creation or change the formal cause, i.e. its form or type the final cause, i.e. its purpose, intention or aim In his Generative Lexicon (GL) framework <cite>(Pustejovsky, 1991)</cite> reused Aristotle's basic factors for the description of the meaning of lexical elements. In fact he introduced so called Qualia Structures by which the meaning of a lexical element is described in terms of four roles:",
  "y": "background"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_2",
  "x": "Agentive: describing factors involved in the bringing about of an object, i.e. its creator or the causal chain leading to its creation Formal: describing that properties which distinguish an object in a larger domain, i.e. orientation, magnitude, shape and dimensionality Telic: describing the purpose or function of an object Most of the qualia structures used in <cite>(Pustejovsky, 1991)</cite> however seem to have a more restricted interpretation. In fact, in most examples the Constitutive role seems to describe the parts or components of an object, while the Agentive role is typically described by a verb denoting an action which typically brings the object in question into existence. The Formal role normally consists in typing information about the object, i.e. its hypernym or superconcept.",
  "y": "background"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_3",
  "x": "In general it is important to mention that by this approach we are not able to detect and separate multiple meanings of words, i.e. to handle polysemy, which is appropriately accounted for in the framework of the Generative Lexicon <cite>(Pustejovsky, 1991)</cite> . ---------------------------------- **THE FORMAL ROLE**",
  "y": "extends differences"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_4",
  "x": "If this value is over a threshold (0.0005 in our case), we assume that it is a valid filler of the Agentive qualia role. Busa, 1996) or <cite>(Pustejovsky, 1991)</cite> , as well as computer, an abstract noun, i.e. conversation, as well as two very specific multi-term words, i.e. natural language processing and data mining. We give the automatically learned weighted Qualia Structures for these entries in Figures 3,  4 , 5 and 6.",
  "y": "background"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_5",
  "x": "In particular, we discuss more in detail the qualia structure for book, knife and beer and leave the detailed assessment of the qualia structures for computer, natural language processing, data mining and conversation to the interested reader. For book, the first four candidates of the Formal role, i.e. product, item, publication and document are very appropriate, but alluding to the physical object meaning of book as opposed to the meaning in the sense of information container (compare <cite>(Pustejovsky, 1991)</cite> . As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to <cite>(Pustejovsky, 1991)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_6",
  "x": "In particular, we discuss more in detail the qualia structure for book, knife and beer and leave the detailed assessment of the qualia structures for computer, natural language processing, data mining and conversation to the interested reader. For book, the first four candidates of the Formal role, i.e. product, item, publication and document are very appropriate, but alluding to the physical object meaning of book as opposed to the meaning in the sense of information container (compare <cite>(Pustejovsky, 1991)</cite> . As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to <cite>(Pustejovsky, 1991)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_7",
  "x": "For the Constitutive role of book we get -besides it at the first position which could be easily filtered out -sign (2nd position), letter (3rd position) and page (6th position), which are quite appropriate. The top four candidates for the Telic role are give, select, read and purchase. It seems that give is emphasizing the role of a book as a gift, read is referring to the most obvious purpose of a book as specified in the ideal qualia structures of <cite>(Pustejovsky, 1991)</cite> as well as (Johnston and Busa, 1996) and purchase denotes the more general purpose of a book, i.e. to be bought.",
  "y": "motivation"
 },
 {
  "id": "dd875dd5c0f2558bb173f31bbdea00_8",
  "x": "Considering the results for the Formal role, the elements drink (1st), alcohol (2nd) and beverage (4th) are much more specific than liquid as given in <cite>(Pustejovsky, 1991)</cite> , while thing at the 3rd position is certainly too general. Furthermore, according to the automatically learned qualia structure, beer is made of rice, malt and hop, which are perfectly reasonable results. Very interesting are the results concoction and libation for the Formal role of beer, which unfortunately were rated low by our evaluator (compare Figure 3) .",
  "y": "motivation"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_0",
  "x": "Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) is rapidly proving itself to be a strong competitor to other statistical machine translation methods. However, it still lags behind other statistical methods on very lowresource language pairs<cite> (Zoph et al., 2016</cite>; Koehn and Knowles, 2017) . A common strategy to improve learning of lowresource languages is to use resources from related languages (Nakov and Ng, 2009) .",
  "y": "motivation"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_1",
  "x": "In particular, they showed that a French-English model could be used to improve translation on a wide range of low-resource language pairs such as Hausa-, Turkish-, and Uzbek-English. In this paper, we explore the opposite scenario, where the parent language pair is also lowresource, but related to the child language pair. We show that, at least in the case of three Turkic languages (Turkish, Uzbek, and Uyghur), the original method of <cite>Zoph et al. (2016)</cite> does not always work, but it is still possible to use the parent model to considerably improve the child model.",
  "y": "uses motivation"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_2",
  "x": "However, adapting these resources is not trivial. NMT offers some simple ways of doing this. For example, <cite>Zoph et al. (2016)</cite> train a parent model on a (possibly unrelated) high-resource language pair, then use this model to initialize a child model which is further trained on a low-resource language pair.",
  "y": "background"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_3",
  "x": "We follow the transfer learning approach proposed by <cite>Zoph et al. (2016)</cite> . In their work, a parent model is first trained on a high-resource language pair. Then the child model's parameter values are copied from the parent's and are fine-tuned on its low-resource data.",
  "y": "uses"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_4",
  "x": "**METHOD** The basic idea of our method is to extend the transfer method of <cite>Zoph et al. (2016)</cite> to share the parent and child's source vocabularies, so that when source word embeddings are transferred, a word that appears in both vocabularies keeps its embedding. In order for this to work, it must be the case that the parent and child languages have considerable vocabulary overlap, and that when a word occurs in both languages, it often has a similar meaning in both languages.",
  "y": "extends uses"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_5",
  "x": "Finally, we evaluated using case-sensitive BLEU. As a baseline, we trained a child model using BPE but without transfer (that is, with weights randomly initialized). We also compared against a word-based baseline (without transfer) and two word-based systems using transfer without vocabulary-sharing, corresponding with the method of <cite>Zoph et al. (2016)</cite> ( \u00a72.2): one where the target word embeddings are fine-tuned, and one where they are frozen.",
  "y": "uses"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_6",
  "x": "**CONCLUSION** In this paper, we have shown that the transfer learning method of <cite>Zoph et al. (2016)</cite> , while appealing, might not always work in a low-resource context. However, by combining it with BPE, we can improve NMT performance on a low-resource language pair by exploiting its lexical similarity with another related, low-resource language.",
  "y": "uses"
 },
 {
  "id": "e2705ae777acffc894f7aa18d42771_7",
  "x": "**CONCLUSION** In this paper, we have shown that the transfer learning method of <cite>Zoph et al. (2016)</cite> , while appealing, might not always work in a low-resource context. However, by combining it with BPE, we can improve NMT performance on a low-resource language pair by exploiting its lexical similarity with another related, low-resource language.",
  "y": "extends"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_0",
  "x": "Recent Image Captioning models of this kind [1, 11, 12, 28] have shown impressive results, much thanks to the powerful language modelling capabilities of Long Short-Term Memory (LSTM) [15] RNNs. However, although MLE training enables models to confidently generate captions that have a high likelihood in the training set, it limits their capacity to generate novel descriptions. Their output exhibits a disproportionate replication of common n-grams and full captions seen in the training set [9, 11,<cite> 26]</cite> .",
  "y": "background"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_1",
  "x": "The subjectivity in what defines a good caption, has made it difficult to identify a single metric for the overall quality of Image Captioning models [5,<cite> 26]</cite> . Benchmarking methods from Machine Translation [3, 19, 23] have been appropriated, while other somewhat similar methods such as CIDEr [27] have been proposed specifically for assessing the quality of image captions. All these approaches unfortunately have a strong focus on replicating common n-grams from the ground-truth captions [5] and do not take into account the richness and diversity of human expression [9,<cite> 26]</cite> .",
  "y": "background"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_2",
  "x": "Benchmarking methods from Machine Translation [3, 19, 23] have been appropriated, while other somewhat similar methods such as CIDEr [27] have been proposed specifically for assessing the quality of image captions. All these approaches unfortunately have a strong focus on replicating common n-grams from the ground-truth captions [5] and do not take into account the richness and diversity of human expression [9,<cite> 26]</cite> . Moreover, it has been found that this class of metrics suffers from poor correlations with human evaluation, with CIDEr and METEOR having the highest correlations among them [5] .",
  "y": "background"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_3",
  "x": "In an effort to measure the amount of generic captions produced by various Image Captioning models, [11] explores the concept of caption diversity. More recently, this concept has been employed as the focus for training and evaluation<cite> [26,</cite> 29] , and it has been proposed that improving caption diversity leads to more human-like captions <cite>[26]</cite> . This research direction is still new and lacks clear benchmarks and standardized metrics.",
  "y": "background"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_4",
  "x": "\u2500 novelty -percentage of generated captions where exact duplicates are not found in the training set [11,<cite> 26,</cite> 29 ] \u2500 diversity -percentage of distinct captions (where duplicates count as a single distinct caption) out of the total number of generated captions [11] \u2500 vocabulary size -number of unique words used in generated captions <cite>[26]</cite> ---------------------------------- **MEANINGFUL DIVERSITY THROUGH SPECIFICITY**",
  "y": "uses"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_5",
  "x": "In [22] , the training is cooperative rather than competitive; both systems adjust to the other to provide the best joint results. We take a slightly different approach from both the joint training in [22] and recent applications of GAN training in Image Captioning [9,<cite> 26]</cite> . Instead of allowing both systems to learn from each other, we freeze the NLU side and allow only the NLG to learn from the NLU; the NLU model is pre-trained on ground-truth captions, without any input from the NLG.",
  "y": "differences"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_7",
  "x": "As can be seen in Table 1 , our models demonstrate increased diversity and novelty, outperforming previously reported results. The vocabulary size also increases but is lower than in <cite>[26]</cite> . When it comes to the specificity metrics, our contrastive models have the advantage over our non-contrastive ones.",
  "y": "differences"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_8",
  "x": "For completeness, we include the best models from [29] in Table 2 ; however, they only report diversity results on multiple (up to 10) candidates per image (where duplicates of a novel caption are counted as multiple novel captions), so they are not directly comparable to the single-best-caption models. Note that [12, 29] use different data splits, while our models and <cite>[26]</cite> use the Karpathy 5k splits [17] . Table 3 .",
  "y": "similarities uses"
 },
 {
  "id": "e3b9c00d792bcddb6eea449179e61e_10",
  "x": "Another example of GAN training is <cite>[26]</cite> where the Discriminator classifies whether a multi-sample set of captions are human-written or generated. In contrast, our evaluator only requires a single caption and uses a much simpler loss function. Furthermore, we let the NLU remain frozen during training, making the training stable and producing more informative learning curves.",
  "y": "differences"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_0",
  "x": "To help agencies monitor gang activity on social media, our past work investigated how features from Twitter profiles, including profile text, profile images, tweet text, emjoi use, and their links to YouTube, may be used to reliably find gang member profiles <cite>[BWDS16]</cite> . The diverse set of features, chosen to combat the fact that gang members often use local terms and hashtags in their posts, offered encouraging results. In this paper, we report our experience in integrating deep learning into our gang member profile classifier.",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_1",
  "x": "To help agencies monitor gang activity on social media, our past work investigated how features from Twitter profiles, including profile text, profile images, tweet text, emjoi use, and their links to YouTube, may be used to reliably find gang member profiles <cite>[BWDS16]</cite> . In this paper, we report our experience in integrating deep learning into our gang member profile classifier.",
  "y": "extends"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_2",
  "x": "These studies investigated a small set of manually curated gang member profiles, often from a small geographic area that may bias their findings. In our previous work <cite>[BWDS16]</cite> , we curated what may be the largest set of gang member profiles to study how gang member Twitter profiles can be automatically identified based on the content they share online. A data collection process involving location neutral keywords used by gang members, with an expanded search of their retweet, friends and follower networks, led to identifying 400 authentic gang member profiles on Twitter.",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_3",
  "x": "In our previous work <cite>[BWDS16]</cite> , we curated what may be the largest set of gang member profiles to study how gang member Twitter profiles can be automatically identified based on the content they share online. Our study discovered that the text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to YouTube music videos, can help a classifier distinguish between gang and non-gang member profiles. While a very promising F 1 measure with low false positive rate was achieved, we hypothesize that the diverse kinds and the multitude of features employed (e.g. unigrams of tweet text) could be amenable to an improved representation for classification. We thus explore the possibility of mapping these features into a considerably smaller feature space through the use of word embeddings.",
  "y": "extends"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_4",
  "x": "Gang member tweets and profile descriptions tend to have few textual indicators that demonstrate their gang affiliations or their tweets/profile text may carry acronyms which can only be deciphered by others involved in gang culture <cite>[BWDS16]</cite> . These gang-related terms are often local to gangs operating in neighborhoods and change rapidly when they form new gangs. Consequently, building a database of keywords, phrases, and other identifiers to find gang members nationally is not feasible.",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_5",
  "x": "An in-depth explanation of these feature selection can be found in <cite>[BWDS16]</cite> . ---------------------------------- **TWEET TEXT:**",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_6",
  "x": "**TWEET TEXT:** In our previous work, we observed that gang members use curse words nearly five times more than the average curse words use on Twitter <cite>[BWDS16]</cite> . Further, we noticed that gang members mainly use Twitter to discuss drugs and money using terms such as smoke, high, hit, money, got, and need while non-gang members mainly discuss their feelings using terms such as new, like, love, know, want, and look.",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_7",
  "x": "**YOUTUBE VIDEOS:** We found that 51.25% of the gang members in our dataset have a tweet that links to a YouTube video. Further, we found that 76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre <cite>[BWDS16]</cite> .",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_8",
  "x": "**EVALUATION SETUP** We consider a dataset of curated gang and non-gang members' Twitter profiles collected from our previous work <cite>[BWDS16]</cite> . It was developed by querying the Followerwonk Web service API 4 with location-neutral seed words known to be used by gang members across the U.S. in their Twitter profiles.",
  "y": "uses"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_9",
  "x": "It was developed by querying the Followerwonk Web service API 4 with location-neutral seed words known to be used by gang members across the U.S. in their Twitter profiles. The dataset was further expanded by examining the friends, follower, and retweet networks of the gang member profiles found by searching for seed words. Specific details about our data curation procedure are discussed in <cite>[BWDS16]</cite> .",
  "y": "background"
 },
 {
  "id": "e3c735811b2ea08d92659272ddcbdd_10",
  "x": "An open source tool of Python, Gensim [\u0158S10] was used to generate the word embeddings. We compare our results with the two best performing systems reported in <cite>[BWDS16]</cite> which are the two state-of-theart models for identifying gang members in Twitter. Both baseline models are built from a random forest classifier trained over term frequencies for unigrams in tweet text, emoji, profile data, YouTube video data and image tags.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_0",
  "x": "More recently, a number of novel insertionbased architectures have been developed for sequence generation (Gu et al., 2019;<cite> Stern et al., 2019</cite>; Welleck et al., 2019) . These frameworks license a diverse set of generation orders, including uniform (Welleck et al., 2019) , random (Gu et al., 2019) , or balanced binary trees <cite>(Stern et al., 2019)</cite> . Some of them also match the quality of state-of-the-art left-to-right models <cite>(Stern et al., 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_1",
  "x": "More recently, a number of novel insertionbased architectures have been developed for sequence generation (Gu et al., 2019;<cite> Stern et al., 2019</cite>; Welleck et al., 2019) . These frameworks license a diverse set of generation orders, including uniform (Welleck et al., 2019) , random (Gu et al., 2019) , or balanced binary trees <cite>(Stern et al., 2019)</cite> . Some of them also match the quality of state-of-the-art left-to-right models <cite>(Stern et al., 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_2",
  "x": "More recently, a number of novel insertionbased architectures have been developed for sequence generation (Gu et al., 2019;<cite> Stern et al., 2019</cite>; Welleck et al., 2019) . These frameworks license a diverse set of generation orders, including uniform (Welleck et al., 2019) , random (Gu et al., 2019) , or balanced binary trees <cite>(Stern et al., 2019)</cite> . Some of them also match the quality of state-of-the-art left-to-right models <cite>(Stern et al., 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_3",
  "x": "Some of them also match the quality of state-of-the-art left-to-right models <cite>(Stern et al., 2019)</cite> . In this paper, we utilize one such framework to explore an extensive collection of generation orders, evaluating them on the WMT'14 English-German and WMT'18 English-Chinese translation tasks. We find that a number of nonstandard choices achieve BLEU scores comparable to those obtained with the classical approach, suggesting that left-to-right generation might not be a necessary ingredient for high-quality translation.",
  "y": "similarities background"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_4",
  "x": "To address these concerns, several recent approaches have been proposed for insertion-based sequence modeling, in which sequences are con-structed by repeatedly inserting tokens at arbitrary locations in the output rather than only at the right-most position. We use one such insertion-based model, the Insertion Transformer <cite>(Stern et al., 2019)</cite> , for our empirical study. We give a brief overview of the model in this section before moving on to the details of our investigation.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_5",
  "x": "A schematic of the architecture is given in Figure 1 for reference. We note that<cite> Stern et al. (2019)</cite> also experimented with a number of other architectural variants, but we use the baseline version of the model described above in our experiments for simplicity. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_6",
  "x": "The temperature \u03c4 \u2208 (0, \u221e) controls the sharpness of the distribution, where \u03c4 \u2192 0 results in a one-hot distribution with all mass on the best-scoring action under the order function O(a), and \u03c4 \u2192 \u221e results in a uniform distribution over all valid actions. Intermediate values of \u03c4 result in distributions which are biased towards better-scoring actions but allow for other valid actions to be taken some of the time. Having defined the target distribution, we take the slot loss L for insertions within a particular slot to be the KL-divergence between the oracle distribution q oracle and the model distribution p. Substituting L in for the slot loss within the training framework of<cite> Stern et al. (2019)</cite> then gives the full sequence generation loss, which we can use to train an Insertion Transformer under any oracle policy rather than just the specific one they propose.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_7",
  "x": "**ROLL-IN POLICY** We follow<cite> Stern et al. (2019)</cite> and use a uniform roll-in policy when sampling partial outputs at training time in which we first select a subset size uniformly at random, then select a random subset of the output of that size. Repeated tokens are handled via greedy left or right alignment to the true output.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_8",
  "x": "In both cases, we train all models for 1M steps using sequencelevel knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016 ) from a base Transformer (Vaswani et al., 2017) . We perform a sweep over temperatures \u03c4 \u2208 {0.5, 1, 2} and EOS penalties \u2208 {0, 0.5, 1, 1.5, . . . , 8} <cite>(Stern et al., 2019)</cite> ----------------------------------",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_9",
  "x": "Next, we measure the quality of our models by evaluating their performance on their respective test sets. The BLEU scores are reported in Table 3 . The uniform loss proposed by<cite> Stern et al. (2019)</cite> serves as a strong baseline for both language pairs, coming within 0.6 points of the original Transformer for En-De at 26.72 BLEU, and attaining a respectable score of 33.1 BLEU on En-Zh.",
  "y": "uses"
 },
 {
  "id": "e3dd013c944cf8dcb6ff90124a0e01_10",
  "x": "As for location-based losses, the binary tree loss is notable in that it achieves the highest score across all losses for both languages. On the other hand, we note that while the soft left-to-right and right-to-left losses perform substantially better than the hard loss employed in the original work by<cite> Stern et al. (2019)</cite> , performance does suffer when using parallel decoding for those models, which is generally untrue of the other orderings. We believe this is due in part to exposure bias issues arising from the monotonic ordering as compared with the uniform roll-in policy that are not shared by the other losses.",
  "y": "differences"
 },
 {
  "id": "e59bd02bb560d80ce08dfcd6b35317_0",
  "x": "We then test the robustness of a popular topic modelling algorithm, Latent Dirichlet Allocation (LDA) using a topic stability measure introduced by <cite>Greene et al. (2014)</cite> over a variety of corpora. ---------------------------------- **TOPIC MODELLING AND METRICS**",
  "y": "uses"
 },
 {
  "id": "e59bd02bb560d80ce08dfcd6b35317_1",
  "x": "For the evaluation of topic models, we follow the approach by <cite>Greene et al. (2014)</cite> for measuring topic model agreement . We can denote a topic list as S = {R 1 , ..., R k }, where R i is a topic with rank i. An individual topic can be described as R = {T 1 , ..., T m }, where T l is a term with rank l belong to the topic. Jaccard index (Jaccard, 1912) compares the number of identical items in two sets, but it neglects ranking order.",
  "y": "uses"
 },
 {
  "id": "e59bd02bb560d80ce08dfcd6b35317_2",
  "x": "**DATASETS** In this paper, we explore two datasets bbc and wikilow<cite> (Greene et al., 2014)</cite> with different document size and corpus size. The bbc corpus includes general BBC news articles.",
  "y": "uses"
 },
 {
  "id": "e59bd02bb560d80ce08dfcd6b35317_3",
  "x": "With high level systematic errors, topic models with 3 times the correct number of topics are most robust. In some corpora, redundant number of topics helps the LDA model through severe systematic errors (Figure 1(b) ). This complements previous work by <cite>Greene et al. (2014)</cite> who investigated how topic stability is influenced by number of topics over noisefree corpora.",
  "y": "similarities"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_0",
  "x": "In one part, collective voice unifies opinions and decisions in a complex process, ideas are biased, and consequently people start acting similarly, talking similarly, and so writing similarly. Twitter conversations<cite> (Danescu-Niculescu-Mizil, Gamon, and Dumais 2011</cite>; Purohit et al. 2013 ) and popular memes (Myers and Leskovec 2012;<cite> Coscia 2013)</cite> prove this similarity in social media. In the other part, when people have a well-defined goal at the end, they tend to reshape their arguments.",
  "y": "background"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_1",
  "x": "We can argue that our opinions are biased when our decisions mostly rely on our previous knowledge, e.g., commonsense, and so richness of opinions kept in each individual is relatively unimportant. We can further argue that commonsense drives an adaptation in extracting knowledge. To measure commonsense for a particular situation is hard, however, adaptations can be easily captured in Twitter conversations<cite> (Danescu-NiculescuMizil, Gamon, and Dumais 2011</cite>; Purohit et al. 2013) , in memes (Myers and Leskovec 2012;<cite> Coscia 2013)</cite> , and faceto-face discussions<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> .",
  "y": "background"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_2",
  "x": "We can argue that our opinions are biased when our decisions mostly rely on our previous knowledge, e.g., commonsense, and so richness of opinions kept in each individual is relatively unimportant. We can further argue that commonsense drives an adaptation in extracting knowledge. To measure commonsense for a particular situation is hard, however, adaptations can be easily captured in Twitter conversations<cite> (Danescu-NiculescuMizil, Gamon, and Dumais 2011</cite>; Purohit et al. 2013) , in memes (Myers and Leskovec 2012;<cite> Coscia 2013)</cite> , and faceto-face discussions<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> .",
  "y": "uses"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_3",
  "x": "To this end, we evaluate the open access data of the United States Supreme Court (Hawes, Lin, and Resnik 2009; Hawes 2009; <cite>Danescu-Niculescu-Mizil et al. 2012</cite> ), prepare conversation groups with different adaptation levels, implement a suitable algorithm to extract linguistic relations in these group conversations, and finally provide a comparison between the groups and the discovered linguistic relations. The rest of the paper is organized as follows: the first section presents the dataset we consider and designed conversation groups out of the data; the second section describes our algorithm in detail; the following section explains how we implement pointwise mutual information for the conversation groups and then link with linguistic relations; finally, we provide experimental results and conclude the paper. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_4",
  "x": "Both the original data and the most updated version used here are publicly available<cite> (Danescu-NiculescuMizil et al. 2012)</cite> . The data gathers oral speeches before the Supreme Court and hosts 50,389 conversational exchanges among Justices and lawyers. Distinct hierarchy between Justices (high power) and lawyers (low power) impose lawyers to tune their arguments under the perspective and understandings of Justices, and as a result, speech adaptation and linguistic coordination leaves their traces in a sudden occurrence of sharing the same adverbs, conjunctions, and pronouns.",
  "y": "uses"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_5",
  "x": "Tracking initial utterances, the sides present a unique and personal speaking, but after a while in the communication, word selections, their forms, and frequencies mirror each other's language preference. The linguistic coordination is systematically quantified by<cite> (Danescu-Niculescu-Mizil et al. 2012</cite> ) and the arguments follow the principles of exchange theory examining behavior dynamics in low and high power groups (Willer 1999; Thye, Willer, and Markovsky 2006) : Lawyers tend to cooperate more to Justices than conversely and demonstrate strong linguistic coordination in their speech. Moreover, lawyers show even more cooperation to unfavorable Justices than favorable ones.",
  "y": "background"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_6",
  "x": "We borrow the textual data of the conversations in the United States Supreme Court pre-processed by (Hawes, Lin, and Resnik 2009; Hawes 2009 ) and enriched by (DanescuNiculescu-Mizil et al. 2012) including the final votes of Justices. The linguistic coordination is systematically quantified by<cite> (Danescu-Niculescu-Mizil et al. 2012</cite> ) and the arguments follow the principles of exchange theory examining behavior dynamics in low and high power groups (Willer 1999; Thye, Willer, and Markovsky 2006) : Lawyers tend to cooperate more to Justices than conversely and demonstrate strong linguistic coordination in their speech.",
  "y": "uses"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_7",
  "x": "Referring exchange theory (Willer 1999; Thye, Willer, and Markovsky 2006) and the measured coordination<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> , one can order the relative power of each Justice and lawyer pair where J and l represent Justices and lawyers, respectively (note that for comparing individually following the social exchange theory, P (J) > P (l) for both supportive and unsupported Justices). The subscript u indicates that Justice doesn't support the side of lawyer and the supportive version is described by s. For instance, in Table 1 , in the communications of 1 and 5; 4 and 6, Justices show supports and play as J s , whereas that of 3 and 5; 2 and 6, lawyers are unsupported by J u .",
  "y": "uses"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_8",
  "x": "The subscript u indicates that Justice doesn't support the side of lawyer and the supportive version is described by s. For instance, in Table 1 , in the communications of 1 and 5; 4 and 6, Justices show supports and play as J s , whereas that of 3 and 5; 2 and 6, lawyers are unsupported by J u . The scenarios and pairs guide to construct groups with different cooperation level induced by P as illustrated in Table 2 . We further add another dimension in the relative power: Winners and Losers, haven't been investigated in the previous study<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> .",
  "y": "differences"
 },
 {
  "id": "e5bff4a27468139762496abdff3436_9",
  "x": "Unlike the previous study<cite> (Danescu-Niculescu-Mizil et al. 2012)</cite> , entirely tracking back and forth utterances and proving the adaptation, e.g., linguistic coordination, by identifying the frequency of selected keywords, we directly utilize their overall conclusion and claim that linguistic relations already preserve the adaptation and any other complex collective linguistic process induced by both cooperation and competition in different power groups. We expect that the variation in M I(R, \u03ba) of gathered utterances of each relative power group, independent of the utterance order, suggests which relations can distinguish the difference in the groups and the magnitude of M I(R, \u03ba) of that difference highlights which relative power groups drastically influence the applied language. We will analyze M I(R, \u03ba) following this discussed understanding in coming Section.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_0",
  "x": "The proposed system outperforms the baseline keyword spotting model in [<cite>1</cite>] due to increased optimizability. Further, it can be more easily adapted for on-device learning applications due to reduced dependency on LVCSR. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_1",
  "x": "Following the successes in general ASR [2, 3] , the neural network based approach has been extensively explored in keyword spotting area with benefits of lowering resource requirements and improving accuracy [4, 5, 6, 7, 8, 9, 10, 11] . Such works include DNN + temporal integration [4, 5, 11, 12] , and HMM + DNN hybrid approaches [6, 7, 8, 9, 10] . Recently introduced end-to-end trainable DNN approaches [<cite>1,</cite> 13] further improved accuracy and lowered resource requirements using highly optimizable system design.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_2",
  "x": "In general, training of such DNN based systems required framelevel labels generated by LVCSR systems [14, <cite>1</cite>] . These approaches make end-to-end optimizable keyword spotting system depend on labels generated from non-end-to-end system trained for a different task. However, for keyword-spotting, the exact position of the keyword is not as relevant as its presence.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_3",
  "x": "Therefore, such strict dependency on frame-level labels may limit further optimization promised by the end-to-end approach. In [<cite>1</cite>] , the top level loss is derived by integrating frame-level losses, which are computed using frame-level labels from LVCSR. Integrating frame-level losses penalizes slightly mis-aligned correct predictions, which can limit detection accuracy, especially for difficult data (e.g. noisy or accented speech) where LVCSR labels may have higher-than-normal uncertainty.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_4",
  "x": "---------------------------------- **SMOOTHED MAX POOLING LOSS FOR TRAINING ENCODER/DECODER KEYWORD SPOTTING MODEL** The proposed model uses the same encoder/decoder structure as [<cite>1</cite>] ( Fig.1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss.",
  "y": "differences uses similarities"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_5",
  "x": "The proposed model uses the same encoder/decoder structure as [<cite>1</cite>] ( Fig.1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss. In [<cite>1</cite>] , both encoder and decoder models are trained with cross entropy (CE) loss using frame level labels. In the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_6",
  "x": "In the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously. The proposed smoothed max pooling loss doesnt strictly depend on phoneme-level alignment, allowing better optimization than the <cite>baseline</cite>. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_7",
  "x": "Both the <cite>baseline</cite> and the proposed model have an encoder which takes spectral domain feature Xt as input and generate (K+1) outputs Y E corresponding to phoneme-like sound units (Fig.1 ). The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword . The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner.",
  "y": "similarities"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_8",
  "x": "The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner. In [<cite>1</cite>] , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen. In [<cite>1</cite>] , the encoder model is trained to predict phonemelevel labels provided from LVCSR.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_9",
  "x": "In [<cite>1</cite>] , the encoder model is trained to predict phonemelevel labels provided from LVCSR. Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt\u2212C l , \u00b7 \u00b7 \u00b7 , xt, \u00b7 \u00b7 \u00b7 , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t. In [<cite>1</cite>] , target label sequence consists of intervals of repeated labels which we call runs.",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_10",
  "x": "In [<cite>1</cite>] , target label sequence consists of intervals of repeated labels which we call runs. These label runs define clearly defined intervals where a model should learn to generate strong activation in label output dimension. While such model behavior can be trained end-to-end, the labels need to be provided from a LVCSR system which is typically non-end-to-end system [2] .",
  "y": "background"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_11",
  "x": "**EXPERIMENTAL SETUP** We compare the model trained with the new smoothed max pooling loss on encoder/decoder architecture with the baseline in [<cite>1</cite>] . Both the <cite>baseline</cite> and the proposed model have the same architecture.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_12",
  "x": "We compare the model trained with the new smoothed max pooling loss on encoder/decoder architecture with the baseline in [<cite>1</cite>] . Both the <cite>baseline</cite> and the proposed model have the same architecture. Only the training losses are different.",
  "y": "similarities"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_13",
  "x": "We used the same frontend feature extract as the baseline [<cite>1</cite>] in our experiments. The front-end extracts and stacks a 40-d feature vector of log-mel filter-bank energies at each frame and stacks them to generate input feature vector Xt. Refer to [<cite>1</cite>] for further details. ----------------------------------",
  "y": "uses similarities"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_15",
  "x": "---------------------------------- **MODEL SETUP** We selected E2E 318K architecture in [<cite>1</cite>] as the baseline and use the same structure for testing all other models.",
  "y": "uses similarities"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_17",
  "x": "For detailed architectural parameters, please refer to [<cite>1</cite>] . We call the <cite>baseline model</cite> as Baseline CE CE where encoder and decoder submodels are trained with CE loss. We call the proposed model as Max4 SMP SMP where both encoder and decoder submodels are trained by SMP (smoothed max pooling) loss.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_18",
  "x": "Max3 CE SMP used <cite>baseline</cite> CE loss for encoder. Model Max4-Max7 are tested to measure the importance of the smoothing operation. MP means max pooling without smoothing (i.e. s(t) = 1).",
  "y": "uses"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_19",
  "x": "The training data consists of 2.1 million anonymized utterances with the keywords Ok Google and Hey Google. Data augmentation similar to [<cite>1</cite>] has been used for better robustness. Evaluation is done on four data sets separate from training data, representing diverse environmental conditions -Clean non-accented set contains 170K non-accented English utterances of keywords in quiet condition.",
  "y": "similarities"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_21",
  "x": "Figure 4 shows the ROC curves of Max4-Max7 models across different conditions. Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve. Max3 CE MP model also performs better than the <cite>baseline</cite> but not as good as Max4.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_22",
  "x": "Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve. Max3 CE MP model also performs better than the <cite>baseline</cite> but not as good as Max4. Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than <cite>baseline.</cite>",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_23",
  "x": "Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than <cite>baseline.</cite> Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss). Especially the proposed Max4 model reduces FR rate to nearly half of the <cite>baseline</cite> in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_24",
  "x": "We presented smoothed max pooling loss for training keyword spotting model with improved optimizability. Experiments show that the proposed approach outperforms the <cite>baseline model</cite> with CE loss by relative 22%-54% across a variety of conditions. Further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the <cite>baseline</cite>.",
  "y": "differences"
 },
 {
  "id": "e6fe4c6c32294072dbc1ee5bb0a606_25",
  "x": "We presented smoothed max pooling loss for training keyword spotting model with improved optimizability. Experiments show that the proposed approach outperforms the <cite>baseline model</cite> with CE loss by relative 22%-54% across a variety of conditions. Further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the <cite>baseline</cite>.",
  "y": "differences"
 },
 {
  "id": "e75e14ff2812f34ff456eb472a36d2_0",
  "x": "In the third part, a series of deep models including deep unfolding (Chien and Lee, 2018) , Bayesian RNN (Gal and Ghahramani, 2016; Chien and Ku, 2016) , sequence-to-sequence learning (Graves et al., 2006; Gehring et al., 2017) , CNN (<cite>Kalchbrenner et al., 2014</cite>; Xingjian et al., 2015; , GAN (Tsai and Chien, 2017) and VAE are introduced. The coffee break is arranged within this part. Next, the fourth part focuses on a variety of advanced studies which illustrate how deep Bayesian learning is developed to infer the sophisticated recurrent models for natural language understanding.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_0",
  "x": "Distributed representations of multimodal embeddings (Feng & Lapata, 2010) are receiving increasing attention recently in the machine learning literature, and techniques developed have found a wide spectrum of applications in the real world. These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary<cite> (Lazaridou et al., 2015</cite>; Glenberg & Robertson, 2000; ). As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; <cite>Lazaridou et al., 2015</cite>; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_1",
  "x": "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; <cite>Lazaridou et al., 2015</cite>; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in<cite> Lazaridou et al. (2015)</cite> sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_2",
  "x": "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; <cite>Lazaridou et al., 2015</cite>; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in<cite> Lazaridou et al. (2015)</cite> sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_3",
  "x": "Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words. As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval. In this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a; b) , , and<cite> Lazaridou et al. (2015)</cite> .",
  "y": "extends"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_4",
  "x": "In the last few years, there has been a wealth of literature on multimodal representational models. As explained in<cite> Lazaridou et al. (2015)</cite> , the majority of this literature focuses on constructing textual and visual representations independently and then combining them under some metrics. Bruni et al. (2014) utilize a direct approach to \"mixing\" the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_5",
  "x": "Other recent work has presented several methods for directly incorporating visual context in neural language models. In Xu et al. (2014) , word context is enhanced by global visual context; i.e., a single image is used as the context for the whole sentence (conversely, the sentence acts as a caption for the image). The multimodal skip-gram architecture proposed by<cite> Lazaridou et al. (2015)</cite> takes a more fine-grained approach by incorporating word-level visual context and concurrently training words to predict other text words in the window as well as their visual representation.",
  "y": "background"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_6",
  "x": "To construct the vectors for the visual representations, we follow a similar experimental set-up as that used by<cite> Lazaridou et al. (2015)</cite> . In each of the cases described above-centroid and hypersphere-, we randomly sample 100 images from the corresponding synsets of Imagenet for each visual word and use a pre-trained convolutional neural network as described in Krizhevsky et al. (2012) via the Caffe toolkit (Jia et al., 2014) to extract a 4096-dimensional vector representation of each image. We then treat the 100 vectors corresponding to each of the 5,100 visual words as clusters in the 4096-dimensional visual space.",
  "y": "similarities uses"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_7",
  "x": "We evaluate three versions of our model on these benchmarks: pseudowords using the centroid method (PSUEDOWORDS-C), pseudowords using the hypersphere method (PSEUDOWORDS-H), and the centroid method with a randomly initialized mapping (PSEUDOWORDS-RAN), as explained below. Existing Multimodal Models We compare our results on these benchmarks against previously published results for other multimodal word embeddings . Using the results published by<cite> Lazaridou et al. (2015)</cite> and a target word embedding of 300, we compare our results to their MMSKIP-GRAM-A and MMSKIP-GRAM-B, which maximize the similarity of the textual and visual representations Table 1 : Spearman correlation between the generated multimodal similarities and the benchmark human judgments.",
  "y": "uses"
 },
 {
  "id": "e7b1c00e747f5bfbb96499d7223496_8",
  "x": "On the other hand, when the mapping is quickly pretrained on existing distributed word representations, the results are greatly improved. In the cases of capturing general relatedness and pure visual similarity, the multimodal model of<cite> Lazaridou et al. (2015)</cite> performs better. However, in the case of capturing semantic word similarity, our model performs signficantly better than MMSKIP-GRAM-B (although it should be noted that these results are roughly on par with the benchmark authors (Silberer & Lapata, 2014) and a point below the non-mapping MMSKIP-GRAM-A).",
  "y": "differences"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_0",
  "x": "In this paper, we extend LSTM to tree structures, in which we learn memory cells that can reflect the history memories of multiple child cells and hence multiple descendant cells. Compared with previous recursive neural networks (<cite>Socher et al., 2013</cite>; 2012) , S-LSTM has the potentials of avoiding gradient vanishing and hence may model long-distance interaction over trees.",
  "y": "motivation differences"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_1",
  "x": "This is a desirable characteristic as many of such structures are deep. S-LSTM can be considered as bringing the merits of a recursive neural network and a recurrent neural network togetherStanford Sentiment Tree Bank (<cite>Socher et al., 2013</cite>) to determine the sentiment for different granularities of phrases in a tree. <cite>The dataset</cite> has favorable properties: in addition to being a benchmark for much previous work, <cite>it</cite> provides with human annotations at all nodes of the trees, enabling us to comprehensively explore the properties of S-LSTM.",
  "y": "motivation"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_2",
  "x": "In recent years, recursive neural networks (RvNN) have been introduced and demonstrated to achieve state-of-the-art performances on different problems such as semantic analysis in natural language processing and image segmentation (<cite>Socher et al., 2013</cite>; 2011) . <cite>These networks</cite> are defined over recursive tree structures-a tree node is a vector computed from its children. In a recursive fashion, the information from the leaf nodes of a tree and its internal nodes are combined in a bottom-up manner through the tree.",
  "y": "background"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_3",
  "x": "Nevertheless, over the often deep structures, the networks are potentially subject to the vanishing gradient problem, resulting in difficulties in leveraging long-distance dependencies in the structures. In this paper, we propose the S-LSTM model that wires memory blocks in recursive structures. We compare our model with the RvNN models presented in (<cite>Socher et al., 2013</cite>) , as we directly replaced the tensor-enhanced composition layer at each tree node with a S-LSTM memory block.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_4",
  "x": "In this paper, we propose the S-LSTM model that wires memory blocks in recursive structures. We compare our model with the RvNN models presented in (<cite>Socher et al., 2013</cite>) , as we directly replaced the tensor-enhanced composition layer at each tree node with a S-LSTM memory block. We show the advantages of our proposed model in achieving significantly better results.",
  "y": "differences"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_5",
  "x": "In this paper, we assume there are two children at each nodes, same as in (<cite>Socher et al., 2013</cite>) and therefore we use <cite>their data</cite> in our experiments. That is, we have two forget gates. Extension of the model to handle more children is rather straightforward.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_6",
  "x": "Backpropagation over structures During training, the gradient of the objective function with respect to each parameter can be calculated efficiently via backpropagation over structures (Goller & Kchler, 1996; <cite>Socher et al., 2013</cite>) . The major difference from that of (<cite>Socher et al., 2013</cite> ) is we use LSTM-like backpropagation, where unlike a regular LSTM, pass of error needs to discriminate between the left and right children, or in a topology with more than two children, needs to discriminate between children. Obtaining the backprop formulas is tedious but we list them below to facilitate duplication of our work 2 .",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_7",
  "x": "The major difference from that of (<cite>Socher et al., 2013</cite> ) is we use LSTM-like backpropagation, where unlike a regular LSTM, pass of error needs to discriminate between the left and right children, or in a topology with more than two children, needs to discriminate between children. Obtaining the backprop formulas is tedious but we list them below to facilitate duplication of our work 2 . We will discuss the specific objective function later in experiments.",
  "y": "differences"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_8",
  "x": "Following (<cite>Socher et al., 2013</cite>) , the overall objective function we used to learn S-LSTM in this paper is simply minimizing the overall cross-entropy errors and a sum of that at all nodes. ---------------------------------- **EXPERIMENT SET-UP**",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_9",
  "x": "We specifically attempt to determine the sentiment of different granularities of phrases in a tree, within the Stanford Sentiment Tree Bank benchmark data (<cite>Socher et al., 2013</cite>) . In obtaining the sentiment of a long piece of text, early work often factorized the problem to consider smaller pieces of component words or phrases with bag-of-words or bag-ofphrases models (Pang & Lee, 2008; Liu & Zhang, 2012) . More recent work has started to model composition (Moilanen & Pulman, 2007; Choi & Cardie, 2008; Socher et al., 2012; Kalchbrenner et al., 2014) , a more principled approach to modeling the formation of semantics.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_10",
  "x": "In this paper, we put the proposed LSTM memory blocks at tree nodes-we replaced the tensorenhanced composition layer at each tree node presented in (<cite>Socher et al., 2013</cite> ) with a S-LSTM memory block. We used the <cite>same dataset</cite>, the Stanford Sentiment Tree Bank, to evaluate the performances of the models. In addition to being a benchmark for much previous work, <cite>the data</cite> provide with human annotations at all nodes of the trees, facilitating a more comprehensive exploration of the properties of S-LSTM.",
  "y": "extends"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_11",
  "x": "We used the <cite>same dataset</cite>, the Stanford Sentiment Tree Bank, to evaluate the performances of the models. In addition to being a benchmark for much previous work, <cite>the data</cite> provide with human annotations at all nodes of the trees, facilitating a more comprehensive exploration of the properties of S-LSTM. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_12",
  "x": "**DATA SET** The Stanford Sentiment Tree Bank (<cite>Socher et al., 2013</cite>) contains about 11,800 sentences from the movie reviews that were originally discussed in (Pang & Lee, 2005) . The sentences were parsed with the Stanford parser (Klein & Manning, 2003) .",
  "y": "background"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_13",
  "x": "Phrases at all the tree nodes were manually annotated with sentiment values. We use the same split of the training and test data as in (<cite>Socher et al., 2013</cite>) to predict the sentiment categories of the roots (sentences) and all phrases (including sentences). For the root sentiment, the training, development, and test sentences are 8544, 1101, and 2210, respectively.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_14",
  "x": "Following (<cite>Socher et al., 2013</cite>) , we also use the classification accuracy to measure the performances. ---------------------------------- **TRAINING DETAILS**",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_15",
  "x": "---------------------------------- **TRAINING DETAILS** As mentioned before, we follow (<cite>Socher et al., 2013</cite>) to minimize the cross-entropy error for all nodes or for roots only, depending on specific experiment settings.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_16",
  "x": "c is the number of classes or categories, and j \u2208 c denotes the j-th element of the multinomial target distribution; i iterates over nodes, \u03b8 are model parameters, and \u03bb is a regularization parameter. We tuned our model against the development data set as split in (<cite>Socher et al., 2013</cite>) . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_17",
  "x": "**DEFAULT SETTING** In the default setting, we conducted experiments as in (<cite>Socher et al., 2013</cite>) . Table 1 shows the accuracies of different models on the test set of the <cite>Stanford Sentiment Tree Bank</cite>.",
  "y": "uses"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_20",
  "x": "RNTN is different from <cite>RvNN</cite> in that when merging two nodes to obtain the hidden vector of their parent, tensor is used to obtain the second-degree polynomial interactions. Table 1 showed that S-LSTM achieved the best predictive performance, when compared to all the models reported in (<cite>Socher et al., 2013</cite>) . The S-LSTM results reported here were obtained by setting the size of the hidden units to be 100, batch size to be 10, and learning rate to be 0.1.",
  "y": "differences"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_21",
  "x": "RNTN is different from <cite>RvNN</cite> in that when merging two nodes to obtain the hidden vector of their parent, tensor is used to obtain the second-degree polynomial interactions. Table 1 showed that S-LSTM achieved the best predictive performance, when compared to all the models reported in (<cite>Socher et al., 2013</cite>) . The S-LSTM results reported here were obtained by setting the size of the hidden units to be 100, batch size to be 10, and learning rate to be 0.1.",
  "y": "differences"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_23",
  "x": "Detailed annotations in the <cite>tree bank</cite> enable much interesting work to be possible, e.g., the study of the effect of negation in changing sentiment (Zhu et al., 2014) . The second setting, corresponding to model (3) and (4) in Table 2 , is only slightly different, in which we keep annotation for the tree leafs as well, to simulate that a sentiment lexicon is available and it covers all leafs (words) (LEAF LBLS along the side of the model names stands for leaf labels), and so there is no out-of-vocabulary concern. Using real sentiment lexicons is expected to have a performance between the two settings here.",
  "y": "future_work"
 },
 {
  "id": "e8c60c9fc3a2d74df632f3b423adae_24",
  "x": "Similarly, we also experimented with a right recursive S-LSTM, S-LSTM-RR, in which words are read from right to left instead. Since for these models, phrase-level training signals are not available-the nodes here do not correspond to that in the <cite>original Standford Sentiment Tree Bank</cite>, but the roots and leafs annotations are still the same, so we run two versions of our experiments: one uses only training signals from roots and the other includes also leaf annotations. It can be observed from Table 3 that the given parsing structure helps improve the predictive accuracy.",
  "y": "similarities differences"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_0",
  "x": "Using the same dataset, Natarajan and Charniak (2011) proposed a sandhi splitter for Sanskrit. The method is an extension of Bayesian word segmentation approach by Goldwater et al. (2006) . <cite>Krishna et al. (2016)</cite> is currently the state of the art in Sanskrit word segmentation.",
  "y": "background"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_1",
  "x": "The additional information will be beneficial in further processing of Sanskrit texts, such as Dependency parsing or summarisation (Krishna et al., 2017) .So far, no system successfully predicts the morphological information of the words in addition to the final word form. Though <cite>Krishna et al. (2016)</cite> has designed their system with this requirement in mind and outlined the possible extension of <cite>their</cite> system for the purpose, <cite>the system</cite> currently only predicts the final word-form. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_2",
  "x": "It needs to be noted that the average length of a string in the Digital Corpus of Sanskrit is 6.7 (<cite>Krishna et al., 2016</cite>) . The proportion of sentences with more than 10 words in our dataset is less than 1 %. The test dataset has slightly more than 4 % sentences with 10 or more words.",
  "y": "background"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_3",
  "x": "The word-level indexing in retrieval systems is often affected by phonetic transformations in words due to sandhi. String matching approaches often result in low precision results. Though this is not semantically meaningful it is lexically valid. Such tools are put to use by some of the existing systems (<cite>Krishna et al., 2016</cite>; Mittal, 2010 ) to obtain additional morphological or syntactic information about the sentences. This limits the scalability of <cite>those systems</cite>, as they cannot handle out of vocabulary words. Scalability of <cite>such systems</cite> is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing. The systems by <cite>Krishna et al. (2016)</cite> and Krishna et al. (2017) assume that the parser by Goyal et al. (2012) , identifies all the possible candidate chunks. Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence.",
  "y": "motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_4",
  "x": "Using a lexicon driven system might alleviate the said issues, but can lead to possible splits which are not semantically compatible. Such tools are put to use by some of the existing systems (<cite>Krishna et al., 2016</cite>; Mittal, 2010 ) to obtain additional morphological or syntactic information about the sentences.",
  "y": "background"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_5",
  "x": "Using a lexicon driven system might alleviate the said issues, but can lead to possible splits which are not semantically compatible. Such tools are put to use by some of the existing systems (<cite>Krishna et al., 2016</cite>; Mittal, 2010 ) to obtain additional morphological or syntactic information about the sentences. This limits the scalability of <cite>those systems</cite>, as they cannot handle out of vocabulary words. Scalability of <cite>such systems</cite> is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing. The systems by <cite>Krishna et al. (2016)</cite> and Krishna et al. (2017) assume that the parser by Goyal et al. (2012) , identifies all the possible candidate chunks.",
  "y": "background"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_6",
  "x": "Using a lexicon driven system might alleviate the said issues, but can lead to possible splits which are not semantically compatible. Such tools are put to use by some of the existing systems (<cite>Krishna et al., 2016</cite>; Mittal, 2010 ) to obtain additional morphological or syntactic information about the sentences. This limits the scalability of <cite>those systems</cite>, as they cannot handle out of vocabulary words. Scalability of <cite>such systems</cite> is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing.",
  "y": "motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_7",
  "x": "Such tools are put to use by some of the existing systems (<cite>Krishna et al., 2016</cite>; Mittal, 2010 ) to obtain additional morphological or syntactic information about the sentences. This limits the scalability of <cite>those systems</cite>, as they cannot handle out of vocabulary words. Scalability of <cite>such systems</cite> is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing.",
  "y": "motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_8",
  "x": "Scalability of <cite>such systems</cite> is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing. The systems by <cite>Krishna et al. (2016)</cite> and Krishna et al. (2017) assume that the parser by Goyal et al. (2012) , identifies all the possible candidate chunks. Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence.",
  "y": "background"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_9",
  "x": "Models that use linguistic resources are at an advantage here. Those systems such as <cite>Krishna et al. (2016)</cite> can be used to identify the morphological tags of the system as <cite>they</cite> currently store the morphological information of predicted candidates, but do not use them for evaluation as of now. Currently, no system exists that performs the prediction of wordform and morphological information jointly for Sanskrit.",
  "y": "background motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_10",
  "x": "Those systems such as <cite>Krishna et al. (2016)</cite> can be used to identify the morphological tags of the system as <cite>they</cite> currently store the morphological information of predicted candidates, but do not use them for evaluation as of now. Currently, no system exists that performs the prediction of wordform and morphological information jointly for Sanskrit. But, we leave this work for future.",
  "y": "motivation"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_11",
  "x": "Those systems such as <cite>Krishna et al. (2016)</cite> can be used to identify the morphological tags of the system as <cite>they</cite> currently store the morphological information of predicted candidates, but do not use them for evaluation as of now. Currently, no system exists that performs the prediction of wordform and morphological information jointly for Sanskrit. In our case, since we learn a new vocabulary altogether, the real word boundaries are opaque to the system.",
  "y": "differences"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_12",
  "x": "In this work we presented a model for word segmentation in Sanskrit using a purely engineering based appraoch. Our model with attention outperforms the current state of the art (<cite>Krishna et al., 2016</cite>) . Since, we tackle the problem with a non-linguistic approach, we hope to extend the work to other Indic languages as well where sandhi is prevalent such as Hindi, Marathi, Malayalam, Telugu etc.",
  "y": "differences"
 },
 {
  "id": "e9e733d38affa8a39a633ffb4d9d71_13",
  "x": "Since, we tackle the problem with a non-linguistic approach, we hope to extend the work to other Indic languages as well where sandhi is prevalent such as Hindi, Marathi, Malayalam, Telugu etc. Since we find that the inclusion of attention is highly beneficial in improving the performance of the system, we intend to experiment with recent advances in the encoder-decoder architectures, such as Vaswani et al. (2017) and Gehring et al. (2017) , where different novel approaches in using attention are experimented with. Our experiments in line with the measures reported in <cite>Krishna et al. (2016)</cite> show that our system performs robustly across strings of varying word size.",
  "y": "uses"
 },
 {
  "id": "eb591565efc03df1706710218a8f19_0",
  "x": "Most approaches utilize complex features to re-estimate the tree structures of a given sentence [1, 2, 3] . Unfortunately, sizes of treebanks are generally small and insufficient, which results in a common problem of data sparseness. Learning knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, <cite>5,</cite> 6] .",
  "y": "background"
 },
 {
  "id": "eb591565efc03df1706710218a8f19_1",
  "x": "---------------------------------- **** knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, <cite>5,</cite> 6] .",
  "y": "background"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_0",
  "x": "We introduce an unsupervised method based on MT for GEC that does not use parallel learner data. In particular, we use methods proposed by Marie and Fujita (2018) ,<cite> Artetxe et al. (2018b)</cite> , and Lample et al. (2018) . These methods are based on phrase-based statistical machine translation (SMT) and phrase table refinements.",
  "y": "uses"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_1",
  "x": "The experimental results show that our system achieved an F 0.5 score of 28.31 points. 2 Unsupervised GEC Algorithm 1 shows the pseudocode for unsupervised GEC. This code is derived from<cite> Artetxe et al. (2018b)</cite> .",
  "y": "uses"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_2",
  "x": "As in<cite> Artetxe et al. (2018b)</cite> , the term was set to 0.001. The backward lexical translation probability lex(e|f ) is calculated in a similar manner. Refinement of SMT system The phrase table created is considered to include noisy phrase pairs.",
  "y": "similarities uses"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_3",
  "x": "The SMT system trained on synthetic data eliminates the noisy phrase pairs using 2 As in<cite> Artetxe et al. (2018b)</cite> , \u03c4 is estimated by maximizing the phrase translation probability between an embedding and the nearest embedding on the opposite side. language models trained on the target-side corpus. This process corresponds to lines 6-10 in Algorithm 1.",
  "y": "similarities uses"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_4",
  "x": "Construction of a comparable corpus This unsupervised method is based on the assumption that the source and target corpora are comparable. In fact, Lample et al. (2018) ,<cite> Artetxe et al. (2018b)</cite> and Marie and Fujita (2018) use the News Crawl of source and target language as training data. To make a comparable corpus for GEC, we use translated texts using Google Translation as the source-side data.",
  "y": "background"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_5",
  "x": "We used moses truecaser for the training data; this truecaser model is learned from processed English News Crawl. We used byte-pair-encoding (Sennrich et al., 2016) learned from processed English News Crawl; the number of operations is 50K. The implementation proposed by<cite> Artetxe et al. (2018b)</cite> 7 was modified to conduct the experiments.",
  "y": "extends"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_6",
  "x": "**RELATED WORK** Unsupervised Machine Translation Studies on unsupervised methods have been conducted for both NMT (Lample et al., 2018; Marie and Fujita, 2018) and SMT <cite>(Artetxe et al., 2018b</cite> Table 4 : Error types for which our best system corrected errors well or mostly did not correct on the dev data. Top2 denotes the top two errors, and Bottom2 denotes the lowest two errors in terms of the F 0.5 10 .",
  "y": "background"
 },
 {
  "id": "ebb79e6e223d4747987aa4abfd1a58_7",
  "x": "Top2 denotes the top two errors, and Bottom2 denotes the lowest two errors in terms of the F 0.5 10 . this study, we apply the USMT method of<cite> Artetxe et al. (2018b)</cite> and Marie and Fujita (2018) to GEC. The UNMT method (Lample et al., 2018) was ineffective under the GEC setting in our preliminary experiments.",
  "y": "uses"
 },
 {
  "id": "ec0ae4e56c069e3efb4a2dc12199cd_0",
  "x": "AL has been successfully applied to speed up the annotation process for many NLP tasks without sacrificing annotation quality (Engelson and Dagan, 1996; Ngai and Yarowsky, 2000; Hwa, 2001; <cite>Tomanek et al., 2007a)</cite> . Once we decide to use AL for meta-data annotation and a reasonable, stable level of annotation quality is reachedafter having run through only a fraction of the documents compared with the traditional annotation approach where a randomly and independently selected amount of documents is sequentially annotated -an obvious question turns up: When do we stop the annotation process to cash in the time savings? Stopping after a certain amount of time has elapsed or a certain amount of data has been annotated is clearly not the best choice since such criteria, easily applicable though, do not take into account how well a classifier trained on the annotated data really performs.",
  "y": "background"
 },
 {
  "id": "ec0ae4e56c069e3efb4a2dc12199cd_1",
  "x": "In <cite>Tomanek et al. (2007a)</cite> we introduced the selection agreement (SA) curve -the average agreement amongst the selected examples plotted over time. When the SA values are close to '1', the committee members almost perfectly agree. So, any further AL iteration would resemble a random selection.",
  "y": "background"
 },
 {
  "id": "ec0ae4e56c069e3efb4a2dc12199cd_2",
  "x": "We employed the committee-based AL approach described in <cite>Tomanek et al. (2007a)</cite> . The committee consists of k = 3 Maximum Entropy (ME) classifiers (Berger et al., 1996) . In each AL iteration, each classifier is trained on a randomly",
  "y": "uses"
 },
 {
  "id": "ec0ae4e56c069e3efb4a2dc12199cd_3",
  "x": "While we made use of ME classifiers during the selection, we employed a NE tagger based on Conditional Random Fields (CRFs) (Lafferty et al., 2001 ) during evaluation time to determine the learning curves. We have already shown that in this scenario, ME classifiers perform equally well for AL-driven selection as CRFs when using the same features. This effect is truly beneficial, especially for real-world annotation projects, due to much lower training times and, by this, shorter annotator idle times <cite>(Tomanek et al., 2007a)</cite> .",
  "y": "background"
 },
 {
  "id": "ee66681690f2c92fe705a09bf7015d_0",
  "x": "Our results demonstrate a significant improvement in accuracy of 7.2% over a statistical machine translation (SMT) system (Zens et al., 2005) and of 2.2% over a perceptron-based edit model <cite>(Freitag and Khadivi, 2007)</cite> . ---------------------------------- **SEQUENCE ALIGNMENT MODEL**",
  "y": "differences background"
 },
 {
  "id": "ee66681690f2c92fe705a09bf7015d_1",
  "x": "For fixed sequences e and f the function s(e, f ) can be efficiently computed using a dynamic programming algorithm, Given a source sequence f computing the best scoring target sequence e = arg max e \u2032 s(e \u2032 , f ) among all possible sequences E * requires a beam search procedure <cite>(Freitag and Khadivi, 2007)</cite> . This procedure can also be used to produce K-best target sequences",
  "y": "uses background"
 },
 {
  "id": "ee66681690f2c92fe705a09bf7015d_2",
  "x": "Given a source sequence f computing the best scoring target sequence e = arg max e \u2032 s(e \u2032 , f ) among all possible sequences E * requires a beam search procedure <cite>(Freitag and Khadivi, 2007)</cite> . This procedure can also be used to produce K-best target sequences . In this paper, we employ the same features as those used by<cite> Freitag and Khadivi (2007)</cite> .",
  "y": "uses"
 },
 {
  "id": "ee66681690f2c92fe705a09bf7015d_3",
  "x": "We use the same training/development/testing (8084/1000/1000) set as the one used in a previous benchmark study <cite>(Freitag and Khadivi, 2007)</cite> . The development and testing data were obtained by randomly removing entries from the training data. The absence of short vowels (e.g. \"a\" in NB\"I, nab'i ), doubled consonants (e.g. \"ww\" in FWAL, fawwal ) and other diacritics in Arabic make the transliteration a hard problem.",
  "y": "uses"
 },
 {
  "id": "ee66681690f2c92fe705a09bf7015d_4",
  "x": "The input features are based on character m-grams for m = 1, 2, 3. Unlike previ-ous generative transliteration models, no additional language model feature is used. We compare our model against a state-of-the-art statistical machine translation (SMT) system (Zens et al., 2005) and an averaged perceptron edit model (PTEM) with identical features <cite>(Freitag and Khadivi, 2007)</cite> .",
  "y": "uses"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_0",
  "x": "By contrast, most of the prior work depend on parallel data in the form of a small bitext (Genzel, 2005) , a gold seed lexicon <cite>(Mikolov et al., 2013b)</cite> , or document-aligned comparable corpora (Vuli\u0107 and Moens, 2015) . Other prior work assumes access to additional resources or features, such as dependency parsers (Dou and Knight, 2013; Dou et al., 2014) , temporal and web-based features (Irvine and Callison-Burch, 2013) , or BabelNet (Wang and Sitbon, 2014) . Our approach consists of two stages: we first create a seed set of translation pairs, and then iteratively expand the lexicon with a bootstrapping procedure.",
  "y": "background"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_1",
  "x": "We filter out non-translation pairs that look similar but differ in meaning (false friends) by imposing a relative-frequency constraint. We then use this noisy seed lexicon to train context vectors via neural network <cite>(Mikolov et al., 2013b)</cite> , inducing a cross-lingual transformation that approximates semantic similarity. Although the initial accuracy of the transformation is low, it is sufficient to identify a certain number of correct translation pairs.",
  "y": "background"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_2",
  "x": "We proceed to iteratively expand our lexicon by alternating the two steps of translation pair identification, and transformation induction. We conduct a series of experiments on English, French, and Spanish. The results demonstrate a substantial error reduction with respect to a word-vector-based method of<cite> Mikolov et al. (2013b)</cite> , when using the same word vectors on six source-target pairs.",
  "y": "similarities uses"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_3",
  "x": "**LEXICON EXPANSION** Since our task is to find translations for each of a given set of source-language words, which we refer to as the source vocabulary, we must expand the seed lexicon to cover all such words. We adapt the approach of<cite> Mikolov et al. (2013b)</cite> for learning a linear transformation between the source and target vector spaces to enable it to function given only a small, noisy seed.",
  "y": "similarities uses"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_4",
  "x": "While<cite> Mikolov et al. (2013b)</cite> derive the translation matrix using five thousand translation pairs obtained via Google Translate, for c iterations do Train source-target TM T on R",
  "y": "differences"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_5",
  "x": "Unlike<cite> Mikolov et al. (2013b)</cite> , our algorithm iteratively expands the lexicon, which gradually increases the accuracy of the translation matrices. The initial translation matrices, derived from a small, noisy seed, are sufficient to identify a small number of correct translation pairs, which are added to the lexicon. The expanded lexicon is then used to derive new translation matrices, leading to more accurate translations.",
  "y": "differences"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_6",
  "x": "**EXPERIMENTS** In this section we compare our method to two prior methods, our reimplementation of the supervised word-vector-based method of<cite> Mikolov et al. (2013b)</cite> (using the same vectors as our method), and the reported results of an EM-based method of Haghighi et al. (2008) . ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_7",
  "x": "**EVALUATION** We evaluate the induced lexicon after 40 iterations of bidirectional bootstrapping by comparing it to the lexicon after the first iteration in a single direction, which is equivalent to the method of<cite> Mikolov et al. (2013b)</cite> . Following Haghighi et al. (2008) , we also report the accuracy of an ED-ITDIST baseline method, which matches words in the source and target vocabularies.",
  "y": "similarities"
 },
 {
  "id": "ee8163c5a76ed9f929a960b3086356_8",
  "x": "The results in Table 2 show that the method of<cite> Mikolov et al. (2013b)</cite> (MIK13-Auto) , represented by the first translation matrix derived on our automatically extracted the seed lexicon, performs well below the edit distance baseline. By contrast, our bootstrapping approach (BootstrapAuto) achieves an average accuracy of 85% on the six datasets. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_0",
  "x": "In this paper, I argue that a large, human sensetagged corpus is also critical as well as necessary to achieve broad coverage, high accuracy word sense disambiguation, where the sense distinction is at the level of a good desk-top dictionary such as WORD-NET. Using the sense-tagged corpus of 192,800 word occurrences reported in<cite> (Ng and Lee, 1996)</cite> , I examine the effect of the number of training examples on the accuracy of an exemplar-based classifier versus the base-line, most-frequent-sense classitier. I also estimate the amount of human sense-tagged corpus and the manual annotation effort needed to build a largescale, broad coverage word sense disambiguation program which can significantly outperform the most-frequent-sense classifier.",
  "y": "uses"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_1",
  "x": "I also address some objections to WSD research. In Section 3, I examine the size of the training corpus on the accuracy of WSD, using a corpus of 192,800 occurrences of 191 words hand tagged with WORDNET senses<cite> (Ng and Lee, 1996)</cite> . In Section 4, I estimate the amount of human sense-tagged corpus and the manual annotation effort needed to build a broad coverage, high accuracy WSD program.",
  "y": "uses"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_2",
  "x": "When the task is to resolve word senses to the fine-grain distinction of WORD-NET senses, the accuracy figures achieved are generally not very high (Miller et al., 1994; <cite>Ng and Lee, 1996)</cite> . This indicates that WSD is a challenging task and much improvement is still needed. However, if one were to resolve word sense to the level of homograph, or coarse sense distinction, then quite high accuracy can be achieved (in excess of 90%), as reported in (Wilks and Stevenson, 1996) .",
  "y": "background"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_3",
  "x": "When the task is to resolve word senses to the fine-grain distinction of WORD-NET senses, the accuracy figures achieved are generally not very high (Miller et al., 1994; <cite>Ng and Lee, 1996)</cite> . This indicates that WSD is a challenging task and much improvement is still needed. However, if one were to resolve word sense to the level of homograph, or coarse sense distinction, then quite high accuracy can be achieved (in excess of 90%), as reported in (Wilks and Stevenson, 1996) .",
  "y": "motivation"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_4",
  "x": "This is in contrast to disambiguating word senses to the refined senses of WoRDNET, where for instance, the average number of senses per noun is 7.8 and the average number of senses per verb is 12.0 for the set of 191 most ambiguous words investigated in<cite> (Ng and Lee, 1996)</cite> . We can readily collapse the refined senses of WORDNET into a smaller set if only a coarse (hot I will only focus on common noun in this paper and ignore proper noun. 2 mographic) sense distinction is needed, say for some NLP applications.",
  "y": "background"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_5",
  "x": "This is in contrast to disambiguating word senses to the refined senses of WoRDNET, where for instance, the average number of senses per noun is 7.8 and the average number of senses per verb is 12.0 for the set of 191 most ambiguous words investigated in<cite> (Ng and Lee, 1996)</cite> . We can readily collapse the refined senses of WORDNET into a smaller set if only a coarse (hot I will only focus on common noun in this paper and ignore proper noun. 2 mographic) sense distinction is needed, say for some NLP applications.",
  "y": "extends"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_6",
  "x": "The sense-tagged corpus SEMCOI~, prepared by (Miller et al., 1994) , contains a substantial subset of the Brown corpus tagged with the refined senses of WORDNET. However, as reported in (Miller et al., 1994) , there are not enough training examples per word in SP.MCOR to yield a broad coverage, high accuracy WSD program, due to the fact that sense tagging is done on every word in a running text in SEMCOR. To overcome this data sparseness problem of WSD, I initiated a mini-project in sense tagging and collected a corpus in which 192,800 occurrences of 191 words have been manually tagged with senses of WORDNET<cite> (Ng and Lee, 1996)</cite> .",
  "y": "uses"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_7",
  "x": "These 192,800 word occurrences consist of only 121 nouns and 70 verbs which are the most frequently occurring and most ambiguous words of English. 2 To investigate the effect of the number of training examples on WSD accuracy, I ran the exemplarbased WSD algorithm L~.XAS on varying number of training examples to obtain learning curves for the 191 words (details of LEXAS are described in<cite> (Ng and Lee, 1996)</cite> ).",
  "y": "uses"
 },
 {
  "id": "eeada4aedbb43b575365a15d75f2ac_8",
  "x": "Contact the LDC at ldc~unagi.cis.upenn.edu for details. The performance figures of LEXAS in Table 1 are higher than those reported in<cite> (Ng and Lee, 1996)</cite> . The classification accuracy of the nearest neighbor algorithm used by LEXAS (Cost and Salzberg, 1993) is quite sensitive to the number of nearest neighbors used to select the best matching example.",
  "y": "differences"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_0",
  "x": "**ABSTRACT** Abstract. Predicting mental health from smartphone and social media data on a longitudinal basis has recently attracted great interest, with very promising results being reported across many studies [3, 9, 13,<cite> 26]</cite> .",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_1",
  "x": "The current approach is to combine census data at the population level [19] , thus failing to capture well-being on an individual basis. The latter is only possible via self-reporting on the basis of established psychological scales, which are hard to acquire consistently on a longitudinal basis, and they capture long-term aggregates instead of the current state of the individual. The widespread use of smart-phones and social media offers new ways of assessing mental well-being, and recent research [1, 2, 3, 5, 9, 10, 13, 14, 22, 23,<cite> 26]</cite> has started exploring the effectiveness of these modalities for automatically assessing",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_2",
  "x": "Again however, the limited number of instances for every user make such models unable to generalize well. In order to overcome these issues, previous work [2, 5, 9, 10, 22,<cite> 26]</cite> has combined the instances {X uj i , y uj i } from different individuals u j and performed evaluation using randomised cross validation (MIXED). While such approaches can attain optimistic performance, the corresponding models fail to generalise to the general population and also fail to ensure effective personalised assessment of the mental health state of a single individual.",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_3",
  "x": "Research in assessing mental health on a longitudinal basis aims to make use of relevant features extracted from various modalities, in order to train models for automatically predicting a user's mental state (target), either in a classification or a regression manner [1, 2, 3, 9, 10, 13,<cite> 26]</cite> . Examples of state-of-the-art work in this domain are listed in Table 2 , along with the number of subjects that was used and the method upon which evaluation took place. Most approaches have used the \"MIXED\" approach to evaluate models [1, 2, 5, 9, 10, 22,<cite> 26]</cite> , which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score.",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_4",
  "x": "Examples of state-of-the-art work in this domain are listed in Table 2 , along with the number of subjects that was used and the method upon which evaluation took place. Most approaches have used the \"MIXED\" approach to evaluate models [1, 2, 5, 9, 10, 22,<cite> 26]</cite> , which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score. LOIOCV approaches that have not ensured that their train/test sets are independent are also vulnerable to bias in a realistic setting [3, 13] .",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_5",
  "x": "To illustrate this problem we have followed the approach by Canzian and Musolesi [3] , as one of the pioneering works on predicting depression with GPS traces, on a longitudinal basis. P3 Predicting users instead of mood scores: Most approaches merge all the instances from different subjects, in an attempt to build user-agnostic models in a randomised cross-validation framework [2, 9, 10,<cite> 26]</cite> . This is problematic, especially when dealing with a small number of subjects, whose behaviour (as captured through their data) and mental health scores differ on an individual basis.",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_6",
  "x": "Such approaches cannot guarantee that they will generalise on either a population-wide (LOUOCV ) or a personalised (LOIOCV ) level. In order to examine this effect in both a regression and a classification setting, we have followed the experimental framework by Tsakalidis et al. <cite>[26]</cite> and Jaques et al. [9] . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_7",
  "x": "---------------------------------- **P3: PREDICTING USERS (LOUOCV)** Tsakalidis et al. <cite>[26]</cite> monitored the behaviour of 19 individuals over four months.",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_8",
  "x": "Self-reported scores on a daily basis served as the ground truth. The authors labelled the instances with the top 30% of all the scores as \"happy\" and the lowest 30% as \"sad\" and randomly separated them into training, validation and test sets, leading to the same user bias issue. Since different users exhibit different mood scores on average <cite>[26]</cite> , by selecting instances from the top and bottom scores, one might end up separating users and convert the mood prediction task into a user identification one.",
  "y": "background"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_10",
  "x": "---------------------------------- **DATASET 1:** We employed the dataset obtained by Tsakalidis et al. <cite>[26]</cite> , a pioneering dataset which contains a mix of longitudinal textual and mobile phone usage data for 30 subjects.",
  "y": "uses"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_11",
  "x": "We followed the evaluation settings of two past works (see section 3.3), with the only difference being the use of 5-fold CV instead of a train/dev/test split that was used in [9] . The features of every instance are extracted from the past day before the completion of a mood form. In Experiment 1 we follow the setup in <cite>[26]</cite> : we perform 5-fold CV (MIXED) using SVM (SVR RBF ) and evaluate performance based on R 2 and RM SE.",
  "y": "uses"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_13",
  "x": "---------------------------------- **P3: PREDICTING USERS** Experiment 1: Table 7 shows the results based on the evaluation setup of Tsakalidis et al. <cite>[26]</cite> .",
  "y": "uses"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_14",
  "x": "Experiment 1: Table 7 shows the results based on the evaluation setup of Tsakalidis et al. <cite>[26]</cite> . In the MIXED cases, the pattern is consistent with <cite>[26]</cite> , indicating that normalising the features on a per-user basis yields better results, when dealing with sparse textual features (positive, negative, wellbeing targets). The explanation of this effect lies within the danger of predicting the user's identity instead of her mood scores.",
  "y": "similarities"
 },
 {
  "id": "ef6f1050651a4c3ac9a53438ac1f87_15",
  "x": "P3: Results following the evaluation setup in <cite>[26]</cite> (MIXED), along with the results obtained in the LOIOCV and LOUOCV settings with (+) and without (-) per-user input normalisation. Table 8 displays our results based on Jaques et al. [9] (see section 3.3). The average accuracy on the \"UNIQ\" setup is higher by 14% compared to the majority classifier in MIXED.",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_0",
  "x": "We explore blindfold (question-only) baselines for <cite>Embodied Question Answering</cite>. The <cite>EmbodiedQA</cite> task requires an agent to answer a question by intelligently navigating in a simulated environment, gathering necessary visual information only through first-person vision before finally answering. Consequently, a blindfold baseline which ignores the environment and visual information is a degenerate solution, yet we show through our experiments on the EQAv1 dataset that a simple question-only baseline achieves state-of-the-art results on the EmbodiedQA task in all cases except when the agent is spawned extremely close to the object.",
  "y": "motivation background"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_1",
  "x": "This is substantiated by embodiment theories in cognitive science that have argued for agent learning to be interactive and multimodal, mimicking key aspects of human learning [9, 17] . To foster and measure progress in such virtual environments, new tasks have been introduced, one of them being <cite>Embodied Question Answering</cite> (<cite>EmbodiedQA</cite>) <cite>[5]</cite> . The <cite>EmbodiedQA</cite> task requires an agent to intelligently navigate in a simulated household environment [25] and answer questions through egocentric vision.",
  "y": "background"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_2",
  "x": "To foster and measure progress in such virtual environments, new tasks have been introduced, one of them being <cite>Embodied Question Answering</cite> (<cite>EmbodiedQA</cite>) <cite>[5]</cite> . The <cite>EmbodiedQA</cite> task requires an agent to intelligently navigate in a simulated household environment [25] and answer questions through egocentric vision. Concretely, an agent is spawned at a random location in an environment (a house or building) and asked a question (e.g. 'What color is the car?').",
  "y": "background"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_3",
  "x": "The goal of the agent is to intelligently navigate the environment and gather visual information necessary for answering the question. Subsequent to the introduction of the task, several methods have been introduced to solve the <cite>EmbodiedQA</cite> task <cite>[5</cite>, 6] , using some combination of reinforcement learning, behavior cloning and hierarchical control. Apart from using the question and images from the environment, these methods also rely on varying degrees of expert supervision such as shortest path demonstrations and subgoal policy sketches.",
  "y": "background"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_4",
  "x": "Subsequent to the introduction of the task, several methods have been introduced to solve the <cite>EmbodiedQA</cite> task <cite>[5</cite>, 6] , using some combination of reinforcement learning, behavior cloning and hierarchical control. Apart from using the question and images from the environment, these methods also rely on varying degrees of expert supervision such as shortest path demonstrations and subgoal policy sketches. In this work, we evaluate simple question-only baselines that never see the environment and receive no form of expert supervision.",
  "y": "motivation"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_5",
  "x": "To our surprise, blindfold baselines achieve state-of-the-art performance on the <cite>EmbodiedQA</cite> task, except in the case when the agent is spawned extremely close to the object. Even in the latter case, blindfold baselines perform surprisingly close to existing state-of-the-art methods. We note that this finding is reminiscent of several recent works in both Computer Vision and Natural Language Processing, where researchers have found that statistical irregularities in the dataset can enable degenerate methods to perform surprisingly well [11, 12, 14, 21] .",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_6",
  "x": "Our findings suggest that current <cite>EmbodiedQA</cite> models are ineffective at leveraging the context from the environment, in fact this context or embodiment in the environment can negatively hamper them. We hope comparison with our baseline results can more effectively demonstrate how well a method is able to leverage embodiment in the environment. Upon further error analysis of our models and qualitative inspection of the dataset, we find that there exist biases in the EQAv1 dataset that allow blindfold models to perform so well.",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_8",
  "x": "**RELATED WORK** <cite>EmbodiedQA</cite> Methods: Das et al. <cite>[5]</cite> introduced the <cite>PACMAN-RL+Q</cite> model which is bootstrapped with expert shortest-path demonstrations and later fine-tuned with REINFORCE [24] . This model consists of a hierarchical navigation module: a planner and a controller, and a question answering module that acts when the navigation module has given up control.",
  "y": "background"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_9",
  "x": "To get rid of peaky answers, an entropy pruning method was applied by <cite>[5]</cite> where questions with normalized entropy below 0.5 were excluded. However this still leaves an uneven answer distribution that can be exploited. We also train the <cite>[5]</cite> text embedding model (an LSTM) with the optimization settings described in <cite>[5]</cite> for 200 epochs.",
  "y": "differences"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_10",
  "x": "The training procedure is run for 200 epochs and we use the checkpoint with minimum validation loss to compute accuracy on the test set. The NN-AnswerDist and the Majority baselines are self-descriptive and there are no specific training details that we apply. We also train the <cite>[5]</cite> text embedding model (an LSTM) with the optimization settings described in <cite>[5]</cite> for 200 epochs.",
  "y": "similarities"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_11",
  "x": "Since the performance of blindfold baselines are not affected based on where the agent is spawned, their accuracy is same across T 10 , T 20 and T 50 . We observe that the BoW model outperforms all existing methods except NMC(BC+A3C) in the case where agent is spawned very close to the target. The Nearest Neighbour method also does pretty well, and only falls behind to <cite>PACMAN</cite> (BC+REINFORCE) and NMC(BC+A3C) in the T 10 case.",
  "y": "differences"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_12",
  "x": "We also observe that the majority baseline achieves an accuracy of only 17.15%, suggesting that the other question-only baselines leverage dataset biases separate from class modes. For completeness, we also include a question only baseline derived directly from the <cite>EmbodiedQA</cite> codebase, which uses only the Question LSTM in the <cite>PACMAN</cite> model, termed as <cite>PACMAN</cite> Q-only (LSTM). Note that we only compare the top-1 accuracy of different methods here, and not the navigation performance since it's not directly applicable to these blindfold baselines.",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_13",
  "x": "To better understand the exact bias exploited by the text only models we observe that (a) The questions from training set are largely repeated in the validation and test set, with only 2 and 6 questions being unique to them respectively. As noted earlier, this means that models don't need to generalize across unseen combinations of rooms/objects/colors to perform well on this task (b) Despite entropy-pruning, there is a noticeable bias in the answer distribution of EQAv1 questions (see [<cite>5</cite>, Appendix A]). Our results on the Nearest Neighbour baseline confirm this source of bias and explain largely the text model performance.",
  "y": "similarities"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_14",
  "x": "Viewing these results holistically, we conclude that current methods for the <cite>EmbodiedQA</cite> task are not effective at using context from the environment, and in fact this negatively hampers them. This shows that there is room for building new models that leverage the context and embodiment in the environment. Oracles: We now examine whether the EQAv1 dataset and the proposed oracle navigation can improve over pure text baselines, to leverage visual information in the most ideal case.",
  "y": "motivation uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_16",
  "x": "For completeness we benchmark an oracle with our BoW embedding model in place of the LSTM with all other settings kept constant. As noted in <cite>[5]</cite> , we re-iterate that these oracles are far from perfect, as they may not contain the best vantage or context to answer the question. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_17",
  "x": "---------------------------------- **T 10** T 20 T 50 T any Navigation + VQA <cite>PACMAN</cite> (BC) <cite>[5]</cite> 48 BOW-CNN VQA-Only 56.5 Table 1 : We compare to the published results from [6] for agent spawned at various steps away from the target: 10, 30, 50, and anywhere in the environment.",
  "y": "uses differences"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_18",
  "x": "Question-only baselines outperform Navigation+VQA methods except when spawned 10 steps from the target object. A VQA-only system with oracle navigation can improve on a pure text baseline but isn't effective when combined with navigation. (*) indicates our reproduction of the model described in <cite>[5]</cite> Error Analysis: To better understand the shortcomings and limitations, we perform an error analysis of the one of the runs of the BoW model on different question types: Here, the color category Preposition Location Color 9.09 51.72 53.31 Table 2 : Accuracy of the BoW model on different question types subsumes color and color_room both.",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_19",
  "x": "**CONCLUSION** We show that simple question only baselines largely outperform or closely compete with existing methods on the <cite>EmbodiedQA</cite> task. Our results indicate existing models are not able to convincingly use sensory inputs from the environment to perform question answering, although they have been demonstrated some ability navigate toward the object of interest.",
  "y": "uses"
 },
 {
  "id": "f1e5584a2139160943d9f0338e6ce0_20",
  "x": "**CONCLUSION** We show that simple question only baselines largely outperform or closely compete with existing methods on the <cite>EmbodiedQA</cite> task. Our results indicate existing models are not able to convincingly use sensory inputs from the environment to perform question answering, although they have been demonstrated some ability navigate toward the object of interest.",
  "y": "future_work"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_0",
  "x": "In order to obtain better embeddings for the questions and answers, we build a convolutional neural network (CNN) structure on top of biLSTM. Secondly, in order to better distinguish candidate answers according to the question, we introduce a simple but efficient attention model to this framework for the answer embedding generation according to the question context. We report experimental results for two answer selection datasets: (1) InsuranceQA<cite> (Feng et al., 2015)</cite> 1 , a recently released large-scale non-factoid QA dataset from the insurance domain.",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_1",
  "x": "While these methods show effectiveness, they might suffer from the availability of additional resources, the effort of feature engineering and the systematic complexity by introducing linguistic tools, such as parse trees and dependency trees. There were prior methods using deep learning technologies for the answer selection task. The approaches for non-factoid question answering generally pursue the solution on the following directions: Firstly, the question and answer representations are learned and matched by certain similarity metrics<cite> (Feng et al., 2015</cite>; Yu et al., 2014; dos Santos et al., 2015) .",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_2",
  "x": "There are two major differences between our approaches and the work in<cite> (Feng et al., 2015)</cite> : (1) The architectures developed in<cite> (Feng et al., 2015)</cite> are only based on CNN, whereas our models are based on bidirectional LSTMs, which are more capable of exploiting long-range sequential context information. Moreover, we also integrate the CNN structures on the top of biLSTM for better performance. (2)<cite> Feng et al. (2015)</cite> tackle the question and answer independently, while the proposed structures develop an efficient attentive models to generate answer embeddings according to the question.",
  "y": "differences"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_3",
  "x": "Following the same ranking loss in<cite> (Feng et al., 2015</cite>; Weston et al., 2014; Hu et al., 2014) , we define the training objective as a hinge loss. where a + is a ground truth answer, a \u2212 is an incorrect answer randomly chosen from the entire answer space, and M is constant margin. We treat any question with more than one ground truth as multiple training examples, each for one ground truth.",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_4",
  "x": "Dropout operation is performed on the QA representations before cosine similarity matching. Finally, from preliminary experiments, we observe that the architectures, in which both question and answer sides share the same network parameters, is significantly better than the one that the question and answer sides own their own parameters separately, and converges much faster. As discussed in<cite> (Feng et al., 2015)</cite> , this is reasonable, because for a shared layer network, the corresponding elements in question and answer vectors represent the same biLSTM outputs.",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_5",
  "x": "In this subsection, we resort to a CNN structure built on the outputs of biLSTM, in order to give a more composite representation of questions and answers. The structure of CNN in this work is similar to the one in<cite> (Feng et al., 2015)</cite> , as shown in Figure 2 . Unlike the traditional forward neural network, where each output is interactive with each input, the convolutional structure only imposes local interactions between the inputs within a filter size m.",
  "y": "similarities"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_6",
  "x": "Having described a number of models in the previous section, we evaluate the proposed approaches on the insurance domain dataset, InsuranceQA, provided by<cite> Feng et al. (2015)</cite> . The InsuranceQA dataset provides a training set, a validation set, and two test sets. We do not see obvious categorical differentiation between two tests' questions.",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_7",
  "x": "One can see the details of InsuranceQA data in<cite> (Feng et al., 2015)</cite> . We list the numbers of questions and answers of the dataset in Table 1 . A question may correspond to multiple answers.",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_8",
  "x": "Architecture-II in<cite> (Feng et al., 2015)</cite> : Instead of using LSTM, a CNN model is employed to learn a distributed vector representation of a given question and its answer candidates, and the answers are scored by cosine similarity with the question. No attention model is used in this baseline. ----------------------------------",
  "y": "background"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_9",
  "x": "This is the model which achieved the best performance in<cite> (Feng et al., 2015)</cite> . ---------------------------------- **RESULTS AND DISCUSSIONS**",
  "y": "background"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_10",
  "x": "We set the filter width m = 2, and we did not see better performance if we increase m to 3 or 4. Row (F) with 4000 filters gets the best validation accuracy, obtained a comparable performance with the best baseline (Row (D) in Table 2 ). Row F shared a highly analogous CNN structure with Architecture II in<cite> (Feng et al., 2015)</cite> , except that the later used a shallow hidden layer to transform the word embeddings into the input of CNN structure, while Row F take the output of biLSTM as CNN input.",
  "y": "similarities differences"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_11",
  "x": "Model (H) is over 8% higher on all datasets compared to (B), and gets improvements from the best baseline by 3%, 2.8% and 1.2% on the validation, Test1 and Test2 sets, respectively. Compared to Architecture II in<cite> (Feng et al., 2015)</cite> , which involved a large number of CNN filters, (H) model also has fewer parameters. Row (I) corresponds to section 3.4, where CNN and attention mechanism are combined.",
  "y": "differences"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_12",
  "x": "We implemented the Architecture II in<cite> (Feng et al., 2015)</cite> from scratch. Wang & Nyberg (2015) and<cite> Feng et al. (2015)</cite> are the best baselines on MAP and MRR respectively. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_13",
  "x": "We implemented the Architecture II in<cite> (Feng et al., 2015)</cite> from scratch. Wang & Nyberg (2015) and<cite> Feng et al. (2015)</cite> are the best baselines on MAP and MRR respectively. ----------------------------------",
  "y": "background"
 },
 {
  "id": "f24dde456e02fdb8e65799685275d2_14",
  "x": "The combination of CNN with QA-LSTM (Model-C) gives greater improvement on both MAP and MRR from Model (A). Model (D), which combines the ideas of Model (B) and (C), achieves the performance, competitive to the best baselines on MAP, and 2\u223c4% improvement on MRR compared to (Wang & Nyberg, 2015) and<cite> (Feng et al., 2015)</cite> . Finally, Model (E), which corresponds to the same model (D) but uses a LSTM hidden vector size of 500, achieves the best results for both metrics and outperforms the baselines.",
  "y": "differences"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_0",
  "x": "As an input, this function operates on various statistics relating to different text features. In this paper, we train a readability classification model using a corpus compiled from textbooks and features inherited from our previous works Islam et al. (2012; and features from<cite> Sinha et al. (2012)</cite> . Later we use the model to classify Bangla news articles for children from different well-known news sources from Bangladesh and West Bengal.",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_1",
  "x": "Due to unavailability of linguistic resources for Bangla, we did not explore any of the linguistically motivated features. We have inherited features from Islam et al. (2012; and<cite> Sinha et al. (2012)</cite> , these features achieve reasonable classification accuracy. Children's reading skills is influenced by their cognitive ability.",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_2",
  "x": "**READABILITY MODELS FOR BANGLA** Recently,<cite> Sinha et al. (2012)</cite> proposed few computational models that are similar to the traditional English readability formulas. A user study was performed to evaluate their performance.",
  "y": "background"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_3",
  "x": "Recently,<cite> Sinha et al. (2012)</cite> proposed few computational models that are similar to the traditional English readability formulas. We also inherited two of their best performing models:",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_4",
  "x": "The PSW30 shows that normalized value of PSW over 30 sentences. Table 3 : Performance of Bangla readability models proposed by<cite> Sinha et al. (Sinha et al., 2012)</cite> . In this paper, we use 20 features to generate feature vectors for the classifier.",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_5",
  "x": "In order to find the best performing training model, we use 20 features from Islam et al. (2012; and<cite> Sinha et al. (2012)</cite> . Note that hundred data sets were randomly generated where 80% of the corpus was used for training and remaining 20% for evaluation. The weighted average of Accuracy and F-score is computed by considering results of all data sets.",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_6",
  "x": "---------------------------------- **TRAINING MODEL** The traditional readability formulas that were proposed for English texts do not work for Bangla texts (Islam et al., 2012; Islam et al., 2014;<cite> Sinha et al., 2012)</cite> .",
  "y": "background"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_7",
  "x": "That is why, we did not explore any of the traditional formulas. At first we build a classifier using two readability models from <cite>Sinha et al(2012)</cite> . The output of these models are used as input for the readability classifier.",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_8",
  "x": "The classification F-Score rises to 87.87 when we combine features from Islam et al. (2014) and<cite> Sinha et al. (Sinha et al., 2012)</cite> . ---------------------------------- **NEWS ARTICLES CLASSIFICATION**",
  "y": "uses"
 },
 {
  "id": "f29baa099b13f38badeb4cbd8789f6_9",
  "x": "Lexical features and features related to information density also have good predictive power to identify text difficulties. The classification results show that candidate articles are appropriate for children. This study also validate that features in our previous study Islam et al. (2014) and features proposed by<cite> Sinha et al. (Sinha et al., 2012)</cite> are useful for Bangla text readability analysis.",
  "y": "similarities"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_0",
  "x": "****CONVOLUTIONAL NEURAL NETWORKS FOR TEXT CATEGORIZATION: SHALLOW WORD-LEVEL VS. DEEP CHARACTER-LEVEL**** **ABSTRACT** This paper reports the performances of shallow word-level convolutional neural networks (CNN), our earlier work (2015) [3,<cite> 4]</cite> , on the eight datasets with relatively large training data that were used for testing the very deep characterlevel CNN in Conneau et al. (2016) [1].",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_1",
  "x": "**INTRODUCTION** Text categorization is the task of labeling documents, which has many important applications such as sentiment analysis and topic categorization. Recently, several variations of convolutional neural networks (CNNs) [7] have been shown to achieve high accuracy on text categorization (see e.g., [3, <cite>4,</cite> 9, 1] and references therein) in comparison with a number of methods including linear methods, which had long been the state of the art.",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_2",
  "x": "Here we focus on two CNN studies that report high performances on categorizing long documents (as opposed to categorizing individual sentences): \u2022 Our earlier work (2015) [3,<cite> 4]</cite> : shallow word-level CNNs (taking sequences of words as input), which we abbreviate as word-CNN. \u2022 Conneau et al. (2016) [1]: very deep character-level CNNs (taking sequences of characters as input), which we abbreviate as char-CNN. Although both studies report higher accuracy than previous work on their respective datasets, it is not clear how they compare with each other due to lack of direct comparison.",
  "y": "background motivation"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_3",
  "x": "Although both studies report higher accuracy than previous work on their respective datasets, it is not clear how they compare with each other due to lack of direct comparison. In [1] , the very deep char-CNN was shown to perform well with larger training data (up to 2.6M documents) but perform relatively poorly with smaller training data; e.g., it underperformed linear methods when trained with 120K documents. In [3,<cite> 4]</cite> the shallow word-CNN was shown to perform well, using training sets (most intensively, 25K documents) that are mostly smaller than those used in [1] .",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_4",
  "x": "While these results imply that the shallow word-CNN is likely to outperform the deep char-CNN when trained with relatively small training sets such as those used in [3,<cite> 4]</cite> , the shallow word-CNN is untested on the training sets as large as those used in [1] . Hence, the purpose of this report is to fill the gap by testing the shallow word-CNNs as in [3,<cite> 4]</cite> on the datasets used in [1] , for direct comparison with the results of very deep char-CNNs reported in [1] . Limitation of work In this work, our new experiments are limited to the shallow word-CNN as in [3,<cite> 4]</cite> .",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_5",
  "x": "In [3,<cite> 4]</cite> the shallow word-CNN was shown to perform well, using training sets (most intensively, 25K documents) that are mostly smaller than those used in [1] . While these results imply that the shallow word-CNN is likely to outperform the deep char-CNN when trained with relatively small training sets such as those used in [3,<cite> 4]</cite> , the shallow word-CNN is untested on the training sets as large as those used in [1] . Hence, the purpose of this report is to fill the gap by testing the shallow word-CNNs as in [3,<cite> 4]</cite> on the datasets used in [1] , for direct comparison with the results of very deep char-CNNs reported in [1] .",
  "y": "motivation"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_6",
  "x": "Limitation of work In this work, our new experiments are limited to the shallow word-CNN as in [3,<cite> 4]</cite> . We do not provide new error rate results for the very deep CNNs proposed by [1] , and we only cite their results. Although it may be natural to assume that the error rates reported in [1] well represent the best performance that the deep char-CNNs can achieve, we note that in [1] , documents were clipped and padded so that they all became 1014 characters long, and we do not know how this pre-processing affected their model accuracy.",
  "y": "similarities"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_7",
  "x": "**PRELIMINARY** We start with briefly reviewing the very deep word-CNN of [1] and the shallow word-CNN of [3,<cite> 4]</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_8",
  "x": "**SHALLOW WORD-LEVEL CNNS AS IN [3,<cite> 4]</cite>** Two types of word-CNN were proposed in [3,<cite> 4]</cite> , which are illustrated in Figure 1 . One is a straightforward application of CNN to text (the base model), and the other involves training of tv-embedding ('tv' stands for two views) to produce additional input to the base model.",
  "y": "background"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_9",
  "x": "explicit word embedding layer before a convolution layer as in, e.g., [6] , which makes x the concatenation of word vectors. See also the supplementary material of<cite> [4]</cite> for the representation power analysis. As illustrated in Figure 1 (a), f (x) is applied to the text regions at every location of a document (ovals in the figure), and pooling aggregates the resulting region vectors into a document vector, which is used as features by a linear classifier.",
  "y": "uses"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_10",
  "x": "In<cite> [4]</cite> , tv-embedding training was done using unlabeled data as an additional resource; therefore, the proposed models were semi-supervised models. In the experiments reported below, due to the lack of standard unlabeled data for the tested datasets, we trained tv-embeddings on the labeled training data ignoring the labels; thus, the resulting models are supervised ones. We trained four tv-embeddings with four distinct one-hot representations of text regions (i.e., input to orange ovals in Figure 1 (b) ): bow representation with region size 5 or 9, and bag-of-{1,2,3}-gram representation with region size 5 or 9.",
  "y": "differences"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_11",
  "x": "Tv-embedding training was done as in<cite> [4]</cite> ; weighted square loss was minimized without regularization while the target regions (adjacent regions) were represented by bow vectors, and the data weights were set so that the negative sampling effect was achieved. Tv-embeddings were fixed (i.e., no weight updating) during the final training with labeled data. Training with labels (either with or without tv-embedding) was done as follows.",
  "y": "uses"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_12",
  "x": "The second best performer is the shallow word-CNN without tv-embedding on all but Ama.f (Amazon full). Whereas the deep char-CNN underperforms traditional linear models when training data is relatively small, the shallow word-CNNs with and without tv-embedding clearly outperform them on all the datasets. We observe that, as in our previous work<cite> [4]</cite> , additional input produced by tv-embeddings led to substantial improvements.",
  "y": "similarities"
 },
 {
  "id": "f2b9a5633600cdf787111841bf9ce6_13",
  "x": "---------------------------------- **SUMMARY OF THE RESULTS** \u2022 The shallow word-CNNs as in [3,<cite> 4]</cite> generally achieved better error rates than those of the very deep char-CNNs reported in [1] .",
  "y": "similarities"
 },
 {
  "id": "f2dfc35b67e47c12cba3cd0ec743a5_0",
  "x": "Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [<cite>17</cite>] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way.",
  "y": "background"
 },
 {
  "id": "f2dfc35b67e47c12cba3cd0ec743a5_1",
  "x": "Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [<cite>17</cite>] ), I believe that the creation of novel semantic-based measurements would improve the state of the art. Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way.",
  "y": "background"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_0",
  "x": "This approach is well suited for non-interactive, static contexts, but recently, there has been increased interest in generation for situated dialog<cite> (Stoia, 2007</cite>; . Most human language use takes place in dynamic situations, and psycholinguistic research on humanhuman dialog has proposed that the production of referring expressions should rather be seen as a process that not only depends on the context and the choices of the speaker, but also on the reactions of the addressee. Thus the result is often not a single noun phrase but a sequence of installments (Clark and Wilkes-Gibbs, 1986) , consisting of multiple utterances which may be interleaved with feedback from the addressee.",
  "y": "background"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_1",
  "x": "<cite>Stoia (2007)</cite> studies instruction giving in a virtual environment and finds that references to target objects are often not made when they first become visible. Instead interaction partners are navigated to a spot from where an easier description is possible. develop a planning-based approach of this behavior. But once their system decides to generate a referring expression, it is delivered in one unit. Thompson (2009) , on the other hand, proposes a game-theoretic model to predict how noun phrases are split up into installments.",
  "y": "background"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_2",
  "x": "The second utterance then picks out the target. <cite>Stoia (2007)</cite> observed that IGs use move instructions to focus the IF's attention on a particular area. This is also common in our data.",
  "y": "background"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_3",
  "x": "The second utterance then picks out the target. <cite>Stoia (2007)</cite> observed that IGs use move instructions to focus the IF's attention on a particular area. This is also common in our data.",
  "y": "similarities"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_4",
  "x": "The NLG system thus needs to be able to decide when a complete identifying description can be given in one utterance and when a description in installments is more effective. <cite>Stoia (2007)</cite> as well as have addressed this question, but their approaches only make a choice between generating an instruction to move or a uniquely identifying referring expression. They do not consider cases in which another type of utterance, for instance, one that refers to a group of objects or gives an initial ambiguous description, is used to draw the attention of the IF to a particular area and they do not generate referring expressions in installments.",
  "y": "differences"
 },
 {
  "id": "f326a3e2a5e349ce84b0a759f8e0b2_5",
  "x": "If the feedback was inserted in the middle of a sentence, if finally has to decide whether this sentence should be completed and how the remainder may have to be adapted. Once we have finished the corpus collection, we plan to use it to study and address the questions discussed above. We are planning on building on the work by <cite>Stoia (2007)</cite> on using machine learning techniques to develop a model that takes into account various contextual factors and on the work by Thompson (2009) on generating references in installments.",
  "y": "future_work"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_0",
  "x": "By introducing an additional blank symbol and a specially defined loss function aggregating many allowed paths within a graph, CTC model can be optimized to generate the correct character sequences from the speech signals regardless of the blank symbols interspersed among. The seq2seq models, on the other hand, simply maximized the likelihood of observing the decoded sequence given the ground truth at every time step. With many recent results [9, 10, 11, 12, <cite>13]</cite> approaching the stateof-the-art, end-to-end deep learning has definitely been a very important direction for speech recognition.",
  "y": "background"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_1",
  "x": "The two steps here are conducted iteratively: (a) a Criticizing Language Model (CLM) is trained to evaluate the quality score given a text sequence, and (b) and ASR model is trained to minimize the sequence loss calculated with ground truth while maximizing the scores given by CLM. 16, 17] have been developed to address such problem by involving unpaired text data (which are relatively easy to obtain) in the training progress. One approach is to utilize unpaired text data to produce a separately trained language model (LM) to rescore the output of the end-to-end approach [18,<cite> 13,</cite> 19, 20] , but at the price of extra computation during testing.",
  "y": "background"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_2",
  "x": "Also, CNN based network is relatively more computationally efficient, which is important in adversarial training. But other network architectures such as RNN-LM<cite> [13]</cite> can also be used here. Loss Function.",
  "y": "background"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_3",
  "x": "Any network architecture for end-toend speech recognition can be used here, while Fig. 3 gives the one used in this work, following the previous work<cite> [13]</cite> of integrating attentioned Seq2seq with CTC. The model takes a sequence of speech features O = o 1 , o 2 , ..., o N with length N as the input. O is encoded into sequence of hidden state H = h 1 , h 2 , ..., h T by the encoder (consists of a VGG extractor performing input downsampling followed by several BLSTM layers) with T being the output sequence length.",
  "y": "similarities uses"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_4",
  "x": "CLM only takes\u1ef9 as input. During testing,\u1ef9 andy are integrated into a single output sequence just as done in the previous work<cite> [13]</cite> . Seq2seq Loss.",
  "y": "similarities uses"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_5",
  "x": "We followed the previous work<cite> [13,</cite> 21] to use 80-dimensional log Mel-filter bank and 3-dimensional pitch features as the acoustic features. Text data are represented by sequences of 5000 subword units one-hot vectors to avoid OOV. For the CLM model, the dimension of the output of all layers were set to 128 except the last.",
  "y": "similarities uses"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_6",
  "x": "For the ASR model, the encoder included a 6layer VGG extractor with downsampling used in the previous work<cite> [13]</cite> and a 5-layer BLSTM with 320 units per direction. 300-dimensional location-aware attention [7] was used in the attention layer. The decoder was a single layer LSTM with 320 units.",
  "y": "similarities uses"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_7",
  "x": "Also, the update frequency of CLM is set to 5 times less than the ASR model to stabilize AT process. Table 1 . Speech recognition performance. \"+LM\" refers to shallow fusion decoding jointly with RNN-LM<cite> [13]</cite> , \"+AT\" refers to the adversarial training proposed here, \"+Both\" indicates training with AT and joint decoding with RNN-LM, and BT is the prior work of back-translation [21] .",
  "y": "background"
 },
 {
  "id": "f32bbd580d93f77ef764c5341b93db_8",
  "x": "**EXPERIMENTAL RESULTS** In the experiments, the ASR model was trained on the 100 hours speech data but combined with different amount of unpaired text utilized in different ways. The results are listed in Table 1 , where \"Baseline\" refers to the plain end-toend speech recognition framework as described in Sec. 2.3, \"+LM\" refers to the shallow fusion decoding with a separately trained RNN language model (RNN-LM)<cite> [13,</cite> 20] and \"+AT\" refers to the adversarial training proposed here.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_0",
  "x": "This field is referred to as \"data-to-text\" [8] and has its place in several application domains (such as journalism [22] or medical diagnosis [25] ) or wide-audience applications (such as financial [26] and weather reports [30] , or sport broadcasting [4, <cite>39]</cite> ). As an example, Figure 1 shows a data-structure containing statistics on NBA basketball games, paired with its corresponding journalistic description. Designing data-to-text models gives rise to two main challenges: 1) understanding structured data and 2) generating associated descriptions.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_1",
  "x": "Recent datato-text models [18, 28, 29, <cite>39]</cite> mostly rely on an encoder-decoder architecture [2] in which the data-structure is first encoded sequentially into a fixed-size vectorial representation by an encoder. Then, a decoder generates words conditioned on this representation. With the introduction of the attention mechanism [19] on one hand, which computes a context focused on important elements from the input at each decoding step and, on the other hand, the copy mechanism Fig. 1 : Example of structured data from the RotoWire dataset.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_2",
  "x": "To address these shortcomings, we propose a new structured-data encoder assuming that structures should be hierarchically captured. Our contribution focuses on the encoding of the data-structure, thus the decoder is chosen to be a classical module as used in [28, <cite>39]</cite> . Our contribution is threefold:",
  "y": "similarities"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_3",
  "x": "-We model the general structure of the data using a two-level architecture, first encoding all entities on the basis of their elements, then encoding the data structure on the basis of its entities; -We introduce the Transformer encoder [36] in data-to-text models to ensure robust encoding of each element/entities in comparison to all others, no matter their initial positioning; -We integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder. We report experiments on the RotoWire benchmark<cite> [39]</cite> which contains around 5K statistical tables of NBA basketball games paired with humanwritten descriptions. Our model is compared to several state-of-the-art models.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_4",
  "x": "In parallel, an emerging task, called \"data-to-text\", aims at describing structured data into a natural language description. This task stems from the neural machine translation (NMT) domain, and early work [1, 15, <cite>39]</cite> represent the data records as a single sequence of facts to be entirely translated into natural language. Wiseman et al.<cite> [39]</cite> show the limits of traditional NMT systems on larger structured-data, where NMT systems fail to accurately extract salient elements.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_5",
  "x": "Wiseman et al.<cite> [39]</cite> show the limits of traditional NMT systems on larger structured-data, where NMT systems fail to accurately extract salient elements. To improve these models, a number of work [16, 28, 40] proposed innovating decoding modules based on planning and templates, to ensure factual and coherent mentions of records in generated descriptions. For example, Puduppully et al. [28] propose a two-step decoder which first targets specific records and then use them as a plan for the actual text generation.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_6",
  "x": "Our contribution differs from previous work in several aspects. First, instead of flatly concatenating elements from the data-structure and encoding them as a sequence [18, 28, <cite>39]</cite> , we constrain the encoding to the underlying structure of the input data, so that the delimitation between entities remains clear throughout the process. Second, unlike all works in the domain, we exploit the Transformer architecture [36] and leverage its particularity to directly compare elements with each others in order to avoid arbitrary assumptions on their ordering.",
  "y": "differences"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_7",
  "x": "This equation is intractable in practice, we approximate a solution using beam search, as in [18, 17, 28, 29, <cite>39]</cite> . Our model follows the encoder-decoder architecture [2] . Because our contribution focuses on the encoding process, we chose the decoding module used in [28, <cite>39]</cite> : a two-layers LSTM network with a copy mechanism.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_8",
  "x": "Our model follows the encoder-decoder architecture [2] . Because our contribution focuses on the encoding process, we chose the decoding module used in [28, <cite>39]</cite> : a two-layers LSTM network with a copy mechanism. In order to supervise this mechanism, we assume that each record value that also appears in the target is copied from the data-structure and we train the model to switch between freely generating words from the vocabulary and copying words from the input.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_9",
  "x": "As outlined in Section 2, most previous work [16, 28, 29,<cite> 39,</cite> 40 ] make use of flat encoders that do not exploit the data structure. To keep the semantics of each element from the data-structure, we propose a hierarchical encoder which relies on two modules. The first one (module A in Figure 2) is called low-level encoder and encodes entities on the basis of their records; the second one (module B), called high-level encoder, encodes the data-structure on the basis of its underlying entities.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_10",
  "x": "To keep the semantics of each element from the data-structure, we propose a hierarchical encoder which relies on two modules. The first one (module A in Figure 2) is called low-level encoder and encodes entities on the basis of their records; the second one (module B), called high-level encoder, encodes the data-structure on the basis of its underlying entities. In the low-level encoder, the traditional embedding layer is replaced by a record embedding layer as in [18, 28, <cite>39]</cite> .",
  "y": "similarities"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_11",
  "x": "The first layer of the network consists in learning two embedding matrices to embed the record keys and values. Keys k i,j are embedded to k i,j \u2208 R d and values v i,j to v i,j \u2208 R d , with d the size of the embedding. As in previous work [18, 28, <cite>39]</cite> , each record embedding r i,j is computed by a linear projection on the concatenation [k i,j ; v i,j ] followed by a non linearity:",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_12",
  "x": "To evaluate the effectiveness of our model, and demonstrate its flexibility at handling heavy data-structure made of several types of entities, we used the Ro-toWire dataset<cite> [39]</cite> . It includes basketball games statistical tables paired with journalistic descriptions of the games, as can be seen in the example of Figure 1 . The descriptions are professionally written and average 337 words with a vocabulary size of 11.3K.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_13",
  "x": "We evaluate our model through two types of metrics. The BLEU score [23] aims at measuring to what extent the generated descriptions are literally closed to the ground truth. The second category designed by<cite> [39]</cite> is more qualitative.",
  "y": "uses background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_14",
  "x": "To do so, we follow the protocol presented in<cite> [39]</cite> . First, we apply an information extraction (IE) system trained on labeled relations from the gold descriptions of the RotoWire train dataset. Entity-value pairs are extracted from the descriptions.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_15",
  "x": "We compare our hierarchical model against three systems. For each of them, we report the results of the best performing models presented in each paper. \u2022 Wiseman<cite> [39]</cite> is a standard encoder-decoder system with copy mechanism.",
  "y": "background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_16",
  "x": "We compare our hierarchical model against three systems. \u2022 Wiseman<cite> [39]</cite> is a standard encoder-decoder system with copy mechanism.",
  "y": "uses background"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_17",
  "x": "---------------------------------- **IMPLEMENTATION DETAILS** The decoder is the one used in [28, 29, <cite>39]</cite> with the same hyper-parameters.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_18",
  "x": "The decoder is the one used in [28, 29, <cite>39]</cite> with the same hyper-parameters. For the encoder module, both the low-level and high-level encoders use a two-layers multi-head self-attention with two heads. To fit with the small number of record keys in our dataset<cite> (39)</cite> , their embedding size is fixed to 20.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_19",
  "x": "Our results on the RotoWire testset are summarized in Table 1 . For each proposed variant of our architecture, we report the mean score over ten runs, as well as the standard deviation in subscript. Results are compared to baselines [28, 29, <cite>39]</cite> and variants of our models.",
  "y": "uses"
 },
 {
  "id": "f34768f61dd3d95648ad9a70e83d2c_20",
  "x": "As reflected in Table 1 , the number of correct mentions (in green) outweights the number of incorrect mentions (in red). Please note that, as in previous work [16, 28, 29, <cite>39]</cite> , generated texts still contain a number of incorrect facts, as well hallucinations (in blue): sentences that have no basis in the input data (e.g. \"[...] he's now averaging 22 points [...].\"). While not the direct focus of our work, this highlights that any operation meant to enrich the semantics of structured data can also enrich the data with incorrect facts.",
  "y": "background"
 },
 {
  "id": "f36b605a9088532e5f430c86ffb363_0",
  "x": "**INTRODUCTION** Vector space models, representing word meanings as points in high-dimensional space, have been used in a variety of semantic relatedness tasks (Sahlgren, 2006; <cite>Pad\u00f3 and Lapata, 2007)</cite> . Graphs are another way of representing relations between linguistic entities, and they have been used to capture semantic relatedness by using both corpus-based evidence and the graph structure of WordNet and Wikipedia (Pedersen et al., 2004; Widdows and Dorow, 2002; Minkov and Cohen, 2008) .",
  "y": "background"
 },
 {
  "id": "f36b605a9088532e5f430c86ffb363_1",
  "x": "Vector space models, representing word meanings as points in high-dimensional space, have been used in a variety of semantic relatedness tasks (Sahlgren, 2006; <cite>Pad\u00f3 and Lapata, 2007)</cite> . We study the relationship between vector space models and graph random walk models by embedding vector space models in graphs.",
  "y": "uses background"
 },
 {
  "id": "f36b605a9088532e5f430c86ffb363_2",
  "x": "Implementation: We extract tuples from the 2-billion word ukWaC corpus, 1 dependency-parsed with MINIPAR. 2 Following<cite> Pad\u00f3 and Lapata (2007)</cite> , we only consider co-occurrences where two target words are connected by certain dependency paths, namely: the top 30 most frequent preposition-mediated noun-to-noun paths (soldier+with+gun), the top 50 transitive-verbmediated noun-to-noun paths (soldier+use+gun), the top 30 direct or preposition-mediated verbnoun paths (kill+obj+victim, kill+in+school), and the modifying and predicative adjective-to-noun paths. Pairs (w 1 , w 2 ) that account for 0.01% or less of the marginal frequency of w 1 were trimmed.",
  "y": "uses"
 },
 {
  "id": "f36b605a9088532e5f430c86ffb363_3",
  "x": "Both models' performances are comparable with the previously reported studies, and above that of random walks. Semantic priming: The next dataset comes from Hodgson (1991) and it is of interest since it requires capturing different forms of semantic relatedness between prime-target pairs: synonyms (synonym), coordinates (coord), antonyms (antonym), free association pairs (conass), superand subordinate pairs (supersub) and phrasal associates (phrasacc). Following previous simulations of this data-set<cite> (Pad\u00f3 and Lapata, 2007)</cite> , we measure the similarity of each related target-prime pair, and we compare it to the average similarity of the target to all the other primes instantiating the same relation, treating the latter quantity as our surrogate of an unrelated target-prime pair.",
  "y": "uses"
 },
 {
  "id": "f3b1a39203ebf0725d8dd2b8f8c7a9_0",
  "x": "**INTRODUCTION AND RELATED WORK** Language models can be optimized to recognize syntax and semantics with great accuracy [1] . However, the output generated can be repetitive and generic leading to monotonous or uninteresting responses (e.g \"I don't know\") regardless of the input <cite>[2]</cite> .",
  "y": "background"
 },
 {
  "id": "f3b1a39203ebf0725d8dd2b8f8c7a9_1",
  "x": "Our experiments suggest that generative text models, while very good at encapsulating semantic, syntactic and domain information, perform better with external feedback from a discriminator for fine-tuning objectiveless decoding tasks like that of creative text. We show this by evaluating our model on three very different creative datasets containing poetry, metaphors and lyrics. Previous work on handling the shortcomings of MLE include length-normalizing sentence probability [6] , future cost estimation [7] , diversity-boosting objective function [8,<cite> 2]</cite> or penalizing repeating tokens [9] .",
  "y": "background"
 },
 {
  "id": "f3b1a39203ebf0725d8dd2b8f8c7a9_2",
  "x": "GumbelGAN [15] uses Gumbel-Softmax distribution that replaces the non-differentiable sample from a categorical distribution with a differentiable sample to propagate stronger gradients. Li et al. <cite>[2]</cite> use a discriminator for a diversity promoting objective. Yu et al. [16] use SeqGAN to generate poetry and comment on the performance of SeqGAN over MLE in human evaluations, encouraging our study of GANs for creative text generation.",
  "y": "background"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_0",
  "x": "Topic models have proven to be an elegant way to build exploratory interfaces (i.e. topic browsers) for visualizing document collections by presenting to the users lists of topics [6, 15, 14] where they select documents of a particular topic of interest. A topic is traditionally represented by a list of t terms with the highest probability. In recent works, short phrases [11, 4] , images<cite> [3]</cite> or summaries [19] have been used as alternatives.",
  "y": "background"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_1",
  "x": "Particularly, images offer a language independent representation of the topic which can also be complementary to textual labels. The visual representation of a topic has been shown to be as effective as the textual labels on retrieving information using a topic browser while it can be understood quickly by the users [1, 2] . The task of labeling topics consists of two main components: (1) a candidate generation component where candidate labels are obtained for a given topic (usually using information retrieval techniques and knowledge bases [11, <cite>3]</cite> ), and (2) a ranking (or label selection) component that scores the candidates according to their relevance to the topic.",
  "y": "background"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_2",
  "x": "In the case of labeling topics with images the candidate labels consist of images. The method presented by<cite> [3]</cite> generates a graph where the candidate images are its nodes. The edges are weighted with a similarity score between the images that connect.",
  "y": "background"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_3",
  "x": "The visual information from the image V is converted into a dense vectorized representation, x v . That is the output of the publicly available 16-layer VGG-net [1<cite>3]</cite> trained over the ImageNet dataset [9] . VGG-net provides a 1000 dimensional vector which is the soft-max classification output of ImageNet classes.",
  "y": "uses"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_4",
  "x": "We evaluate our model on the publicly available data set provided by<cite> [3]</cite> . It consists of 300 topics generated using Wikipedia articles and news articles taken from the New York Times. Each topic is represented by ten terms with the highest probability.",
  "y": "uses"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_5",
  "x": "The task is to choose the image with the highest rating from the set of the 20 candidates for a given topic. The 20 candidate image labels per topic are collected by<cite> [3]</cite> using an information retrieval engine (Google). Hence most of them are expected to be relevant to the topic.",
  "y": "uses"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_6",
  "x": "For each topic we sample another 20 images from random topics in the training set and assign them a relevance score of 0. These extra images are added into the training data. Our evaluation follows prior work [11, <cite>3]</cite> using two metrics.",
  "y": "uses"
 },
 {
  "id": "f3c2c538019b1d9daa8e6c932d9826_7",
  "x": "---------------------------------- **RESULTS AND DISCUSSION** We compare our approach to the state-of-the-art method that uses Personalized PageRank<cite> [3]</cite> to re-rank image candidates (Local PPR) and an adapted version that computes the PageRank scores of all the available images in the test set (Global PPR).",
  "y": "uses"
 },
 {
  "id": "f3e9e5d7fb4001e3d29a171b5eb4a4_1",
  "x": "Similarly to<cite> Liang et al. (2016)</cite> , we employ a sequence-to-sequence model to learn query expressions and their compositions. Instead of inducing the programs through question-answer pairs, we expect a semi-supervised approach, where alignments between questions and queries are built through templates. Although query induction can save a considerable amount of supervision effort <cite>(Liang et al., 2016</cite>; Zhong et al., 2017) , a pseudo-gold program is not guaranteed to be correct when the same answer can be found with more than one query (e.g., as the capital is often the largest city of a country, predicates might be confused).",
  "y": "similarities uses"
 },
 {
  "id": "f3e9e5d7fb4001e3d29a171b5eb4a4_2",
  "x": "Similarly to<cite> Liang et al. (2016)</cite> , we employ a sequence-to-sequence model to learn query expressions and their compositions. Instead of inducing the programs through question-answer pairs, we expect a semi-supervised approach, where alignments between questions and queries are built through templates. Although query induction can save a considerable amount of supervision effort <cite>(Liang et al., 2016</cite>; Zhong et al., 2017) , a pseudo-gold program is not guaranteed to be correct when the same answer can be found with more than one query (e.g., as the capital is often the largest city of a country, predicates might be confused).",
  "y": "background"
 },
 {
  "id": "f3e9e5d7fb4001e3d29a171b5eb4a4_3",
  "x": "Knowledge graph jointly embedded with SPARQL operators (Wang et al., 2014) can be utilized in the target space. A curriculum learning (Bengio et al., 2009 ) paradigm can learn graph pattern and SPARQL operator composition, in a similar fashion of<cite> Liang et al. (2016)</cite> . We argue that the coverage of language utterances can be expanded using techniques such as Question (Abujabal et al., 2017; Elsahar et al., 2018; Abujabal et al., 2018) and Query Generation (Zafar et al., 2018) as well as Universal Sentence Encoders (Cer et al., 2018 ).",
  "y": "background"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_0",
  "x": "A more recent step was taken to move beyond distributed representation of words. This is to find a distributed representation for sentences, paragraphs and documents. Most of the presented works study the interrelationship between words in a text snip- pet (Hill et al., 2016; Kiros et al., 2015; <cite>Le and Mikolov, 2014)</cite> in an unsupervised fashion.",
  "y": "background"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_1",
  "x": "This makes our work distinguished from to the work of<cite> (Le and Mikolov, 2014</cite>; Hill et al., 2016; Kiros et al., 2015) where they study the interrelationship of words in the text snippet. ---------------------------------- **TOY EXAMPLE**",
  "y": "differences"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_2",
  "x": "**TOY EXAMPLE** We show a toy example to highlight the differences between DoCoV vector, the Mean vector and paragraph vector<cite> (Le and Mikolov, 2014)</cite> . First, we used Gensim library 1 to generate word vectors and paragraph vectors using a dummy training corpus.",
  "y": "differences"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_3",
  "x": "Below we describe our motivation towards the proposal of our novel representation: (1) Some neural-based paragraph representations such as paragraph vectors<cite> (Le and Mikolov, 2014)</cite> , FastSent (Hill et al., 2016) use a shared space between the words and paragraphs. This is counter intuitive, as the paragraph is a different entity other than the words.",
  "y": "background"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_4",
  "x": "Figure 1 illustrates that point, we do not see a clear interpretation of why the paragraph vectors<cite> (Le and Mikolov, 2014)</cite> are positioned in the space as in figure 1 . (2) The covariance matrix represents the second order summary statistic of multivariate data. This distinguishes the covariance matrix from the mean vector.",
  "y": "similarities"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_5",
  "x": "(4) The computation of the covariance descriptor is known to be fast and highly parallelizable. Moreover, there is no inference steps involved while computing the covariance matrix given its observations. This is an advantage compared to existing methods for generating paragraph vectors, such as<cite> (Le and Mikolov, 2014</cite>; Hill et al., 2016) .",
  "y": "differences"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_6",
  "x": "These alternatives share the same objective of finding a fixed-length vectorized representation for words to capture the semantic and syntactic regularities between words. These efforts paved the way for many researchers to judge document similarity based on word embedding. Some efforts aimed at finding a global representation of a text snippet using a paragraph-level representation such as paragraph vectors<cite> (Le and Mikolov, 2014)</cite> .",
  "y": "similarities"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_7",
  "x": "**DATASETS AND BASELINES** We contrast our results against the methods reported in (Hill et al., 2016) . The competing methods are the paragraph vectors<cite> (Le and Mikolov, 2014)</cite> , skip-thought vectors (Kiros et al., 2015) , Fastsent (Hill et al., 2016) , Sequential (Denoising) Autoencoders (SDAE) (Hill et al., 2016) .",
  "y": "similarities uses"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_8",
  "x": "We observe that DoCoV representation outperforms other representations in this task. Other models such as skipthought vectors (Kiros et al., 2015) and SDAE (Hill et al., 2016) requires building an encoder-decoder model which takes time 3 to learn. For other models like paragraph vectors<cite> (Le and Mikolov, 2014)</cite> and Fastsent vectors (Hill et al., 2016) , they require a gradient descent inference step to compute the paragraph/sentence vectors.",
  "y": "background"
 },
 {
  "id": "f587fc2bbbf3c1327b03d556e4bc05_9",
  "x": "This means each feature is capturing different discriminating information. This justifies the choice of concatenating DoCoV with other features. We further observe that DoCoV is consistently better than the paragraph vectors<cite> (Le and Mikolov, 2014)</cite> , Fastsent and SDAE (Hill et al., 2016) .",
  "y": "differences"
 },
 {
  "id": "f5ad574acf9ea27c0be3129238fd92_0",
  "x": "**INTRODUCTION** Recent work has shown that state-of-the-art neural models of language and translation can be successfully trained on multiple languages simultaneously without changing the model architecture (\u00d6stling and Tiedemann, 2017; <cite>Johnson et al., 2017)</cite> . In some cases this leads to improved performance compared to models only trained on a specific language, suggesting that multilingual models learn to share useful knowledge crosslingually through their learned representations.",
  "y": "background"
 },
 {
  "id": "f5ad574acf9ea27c0be3129238fd92_1",
  "x": "**BACKGROUND** The recent advances in neural networks have opened the way to the design of architecturally simple multilingual models for various NLP tasks, such as language modeling or next word prediction (Tsvetkov et al., 2016; \u00d6stling and Tiedemann, 2017; Malaviya et al., 2017; Tiedemann, 2018) , translation (Dong et al., 2015; Zoph et al., 2016; Firat et al., 2016; <cite>Johnson et al., 2017)</cite> , morphological reinflection (Kann et al., 2017) and more (Bjerva, 2017) . A practical benefit of training models multilingually is to transfer knowledge from high-resource languages to lowresource ones and improve task performance in the latter.",
  "y": "background"
 },
 {
  "id": "f5ad574acf9ea27c0be3129238fd92_2",
  "x": "---------------------------------- **EXPERIMENT** We consider the scenario where L1 is overresourced compared to L2 and train our bilingual models by joint training on a mixed L1/L2 corpus so that supervision is provided simultaneously in the two languages (\u00d6stling and Tiedemann, 2017; <cite>Johnson et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "f5bf9a833c3d46b00d70498e4f1c1b_1",
  "x": "Each node contains a probability distribution over labels, which may be empty for unlabelled nodes, and these labels are propagated over the graph in an iterative fashion. Modified Adsorption (mad) [6] , is an extension that allows more control of the random walk through the graph. Applications of lp and mad are varied, including video recommendation [1] and sentiment analysis over Twitter<cite> [5]</cite> .",
  "y": "background"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_0",
  "x": "****FRUSTRATINGLY EASY SEMI-SUPERVISED DOMAIN ADAPTATION**** **ABSTRACT** In this work, we propose a semisupervised extension to a well-known supervised domain adaptation approach (EA) <cite>(Daum\u00e9 III, 2007)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_1",
  "x": "---------------------------------- **INTRODUCTION** A domain adaptation approach for sequential labeling tasks in NLP was proposed in <cite>(Daum\u00e9 III, 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_2",
  "x": "However, the proposed techniques extend to non-linear hypotheses, as mentioned in <cite>(Daum\u00e9 III, 2007)</cite> . Source and target empirical errors for hypothesis h are denoted b\u0177 \u01eb s (h, f s ) and\u01eb t (h, f t ) respectively, where f s and f t are source and target labeling functions. Similarly, the corresponding expected errors are denoted by \u01eb s (h, f s ) and \u01eb t (h, f t ).",
  "y": "extends differences"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_3",
  "x": "---------------------------------- **EASYADAPT (EA)** In this section, we give a brief overview of EASYADAPT proposed in <cite>(Daum\u00e9 III, 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_4",
  "x": "Almost any standard supervised learning approach for linear classifiers (for e.g., SVMs, perceptrons) can be used to learn a linear hypothesish \u2208 R 3d in the augmented space. As mentioned earlier, this work considers linear hypotheses only and the the proposed techniques can be extended <cite>(Daum\u00e9 III, 2007)</cite> to non-linear hypotheses. Let us denot\u0207 h = h c , h s , h t , where each of h c , h s , h t is of dimension d and represent the common, sourcespecific and target-specific components ofh, respectively.",
  "y": "extends differences"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_5",
  "x": "During prediction on target data, the incoming target feature x is transformed to obtain \u03a6 t (x) andh is applied on this transformed feature. This is equivalent to applying (h c + h t ) on x. A good intuitive insight into why this simple algorithm works so well in practice and outperforms most state-of-the-art algorithms is given in <cite>(Daum\u00e9 III, 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_6",
  "x": "The commonality between the domains is represented by h c whereas the source and target domain specific information is captured by h s and h t , respectively. This technique can be easily extended to a multi-domain scenario by making more copies of the original feature space ((K + 1) copies in case of K domains). A kernelized version of the algorithm has also been presented in <cite>(Daum\u00e9 III, 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_7",
  "x": "We follow the same experimental setup used in <cite>(Daum\u00e9 III, 2007)</cite> and perform two sequence labelling tasks (a) named-entity-recognition (NER), and (b) part-of-speech-tagging (POS )on the following datasets: PubMed-POS: Introduced by (Blitzer et al., 2006) , this dataset consists of two domains. task is to perform part-of-speech tagging on unlabeled PubMed abstracts with a classifier trained on labeled WSJ and PubMed data.",
  "y": "similarities uses"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_8",
  "x": "We also note that EA performs poorly for some cases, as was shown <cite>(Daum\u00e9 III, 2007)</cite> earlier. ---------------------------------- **SUMMARY**",
  "y": "similarities"
 },
 {
  "id": "f5d1c0d3ac45ea4949f7d01d1704f6_9",
  "x": "We term such algorithms as Feature Sharing Algorithms. Feature sharing algorithms are effective for domain adaptation because they are simple, easy to implement as a preprocessing step and outperform many existing state-of-the-art techniques (shown previously for domain adaptation <cite>(Daum\u00e9 III, 2007)</cite> ). However, despite their simplicity and empirical success, it is not theoretically apparent why these algorithms perform so well.",
  "y": "background"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_0",
  "x": "Subsequent work has proposed alternative optimization objectives to learn better mappings. <cite>Xing et al. (2015)</cite> incorporate length normalization in the training of word embeddings and try to maximize the cosine similarity instead, introducing an orthogonality constraint to preserve the length normalization after the projection. Faruqui and Dyer (2014) use canonical correlation analysis to project the embeddings in both languages to a shared vector space.",
  "y": "background"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_1",
  "x": "We start with a basic optimization objective (Mikolov et al., 2013b) and introduce several meaningful and intuitive constraints that are equivalent or closely related to previously proposed methods (Faruqui and Dyer, 2014; <cite>Xing et al., 2015)</cite> . Our framework provides a more general view of bilingual word embedding mappings, showing the underlying connection between the existing methods, revealing some flaws in their theoretical justification and providing an alternative theoretical interpretation for them. Our experiments on an existing English-Italian word translation induction and an English word analogy task give strong empirical evidence in favor of our theoretical reasoning, while showing that one of our models clearly outperforms previous alternatives.",
  "y": "similarities"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_2",
  "x": "This last optimization objective coincides with <cite>Xing et al. (2015)</cite> , but <cite>their work</cite> was motivated by an hypothetical inconsistency in Mikolov et al. (2013b) , where the optimization objective to learn word embeddings uses dot product, the objective to learn mappings uses Euclidean distance and the similarity computations use cosine. However, the fact is that, as long as W is orthogonal, optimizing the squared Euclidean distance of length-normalized embeddings is equivalent to optimizing the cosine, and therefore, the mapping objective proposed by <cite>Xing et al. (2015)</cite> is equivalent to that used by Mikolov et al. (2013b) with orthogonality constraint and unit vectors. In fact, our experiments show that orthogonality is more relevant than length normalization, in contrast to <cite>Xing et al. (2015)</cite> , <cite>who introduce</cite> orthogonality only to ensure that unit length is preserved after mapping.",
  "y": "differences"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_3",
  "x": "We implemented the proposed method in Python using NumPy, and make it available as an open source project 5 . The code for Mikolov et al. (2013b) and <cite>Xing et al. (2015)</cite> is not publicly available, so we implemented and tested them as part of the proposed framework, which only differs from the original systems in the optimization method (exact solution instead of gradient descent) and the length normalization approach in the case of <cite>Xing et al. (2015)</cite> (postprocessing instead of constrained training). As for the method by Faruqui and Dyer (2014) , we used their original implementation in Python and MAT-LAB 6 , which we extended to cover cases where the dictionary contains more than one entry for the same word.",
  "y": "uses"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_4",
  "x": "As discussed before, (Mikolov et al., 2013b) and <cite>(Xing et al., 2015)</cite> were implemented as part of our framework, so they correspond to our uncostrained mapping with no preprocessing and orthogonal mapping with length normalization, respectively. ---------------------------------- **COMPARISON TO OTHER WORK**",
  "y": "similarities uses"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_5",
  "x": "**COMPARISON TO OTHER WORK** As it can be seen, the method by <cite>Xing et al. (2015)</cite> performs better than that of Mikolov et al. (2013b) in the translation induction task, which is in line with what <cite>they report</cite> in their paper. Moreover, thanks to the orthogonality constraint <cite>their monolingual performance</cite> in the word analogy task does not degrade, whereas the accuracy of Mikolov et al. (2013b) drops by 2.86% in absolute terms with respect to the original embeddings.",
  "y": "background"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_6",
  "x": "Since Faruqui and Dyer (2014) Mikolov et al. (2013b) 34.93% 73. 80% <cite>Xing et al. (2015)</cite> 36.87% 76.66% Faruqui and Dyer (2014) CCA to perform dimensionality reduction, we tested several values for it and report the best (180 dimensions). This beats the method by <cite>Xing et al. (2015)</cite> in the bilingual task, although it comes at the price of a considerable degradation in monolingual quality. In any case, it is our proposed method with the orthogonality constraint and a global preprocessing with length normalization followed by dimensionwise mean centering that achieves the best accuracy in the word translation induction task.",
  "y": "differences"
 },
 {
  "id": "f6694f359ae948b6e4563b927a672c_7",
  "x": "**CONCLUSIONS** This paper develops a new framework to learn bilingual word embedding mappings, generalizing previous work and providing an efficient exact method to learn the optimal transformation. Our experiments show the effectiveness of the proposed model and give strong empirical evidence in favor of our reinterpretation of <cite>Xing et al. (2015)</cite> and Faruqui and Dyer (2014) .",
  "y": "extends differences"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_0",
  "x": "<cite>(Herbelot and Vecchi, 2015)</cite> explored word embeddings and their utility for modeling language semantics. In particular, they presented an approach to automatically map a standard distributional semantic space onto a set-theoretic model using partial least squares regression. We show in this paper that a simple baseline achieves a +51% relative improvement compared to their model on one of the two datasets they used, and yields competitive results on the second dataset.",
  "y": "background"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_1",
  "x": "Understanding what information word embeddings contain is subsequently of high interest. <cite>(Herbelot and Vecchi, 2015)</cite> investigated a method to map word embeddings to formal semantics, which is the center of interest of this paper. Specifically, given a feature and a word vector of a concept, they tried to automatically find how often the given concept has the given feature.",
  "y": "uses background"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_2",
  "x": "---------------------------------- **TASK** In this section, we summarize the task presented in <cite>(Herbelot and Vecchi, 2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_3",
  "x": "In the previous section, we have seen how to convert a concept into a model-theoretic vector based on human annotations. The goal of <cite>(Herbelot and Vecchi, 2015)</cite> is to analyze whether there exists a transformation from the word embedding of a concept to its model-theoretic vector, the gold standard being the human annotations. The word embeddings are taken from the word embeddings pre-trained with word2vec GoogleNews-vectors-negative300 1 (300 dimensions), which were trained on part of the Google News dataset, consisting of approximately 100 billion words.",
  "y": "background"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_4",
  "x": "The transformation used in <cite>(Herbelot and Vecchi, 2015)</cite> is based on Partial Least Squares Regression (PLSR). The PLSR is fitted on the training set: the inputs are the word embeddings for each concept, and the outputs are the model-theoretic vectors for each concept. To assess the quality of the predictions, the Spearman rank-order correlation coefficient is computed between the predictions and the gold modeltheoretic vectors, ignoring all features for which a concept has not been annotated.",
  "y": "background"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_5",
  "x": "The method should therefore not be penalized for not suggesting them. Figure 1 illustrates the model. <cite>(Herbelot and Vecchi, 2015)</cite> 's system.",
  "y": "uses"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_6",
  "x": "---------------------------------- **EXPERIMENTS** We compare <cite>(Herbelot and Vecchi, 2015)</cite> 's model (<cite>PLSR + word2vec</cite>) against three baselines: random vectors, mode, and nearest neighbor.",
  "y": "uses"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_7",
  "x": "Similarity is based on the cosine similarity of the word vectors. This is a simple nearest neighbor predictor. \u2022 Random vectors: <cite>(Herbelot and Vecchi, 2015)</cite> used pre-trained word embeddings as input to the PLSR, we instead simply use random vectors of same dimension (300, continuous uniform distribution between 0 and 1).",
  "y": "differences"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_8",
  "x": "<cite>(Herbelot and Vecchi, 2015)</cite> ) in the last row. PLSR stands for partial least squares regression, NN for nearest neighbor, ppdb for the Paraphrase Database (Ganitkevitch et al., 2013) . There are two ways to compute the mode: either taking the mode of the means of the 3 annotations (mode), or the mode for all annotations (true-mode).",
  "y": "uses"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_9",
  "x": "6 Results and discussion Table 1 presents the results, using the Spearman correlation as the performance metric. The experiment was coded in Python using scikit-learn (Pedregosa et al., 2011) and the source as well as the complete result log and the two datasets are available online 3 . We could reproduce the results for the QMR dataset using PLSR and word2vec embeddings (0.346 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.332 in our experiments, but we could not ex-3 https://github.com/Franck-Dernoncourt/ model-theoretic actly reproduce the results for the AD dataset (0.634 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.572 in our experiments): this discrepancy most likely results from the choice of the training set.",
  "y": "uses"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_10",
  "x": "For each run, a train/test split was randomly chosen (60 training samples for AD, 400 for QMR, in order to have the same number of training samples as in <cite>(Herbelot and Vecchi, 2015)</cite> 's Table 2 ). ---------------------------------- **AD**",
  "y": "similarities"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_11",
  "x": "We could reproduce the results for the QMR dataset using PLSR and word2vec embeddings (0.346 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.332 in our experiments, but we could not ex-3 https://github.com/Franck-Dernoncourt/ model-theoretic actly reproduce the results for the AD dataset (0.634 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.572 in our experiments): this discrepancy most likely results from the choice of the training set. Our experiments' results are averaged over 1000 runs, and for each run the training/test split is randomly chosen, the only constraint being having the same number of training samples as in <cite>(Herbelot and Vecchi, 2015)</cite> . For the AD dataset, our worst run achieved 0.435, and our best run achieved 0.713, which emphasizes the lack of robustness of the results with respect to the train/test split.",
  "y": "similarities"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_12",
  "x": "QMR dataset (min: 0.244; max: 0.407), which is expected since QMR is significantly larger than AD. Furthermore, the mode baseline yields results that are good on the AD dataset (0.554, vs. 0.634 in <cite>(Herbelot and Vecchi, 2015)</cite> vs. 0.572 in our PLSR + word2vec implementation), and significantly better than all other models on the QMR dataset (0.522, vs. 0.346 in <cite>(Herbelot and Vecchi, 2015)</cite> , i.e. +51% improvement). To get an intuition of why the mode baseline works well, Figures 2 and 3 show that most features tend to have one clearly dominant quantifier in the AD dataset.",
  "y": "differences"
 },
 {
  "id": "f7e80cf0a6724675cab2825cbf7e10_13",
  "x": "**CONCLUSION** In this paper we have presented several baselines for mapping distributional to model-theoretic semantic spaces. The mode baseline significantly outperforms <cite>(Herbelot and Vecchi, 2015)</cite> 's model on the QMR dataset, and yields competitive results on the AD dataset.",
  "y": "differences"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_0",
  "x": "A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006) , often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al., 2003) , and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones <cite>(Yannakoudakis et al., 2011)</cite> . As multiple factors influence the linguistic quality of texts, such systems exploit features that correspond to different properties of texts, such as grammar, style, vocabulary usage, topic similarity, and discourse coherence and cohesion. Cohesion refers to the use of explicit linguistic cohesive devices (e.g., anaphora, lexical semantic relatedness, discourse markers, etc.) within a text that can signal primarily suprasentential discourse relations between textual units (Halliday and Hasan, 1976) .",
  "y": "background"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_2",
  "x": "We examine the predictive power of a number of different coherence models by measuring the effect on performance when combined with an AA system that achieves state-of-the-art results, but does not use discourse coherence features. Specifically, we describe a number of different experiments improving on the AA system presented in<cite> Yannakoudakis et al. (2011)</cite> ; AA is treated as a rank preference supervised learning problem and ranking Support Vector Machines (SVMs) (Joachims, 2002) are used to explicitly model the grade relationships between scripts. This system uses a number of different linguistic features that achieve good performance on the AA task.",
  "y": "extends"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_4",
  "x": "---------------------------------- **DATASET & EXPERIMENTAL SETUP** We use the First Certificate in English (FCE) ESOL examination scripts 2 (upper-intermediate level assessment) described in detail in<cite> Yannakoudakis et al. (2011)</cite> , extracted from the Cambridge Learner Corpus 3 (CLC).",
  "y": "uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_5",
  "x": "For all experiments, we use a series of 5-fold cross-validation runs on 1,141 texts from the examination year 2000 to evaluate performance as well as generalization of numerous models. Moreover, we identify the best model on year 2000 and we also test it on 97 texts from the examination year 2001, previously used in<cite> Yannakoudakis et al. (2011)</cite> to report the best published results. Validating the results on a different examination year tests generalization to some prompts not used in 2000, and also allows us to test correlation between examiners and the AA system.",
  "y": "uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_7",
  "x": "As in<cite> Yannakoudakis et al. (2011)</cite> , we analyze all texts using the RASP toolkit (Briscoe et al., 2006) 4 . ---------------------------------- **'SUPERFICIAL' PROXIES**",
  "y": "similarities uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_8",
  "x": "---------------------------------- **PART-OF-SPEECH (POS) DISTRIBUTION** The AA system described in<cite> Yannakoudakis et al. (2011)</cite> exploited features based on POS tag sequences, but did not consider the distribution of POS types across grades.",
  "y": "background"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_9",
  "x": "We explore the utility of inter-sentential feature types for assessing discourse coherence. Among the features used in<cite> Yannakoudakis et al. (2011)</cite> , none explicitly captures coherence and none models intersentential relationships. Incremental Semantic analysis (ISA) (Baroni et al., 2007) is a word-level distributional model that induces a semantic space from input texts.",
  "y": "differences"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_10",
  "x": "In particular, we use the system described in<cite> Yannakoudakis et al. (2011)</cite> as our baseline AA system. Discourse coherence is a strong indicator of thorough knowledge of a second language and thus we expect coherence features to further improve performance of AA systems. We evaluate the grade predictions of our models against the gold standard grades in the dataset using Pearson's product-moment correlation coeffi-16 http://goo.gl/yQ0Q0 cient (r) and Spearman's rank correlation coefficient (\u03c1) as is standard in AA research (Briscoe et al., 2010) .",
  "y": "uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_11",
  "x": "Moreover, combining all feature classes together in row 17 does not yield higher results than those obtained with ISA, while \u03c1 is no better than the baseline. In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in<cite> Yannakoudakis et al. (2011)</cite> to report results of the final best system. Validating the model on a different exam year also shows us the extent to which it generalizes between years.",
  "y": "uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_12",
  "x": "Again, our extended model improves over the baseline. Finally, we explore the utility of our best model for assessing the publically available 'outlier' texts used in<cite> Yannakoudakis et al. (2011)</cite> . The previous AA system is unable to downgrade appropriately 'outlier' scripts containing individually high-scoring sentences with poor overall coherence, created by randomly ordering a set of highly-marked texts.",
  "y": "uses"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_14",
  "x": "ISA, LOWBOW, the POS IBM model and word length are the best individual features for assessing coherence. A significant improvement over the AA system presented in<cite> Yannakoudakis et al. (2011)</cite> and the best published result on the FCE dataset was obtained by augmenting the system with an ISA-based local coherence feature. However, it is quite likely that further experimentation with LOWBOW features, given the large range of possible parameter settings, would yield better results too.",
  "y": "differences"
 },
 {
  "id": "f8c992a887a7b7af8b3aa45f72dca7_15",
  "x": "Our contribution is threefold: 1) we present the first systematic analysis of several methods for assessing discourse coherence in the framework of AA of learner free-text responses, 2) we identify new discourse features that serve as proxies for the level of (in)coherence in texts and outperform previously developed techniques, and 3) we improve the best results reported by<cite> Yannakoudakis et al. (2011)</cite> on the publically available 'English as a Second or Other Language' (ESOL) corpus of learner texts (to date, this is the only public-domain corpus that contains grades). Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in<cite> Yannakoudakis et al. (2011)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_0",
  "x": "---------------------------------- **INTRODUCTION** Unsupervised machine translation has become an emerging research interest in recent years (Artetxe et al., 2017; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019;<cite> Lample and Conneau, 2019)</cite> .",
  "y": "background"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_1",
  "x": "performance as pointed in , Artetxe et al. (2018b) and Ren et al. (2019) . Previous approaches benefit mostly from crosslingual n-gram embeddings, but recent work proves that cross-lingual language model pretraining could be a more effective way to build initial unsupervised machine translation models <cite>(Lample and Conneau, 2019)</cite> . However, in their method, the cross-lingual information is mostly obtained from shared Byte Piece Encoding (BPE) (Sennrich et al., 2016b) spaces during pre-training, which is inexplicit and limited.",
  "y": "background motivation"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_2",
  "x": "Based on BERT,<cite> Lample and Conneau (2019)</cite> propose a cross-lingual version called XLM and reach the state-of-the-art performance on some crosslingual NLP tasks including cross-lingual classification , machine translation, and unsupervised cross-lingual word embedding. The basic points of XLM are mainly two folds. The first one is to use a shared vocabulary of BPE (Sennrich et al., 2016b ) to provide potential crosslingual information between two languages just as mentioned in , in an inexplicit way though.",
  "y": "background"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_3",
  "x": "For each source ngram, all of the retrieved k translation candidates are used to calculate the cross entropy loss, which are weighted with their translation probabilities in the n-gram table. Given a language pair X \u2212 Y , we process both languages with the same shared BPE vocabulary using their monolingual sentences together during pre-training. Following Devlin et al. (2018) ;<cite> Lample and Conneau (2019)</cite> , in our CMLM objective, we randomly sample 15% of the BPE ngrams from the text streams, and replace them by [MASK] tokens 70% of the time.",
  "y": "extends differences"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_4",
  "x": "It is also interesting to explore the usage of pre-trained decoders in the translation model. It seems that pre-training decoders has a smaller effect on the final performance than pre-training encoders <cite>(Lample and Conneau, 2019)</cite> , one reason for which could be that the encoder-to-decoder attention is not pre-trained. Therefore, the parameters of the decoder need to be re-adjusted substantially in the following tuning process for MT task.",
  "y": "differences"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_5",
  "x": "Our CMLM is optimized based on the pre-trained models released by<cite> Lample and Conneau (2019)</cite> 1 , which are trained with Wikipedia dumps. For fair comparison, we use newstest 2014 as the test set for en-fr, and newstest 2016 for en-de and en-ro. 1 https://github.com/facebookresearch/XLM We use Moses scripts for tokenization, and use fastBPE 2 to split words into subword units with their released BPE codes 1 .",
  "y": "similarities uses"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_6",
  "x": "The results indicate that the explicit cross-lingual information incorporated by our proposed CMLM is beneficial to the unsupervised machine translation task. Notice that by doing another iteration (\"Iter 2\") with updated ngram tables as described in Section 3.4, we further improve the performance a bit for most translation directions with the improvements of en2fr and ro2en bigger than 0.5 BLEU point, which confirms the potential that fine-tuned machine translation models contain more beneficial cross-lingual information than the initial n-gram translation tables, which can be used to enhance the pre-trained model iteratively. The improvement made by<cite> Lample and Conneau (2019)</cite> compared with the first five baselines shows that cross-lingual pre-training can be necessary for unsupervised MT.",
  "y": "future_work"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_7",
  "x": "However, the crosslingual information learned with this method during pre-training is mostly from the shared subword space, which is inexplicit and not strong enough. Our proposed method can give the model more explicit and strong cross-lingual training signals so that the pre-trained model contains much beneficial cross-lingual information for unsupervised machine translation. As a result, we can further improve the translation performance significantly, compared with<cite> Lample and Conneau (2019)</cite> (with the significance level of p<0.01).",
  "y": "future_work"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_8",
  "x": "Given two parallel sentences in English and French respectively, we feed each sentence into the pre-trained cross-lingual encoder and get its respective outputs. Then, we calculate the similarities between the outputs of the two sentences and choose target words with max similarity scores as the alignments of corre- sponding source words. We compare the context-unaware method (i.e., directly calculating the similarity scores between unsupervised cross-lingual embeddings (Artetxe et al., 2018a ) of source and target words), XLM <cite>(Lample and Conneau, 2019)</cite> and our proposed CMLM pre-training method in the Table 3 .",
  "y": "similarities"
 },
 {
  "id": "fa5413db2c8e0a32bc3805d25cd0e7_10",
  "x": "also build a hybrid system by combining the best pseudo data that SMT models generate into the training of the NMT model while Ren et al. (2019) alternately train SMT and NMT models with the framework of posterior regularization. More recently,<cite> Lample and Conneau (2019)</cite> reach new state-of-the-art performance on unsupervised en-fr and en-de translation tasks. They propose a cross-lingual language model pretraining method based on BERT (Devlin et al., 2018) , and then treat two cross-lingual language models as the encoder and decoder to finish the translation.",
  "y": "background"
 },
 {
  "id": "fa641aca676761c79c0469c195f336_0",
  "x": "It is only very recently that some groups of researchers looked into readability assessment in Bengali. They observed that English readability formulas did not work well on Bengali texts <cite>[11]</cite> , [21] . This observation is not surprising, because Bengali is very different than English.",
  "y": "background"
 },
 {
  "id": "fa641aca676761c79c0469c195f336_1",
  "x": "We found only three lines of work that specifically looked into Bengali readability [6] , <cite>[11]</cite> , [21] . Das and Roychoudhury worked with a miniature model of two parameters in their pioneering study [6] . They found that the two-parameter model was a better predictor of readability than the one-parameter model.",
  "y": "background"
 },
 {
  "id": "fa641aca676761c79c0469c195f336_2",
  "x": "They further showed that English readability indices were inadequate for Bengali, and built their own readability model on 16 texts. Around the same time, Islam et al. independently reached the same conclusion <cite>[11]</cite> . They designed a Bengali readability classifier on lexical and information-theoretic features, resulting in an F-score 50% higher than that from traditional scoring approaches.",
  "y": "background"
 },
 {
  "id": "fa641aca676761c79c0469c195f336_3",
  "x": "We found only three lines of work that specifically looked into Bengali readability [6] , <cite>[11]</cite> , [21] . Around the same time, Islam et al. independently reached the same conclusion <cite>[11]</cite> . While all the above studies are very important and insightful, none of them explicitly performed an inter-rater agreement study. Further, none of these studies made available their readability-annotated gold standard datasets, thereby stymieing further research. We attempt to bridge these gaps in our work.",
  "y": "motivation"
 },
 {
  "id": "fa641aca676761c79c0469c195f336_4",
  "x": "An important limitation of our study is the small corpus size. We only have 30 annotated passages at our disposal, whereas Islam et al. <cite>[11]</cite> had around 300. But Islam et al.'s dataset is not annotated in as fine-grained a fashion as ours. Note also that our dataset is larger than both Sinha et al.'s 16document dataset [21] , and Das and Roychoudhury's seven document dataset [6] .",
  "y": "differences"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_0",
  "x": "Van Hee et al. (2015b) identifies discriminative remarks (racist, sexist) as a subset of \"insults\", whereas Nobata et al. (2016) classifies similar remarks as \"hate speech\" or \"derogatory language\". Waseem and Hovy (2016) only consider \"hate speech\" without regard to any potential overlap with bullying or otherwise offensive language, while <cite>Davidson et al. (2017)</cite> distinguish hate speech from generally offensive language. Wulczyn et al. (2017) annotates for personal attacks, which likely encompasses identifying cyberbullying, hate speech, and offensive language.",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_1",
  "x": "Waseem and Hovy (2016) only consider \"hate speech\" without regard to any potential overlap with bullying or otherwise offensive language, while <cite>Davidson et al. (2017)</cite> distinguish hate speech from generally offensive language. Wulczyn et al. (2017) annotates for personal attacks, which likely encompasses identifying cyberbullying, hate speech, and offensive language. The lack of consensus has resulted in contradictory annotation guidelines -some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al. (2016) and <cite>Davidson et al. (2017)</cite> .",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_2",
  "x": "---------------------------------- **A TYPOLOGY OF ABUSIVE LANGUAGE** Much of the work on abusive language subtasks can be synthesized in a two-fold typology that conExplicit Implicit Directed \"Go kill yourself\", \"You're a sad little f*ck\" (Van Hee et al., 2015a) , \"@User shut yo beaner ass up sp*c and hop your f*ggot ass back across the border little n*gga\"<cite> (Davidson et al., 2017)</cite> , \"Youre one of the ugliest b*tches Ive ever fucking seen\" (Kontostathis et al., 2013) .",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_3",
  "x": "Previous work has identified instances of hate speech that are both directed and generalized (Burnap and Williams, 2015; Waseem and Hovy, 2016;<cite> Davidson et al., 2017)</cite> , although Nobata et al. (2016) come closest to making a distinction between directed and generalized hate. The other dimension is the extent to which abusive language is explicit or implicit. This is roughly analogous to the distinction in linguistics and semiotics between denotation, the literal meaning of a term or symbol, and connotation, its sociocultural associations, famously articulated by Barthes (1957) .",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_4",
  "x": "This is roughly analogous to the distinction in linguistics and semiotics between denotation, the literal meaning of a term or symbol, and connotation, its sociocultural associations, famously articulated by Barthes (1957) . Explicit abusive lan-guage is that which is unambiguous in its potential to be abusive, for example language that contains racial or homophobic slurs. Previous research has indicated a great deal of variation within such language (Warner and Hirschberg, 2012;<cite> Davidson et al., 2017)</cite> , with abusive terms being used in a colloquial manner or by people who are victims of abuse.",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_5",
  "x": "Annotation (via crowd-sourcing and other methods) tends to be more straightforward when explicit instances of abusive language can be identified and agreed upon (Waseem, 2016b) , but is considerably more difficult when implicit abuse is considered (Dadvar et al., 2013; Justo et al., 2014; Dinakar et al., 2011) . The connotations of language can be difficult to classify without domainspecific knowledge. Furthermore, while some argue that detailed guidelines can help annotators to make more subtle distinctions<cite> (Davidson et al., 2017)</cite> , others find that they do not improve the reliability of non-expert classifications (Ross et al., 2016) .",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_6",
  "x": "<cite>Davidson et al. (2017)</cite> , for instance, show that annotators tend to code racism as hate speech at a higher rate than sexism. As such, it is important that researchers consider the social biases that may lead people to disregard certain types of abuse. The type of abuse that researchers are seeking to identify should guide the annotation strategy.",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_7",
  "x": "A number of studies on hate speech use part-of-speech sequences to model the expression of hatred (Warner and Hirschberg, 2012; Gitari et al., 2015;<cite> Davidson et al., 2017)</cite> . Typed dependencies offer a more sophisticated way to capture the relationship between terms (Burnap and Williams, 2015) . Overall, there are many tools that researchers can use to model the relationship between abusive language and targets, although many of these require high-quality annotations to use as training data.",
  "y": "background"
 },
 {
  "id": "faeac0a0e3c0cad79d39dea04ec59a_8",
  "x": "Further research is necessary to determine if there are underlying syntactic structures associated with generalized abusive language. Explicit abuse Explicit abuse, whether directed or generalized, is often indicated by specific keywords. Hence, dictionary-based approaches may be well suited to identify this type of abuse (Warner and Hirschberg, 2012; Nobata et al., 2016) , although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways (Kwok and Wang, 2013;<cite> Davidson et al., 2017)</cite> .",
  "y": "future_work"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_0",
  "x": "Prior work on conversation thread disentanglement is often based on pairwise message compar- ison. Some solutions use unsupervised clustering methods with hand-engineered features (Wang and Oard, 2009; Shen et al., 2006) , while others use supervised approaches with statistical (Du et al., 2017) or linguistic features (Wang et al., 2008; Wang and Ros\u00e9, 2010; Elsner and Charniak, 2008 , 2011 Mayfield et al., 2012) . Recent work by <cite>(Jiang et al., 2018</cite>; Mayfield et al., 2012) adopt deep learning approaches to compute message pair similarity, using a combination of message content and simple contextual features (e.g., authorship and timestamps).",
  "y": "background"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_1",
  "x": "We also combine them with a dynamic gate for further performance improvement, followed by an efficient beam search mechanism in the inference step. The evaluation proves our approach improves over the existing methods. The contribution of this work is two-fold: 1) We propose context-aware deep learning models for thread detection and it advances the state-ofthe-art; 2) Based on the dataset in<cite> (Jiang et al., 2018)</cite> , we develop and release a more realistic multi-party multi-thread conversation dataset for future research.",
  "y": "uses"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_2",
  "x": "These two features are also used in<cite> (Jiang et al., 2018)</cite> , and another baseline model GTM uses only these features (Elsner and Charniak, 2008) . Given a message sequence M i 1 , which has been detected with L threads i 1 indicates that m i starts a new thread.",
  "y": "similarities"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_3",
  "x": "Intuitively, if they are close, both FLOW and MATCH will be considered equally for prediction. Otherwise, the model dynamically computes the weights of MATCH and FLOW. Training Procedure: Following<cite> (Jiang et al., 2018)</cite> , apart from a new thread, we consider the candidate threads (Active Threads) in Eq. 1 only from those appearing in one hour time-frame before m i .",
  "y": "extends differences"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_4",
  "x": "**EXPERIMENTS** Datasets: We conduct extensive experiments on three publicly available datasets from Reddit datasets. We strictly follow<cite> (Jiang et al., 2018)</cite> to construct our data.",
  "y": "similarities uses"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_5",
  "x": "Reddit Dataset Improvement: We use the same pre-processing method in<cite> (Jiang et al., 2018)</cite> : we discard the messages which have less than 10 words or more than 100 words. Conversations less than 10 messages are also discarded. We guarantee that no more than 10 conversations happen at the same time.",
  "y": "similarities uses"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_6",
  "x": "We train 50 epochs and select the model with the best validation-set performance. Baseline: (1) CISIR-SHCNN<cite> (Jiang et al., 2018)</cite> : A recently proposed model based on CNN and ranking message pairs. (2) CISIR-USE: We replace CNN encoder in CISIR with a USE to test the effect of different sentence encoders.",
  "y": "background"
 },
 {
  "id": "fb1061d28dbf80858c1a630621a975_7",
  "x": "NMI ARI F1 A COMBINE (share) .832 .420 .422 B COMBINE (concat) .828 .446 .448 C SPLIT .824 .417 .420 D FLOW (K=5) .813 .395 .398 E FLOW (K=10) .823 .414 .417 F FLOW (K=20) .826 .420 .423 G MATCH (K=5) .820 .399 .402 H MATCH (K=10) .823 .405 .408 I MATCH (K=20) .831 .427 .430 J MATCH (K=20, bi-LSTM) .832 .428 .430 K COMBINE (K=5) .811 .378 .381 L COMBINE (K=10) .822 .403 .405 M COMBINE (K=20, B=1) . 828 .452 .455 N COMBINE (K=20, B=5) .834 .461 .464 O COMBINE (K=20, B=10) . 833 .431 .433 Evaluation Metrics: Normalized mutual information (NMI), Adjusted rand index (ARI) and F1 score, following<cite> (Jiang et al., 2018)</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "fc58a9813b80afc9811b8ee27679b7_0",
  "x": "For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009 ). Current approaches for learning such patterns include bootstrapping techniques<cite> (Huang and Riloff, 2012a</cite>; Yangarber et al., 2000) , weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006) , fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009 ) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge.",
  "y": "background"
 },
 {
  "id": "fc58a9813b80afc9811b8ee27679b7_1",
  "x": "Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to apply a system on a different domain with less annotated data without reconsidering the design of the features used. An important step forwards is TIER light<cite> (Huang and Riloff, 2012a</cite> ) that targeted the minimization of human supervision with a bootstrapping technique for event roles detection. Also, PIPER (Patwardhan and Riloff, 2007; Patwardhan, 2010) distinguishes between relevant and irrelevant regions and learns domain-relevant extraction patterns using a semantic affinity measure.",
  "y": "background"
 },
 {
  "id": "fc58a9813b80afc9811b8ee27679b7_2",
  "x": "In this work, we approach the event extraction task by learning word representations from a domainspecific data set and by using these representations to identify the event roles. This idea relies on the assumption that the different words used for a given event role in the text share some semantic properties, related to their context of use and that these similarities can be captured by specific representations that can be automatically induced from the text, in an unsupervised way. We then propose to rely only on these word representations to detect the event roles whereas, in most works (Riloff, 1996; Patwardhan and Riloff, 2007;<cite> Huang and Riloff, 2012a</cite>; Huang and Riloff, 2012b) , the role fillers are represented by a set of different features (raw words, their parts-ofspeech, syntactic or semantic roles in the sentence).",
  "y": "differences"
 },
 {
  "id": "fc58a9813b80afc9811b8ee27679b7_3",
  "x": "Following previous works (Huang and Riloff, 2011;<cite> Huang and Riloff, 2012a)</cite> , we only consider the \"String Slots\" in this work (other slots need different treatments) and we group certain slots to finally consider the five slot types PerpInd (individual perpetrator), PerpOrg (organizational perpetrator), Target (physical target), Victim (human target name or description) and Weapon (instrument id or type). We used 1,300 documents (DEV) for training, 200 documents (TST1+TST2) for tuning, and 200 documents (TST3+TST4) as the blind test set. To compare with similar works, we do not evaluate the template construction and only focus on the identification of the slot fillers: for each answer key in a reference template, we check if we find it correctly with our extraction method, using head noun matching (e.g., the victim her mother Martha Lopez Orozco de Lopez is considered to match Matha Lopez), and merging duplicate extractions (so that different extracted slot fillers sharing the same head noun are counted only once).",
  "y": "uses similarities"
 },
 {
  "id": "fc58a9813b80afc9811b8ee27679b7_4",
  "x": "Figure 1: F1-score results for event role labeling on MUC-4 data, for different size of training data, of \"String Slots\" on the TST3+TST4 with different parameters, compared to the learning curve of TIER<cite> (Huang and Riloff, 2012a)</cite> . The grey points represent the performances of other IE systems. Figure 1 presents the average F1-score results, computed over the slots PerpInd, PerpOrg, Target, Victim and Weapon.",
  "y": "differences"
 },
 {
  "id": "fd50c8cf386e3ce8c8dd8dc46c467f_0",
  "x": "Later,<cite> Yang et al. (2015)</cite> formulated a classifier to distinguish between humorous and non-humorous instances, and also created computational models to discover the latent semantic structure behind humor from four perspectives: incongruity, ambiguity, interpersonal effect and phonetic style. Recently, with the rise of artificial neural networks, many studies utilize the methods for humor recognition. Luke and Alfredo applied recurrent neural network (RNN) to humor detec-tion from reviews in Yelp dataset.",
  "y": "background"
 },
 {
  "id": "fd50c8cf386e3ce8c8dd8dc46c467f_2",
  "x": "In our work, we build the humor recognizer by using CNNs with extensive filter size and number, and the result shows higher accuracy from previous CNNs models. We conducted experiments on two different dataset, which were used in the previous studies. One is Pun of the Day <cite>(Yang et al., 2015)</cite> , and the other is 16000 One-Liners (Mihalcea and Strapparava, 2005) .",
  "y": "uses"
 },
 {
  "id": "fd50c8cf386e3ce8c8dd8dc46c467f_3",
  "x": "The datasets we use to construct humor recognition experiments includes four parts: Pun of the Day <cite>(Yang et al., 2015)</cite> , 16000 OneLiners (Mihalcea and Strapparava, 2005) , Short Jokes dataset and PTT jokes. The four datasets have different joke types, sentence lengths, data sizes and languages that allow us to conduct more comprehensive and comparative experiments. We would like to thank Yang and Mihalcea for their kindly provision of two former datasets. And we depict how we collect the latter two datasets in the following subsections.",
  "y": "uses"
 },
 {
  "id": "fd50c8cf386e3ce8c8dd8dc46c467f_4",
  "x": "Table 2 shows the experiments on both 16000 One-Liners and Pun of the Day. We set the baseline on the previous works of<cite> Yang et al. (2015)</cite> by Random Forest with Word2Vec + Human Centric Feature (Word2Vec + HCF) and Chen and Lee (2017) by Convolutional Neural Networks. We choose a dropout rate at 0.5 and test our model's performance with two factors F and HN.",
  "y": "uses"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_0",
  "x": "Paragraph vectors (Le and Mikolov, 2014 ) are a recent method for embedding pieces of natural language text as fixed-length, real-valued vectors. Extending the word2vec framework <cite>(Mikolov et al., 2013b)</cite> , paragraph vectors are typically presented as neural language models, and compute a single vector representation for each paragraph. Unlike word embeddings, paragraph vectors are not shared across the entire corpus, but are instead local to each paragraph. When interpreted as a latent variable, we expect them to have higher uncertainty when the paragraphs are short.",
  "y": "background motivation"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_1",
  "x": "Barkan (2017) pointed out that the skip-gram model with negative sampling, also known as word2vec <cite>(Mikolov et al., 2013b)</cite> , admits a Bayesian interpretation. The Bayesian skip-gram model allows uncertainty to be taken into account in a principled way, and lays the basis for our proposed Bayesian paragraph vector model. Many tasks in natural language processing require fixed-length features for text passages of variable length, such as sentences, paragraphs, or documents (in this paper, we treat these three terms interchangeably).",
  "y": "background"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_2",
  "x": "The Bayesian skip-gram model (Barkan, 2017 ) is a probabilistic interpretation of word2vec <cite>(Mikolov et al., 2013b)</cite> . The left part of Figure 1 shows the generative process. For each word i in the vocabulary, the model draws a latent word embedding vector U i \u2208 R E and a latent context embedding vector V i \u2208 R E from a Gaussian prior N (0, \u03bb 2 I).",
  "y": "background"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_3",
  "x": "The pairs with label z ij = 1 form the so-called positive examples, and are assumed to correspond to occurrences of the word i in the context of word j somewhere in the corpus. The so-called negative examples with label z ij = 0 do not correspond to any observation in the corpus. When training the model, we resort to the heuristics proposed in <cite>(Mikolov et al., 2013b)</cite> to create artificial evidence for the negative examples (see Section 3.2 below).",
  "y": "uses"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_4",
  "x": "Following<cite> Mikolov et al. (2013b)</cite> , we construct artificial evidence X \u2212 n for negative pairs by sampling from the noise distribution , where f is the empirical unigram frequency across the training corpus. The log-likelihood of the entire data is thus",
  "y": "uses"
 },
 {
  "id": "fd7bae08fd3e69744a3980daa1a649_5",
  "x": [
   "**ABSTRACT** Word2vec (Mikolov et al., 2013b ) has proven to be successful in natural language processing by capturing the semantic relationships between different words. Built on top of single-word embeddings, paragraph vectors (Le and Mikolov, 2014) find fixed-length representations for pieces of text with arbitrary lengths, such as documents, paragraphs, and sentences."
  ],
  "y": "background"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_0",
  "x": "To this end, various sense-specific word embeddings have been proposed to account for the contextual subtlety of language (Reisinger and Mooney, 2010b,a;<cite> Huang et al., 2012</cite>; Neelakantan et al., 2015; Tian et al., 2014; Li and Jurafsky, 2015; Arora et al., 2016) . A majority of these methods propose to learn multiple vectors for each word via clustering. (Reisinger and Mooney, 2010b;<cite> Huang et al., 2012</cite>; Neelakantan et al., 2015) uses neural networks to learn cluster embeddings in order to matcha polysemous word with its correct sense embeddings.",
  "y": "background"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_1",
  "x": "To this end, various sense-specific word embeddings have been proposed to account for the contextual subtlety of language (Reisinger and Mooney, 2010b,a;<cite> Huang et al., 2012</cite>; Neelakantan et al., 2015; Tian et al., 2014; Li and Jurafsky, 2015; Arora et al., 2016) . A majority of these methods propose to learn multiple vectors for each word via clustering. (Reisinger and Mooney, 2010b;<cite> Huang et al., 2012</cite>; Neelakantan et al., 2015) uses neural networks to learn cluster embeddings in order to matcha polysemous word with its correct sense embeddings.",
  "y": "background"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_2",
  "x": "Instead of a joint optimization to learn both base vector and combination weights, we propose to use the standard unisense word representation for the base vectors, and the (suitably normalized) word co-occurrence statistics as the linear combination weights; no further training computations are required in our approach. We evaluate our approach on various tasks that require contextual understanding of words, combining existing and new test datasets and evaluation metrics: word-sense induction ( (Koeling et al., 2005; Bartunov et al., 2015) ), contextual word similarity<cite> ((Huang et al., 2012</cite> ) and a new test set), and relevance detection ( (Arora et al., 2016) and a new test set). To the best of our knowledge, no prior literature has provided a comprehensive evaluation of all these multisense-specific tasks.",
  "y": "uses"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_3",
  "x": "**OUR CONTEXTUAL EMBEDDING MODEL** Like unisense vectors, sense-specific vectors should be closely aligned to words in that sense. This idea of local similarity has been widely used to obtain context sense representation<cite> Huang et al., 2012</cite>; Le and Mikolov, 2014; Neelakantan et al., 2015) .",
  "y": "background"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_5",
  "x": "Cos-distance works best for all embeddings. However, for embeddings from<cite> (Huang et al., 2012)</cite> and (Neelakantan et al., 2015) , which all have norm \u2248 1, the choice of \u03b1 makes little difference. On the other hand, using the embeddings from and our method, which both have highly varying norms, the choice of \u03b1 greatly affects performance.",
  "y": "differences"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_7",
  "x": "Since the code in<cite> (Huang et al., 2012)</cite> allows choosing various distance functions, we pick all and report the best scores. For (Neelakantan et al., 2015; we use the cosine distance as recommended. Overall Performance Table 5 shows that our method consistently outperforms<cite> (Huang et al., 2012</cite>; Neelakantan et al., 2015) .",
  "y": "motivation"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_8",
  "x": "Overall Performance Table 5 shows that our method consistently outperforms<cite> (Huang et al., 2012</cite>; Neelakantan et al., 2015) . We note that ) is learned using additional supervision from the WordNet knowledge-base in clustering; therefore, it achieves comparably much higher scores in WSR and CWS tasks in which the evaluation is also based on WordNet. We now describe each task in detail.",
  "y": "differences"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_9",
  "x": "Word-Context Relevance (WCR) This task is proposed in (Arora et al., 2016) and aims to detect when word-context pairs are relevant. In<cite> (Huang et al., 2012</cite>; Neelakantan et al., 2015; , the relevance metric can be seen as the distance (cosine or Euclidean) between the query word and the context cluster center. In our method, we rely on the filtering effect of W ij values to diminish the norm of words in irrelevant contexts; thus we propose the 2 -norms of the contextual embedding as the metric of relevance, where the target word is excluded from the context if present.",
  "y": "background"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_12",
  "x": "In comparison, our methoduses vector norms to distinguish relevance, the separation of which is shown in Figure 1 . set to only include words present in vocabularies available to all embeddngs. Following<cite> (Huang et al., 2012)</cite> , we sort all the n = 2003 test pairs based on predicted similarity score and compare such ranking against the ground-truth ranking indicated by the average human evaluation score.",
  "y": "uses"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_13",
  "x": "score: 2.3 Table 8 : An example of a pair of word-contexts for a single SCWS task. We note that in<cite> (Huang et al., 2012</cite> ) the similarity between two word-context pairs is the measured using avgSimC, a weighted average of cosine similarities between all possible representation vectors of w 1 and w 2 . This metric, however, can not be applied to our approach since we have an infinite number of possible contextual representation for each word.",
  "y": "differences"
 },
 {
  "id": "fde7f77d4685e1c9ce32a82aed4683_15",
  "x": "One thing to note is that the SCWS Spearman scores of the<cite> (Huang et al., 2012)</cite> listed here are much smaller than that first reported. This is entirely attributed to the fact that we use direct cosine similarity between word embeddings, whereas they use an averaged similarity across their provided context words. Both are perfectly valid metrics; our choice is solely so that the identical metric can be applied to all embeddings, where this averaged similarity metric cannot be used.",
  "y": "differences"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_0",
  "x": "Early work focused on structured representations of this knowledge (called scripts (Schank & Abelson, 1977) ) and manual construction of script knowledge bases. However, these approaches do not scale to complex domains (Mueller, 1998; Gordon, 2001 ). More recently, automatic induction of script knowledge from text have started to attract attention: these methods exploit either natural texts (Chambers & Jurafsky, 2008; 2009) or crowdsourced data<cite> (Regneri et al., 2010)</cite> , and, consequently, do not require expensive expert annotation.",
  "y": "background"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_1",
  "x": "However, these approaches do not scale to complex domains (Mueller, 1998; Gordon, 2001 ). More recently, automatic induction of script knowledge from text have started to attract attention: these methods exploit either natural texts (Chambers & Jurafsky, 2008; 2009) or crowdsourced data<cite> (Regneri et al., 2010)</cite> , and, consequently, do not require expensive expert annotation. Given a text corpus, they extract structured representations (i.e. graphs), for example chains (Chambers & Jurafsky, 2008) or more gen- eral directed acyclic graphs<cite> (Regneri et al., 2010)</cite> .",
  "y": "background"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_2",
  "x": "In order to get an intuition why the embedding approach may be attractive, consider a situation where a prototypical ordering of events the bus disembarked passengers and the bus drove away needs to be predicted. An approach based on frequency of predicate pairs (Chambers & Jurafsky, 2008) , is unlikely to make a right prediction as driving usually precedes disembarking. Similarly, an approach which treats the whole predicate-argument structure as an atomic unit<cite> (Regneri et al., 2010)</cite> will probably fail as well, as such a sparse model is unlikely to be effectively learnable even from large amounts of data.",
  "y": "background"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_3",
  "x": "The approach is evaluated on crowdsourced dataset of <cite>Regneri et al. (2010)</cite> and we demonstrate that using our model results in the 13.5% absolute improvement in F 1 on event ordering with respect to their graph induction method (84% vs. 71%). ---------------------------------- **MODEL**",
  "y": "uses"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_4",
  "x": "**EXPERIMENTS** We evaluate our approach on crowdsourced data collected for script induction by <cite>Regneri et al. (2010)</cite> , though, in principle, the method is applicable in arguably more general setting of Chambers & Jurafsky (2008) . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_5",
  "x": "<cite>Regneri et al. (2010)</cite> collected short textual descriptions (called event sequence descriptions, ESDs) of various types of human activities (e.g., going to a restaurant, ironing clothes) using crowdsourcing (Amazon Mechanical Turk), this dataset was also complemented by descriptions provided in the OMICS corpus (Gupta & Kochenderfer, 2004) . The datasets are fairly small, containing 30 ESDs per activity type in average (we will refer to different activities as scenarios), but the collection can easily be extended given the low cost of crowdsourcing. The ESDs are written in a bullet-point style and the annotators were asked to follow the temporal order in writing.",
  "y": "background"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_6",
  "x": "Though individual ESDs may seem simple, the learning task is challenging because of the limited amount of training data, variability in the used vocabulary, optionality of events (e.g., going to the coffee machine may not be mentioned in a ESD), different granularity of events and variability in the ordering (e.g., coffee may be put in a filter before placing it in a coffee maker). Unlike our work, <cite>Regneri et al. (2010)</cite> relies on WordNet to provide extra signal when using the Multiple Sequence Alignment (MSA) algorithm. As in their work, each description was preprocessed to extract a predicate and heads of argument noun phrases to be used in the model.",
  "y": "differences"
 },
 {
  "id": "fe0f9312caccf41def06e4311d15fb_7",
  "x": "Unlike our work, <cite>Regneri et al. (2010)</cite> relies on WordNet to provide extra signal when using the Multiple Sequence Alignment (MSA) algorithm. As in their work, each description was preprocessed to extract a predicate and heads of argument noun phrases to be used in the model. The methods are evaluated on human annotated scenariospecific tests: the goal is to classify event pairs as appearing in a given stereotypical order or not<cite> (Regneri et al., 2010</cite> ).",
  "y": "background"
 },
 {
  "id": "fe2f22d3d25358b23d0b75a6edee57_0",
  "x": "These approaches typically involve end-to-end supervised learning to generate questions. Du et al. (2017) proposed sequence-to-sequence learning for question generation from text passages. <cite>Zhou et al. (2017)</cite> utilized the answer-position, and linguistic features such as named entity recognition (NER) and parts of speech (POS) information to further improve the QG performance as the model is aware that for which answer a question need to be generated.",
  "y": "background"
 },
 {
  "id": "fe2f22d3d25358b23d0b75a6edee57_2",
  "x": "**ENTITY LINKING** In previous works<cite> (Zhou et al., 2017</cite>; Harrison and Walker, 2018) , named entity type features have been used. These features, however, only allow for the encoding of coarse level information such as knowledge of if an entity belongs to a set of predefined categories such as 'PERSON', 'LOCATION' and 'ORGANI-ZATION'.",
  "y": "motivation"
 },
 {
  "id": "fe2f22d3d25358b23d0b75a6edee57_3",
  "x": "SQuAD is composed of more than 100K questions posed by crowd workers on 536 Wikipedia articles. We used the same split as<cite> (Zhou et al., 2017)</cite> . MS MARCO datasets contains 1 million queries with corresponding answers and passages.",
  "y": "similarities uses"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_0",
  "x": "This makes standard similarity calculations as proposed in (Lin, 1998;<cite> Curran, 2002</cite>; Lund and Burgess, 1996; Weeds et al., 2004) computationally infeasible. These approaches first calculate an information measure between each word and the according context and then calculate the similarity between all words, based on the information measure for all shared contexts. ----------------------------------",
  "y": "background"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_1",
  "x": "The representation can be formalized by the pair <x,y> where x is the term and y represents the context feature. The position of x in y is denoted by the hole symbol '@'. As an example the dependency relation (nsub;gave 2 ;I 1 ) could be transferred to <gave 2 ,(nsub;@;I 1 )> and <I 1 ,(nsub;gave 2 ;@)>. This representation scheme is more generic then the schemes introduced in (Lin, 1998;<cite> Curran, 2002)</cite> , as it allows to characterise pairs by several holes, which could be used to learn analogies, cf.",
  "y": "differences"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_2",
  "x": "Whereas the method introduced by (Pantel and Lin, 2002) is very similar to the one proposed in this paper (the similarity between terms is calculated solely by the number of features two terms share), they use PMI to rank features and do not use pruning to scale to large corpora, as they use a rather small corpus. Additionally, they do not evaluate the effect of such pruning. In contrast to the best measures proposed by Lin (1998;<cite> Curran (2002</cite>; Pantel et al. (2009; Goyal et al. (2010) we do not calculate any information measure using frequencies of features and terms (we use significance ranking instead), as shown in Table 1 .",
  "y": "differences"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_4",
  "x": "Our evaluation is performed using the same 1000 frequent and 1000 infrequent nouns as previously employed by Weeds et al. (2004) . We create a gold standard, by extracting reasonable entries of these 2000 nouns using Roget's 1911 thesaurus, Moby Thesaurus, Merriam Webster's Thesaurus, the Big Huge Thesaurus and the OpenOffice Thesaurus and employ the inverse ranking measure<cite> (Curran, 2002)</cite> to evaluate the DTs. Furthermore, we introduce a WordNet-based method.",
  "y": "uses"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_5",
  "x": "It is surprising that recent works use PMI to calculate similarities between terms (Goyal et al., 2010; Pantel et al., 2009) , who, however evaluate their approach only with respect to their own implementation or extrinsically, and do not prune on saliency. Apart from the PMI measure, Curran's measure leads to the weakest results. We could not confirm that his measure outperforms Lin's measure as stated in<cite> (Curran, 2002)</cite> 1 .",
  "y": "differences"
 },
 {
  "id": "fe30705e03f0475f9ab9d044a3c9ca_6",
  "x": "Again, the inverse ranking and the WordNet Path measure are highly correlated. 2 Building a gold standard thesaurus following<cite> Curran (2002)</cite> needs access to all the used thesauri. Whereas for some, programming interfaces exist, often with limited access and licence restrictions, others have to be extracted manually.",
  "y": "background"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_0",
  "x": "We also study an emotion-based feature and a linguistic processes feature based on LIWC variables. We evaluated the proposed features with a Support Vector Machines (SVM) classifier using a corpus of 1600 reviews of hotels (<cite>Ott et al., 2011</cite>; Ott et al., 2013) . We show an experimental study evaluating the single features and combining them with the intention to obtain better features.",
  "y": "uses"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_1",
  "x": "In order to evaluate our proposal, we have performed some experimental study on the first publicly available opinion spam dataset gathered and presented in (<cite>Ott et al., 2011</cite>; Ott et al., 2013) . We first describe the corpus and then we show the different experiments made. Finally we compare our results with those published previously.",
  "y": "uses"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_2",
  "x": "---------------------------------- **OPINION SPAM CORPUS** The Opinion Spam corpus presented in (<cite>Ott et al., 2011</cite>; Ott et al., 2013 ) is composed of 1600 positive and negative opinions for hotels with the corresponding gold-standard.",
  "y": "background"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_3",
  "x": "From the 800 positive reviews (<cite>Ott et al., 2011</cite>) , the 400 truthful where mined from TripAdvisor 5-star reviews about the 20 most popular hotels in Chicago area. All reviews were written in English, have at least 150 characters and correspond to users who had posted opinions previously on TripAdvisor (non first-time authors). The 400 deceptive opinions correspond to the same 20 hotels and were gathered using Amazon Mechanical Turk crowdsourcing service.",
  "y": "background"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_4",
  "x": "In (Ren et al., 2014) a semi-supervised model called mixing population and individual property PU learning, is presented. The model is then incorporated to a SVM classifier. In (<cite>Ott et al., 2011</cite> ) <cite>the authors</cite> used the 80 dimensions of LIWC2007, unigrams and bigrams as set of features with a SVM classifier.",
  "y": "background"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_5",
  "x": "In Table 4 we can observe the indirect comparison of our results with those of (Banerjee and Chua, 2014) and (Ren et al., 2014) obtained with a 10 fold cross validation experiment, and then, with a 5 fold cross validation in order to make a fair comparison with the results of (<cite>Ott et al., 2011</cite>) and (Feng and Hirst, 2013) . Note that the results are expressed in terms of the accuracy as those were published by <cite>the authors</cite>; the results correspond only to positive reviews of the Opinion Spam corpus because the authors experimented in that corpus alone. From the Table 4 we can observe that the combination of 13 independent variables seems to have the lowest prediction accuracy (accuracy = 70.50%).",
  "y": "uses"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_6",
  "x": "As we stated before, two kinds of comparisons are shown: an indirect (we could not obtain the complete set of results reported by the authors) and a direct (the authors kindly made available the results and a statistical comparison can be performed). In Table 4 we can observe the indirect comparison of our results with those of (Banerjee and Chua, 2014) and (Ren et al., 2014) obtained with a 10 fold cross validation experiment, and then, with a 5 fold cross validation in order to make a fair comparison with the results of (<cite>Ott et al., 2011</cite>) and (Feng and Hirst, 2013) . Note that the results are expressed in terms of the accuracy as those were published by <cite>the authors</cite>; the results correspond only to positive reviews of the Opinion Spam corpus because the authors experimented in that corpus alone.",
  "y": "uses"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_7",
  "x": "Then, they could obtain an accurate SVM classifier. Regarding the experiments with the 5 fold cross-validation, we obtained similar results to those of (<cite>Ott et al., 2011</cite>) and slightly lower than the ones of (Feng and Hirst, 2013) . From this last experiment we can observe that using the representation of (Feng and Hirst, 2013 ) with more than 20138 attributes it is possible to obtain comparable results with those of our approach where we use a smaller representation (1533 attributes).",
  "y": "similarities"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_8",
  "x": "5 fold cross-validation (<cite>Ott et al., 2011</cite>) 89.8% (Feng and Hirst, 2013) 91.3% Our approach 89.8% Table 4 : Indirect comparison of the performance. Deceptive opinions detection for positive reviews of Opinion Spam corpus (800 opinions). In Table 5 we can observe the direct comparison of the performance for the positive and negative polarities reviews of the Opinion Spam corpus considering the proposal of (Hern\u00e1ndez Fusilier et al., 2015b) .",
  "y": "similarities"
 },
 {
  "id": "fe539365c7bb4555280fd1a5478aba_9",
  "x": "For the experimental study we have used the positive and negative polarities reviews corresponding to the corpora proposed by (<cite>Ott et al., 2011</cite>; Ott et al., 2013) with 800 reviews each one (400 true and 400 false opinions). We have used both corpora in a separate way but we have performed experiments joining both polarities reviews in a combined corpus of 1600 reviews. From the results obtained with the different features we have concluded that character 4-grams in tokens with LIWC variables performs the best using a SVM classifier.",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_0",
  "x": "Then, we explore a rich variety of novel sociolinguistic and discourse-based features, including mean utterance length, passive/active usage, percentage domination of the conversation, speaking rate and filler word usage. Cumulatively up to 20% error reduction is achieved relative to the standard <cite>Boulis and Ostendorf (2005)</cite> algorithm for classifying individual conversations on Switchboard, and accuracy for gender detection on the Switchboard corpus (aggregate) and Gulf Arabic corpus exceeds 95%. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_1",
  "x": "While small-scale sociolinguistic studies on monologues have shed some light on important features, we focus on modeling attributes from spoken conversations, building upon the work of <cite>Boulis and Ostendorf (2005)</cite> and show how gender and other attributes can be accurately predicted based on the following original contributions: 1. Modeling Partner Effect: A speaker may adapt his or her conversation style depending on the partner and we show how conditioning on the predicted partner class using a stacked model can provide further performance gains in gender classification. 2.",
  "y": "extends"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_2",
  "x": "2. Sociolinguistic features: The paper explores a rich set of lexical and non-lexical features motivated by the sociolinguistic literature for gender classification, and show how they can effectively augment the standard ngrambased model of <cite>Boulis and Ostendorf (2005)</cite> . 3. Application to Arabic Language: We also report results for Arabic language and show that the ngram model gives reasonably high accuracy for Arabic as well.",
  "y": "extends"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_3",
  "x": "5. Application to new attributes: We show how the lexical model of <cite>Boulis and Ostendorf (2005)</cite> can be extended to Age and Native vs. Non-native prediction, with further improvements gained from our partner-sensitive models and novel sociolinguistic features. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_4",
  "x": "While prosodic features have been shown to be useful in gender/age classification (e.g. Shafran et al., 2003) , their work makes use of speech transcripts along the lines of <cite>Boulis and Ostendorf (2005)</cite> in order to build a general model that can be applied to electronic conversations as well. While <cite>Boulis and Ostendorf (2005)</cite> observe that the gender of the partner can have a substantial effect on <cite>their</cite> classifier accuracy, given that same-gender conversations are easier to classify than mixed-gender classifications, <cite>they</cite> don't utilize this observation in <cite>their</cite> work. In Section 5.3, we show how the predicted gender/age etc.",
  "y": "background"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_5",
  "x": "of the partner/interlocutor can be used to improve overall performance via both dyadic modeling and classifier stacking. <cite>Boulis and Ostendorf (2005)</cite> have also constrained themselves to lexical n-gram features, while we show improvements via the incorporation of non-lexical features such as the percentage domination of the conversation, degree of passive usage, usage of subordinate clauses, speaker rate, usage profiles for filler words (e.g. \"umm\"), mean-utterance length, and other such properties. We also report performance gains of our models for a new genre (email) and a new language (Arabic), indicating the robustness of the models explored in this paper.",
  "y": "background"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_6",
  "x": "While prosodic features have been shown to be useful in gender/age classification (e.g. Shafran et al., 2003) , their work makes use of speech transcripts along the lines of <cite>Boulis and Ostendorf (2005)</cite> in order to build a general model that can be applied to electronic conversations as well. While <cite>Boulis and Ostendorf (2005)</cite> observe that the gender of the partner can have a substantial effect on <cite>their</cite> classifier accuracy, given that same-gender conversations are easier to classify than mixed-gender classifications, <cite>they</cite> don't utilize this observation in <cite>their</cite> work. <cite>Boulis and Ostendorf (2005)</cite> have also constrained themselves to lexical n-gram features, while we show improvements via the incorporation of non-lexical features such as the percentage domination of the conversation, degree of passive usage, usage of subordinate clauses, speaker rate, usage profiles for filler words (e.g. \"umm\"), mean-utterance length, and other such properties. We also report performance gains of our models for a new genre (email) and a new language (Arabic), indicating the robustness of the models explored in this paper.",
  "y": "motivation"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_7",
  "x": "**CORPUS DETAILS** Consistent with <cite>Boulis and Ostendorf (2005)</cite> , we utilized the Fisher telephone conversation corpus (Cieri et al., 2004) and we also evaluated performance on the standard Switchboard conversational corpus (Godfrey et al., 1992) , both collected and annotated by the Linguistic Data Consortium. In both cases, we utilized the provided metadata (including true speaker gender, age, native language, etc.) as only class labels for both training and evaluation, but never as features in the classification.",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_8",
  "x": "Consistent with <cite>Boulis and Ostendorf (2005)</cite> , we utilized the Fisher telephone conversation corpus (Cieri et al., 2004) and we also evaluated performance on the standard Switchboard conversational corpus (Godfrey et al., 1992) , both collected and annotated by the Linguistic Data Consortium. In both cases, we utilized the provided metadata (including true speaker gender, age, native language, etc.) as only class labels for both training and evaluation, but never as features in the classification. The primary task we employed was identical to <cite>Boulis and Ostendorf (2005)</cite> , namely the classification of gender, etc.",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_9",
  "x": "of each speaker in an isolated conversation, but we also evaluate performance when classifying speaker attributes given the combination of multiple conversations in which the speaker has participated. The Fisher corpus contains a total of 11971 speakers and each speaker participated in 1-3 conversations, resulting in a total of 23398 conversation sides (i.e. the transcript of a single speaker in a single conversation). We followed the preprocessing steps and experimental setup of <cite>Boulis and Ostendorf (2005)</cite> as closely as possible given the details presented in <cite>their</cite> paper, although some details such as the exact training/test partition were not currently obtainable from either the paper or personal communication.",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_11",
  "x": "As our reference algorithm, we used the current state-of-the-art system developed by <cite>Boulis and Ostendorf (2005)</cite> using unigram and bigram features in a SVM framework. We reimplemented <cite>this</cite> model as our reference for gender classification, further details of which are given below: ----------------------------------",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_12",
  "x": "Also, only the ngrams with frequency greater than 5 were retained in the feature set following <cite>Boulis and Ostendorf (2005)</cite> . This resulted in a total of 227,450 features for the Fisher corpus and 57,914 features for the Switchboard corpus. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_13",
  "x": "For compatibility with <cite>Boulis and Ostendorf (2005)</cite> , no special pre- processing for names is performed, and they are treated as just any other unigrams or bigrams 1 . Furthermore, the ngram-based approach scales well with varying the amount of conversation utilized in training the model as shown in Figure 1 . The \"<cite>Boulis and Ostendorf</cite>, 05\" rows in Table 3 show the performance of this reimplemented algorithm on both the Fisher (90.84%) and Switchboard (90.22%) corpora, under the identical training and test conditions used elsewhere in our paper for direct comparison with subsequent results 2 .",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_14",
  "x": "For compatibility with <cite>Boulis and Ostendorf (2005)</cite> , no special pre- processing for names is performed, and they are treated as just any other unigrams or bigrams 1 . Furthermore, the ngram-based approach scales well with varying the amount of conversation utilized in training the model as shown in Figure 1 . The \"<cite>Boulis and Ostendorf</cite>, 05\" rows in Table 3 show the performance of this reimplemented algorithm on both the Fisher (90.84%) and Switchboard (90.22%) corpora, under the identical training and test conditions used elsewhere in our paper for direct comparison with subsequent results 2 .",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_15",
  "x": "While ngram features are able to reasonably predict speaker gender due to their high detail and coverage and the overall importance of lexical choice in gender differences while speaking, the sociolinguistics literature suggests that other nonlexical features can further help improve performance, and more importantly, advance our understanding of gender differences in discourse. Thus, on top of the standard <cite>Boulis and Ostendorf (2005)</cite> model, we also investigated the following features motivated by the sociolinguistic literature on gender differences in discourse (Macaulay, 2005) : 1. % of conversation spoken: We measured the speaker's fraction of conversation spoken via three features extracted from the transcripts: % of words, utterances and time.",
  "y": "extends"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_16",
  "x": "As noted before, the standard reference algorithm is <cite>Boulis and Ostendorf (2005)</cite> , and all cited relative error reductions are based on this established standard, as implemented in this paper. Also, as a second reference, performance is also cited for the popular \"Gender Genie\", an online gender-detector 7 , based on the manually weighted word-level sociolinguistic features discussed in Argamon et al. (2003) . The additional table rows are described in Sections 4-6, and cumulatively yield substantial improvements over the <cite>Boulis and Ostendorf (2005)</cite> with the work reported by <cite>Boulis and Ostendorf (2005)</cite> ), all of the above models can be easily extended to per-speaker evaluation by pooling in the predictions from multiple conversations of the same speaker.",
  "y": "uses"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_17",
  "x": "Also, as a second reference, performance is also cited for the popular \"Gender Genie\", an online gender-detector 7 , based on the manually weighted word-level sociolinguistic features discussed in Argamon et al. (2003) . The additional table rows are described in Sections 4-6, and cumulatively yield substantial improvements over the <cite>Boulis and Ostendorf (2005)</cite> with the work reported by <cite>Boulis and Ostendorf (2005)</cite> ), all of the above models can be easily extended to per-speaker evaluation by pooling in the predictions from multiple conversations of the same speaker. Table 5 shows the result of each model on a per-speaker basis using a majority vote of the predictions made on the individual conversations of the respective speaker.",
  "y": "differences"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_21",
  "x": "Table 8 shows the most discriminative ngrams for binary classification of age, it is interesting to see the use of \"well\" right on top of the list for older speakers, also found in the sociolinguistic studies for age (Macaulay, 2005) . We also see that older speakers talk about their children (\"my daughter\") and younger speakers talk about their parents (\"my mom\"), the use of words such as \"wow\", \"kinda\" and \"cool\" is also common in younger speakers. To give maximal consistency/benefit to the <cite>Boulis and Ostendorf (2005)</cite> n-gram-based model, we did not filter the self-reporting n-grams such as \"im forty\" and \"im thirty\", putting our sociolinguisticliterature-based and discourse-style-based features at a relative disadvantage.",
  "y": "differences"
 },
 {
  "id": "ff7bafb8f21118ca3c908603ef32d0_22",
  "x": [
   "The robustness of the partner-model is substantially supported based on the consistent performance gains achieved in diverse languages and attributes. This paper has also explored a rich variety of novel sociolinguistic and discourse-based features, including mean utterance length, passive/active usage, percentage domination of the conversation, speaking rate and filler word usage. In addition to these novel models, the paper also shows how these models and the previous work extend to new languages and genres."
  ],
  "y": "differences"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_1",
  "x": "In this work, we focus specifically on modeling physical plausibility as presented by <cite>Wang et al. (2018)</cite> . This is the problem of determining if a given event, represented as an s-v-o triple, is physically plausible (Table 1) . We show that in the original supervised setting a distributional model, namely a novel application of BERT (Devlin et al., 2019) , significantly outperforms the best existing method which has access to manually labeled physical features<cite> (Wang et al., 2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_2",
  "x": "Still, the generalization ability of supervised models is limited by the coverage of the training set. We therefore present the more difficult problem of learning physical plausibility directly from text. We create a training set by parsing and extracting attested s-v-o triples from English Wikipedia, and we provide a baseline for training on this dataset and evaluating on <cite>Wang et al. (2018)</cite> 's physical plausibility task.",
  "y": "uses"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_3",
  "x": "We create a training set by parsing and extracting attested s-v-o triples from English Wikipedia, and we provide a baseline for training on this dataset and evaluating on <cite>Wang et al. (2018)</cite> 's physical plausibility task. We also experiment training on a large set of s-v-o triples extracted from the web as part of the NELL project (Carlson et al., 2010) , and find that Wikipedia triples result in better performance. arXiv:1911.05689v1 [cs.CL] 13 Nov 2019 <cite>Wang et al. (2018)</cite> present the semantic plausibility dataset that we use for evaluation in this work, and they show that distributional methods fail on this dataset.",
  "y": "uses background"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_4",
  "x": "Closely related to semantic plausibility is selectional preference (Resnik, 1996) which concerns the semantic preference of a predicate for its arguments. Here, preference refers to the typicality of arguments: while it is plausible that a gorilla rides a camel, it is not preferred. Current approaches to selectional preference are distributional (Erk et al., 2010; Van de Cruys, 2014) and have shown limited performance in capturing semantic plausibility<cite> (Wang et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_5",
  "x": "Following existing work, we focus on the task of single-event, physical plausibility. This is the problem of determining if a given event, represented as an s-v-o triple, is physically plausible. We use <cite>Wang et al. (2018)</cite> 's physical plausibility dataset for evaluation.",
  "y": "uses"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_6",
  "x": "We follow the same evaluation procedure as previous work and perform cross validation on the 3,062 labeled triples<cite> (Wang et al., 2018)</cite> . ---------------------------------- **LEARNING FROM TEXT**",
  "y": "similarities uses"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_7",
  "x": "For evaluation, we split <cite>Wang et al. (2018)</cite>'s 3,062 triples into equal sized validation and test sets. Each set thus consists of 1,531 triples. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_8",
  "x": "This method is a two-layer artificial neural network (NN) over static embeddings. Supervised. We reproduce the results of <cite>Wang et al. (2018)</cite> using GloVe embeddings and the same hyperparameter settings.",
  "y": "similarities"
 },
 {
  "id": "ffd65a1a02c852a2670b471fb4b110_9",
  "x": "For the supervised setting, we follow the same evaluation procedure as <cite>Wang et al. (2018)</cite> : we perform 10-fold cross validation on the dataset of 3,062 s-v-o triples, and report the mean accuracy of running this procedure 20 times all with the same model initialization (Table 3) . BERT outperforms existing methods by a large margin, including those with access to manually labeled physical features. We conclude from Model Accuracy Random 0.50 NN (Van de Cruys, 2014) 0.68 NN+WK<cite> (Wang et al., 2018)</cite> 0.76 Fine-tuned BERT 0.89 these results that distributional data does provide a strong cue for semantic plausibility in the supervised setting of <cite>Wang et al. (2018)</cite> .",
  "y": "similarities uses"
 }
]