[
 {
  "id": "021c423c731ecbe3e26b3ce234b390_0",
  "x": "Automatic detection of fake from legitimate news in different formats such as headlines, tweets and full news articles has been approached in recent Natural Language Processing literature (Vlachos and Riedel, 2014; Vosoughi, 2015; Jin et al., 2016;<cite> Rashkin et al., 2017</cite>; Wang, 2017; Pomerleau and Rao, 2017; Thorne et al., 2018) . The most important challenge in automatic misinformation detection using modern NLP techniques, especially at the level of full news articles, is data. Most previous systems built to identify fake news articles rely on training data labeled with respect to the general reputation of the sources, i.e., domains/user accounts (Fogg et al., 2001; Lazer et al., 2017;<cite> Rashkin et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "021c423c731ecbe3e26b3ce234b390_1",
  "x": "The most important challenge in automatic misinformation detection using modern NLP techniques, especially at the level of full news articles, is data. Most previous systems built to identify fake news articles rely on training data labeled with respect to the general reputation of the sources, i.e., domains/user accounts (Fogg et al., 2001; Lazer et al., 2017;<cite> Rashkin et al., 2017)</cite> . Even though some of these studies try to identify fake news based on linguistic cues, the question is whether they learn publishers' general writing style (e.g., common writing features of a few clickbaity websites) or deceptive style (similarities among news articles that contain misinformation).",
  "y": "motivation"
 },
 {
  "id": "021c423c731ecbe3e26b3ce234b390_2",
  "x": "A few recent studies have examined full articles (i.e., actual 'fake news') to extract discriminative linguistic features of misinformation<cite> Rashkin et al., 2017</cite>; Horne and Adali, 2017) . The issue with these studies is the data collection methodology. Texts are harvested from websites that are assumed to be fake news publishers (according to a list of suspicious websites), with no individual labeling of data.",
  "y": "background"
 },
 {
  "id": "021e5dbe22bf0f4ebda4d37040d0a6_0",
  "x": "In the cross-lingual study of<cite> McDonald et al. (2011)</cite> , where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009 ), but we are only aware of a few systematic attempts to create homogenous syntactic dependency annotation in multiple languages.",
  "y": "background motivation"
 },
 {
  "id": "021e5dbe22bf0f4ebda4d37040d0a6_1",
  "x": "In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012) , have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T\u00e4ckstr\u00f6m et al., 2013) . We aim to do the same for syntactic dependencies and present cross-lingual parsing experiments to highlight some of the benefits of cross-lingually consistent annotation. First, results largely conform to our expectations of which target languages should be useful for which source languages, unlike in the study of<cite> McDonald et al. (2011)</cite> .",
  "y": "differences background"
 },
 {
  "id": "021e5dbe22bf0f4ebda4d37040d0a6_2",
  "x": "The selected sentences were pre-processed using cross-lingual taggers (Das and Petrov, 2011) and parsers <cite>(McDonald et al., 2011)</cite> . The annotators modified the pre-parsed trees using the TrEd 2 tool. At the beginning of the annotation process, double-blind annotation, followed by manual arbitration and consensus, was used iteratively for small batches of data until the guidelines were finalized.",
  "y": "uses"
 },
 {
  "id": "021e5dbe22bf0f4ebda4d37040d0a6_3",
  "x": "One of the motivating factors in creating such a data set was improved cross-lingual transfer evaluation. To test this, we use a cross-lingual transfer parser similar to that of<cite> McDonald et al. (2011)</cite> . In particular, it is a perceptron-trained shift-reduce parser with a beam of size 8.",
  "y": "extends background"
 },
 {
  "id": "021e5dbe22bf0f4ebda4d37040d0a6_4",
  "x": "We can make several interesting observations. Most notably, for the Germanic and Romance target languages, the best source language is from the same language group. This is in stark contrast to the results of<cite> McDonald et al. (2011)</cite> , who observe that this is rarely the case with the heterogenous CoNLL treebanks.",
  "y": "differences background"
 },
 {
  "id": "021e5dbe22bf0f4ebda4d37040d0a6_5",
  "x": "Finally, Korean emerges as a very clear outlier (both as a source and as a target language), which again is supported by typological considerations as well as by the difference in tokenization. With respect to evaluation, it is interesting to compare the absolute numbers to those reported in<cite> McDonald et al. (2011)</cite> ----------------------------------",
  "y": "background"
 },
 {
  "id": "022049c0e75a490978b2c49da41deb_0",
  "x": "Multiword expressions (MWEs) are word combinations that display some form of idiomaticity (Baldwin and Kim, 2009 ), including semantic idiomaticity, wherein the semantics of the MWE (e.g. ivory tower) cannot be predicted from the semantics of the component words (e.g. ivory and tower). Recent NLP work on semantic idiomaticity has focused on the task of \"compositionality prediction\", in the form of a regression task whereby a given MWE is mapped onto a continuous-valued compositionality score, either for the MWE as a whole or for each of its component words (Reddy et al., 2011; Schulte im Walde et al., 2013;<cite> Salehi et al., 2014b)</cite> . Separately in NLP, there has been a recent surge of interest in learning distributed representations of word meaning, in the form of \"word embeddings\" (Collobert and Weston, 2008; Mikolov et al., 2013a) and composition over distributed representations (Socher et al., 2012; Baroni et al., 2014) .",
  "y": "background"
 },
 {
  "id": "022049c0e75a490978b2c49da41deb_1",
  "x": "Despite the wealth of research applying word embeddings within NLP, they have not yet been considered for predicting the compositionality of MWEs. Much prior work on MWEs has been tailored to specific kinds of MWEs in particular languages (e.g. English verb-noun combinations (Fazly et al., 2009) ). There has however been recent interest in approaches to MWEs that are more broadly applicable to a wider range of languages and MWE types (Brooke et al., 2014;<cite> Salehi et al., 2014b</cite>; Schneider et al., 2014) .",
  "y": "background"
 },
 {
  "id": "022049c0e75a490978b2c49da41deb_2",
  "x": "**COUNT-BASED DISTRIBUTIONAL SIMILARITY** Our first method for building vectors is that of<cite> Salehi et al. (2014b)</cite> : the top 50 most-frequent words in the training corpus are considered to be stopwords and discarded, and words with frequency rank 51-1051 are considered to be the content-bearing words, which form the dimensions for our vectors, in the manner of Sch\u00fctze (1997) . To measure the similarity of the MWE vector and the component word vectors, we considered two different approaches.",
  "y": "uses"
 },
 {
  "id": "022049c0e75a490978b2c49da41deb_3",
  "x": "The state-of-the-art method for this dataset <cite>(Salehi et al., 2014b</cite> ) is a supervised support vector regression model, trained over the distributional method from Section 3.1 as applied to both English and 51 target languages (under word and MWE translation). The EVPC dataset consists of 160 English verb particle constructions, and is manually annotated for compositionality on a binary scale for each of the head verb and particle (Bannard, 2006) . In order to translate the dataset into a regression task, we calculate the overall compositionality as the number of annotations of entailment for the verb, divided by the total number of verb annotations for that VPC.",
  "y": "background"
 },
 {
  "id": "022049c0e75a490978b2c49da41deb_4",
  "x": "The EVPC dataset consists of 160 English verb particle constructions, and is manually annotated for compositionality on a binary scale for each of the head verb and particle (Bannard, 2006) . In order to translate the dataset into a regression task, we calculate the overall compositionality as the number of annotations of entailment for the verb, divided by the total number of verb annotations for that VPC. The state-of-the-art method for this dataset <cite>(Salehi et al., 2014b</cite> ) is a linear combination of: (1) the distributional method from Section 3.1; (2) the same method applied to 10 target languages (under word and MWE translation, selecting the languages using supervised learning); and (3) the string similarity method of Salehi and Cook (2013) .",
  "y": "background"
 },
 {
  "id": "022049c0e75a490978b2c49da41deb_5",
  "x": "over a range of hyper-parameter settings for each of WORD2VEC (vector dimensionality d; we also present results for CBOW vs. C-SKIP) and MSSG (vector dimensionality d and window size w), informed by the experimental results in the respective publications. Note that for EVPC, we don't use the vector for the particle, in keeping with<cite> Salehi et al. (2014b)</cite> ; as such, there are no results for comp 2 . For comp 1 , \u03b1 is set to 1.0 for EVPC, and 0.7 for both ENC and GNC, also based on the findings of<cite> Salehi et al. (2014b)</cite> .",
  "y": "similarities"
 },
 {
  "id": "022049c0e75a490978b2c49da41deb_6",
  "x": "over a range of hyper-parameter settings for each of WORD2VEC (vector dimensionality d; we also present results for CBOW vs. C-SKIP) and MSSG (vector dimensionality d and window size w), informed by the experimental results in the respective publications. Note that for EVPC, we don't use the vector for the particle, in keeping with<cite> Salehi et al. (2014b)</cite> ; as such, there are no results for comp 2 . For comp 1 , \u03b1 is set to 1.0 for EVPC, and 0.7 for both ENC and GNC, also based on the findings of<cite> Salehi et al. (2014b)</cite> .",
  "y": "uses"
 },
 {
  "id": "022049c0e75a490978b2c49da41deb_7",
  "x": [
   "**CONCLUSIONS** We presented the first approach to using word embeddings to predict the compositionality of MWEs. We showed that this approach, in combination with information from string similarity, surpassed, or was competitive with, the current state-of-the-art on three compositionality datasets."
  ],
  "y": "similarities future_work"
 },
 {
  "id": "023a954d97b5d761b01f09bb242d19_0",
  "x": "Abstract Meaning Representation (AMR) forms a rooted acyclic directed graph that represents the content of a sentence. All nodes and edges of the AMR graph are labeled according to the sense of the words in a sentence. AMR parsing is the task of converting a given sentence to a corresponding graph. AMRs have been applied to several applications such as event extraction [13, 7] , text summarization [6, 11] and text generation [15, 14] . However, AMR annotation which requires a lot of human effort limits the outcome of data-driven approaches, one of which being neural network based methods<cite> [10,</cite> 3] . Therefore, a highly accurate parser is necessary in order to intensify other applications which are based on AMR.",
  "y": "background motivation"
 },
 {
  "id": "023a954d97b5d761b01f09bb242d19_1",
  "x": "Their later works have investigated a richer feature set including co-reference, semantic role labeling, word cluster [17] ; rich name entity tag, and ISI verbalization list [16] . NeuralAMR <cite>[10]</cite> has succeeded at both AMR parsing and sentence generation as the result of a bootstrapping training strategy on a 20-million-sentence unsupervised dataset. An efficient adaptation of machine translation to AMR parsing by Barzdins et al [2] indicates that character-based features are better than [12] .",
  "y": "background"
 },
 {
  "id": "023a954d97b5d761b01f09bb242d19_2",
  "x": "Although recent studies have utilized Long Short-Term Memory (LSTM) in AMR parsing<cite> [10,</cite> 1] , there are several disadvantages of employing LSTM compared to CNN. First, LSTM models long dependency, which might be noise to generate a linearized graph, whereas CNN provides a shorter dependency which is advantageous to generate graph traversal. Secondly, LSTM requires a chronologically computing process that restrains the ability of parallelization; on the contrary, CNN enables simultaneous parsing.",
  "y": "background motivation"
 },
 {
  "id": "023a954d97b5d761b01f09bb242d19_3",
  "x": "Unlike the prior work <cite>[10]</cite> , in our model, the graphs pass through a much simpler pre-processing series which consists of variable removal, graph linearization, and infrequent word replacement. For stripping the AMR text, we modified the depth-first-search traversal from the work of Kontas et al <cite>[10]</cite> in the way of marking the end of a path. The left parentheses are ignored and the right parentheses are replaced by doubling the concept of the terminal node. The process of recovering the stripped text from the graph is called de-linearization. The graph which contains multiple nodes of a single concept might not be perfectly reversed because those nodes have been collapsed into one.",
  "y": "extends"
 },
 {
  "id": "033ce75c882764e08fb3871656a8d1_0",
  "x": "To the best of our knowledge, we cannot find the answer for French in literature. Indeed, compares the performance of mwetoolkit with another toolkit on English and French corpora, but they never use the data generated by mwetoolkit to train a model. In contrast, <cite>Zilio et al. (2011)</cite> make a study involving training a model but use it only on English and use extra lexical resources to complement the machine learning method, so their study does not focus just on classifier evaluation.",
  "y": "background"
 },
 {
  "id": "033ce75c882764e08fb3871656a8d1_1",
  "x": "In contrast, <cite>Zilio et al. (2011)</cite> make a study involving training a model but use it only on English and use extra lexical resources to complement the machine learning method, so their study does not focus just on classifier evaluation. This paper presents the first evaluation of mwetoolkit on French together with two resources very commonly used by the French NLP community: the tagger TreeTagger (Schmid, 1994) and the dictionary Dela. 1 Training and test data are taken from the French Europarl corpus (Koehn, 2005) and classifiers are trained using the Weka machine learning toolkit (Hall et al., 2009) .",
  "y": "motivation"
 },
 {
  "id": "033ce75c882764e08fb3871656a8d1_2",
  "x": "Ramisch et al. (2010b) provide experiments on Portuguese, English and Greek. <cite>Zilio et al. (2011)</cite> provide experiments with this tool as well. In the latter study, after having trained a machine on bigram MWEs, they try to extract full n-gram expressions from the Europarl corpus.",
  "y": "background"
 },
 {
  "id": "033ce75c882764e08fb3871656a8d1_3",
  "x": "That is the reason why we will run three experiments close to the one of <cite>Zilio et al. (2011)</cite> but were the only changing parameter is the pattern that we train our classifiers on. ---------------------------------- **GENERATING TRAINING DATA**",
  "y": "motivation uses"
 },
 {
  "id": "033ce75c882764e08fb3871656a8d1_4",
  "x": "**CHOICE OF PATTERNS** In contrast to <cite>Zilio et al. (2011)</cite> we run our experiment on French. The choice of a different language requires an adaptation of the patterns.",
  "y": "differences"
 },
 {
  "id": "033ce75c882764e08fb3871656a8d1_5",
  "x": "**PREPROCESSING** For preprocessing we used the same processes as described in <cite>Zilio et al. (2011)</cite> . First we ran the sentence splitter and the tokenizer provided with the Europarl corpus.",
  "y": "uses"
 },
 {
  "id": "033ce75c882764e08fb3871656a8d1_6",
  "x": "**TRAINING ON NA** We tested several algorithms offered by Weka as well as the training options suggested by <cite>Zilio et al. (2011)</cite> . We also tried to remove some features and to keep only the most informative ones (MLE, T-score and log-likelihood according to information gain ratio) but we noticed each time a loss in the recall.",
  "y": "uses"
 },
 {
  "id": "03b7c2e050957dcff336183823e6f1_0",
  "x": "The multilingual track of the CoNLL-2007 shared task (Nivre et al., 2007) considers dependency parsing of texts written in different languages. It requires use of a single dependency parsing model for the entire set of languages; model parameters are estimated individually for each language on the basis of provided training sets. We use a recently proposed dependency parser <cite>(Titov and Henderson, 2007b )</cite> 1 which has demonstrated state-of-theart performance on a selection of languages from the CoNLL-X shared task (Buchholz and Marsi, 2006) .",
  "y": "uses"
 },
 {
  "id": "03b7c2e050957dcff336183823e6f1_1",
  "x": "First we predict the fine-grain part-of-speech tag for the word, then the set of word features (treating each set as an atomic value), and only then the particu-lar word form. This approach allows us to both decrease the effect of sparsity and to avoid normalization across all the words in the vocabulary, significantly reducing the computational expense of word prediction. When conditioning on words, we treated each word feature individually, as this proved to be useful in <cite>(Titov and Henderson, 2007b)</cite> .",
  "y": "motivation"
 },
 {
  "id": "03b7c2e050957dcff336183823e6f1_2",
  "x": "However, as long as there exists some chain of relationships between any two states, then any statistical dependency which is clearly manifested in the data can be learned, even if it was not foreseen by the designer. This provides a potentially powerful form of feature induction, which is nonetheless biased toward a notion of locality appropriate for the nature of the problem. In our experiments we use the same definition of structural locality as was proposed for the ISBN dependency parser in <cite>(Titov and Henderson, 2007b)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "03b7c2e050957dcff336183823e6f1_3",
  "x": "Unlike <cite>(Titov and Henderson, 2007b )</cite>, in the shared task we used only the simplest feed-forward approximation, which replicates the computation of a neural network of the type proposed in (Henderson, 2003) . We would expect better performance with the more accurate approximation based on variational inference proposed and evaluated in (Titov and Henderson, 2007a ). We did not try this because, on larger treebanks it would have taken too long to tune the model with this better approximation, and using different approximation methods for different languages would not be compatible with the shared task rules.",
  "y": "differences"
 },
 {
  "id": "03b7c2e050957dcff336183823e6f1_4",
  "x": "We would expect better performance with the more accurate approximation based on variational inference proposed and evaluated in (Titov and Henderson, 2007a ). We did not try this because, on larger treebanks it would have taken too long to tune the model with this better approximation, and using different approximation methods for different languages would not be compatible with the shared task rules. To search for the most probable parse, we use the heuristic search algorithm described in <cite>(Titov and Henderson, 2007b)</cite> , which is a form of beam search.",
  "y": "uses"
 },
 {
  "id": "03b7c2e050957dcff336183823e6f1_5",
  "x": "This makes an ISBN parser a particularly good baseline when considering a new treebank or language, be-Ara Bas Cat Chi Cze Eng Gre Hun Ita cause it does not require much effort in feature engineering. As was demonstrated in <cite>(Titov and Henderson, 2007b)</cite> , even a minimal set of local explicit features achieves results which are non-significantly different from a carefully chosen set of explicit features, given the language independent definition of locality described in section 2. It is also important to note that the model is quite efficient.",
  "y": "similarities"
 },
 {
  "id": "0410820bea04fb68908f4885089081_0",
  "x": "tagger (e.g. the <cite>Brill tagger</cite> <cite>[2]</cite> ) for the English documents. For the next step we map the specific tagset of the used tagger to the tagset of the chart parser used in syntactic analysis. This parser also needs a grammar, which is dependent on the language or the specific characteristics of the language in the domain.",
  "y": "uses"
 },
 {
  "id": "04b525b91b48e31258287a015d0401_0",
  "x": "The keyword-based approach still requires nonnegligible manual work in creating a representative keyword list per category. <cite>(Gliozzo et al., 2005)</cite> succeeded eliminating this requirement by using the category name alone as the initial keyword, yet obtaining superior performance within the keywordbased approach. This was achieved by measuring similarity between category names and documents in Latent Semantic space (LSA), which implicitly captures contextual similarities for the category name through unsupervised dimensionality reduction.",
  "y": "background"
 },
 {
  "id": "04b525b91b48e31258287a015d0401_1",
  "x": "<cite>(Gliozzo et al., 2005)</cite> succeeded eliminating this requirement by using the category name alone as the initial keyword, yet obtaining superior performance within the keywordbased approach. The goal of our research is to further improve the scheme of text categorization from category name, which was hardly explored in prior work.",
  "y": "motivation"
 },
 {
  "id": "04b525b91b48e31258287a015d0401_2",
  "x": "When analyzing the behavior of the LSA representation of <cite>(Gliozzo et al., 2005)</cite> we noticed that <cite>it captures</cite> two types of similarities between the category name and document terms. <cite>One type</cite> regards words which refer specifically to the category name's meaning, such as pitcher for the category Baseball. However, typical context words for the category which do not necessarily imply its specific meaning, like stadium, also come up as similar to baseball in LSA space.",
  "y": "background"
 },
 {
  "id": "04b525b91b48e31258287a015d0401_3",
  "x": "When analyzing the behavior of the LSA representation of <cite>(Gliozzo et al., 2005)</cite> we noticed that <cite>it captures</cite> two types of similarities between the category name and document terms. <cite>One type</cite> regards words which refer specifically to the category name's meaning, such as pitcher for the category Baseball. However, typical context words for the category which do not necessarily imply its specific meaning, like stadium, also come up as similar to baseball in LSA space. This limits <cite>the method's precision</cite>, due to false-positive classifications of contextually-related documents that do not discuss the specific category topic (such as other sports documents wrongly classified to Baseball). <cite>This behavior</cite> is quite typical for query expansion methods, which expand a query with contextually correlated terms. We propose a novel scheme that models separately these two types of similarity.",
  "y": "motivation"
 },
 {
  "id": "04b525b91b48e31258287a015d0401_4",
  "x": "As described in Section 1, the keyword list in <cite>(Gliozzo et al., 2005)</cite> consisted of the category name alone. <cite>This was accompanied</cite> by representing the category names and documents (step 2) in LSA space, obtained through cooccurrence-based dimensionality reduction. In <cite>this space</cite>, words that tend to cooccur together, or occur in similar contexts, are represented by similar vectors.",
  "y": "background"
 },
 {
  "id": "04b525b91b48e31258287a015d0401_5",
  "x": "They further showed that an entailing text (in the textual entailment setting) typically includes a concrete reference to each term in the entailed statement. Analogously, we assume that a relevant document for a category typically includes concrete terms that refer specifically to the category name's meaning. We thus extend the scheme in Figure 1 by creating two vectors per category (in steps 1 and 2): a reference vector c ref in term space, consisting of referring terms for the category name; and a context vector c con , representing the category name in LSA space, as in <cite>(Gliozzo et al., 2005)</cite> .",
  "y": "uses"
 },
 {
  "id": "04b525b91b48e31258287a015d0401_6",
  "x": "However, this may yield false positive classifications in two cases: (a) inappropriate sense of an ambiguous referring term, e.g., the narcotic sense of drug should not yield classification to Medicine; (b) a passing reference, e.g., an analogy to cars in a software document, should not yield classification to Autos. In both these cases the overall context in the document is expected to be atypical for the triggered category. We therefore measure the contextual similarity between a category c and a document d utilizing LSA space, replicating the method in <cite>(Gliozzo et al., 2005)</cite> : c con and d LSA are taken as the LSA vectors of the category name and the document, respectively, yielding Sim con (c, d) = cos( c con , d LSA )).",
  "y": "uses"
 },
 {
  "id": "04b525b91b48e31258287a015d0401_7",
  "x": "We tested our method on the two corpora used in <cite>(Gliozzo et al., 2005)</cite> : 20-NewsGroups, classified by a single-class scheme (single category per document), and Reuters-10 3 , of a multi-class scheme. As in <cite>their work</cite>, non-standard category names were adjusted, such as Foreign exchange for Money-fx. Table 2 presents the results of the initial classification (step 3).",
  "y": "uses"
 },
 {
  "id": "04b525b91b48e31258287a015d0401_8",
  "x": "---------------------------------- **INITIAL CLASSIFICATION** As we hypothesized, the Reference model achieves much better precision than the Context model from <cite>(Gliozzo et al., 2005)</cite> resources, yielding a lower F1.",
  "y": "differences"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_0",
  "x": "****<cite>TRANSFORMER</cite> DISSECTION: A UNIFIED UNDERSTANDING OF <cite>TRANSFORMER'S</cite> ATTENTION VIA THE LENS OF KERNEL**** **ABSTRACT** <cite>Transformer</cite> is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_1",
  "x": "<cite>Transformer</cite> is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction. At the core of the <cite>Transformer</cite> is the attention mechanism, which concurrently processes all inputs in the streams. In this paper, we present a new formulation of attention via the lens of the kernel.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_2",
  "x": "In this paper, we present a new formulation of attention via the lens of the kernel. To be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs. This new formulation gives us a better way to understand individual components of the <cite>Transformer's</cite> attention, such as the better way to integrate the positional embedding.",
  "y": "motivation"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_3",
  "x": "This new formulation gives us a better way to understand individual components of the <cite>Transformer's</cite> attention, such as the better way to integrate the positional embedding. Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing <cite>Transformer</cite>'s attention. As an example, we propose a new variant of <cite>Transformer's</cite> attention which models the input as a product of symmetric kernels.",
  "y": "motivation"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_4",
  "x": "As an example, we propose a new variant of <cite>Transformer's</cite> attention which models the input as a product of symmetric kernels. This approach achieves competitive performance to the current state of the art model with less computation. In our experiments, we empirically study different kernel construction strategies on two widely used tasks: neural machine translation and sequence prediction.",
  "y": "extends"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_5",
  "x": "**INTRODUCTION** <cite>Transformer</cite> <cite>(Vaswani et al., 2017 )</cite> is a relative new architecture which outperforms traditional deep learning models such as Recurrent Neural Networks (RNNs) (Sutskever et al., 2014) and Temporal Convolutional Networks (TCNs) (Bai et al., 2018) for sequence modeling tasks across neural machine translations <cite>(Vaswani et al., 2017)</cite> , language understanding (Devlin et al., 2018) , sequence prediction (Dai et al., 2019) , image generation (Child et al., 2019) , video activity classification (Wang et al., 2018) , music generation (Huang et al., 2018a) , and multimodal sentiment analysis (Tsai et al., 2019a) . Instead of performing recurrence (e.g., RNN) or convolution (e.g., TCN) over the sequences, <cite>Transformer</cite> is a feed-forward model that concurrently processes the entire sequence.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_6",
  "x": "<cite>Transformer</cite> <cite>(Vaswani et al., 2017 )</cite> is a relative new architecture which outperforms traditional deep learning models such as Recurrent Neural Networks (RNNs) (Sutskever et al., 2014) and Temporal Convolutional Networks (TCNs) (Bai et al., 2018) for sequence modeling tasks across neural machine translations <cite>(Vaswani et al., 2017)</cite> , language understanding (Devlin et al., 2018) , sequence prediction (Dai et al., 2019) , image generation (Child et al., 2019) , video activity classification (Wang et al., 2018) , music generation (Huang et al., 2018a) , and multimodal sentiment analysis (Tsai et al., 2019a) . Instead of performing recurrence (e.g., RNN) or convolution (e.g., TCN) over the sequences, <cite>Transformer</cite> is a feed-forward model that concurrently processes the entire sequence. At the core of the <cite>Transformer</cite> is its attention mechanism, which is proposed to integrate the dependencies between the inputs.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_7",
  "x": "At the core of the <cite>Transformer</cite> is its attention mechanism, which is proposed to integrate the dependencies between the inputs. There are up to three types of attention within the full <cite>Transformer</cite> model as exemplified with neural machine translation application <cite>(Vaswani et al., 2017)</cite> : 1) Encoder self-attention considers the source sentence as input, generating a sequence of encoded representations, where each encoded token has a global dependency with other tokens in the input sequence. 2) Decoder self-attention considers the target sentence (e.g., predicted target sequence for translation) as input, generating a sequence of decoded representations 1 , where each decoded token depends on previous decoded tokens.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_8",
  "x": "At the core of the <cite>Transformer</cite> is its attention mechanism, which is proposed to integrate the dependencies between the inputs. There are up to three types of attention within the full <cite>Transformer</cite> model as exemplified with neural machine translation application <cite>(Vaswani et al., 2017)</cite> : 1) Encoder self-attention considers the source sentence as input, generating a sequence of encoded representations, where each encoded token has a global dependency with other tokens in the input sequence. 2) Decoder self-attention considers the target sentence (e.g., predicted target sequence for translation) as input, generating a sequence of decoded representations 1 , where each decoded token depends on previous decoded tokens.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_9",
  "x": "In all cases, the <cite>Transformer's</cite> attentions follow the same general mechanism. At the high level, the attention can be seen as a weighted combination of the input sequence, where the weights are determined by the similarities between elements of the input sequence. We note that this operation is orderagnostic to the permutation in the input se-quence (order is encoded with extra positional embedding <cite>(Vaswani et al., 2017</cite>; Shaw et al., 2018; Dai et al., 2019) ).",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_10",
  "x": "In all cases, the <cite>Transformer's</cite> attentions follow the same general mechanism. At the high level, the attention can be seen as a weighted combination of the input sequence, where the weights are determined by the similarities between elements of the input sequence. We note that this operation is orderagnostic to the permutation in the input se-quence (order is encoded with extra positional embedding <cite>(Vaswani et al., 2017</cite>; Shaw et al., 2018; Dai et al., 2019) ).",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_11",
  "x": "At the high level, the attention can be seen as a weighted combination of the input sequence, where the weights are determined by the similarities between elements of the input sequence. We note that this operation is orderagnostic to the permutation in the input se-quence (order is encoded with extra positional embedding <cite>(Vaswani et al., 2017</cite>; Shaw et al., 2018; Dai et al., 2019) ). The above observation inspires us to connect <cite>Transformer's</cite> attention to kernel learning (Scholkopf and Smola, 2001) : they both concurrently and order-agnostically process all inputs by calculating the similarity between the inputs.",
  "y": "motivation"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_12",
  "x": "The above observation inspires us to connect <cite>Transformer's</cite> attention to kernel learning (Scholkopf and Smola, 2001) : they both concurrently and order-agnostically process all inputs by calculating the similarity between the inputs. Therefore, in the paper, we present a new formulation for <cite>Transformer's</cite> attention via the lens of kernel. To be more precise, the new formulation can be interpreted as a kernel smoother (Wasserman, 2006) over the inputs in a sequence, where the kernel measures how similar two different inputs are.",
  "y": "motivation"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_13",
  "x": "The main advantage of connecting attention to kernel is that it opens up a new family of attention mechanisms that can relate to the well-established literature in kernel learning (Scholkopf and Smola, 2001) . As a result, we develop a new variant of attention which simply considers a product of symmetric kernels when modeling non-positional and positional embedding. Furthermore, our proposed formulation highlights naturally the main components of <cite>Transformer's</cite> attention, enabling a better understanding of this mechanism: recent variants of <cite>Transformers</cite> (Shaw et al., 2018; Huang et al., 2018b; Dai et al., 2019; Child et al., 2019; Wang et al., 2018; Tsai et al., 2019a) can be expressed through these individual components.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_15",
  "x": "Next, we show that this new formulation allows us to explore new family of attention while at the same time offering a framework to categorize previous attention variants <cite>(Vaswani et al., 2017</cite>; Shaw et al., 2018; Huang et al., 2018b; Dai et al., 2019; Child et al., 2019; Wang et al., 2018; Tsai et al., 2019a) . Last, we present a new form of attention, which requires fewer parameters and empirically reaches competitive performance as the state-of-the-art models. For notation, we use lowercase representing a vector (e.g., x), bold lowercase representing a matrix (e.g., x), calligraphy letter denoting a space (e.g., X ), and S denoting a set.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_17",
  "x": "---------------------------------- **TECHNICAL BACKGROUND** Unlike recurrent computation (Sutskever et al., 2014 ) (i.e., RNNs) and temporal convolutional computation (Bai et al., 2018 ) (i.e., TCNs), <cite>Transformer's</cite> attention is an order-agnostic operation given the order in the inputs <cite>(Vaswani et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_18",
  "x": "As a result, <cite>Transformer</cite> <cite>(Vaswani et al., 2017)</cite> introduced positional embedding to indicate the positional relation for the inputs. Formally, a sequence x = [x 1 , x 2 , \u22ef, x T ] defines each element as x i = (f i , t i ) with f i \u2208 F being the nontemporal feature at time i and t i \u2208 T as an temporal feature (or we called it positional embedding). Note that f i can be the word representation (in neural machine translation <cite>(Vaswani et al., 2017)</cite> ), a pixel in a frame (in video activity recognition (Wang et al., 2018) ), or a music unit (in music generation (Huang et al., 2018b) ).",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_19",
  "x": "As a result, <cite>Transformer</cite> <cite>(Vaswani et al., 2017)</cite> introduced positional embedding to indicate the positional relation for the inputs. Formally, a sequence x = [x 1 , x 2 , \u22ef, x T ] defines each element as x i = (f i , t i ) with f i \u2208 F being the nontemporal feature at time i and t i \u2208 T as an temporal feature (or we called it positional embedding). Note that f i can be the word representation (in neural machine translation <cite>(Vaswani et al., 2017)</cite> ), a pixel in a frame (in video activity recognition (Wang et al., 2018) ), or a music unit (in music generation (Huang et al., 2018b) ).",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_20",
  "x": "Note that f i can be the word representation (in neural machine translation <cite>(Vaswani et al., 2017)</cite> ), a pixel in a frame (in video activity recognition (Wang et al., 2018) ), or a music unit (in music generation (Huang et al., 2018b) ). t i can be a mixture of sine and cosine functions <cite>(Vaswani et al., 2017)</cite> or parameters that can be learned during back-propagation (Dai et al., 2019; Ott et al., 2019) . The feature vector are defined over a joint space X \u2236= (F \u00d7 T ).",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_21",
  "x": "The feature vector are defined over a joint space X \u2236= (F \u00d7 T ). The resulting permutationinvariant set is: Followed the definition by <cite>Vaswani et al. (2017)</cite> , we use queries(q)/keys(k)/values(v) to represent the inputs for the attention.",
  "y": "uses"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_22",
  "x": "To be more precise, x {q k v} is used for denoting a query/key/value data in the query/key/value sequence x {q k v} (x {q k v} \u2208 S x { q k v} ) with S x { q k v} being its set representation. We note that the input sequences are the same (x q = x k ) for self-attention and are different (x q from decoder and x k from encoder) for encoder-decoder attention. Given the introduced notation, the attention mechanism in original <cite>Transformer</cite> <cite>(Vaswani et al., 2017)</cite> can be presented as:",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_23",
  "x": "Particularly, decoder self-attention considers the decoded sequence as inputs (x k = x q ), where the decoded token at time t is not allowed to access the future decoded tokens (i.e., tokens decoded at time greater than t). On the contrary, encoder selfattention and decoder-encoder attention consider no additional mask to Eq. (1). Recent work (Shaw et al., 2018; Dai et al., 2019; Huang et al., 2018b; Child et al., 2019; Parmar et al., 2018; Tsai et al., 2019a) proposed modifications to the <cite>Transformer</cite> for the purpose of better modeling inputs positional relation (Shaw et al., 2018; Huang et al., 2018b; Dai et al., 2019) , appending additional keys in S x k (Dai et al., 2019) , modifying the mask applied to Eq. (1) (Child et al., 2019) , or applying to distinct feature types Parmar et al., 2018; Tsai et al., 2019a) .",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_24",
  "x": "The filtering function M (\u22c5, \u22c5) plays as the role of the mask in decoder self-attention <cite>(Vaswani et al., 2017)</cite> . Putting these altogether, we re-represent Eq. (1) into the following definition. , and a value function v(\u22c5) \u2236 X \u2192 Y, the Attention function taking the input of a query feature x q \u2208 X is defined as",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_26",
  "x": "Note that the kernel form k(x q , x k ) in the original <cite>Transformer</cite> <cite>(Vaswani et al., 2017 )</cite> is a asymmetric exponential kernel with additional mapping W q and W k (Wilson et al., 2016; Li et al., 2017) 2 . The new formulation defines a larger space for composing attention by manipulating its individual components, and at the same time it is able to categorize different variants of attention in prior work (Shaw et al., 2018; Huang et al., 2018b; Dai et al., 2019; Child et al., 2019; Wang et al., 2018; Tsai et al., 2019a) . In the following, we study these components by dissecting Eq. (2) into: 1) kernel feature space X , 2) kernel construction k(\u22c5, \u22c5), 3) value function v(\u22c5), and 4) set filtering function M (\u22c5, \u22c5).",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_27",
  "x": "2.2.1 Kernel Feature Space X In Eq. (2), to construct a kernel on X , the first thing is to identify the kernel feature space X . In addition to modeling sequences like word sentences <cite>(Vaswani et al., 2017)</cite> or music signals (Huang et al., 2018b) , the <cite>Transformer</cite> can also be applied to images (Parmar et al., 2018) , sets , and multimodal sequences (Tsai et al., 2019a) . Due to distinct data types, these applications admit various kernel feature space: <cite>(Vaswani et al., 2017</cite>; Dai et al., 2019) :",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_28",
  "x": "Due to distinct data types, these applications admit various kernel feature space: <cite>(Vaswani et al., 2017</cite>; Dai et al., 2019) : with F being non-positional feature space and T being the positional embedding space of the position in the sequence. (ii) Image <cite>Transformer</cite> (Parmar et al., 2018) :",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_33",
  "x": "---------------------------------- **KERNEL CONSTRUCTION AND THE ROLE OF** Positional Embedding k(\u22c5, \u22c5) The kernel construction on X = (F \u00d7 T ) has distinct design in variants of <cite>Transformers</cite> <cite>(Vaswani et al., 2017</cite>; Dai et al., 2019; Huang et al., 2018b; Shaw et al., 2018; Child et al., 2019) .",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_34",
  "x": "Kernel construction on X = (F \u00d7 T ). The designs for integrating the positional embedding t q and t k are listed in the following. (i) Absolute Positional Embedding <cite>(Vaswani et al., 2017</cite>; Dai et al., 2019; Ott et al., 2019) : For the original <cite>Transformer</cite> <cite>(Vaswani et al., 2017)</cite> , each t i is represented by a vector with each dimension being sine or cosine functions.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_35",
  "x": "(ii) Relative Positional Embedding in <cite>Transformer</cite>-XL (Dai et al., 2019) : t represents the indicator of the position in the sequence, and the kernel is chosen to be asymmetric of mixing sine and cosine functions: with k fq t q , t k being an asymmetric kernel with coefficients inferred by f q : log k fq t q , t k = \u2211 (iii) Relative Positional Embedding of Shaw et al. (2018) and Music <cite>Transformer</cite> (Huang et al., 2018b) : t \u22c5 represents the indicator of the position in the sequence, and the kernel is modified to be indexed by a look-up table: where L tq\u2212t k ,fq = exp(f q W q a tq\u2212t k ) with a \u22c5 being a learnable matrix having matrix width to be the length of the sequence.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_36",
  "x": "Via the lens of kernel, the kernel similarity is defined as (ii) Relative Positional Embedding in <cite>Transformer</cite>-XL (Dai et al., 2019) : t represents the indicator of the position in the sequence, and the kernel is chosen to be asymmetric of mixing sine and cosine functions: with k fq t q , t k being an asymmetric kernel with coefficients inferred by f q : log k fq t q , t k = \u2211 (iii) Relative Positional Embedding of Shaw et al. (2018) and Music <cite>Transformer</cite> (Huang et al., 2018b) : t \u22c5 represents the indicator of the position in the sequence, and the kernel is modified to be indexed by a look-up table:",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_37",
  "x": "**VALUE FUNCTION V(\u22c5)** The current <cite>Transformers</cite> consider two different value function construction: <cite>(Vaswani et al., 2017)</cite> and Sparse <cite>Transformer</cite> (Child et al., 2019) : (ii) <cite>Transformer</cite>-XL (Dai et al., 2019) , Music <cite>Transformer</cite> (Huang et al., 2018b) , Self-Attention with Relative Positional Embedding (Shaw et al., 2018) :",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_39",
  "x": "In Eq. (2), the returned set by the set filtering function M (x q , S x k ) defines how many keys and which keys are operating with x q . In the following, we itemize the corresponding designs for the variants in <cite>Transformers</cite>: (i) Encoder Self-Attention in original <cite>Transformer</cite> <cite>(Vaswani et al., 2017)</cite> : For each query x q in the encoded sequence, M (x q , S x k ) = S x k contains the keys being all the tokens in the encoded sequence. Note that encoder self-attention considers x q = x k with x q being the encoded sequence.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_40",
  "x": "(ii) Encoder-Decoder Attention in original <cite>Transformer</cite> <cite>(Vaswani et al., 2017)</cite> : For each query x q in decoded sequence, M (x q , S x k ) = S x k contains the keys being all the tokens in the encoded sequence. Note that encode-decoder attention considers x q \u2260 x k with x q being the decoded sequence and x k being the encoded sequence. (iii) Decoder Self-Attention in original <cite>Transformer</cite> <cite>(Vaswani et al., 2017)</cite> : For each query x q in the decoded sequence, M (x q , S x k ) returns a subset of S x k (M (x q , S x k ) \u2282 S x k ).",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_41",
  "x": "(iii) Decoder Self-Attention in original <cite>Transformer</cite> <cite>(Vaswani et al., 2017)</cite> : For each query x q in the decoded sequence, M (x q , S x k ) returns a subset of S x k (M (x q , S x k ) \u2282 S x k ). Note that decoder self-attention considers x q = x k with x q being the decoded sequence. Since the decoded sequence is the output for previous timestep, the query at position i can only observe the keys being the tokens that are decoded with position < i. For convenience, let us define S 1 as the set returned by original <cite>Transformer</cite> <cite>(Vaswani et al., 2017 )</cite> from M (x q , S x k ), which we will use it later.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_42",
  "x": "Note that decoder self-attention considers x q = x k with x q being the decoded sequence. Since the decoded sequence is the output for previous timestep, the query at position i can only observe the keys being the tokens that are decoded with position < i. For convenience, let us define S 1 as the set returned by original <cite>Transformer</cite> <cite>(Vaswani et al., 2017 )</cite> from M (x q , S x k ), which we will use it later. (iv) Decoder Self-Attention in <cite>Transformer</cite>-XL (Dai et al., 2019) : For each query x q in the decoded sequence, M (x q , S x k ) returns a set containing S 1 and additional memories (M (x q , S x k ) = S 1 + S mem , M (x q , S x k ) \u2283 S 1 ).",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_43",
  "x": "Since the decoded sequence is the output for previous timestep, the query at position i can only observe the keys being the tokens that are decoded with position < i. For convenience, let us define S 1 as the set returned by original <cite>Transformer</cite> <cite>(Vaswani et al., 2017 )</cite> from M (x q , S x k ), which we will use it later. (iv) Decoder Self-Attention in <cite>Transformer</cite>-XL (Dai et al., 2019) : For each query x q in the decoded sequence, M (x q , S x k ) returns a set containing S 1 and additional memories (M (x q , S x k ) = S 1 + S mem , M (x q , S x k ) \u2283 S 1 ). S mem refers to additional memories.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_44",
  "x": "(v) Decoder Self-Attention in Sparse <cite>Transformer</cite> (Child et al., 2019) : For each query x q in the decoded sentence, M (x q , S x k ) returns a subset of S 1 (M (x q , S x k ) \u2282 S 1 ). To compare the differences for various designs, we see the computation time is inversely proportional to the number of elements in M (x q , S x k ). For performance-wise comparisons, <cite>Transformer</cite>-XL (Dai et al., 2019) showed that, the additional memories in M (x q , S x k ) are able to capture longer-term dependency than the original <cite>Transformer</cite> <cite>(Vaswani et al., 2017)</cite> and hence results in better performance.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_45",
  "x": "For performance-wise comparisons, <cite>Transformer</cite>-XL (Dai et al., 2019) showed that, the additional memories in M (x q , S x k ) are able to capture longer-term dependency than the original <cite>Transformer</cite> <cite>(Vaswani et al., 2017)</cite> and hence results in better performance. Sparse <cite>Transformer</cite> (Child et al., 2019) showed that although having much fewer elements in M (x q , S x k ), if the elements are carefully chosen, the attention can still reach the same performance as <cite>Transformer</cite>-XL (Dai et al., 2019) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_46",
  "x": "Sparse <cite>Transformer</cite> (Child et al., 2019) showed that although having much fewer elements in M (x q , S x k ), if the elements are carefully chosen, the attention can still reach the same performance as <cite>Transformer</cite>-XL (Dai et al., 2019) . ---------------------------------- **EXPLORING THE DESIGN OF ATTENTION**",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_48",
  "x": "The new form considers product of kernels with the first kernel measuring similarity between non-temporal features and the second kernel measuring similarity between temporal features. Both kernels are symmetric exponential kernel. Note that t i here is chosen as the mixture of sine and cosine functions as in the prior work <cite>(Vaswani et al., 2017</cite>; Ott et al., 2019) .",
  "y": "uses"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_50",
  "x": "We conduct experiments on neural machine translation (NMT) and sequence prediction (SP) tasks since these two tasks are commonly chosen for studying <cite>Transformers</cite> <cite>(Vaswani et al., 2017</cite>; Dai et al., 2019) . Note that NMT has three different types of attentions (e.g., encoder selfattention, decoder-encoder attention, decoder selfattention) and SP has only one type of attention (e.g., decoder self-attention). For the choice of datasets, we pick IWSLT'14 German-English (De-En) dataset (Edunov et al., 2017) for NMT and WikiText-103 dataset (Merity et al., 2016) for SP as suggested by Edunov et al. (Edunov et al., 2017) and Dai et al. (Dai et al., 2019) .",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_52",
  "x": "Similar to prior work <cite>(Vaswani et al., 2017</cite>; Dai et al., 2019) , we report BLEU score for NMT and perplexity for SP. ---------------------------------- **INCORPORATING POSITIONAL EMBEDDING**",
  "y": "similarities"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_53",
  "x": "\u2191 means the upper the better and \u2193 means the lower the better. Table 2 : Kernel Types. Other than manipulating the kernel choice of the non-positional features, we fix the configuration by <cite>Vaswani et al. (2017)</cite> for NMT and the configuration by Dai et al. (2019) for SP.",
  "y": "differences uses"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_54",
  "x": "The numbers are shown in Table 2 . Note that, for fairness, other than manipulating the kernel choice of the non-positional features, we fix the configuration by <cite>Vaswani et al</cite>. <cite>(Vaswani et al., 2017)</cite> for NMT and the configuration by Dai et al. (Dai et al., 2019) for SP. We first observe that the linear kernel does not converge for both NMT and SP.",
  "y": "uses differences"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_56",
  "x": "The need of the positional embedding (PE) in the attention mechanism is based on the argument that the attention mechanism is an order-agnostic (or, permutation equivariant) operation <cite>(Vaswani et al., 2017</cite>; Shaw et al., 2018; Huang et al., 2018b; Dai et al., 2019; Child et al., 2019) . However, we show that, for decoder self-attention, the operation is not order-agnostic. For clarification, we are not attacking the claim made by the prior work <cite>(Vaswani et al.,</cite> 2017; Shaw et al., 2018;  Huang et al., 2018b; Dai et al., 2019; Child et al., 2019 ), but we aim at providing a new look at the order-invariance problem when considering the attention mechanism with masks (masks refer to the set filtering function in our kernel formulation).",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_57",
  "x": "The need of the positional embedding (PE) in the attention mechanism is based on the argument that the attention mechanism is an order-agnostic (or, permutation equivariant) operation <cite>(Vaswani et al., 2017</cite>; Shaw et al., 2018; Huang et al., 2018b; Dai et al., 2019; Child et al., 2019) . However, we show that, for decoder self-attention, the operation is not order-agnostic. For clarification, we are not attacking the claim made by the prior work <cite>(Vaswani et al.,</cite> 2017; Shaw et al., 2018;  Huang et al., 2018b; Dai et al., 2019; Child et al., 2019 ), but we aim at providing a new look at the order-invariance problem when considering the attention mechanism with masks (masks refer to the set filtering function in our kernel formulation).",
  "y": "motivation"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_58",
  "x": "Denote \u03a0 as the set of all permutations over [n] = {1, \u22ef, n}. A function f unc \u2236 X n \u2192 Y n is permutation equivariant iff for any permutation \u03c0 \u2208 \u03a0, f unc(\u03c0x) = \u03c0f unc(x). showed that the standard attention (encoder self-attention <cite>(Vaswani et al., 2017</cite>; Dai et al., 2019) ) is permutation equivariant. Here, we present the non-permutation-equivariant problem on the decoder self-attention: <cite>(Vaswani et al., 2017</cite>; Dai et al., 2019) is not permutation equivariant. To proceed the proof, we need the following definition and propositions.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_60",
  "x": "Nonetheless, the performance is slightly better than considering PE from the original <cite>Transformer</cite> <cite>(Vaswani et al., 2017)</cite> . ---------------------------------- **POSITIONAL EMBEDDING IN VALUE FUNCTION**",
  "y": "differences"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_61",
  "x": "---------------------------------- **RELATED WORK** Other than relating <cite>Transformer's</cite> attention mechanism with kernel methods, the prior work (Wang et al., 2018; Shaw et al., 2018; Tsai et al., 2019b ) related the attention mechanism with graph-structured learning.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_62",
  "x": "Others (Shaw et al., 2018; Tsai et al., 2019b) linked the attention to the message passing in graphical models. In addition to the fundamental difference between graph-structured learning and kernel learning, the prior work (Wang et al., 2018; Shaw et al., 2018; Tsai et al., 2019b) focused on presenting <cite>Transformer</cite> for its particular application (e.g., video classification (Wang et al., 2018) and neural machine translation (Shaw et al., 2018) ). Alternatively, our work focuses on presenting a new formulation of <cite>Transformer's</cite> attention mechanism that gains us the possibility for understanding the attention mechanism better.",
  "y": "background"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_63",
  "x": "Others (Shaw et al., 2018; Tsai et al., 2019b) linked the attention to the message passing in graphical models. In addition to the fundamental difference between graph-structured learning and kernel learning, the prior work (Wang et al., 2018; Shaw et al., 2018; Tsai et al., 2019b) focused on presenting <cite>Transformer</cite> for its particular application (e.g., video classification (Wang et al., 2018) and neural machine translation (Shaw et al., 2018) ). Alternatively, our work focuses on presenting a new formulation of <cite>Transformer's</cite> attention mechanism that gains us the possibility for understanding the attention mechanism better.",
  "y": "motivation"
 },
 {
  "id": "04f6b9d4296dee4bbf965f9911bf98_64",
  "x": "In this paper, we presented a kernel formulation for the attention mechanism in <cite>Transformer</cite>, which allows us to define a larger space for designing attention. As an example, we proposed a new variant of attention which reaches competitive performance when compared to previous state-of-the-art models. Via the lens of the kernel, we were able to better understand the role of individual components in <cite>Transformer's</cite> attention and categorize previous attention variants in a unified formulation.",
  "y": "differences"
 },
 {
  "id": "0526911ab71c85bfa4a20b630f34ae_0",
  "x": "**OVERVIEW** Morphologically rich languages like Arabic <cite>(Beesley, K. 1996</cite> ) present significant challenges to many natural language processing applications as the one described above because a word often conveys complex meanings decomposable into several morphemes (i.e. prefix, stem, suffix) . By segmenting words into morphemes, we can improve the performance of natural language systems including machine translation (Brown et al. 1993 ) and information retrieval (Franz, M. and McCarley, S. 2002) .",
  "y": "background"
 },
 {
  "id": "0526911ab71c85bfa4a20b630f34ae_1",
  "x": "**OVERVIEW** Morphologically rich languages like Arabic <cite>(Beesley, K. 1996</cite> ) present significant challenges to many natural language processing applications as the one described above because a word often conveys complex meanings decomposable into several morphemes (i.e. prefix, stem, suffix) . By segmenting words into morphemes, we can improve the performance of natural language systems including machine translation (Brown et al. 1993 ) and information retrieval (Franz, M. and McCarley, S. 2002) .",
  "y": "extends"
 },
 {
  "id": "05b53f9e0a347c4f47d0fd066538c7_0",
  "x": "Event mentions can appear with varying degrees of uncertainty/factuality to reflect the intent of the writers. In order for the event mentions to be useful (i.e., for knowledge extraction tasks), it is important to determine their factual certainty so the actual event mentions can be retrieved (i.e., the event factuality prediction problem (EFP)). In this work, we focus on the recent regression formulation of EFP that aims to predict a real score in the range of [-3,+3 ] to quantify the occurrence possibility of a given event mention (Stanovsky et al., 2017;<cite> Rudinger et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "05b53f9e0a347c4f47d0fd066538c7_1",
  "x": "For instance, the word \"left\" in the sentence \"She left yesterday.\" would express an event that certainly happened (i.e., corresponding to a score of +3 in the benchmark datasets) while the event mention associated with \"leave\" in the sentence \"She forgot to leave yesterday.\" would certainly not happen (i.e., a score of -3). EFP is a challenging problem as different context words might jointly participate to reveal the factuality of the event mentions (i.e., the cue words), possibly located at different parts of the sentences and scattered far away from the anchor words of the events. There are two major mechanisms that can help the models to identify the cue words and link them to the anchor words, i.e., the syntactic trees (i.e., the dependency trees) and the semantic information<cite> (Rudinger et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "05b53f9e0a347c4f47d0fd066538c7_2",
  "x": "For the syntactic trees, they can connect the anchor words to the functional words (i.e., negation, modal auxiliaries) that are far away, but convey important information to affect the factuality of the event mentions. For instance, the dependency tree of the sentence \"I will, after seeing the treatment of others, go back when I need medical care.\" will be helpful to directly link the anchor word \"go\" to the modal auxiliary \"will\" to successfully predict the non-factuality of the event mention. Regarding the semantic information, the meaning of the some important context words in the sentences can contribute significantly to the factuality of an event mention. For example, in the sentence \"Knight lied when he said I went to the ranch.\", the meaning represented by the cue word \"lied\" is crucial to classify the event mention associated with the anchor word \"went\" as non-factual. The meaning of such cue words and their interactions with the anchor words can be captured via their distributed representations (i.e., with word embeddings and long-short term memory networks (LSTM))<cite> (Rudinger et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "05b53f9e0a347c4f47d0fd066538c7_3",
  "x": "The current state-of-the-art approach for EFP has involved deep learning models <cite>(Rudinger et al., 2018</cite> ) that examine both syntactic and semantic information in the modeling process. However, in these models, the syntactic and semantic information are only employed separately in the different deep learning architectures to generate syntactic and semantic representations. Such representations are only concatenated in the final stage to perform the factuality prediction. A major problem with this approach occurs in the event mentions when the syntactic and semantic information cannot identify the important structures for EFP individually (i.e., by itself). In such cases, both the syntactic and semantic representations from the separate deep learning models would be noisy and/or insufficient, causing the poor quality of their simple combination for EFP.",
  "y": "background motivation"
 },
 {
  "id": "05b53f9e0a347c4f47d0fd066538c7_4",
  "x": "The early work on this problem has employed the rule-based approaches (Nairn et al., 2006; Saur\u00ed, 2008; Lotan et al., 2013) or the machine learning approaches (with manually designed features) (Diab et al., 2009; Prabhakaran et al., 2010; De Marneffe et al., 2012; Lee et al., 2015) , or the hybrid approaches of both (Saur\u00ed and Pustejovsky, 2012; Qian et al., 2015) . Recently, deep learning has been applied to solve EFP. (Qian et al., 2018) employ Generative Adversarial Networks (GANs) for EFP while<cite> (Rudinger et al., 2018)</cite> utilize LSTMs for both sequential and dependency representations of the input sentences.",
  "y": "background"
 },
 {
  "id": "05b53f9e0a347c4f47d0fd066538c7_5",
  "x": "We denote such word embeddings for the words in (x 1 , x 2 , . . . , x n ) as (e 1 , e 2 , . . . , e n ) respectively. In the next step, we further abstract (e 1 , e 2 , . . . , e n ) for EFP by feeding them into two layers of bidirectional LSTMs (as in<cite> (Rudinger et al., 2018)</cite> ). This produces (h 1 , h 2 , . . . , h n ) as the hidden vector sequence in the last bidirectional LSTM layer (i.e., the second one).",
  "y": "uses"
 },
 {
  "id": "05b53f9e0a347c4f47d0fd066538c7_6",
  "x": "Given the hidden representation (h 1 , h 2 , . . . , h n ), it is possible to use the hidden vector corresponding to the anchor word h k as the features to perform factuality prediction (as done in<cite> (Rudinger et al., 2018)</cite> ). However, despite the rich context information over the whole sentence, the features in h k are not directly designed to focus on the import context words for factuality prediction. In order to explicitly encode the information of the cue words into the representations for the anchor word, we propose to learn an importance matrix A = (a ij ) i,j=1..n in which the value in the cell a ij quantifies the contribution of the context word x i for the hidden representation at x j if the representation vector at x j is used to form features for EFP.",
  "y": "differences"
 },
 {
  "id": "05b53f9e0a347c4f47d0fd066538c7_7",
  "x": "where W a 1 , W a 2 and W a 3 are the model parameters. The attention weights \u03b1 \u2032 i would help to promote the contribution of the important context words for the feature vector V for EFP. Finally, similar to<cite> (Rudinger et al., 2018)</cite> , the feature vector V is fed into a regression model with two layers of feed-forward networks to produce the factuality score.",
  "y": "uses"
 },
 {
  "id": "05b53f9e0a347c4f47d0fd066538c7_8",
  "x": "Finally, similar to<cite> (Rudinger et al., 2018)</cite> , the feature vector V is fed into a regression model with two layers of feed-forward networks to produce the factuality score. Following<cite> (Rudinger et al., 2018)</cite> , we train the proposed model by optimizing the Huber loss with \u03b4 = 1 and the Adam optimizer with learning rate = 1.0. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "05b53f9e0a347c4f47d0fd066538c7_9",
  "x": "For the fourth dataset (i.e., UDS-IH2), we follow the instructions in<cite> (Rudinger et al., 2018)</cite> to scale the scores to the range of [-3, +3] . Each dataset comes with its own training data, test data and development data. Table 2 shows the numbers of examples in all data splits for each dataset used in this paper.",
  "y": "uses"
 },
 {
  "id": "05b53f9e0a347c4f47d0fd066538c7_10",
  "x": "Following the previous work (Stanovsky et al., 2017;<cite> Rudinger et al., 2018)</cite> , we evaluate the proposed EFP model using four benchmark datasets: FactBack (Saur\u00ed and Pustejovsky, 2009 ), UW (Lee et al., 2015) , Meantime (Minard et al., 2016) and UDS-IH2<cite> (Rudinger et al., 2018)</cite> . The first three datasets (i.e., FactBack, UW, and Meantime) are the unified versions described in (Stanovsky et al., 2017) where the original annotations for these datasets are scaled to a number in [-3, +3] . For the fourth dataset (i.e., UDS-IH2), we follow the instructions in<cite> (Rudinger et al., 2018)</cite> to scale the scores to the range of [-3, +3] .",
  "y": "uses"
 },
 {
  "id": "05b53f9e0a347c4f47d0fd066538c7_11",
  "x": "We compare the proposed model with the best reported systems in the literature with linguistic features (Lee et al., 2015; Stanovsky et al., 2017) and deep learning<cite> (Rudinger et al., 2018)</cite> . Table 1 shows the performance. Importantly, to achieve a fair comparison, we obtain the actual implementation of the current state-of-the-art EFP models from<cite> (Rudinger et al., 2018)</cite> , introduce the BERT embeddings as the inputs for those models and compare them with the proposed models (i.e., the rows with \"+BERT\").",
  "y": "background"
 },
 {
  "id": "05b53f9e0a347c4f47d0fd066538c7_12",
  "x": "Importantly, to achieve a fair comparison, we obtain the actual implementation of the current state-of-the-art EFP models from<cite> (Rudinger et al., 2018)</cite> , introduce the BERT embeddings as the inputs for those models and compare them with the proposed models (i.e., the rows with \"+BERT\"). Following the prior work, we use MAE (Mean Absolute Error), and r (Pearson Correlation) as the performance measures. In the table, we distinguish two methods to train the models investigated in the previous work: (i) training and evaluating the models on separate datasets (i.e., the rows associated with *), and (ii) training the models on the union of FactBank, UW and Meantime, resulting in single models to be evaluated on the separate datasets (i.e., the rows with **).",
  "y": "extends"
 },
 {
  "id": "05d1ecc230c7907d9a14d3351070c3_0",
  "x": "The introduction of pre-trained language models, such as BERT <cite>[2]</cite> and Open-GPT [3] , among many others, has brought tremendous progress to the NLP research and industrial communities. The contribution of these models can be categorized into two aspects. First, pre-trained language models allow modelers to achieve reasonable accuracy without the need an excessive amount of manually labeled data.",
  "y": "background"
 },
 {
  "id": "05d1ecc230c7907d9a14d3351070c3_1",
  "x": "The aforementioned deficiencies prompt researchers to propose deep neural networks that are able to be trained in an unsupervised fashion while being able to capture the contextual meaning of the words presented in the texts. Some early attempts include pre-trained models includes, CoVe [12] , CVT [13, 14] , ELMo [15] and ULMFiT [16] . However, the most successful ones are BERT <cite>[2]</cite> and Open-GPT [3] .",
  "y": "background"
 },
 {
  "id": "05d1ecc230c7907d9a14d3351070c3_2",
  "x": "In the presence of the success of pre-trained language models, especially BERT <cite>[2]</cite> , it is natural to ask how to best utilize the pre-trained language models to achieve new state-of-the-art results. In this line of work, Liu et al. [21] investigated the linguistic knowledge and transferability of contextual representations by comparing BERT <cite>[2]</cite> with ELMo [15] , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models. Stickland and Murray [22] invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of Devlin et al. <cite>[2]</cite> .",
  "y": "motivation"
 },
 {
  "id": "05d1ecc230c7907d9a14d3351070c3_3",
  "x": "In the presence of the success of pre-trained language models, especially BERT <cite>[2]</cite> , it is natural to ask how to best utilize the pre-trained language models to achieve new state-of-the-art results. In this line of work, Liu et al. [21] investigated the linguistic knowledge and transferability of contextual representations by comparing BERT <cite>[2]</cite> with ELMo [15] , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models. Stickland and Murray [22] invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of Devlin et al. <cite>[2]</cite> .",
  "y": "background motivation"
 },
 {
  "id": "05d1ecc230c7907d9a14d3351070c3_4",
  "x": "In this line of work, Liu et al. [21] investigated the linguistic knowledge and transferability of contextual representations by comparing BERT <cite>[2]</cite> with ELMo [15] , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models. Stickland and Murray [22] invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of Devlin et al. <cite>[2]</cite> . Xu et al. [23] propose a \"post-training\" algorithms, which does not directly fine-tune BERT, but rather first \"post-train\" BERT on the task related corpus using the masked language prediction task next sentence prediction task, which helps to reduce the bias in the training corpus.",
  "y": "background"
 },
 {
  "id": "05d1ecc230c7907d9a14d3351070c3_5",
  "x": "Pre-training models have been used to obtain more effective word representations through the study of a large number of corpora. In the paradigm proposed in the original work by Devlin et al. <cite>[2]</cite> , the author directly trained BERT along with with a light-weighted task-specific head. In our case though, we top BERT with a more complex network structure, using Kaiming initialization [29] .",
  "y": "background"
 },
 {
  "id": "05d1ecc230c7907d9a14d3351070c3_6",
  "x": "We tried to set the learning rate of the first stage to 1e \u22121 ,1e \u22122 ,5e \u22123 ,1e \u22123 and 5e \u22124 , and set it to a smaller number in the latter stage, such as 1e \u22123 ,1e \u22124 ,5e \u22125 and 1e \u22125 . After our experiments, we found that it gets better results while the learning rate is set to 0.001 in the stage of training only the upper model and set to 5e \u22125 in the later stage. Since BERT-Adam <cite>[2]</cite> has excellent performance, in our experiments, we use it as an optimizer with \u03b2 1 = 0.9, \u03b2 2 = 0.999,L 2 -weight decay of 0.01.We apply a dropout trick on all layers and set the dropout probability as 0.1.",
  "y": "similarities uses"
 },
 {
  "id": "05d1ecc230c7907d9a14d3351070c3_7",
  "x": "**EXPERIMENT A: SEQUENCE LABELING** In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset [6] , which is a public available used in many studies to test the accuracy of their proposed methods [30, 31, 32, 33,<cite> 2]</cite> . For strategy finetune-only and strategy stack-and-finetune, we implemented two models: one with BERT and the other with BERT adding a Bi-LSTM on top.",
  "y": "similarities uses"
 },
 {
  "id": "05eecafea7684dc8de13c29a76b767_0",
  "x": "The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users<cite> (Eisenstein et al., 2010</cite>; Roller et al., 2012; Rout et al., 2013; Wing and Baldridge, 2014) or dialectology Eisenstein, 2015) . In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (Pavalanathan and Eisenstein, 2015) . Lexical dialectology is (in part) the converse of user geolocation (Eisenstein, 2015) : given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions.",
  "y": "background"
 },
 {
  "id": "05eecafea7684dc8de13c29a76b767_1",
  "x": "Text-based methods make use of the geographical biases of language use, and networkbased methods rely on the geospatial homophily of user-user interactions. In both cases, the assumption is that users who live in the same geographic area share similar features (linguistic or interactional). Three main text-based approaches are: (1) the use of gazetteers Quercini et al., 2010) ; (2) unsupervised text clustering based on topic models or similar<cite> (Eisenstein et al., 2010</cite>; Hong et al., 2012; Ahmed et al., 2013) ; and (3) supervised classification (Ding et al., 2000; Backstrom et al., 2008; Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Wing and Baldridge, 2011; Han et al., 2012; Rout et al., 2013) , which unlike gazetteers can be applied to informal text and compared to topic models, scales better.",
  "y": "background"
 },
 {
  "id": "05eecafea7684dc8de13c29a76b767_2",
  "x": "There have also been attempts to automatically identify such words from geotagged documents<cite> (Eisenstein et al., 2010</cite>; Ahmed et al., 2013; Eisenstein, 2015) . The main idea is to find lexical variables that are disproportionately distributed in different locations either via model-based or statistical methods (Monroe et al., 2008) . There is a research gap in evaluating the geolocation models in terms of their usability in retrieving dialect terms given a geographic region.",
  "y": "background"
 },
 {
  "id": "05eecafea7684dc8de13c29a76b767_3",
  "x": "---------------------------------- **DATA** We use three existing Twitter user geolocation datasets: (1) GEOTEXT<cite> (Eisenstein et al., 2010)</cite> , (2) TWITTER-US (Roller et al., 2012) , and (3) TWITTER-WORLD (Han et al., 2012) .",
  "y": "uses"
 },
 {
  "id": "05eecafea7684dc8de13c29a76b767_4",
  "x": "The number of regions, regularisation strength, hidden layer and mini-batch size are tuned over development data and set to (32, 10 \u22125 , 896, 100), (256, 10 \u22126 , 2048, 10000) and (930, 10 \u22126 , 3720, 10000) for GEOTEXT, TWITTER-US and TWITTER-WORLD, respectively. The parameters are optimised using Adamx (Kingma and Ba, 2014) using Lasagne/Theano (Theano Development Team, 2016) . Following Cheng (2010) and<cite> Eisenstein (2010)</cite> , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\").",
  "y": "uses"
 },
 {
  "id": "05fe3e9c1598f5b36b6efa79216309_0",
  "x": "Later, Dong et. al. presented an efficient method to generate the next word in a sequence when it is added an attention mechanism, improving the performance for long textual sequences <cite>[1]</cite> .",
  "y": "background"
 },
 {
  "id": "05fe3e9c1598f5b36b6efa79216309_1",
  "x": "In this paper, we propose a character-level attention-enhanced long short-term memory (LSTM) model to generate personalized natural language explanations based on user-generated reviews. The model is trained using two real-world datasets: BeerAdvocate [5] and Amazon book reviews <cite>[1]</cite> . The datasets present user reviews describing their opinion about items in natural language.",
  "y": "uses"
 },
 {
  "id": "05fe3e9c1598f5b36b6efa79216309_2",
  "x": "\u2022 Attention mechanism The attention mechanism, adaptively learns soft alignments c t between character dependencies H t and attention inputs a. Eq. 1 formally defines the new character dependencies using attention layer H attention t <cite>[1]</cite> . \u2022 Generating Text The explanation is generated character by character. The characters are given by maximizing the softmax conditional probability p, based on the new character dependencies H attention t <cite>[1]</cite> , as presented in Eq. 2 p = softmax(H attention t W + b), char = arg max p (2)",
  "y": "uses"
 },
 {
  "id": "05fe3e9c1598f5b36b6efa79216309_3",
  "x": "\u2022 Generating Text The explanation is generated character by character. The characters are given by maximizing the softmax conditional probability p, based on the new character dependencies H attention t <cite>[1]</cite> , as presented in Eq. 2 p = softmax(H attention t W + b), char = arg max p (2) ----------------------------------",
  "y": "uses"
 },
 {
  "id": "05fe3e9c1598f5b36b6efa79216309_4",
  "x": "**CONCLUSION** The work provides preliminary results in automatically generating natural language explanations. The model differs from recent works <cite>[1,</cite> 6] , due to the use of attention layer combined with character-level LSTM.",
  "y": "differences"
 },
 {
  "id": "06276db79ed5aa04bb24a31c10d3a9_0",
  "x": "AMRs can be seen as graphs connecting concepts by relations. Each concept is represented by a named instance. Co-reference is established by re-using these instances. For example, the AMRs corresponding to examples (1) and (2) above are given in Figure 1 . Note that, due to the bracketing, the variable b encapsulates the whole entity person :name \"Bob\" and not just person, i.e. b stands for a person with the name Bob. That there is a lot to gain in this area can be seen by applying the AMR evaluation suite of Damonte et al. (2017) , which calculates nine different metrics to evaluate AMR parsing, reentrancy being one of them. Out of the four systems that made these scores available (all scores reported in<cite> van Noord and Bos (2017)</cite> ), the reentrancy metric obtained the lowest F-score for three of them.",
  "y": "background"
 },
 {
  "id": "06276db79ed5aa04bb24a31c10d3a9_1",
  "x": "Various methods have been proposed to automatically parse AMRs, ranging from syntax-based approaches (e.g. Flanigan et al. (2014) ; Wang et al. (2015) ; Pust et al. (2015) ; Damonte et al. (2017) ) to the more recent neural approaches (Peng et al. (2017) ; Buys and Blunsom (2017) ; Konstas et al. (2017) ; Foland and Martin (2017);<cite> van Noord and Bos (2017)</cite> ). Especially the neural approaches are interesting, since they all use some sort of linearization method and therefore need a predefined way to handle reentrancy. Peng et al. (2017) and Buys and Blunsom (2017) use a special character to indicate reentrancy and restore co-referring variables in a post-processing step.",
  "y": "background motivation"
 },
 {
  "id": "06276db79ed5aa04bb24a31c10d3a9_2",
  "x": "Foland and Martin (2017) and<cite> van Noord and Bos (2017)</cite> use the same input transformation as Konstas et al. (2017) , but do try to restore co-referring nodes by merging all equal concepts into a single concept in a post-processing step. All these methods have in common that they are not very sophisticated, but more importantly, that it is not clear what the exact impact of these methods is on the final performance of the model, making it unclear what the best implementation is for future neural AMR parsers. In this paper we present three methods to handle reentrancy for AMR parsing.",
  "y": "background motivation"
 },
 {
  "id": "06276db79ed5aa04bb24a31c10d3a9_3",
  "x": "In this paper we present three methods to handle reentrancy for AMR parsing. The first two methods are based on the previous work described above, while the third is a new, more principled method. These methods are applied on the model that reported the best results in the literature, the character-level neural semantic parsing method of<cite> van Noord and Bos (2017)</cite> . In a nutshell, this method uses a character-based sequence-to-sequence model to translate sentences to AMRs. To enable this process, pre-processing and post-processing steps are needed.",
  "y": "uses background"
 },
 {
  "id": "06276db79ed5aa04bb24a31c10d3a9_4",
  "x": "Method 1B: Reentrancy Restoring This method is created to restore reentrancy nodes in the output of the baseline model. It operates on a very ad hoc principle: if two nodes have the same concept, the second one was actually a reference to the first one. We therefore replace each node that has already occurred in the AMR by the variable of the antecedent node. This approach was applied by<cite> van Noord and Bos (2017)</cite> and Foland and Martin (2017) .",
  "y": "uses"
 },
 {
  "id": "06276db79ed5aa04bb24a31c10d3a9_5",
  "x": "The parameter settings are the same as in<cite> van Noord and Bos (2017)</cite> and are shown in Table 2 . It is trained for 20 epochs, after which the model that performs best on the development set is used to decode the test set. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "06276db79ed5aa04bb24a31c10d3a9_6",
  "x": "---------------------------------- **EXPERIMENTS** We test the impact of the different methods on two of our earlier models, described in<cite> van Noord and Bos (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "06276db79ed5aa04bb24a31c10d3a9_7",
  "x": "We must note that CAMR is not particularly keen on outputting coreference, as the 100,000 silver AMRs only produced 18,865 new reentrancy nodes. The second approach also employs the postprocessing methods Wikification and pruning, as explained in<cite> van Noord and Bos (2017)</cite>. The Wikification step simply adds wiki links to :name nodes, since those links were removed in the input.",
  "y": "uses background"
 },
 {
  "id": "06db17253d76150772c0926e11131d_0",
  "x": "Solving the VQA task could have tremendous impacts on real-world applications such as aiding visually impaired users in understanding their physical and online surroundings, searching through large quantities of visual data via natural language interfaces, or even communicating with robots using more efficient and intuitive interfaces. Several large real image VQA datasets have recently emerged [8] [9]<cite> [10]</cite> [11] [12] [13] [14] . Each one of them targets specific abilities that a VQA model would need to be used in real-world settings such as fine-grained recognition, object detection, counting, activity recognition, commonsense reasoning, etc.",
  "y": "background"
 },
 {
  "id": "06db17253d76150772c0926e11131d_1",
  "x": "reasoning [23] . However, it has been shown that they tend to exploit statistical regularities between answer occurrences and certain patterns in the question [24, <cite>10,</cite> 25, 23, 13] . While they are designed to merge information from both modalities, in practice they often answer without considering the image modality.",
  "y": "motivation"
 },
 {
  "id": "06db17253d76150772c0926e11131d_2",
  "x": "However, when evaluated on a test set that displays different statistical regularities, they usually suffer from a significant drop in accuracy<cite> [10,</cite> 25] . Unfortunately, these statistical regularities are hard to avoid when collecting real datasets. As illustrated in Figure 1 , there is a crucial need to develop new strategies to reduce the amount of biases coming from the question modality in order to learn better behaviors.",
  "y": "motivation"
 },
 {
  "id": "06db17253d76150772c0926e11131d_3",
  "x": "We run extensive experiments on VQA-CP v2<cite> [10]</cite> and demonstrate the ability of RUBi to surpass current state-of-the-art results from a significant margin. This dataset has been specifically designed to assess the capacity of VQA models to be robust to biases from the question modality. We show that our RUBi learning framework provides gains when applied on several VQA architectures such as Stacked Attention Networks [26] and Top-Down Bottom-Up Attention [15] .",
  "y": "uses"
 },
 {
  "id": "06db17253d76150772c0926e11131d_4",
  "x": "Unfortunately, biased models that exploit statistical shortcuts from one modality usually reach impressive accuracy on most of the current benchmarks. VQA-CP v2 and VQA-CP v1<cite> [10]</cite> were recently introduced as diagnostic datasets containing different answer distributions for each questiontype between train and test splits. Consequentially, models biased towards the question modality fail on these benchmarks.",
  "y": "background"
 },
 {
  "id": "06db17253d76150772c0926e11131d_5",
  "x": "VQA-CP v2 and VQA-CP v1<cite> [10]</cite> were recently introduced as diagnostic datasets containing different answer distributions for each questiontype between train and test splits. Consequentially, models biased towards the question modality fail on these benchmarks. We use the more challenging VQA-CP v2 dataset extensively in order to show the ability of our approach to reduce the learning of biases coming from the question modality.",
  "y": "uses"
 },
 {
  "id": "06db17253d76150772c0926e11131d_6",
  "x": "For instance, VQA v2 [9] has been introduced to weaken language priors in the VQA v1 dataset [8] by identifying complementary images. For a given VQA v1 question, VQA v2 also contains a similar image with a different answer to the same question. However, even with this additional balancing, statistical biases from the question remain and can be leveraged<cite> [10]</cite> .",
  "y": "motivation"
 },
 {
  "id": "06db17253d76150772c0926e11131d_7",
  "x": "VQA models are inclined to learn unimodal biases from the datasets<cite> [10]</cite> . This can be shown by evaluating models on datasets that have different distributions of answers for the test set, such as VQA-CP v2. In other words, they rely on statistical regularities from one modality to provide accurate predictions without having to consider the other modality.",
  "y": "background"
 },
 {
  "id": "06db17253d76150772c0926e11131d_8",
  "x": "However, even with this additional balancing, statistical biases from the question remain and can be leveraged<cite> [10]</cite> . That is why we propose an approach to reduce unimodal biases during training. It is designed to learn unbiased models from biased datasets.",
  "y": "motivation"
 },
 {
  "id": "06db17253d76150772c0926e11131d_9",
  "x": "---------------------------------- **EXPERIMENTS** Experimental setup We train and evaluate our models on VQA-CP v2<cite> [10]</cite> .",
  "y": "uses"
 },
 {
  "id": "06db17253d76150772c0926e11131d_10",
  "x": "We compute the average accuracy over 5 experiments with different random seeds. Our RUBi approach reaches an average overall accuracy of 47.11% with a low standard deviation of \u00b10.51. This accuracy corresponds to a gain of +5.94 percentage points over the current state-of-the-art UpDn + Q-Adv + DoE. It also corresponds to a gain of +15.88 over GVQA<cite> [10]</cite> , which is a specific architecture designed for VQA-CP.",
  "y": "differences"
 },
 {
  "id": "06db17253d76150772c0926e11131d_11",
  "x": "We report a drop of 1.94 percentage points with respect to our baseline, while<cite> [10]</cite> report a drop of 3.78 between GVQA and their SAN baseline. [25] report drops of 0.05, 0.73 and 2.95 for their three learning strategies with the UpDn architecture which uses the same visual features as RUBi. As shown in this section, RUBi improves the accuracy on VQA-CP v2 from a large margin, while maintaining competitive performance on the standard VQA v2 dataset compared to similar approaches.",
  "y": "differences"
 },
 {
  "id": "06de9a8e72b832beea9c2f17e0862a_0",
  "x": "Later on, pressure from language researchers forced us to replace it with terms such as \"online memory minimization\"<cite> [5]</cite> because our initial formulation was obscure to them. Recently, researchers from all over the world have been granted to use the term \"dependency length minimization\" by the popes thanks to whom [6] came into light. Although \"length\" is a particular case of distance in this context and thus downsizes our original formulation, it is still abstract enough to allow for progress in theoretical research [7] and frees us from the heavy burden of contingency, i.e. the real implementation of the principle (at present believed to result from decay and interference as reviewed by Liu et al) or the current view of the architecture of memory [8, 9] .",
  "y": "uses background"
 },
 {
  "id": "06de9a8e72b832beea9c2f17e0862a_1",
  "x": "Our position is grounded on the high predictive power of that principle per se<cite> [5]</cite> . However, the lower generality of the term \"dependency length\" can be an Email address: rferrericancho@cs.upc.edu (R. Ferrer-i-Cancho) 1 These were pieces of our PhD thesis [4] that were submitted for publication before its defense.",
  "y": "uses"
 },
 {
  "id": "06de9a8e72b832beea9c2f17e0862a_2",
  "x": "For sociological reasons, these arguments started appearing in print many years later [20, <cite>5,</cite> 21] . For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise) [21, 20] . Liu et al review two major sources of evidence of dependency length minimization: the analysis of dependency treebanks and psychological experiments.",
  "y": "background"
 },
 {
  "id": "06de9a8e72b832beea9c2f17e0862a_4",
  "x": "By the turn of the 20th century, we put forward a \"Euclidean distance minimization\" hypothesis for the distance between syntactically linked words and various word order phenomena [2, 3] 1 . Later on, pressure from language researchers forced us to replace it with terms such as \"online memory minimization\"<cite> [5]</cite> because our initial formulation was obscure to them. Recently, researchers from all over the world have been granted to use the term \"dependency length minimization\" by the popes thanks to whom [6] came into light.",
  "y": "uses background"
 },
 {
  "id": "06de9a8e72b832beea9c2f17e0862a_5",
  "x": "Although \"length\" is a particular case of distance in this context and thus downsizes our original formulation, it is still abstract enough to allow for progress in theoretical research [7] and frees us from the heavy burden of contingency, i.e. the real implementation of the principle (at present believed to result from decay and interference as reviewed by Liu et al) or the current view of the architecture of memory [8, 9] . Our position is grounded on the high predictive power of that principle per se<cite> [5]</cite> . However, the lower generality of the term \"dependency length\" can be an obstacle to the construction of a fully-fledged scientific field [10] .",
  "y": "uses"
 },
 {
  "id": "06de9a8e72b832beea9c2f17e0862a_6",
  "x": "In 2009, we put forward another fundamental word order principle, i.e. predictability maximization, and presented a theoretical framework culminating in a conflict between dependency length minimization and predictability maximization [19] . For sociological reasons, these arguments started appearing in print many years later [20, <cite>5,</cite> 21] . For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise) [21, 20] .",
  "y": "background"
 },
 {
  "id": "0706cab049274ffc82c5e2ef6f7b99_0",
  "x": "For example, the coding manual for the Switchboard DAMSL dialogue act annotation scheme (Jurafsky, Shriberg, and Biasca 1997, page 2) states that kappa is used to \"assess labelling accuracy,\" and Di<cite> Eugenio and Glass (2004)</cite> relate reliability to \"the objectivity of decisions,\" whereas Carletta (1996) regards reliability as the degree to which we understand the judgments that annotators are asked to make. Although most researchers recognize that reporting agreement statistics is an important part of evaluating coding schemes, there is frequently a lack of understanding of what the figures actually mean. The intended meaning of reliability should refer to the degree to which the data generated by coders applying a scheme can be relied upon.",
  "y": "background"
 },
 {
  "id": "0706cab049274ffc82c5e2ef6f7b99_1",
  "x": "Any differences in the results between corpora are a function of the variance between samples and not of the reliability of the coding scheme. Di<cite> Eugenio and Glass (2004)</cite> identify three general classes of agreement statistics and suggest that all three should be used in conjunction in order to accurately evaluate coding schemes. However, this suggestion is founded on some misunderstandings of the role of agreement measure in reliability studies.",
  "y": "background"
 },
 {
  "id": "0706cab049274ffc82c5e2ef6f7b99_2",
  "x": "The justification given for using percentage agreement is that it does not suffer from what Di<cite> Eugenio and Glass (2004)</cite> referred to as the \"prevalence problem. \" Prevalence refers to the unequal distribution of label use by coders. For example, Table 1 shows an example taken from Di<cite> Eugenio and Glass (2004)</cite> showing the classification of the utterance Okay as an acceptance or acknowledgment.",
  "y": "differences"
 },
 {
  "id": "0706cab049274ffc82c5e2ef6f7b99_3",
  "x": "For example, Table 1 shows an example taken from Di<cite> Eugenio and Glass (2004)</cite> showing the classification of the utterance Okay as an acceptance or acknowledgment. It represents a confusion matrix describing the number of occasions that coders used pairs of labels for a given turn. This table shows that the two coders favored the use of accept strongly over acknowledge.",
  "y": "background"
 },
 {
  "id": "0706cab049274ffc82c5e2ef6f7b99_4",
  "x": "We therefore require a comparable increase in observed agreement to accommodate this. Di<cite> Eugenio and Glass (2004)</cite> perceive this as an \"unpleasant behavior\" of chancecorrected tests, one that prevents us from concluding that the example given in Table 1 shows satisfactory levels of agreement. Instead they use percentage agreement to arrive at this conclusion.",
  "y": "differences"
 },
 {
  "id": "0706cab049274ffc82c5e2ef6f7b99_5",
  "x": "The second class of agreement measure recommended in Di<cite> Eugenio and Glass (2004)</cite> is that of chance-corrected tests that do not assume an equal distribution of categories between coders. Chance-corrected tests compute agreement according to the ratio of observed (dis)agreement to that which we could expect by chance, estimated from the data. The measures differ in the way in which this expected (dis)agreement is estimated.",
  "y": "background"
 },
 {
  "id": "0706cab049274ffc82c5e2ef6f7b99_6",
  "x": "In circumstances in which mechanisms other than nominal labels are used to annotate data, alpha has the benefit of being able to deal with different degrees of disagreement between pairs of interval, ordinal, and ratio values, among others. Di<cite> Eugenio and Glass (2004)</cite> conclude with the proposal that these three forms of agreement measure collectively provide better means with which to judge agreement than any individual test. We would argue, to the contrary, that applying three different metrics to measure the same property suggests a lack of confidence in any of them.",
  "y": "background"
 },
 {
  "id": "0706cab049274ffc82c5e2ef6f7b99_7",
  "x": "Most commonly this is Krippendorff's decision criterion, in which scores greater than 0.8 are considered satisfactory and scores greater than 0.667 allow tentative conclusions to be drawn (Krippendorff 2004a) . The prevalent use of this criterion despite repeated advice that it is unlikely to be suitable for all studies (Carletta 1996; <cite>Di Eugenio and Glass 2004</cite>; Krippendorff 2004a ) is probably due to a desire for a simple system that can be easily applied to a scheme. Unfortunately, because of the diversity of both the phenomena being coded and the applications of the results, it is impossible to prescribe a scale against which all coding schemes can be judged.",
  "y": "background"
 },
 {
  "id": "0763666190b6b4be1bcf494d7c6fe2_0",
  "x": "Grenager and Manning (2006) proposed a directed graphical model for role induction that exploits linguistic priors for syntactic and semantic inference. Following this work, Lang and Lapata (2010) formulated role induction as the problem of detecting alternations and mapping non-standard linkings to cannonical ones, and later as a graph partitioning problem in (Lang and Lapata, 2011b) . They also proposed an algorithm that uses successive splits and merges of semantic roles clusters in order to improve their quality in<cite> (Lang and Lapata, 2011a)</cite> .",
  "y": "background"
 },
 {
  "id": "0763666190b6b4be1bcf494d7c6fe2_1",
  "x": "Following common practice<cite> (Lang and Lapata, 2011a</cite>; Titov and Klementiev, 2012) , we assume oracle argument identification and focus on argument labeling. The approach we propose is an unsupervised generative Bayesian model that clusters arguments into classes each of which can be associated with a semantic role. The model starts by generating a frame assignment to each verb instance where a frame is a clustering of verbs and associated roles.",
  "y": "similarities uses"
 },
 {
  "id": "0763666190b6b4be1bcf494d7c6fe2_2",
  "x": "As done in<cite> (Lang and Lapata, 2011a)</cite> and (Titov and Klementiev, 2012) , we use purity and collocation measures to assess the quality of our role induction process. For each verb, the purity of roles' clusters is computed as follows: where C i is the set of arguments in the i th cluster found, G j is the set of arguments in the j th gold class, and N is the number of argument instances.",
  "y": "similarities uses"
 },
 {
  "id": "0763666190b6b4be1bcf494d7c6fe2_3",
  "x": "In the same way as<cite> (Lang and Lapata, 2011a)</cite> , we use the micro-average obtained by weighting the scores for individual verbs proportionally to the number of argument instances for that verb. Finally the F1 measure is the harmonic mean of the aggregated values of purity and collocation: ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "0763666190b6b4be1bcf494d7c6fe2_4",
  "x": "Table 1 shows the set of roles used and their relation to VerbNet roles. This constitutes our gold evaluation corpus. The baseline model is the \"syntactic function\" used for instance in<cite> (Lang and Lapata, 2011a)</cite> , which simply clusters predicate arguments according to the dependency relation to their head.",
  "y": "similarities"
 },
 {
  "id": "0763666190b6b4be1bcf494d7c6fe2_5",
  "x": "We made our best to follow the setup used in previous work<cite> (Lang and Lapata, 2011a</cite>; Titov and Kle-mentiev, 2012) , in order to compare with the current state of the art. The data used is the standard CoNLL 2008 shared task (Surdeanu et al., 2008) version of Penn Treebank WSJ and PropBank. Our model is evaluated on gold generated parses, using the gold PropBank annotations.",
  "y": "similarities uses"
 },
 {
  "id": "0763666190b6b4be1bcf494d7c6fe2_6",
  "x": "Besides, roles A0 and A1 attempt to capture Proto-Agent and Proto-Patient roles (Dowty, 1991) , and thus are more valid across verbs and verb instances than A2-A5 roles. Table 3 reports the evaluation results of the proposed model along with those of the baseline system and of some of the latest state-of-the-art results. We can first note that, despite our efforts to reproduce the same baseline, there is still a difference between our baseline (Synt.Func.) and the baseline reported in<cite> (Lang and Lapata, 2011a)</cite>",
  "y": "differences"
 },
 {
  "id": "0763666190b6b4be1bcf494d7c6fe2_7",
  "x": "The other results respectively correspond to the Split Merge approach presented in<cite> (Lang and Lapata, 2011a</cite> ) (Split Merge), the Graph Partitioning algorithm (Graph Part.) presented in (Lang and Lapata, 2011b) , and two Bayesian approaches presented in (Titov and Klementiev, 2012) , which achieve the best current unsupervised SRL results. The first such model (TK-Bay.1) clusters argument fillers and directly maps some syntactic labels to semantic roles for some adjunct like modifiers that are explicitly represented in the syntax, while the second model (TK-Bay.2) does not include these two features. Two versions of the proposed model are reported in the last rows of Table 3 : one with random (uniform) initialization of all variables, and the other with deterministic initialization of all R i from the syntactic function.",
  "y": "similarities"
 },
 {
  "id": "07b062d569749924fa6ee1b2223411_0",
  "x": "**INTRODUCTION** A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006) . One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002;<cite> Clark and Curran, 2004b</cite>; Malouf and van Noord, 2004; Miyao and Tsujii, 2005) .",
  "y": "background"
 },
 {
  "id": "07b062d569749924fa6ee1b2223411_1",
  "x": "Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local (Miyao and Tsujii, 2002) ; however, the memory requirements can be prohibitive, especially for automatically extracted, wide-coverage grammars. In<cite> Clark and Curran (2004b)</cite> we use cluster computing resources to solve this problem.",
  "y": "background"
 },
 {
  "id": "07b062d569749924fa6ee1b2223411_2",
  "x": "Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local (Miyao and Tsujii, 2002) ; however, the memory requirements can be prohibitive, especially for automatically extracted, wide-coverage grammars. In<cite> Clark and Curran (2004b)</cite> we use cluster computing resources to solve this problem. Parsing research has also begun to adopt discriminative methods from the Machine Learning literature, such as the perceptron (Freund and Schapire, 1999; Collins and Roark, 2004) and the largemargin methods underlying Support Vector Machines (Taskar et al., 2004; McDonald, 2006) .",
  "y": "background"
 },
 {
  "id": "07b062d569749924fa6ee1b2223411_3",
  "x": "Hence, for efficient training, these methods require an efficient decoder; in fact, for methods like the perceptron, the update procedure is so trivial that the training algorithm essentially is decoding. This paper describes a decoder for a lexicalizedgrammar parser which is efficient enough for practical discriminative training. We use a lexicalized phrase-structure parser, the CCG parser of<cite> Clark and Curran (2004b)</cite> , together with a DP-based decoder.",
  "y": "uses"
 },
 {
  "id": "07b062d569749924fa6ee1b2223411_4",
  "x": "Previous discriminative models for CCG <cite>(Clark and Curran, 2004b)</cite> required cluster computing resources to train. In this paper we reduce the memory requirements from 20 GB of RAM to only a few hundred MB, but without greatly increasing the training time or reducing parsing accuracy. This provides state-of-the-art CCG parsing with a practical development environment.",
  "y": "motivation"
 },
 {
  "id": "07b062d569749924fa6ee1b2223411_5",
  "x": "Second, we provide a practical framework for developing discriminative models for CCG, reducing the memory requirements from over 20 GB to a few hundred MB. And third, given the significantly shorter training time compared to other discriminative parsing models (Taskar et al., 2004) , we provide a practical framework for investigating discriminative training methods more generally. 2 The CCG Parser<cite> Clark and Curran (2004b)</cite> describes the CCG parser. The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003) .",
  "y": "background"
 },
 {
  "id": "07b062d569749924fa6ee1b2223411_6",
  "x": "Hence the packed charts need to be stored in memory. In<cite> Clark and Curran (2004b)</cite> we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. The need for cluster computing resources presents a barrier to the development of further CCG parsing models.",
  "y": "motivation"
 },
 {
  "id": "07b062d569749924fa6ee1b2223411_7",
  "x": "Hence the packed charts need to be stored in memory. In<cite> Clark and Curran (2004b)</cite> we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. The need for cluster computing resources presents a barrier to the development of further CCG parsing models.",
  "y": "background"
 },
 {
  "id": "07b062d569749924fa6ee1b2223411_8",
  "x": "In this paper, Y is the set of possible CCG derivations and GEN(x) enumerates the set of derivations for sentence x. We use the same feature representation \u03a6(x, y) as in<cite> Clark and Curran (2004b)</cite> , to allow comparison with the log-linear model. The features are defined in terms of local subtrees in the derivation, consisting of a parent category plus one or two children. Some features are lexicalized, encoding word-word dependencies.",
  "y": "uses"
 },
 {
  "id": "07b062d569749924fa6ee1b2223411_9",
  "x": "In practice, this means that equivalent entries have the same span, and form the same structures and generate the same features in any further parsing of the sentence. Back pointers to the daughters indicate how an individual entry was created, so that any derivation can be recovered from the chart. A feature forest is essentially a packed chart with only the feature information retained (see Miyao and Tsujii (2002) and<cite> Clark and Curran (2004b)</cite> for the details).",
  "y": "background"
 },
 {
  "id": "07b062d569749924fa6ee1b2223411_10",
  "x": "A feature forest is essentially a packed chart with only the feature information retained (see Miyao and Tsujii (2002) and<cite> Clark and Curran (2004b)</cite> for the details). Dynamic programming algorithms can be used with the feature forests for efficient estimation. For the log-linear parsing model in<cite> Clark and Curran (2004b)</cite> , the inside-outside algorithm is used to calculate feature expectations, which are then used by the BFGS algorithm to optimise the likelihood function.",
  "y": "background"
 },
 {
  "id": "07b062d569749924fa6ee1b2223411_11",
  "x": "Then the parser was run on the supertagged sentences, using the CKY algorithm and the CCG combinatory rules. We applied the same normal-form restrictions used in<cite> Clark and Curran (2004b)</cite> : categories can only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints. This part of the process requires a few hundred MB of RAM to run the parser, and takes a few hours for Sections 2-21 of CCGbank.",
  "y": "uses"
 },
 {
  "id": "07b062d569749924fa6ee1b2223411_12",
  "x": "In<cite> Clark and Curran (2004b)</cite> we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM. The feature forest representation, and our implementation of it, is so compact that the perceptron training requires only 20 MB of RAM. Since the supertagger has already removed much of the practical parsing complexity, decoding one of the forests is extremely quick, and much of the training time is taken with continually reading the forests off disk.",
  "y": "differences"
 },
 {
  "id": "07b062d569749924fa6ee1b2223411_13",
  "x": "The F-scores are based only on the sentences for which there is an analysis. Following<cite> Clark and Curran (2004b)</cite> , accuracy is measured using F-score over the goldstandard predicate-argument dependencies in CCGbank. The table shows that the accuracy increases initially with the number of iterations, but converges quickly after only 4 iterations.",
  "y": "uses"
 },
 {
  "id": "0924035155d4bbac7768c65fbe8f9a_1",
  "x": "Since the shared task graphs used relations between nodes which were often not easily mappable to native OpenCCG relations, we trained a maxent classifier to tag the most likely relation, as well as an auxiliary maxent classifier to POS tag the graph nodes, much like hypertagging <cite>(Espinosa et al., 2008)</cite> . Training data for the classifier was extracted by comparing each relation between two nodes in the input shared task graph with the corresponding relation in the HLDS logical form. In case a labeled relation did not exist in the HLDS graph, a NoRel relation label was assigned.",
  "y": "similarities"
 },
 {
  "id": "09f627b9a70966dc7b63316c56a2a0_0",
  "x": "We present a simple algorithm to efficiently train language models with noise-contrastive estimation (NCE) on graphics processing units (GPUs). Our NCE-trained language models achieve significantly lower perplexity on the One Billion Word Benchmark language modeling challenge, and contain one sixth of the parameters in the best single model in<cite> Chelba et al. (2013)</cite> . When incorporated into a strong Arabic-English machine translation system they give a strong boost in translation quality.",
  "y": "differences"
 },
 {
  "id": "09f627b9a70966dc7b63316c56a2a0_1",
  "x": "In recent years, continuousspace language models such as feed-forward neural probabilistic language models (NPLMs) and recurrent neural network language models (RNNs) 1 * Equal contribution. 1 Henceforth we will use terms like \"RNN\" and \"LSTM\" with the understanding that we are referring to language models that use these formalisms have outperformed their count-based counterparts <cite>(Chelba et al., 2013</cite>; Zaremba et al., 2014; Mikolov, 2012) . RNNs are more powerful than n-gram language models, as they can exploit longer word contexts to predict words.",
  "y": "differences"
 },
 {
  "id": "09f627b9a70966dc7b63316c56a2a0_2",
  "x": "Using our new objective, we train large multi-layer LSTMs on the One Billion Word benchmark<cite> (Chelba et al., 2013)</cite> , with its full 780k word vocabulary. We achieve significantly lower perplexities with a single model, while using only a sixth of the parameters of a very strong baseline model<cite> (Chelba et al., 2013)</cite> . We release our toolkit 2 to allow researchers to train large-scale, large-vocabulary LSTMs with NCE.",
  "y": "uses"
 },
 {
  "id": "09f627b9a70966dc7b63316c56a2a0_3",
  "x": "Using our new objective, we train large multi-layer LSTMs on the One Billion Word benchmark<cite> (Chelba et al., 2013)</cite> , with its full 780k word vocabulary. We achieve significantly lower perplexities with a single model, while using only a sixth of the parameters of a very strong baseline model<cite> (Chelba et al., 2013)</cite> . We release our toolkit 2 to allow researchers to train large-scale, large-vocabulary LSTMs with NCE.",
  "y": "differences"
 },
 {
  "id": "09f627b9a70966dc7b63316c56a2a0_4",
  "x": "The contributions in this paper are the following: 2 www.github.com/isi-nlp/Zoph_RNN \u2022 Significantly improved perplexities (43.2) on the One Billion Word benchmark over<cite> Chelba et al. (2013)</cite> \u2022 Extrinsic machine translation improvement over a strong baseline.",
  "y": "extends differences"
 },
 {
  "id": "09f627b9a70966dc7b63316c56a2a0_5",
  "x": "We conducted two series of experiments to validate the efficiency of our approach and the quality of the models we learned using it: An intrinsic study of language model perplexity using the standard One Billion Word benchmark<cite> (Chelba et al., 2013)</cite> and an extrinsic end-to-end statistical machine translation task that uses an LSTM as one of several feature functions in re-ranking. Both experiments achieve excellent results. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "09f627b9a70966dc7b63316c56a2a0_6",
  "x": "---------------------------------- **LANGUAGE MODELING** For our language modeling experiment we use the One Billion Word benchmark proposed by<cite> Chelba et al. (2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "09f627b9a70966dc7b63316c56a2a0_7",
  "x": "Our perplexity results are shown in Table 1 , where we get significantly lower perplexities than the best single model from<cite> Chelba et al. (2013)</cite> , while having almost 6 times fewer parameters. We also compute the partition function values, log Z(u) , for our development set and we find that the mean is 0.058 and the variance is 0.139, indicating that training has encouraged self-normalization. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "09f627b9a70966dc7b63316c56a2a0_8",
  "x": "---------------------------------- **MODEL** Parameters Perplexity<cite> Chelba et al. (2013)</cite> 20m 51.3 NCE (ours) 3.4m 43.2 Recently, (J\u00f3zefowicz et al., 2016) achieved stateof-the-art language modeling perplexities (30.0) on the billion word dataset with a single model, using importance sampling to approximate the normalization constant, Z(u).",
  "y": "differences"
 },
 {
  "id": "0a538968f0cd121a1ef63b58a0c9f7_1",
  "x": "We follow the same data split of 1115 training and 19 test conversations as in the baseline approach (Stolcke et al., 2000;<cite> Kalchbrenner and Blunsom, 2013)</cite> . Table 3 shows the results of the proposed model with several setups, first without the context, then with one, two, and so on preceding utterances in the context. We examined different values for the number of the hidden units of the RNN, empirically 64 was identified as best and used throughout the experiments.",
  "y": "similarities uses"
 },
 {
  "id": "0a55859a36d0887ba4febc98762715_0",
  "x": "This paper proposes a new scalable and accurate neural dialogue state tracking model, based on the recently proposed Global-Local Self-Attention encoder (GLAD) model by <cite>Zhong et al. (2018)</cite> which uses global modules to share parameters between estimators for different types (called slots) of dialogue states, and uses local modules to learn slot-specific features. By using only one recurrent networks with global conditioning, compared to (1 + # slots) recurrent networks with global and local conditioning used in the GLAD model, our proposed model reduces the latency in training and inference times by 35% on average, while preserving performance of belief state tracking, by 97.38% on turn request and 88.51% on joint goal and accuracy. Evaluation on Multi-domain dataset (Multi-WoZ) also demonstrates that our model outperforms GLAD on turn inform and joint goal accuracy.",
  "y": "uses"
 },
 {
  "id": "0a55859a36d0887ba4febc98762715_1",
  "x": "Recently, <cite>Zhong et al. (2018)</cite> proposed a model based on training a binary classifier for each slot-value, Global-Locally Self Attentive encoder (GLAD, by employing recurrent and self attention for each utterance and previous system actions, and measuring similaity of these computed representation to each slot-value, which achieve state of the art results on WoZ and DSTC2 (Williams et al., 2013) datasets. Although the proposed neural based models achieves state of the art results on several benchmark, they are still inefficient for deployment in production system, due to their latency which stems from using recurrent networks. In this paper, we propose a new encoder, by improving GLAD architecture<cite> (Zhong et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "0a55859a36d0887ba4febc98762715_2",
  "x": "Then, the retrieved information is used as a conditionining input to the decoder, to generate the system response. Recently, <cite>Zhong et al. (2018)</cite> proposed a model based on training a binary classifier for each slot-value, Global-Locally Self Attentive encoder (GLAD, by employing recurrent and self attention for each utterance and previous system actions, and measuring similaity of these computed representation to each slot-value, which achieve state of the art results on WoZ and DSTC2 (Williams et al., 2013) datasets. Although the proposed neural based models achieves state of the art results on several benchmark, they are still inefficient for deployment in production system, due to their latency which stems from using recurrent networks.",
  "y": "motivation"
 },
 {
  "id": "0a55859a36d0887ba4febc98762715_3",
  "x": "Recently, <cite>Zhong et al. (2018)</cite> proposed a model based on training a binary classifier for each slot-value, Global-Locally Self Attentive encoder (GLAD, by employing recurrent and self attention for each utterance and previous system actions, and measuring similaity of these computed representation to each slot-value, which achieve state of the art results on WoZ and DSTC2 (Williams et al., 2013) datasets. Although the proposed neural based models achieves state of the art results on several benchmark, they are still inefficient for deployment in production system, due to their latency which stems from using recurrent networks. In this paper, we propose a new encoder, by improving GLAD architecture<cite> (Zhong et al., 2018)</cite> .",
  "y": "motivation uses"
 },
 {
  "id": "0a55859a36d0887ba4febc98762715_4",
  "x": "In this paper, we propose a new encoder, by improving GLAD architecture<cite> (Zhong et al., 2018)</cite> . The proposed encoder is based on removing slot-dependent recurrent network for utterance and system action encoder, and employing a global conditioning of aforementioned encoder on the slot type embedding vector. By removing the slot-dependent recurrent network, the proposed model is able to preserve the performance in predicting correct belief state, while improving computational complexity.",
  "y": "uses"
 },
 {
  "id": "0a55859a36d0887ba4febc98762715_5",
  "x": "First, section 2.1 explains the recently proposed GLAD encoder<cite> (Zhong et al., 2018)</cite> architecture, followed by our proposed encoder in section 2.2. ---------------------------------- **GLOBAL-LOCALLY SELF-ATTENTIVE MODEL**",
  "y": "uses background"
 },
 {
  "id": "0a55859a36d0887ba4febc98762715_6",
  "x": "**GLOBALLY-CONDITIONED ENCODER (GCE)** In this section, we describe the proposed globally-conditioned encoder (GCE) model. Here, we employ the similar approach of learning slot-specific temporal and context representation of user utterance and system actions, as proposed in GLAD<cite> (Zhong et al., 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "0a55859a36d0887ba4febc98762715_7",
  "x": "where U denotes the user utterance word embeddings, A j is the j-th previous system action, and V is the current slot value pair to be evaluated (e.g food=italian). Scoring Model: We follow the proposed architecture in GLAD<cite> (Zhong et al., 2018)</cite> for computing score of each slot-value pair, in the user utterance and previous system actions. To determine whether the user has mentioned a specific value of slot k, we compute the slot-kth conditioned scores for its values.",
  "y": "uses"
 },
 {
  "id": "0a55859a36d0887ba4febc98762715_8",
  "x": "The evaluation metric is based on joint goal and turn-level request and joint goal tracking accuracy. The joint goal is the accumulation of turn goals as described in <cite>Zhong et al. (2018)</cite> . The fixed pretrained GLoVe embedding (Pennington et al., 2014) with character-n gram embedding (Hashimoto et al., 2017) are used in embedding layer.",
  "y": "uses similarities"
 },
 {
  "id": "0a55859a36d0887ba4febc98762715_9",
  "x": "The evaluation metric is based on joint goal and turn-level request and joint goal tracking accuracy. The joint goal is the accumulation of turn goals as described in <cite>Zhong et al. (2018)</cite> . The fixed pretrained GLoVe embedding (Pennington et al., 2014) with character-n gram embedding (Hashimoto et al., 2017) are used in embedding layer.",
  "y": "uses"
 },
 {
  "id": "0a93feafef3ba2d4bb5360ff215171_0",
  "x": "have been proposed: One, multiple-choice, requires models to pick up the correct answer among a limited set of options; the other, open-ended, challenges systems to guess the correct answer from the whole vocabulary. Several metrics have been proposed recently for evaluating VQA systems (see section 2), but accuracy is still the most commonly used evaluation criterion [4, 11, 23, 42, 44, <cite>1</cite>, 5, 14, 45, 2] . In the multiple-choice setting, where only one answer is correct, accuracy is given by the proportion of correctly-predicted cases.",
  "y": "background"
 },
 {
  "id": "0a93feafef3ba2d4bb5360ff215171_1",
  "x": "**RELATED WORK** In recent years, a number of VQA datasets have been proposed: VQA 1.0 [4] , VQA-abstract [<cite>1</cite>] , VQA 2.0 [47, 14] , FM-IQA [13] , DAQUAR [24] , COCO-QA [30] , Visual Madlibs [46] , Visual Genome [20] , VizWiz [16] , Visual7W [48] , TDIUC [18] , CLEVR [17] , SHAPES [3] , Visual Reasoning [34] , Embodied QA [7] . What all these resources have in common is the task for which they were designed: Given an image (either real or abstract) and a question in natural language, models are asked to correctly answer the question. Depending on the characteristics of the dataset and the models proposed, various ways to evaluate performance have been explored.",
  "y": "background"
 },
 {
  "id": "0a93feafef3ba2d4bb5360ff215171_2",
  "x": "Being simple to compute and interpret, this metric (hence, VQA3+) is the standard evaluation criterion for open-ended VQA [4, <cite>1</cite>, 16, 47, 14] . However, it has some important limitations. (a) It ignores whether an answer that was chosen more than 3 annotators is the most frequent or not.",
  "y": "background"
 },
 {
  "id": "0a93feafef3ba2d4bb5360ff215171_3",
  "x": "Moreover, it only works with rigid semantic concepts, making it not suitable for phrasal or sentence answers that can be found in [4, <cite>1</cite>, 16, 47, 14] . Visual Turing Test has been proposed as a human-based evaluation metric for VQA by [13] . Based on the characteristics of the FM-IQA dataset, whose answers are often long and complex sentences, the authors tackled the task as an answer-generation rather than a classification problem (see also [49, 39, 40, 36, 37] ).",
  "y": "background"
 },
 {
  "id": "0a93feafef3ba2d4bb5360ff215171_4",
  "x": "This is crucial since, as shown in Figure 3 , in current datasets the proportion of samples with a perfect inter-annotator agreement (i.e., 1 unique answer) is relatively low: 35% in VQA 1.0 [4] , 33% in VQA 2.0 [14] , 43% in VQA-abstract [<cite>1</cite>] , and only 3% in VizWiz [16] . Moreover, we compute this score independently from the predictions of the models, thus providing a self-standing measure for the analysis of any VQA dataset. As clearly depicted in Figure 3 , subjectivity is indeed a property of the datasets: In VizWiz, only 30% of samples display 3 or less unique answers, whereas this percentage exceeds 70% in the other datasets.",
  "y": "background"
 },
 {
  "id": "0a93feafef3ba2d4bb5360ff215171_5",
  "x": "---------------------------------- **EXPERIMENTS** We tested the validity of our metric by experimenting with four VQA datasets: VQA 1.0 [4] , VQA 2.0 [14] , VQA-abstract [<cite>1</cite>] , and VizWiz [16] .",
  "y": "uses"
 },
 {
  "id": "0a93feafef3ba2d4bb5360ff215171_6",
  "x": "To enable a fair comparison across the datasets, for each dataset we followed the same pipeline: The standard VQA model used in [<cite>1</cite>] was trained on the training split and tested on the validation split. Model predictions were evaluated by means of three metrics: VQA3+ [4] (using the evaluation tools), WUPS [25] , and our MASSES. WUPS was tested in both its consensus versions, i.e. ACM and MCM with a threshold of 0.9.",
  "y": "uses"
 },
 {
  "id": "0ae49d1618e18eb794666543d924ed_0",
  "x": "We then assess the utility of directly adding simple features based on this CLM implementation to an existing NER system, and show that they have a significant positive impact on performance across many of the languages we tried. By adding very simple CLM-based features to the system, our scores approach those of a state-of-the-art NER system<cite> (Lample et al., 2016)</cite> across multiple languages, demonstrating both the unique importance and the broad utility of this approach. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "0ae49d1618e18eb794666543d924ed_1",
  "x": "We tuned LB, and report results with embedding size 150, and learning rate 0.1. Despite tuning the neural models, the simple N-gram model outperforms them significantly, perhaps because of the relatively small amount of training data. 4 We compare the CLM's Entity Identification against two state-of-the-art NER systems: CogCompNER (Khashabi et al., 2018) and LSTM-CRF<cite> (Lample et al., 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "0ae49d1618e18eb794666543d924ed_2",
  "x": "4 We compare the CLM's Entity Identification against two state-of-the-art NER systems: CogCompNER (Khashabi et al., 2018) and LSTM-CRF<cite> (Lample et al., 2016)</cite> . As Table 2 shows, the result of Ngram CLM, which yields the highest performance, is remarkably close to the result of state-of-theart NER systems (especially for English) given the simplicity of the model.",
  "y": "similarities"
 },
 {
  "id": "0ae49d1618e18eb794666543d924ed_3",
  "x": "Otherwise, we return False. Table 3 : NER results on 8 languages show that even a simplistic addition of CLM features to a standard NER model boosts performance. CogCompNER is run with standard features, including Brown clusters;<cite> (Lample et al., 2016)</cite> is run with default parameters and pre-trained embeddings.",
  "y": "uses"
 },
 {
  "id": "0ae49d1618e18eb794666543d924ed_4",
  "x": "For other languages, due to the limited training data, we only use the \"isEntity\" feature. We compare with the state-of-theart character-level neural NER system of<cite> (Lample et al., 2016)</cite> , which inherently encodes comparable information to CLMs, as a way to investigate how much of that system's performance can be attributed directly to name-internal structure. The results in Table 3 show that for six of the eight languages we studied, the baseline NER can be significantly improved by adding simple CLM features; for English and Arabic, it performs better even than the neural NER model of<cite> (Lample et al., 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "0ae49d1618e18eb794666543d924ed_5",
  "x": "For other languages, due to the limited training data, we only use the \"isEntity\" feature. We compare with the state-of-theart character-level neural NER system of<cite> (Lample et al., 2016)</cite> , which inherently encodes comparable information to CLMs, as a way to investigate how much of that system's performance can be attributed directly to name-internal structure. The results in Table 3 show that for six of the eight languages we studied, the baseline NER can be significantly improved by adding simple CLM features; for English and Arabic, it performs better even than the neural NER model of<cite> (Lample et al., 2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "0ae49d1618e18eb794666543d924ed_6",
  "x": "Our results demonstrate the power of CLMs for recognizing named entity tokens in a diverse range of languages, and that in many cases they can improve off-the-shelf NER system performance even when integrated in a simplistic way. However, the results from Section 4.2 show that this is not true for all languages, especially when only considering unseen entities in Test: Tagalog and Farsi do not follow the trend for the other languages we assessed even though CLM performs well for Named Entity Identification. While the end-to-end model developed by<cite> (Lample et al., 2016)</cite> clearly includes information comparable to that in the CLM, it requires a fully annotated NER corpus, takes significant time and computational resources to train, and is non-trivial to integrate into a new NER system.",
  "y": "motivation"
 },
 {
  "id": "0ae49d1618e18eb794666543d924ed_7",
  "x": "However, the results from Section 4.2 show that this is not true for all languages, especially when only considering unseen entities in Test: Tagalog and Farsi do not follow the trend for the other languages we assessed even though CLM performs well for Named Entity Identification. While the end-to-end model developed by<cite> (Lample et al., 2016)</cite> clearly includes information comparable to that in the CLM, it requires a fully annotated NER corpus, takes significant time and computational resources to train, and is non-trivial to integrate into a new NER system. The CLM approach captures a very large fraction of the entity/non-entity distinction capacity of full NER systems, and can be rapidly trained using only entity and non-entity token lists -i.e., it is corpus-agnostic.",
  "y": "differences motivation"
 },
 {
  "id": "0ae49d1618e18eb794666543d924ed_8",
  "x": "<cite>Lample et al. (2016)</cite> use character embeddings in an LSTM-CRF model. Their ablation studies show that character-level features improve performance significantly. We are not aware of any work that directly evaluates CLMs for identifying name tokens, nor of work that demonstrates the utility of characterlevel information for identifying names in multiple languages.",
  "y": "background"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_0",
  "x": "We present a replication study of BERT pretraining<cite> (Devlin et al., 2019)</cite> that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD.",
  "y": "uses"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_1",
  "x": "**INTRODUCTION** Self-training methods such as ELMo (Peters et al., 2018) , GPT (Radford et al., 2018) , BERT<cite> (Devlin et al., 2019)</cite> , XLM (Lample and Conneau, 2019) , and XLNet have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most. Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances.",
  "y": "background motivation"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_2",
  "x": "We present a replication study of BERT pretraining<cite> (Devlin et al., 2019)</cite> , which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. We find that BERT was significantly undertrained and propose an improved recipe for training BERT models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods. Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data.",
  "y": "uses"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_3",
  "x": "Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances. We present a replication study of BERT pretraining<cite> (Devlin et al., 2019)</cite> , which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. We find that BERT was significantly undertrained and propose an improved recipe for training BERT models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods.",
  "y": "extends"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_4",
  "x": "In this section, we give a brief overview of the BERT<cite> (Devlin et al., 2019)</cite> pretraining approach and some of the training choices that we will examine experimentally in the following section. ---------------------------------- **SETUP**",
  "y": "uses background"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_5",
  "x": "Unlike<cite> Devlin et al. (2019)</cite> , we do not randomly inject short sequences, and we do not train with a reduced sequence length for the first 90% of updates. We train only with full-length sequences. We train with mixed precision floating point arithmetic on DGX-1 machines, each with 8 \u00d7 32GB Nvidia V100 GPUs interconnected by Infiniband (Micikevicius et al., 2018) .",
  "y": "differences"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_6",
  "x": "The GLUE organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held-out test data. For the replication study in Section 4, we report results on the development sets after finetuning the pretrained models on the corresponding singletask training data (i.e., without multi-task training or ensembling). Our finetuning procedure follows the original BERT paper<cite> (Devlin et al., 2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_7",
  "x": "V2.0 some questions are not answered in the provided context, making the task more challenging. For SQuAD V1.1 we adopt the same span prediction method as BERT<cite> (Devlin et al., 2019)</cite> . For SQuAD V2.0, we add an additional binary classifier to predict whether the question is answerable, which we train jointly by summing the classification and span loss terms.",
  "y": "uses"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_8",
  "x": "This becomes crucial when pretraining for more steps or with larger datasets. Results Table 1 compares the published BERT BASE results from<cite> Devlin et al. (2019)</cite> to our reimplementation with either static or dynamic masking. We find that our reimplementation with static masking performs similar to the original BERT model, and dynamic masking is comparable or slightly better than static masking.",
  "y": "uses"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_9",
  "x": "This becomes crucial when pretraining for more steps or with larger datasets. Results Table 1 compares the published BERT BASE results from<cite> Devlin et al. (2019)</cite> to our reimplementation with either static or dynamic masking. We find that our reimplementation with static masking performs similar to the original BERT model, and dynamic masking is comparable or slightly better than static masking.",
  "y": "uses similarities"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_10",
  "x": "To better understand this discrepancy, we compare several alternative training formats: \u2022 SEGMENT-PAIR+NSP: This follows the original input format used in BERT<cite> (Devlin et al., 2019)</cite> , with the NSP loss. Each input has a pair of segments, which can each contain multiple natural sentences, but the total combined length must be less than 512 tokens.",
  "y": "uses"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_11",
  "x": "We remove the NSP loss. Results Table 2 shows results for the four different settings. We first compare the original SEGMENT-PAIR input format from<cite> Devlin et al. (2019)</cite> to the SENTENCE-PAIR format; both formats retain the NSP loss, but the latter uses single sentences.",
  "y": "uses"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_12",
  "x": "We find that using individual sentences hurts performance on downstream tasks, which we hypothesize is because the model is not able to learn long-range dependencies. We next compare training without the NSP loss and training with blocks of text from a single document (DOC-SENTENCES). We find that this setting outperforms the originally published BERT BASE results and that removing the NSP loss matches or slightly improves downstream task performance, in contrast to<cite> Devlin et al. (2019)</cite> .",
  "y": "differences"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_13",
  "x": "The original BERT implementation<cite> (Devlin et al., 2019)</cite> uses a character-level BPE vocabulary of size 30K, which is learned after preprocessing the input with heuristic tokenization rules. Following Radford et al. (2019) , we instead consider training BERT with a larger byte-level BPE vocabulary containing 50K subword units, without any additional preprocessing or tokenization of the input. This adds approximately 15M and 20M additional parameters for BERT BASE and BERT LARGE , respectively.",
  "y": "background"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_14",
  "x": "This functionality is supported natively in FAIRSEQ (Ott et al., 2019) . The original BERT implementation<cite> (Devlin et al., 2019)</cite> uses a character-level BPE vocabulary of size 30K, which is learned after preprocessing the input with heuristic tokenization rules. Following Radford et al. (2019) , we instead consider training BERT with a larger byte-level BPE vocabulary containing 50K subword units, without any additional preprocessing or tokenization of the input.",
  "y": "differences background"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_15",
  "x": "Specifically, RoBERTa is trained with dynamic masking (Section 4.1), FULL-SENTENCES without NSP loss (Section 4.2), large mini-batches (Section 4.3) and a larger byte-level BPE (Section 4.4). Additionally, we investigate two other important factors that have been under-emphasized in previous work: (1) the data used for pretraining, and (2) the number of training passes through the data. For example, the recently proposed XLNet architecture ) is pretrained using nearly 10 times more data than the original BERT<cite> (Devlin et al., 2019)</cite> .",
  "y": "differences"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_16",
  "x": "We pretrain for 100K steps over a comparable BOOK-CORPUS plus WIKIPEDIA dataset as was used in<cite> Devlin et al. (2019)</cite> . We pretrain our model using 1024 V100 GPUs for approximately one day. Results We present our results in Table 4 .",
  "y": "similarities"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_17",
  "x": "QNLI: Recent submissions on the GLUE leaderboard adopt a pairwise ranking formulation for the QNLI task, in which candidate answers are mined from the training set and compared to one another, and a single (question, candidate) pair is classified as positive (Liu et al., 2019b,a; Yang et al., 2019) . This formulation significantly simplifies the task, but is not directly comparable to BERT<cite> (Devlin et al., 2019)</cite> . Following recent work, we adopt the ranking approach for our test submission, but for direct comparison with BERT we report development set results based on a pure classification approach.",
  "y": "differences"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_18",
  "x": "In particular, while both BERT<cite> (Devlin et al., 2019)</cite> and XLNet augment their training data with additional QA datasets, we only finetune RoBERTa using the provided SQuAD training data. Yang et al. (2019) also employed a custom layer-wise learning rate schedule to finetune results could potentially be improved by augmenting this with additional pronoun disambiguation datasets. XLNet, while we use the same learning rate for all layers.",
  "y": "differences"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_19",
  "x": "Yang et al. (2019) also employed a custom layer-wise learning rate schedule to finetune results could potentially be improved by augmenting this with additional pronoun disambiguation datasets. XLNet, while we use the same learning rate for all layers. For SQuAD v1.1 we follow the same finetuning procedure as<cite> Devlin et al. (2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_20",
  "x": "We also submit RoBERTa to the public SQuAD 2.0 leaderboard and evaluate its performance relative to other systems. Most of the top systems build upon either BERT<cite> (Devlin et al., 2019)</cite> or XLNet , both of which rely on additional external training data. In contrast, our submission does not use any additional data.",
  "y": "background"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_21",
  "x": "Most of the top systems build upon either BERT<cite> (Devlin et al., 2019)</cite> or XLNet , both of which rely on additional external training data. In contrast, our submission does not use any additional data. Our single RoBERTa model outperforms all but one of the single model submissions, and is the top scoring system among those that do not rely on data augmentation.",
  "y": "differences"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_22",
  "x": "---------------------------------- **RELATED WORK** Pretraining methods have been designed with different training objectives, including language modeling (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018) , machine translation (McCann et al., 2017) , and masked language modeling <cite>(Devlin et al., 2019</cite>; Lample and Conneau, 2019) .",
  "y": "background"
 },
 {
  "id": "0af8cacc0f85bb557e1943e32450e2_23",
  "x": "Many recent papers have used a basic recipe of finetuning models for each end task (Howard and Ruder, 2018; Radford et al., 2018) , and pretraining with some variant of a masked language model objective. However, newer methods have improved performance by multi-task fine tuning (Dong et al., 2019) , incorporating entity embeddings (Sun et al., 2019) , span prediction (Joshi et al., 2019) , and multiple variants of autoregressive pretraining Chan et al., 2019; Yang et al., 2019) . Performance is also typically improved by training bigger models on more data <cite>(Devlin et al., 2019</cite>; Yang et al., 2019; Radford et al., 2019) .",
  "y": "background"
 },
 {
  "id": "0b2e3651610aba4bd7150eee50797f_0",
  "x": "It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010) . Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010) , or of high computational complexity (Chung and Gildea 2009;<cite> Duan et al., 2010)</cite> .",
  "y": "background"
 },
 {
  "id": "0b2e3651610aba4bd7150eee50797f_1",
  "x": "Another advantage of character alignment is the reduction in alignment errors caused by word seg- Table 2 #UT and #UTP in CCorpus and WCorpus mentation errors. For example, \"\u5207\u5c3c (Cheney)\" and \"\u613f (will)\" are wrongly merged into one word \u5207 \u5c3c \u613f by the word segmenter, and \u5207 \u5c3c \u613f wrongly aligns to a comma in English sentence in the word alignment; However, both \u5207 and \u5c3c align to \"Cheney\" correctly in the character alignment. However, this kind of errors cannot be fixed by methods which learn new words by packing already segmented words, such as word packing (Ma et al., 2007) and Pseudo-word <cite>(Duan et al., 2010)</cite> .",
  "y": "background"
 },
 {
  "id": "0b2e3651610aba4bd7150eee50797f_2",
  "x": "We see that our proposed systems outperformed WordSys in all segmentation specifications settings. Table 5 lists the results of CharSys in small-scale task. In this setting, we gradually set the phrase length and the distortion limits of the phrase-based decoder (context size) to 7, 9, 11 and 13, in order to remove the disadvantage of shorter context size of using character as WSR for fair comparison with WordSys as suggested by<cite> Duan et al. (2010)</cite> .",
  "y": "uses"
 },
 {
  "id": "0b334057bc358f5537497ed15344c1_1",
  "x": "NER research presented here involves development of new statistical and hybrid approaches to identification and disambiguation of gene [1] , protein [2] , chemical names [3] , and clinical entities. Overwhelmingly, researchers chose statistical or hybrid approaches to the tasks at hand. This is probably the reason for growing interest in creation of annotated corpora [4] , development of methods for augmenting the existing annotation [5] , speeding up the annotation process [5] , and reducing its cost; evaluating the comparability of results obtained applying the same methods to different collections<cite> [6]</cite> , And increasing compatibility of different annotations [7] .",
  "y": "background"
 },
 {
  "id": "0b334057bc358f5537497ed15344c1_2",
  "x": "Overwhelmingly, researchers chose statistical or hybrid approaches to the tasks at hand. This is probably the reason for growing interest in creation of annotated corpora [4] , development of methods for augmenting the existing annotation [5] , speeding up the annotation process [5] , and reducing its cost; evaluating the comparability of results obtained applying the same methods to different collections<cite> [6]</cite> , And increasing compatibility of different annotations [7] . Increasingly sophisticated relation extraction methods <cite>[6,</cite> 8] are being applied to a broader set of iii relations [9] .",
  "y": "background"
 },
 {
  "id": "0c233d68fb2ccdf033fc6a08c8f4bf_0",
  "x": "The goal of the <cite>Penn Discourse Treebank (PDTB)</cite> project is to develop a large-scale corpus, annotated with coherence relations marked by discourse connectives. Currently, the primary application of the <cite>PDTB</cite> annotation has been to news articles. In this study, we tested whether the <cite>PDTB</cite> guidelines can be adapted to a different genre.",
  "y": "background"
 },
 {
  "id": "0c233d68fb2ccdf033fc6a08c8f4bf_1",
  "x": "Currently, the primary application of the <cite>PDTB</cite> annotation has been to news articles. In this study, we tested whether the <cite>PDTB</cite> guidelines can be adapted to a different genre. We annotated discourse connectives and <cite>their</cite> arguments in one 4,937-token full-text biomedical article.",
  "y": "motivation"
 },
 {
  "id": "0c233d68fb2ccdf033fc6a08c8f4bf_2",
  "x": "Currently, the primary application of the <cite>PDTB</cite> annotation has been to news articles. In this study, we tested whether the <cite>PDTB</cite> guidelines can be adapted to a different genre. We annotated discourse connectives and <cite>their</cite> arguments in one 4,937-token full-text biomedical article.",
  "y": "uses"
 },
 {
  "id": "0c233d68fb2ccdf033fc6a08c8f4bf_3",
  "x": "Thus our experiments suggest that the <cite>PDTB</cite> annotation can be adapted to new domains by minimally adjusting the guidelines and by adding some further domain-specific linguistic cues. ---------------------------------- **INTRODUCTION**",
  "y": "extends"
 },
 {
  "id": "0c233d68fb2ccdf033fc6a08c8f4bf_4",
  "x": "Large scale annotated corpora, e.g., the Penn TreeBank (PTB) project (Marcus et al. 1993) , have played an important role in text-mining. The <cite>Penn Discourse Treebank (PDTB)</cite> (http://www.seas.upenn.edu/~pdtb) (<cite>Prasad et al. 2008a</cite>) annotates the argument structure, semantics, and attribution of discourse connectives and their arguments. The current release of PDTB-2.0 contains the annotations of 1,808 Wall Street Journal articles (~1 million words) from the Penn TreeBank (Marcus et al. 1993 ) II distribution and a total of 40,600 discourse connective tokens (Prasad et al. 2008b ).",
  "y": "background"
 },
 {
  "id": "0c233d68fb2ccdf033fc6a08c8f4bf_5",
  "x": "The <cite>Penn Discourse Treebank (PDTB)</cite> (http://www.seas.upenn.edu/~pdtb) (<cite>Prasad et al. 2008a</cite>) annotates the argument structure, semantics, and attribution of discourse connectives and their arguments. The current release of PDTB-2.0 contains the annotations of 1,808 Wall Street Journal articles (~1 million words) from the Penn TreeBank (Marcus et al. 1993 ) II distribution and a total of 40,600 discourse connective tokens (Prasad et al. 2008b ). This work examines whether the <cite>PDTB</cite> annotation guidelines can be adapted to a different genre, the biomedical literature.",
  "y": "motivation"
 },
 {
  "id": "0c233d68fb2ccdf033fc6a08c8f4bf_6",
  "x": "---------------------------------- **A PILOT ANNOTATION** Following the <cite>PDTB</cite> annotation manual (Prasad et al. 2008b ), we conducted a pilot annotation of discourse connectivity in biomedical text.",
  "y": "uses"
 },
 {
  "id": "0c233d68fb2ccdf033fc6a08c8f4bf_7",
  "x": "Two linguist annotators independently annotated one full-text biomedical article (Verpy et al. 1999 ) that we randomly selected. The article is 4,937 tokens long. When the annotation work was completed, we measured the inter-annotator agreement, following the <cite>PDTB</cite> exact match criterion (Miltsakaki et al. 2004 ).",
  "y": "uses"
 },
 {
  "id": "0c233d68fb2ccdf033fc6a08c8f4bf_8",
  "x": "In addition, we also measured the agreement in the components (i.e., discourse connectives and the arguments). We discussed the annotation results and made suggestions to adapt the <cite>PDTB</cite> guidelines to biomedical text. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "0c233d68fb2ccdf033fc6a08c8f4bf_9",
  "x": "The overall agreement for the 68 discourse relations is 45.6% for exact match, 45.6% for Arg1, and 79.4% for Arg2. The <cite>PDTB</cite> also reported a higher level of agreement in annotating Arg2 than in annotating Arg1 (Miltsakaki et al. 2004) . We manually analyzed the cases with disagreement.",
  "y": "background"
 },
 {
  "id": "0c233d68fb2ccdf033fc6a08c8f4bf_10",
  "x": "The overall agreement for the 68 discourse relations is 45.6% for exact match, 45.6% for Arg1, and 79.4% for Arg2. The <cite>PDTB</cite> also reported a higher level of agreement in annotating Arg2 than in annotating Arg1 (Miltsakaki et al. 2004) . We manually analyzed the cases with disagreement.",
  "y": "differences"
 },
 {
  "id": "0c233d68fb2ccdf033fc6a08c8f4bf_11",
  "x": "**NEW CONVENTIONS** After the completion of the pilot annotation and the discussion, we decided to add the following conventions to the <cite>PDTB</cite> annotation guidelines to address the characteristics of biomedical text: i. Citation references are to be annotated as a part of an argument because the inclusion will benefit many text-mining tasks including identifying the semantic relations among citations.",
  "y": "extends"
 },
 {
  "id": "0c233d68fb2ccdf033fc6a08c8f4bf_12",
  "x": "Clausal supplements (e.g., relative or parenthetical constructions) that modify arguments but are not minimally necessary for the interpretation of the relation, are annotated as part of the arguments. iii. We will annotate a wider variety of nominalizations as arguments than allowed by the <cite>PDTB</cite> guidelines.",
  "y": "extends"
 },
 {
  "id": "0c3f9588b6f587d04c286384ca24e0_0",
  "x": "In this paper we aim to improve the state-of-the-art for the task of learning a TAG supertagger from an annotated treebank <cite>(Kasai et al., 2018)</cite> . We observe that supertag prediction does not take full advantage of the complex structural information contained within each supertag. Neural models have been used to learn embeddings over these supertags and thereby share weights among similar supertags.",
  "y": "uses"
 },
 {
  "id": "0c3f9588b6f587d04c286384ca24e0_1",
  "x": "Our approach is also distinct in that we take advantage of the structure of the supertags by deconstructing the tree structure implicit in each supertag. Our experimental results show that our novel multi-task learning framework leads to a new state-of-the-art accuracy score of 91.39% for TAG supertagging on the Penn Treebank dataset (Marcus et al., 1993; Chen et al., 2006) which is a significant improvement over the previous multi-task result for supertagging that combines supertagging with graph-based parsing <cite>(Kasai et al., 2018)</cite> . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "0c3f9588b6f587d04c286384ca24e0_2",
  "x": "For all these reasons, in this paper we focus on the supertagging task. TAG and CCG can be parsed using graph-parsing methods in O(n 3 ) but the complexity of unrestricted parsing for both formalisms is O(n 6 ) which is prohibitive on real-world data. Neural linear-time transition based parsers are still not accurate enough to compete with the state-of-the-art supertagging models or parsers that use supertagging as the initial step (Chung et al., 2016;<cite> Kasai et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "0c3f9588b6f587d04c286384ca24e0_3",
  "x": "**BASELINE SUPERTAGGING MODEL** For our baseline supertagging model we use the state-of-the-art model that currently has the highest accuracy on the Penn treebank dataset <cite>(Kasai et al., 2018)</cite> . For the supertagging model the main contribution of<cite> Kasai et al. (2018)</cite> was two-fold: the first was to add a character CNN for modeling word embeddings using subword features, and the second was to add highway connections to add more layers to a standard bidirectional LSTM.",
  "y": "uses"
 },
 {
  "id": "0c3f9588b6f587d04c286384ca24e0_4",
  "x": "Another extension to the standard sequence prediction model in<cite> Kasai et al. (2018)</cite> was to combine supertagging with graph-based parsing. In this paper, we focus on the supertagging model and compare only on supertagging accuracy. The neural model for supertagging that we use as a baseline uses graph-based parsing as an auxiliary task and has the current highest accuracy score on the Penn treebank (90.81%).",
  "y": "extends"
 },
 {
  "id": "0c3f9588b6f587d04c286384ca24e0_5",
  "x": "<cite>(Kasai et al., 2018)</cite> we use two components in the word embedding: \u2022 a 30-dimensional character level embedding vector computed using a char-CNN which captures the morphological information (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016;<cite> Kasai et al., 2018)</cite> . Each character is encoded as a 30-dimensional vector, and then we apply 30 convolutional filters with a window size of 5.",
  "y": "uses"
 },
 {
  "id": "0c3f9588b6f587d04c286384ca24e0_6",
  "x": "For words that do not appear in GloVe, we randomly initialized the word embedding. A start of sentence token and an end of sentence token is added into the beginning and ending position of each sentence, but is not included in the computation of loss and accuracy. Unlike <cite>(Kasai et al., 2018)</cite> we do not use predicted part of speech (POS) tags as part of the input sequence.",
  "y": "differences"
 },
 {
  "id": "0c3f9588b6f587d04c286384ca24e0_7",
  "x": "The core of this base model is a bidirectional recurrent neural network, in particular a Long Short-Term Memory neural network (Graves and Schmidhuber, 2005) . For the hyperparameters, we use the settings in<cite> Kasai et al. (2018)</cite> in order to ensure a fair comparison. Unlike <cite>(Kasai et al., 2018)</cite> we do not use highway connections in our model.",
  "y": "uses"
 },
 {
  "id": "0c3f9588b6f587d04c286384ca24e0_8",
  "x": "Unlike <cite>(Kasai et al., 2018)</cite> we do not use highway connections in our model. We did experiment with the addition of highway connections but we found no improvement in accuracy over the baseline BiLSTM-only model with a significant increase in training time. The bidirectional representation has 1024 units, a combination of the 512 forward and backward units each.",
  "y": "differences"
 },
 {
  "id": "0c3f9588b6f587d04c286384ca24e0_9",
  "x": "In our case, because we re-use the same training set for multi-task learning, we have made sure our experimental settings exactly match the previous best state-of-the-art method for supertagging <cite>(Kasai et al., 2018)</cite> and we use the same pre-trained word embeddings to ensure a fair comparison. We train six different neural sequence prediction models independently on the supertagging task, root node prediction (ROOT), head node prediction (HEAD), tree type prediction (TYPE), tree sketch prediction (SKETCH) and tree spine prediction (SPINE) tasks. For each task, we use the state-of-the-art baseline supertagging model as defined in Section 3.",
  "y": "uses"
 },
 {
  "id": "0c3f9588b6f587d04c286384ca24e0_10",
  "x": "We use the dataset that has been widely used by previous work in supertagging and TAG parsing (Bangalore et al., 2009; Chung et al., 2016; Friedman et al., 2017;<cite> Kasai et al., , 2018</cite> . We use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al. (2006) . As in previous work, we use Sections 01-22 as the training set, Section 00 as the dev set, and Section 23 as the test set.",
  "y": "uses"
 },
 {
  "id": "0c3f9588b6f587d04c286384ca24e0_12",
  "x": "All of those words are<cite> Kasai et al. (2018)</cite> refers to highway connections, and POS refers to the use of predicted part-of-speech tags as inputs. We do not use HW or POS in our models as they do not provide any benefit. correctly predicted by the multi-task model.",
  "y": "differences"
 },
 {
  "id": "0c3f9588b6f587d04c286384ca24e0_13",
  "x": "Neural network based supertagging models in TAG <cite>(Kasai et al., 2018)</cite> and CCG (Xu Lewis et al., 2016; Xu, 2016; Vaswani et al., 2016) have shown substantial improvement in performance, but the supertagging models are all quite similar as they all use a bi-directional RNN feeding into a prediction layer. Structural features of supertags are heavily used in pre-neural statistical parsing methods (Bangalore et al., 2009 ) and proved to be useful. The use of supertag structure was explored in (Friedman et al., 2017) where they adopt grammar features into a tree-structured neural model over the supertags but this model was unable to beat the state-of-the-art.",
  "y": "background"
 },
 {
  "id": "0c3f9588b6f587d04c286384ca24e0_14",
  "x": "<cite>(Kasai et al., 2018)</cite> combines supertagging with parsing which does provide state-of-the-art accuracy but at the expense of computational complexity. extends the BiLSTM model with predicted part-of-speech tags and suffix embeddings as inputs, then<cite> Kasai et al. (2018)</cite> further extends the BiLSTM model with highway connection as well as character CNN as input, and jointly train the supertagging model with parsing model and this work had the state-of-the-art accuracy before our paper on the Penn treebank dataset. Friedman et al. (2017) investigated a recursive treebased vector representation of TAG supertags, but while their model can learn useful facts about supertags, about how one can be related to another, there was no performance improvement as a result of their model on the supertagging task.",
  "y": "background"
 },
 {
  "id": "0c3f9588b6f587d04c286384ca24e0_15",
  "x": "<cite>(Kasai et al., 2018)</cite> combines supertagging with parsing which does provide state-of-the-art accuracy but at the expense of computational complexity. extends the BiLSTM model with predicted part-of-speech tags and suffix embeddings as inputs, then<cite> Kasai et al. (2018)</cite> further extends the BiLSTM model with highway connection as well as character CNN as input, and jointly train the supertagging model with parsing model and this work had the state-of-the-art accuracy before our paper on the Penn treebank dataset. Friedman et al. (2017) investigated a recursive treebased vector representation of TAG supertags, but while their model can learn useful facts about supertags, about how one can be related to another, there was no performance improvement as a result of their model on the supertagging task.",
  "y": "background"
 },
 {
  "id": "0cc576e90c5ee2af043e09234792f5_0",
  "x": "It is therefore far from obvious that there is a simple linear or monotonic relationship between the distribution of the association scores (ASs) in a text and its quality. Finally, it would be interesting to determine whether using ASs extracted from a corpus of native texts enables a better prediction than that obtained by using the simple frequency of the unigrams and bigrams<cite> (Yannakoudakis et al., 2011)</cite> . This study attempts to answer these questions by extracting from the bigrams in EFL texts richer features from several association measures as described in Section 2, and by comparing the effectiveness of these collocational features to that of lexical features (Section 3).",
  "y": "future_work"
 },
 {
  "id": "0cc576e90c5ee2af043e09234792f5_1",
  "x": "Dataset: The analyses were conducted on the First Certificate in English (FCE) ESOL examination scripts described in <cite>Yannakoudakis et al. (2011</cite> Yannakoudakis et al. ( , 2012 . Extracted from the Cambridge Learner Corpus, this dataset consists of 1238 texts of between 200 and 400 words, to which an overall mark has been assigned. As in<cite> Yannakoudakis et al. (2011)</cite> , the 1141 texts from the year 2000 were used for training, while the 97 texts from the year 2001 were used for testing.",
  "y": "similarities uses"
 },
 {
  "id": "0cc576e90c5ee2af043e09234792f5_2",
  "x": "As in<cite> Yannakoudakis et al. (2011)</cite> , the 1141 texts from the year 2000 were used for training, while the 97 texts from the year 2001 were used for testing. Collocational Features: The global statistical features in Somasundaran et al. (2015) and were used: the mean, the median, the maximum and the minimum of the ASs, and the proportion of bigrams that are present in the learner text but absent from the reference corpus. Because the best number of bins for discretizing the distributions was not known, the following ones were compared: 3, 5, 8, 10, 15, 20, 25, 33, 50, 75 and 100. To get all these features, each learner text was tokenized and POS-tagged by means of CLAWS7 2 and all bigrams were extracted.",
  "y": "similarities"
 },
 {
  "id": "0cc576e90c5ee2af043e09234792f5_3",
  "x": "Each bigram was then looked up in the 100 million word British National Corpus (BNC 3 ) and, if found, assigned its ASs. The collocational features were then computed on the basis of all the different bigrams present in each text (types) to give more weight to their diversity (Durrant and Schmitt, 2009 ). Lexical Features: As a benchmark for comparison, the lexical features that were showed to be good predictors of the quality of the texts in this dataset<cite> (Yannakoudakis et al., 2011)</cite> were chosen.",
  "y": "similarities uses"
 },
 {
  "id": "0cc576e90c5ee2af043e09234792f5_4",
  "x": "They consist of the frequency of the word unigrams and bigrams. This baseline is particularly relevant because it includes the lexical bigrams that are the basis of the collocational features. These features were extracted as described in<cite> Yannakoudakis et al. (2011)</cite> ; the only difference is that they used the RASP tagger and not the CLAWS tagger.",
  "y": "extends differences"
 },
 {
  "id": "0cc576e90c5ee2af043e09234792f5_5",
  "x": "These features were extracted as described in<cite> Yannakoudakis et al. (2011)</cite> ; the only difference is that they used the RASP tagger and not the CLAWS tagger. Supervised Learning Approach and Evaluation: As in<cite> Yannakoudakis et al. (2011)</cite> , the automated scoring task was treated as a rankpreference learning problem by means of the SVM-Rank package (Joachims, 2006) , which is a much faster version of the SVM-Light package used by<cite> Yannakoudakis et al. (2011)</cite> . The procedure was identical to that described in their study.",
  "y": "extends differences"
 },
 {
  "id": "0cc576e90c5ee2af043e09234792f5_6",
  "x": "Since the quality ratings are distributed on a zero to 40 scale, I chose Pearson's correlation coefficient, also used by<cite> Yannakoudakis et al. (2011)</cite> , as the measure of performance. ---------------------------------- **RESULTS**",
  "y": "similarities uses"
 },
 {
  "id": "0cc576e90c5ee2af043e09234792f5_7",
  "x": "If MI is always one of the best performing ASs, the differences between the ASs are quite low. For all numbers of bins, using all the ASs allows the best performance. To get an idea of how well the collocational and lexical features perform, the correlations in Table 2 can be compared to the average correlation between the Examiners' scores reported by<cite> Yannakoudakis et al. (2011)</cite> , which give an upper bound of 0.80 while the All models with more than three bins obtain a correlation of at least 0.75.",
  "y": "similarities"
 },
 {
  "id": "0d06c8509ebbdc61985bebcdb26e6c_0",
  "x": "In a similar work, Mnih et al. <cite>[13]</cite> proposed to use Noise Contrastive Estimation (NCE) [14] to speed-up the training. NCE treats the learning as a binary classification problem between a target word and noise samples, which are drawn from a noise distribution. Moreover, NCE considers the normalization term as an additional parameter that can be learned during training or fixed beforehand.",
  "y": "background"
 },
 {
  "id": "0d06c8509ebbdc61985bebcdb26e6c_1",
  "x": "Hence, an adaptive IS may use a large number of samples to solve this problem whereas NCE is more stable and requires a fixed small number of noise samples (e.g., 100) to achieve a good performance<cite> [13,</cite> 16] . Furthermore, the network learns to self-normalize during training using NCE. As a results, and on the contrary to IS, the softmax is no longer required during evaluation, which makes NCE an attractive choice to train large vocabulary NNLM.",
  "y": "background"
 },
 {
  "id": "0d06c8509ebbdc61985bebcdb26e6c_2",
  "x": "This approach does not require any sampling and can be formulated using dense matrix operations. Furthermore, we can show that this solution optimally approximates the sampling from a unigram distribution, which has been shown to be a good noise distribution choice<cite> [13,</cite> 16] . The main idea here is to restrict the vocabulary, at each forward-backward pass, to the target words in the batch (words to predict) and then replace the softmax function by NCE.",
  "y": "background"
 },
 {
  "id": "0d06c8509ebbdc61985bebcdb26e6c_3",
  "x": "Hence, we solely focus our experiments on NCE as a major approach to achieve this goal [17,<cite> 13,</cite> 16] in comparison to the reference full softmax function. Comparison to other training approaches such as importance sampling will be conducted in future work. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "0d06c8509ebbdc61985bebcdb26e6c_4",
  "x": "Moreover, B-NCE and S-NCE use the unigram as noise distribution pn. Following the setup proposed in<cite> [13,</cite> 16] , S-NCE uses K = 100 noise samples, whereas B-NCE uses only the target words in the batch (K=0). Note that S-NCE will process and update B + K words at its output layer during each forward-backward pass, whereas B-NCE updates only B words.",
  "y": "uses"
 },
 {
  "id": "0d1fb27d847ca44af36862cf78744e_0",
  "x": "Still, from a theoretical point of view, projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal. There exist a few robust broad-coverage parsers that produce non-projective dependency structures, notably Tapanainen and J\u00e4rvinen (1997) and Wang and Harper (2004) for English, Foth et al. (2004) for German, and Holan (2004) for Czech. In addition, there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington, 1990;<cite> Kahane et al., 1998</cite>; Duchier and Debusmann, 2001; Holan et al., 2001; Hellwig, 2003) .",
  "y": "background"
 },
 {
  "id": "0d1fb27d847ca44af36862cf78744e_1",
  "x": "Finally, since non-projective constructions often involve long-distance dependencies, the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill et al., 2004; Levy and Manning, 2004; Campbell, 2004) . In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques. First, the training data for the parser is projectivized by applying a minimal number of lifting operations<cite> (Kahane et al., 1998)</cite> and encoding information about these lifts in arc labels.",
  "y": "background"
 },
 {
  "id": "0d1fb27d847ca44af36862cf78744e_3",
  "x": "An arc w i \u2192 w k is projective iff, for every word w j occurring between w i and w k in the string The arc connecting the head jedna (one) to the dependent Z (out-of) spans the token je (is), which is not dominated by jedna. As observed by <cite>Kahane et al. (1998)</cite> , any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc w j \u2192 w k by a projective arc w i \u2192 w k such that w i \u2192 * w j holds in the original graph.",
  "y": "background"
 },
 {
  "id": "0d1fb27d847ca44af36862cf78744e_4",
  "x": "The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right). Applying the function PROJECTIVIZE to the graph in Figure 1 yields the graph in Figure 2 , where the problematic arc pointing to Z has been lifted from the original head jedna to the ancestor je. Using the terminology of <cite>Kahane et al. (1998)</cite> , we say that jedna is the syntactic head of Z, while je is its linear head in the projectivized representation.",
  "y": "uses"
 },
 {
  "id": "0d1fb27d847ca44af36862cf78744e_5",
  "x": "Using the terminology of <cite>Kahane et al. (1998)</cite> , we say that jedna is the syntactic head of Z, while je is its linear head in the projectivized representation. Unlike <cite>Kahane et al. (1998)</cite> , we do not regard a projectivized representation as the final target of the parsing process. Instead, we want to apply an in-Lifted arc label Path labels Number of labels Baseline d p n Head d\u2191h p n(n + 1) Head+Path d\u2191h p\u2193 2n(n + 1) Path d\u2191 p\u2193 4n Table 1 : Encoding schemes (d = dependent, h = syntactic head, p = path; n = number of dependency types) verse transformation to recover the underlying (nonprojective) dependency graph.",
  "y": "differences"
 },
 {
  "id": "0d798fcdee6ee5722d6dc5638210c2_0",
  "x": "The agent must navigate through the environment, conditioning on the instruction as well as the visual imagery that it observes along the route, to stop at the location specified by the instruction (e.g. the mirror). Recent state-of-the-art models (Wang et al., 2018;<cite> Fried et al., 2018b</cite>; Ma et al., 2019) have demonstrated large gains in accuracy on the VLN task. However, it is unclear which modality these go past the couch \u2026 Figure 1 : We factor the grounding of language instructions into visual appearance, route structure, and object detections using a mixture-of-experts approach.",
  "y": "background"
 },
 {
  "id": "0d798fcdee6ee5722d6dc5638210c2_1",
  "x": "These approaches have significantly improved performance on the VLN task, when evaluated by metrics such as success rate. However, it is unclear where the high performance comes from. In this paper, we find that agents without any visual input can achieve competitive performance, matching or even outperforming their vision-based counterparts under two state-of-theart model models<cite> (Fried et al., 2018b</cite>; Ma et al., 2019) .",
  "y": "motivation"
 },
 {
  "id": "0d798fcdee6ee5722d6dc5638210c2_2",
  "x": "Anand et al. (2018) find that stateof-the-art results can be achieved on the EmbodiedQA task (Das et al., 2018 ) using an agent without visual inputs. Work concurrent to ours evaluates the performance of single-modality models for several embodied tasks including VLN (Thomason et al., 2019) , finding that high performance can be achieved on the R2R dataset using a non-visual version of the baseline model (Anderson et al., 2018) . In this paper, we show that the same trends hold for two recent state-of-the-art architectures (Ma et al., 2019;<cite> Fried et al., 2018b)</cite> for the VLN task; we also analyze to what extent object-based representations and mixture-ofexperts methods can address these issues.",
  "y": "similarities"
 },
 {
  "id": "0d798fcdee6ee5722d6dc5638210c2_3",
  "x": "At each timestep, the agent receives the panoramic image for the viewpoint it is currently located at, and either predicts to move to one of the adjacent connected viewpoints, or to stop. When the agent predicts the stop action, it is evaluated on whether it has correctly reached the end of the route that the human annotator was asked to describe. In this work, we analyze two recent VLN models, which typify the visual grounding approaches of VLN work: the panoramic \"follower\" model from the Speaker-Follower (SF) system of<cite> Fried et al. (2018b)</cite> and the Self-Monitoring (SM) model of Ma et al. (2019) .",
  "y": "similarities uses"
 },
 {
  "id": "0d798fcdee6ee5722d6dc5638210c2_4",
  "x": "We investigate how well these models ground instructions into visual features of the environment, by training and evaluating them without access to the visual context: setting their visual feature vectors to zeroes during training and testing. We compare performance on the validation sets of the R2R dataset: the val-seen split, consisting of the same environments as in training, and the val- Table 1 : Success rate (SR) of the vision-based full agent (\"RN\", using ResNet) and the non-visual agent (\"no vis.\", setting all visual features to zero) on the R2R dataset under different model architectures (SpeakerFollower (SF)<cite> (Fried et al., 2018b)</cite> and Self-Monitoring (SM) (Ma et al., 2019) ) and training schemes. unseen split of novel environments.",
  "y": "extends differences"
 },
 {
  "id": "0d798fcdee6ee5722d6dc5638210c2_5",
  "x": "We construct a set of vectors {x obj,j } representing detected objects and their attributes. Each vector x obj,j (j-th detected object in the scene) is a concatenation of summed GloVe vectors (Pennington et al., 2014) for the detected object label (e.g. door) and attribute labels (e.g. white) and a location vector from the object's bounding box coordinates. We then use the same visual attention mechanism as in<cite> Fried et al. (2018b)</cite> and Ma et al. (2019) to obtain an attended object representation x obj,att over these {x obj,j } vectors.",
  "y": "similarities uses"
 },
 {
  "id": "0d798fcdee6ee5722d6dc5638210c2_7",
  "x": "**A DETAILS ON THE COMPARED VLN MODELS** The Speaker-Follower (SF) model<cite> (Fried et al., 2018b</cite> ) and the Self-Monitoring (SM) model (Ma et al., 2019) which we analyze both use sequenceto-sequence model (Cho et al., 2014) with attention (Bahdanau et al., 2015) as their base instruction-following agent. Both use an encoder LSTM (Hochreiter and Schmidhuber, 1997 ) to represent the instruction text, and a decoder LSTM to predict actions sequentially.",
  "y": "similarities"
 },
 {
  "id": "0e4ca87c0e2b899bfd1f36dc5974b9_0",
  "x": "Pre-trained language models, especially BERT-the Bidirectional Encoder Representations from Transformers [Devlin et al., 2019] , have recently become extremely popular and helped to produce significant improvement gains for various NLP tasks. The success of pre-trained BERT and its variants has largely been limited to the English language. For other languages, one could retrain a language-specific model using the BERT architecture Martin et al., 2019; de Vries et al., 2019] or employ existing pre-trained multilingual BERT-based models [Devlin et al., 2019;<cite> Conneau et al., 2019</cite>;<cite> Conneau and Lample, 2019]</cite> .",
  "y": "background"
 },
 {
  "id": "0e4ca87c0e2b899bfd1f36dc5974b9_1",
  "x": "In terms of Vietnamese language modeling, to the best of our knowledge, there are two main concerns: (i) The Vietnamese Wikipedia corpus is the only data used to train all monolingual language models , and it also is the only Vietnamese dataset included in the pre-training data used by all multilingual language models except XLM-R<cite> [Conneau et al., 2019]</cite> . It is worth noting that Wikipedia data is not representative of a general language use, and the Vietnamese Wikipedia data is relatively small (1GB in size uncompressed), while pre-trained language models can be significantly improved by using more data [Liu et al., 2019] . (ii) All monolingual and multilingual models, except ETNLP , are not aware of the difference between Vietnamese syllables and word tokens (this ambiguity comes from the fact that the white space is also used to separate syllables that constitute words when written in Vietnamese).",
  "y": "background motivation"
 },
 {
  "id": "0e4ca87c0e2b899bfd1f36dc5974b9_2",
  "x": "Experimental setup: For the two most common Vietnamese POS tagging and NER tasks, we follow the VnCoreNLP setup , using standard benchmarks of the VLSP 2013 POS tagging dataset and the VLSP 2016 NER dataset [Nguyen et al., 2019a] . For NLI, we use the Vietnamese validation and test sets from the XNLI corpus v1.0 <cite>[Conneau et al., 2018]</cite> where the Vietnamese training data is machinetranslated from English. Unlike the 2013 POS tagging and 2016 NER datasets which provide the gold word segmentation, for NLI, we use RDRSegmenter to segment the text into words before applying fastBPE to produce subwords from word tokens.",
  "y": "uses"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_0",
  "x": "The model can also incorporate other modalities, such as gaze or pointing cues (deixis) incrementally. We also model the saliency of the context, and show that the model can easily take such contextual information into account. The model improves over previous work on reference resolution applied to the same data (Iida et al., 2010;<cite> Iida et al., 2011)</cite> .",
  "y": "differences"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_1",
  "x": "It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen (Prasov and Chai, 2008) , in human-human interactive puzzle tasks (Iida et al., 2010;<cite> Iida et al., 2011)</cite> , in web browsing (Hakkani-t\u00fcr et al., 2014) , and in a moving car where speakers look at objects in their vicinity (Misu et al., 2014) . Incorporating pointing (deictic) gestures is also potentially useful in situated RR; as for example Matuszek et al. (2014) have shown in work on resolving objects processed by computer vision techniques. Chen and Eugenio (2012) looked into reference in multi-modal settings, with focus on co-referential pronouns and pointing gestures.",
  "y": "background"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_2",
  "x": "The corpora presented in <cite>Iida et al. (2011)</cite> and Spanger et al. (2012) are a collection of human/human interaction data where the participants collaboratively solved Tangram puzzles. In this setting, anaphoric references (i.e., pronoun references to entities in an earlier utterance, e.g., \"move it to the left\") and exophoric references via definite descriptions (i.e., references to real-world objects, e.g., \"that one\" or \"the big triangle\") are common (note that both refer in different ways to objects that are physically present). The corpus also records an added modality: the gaze of the puzzle solver (SV) who gives the instructions and that of the operator (OP), who moves the tangram pieces.",
  "y": "background"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_3",
  "x": "Further details of the corpus can be found in<cite> (Iida et al., 2011)</cite> . In order to directly compare our work with previous work, in our evaluations below we consider the same annotated REs. <cite>Iida et al. (2011)</cite> applied a support vector machine-based ranking algorithm (Joachims, 2002) to the task of resolving REs in this corpus.",
  "y": "uses"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_4",
  "x": "<cite>Iida et al. (2011)</cite> applied a support vector machine-based ranking algorithm (Joachims, 2002) to the task of resolving REs in this corpus. They used a total of 36 binary features in the SVM classifier, which predicted the referred object. They further used a separate model for pronoun utterances and non-pronoun utterances, allowing the classifier to learn patterns without confusing utterance types.",
  "y": "background"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_5",
  "x": "Procedure The procedure for this experiment is as follows. In order to compare our results directly with those of <cite>Iida et al. (2011)</cite> , we provide our model with the same training and evaluation data, in a 10-fold cross-validation of the 1192 REs from 27 dialogues (the T2009-11 corpus in ). For development, we used a separate part of the REX corpus (N2009-11) that was structured similarly to the one used in our evaluation.",
  "y": "uses"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_6",
  "x": "The argmax of the distribution is chosen as the hypothesised referred object. P(R|I) P (R|I) models the likelihood of selecting a property of a candidate object for verbalisation; this likelihood is assumed to be uniform for all the properties that the candidate object has. 3 We derive these properties from a representation of the scene; similar to how <cite>Iida et al. (2011)</cite> computed features to present to their classifier: namely Ling (linguistic features), TaskSp (task specific features), and Gaze (from SV only).",
  "y": "similarities"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_7",
  "x": "These properties differ somewhat from the features for the Ling model presented in <cite>Iida et al. (2011)</cite> . Three features that we did use as properties had to do with reference recency: the most recently referred object received the referred X properties, if an object was referred to in the past 5, 10, or 20 seconds. TaskSp <cite>Iida et al. (2011)</cite> used 14 task-specific features, three of which they found to be the most informative in their model.",
  "y": "differences"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_8",
  "x": "These properties differ somewhat from the features for the Ling model presented in <cite>Iida et al. (2011)</cite> . Three features that we did use as properties had to do with reference recency: the most recently referred object received the referred X properties, if an object was referred to in the past 5, 10, or 20 seconds. TaskSp <cite>Iida et al. (2011)</cite> used 14 task-specific features, three of which they found to be the most informative in their model.",
  "y": "background"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_9",
  "x": "TaskSp <cite>Iida et al. (2011)</cite> used 14 task-specific features, three of which they found to be the most informative in their model. Here, we will only use the two most informative features as properties (the third one, whether or not an object was being manipulated at the beginning of the RE, did not improve results in a held-out test): the object that was most recently moved received the most recent move property and objects that have the mouse cursor over them received the mouse pointed property (see Figure 2 ; object 4 would receive both of these properties, but only for the duration that the mouse was actually over it). Each of these properties can be extracted directly from the corpus annotations.",
  "y": "differences"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_10",
  "x": "Gaze Similar to <cite>Iida et al. (2011)</cite> , we consider gaze during a window of 1500ms before the onset of the RE. The object that was gazed at the longest during that time received a longest gazed at property, the object which was fixated upon most recently during that interval before the RE onset received a recent fixation property, and the object which had the most fixations received the most gazed at property. During a RE, an object received the gazed at in utt property if it is gazed at during the RE up until that point.",
  "y": "similarities"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_11",
  "x": "Other gaze features are not really accessible to an incremental model such as this, as gaze features extracted from gaze activity over the RE can only be computed when it is complete. Our Gaze properties are made up of these 4 properties, as opposed to the 14 features in <cite>Iida et al. (2011)</cite> . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_12",
  "x": ".162 Table 5 : Application of RE small triangle, where 1 is the referred object Evaluation Metrics We report results of our evaluation in referential accuracy on utterances that were annotated as referring to a single object (references to group objects is left for future work). Going beyond <cite>Iida et al. (2011)</cite> , our model computes a resolution hypothesis incrementally; for the performance of this aspect of the system we followed previously used metrics for evaluation : first correct: how deep into the RE does the model predict the referent for the first time? first final: how deep into the RE does the model predict the correct referent and keep that decision until the end? edit overhead: how often did the model unnecessarily change its prediction (the only necessary prediction happens when it first makes a correct prediction)? We compare non-incremental results to three evaluations performed in <cite>Iida et al. (2011)</cite> , namely when Ling is used alone, Ling+TaskSP used together, and Ling+TaskSp+Gaze.",
  "y": "extends"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_13",
  "x": "Going beyond <cite>Iida et al. (2011)</cite> , our model computes a resolution hypothesis incrementally; for the performance of this aspect of the system we followed previously used metrics for evaluation : first correct: how deep into the RE does the model predict the referent for the first time? first final: how deep into the RE does the model predict the correct referent and keep that decision until the end? edit overhead: how often did the model unnecessarily change its prediction (the only necessary prediction happens when it first makes a correct prediction)? We compare non-incremental results to three evaluations performed in <cite>Iida et al. (2011)</cite> , namely when Ling is used alone, Ling+TaskSP used together, and Ling+TaskSp+Gaze. Furthermore, they show results of models where a separate part handled REs that used pronouns, as well as a part that handled the non-pronoun REs, and a combined model that handled both types of expressions.",
  "y": "uses"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_14",
  "x": "**REFERENCE RESOLUTION** Results of our evaluation are shown in Figure 4 . The SIUM model performs better than the combined approach of <cite>Iida et al. (2011)</cite> , and performs better than their separated model-when not including gaze (there is a significant difference between SIUM and the separated models for Ling+TaskSp, though (2011) SIUM only got one more correct than the separated model).",
  "y": "differences"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_15",
  "x": "This is the case for several reasons: first, their models use features that are not available to our incremental model (e.g., their model uses 14 gaze features, some of which were based on the entire RE, ours only uses 4 properties). Second, and more importantly, separated models means less feature confusion: in <cite>Iida et al. (2011)</cite> (Section 5.2) , the authors give a comparison of the most informative features for each model; task and gaze features were prominent for the pronoun model, whereas gaze and language features were prominent for the non-pronoun model. We also tested SIUM under separated conditions to better compare with the approaches presented here.",
  "y": "background"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_17",
  "x": "The property names are arbitrary as long as they are consistent. In contrast, previous work in RR<cite> (Iida et al., 2011</cite>; Chai et al., 2014 ) used a hand-coded concept-labeled semantic representation and checked if aspects of the RE match that of a particular object. If so, a binary compatibility feature was set.",
  "y": "differences background"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_18",
  "x": "However, in the current work we observed that REs with pronouns were more difficult for the model to resolve than the model presented in <cite>Iida et al. (2011)</cite> . We surmise that SIUM had a difficult time grounding certain properties, as the Japanese pronoun sore can be used anaphorically or demonstratively in this kind of context (i.e., sometimes sore refers to previously-manipulated objects, or objects that are newly identified with a mouse pointer over them); the model presented in <cite>Iida et al. (2011)</cite> made more use of contextual information when pronouns were used, particularly in the combined model which incorporated gaze information, as shown above. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "0e5c3df8309dbaf93d10c94fb292fc_19",
  "x": "We surmise that SIUM had a difficult time grounding certain properties, as the Japanese pronoun sore can be used anaphorically or demonstratively in this kind of context (i.e., sometimes sore refers to previously-manipulated objects, or objects that are newly identified with a mouse pointer over them); the model presented in <cite>Iida et al. (2011)</cite> made more use of contextual information when pronouns were used, particularly in the combined model which incorporated gaze information, as shown above. ---------------------------------- **CONCLUSION**",
  "y": "differences"
 },
 {
  "id": "0f0e13e275c4bc4021b1b0d26f3e0c_0",
  "x": "**INTRODUCTION** For the task of fact extraction from billions of Web pages the method of Open Information Extraction (OIE) <cite>(Fader et al., 2011)</cite> trains domainindependent extractors. This important characteristic enables a potential application of OIE for even very large corpora, such as the Web.",
  "y": "background"
 },
 {
  "id": "0f0e13e275c4bc4021b1b0d26f3e0c_1",
  "x": "This important characteristic enables a potential application of OIE for even very large corpora, such as the Web. Existing approaches for OIE, such as REVERB <cite>(Fader et al., 2011)</cite> , WOE (Wu and Weld, 2010) or WANDER-LUST (Akbik and Bross, 2009 ) focus on the extraction of binary facts, e.g. facts that consist of only two arguments, as well as a fact phrase which denotes the nature of the relationship between the arguments. However, a recent analysis of OIE based on Semantic Role Labeling (Christensen et al., 2011) revealed that N-ary facts (facts that connect more than two arguments) were present in 40% of surveyed English sentences.",
  "y": "background"
 },
 {
  "id": "0f0e13e275c4bc4021b1b0d26f3e0c_2",
  "x": "Existing approaches for OIE, such as REVERB <cite>(Fader et al., 2011)</cite> , WOE (Wu and Weld, 2010) or WANDER-LUST (Akbik and Bross, 2009 ) focus on the extraction of binary facts, e.g. facts that consist of only two arguments, as well as a fact phrase which denotes the nature of the relationship between the arguments. However, a recent analysis of OIE based on Semantic Role Labeling (Christensen et al., 2011) revealed that N-ary facts (facts that connect more than two arguments) were present in 40% of surveyed English sentences. Worse, the analyses performed in <cite>(Fader et al., 2011)</cite> and (Akbik and Bross, 2009) show that incorrect handling of N-ary facts leads to extraction errors, such as incomplete, uninformative or erroneous facts.",
  "y": "background"
 },
 {
  "id": "0f0e13e275c4bc4021b1b0d26f3e0c_3",
  "x": "Worse, the analyses performed in <cite>(Fader et al., 2011)</cite> and (Akbik and Bross, 2009) show that incorrect handling of N-ary facts leads to extraction errors, such as incomplete, uninformative or erroneous facts. Our first example illustrates the case of a significant information loss: a) In the 2002 film Bubba Ho-tep, Elvis lives in a nursing home. REVERB: LivesIn(Elvis, nursing home)",
  "y": "motivation"
 },
 {
  "id": "0f0e13e275c4bc4021b1b0d26f3e0c_4",
  "x": "In order to investigate the need and feasibility for N-ary OIE we have performed the following, the results of which we present in this paper: 1. We introduce the OIE system KRAKEN, which has been built specifically for capturing complete facts from sentences and is capable of extracing unary, binary and higher order N-ary facts. 2. We examine intra sentence fact correctness (true/false) and fact completeness for KRAKEN and REVERB on the corpus of <cite>(Fader et al., 2011)</cite> .",
  "y": "uses"
 },
 {
  "id": "0f0e13e275c4bc4021b1b0d26f3e0c_5",
  "x": "In their evaluation they showed that using deep syntactic parsing improves the precision of their system, however at a high cost in extraction speed. The OIE system REVERB <cite>(Fader et al., 2011)</cite> by contrast uses a fast shallow syntax parser for labeling sentences and applies syntactic and a lexical constraints for identifying binary facts. However, the shallow syntactic analysis limits the capability of REVERB of extracting higher order N-ary facts.",
  "y": "background"
 },
 {
  "id": "0f0e13e275c4bc4021b1b0d26f3e0c_6",
  "x": "**EXPERIMENTAL SETUP** Data set: We use the data set from <cite>(Fader et al., 2011)</cite> which consists of 500 sentences sampled from the Web using Yahoo's random link service. 2 The sentences were labeled both with facts found with KRAKEN and the current version of REVERB.",
  "y": "uses"
 },
 {
  "id": "0f5c87e5434785a612c6578244543d_0",
  "x": "<cite>Faruqui and Dyer (2014)</cite> use canonical correlation analysis to project the embeddings in both languages to a shared vector space. Beyond linear mappings, Lu et al. (2015) apply deep canonical correlation analysis to learn a nonlinear transformation for each language. Finally, additional techniques have been used to address the hubness problem in Mikolov et al. (2013b) , both through the neighbor retrieval method and the training itself .",
  "y": "background"
 },
 {
  "id": "0f5c87e5434785a612c6578244543d_1",
  "x": "In this paper, we propose a general framework to learn bilingual word embeddings. We start with a basic optimization objective (Mikolov et al., 2013b) and introduce several meaningful and intuitive constraints that are equivalent or closely related to previously proposed methods <cite>(Faruqui and Dyer, 2014</cite>; Xing et al., 2015) . Our framework provides a more general view of bilingual word embedding mappings, showing the underlying connection between the existing methods, revealing some flaws in their theoretical justification and providing an alternative theoretical interpretation for them.",
  "y": "extends uses"
 },
 {
  "id": "0f5c87e5434785a612c6578244543d_2",
  "x": "In this paper, we propose a general framework to learn bilingual word embeddings. We start with a basic optimization objective (Mikolov et al., 2013b) and introduce several meaningful and intuitive constraints that are equivalent or closely related to previously proposed methods <cite>(Faruqui and Dyer, 2014</cite>; Xing et al., 2015) . Our framework provides a more general view of bilingual word embedding mappings, showing the underlying connection between the existing methods, revealing some flaws in their theoretical justification and providing an alternative theoretical interpretation for them.",
  "y": "extends differences"
 },
 {
  "id": "0f5c87e5434785a612c6578244543d_3",
  "x": "We start with a basic optimization objective (Mikolov et al., 2013b) and introduce several meaningful and intuitive constraints that are equivalent or closely related to previously proposed methods <cite>(Faruqui and Dyer, 2014</cite>; Xing et al., 2015) . Our framework provides a more general view of bilingual word embedding mappings, showing the underlying connection between the existing methods, revealing some flaws in their theoretical justification and providing an alternative theoretical interpretation for them. Our experiments on an existing English-Italian word translation induction and an English word analogy task give strong empirical evidence in favor of our theoretical reasoning, while showing that one of our models clearly outperforms previous alternatives.",
  "y": "differences"
 },
 {
  "id": "0f5c87e5434785a612c6578244543d_4",
  "x": "As long as W is orthogonal, this is equivalent to maximizing the sum of dimensionwise covariance for the dictionary entries: where C m denotes the centering matrix This equivalence reveals that the method proposed by <cite>Faruqui and Dyer (2014)</cite> is closely related to our framework. More concretely, <cite>Faruqui and Dyer (2014)</cite> use Canonical Correlation Analysis (CCA) to project the word embeddings in both languages to a shared vector space.",
  "y": "similarities"
 },
 {
  "id": "0f5c87e5434785a612c6578244543d_5",
  "x": "where C m denotes the centering matrix This equivalence reveals that the method proposed by <cite>Faruqui and Dyer (2014)</cite> is closely related to our framework. More concretely, <cite>Faruqui and Dyer (2014)</cite> use Canonical Correlation Analysis (CCA) to project the word embeddings in both languages to a shared vector space. Therefore, the only fundamental difference between both methods is that, while our model enforces monolingual invariance, <cite>Faruqui and Dyer (2014)</cite> do change the monolingual embeddings to meet this restriction.",
  "y": "similarities differences"
 },
 {
  "id": "0f5c87e5434785a612c6578244543d_6",
  "x": "The code for Mikolov et al. (2013b) and Xing et al. (2015) is not publicly available, so we implemented and tested them as part of the proposed framework, which only differs from the original systems in the optimization method (exact solution instead of gradient descent) and the length normalization approach in the case of Xing et al. (2015) (postprocessing instead of constrained training). As for the method by <cite>Faruqui and Dyer (2014)</cite> , we used their original implementation in Python and MAT-LAB 6 , which we extended to cover cases where the dictionary contains more than one entry for the same word. ----------------------------------",
  "y": "extends uses"
 },
 {
  "id": "0f5c87e5434785a612c6578244543d_8",
  "x": "In any case, it is our proposed method with the orthogonality constraint and a global preprocessing with length normalization followed by dimensionwise mean centering that achieves the best accuracy in the word translation induction task. Moreover, it does not suffer from any considerable degradation in monolingual quality, with an anecdotal drop of only 0.07% in contrast with 2.86% for Mikolov et al. (2013b) and 7.02% for <cite>Faruqui and Dyer (2014)</cite> . When compared to Xing et al. (2015) , our results in Table 1 reinforce our theoretical interpretation for their method (cf. Section 2.2), as it empirically shows that its improvement with respect to Mikolov et al. (2013b) comes solely from the orthogonality constraint, and not from solving any inconsistency.",
  "y": "differences"
 },
 {
  "id": "0f5c87e5434785a612c6578244543d_9",
  "x": "When compared to Xing et al. (2015) , our results in Table 1 reinforce our theoretical interpretation for their method (cf. Section 2.2), as it empirically shows that its improvement with respect to Mikolov et al. (2013b) comes solely from the orthogonality constraint, and not from solving any inconsistency. It should be noted that the implementation by <cite>Faruqui and Dyer (2014)</cite> also length-normalizes the word embeddings in a preprocessing step. Following the discussion in Section 2.3, this means that our best performing configuration is conceptually very close to the method by <cite>Faruqui and Dyer (2014)</cite> , as they both coincide on maximizing the average dimension-wise covariance and length-normalize the embeddings in both languages first, the only difference being that our model enforces monolingual invariance after the normalization while theirs does change the monolingual embeddings to make different dimensions have the same variance and be uncorrelated among themselves.",
  "y": "similarities"
 },
 {
  "id": "0f5c87e5434785a612c6578244543d_10",
  "x": "Following the discussion in Section 2.3, this means that our best performing configuration is conceptually very close to the method by <cite>Faruqui and Dyer (2014)</cite> , as they both coincide on maximizing the average dimension-wise covariance and length-normalize the embeddings in both languages first, the only difference being that our model enforces monolingual invariance after the normalization while theirs does change the monolingual embeddings to make different dimensions have the same variance and be uncorrelated among themselves. However, our model performs considerably better than any configuration from <cite>Faruqui and Dyer (2014)</cite> in both the monolingual and the bilingual task, supporting our hypothesis that these two constraints that are implicit in their method are not only conceptually confusing, 2292 but also have a negative impact. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "0f5c87e5434785a612c6578244543d_11",
  "x": "Following the discussion in Section 2.3, this means that our best performing configuration is conceptually very close to the method by <cite>Faruqui and Dyer (2014)</cite> , as they both coincide on maximizing the average dimension-wise covariance and length-normalize the embeddings in both languages first, the only difference being that our model enforces monolingual invariance after the normalization while theirs does change the monolingual embeddings to make different dimensions have the same variance and be uncorrelated among themselves. However, our model performs considerably better than any configuration from <cite>Faruqui and Dyer (2014)</cite> in both the monolingual and the bilingual task, supporting our hypothesis that these two constraints that are implicit in their method are not only conceptually confusing, 2292 but also have a negative impact. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "0f5c87e5434785a612c6578244543d_12",
  "x": "This paper develops a new framework to learn bilingual word embedding mappings, generalizing previous work and providing an efficient exact method to learn the optimal transformation. Our experiments show the effectiveness of the proposed model and give strong empirical evidence in favor of our reinterpretation of Xing et al. (2015) and <cite>Faruqui and Dyer (2014)</cite> . It is the proposed method with the orthogonality constraint and a global preprocessing with length normalization and dimension-wise mean centering that achieves the best overall results both in monolingual and bilingual terms, surpassing those previous methods.",
  "y": "differences"
 },
 {
  "id": "0fed8b9e785426880fa8e5641116a4_0",
  "x": "**INTRODUCTION** AMBER is a machine translation evaluation metric first described in <cite>(Chen and Kuhn, 2011)</cite> . It is designed to have the advantages of BLEU (Papineni et al., 2002) , such as nearly complete language independence and rapid computability, while attaining even higher correlation with human judgment.",
  "y": "background"
 },
 {
  "id": "0fed8b9e785426880fa8e5641116a4_1",
  "x": "The penalty part is a weighted product of several different penalties (Equation 3). Our original AMBER paper <cite>(Chen and Kuhn, 2011)</cite> describes the ten penalties used at that time; two of these penalties, the normalized Spearman's correlation penalty and the normalized Kendall's correlation penalty, model word reordering. In addition to the more complex score and penalty factors, AMBER differs from BLEU in two other ways:",
  "y": "background"
 },
 {
  "id": "0fed8b9e785426880fa8e5641116a4_2",
  "x": "**\u2022** The AMBER score can be computed with different types of text preprocessing, i.e. different combinations of several text preprocessing techniques: lowercasing, tokenization, stemming, word splitting, etc. 8 types were tried in <cite>(Chen and Kuhn, 2011)</cite> .",
  "y": "background"
 },
 {
  "id": "0fed8b9e785426880fa8e5641116a4_3",
  "x": "In <cite>(Chen and Kuhn, 2011)</cite> , we manually set the 17 free parameters of AMBER (see section 3.2 of that paper). In the experiments reported below, we tuned the 18 free parameters -the original 17 plus the ordering metric v described in the previous section -automatically, using the downhill simplex method of (Nelder and Mead, 1965) as described in (Press et al., 2002) . This is a multidimensional optimization technique inspired by geometrical considerations that has shown good performance in a variety of applications.",
  "y": "differences"
 },
 {
  "id": "0fed8b9e785426880fa8e5641116a4_4",
  "x": "**CONCLUSION** We have made two changes to AMBER, a metric described in <cite>(Chen and Kuhn, 2011)</cite> . In our experiments, the new version of AMBER was shown to be an improvement on the original version in terms of correlation with human judgment.",
  "y": "extends"
 },
 {
  "id": "1056d36c5ed22c7a34f6fe82b4962f_0",
  "x": "While there is a substantial amount of work on statistical (Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2014; Yannakoudakis et al., 2017) and neural (Ji et al., 2017; Xie et al., 2016; Yuan and Briscoe, 2016; Chollampatt et al., 2016; Chollampatt and Ng, 2017; Chollampatt and Ng, 2018) machine translation methods for GEC, we follow the approach of <cite>Bryant and Briscoe (2018)</cite> and explore how such models would fare in this task when treated as simple language models. More specifically, <cite>Bryant and Briscoe (2018)</cite> train a 5-gram language model on the One Billion Word Benchmark (Chelba et al., 2013) dataset and find that it produces competitive baseline results without any supervised training. In our work, we extend <cite>this work</cite> by substituting the n-gram model for several publicly available implementations of state-of-the-art Transformer language models trained on large linguistic corpora and assess their performance on GEC without any supervised training.",
  "y": "uses"
 },
 {
  "id": "1056d36c5ed22c7a34f6fe82b4962f_1",
  "x": "While there is a substantial amount of work on statistical (Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2014; Yannakoudakis et al., 2017) and neural (Ji et al., 2017; Xie et al., 2016; Yuan and Briscoe, 2016; Chollampatt et al., 2016; Chollampatt and Ng, 2017; Chollampatt and Ng, 2018) machine translation methods for GEC, we follow the approach of <cite>Bryant and Briscoe (2018)</cite> and explore how such models would fare in this task when treated as simple language models. More specifically, <cite>Bryant and Briscoe (2018)</cite> train a 5-gram language model on the One Billion Word Benchmark (Chelba et al., 2013) dataset and find that it produces competitive baseline results without any supervised training. In our work, we extend <cite>this work</cite> by substituting the n-gram model for several publicly available implementations of state-of-the-art Transformer language models trained on large linguistic corpora and assess their performance on GEC without any supervised training.",
  "y": "extends motivation"
 },
 {
  "id": "1056d36c5ed22c7a34f6fe82b4962f_2",
  "x": "However, <cite>Bryant and Briscoe (2018)</cite> recently revived the idea, achieving competitive performance with the state-ofthe-art, demonstrating the effectiveness of the approaches to the task without using any annotated data for training. ---------------------------------- **METHODOLOGY**",
  "y": "background"
 },
 {
  "id": "1056d36c5ed22c7a34f6fe82b4962f_3",
  "x": "---------------------------------- **METHODOLOGY** In this work, we follow the setup from <cite>Bryant and Briscoe (2018)</cite> substituting the 5-gram language model for different language models based on the Transformer architecture.",
  "y": "extends"
 },
 {
  "id": "1056d36c5ed22c7a34f6fe82b4962f_4",
  "x": "Since our systems do not generate novel sequences, we follow <cite>Bryant and Briscoe (2018)</cite> and use simple heuristics to generate a confusion set of sentences that our language models score. For prepositions and determiners, the confusion set includes the set of all prepositions and determiners plus an empty string \u01eb to remove unnecessary additions. For morphological errors (e.g., past tense or pluralization), we use the Automatically Generated Inflection Database (AGID) which contains different morphological forms for each word.",
  "y": "uses"
 },
 {
  "id": "1056d36c5ed22c7a34f6fe82b4962f_5",
  "x": "Note that there is no reason to re-use the vocabulary of the training sets as any large English wordlist would achieve a similar effect. Finally, for spelling mistakes, we, again, follow <cite>Bryant and Briscoe (2018)</cite> and use CyHunSpell 3 to generate alternatives for non-words. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "1056d36c5ed22c7a34f6fe82b4962f_6",
  "x": "Given that our confusion set is prone to errors (due to its automatic generation procedure) as well as the fact that we cannot target all potential errors (e.g., insertions), we bias our method to prefer the original sentence unless a much better the alternative is found. We quantify this margin by imposing a threshold above which we accept a candidate sentence as a better alternative. Concretely, let P (s c ) be the probability of the candidate sentence and P (s o ) the probability of the Table 2 : Results of our Transformer-Language Model approach against similar approaches <cite>(Bryant and Briscoe, 2018)</cite> and state-of-the-art on Grammatical Error Correction.",
  "y": "uses"
 },
 {
  "id": "1056d36c5ed22c7a34f6fe82b4962f_8",
  "x": "Unfortunately, due to licensing issues, we were unable to obtain permission to use the JFLEG (Napoles et al., 2017) corpus for evaluation. Note that in our method, we do not make use of the training sets commonly used with these datasets. However, we use the development sets used by <cite>Bryant and Briscoe (2018)</cite> to tune the hyperparameter \u03c4 .",
  "y": "differences uses"
 },
 {
  "id": "1056d36c5ed22c7a34f6fe82b4962f_9",
  "x": "The number of sentences and tokens for the datasets we used can be found in Table 1 . Similar to <cite>Bryant and Briscoe (2018)</cite> , we report results on three metrics. We use the MaxMatch (M 2 ) Precision, Recall and F 0.5 (Dahlmeier and Ng, 2012b) and ERRANT Precision, Recall and F 0.5 (Bryant et al., 2017) .",
  "y": "similarities"
 },
 {
  "id": "1056d36c5ed22c7a34f6fe82b4962f_10",
  "x": "GPT-2 They all know where the conference is and when. Table 3 : Source sentences along with the gold edits and the proposed candidates from each of our models. Table 2 presents the results of our method comparing them against recent state-of-the-art supervised models and the simple n-gram language model used by <cite>Bryant and Briscoe (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "1056d36c5ed22c7a34f6fe82b4962f_11",
  "x": "Our key motivation was to corroborate and extend the results of <cite>Bryant and Briscoe (2018)</cite> to current state-of-the-art language models which have been trained in several languages and show that these models are tough baselines to beat for novel GEC systems. While the results of the Transformer language models shown in Table 2 demonstrate that they are a tough baseline to beat, it is worth noting that the present approach is not without its limitations. We believe that our methodology should not be considered a panacea to GEC.",
  "y": "extends motivation"
 },
 {
  "id": "10de18ba49c0da530b15ff2d14f343_0",
  "x": "For example, in a hotel review \"Tempat tidur di hotel ini tidak bersih\" (The bed in this hotel is not clean), extraction process returns \"Tempat tidur\" (bed) as aspect term and \"tidak bersih\" (not clean) as opinion term. Aspect and/or opinion terms extraction research has been conducted by Wang et al. [2] and Xu et al. <cite>[3]</cite> that outperformed the best systems in the aspect-based sentiment analysis task on the International Workshop on Semantic Evaluation (SemEval) for aspect and opinion terms extraction. Wang et al. [2] proposed a deep learning model for aspect and opinion terms extraction, named Coupled MultiLayer Attentions (CMLA), with word embedding as its feature.",
  "y": "background"
 },
 {
  "id": "10de18ba49c0da530b15ff2d14f343_1",
  "x": "The model achieved F1-measure of 0.7073 and 0.7368 for aspect and opinion term extraction respectively using SemEval 2015 task 12 subtask 1 restaurant dataset [4] . Xu et al. <cite>[3]</cite> proposed a Convolutional Neural Network (CNN) model employing two types of pre-trained word embeddings, general-purpose embeddings and domainspecific embeddings, for aspect term extraction. The two embeddings are concatenated into one word embedding called double embeddings.",
  "y": "background"
 },
 {
  "id": "10de18ba49c0da530b15ff2d14f343_2",
  "x": "The two embeddings are concatenated into one word embedding called double embeddings. The model achieved F1-measure of 0.7437 for aspect term extraction using SemEval 2016 task 5 subtask 1 restaurant dataset [5] . Wang et al. [2] and Xu et al. <cite>[3]</cite> approaches have not been applied for Indonesian reviews.",
  "y": "background"
 },
 {
  "id": "10de18ba49c0da530b15ff2d14f343_3",
  "x": "Wang et al. [2] and Xu et al. <cite>[3]</cite> approaches have not been applied for Indonesian reviews. This paper aims to perform aspect and opinion terms extraction in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism <cite>[3]</cite> . The adaption in this paper is conducted by changing the English resources used in word embedding into Indonesian version.",
  "y": "similarities uses"
 },
 {
  "id": "10de18ba49c0da530b15ff2d14f343_4",
  "x": "The coupled multilayer attentions that was proposed by [2] models the relations among tokens automatically without any syntactic/dependency parsing or linguistic resources as additional information for the input and achieves good performance for aspect and opinion terms extraction. The coupled attentions are used to exploit the correlations between aspect and opinion terms using tensor operators [2] . Xu et al. <cite>[3]</cite> use double embeddings that leverage both general embeddings and domain embeddings as a feature for a CNN model and let the CNN model decide which embeddings have more useful information.",
  "y": "background"
 },
 {
  "id": "10de18ba49c0da530b15ff2d14f343_5",
  "x": "The experiment conducted in <cite>[3]</cite> demonstrated that double embedding mechanism achieved better performance for aspect terms extraction compared to the use of general embeddings or domain embeddings alone. ---------------------------------- **III. PROPOSED APPROACH**",
  "y": "background"
 },
 {
  "id": "10de18ba49c0da530b15ff2d14f343_6",
  "x": "**III. PROPOSED APPROACH** As stated previously, the goal of this work is to extract aspect and opinion terms in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism <cite>[3]</cite> . The architecture of the model used in this work can be seen in Fig. 1 .",
  "y": "uses similarities"
 },
 {
  "id": "10de18ba49c0da530b15ff2d14f343_7",
  "x": "We use various types of word embeddings adapted from <cite>[3]</cite> . Specifically, we conduct experiment using double embeddings, general embeddings, domain embeddings, and hybrid embeddings as the feature used by the model and choose the word embedding that gives the best performance as the feature used by the final model. The description of each type of word embeddings can be seen in Table II.",
  "y": "uses"
 },
 {
  "id": "10de18ba49c0da530b15ff2d14f343_8",
  "x": "The hybrid embeddings use the combined corpus between Indonesian Wikipedia articles and Indonesian hotel reviews. All of the word embeddings are trained using fastText [16] . For the general embeddings and domain embeddings, we use the same dimension and number of iterations as in <cite>[3]</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "10f17930192132077f0d4526e7d755_0",
  "x": "In line with WHO's Mental Health Action Plan (Saxena et al., 2013) , the natural language processing community helps the gathering of information and evidence on mental conditions, focusing on text analysis of authors affected by mental illnesses. Researchers can utilize large amounts of text on social media sites to get a deeper understanding of mental health and develop models for early detection of various mental disorders (De Choudhury et al., 2013a; Coppersmith et al., 2014; Gkotsis et al., 2016; Benton et al., 2017; Sekuli\u0107 et al., 2018; Zomick et al., 2019) . In this work, we experiment with the <cite>Self-reported Mental Health Diagnoses</cite> (<cite>SMHD</cite>) dataset (<cite>Cohan et al., 2018</cite>) , consisting of thousands of Reddit users diagnosed with one or more mental illnesses.",
  "y": "uses"
 },
 {
  "id": "10f17930192132077f0d4526e7d755_1",
  "x": "---------------------------------- **SELF-REPORTED MENTAL HEALTH DIAGNOSES DATASET** The <cite>SMHD</cite> dataset (<cite>Cohan et al., 2018</cite>) is a largescale dataset of Reddit posts from users with one or multiple mental health conditions.",
  "y": "background"
 },
 {
  "id": "10f17930192132077f0d4526e7d755_2",
  "x": "The nine disorders and the number of users per disorder, as well as average number of posts per user, are shown in Table 1 . For each disorder, <cite>Cohan et al. (2018)</cite> analyze the differences in language use between diagnosed users and their respective control groups. <cite>They</cite> also provide benchmark results for the binary classification task of predicting whether the user belongs to the diagnosed or the control group.",
  "y": "background"
 },
 {
  "id": "10f17930192132077f0d4526e7d755_3",
  "x": "The nine disorders and the number of users per disorder, as well as average number of posts per user, are shown in Table 1 . For each disorder, <cite>Cohan et al. (2018)</cite> analyze the differences in language use between diagnosed users and their respective control groups. <cite>They</cite> also provide benchmark results for the binary classification task of predicting whether the user belongs to the diagnosed or the control group.",
  "y": "background"
 },
 {
  "id": "10f17930192132077f0d4526e7d755_4",
  "x": "For each disorder, <cite>Cohan et al. (2018)</cite> analyze the differences in language use between diagnosed users and their respective control groups. <cite>They</cite> also provide benchmark results for the binary classification task of predicting whether the user belongs to the diagnosed or the control group. We reproduce <cite>their</cite> baseline models for each disorder and compare to our deep learning-based model, explained in Section 2.3.",
  "y": "uses"
 },
 {
  "id": "10f17930192132077f0d4526e7d755_5",
  "x": "**SELECTING THE CONTROL GROUP** <cite>Cohan et al. (2018)</cite> select nine or more control users for each diagnosed user and run their experiments with these mappings. With this exact mapping not being available, for each of the nine conditions, we had to select the control group ourselves.",
  "y": "background"
 },
 {
  "id": "10f17930192132077f0d4526e7d755_6",
  "x": "**SELECTING THE CONTROL GROUP** <cite>Cohan et al. (2018)</cite> select nine or more control users for each diagnosed user and run their experiments with these mappings. With this exact mapping not being available, for each of the nine conditions, we had to select the control group ourselves.",
  "y": "extends differences"
 },
 {
  "id": "10f17930192132077f0d4526e7d755_7",
  "x": "<cite>Cohan et al. (2018)</cite> select nine or more control users for each diagnosed user and run their experiments with these mappings. With this exact mapping not being available, for each of the nine conditions, we had to select the control group ourselves. For each diagnosed user, we draw exactly nine control users from the pool of 335,952 control users present in <cite>SMHD</cite> and proceed to train and test our binary classifiers on the newly created sub-datasets.",
  "y": "extends uses"
 },
 {
  "id": "10f17930192132077f0d4526e7d755_8",
  "x": "With this exact mapping not being available, for each of the nine conditions, we had to select the control group ourselves. For each diagnosed user, we draw exactly nine control users from the pool of 335,952 control users present in <cite>SMHD</cite> and proceed to train and test our binary classifiers on the newly created sub-datasets. In order to create a statistically-fair comparison, we run the selection process multiple times, as well as reimplement the benchmark models used in <cite>Cohan et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "10f17930192132077f0d4526e7d755_9",
  "x": "The model that proves best on the development set is selected. We implement the baselines as in <cite>Cohan et al. (2018)</cite> . Logistic regression and the linear SVM were trained on tf-idf weighted bag-of-words features, where users' posts are all concatenated and all the tokens lower-cased.",
  "y": "uses"
 },
 {
  "id": "10f17930192132077f0d4526e7d755_10",
  "x": "Logistic regression and linear SVM achieve higher scores where there is a smaller number of diagnosed users. In contrast to <cite>Cohan et al. (2018)</cite> , supervised FastText yields worse results than tuned linear models. We further investigate the impact of the size of the dataset on the final results of classification.",
  "y": "differences"
 },
 {
  "id": "10f17930192132077f0d4526e7d755_11",
  "x": "We examine attention weights on a word level and compare the most attended words to prior research on depression. Depression is selected as the most prevalent disorder in the <cite>SMHD</cite> dataset with a number of studies in the field (Rude et al., 2004; Chung and Pennebaker, 2007; De Choudhury et al., 2013b; Park et al., 2012) . For each post, we extracted two words with the highest attention weight as being the most relevant for the classification.",
  "y": "uses"
 },
 {
  "id": "10f17930192132077f0d4526e7d755_12",
  "x": "The importance of personal pronouns in distinguishing depressed authors from the control group is supported by multiple studies (Rude et al., 2004; Chung and Pennebaker, 2007; De Choudhury et al., 2013b; <cite>Cohan et al., 2018</cite>) . In the categories Affective processes, Social processes, and Biological processes, <cite>Cohan et al. (2018)</cite> report significant differences between depressed and control group, similar to some other disorders. Except the above mentioned words and their abbreviations, among most commonly attended are swear words, as well as other forms of informal language.",
  "y": "background"
 },
 {
  "id": "10f17930192132077f0d4526e7d755_13",
  "x": "The importance of personal pronouns in distinguishing depressed authors from the control group is supported by multiple studies (Rude et al., 2004; Chung and Pennebaker, 2007; De Choudhury et al., 2013b; <cite>Cohan et al., 2018</cite>) . In the categories Affective processes, Social processes, and Biological processes, <cite>Cohan et al. (2018)</cite> report significant differences between depressed and control group, similar to some other disorders. Except the above mentioned words and their abbreviations, among most commonly attended are swear words, as well as other forms of informal language.",
  "y": "background"
 },
 {
  "id": "10f17930192132077f0d4526e7d755_14",
  "x": "In recent years, social media has been a valuable source for psychological research. While most studies use Twitter data (Coppersmith et al., 2015a (Coppersmith et al., , 2014 Benton et al., 2017; Coppersmith et al., 2015b) , a recent stream turns to Reddit as a richer source of high-volume data (De Choudhury and De, 2014; Shen and Rudzicz, 2017; Gjurkovi\u0107 and\u0160najder, 2018; <cite>Cohan et al., 2018</cite>; Sekuli\u0107 et al., 2018; Zirikly et al., 2019) . Previous approaches to author's mental health prediction usually relied on linguistic and stylistic features, e.g., Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001 ) -a widely used feature extractor for various studies regarding mental health (Rude et al., 2004; Coppersmith et al., 2014; Sekuli\u0107 et al., 2018; Zomick et al., 2019) .",
  "y": "background"
 },
 {
  "id": "119d473a0a5a4c42de193e51564f1f_0",
  "x": "[Explicit CONTRAST] Figure. 1: No explicit relation is detected in the complex sentence (left), but an explicit CONTRAST relation is identified in the simple sentence (right). The example is taken from the Simple English Wikipedia corpus <cite>(Coster and Kauchak, 2011)</cite> connectives do not belong to any linguistic class and except for a few discourse connectives such as oh and well, most carry meaning.",
  "y": "background"
 },
 {
  "id": "119d473a0a5a4c42de193e51564f1f_1",
  "x": "**DATA SETS** To discover AltLexes automatically, we created two sentence-aligned data sets using standard corpora in text simplification. The first data set was created from the Simple English Wikipedia corpus <cite>(Coster and Kauchak, 2011)</cite> ; the other was created from the Newsela corpus (Xu et al., 2015) .",
  "y": "extends"
 },
 {
  "id": "119d473a0a5a4c42de193e51564f1f_2",
  "x": "The first data set was created from the Simple English Wikipedia corpus <cite>(Coster and Kauchak, 2011)</cite> ; the other was created from the Newsela corpus (Xu et al., 2015) . The Simple English Wikipedia (SEW) corpus <cite>(Coster and Kauchak, 2011)</cite> contains two sections: 1) article-aligned and 2) sentence-aligned. Here, we used the sentence-aligned section, which contains 167,686 pairs of aligned sentences.",
  "y": "extends uses"
 },
 {
  "id": "119d473a0a5a4c42de193e51564f1f_3",
  "x": "This corpus contains 1,911 English news articles which have been manually re-written at most 5 times, each time with decreasing complexity level. We used this article-aligned corpus to align it at the sentence-level using an approach similar to <cite>(Coster and Kauchak, 2011)</cite> . Then, two native English speakers evaluated the alignments.",
  "y": "similarities"
 },
 {
  "id": "123d8e8ddef15fed120908c5c20656_0",
  "x": "While most of the work in this direction has been devoted to learning the acoustic model directly from sequences of phonemes or characters without intermediate alignment step or phone-state/senome induction, the other end of the pipeline model -namely, learning directly from the waveform rather than from speech features such as mel-filterbanks or MFCC -has recently received attention [1, 2, 3, 4, 5, 6, 7,<cite> 8]</cite> , but the performances on the master task of speech recognition still seem to be lagging behind those of models trained on speech features [9, 10] . Yet, promising results have already been obtained by learning the front-end of speech recognition systems. We focus the discussion on trainable components that can be plugged in as replacement of mel-filterbanks without modification of the acoustic model.",
  "y": "background"
 },
 {
  "id": "123d8e8ddef15fed120908c5c20656_1",
  "x": "The approach inspired by gammatone filterbanks of Hoshen et al. and Sainath et al. [3, 4] achieved similar or better results than comparable mel-filterbanks on multichannel speech recognition and on far-field/noisy recording conditions. More recently, Zeghidour et al. <cite>[8]</cite> proposed an alternative learnable architecture based on a convolutional architecture that computes a scattering transform and can be initialized as an approximation of mel-filterbanks, and obtained promising results on endto-end phone recognition on TIMIT. However, these approaches have not been proven to improve on speech features on largescale, end-to-end speech recognition in clean recording conditions on English -admittedly one of the tasks for which melfilterbanks have been the most extensively tuned.",
  "y": "background"
 },
 {
  "id": "123d8e8ddef15fed120908c5c20656_2",
  "x": "2. The low-pass filter previously used in the scattering-based learnable filterbanks stabilizes the training of gammatone filterbanks, compared to the max-pooling that was originally proposed [3, 4] ; 3. For scattering-based trainable filterbanks, keeping the lowpass filter fixed during training allows to efficiently learn the filters from a random initialization, whereas the results of <cite>[8]</cite> with random initialization of both the filters and the lowpass filter showed poor performances compared to a suitable initialization; 4. Both trainable filterbanks improve against the melfilterbanks baseline on word error rate on the Wall Street Journal dataset, in similar conditions (same number of filters, same end-to-end training convolutional architecture).",
  "y": "differences"
 },
 {
  "id": "123d8e8ddef15fed120908c5c20656_3",
  "x": "The first architecture we consider is inspired by [3, 4] , the second one is taken from <cite>[8]</cite> . They are described in Table 1 . In both architectures, a convolutional layer with window length 25ms (to match the standard frame size used in melfilterbanks) is applied with a stride of 1 sample, and is followed by a nonlinearity to give 40 output channels for each sample.",
  "y": "uses"
 },
 {
  "id": "123d8e8ddef15fed120908c5c20656_4",
  "x": "Hoshen et al. and Sainath et al. use 40 realvalued filters with ReLU non-linearity, and rely on gammatones as filter values to approximate mel-filterbanks [3, 4] . In their work, they use a max-pooling operator for low-pass filtering. In contrast, Zeghidour et al. <cite>[8]</cite> use 40 complex-valued filters with a square modulus operator as non-linearity.",
  "y": "background"
 },
 {
  "id": "123d8e8ddef15fed120908c5c20656_5",
  "x": "For both architectures, we also propose to keep this low-pass filter fixed while learning the convolution filter weights, a setting that was not explored by Zeghidour et al. <cite>[8]</cite> , who learnt the lowpass filter weights when randomly initializing the convolutions. ---------------------------------- **INSTANCE NORMALIZATION**",
  "y": "differences"
 },
 {
  "id": "123d8e8ddef15fed120908c5c20656_6",
  "x": "Thus, after the nonlinearity, both architectures have 40 filters. 2 <cite>[8]</cite> use 1 to prevent log(0) and [3, 4] use 0.01. We kept the values initially used by the authors of the respective papers and did not try alternatives.",
  "y": "uses"
 },
 {
  "id": "123d8e8ddef15fed120908c5c20656_8",
  "x": "**INSTANCE NORMALIZATION** As described in Section 2.2, we evaluate the integration of instance normalization after the log-compression in the trainable filterbanks, which was not used in previous work [3, 4, 7,<cite> 8]</cite> but is used in our baseline. Figure 1 shows training LER as a function of the number of epochs for scattering-based and gammatone-based filterbanks models, with and without instance normalization.",
  "y": "differences"
 },
 {
  "id": "123d8e8ddef15fed120908c5c20656_9",
  "x": "The tendency is that the Han-fixed setting consistently improves the results in LER and WER of both trainable filterbanks. More importantly, using either an Han-fixed or Han-learnt filter when learning scatteringbased filterbanks from a random initialization removes the gap in performance with the Gabor wavelet initialization that was observed in <cite>[8]</cite> where the lowpass filter was also initialized randomly. This is an important result since carefully initializing the convolutional filters is both technically non-trivial, and also relies on the prior knowledge of mel-filterbanks.",
  "y": "differences"
 },
 {
  "id": "12ab280d48ef6bfae0ff27a400e2ab_0",
  "x": "This session focused on experimental or planned approaches to human language technology evaluation and included an overview and five papers: two papers on experimental evaluation approaches [l, 2], and three about the ongoing work in new annotation and evaluation approaches for human language technology [3, <cite>4,</cite> 5] . This was followed by fifteen minutes of general discussion. When considering evaluation, it is important to consider the basic issues involved in evaluation:",
  "y": "background"
 },
 {
  "id": "12ab280d48ef6bfae0ff27a400e2ab_1",
  "x": "The last three papers ([3, <cite>4,</cite> 5]) take various approaches to the issue of predicate-argument 1The Penn Treebank parse annotations provide an interesting case where annotation supported evaluation. By creating a theory-neutral description of a correct parse, the Treebank annotation enabled researchers to take the next step in agreeing to use the parse annotations (bracketings) as a \"gold standard\" against which to compare system-derived bracketings [9] . This evaluation, in turn, has enabled interesting automated teaming approaches to parsing.",
  "y": "background"
 },
 {
  "id": "12ab280d48ef6bfae0ff27a400e2ab_2",
  "x": "This session focused on experimental or planned approaches to human language technology evaluation and included an overview and five papers: two papers on experimental evaluation approaches [l, 2] , and three about the ongoing work in new annotation and evaluation approaches for human language technology [3, <cite>4,</cite> 5] . This was followed by fifteen minutes of general discussion. When considering evaluation, it is important to consider the basic issues involved in evaluation:",
  "y": "background"
 },
 {
  "id": "12ab280d48ef6bfae0ff27a400e2ab_3",
  "x": "This kind of evaluation computes output as a simple function of input to the language system. Unfortunately, it is not always possible to measure a meaningful output-for example, researchers have struggled long and hard with measurements for understanding -how can a system demonstrate that it has understood? If we had a general semantic representation, then we could insert a probe on the output side of the semantic component, independent of any specific application. The last three papers ( [3, <cite>4,</cite> 5] ) take various approaches to the issue of predicate-argument 1The Penn Treebank parse annotations provide an interesting case where annotation supported evaluation.",
  "y": "background"
 },
 {
  "id": "12ab280d48ef6bfae0ff27a400e2ab_4",
  "x": "Given this level of investment, it is critical to co-ordinate effort and obtain maximum leverage. The last three papers [3, <cite>4,</cite> 5] all reflect a concern to develop better evaluation methods for semantics, with a shared focus on predicate-argument evaluation. The Treebank annotation paper [3] discusses the new predicate-argument annotation work under Treebank.",
  "y": "background"
 },
 {
  "id": "12ab280d48ef6bfae0ff27a400e2ab_5",
  "x": "The resulting interchange helped to clarify the relationship between these three proposals. Both Marcus and Grishman argued that the Treebank annotation should directly support the MUC-style predicate-argument evaluation outlined in<cite> [4]</cite> , although the Treebank annotations may be a sub-set of what is used for MUC predicate-argument evaluation. The relation of the spoken language \"predicate-argument\" evaluation to the other two was less clear.",
  "y": "background"
 },
 {
  "id": "12c5d72fad925c8ec025cda87a0fd9_0",
  "x": "**INTRODUCTION** Multiword expressions (MWEs) are combinations of multiple words that exhibit some degree of idiomaticity (Baldwin and Kim, 2010) . Verb-noun combinations (VNCs), consisting of a verb with a noun in its direct object position, are a common type of semantically-idiomatic MWE in English and cross-lingually (<cite>Fazly et al., 2009</cite> ).",
  "y": "background"
 },
 {
  "id": "12c5d72fad925c8ec025cda87a0fd9_1",
  "x": "In this paper we further incorporate knowledge of the lexico-syntactic fixedness of VNCs -automatically acquired from corpora using the method of <cite>Fazly et al. (2009)</cite> -into our various embedding-based approaches. Our experimental results show that this leads to substantial improve-ments, indicating that this rich linguistic knowledge is complementary to that available in distributed representations. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "12c5d72fad925c8ec025cda87a0fd9_2",
  "x": "---------------------------------- **RELATED WORK** Much research on MWE identification has focused on specific kinds of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005) , including English VNCs (e.g., <cite>Fazly et al., 2009</cite>; Salton et al., 2016) , although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017) .",
  "y": "background"
 },
 {
  "id": "12c5d72fad925c8ec025cda87a0fd9_3",
  "x": "Much research on MWE identification has focused on specific kinds of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005) , including English VNCs (e.g., <cite>Fazly et al., 2009</cite>; Salton et al., 2016) , although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017) . Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., <cite>Fazly et al., 2009</cite>; Fothergill and Baldwin, 2012) , treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008) , incorporated topic models (e.g., Li et al., 2010) , and made use of distributed representations of words (Gharbieh et al., 2016) . In the most closely related work to ours, Salton et al. (2016) represent token instances of VNCs by embedding the sentence that they occur in using skip-thoughts (Kiros et al., 2015) -an encoderdecoder model that can be viewed as a sentencelevel counterpart to the word2vec (Mikolov et al., 2013 ) skip-gram model.",
  "y": "background"
 },
 {
  "id": "12c5d72fad925c8ec025cda87a0fd9_4",
  "x": "Salton et al. then use these sentence embeddings, representing VNC token instances, as features in a supervised classifier. We treat this skip-thoughts based approach as a strong baseline to compare against. <cite>Fazly et al. (2009)</cite> formed a set of eleven lexicosyntactic patterns for VNC instances capturing the voice of the verb (active or passive), determiner (e.g., a, the), and number of the noun (singular or plural).",
  "y": "background"
 },
 {
  "id": "12c5d72fad925c8ec025cda87a0fd9_5",
  "x": "We treat this skip-thoughts based approach as a strong baseline to compare against. <cite>Fazly et al. (2009)</cite> formed a set of eleven lexicosyntactic patterns for VNC instances capturing the voice of the verb (active or passive), determiner (e.g., a, the), and number of the noun (singular or plural). <cite>They</cite> then determine the canonical form, C(v, n), for a given VNC as follows: 2",
  "y": "background"
 },
 {
  "id": "12c5d72fad925c8ec025cda87a0fd9_6",
  "x": "where f (\u00b7) is the frequency of a VNC occurring in a given pattern in a corpus, 3 and f and s are the mean and standard deviations for all patterns for the given VNC, respectively. <cite>Fazly et al. (2009)</cite> showed that idiomatic usages of a VNC tend to occur in that expression's canonical form, while literal usages do not. This approach provides a strong, linguistically-informed, unsupervised baseline, referred to as CForm, for predicting whether VNC instances are idiomatic or literal.",
  "y": "background"
 },
 {
  "id": "12c5d72fad925c8ec025cda87a0fd9_7",
  "x": "We use the VNC-Tokens dataset (Cook et al., 2008) -the same dataset used by <cite>Fazly et al. (2009)</cite> and Salton et al. (2016) -to train and evaluate our models. This dataset consists of sentences containing VNC usages drawn from the British National Corpus (Burnard, 2000) , 7 along with a label indicating whether the VNC is an idiomatic or literal usage (or whether this cannot be determined, in which case it is labelled \"unknown\"). VNC-Tokens is divided into DEV and TEST sets that each include fourteen VNC types and a total of roughly six hundred instances of these types annotated as literal or idiomatic.",
  "y": "similarities uses"
 },
 {
  "id": "12c5d72fad925c8ec025cda87a0fd9_8",
  "x": "<cite>Fazly et al. (2009)</cite> and Salton et al. (2016) structured their experiments differently. <cite>Fazly et al.</cite> report results over DEV and TEST separately. In this setup TEST consists of expressions that were not seen during model development (done on DEV).",
  "y": "background"
 },
 {
  "id": "12c5d72fad925c8ec025cda87a0fd9_9",
  "x": "<cite>Fazly et al. (2009)</cite> and Salton et al. (2016) structured their experiments differently. <cite>Fazly et al.</cite> report results over DEV and TEST separately. In this setup TEST consists of expressions that were not seen during model development (done on DEV).",
  "y": "background"
 },
 {
  "id": "12c5d72fad925c8ec025cda87a0fd9_10",
  "x": "We therefore use accuracy to evaluate our models following <cite>Fazly et al. (2009)</cite> because the classes are roughly balanced. We randomly divide both DEV and TEST into training and testing portions ten times, following Salton et al. (2016) . For each of the ten runs, we compute the accuracy for each expression, and then compute the average accuracy over the expressions.",
  "y": "uses motivation"
 },
 {
  "id": "12c5d72fad925c8ec025cda87a0fd9_11",
  "x": "**DEV AND TEST RESULTS** In Table 2 we report results on DEV and TEST for each model, as well as the unsupervised CForm model of <cite>Fazly et al. (2009)</cite> , which simply labels a VNC as idiomatic if it occurs in its canonical form, and as literal otherwise. We further consider each model (other than CForm) in two setups.",
  "y": "differences"
 },
 {
  "id": "12c5d72fad925c8ec025cda87a0fd9_12",
  "x": "We do this using CForm, and the word2vec model with and without the canonical form feature. Results are shown in Table 3 . In line with the findings of <cite>Fazly et al. (2009)</cite> , CForm achieves higher precision and recall on idiomatic usages than literal ones.",
  "y": "similarities"
 },
 {
  "id": "13249ad2fd022b9b4f1d22d2ca77cd_0",
  "x": "State of the art statistical machine translation (SMT) models traditionally consist of a small number (<20) of sub-models whose scores are linearly combined to choose the best translation candidate. The weights of this linear combination are usually trained to maximise some automatic translation metric (e.g. BLEU) [1] using Minimum Error Rate Training (MERT) [2,<cite> 3]</cite> or a variant of the Margin Infused Relaxed Algorithm (MIRA) [4, 5] . These algorithms are heavily adapted to exploit the properties of the translation search space.",
  "y": "background"
 },
 {
  "id": "13249ad2fd022b9b4f1d22d2ca77cd_1",
  "x": "Since the translation metrics (e.g. BLEU score) can only be evaluated between the selected translations and reference translations (i.e. the standard manual translations from the parallel training data), meanwhile decoding new translations following Equation 1 is very time consuming, we cannot tune the linear weights directly as in ordinary classification tasks. The most common approach is an iterative algorithm MERT <cite>[3]</cite> which employs N-best lists (the best N translations decoded with a weight set from a previous iteration) as candidate translations C. In this way, the loss function is constructed as E(\u0112,\u00ca) = S s=1 E(\u0113 s ,\u00ea s ), where\u0113 is the reference sentence,\u00ea is selected from N-best lists by\u00ea s = arg max e\u2208C K k=1 w k H k (e, f s ) and S represents the volume of sentences. By exploiting the fact that the error surface is piece-wise linear, MERT iteratively applies line search to find the optimal parameters along the randomly chosen directions via",
  "y": "background"
 },
 {
  "id": "134baefab4d27e9dafd0c050c43775_0",
  "x": "Direct annotation in CCG has so far mostly been limited to small datasets for seeding or testing semantic parsers (e.g., Artzi et al., 2015) , and no graphical annotation interface is available to support such efforts, making the annotation process difficult to scale. The only exceptions we are aware of are the Groningen Meaning Bank and the Parallel Meaning Bank <cite>(Abzianidze et al., 2017)</cite> , two annotation efforts which use a graphical user interface for annotating sentences with CCG derivations and other annotation layers, and which have produced CCG treebanks for English, German, Italian, and Dutch. However, these efforts are focused on semantics and have not released explicit guidelines for syntactic annotation.",
  "y": "background"
 },
 {
  "id": "13d1d79a4922d3b5d215d6f8f722ba_0",
  "x": "De Cao et al. <cite>[2]</cite> proposed a method to detect the set of suitable WordNet senses able to evoke the same frame by exploiting the hypernym hierarchies that capture the largest number of LUs in the frame. For all above mentioned approaches, a real evaluation on randomly selected frames is missing, and accuracy was mainly computed over the new lexical units obtained for a frame, not on a gold standard where one or more synsets are assigned to every lexical unit in a frame. Besides, it seems that the most common approach to carry out the mapping relies on some similarity measures that perform better on richer sets of lexical units.",
  "y": "background"
 },
 {
  "id": "13d1d79a4922d3b5d215d6f8f722ba_1",
  "x": "The only comparable evaluation available is reported in [5] , and shows that our results are promising. De Cao at al. <cite>[2]</cite> reported a better performance, particularly for recall, but evaluation of their mapping algorithm relied on a gold standard of 4 selected frames having at least 10 LUs and a given number of corpus instantiations. In the future, we plan to improve the algorithm by shallow parsing the LU definitions and the WordNet glosses.",
  "y": "differences"
 },
 {
  "id": "13d3d973a4be832f66b049b364fea5_0",
  "x": "They employ statistical approaches that exploit a wide range of textual features. A recent direction of research has focused on applying deep learning to the AA task in order to circumvent the heavy feature engineering involved in traditional systems. Several neural architectures have been employed including variants of Long Short-Term Memory (LSTM)<cite> (Alikaniotis et al., 2016</cite>; Taghipour and Ng, 2016) and Convolutional Neural Networks (CNN) (Dong and Zhang, 2016) .",
  "y": "background"
 },
 {
  "id": "13d3d973a4be832f66b049b364fea5_1",
  "x": "For instance, <cite>Alikaniotis et al. (2016)</cite> developed score-specific word embeddings (SSWE) to address the AA task on the ASAP dataset. Their embeddings are constructed by ranking correct ngrams against their \"noisy\" counterparts, in addition to capturing words' informativeness measured by their contribution to the overall score of the essay. We propose a task-specific approach to pre-train word embeddings, utilized by neural AA models, in an error-oriented fashion.",
  "y": "motivation"
 },
 {
  "id": "13d3d973a4be832f66b049b364fea5_2",
  "x": "Bootstrapping the assessment neural model with those learned embeddings could help detect wrong patterns in writing which should improve its accuracy of predicting the script's holistic score. We implement a CNN as the AA model and compare its performance when initialized with our embeddings, tuned based on natural writing errors, to the one obtained when bootstrapped with the SSWE, proposed by <cite>Alikaniotis et al. (2016)</cite> , that relies on random noisy contexts and script scores. Furthermore, we implement another version of our model that augments ngram errors with their corrections and investigate the effect on performance.",
  "y": "similarities uses"
 },
 {
  "id": "13d3d973a4be832f66b049b364fea5_3",
  "x": "<cite>Alikaniotis et al. (2016)</cite> assessed the same dataset by building a bidirectional double-layer LSTM which outperformed Distributed Memory Model of Paragraph Vectors (PV-DM) (Le and Mikolov, 2014) and Support Vector Machines (SVM) baselines. Dong and Zhang (2016) implemented a CNN where the first layer convolves a filter of weights over the words in each sentence followed by an aggregative pooling function to construct sentence representations. Subsequently, a second filter is applied over sentence representations followed by a pooling operation then a fully-connected layer to predict the final score.",
  "y": "background"
 },
 {
  "id": "13d3d973a4be832f66b049b364fea5_4",
  "x": "Similarly, they generated correct POS ngrams from grammatically correct texts, classified the rest as \"bad POS ngrams\" and used them along with the useful ngrams and other shallow lexical features as bag-of-words features. They applied Bayesian linear ridge regression (BLRR) and SVM regression for domain-adaptation essay scoring using the ASAP dataset. <cite>Alikaniotis et al. (2016)</cite> applied a similar idea; in their SSWE model, they trained word embeddings to distinguish between correct and noisy contexts in addition to focusing more on each word's contribution to the overall text score.",
  "y": "motivation background"
 },
 {
  "id": "13d3d973a4be832f66b049b364fea5_5",
  "x": "---------------------------------- **WORD EMBEDDING PRE-TRAINING** In this section, we describe three different neural networks to pre-train word representations: the model implemented by <cite>Alikaniotis et al. (2016)</cite> and the two error-oriented models we propose in this work.",
  "y": "uses"
 },
 {
  "id": "13d3d973a4be832f66b049b364fea5_6",
  "x": "---------------------------------- **SCORE-SPECIFIC WORD EMBEDDINGS (SSWE).** We compare our pre-training models to the SSWE developed by <cite>Alikaniotis et al. (2016)</cite> .",
  "y": "similarities"
 },
 {
  "id": "13d3d973a4be832f66b049b364fea5_7",
  "x": "Table 2 demonstrates that learning from the er-9 Using the same parameters as <cite>Alikaniotis et al. (2016)</cite> . 10 Tuning the filter sizes was done for each model separately; for the Glove and Word2Vec models, a filter of size 3 performed better than 9, on both datasets. rors and their corrections enhances the error pretraining performance on public FCE which indicates the usefulness of the approach and its ability to mitigate the effects of data sparsity.",
  "y": "similarities uses"
 },
 {
  "id": "1527ce2786adfe0decf8c926a3d846_0",
  "x": "Currently lexical simplification pipelines for scientific texts are rare. The vast majority of prior methods assume a domain independent context, and rely on Wikipedia and Simple English Wikipedia, a subset of Wikipedia using simplified grammar and terminology, to learn simplifications <cite>(Biran et al., 2011</cite>; Paetzold and Specia, 2015) , with translationbased approaches using an aligned version (Coster and Kauchak, 2011; Horn et al., 2014; Yatskar et al., 2010) . However, learning simplifications from Wikipedia is not well suited to lexical simplification of scientific terms.",
  "y": "background"
 },
 {
  "id": "1527ce2786adfe0decf8c926a3d846_1",
  "x": "Further, some approaches work by detecting all pairs of words in a corpus and filtering to isolate synonym or hypernym-relationship pairs using WordNet<cite> (Biran et al., 2011)</cite> . Like Wikipedia, WordNet is a general purpose semantic database (Miller, 1995) , and does not cover all branches of science nor integrate new terminology quickly. Word embeddings do not require the use of prebuilt ontologies to identify associated terms like simplifications.",
  "y": "background"
 },
 {
  "id": "1527ce2786adfe0decf8c926a3d846_2",
  "x": "One approach identifies all pairwise permutations of 'content' terms and then applies semantic (i.e., WordNet) and simplicity filters to eliminate pairs that are not simplifications<cite> (Biran et al., 2011)</cite> . We adopt a similar pipeline but leverage distance metrics on word embeddings and a simpler frequency filter in place of WordNet. Embeddings identify words that share context in an unsupervised, scalable way and are more efficient than constructing co-occurrence matrices<cite> (Biran et al., 2011)</cite> .",
  "y": "background"
 },
 {
  "id": "1527ce2786adfe0decf8c926a3d846_3",
  "x": "We adopt a similar pipeline but leverage distance metrics on word embeddings and a simpler frequency filter in place of WordNet. Embeddings identify words that share context in an unsupervised, scalable way and are more efficient than constructing co-occurrence matrices<cite> (Biran et al., 2011)</cite> . As our experiments demonstrate, our approach improves performance on a scientific test set over prior work.",
  "y": "background motivation"
 },
 {
  "id": "1527ce2786adfe0decf8c926a3d846_4",
  "x": "The POS of each word is determined by Morphadorner (Burns, 2013) and pairs that differ in POS are omitted (e.g., permutation (noun), change(d) (verb)); Finally, we omit rules where one word is a prefix of the other and the suffix is one of s, es, ed, ly, er, or ing. To retain only rules of the form complex word \u2192 simple word we calculate the corpus complexity, C<cite> (Biran et al., 2011)</cite> of each word w as the ratio between the frequency (f ) in the scientific versus general corpus: C w = f w,scientif ic /f w,general . The lexical complexity, L, of a word is calculated as the word's character length, and the final complexity of the word as C w \u00d7 L w .",
  "y": "similarities"
 },
 {
  "id": "1527ce2786adfe0decf8c926a3d846_5",
  "x": "We require that the final complexity score of the first word in the rule be greater than the second. While this simplicity filter has been shown to work well in general corpora<cite> (Biran et al., 2011)</cite> , it is sensitive to very small differences in the frequencies with which both words appear in the corpora. This is problematic given the distribution of terms in our corpora, where many rarer scientific terms may appear in small numbers in both corpora.",
  "y": "differences"
 },
 {
  "id": "1527ce2786adfe0decf8c926a3d846_6",
  "x": "In prior context-aware simplification systems, the decision of whether to apply a simplification rule in an input sentence is complex, involving several similarity operations on word co-occurrence matrices<cite> (Biran et al., 2011)</cite> or using embeddings to incorporate co-occurrence context for pairs generated using other means (Paetzold and Specia, 2015) . However, the SimpleScience pipline already considers the context of appearance for each word in deriving simplifications via word embeddings learned from a large corpus. We see no additional improvements in F-measure when we apply two variants of context similarity thresholds to decide whether to apply a rule to an input sentence.",
  "y": "background"
 },
 {
  "id": "1527ce2786adfe0decf8c926a3d846_7",
  "x": "The first is the cosine similarity between the distributed representation of the simple word and the sum of the distributed representations of all words within a window l surrounding the complex word in the input sentence (Paetzold and Specia, 2015) . The second is the cosine similarity of a minimum shared frequency co-occurrence matrix for the words in the pair and the co-occurrence matrix for the input sentence<cite> (Biran et al., 2011)</cite> . In fully automated applications, the top rule from the ranked candidate rules is used.",
  "y": "background"
 },
 {
  "id": "1527ce2786adfe0decf8c926a3d846_8",
  "x": "Following the evaluation method used in Paetzold and Specia (2015), we calculate potential as the proportion of instances for which at least one of the substitutions generated is present in the gold standard set, precision as the proportion of generated instances which are present in the gold standard set, and F-measure as their harmonic mean. Our SimpleScience approach outperforms the original approach by<cite> Biran et al. (2011)</cite> applied to the Wikipedia and SEW corpus as well as to the scientific corpus (Table 1) . ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "1527ce2786adfe0decf8c926a3d846_9",
  "x": "Adding techniques to filter antonym rules, such as using co-reference chains<cite> (Adel and Sch\u00fctze, 2014)</cite> , is important in future work. We achieve a precision of 0.389 at the top slot on our SimpleSciGold standard set when we apply our generation method and rank candidates by cosine similarity. This level of precision is higher than that achieved by various prior ranking methods used in Lexenstein (Paetzold and Specia, 2015) , with the exception of using machine learning techniques like SVM (Paetzold and Specia, 2015) .",
  "y": "future_work"
 },
 {
  "id": "1540b0b172971ac75771b414765f1d_0",
  "x": "Resources for historical text normalization are scarce. Even for major languages like English and German, we have very little training data for inducing normalization models, and the models we induce may be very specific to these datasets and not scale to writings from other historic periodsor even just writings from another monastery or by another author. Bollmann and S\u00f8gaard (2016) and <cite>Bollmann et al. (2017)</cite> recently showed that we can obtain more robust historical text normalization models by exploiting synergies across historical text normalization datasets and with related tasks.",
  "y": "background"
 },
 {
  "id": "1540b0b172971ac75771b414765f1d_1",
  "x": "Even for major languages like English and German, we have very little training data for inducing normalization models, and the models we induce may be very specific to these datasets and not scale to writings from other historic periodsor even just writings from another monastery or by another author. Bollmann and S\u00f8gaard (2016) and <cite>Bollmann et al. (2017)</cite> recently showed that we can obtain more robust historical text normalization models by exploiting synergies across historical text normalization datasets and with related tasks. Specifically, <cite>Bollmann et al. (2017)</cite> showed that multitask learning with German grapheme-to-phoneme translation as an auxiliary task improves a stateof-the-art sequence-to-sequence model for historical text normalization of medieval German manuscripts.",
  "y": "background"
 },
 {
  "id": "1540b0b172971ac75771b414765f1d_2",
  "x": "---------------------------------- **DATASETS** We consider 10 datasets from 8 different languages: German, using the <cite>Anselm dataset</cite> (taken from <cite>Bollmann et al., 2017</cite>) and texts from the RIDGES corpus (Odebrecht et al., 2016) <cite>Bollmann et al. (2017)</cite> to obtain a single dataset.",
  "y": "uses"
 },
 {
  "id": "1540b0b172971ac75771b414765f1d_3",
  "x": "Specifically, we evaluate a state-ofthe-art approach to historical text normalization <cite>(Bollmann et al., 2017)</cite> with and without various auxiliary tasks, across 10 historical text normalization datasets. We also include an experiment in English historical text normalization using data from Twitter and a grammatical error correction corpus (FCE) as auxiliary datasets. Across the board, we find that, unlike what has been observed for other NLP tasks, multi-task learning only helps when target task data is scarce.",
  "y": "uses motivation"
 },
 {
  "id": "1540b0b172971ac75771b414765f1d_4",
  "x": "Model We use the same encoder-decoder architecture with attention as described in <cite>Bollmann et al. (2017)</cite> . 4 This is a fairly standard model consisting of one bidirectional LSTM unit in the encoder and one (unidirectional) LSTM unit in the decoder. The input for the encoder is a single historical word form represented as a sequence of characters and padded with word boundary symbols; i.e., we only input single tokens in isolation, not full sentences.",
  "y": "uses"
 },
 {
  "id": "1540b0b172971ac75771b414765f1d_5",
  "x": "The hyperparameters were set on a randomly selected subset of 50,000 tokens from each of the following datasets: English, German (<cite>Anselm</cite>), Hungarian, Icelandic, and Slovene (Gaj). <cite>Bollmann et al. (2017)</cite> also describe a multi-task learning (MTL) scenario where the encoder-decoder model is trained on two datasets in parallel. We perform similar experiments on pairwise combinations of our datasets.",
  "y": "uses"
 },
 {
  "id": "1540b0b172971ac75771b414765f1d_6",
  "x": "Training is done on mini-batches of 50 samples with early stopping based on validation on the individual development sets. The hyperparameters were set on a randomly selected subset of 50,000 tokens from each of the following datasets: English, German (<cite>Anselm</cite>), Hungarian, Icelandic, and Slovene (Gaj). <cite>Bollmann et al. (2017)</cite> also describe a multi-task learning (MTL) scenario where the encoder-decoder model is trained on two datasets in parallel.",
  "y": "background"
 },
 {
  "id": "1540b0b172971ac75771b414765f1d_7",
  "x": "The hyperparameters were set on a randomly selected subset of 50,000 tokens from each of the following datasets: English, German (<cite>Anselm</cite>), Hungarian, Icelandic, and Slovene (Gaj). <cite>Bollmann et al. (2017)</cite> also describe a multi-task learning (MTL) scenario where the encoder-decoder model is trained on two datasets in parallel. We perform similar experiments on pairwise combinations of our datasets.",
  "y": "similarities"
 },
 {
  "id": "1540b0b172971ac75771b414765f1d_8",
  "x": "There is a wide range of design questions and sharing strategies that we ignore here, focusing instead on under what circumstances the approach advocated in <cite>(Bollmann et al., 2017)</cite> works. Our main observation-that the size of the target dataset is most predictive of multi-task learning gains-runs counter previous findings for other NLP tasks (Mart\u00ednez Alonso and Plank, 2017; Bingel and S\u00f8gaard, 2017) . Mart\u00ednez Alonso and Plank (2017) find that the label entropy of the auxiliary dataset is more predictive; Bingel and S\u00f8-gaard (2017) find that the relative differences in the steepness of the two single-task loss curves is more predictive.",
  "y": "uses"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_0",
  "x": "LR-decoding algorithms exist for phrasebased (Koehn, 2004; Galley and Manning, 2010) and syntax-based (Huang and Mi, 2010; Feng et al., 2012 ) models and also for hierarchical phrasebased models (Watanabe et al., 2006; <cite>Siahbani et al., 2013</cite>) , which is our focus in this paper. Watanabe et al. (2006) first proposed left-toright (LR) decoding for Hiero (LR-Hiero henceforth) which uses beam search and runs in O(n 2 b) in practice where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010) . To simplify target generation, SCFG rules are constrained to be prefix-lexicalized on target side aka Griebach Normal Form (GNF).",
  "y": "uses background"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_1",
  "x": "Throughout this paper we abuse the notation for simplicity and use the term GNF grammars for such SCFGs. This constraint drastically reduces the size of grammar for LR-Hiero in comparison to Hiero grammar (<cite>Siahbani et al., 2013</cite>) . However, the original LR-Hiero decoding algorithm does not perform well in comparison to current state-of-the-art Hiero and phrase-based translation systems.",
  "y": "uses"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_2",
  "x": "To simplify target generation, SCFG rules are constrained to be prefix-lexicalized on target side aka Griebach Normal Form (GNF). Throughout this paper we abuse the notation for simplicity and use the term GNF grammars for such SCFGs. This constraint drastically reduces the size of grammar for LR-Hiero in comparison to Hiero grammar (<cite>Siahbani et al., 2013</cite>) .",
  "y": "background"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_3",
  "x": "However, the original LR-Hiero decoding algorithm does not perform well in comparison to current state-of-the-art Hiero and phrase-based translation systems. <cite>Siahbani et al. (2013)</cite> propose an augmented version of LR decoding to address some limitations in the original LR-Hiero algorithm in terms of translation quality and time efficiency. Although, LR-Hiero performs much faster than Hiero in decoding and obtains BLEU scores comparable to phrase-based translation system on some language pairs, there is still a notable gap between CKY-Hiero and LR-Hiero (<cite>Siahbani et al., 2013</cite>) .",
  "y": "background"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_4",
  "x": "Although, LR-Hiero performs much faster than Hiero in decoding and obtains BLEU scores comparable to phrase-based translation system on some language pairs, there is still a notable gap between CKY-Hiero and LR-Hiero (<cite>Siahbani et al., 2013</cite>) . We show in this paper using instructive examples that CKY-Hiero can capture some complex phrasal re-orderings that are observed in language pairs such as Chinese-English that LR-Hiero cannot (c.f. Sec.3). We introduce two improvements to LR decoding of GNF grammars: (1) We add queue diversity to the <cite>cube pruning algorithm for LR-Hiero</cite>, and (2) We extend the LR-Hiero decoder to capture all the hierarchical phrasal alignments that are reachable in CKY-Hiero (restricted to using GNF grammars).",
  "y": "extends uses"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_5",
  "x": "However, the original LR-Hiero decoding algorithm does not perform well in comparison to current state-of-the-art Hiero and phrase-based translation systems. <cite>Siahbani et al. (2013)</cite> propose an augmented version of LR decoding to address some limitations in the original LR-Hiero algorithm in terms of translation quality and time efficiency. Although, LR-Hiero performs much faster than Hiero in decoding and obtains BLEU scores comparable to phrase-based translation system on some language pairs, there is still a notable gap between CKY-Hiero and LR-Hiero (<cite>Siahbani et al., 2013</cite>) .",
  "y": "uses"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_6",
  "x": "This search is integrated with beam search or cube pruning to find the k-best translations. Algorithm 1 shows the pseudocode for LRHiero decoding with cube pruning (Chiang, 2007) (CP). LR-Hiero with CP was introduced in (<cite>Siahbani et al., 2013</cite>) .",
  "y": "extends background"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_7",
  "x": "d=1 in standard cube pruning for LR-Hiero (<cite>Siahbani et al., 2013</cite>) . We apply the idea of diversity at queue level, before generating K best hypothesis, such that the GetBestHypotheses routine generates d best hypotheses from each cube and all these hypotheses are pushed to the priority queue (line 22-23). We fill each stack differently from CKY-Hiero and so queue diversity is different from lazy cube pruning (Pust and Knight, 2009) or cube growing (Huang and Chiang, 2007; Vilar and Ney, 2009; Xu and Koehn, 2012) .",
  "y": "uses"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_8",
  "x": "We use a 5-gram LM trained on the Gigaword corpus and use KenLM (Heafield, 2011) . We tune weights by minimizing BLEU loss on the dev set through MERT (Och, 2003) and report BLEU scores on the test set. Pop limit for Hiero and <cite>LRHiero+CP</cite> is 500 and beam size LR-Hiero is 500.",
  "y": "uses background"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_9",
  "x": "We extend the LR-Hiero decoder to handle such cases by making the GNF grammar more expressive. Pop limit for Hiero and <cite>LRHiero+CP</cite> is 500 and beam size LR-Hiero is 500. Other extraction and decoder settings such as maximum phrase length, etc. were identical across settings.",
  "y": "uses"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_10",
  "x": "We tune weights by minimizing BLEU loss on the dev set through MERT (Och, 2003) and report BLEU scores on the test set. Pop limit for Hiero and <cite>LRHiero+CP</cite> is 500 and beam size LR-Hiero is 500. Other extraction and decoder settings such as maximum phrase length, etc. were identical across settings.",
  "y": "uses background"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_11",
  "x": "Other extraction and decoder settings such as maximum phrase length, etc. were identical across settings. To make the results comparable we use the same feature set for all baselines, Hiero as well (including new features proposed by (<cite>Siahbani et al., 2013</cite>) ). We use 3 baselines: (i) our implementation of (Watanabe et al., 2006) : LR-Hiero with beam search (LR-Hiero) and (ii) LR-Hiero with cube pruning (<cite>Siahbani et al., 2013</cite>) : (<cite>LR-Hiero+CP</cite>); and (iii) Kriya, an open-source implementation of Hiero in Python, which performs comparably to other open-source Hiero systems (Sankaran et al., 2012) .",
  "y": "similarities uses"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_12",
  "x": "To make the results comparable we use the same feature set for all baselines, Hiero as well (including new features proposed by (<cite>Siahbani et al., 2013</cite>) ). We use 3 baselines: (i) our implementation of (Watanabe et al., 2006) : LR-Hiero with beam search (LR-Hiero) and (ii) LR-Hiero with cube pruning (<cite>Siahbani et al., 2013</cite>) : (<cite>LR-Hiero+CP</cite>); and (iii) Kriya, an open-source implementation of Hiero in Python, which performs comparably to other open-source Hiero systems (Sankaran et al., 2012) . Table 3 shows model sizes for LR-Hiero (GNF) and Hiero (SCFG).",
  "y": "uses"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_13",
  "x": "Table 2a shows the translation quality of different systems in terms of BLEU score. Row 3 is from (<cite>Siahbani et al., 2013</cite>) 5 . As we discussed in Section 2, <cite>LR-Hiero+CP</cite> suffers from severe search errors on Zh-En (1.5 BLEU) but using queue diversity (QD=15) we fill this gap. We use the same QD(=15) in next rows for Zh-en.",
  "y": "uses differences motivation"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_14",
  "x": "Table 2a shows the translation quality of different systems in terms of BLEU score. Row 3 is from (<cite>Siahbani et al., 2013</cite>) 5 . As we discussed in Section 2, <cite>LR-Hiero+CP</cite> suffers from severe search errors on Zh-En (1.5 BLEU) but using queue diversity (QD=15) we fill this gap. We achieve better results than <cite>our previous work</cite> (<cite>Siahbani et al., 2013</cite>) type (c) rules.",
  "y": "differences"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_15",
  "x": "For Cs-En and De-En we use regular cube pruning (QD=1), as it works as well as beam search (compare rows 4 and 2). We measure the benefit of the new modified rules from Section 3: (ab): adding modifications for rules type (a) and (b); (abc): modification of all rules. We can see that for all language pairs (ab) constantly improves performance of LRHiero, significantly better than <cite>LR-Hiero+CP</cite> and LR-Hiero (p-value<0.05) on Cs-En and Zh-En, evaluated by MultEval (Clark et al., 2011) .",
  "y": "differences"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_16",
  "x": "Table 2a shows the translation quality of different systems in terms of BLEU score. Row 3 is from (<cite>Siahbani et al., 2013</cite>) 5 . As we discussed in Section 2, <cite>LR-Hiero+CP</cite> suffers from severe search errors on Zh-En (1.5 BLEU) but using queue diversity (QD=15) we fill this gap. Row 4 is the same translation system as row 3 (<cite>LR-Hiero+CP</cite>).",
  "y": "differences"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_17",
  "x": "We can see that for all language pairs (ab) constantly improves performance of LRHiero, significantly better than <cite>LR-Hiero+CP</cite> and LR-Hiero (p-value<0.05) on Cs-En and Zh-En, evaluated by MultEval (Clark et al., 2011) . But modifying rule type (c) does not show any improvement due to spurious ambiguity created by 5 We report results on Cs-En and De-En in (<cite>Siahbani et al., 2013</cite>) . Row 4 is the same translation system as row 3 (<cite>LR-Hiero+CP</cite>).",
  "y": "similarities uses"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_18",
  "x": "Table 2a shows the translation quality of different systems in terms of BLEU score. We can see that for all language pairs (ab) constantly improves performance of LRHiero, significantly better than <cite>LR-Hiero+CP</cite> and LR-Hiero (p-value<0.05) on Cs-En and Zh-En, evaluated by MultEval (Clark et al., 2011) . But modifying rule type (c) does not show any improvement due to spurious ambiguity created by 5 We report results on Cs-En and De-En in (<cite>Siahbani et al., 2013</cite>) .",
  "y": "similarities differences"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_19",
  "x": "Row 4 is the same translation system as row 3 (<cite>LR-Hiero+CP</cite>). We achieve better results than <cite>our previous work</cite> (<cite>Siahbani et al., 2013</cite>) type (c) rules. Figure 2b shows the results in terms of average number of language model queries on a sample set of 50 sentences from test sets.",
  "y": "differences"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_20",
  "x": "Figure 2b shows the results in terms of average number of language model queries on a sample set of 50 sentences from test sets. All of the baselines use the same wrapper to KenLM (Heafield, 2011) to query the language model, and we have instrumented the wrapper to count the statistics. In (<cite>Siahbani et al., 2013</cite>) we discuss that LR-Hiero with beam search (Watanabe et al., 2006) does not perform at the same level of state-of-the-art Hiero (more LM calls and less translation quality).",
  "y": "background"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_21",
  "x": "In (<cite>Siahbani et al., 2013</cite>) we discuss that LR-Hiero with beam search (Watanabe et al., 2006) does not perform at the same level of state-of-the-art Hiero (more LM calls and less translation quality). As we can see in this figure, adding new modified rules slightly increases the number of language model queries on Cs-En and De-En so that <cite>LR-Hiero+CP</cite> still works 2 to 3 times faster than Hiero. On Zh-En, <cite>LR-Hiero+CP</cite> applies queue diversity (QD=15) which reduces search errors and improves translation quality but increases the number of hypothesis generation as well.",
  "y": "uses"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_22",
  "x": "As we can see in this figure, adding new modified rules slightly increases the number of language model queries on Cs-En and De-En so that <cite>LR-Hiero+CP</cite> still works 2 to 3 times faster than Hiero. On Zh-En, <cite>LR-Hiero+CP</cite> applies queue diversity (QD=15) which reduces search errors and improves translation quality but increases the number of hypothesis generation as well. <cite>LRHiero+CP</cite> with our modifications works substantially faster than LR-Hiero while obtain significantly better translation quality on Zh-En.",
  "y": "differences"
 },
 {
  "id": "1542325bbf9bed87c22d34d12ee40e_23",
  "x": "In (<cite>Siahbani et al., 2013</cite>) we discuss that LR-Hiero with beam search (Watanabe et al., 2006) does not perform at the same level of state-of-the-art Hiero (more LM calls and less translation quality). As we can see in this figure, adding new modified rules slightly increases the number of language model queries on Cs-En and De-En so that <cite>LR-Hiero+CP</cite> still works 2 to 3 times faster than Hiero. On Zh-En, <cite>LR-Hiero+CP</cite> applies queue diversity (QD=15) which reduces search errors and improves translation quality but increases the number of hypothesis generation as well.",
  "y": "differences"
 },
 {
  "id": "155920441b8e81dff4e2b8e110383d_0",
  "x": "Here, we also try to mimic the word2vec <cite>(Mikolov et al., 2013)</cite> embeddings (i.e. that are the expected outputs of the model) to learn the rare word representations with a complex morphology. Our model shows some architectural similarities to that of Cao and Rei (2016) . Both models use the attention mechanism to up-weight the correct morphological segmentation of a word.",
  "y": "uses"
 },
 {
  "id": "155920441b8e81dff4e2b8e110383d_1",
  "x": "Classical word representation models such as word2vec <cite>(Mikolov et al., 2013)</cite> have been successful in learning word representations for frequent words. Since these classical models are based on collecting contextual information in a very large corpus, they estimate deficient word representations for rare words due to insufficient contextual information. This has a negative consequence in some natural language processing tasks that make use of the word representations.",
  "y": "background"
 },
 {
  "id": "155920441b8e81dff4e2b8e110383d_2",
  "x": "Lazaridou et al. (2013) apply compositional methods by having the stem and affix representations in order to estimate the distributional representation of morphologically complex words. Bojanowski et al. (2017) introduce an extension to word2vec <cite>(Mikolov et al., 2013)</cite> by representing each word in terms of the vector representations of its n-grams, which was earlier applied by Sch\u00fctze (1993) that learns the representations of fourgrams by applying singular value decomposition (SVD). Analogously, Alexandrescu and Kirchhoff (2006) represent each character n-gram with a vector representation and words are estimated by the summation of the subword representations.",
  "y": "background"
 },
 {
  "id": "155920441b8e81dff4e2b8e110383d_3",
  "x": "W \u00b7v s i denotes the corresponding column in the weight matrix of the feed-forward layer in the attention. For training, we use the pre-trained word2vec <cite>(Mikolov et al., 2013)</cite> vectors in order to minimize the cost between the learned and pre-trained vectors with the following objective function: where h(w k ) is the cost for the kth word w k in a training set of size N with a L2 regularization term on the model parameters \u03b8.",
  "y": "uses"
 },
 {
  "id": "155920441b8e81dff4e2b8e110383d_4",
  "x": "The output of the Bi-LSTMs is reduced to half after feeding the output through a feed-forward layer that results with a word vector dimension of d word = 300. Our model is implemented in Keras, and publicly available 3 . For the pre-trained word vectors, we used the word vectors of dimension 300 that were obtained by training word2vec <cite>(Mikolov et al., 2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "155920441b8e81dff4e2b8e110383d_6",
  "x": "The results are given in Table 3 . English words mostly do not involve any suffixes, which hinders our model's performance. However, our model performs better than both fasttext (Bojanowski et al., 2017) and word2vec <cite>(Mikolov et al., 2013)</cite> on Turkish despite the highly agglutinative morphological structure of the language.",
  "y": "differences"
 },
 {
  "id": "155920441b8e81dff4e2b8e110383d_7",
  "x": "Here, we tested only the syntactic analogy on a list of word tuples since our focus is especially morphologically complex languages. For English, we used the syntactic relations section provided in the Google analogy dataset <cite>(Mikolov et al., 2013)</cite> that involves 10675 questions. Since there is no analogy dataset for Turkish, we prepared a Turkish analogy set SynAnalogyTr 8 with 206 syntactic questions that involves inflected word forms.",
  "y": "uses"
 },
 {
  "id": "155920441b8e81dff4e2b8e110383d_8",
  "x": "The results are given in Table 6 and Table 7 for English and Turkish. The results show that our model outperforms both word2vec <cite>(Mikolov et al., 2013)</cite> and fasttext (Bojanowski et al., 2017) on both Turkish and English languages. Additionally, some examples to analogy results are given in Table 9 and the nearest neighbors of the Turkish word kitap-lar-dan-m\u0131\u015f (it was from the books) are given in Table 8 .",
  "y": "differences"
 },
 {
  "id": "155920441b8e81dff4e2b8e110383d_11",
  "x": "Our morpheme-based model morph2vec learns better word representations for morphologically complex words compared to the word-based model word2vec <cite>(Mikolov et al., 2013)</cite> , character-based model char2vec (Cao and Rei, 2016) , and the character n-gram level model fasttext (Bojanowski et al., 2017) . Our results are also competitive for the English language. We leave other languages and experiments such as morphological segmentation task for the future work.",
  "y": "differences"
 },
 {
  "id": "15bacab4a8c520cfcdd7e7bd1e9ec5_1",
  "x": "We will introduce an in-depth case study of Generative Adversarial Networks for NLP, with a focus on dialogue generation <cite>(Li et al., 2017)</cite> . This tutorial aims at introducing deep adversarial learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in adversarial learning.",
  "y": "background"
 },
 {
  "id": "15bacab4a8c520cfcdd7e7bd1e9ec5_2",
  "x": "We then focus on introducing Seq-GAN (Yu et al., 2017) , an early solution of textual models of GAN, with a focus on policy gradient and Monte Carlo Tree Search. Finally, we provide an in-depth case study of deploying two-agent GAN models for conversational AI <cite>(Li et al., 2017)</cite> . We will summarize the lessons learned, and how we can move forward to investigate game-theoretical approaches in advancing NLP problems.",
  "y": "uses"
 },
 {
  "id": "15c8ca572430c214d9c571fbe0db95_0",
  "x": "**INTRODUCTION** In recent years, phrase-based systems for statistical machine translation (Och et al., 1999; Koehn et al., 2003; Venugopal et al., 2003) have delivered state-of-the-art performance on standard translation tasks. In this paper, we present a phrase-based unigram system similar to the one in (<cite>Tillmann and Xia, 2003</cite>) , which is extended by an unigram orientation model.",
  "y": "similarities"
 },
 {
  "id": "15c8ca572430c214d9c571fbe0db95_1",
  "x": "Enumeration does not allow us to capture position dependent distortion probabilities, but we can compute statistics about adjacent block predecessors. Our baseline model is the unigram monotone model described in (<cite>Tillmann and Xia, 2003</cite>) . Here, we select blocks from word-aligned training data and unigram block occurrence counts 0 \u00a1 \u00a8 a re computed: all blocks for a training sentence pair are enumerated in some order and we count how often a given block occurs in the parallel training data \u00a1 of the predecessor is ignored.",
  "y": "uses"
 },
 {
  "id": "15c8ca572430c214d9c571fbe0db95_2",
  "x": "Using fewer 'bigger' blocks to carry out the translation generally seems to improve translation performance. Since normalization does not influence the number of blocks used to carry out the translation, it might be less important for our segmentation model. We use a DP-based beam search procedure similar to the one presented in (<cite>Tillmann and Xia, 2003</cite>) .",
  "y": "similarities"
 },
 {
  "id": "15c8ca572430c214d9c571fbe0db95_3",
  "x": "The training data comes from the UN news sources: . This is the model presented in (<cite>Tillmann and Xia, 2003</cite>) . For the ) model, the sentence is translated mostly monotonously, and only neighbor blocks are allowed to be swapped (at most`block is skipped).",
  "y": "uses"
 },
 {
  "id": "15df1d107fb349f78c313b0c3342b8_0",
  "x": "The system that we propose builds on top of one of the latest neural MT architectures called the Transformer <cite>(Vaswani et al., 2017)</cite> . This architecture is an encoderdecoder structure which uses attention-based mechanisms as an alternative to recurrent neural networks proposed in initial architectures (Sutskever et al., 2014; Cho et al., 2014) . This new architecture has been proven more efficient and better than all previous proposed so far <cite>(Vaswani et al., 2017)</cite> .",
  "y": "extends"
 },
 {
  "id": "15df1d107fb349f78c313b0c3342b8_1",
  "x": "This section provides a brief high-level explanation of the neural MT approach that we are using as a baseline system, which is one of the strongest systems presented recently <cite>(Vaswani et al., 2017)</cite> , as well as a glance of its differences with other popular neural machine translation architectures. Sequence-to-sequence recurrent models (Sutskever et al., 2014; Cho et al., 2014) have been the standard approach for neural machine translation, especially since the incorporation of attention mechanisms Luong et al., 2015) , which enables the system to learn to identify the information which is relevant for producing each word in the translation. Convolutional networks (Gehring et al., 2017) were the second paradigm to effectively approach sequence transduction tasks like machine translation.",
  "y": "background motivation"
 },
 {
  "id": "15df1d107fb349f78c313b0c3342b8_2",
  "x": "In this paper we make use of the third paradigm for neural machine translation, proposed in <cite>(Vaswani et al., 2017)</cite> , namely the Transformer architecture, which is based on a feed-forward encoder-decoder scheme with attention mechanisms. The type of attention mechanism used in the system, referred to as multi-head attention, allows to train several attention modules in parallel, combining also self-attention with standard attention. Self-attention differs from standard attention in the use of the same sentence as input and trains over it allowing to solve issues as coreference resolution.",
  "y": "uses"
 },
 {
  "id": "15df1d107fb349f78c313b0c3342b8_3",
  "x": "Self-attention differs from standard attention in the use of the same sentence as input and trains over it allowing to solve issues as coreference resolution. Equations and details about the transformer system can be found in the original paper <cite>(Vaswani et al., 2017)</cite> and are out of the scope of this paper. For the definition of the vocabulary to be used as input for the neural network, we used the sub-word mechanism from tensor2tensor package, which is similar to BytePair Encoding (BPE) from (Sennrich et al., 2016) .",
  "y": "background"
 },
 {
  "id": "167511f278a8596aed0124c3a4242b_0",
  "x": "Current Simultaneous Neural Machine Translation (SNMT) systems (Satija and Pineau, 2016; Cho and Esipova, 2016; <cite>Gu et al., 2017)</cite> use an AGENT to control an incremental encoder-decoder (or sequence to sequence) NMT model. Each READ adds more information to the encoder RNN, and each WRITE produces more output using the decoder RNN. In this paper, we propose adding a new action to the AGENT: a PREDICT action that predicts what words might appear in the input stream.",
  "y": "background"
 },
 {
  "id": "167511f278a8596aed0124c3a4242b_1",
  "x": "Current Simultaneous Neural Machine Translation (SNMT) systems (Satija and Pineau, 2016; Cho and Esipova, 2016; <cite>Gu et al., 2017)</cite> use an AGENT to control an incremental encoder-decoder (or sequence to sequence) NMT model. In this paper, we propose adding a new action to the AGENT: a PREDICT action that predicts what words might appear in the input stream.",
  "y": "extends"
 },
 {
  "id": "167511f278a8596aed0124c3a4242b_2",
  "x": "**SIMULTANEOUS TRANSLATION FRAMEWORK** An agent-based framework whose actions decide whether to translate or wait for more input is a natural way to extend neural MT to simultaneous neural MT and has been explored in (Satija and Pineau, 2016; <cite>Gu et al., 2017)</cite> which contains two main components: The ENVIRONMENT which receives the input words X = {x 1 , . . . , x N } from the source language and incrementally generates translated words W = {w 1 , . . . , w M } in the target language; And the AGENT which decides an action for each time step, a t . The AGENT generates an action sequence A = {a 1 , . . . , a T } to control the ENVIRONMENT.",
  "y": "background"
 },
 {
  "id": "167511f278a8596aed0124c3a4242b_3",
  "x": "**AGENT** The AGENT is a separate component which examines the ENVIRONMENT at each time step and decides on the actions that lead to better translation quality and lower delay. The agent in the greedy decoding framework<cite> (Gu et al., 2017)</cite> was trained using reinforcement learning with the policy gradient algorithm (Williams, 1992) , which observes the current state of the ENVIRONMENT at time step t as o t where o t = [c t ; s t ; w m ].",
  "y": "uses"
 },
 {
  "id": "167511f278a8596aed0124c3a4242b_4",
  "x": "Where s(t) denotes the number of source words the WRITE action uses at time step t (for any other actions, s(t) would be zero). The delay reward is smoothed using a Target Delay which is a scalar constant denoted by d \u21e4<cite> (Gu et al., 2017)</cite> : \u21e4 c Prediction Rewards for Quality and Delay alone do not motivate the AGENT to choose prediction and in preliminary experiments, after a number of steps, the number of prediction actions became zero.",
  "y": "uses"
 },
 {
  "id": "167511f278a8596aed0124c3a4242b_5",
  "x": "The final reward function is calculated as the combination of quality, delay, and prediction rewards: (1) The trade-off between better translation quality and minimal delay is achieved by modifying the parameters \u21b5, , and . Reinforcement Learning is used to train the AGENT using a policy gradient algorithm<cite> (Gu et al., 2017</cite>; Williams, 1992) which searches for the maximum in",
  "y": "uses"
 },
 {
  "id": "167511f278a8596aed0124c3a4242b_6",
  "x": "We use WMT 2015 for training and Newstest 2013 for validation and testing. All sentences have been tokenized and the words are segmented using byte pair encoding (BPE) (Sennrich et al., 2016 Model Configuration For a fair comparison, we follow the settings that worked the best for the greedy decoding model in<cite> (Gu et al., 2017)</cite> and set the target delay d \u21e4 for the AGENT to 0.7. The EN-VIRONMENT consists of two unidirectional layers with 1028 GRU units for encoder and decoder.",
  "y": "uses"
 },
 {
  "id": "167511f278a8596aed0124c3a4242b_7",
  "x": "We modified the SNMT trainable agent in<cite> (Gu et al., 2017)</cite> and added a new non-trivial PREDICT action to the agent. We compare to their model and show better results in delay and quality. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "16780bd3c2b350f6d61f2f55f9f88c_0",
  "x": "For our study, we use a small corpus of Enron email threads which has been previously annotated with dialog acts <cite>(Hu et al., 2009</cite> ). The corpus contains 122 email threads with 360 messages, 1734 utterances and 20,740 word tokens. We trained an annotator using the definition for ODP given in Section 3. She was given full email threads whose messages were already segmented into utterances.",
  "y": "uses"
 },
 {
  "id": "16780bd3c2b350f6d61f2f55f9f88c_1",
  "x": "An utterance has one of 5 dialog acts: RequestAction, RequestInformation, Inform, Commit and Conventional (see <cite>(Hu et al., 2009</cite> ) for details). For example, for utterance s2, FV would be 'need' and DA would be 'Inform'. 3 ----------------------------------",
  "y": "background"
 },
 {
  "id": "16780bd3c2b350f6d61f2f55f9f88c_2",
  "x": "We use the manual gold dialog act annotations present in our corpus, which use a very small dialog act tag set. An utterance has one of 5 dialog acts: RequestAction, RequestInformation, Inform, Commit and Conventional (see <cite>(Hu et al., 2009</cite> ) for details). For example, for utterance s2, FV would be 'need' and DA would be 'Inform'. 3",
  "y": "uses background"
 },
 {
  "id": "16780bd3c2b350f6d61f2f55f9f88c_3",
  "x": "**NOT USING GOLD DIALOG ACTS** We also evaluate the performance of our ODP tagger without using gold DA tags. We instead use the DA tagger of<cite> Hu et al. (2009)</cite> , which we re-trained using the training sets for each of our cross validation folds, applying it to the test set of that fold.",
  "y": "uses"
 },
 {
  "id": "17252628fa9c03c2fe0b44763fc7a2_0",
  "x": "Then, syntactic reordering rules are applied to these parse trees with the goal of reordering the source language sentences into the word order of the target language. Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004) , German-English (Collins et al., 2005) , Chinese-English <cite>(Wang et al., 2007</cite>; Zhang et al., 2008) , and English-Japanese (Lee et al., 2010) . As a kind of constituent structure, HPSG (Pollard and Sag, 1994) parsing-based pre-ordering showed improvements in SVO-SOV translations, such as English-Japanese (Isozaki et al., 2010; Wu et al., 2011) and Chinese-Japanese (Han et al., 2012) .",
  "y": "background"
 },
 {
  "id": "17252628fa9c03c2fe0b44763fc7a2_1",
  "x": "As a kind of constituent structure, HPSG (Pollard and Sag, 1994) parsing-based pre-ordering showed improvements in SVO-SOV translations, such as English-Japanese (Isozaki et al., 2010; Wu et al., 2011) and Chinese-Japanese (Han et al., 2012) . Since dependency parsing is more concise than constituent parsing in describing sentences, some research has used dependency parsing in pre-ordering approaches for language pairs such as Arabic-English (Habash, 2007) , and English-SOV languages (Xu et al., 2009; Katz-Brown et al., 2011) . The pre-ordering rules can be made manually (Collins et al., 2005;<cite> Wang et al., 2007</cite>; Han et al., 2012) or extracted automatically from a parallel corpus (Xia and McCord, 2004; Habash, 2007; Zhang et al., 2007; Wu et al., 2011) .",
  "y": "background"
 },
 {
  "id": "17252628fa9c03c2fe0b44763fc7a2_2",
  "x": "Since dependency parsing is more concise than constituent parsing in describing sentences, some research has used dependency parsing in pre-ordering approaches for language pairs such as Arabic-English (Habash, 2007) , and English-SOV languages (Xu et al., 2009; Katz-Brown et al., 2011) . The pre-ordering rules can be made manually (Collins et al., 2005;<cite> Wang et al., 2007</cite>; Han et al., 2012) or extracted automatically from a parallel corpus (Xia and McCord, 2004; Habash, 2007; Zhang et al., 2007; Wu et al., 2011) . The purpose of this paper is to introduce a novel dependency-based pre-ordering approach through creating a pre-ordering rule set and applying it to the Chinese-English PBSMT system.",
  "y": "background"
 },
 {
  "id": "17252628fa9c03c2fe0b44763fc7a2_3",
  "x": "Since dependency parsing is more concise than constituent parsing in describing sentences, some research has used dependency parsing in pre-ordering approaches for language pairs such as Arabic-English (Habash, 2007) , and English-SOV languages (Xu et al., 2009; Katz-Brown et al., 2011) . The pre-ordering rules can be made manually (Collins et al., 2005;<cite> Wang et al., 2007</cite>; Han et al., 2012) or extracted automatically from a parallel corpus (Xia and McCord, 2004; Habash, 2007; Zhang et al., 2007; Wu et al., 2011) . The purpose of this paper is to introduce a novel dependency-based pre-ordering approach through creating a pre-ordering rule set and applying it to the Chinese-English PBSMT system.",
  "y": "motivation"
 },
 {
  "id": "17252628fa9c03c2fe0b44763fc7a2_4",
  "x": "Experiment results showed that our pre-ordering rule set improved the BLEU score on the NIST 2006 evaluation data by 1.61. Moreover, this rule set substantially decreased the total times of rule application about 60%, compared with a constituent-based approach<cite> (Wang et al., 2007)</cite> . We also conducted human evaluations in order to assess its accuracy.",
  "y": "differences"
 },
 {
  "id": "17252628fa9c03c2fe0b44763fc7a2_5",
  "x": "To our knowledge, our manually created pre-ordering rule set is the first Chinese-English dependencybased pre-ordering rule set. The most similar work to this paper is that of<cite> Wang et al. (2007)</cite> . They created a set of preordering rules for constituent parsers for ChineseEnglish PBSMT.",
  "y": "similarities"
 },
 {
  "id": "17252628fa9c03c2fe0b44763fc7a2_6",
  "x": "They created a set of preordering rules for constituent parsers for ChineseEnglish PBSMT. In contrast, we propose a set of pre-ordering rules for dependency parsers. We argue that even though the rules by<cite> Wang et al. (2007)</cite> exist, it is almost impossible to automatically convert their rules into rules that are applicable to dependency parsers.",
  "y": "motivation"
 },
 {
  "id": "17252628fa9c03c2fe0b44763fc7a2_7",
  "x": "The most similar work to this paper is that of<cite> Wang et al. (2007)</cite> . They created a set of preordering rules for constituent parsers for ChineseEnglish PBSMT. In contrast, we propose a set of pre-ordering rules for dependency parsers.",
  "y": "similarities differences"
 },
 {
  "id": "17252628fa9c03c2fe0b44763fc7a2_8",
  "x": "We used the MOSES PBSMT system in our experiments. The training data, which included those data used in<cite> Wang et al. (2007)</cite> , contained 1 million pairs of sentences extracted from the Linguistic Data Consortium's parallel news corpora. Our development set was the official NIST MT evaluation data from 2002 to 2005, consisting of 4476 Chinese-English sentences pairs.",
  "y": "differences"
 },
 {
  "id": "17252628fa9c03c2fe0b44763fc7a2_9",
  "x": "**EXPERIMENTS** We used the MOSES PBSMT system in our experiments. The training data, which included those data used in<cite> Wang et al. (2007)</cite> , contained 1 million pairs of sentences extracted from the Linguistic Data Consortium's parallel news corpora.",
  "y": "similarities"
 },
 {
  "id": "17252628fa9c03c2fe0b44763fc7a2_10",
  "x": "For evaluation, we used BLEU scores (Papineni et al., 2002) . We implemented the constituent-based preordering rule set in<cite> Wang et al. (2007)</cite> for comparison, which is called WR07 below. The Berkeley Parser (Petrov et al., 2006) was employed for parsing the Chinese sentences.",
  "y": "uses"
 },
 {
  "id": "17252628fa9c03c2fe0b44763fc7a2_11",
  "x": "Similar to<cite> Wang et al. (2007)</cite> , we carried out human evaluations to assess the accuracy of our dependency-based pre-ordering rules by employing the system \"OUR DEP 2\" in Table 1 . The evaluation set contained 200 sentences randomly selected from the development set. Among them, 107 sentences contained at least one rule and the rules were applied 185 times totally.",
  "y": "uses similarities"
 },
 {
  "id": "17252628fa9c03c2fe0b44763fc7a2_12",
  "x": "A bilingual speaker of Chinese and English looked at an original Chinese phrase and the pre-ordered one with their corresponding English phrase and judged whether the pre-ordering obtained a Chinese phrase that had a closer word order to the English one. Table 2 shows the accuracies of three categories of our dependency-based pre-ordering rules. The overall accuracy of this rule set is 60.0%, which is almost at the same level as the WR07 rule set (62.1%), according to the similar evaluation (200 sentences and one annotator) conducted in<cite> Wang et al. (2007)</cite> .",
  "y": "similarities"
 },
 {
  "id": "17252628fa9c03c2fe0b44763fc7a2_13",
  "x": "The overall accuracy of this rule set is 60.0%, which is almost at the same level as the WR07 rule set (62.1%), according to the similar evaluation (200 sentences and one annotator) conducted in<cite> Wang et al. (2007)</cite> . Notice that some of the incorrect pre-orderings may be caused by erroneous parsing as also suggested by<cite> Wang et al. (2007)</cite> . Through human evaluations, we found that 19 out of the total 74 incorrect pre-orderings resulted from errors in parsing.",
  "y": "similarities"
 },
 {
  "id": "17d44521cfdd351d29b4e5f80d41cd_0",
  "x": "**INTRODUCTION** Transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre et al., 2006b; Zhang and Clark, 2008;<cite> Huang and Sagae, 2010</cite> ) utilize a deterministic shift-reduce process for making structural predictions. Compared to graph-based dependency parsing, it typically offers linear time complexity and the comparative freedom to define non-local features, as exemplified by the comparison between MaltParser and MSTParser (Nivre et al., 2006b; McDonald et al., 2005; McDonald and Nivre, 2007) .",
  "y": "background"
 },
 {
  "id": "17d44521cfdd351d29b4e5f80d41cd_1",
  "x": "Recent research has addressed two potential disadvantages of systems like MaltParser. In the aspect of decoding, beam-search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2009 ) and partial dynamic-programming<cite> (Huang and Sagae, 2010)</cite> have been applied to improve upon greedy one-best search, and positive results were reported. In the aspect of training, global structural learning has been used to replace local learning on each decision (Zhang and Clark, 2008; Huang et al., 2009) , although the effect of global learning has not been separated out and studied alone.",
  "y": "background"
 },
 {
  "id": "17d44521cfdd351d29b4e5f80d41cd_2",
  "x": "A set of shiftreduce actions are defined, which consume words from the queue and build the output parse. Recent research have focused on action sets that build projective dependency trees in an arc-eager (Nivre et al., 2006b; Zhang and Clark, 2008) or arc-standard (Yamada and Matsumoto, 2003;<cite> Huang and Sagae, 2010)</cite> process. We adopt the arc-eager system 1 , for which the actions are:",
  "y": "background"
 },
 {
  "id": "17d44521cfdd351d29b4e5f80d41cd_3",
  "x": "Recent research have focused on action sets that build projective dependency trees in an arc-eager (Nivre et al., 2006b; Zhang and Clark, 2008) or arc-standard (Yamada and Matsumoto, 2003;<cite> Huang and Sagae, 2010)</cite> process. We adopt the arc-eager system 1 , for which the actions are: \u2022 Shift, which removes the front of the queue and pushes it onto the top of the stack; \u2022 Reduce, which pops the top item off the stack; \u2022 LeftArc, which pops the top item off the stack, and adds it as a modifier to the front of the queue; \u2022 RightArc, which removes the front of the queue, pushes it onto the stack and adds it as a modifier to the top of the stack.",
  "y": "uses"
 },
 {
  "id": "17d44521cfdd351d29b4e5f80d41cd_4",
  "x": "with S 0 , the front items from the queue with N 0 , N 1 , and N 2 , the head of S 0 (if any) with S 0h , the leftmost and rightmost modifiers of S 0 (if any) with S 0l and S 0r , respectively, and the leftmost modifier of N 0 (if any) with N 0l , the baseline features are shown in Table 1 . These features are mostly taken from Zhang and Clark (2008) and<cite> Huang and Sagae (2010)</cite> , and our parser reproduces the same accuracies as reported by both papers. In this table, w and p represents the word and POS-tag, respectively.",
  "y": "similarities uses"
 },
 {
  "id": "17d44521cfdd351d29b4e5f80d41cd_5",
  "x": "Bracketed sentences from PTB were transformed into dependency formats using the Penn2Malt tool. 2 Following<cite> Huang and Sagae (2010)</cite>, we assign POS-tags to the training data using ten-way jackknifing. We used our implementation of the Collins (2002) tagger (with 97.3% accuracy on a standard Penn Treebank test) to perform POS-tagging.",
  "y": "uses"
 },
 {
  "id": "17d44521cfdd351d29b4e5f80d41cd_6",
  "x": "Table 4 shows the final test results of our parser for English. We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row 'Z&C08 transition'), the dynamic-programming arc-standard parser of<cite> Huang and Sagae (2010)</cite> (row 'H&S10'), and graphbased models including MSTParser (McDonald and Pereira, 2006) , the baseline feature parser of Koo et al. (2008) (row 'K08 baeline') , and the two models of Koo and Collins (2010 ing the highest attachment score reported for a transition-based parser, comparable to those of the best graph-based parsers. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "17d44521cfdd351d29b4e5f80d41cd_7",
  "x": "Table 5 shows the results of our final parser, the pure transition-based parser of Zhang and Clark (2008) , and the parser of<cite> Huang and Sagae (2010)</cite> on Chinese. We take the standard split of CTB and use gold segmentation and POS-tags for the input. Our scores for this test set are the best reported so far and significantly better than the previous systems.",
  "y": "uses"
 },
 {
  "id": "17d44521cfdd351d29b4e5f80d41cd_8",
  "x": "Table 5 shows the results of our final parser, the pure transition-based parser of Zhang and Clark (2008) , and the parser of<cite> Huang and Sagae (2010)</cite> on Chinese. Our scores for this test set are the best reported so far and significantly better than the previous systems.",
  "y": "differences"
 },
 {
  "id": "17d44521cfdd351d29b4e5f80d41cd_9",
  "x": "We have shown that enriching the feature representation significantly improves the accuracy of our transition-based dependency parser. The effect of the new features appears to outweigh the effect of combining transition-based and graph-based models, reported by Zhang and Clark (2008) , as well as the effect of using dynamic programming, as in-<cite> Huang and Sagae (2010)</cite> . This shows that feature definition is a crucial aspect of transition-based pars-191 ing.",
  "y": "differences"
 },
 {
  "id": "17eb0ea80e5a2f18096ef41521af4e_0",
  "x": "One current paradigm to learn from raw data is open information extraction (Downey et al., 2004; Banko, 2009) , which without any prior knowledge aims at discovering all possible relations between pairs of entities occurring in text. Our work tries to learn the main concepts making up the template structure in domain summaries, similar to <cite>(Chambers and Jurafsky, 2011)</cite> . However, we do not rely on any source of external knowledge (i.e. WordNet) to do so.",
  "y": "similarities"
 },
 {
  "id": "17eb0ea80e5a2f18096ef41521af4e_1",
  "x": "In the aviation domain, for example, numeric expressions constitute the extensions of different concepts including: number of victims, crew members, and number of survivors; it is a rather common feature in the aviation domain to include these different concepts together in one sentence, making their \"separation\" complicated. Same explanations apply to other tested domains: for example locations playing the role of origin and destination of a given train or airplace are also sometimes confused. Our work demonstrates the possibility of learning conceptual information in several domains and languages, while previous work <cite>(Chambers and Jurafsky, 2011)</cite> has addressed sets of related domains (e.g., MUC-4 templates) in English.",
  "y": "differences"
 },
 {
  "id": "188f10a5b78a5e691e10d180dfde6f_0",
  "x": "Various NLP tasks have benefited from domain adaptation techniques, including part-ofspeech tagging (Blitzer et al., 2006; Huang and Yates, 2010a) , chunking (Daum\u00e9 III, 2007;<cite> Huang and Yates, 2009)</cite> , named entity recognition (Guo et al., 2009; Turian et al., 2010) , dependency parsing (Dredze et al., 2007; Sagae and Tsujii, 2007) and semantic role labeling (Dahlmeier and Ng, 2010; Huang and Yates, 2010b) . In a typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedical (Blitzer et al., 2006) ). Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabularies and produce cross-domain feature distribution divergence and feature sparsity issue.",
  "y": "background"
 },
 {
  "id": "188f10a5b78a5e691e10d180dfde6f_1",
  "x": "Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabularies and produce cross-domain feature distribution divergence and feature sparsity issue. A number of techniques have been developed in the literature to tackle the problem of cross-domain feature divergence and feature sparsity, including clustering based word representation learning methods<cite> (Huang and Yates, 2009</cite>; Candito et al., 2011) , word embedding based representation learning methods (Turian et al., 2010; Hovy et al., 2015) and some other representation learning methods (Blitzer et al., 2006) . In this paper, we extend the standard hidden Markov models (HMMs) to perform distributed state representation learning and induce contextaware distributed word representations for domain adaptation.",
  "y": "background"
 },
 {
  "id": "188f10a5b78a5e691e10d180dfde6f_2",
  "x": "The hidden states of each word in a sentence can be decoded using the standard Viterbi decoding procedure of HMMs, and its distributed representation can be obtained by a simple mapping with the state embedding matrix. We then use the context-aware distributed representations of the words as their augmenting features to perform cross-domain part-of-speech (POS) tagging and noun-phrase (NP) chunking. The proposed approach is closely related to the clustering based method<cite> (Huang and Yates, 2009</cite> ) as we both use latent state representations as generalizable features.",
  "y": "similarities uses"
 },
 {
  "id": "188f10a5b78a5e691e10d180dfde6f_3",
  "x": "For example,<cite> Huang and Yates (2009)</cite> used the discrete hidden state of a word under HMMs as augmenting features for cross-domain POS tagging and NP chunking. Brown clusters (Brown et al., 1992) , which was used as latent features for simple in-domain dependency parsing (Koo et al., 2008) , has recently been exploited for out-ofdomain statistical parsing (Candito et al., 2011) . The word embedding based representation learning methods learn a dense real-valued representation vector for each word as latent features for domain adaptation.",
  "y": "background"
 },
 {
  "id": "188f10a5b78a5e691e10d180dfde6f_4",
  "x": ". , s T } be the sequence of T hidden states, where each hidden state s t has a discrete state value from a total H hidden states H = {1, 2, . . . , H}. Besides, we assume that there is a low-dimensional distributed representation vector associated with each hidden state. Let M \u2208 R H\u00d7m be the state embedding matrix where the i-th row M i: denotes the m-dimensional representation vector for the i-th state. Previous works have demonstrated the usefulness of discrete hidden states induced from a HMM on addressing feature sparsity in domain adaptation<cite> (Huang and Yates, 2009</cite> ).",
  "y": "background"
 },
 {
  "id": "188f10a5b78a5e691e10d180dfde6f_5",
  "x": "**EXPERIMENTS** We conducted experiments on cross-domain partof-speech (POS) tagging and noun-phrase (NP) chunking tasks. We used the same experimental datasets as in<cite> (Huang and Yates, 2009</cite> ) for cross-domain POS tagging from Wall Street Journal (WSJ) domain (Marcus et al., 1993) to MED-LINE domain (PennBioIE, 2005) and for crossdomain NP chunking from CoNLL shared task dataset (Tjong et al., 2000) to Open American National Corpus (OANC) (Reppen et al., 2005) .",
  "y": "uses similarities"
 },
 {
  "id": "18a44fac8d2f450aee62fc15c00c6f_0",
  "x": "In particular, multi-head attention is akin to having a number of infinitely-wide filters whose weights adapt to the content (allowing fewer \"filters\" to suffice). One can also assign interpretations; for example, <cite>[27]</cite> argue their LAS self-attention heads are differentiated phoneme detectors. Further inductive biases like filter widths and causality could be expressed through time-restricted self-attention [26] and directed self-attention [25] , respectively.",
  "y": "background"
 },
 {
  "id": "18a44fac8d2f450aee62fc15c00c6f_1",
  "x": "Time-restricted self-attention was used as a drop-in replacement for individual layers in the state-of-theart lattice-free MMI model [26] , an HMM-NN system. Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) <cite>[27]</cite> , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems. In this work, we propose and evaluate fully self-attentional networks for CTC (SAN-CTC).",
  "y": "background"
 },
 {
  "id": "18a44fac8d2f450aee62fc15c00c6f_2",
  "x": "Unlike past works, we do not require convolutional frontends [19] or interleaved recurrences <cite>[27]</cite> to train self-attention for ASR. In Section 2, we motivate the model and relevant design choices (position, downsampling) for ASR. In Section 3, we validate SAN-CTC on the Wall Street Journal and LibriSpeech datasets by outperforming existing CTC models and most encoder-decoder models in character error rates (CERs), with fewer parameters or less training time.",
  "y": "differences"
 },
 {
  "id": "18a44fac8d2f450aee62fc15c00c6f_3",
  "x": "Wide contexts also enable incorporation of noise/speaker contexts, as <cite>[27]</cite> suggest regarding the broad-context attention heads in the first layer of their self-attentional LAS model. ---------------------------------- **MOTIVATING THE SELF-ATTENTION LAYER**",
  "y": "background"
 },
 {
  "id": "18a44fac8d2f450aee62fc15c00c6f_4",
  "x": "**MOTIVATING THE SELF-ATTENTION LAYER** We now replace recurrent and convolutional layers for CTC with self-attention [24] . Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder [22] , previous explorations of self-attention in ASR [19,<cite> 27]</cite> , and defined in Section 2.3.",
  "y": "similarities uses"
 },
 {
  "id": "18a44fac8d2f450aee62fc15c00c6f_5",
  "x": "\u2208 R T \u00d7T is created, giving the T 2 factor in Table 1 . A convolutional frontend is a typical downsampling strategy [8, 19] ; however, we leave integrating other layer types into SAN-CTC as future work. Instead, we consider three fixed approaches, from least-to most-preserving of the input data: subsampling, which only takes every k-th frame; pooling, which aggregates every k consecutive frames via a statistic (average, maximum); reshaping, where one concatenates k consecutive frames into one <cite>[27]</cite> .",
  "y": "extends differences"
 },
 {
  "id": "18a44fac8d2f450aee62fc15c00c6f_6",
  "x": "We use standard trigonometric embeddings, where for 0 \u2264 i \u2264 demb/2, we define for position t. We consider three approaches: content-only [21] , which forgoes position encodings; additive [19] , which takes demb = dh and adds the encoding to the embedding; and concatenative, where one takes demb = 40 and concatenates it to the embedding. The latter was found necessary for self-attentional LAS <cite>[27]</cite> , as additive encodings did not give convergence.",
  "y": "differences"
 },
 {
  "id": "18a44fac8d2f450aee62fc15c00c6f_7",
  "x": "Here, we consider the effects of downsampling and position encoding on accuracy for our fixed training regime. We see that unlike self-attentional LAS <cite>[27]</cite> , SAN-CTC works respectably even with no position en- coding; in fact, the contribution of position is relatively minor (compare with [21] , where location in an encoder-decoder system improved CER by 3% absolute). Lossy downsampling appears to preserve performance in CER while degrading WER (as information about frame transitions is lost).",
  "y": "differences"
 },
 {
  "id": "18a44fac8d2f450aee62fc15c00c6f_8",
  "x": "Lossy downsampling appears to preserve performance in CER while degrading WER (as information about frame transitions is lost). We believe these observations align with the monotonicity and independence assumptions of CTC. Inspired by <cite>[27]</cite> , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details.",
  "y": "similarities uses"
 },
 {
  "id": "18a44fac8d2f450aee62fc15c00c6f_9",
  "x": "Inspired by <cite>[27]</cite> , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details. In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike <cite>[27]</cite> we still see mild differentiation of variances. Inspired by [26] , we further plot the attention weights relative to the current time position (here, per head).",
  "y": "differences"
 },
 {
  "id": "193d388c3f4c346cb62711f3f04c0f_0",
  "x": "---------------------------------- **INTRODUCTION** State-of-the-art deep neural networks leverage task-specific architectures to develop hierarchical representations of their input, with each layer building a refined abstraction of the layer that came before it<cite> (Conneau et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "193d388c3f4c346cb62711f3f04c0f_1",
  "x": "State-of-the-art deep neural networks leverage task-specific architectures to develop hierarchical representations of their input, with each layer building a refined abstraction of the layer that came before it<cite> (Conneau et al., 2016)</cite> . In a departure from this philosophy, we propose a divide-and-conquer approach, where a team of readers each focus on different aspects of the text, and then combine their representations to make a joint decision.",
  "y": "differences"
 },
 {
  "id": "193d388c3f4c346cb62711f3f04c0f_2",
  "x": "Furthermore, different sentences may look similar under one view but different under another, allowing the network to devote particular views to distinguishing between subtle differences in sentences, resulting in more discriminative representations. Unlike existing multi-view neural network approaches for image processing (Zhu et al., 2014; Su et al., 2015) , where multiple views are provided as part of the input, our MVN learns to automatically create views from its input text by focusing on different sets of words. Compared to deep Convolutional Networks (CNN) for text (Zhang et al., 2015; <cite>Conneau et al., 2016)</cite> , the MVN strategy emphasizes network width over depth.",
  "y": "differences"
 },
 {
  "id": "193d388c3f4c346cb62711f3f04c0f_3",
  "x": "That is, we replace Equation 5 with v i = s<cite> (Conneau et al., 2016)</cite> ---------------------------------- **AG'S ENGLISH NEWS CATEGORIZATION**",
  "y": "uses"
 },
 {
  "id": "193d388c3f4c346cb62711f3f04c0f_4",
  "x": "---------------------------------- **AG'S ENGLISH NEWS CATEGORIZATION** The AG corpus (Zhang et al., 2015; <cite>Conneau et al., 2016)</cite> contains categorized news articles from more than 2,000 news outlets on the web.",
  "y": "background"
 },
 {
  "id": "193d388c3f4c346cb62711f3f04c0f_5",
  "x": "The AG corpus (Zhang et al., 2015; <cite>Conneau et al., 2016)</cite> contains categorized news articles from more than 2,000 news outlets on the web. A random sample of the training set was used for hyper-parameter tuning.",
  "y": "uses"
 },
 {
  "id": "193d388c3f4c346cb62711f3f04c0f_6",
  "x": "These results show that the bag-of-words MVN outperforms the state-of-theart accuracy obtained by the non-neural n-gram TFIDF approach (Zhang et al., 2015) , as well as several very deep CNNs<cite> (Conneau et al., 2016)</cite> . Accuracy was further improved when the MVN was augmented with 4 convolutional features. In Figure 4 , we show how accuracy and loss evolve on the validation set during MVN training.",
  "y": "differences"
 },
 {
  "id": "197b557d7b5c7c2d195be84990719b_0",
  "x": "The examination of word co-occurrences has proven to be a fruitful research paradigm. For example, <cite>Mikolov et al. (2013)</cite> utilize Skipgram NegativeSampling (SGNS) to train word embeddings using word-context pairs formed from windows moving across a text corpus. These vector representations ultimately encode remarkable linearities such as king \u2212 man + woman = queen.",
  "y": "background"
 },
 {
  "id": "197b557d7b5c7c2d195be84990719b_1",
  "x": "This section describes the model for lda2vec. We are interested in modifying the Skipgram Negative-Sampling (SGNS) objective in<cite> (Mikolov et al., 2013)</cite> to utilize document-wide feature vectors while simultaneously learning continuous document weights loading onto topic vectors. The network architecture is shown in Figure 1 .",
  "y": "extends"
 },
 {
  "id": "197b557d7b5c7c2d195be84990719b_2",
  "x": "For every pivot-target pair of words the pivot word is used to predict the nearby target word. Each word is represented with a fixedlength dense distributed-representation vector, but unlike <cite>Mikolov et al. (2013)</cite> the same word vectors are used in both the pivot and target representations. The SGNS loss shown in (2) attempts to discriminate context-word pairs that appear in the corpus from those randomly sampled from a 'negative' pool of words.",
  "y": "differences"
 },
 {
  "id": "197b557d7b5c7c2d195be84990719b_3",
  "x": "As in <cite>Mikolov et al. (2013)</cite> , pairs of pivot and target words (j, i) are extracted when they cooccur in a moving window scanning across the corpus. In our experiments, the window contains five tokens before and after the pivot token. For every pivot-target pair of words the pivot word is used to predict the nearby target word.",
  "y": "uses"
 },
 {
  "id": "197b557d7b5c7c2d195be84990719b_4",
  "x": "The distribution from which tokens are drawn is u \u03b2 , where u denotes the overall word frequency normalized by the total corpus size. Unless stated otherwise, the negative sampling power beta is set to 3/4 and the number of negative samples is fixed to n = 15 as in <cite>Mikolov et al. (2013)</cite> . Note that a distribution of u 0.0 would draw negative tokens from the vocabulary with no notion of popularity while a distribution proportional with u 1.0 draws from the empirical unigram distribution.",
  "y": "uses"
 },
 {
  "id": "197b557d7b5c7c2d195be84990719b_5",
  "x": "lda2vec embeds both words and document vectors into the same space and trains both representations simultaneously. By adding the pivot and document vectors together, both spaces are effectively joined. <cite>Mikolov et al. (2013)</cite> provide the intuition that word vectors can be summed together to form a semantically meaningful combination of both words.",
  "y": "background"
 },
 {
  "id": "197b557d7b5c7c2d195be84990719b_6",
  "x": "Word vectors are initialized to the pretrained values found in <cite>Mikolov et al. (2013)</cite> but otherwise updates are allowed to these vectors at training time. A range of lda2vec parameters are evaluated by varying the number of topics n \u2208 20, 30, 40, 50 and the negative sampling exponent \u03b2 \u2208 0.75, 1.0. The best topic coherences were achieved with n = 20 topics and with negative sampling power \u03b2 = 0.75 as summarized in Figure 2 .",
  "y": "uses"
 },
 {
  "id": "197b557d7b5c7c2d195be84990719b_7",
  "x": "These topics demonstrate that the major themes of the corpus are reproduced and represented in learned topic vectors in a similar fashion as in LDA (Blei et al., 2003) . The first, which we Figure 5 demonstrates that token similarities are learned in a similar fashion as in SGNS<cite> (Mikolov et al., 2013</cite> ) but specialized to the Hacker News corpus. Tokens similar to the token Artificial sweeteners include other sugar-related tokens like fructose and food-related tokens such as paleo diet.",
  "y": "similarities"
 },
 {
  "id": "197b557d7b5c7c2d195be84990719b_8",
  "x": "This work demonstrates a simple model, lda2vec, that extends SGNS<cite> (Mikolov et al., 2013)</cite> to build unsupervised document representations that yield coherent topics. Word, topic, and document vectors are jointly trained and embedded in a common representation space that preserves semantic regularities between the learned word vectors while still yielding sparse and interpretable documentto-topic proportions in the style of LDA (Blei et al., 2003) . Topics formed in the Twenty Newsgroups corpus yield high mean topic coherences which have been shown to correlate with human evaluations of topics (R\u00f6der et al., 2015) .",
  "y": "extends"
 },
 {
  "id": "19b647ab74d28b59b7df2be729b2d7_0",
  "x": "Likewise, in order to avoid stale and repetitive utterances, we can alter and repurpose the candidate utterances; for example, we can use paraphrase or summarization to create new ways of saying the same thing, or to select utterance candidates according to the desired sentiment [12, 13] . The style of an utterance can be altered based on requirements; introducing elements of sarcasm, or aspects of factual and emotional argumentation styles<cite> [15,</cite> 14] . Changes in the perceived speaker personality can also make more personable conversations [11] .",
  "y": "background"
 },
 {
  "id": "19b647ab74d28b59b7df2be729b2d7_1",
  "x": "Of course, while many data sources may be of interest for indexing knowledge for a dialogue system, annotations are not always available or easy to obtain. By using machine learning models designed to classify different classes of interest, such as sentiment, sarcasm, and topic, data can be bootstrapped to greatly increase the amount of data available for indexing and utterance selection <cite>[15]</cite> . There is no shortage of human generated dialogues, but the challenge is to analyze and harness them appropriately for social-dialogue generation.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_0",
  "x": "The second aspect is that on which most variations between existing approaches occur. Multiple Deep Neural Network (DNN) architectures have been explored to combine the token representations into a single segment representation that captures relevant information for the task. Of the two state-of-the-art approaches on dialog act recognition, one uses a deep stack of Recurrent Neural Networks (RNNs) (Schmidhuber, 1990) to capture long distance relations between tokens (Khanpour et al., 2016) , while the other uses multiple parallel temporal Convolutional Neural Networks (CNNs) (Fukushima, 1980) to capture relevant functional patterns with different length <cite>(Liu et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_1",
  "x": "The latter are hard to capture and are usually not available for a dialog system. Thus, only speaker information that is directly related to the dialog, such as turn-taking <cite>(Liu et al., 2017)</cite> , is typically considered. Concerning information from the surrounding segments, its influence, especially that of preceding segments, has been thoroughly explored in at least two studies (Ribeiro et al., 2015; <cite>Liu et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_2",
  "x": "Thus, only speaker information that is directly related to the dialog, such as turn-taking <cite>(Liu et al., 2017)</cite> , is typically considered. Concerning information from the surrounding segments, its influence, especially that of preceding segments, has been thoroughly explored in at least two studies (Ribeiro et al., 2015; <cite>Liu et al., 2017)</cite> . However, in both cases, although it is one of its most important characteristics, that information was represented in ways that are not appropriate to capture its sequentiality.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_3",
  "x": "This way, the model is able to capture long distance relations between tokens. On the convolutional side, <cite>Liu et al. (2017)</cite> generated the segment representation by combining the outputs of three parallel CNNs with different context window sizes, in order to capture different functional patterns. In both cases, pre-trained word-embeddings were used as input to the network.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_4",
  "x": "Khanpour et al. (2016) compared the performance of embeddings with different dimensionality trained on multiple corpora using GloVe and Word2Vec (Mikolov et al., 2013) . The best results were achieved when using 150-dimensional embeddings trained on Wikipedia data using Word2Vec. <cite>Liu et al. (2017)</cite> used 200-dimensional Word2Vec embeddings trained on Facebook data.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_5",
  "x": "<cite>Liu et al. (2017)</cite> used 200-dimensional Word2Vec embeddings trained on Facebook data. Overall, from the reported results, it is not possible to state which is the top performing segment representation approach since the evaluation was performed on different subsets of the corpus. Still, Khanpour et al. (2016) reported 73.9% accuracy on the validation set and 80.1% on the test set, while <cite>Liu et al. (2017)</cite> reported 74.5% and 76.9% accuracy on the two sets used to evaluate <cite>their experiments</cite>.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_6",
  "x": "Still, Khanpour et al. (2016) reported 73.9% accuracy on the validation set and 80.1% on the test set, while <cite>Liu et al. (2017)</cite> reported 74.5% and 76.9% accuracy on the two sets used to evaluate <cite>their experiments</cite>. Additionally, <cite>Liu et al. (2017)</cite> explored the use of context information concerning speaker changes and from the surrounding segments. The first was provided as a flag and concatenated to the segment representation.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_7",
  "x": "Additionally, <cite>Liu et al. (2017)</cite> explored the use of context information concerning speaker changes and from the surrounding segments. The first was provided as a flag and concatenated to the segment representation. Concerning the latter, <cite>they explored</cite> the use of discourse models, as well as of approaches that concatenated the context information directly to the segment representation.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_8",
  "x": "Additionally, <cite>Liu et al. (2017)</cite> explored the use of context information concerning speaker changes and from the surrounding segments. The first was provided as a flag and concatenated to the segment representation. Concerning the latter, <cite>they explored</cite> the use of discourse models, as well as of approaches that concatenated the context information directly to the segment representation. <cite>The discourse models</cite> transform the model into a hierarchical one by generating a sequence of dialog act classifications from the sequence of segment representations.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_9",
  "x": "Additionally, <cite>Liu et al. (2017)</cite> explored the use of context information concerning speaker changes and from the surrounding segments. The first was provided as a flag and concatenated to the segment representation. Concerning the latter, <cite>they explored</cite> the use of discourse models, as well as of approaches that concatenated the context information directly to the segment representation. <cite>The discourse models</cite> transform the model into a hierarchical one by generating a sequence of dialog act classifications from the sequence of segment representations. Thus, when predicting the classification of a segment, the surrounding ones are also taken into account. However, when the <cite>discourse model</cite> is based on a CNN or a bidirectional LSTM unit, it considers information from future segments, which is not available for a dialog system.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_10",
  "x": "Additionally, <cite>Liu et al. (2017)</cite> explored the use of context information concerning speaker changes and from the surrounding segments. The first was provided as a flag and concatenated to the segment representation. Concerning the latter, <cite>they explored</cite> the use of discourse models, as well as of approaches that concatenated the context information directly to the segment representation. <cite>The discourse models</cite> transform the model into a hierarchical one by generating a sequence of dialog act classifications from the sequence of segment representations. Thus, when predicting the classification of a segment, the surrounding ones are also taken into account. However, when the <cite>discourse model</cite> is based on a CNN or a bidirectional LSTM unit, it considers information from future segments, which is not available for a dialog system. Still, even when relying on future information, the approaches based on <cite>discourse models</cite> performed worse than those that concatenated the context information directly to the segment representation.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_11",
  "x": "Still, even when relying on future information, the approaches based on <cite>discourse models</cite> performed worse than those that concatenated the context information directly to the segment representation. In this sense, similarly to our previous study using SVMs (Ribeiro et al., 2015) , <cite>Liu et al. (2017)</cite> concluded that providing that information in the form of the classification of the surrounding segments leads to better results than using <cite>their words</cite>, even when those classifications are obtained automatically. Furthermore, both studies have shown that the first preceding segment is the most important and that the influence decays with the distance.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_12",
  "x": "Furthermore, both studies have shown that the first preceding segment is the most important and that the influence decays with the distance. Using the setup with gold standard labels from three preceding segments, <cite>Liu et al. (2017)</cite> achieved 79.6% and 81.8% on the two sets used to evaluate the approach. ----------------------------------",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_13",
  "x": "The resulting word embeddings are 200-dimensional as in the study by <cite>Liu et al. (2017)</cite> . Different tokenization and embedding approaches are explored in Section 6. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_14",
  "x": "**DIMENSIONALITY REDUCTION LAYER** In order to avoid result differences caused by using representations with different dimensionality, the network includes a dimensionality reduction layer. This is a dense layer which maps the segment representations into a 100-dimensional space, as in the study by <cite>Liu et al. (2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_15",
  "x": "As stated in Section 3, of the two state-of-the-art approaches on dialog act recognition, one uses a RNN-based approach (Khanpour et al., 2016) for segment representation, while the other uses one based on CNNs <cite>(Liu et al., 2017)</cite> . Both have their own advantages, as while the first focuses on capturing information from relevant sequences of tokens, the latter focuses on the context surrounding each token and, thus, captures information concerning neighboring tokens. Concerning the task at hand, this is relevant since, among other aspects, while some dialog acts are distinguishable due to the order of the tokens in the segment (e.g. subject-auxiliary inversion in questions), others are distinguishable due to the presence of certain tokens or sequences of tokens independently of where they occur in the segment (e.g. greetings).",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_16",
  "x": "---------------------------------- **CNN-BASED SEGMENT REPRESENTATION** As described in Section 3, the convolutional approach by <cite>Liu et al. (2017)</cite> uses a set of parallel temporal CNNs with different window size, each followed by a max pooling operation.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_17",
  "x": "This way, the representation contains information concerning groups of tokens with different sizes. To achieve the results presented in <cite>their paper</cite>, <cite>Liu et al. (2017)</cite> used three CNNs with 100 filters and 1, 2, and 3 as context window sizes. In a previous study using the same architecture for different tasks, Kim (2014) used 3, 4, and 5 as window sizes.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_18",
  "x": "As stated in Section 3, Khanpour et al. (2016) explored embedding spaces with dimensionality 75, 150, and 300 together with different embedding approaches. In every case, the embedding space with dimensionality 150 led to the best results. <cite>Liu et al. (2017)</cite> used a different dimensionality value, 200, in <cite>their study</cite>.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_19",
  "x": "Khanpour et al. (2016) used pre-trained embeddings using both approaches in their study and achieved their best results using Word2Vec embeddings trained on Wikipedia data. <cite>Liu et al. (2017)</cite> also used Word2Vec embeddings, but trained on Facebook data. Since we have access to the embeddings trained on Wikipedia data, but not to those trained on Facebook data, we used the first in our experiments.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_20",
  "x": "In <cite>their study</cite>, <cite>Liu et al. (2017)</cite> used pre-trained embeddings but let them adapt to the task during the training phase. However, they did not perform a comparison with the case where the embeddings are not adaptable. Thus, in our study we experimented with both fixed and adaptable embeddings.",
  "y": "motivation"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_22",
  "x": "**RESULTS** Starting with the dimensionality of the embedding space, in Table 3 we can see that using an embedding space with 200 dimensions, such as in the study by <cite>Liu et al. (2017)</cite> , leads to better results than any of the dimensionality values used by Khanpour et al. (2016) . Furthermore, considering the three values they used, our experiments revealed better performance when using 300 dimensions than when using 150 dimensions, which was the best value in their study.",
  "y": "similarities"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_23",
  "x": "for dialog act recognition is the dialog history, with influence decaying with distance (Ribeiro et al., 2015; Lee & Dernoncourt, 2016; <cite>Liu et al., 2017)</cite> . However, information concerning the speakers and, more specifically, turn-taking has also been proved important <cite>(Liu et al., 2017)</cite> . Thus, in our study, we explore both the surrounding segments and speaker information as sources of context information.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_24",
  "x": "for dialog act recognition is the dialog history, with influence decaying with distance (Ribeiro et al., 2015; Lee & Dernoncourt, 2016; <cite>Liu et al., 2017)</cite> . However, information concerning the speakers and, more specifically, turn-taking has also been proved important <cite>(Liu et al., 2017)</cite> . Thus, in our study, we explore both the surrounding segments and speaker information as sources of context information.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_25",
  "x": "Still, in order to assess the importance of future information and simulate the annotation environment, we also performed some experiments using that information. As stated in Section 3, considering the preceding segments, we have shown in a previous study (Ribeiro et al., 2015) that providing information in the form of segment classifications leads to better results than in the form of words. <cite>Liu et al. (2017)</cite> further showed that using a single label per segment is better than using the probability of each class.",
  "y": "background"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_26",
  "x": "Thus, in order to simplify the experiments and obtain an upper bound for the approach, in this study we just use the manual annotations. In our previous study, we have used up to five preceding segments and showed that the gain becomes smaller as the number of preceding segments increases, which supports the claim that the closest segments are the most relevant. <cite>Liu et al. (2017)</cite> stopped at three preceding segments, but noticed a similar pattern.",
  "y": "similarities"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_27",
  "x": "<cite>Liu et al. (2017)</cite> stopped at three preceding segments, but noticed a similar pattern. In this study we explore up to five preceding segments, as well as the entire dialog history. Although both our previous study and that by <cite>Liu et al. (2017)</cite> used the classifications of preceding segments as context information, none of them took into account that those segments have a sequential nature and simply flattened the sequence before appending it to the segment representation.",
  "y": "motivation"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_28",
  "x": "Thus, turn-taking information is relevant for dialog act recognition. In fact, this has been confirmed in the study by <cite>Liu et al. (2017)</cite> . Thus, we also use turn-taking information in this study.",
  "y": "motivation"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_29",
  "x": "**RESULTS** Starting with the reproduction of the flat label sequence approach, in Table 9 we can see that the results follow the same pattern as in our previous study and that by <cite>Liu et al. (2017)</cite> . The first preceding segment is the most important, leading to a 3.21 percentage point improvement on the validation set and 3.56 on the test set.",
  "y": "similarities"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_30",
  "x": "We started with the segment representation approaches, since that is the step with higher variation among the previous studies described in Section 3 and that which introduces more changes in the overall architecture. We used adaptations of the approaches with top performance in previous studies, namely the RNN-based approach by Khanpour et al. (2016) and the CNN-based approach by <cite>Liu et al. (2017)</cite> . However, those approaches focus on capturing different kinds of information, both of which are relevant for the task.",
  "y": "extends"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_31",
  "x": "This approach outperformed the other two, proving that both token sequences and the contexts that surround each token are relevant for the task. In terms of token embedding, we have explored approaches at the character, word, and functional levels. Starting with the typically used word-level, we have shown that using an embedding space with 200 dimensions as used by <cite>Liu et al. (2017)</cite> in <cite>their study</cite> leads to better results than any of the dimensionality values used by Khanpour et al. (2016) .",
  "y": "uses"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_32",
  "x": "Thus, we assume that the discrepancy of over 6 percentage points between the results presented for the two sets in their paper was due to the fact they considered the outcome of a single run, with a specific initialization. Furthermore, our study has shown that their approach can be improved in many aspects. In the case of <cite>the study</cite> by <cite>Liu et al. (2017)</cite> , direct result comparison with those reported is not possible since they were obtained on different sets.",
  "y": "differences"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_33",
  "x": "In the case of <cite>the study</cite> by <cite>Liu et al. (2017)</cite> , direct result comparison with those reported is not possible since they were obtained on different sets. However, the result differences between overlapping steps in our experiments are consistent with those described in <cite>their paper</cite>. Thus, we can safely state that <cite>their approach</cite> can be improved by using five parallel CNNs, dependency-based word embeddings, and the summary representation of context information.",
  "y": "similarities differences"
 },
 {
  "id": "1ab7893c2a930bc5af3c34a5912dd2_34",
  "x": "In the case of <cite>the study</cite> by <cite>Liu et al. (2017)</cite> , direct result comparison with those reported is not possible since they were obtained on different sets. However, the result differences between overlapping steps in our experiments are consistent with those described in <cite>their paper</cite>. Thus, we can safely state that <cite>their approach</cite> can be improved by using five parallel CNNs, dependency-based word embeddings, and the summary representation of context information.",
  "y": "similarities differences"
 },
 {
  "id": "1baddfeea7d11fc02cc26ff698a601_0",
  "x": "Examples include n-gram LMs [1] and neural network (NN) LMs [2, 3] . We have recently introduced a new transdimensional random field (TRF 1 ) LM <cite>[4]</cite> , where the whole sentence is modeled as a random field. As the random field approach avoids local normalization which is required in the conditional approach, it is computationally more efficient in computing sentence probabilities and has the potential advantage of being able to flexibly integrating a richer set of features.",
  "y": "background"
 },
 {
  "id": "1baddfeea7d11fc02cc26ff698a601_1",
  "x": "Improvements: First, in <cite>[4]</cite> , the diagonal elements of the Hessian matrices are online estimated during the SA iterations to rescale the gradients, which is shown to benefit the convergence of the training algorithm. In this paper, inspired from [6, 7] , we propose a simpler but more effective method which directly uses the empirical variances to rescale the gradients. As the empirical variances are calculated offline, this also reduces computational and memory cost during the SA iterations.",
  "y": "motivation background"
 },
 {
  "id": "1baddfeea7d11fc02cc26ff698a601_2",
  "x": "As defined in <cite>[4]</cite> , a trans-dimensional random field model represents the joint probability of the pair (l, x l ) as where n l /n is the empirical probability of length l. f ( T is the feature vector, which is usually defined to be position-independent and length-independent, e.g. is the normalization constant of length l. By making explicit the role of length in model definition, it is clear that the model in (1) is a mixture of random fields on sentences of different lengths (namely on subspaces of different dimensions), and hence will be called a trans-dimensional random field (TRF).",
  "y": "background"
 },
 {
  "id": "1baddfeea7d11fc02cc26ff698a601_3",
  "x": "In the joint SA training algorithm <cite>[4]</cite> , we define another form of mixture distribution as follows: where \u03b6 = {\u03b61, . . . , \u03b6m} with \u03b61 = 0 and \u03b6 l is the hypothesized value of the log ratio of Z l (\u03bb) with respect to Z1(\u03bb), namely log . Z1(\u03bb) is chosen as the reference value and can be calculated exactly.",
  "y": "extends"
 },
 {
  "id": "1baddfeea7d11fc02cc26ff698a601_4",
  "x": "---------------------------------- **IMPROVED STOCHASTIC APPROXIMATION** In order to make use of Hessian information in parameter optimization, we use the online estimated Hessian diagonal elements to rescale the gradients in <cite>[4]</cite> .",
  "y": "uses background"
 },
 {
  "id": "1baddfeea7d11fc02cc26ff698a601_5",
  "x": "At each iteration t (from 1 to tmax), we perform two steps. Step I: MCMC sampling: Generate a sample set B (t) with p(l, x l ; \u03bb (t\u22121) , \u03b6 (t\u22121) ) as the stationary distribution, using the trans-dimensional mixture sampling method (See Section 3.3 in <cite>[4]</cite> ). Step II: SA updating: Compute",
  "y": "uses background"
 },
 {
  "id": "1baddfeea7d11fc02cc26ff698a601_6",
  "x": "Fig.1 show an example of convergence curves of the SA training algorithm in <cite>[4]</cite> and the new improved SA. ---------------------------------- **MODELING RARE VERY LONG SENTENCES**",
  "y": "background"
 },
 {
  "id": "1baddfeea7d11fc02cc26ff698a601_7",
  "x": "In this way we can leverage long distance context without increasing the model size. Note that for all the feature types in Tab.1, only the features observed in the training data are used. The improved SA algorithm (in Section 2.2) is used to train the TRF LMs, in conjunction with the trans-dimensional mixture sampling proposed in Section 3.3 of <cite>[4]</cite> .",
  "y": "uses background"
 },
 {
  "id": "1baddfeea7d11fc02cc26ff698a601_8",
  "x": "The learning rates of \u03bb and \u03b6 are set as suggested in <cite>[4]</cite> : where tc, t0 are constants and 0.5 < \u03b2 \u03bb , \u03b2 \u03b6 < 1. The class information is also used to accelerate the sampling, and more than one CPU cores are used to parallelize the algorithm, as described in <cite>[4]</cite> .",
  "y": "uses background"
 },
 {
  "id": "1baddfeea7d11fc02cc26ff698a601_9",
  "x": "where tc, t0 are constants and 0.5 < \u03b2 \u03bb , \u03b2 \u03b6 < 1. The class information is also used to accelerate the sampling, and more than one CPU cores are used to parallelize the algorithm, as described in <cite>[4]</cite> . Also, we examine how the TRF models can be interpolated with NN models to further improve the performance.",
  "y": "uses background"
 },
 {
  "id": "1baddfeea7d11fc02cc26ff698a601_10",
  "x": "In this section, speech recognition and 1000-best list rescoring experiments are conducted as configured in <cite>[4]</cite> . The maximum length of TRFs is m = 82, which is equal to the maximum length of the training sentences. The other configurations are: K = 300, \u03b2 \u03bb = 0.8, \u03b2 \u03b6 = 0.6, tc = 3000, t0 = 2000, tmax = 20, 000. L2 regularization with constant 4 \u00d7 10 \u22125 is used to avoid over-fitting. 6 CPU cores are used to parallelize the algorithm. The word error rates (WERs) and perplexities (PPLs) on WSJ'92 test set are shown in Tab.4.",
  "y": "uses"
 },
 {
  "id": "1baddfeea7d11fc02cc26ff698a601_13",
  "x": "In contrast, TRF LMs eliminate local normalization from the root and thus are much more efficient in testing with theoretical guarantee. Empirically in our experiments reported in Section 3.3, the average time costs for re-ranking of the 1000-best list for a sentence are 0.16 sec vs. 40 sec, based on TRF and RNN respectively (no GPU used). Equally importantly, evaluations in this paper and also in <cite>[4]</cite> have shown that TRF LMs are able to perform as good as NN LMs (either RNN or FNN) on a variety of tasks.",
  "y": "background"
 },
 {
  "id": "1c0d971cf771f351b51661950f4b14_0",
  "x": "These shared interlingual embedding spaces may then be used in a myriad of multilingual natural language processing tasks, such as fundamental tasks of computing cross-lingual and multilingual semantic word similarity and bilingual lexicon induction (BLI), etc. However, all these models critically require at least sentence-aligned parallel data and/or readilyavailable translation dictionaries to induce bilingual word embeddings (BWEs) that are consistent and closely aligned over languages in the same semantic space. Contributions In this work, we alleviate the requirements: (1) We present the first model that is able to induce bilingual word embeddings from non-parallel data without any other readily available translation resources such as pre-given bilingual lexicons; (2) We demonstrate the utility of BWEs induced by this simple yet effective model in the BLI task from comparable Wikipedia data on benchmarking datasets for three language pairs<cite> (Vuli\u0107 and Moens, 2013b</cite> ).",
  "y": "similarities"
 },
 {
  "id": "1c0d971cf771f351b51661950f4b14_1",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** Training Data We use comparable Wikipedia data introduced in (Vuli\u0107 and Moens, 2013a;<cite> Vuli\u0107 and Moens, 2013b</cite> ) available in three language pairs to induce bilingual word embeddings: (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection of 18, 898 ItalianEnglish Wikipedia article pairs (IT-EN), and (iii) a collection of 7, 612 Dutch-English Wikipedia article pairs (NL-EN).",
  "y": "similarities"
 },
 {
  "id": "1c0d971cf771f351b51661950f4b14_2",
  "x": "All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations. Following prior work (Haghighi et al., 2008; Prochasson and Fung, 2011;<cite> Vuli\u0107 and Moens, 2013b)</cite> , we retain only nouns that occur at least 5 times in the corpus. Lemmatized word forms are recorded when available, and original forms otherwise.",
  "y": "extends differences"
 },
 {
  "id": "1c0d971cf771f351b51661950f4b14_3",
  "x": "(2) Assoc-BLI -A BLI model that represents words as vectors of association norms (Roller and Schulte im Walde, 2013) over both vocabularies, where these norms are computed using a multilingual topic model (Vuli\u0107 and Moens, 2013a) . (3) PPMI+cos -A standard distributional model for BLI relying on positive pointwise mutual information and cosine similarity (Bullinaria and Levy, 2007) . The seed lexicon is bootstrapped using the method from (Peirsman and Pad\u00f3, 2011;<cite> Vuli\u0107 and Moens, 2013b)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "1c0d971cf771f351b51661950f4b14_4",
  "x": "All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (Steyvers and Griffiths, 2007; Vuli\u0107 and Moens, 2013a;<cite> Vuli\u0107 and Moens, 2013b</cite>; Kiela and Clark, 2014) . Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Pad\u00f3, 2011; Tamura et al., 2012; Vuli\u0107 and Moens, 2013a;<cite> Vuli\u0107 and Moens, 2013b)</cite> . Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN) (Vuli\u0107 and Moens, 2013a;<cite> Vuli\u0107 and Moens, 2013b)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "1c0d971cf771f351b51661950f4b14_5",
  "x": "All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (Steyvers and Griffiths, 2007; Vuli\u0107 and Moens, 2013a;<cite> Vuli\u0107 and Moens, 2013b</cite>; Kiela and Clark, 2014) . Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Pad\u00f3, 2011; Tamura et al., 2012; Vuli\u0107 and Moens, 2013a;<cite> Vuli\u0107 and Moens, 2013b)</cite> . Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN) (Vuli\u0107 and Moens, 2013a;<cite> Vuli\u0107 and Moens, 2013b)</cite> .",
  "y": "background"
 },
 {
  "id": "1c0d971cf771f351b51661950f4b14_6",
  "x": "All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (Steyvers and Griffiths, 2007; Vuli\u0107 and Moens, 2013a;<cite> Vuli\u0107 and Moens, 2013b</cite>; Kiela and Clark, 2014) . Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Pad\u00f3, 2011; Tamura et al., 2012; Vuli\u0107 and Moens, 2013a;<cite> Vuli\u0107 and Moens, 2013b)</cite> . Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN) (Vuli\u0107 and Moens, 2013a;<cite> Vuli\u0107 and Moens, 2013b)</cite> .",
  "y": "similarities"
 },
 {
  "id": "1c0d971cf771f351b51661950f4b14_7",
  "x": "EN words are given in italic. The correct one-to-one translation for each source word is marked by (+). lation in the other language (EN) according to the ground truth over the total number of ground truth translation pairs (=1000) (Gaussier et al., 2004; Tamura et al., 2012;<cite> Vuli\u0107 and Moens, 2013b)</cite> .",
  "y": "background"
 },
 {
  "id": "1c0d971cf771f351b51661950f4b14_8",
  "x": "The low-cost BWEs may be used in other (semantic) tasks besides the ones discussed here, and it would be interesting to experiment with other types of context aggregation and selection beyond random shuffling, and other objective functions. Preliminary studies also demonstrate the utility of the BWEs in monolingual and cross-lingual information retrieval (Vuli\u0107 and Moens, 2015) . Finally, we may use the knowledge of BWEs obtained by BWESG from document-aligned data to learn bilingual correspondences (e.g., word translation pairs or lists of semantically similar words across languages) which may in turn be used for representation learning from large unaligned multilingual datasets as proposed in (Haghighi et al., 2008; Mikolov et al., 2013b;<cite> Vuli\u0107 and Moens, 2013b)</cite> .",
  "y": "motivation"
 },
 {
  "id": "1c1b524d2bfe00c62a5a2e1a05ffc7_0",
  "x": "State-of-the-art results on neural machine translation often use attentional sequence-to-sequence models with some form of convolution or recursion. <cite>Vaswani et al. (2017)</cite> propose a new architecture that avoids recurrence and convolution completely. Instead, it uses only self-attention and feed-forward layers.",
  "y": "background"
 },
 {
  "id": "1c1b524d2bfe00c62a5a2e1a05ffc7_1",
  "x": "However, in these models, the operations needed to learn dependencies between distant positions can be difficult to learn (Hochreiter et al., 2001; Hochreiter, 1998) . Attention mechanisms, often used in conjunction with recurrent models, have become an integral part of complex sequential tasks because they facilitate learning of such dependencies (Luong et al., 2015; Parikh et al., 2016; Paulus et al., 2017; Kim et al., 2017) . In <cite>Vaswani et al. (2017)</cite> , the authors introduce the Transformer network, a novel architecture that avoids the recurrence equation and maps the input sequences into hidden states solely using attention.",
  "y": "background"
 },
 {
  "id": "1c1b524d2bfe00c62a5a2e1a05ffc7_2",
  "x": "In <cite>Vaswani et al. (2017)</cite> , the authors introduce the Transformer network, a novel architecture that avoids the recurrence equation and maps the input sequences into hidden states solely using attention. We propose a variant of the Transformer network which we call Weighted Transformer that uses self-attention branches in lieu of the multi-head attention.",
  "y": "extends"
 },
 {
  "id": "1c1b524d2bfe00c62a5a2e1a05ffc7_3",
  "x": "Several architectures have been proposed to reduce the computational load associated with recurrence-based computation (Gehring et al., 2016; 2017; Kaiser & Bengio, 2016; Kalchbrenner et al., 2016) . Self-attention, which relies on dot-products between elements of the input sequence to compute a weighted sum (Lin et al., 2017; Parikh et al., 2016; Kim et al., 2017) , has also been a critical ingredient in modern NMT architectures. The Transformer network<cite> (Vaswani et al., 2017)</cite> avoids the recurrence completely and uses only self-attention.",
  "y": "background"
 },
 {
  "id": "1c1b524d2bfe00c62a5a2e1a05ffc7_4",
  "x": "Self-attention, which relies on dot-products between elements of the input sequence to compute a weighted sum (Lin et al., 2017; Parikh et al., 2016; Kim et al., 2017) , has also been a critical ingredient in modern NMT architectures. The Transformer network<cite> (Vaswani et al., 2017)</cite> avoids the recurrence completely and uses only self-attention. We propose a modified Transformer network wherein the multi-head attention layer is replaced by a branched self-attention layer.",
  "y": "extends"
 },
 {
  "id": "1c1b524d2bfe00c62a5a2e1a05ffc7_5",
  "x": "Note that where h denotes the number of heads in the multi-head attention. <cite>Vaswani et al. (2017)</cite> proportionally reduce d k = d v = d model so that the computational load of the multi-head attention is the same as simple self-attention.",
  "y": "background"
 },
 {
  "id": "1c1b524d2bfe00c62a5a2e1a05ffc7_6",
  "x": "For the sake of brevity, we refer the reader to <cite>Vaswani et al. (2017)</cite> for additional details regarding the architecture. For regularization and ease of training, the network uses layer normalization (Ba et al., 2016) after each sub-layer and a residual connection around each full layer . Analogously, each layer of the decoder contains the two sub-layers mentioned above as well as an additional multi-head attention sub-layer that receives as inputs (V, K) from the output of the corresponding encoding layer.",
  "y": "background"
 },
 {
  "id": "1c1b524d2bfe00c62a5a2e1a05ffc7_7",
  "x": "In the case of the decoder multi-head attention sub-layers, the scaled dot-product attention is masked to prevent future positions from being attended to, or in other words, to prevent illegal leftward-ward information flow. One natural question regarding the Transformer network is why self-attention should be preferred to recurrent or convolutional models. <cite>Vaswani et al. (2017)</cite> state three reasons for the preference: (a) computational complexity of each layer, (b) concurrency, and (c) path length between long-range dependencies.",
  "y": "background"
 },
 {
  "id": "1c1b524d2bfe00c62a5a2e1a05ffc7_8",
  "x": "In Equations (3) and (4), we described the attention layer proposed in <cite>Vaswani et al. (2017)</cite> comprising the multi-head attention sub-layer and a FFN sub-layer. For the Weighted Transformer, we propose a branched attention that modifies the entire attention layer in the Transformer network (including both the multi-head attention and the feed-forward network). The proposed attention layer can be described as:",
  "y": "uses"
 },
 {
  "id": "1c1b524d2bfe00c62a5a2e1a05ffc7_9",
  "x": "As in <cite>Vaswani et al. (2017)</cite> , we used the Adam optimizer (Kingma & Ba, 2014) with (\u03b2 1 , \u03b2 2 ) = (0.9, 0.98) and = 10 \u22129 . We also use the learning rate warm-up strategy for Adam wherein the learning rate lr takes on the form: for the all parameters except (\u03b1, \u03ba) and",
  "y": "similarities"
 },
 {
  "id": "1c1b524d2bfe00c62a5a2e1a05ffc7_10",
  "x": "We train the Weighted Transformer for the respective variants for 60K and 250K iterations. We found that the objective did not significantly improve by running it for longer. Further, we do not use any averaging strategies employed in <cite>Vaswani et al. (2017)</cite> and simply return the final model for testing purposes.",
  "y": "differences"
 },
 {
  "id": "1c1b524d2bfe00c62a5a2e1a05ffc7_13",
  "x": "Transformer (large)<cite> (Vaswani et al., 2017)</cite> 28.4 41.0 Weighted Transformer (large) 28.9 41.4 ByteNet (Kalchbrenner et al., 2016) 23.7 -Deep-Att+PosUnk (Zhou et al., 2016) -39.2 GNMT+RL (Wu et al., 2016) 24.6 39.9 ConvS2S (Gehring et al., 2017) 25.2 40.5 MoE 26.0 40.6 Table 1 : Experimental results on the WMT 2014 English-to-German (EN-DE) and English-toFrench (EN-FR) translation tasks. Our proposed model outperforms the state-of-the-art models including the Transformer<cite> (Vaswani et al., 2017)</cite> .",
  "y": "differences"
 },
 {
  "id": "1c86f563ababf5ec3c67cbf259252b_0",
  "x": "While extractive summarization only selects important sentences from the input, abstractive summarization generates content without explicitly re-using whole sentences (Nenkova et al., 2011) resulting summaries that are more fluent. In recent years, a number of successful approaches have been proposed for both extractive (Nallapati et al., 2017; Narayan et al., 2018) and abstractive (See et al., 2017;<cite> Chen and Bansal, 2018)</cite> summarization paradigms. State-of-the-art abstractive approaches are supervised, relying on large collections of paired articles and summaries.",
  "y": "background"
 },
 {
  "id": "1c86f563ababf5ec3c67cbf259252b_1",
  "x": "State-ofthe-art approaches are typically trained to generate summaries either in a fully end-to-end fashion (See et al., 2017) , processing the entire article at once; or hierarchically, first extracting content and then paraphrasing it sentence-by-sentence <cite>(Chen and Bansal, 2018)</cite> . Both approaches rely on large collections of article-summary pairs such as the annotated Gigaword (Napoles et al., 2012) or the CNN/DailyMail (Nallapati et al., 2016) dataset. The heavy reliance on manually created resources prohibits the use of abstractive summarization in domains other than news articles, or languages other than English, where parallel data may not be as abundantly available.",
  "y": "background"
 },
 {
  "id": "1c86f563ababf5ec3c67cbf259252b_2",
  "x": "Our approach is similar to <cite>(Chen and Bansal, 2018)</cite> , except that they use parallel data to train their extractors and abstractors. In contrast, during training, we only assume access to example summaries S S S = {s s s 0 , .., s s s M } without matching articles. During testing, given an input article a a a = {a 0 , ..., a N } consisting of N sentences, our system is capable of generating a multi-sentence abstractive summary consisting of K sentences (where K is a hyperparameter).",
  "y": "similarities differences"
 },
 {
  "id": "1c86f563ababf5ec3c67cbf259252b_3",
  "x": "We use the CNN/DailyMail (CNN/DM) dataset (Hermann et al., 2015) consisting of pairs of news articles from CNN and Daily Mail, along with summaries in the form of bullet points. We choose this dataset because it allows us to compare our approach to existing fully supervised methods and to measure the gap between unsupervised and supervised summarization. We follow the preprocessing pipeline of <cite>(Chen and Bansal, 2018)</cite> , splitting the dataset into 287k/11k/11k pairs for training/validation/testing.",
  "y": "uses"
 },
 {
  "id": "1c86f563ababf5ec3c67cbf259252b_4",
  "x": "P PM and P BT are both implemented as bidirectional LSTM encoderdecoder models with 256 hidden units, embedding dimension 128, and an attention mechanism (Bahdanau et al., 2014) . We pick this model size to be comparable to recent work (See et al., 2017;<cite> Chen and Bansal, 2018)</cite> . Our models are initialized and trained separately, but they share the same 50k byte pair encoding (Sennrich et al., 2016b) vocab- 1 We also experimented with sampling (Edunov et al., 2018) but found it to be too noisy in the current setting.",
  "y": "similarities uses"
 },
 {
  "id": "1c86f563ababf5ec3c67cbf259252b_5",
  "x": "EXT-ABS is the hierarchical model from <cite>(Chen and Bansal, 2018)</cite> , consisting of a supervised LSTM extractor and separate abstractor, both of which are individually trained on the CNN/DM dataset by aligning summary to article sentences. Our work best resembles EXT-ABS except that we do not rely on any parallel data. EXT-ABS-RL is a state-of-theart summarization system that extends EXT-ABS by jointly tuning the two supervised components using reinforcement learning.",
  "y": "uses similarities differences"
 },
 {
  "id": "1c86f563ababf5ec3c67cbf259252b_6",
  "x": "EXT-ABS is the hierarchical model from <cite>(Chen and Bansal, 2018)</cite> , consisting of a supervised LSTM extractor and separate abstractor, both of which are individually trained on the CNN/DM dataset by aligning summary to article sentences. Our work best resembles EXT-ABS except that we do not rely on any parallel data. EXT-ABS-RL is a state-of-theart summarization system that extends EXT-ABS by jointly tuning the two supervised components using reinforcement learning.",
  "y": "background"
 },
 {
  "id": "1dd3adcb79c8bc4b5187b85d836ceb_0",
  "x": "This approach has been shown to be accurate, relatively efficient, and robust using both generative and discriminative models (Roark, 2001; Roark, 2004; <cite>Collins and Roark, 2004</cite>) . The key to effective beam-search parsing is comparability of analyses when the pruning is done. If two competing parses are at different points in their respective derivations, e.g. one is near the end of the derivation and another is near the beginning, then it will be difficult to evaluate which of the two is likely to result in a better parse.",
  "y": "uses"
 },
 {
  "id": "1dd3adcb79c8bc4b5187b85d836ceb_1",
  "x": "Beam-search parsing using an unnormalized discriminative model, as in <cite>Collins and Roark (2004)</cite> , requires a slightly different search strategy than the original generative model described in Roark (2001; 2004) . This alternate search strategy is closer to the approach taken in Costa et al. (2001; 2003) , in that it enumerates a set of possible ways of attaching the next word before evaluating with the model. This ensures comparability for models that do not have the sort of behavior described above for the generative models, rendering look-ahead statistics difficult to estimate.",
  "y": "uses"
 },
 {
  "id": "1dd3adcb79c8bc4b5187b85d836ceb_2",
  "x": "This approach is effective, although somewhat less so than when a look-ahead statistic is used. A generative parsing model can be used on its own, and it was shown in <cite>Collins and Roark (2004)</cite> that a discriminative parsing model can be used on its own. Most discriminative parsing approaches, e.g. (Johnson et al., 1999; Collins, 2000; Collins and Duffy, 2002) , are re-ranking approaches, in which another model (typically a generative model) presents a relatively small set of candidates, which are then re-scored using a second, discriminatively trained model.",
  "y": "background"
 },
 {
  "id": "1dd3adcb79c8bc4b5187b85d836ceb_3",
  "x": "For good parses, the look-ahead statistic should increase with each step of the derivation, ensuring a certain degree of comparability among competing parses with the same look-ahead. Beam-search parsing using an unnormalized discriminative model, as in <cite>Collins and Roark (2004)</cite> , requires a slightly different search strategy than the original generative model described in Roark (2001; 2004) . This alternate search strategy is closer to the approach taken in Costa et al. (2001; 2003) , in that it enumerates a set of possible ways of attaching the next word before evaluating with the model.",
  "y": "uses"
 },
 {
  "id": "1dd3adcb79c8bc4b5187b85d836ceb_4",
  "x": "This ensures comparability for models that do not have the sort of behavior described above for the generative models, rendering look-ahead statistics difficult to estimate. This approach is effective, although somewhat less so than when a look-ahead statistic is used. A generative parsing model can be used on its own, and it was shown in <cite>Collins and Roark (2004)</cite> that a discriminative parsing model can be used on its own.",
  "y": "background"
 },
 {
  "id": "1deb67be8226867fe6b9514cdecdec_0",
  "x": "**INTRODUCTION** Recent successes in statistical syntactic parsing based on supervised learning techniques trained on a large corpus of syntactic trees (Collins, 1999; Charniak, 2000;<cite> Henderson, 2003)</cite> have brought forth the hope that the same approaches could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence. Moving towards a shallow semantic level of representation is a first initial step towards the distant goal of natural language understanding and has immediate applications in question-answering and information extraction.",
  "y": "background"
 },
 {
  "id": "1deb67be8226867fe6b9514cdecdec_1",
  "x": "While some of these models are based on full parse trees (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002) , other methods have been proposed that eschew the need for a full parse (CoNNL, 2004; CoNLL, 2005) . Because of the way the problem has been formulated -as a pipeline of parsing (or chunking) feeding into labelling -specific investigations of integrated approaches that solve both the parsing and the semantic role labelling problems at the same time have not been studied. We present work to test the hypothesis that a current statistical parser <cite>(Henderson, 2003)</cite> can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics.",
  "y": "extends uses"
 },
 {
  "id": "1deb67be8226867fe6b9514cdecdec_2",
  "x": "**THE BASIC PARSING ARCHITECTURE** To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers <cite>(Henderson, 2003)</cite> , which do not make any explicit independence assumptions, and are therefore likely to adapt without much modification to the current problem. This architecture has shown state-of-the-art performance.",
  "y": "uses motivation"
 },
 {
  "id": "1deb67be8226867fe6b9514cdecdec_3",
  "x": "Because the history representation computed for the move i \u2212 1 is included in the inputs to the computation of the representation for the next move i, virtually any information about the derivation history could flow from history representation to history representation and be used to estimate the probability of a derivation move. However, the recency preference exhibited by recursively defined neural networks biases learning towards information which flows through fewer history representations. <cite>(Henderson, 2003)</cite> exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step.",
  "y": "background"
 },
 {
  "id": "1deb67be8226867fe6b9514cdecdec_4",
  "x": "Because the history representation computed for the move i \u2212 1 is included in the inputs to the computation of the representation for the next move i, virtually any information about the derivation history could flow from history representation to history representation and be used to estimate the probability of a derivation move. However, the recency preference exhibited by recursively defined neural networks biases learning towards information which flows through fewer history representations. <cite>(Henderson, 2003)</cite> exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step.",
  "y": "motivation"
 },
 {
  "id": "1deb67be8226867fe6b9514cdecdec_5",
  "x": "The directed arcs represent the information that flows from one node to another. According to the original SSN model in <cite>(Henderson, 2003)</cite> , only the information carried over by the leftmost child and the most recent child of a constituent directly flows to that constituent. In the figure above, only the information conveyed by the nodes \u03b1 and \u03b4 is directly input to the node S. Similarly, the only bottom-up information directly input to the VP node is conveyed by the child nodes \u01eb and \u03b8.",
  "y": "background"
 },
 {
  "id": "1deb67be8226867fe6b9514cdecdec_7",
  "x": "The third line of Table 1 gives the performance on the simpler PTB parsing task of the original SSN parser <cite>(Henderson, 2003)</cite> , that was trained on the PTB data sets contrary to our SSN model trained on the PropBank data sets. These results clearly indicate that our model can perform the PTB parsing task at levels of per-3 Such pairs consists of a tag and a word token.",
  "y": "similarities"
 },
 {
  "id": "1ebbddc6c6740aea71ade2ed915de4_0",
  "x": "Even training of a single state-of-the-art sentencelevel translational autoencoder requires days of GPU computing (Barzdins & Gosko, 2016) ) in TensorFlow (Abadi et al., 2015) seq2seq model (Sutskerev, Vinyals & Le, 2014; Bahdanau, Cho & Bengio, 2014) . To avoid complexities of asynchronous parallel training with shared parameter server (Dean et al., 2012) , the architecture in Fig.2 and Fig. 3 instead can be trained using the alternating training approach proposed in <cite>(Luong et al., 2016)</cite> , where each task is optimized for a fixed number of parameter updates (or mini-batches) before switching to the next task (which is a different language pair). Although such alternating approach prolongs the training process, it is preferred for simplicity and robustness reasons.",
  "y": "background"
 },
 {
  "id": "1ebbddc6c6740aea71ade2ed915de4_1",
  "x": "Neural translation attention mechanism (Bahdanau, Cho & Bengio, 2014) has been shown to be highly beneficial for bi-lingual neural translation of long sentences, but it is not compatible with the multi-task multilingual translation models (Dong et al., 2015;<cite> Luong et al, 2016)</cite> described in the previous Section and character-level translation models (Barzdins & Gosko, 2016) described in this Section. For these reasons we replace the neural translation attention mechanism with much simpler sliding-window translation (Barzdins & Gosko, 2016; Karpathy, 2015; Jozefowicz, 2016 ). Moving from wordlevel to character-level neural translation makes it even harder to cope with long sentences presenting additional reason to employ the sliding-window translation approach.",
  "y": "background"
 },
 {
  "id": "1f48420f55771e243c73babf54632f_0",
  "x": "**TUTORIAL DESCRIPTION** This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (Saon and Chien, 2012; Chan et al., 2016) to document summarization (Chang and Chien, 2009 ), text classification (Blei et al., 2003; Zhang et al., 2015) , text segmentation (Chien and Chueh, 2012) , information extraction (Narasimhan et al., 2016) , image caption generation (Vinyals et al., 2015; Xu et al., 2015) , sentence generation (<cite>Li et al., 2016b</cite>) , dialogue control (Zhao and Eskenazi, 2016; Li et al., 2016a) , sentiment classification, recommendation system, question answering (Sukhbaatar et al., 2015) and machine translation , to name a few. Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model.",
  "y": "uses"
 },
 {
  "id": "201aa2a740b5d45f273ee298595f5a_0",
  "x": "Computational linguists have recognised the importance of investigating the concreteness of contexts in empirical models, for example for the automatic identification of non-literal language usage (Turney et al., 2011; K\u00f6per and Schulte im Walde, 2016; Aedmaa et al., 2018) . Recently, multiple studies have focussed on providing a fine-grained analysis of the nature of concrete vs. abstract words from a corpus-based perspective (Bhaskar et al., 2017; Frassinelli et al., 2017; <cite>Naumann et al., 2018)</cite> . In these studies, the authors have shown a general but consistent pattern: concrete words have a preference to co-occur with other concrete words, while abstract words co-occur more frequently with abstract words.",
  "y": "background"
 },
 {
  "id": "201aa2a740b5d45f273ee298595f5a_1",
  "x": "In these studies, the authors have shown a general but consistent pattern: concrete words have a preference to co-occur with other concrete words, while abstract words co-occur more frequently with abstract words. Specifically,<cite> Naumann et al. (2018)</cite> performed their analyses across parts-of-speech by comparing the behaviour of nouns, verbs and adjectives in large-scale corpora. These results are not fully in line with various theories of cognition which suggest that both concrete and abstract words should co-occur more often with concrete words because concrete information links the real-world usage of both concrete and abstract words to their mental representation (Barsalou, 1999; Pecher et al., 2011) .",
  "y": "background"
 },
 {
  "id": "201aa2a740b5d45f273ee298595f5a_3",
  "x": "Given that participants did not have any overt information about part-of-speech (henceforth, POS) while performing the norming study, Brysbaert et al. added this information post-hoc from the SUBTLEX-US, a 51-million word subtitle corpus (Brysbaert and New, 2009) . In order to align the POS information to the current study, we disambiguated the POS of the normed words by extracting their most frequent POS from the 10-billion word corpus ENCOW16AX (see below for details). Moreover, as discussed in previous studies by<cite> Naumann et al. (2018)</cite> and Pollock (2018) , mid-range concreteness scores indicate words that are difficult to categorise unambiguously regarding their concreteness.",
  "y": "background"
 },
 {
  "id": "201aa2a740b5d45f273ee298595f5a_4",
  "x": "The overall distributions in Figure 2 are extremely consistent across syntactic relations: when looking at the means, the concreteness of nouns subcategorised by concrete verbs is significantly higher than the concreteness of nouns subcategorised by abstract verbs (all p-values < 0.001). This result is perfectly in line with the more general analysis by<cite> Naumann et al. (2018)</cite> . Table 1 investigates more deeply the interaction between the concreteness of verbs and nouns for different syntactic functions.",
  "y": "similarities"
 },
 {
  "id": "201aa2a740b5d45f273ee298595f5a_5",
  "x": "The general pattern already described in<cite> Naumann et al. (2018)</cite> is confirmed by our quantitative analysis: overall, concrete verbs predominantly subcategorise concrete nouns as subjects and direct objects, while abstract verbs predominantly subcategorise abstract nouns as subjects and direct objects. A qualitative analysis revealed that exceptions to the predominant same-class interaction indicate semantic effects in verb-noun interaction: collocation, metaphor and metonymy, which shows the usefulness of detecting abstractness in the contexts of verbs as salient features in automatic non-literal language identification. A slightly more variable pattern emerges when looking at prepositional objects.",
  "y": "similarities"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_0",
  "x": "For example, in Table 1 , the question Who cut something captures the traditional \"agent\" role. Previous attempts to annotate QA-SRL initially involved trained annotators (He et al., 2015) but later resorted to crowdsourcing (<cite>Fitzgerald et al., 2018</cite>) to achieve scalability. Naturally, employing crowd workers raises challenges when annotating semantic structures like SRL.",
  "y": "background"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_1",
  "x": "Previous attempts to annotate QA-SRL initially involved trained annotators (He et al., 2015) but later resorted to crowdsourcing (<cite>Fitzgerald et al., 2018</cite>) to achieve scalability. Naturally, employing crowd workers raises challenges when annotating semantic structures like SRL. As <cite>Fitzgerald et al. (2018)</cite> acknowledged, the main shortage of the large-scale 2018 dataset is the lack of recall, estimated by experts to be in the lower 70s. In light of this and other annotation inconsistencies, we propose an improved QA-SRL crowdsourcing protocol for high-quality annotation, allowing for substantially more reliable performance evaluation of QA-SRL parsers.",
  "y": "motivation"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_2",
  "x": "As <cite>Fitzgerald et al. (2018)</cite> acknowledged, the main shortage of the large-scale 2018 dataset is the lack of recall, estimated by experts to be in the lower 70s. In light of this and other annotation inconsistencies, we propose an improved QA-SRL crowdsourcing protocol for high-quality annotation, allowing for substantially more reliable performance evaluation of QA-SRL parsers. To address worker quality, we systematically screen workers, provide concise yet effective guidelines, and perform a short training procedure, all within a crowd-sourcing platform.",
  "y": "motivation"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_3",
  "x": "For example, in Table 1 , the question Who cut something captures the traditional \"agent\" role. Previous attempts to annotate QA-SRL initially involved trained annotators (He et al., 2015) but later resorted to crowdsourcing (<cite>Fitzgerald et al., 2018</cite>) to achieve scalability. Naturally, employing crowd workers raises challenges when annotating semantic structures like SRL.",
  "y": "motivation"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_4",
  "x": "Previous attempts to annotate QA-SRL initially involved trained annotators (He et al., 2015) but later resorted to crowdsourcing (<cite>Fitzgerald et al., 2018</cite>) to achieve scalability. Naturally, employing crowd workers raises challenges when annotating semantic structures like SRL. In light of this and other annotation inconsistencies, we propose an improved QA-SRL crowdsourcing protocol for high-quality annotation, allowing for substantially more reliable performance evaluation of QA-SRL parsers.",
  "y": "motivation"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_5",
  "x": "To address coverage, we employ two independent workers plus an additional one for consolidation -similar to conventional expert-annotation practices. In addition to yielding 25% more roles, our coverage gain is demonstrated by evaluating against expertly annotated data and comparison with PropBank (Section 4). To foster future research, we release an assessed high-quality gold dataset along with our reproducible protocol and evaluation scheme, and report the performance of the existing <cite>parser</cite> (<cite>Fitzgerald et al., 2018</cite>) as a baseline.",
  "y": "uses"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_6",
  "x": "In subsequent work, <cite>Fitzgerald et al. (2018)</cite> constructed a large-scale corpus and used it to train a <cite>parser</cite>. 1 <cite>They</cite> crowdsourced 133K verbs with 2.0 QA pairs per verb on average. Since crowd-workers had no prior training, quality was established using an additional validation step, where workers had to ascertain the validity of the question, but not of its answers.",
  "y": "background"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_7",
  "x": "Even though multiple annotators were shown to produce greater coverage, their released dataset was produced using only a single annotator per verb. In subsequent work, <cite>Fitzgerald et al. (2018)</cite> constructed a large-scale corpus and used it to train a <cite>parser</cite>. 1 <cite>They</cite> crowdsourced 133K verbs with 2.0 QA pairs per verb on average.",
  "y": "background"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_8",
  "x": "Corpora The original 2015 QA-SRL dataset (He et al., 2015) was annotated by non-expert workers after completing a brief training procedure. In subsequent work, <cite>Fitzgerald et al. (2018)</cite> constructed a large-scale corpus and used it to train a <cite>parser</cite>. As both 2015 and <cite>2018 datasets</cite> use a single question generator, both struggle with maintaining coverage.",
  "y": "motivation"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_9",
  "x": "Corpora The original 2015 QA-SRL dataset (He et al., 2015) was annotated by non-expert workers after completing a brief training procedure. In subsequent work, <cite>Fitzgerald et al. (2018)</cite> constructed a large-scale corpus and used it to train a <cite>parser</cite>. As both 2015 and <cite>2018 datasets</cite> use a single question generator, both struggle with maintaining coverage. Also noteworthy, is that while traditional SRL annotations contain a single authoritative and nonredundant annotation, the <cite>2018 dataset</cite> provides the raw annotations of all annotators. We found that these characteristics of the dataset impede its utility for future development of parsers.",
  "y": "motivation"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_10",
  "x": "In subsequent work, <cite>Fitzgerald et al. (2018)</cite> constructed a large-scale corpus and used it to train a <cite>parser</cite>. Also noteworthy, is that while traditional SRL annotations contain a single authoritative and nonredundant annotation, the <cite>2018 dataset</cite> provides the raw annotations of all annotators.",
  "y": "motivation"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_11",
  "x": "Screening and Training Our pool of annotators is selected after several short training rounds, with up to 15 predicates per round, in which they received extensive personal feedback. 1 out of 3 participants were selected after exhibiting good performance, tested against expert annotations. Annotation We adopt the annotation machinery of (<cite>Fitzgerald et al., 2018</cite>) implemented using Amazon's Mechanical Turk, 2 and annotate each predicate by 2 trained workers independently, while a third consolidates their annotations into a final set of roles and arguments.",
  "y": "uses"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_12",
  "x": "We annotated a sample taken from the Dense set on Wikinews and Wikipedia domains, each with 1000 sentences, equally divided between development and test. QA generating annotators are paid the same as in <cite>Fitzgerald et al. (2018)</cite> , while the consolidator is rewarded 5\u00a2 per verb and 3\u00a2 per question. Per predicate, on average, our cost is 54.2\u00a2, yielding 2.9 roles, compared to reported 2.3 valid roles with an approximated cost of 51\u00a2 per predicate for Dense.",
  "y": "uses"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_13",
  "x": "Evaluation in QA-SRL involves aligning predicted and ground truth argument spans and evaluating role label equivalence. Since detecting question paraphrases is still an open challenge, we propose both unlabeled and labeled evaluation metrics. Unlabeled Argument Detection (UA) Inspired by the method presented in (<cite>Fitzgerald et al., 2018</cite>) , arguments are matched using a span matching criterion of intersection over union \u2265 0.5 .",
  "y": "extends"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_14",
  "x": "Unlabeled Argument Detection (UA) Inspired by the method presented in (<cite>Fitzgerald et al., 2018</cite>) , arguments are matched using a span matching criterion of intersection over union \u2265 0.5 . To credit each argument only once, we employ maximal bipartite matching 4 between the two sets of arguments, drawing an edge for each pair that passes the above mentioned criterion. The resulting maximal matching determines the true-positive set, while remaining non-aligned arguments become false-positives or false-negatives.",
  "y": "extends background"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_15",
  "x": "---------------------------------- **LABELED ARGUMENT DETECTION (LA)** All aligned arguments from the previous step are inspected for label equivalence, similar to the joint evaluation reported in (<cite>Fitzgerald et al., 2018</cite>) .",
  "y": "uses"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_16",
  "x": "We later manually estimate the rate of correct equivalences missed by this conservative method. As we will see, our evaluation heuristics, adapted from those in <cite>Fitzgerald et al. (2018)</cite> , significantly underestimate agreement between annotations, hence reflecting performance lower bounds. Devising more tight evaluation measures remains a challenge for future research.",
  "y": "uses"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_17",
  "x": "We extend our metric for evaluating manual or automatic redundant annotations, like the Dense dataset or <cite>the parser</cite> in (<cite>Fitzgerald et al., 2018</cite>) , which predicts argument spans independently of each other. To that end, we ignore predicted arguments that match ground-truth but are not selected by the bipartite matching due to redundancy. After con-necting unmatched predicted arguments that overlap, we count one false positive for every connected component to avoid penalizing precision too harshly when predictions are redundant.",
  "y": "uses similarities"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_18",
  "x": "To illustrate the effectiveness of our new goldstandard, we use its Wikinews development set to evaluate the currently available <cite>parser</cite> from (<cite>Fitzgerald et al., 2018</cite>) . While <cite>the parser</cite> correctly predicts 82% of non-implied roles, it skips half of the implied ones.",
  "y": "motivation"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_19",
  "x": "**BASELINE PARSER EVALUATION** To illustrate the effectiveness of our new goldstandard, we use its Wikinews development set to evaluate the currently available <cite>parser</cite> from (<cite>Fitzgerald et al., 2018</cite>) . For each predicate, <cite>the parser</cite> classifies every span for being an argument, independently of the other spans.",
  "y": "uses background"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_20",
  "x": "To illustrate the effectiveness of our new goldstandard, we use its Wikinews development set to evaluate the currently available <cite>parser</cite> from (<cite>Fitzgerald et al., 2018</cite>) . As expected, <cite>the parser</cite>'s recall against our gold is substantially lower than the 84.2 recall reported in (<cite>Fitzgerald et al., 2018</cite>) against Dense, due to the limited recall of Dense relative to our gold set.",
  "y": "uses differences background"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_21",
  "x": "To illustrate the effectiveness of our new goldstandard, we use its Wikinews development set to evaluate the currently available <cite>parser</cite> from (<cite>Fitzgerald et al., 2018</cite>) . Based on this inspection, <cite>the parser</cite> completely misses 23% of the 154 roles present in the gold-data, out of which, 17% are implied.",
  "y": "motivation"
 },
 {
  "id": "211b889125682f2596f708be1e83b9_22",
  "x": "---------------------------------- **REDUNDANT PARSER OUTPUT** As mentioned in the paper body, the <cite>Fitzgerald et al. parser</cite> generates redundant role questions and answers.",
  "y": "motivation differences background"
 },
 {
  "id": "22253d7b7cd43697b99909e09e7ebb_0",
  "x": "---------------------------------- **DYNAMICS IN THE SPACE OF CODE PARAMETERS** In <cite>[17]</cite> a dynamical model of language change was proposed, based on a spin glass model for syntactic parameters and language interactions.",
  "y": "background"
 },
 {
  "id": "22253d7b7cd43697b99909e09e7ebb_1",
  "x": "In the case of syntactic parameters behaving as independent variables, in the low temperature regime (see <cite>[17]</cite> for a discussion of the interpretation of the temperature parameter in this model) the dynamics converges rapidly towards an equilibrium state where all the spin variables corresponding to a given syntactic feature for the various languages align to the value most prevalent in the initial configuration. The SSWL database does not record relations between parameters, although it can be shown by other approaches that interesting relations are present, see [14] , [15] . Using syntactic data from [6] , [7] , which record explicit entailment relation between different parameter, it was shown in <cite>[17]</cite> , for small graph examples, that in the presence of relations the dynamics settles on equilibrium states that are not necessarily given by completely aligned spins.",
  "y": "background"
 },
 {
  "id": "22253d7b7cd43697b99909e09e7ebb_2",
  "x": "In the case of syntactic parameters behaving as independent variables, in the low temperature regime (see <cite>[17]</cite> for a discussion of the interpretation of the temperature parameter in this model) the dynamics converges rapidly towards an equilibrium state where all the spin variables corresponding to a given syntactic feature for the various languages align to the value most prevalent in the initial configuration. The SSWL database does not record relations between parameters, although it can be shown by other approaches that interesting relations are present, see [14] , [15] . Using syntactic data from [6] , [7] , which record explicit entailment relation between different parameter, it was shown in <cite>[17]</cite> , for small graph examples, that in the presence of relations the dynamics settles on equilibrium states that are not necessarily given by completely aligned spins.",
  "y": "background"
 },
 {
  "id": "22253d7b7cd43697b99909e09e7ebb_3",
  "x": "Using syntactic data from [6] , [7] , which record explicit entailment relation between different parameter, it was shown in <cite>[17]</cite> , for small graph examples, that in the presence of relations the dynamics settles on equilibrium states that are not necessarily given by completely aligned spins. 4.1. Spin glass models for syntactic parameters. When we interpret the dynamics of the model considered in <cite>[17]</cite> in terms of codes and the space of code parameters, the initial datum of the set of languages L at the vertices of the graph, with its given list of syntactic binary variables, determines a code C L .",
  "y": "uses"
 },
 {
  "id": "22253d7b7cd43697b99909e09e7ebb_4",
  "x": "Thus, in this case where each syntactic variables runs as an independent Ising model, the minimum is achieved where , that is, when all the spins align. In the presence of entailment relations between different syntactic variables, it was shown in <cite>[17]</cite> that the Hamiltonian should be modified by a term that introduces the relations as a Lagrange multiplier.",
  "y": "background"
 },
 {
  "id": "22253d7b7cd43697b99909e09e7ebb_5",
  "x": "Given an initial condition x 0 \u2208 {0, 1} n L and the datum (J e ) e\u2208E(G L ) of the strengths of the interaction energies along the edges, the same method used in <cite>[17]</cite> , based on the standard Metropolis-Hastings algorithm, can be used to study the dynamics in this setting, with a similar behavior. In the space of code parameters, given the code point (\u03b4 0 , R 0 ) = (\u03b4(C(x 0 )), R(C(x 0 ))) associated to the initial condition x 0 , the dynamics moves the code point along the line with constant R = R 0 . As the dynamics approaches the minimum of the action, the code point enters the region below the Gilbert-Varshamov bound, as it moves towards smaller values of \u03b4.",
  "y": "uses background"
 },
 {
  "id": "22253d7b7cd43697b99909e09e7ebb_6",
  "x": "Consider the very small example, with just two entailed syntactic variables and four languages, discussed in <cite>[17]</cite> , where the chosen languages are L = { 1 , 2 , 3 , 4 } = {English, Welsh, Russian, Bulgarian} and the two syntactic parameters are {x 1 , x 2 } = {StrongDeixis, StrongAnaphoricity}. Since we have an entailment relation, the possible values of the variables x i are now ternary, x i ( ) \u2208 {0, \u22121, +1}, that is, we consider here codes C \u2282 F n 3 . In this example n = 2. The initial condition x 0 is given by",
  "y": "background"
 },
 {
  "id": "22253d7b7cd43697b99909e09e7ebb_7",
  "x": "In these models, where part of the syntactic variables x i \u2208 B are seen as binary variables and part x j \u2208 T as ternary variables, for the purpose of coding theory, we consider the whole x = (x i ) n i=1 as a vector in F n 3 , in order to compute the code parameters of the resulting code C(L) \u2282 F n 3 . One can see already in a very simple example, and using the dynamical system in the form described in <cite>[17]</cite> , that the dynamics in the space of code parameters now does not need to move towards the \u03b4 = 0 line. Consider the very small example, with just two entailed syntactic variables and four languages, discussed in <cite>[17]</cite> , where the chosen languages are L = { 1 , 2 , 3 , 4 } = {English, Welsh, Russian, Bulgarian} and the two syntactic parameters are {x 1 , x 2 } = {StrongDeixis, StrongAnaphoricity}. Since we have an entailment relation, the possible values of the variables x i are now ternary, x i ( ) \u2208 {0, \u22121, +1}, that is, we consider here codes C \u2282 F n 3 .",
  "y": "background"
 },
 {
  "id": "22253d7b7cd43697b99909e09e7ebb_8",
  "x": "We consider in this case the same dynamical system used in <cite>[17]</cite> to model the case with entailment, which is a modification of the Ising model to a coupling of an Ising and a Potts model with q = 3 at the vertices of the graph. This dynamics, which depends on the temperature parameter T = 1/\u03b2 an on an auxiliary parameter E, the \"entailment energy\", that measures how strongly the entailment relation is enforced. In the cases with high temperature and either high or low entailment energy, it is shown in <cite>[17]</cite> that one can have equilibrium states like",
  "y": "uses"
 },
 {
  "id": "22253d7b7cd43697b99909e09e7ebb_9",
  "x": "In the cases with high temperature and either high or low entailment energy, it is shown in <cite>[17]</cite> that one can have equilibrium states like for the high entailment energy case, or for the low entailment energy case.",
  "y": "background"
 },
 {
  "id": "22253d7b7cd43697b99909e09e7ebb_10",
  "x": "The example mentioned above is too simple and artificial to be significant, but we can analyze a more general situation, where we consider the full syntactic data of [6] , [7] , with all the entailment relations taken into account, and the same interaction energies along the edges as in <cite>[17]</cite> , taken from the data of [16] , which can be regarded as roughly proportional to a measure of the amount of bilingualism. When we work with the full set of data from [6] , [7] , involving 63 parameters for 28 languages (from which we exclude those that do not occur in the [16] data), we see that the large size of the graph and the presence of many entailment relations render the dynamics Figure 8 . Dynamics in the space of code parameters: average distance.",
  "y": "uses"
 },
 {
  "id": "22253d7b7cd43697b99909e09e7ebb_11",
  "x": "Dynamics in the space of code parameters: average distance. a lot more complicated than the simple examples discussed in <cite>[17]</cite> . Indeed for such a large graph the convergence of the dynamics becomes extremely slow, even in the low temperature case and even when entailment relations are switched off, as shown in the graph of the average magnetization in Figure 6 .",
  "y": "differences"
 },
 {
  "id": "22dc2a38e29a1f5ac55c9ac220782b_0",
  "x": "Recently, <cite>Vaswani et al. (2017)</cite> proposed the Transformer architecture for machine translation. It relies only on attention mechanisms, instead of making use of either recurrent or convolutional * Authors contributed equally to this work. neural networks.",
  "y": "background motivation"
 },
 {
  "id": "22dc2a38e29a1f5ac55c9ac220782b_1",
  "x": "They were introduced by Bahdanau et al. (2014) , where they achieved state-of-the-art in machine translation. Since then, attention mechanisms have been used in other language modeling tasks such as image captioning (Xu et al., 2015) , question answer-ing (Sukhbaatar et al., 2015; Choi et al., 2017) , and text classification (Yang et al., 2016) . The concept of self-attention (Cheng et al., 2016; Parikh et al., 2016) , central to our proposed approach, has shown great promises in natural language processing; It produced state-of-the-art results for machine translation<cite> (Vaswani et al., 2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "22dc2a38e29a1f5ac55c9ac220782b_2",
  "x": "The main difference with our work is, while being interpretable, these approaches do not perform true word-on-word attention across a whole sequence such as our self-attention layer. 3 SANet: Self-Attention Network Inspired by the Transformer architecture<cite> (Vaswani et al., 2017)</cite> which performed machine translation without recurrent or convolutional layers, we propose the Self-Attention Network (SANet) architecture targeting instead text classification. One key difference between our approach and <cite>Vaswani et al. (2017)</cite> 's is that we only perform input-input attention with self-attention, as we do not have sequences as output but a text classification.",
  "y": "differences"
 },
 {
  "id": "22dc2a38e29a1f5ac55c9ac220782b_3",
  "x": "One key difference between our approach and <cite>Vaswani et al. (2017)</cite> 's is that we only perform input-input attention with self-attention, as we do not have sequences as output but a text classification. Moreover, we employ global max pooling at the top, which enables our architecture to process input sequences of arbitrary length. . . . ; x T n ] be the concatenation of a sequence of n vectors giving a matrix X \u2208 R n\u00d7d such that x i \u2208 R d .",
  "y": "differences"
 },
 {
  "id": "22dc2a38e29a1f5ac55c9ac220782b_4",
  "x": "<cite>Vaswani et al. (2017)</cite> defined attention as a function with as input a triplet containing queries Q, keys K with associated values V . In the case of self-attention, Q, K and V are linear projections of X. Thus, we define the dot-product<cite> (Vaswani et al., 2017)</cite> . The self-attention block is repeated N times.",
  "y": "background"
 },
 {
  "id": "22dc2a38e29a1f5ac55c9ac220782b_5",
  "x": ". . . ; x T n ] be the concatenation of a sequence of n vectors giving a matrix X \u2208 R n\u00d7d such that x i \u2208 R d . <cite>Vaswani et al. (2017)</cite> defined attention as a function with as input a triplet containing queries Q, keys K with associated values V . In the case of self-attention, Q, K and V are linear projections of X. Thus, we define the dot-product<cite> (Vaswani et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "22dc2a38e29a1f5ac55c9ac220782b_6",
  "x": "We use the positional encoding vectors that were defined by <cite>Vaswani et al. (2017)</cite> as follows. A linear layer then performs dimensionality reduction/augmentation of the embedding space to a vector space of dimension d, which is kept constant throughout the network. It is followed by one or several \"self-attention blocks\" stacked one onto another.",
  "y": "similarities"
 },
 {
  "id": "22dc2a38e29a1f5ac55c9ac220782b_7",
  "x": "Contrary to <cite>Vaswani et al. (2017)</cite> , we only use a single attention head, with attention performed on the complete sequence with constant d-dimensional inputs. The feed-forward network consists of a single hidden layer with a ReLU. Where W 1 , W 2 \u2208 R d\u00d7d are learned parameters.",
  "y": "differences"
 },
 {
  "id": "23119eff3cfd71370e8ad408fc75e1_0",
  "x": "Very recently,<cite> Lee et al. (2017)</cite> proposed the first state-of-the-art end-to-end neural coreference resolution system. They consider all text spans as potential mentions and therefore eliminate the need of carefully hand-engineered mention detection systems. In addition, thanks to the representation power of pre-trained word embeddings and deep neural networks, the model only uses a minimal set of hand-engineered features (speaker ID, document genre, span distance, span width).",
  "y": "background"
 },
 {
  "id": "23119eff3cfd71370e8ad408fc75e1_1",
  "x": "Figure 1 illustrates our model. We adopt the same span representation approach as in<cite> Lee et al. (2017)</cite> using bidirectional LSTMs and a headfinding attention. Thereafter, a feed forward network produces scores for spans being entity mentions.",
  "y": "similarities uses"
 },
 {
  "id": "23119eff3cfd71370e8ad408fc75e1_2",
  "x": "Compared with the traditional FFNN approach in<cite> Lee et al. (2017)</cite> , biaffine attention directly models both the compatibility of s i and s j by\u015d j U bi\u015di and the prior likelihood of s i having an antecedent by v bi\u015d i . Inference The final coreference score s(i, j) for span s i and span s j consists of three terms: (1) if s i is a mention, (2) if s j is a mention, (3) if s j is an antecedent for s i . Furthermore, for dummy antecedent , we fix the final score to be 0:",
  "y": "uses similarities"
 },
 {
  "id": "23119eff3cfd71370e8ad408fc75e1_3",
  "x": "**JOINT MENTION DETECTION AND MENTION CLUSTER** During training, only mention cluster labels are available rather than antecedent links. Therefore,<cite> Lee et al. (2017)</cite> train the model end-to-end by maximizing the following marginal log-likelihood where GOLD(i) are gold antecedents for s i :",
  "y": "background"
 },
 {
  "id": "23119eff3cfd71370e8ad408fc75e1_4",
  "x": "We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in<cite> Lee et al. (2017)</cite> . We consider all spans up to 10 words and up to 250 antecedents.",
  "y": "similarities uses"
 },
 {
  "id": "23119eff3cfd71370e8ad408fc75e1_5",
  "x": "F1 Our model (single) 67.8 without mention detection loss 67.5 without biaffine attention 67.4<cite> Lee et al. (2017)</cite> 67.3 Table 2 : Ablation study on the development set. genre, span distance, span width) features as 20-dimensional learned embeddings. Word and character embeddings use 0.5 dropout.",
  "y": "background"
 },
 {
  "id": "23119eff3cfd71370e8ad408fc75e1_6",
  "x": "Based on the results on the development set, \u03bb detection = 0.1 works best from {0.05, 0.1, 0.5, 1.0}. Model is trained with ADAM optimizer (Kingma and Ba, 2015) and converges in around 200K updates, which is faster than that of<cite> Lee et al. (2017)</cite> . In particular, compared with<cite> Lee et al. (2017)</cite> , our improvement mainly results from the precision scores.",
  "y": "differences"
 },
 {
  "id": "23119eff3cfd71370e8ad408fc75e1_7",
  "x": "While Moosavi and Strube (2017) observe that there is a large overlap between the gold mentions of the training and dev (test) sets, we find that our model can correctly detect 1048 mentions which are not detected by<cite> Lee et al. (2017)</cite> , consisting of 386 mentions existing in training data and 662 mentions not existing in training data. From those 662 mentions, some examples are (1) a suicide murder (2) Hong Kong Island (3) a US Airforce jet carrying robotic undersea vehicles (4) the investigation into who was behind the apparent suicide attack. This shows that our mention loss helps detection by generalizing to new mentions in test data rather than memorizing the existing mentions in training data.",
  "y": "differences"
 },
 {
  "id": "23119eff3cfd71370e8ad408fc75e1_8",
  "x": "As summarized by Ng (2010) , learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008) . (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; <cite>Lee et al., 2017)</cite> . (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b) .",
  "y": "background"
 },
 {
  "id": "24506b0aa7a859eb8744e390f9fb60_0",
  "x": "Meanwhile, several previous works (Carreras, 2007; <cite>Koo and Collins, 2010)</cite> have shown that grandchild interactions provide important information for dependency parsing. However, the computational cost of the parsing algorithm increases with the need for more expressive factorizations. Consequently, the existing most powerful parser<cite> (Koo and Collins, 2010</cite> ) is limited to third-order parts, which requires O(n 4 ) time and O(n 3 ) space.",
  "y": "background"
 },
 {
  "id": "24506b0aa7a859eb8744e390f9fb60_1",
  "x": "Several previous works have shown that higher-order parsers utilizing richer contextual information achieve higher accuracy than lower-order ones- Chen et al. (2010) illustrated that a wide range of decision history can lead to significant improvements in accuracy for graph-based dependency parsing models. Meanwhile, several previous works (Carreras, 2007; <cite>Koo and Collins, 2010)</cite> have shown that grandchild interactions provide important information for dependency parsing. However, the computational cost of the parsing algorithm increases with the need for more expressive factorizations.",
  "y": "motivation"
 },
 {
  "id": "24506b0aa7a859eb8744e390f9fb60_2",
  "x": "Meanwhile, several previous works (Carreras, 2007; <cite>Koo and Collins, 2010)</cite> have shown that grandchild interactions provide important information for dependency parsing. However, the computational cost of the parsing algorithm increases with the need for more expressive factorizations. Consequently, the existing most powerful parser<cite> (Koo and Collins, 2010</cite> ) is limited to third-order parts, which requires O(n 4 ) time and O(n 3 ) space.",
  "y": "motivation"
 },
 {
  "id": "24506b0aa7a859eb8744e390f9fb60_3",
  "x": "Following<cite> Koo and Collins (2010)</cite> , we refer to these augmented structures as g-spans. The second-order parser proposed in Carreras (2007) is capable of scoring both sibling and grandchild parts with complexities of O(n 4 ) time and O(n 3 ) space. However, the parser suffers a crucial limitation that it can only evaluate events of grandchild parts for outermost grandchildren.",
  "y": "uses"
 },
 {
  "id": "24506b0aa7a859eb8744e390f9fb60_4",
  "x": "Following previous works (McDonald and Pereira, 2006; <cite>Koo and Collins, 2010)</cite> , the fourthorder parser captures not only features associated with corresponding fourth-order grand-trisibling parts, but also the features of relevant lower-order parts that are enclosed in its factorization. The lower-order features (first-order features of dependency parts and second-order features of grandchild and sibling parts) are based on feature sets from previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007) . We added lexicalized versions of several features.",
  "y": "uses"
 },
 {
  "id": "24506b0aa7a859eb8744e390f9fb60_5",
  "x": "The first set of features is defined to be 5-gram features that is a 5-tuple consisting of five relevant indices using words and POS tags. The second set of features is defined as backed-off features<cite> (Koo and Collins, 2010)</cite> for grand-tri-sibling part (g, s, r, m, t)-the 4-gram (g, r, m, t), which never exist in any lower-order part. The determination of this feature set is based on on experiments on the development data for both English and Chinese.",
  "y": "uses"
 },
 {
  "id": "24506b0aa7a859eb8744e390f9fb60_6",
  "x": "For English, POS tags are automatically assigned by the SVMTool tagger (Gimenez and Marquez, 2004) . The accuracy of the SVMTool tagger on PTB is 97.3%; For Chinese, we used gold-standard POS tags in CTB. Following<cite> Koo and Collins (2010)</cite> , two versions of POS tags are used for any features involve POS: one using is normal POS tags and another is a coarsened version of the POS tags.",
  "y": "uses"
 },
 {
  "id": "24506b0aa7a859eb8744e390f9fb60_7",
  "x": "We compare our method to first-order and secondorder sibling dependency parsers (McDonald and Pereira, 2006) , and two third-order graphbased parsers<cite> (Koo and Collins, 2010)</cite> . Additionally, we compare to a state-of-the-art graphbased parser (Zhang and McDonald, 2012) as well as a state-of-the-art transition-based parser (Zhang and Nivre, 2011) . Our experimental results show an improvement in performance over the results in Zhang and Nivre (2011) , which are based on a transition-based dependency parser with rich non-local features.",
  "y": "uses"
 },
 {
  "id": "24506b0aa7a859eb8744e390f9fb60_8",
  "x": "Our results are also better than the results of the two third-order graph-based dependency parsing models in<cite> Koo and Collins (2010)</cite> . Moreover, our algorithm achieves better parsing performance than the generalized higher-order parser with cubepruning (Zhang and McDonald, 2012) , which is the state-of-the-art graph-based dependency parser so far. The models marked \u2020 or \u2021 are not directly comparable to our work.",
  "y": "differences"
 },
 {
  "id": "24506b0aa7a859eb8744e390f9fb60_9",
  "x": "Here we compare our method to an implement of the third-order grand-sibling parser -whose parsing performance on CTB is not reported in<cite> Koo and Collins (2010)</cite> , and the dynamic programming transition-based parser of Huang and Sagae (2010) . Additionally, we compare to the state-of-the-art graph-based dependency parser (Zhang and McDonald, 2012) as well as a state-of-the-art transition-based parser (Zhang and Nivre, 2011) . The results indicates that our parser achieved significant improvement of the previous systems on this data set.",
  "y": "differences"
 },
 {
  "id": "247bbc4eb671895222065ed425f968_0",
  "x": "More recently, by employing various deep learning techniques, performance of speech recognition systems on the CTS task is getting close to human parity. Several sites have made significant progress to lower the WER to within the 5%-10% range on the Switchboard-CallHome subsets of the Hub5 2000 evaluation <cite>[2</cite>, 3, 4, 5] . Given the progress on conversational telephone speech, we focus on the other closely related broadcast news recognition task that received similar attention within the DARPA EARS program.",
  "y": "background"
 },
 {
  "id": "247bbc4eb671895222065ed425f968_1",
  "x": "More recently, by employing various deep learning techniques, performance of speech recognition systems on the CTS task is getting close to human parity. Several sites have made significant progress to lower the WER to within the 5%-10% range on the Switchboard-CallHome subsets of the Hub5 2000 evaluation <cite>[2</cite>, 3, 4, 5] . Given the progress on conversational telephone speech, we focus on the other closely related broadcast news recognition task that received similar attention within the DARPA EARS program.",
  "y": "motivation"
 },
 {
  "id": "247bbc4eb671895222065ed425f968_2",
  "x": "In terms of the amount of training data available from the DARPA EARS program for training systems on CTS and BN, there are a few significant differences as well. The CTS acoustic training corpus consists of approximately 2000 hours of speech with human transcriptions <cite>[2]</cite> . In other words, models being developed for BN typically use lightly supervised transcripts for training [6] .",
  "y": "background"
 },
 {
  "id": "247bbc4eb671895222065ed425f968_3",
  "x": "In <cite>[2,</cite> 3] we describe state-of-the-art speech recognition systems on the CTS task using multiple LSTM and ResNet acoustic models trained on various acoustic features along with word and character LSTMs and convolutional WaveNet-style language models. In this paper we develop a similar but simpler variant for BN.",
  "y": "motivation similarities"
 },
 {
  "id": "247bbc4eb671895222065ed425f968_4",
  "x": "We then train LSTM and residual network based acoustic models with a combination of n-gram and neural network language models on this selected data. In addition to automatic speech recognition results, similar to <cite>[2]</cite> , we also present human performance on the same BN test sets. These evaluations allow us to properly benchmark our automatic system performance.",
  "y": "similarities"
 },
 {
  "id": "247bbc4eb671895222065ed425f968_5",
  "x": "---------------------------------- **HUMAN TRANSCRIPTION EXPERIMENTS** Similar to <cite>[2]</cite> , human performance measurements on two broadcast news tasks -RT04 and DEV04F -are carried out by Appen.",
  "y": "similarities"
 },
 {
  "id": "247bbc4eb671895222065ed425f968_6",
  "x": "Both passes involved listening to the audio multiple times: around 3-4 times for the first pass and 1-2 times for the second. In order to use NISTs scoring tool, sclite [17] , the human annotations were converted into CTM files which have time-marked word boundary information. The transcriptions were also filtered to remove non-speech markers, partial words, punctuation marks etc as described in <cite>[2]</cite> .",
  "y": "uses"
 },
 {
  "id": "247bbc4eb671895222065ed425f968_8",
  "x": "In <cite>[2]</cite> , two kinds of acoustic models, a convolutional and a non-convolutional acoustic model with comparable performance, are used since they produce good complementary outputs which can be further combined for improved performance. The convolutional network used in that work is a residual network (ResNet) and an LSTM is used as the non-convolutional network. Similar to that work, in this paper also we train ResNet and LSTM based acoustic models.",
  "y": "similarities"
 },
 {
  "id": "247bbc4eb671895222065ed425f968_9",
  "x": "After the cross-entropy based training on the BN-1300 Corpus has converged we also sequence train the model using the 144 hours of carefully transcribed audio. To complement the LSTM acoustic model, we train a deep Residual Network based on the best performing architecture proposed in <cite>[2]</cite> . The ResNet has 12 residual blocks followed by 5 fully connected layers.",
  "y": "uses"
 },
 {
  "id": "247bbc4eb671895222065ed425f968_10",
  "x": "Table 3 shows the individual and combined results we obtain on both the test sets. In comparison with the results obtained on the CTS evaluation with similar acoustic models <cite>[2]</cite> , the LSTM and ResNet operate at similar WERs. Unlike results observed on the CTS task, no significant reduction in WER is obtained after scores from both the LSTM and ResNet models are combined.",
  "y": "similarities"
 },
 {
  "id": "247bbc4eb671895222065ed425f968_11",
  "x": "LSTM2-LM is also used to rescore word lattices independently. Table 4 shows the results after our rescoring experiments. We observe significant WER gains after using the LSTM LMs similar to those reported in <cite>[2]</cite> .",
  "y": "similarities"
 },
 {
  "id": "247bbc4eb671895222065ed425f968_12",
  "x": "Table 7 . Most frequent deletion and insertion errors for humans and ASR systems on DEV04F and RT04 4. Compared to the telephone conversation confusions recorded in <cite>[2]</cite> -one symbol that is clearly missing is the back-channel response -this is probably from the very nature of the BN domain.",
  "y": "differences"
 },
 {
  "id": "247bbc4eb671895222065ed425f968_13",
  "x": "4. Compared to the telephone conversation confusions recorded in <cite>[2]</cite> -one symbol that is clearly missing is the back-channel response -this is probably from the very nature of the BN domain. 5. Similar to telephone conversation confusions reported in <cite>[2]</cite> , humans performance is much higher because the number of deletions is significantly lower -compare 2.3% vs 0.8%/0.6% for deletion errors in Table 5 . ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "24b38363d53468175e0274ac0b4fd3_0",
  "x": "Excitingly, the state of the art has recently shifted toward novel semi-supervised techniques such as the incorporation of word embeddings to represent the context of words and concepts<cite> (Tang et al., 2014b)</cite> . Moreover, it is important to be able to identify sentiment in relation to particular entities, topics or events (aspect-based sentiment). We have followed a hybrid approach which incorporates traditional lexica, unigrams and bigrams as well as word embeddings using word2vec (Mikolov et al., 2013) to train classifiers for subtasks A and B. For subtask C, sentiment targeted towards a particular topic, we have developed a set of different strategies which use either syntactic dependencies or token-level associations with the topic word in combination with our A classifier to produce sentiment annotations.",
  "y": "background"
 },
 {
  "id": "24b38363d53468175e0274ac0b4fd3_1",
  "x": "Here we employ a combination of lexical features and word embeddings to maximise our performance in task A. We build phrase-based classifiers both with an emphasis on the distinction between positive and negative sentiment, which conforms to the distribution of training data in task A, as well as phrasebased classifiers trained on a balanced set of positive, negative and neutral tweets. We use the latter to identify sentiment in the vicinity of topic words in task C, for targeted sentiment assignment. In previous work (Tang et al., 2014a; <cite>Tang et al., 2014b)</cite> sentiment-specific word embeddings have been used as features for identification of tweet-level sentiment but not phrase-level sentiment.",
  "y": "background"
 },
 {
  "id": "24b38363d53468175e0274ac0b4fd3_2",
  "x": "In previous work (Tang et al., 2014a; <cite>Tang et al., 2014b)</cite> sentiment-specific word embeddings have been used as features for identification of tweet-level sentiment but not phrase-level sentiment. In this work we present two different strategies for learning phrase level sentiment specific word embeddings.",
  "y": "background motivation"
 },
 {
  "id": "24b38363d53468175e0274ac0b4fd3_3",
  "x": "For each strategy, class and dimension, we used the functions suggested by<cite> (Tang et al., 2014b</cite> ) (average, maximum and minimum), resulting in 2,400 features. Extra Features: We used several features, potentially indicative of sentiment, a subset of those in (Mohammad et al., 2013) . These include: the total number of words of the target phrase, its position within the tweet (\"start\", \"end\", or \"other\"), the average word length of the target/context and the presence of elongated words, URLs and user mentions.",
  "y": "uses"
 },
 {
  "id": "24b38363d53468175e0274ac0b4fd3_4",
  "x": "We learned positive and negative word embeddings separately by training on the HAPPY and NON-HAPPY tweets from Purver & Battersby's multi-class Twitter emoticon and hashtag corpus (Purver and Battersby, 2012) , as with subtask A. The difference with subtask A is that here we used the whole tweet as our input (compared to the two-sided window around a polarised word in subtask A) in order to create tweet-level representations. We set the word embeddings dimension to 100 in order to gain enough semantic information whilst reducing training time. We also employed the word embeddings encoding sentiment information generated through the unified models in<cite> (Tang et al., 2014b)</cite> .",
  "y": "uses"
 },
 {
  "id": "24b38363d53468175e0274ac0b4fd3_5",
  "x": "Table 2 demonstrates that representing the tweet with positive and negative word embeddings is the most effective feature (performance is affected the most when we remove these) followed by the manually generated lexicon-based features. This combined with a 2% reduction in F1 score when the embeddings are removed, indicates that the embeddings improve sentiment analysis performance. Contrary to the approach by<cite> (Tang et al., 2014b)</cite> , we didn't integrate the sentiment information in the word embeddings training process, but rather the sentiment-specific nature of the embeddings was reflected in the choice of different training datasets, yielding different word embedding features for positive and negative tweets.",
  "y": "differences"
 },
 {
  "id": "24ee9b2bd8c97cbe923bc747b09806_0",
  "x": "In the current study we present an image-caption retrieval model that extends our previous work to spoken input. In [12, 13] , the authors adapted text based caption-image retrieval (e.g. [9] ) and showed that it is possible to perform speech-image retrieval using convolutional neural networks on spectral features. Our work is most closely related to the models presented in [12, 13, 14,<cite> 15]</cite> .",
  "y": "similarities"
 },
 {
  "id": "24ee9b2bd8c97cbe923bc747b09806_1",
  "x": "Our work is most closely related to the models presented in [12, 13, 14,<cite> 15]</cite> . In the current study we improve upon these previous approaches to visual grounding of speech and present state-of-the-art image-caption retrieval results. The work by [12, 13, 14,<cite> 15]</cite> and the results presented here are a step towards more cognitively plausible models of language learning as it is more natural to learn language without prior assumptions about the lexical level.",
  "y": "extends similarities"
 },
 {
  "id": "24ee9b2bd8c97cbe923bc747b09806_2",
  "x": "In the current study we improve upon these previous approaches to visual grounding of speech and present state-of-the-art image-caption retrieval results. The work by [12, 13, 14,<cite> 15]</cite> and the results presented here are a step towards more cognitively plausible models of language learning as it is more natural to learn language without prior assumptions about the lexical level. For instance, research indicates that the adult lexicon contains many relatively fixed multi-word expressions (e.g., 'how-are-you-doing') [16] .",
  "y": "background"
 },
 {
  "id": "24ee9b2bd8c97cbe923bc747b09806_3",
  "x": "Our model consists of two parts; an image encoder and a sentence encoder as depicted in Figure 1 . The approach is based on our own text-based model described in [8] and on the speech-based models described in [13,<cite> 15]</cite> and we refer to those studies for more details. Here, we focus on the differences with previous work.",
  "y": "uses background"
 },
 {
  "id": "24ee9b2bd8c97cbe923bc747b09806_4",
  "x": "We use importance sampling to select the mismatched pairs; rather than using all the other samples in the mini-batch as mismatched pairs (as done in [8,<cite> 15]</cite> ), we calculate the loss using only the hardest examples (i.e. mismatched pairs with high cosine similarity). While [10] used only the single hardest example in the batch for text-captions, we found that this did not work for the spoken captions. Instead we found that using the hardest 25 percent worked well.",
  "y": "differences"
 },
 {
  "id": "24ee9b2bd8c97cbe923bc747b09806_5",
  "x": "For ensembling we use the two snapshots with the highest performance on the development data and simply sum their embeddings. The main differences with the approaches described in [13,<cite> 15]</cite> are the use of multi-layered GRUs, importance sampling, the cyclic learning rate, snapshot ensembling and the use of vectorial rather than scalar attention. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "24ee9b2bd8c97cbe923bc747b09806_6",
  "x": "**WORD PRESENCE DETECTION** While our model is not explicitly trained to recognise words or segment the speech signal, previous work has shown that such information can be extracted by visual grounding models<cite> [15,</cite> 28] . <cite>[15]</cite> use a binary decision task: given a word and a sentence embedding, decide if the word occurs in the sentence.",
  "y": "background"
 },
 {
  "id": "24ee9b2bd8c97cbe923bc747b09806_7",
  "x": "While our model is not explicitly trained to recognise words or segment the speech signal, previous work has shown that such information can be extracted by visual grounding models<cite> [15,</cite> 28] . <cite>[15]</cite> use a binary decision task: given a word and a sentence embedding, decide if the word occurs in the sentence. Our approach is similar to the spoken-bag-of-words prediction task described in [28] .",
  "y": "background"
 },
 {
  "id": "24ee9b2bd8c97cbe923bc747b09806_8",
  "x": "The caption embeddings are ranked by cosine distance to the image and vice versa where R@N is the percentage of test items for which the correct image or caption was in the top N results. We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison. [12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention.",
  "y": "uses"
 },
 {
  "id": "24ee9b2bd8c97cbe923bc747b09806_9",
  "x": "We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison. [12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions The results of the word presence detection task are shown in Figure 2 and Table 2 .",
  "y": "background"
 },
 {
  "id": "24ee9b2bd8c97cbe923bc747b09806_10",
  "x": "We compare our models to [12] and <cite>[15]</cite> , and include our own character-based model for comparison. [12] is a convolutional approach, whereas <cite>[15]</cite> is an approach using recurrent highway networks with scalar attention. The character-based model is similar to the model we use here and was trained on the original Flickr8k text captions The results of the word presence detection task are shown in Figure 2 and Table 2 .",
  "y": "uses"
 },
 {
  "id": "24ee9b2bd8c97cbe923bc747b09806_11",
  "x": "Our results on both MBN and MFCC features are significantly higher than the previous state-of-the-art. The largest improvement comes from using the learned MBN features but our approach also improves results for MFCCs, which are the same features as were used in <cite>[15]</cite> . The learned MBN features provide better performance whereas the MFCCs are more cognitively plausible input features.",
  "y": "uses"
 },
 {
  "id": "24ee9b2bd8c97cbe923bc747b09806_12",
  "x": "In a probing task, we show that the model learns to recognise words in the input speech signal. We are currently collecting the Semantic Textual Similarity (STS) database in spoken format and the next step will be to investigate whether the model presented here also learns to capture sentence level semantic information and understand language in a deeper sense than recognising word presence. The work presented in <cite>[15]</cite> has made the first efforts in this regard and we aim to extend this to a larger database with sentences from multiple domains.",
  "y": "extends"
 },
 {
  "id": "24ee9b2bd8c97cbe923bc747b09806_13",
  "x": "We are currently collecting the Semantic Textual Similarity (STS) database in spoken format and the next step will be to investigate whether the model presented here also learns to capture sentence level semantic information and understand language in a deeper sense than recognising word presence. The work presented in <cite>[15]</cite> has made the first efforts in this regard and we aim to extend this to a larger database with sentences from multiple domains. Furthermore, we want to investigate the linguistic units that our model learns to recognise.",
  "y": "background"
 },
 {
  "id": "2504d707a8123774791d98b755551a_0",
  "x": "We present a simple two-stage approach where our second CRF uses features derived from the output of the first CRF. This gives us the advantage of defining a rich set of features to model non-local dependencies, and also eliminates the need to do approximate inference, since we do not explicitly capture the non-local dependencies in a single model, like the more complex existing approaches. This also enables us to do inference efficiently since our inference time is merely the inference time of two sequential CRF's; in contrast<cite> Finkel et al. (2005)</cite> reported an increase in running time by a factor of 30 over the sequential CRF, with their Gibbs sampling approximate inference.",
  "y": "differences"
 },
 {
  "id": "2504d707a8123774791d98b755551a_2",
  "x": "Here, we would expect that a subsequence would gain much more by knowing the label of a supersequence, than the other way around. However, as can be seen from table 2, we find that the consistency constraint does not hold nearly so strictly in this case. A very common case of this in the CoNLL dataset is that of documents containing references to both The China Daily, a newspaper, and China, the country<cite> (Finkel et al., 2005)</cite> .",
  "y": "similarities"
 },
 {
  "id": "2504d707a8123774791d98b755551a_3",
  "x": "With our two-stage approach, we manage to get improvements on the F1 measure over existing approaches that model non-local dependencies. At the same time, the simplicity of our two-stage approach keeps inference time down to just the inference time of two sequential CRFs, when compared to approaches such as those of<cite> Finkel et al. (2005)</cite> who report that their inference time with Gibbs sampling goes up by a factor of about 30, compared to the Viterbi algorithm for the sequential CRF. Below, we give some intuition about areas for improvement in existing work and explain how our approach incorporates the improvements.",
  "y": "differences"
 },
 {
  "id": "2504d707a8123774791d98b755551a_4",
  "x": "\u2022 Most existing work to capture labelconsistency, has attempted to create all n 2 pairwise dependencies between the different occurrences of an entity,<cite> (Finkel et al., 2005</cite>; Sutton and McCallum, 2004) , where n is the number of occurrences of the given entity. This complicates the dependency graph making inference harder. It also leads to the penalty for deviation in labeling to grow linearly with n, since each entity would be connected to \u0398(n) entities.",
  "y": "background"
 },
 {
  "id": "2504d707a8123774791d98b755551a_5",
  "x": "Below, we give some intuition about areas for improvement in existing work and explain how our approach incorporates the improvements. \u2022 Most existing work to capture labelconsistency, has attempted to create all n 2 pairwise dependencies between the different occurrences of an entity,<cite> (Finkel et al., 2005</cite>; Sutton and McCallum, 2004) , where n is the number of occurrences of the given entity. This complicates the dependency graph making inference harder.",
  "y": "motivation"
 },
 {
  "id": "2504d707a8123774791d98b755551a_6",
  "x": "We use this intuition to approximate the aggregate information about labels assigned to other occurrences of the entity by the nonlocal model, with the aggregate information about labels assigned to other occurrences of the entity by the sequence model. This intuition enables us to learn weights for non-local dependencies in two stages; we first get predictions from a regular sequential CRF and in turn use aggregate information about predictions made by the CRF as extra features to train a second CRF. \u2022 Most work has looked to model non-local dependencies only within a document<cite> (Finkel et al., 2005</cite>; Chieu and Ng, 2002; Sutton and McCallum, 2004; Bunescu and Mooney, 2004) .",
  "y": "background"
 },
 {
  "id": "2504d707a8123774791d98b755551a_7",
  "x": "Since this value is less than the cutoff threshold of 0.05, we reject the null hypothesis. The simplicity of our approach makes it easy to incorporate dependencies across the whole corpus, which would be relatively much harder to incorporate in approaches like (Bunescu and Mooney, 2004) and<cite> (Finkel et al., 2005)</cite> . Additionally, our approach makes it possible to do inference in just about twice the inference time with a single sequential CRF; in contrast, approaches like Gibbs Sampling that model the dependencies directly can increase inference time by a factor of 30<cite> (Finkel et al., 2005</cite> ).",
  "y": "differences"
 },
 {
  "id": "2504d707a8123774791d98b755551a_8",
  "x": "Additionally, our approach makes it possible to do inference in just about twice the inference time with a single sequential CRF; in contrast, approaches like Gibbs Sampling that model the dependencies directly can increase inference time by a factor of 30<cite> (Finkel et al., 2005</cite> ). An analysis of errors by the first stage CRF revealed that most errors are that of single token entities being mislabeled or missed altogether followed by a much smaller percentage of multiple token entities mislabelled completely. All our features directly encode information that is useful to reducing these errors.",
  "y": "differences"
 },
 {
  "id": "2504d707a8123774791d98b755551a_9",
  "x": "We also compare our performance against (Bunescu and Mooney, 2004) and<cite> (Finkel et al., 2005)</cite> and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF. named entities. Other kinds of boundary detection errors involving multiple tokens are very rare.",
  "y": "differences"
 },
 {
  "id": "2504d707a8123774791d98b755551a_10",
  "x": "Recent work looking to directly model non-local dependencies and do approximate inference are that of Bunescu and Mooney (2004) , who use a Relational Markov Network (RMN) (Taskar et al., 2002) to explicitly model long-distance dependencies, Sutton and McCallum (2004) , who introduce skip-chain CRFs, which add additional non-local edges to the underlying CRF sequence model (which Bunescu and Mooney (2004) lack) and<cite> Finkel et al. (2005)</cite> who hand-set penalties for inconsistency in labels based on the training data and then use Gibbs Sampling for doing approximate inference where the goal is to obtain the label sequence that maximizes the product of the CRF objective function and their penalty. Unfortunately, in the RMN model, the dependencies must be defined in the model structure before doing any inference, and so the authors use heuristic part-of-speech patterns, and then add dependencies between these text spans using clique templates. This generates an extremely large number of overlapping candidate entities, which renders necessary additional templates to enforce the constraint that text subsequences cannot both be different entities, something that is more naturally modeled by a CRF.",
  "y": "background"
 },
 {
  "id": "2504d707a8123774791d98b755551a_11",
  "x": "The approach of<cite> Finkel et al. (2005)</cite> makes it possible a to model a broader class of longdistance dependencies than Sutton and McCallum (2004) , because they do not need to make any initial assumptions about which nodes should be connected and they too model dependencies between whole token sequences representing entities and between entity token sequences and their token supersequences that are entities. The disadvantage of their approach is the relatively ad-hoc selection of penalties and the high computational cost of running Gibbs sampling. Early work in discriminative NER employed two stage approaches that are broadly similar to ours, but the effectiveness of this approach appears to have been overlooked in more recent work.",
  "y": "background"
 },
 {
  "id": "250a88831a4911f76acca3c9d318de_0",
  "x": "In this paper, we describe a novel algorithm that combines the last two steps into a single scan process. The algorithm, which is an extension of<cite> Sassano's (2004)</cite> , allows us to chunk morphemes into base phrases and decide dependency relations of the phrases in a strict left-toright manner. We show a pseudo code of the algorithm and evaluate its performance empirically with the voted perceptron on the Kyoto University Corpus (Kurohashi and Nagao, 1998) .",
  "y": "uses"
 },
 {
  "id": "250a88831a4911f76acca3c9d318de_1",
  "x": "A bunsetsu is a base phrasal unit and consists of one or more content words followed by zero or more function words. In addition, most of algorithms of Japanese dependency parsing, e.g., (Sekine et al., 2000;<cite> Sassano, 2004)</cite> , assume the three constraints below. (1) Each bunsetsu has only one head except the rightmost one. (2) Dependency links between bunsetsus go from left to right. (3) Dependency links do not cross one another.",
  "y": "background"
 },
 {
  "id": "250a88831a4911f76acca3c9d318de_2",
  "x": "Most of the modern dependency parsers for Japanese require bunsetsu chunking (base phrase chunking) before dependency parsing (Sekine et al., 2000; Kudo and Matsumoto, 2002;<cite> Sassano, 2004)</cite> . Although wordbased parsers are proposed in (Mori et al., 2000; Mori, 2002) , they do not build bunsetsus and are not compatible with other Japanese dependency parsers. Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences. But they are basically word-based.",
  "y": "background"
 },
 {
  "id": "250a88831a4911f76acca3c9d318de_3",
  "x": "---------------------------------- **PSEUDO CODE FOR THE PROPOSED ALGORITHM** The algorithm that we propose is based on<cite> (Sassano, 2004)</cite> , which is considered to be a simple form of shift-reduce parsing.",
  "y": "uses"
 },
 {
  "id": "250a88831a4911f76acca3c9d318de_4",
  "x": "The variable h j holds the head ID and the variable t j has the type of dependency relation. For example, the head and the dependency relation type of \"Meg\" in Figure 2 are represented as h 0 = 1 and t 0 = \"B\" respectively. The flow of the algorithm, which has the same structure as<cite> Sassano's (2004)</cite> , is controlled with a stack that holds IDs for modifier morphemes.",
  "y": "uses"
 },
 {
  "id": "250a88831a4911f76acca3c9d318de_5",
  "x": "Due to space limitation, we do not discuss its complexity here. See<cite> (Sassano, 2004)</cite> for further details. ----------------------------------",
  "y": "background"
 },
 {
  "id": "250a88831a4911f76acca3c9d318de_6",
  "x": "We have designed rather simple features based on the common feature set (Uchimoto et al., 1999; Kudo and Matsumoto, 2002;<cite> Sassano, 2004)</cite> for bunsetsu-based parsers. We use the following features for each morpheme: Gap features between two morphemes are also used since they have proven to be very useful and contribute to the accuracy (Uchimoto et al., 1999; Kudo and Matsumoto, 2002) . They are represented as a binary feature and include distance (1, 2, 3, 4 -10, or 11 \u2264), particles, parentheses, and punctuation.",
  "y": "uses"
 },
 {
  "id": "250a88831a4911f76acca3c9d318de_7",
  "x": "We assume that we have the j-th morpheme and the i-th one in Figure 3 . We also use the j \u2212 n, ..., j \u2212 1, j + 1, ..., j + n morphemes and the i \u2212 n, ..., i \u2212 1, i + 1, ..., i + n ones, where n Table 2 : Dependency accuracy. The system with the previous method employs the algorithm<cite> (Sassano, 2004</cite> ) with the voted perceptron.",
  "y": "extends uses"
 },
 {
  "id": "250a88831a4911f76acca3c9d318de_8",
  "x": "We implemented a parser that employs the algorithm of<cite> (Sassano, 2004)</cite> with the commonly used features and runs with VP instead of SVM, which <cite>Sassano (2004)</cite> originally used. His parser, which cannot do bunsetsu chunking, accepts only a chunked sentence and then produces a bunsetsu-based dependency structure. Thus we cannot directly compare results with ours.",
  "y": "uses"
 },
 {
  "id": "250a88831a4911f76acca3c9d318de_9",
  "x": "His parser, which cannot do bunsetsu chunking, accepts only a chunked sentence and then produces a bunsetsu-based dependency structure. Thus we cannot directly compare results with ours. To enable us to compare them we gave bunsetsu chunked sentences by our parser to the parser of<cite> (Sassano, 2004)</cite> in the Kyoto University Corpus. And then we received results from the parser of<cite> (Sassano, 2004)</cite> , which are bunsetsu-based dependency structures, and converted them to morpheme-based structures that follow the scheme we propose in this paper.",
  "y": "extends uses"
 },
 {
  "id": "250a88831a4911f76acca3c9d318de_10",
  "x": "We implemented a parser that employs the algorithm of<cite> (Sassano, 2004)</cite> with the commonly used features and runs with VP instead of SVM, which <cite>Sassano (2004)</cite> originally used. His parser, which cannot do bunsetsu chunking, accepts only a chunked sentence and then produces a bunsetsu-based dependency structure. Thus we cannot directly compare results with ours.",
  "y": "differences motivation"
 },
 {
  "id": "253d635829c733309bb49fc1fcc1cd_0",
  "x": "---------------------------------- **INTRODUCTION** Automatic detection of fake from legitimate news in different formats such as headlines, tweets and full news articles has been approached in recent Natural Language Processing literature (Vlachos and Riedel, 2014; Vosoughi, 2015; Jin et al., 2016; Rashkin et al., 2017;<cite> Wang, 2017</cite>; Pomerleau and Rao, 2017; Thorne et al., 2018) .",
  "y": "background"
 },
 {
  "id": "253d635829c733309bb49fc1fcc1cd_1",
  "x": "The key to collect more reliable data, then, is to not rely on the source but on the text of the article itself, and only after the text has been assessed by human annotators and determined to contain false information. Currently, there exists only small collections of reliably-labeled news articles (Rubin et al., 2016; Allcott and Gentzkow, 2017; Zhang et al., 2018; because this type of annotation is laborious. The Liar dataset<cite> (Wang, 2017)</cite> is the first large dataset collected through reliable annotation, but it contains only short statements.",
  "y": "background"
 },
 {
  "id": "253d635829c733309bb49fc1fcc1cd_2",
  "x": "In text classification, Convolutional Neural Networks (CNNs) have been competing with the TF-IDF model, a simple but strong baseline using scored n-grams (Le and Mikolov, 2014; Zhang et al., 2015; Conneau et al., 2017; Medvedeva et al., 2017) . These methods have been used for fake news detection in previous work (Rashkin et al., 2017;<cite> Wang, 2017</cite> fore, we use this model to demonstrate how a classifier trained on data labeled according to publisher's reputation would identify misinformative news articles. It is evident in the first section of Figure 1 , that the model performs well on similarly collected test items, i.e., Hoax, Satire, Propaganda and Trusted news articles within Rashkin et al.'s test dataset.",
  "y": "uses background"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_0",
  "x": "---------------------------------- **INTRODUCTION** Generative models defining joint distributions over parse trees and sentences are good theoretical models for interpreting natural language data, and appealing tools for tasks such as parsing, grammar induction and language modeling (Collins, 1999; Henderson, 2003; Titov and Henderson, 2007; Petrov and Klein, 2007;<cite> Dyer et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_1",
  "x": "Generative models defining joint distributions over parse trees and sentences are good theoretical models for interpreting natural language data, and appealing tools for tasks such as parsing, grammar induction and language modeling (Collins, 1999; Henderson, 2003; Titov and Henderson, 2007; Petrov and Klein, 2007;<cite> Dyer et al., 2016)</cite> . However, they often impose strong independence assumptions which restrict the use of arbitrary features for effective disambiguation. Moreover, generative parsers are typically trained by maximizing the joint probability of the parse tree and the sentence-an objective that only indirectly relates to the goal of parsing. In this work, we propose a parsing and language modeling framework that marries a generative model with a discriminative recognition algorithm in order to have the best of both worlds.",
  "y": "motivation"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_2",
  "x": "We showcase the framework using Recurrent Neural Network Grammars (RNNGs;<cite> Dyer et al. 2016</cite> ), a recently proposed probabilistic model of phrase-structure trees based on neural transition systems. Different from this work which introduces separately trained discriminative and generative models, we integrate the two in an auto-encoder which fits our training objective. We show how the framework enables grammar induction, parsing and language modeling within a single implementation.",
  "y": "uses"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_3",
  "x": "**PRELIMINARIES** In this section we briefly describe Recurrent Neural Network Grammars (RNNGs;<cite> Dyer et al. 2016</cite> ), a top-down transition-based algorithm for parsing and generation. There are two versions of RNNG, one discriminative, the other generative.",
  "y": "uses"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_4",
  "x": "To satisfy the independence assumptions imposed by the generative model, u t uses only a restricted set of features defined over the output buffer and the stack -we consider p(a) as a context insensitive prior distribution. Specifically, we use the following features: 1) the stack embedding d t which encodes the stack of the decoder and is obtained with a stack-LSTM (Dyer et al., 2015 <cite>(Dyer et al., , 2016</cite> ; 2) the output buffer embedding o t ; we use a standard LSTM to compose the output buffer and o t is represented as the most recent state of the LSTM; and 3) the parent non-terminal embedding n t which is accessible in the generative model because the RNNG employs a depth-first generation order. Finally, u t is computed as:",
  "y": "uses"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_5",
  "x": "where L x and L a can be balanced with the task focus (e.g, language modeling or parsing). 5 Here, GEN and SHIFT refer to the same action with different definitions for encoding and decoding. 6 See \u00a7 4 and Appendix A for comparison between this objective and the importance sampler of<cite> Dyer et al. (2016</cite>",
  "y": "uses"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_6",
  "x": "To approximate this quantity, we can use Equation (8) to compute a lower bound of the log likelihood log p(x) and then exponentiate it to get a pessimistic approximation of p(x). 7 Another way of computing p(x) (without lower bounding) would be to use the variational approximation q(a|x) as the proposal distribution as in the importance sampler of<cite> Dyer et al. (2016)</cite> . We discuss details in Appendix A.",
  "y": "uses"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_9",
  "x": "This work argmax a q(a|x) 89.3 argmax a p(a, x) 90.1 Table 2 : Parsing results (F1) on the PTB test set. . Another interpretation of the proposed framework is from the perspective of guided policy search in reinforcement learning (Bachman and Precup, 2015) , where a generative parser is trained to imitate the trace of a discriminative parser. Further connections can be drawn with the importance-sampling based inference of<cite> Dyer et al. (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_10",
  "x": "To find the MAP parse tree argmax a p(a, x) (where p(a, x) is used rank the output of q(a|x)) and to compute the language modeling perplexity (where a \u223c q(a|x)), we collect 100 samples from q(a|x), same as<cite> Dyer et al. (2016)</cite> . Experimental results for constituency parsing and language modeling are shown in Tables 2 and 3, respectively. As can be seen, the single framework we propose obtains competitive parsing performance.",
  "y": "uses"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_12",
  "x": "4<cite> Dyer et al. (2016)</cite> 102.4 This work: a \u223c q(a|x) 99.8 Table 3 : Language modeling results (perplexity). methods for parsing, ranking approximated MAP trees from q(a|x) with respect to p(a, x) yields a small improvement, as in<cite> Dyer et al. (2016)</cite> . It is worth noting that our parsing performance lags behind<cite> Dyer et al. (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_13",
  "x": "4<cite> Dyer et al. (2016)</cite> 102.4 This work: a \u223c q(a|x) 99.8 Table 3 : Language modeling results (perplexity). methods for parsing, ranking approximated MAP trees from q(a|x) with respect to p(a, x) yields a small improvement, as in<cite> Dyer et al. (2016)</cite> . It is worth noting that our parsing performance lags behind<cite> Dyer et al. (2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_14",
  "x": "We believe this is due to implementation disparities, such as the modeling of the reduce operation. While<cite> Dyer et al. (2016)</cite> use an LSTM as the syntactic composition function of each subtree, we adopt a rather simple composition function based on embedding averaging, which gains computational efficiency but loses accuracy. On language modeling, our framework achieves lower perplexity compared to<cite> Dyer et al. (2016)</cite> and baseline models.",
  "y": "differences"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_15",
  "x": "While<cite> Dyer et al. (2016)</cite> use an LSTM as the syntactic composition function of each subtree, we adopt a rather simple composition function based on embedding averaging, which gains computational efficiency but loses accuracy. On language modeling, our framework achieves lower perplexity compared to<cite> Dyer et al. (2016)</cite> and baseline models. This gain possibly comes from the joint optimization of both the generative and discriminative components towards a language modeling objective.",
  "y": "differences"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_16",
  "x": "However, we acknowledge a subtle difference between<cite> Dyer et al. (2016)</cite> and our approach compared to baseline language models: while the latter incrementally estimate the next word probability, our approach<cite> (and Dyer et al. 2016</cite> ) directly assigns probability to the entire sentence. Overall, the advantage of our framework compared to<cite> Dyer et al. (2016)</cite> is that it opens an avenue to unsupervised training. ----------------------------------",
  "y": "similarities differences"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_17",
  "x": "This gain possibly comes from the joint optimization of both the generative and discriminative components towards a language modeling objective. However, we acknowledge a subtle difference between<cite> Dyer et al. (2016)</cite> and our approach compared to baseline language models: while the latter incrementally estimate the next word probability, our approach<cite> (and Dyer et al. 2016</cite> ) directly assigns probability to the entire sentence. Overall, the advantage of our framework compared to<cite> Dyer et al. (2016)</cite> is that it opens an avenue to unsupervised training.",
  "y": "differences"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_18",
  "x": "We demonstrated that a unified framework, which relates to expectation maximization and variational inference, enables effective parsing and language modeling algorithms. Evaluation on the English Penn Treebank, revealed that our framework obtains competitive performance on constituency parsing and state-of-the-art results on single-model language modeling. In the future, we would like to perform grammar induction based on Equation (8), with gradient descent and posterior regularization techniques (Ganchev et al., 2010 A Comparison to Importance Sampling<cite> (Dyer et al., 2016)</cite> In this appendix we highlight the connections between importance sampling and variational inference, thereby comparing our method with<cite> Dyer et al. (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_19",
  "x": "As shown in Rubinstein and Kroese (2008) , the optimal choice of the proposal distribution is in fact the true posterior p(a|x), in which case the importance weight p(a,x) p(a|x) = p(x) is constant with respect to a. In<cite> Dyer et al. (2016)</cite> , the proposal distribution depends on x, i.e., q(a) q(a|x), and is computed with a separately-trained, discriminative model. This proposal choice is close to optimal, since in a fully supervised setting a is also observed and the discriminative model can be trained to approximate the true posterior well. We hypothesize that the performance of their importance sampler is dependent on this specific proposal distribution.",
  "y": "background"
 },
 {
  "id": "25e03048cd34685cec34754bdade4e_20",
  "x": "As shown in Rubinstein and Kroese (2008) , the optimal choice of the proposal distribution is in fact the true posterior p(a|x), in which case the importance weight p(a,x) p(a|x) = p(x) is constant with respect to a. In<cite> Dyer et al. (2016)</cite> , the proposal distribution depends on x, i.e., q(a) q(a|x), and is computed with a separately-trained, discriminative model. This proposal choice is close to optimal, since in a fully supervised setting a is also observed and the discriminative model can be trained to approximate the true posterior well. We hypothesize that the performance of their importance sampler is dependent on this specific proposal distribution.",
  "y": "motivation"
 },
 {
  "id": "260489da0fb3f7a201a6a1cce8f03b_0",
  "x": "End-to-end neural machine translation (NMT) is a newly proposed paradigm for machine translation [Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014;<cite> Bahdanau et al., 2015]</cite> . Without explicitly modeling latent structures that are vital for conventional statistical machine translation [Brown et al., 1993; Koehn et al., 2003; Chiang, 2005] , NMT builds on an encoder-decoder framework: the encoder transforms a source-language sentence into a continuous-space representation, from which the decoder generates a target-language sentence. While early NMT models encode a source sentence as a fixed-length vector, <cite>Bahdanau et al. [2015]</cite> advocate the use of attention in NMT.",
  "y": "background"
 },
 {
  "id": "260489da0fb3f7a201a6a1cce8f03b_1",
  "x": "End-to-end neural machine translation (NMT) is a newly proposed paradigm for machine translation [Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014;<cite> Bahdanau et al., 2015]</cite> . Without explicitly modeling latent structures that are vital for conventional statistical machine translation [Brown et al., 1993; Koehn et al., 2003; Chiang, 2005] , NMT builds on an encoder-decoder framework: the encoder transforms a source-language sentence into a continuous-space representation, from which the decoder generates a target-language sentence. While early NMT models encode a source sentence as a fixed-length vector, <cite>Bahdanau et al. [2015]</cite> advocate the use of attention in NMT.",
  "y": "background"
 },
 {
  "id": "260489da0fb3f7a201a6a1cce8f03b_2",
  "x": "Such an attentional mechanism has proven to be an effective technique in text generation tasks such as machine translation<cite> [Bahdanau et al., 2015</cite>; Luong et al., 2015] and image caption generation [Xu et al., 2015] . The encoder-decoder framework [Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014;<cite> Bahdanau et al., 2015]</cite> usually uses a recurrent neural network (RNN) to encode the source sentence into a sequence of hidden states h = h 1 , . . . , h m , . . . , h M : where h m is the hidden state of the m-th source word and f is a non-linear function.",
  "y": "background"
 },
 {
  "id": "260489da0fb3f7a201a6a1cce8f03b_3",
  "x": "Note that there are many ways to obtain the hidden states. For example, <cite>Bahdanau et al. [2015]</cite> use a bidirectional RNN and concatenate the forward and backward states as the hidden state of a source word to capture both forward and backward contexts. Figure 1 illustrates how the decoder generates the first target word y 1 and the target hidden state s 1 given the concatenation of forward and backward source hidden states.",
  "y": "background"
 },
 {
  "id": "260489da0fb3f7a201a6a1cce8f03b_4",
  "x": "<cite>Bahdanau et al. [2015]</cite> define the conditional probability in Eq. (1) as where g is a non-linear function and s n is the hidden state corresponding to the n-th target word computed by",
  "y": "background"
 },
 {
  "id": "260489da0fb3f7a201a6a1cce8f03b_5",
  "x": "We follow <cite>Bahdanau et al. [2015]</cite> to restrict that sentences are no longer than 50 words. The concatenation of news-test-2012 and news-test-2013 is used as the validation set and news-test-2014 as the test set. The French-English evaluation sets can be easily obtained by reversing the English-French datasets.",
  "y": "uses"
 },
 {
  "id": "260489da0fb3f7a201a6a1cce8f03b_6",
  "x": "GroundHog is an attention-based neural machine translation system<cite> [Bahdanau et al., 2015]</cite> . We introduce agreement-based joint training for bidirectional attention-based NMT. NIST06 is the validation set and NIST02-05, 08 are test sets.",
  "y": "background"
 },
 {
  "id": "260489da0fb3f7a201a6a1cce8f03b_7",
  "x": "We compared our approach with two state-of-the-art SMT and NMT systems: [Koehn and Hoang, 2007] . GroundHog is an attention-based neural machine translation system<cite> [Bahdanau et al., 2015]</cite> . 1. Moses [Koehn and Hoang, 2007] : a phrase-based SMT system; 2. GroundHog<cite> [Bahdanau et al., 2015]</cite> : an attention-based NMT system.",
  "y": "uses"
 },
 {
  "id": "260489da0fb3f7a201a6a1cce8f03b_8",
  "x": "GroundHog is an attention-based neural machine translation system<cite> [Bahdanau et al., 2015]</cite> . Our approach simply extends GroundHog by replacing independent training with agreement-based joint training.",
  "y": "extends background"
 },
 {
  "id": "260489da0fb3f7a201a6a1cce8f03b_9",
  "x": "Our work is inspired by two lines of research: (1) attention-based neural machine translation and (2) agreement-based learning. <cite>Bahdanau et al. [2015]</cite> first introduce the attentional mechanism into neural machine translation to enable the decoder to focus on relevant parts of the source sentence during decoding. The attention mechanism allows a neural model to cope better with long sentences because it does not need to encode all the information of a source sentence into a fixed-length vector regardless of its length.",
  "y": "background"
 },
 {
  "id": "260489da0fb3f7a201a6a1cce8f03b_10",
  "x": "**ATTENTION-BASED NEURAL MACHINE TRANSLATION** After analyzing the alignment matrices generated by GroundHog<cite> [Bahdanau et al., 2015]</cite> , we find that modeling the structural divergence of natural languages is so challenging that unidirectional models can only capture part of alignment regularities. This finding inspires us to improve attention-based NMT by combining two unidirectional models.",
  "y": "motivation"
 },
 {
  "id": "260489da0fb3f7a201a6a1cce8f03b_11",
  "x": "After analyzing the alignment matrices generated by GroundHog<cite> [Bahdanau et al., 2015]</cite> , we find that modeling the structural divergence of natural languages is so challenging that unidirectional models can only capture part of alignment regularities. This finding inspires us to improve attention-based NMT by combining two unidirectional models. In this work, we only apply agreement-based joint learning to GroundHog.",
  "y": "motivation"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_0",
  "x": "Recently we witness a tremendous progress in the machine perception [1, 2, 3, 4, 5, 6, 7, 8] and in the language understanding [9, 10, 11, 12, 13] tasks. The progress in both fields has inspired researchers to build holistic architectures for challenging grounding [14, 15] , natural language generation from image/video [16, 17, 18] , image-to-sentence alignment [19, 20, 21, 22] , and recently presented question-answering problems [23, 24, 25, 26, <cite>27]</cite> . In this paper we argue for a Visual Turing Test -an open domain task of question-answering based on real-world images that resemblances the famous Turing Test [28, 29] and deviates from other attempts [30, 31, 32] -and discuss challenges together with tools to benchmark different models on such task.",
  "y": "background"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_1",
  "x": "Third, if our aim is to mimic human response, we have to deal with inherent ambiguities due to human judgement that stem from issues like binding, reference frames, social conventions. For instance <cite>[27]</cite> reports that for a question answering task on real-world images even human answers are inconsistent. Obviously this cannot be a problem of humans but rather argues for inherent ambiguities in the task.",
  "y": "motivation"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_2",
  "x": "Although the idea is not entirely new [35, 36, 37] , we believe it sits at the core of building more open and holistic challenges. We exemplify some of our findings on the <cite>DAQUAR dataset</cite> <cite>[27]</cite> with the aim of demonstrating different challenges that are present in <cite>the dataset</cite>. We hope that our exposition is helpful towards building a public visual turing challenge and will generate a discussion for the agreeable evaluation procedure and designing systems that can address open domain tasks.",
  "y": "uses"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_3",
  "x": "E.g. white in \"white\" elephant is surly different from \"white\" in white snow. Ambiguity in reference resolution: Reliably answering on questions is challenging even for humans. The quality of an answer depends on how ambiguous and latent notions of reference frames and intentions are understood <cite>[27</cite>, 44] .",
  "y": "background"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_5",
  "x": "---------------------------------- **<cite>DAQUAR</cite>: <cite>BUILDING A DATASET FOR VISUAL TURING CHALLENGE</cite>** <cite>DAQUAR</cite> <cite>[27]</cite> is a challenging, large dataset for a question answering task based on real-world images.",
  "y": "background"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_6",
  "x": "<cite>DAQUAR</cite> <cite>[27]</cite> is a challenging, large dataset for a question answering task based on real-world images. The images present real-world indoor scenes [50] , while the questions are unconstrained natural language sentences. <cite>DAQUAR's</cite> language scope is beyond the nouns or tuples that are typical to recognition datasets [51, 52, 53] .",
  "y": "background"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_7",
  "x": "<cite>DAQUAR's</cite> language scope is beyond the nouns or tuples that are typical to recognition datasets [51, 52, 53] . Other, linguistically rich datasets either do not tackle images at all [54, 55] or consider only few in very constrained domain [15] , or are more suitable for the learning an embedding/image-sentence retrieval or language generation [22, 56, 57, 58] . In this section we discuss in isolation different challenges reflected in <cite>DAQUAR</cite>.",
  "y": "motivation"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_8",
  "x": "---------------------------------- **VISION AND LANGUAGE** The machine world in <cite>DAQUAR</cite> is represented as a set of images and questions about their content.",
  "y": "background"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_9",
  "x": "**VISION AND LANGUAGE** The machine world in <cite>DAQUAR</cite> is represented as a set of images and questions about their content. <cite>DAQUAR</cite> contains 1088 different nouns in the question, 803 in the answers, and 1586 altogether (we use the Stanford POS Tagger [59] to extract the nouns from the questions).",
  "y": "background"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_10",
  "x": "<cite>DAQUAR</cite> also contains other parts of speech where only colors and spatial prepositions are grounded in <cite>[27]</cite> . Moreover, ambiguities naturally emerge due to fine grained categories that exist in <cite>DAQUAR</cite>. For instance 'night stand', 'stool' and 'cabinet' sometimes refer to the same thing.",
  "y": "background"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_11",
  "x": "The current state-of-the-art semantic segmentation methods on the NYU-Depth V2 dataset [50] can discriminate only between up to 37 object categories [2, 60, 61] , much fewer to what is needed. <cite>DAQUAR</cite> also contains other parts of speech where only colors and spatial prepositions are grounded in <cite>[27]</cite> . Moreover, ambiguities naturally emerge due to fine grained categories that exist in <cite>DAQUAR</cite>.",
  "y": "background"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_12",
  "x": "There is also a variation in the naming of colors among the annotations. Questions rely heavily on the spatial concepts with different frame of reference. <cite>DAQUAR</cite> includes various challenges related to natural language understanding.",
  "y": "motivation"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_13",
  "x": "<cite>DAQUAR</cite> includes various challenges related to natural language understanding. Any semantic representation needs to work with the large number of predicates (reaching about 4 million to account different interpretations of the external world), with questions of substantial length (10.5 words in average with variance 5.5; the longest question has 30 words), and possible language errors in the questions. Common sense knowledge <cite>DAQUAR</cite> includes questions that can be reliably answered using common sense knowledge.",
  "y": "background"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_14",
  "x": "Moreover, some annotators hypothesize missing parts of the object based on their common sense. To sum up, we believe that common sense knowledge is an interesting venue to explore with <cite>DAQUAR</cite>. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_15",
  "x": "Some authors [23, 24, <cite>27]</cite> treat the grounding (understood here as the logical representation of the meaning of the question) as a latent variable in the question answering task. Others [44] have modeled the pragmatic effects in the question answering task, but such approaches have never been shown to work in less constrained environments. ----------------------------------",
  "y": "background"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_16",
  "x": "We exemplify the aforementioned requirements by illustrating the WUPS scorean automatic metric that quantifies performance of the holistic architectures proposed by <cite>[27]</cite> . This metric is motivated by the development of a 'soft' generalization of accuracy that takes ambiguities of different concepts into account via the set membership measure \u00b5: where for each i-th question, A i and T i are the answers produced by the architecture and human respectively, and they are represented as bags of words.",
  "y": "uses"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_17",
  "x": "The authors of <cite>[27]</cite> have proposed using WUP similarity [62] as the membership measure \u00b5 in the WUPS score. Such choice of \u00b5 suffers from the aforementioned coverage problem and the whole metric takes only one human interpretation of the question into account. Future directions for defining metrics Recent work provides several directions towards improving scores.",
  "y": "future_work"
 },
 {
  "id": "2606ecb66287c0199f3aa6d95f6774_18",
  "x": "**SUMMARY** The goal of this contribution is to sparkle the discussions about benchmarking holistic architectures on complex and more open tasks. We identify particular challenges that holistic tasks should exhibit and exemplify how they are manifested in <cite>a recent question answering challenge</cite> <cite>[27]</cite> .",
  "y": "future_work"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_0",
  "x": "For example, Serban et al. [3] - [5] and Xing et al. [6] introduce a Hierarchical Recurrent Encoder-Decoder (HRED) network architecture that combines a series of recurrent neural networks to capture long-term context state within a dialogue. However, the HRED system suffers from lack of diversity and does not support any guarantees on the generator output since the output conditional probability is not calibrated. <cite>Olabiyi et al. [7]</cite> tackle this problem by training a modified HRED generator alongside an adversarial discriminator in order to provide a stronger guarantee to the generator's output.",
  "y": "background"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_1",
  "x": "For example, Serban et al. [3] - [5] and Xing et al. [6] introduce a Hierarchical Recurrent Encoder-Decoder (HRED) network architecture that combines a series of recurrent neural networks to capture long-term context state within a dialogue. However, the HRED system suffers from lack of diversity and does not support any guarantees on the generator output since the output conditional probability is not calibrated. <cite>Olabiyi et al. [7]</cite> tackle this problem by training a modified HRED generator alongside an adversarial discriminator in order to provide a stronger guarantee to the generator's output.",
  "y": "background"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_2",
  "x": "However, the HRED system suffers from lack of diversity and does not support any guarantees on the generator output since the output conditional probability is not calibrated. <cite>Olabiyi et al. [7]</cite> tackle this problem by training a modified HRED generator alongside an adversarial discriminator in order to provide a stronger guarantee to the generator's output. While the hredGAN system improves upon response quality, it does not capture speaker and other attributes modalities within a dataset and fails to generate persona-specific responses in datasets with multiple modalities.",
  "y": "background motivation"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_3",
  "x": "<cite>Olabiyi et al. [7]</cite> tackle this problem by training a modified HRED generator alongside an adversarial discriminator in order to provide a stronger guarantee to the generator's output. While the hredGAN system improves upon response quality, it does not capture speaker and other attributes modalities within a dataset and fails to generate persona-specific responses in datasets with multiple modalities. At the same time, there has been some recent work on introducing persona into dialogue models.",
  "y": "background"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_4",
  "x": "This is evident in the relatively short and generic responses even though they generally capture the persona of the speaker. To overcome these limitations, we propose phredGAN , a multi-modal hredGAN dialogue system which additionally conditions the adversarial framework proposed by <cite>Olabiyi et al. [7]</cite> on speaker and/or utterance attributes in order to maintain response quality of hredGAN and still capture speaker and other modalities within a conversation. The attributes can be seen as another input modality as the utterance.",
  "y": "extends uses"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_5",
  "x": "Since the attributes are discrete, it also allows for exploring what-if scenarios of model responses. We train and sample the proposed phredGAN similar to the procedure for hredGAN <cite>[7]</cite> . To demonstrate model capability, we train on customer service related data such as the Ubuntu Dialogue Corpus (UDC) that is strongly bimodal between question poster and answerer, and character consistent TV scripts from two popular series, The Big Bang Theory and Friends with quantitative and qualitative analysis.",
  "y": "uses similarities"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_6",
  "x": "We train and sample the proposed phredGAN similar to the procedure for hredGAN <cite>[7]</cite> . We demonstrate system superiority over hredGAN and the state-of-the-art persona conversational model in terms",
  "y": "extends differences"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_7",
  "x": [
   "We demonstrate system superiority over hredGAN and the state-of-the-art persona conversational model in terms ---------------------------------- **II. MODEL ARCHITECTURE**"
  ],
  "y": "differences"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_8",
  "x": [
   "The hredGAN proposed by Olabiyi et. al <cite>[7]</cite> contains three major components. Encoder: The encoder consists of three RNNs, the aRN N that encodes an utterance for attention memory, the eRN N that encodes an utterance for dialogue context, and the cRN N that encodes a multi-turn dialogue context from the eRN N outputs."
  ],
  "y": "background"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_9",
  "x": "In the case of hredGAN <cite>[7]</cite> , it is a bidirectional RNN that discriminates at the word level to capture both the syntactic and semantic difference between the ground truth and the generator output. Problem Formulation: The hredGAN <cite>[7]</cite> formulates multi-turn dialogue response generation as: given a dialogue history of sequence of utterances, where T i is the number of generated tokens.",
  "y": "background"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_10",
  "x": "Problem Formulation: The hredGAN <cite>[7]</cite> formulates multi-turn dialogue response generation as: given a dialogue history of sequence of utterances, where T i is the number of generated tokens. The framework uses a conditional GAN structure to learn a mapping from an observed dialogue history to a sequence of output tokens.",
  "y": "background"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_11",
  "x": "The generator, G, is trained to produce sequences that cannot be distinguished from the ground truth by an adversarially trained discriminator, D, akin to a two-player min-max optimization problem. The generator is also trained to minimize the cross-entropy loss L MLE (G) between the ground truth X i+1 , and the generator output Y i . The following objective summarizes both goals: where \u03bb G and \u03bb M are hyperparameters and L cGAN (G, D) and L MLE (G) are defined in Eqs. (5) and (7) of <cite>[7]</cite> respectively.",
  "y": "background"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_12",
  "x": [
   "---------------------------------- **B. PHREDGAN : PERSONA ADVERSARIAL LEARNING FRAMEWORK** The proposed architecture of phredGAN is very similar to that of hredGAN summarized above."
  ],
  "y": "similarities"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_13",
  "x": [
   "**B. PHREDGAN : PERSONA ADVERSARIAL LEARNING FRAMEWORK** The proposed architecture of phredGAN is very similar to that of hredGAN summarized above. The only difference is that the dialogue history is now X i = (X 1 , C 1 ), (X 2 , C 2 ), \u00b7 \u00b7 \u00b7 , (X i , C i ) where C i is additional input that represents the speaker and/or utterance attributes."
  ],
  "y": "extends differences"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_14",
  "x": "Discriminator: In addition to the D RN N in the discriminator of hredGAN , if the attribute C i+1 is a sequence of tokens, then the same tattRN N is used to summarize the attribute token embeddings; otherwise the single attribute embedding is concatenated with the other inputs of D RN N in Fig. 1 of <cite>[7]</cite> . Noise Injection: Although <cite>[7]</cite> demonstrated that injecting noise at the word level seems to perform better than at the utterance level for hredGAN , we found that this is datasetdependent for phredGAN . The phredGAN model with utterance-level noise injection and word-level noise injection are tagged phredGAN u and phredGAN w respectively.",
  "y": "background"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_15",
  "x": "The modified system is as follows: Discriminator: In addition to the D RN N in the discriminator of hredGAN , if the attribute C i+1 is a sequence of tokens, then the same tattRN N is used to summarize the attribute token embeddings; otherwise the single attribute embedding is concatenated with the other inputs of D RN N in Fig. 1 of <cite>[7]</cite> .",
  "y": "uses"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_16",
  "x": "Noise Injection: Although <cite>[7]</cite> demonstrated that injecting noise at the word level seems to perform better than at the utterance level for hredGAN , we found that this is datasetdependent for phredGAN . The phredGAN model with utterance-level noise injection and word-level noise injection are tagged phredGAN u and phredGAN w respectively. The losses, L cGAN (G, D) and L MLE (G) in eq. (1) are then respectively updated as:",
  "y": "differences"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_17",
  "x": "The modified system is as follows: Noise Injection: Although <cite>[7]</cite> demonstrated that injecting noise at the word level seems to perform better than at the utterance level for hredGAN , we found that this is datasetdependent for phredGAN .",
  "y": "extends"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_18",
  "x": "---------------------------------- **A. MODEL TRAINING** We train both the generator and the discriminator (with a shared encoder) of phredGAN using the same training procedure in Algorithm 1 with \u03bb G = \u03bb M = 1 <cite>[7]</cite> .",
  "y": "uses"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_19",
  "x": "All RNN units are 3-layer GRU cells with a hidden state size of 512. We use a word vocabulary size, V = 50, 000, with a word embedding size of 512. The number of attributes, V c is dataset-dependent Compute the generator output similar to Eq. (11) in <cite>[7]</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_20",
  "x": "The parameter update is conditioned on the discriminator accuracy performance as in <cite>[7]</cite> with acc D th = 0.99 and acc G th = 0.75. The model is trained end-to-end using the stochastic gradient descent algorithm. Finally, the model is implemented, trained, and evaluated using the TensorFlow deep learning framework.",
  "y": "uses"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_21",
  "x": "For the modified noise sample, we perform a linear search for \u03b1 with sample size L = 1 based on the average discriminator loss, \u2212logD(G(.)) <cite>[7]</cite> using trained models run in autoregressive mode to reflect performance in actual deployment. The optimum \u03b1 value is then used for all inferences and evaluations. During inference, we condition the dialogue response generation on the encoder outputs, noise samples, word embedding, and the attribute embedding of the intended responder.",
  "y": "uses"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_22",
  "x": "---------------------------------- **IV. EXPERIMENTS AND RESULTS** In this section, we explore phredGAN 's results on two conversational datasets and compare its performance to the persona system in Li et al. [8] and hredGAN <cite>[7]</cite> in terms of quantitative and qualitative measures.",
  "y": "similarities differences"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_23",
  "x": "We train our model on 1.85 million conversations of multi-turn dialogue from the Ubuntu community hub, with an average of 5 utterances per conversation. We assign two types of speaker IDs to utterances in this dataset: questioner and helper. We follow the same training, development, and test split as the UDC dataset in <cite>[7]</cite> , with 90%, 5%, and 5% proportions, respectively.",
  "y": "uses"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_24",
  "x": "We use similar evaluation metrics as in <cite>[7]</cite> including perplexity, BLEU [15] , ROUGE [16] , and distinct n-gram [17] scores. ---------------------------------- **C. BASELINE**",
  "y": "uses"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_25",
  "x": "For fair comparison, we use the same TV drama series dataset used in their study. We also compare our system to hredGAN from <cite>[7]</cite> in terms of perplexity, ROGUE, and distinct n-grams scores. In <cite>[7]</cite> , the authors recommend the version with word-level noise injection, hredGAN w , so we use this version in our comparison.",
  "y": "similarities differences"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_26",
  "x": "We also compare our system to hredGAN from <cite>[7]</cite> in terms of perplexity, ROGUE, and distinct n-grams scores. In <cite>[7]</cite> , the authors recommend the version with word-level noise injection, hredGAN w , so we use this version in our comparison. Also for fair comparison, we use the same UDC dataset as reported in <cite>[7]</cite> .",
  "y": "uses"
 },
 {
  "id": "264bdb348c13f167768fd859b047e8_27",
  "x": "In <cite>[7]</cite> , the authors recommend the version with word-level noise injection, hredGAN w , so we use this version in our comparison. Also for fair comparison, we use the same UDC dataset as reported in <cite>[7]</cite> . The only addition we made is to add the speaker attribute to the utterances of the dataset as described in the Dataset subsection.",
  "y": "uses"
 },
 {
  "id": "26b00c6e5b499eea30e9cef0bbaf9f_0",
  "x": "Distributed word representations have become a mainstay in natural language processing, enjoying a slew of applications (Sebastiani, 2002; Turian et al., 2010; . Though Baroni et al. (2014) suggested that predictive models which use neural networks to generate the distributed word representations (also known as embeddings in this context) outperform counting models which work on co-occurrence matrices, recent work shows evidence to the contrary (Levy et al., 2014;<cite> Salle et al., 2016)</cite> . In this paper, we focus on improving a state-ofthe-art counting model, LexVec <cite>(Salle et al., 2016)</cite> , which performs factorization of the positive pointwise mutual information (PPMI) matrix using window sampling and negative sampling (WSNS).",
  "y": "motivation"
 },
 {
  "id": "26b00c6e5b499eea30e9cef0bbaf9f_1",
  "x": "Though Baroni et al. (2014) suggested that predictive models which use neural networks to generate the distributed word representations (also known as embeddings in this context) outperform counting models which work on co-occurrence matrices, recent work shows evidence to the contrary (Levy et al., 2014;<cite> Salle et al., 2016)</cite> . In this paper, we focus on improving a state-ofthe-art counting model, LexVec <cite>(Salle et al., 2016)</cite> , which performs factorization of the positive pointwise mutual information (PPMI) matrix using window sampling and negative sampling (WSNS). Salle et al. (2016) suggest that LexVec matches and often outperforms competing models in word similarity and semantic analogy tasks.",
  "y": "uses"
 },
 {
  "id": "26b00c6e5b499eea30e9cef0bbaf9f_2",
  "x": "Distributed word representations have become a mainstay in natural language processing, enjoying a slew of applications (Sebastiani, 2002; Turian et al., 2010; . Though Baroni et al. (2014) suggested that predictive models which use neural networks to generate the distributed word representations (also known as embeddings in this context) outperform counting models which work on co-occurrence matrices, recent work shows evidence to the contrary (Levy et al., 2014;<cite> Salle et al., 2016)</cite> . In this paper, we focus on improving a state-ofthe-art counting model, LexVec <cite>(Salle et al., 2016)</cite> , which performs factorization of the positive pointwise mutual information (PPMI) matrix using window sampling and negative sampling (WSNS).",
  "y": "motivation"
 },
 {
  "id": "26b00c6e5b499eea30e9cef0bbaf9f_3",
  "x": "using WSNS. P n is the distribution used for drawing negative samples, chosen to be with \u03b1 = 3/4 (Mikolov et al., 2013b;<cite> Salle et al., 2016)</cite> , and #(w) the unigram frequency of w. Two methods were defined for the minimization of eqs. (2) and (3): Mini-batch and Stochastic <cite>(Salle et al., 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "26b00c6e5b499eea30e9cef0bbaf9f_4",
  "x": "P n is the distribution used for drawing negative samples, chosen to be with \u03b1 = 3/4 (Mikolov et al., 2013b;<cite> Salle et al., 2016)</cite> , and #(w) the unigram frequency of w. Two methods were defined for the minimization of eqs. (2) and (3): Mini-batch and Stochastic <cite>(Salle et al., 2016)</cite> . Since the latter is more computationally efficient and yields equivalent results, we adopt it in this paper.",
  "y": "uses"
 },
 {
  "id": "26b00c6e5b499eea30e9cef0bbaf9f_5",
  "x": "**POSITIONAL CONTEXTS** As suggested by Levy et al. (2015) and<cite> Salle et al. (2016)</cite> , positional contexts (introduced in Levy et al. (2014) ) are a potential solution to poor performance on syntactic analogy tasks. Rather than only accounting for which context words appear around a target word, positional contexts also account for their position relative to the target word.",
  "y": "motivation background"
 },
 {
  "id": "26b00c6e5b499eea30e9cef0bbaf9f_6",
  "x": "**MATERIALS** We report results from<cite> Salle et al. (2016)</cite> and use the same training corpus and parameters to train LexVec with positional contexts and external memory. The corpus is a Wikipedia dump from June 2015, tokenized, lowercased, and split into sentences, removing punctuation and converting numbers to words, for a final vocabulary of 302,203 words.",
  "y": "uses"
 },
 {
  "id": "26b00c6e5b499eea30e9cef0bbaf9f_7",
  "x": "As recommended in Levy et al. (2015) and used in<cite> Salle et al. (2016)</cite> , the PPMI matrix used in all LexVec models and in PPMI-SVD is transformed using context distribution smoothing exponentiating context frequencies to the power 0.75. PPMI-SVD is the singular value decomposition of the PPMI matrix. LexVec and PPMI-SVD use symmetric windows of size 2.",
  "y": "uses"
 },
 {
  "id": "26b00c6e5b499eea30e9cef0bbaf9f_8",
  "x": "For PPMI-SVD and GloVe we report (W +W ), and for SGNS, W , that correspond to their best results. The goal of our evaluation is to determine whether: 1) Positional contexts improve syntactic performance 2) The use of external memory is a good approximation of WSNS. Therefore, we perform the exact same evaluation as<cite> Salle et al. (2016)</cite> , namely the WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001) , MEN (Bruni et al., 2012) , MTurk (Radinsky et al., 2011) , RW (Luong et al., 2013) , SimLex-999 (Hill et al., 2015) , MC (Miller and Charles, 1991) , RG (Rubenstein and Goodenough, 1965) , and SCWS (Huang et al., 2012) word similarity tasks 1 , and the Google semantic (GSem) and syntactic (GSyn) analogy (Mikolov et al., 2013a) and MSR syntactic analogy dataset (Mikolov et al., 2013c) tasks.",
  "y": "uses"
 },
 {
  "id": "26fbf9f4ae740513d8889160ad9f63_0",
  "x": "Several work have shown that discourse relations can improve the results of summarization in the case of factual texts or news articles (e.g.<cite> (Otterbacher et al., 2002)</cite> ). However, to our knowledge no work has evaluated the usefulness of discourse relations for the summarization of informal and opinionated texts, as those found in the social media. In this paper, we consider the most frequent discourse relations found in blogs: namely comparison, contingency, illustration, attribution, topic-opinion, and attributive and evaluate the effect of each relation on informal text summarization using the Text Analysis Conference (TAC) 2008 opinion summarization dataset 1 .",
  "y": "background"
 },
 {
  "id": "26fbf9f4ae740513d8889160ad9f63_1",
  "x": "In some work (e.g. (Bosma, 2004; Blair-Goldensohn and McKeown, 2006) ), discourse relations have been exploited successfully for multi-document summarization. In particular,<cite> (Otterbacher et al., 2002)</cite> experimentally showed that discourse relations can improve the coherence of multi-document summaries. (Bosma, 2004) showed how discourse relations can be used effectively to incorporate additional contextual information for a given question in a query-based summarization.",
  "y": "background"
 },
 {
  "id": "26fbf9f4ae740513d8889160ad9f63_2",
  "x": "From our corpus analysis, we have identified the six most prevalent discourse relations in this blog dataset, namely comparison, contingency, illustration, attribution, topic-opinion, and attributive. The comparison, contingency, and illustration relations are also considered by most of the work in the field of discourse analysis such as the PDTB: Penn Discourse TreeBank research group <cite>(Prasad et al., 2008)</cite> and the RST Discourse Treebank research group (Carlson and Marcu, 2001 ). We considered three additional classes of relations: attributive, attribution, and topic-opinion.",
  "y": "background"
 },
 {
  "id": "26fbf9f4ae740513d8889160ad9f63_3",
  "x": "From our corpus analysis, we have identified the six most prevalent discourse relations in this blog dataset, namely comparison, contingency, illustration, attribution, topic-opinion, and attributive. The comparison, contingency, and illustration relations are also considered by most of the work in the field of discourse analysis such as the PDTB: Penn Discourse TreeBank research group <cite>(Prasad et al., 2008)</cite> and the RST Discourse Treebank research group (Carlson and Marcu, 2001 ). We considered three additional classes of relations: attributive, attribution, and topic-opinion.",
  "y": "similarities"
 },
 {
  "id": "26fbf9f4ae740513d8889160ad9f63_4",
  "x": "From our corpus analysis, we have identified the six most prevalent discourse relations in this blog dataset, namely comparison, contingency, illustration, attribution, topic-opinion, and attributive. The comparison, contingency, and illustration relations are also considered by most of the work in the field of discourse analysis such as the PDTB: Penn Discourse TreeBank research group <cite>(Prasad et al., 2008)</cite> and the RST Discourse Treebank research group (Carlson and Marcu, 2001 ). We considered three additional classes of relations: attributive, attribution, and topic-opinion.",
  "y": "extends differences"
 },
 {
  "id": "26fbf9f4ae740513d8889160ad9f63_5",
  "x": "These discourse relations are summarized in Figure 1 while a description of these relations is given below. Illustration: Is used to provide additional information or detail about a situation. For example: \"Allied Capital is a closed-end management investment company that will operate as a business development concern.\" As shown in Figure 1 , illustration relations can be sub-divided into sub-categories: joint, list, disjoint, and elaboration relations according to the RST Discourse Treebank (Carlson and Marcu, 2001 ) and the Penn Discourse TreeBank <cite>(Prasad et al., 2008)</cite> .",
  "y": "background"
 },
 {
  "id": "26fbf9f4ae740513d8889160ad9f63_6",
  "x": "Contingency: Provides cause, condition, reason or evidence for a situation, result or claim. For example: \"The meat is good because they slice it right in front of you.\" As shown in Figure 1 , the contingency relation subsumes several more specific relations: explanation, evidence, reason, cause, result, consequence, background, condition, hypothetical, enablement, and purpose relations according to the Penn Discourse TreeBank <cite>(Prasad et al., 2008)</cite> .",
  "y": "background"
 },
 {
  "id": "26fbf9f4ae740513d8889160ad9f63_7",
  "x": "Comparison: Gives a comparison and contrast among different situations. For example, \"Its fastforward and rewind work much more smoothly and consistently than those of other models I've had.\" The comparison relation subsumes the contrast relation according to the Penn Discourse TreeBank <cite>(Prasad et al., 2008)</cite> and the analogy and preference relations according to the RST Discourse Treebank (Carlson and Marcu, 2001) .",
  "y": "background"
 },
 {
  "id": "26fbf9f4ae740513d8889160ad9f63_8",
  "x": "To identify illustration, contingency, comparison, and attribution relations, we have used SPADE discourse parser. However, we have complemented this parser with three other approaches: (Jindal and Liu, 2006 )'s approach is used to identify intra-sentence comparison relations; we have designed a tagger based on (Fei et al., 2008) 's approach to identify topic-opinion relations; and we have proposed a new approach to tag attributive relations<cite> (Mithun, 2012)</cite> . A description and evaluation of these approaches can be found in<cite> (Mithun, 2012)</cite> .",
  "y": "extends uses"
 },
 {
  "id": "26fbf9f4ae740513d8889160ad9f63_9",
  "x": "To identify illustration, contingency, comparison, and attribution relations, we have used SPADE discourse parser. However, we have complemented this parser with three other approaches: (Jindal and Liu, 2006 )'s approach is used to identify intra-sentence comparison relations; we have designed a tagger based on (Fei et al., 2008) 's approach to identify topic-opinion relations; and we have proposed a new approach to tag attributive relations<cite> (Mithun, 2012)</cite> . A description and evaluation of these approaches can be found in<cite> (Mithun, 2012)</cite> .",
  "y": "background"
 },
 {
  "id": "26fbf9f4ae740513d8889160ad9f63_10",
  "x": "To identify illustration, contingency, comparison, and attribution relations, we have used SPADE discourse parser. However, we have complemented this parser with three other approaches: (Jindal and Liu, 2006 )'s approach is used to identify intra-sentence comparison relations; we have designed a tagger based on (Fei et al., 2008) 's approach to identify topic-opinion relations; and we have proposed a new approach to tag attributive relations<cite> (Mithun, 2012)</cite> . A description and evaluation of these approaches can be found in<cite> (Mithun, 2012)</cite> .",
  "y": "uses"
 },
 {
  "id": "26fbf9f4ae740513d8889160ad9f63_11",
  "x": "To measure the usefulness of discourse relations for the summarization of informal texts, we have tested the effect of each relation with four different summarizers: BlogSum<cite> (Mithun, 2012)</cite> , MEAD<cite> (Radev et al., 2004)</cite> , the best scoring system at TAC 2008 5 and the best scoring system at DUC 2007 6 . We have evaluated the effect of each discourse relation on the summaries generated and compared the results. Let us first describe the BlogSum summarizer.",
  "y": "uses"
 },
 {
  "id": "26fbf9f4ae740513d8889160ad9f63_12",
  "x": "To measure the usefulness of discourse relations for the summarization of informal texts, we have tested the effect of each relation with four different summarizers: BlogSum<cite> (Mithun, 2012)</cite> , MEAD<cite> (Radev et al., 2004)</cite> , the best scoring system at TAC 2008 5 and the best scoring system at DUC 2007 6 . Finally the most appropriate schema is selected based on a given question type; and candidate sentences fill particular slots in the selected schema based on which discourse relations they contain in order to create the final summary (details of BlogSum can be found in<cite> (Mithun, 2012)</cite> ).",
  "y": "uses background"
 },
 {
  "id": "26fbf9f4ae740513d8889160ad9f63_13",
  "x": "Finally the most appropriate schema is selected based on a given question type; and candidate sentences fill particular slots in the selected schema based on which discourse relations they contain in order to create the final summary (details of BlogSum can be found in<cite> (Mithun, 2012)</cite> ). ---------------------------------- **EVALUATION OF DISCOURSE RELATIONS ON BLOGS**",
  "y": "background"
 },
 {
  "id": "26fbf9f4ae740513d8889160ad9f63_14",
  "x": "This result indicates that the use of discourse relations as a whole helps to include more question relevant sentences and improve the summary content. To ensure that the results were not specific to our summarizer, we performed the same experiments with two other systems: the MEAD summarizer<cite> (Radev et al., 2004)</cite> , a publicly available and a widely used summarizer, and with the output of the TAC best-scoring system. For MEAD, we first generated candidate sentences using MEAD, then these candidate sentences were tagged using discourse relation taggers used under BlogSum.",
  "y": "uses"
 },
 {
  "id": "27dbdd4827554df0f53013966242dc_0",
  "x": "Our work is based on the SummaRuNNer model <cite>[5]</cite> . It consists of a two-layer bi-directional Gated Recurrent Unit (GRU) Recurrent Neural Network (RNN) which treats the summarization problem as a binary sequence classification problem, where each sentence is classified sequentially as sentence to be included or not in the summary. However, we introduced two modifications to the original SummaRuNNer architecture, leading to better results while reducing complexity: arXiv:1911.06121v1 [cs.CL] 13 Nov 2019 Fig. 1 .",
  "y": "extends"
 },
 {
  "id": "27dbdd4827554df0f53013966242dc_2",
  "x": "---------------------------------- **DATA** In contrast to <cite>[5]</cite> , we trained our model only on CNN articles from the CNN/Daily Mail corpus [2] .",
  "y": "differences"
 },
 {
  "id": "27dbdd4827554df0f53013966242dc_3",
  "x": "In contrast to <cite>[5]</cite> , we trained our model only on CNN articles from the CNN/Daily Mail corpus [2] . Due to the limited number of provided news articles, we automatically annotated a large corpus of CNN articles from which an abstractive summary was available. In a similar approach to <cite>[5]</cite> , we calculated the ROUGE-1 F1 score between each sentence and its article's abstractive summary.",
  "y": "similarities"
 },
 {
  "id": "27ee0fbed3a88854ebe945dfffefd8_0",
  "x": "A review of the methods in the article <cite>[35]</cite> about the recognition of timexes for English and Spanish has shown a certain shift within the most popular solutions. As with the normalisation of timexes, the best results are still achieved with rule-based methods, many new solutions have been introduced in the area of recognition. The best systems listed in <cite>[35]</cite> , called TIPSem [16] and ClearTK [1] , use CRFs for recognition, so initially, we decided to apply the CRF-based approach for this task.",
  "y": "uses"
 },
 {
  "id": "27ee0fbed3a88854ebe945dfffefd8_1",
  "x": "A review of the methods in the article <cite>[35]</cite> about the recognition of timexes for English and Spanish has shown a certain shift within the most popular solutions. As with the normalisation of timexes, the best results are still achieved with rule-based methods, many new solutions have been introduced in the area of recognition. The best systems listed in <cite>[35]</cite> , called TIPSem [16] and ClearTK [1] , use CRFs for recognition, so initially, we decided to apply the CRF-based approach for this task.",
  "y": "uses"
 },
 {
  "id": "27ee0fbed3a88854ebe945dfffefd8_2",
  "x": "---------------------------------- **EXPERIMENTS AND RESULTS** Experiments were carried out by the method proposed in <cite>[35]</cite> .",
  "y": "uses"
 },
 {
  "id": "27ee0fbed3a88854ebe945dfffefd8_3",
  "x": "The results are presented in Tables 6, 7 and 8. We chose the best 3 results from each word embeddings group (EE, EP, EC) from Table 8 presenting F1-scores for all models. Then we evaluated these results using more detailed measures for timexes, presented in <cite>[35]</cite> .",
  "y": "uses"
 },
 {
  "id": "27ee0fbed3a88854ebe945dfffefd8_5",
  "x": "[Sunday] and [Sunday morning] <cite>[35]</cite> . If there was an overlap, a relaxed type F1-score (Type.F1) was calculated <cite>[35]</cite> . The results are presented in Table 9 .",
  "y": "uses"
 },
 {
  "id": "27ee0fbed3a88854ebe945dfffefd8_6",
  "x": "Then we evaluated these results using more detailed measures for timexes, presented in <cite>[35]</cite> . F1) evaluation has also been carried out to determine whether there is an overlap between the system entity and gold entity, e.g. [Sunday] and [Sunday morning] <cite>[35]</cite> .",
  "y": "uses"
 },
 {
  "id": "27ee0fbed3a88854ebe945dfffefd8_7",
  "x": [
   "Also, previous models built using KGR10 (EP) are probably less accurate due to an incorrect tokenisation of the corpus. We used WCRFT tagger [29] , which utilises Toki [30] to tokenise the input text before the creation of the embeddings model. The comparison of EC1 with previous results obtained using only CRF [9] show the significant improvement across all the tested metrics: 3.6pp increase in strict F1-score, 1.36pp increase in relaxed precision, 5.61pp increase in relaxed recall and 3.51pp increase in relaxed F1-score."
  ],
  "y": "uses"
 },
 {
  "id": "28038a4fa4182ccdc6134f2138c0da_0",
  "x": "**INTRODUCTION** The task of definition modeling, introduced by <cite>Noraset et al. (2017)</cite> , consists in generating the dictionary definition of a specific word: for instance, given the word \"monotreme\" as input, the system would need to produce a definition such as \"any of an order (Monotremata) of egg-laying mammals comprising the platypuses and echidnas\". 1 Following the tradition set by lexicographers, we call the word being defined a definiendum (pl. definienda), whereas a word occurring in its definition is called a definiens (pl. definientia).",
  "y": "background"
 },
 {
  "id": "28038a4fa4182ccdc6134f2138c0da_1",
  "x": "Such systems may also be 1 Definition from Merriam-Webster. able to provide reading help by giving definitions for words in the text. A major intended application of definition modeling is the explication and evaluation of distributed lexical representations, also known as word embeddings<cite> (Noraset et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "28038a4fa4182ccdc6134f2138c0da_2",
  "x": "In their seminal work on definition modeling, <cite>Noraset et al. (2017)</cite> likened systems generating definitions to language models, which can naturally be used to generate arbitrary text. They built a sequential LSTM seeded with the embedding of the definiendum; its output at each time-step was mixed through a gating mechanism with a feature vector derived from the definiendum. Gadetsky et al. (2018) stressed that a definiendum outside of its specific usage context is ambiguous between all of its possible definitions.",
  "y": "background"
 },
 {
  "id": "28038a4fa4182ccdc6134f2138c0da_3",
  "x": "Quoting from the example above, the context \"enough around-let's get back to work!\" sufficiently characterizes the meaning of the omitted verb to allow for an approximate definition for it even if the blank is not filled (Taylor, 1953; Devlin et al., 2018) . This reformulation can appear contrary to the original proposal by <cite>Noraset et al. (2017)</cite> , which conceived definition modeling as a \"word-tosequence task\". They argued for an approach related to, though distinct from sequence-to-sequence architectures.",
  "y": "background"
 },
 {
  "id": "28038a4fa4182ccdc6134f2138c0da_4",
  "x": "Though different kinds of linguistic contexts have been suggested throughout the literature, we remark here that sentential context may sometimes suffice to guess the meaning of a word that we don't know (Lazaridou et al., 2017) . Quoting from the example above, the context \"enough around-let's get back to work!\" sufficiently characterizes the meaning of the omitted verb to allow for an approximate definition for it even if the blank is not filled (Taylor, 1953; Devlin et al., 2018) . This reformulation can appear contrary to the original proposal by <cite>Noraset et al. (2017)</cite> , which conceived definition modeling as a \"word-tosequence task\".",
  "y": "differences"
 },
 {
  "id": "28038a4fa4182ccdc6134f2138c0da_5",
  "x": "Despite some key differences, all of the previously proposed architectures we are aware of<cite> (Noraset et al., 2017</cite>; Gadetsky et al., 2018; followed a pattern similar to sequence-to-sequence models. They all implicitly or explicitly used distinct submodules to encode the definiendum and to generate the definientia. In the case of <cite>Noraset et al. (2017)</cite> , the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters derived from a characterlevel CNN, and its \"hypernym embedding\".",
  "y": "similarities"
 },
 {
  "id": "28038a4fa4182ccdc6134f2138c0da_6",
  "x": "They all implicitly or explicitly used distinct submodules to encode the definiendum and to generate the definientia. In the case of <cite>Noraset et al. (2017)</cite> , the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters derived from a characterlevel CNN, and its \"hypernym embedding\". Gadetsky et al. (2018) used a sigmoid-based gating module to tweak the definiendum embedding.",
  "y": "background"
 },
 {
  "id": "28038a4fa4182ccdc6134f2138c0da_7",
  "x": "Should we mark the definiendum before encoding, then only the definiendum embedding is passed into the encoder: the resulting system provides out-of-context definitions, like in <cite>Noraset et al. (2017)</cite> where the definition is not linked to the context of a word but to its definiendum only. For context to be taken into account under the multiplicative strategy, tokens w k must be encoded and contextualized before integration with the indicator i k . Figure 1a presents the contextual SELECT mechanism visually.",
  "y": "similarities"
 },
 {
  "id": "28038a4fa4182ccdc6134f2138c0da_8",
  "x": "We do not share vocabularies between the encoder and the decoder: therefore output tokens can only correspond to words attested as definientia. 4 The dropout rate and warmup steps number were set using a hyperparameter search on the dataset from <cite>Noraset et al. (2017)</cite> , during which encoder and decoder vocabulary were merged for computational simplicity and models stopped after 12,000 steps. We first fixed dropout to 0.1 and tested warmup step values between 1000 and 10,000 by increments of 1000, then focused on the most promising span (1000-4000 steps) and exhaustively tested dropout rates from 0.2 to 0.8 by increments of 0.1.",
  "y": "uses"
 },
 {
  "id": "28038a4fa4182ccdc6134f2138c0da_9",
  "x": "We train our models on three distinct datasets, which are all borrowed or adapted from previous works on definition modeling. As a consequence, our experiments focus on the English language. The dataset of <cite>Noraset et al. (2017)</cite> (henceforth D Nor ) maps definienda to their respective definientia, as well as additional information not used here.",
  "y": "differences"
 },
 {
  "id": "28038a4fa4182ccdc6134f2138c0da_10",
  "x": "We train our models on three distinct datasets, which are all borrowed or adapted from previous works on definition modeling. The dataset of <cite>Noraset et al. (2017)</cite> (henceforth D Nor ) maps definienda to their respective definientia, as well as additional information not used here.",
  "y": "uses"
 },
 {
  "id": "28038a4fa4182ccdc6134f2138c0da_11",
  "x": "However consider for instance the word \"elation\": that it be defined either as \"mirth\" or \"joy\" should only influence our metric slightly, and not be discounted as a completely wrong prediction. , as they did not report the perplexity of their system and focused on a different dataset; likewise, consider only the Chinese variant of the task. Perplexity measures for <cite>Noraset et al. (2017)</cite> and Gadetsky et al. (2018) are taken from the authors' respective publications. All our models perform better than previous proposals, by a margin of 4 to 10 points, for a relative improvement of 11-23%.",
  "y": "background"
 },
 {
  "id": "28038a4fa4182ccdc6134f2138c0da_12",
  "x": "However consider for instance the word \"elation\": that it be defined either as \"mirth\" or \"joy\" should only influence our metric slightly, and not be discounted as a completely wrong prediction. , as they did not report the perplexity of their system and focused on a different dataset; likewise, consider only the Chinese variant of the task. Perplexity measures for <cite>Noraset et al. (2017)</cite> and Gadetsky et al. (2018) are taken from the authors' respective publications. All our models perform better than previous proposals, by a margin of 4 to 10 points, for a relative improvement of 11-23%.",
  "y": "differences"
 },
 {
  "id": "28038a4fa4182ccdc6134f2138c0da_13",
  "x": "This lends empirical support to our claim that definition modeling is a nontrivial sequence-to-sequence task, which can be better treated with sequence methods. The stability of the performance improvement over the noncontextual variant in both contextual datasets also highlights that our proposed additive marking is fairly robust, and functions equally well when confronted to somewhat artificial inputs, as in D Gad , or to linguistically coherent sequences, as in D Ctx . A manual analysis of definitions produced by our system reveals issues similar to those discussed by <cite>Noraset et al. (2017)</cite> , namely selfreference, 7 POS-mismatches, over-and underspecificity, antonymy, and incoherence.",
  "y": "similarities"
 },
 {
  "id": "28038a4fa4182ccdc6134f2138c0da_14",
  "x": "Self-referring definitions highlight that our models equate the meaning of the definiendum to the composed meaning of its definientia. Simply masking the corresponding output embedding might suffice to prevent this specific problem; preliminary experiments in that direction suggest that this may also help decrease perplexity further. As for POS-mismatches, we do note that the work of <cite>Noraset et al. (2017)</cite> had a much lower rate of 4.29%: we suggest that this may be due to the fact that they employ a learned character-level convolutional network, which arguably would be able to capture orthography and rudiments of morphology.",
  "y": "differences"
 },
 {
  "id": "291a6ac3f0c2d27ca69ee8f5f266f5_0",
  "x": "This paper proposes an expansion of set of primitive constraints available within the Primitive Optimality Theory framework <cite>(Eisner, 1997a)</cite> . This expansion consists of the addition of a new family of constraints--existential implicational constraints, which allow the specification of faithfulness constraints that can be satisfied at a distance--and the definition of two ways to combine simple constraints into com: plex constraints, that is, constraint disjunction (Crowhurst and Hewitt, 1995) and local constraint conjunction (Smolensky, 1995) . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "291a6ac3f0c2d27ca69ee8f5f266f5_1",
  "x": "This paper proposes an expansion of set of primitive constraints available within the Primitive Optimality Theory framework <cite>(Eisner, 1997a)</cite> . This expansion consists of the addition of a new family of constraints--existential implicational constraints, which allow the specification of faithfulness constraints that can be satisfied at a distance--and the definition of two ways to combine simple constraints into com: plex constraints, that is, constraint disjunction (Crowhurst and Hewitt, 1995) and local constraint conjunction (Smolensky, 1995) . ----------------------------------",
  "y": "extends"
 },
 {
  "id": "291a6ac3f0c2d27ca69ee8f5f266f5_2",
  "x": "---------------------------------- **INTRODUCTION** Primitive Optimality Theory (OTP) <cite>(Eisner, 1997a)</cite> , and extensions to it (e.g., Albro (1998) ), can be useful as a formal system in which phonological analyses can be implemented and evaluated.",
  "y": "background"
 },
 {
  "id": "291a6ac3f0c2d27ca69ee8f5f266f5_3",
  "x": "Primitive Optimality Theory (OTP) <cite>(Eisner, 1997a)</cite> , and extensions to it (e.g., Albro (1998) ), can be useful as a formal system in which phonological analyses can be implemented and evaluated. However, for certain types of constraints, translation into the primitives of OTP (Eisner (1997b) ) can only be accomplished by adding to the grammar a number of ad hoc phonological tiers. Because these tiers serve no phonological purpose other than to allow calculation of the constraints without adding new primitives, and because the addition of phonological tiers to an OTP grammar can have a dramatic negative impact on the efficiency of OTP implementations 1, it is preferable to avoid the addition of ad hoc tiers by adding new primitives to the system.",
  "y": "motivation"
 },
 {
  "id": "291a6ac3f0c2d27ca69ee8f5f266f5_4",
  "x": "This paper looks at three types of constraints employed throughout the Optimality Theoretic literature that cannot be translated in to the 1The computation time for an Optimality Theoretic derivation within the implementation of Albro (1998) increases exponentially with the number of tiers. The same is true for the implementation described in<cite> Eisner (1997a)</cite> , although a proposal is given there for a method that might improve the situation. primitives of OTP without reference to ad hoc tiers, and proposes a formalization of these constraints that is compatible with the finite state model described in<cite> Eisner (1997a)</cite> and Albro (1998) .",
  "y": "motivation uses"
 },
 {
  "id": "291a6ac3f0c2d27ca69ee8f5f266f5_5",
  "x": "This paper looks at three types of constraints employed throughout the Optimality Theoretic literature that cannot be translated in to the 1The computation time for an Optimality Theoretic derivation within the implementation of Albro (1998) increases exponentially with the number of tiers. The same is true for the implementation described in<cite> Eisner (1997a)</cite> , although a proposal is given there for a method that might improve the situation. primitives of OTP without reference to ad hoc tiers, and proposes a formalization of these constraints that is compatible with the finite state model described in<cite> Eisner (1997a)</cite> and Albro (1998) .",
  "y": "background"
 },
 {
  "id": "291a6ac3f0c2d27ca69ee8f5f266f5_6",
  "x": "These are constraints of existential implication (that is, of faithfulness without the requirement of alignment), constraint disjunction, and local constraint conjunction. 2 Existential Implication 2.1 Motivation OWP as described in<cite> Eisner (1997a)</cite> provides some support for correspondence constraints (input-output only). These may be defined by means of implication constraints of the form P --4 P or P --+ P, which can be interpreted as requiring, in the first case, that each surface constituent representing property P be aligned with an underlying constituent representing that property, and in the second case that every underlying constituent representing property P be aligned with a surface constituent representing that property.",
  "y": "background"
 },
 {
  "id": "291a6ac3f0c2d27ca69ee8f5f266f5_7",
  "x": "These constraints take represented by this notation outputs a violation for each domain 9,, where 9' represents the intersection of the domains 9,k, in which the time slice represented by the oq occurs, but no/3j occurs. Using the FST notation of<cite> Eisner (1997a)</cite> , the implementation for this constraint would be the following FST: [ represents \"((in or begin all 9,k) -(in all 9,k)),\" and ] represents \"((in or end all 9,k) -(in all 9,k)).\" That is, the machine moves from state S to state 1 if the domain 9, is entered.",
  "y": "uses"
 },
 {
  "id": "29294f2ed3cc2772ca57fd4294274c_0",
  "x": "Here we report similar findings on more challenging data, by exploring a dialogue system with a less structured understanding component, using off-the-shelf rather than domainadapted machine translation, and with languages that are not as closely related. Question-answering characters are designed to sustain a conversation driven primarily by the user asking questions. <cite>Leuski et al. (2006)</cite> developed algorithms for training such characters using linked questions and responses in the form of unstructured natural language text.",
  "y": "background"
 },
 {
  "id": "29294f2ed3cc2772ca57fd4294274c_1",
  "x": "Given a novel user question, the character finds an appropriate response from a list of available responses, and when a direct answer is not available, the character selects an \"off-topic\" response according to a set policy, ensuring that the conversation remains coherent even with a finite number of responses. The response selection algorithms are languageindependent, also allowing the questions and responses to be in separate languages. These algorithms have been incorporated into a tool which has been used to create characters for a variety of applications (e.g.<cite> Leuski et al., 2006</cite>; Artstein et al., 2009; Swartout et al., 2010) .",
  "y": "background"
 },
 {
  "id": "29294f2ed3cc2772ca57fd4294274c_2",
  "x": "**RESPONSE RANKING** We reimplemented parts of the response ranking algorithms of <cite>Leuski et al. (2006)</cite> , including both the language modeling (LM) and cross-language modeling (CLM) approaches. The LM approach constructs language models for both questions and responses using the question vocabulary.",
  "y": "extends differences"
 },
 {
  "id": "29294f2ed3cc2772ca57fd4294274c_3",
  "x": "The sum in eq. (5) is over all linked questionresponse pairs {S j , R j } in the training data, and the product is an estimate the probability of the question Q given the training question S j . In eq. (6), V R is the entire response vocabulary. We did not implement the parameter learning of <cite>Leuski et al. (2006)</cite> ; instead we use a constant smoothing parameter \u03bb \u03c0 = \u03bb \u03c6 = 0.1.",
  "y": "differences"
 },
 {
  "id": "29294f2ed3cc2772ca57fd4294274c_4",
  "x": "In eq. (6), V R is the entire response vocabulary. We did not implement the parameter learning of <cite>Leuski et al. (2006)</cite> ; instead we use a constant smoothing parameter \u03bb \u03c0 = \u03bb \u03c6 = 0.1. We also do not use the response threshold parameter, which <cite>Leuski et al. (2006)</cite> use to determine whether the top-ranked response is good enough.",
  "y": "differences"
 },
 {
  "id": "29294f2ed3cc2772ca57fd4294274c_5",
  "x": "**EVALUATION** We use accuracy as our success measure: the top ranked response to a test question is considered correct if it is identified as a correct response in the linked test data (there are up to 4 correct responses per question). This measure does not take into account non-understanding, that is the classifier's determination that the best response is not good enough<cite> (Leuski et al., 2006)</cite> , since this capability was not implemented; however, since all of our test questions are known to have at least one appropriate response, any non-understanding of a question would necessarily count against accuracy anyway.",
  "y": "differences background"
 },
 {
  "id": "29294f2ed3cc2772ca57fd4294274c_6",
  "x": "**RESULTS** The results of the experiments with matched question and response languages are reported in Table 1. The LM approach almost invariably produced better results than the CLM approach; this is the opposite of the findings of <cite>Leuski et al. (2006)</cite> , where CLM fared consistently better.",
  "y": "differences"
 },
 {
  "id": "29294f2ed3cc2772ca57fd4294274c_7",
  "x": "Another alternative is to use both languages together for classification; the fact that the manual Tamil translation identified some responses missed by the English classifier suggests that there may be benefit to this approach. Another direction for future work is identifying bad responses by using the distance between question and response to plot the tradeoff curve between errors and return rates (Artstein, 2011) . In our experiments the LM approach consistently outperforms the CLM approach, contra <cite>Leuski et al. (2006)</cite> .",
  "y": "differences"
 },
 {
  "id": "2a01f96893f9c0630a01ecce320184_0",
  "x": "Several research works have been proposed to detect propaganda on document-level (Rashkin et al., 2017; Barr\u00f3n-Cede\u00f1o et al., 2019b) , sentencelevel and fragment-level <cite>(Da San Martino et al., 2019)</cite> . Sentence-level detection or classification (SLC) is to determine whether a given sentence is propagandistic and it is a special binary classification problem, while the goal of fragment-level classification (FLC) is to extract fragments and assign with given labels such as loaded language, flag-waving and causal oversimplification, and it could be treated as a sequence labeling problem. Compared with document-level, sentence-level and fragment-level detection are much more helpful, since detection on sentences and fragments are more practical for real-life applications.",
  "y": "background"
 },
 {
  "id": "2a01f96893f9c0630a01ecce320184_1",
  "x": "Although Da San<cite> Martino et al. (2019)</cite> indicates that multi-task learning of both the SLC and the FLC could be beneficial for the SLC, in this paper, we only focus on the SLC task so as to better investigate whether context information could improve the performance of our system. Since several pretrained language models (Devlin et al., 2019; Liu et al., 2019) have been proved to be effective for text classification and other natural language understanding tasks, we use the pretrained BERT (Devlin et al., 2019) for the SLC task. This paper elaborates our BERT-based system for which we construct sentence-title pairs and sentence-context pairs as input.",
  "y": "differences"
 },
 {
  "id": "2a01f96893f9c0630a01ecce320184_2",
  "x": "Barr\u00f3n-Cede\u00f1o et al. (2019b) proposed to use Maximum Entropy classifier (Berger et al., 1996) with different features replicating the same experimental setup of Rashkin et al. (2017) for twoway and four-way classifications. A fine-grained propaganda corpus was proposed in Da San<cite> Martino et al. (2019)</cite> which includes both sentencelevel and fragment-level information. Based on this corpus and the pretrained BERT which is one of the most powerful pretrained language model, a multi-granularity BERT was proposed and it outperformed several strong BERT-based baselines.",
  "y": "background"
 },
 {
  "id": "2a01f96893f9c0630a01ecce320184_3",
  "x": "According to the statistics, only 29% of the training sentences are labeled as propaganda, and thus in this paper, we treat propaganda sentences as positive samples and non-propaganda sentences as negative samples. More details of the dataset could be found in Da San<cite> Martino et al. (2019)</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "2a01f96893f9c0630a01ecce320184_4",
  "x": "As described in Da San<cite> Martino et al. (2019)</cite> , the source of the dataset that we use is news articles, and since the title is usually the summarization of a news article, we use the title as supplementary information. Sentence-Context Pair: In addition to setting the title as the supplementary information, we construct the sentence-context pair which also includes preceding sentences as additional context, since preceding sentences usually convey the same or related events and this historical content is closely related to the current sentence. Figure 1 . shows the details of this kind of input pair in which the preceding sentence and the title are directly concatenated.",
  "y": "uses background"
 },
 {
  "id": "2a01f96893f9c0630a01ecce320184_5",
  "x": [
   "In the sentencelevel propaganda detection task, we construct sentence-title pairs and sentence-context pairs in order to better utilize context information to improve the performance of our system. Furthermore, the undersampling method is utilized to tackle the data imbalanced problem. Experiments show that both sentence-title/context pairs and the undersampling method could boost the performance of BERT on the SLC task."
  ],
  "y": "similarities future_work"
 },
 {
  "id": "2a84615479af66bbf875517a3a753b_0",
  "x": "In our previous work <cite>[7]</cite> , we applied a dual RNN in order to obtain a richer representation by blending the content and acoustic knowledge. In this paper, we improve upon our earlier work by incorporating an attention mechanism in the emotion recognition framework. The proposed attention mechanism is trained to exploit both textual and acoustic information in tandem.",
  "y": "background"
 },
 {
  "id": "2a84615479af66bbf875517a3a753b_1",
  "x": "Experimental evidence shows the potential of the approach. In our previous work <cite>[7]</cite> , we applied a dual RNN in order to obtain a richer representation by blending the content and acoustic knowledge. In this paper, we improve upon our earlier work by incorporating an attention mechanism in the emotion recognition framework.",
  "y": "extends"
 },
 {
  "id": "2a84615479af66bbf875517a3a753b_2",
  "x": "[5] identified emotional key phrases and salience of verbal cues from both phoneme sequences and words. Recently,<cite> [7,</cite> 18] combined acoustic information and conversation transcripts using a neural network-based model to improve emotion classification accuracy. However, none of these studies utilized attention method over audio and text modality in tandem for contextual understanding of the emotion in audio recording.",
  "y": "background"
 },
 {
  "id": "2a84615479af66bbf875517a3a753b_3",
  "x": "Recently,<cite> [7,</cite> 18] combined acoustic information and conversation transcripts using a neural network-based model to improve emotion classification accuracy. However, none of these studies utilized attention method over audio and text modality in tandem for contextual understanding of the emotion in audio recording. ----------------------------------",
  "y": "motivation background"
 },
 {
  "id": "2a84615479af66bbf875517a3a753b_4",
  "x": "**BIDIRECTIONAL RECURRENT ENCODER** Motivated by the architecture used in<cite> [7,</cite> 17, 19] , we train a recurrent encoder to predict the categorical class of a given audio signal. To model the sequential nature of the speech signal, we use a bidirectional recurrent encoder (BRE) as shown in the Figure 1 (a).",
  "y": "motivation"
 },
 {
  "id": "2a84615479af66bbf875517a3a753b_5",
  "x": "where f \u03b8 , f \u03b8 are the forward and backward long short-term memory (LSTM) with weight parameter \u03b8, ht represents the hidden state at t-th time step, and xt represents the t-th MFCC features in audio signal. The hidden representations ( \u2212 \u2192 h t, \u2190 \u2212 h t) from forward/backward LSTMs are concatenated for produce the feature, ot. To follow previous research <cite>[7]</cite> , we also add another prosodic feature vector, p, with each ot to generate a more informative vector representation of the signal, o A t .",
  "y": "uses"
 },
 {
  "id": "2a84615479af66bbf875517a3a753b_6",
  "x": "We propose a novel multi-hop attention method to predict the importance of audio and text, referred to multi-hop attention (MHA). Figure 1 shows the architecture of the proposed MHA model. Previous research used multi-modal information independently using neural network model by concatenating features from each modality<cite> [7,</cite> 21] .",
  "y": "background"
 },
 {
  "id": "2a84615479af66bbf875517a3a753b_7",
  "x": "Previous research used multi-modal information independently using neural network model by concatenating features from each modality<cite> [7,</cite> 21] . As opposed to this approach, we propose a neural network architecture that exploits information in each modality by extracting relevant segments of the speech data using information from the lexical content (and vice-versa). First, the acoustic and textual data are encoded with the audio-BRE and text-BRE, respectively, using equation (1).",
  "y": "differences"
 },
 {
  "id": "2a84615479af66bbf875517a3a753b_8",
  "x": "Total 10 unique speakers participated in this work. For consistent comparison with previous works<cite> [7,</cite> 18] , all utterances labeled \"excitement\" are merged with those labeled \"happiness\". We assign single categorical emotion to the utterance with majority of annotators agreed on the emotion labels.",
  "y": "uses"
 },
 {
  "id": "2a84615479af66bbf875517a3a753b_9",
  "x": "---------------------------------- **FEATURE EXTRACTION AND IMPLEMENTATION DETAILS** As this research is extended work from previous research <cite>[7]</cite> , we use the same feature extraction method as done in our previous work.",
  "y": "extends"
 },
 {
  "id": "2a84615479af66bbf875517a3a753b_10",
  "x": "To measure the performance of systems, we report the weighted accuracy (WA) and unweighted accuracy (UA) averaging over the 10-fold cross-validation experiments. We use the same dataset and features as other researchers<cite> [7,</cite> 18] . Table 1 presents performances of proposed approaches for recognizing speech emotion in comparison with various models.",
  "y": "uses"
 },
 {
  "id": "2a84615479af66bbf875517a3a753b_11",
  "x": "Even with the erroneous transcripts (WER = 5.53%), however, the proposed approach (MHA-2-ASR) outperforms the best baseline system (MDRE) by 1.6% relative (0.718 to 0.730) in terms of WA. Figure 2 shows the confusion matrices of the proposed systems. In audio-BRE (Fig. 2(a) ), most of the emotion labels are frequently misclassified as neutral class, supporting the claims of<cite> [7,</cite> 25] .",
  "y": "similarities"
 },
 {
  "id": "2b10893f03b4f5eaac0fe06b4d6115_0",
  "x": "In order to compare the performance of our system with others, we also used the dataset of<cite> Tu and Roth (2012)</cite> , which contains 1,348 sentences taken from different parts of the British National Corpus. However, they only focused on VPCs in this dataset, where 65% of the sentences contain a phrasal verb and 35% contain a simplex verbpreposition combination. As Table 1 indicates, the Tu&Roth dataset only focused on 23 different VPCs, but 342 unique VPCs were annotated in the Wiki50 corpus.",
  "y": "uses"
 },
 {
  "id": "2b10893f03b4f5eaac0fe06b4d6115_1",
  "x": "One example is<cite> Tu and Roth (2012)</cite> , where the authors examined a verbparticle combination only if the verbal components were formed with one of the previously given six verbs (i.e. make, take, have, give, do, get). Since Wiki50 was annotated for all VPC occurrences, we were able to check what percentage of VPCs could be covered if we applied this selection. As Table 3 shows, the six verbs used by<cite> Tu and Roth (2012)</cite> are responsible for only 50 VPCs on the Wiki50 corpus, so it covers only 11.16% of all gold standard VPCs.",
  "y": "background"
 },
 {
  "id": "2b10893f03b4f5eaac0fe06b4d6115_2",
  "x": "One example is<cite> Tu and Roth (2012)</cite> , where the authors examined a verbparticle combination only if the verbal components were formed with one of the previously given six verbs (i.e. make, take, have, give, do, get). Since Wiki50 was annotated for all VPC occurrences, we were able to check what percentage of VPCs could be covered if we applied this selection. As Table 3 shows, the six verbs used by<cite> Tu and Roth (2012)</cite> are responsible for only 50 VPCs on the Wiki50 corpus, so it covers only 11.16% of all gold standard VPCs.",
  "y": "background"
 },
 {
  "id": "2b10893f03b4f5eaac0fe06b4d6115_3",
  "x": "As can be seen, the top 10 VPCs are responsible for only 17.41% of the VPC occurrences, while the top 10 verbal components are responsible for 41.07% of the VPC occurrences in the Wiki50 corpus. Furthermore, 127 different verbal component occurred in Wiki50, but the verbs have and do -which are used by<cite> Tu and Roth (2012)</cite> -do not appear in the corpus as verbal component of VPCs. All this indicates that applying lexical restrictions and focusing on a reduced set of verbs will lead to the exclusion of a considerable number of VPCs occurring in free texts and so, real-world tasks would hardly profit from them.",
  "y": "background"
 },
 {
  "id": "2b10893f03b4f5eaac0fe06b4d6115_4",
  "x": "The J48 classifier of the WEKA package (Hall et al., 2009) was trained with its default settings on the abovementioned feature set, which implements the C4.5 (Quinlan, 1993) decision tree algorithm. Moreover, Support Vector Machines (SVM) (Cortes and Vapnik, 1995) results are also reported to compare the performance of our methods with that of<cite> Tu and Roth (2012)</cite> . As the investigated corpora were not sufficiently large for splitting them into training and test sets of appropriate size, we evaluated our models in a cross validation manner on the Wiki50 corpus and the Tu&Roth dataset.",
  "y": "uses"
 },
 {
  "id": "2b10893f03b4f5eaac0fe06b4d6115_5",
  "x": "Moreover, Support Vector Machines (SVM) (Cortes and Vapnik, 1995) results are also reported to compare the performance of our methods with that of<cite> Tu and Roth (2012)</cite> . As the investigated corpora were not sufficiently large for splitting them into training and test sets of appropriate size, we evaluated our models in a cross validation manner on the Wiki50 corpus and the Tu&Roth dataset. As<cite> Tu and Roth (2012)</cite> presented only the accuracy scores on the Tu & Roth dataset, we also employed an accuracy score as an evaluation metric on this dataset, where positive and negative examples were also marked.",
  "y": "similarities"
 },
 {
  "id": "2b10893f03b4f5eaac0fe06b4d6115_6",
  "x": "In this case, we applied the same VPC list that was described among the lexical features. Then we marked candidates of the syntax-based method as VPC if the candidate VPC was found in the list. We also compared our results with the rule-based results available for Wiki50 and also with the 5-fold cross validation results of<cite> Tu and Roth (2012)</cite> .",
  "y": "uses"
 },
 {
  "id": "2b10893f03b4f5eaac0fe06b4d6115_7",
  "x": "---------------------------------- **RESULTS** In order to compare the performance of our system with others, we evaluated it on the Tu&Roth dataset <cite>(Tu and Roth, 2012)</cite> .",
  "y": "uses"
 },
 {
  "id": "2b10893f03b4f5eaac0fe06b4d6115_8",
  "x": "over, it also lists the results of<cite> Tu and Roth (2012)</cite> and the VPCTagger evaluated in the 5-fold cross validation manner, as<cite> Tu and Roth (2012)</cite> applied this evaluation schema. As in the Tu&Roth dataset positive and negative examples were also marked, we were able to use accuracy as evaluation metric besides the F \u03b2=1 scores. It is revealed that the dictionary lookup and the rule-based method achieved an F-score of about 50, but our method seems the most successful on this dataset, as it can yield an accuracy 3.32% higher than that for the Tu&Roth system.",
  "y": "uses"
 },
 {
  "id": "2b10893f03b4f5eaac0fe06b4d6115_9",
  "x": "In addition, the dependency parsers achieve high precision with lower recall scores. Moreover, the results obtained with our machine learning approach on the Tu&Roth dataset outperformed those reported in<cite> Tu and Roth (2012)</cite> . This may be attributed to the inclusion of a rich feature set with new features like semantic and contextual features that were used in our system.",
  "y": "differences"
 },
 {
  "id": "2b10893f03b4f5eaac0fe06b4d6115_10",
  "x": "But the Wiki50 corpus may contain some rare examples and it probably reflects a more realistic distribution as it contains 342 unique VPCs. A striking difference between the Tu & Roth database and Wiki50 is that while<cite> Tu and Roth (2012)</cite> included the verbs do and have in their data, they do not occur at all among the VPCs collected from Wiki50. Moreover, these verbs are just responsible for 25 positive VPCs examples in the Tu & Roth dataset.",
  "y": "background"
 },
 {
  "id": "2b10893f03b4f5eaac0fe06b4d6115_11",
  "x": "Furthermore, we compared our methods with others when we evaluated our approach on the Tu&Roth dataset. Our method yielded better results than those got using the dependency parsers on the Wiki50 corpus and the method reported in <cite>(Tu and Roth, 2012)</cite> on the Tu&Roth dataset. Here, we also showed how dependency parsers performed on identifying VPCs, and our results indicate that although the dependency label provided by the parsers is an essential feature in determining whether a specific VPC candidate is a genuine VPC or not, the results can be further improved by extending the system with additional features like lexical and semantic features.",
  "y": "differences"
 },
 {
  "id": "2b148e376c39eae7f674610118e588_0",
  "x": "Earlier studies have focused on the agents' symbol usage, rather than on their representation of visual input. In this paper, we consider the referential games of <cite>Lazaridou et al. (2017)</cite> , and investigate the representations the agents develop during their evolving interaction. We find that the agents establish successful communication by inducing visual representations that almost perfectly align with each other, but, surprisingly, do not capture the conceptual properties of the objects depicted in the input images.",
  "y": "motivation"
 },
 {
  "id": "2b148e376c39eae7f674610118e588_1",
  "x": "There has recently been a revival of interests in language emergence simulations involving agents interacting in visually-grounded games. Unlike earlier work (e.g., Briscoe, 2002; Cangelosi and Parisi, 2002; Steels, 2012) , many recent simulations consider realistic visual input, for example, by playing referential games with real-life pictures (e.g., Jorge et al., 2016; <cite>Lazaridou et al., 2017</cite>; Havrylov and Titov, 2017; Lee et al., 2018; Evtimova et al., 2018) . This setup allows us to address the exciting issue of whether the needs of goal-directed communication will lead agents to associate visually-grounded conceptual representations to discrete symbols, developing naturallanguage-like word meanings.",
  "y": "background motivation"
 },
 {
  "id": "2b148e376c39eae7f674610118e588_2",
  "x": "However, while most studies present some analysis of the agents' symbol usage, they pay little or no attention to the representation of the visual input that the agents develop as part of their evolving interaction. We study here agent representations following the model and setup of <cite>Lazaridou et al. (2017)</cite> . This is an ideal starting point, since it involves an extremely simple signaling game (Lewis, 1969) , that is however played with naturalistic images, thus allowing us to focus on the question of how the agents represent these images, and whether such representations meet our expectations for natural word meanings.",
  "y": "motivation"
 },
 {
  "id": "2b148e376c39eae7f674610118e588_3",
  "x": "We study here agent representations following the model and setup of <cite>Lazaridou et al. (2017)</cite> . This is an ideal starting point, since it involves an extremely simple signaling game (Lewis, 1969) , that is however played with naturalistic images, thus allowing us to focus on the question of how the agents represent these images, and whether such representations meet our expectations for natural word meanings. In their first game, <cite>Lazaridou</cite>'s Sender and Receiver are exposed to the same pair of images, one of them being randomly marked as the \"target\".",
  "y": "background"
 },
 {
  "id": "2b148e376c39eae7f674610118e588_4",
  "x": "The Receiver sees the images in random order, together with the sent symbol, and it tries to guess which image is the target. In case of success, both players get a payoff of 1. Since an analysis of vocabulary usage brings inconclusive evidence that the agents are using the symbols to represent natural concepts (such as beaver or bayonet), <cite>Lazaridou and colleagues</cite> next modify the game, by presenting to the Sender and the Receiver different images for each of the two concepts (e.g., the Sender must now signal that the target is a beaver, while seeing a different beaver from the one shown to the Receiver).",
  "y": "background"
 },
 {
  "id": "2b148e376c39eae7f674610118e588_5",
  "x": "<cite>Lazaridou and colleagues</cite> present preliminary evidence suggesting that, indeed, agents are now developing conceptual symbol meanings. We replicate <cite>Lazaridou</cite>'s games, and we find that, in both, the agents develop successfully aligned representations that, however, are not capturing conceptual properties at all. In what is perhaps our most striking result, agents trained in either version of the game succeed at communicating about pseudoimages generated from random noise (Fig. 2) .",
  "y": "background"
 },
 {
  "id": "2b148e376c39eae7f674610118e588_6",
  "x": "This setup should encourage conceptlevel thinking, since the two agents should not be able to communicate about low-level perceptual characteristics of images they do not share. <cite>Lazaridou and colleagues</cite> present preliminary evidence suggesting that, indeed, agents are now developing conceptual symbol meanings. We replicate <cite>Lazaridou</cite>'s games, and we find that, in both, the agents develop successfully aligned representations that, however, are not capturing conceptual properties at all.",
  "y": "motivation uses"
 },
 {
  "id": "2b148e376c39eae7f674610118e588_7",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** Architecture We re-implement <cite>Lazaridou</cite>'s Sender and Receiver architectures (using their better-behaved \"informed\" Sender).",
  "y": "uses"
 },
 {
  "id": "2b148e376c39eae7f674610118e588_8",
  "x": "The whole architecture is jointly trained by letting the agents play, and updating their parameters with Reinforce (Williams, 1992) . See <cite>Lazaridou et al. (2017</cite>) for details. Data Following <cite>Lazaridou et al. (2017)</cite> , for each of the 463 concepts <cite>they</cite> used, we randomly sample 100 images from ImageNet (Deng et al., 2009 ).",
  "y": "background"
 },
 {
  "id": "2b148e376c39eae7f674610118e588_9",
  "x": "Data Following <cite>Lazaridou et al. (2017)</cite> , for each of the 463 concepts <cite>they</cite> used, we randomly sample 100 images from ImageNet (Deng et al., 2009 ). We construct 50, 000 mini-batches of 32 image pairs during training and 1, 024 pairs for validation. We construct a held-out test set in the same way by sampling 10 images per concept from ImageNet (for 2 concepts, we were not able to assemble enough further images), for a total of 4, 610.",
  "y": "uses similarities"
 },
 {
  "id": "2b148e376c39eae7f674610118e588_10",
  "x": "We also use the heldout set to construct mini-batches of images pairs to compute test performance. Following <cite>Lazaridou</cite>, the images are passed through a pre-trained VGG ConvNet (Simonyan and Zisserman, 2015) . The input vector fed to the agents is the second-tolast 4096-D fully connected layer 1 .",
  "y": "similarities uses"
 },
 {
  "id": "2b148e376c39eae7f674610118e588_11",
  "x": "Games We re-implement both <cite>Lazaridou</cite>'s same-image game, where Sender and Receiver are shown the same two images (always of different concepts), and their different-image game, where the Receiver sees different images than the Sender's. We repeat all experiments using 100 random initialization seeds. As we faithfully reproduced the setup of <cite>Lazaridou et al. (2017)</cite> , we refer the reader there for hyper-parameters and training details.",
  "y": "uses"
 },
 {
  "id": "2b148e376c39eae7f674610118e588_12",
  "x": "Games We re-implement both <cite>Lazaridou</cite>'s same-image game, where Sender and Receiver are shown the same two images (always of different concepts), and their different-image game, where the Receiver sees different images than the Sender's. We repeat all experiments using 100 random initialization seeds. As we faithfully reproduced the setup of <cite>Lazaridou et al. (2017)</cite> , we refer the reader there for hyper-parameters and training details.",
  "y": "similarities background"
 },
 {
  "id": "2b148e376c39eae7f674610118e588_13",
  "x": "Contrarily, for an image of a cabin in a field and an image of a telephone that have an intuitively correct very low input similarity of 0.02, the Sender similarity for these images is 0.94 (Receiver similarity is 0.95). <cite>Lazaridou et al. (2017)</cite> designed <cite>their</cite> second game to encourage more general, concept-like referents. Unfortunately, we replicate the anomalies above in the different-image setup, although to a less marked extent.",
  "y": "similarities background"
 },
 {
  "id": "2b148e376c39eae7f674610118e588_14",
  "x": "Existing literature in game theory already showed that convergence towards successful communication is ensured under specific conditions (see Skyrms (2010) and references therein). However, the important contribution of <cite>Lazaridou et al. (2017)</cite> is to play a signaling game with real-life images instead of artificial symbols. This raises new empirical questions that are not answered by the general mathematical results, such as: When the agents do succeed at communicating, what are the input features they rely upon?",
  "y": "motivation"
 },
 {
  "id": "2b6dd9388c43df4416c738b2d1ed5f_0",
  "x": "In this work, we use the datasets released by <cite>(Davidson et al. 2017 )</cite> and HEOT dataset provided by (Mathur et al. 2018) . The datasets obtained pass through these steps of processing: (i) Removal of punctuatios, stopwords, URLs, numbers, emoticons, etc. This was then followed by transliteration using the Xlit-Crowd conversion dictionary 3 and translation of each word to English using Hindi to English dictionary 4 .",
  "y": "uses"
 },
 {
  "id": "2b6dd9388c43df4416c738b2d1ed5f_1",
  "x": "The embeddings were trained on both the datasets provided by <cite>(Davidson et al. 2017 )</cite> and HEOT. These embeddings help to learn distributed representations of tweets. After experimentation, we kept the size of embeddings fixed to 100.",
  "y": "uses"
 },
 {
  "id": "2b6dd9388c43df4416c738b2d1ed5f_2",
  "x": "As indicated by the Figure 1 , the model was initially trained on the dataset provided by <cite>(Davidson et al. 2017)</cite> , and then re-trained on the HEOT dataset so as to benefit from the transfer of learned features in the last stage. The model hyperparameters were experimentally selected by trying out a large number of combinations through grid search. Results Table 3 shows the performance of our model (after getting trained on <cite>(Davidson et al. 2017)</cite> ) with two types of embeddings in comparison to the models by (Mathur et al. 2018) and <cite>(Davidson et al. 2017 )</cite> on the HEOT dataset averaged over three runs.",
  "y": "uses"
 },
 {
  "id": "2b6dd9388c43df4416c738b2d1ed5f_3",
  "x": "For comparison purposes, in Table 4 we have also evaluated our results on the dataset by <cite>(Davidson et al. 2017 )</cite>. ---------------------------------- **CONCLUSION**",
  "y": "uses"
 },
 {
  "id": "2b6dd9388c43df4416c738b2d1ed5f_4",
  "x": "**CLASSIFIER MODEL** Both the HEOT and <cite>(Davidson et al. 2017 )</cite> datasets contain tweets which are annotated in three categories: offensive, abusive and none (or benign). Some examples from the dataset are shown in Table 2 .",
  "y": "background"
 },
 {
  "id": "2b6dd9388c43df4416c738b2d1ed5f_5",
  "x": "Both the HEOT and <cite>(Davidson et al. 2017 )</cite> datasets contain tweets which are annotated in three categories: offensive, abusive and none (or benign). We use a LSTM based classifier model for training our model to classify these tweets into these three categories.",
  "y": "uses"
 },
 {
  "id": "2b6dd9388c43df4416c738b2d1ed5f_6",
  "x": "Results Table 3 shows the performance of our model (after getting trained on <cite>(Davidson et al. 2017)</cite> ) with two types of embeddings in comparison to the models by (Mathur et al. 2018) and <cite>(Davidson et al. 2017 )</cite> on the HEOT dataset averaged over three runs. We also compare results on pre-trained embeddings. As shown in the table, our model when given Glove embeddings performs better than all other models.",
  "y": "uses similarities differences"
 },
 {
  "id": "2b7267b7b192aeca15c0d10a5f0a4b_0",
  "x": "Deep learning techniques and distributed word representations appeared on recent studies like [17] where the role of RNNs (Recurrent Neural Networks), and CNNs (Convolutional Neural Networks) is explored. The author reports that CNNs perform best. An important work that has relevance here is <cite>[8]</cite> where authors present an even larger movie review dataset of 50,000 movie reviews from IMBD.",
  "y": "background"
 },
 {
  "id": "2b7267b7b192aeca15c0d10a5f0a4b_1",
  "x": "Again wikigiga models are positioned in the middle of the list and the worst performing models are MoodyCorpus and Text8Corpus. Our scores on this task are somehow lower than those reported from various studies that explore advanced deep learning constructs on same dataset. In <cite>[8]</cite> for example, authors who created movie review dataset try on it their probabilistic model that is able to capture semantic similarities between words.",
  "y": "background"
 },
 {
  "id": "2b7267b7b192aeca15c0d10a5f0a4b_2",
  "x": "Our scores on this task are somehow lower than those reported from various studies that explore advanced deep learning constructs on same dataset. In <cite>[8]</cite> for example, authors who created movie review dataset try on it their probabilistic model that is able to capture semantic similarities between words. They report a maximal accuracy of 0.88.",
  "y": "differences"
 },
 {
  "id": "2bb41cea97a0375f67eab3a77c3a97_0",
  "x": "Sample relations include people's titles, birth places, and marriage relationships. Traditional relation-extraction systems rely on manual annotations or domain-specific rules provided by experts, both of which are scarce resources that are not portable across domains. To remedy these problems, recent years have seen interest in the distant supervision approach for relation extraction (Wu and Weld, 2007; <cite>Mintz et al., 2009)</cite> .",
  "y": "motivation"
 },
 {
  "id": "2bb41cea97a0375f67eab3a77c3a97_1",
  "x": "Our primary contribution is to empirically assess how scaling these inputs to distant supervision impacts its result quality. We study this question with input data sets that are orders of magnitude larger than those in prior work. While the largest corpus (Wikipedia and New York Times) employed by recent work on distant supervision<cite> (Mintz et al., 2009</cite>; Hoffmann et al., 2011) contain about 2M documents, we run experiments on a 100M-document (50X more) corpus drawn from ClueWeb.",
  "y": "background"
 },
 {
  "id": "2bb41cea97a0375f67eab3a77c3a97_2",
  "x": "More closely related to relation extraction is the work of Lin and Patel (2001) that uses dependency paths to find answers that express the same relation as in a question. Since<cite> Mintz et al. (2009)</cite> coined the name \"distant supervision,\" there has been growing interest in this technique. For example, distant supervision has been used for the TAC-KBP slot-filling tasks and other relation-extraction tasks (Hoffmann et al., 2010; Carlson et al., 2010; Nguyen and Moschitti, 2011a; Nguyen and Moschitti, 2011b) .",
  "y": "background"
 },
 {
  "id": "2bb41cea97a0375f67eab3a77c3a97_3",
  "x": "At each step of the distant supervision process, we closely follow the recent literature<cite> (Mintz et al., 2009</cite>; . ---------------------------------- **DISTANT SUPERVISION**",
  "y": "similarities"
 },
 {
  "id": "2bb41cea97a0375f67eab3a77c3a97_4",
  "x": "For example, R i may contain pairs of married people. 4 We use the facts in R i combined with C to generate examples. Following recent work<cite> (Mintz et al., 2009</cite>; Hoffmann et al., 2011) , we use Freebase 5 as the knowledge base for seed facts.",
  "y": "similarities uses"
 },
 {
  "id": "2bb41cea97a0375f67eab3a77c3a97_5",
  "x": "As in previous work, we impose the constraint that both mentions (m 1 , m 2 ) \u2208 R + i are contained in the same sentence<cite> (Mintz et al., 2009</cite>; Hoffmann et al., 2011) . To generate negative examples for each relation, we follow the assumption in<cite> Mintz et al. (2009)</cite> that relations are disjoint and sample from other relations, i.e., R ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "2bb41cea97a0375f67eab3a77c3a97_6",
  "x": "As in previous work, we impose the constraint that both mentions (m 1 , m 2 ) \u2208 R + i are contained in the same sentence<cite> (Mintz et al., 2009</cite>; Hoffmann et al., 2011) . To generate negative examples for each relation, we follow the assumption in<cite> Mintz et al. (2009)</cite> that relations are disjoint and sample from other relations, i.e., R ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "2bb41cea97a0375f67eab3a77c3a97_7",
  "x": "Once we have constructed the set of possible mention pairs, the state-of-the-art technique to generate feature vectors uses linguistic tools such as partof-speech taggers, named-entity recognizers, dependency parsers, and string features. Following recent work on distant supervision<cite> (Mintz et al., 2009</cite>; Hoffmann et al., 2011) , we use both lexical and syntactic features. After this stage, we have a well-defined machine learning problem that is solvable using standard supervised techniques.",
  "y": "similarities uses"
 },
 {
  "id": "2bb41cea97a0375f67eab3a77c3a97_8",
  "x": "Interestingly, the Freebase held-out metric<cite> (Mintz et al., 2009</cite>; Hoffmann et al., 2011 ) turns out to be heavily biased toward distantly labeled data (e.g., increasing human feedback hurts precision; see Section 4.6). ---------------------------------- **EXPERIMENTAL SETUP**",
  "y": "differences"
 },
 {
  "id": "2bb41cea97a0375f67eab3a77c3a97_9",
  "x": "**FREEBASE HELD-OUT METRIC** In addition to the TAC-KBP benchmark, we also follow prior work<cite> (Mintz et al., 2009</cite>; Hoffmann et al., 2011) and measure the quality using held-out data from Freebase. We randomly partition both Freebase and the corpus into two halves.",
  "y": "differences"
 },
 {
  "id": "2c3a2999390b82f4e29b00d59f90f2_0",
  "x": "The most frequently applied technique in the CoNLL-2003 shared task is the Maximum Entropy Model. Three systems used Maximum Entropy Models in isolation (Bender et al., 2003; Chieu and Ng, 2003; Curran and Clark, 2003) . Two more systems used them in combination with other techniques<cite> (Florian et al., 2003</cite>; Klein et al., 2003) .",
  "y": "background"
 },
 {
  "id": "2c3a2999390b82f4e29b00d59f90f2_1",
  "x": "Two more systems used them in combination with other techniques<cite> (Florian et al., 2003</cite>; Klein et al., 2003) . Maximum Entropy Models seem to be a good choice for this kind of task: the top three results for English and the top two results for German were obtained by participants who employed them in one way or another. Hidden Markov Models were employed by four of the systems that took part in the shared task<cite> (Florian et al., 2003</cite>; Klein et al., 2003; Mayfield et al., 2003; Whitelaw and Patrick, 2003) .",
  "y": "background"
 },
 {
  "id": "2c3a2999390b82f4e29b00d59f90f2_2",
  "x": "Learning methods that were based on connectionist approaches were applied by four systems. Zhang and Johnson (2003) used robust risk minimization, which is a Winnow technique. <cite>Florian et al. (2003)</cite> employed the same technique in a combination of learners.",
  "y": "background"
 },
 {
  "id": "2c3a2999390b82f4e29b00d59f90f2_3",
  "x": "MH (Carreras et al., 2003b; Wu et al., 2003) and two other groups employed memory-based learning (De Meulder and Daelemans, 2003; Hendrickx and Van den Bosch, 2003) . Transformation-based learning<cite> (Florian et al., 2003)</cite> , Support Vector Machines (Mayfield et al., 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each. Combination of different learning systems has proven to be a good method for obtaining excellent results.",
  "y": "background"
 },
 {
  "id": "2c3a2999390b82f4e29b00d59f90f2_4",
  "x": "Five participating groups have applied system combination. <cite>Florian et al. (2003)</cite> tested different methods for combining the results of four systems and found that robust risk minimization worked best. Klein et al. (2003) employed a stacked learning system which contains Hidden Markov Models, Maximum Entropy Models and Conditional Markov Models.",
  "y": "background"
 },
 {
  "id": "2c3a2999390b82f4e29b00d59f90f2_5",
  "x": "A reasonable number of groups have also employed unannotated data for obtaining capitalization features for words. One participating team has used externally trained named entity recognition systems for English as a part in a combined system<cite> (Florian et al., 2003)</cite> . with extra information compared to while using only the available training data.",
  "y": "background"
 },
 {
  "id": "2c3a2999390b82f4e29b00d59f90f2_6",
  "x": "One participating team has used externally trained named entity recognition systems for English as a part in a combined system<cite> (Florian et al., 2003)</cite> . with extra information compared to while using only the available training data. The inclusion of extra named entity recognition systems seems to have worked well<cite> (Florian et al., 2003)</cite> .",
  "y": "background"
 },
 {
  "id": "2c3a2999390b82f4e29b00d59f90f2_7",
  "x": "We assume that performance A is significantly different from performance B if A is not within the center 90% of the distribution of B. The performances of the sixteen systems on the two test data sets can be found in Table 5 . For English, the combined classifier of <cite>Florian et al. (2003)</cite> achieved the highest overall F \u03b2=1 rate.",
  "y": "background"
 },
 {
  "id": "2c3a2999390b82f4e29b00d59f90f2_8",
  "x": "An important feature of the best system that other participants did not use, was the inclusion of the output of two externally trained named entity recognizers in the combination process. <cite>Florian et al. (2003)</cite> have also obtained the highest F \u03b2=1 rate for the German data. Here there is no significant difference between them and the systems of Klein et al. (2003) and Zhang and Johnson (2003) .",
  "y": "background"
 },
 {
  "id": "2c3a2999390b82f4e29b00d59f90f2_9",
  "x": "A majority vote of five systems (Chieu and Ng, 2003;<cite> Florian et al., 2003</cite>; Klein et al., 2003; McCallum and Li, 2003; Whitelaw and Patrick, 2003) performed best on the English development data. Another combination of five systems (Carreras et al., 2003b; Mayfield et al., 2003; McCallum and Li, 2003; Munro et al., 2003; Zhang and Johnson, 2003) obtained the best result for the German development data. We have performed a majority vote with these sets of systems on the related test sets and obtained F \u03b2=1 rates of 90.30 for English (14% error reduction compared with the best system) and 74.17 for German (6% error reduction).",
  "y": "background"
 },
 {
  "id": "2c3a2999390b82f4e29b00d59f90f2_10",
  "x": "The best performance for both languages has been obtained by a combined learning system that used Maximum Entropy Models, transformation-based learning, Hidden Markov Models as well as robust risk minimization<cite> (Florian et al., 2003)</cite> . Apart from the training data, this system also employed gazetteers and the output of two externally trained named entity recognizers. The performance of the system of Chieu et al. (2003) was not significantly different from the best performance for English and the method of Klein et al. (2003) and the approach of Zhang and Johnson (2003) were not significantly worse than the best result for German.",
  "y": "background"
 },
 {
  "id": "2cedb1a0f0c0fbb9bd95d5b54e4967_0",
  "x": "Reading comprehension is an intriguing task that assesses a machine's ability in understanding evidence contexts through question answering. Most previous work in reading comprehension has focused on either formal documents Rajpurkar et al. [2016] or children's stories Richardson, Burges, and Renshaw [2013] . Only few approaches have attempted comprehension on multiparty dialogue <cite>Ma, Jurczyk, and Choi [2018]</cite> .",
  "y": "background motivation"
 },
 {
  "id": "2cedb1a0f0c0fbb9bd95d5b54e4967_1",
  "x": "Inspired by various options of analytic models and the potential of the dialogue processing market, we extend the corpus presented by <cite>Ma, Jurczyk, and Choi [2018]</cite> for comprehensive predictions of personal entities in multiparty dialogue and develop deep learning models to make robust inference on their contexts. Passage completion on multiparty dialogue is one of the reading comprehension tasks that requires a model to match the conversational dialogues with the formal (passage) writings. By building a robust model for this task, people can tell the status of their favorite characters by checking synopsis to see if their favorite characters appear in specific episodes without watching the entire series, thus allowing them an efficient way to decide whether to watch a particular episode.",
  "y": "uses"
 },
 {
  "id": "2cedb1a0f0c0fbb9bd95d5b54e4967_2",
  "x": "Involving matching contexts between colloquial (dialog) and formal (passage) writings makes this task extremely challenging. Distinguished from the previous work that only focused on a single variable per passage <cite>Ma, Jurczyk, and Choi [2018]</cite> , we propose two new passage completion tasks on multiparty dialogue which increase the task complexity by replacing more character mentions with variables with a better motivated data split. The details of these tasks are in Section .",
  "y": "extends"
 },
 {
  "id": "2cedb1a0f0c0fbb9bd95d5b54e4967_3",
  "x": "Unlike the above tasks where documents and queries are written in a similar writing style, the multiparty dialogue reading comprehension task introduced by <cite>Ma, Jurczyk, and Choi [2018]</cite> has a very different writing style between dialogues and queries. However, their randomized assignment arXiv:1911.00773v1 [cs.CL] 2 Nov 2019 of samples to training and test data substantially decreases the complexity of cloze-style reading comprehension. In this paper, we address this issue by introducing a chronological data split and new variants of the cloze-style reading comprehension task to challenge even higher task complexity.",
  "y": "background"
 },
 {
  "id": "2cedb1a0f0c0fbb9bd95d5b54e4967_4",
  "x": "The first two seasons of the show for an entity linking task was annotated by Chen and Choi [2016] . Plot summaries of all episodes for the first eight seasons were collected by Jurczyk and Choi [2017] to evaluate a document retrieval task. The rest of the plot summaries were collected by <cite>Ma, Jurczyk, and Choi [2018]</cite> .",
  "y": "background"
 },
 {
  "id": "2cedb1a0f0c0fbb9bd95d5b54e4967_5",
  "x": "The rest of the plot summaries were collected by <cite>Ma, Jurczyk, and Choi [2018]</cite> . Table 1 shows the statistical data of the corpus from <cite>Ma, Jurczyk, and Choi [2018]</cite> . Based on the above corpus we created a new data split different from <cite>Ma, Jurczyk, and Choi [2018]</cite> 's data split.",
  "y": "uses"
 },
 {
  "id": "2cedb1a0f0c0fbb9bd95d5b54e4967_6",
  "x": "Table 1 shows the statistical data of the corpus from <cite>Ma, Jurczyk, and Choi [2018]</cite> . Based on the above corpus we created a new data split different from <cite>Ma, Jurczyk, and Choi [2018]</cite> 's data split. In the previous work of <cite>Ma, Jurczyk, and Choi [2018]</cite> , they used a random data split where 1,187 of 1,349 queries in the development set and 1,207 of 1,353 queries in the test set are generated from the same plot summaries as some queries in the training set with only masking the different character entities which makes the model can see the right answer in the training set.",
  "y": "motivation"
 },
 {
  "id": "2cedb1a0f0c0fbb9bd95d5b54e4967_7",
  "x": "We propose three tasks, one is from <cite>Ma, Jurczyk, and Choi [2018]</cite> , and another two tasks are new tasks designed by us. ---------------------------------- **SINGLE VARIABLE TASK(SV)**",
  "y": "uses"
 },
 {
  "id": "2cedb1a0f0c0fbb9bd95d5b54e4967_8",
  "x": "We propose three tasks, one is from <cite>Ma, Jurczyk, and Choi [2018]</cite> , and another two tasks are new tasks designed by us. The single variable task from <cite>Ma, Jurczyk, and Choi [2018]</cite> consists a dialogue passage p, a query q which is from plot summary of the dialogue passage and an answer a. In this 1 https://github.com/emorynlp/character-mining task, a query q replaces only one character entity with an unknown variable x and the machine is asked to infer the replaced character entity (answer a) from all the possible entities appear in the dialogue passage p. This task is evaluated by computing the accuracy of predictions (see Section ).",
  "y": "uses"
 },
 {
  "id": "2cedb1a0f0c0fbb9bd95d5b54e4967_9",
  "x": "---------------------------------- **SINGLE VARIABLE TASK(SV)** The single variable task from <cite>Ma, Jurczyk, and Choi [2018]</cite> consists a dialogue passage p, a query q which is from plot summary of the dialogue passage and an answer a. In this 1 https://github.com/emorynlp/character-mining task, a query q replaces only one character entity with an unknown variable x and the machine is asked to infer the replaced character entity (answer a) from all the possible entities appear in the dialogue passage p. This task is evaluated by computing the accuracy of predictions (see Section ).",
  "y": "background"
 },
 {
  "id": "2cedb1a0f0c0fbb9bd95d5b54e4967_10",
  "x": "---------------------------------- **CNN+BILSTM** Based on <cite>Ma, Jurczyk, and Choi [2018]</cite> , we first use CNN to extract the gram-level features of utterances and then use @ent04 asks @ent00 how someone could get a hold of @ent00 's credit card number and @ent00 is surprised at how much was spent .",
  "y": "uses"
 },
 {
  "id": "2cedb1a0f0c0fbb9bd95d5b54e4967_11",
  "x": "This method is the SOTA method last year in <cite>Ma, Jurczyk, and Choi [2018]</cite> 's data split which is also selected as one of our experimental methods. Similar to the last one, they still use CNN to extract token-level n-gram features of utterances and use LSTM to capture the sequence feature, however, they added the utterance-level attention and documentlevel attention to extract more features related to similarity between query and utterances. This utterance-level attention basically is to compute the similarity between each utterance and query.",
  "y": "uses"
 },
 {
  "id": "2cedb1a0f0c0fbb9bd95d5b54e4967_12",
  "x": "BiL-STM is good at capturing the sequence information of sentences; however, since it only finds some kind of answer distributions on the sequence information, it cannot capture the information of the relation between query and utterance. Adding a CNN can achieve even lower accuracy because passing sequences to the CNN only keeps important information after the pooling operation, but for dialogue data, most of the time the replaced entity needs to be decided by <cite>Ma, Jurczyk, and Choi [2018]</cite> are not helpful for these tasks on our data split because dialogues contain so many informal expressions and the size of the corpus is small. With limited data size, the attention mechanism does not perform well to match different expressions with the same meaning.",
  "y": "motivation"
 },
 {
  "id": "2cedb1a0f0c0fbb9bd95d5b54e4967_13",
  "x": "Results Table 4 shows the results of our experiment. BiL-STM is good at capturing the sequence information of sentences; however, since it only finds some kind of answer distributions on the sequence information, it cannot capture the information of the relation between query and utterance. Adding a CNN can achieve even lower accuracy because passing sequences to the CNN only keeps important information after the pooling operation, but for dialogue data, most of the time the replaced entity needs to be decided by <cite>Ma, Jurczyk, and Choi [2018]</cite> are not helpful for these tasks on our data split because dialogues contain so many informal expressions and the size of the corpus is small.",
  "y": "differences uses"
 },
 {
  "id": "2d2da2e9215691bffad74bfb97dbf3_0",
  "x": "This was the case in SemEval-2013, whose task 2 <cite>(Wilson et al., 2013)</cite> required sentiment analysis of Twitter and SMS text messages. Being the pre-decessor task of the challenge for which this work was developed, it is similar to this year's Task 9. The participating systems achieved better results in contextual polarity subtask (A) than those obtained for the overall message polarity subtask (B).",
  "y": "background"
 },
 {
  "id": "2d2da2e9215691bffad74bfb97dbf3_1",
  "x": "And perhaps this is the cause for lower score in the unconstrained mode, something that happened also with many systems in the past edition <cite>(Wilson et al., 2013)</cite> . This time, we implemented the contextual polarity solution based on the subtask B classifier. Given the results, we intend to do, in the near future, a new iteration of our system where the overall classifier will depend on (or receive features from) the current subtask A classifier.",
  "y": "similarities"
 },
 {
  "id": "2d2ec7230a651d1d6786d0f8a71f7e_0",
  "x": "Additionally, lexical items near in the embedding space to the lexical item under scrutiny can be considered as approximating its meaning at a given point in time or in a specific domain. These two lines of research converge in prior work to show, e.g., the increasing association of the lexical item 'gay' with the meaning dimension of homosexuality<cite> (Kim et al., 2014</cite>; Kulkarni et al., 2015) . Neural word embeddings (Mikolov et al., 2013) are probably the most influential among all embedding types (see Section 2.1).",
  "y": "background"
 },
 {
  "id": "2d2ec7230a651d1d6786d0f8a71f7e_1",
  "x": "Our investigation was performed on both historical (for the time span of 1900 to 1904) and contemporary texts (for the time span of 2005 to 2009) in two languages, English and German. It is thus a continuation of prior work, in which we investigated historical English texts only (Hellrich and Hahn, 2016a) , and also influenced by the design decisions of <cite>Kim et al. (2014)</cite> and Kulkarni et al. (2015) which were the first to use word embeddings in diachronic studies. Our results cast doubt on the reproducibility of such experiments where neighborhoods between words in embedding space are taken as a computationally valid indicator for properly capturing lexical meaning (and, consequently, meaning shifts).",
  "y": "uses"
 },
 {
  "id": "2d2ec7230a651d1d6786d0f8a71f7e_2",
  "x": "Word embeddings can be used rather directly for tracking semantic changes, namely by measuring the similarity of word representations generated for one word at different points in time-words which underwent semantic shifts will be dissimilar with themselves. These models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor<cite> (Kim et al., 2014</cite>; Hellrich and Hahn, 2016b) , or a mapping between models for different points in time must be calculated (Kulkarni et al., 2015; Hamilton et al., 2016) . The first approach cannot be performed in parallel and is thus rather time-consuming, if texts are not subsampled.",
  "y": "background motivation"
 },
 {
  "id": "2d2ec7230a651d1d6786d0f8a71f7e_3",
  "x": "**DIACHRONIC APPLICATION** Word embeddings can be used rather directly for tracking semantic changes, namely by measuring the similarity of word representations generated for one word at different points in time-words which underwent semantic shifts will be dissimilar with themselves. These models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor<cite> (Kim et al., 2014</cite>; Hellrich and Hahn, 2016b) , or a mapping between models for different points in time must be calculated (Kulkarni et al., 2015; Hamilton et al., 2016) .",
  "y": "background"
 },
 {
  "id": "2d2ec7230a651d1d6786d0f8a71f7e_4",
  "x": "The averaged cosine values between word embeddings before and after an epoch are used as a convergence measure c<cite> (Kim et al., 2014</cite>; Kulkarni et al., 2015) . It is defined for a vocabulary with n words and a matrix W containing word embedding vectors (normalized to length 1) for words i from training epochs e and e-1: We also define \u2206c, the change of c during subsequent epochs e-1, as another convergence criterion:",
  "y": "uses"
 },
 {
  "id": "2d2ec7230a651d1d6786d0f8a71f7e_5",
  "x": "Few changes occur after 4-6 epochs, which could be alternatively expressed as a \u2206c of about 0.003. The convergence criterion proposed by Kulkarni et al. (2015) , i.e., c = 0.9999, was never reached (this observation might be explained by Kulkarni et al.'s decision not to reset the learning rate for each training epoch, as was done by us and <cite>Kim et al. (2014)</cite> ). SVD PPMI , which are conceptually not bothered by the reliability problems we discussed here, were not a good fit for the hyperparameters we adopted from Kulkarni et al. (2015) .",
  "y": "similarities"
 },
 {
  "id": "2d7e98487698b0b6ae85f052402f7c_0",
  "x": "Prosodic Cues for DA Recognition: It has also been noted that prosodic knowledge plays a major role in DA identification for certain DA types<cite> Stolcke et al., 2000)</cite> . The main reason is that the acoustic signal of the same utterance can be very different in a different DA class. This indicates that if one wants to classify DA classes only from the text, the context must be an important aspect to consider: simply classifying single utterances might not be enough, but considering the preceding utterances as a context is important.",
  "y": "background"
 },
 {
  "id": "2d7e98487698b0b6ae85f052402f7c_1",
  "x": "**MODELLING APPROACHES** Lexical, Prosodic, and Syntactic Cues: Many studies have been carried out to find out the lexical, prosodic and syntactic cues <cite>(Stolcke et al., 2000</cite>; Surendran and Levow, 2006; O'Shea et al., 2012; Yang et al., 2014) . For the SwDA corpus, the state-of-the-art baseline result was 71%",
  "y": "background"
 },
 {
  "id": "2d7e98487698b0b6ae85f052402f7c_2",
  "x": "For the SwDA corpus, the state-of-the-art baseline result was 71% for more than a decade using a standard Hidden Markov Model (HMM) with language features such as words and n-grams<cite> (Stolcke et al., 2000)</cite> . The inter-annotator agreement accuracy for the same corpus is 84%, and in this particular case, we are still far from achieving human accuracy. However, words like 'yeah' appear in many classes such as backchannel, yes-answer, agree/accept etc.",
  "y": "background motivation"
 },
 {
  "id": "2d7e98487698b0b6ae85f052402f7c_3",
  "x": "**RESULTS** We follow the same data split of 1115 training and 19 test conversations as in the baseline approach <cite>(Stolcke et al., 2000</cite>; Kalchbrenner and Blunsom, 2013) . Table 3 shows the results of the proposed model with several setups, first without the context, then with one, two, and so on preceding utterances in the context.",
  "y": "uses"
 },
 {
  "id": "2db25254f275303c41f1e7ab15a5e0_0",
  "x": "However, Sporleder and Lascarides (2008) show that models trained on explicitly marked examples generalize poorly to implicit relation identification. They argued that explicit and implicit examples may be linguistically dissimilar, as writers tend to avoid discourse connectives if the discourse relation could be inferred from context (Grice, 1975) . Similar observations are made by <cite>Rutherford and Xue (2015)</cite> , who attempt to add automatically-labeled instances to improve supervised classification of implicit discourse relations. In this paper, we approach this problem from the perspective of domain adaptation.",
  "y": "background motivation"
 },
 {
  "id": "2db25254f275303c41f1e7ab15a5e0_1",
  "x": "<cite>Rutherford and Xue (2015)</cite> explore several selection heuristics for adding automatically-labeled examples from Gigaword to their system for implicit relation detection, obtaining a 2% improvement in Macro-F 1 . Our work differs from these previous efforts in that we focus exclusively on training from automaticallylabeled explicit instances, rather than supplementing a training set of manually-labeled implicit examples. Learning good feature representations (BenDavid et al., 2007) and reducing mismatched label distributions (Joshi et al., 2012) are two main ways to make a domain adaptation task successful.",
  "y": "differences background"
 },
 {
  "id": "2db25254f275303c41f1e7ab15a5e0_2",
  "x": "As this requires the label distribution from the target domain, it is no longer purely unsupervised domain adaptation; instead, we call it resampling with minimal supervision. It may also be desirable to ensure that the source and target training instances are similar in terms of their observed features; this is the idea behind the instance weighting approach to domain adaptation (Jiang and Zhai, 2007) . Motivated by this idea, we require that sampled instances from the source domain have a cosine similarity of at least \u03c4 with at least one target domain instance<cite> (Rutherford and Xue, 2015)</cite> .",
  "y": "similarities background"
 },
 {
  "id": "2db25254f275303c41f1e7ab15a5e0_3",
  "x": "For this data, we also need to extract the arguments of the identified connectives: for every identified connective, the sentence following this connective is labeled as Arg2 and the preceding sentence is labeled as Arg1, as suggested by Biran and McKeown (2013) . In a pilot study we found that larger amounts of additional training data yielded no further improvements, which is consistent with the recent results of <cite>Rutherford and Xue (2015)</cite> . Model selection We use a linear support vector machine (Fan et al., 2008) as the classification model.",
  "y": "similarities"
 },
 {
  "id": "2db25254f275303c41f1e7ab15a5e0_4",
  "x": "We have presented two methods -feature representation learning and resampling -from domain adaptation to close the gap of using explicit examples for unsupervised implicit discourse relation identification. Future work will explore the combination of this approach with more sophisticated techniques for instance selection<cite> (Rutherford and Xue, 2015)</cite> and feature selection (Park and Cardie, 2012; Biran and McKeown, 2013) , while also tackling the more difficult problems of multi-class relation classification and fine-grained level-2 discourse relations.",
  "y": "future_work"
 },
 {
  "id": "2eaa48dbc5e42a5934e905ec2288ac_0",
  "x": "AES is a challenging task as it relies on grammar as well as semantics, pragmatics and discourse (Song et al., 2017) . Although traditional AES methods typically rely on handcrafted features (Larkey, 1998; Foltz et al., 1999; Attali and Burstein, 2006; Dikli, 2006; Wang and Brown, 2008; Chen and He, 2013; Somasundaran et al., 2014; Yannakoudakis et al., 2014; Phandi et al., 2015) , recent results indicate that state-of-the-art deep learning methods reach better performance (Alikaniotis et al., 2016; Dong and Zhang, 2016; Taghipour and Ng, 2016; Song et al., 2017; <cite>Tay et al., 2018</cite>) , perhaps because <cite>these methods</cite> are able to capture subtle and complex information that is relevant to the task (Dong and Zhang, 2016) . In this paper, we propose to combine string kernels (low-level character n-gram features) and word embeddings (high-level semantic features) to obtain state-of-the-art AES results.",
  "y": "background"
 },
 {
  "id": "2eaa48dbc5e42a5934e905ec2288ac_1",
  "x": "The empirical results indicate that our approach yields a better performance than state-of-the-art approaches (Phandi et al., 2015; Dong and Zhang, 2016; <cite>Tay et al., 2018</cite>) . ---------------------------------- **METHOD**",
  "y": "differences"
 },
 {
  "id": "2eaa48dbc5e42a5934e905ec2288ac_2",
  "x": "The ASAP data set contains 8 prompts of different genres. The number of essays per prompt along with the score ranges are presented in Table 1 . Since the official test data of the ASAP competition is not released to the public, we, as well as others before us (Phandi et al., 2015; Dong and Zhang, 2016;  1 https://www.kaggle.com/c/asap-aes/data <cite>Tay et al., 2018</cite>) , use only the training data in our experiments.",
  "y": "similarities"
 },
 {
  "id": "2eaa48dbc5e42a5934e905ec2288ac_3",
  "x": "Baselines. We compare our approach with stateof-the-art methods based on handcrafted features (Phandi et al., 2015) , as well as deep features (Dong and Zhang, 2016; <cite>Tay et al., 2018</cite>) . We note that results for the cross-domain setting are reported only in some of these recent works (Phandi et al., 2015; Dong and Zhang, 2016) .",
  "y": "uses"
 },
 {
  "id": "2eaa48dbc5e42a5934e905ec2288ac_4",
  "x": "We used functions from the VLFeat li- Table 2 : In-domain automatic essay scoring results of our approach versus several state-of-the-art methods (Phandi et al., 2015; Dong and Zhang, 2016; <cite>Tay et al., 2018</cite>) . Results are reported in terms of the quadratic weighted kappa (QWK) measure, using 5-fold cross-validation. The best QWK score (among the machine learning systems) for each prompt is highlighted in bold.",
  "y": "differences"
 },
 {
  "id": "2eaa48dbc5e42a5934e905ec2288ac_5",
  "x": "In our empirical study, we also include feature ablation results. We report the QWK measure on each prompt as well as the overall average. We first note that the histogram intersection string kernel alone reaches better overall performance (0.780) than all previous works (Phandi et al., 2015; Dong and Zhang, 2016; <cite>Tay et al., 2018</cite>) .",
  "y": "differences"
 },
 {
  "id": "2eaa48dbc5e42a5934e905ec2288ac_6",
  "x": "We first note that the histogram intersection string kernel alone reaches better overall performance (0.780) than all previous works (Phandi et al., 2015; Dong and Zhang, 2016; <cite>Tay et al., 2018</cite>) . Remarkably, the overall performance of the HISK is also higher than the inter-human agreement (0.754). Although the BOSWE model can be regarded as a shallow approach, its overall results are comparable to those of deep learning approaches (Dong and Zhang, 2016; <cite>Tay et al., 2018</cite>) .",
  "y": "similarities"
 },
 {
  "id": "2eaa48dbc5e42a5934e905ec2288ac_7",
  "x": "The average QWK score of HISK and BOSWE (0.785) is more than 2% better the average scores of the best-performing state-of-the-art approaches <cite>Tay et al., 2018</cite>) . Cross-domain results. The results for the crossdomain automatic essay scoring task are presented in Table 3 .",
  "y": "differences"
 },
 {
  "id": "2eaa48dbc5e42a5934e905ec2288ac_8",
  "x": "**CONCLUSION** In this paper, we described an approach based on combining string kernels and word embeddings for automatic essay scoring. We compared our approach on the Automated Student Assessment Prize data set, in both in-domain and crossdomain settings, with several state-of-the-art approaches (Phandi et al., 2015; Dong and Zhang, 2016; <cite>Tay et al., 2018</cite>) .",
  "y": "uses"
 },
 {
  "id": "2eaa48dbc5e42a5934e905ec2288ac_9",
  "x": "In this paper, we described an approach based on combining string kernels and word embeddings for automatic essay scoring. We compared our approach on the Automated Student Assessment Prize data set, in both in-domain and crossdomain settings, with several state-of-the-art approaches (Phandi et al., 2015; Dong and Zhang, 2016; <cite>Tay et al., 2018</cite>) . Overall, the in-domain and the cross-domain comparative studies indicate that string kernels, both alone and in combination with word embeddings, attain the best performance on the automatic essay scoring task.",
  "y": "differences"
 },
 {
  "id": "2ef456a3f6b043350121c4c5cfd404_0",
  "x": "Hence, an adaptive IS may use a large number of samples to solve this problem whereas NCE is more stable and requires a fixed small number of noise samples (e.g., 100) to achieve a good performance [13, <cite>16]</cite> . Furthermore, the network learns to self-normalize during training using NCE. As a results, and on the contrary to IS, the softmax is no longer required during evaluation, which makes NCE an attractive choice to train large vocabulary NNLM.",
  "y": "background"
 },
 {
  "id": "2ef456a3f6b043350121c4c5cfd404_1",
  "x": "As a result, the training time significantly increases. To alleviate this problem, noise samples can be shared across the batch<cite> [16]</cite> . This paper proposes an extension of NCE to batch mode (B-NCE) training.",
  "y": "background"
 },
 {
  "id": "2ef456a3f6b043350121c4c5cfd404_2",
  "x": "Furthermore, we can show that this solution optimally approximates the sampling from a unigram distribution, which has been shown to be a good noise distribution choice [13, <cite>16]</cite> . The main idea here is to restrict the vocabulary, at each forward-backward pass, to the target words in the batch (words to predict) and then replace the softmax function by NCE. In particular, these words play alternatively the role of targets and noise samples.",
  "y": "background"
 },
 {
  "id": "2ef456a3f6b043350121c4c5cfd404_3",
  "x": "The proposed B-NCE approach as defined above uses a fixed number of noise samples (B \u2212 1), which is dependent on the batch size. In cases where the latter is small (e.g., B \u2264 100), B-NCE can be extended to use an additional K noise samples. This can be done by simply drawing an additional K samples form the noise distribution pn, and share them across the batch as it was done in<cite> [16]</cite> .",
  "y": "background"
 },
 {
  "id": "2ef456a3f6b043350121c4c5cfd404_4",
  "x": "We also report results after adding a bottleneck fully-connected ReLu layer right before the output layer in the recurrent models. These models are marked with the prefix ReLu in the tables below. Each of the models is trained using the proposed B-NCE approach and the shared noise NCE (S-NCE)<cite> [16]</cite> .",
  "y": "uses"
 },
 {
  "id": "2ef456a3f6b043350121c4c5cfd404_6",
  "x": "We also use a norm-based gradient clipping with a threshold of 5 but we do not use dropout. Moreover, B-NCE and S-NCE use the unigram as noise distribution pn. Following the setup proposed in [13, <cite>16]</cite> , S-NCE uses K = 100 noise samples, whereas B-NCE uses only the target words in the batch (K=0).",
  "y": "uses"
 },
 {
  "id": "2ef456a3f6b043350121c4c5cfd404_7",
  "x": "Moreover, the performance of the small ReLu-LSTM is comparable to the LSTM models proposed in<cite> [16]</cite> and [18] which use large hidden layers. In particular, the first paper trains a large 4-layers LSTM model using S-NCE on 4 GPUs (PPL = 43.2 and NoP = 3.4B), whereas the second uses a recurrent bottleneck layer [23] and a total of K = 8192 noise samples with importance sampling on 32 Tesla K40 GPUs. ----------------------------------",
  "y": "background"
 },
 {
  "id": "2f7b64db6939786a5026fc033c85bd_0",
  "x": "Until recently, GRE algorithms have focussed on the generation of distinguishing descriptions that are either as short as possible (e.g. (Dale, 1992; Gardent, 2002) ) or almost as short as possible (e.g. <cite>(Dale and Reiter, 1995)</cite> ). Since reductions in ambiguity are achieved by increases in length, there is a tension between these factors, and algorithms usually resolve this in some fixed way. However, the need for a distinguishing description is usually assumed, and typically built in to GRE algorithms.",
  "y": "background"
 },
 {
  "id": "2f7b64db6939786a5026fc033c85bd_1",
  "x": "Even with this decomposition of cost, some existing algorithms can still be seen as cost-minimisation. For example, the cost functions: allow the Full Brevity algorithm (Dale, 1992) to be viewed as minimising cost(S), and the incremental algorithm <cite>(Dale and Reiter, 1995)</cite> as hill-climbing (strictly, hill-descending), guided by the property-ordering which that algorithm requires.",
  "y": "background"
 },
 {
  "id": "2f7b64db6939786a5026fc033c85bd_2",
  "x": "**UNDERSPECIFICATION IN DIALOGUE** Standard GRE algorithms assume that the speaker knows what the hearer knows <cite>(Dale and Reiter, 1995)</cite> . In practice, speakers can often only guess.",
  "y": "background"
 },
 {
  "id": "2fbf5397a8219923d1d9bc0464cb59_0",
  "x": "Related work on exploring syntactic structured information in pronoun resolution can be typically classified into three categories: parse tree-based search algorithms ( Hobbs 1978) , feature-based (Lappin and Leass 1994; Bergsma and Lin 2006) and tree kernel-based methods<cite> (Yang et al 2006)</cite> . As a representative for parse tree-based search algorithms, Hobbs (1978) found the antecedent for a given pronoun by searching the parse trees of current text. It processes one sentence at a time from current sentence to the first sentence in text until an antecedent is found.",
  "y": "background"
 },
 {
  "id": "2fbf5397a8219923d1d9bc0464cb59_1",
  "x": "Another problem is that they may fail to effectively capture complex structured parse tree information. As for tree kernel-based methods, <cite>Yang et al (2006)</cite> captured syntactic structured information for pronoun resolution by using the convolution tree kernel (Collins and Duffy 2001) to measure the common sub-trees enumerated from the parse trees and achieved quite success on the ACE 2003 corpus. They also explored different tree span schemes and found that the simple-expansion scheme performed best.",
  "y": "background"
 },
 {
  "id": "2fbf5397a8219923d1d9bc0464cb59_2",
  "x": "Compared with Collins and Duffy's kernel and its application in pronoun resolution<cite> (Yang et al 2006)</cite> , the context-sensitive convolution tree kernel enumerates not only context-free sub-trees but also context-sensitive sub-trees by taking their ancestor node paths into consideration. Moreover, this paper also implements a dynamic-expansion tree span scheme by taking predicate-and antecedent competitor-related information into consideration. ----------------------------------",
  "y": "background"
 },
 {
  "id": "2fbf5397a8219923d1d9bc0464cb59_3",
  "x": "To deal with the cases that an anaphor and an antecedent candidate do not occur in the same sentence, we construct a pseudo parse tree for an entire text by attaching the parse trees of all its sentences to an upper \"S \" node, similar to <cite>Yang et al (2006)</cite> . Given the parse tree of a text, the problem is how to choose a proper tree span to well cover syntactic structured information in the tree kernel computation. Generally, the more a tree span includes, the more syntactic structured information would be provided, at the expense of more noisy information.",
  "y": "similarities"
 },
 {
  "id": "2fbf5397a8219923d1d9bc0464cb59_4",
  "x": "Figure 2 shows the three tree span schemes explored in <cite>Yang et al (2006)</cite> : MinExpansion (only including the shortest path connecting the anaphor and the antecedent candidate), Simple-Expansion (containing not only all the nodes in Min-Expansion but also the first level children of these nodes) and Full-Expansion (covering the sub-tree between the anaphor and the candidate), such as the sub-trees inside the dash circles of Figures 2(a) , 2(b) and 2(c) respectively. It is found<cite> (Yang et al 2006)</cite> that the simpleexpansion tree span scheme performed best on the ACE 2003 corpus in pronoun resolution. This suggests that inclusion of more structured information in the tree span may not help in pronoun resolution.",
  "y": "background"
 },
 {
  "id": "2fbf5397a8219923d1d9bc0464cb59_5",
  "x": "It is found<cite> (Yang et al 2006)</cite> that the simpleexpansion tree span scheme performed best on the ACE 2003 corpus in pronoun resolution. This suggests that inclusion of more structured information in the tree span may not help in pronoun resolution. To better capture structured information in the parse tree, this paper presents a dynamic-expansion scheme by trying to include necessary structured information in a parse tree.",
  "y": "background"
 },
 {
  "id": "2fbf5397a8219923d1d9bc0464cb59_6",
  "x": "As a specialized convolution kernel, the convolution tree kernel, proposed in Collins and Duffy (2001) , counts the number of common subtrees (sub-structures) as the syntactic structure similarity between two parse trees. This convolution tree kernel has been successfully applied by <cite>Yang et al (2006)</cite> in pronoun resolution. However, there is one problem with this tree kernel: the subtrees involved in the tree kernel computation are context-free (That is, they do not consider the information outside the sub-trees.).",
  "y": "background"
 },
 {
  "id": "2fbf5397a8219923d1d9bc0464cb59_7",
  "x": "In this paper, the m parameter in our contextsensitive convolution tree kernel as shown in Equation (1) indicates the maximal length of root node paths and is optimized to 3 using 5-fold cross validation on the training data. Table 1 systematically evaluates the impact of different m in our context-sensitive convolution tree kernel and compares our dynamic-expansion tree span scheme with the existing three tree span schemes, min-, simple-and full-expansions as described in <cite>Yang et al (2006)</cite> . It also shows that that our tree kernel achieves best performance with m = 3 on the test data, which outperforms the one with m = 1 by ~2.2 in F-measure.",
  "y": "similarities uses"
 },
 {
  "id": "2fdfa1b36fcf0d77826c96101ac428_0",
  "x": "In particular, we start with the gentle introduction to the fundamentals of reinforcement learning (Sutton and Barto, 1998; Sutton et al., 2000) . We further discuss We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; <cite>Xiong et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "2fdfa1b36fcf0d77826c96101ac428_1",
  "x": "We further discuss We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; <cite>Xiong et al., 2017)</cite> . We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (Wang et al., 2018b) , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems.",
  "y": "differences"
 },
 {
  "id": "2fdfa1b36fcf0d77826c96101ac428_2",
  "x": "In particular, we start with the gentle introduction to the fundamentals of reinforcement learning (Sutton and Barto, 1998; Sutton et al., 2000) . We further discuss their modern deep learning extensions such as Deep QNetworks (Mnih et al., 2015) , Policy Networks (Silver et al., 2016) , and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016) . We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016) , semi-supervised text classification (Wu et al., 2018) , coreference (Clark and Manning, 2016; Yin et al., 2018) , knowledge graph reasoning<cite> (Xiong et al., 2017</cite> ), text games (Narasimhan et al., 2015; He et al., 2016a) , social media (He et al., 2016b; Zhou and Wang, 2018) , information extraction (Narasimhan et al., 2016; Qin et al., 2018) , language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018) , etc.",
  "y": "background"
 },
 {
  "id": "2fdfa1b36fcf0d77826c96101ac428_3",
  "x": "We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; <cite>Xiong et al., 2017)</cite> . We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (Wang et al., 2018b) , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems.",
  "y": "background"
 },
 {
  "id": "2fdfa1b36fcf0d77826c96101ac428_4",
  "x": "We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; <cite>Xiong et al., 2017)</cite> . We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (Wang et al., 2018b) , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems.",
  "y": "differences"
 },
 {
  "id": "304773c64de1f0906f0246f2aa0d29_0",
  "x": "To extract opinion targets, pervious approaches usually relied on opinion words which are the words used to express the opinions (Hu and Liu, 2004a; Popescu and Etzioni, 2005; Liu et al., 2005; Wang and Wang, 2008; Qiu et al., 2011;<cite> Liu et al., 2012)</cite> . Intuitively, opinion words often appear around and modify opinion targets, and there are opinion relations and associations between them. If we have known some words to be opinion words, the words which those opinion words modify will have high probability to be opinion targets.",
  "y": "background"
 },
 {
  "id": "304773c64de1f0906f0246f2aa0d29_1",
  "x": "As a result, the syntax-based methods which heavily depended on the parsing performance would suffer from parsing errors . To improve the extraction performance, we can only employ some exquisite highprecision patterns. But this strategy is likely to miss many opinion targets and has lower recall with the increase of corpus size. To resolve these problems,<cite> Liu et al. (2012)</cite> formulated identifying opinion relations between words as an monolingual alignment process.",
  "y": "background"
 },
 {
  "id": "304773c64de1f0906f0246f2aa0d29_2",
  "x": "Although <cite>(Liu et al., 2012)</cite> had proved the effectiveness of WAM, they mainly performed experiments on the dataset with medium size. We are still curious about that when the size of dataset is larger or smaller, can we obtain the same conclusion? To our best knowledge, these problems have not been studied before.",
  "y": "motivation"
 },
 {
  "id": "304773c64de1f0906f0246f2aa0d29_3",
  "x": "In addition they used the HITS (Kleinberg, 1999) algorithm to compute opinion target confidences to improve the precision. <cite>(Liu et al., 2012)</cite> formulated identifying opinion relations between words as an alignment process. They used a completely unsupervised WAM to capture opinion relations in sentences.",
  "y": "background"
 },
 {
  "id": "304773c64de1f0906f0246f2aa0d29_4",
  "x": "(Liu et al., 2013 ) extend Liu's method, which is similar to our method and also used a partially supervised alignment model to extract opinion targets from reviews. We notice these two methods ( <cite>(Liu et al., 2012)</cite> and (Liu et al., 2013) ) only performed experiments on the corpora with a medium size. Although both of them proved that WAM model is better than the methods based on syntactic patterns, they didn't discuss the performance variation when dealing with the corpora with different sizes, especially when the size of the corpus is less than 1,000 and more than 10,000.",
  "y": "motivation"
 },
 {
  "id": "304773c64de1f0906f0246f2aa0d29_5",
  "x": "To extract opinion targets from reviews, we adopt the framework proposed by <cite>(Liu et al., 2012)</cite> , which is a graph-based extraction framework and has two main components as follows. 1) The first component is to capture opinion relations in sentences and estimate associations between opinion target candidates and potential opinion words. In this paper, we assume opinion targets to be nouns or noun phrases, and opinion words may be adjectives or verbs, which are usually adopted by (Hu and Liu, 2004a; Qiu et al., 2011; Wang and Wang, 2008;<cite> Liu et al., 2012)</cite> .",
  "y": "uses"
 },
 {
  "id": "304773c64de1f0906f0246f2aa0d29_6",
  "x": "To extract opinion targets from reviews, we adopt the framework proposed by <cite>(Liu et al., 2012)</cite> , which is a graph-based extraction framework and has two main components as follows. 1) The first component is to capture opinion relations in sentences and estimate associations between opinion target candidates and potential opinion words. In this paper, we assume opinion targets to be nouns or noun phrases, and opinion words may be adjectives or verbs, which are usually adopted by (Hu and Liu, 2004a; Qiu et al., 2011; Wang and Wang, 2008;<cite> Liu et al., 2012)</cite> .",
  "y": "similarities"
 },
 {
  "id": "304773c64de1f0906f0246f2aa0d29_7",
  "x": "In this subsection, we present our method for capturing opinion relations using unsupervised word alignment model. Similar to <cite>(Liu et al., 2012)</cite> , every sentence in reviews is replicated to generate a parallel sentence pair, and the word alignment algorithm is applied to the monolingual scenario to align a noun/noun phase with its modifiers. We select IBM-3 model (Brown et al., 1993) as the alignment model.",
  "y": "uses"
 },
 {
  "id": "304773c64de1f0906f0246f2aa0d29_8",
  "x": "At the same time, we can obtain conditional probability P (w o |w t ). Then, similar to <cite>(Liu et al., 2012)</cite> , the association between an opinion target candidate and its modifier is estimated as follows. Association(w t , w o ) = (\u03b1 \u00d7 P (w t |w o ) + (1 \u2212 \u03b1) \u00d7 P (w o |w t )) \u22121 , where \u03b1 is the harmonic factor.",
  "y": "uses"
 },
 {
  "id": "304773c64de1f0906f0246f2aa0d29_9",
  "x": "In the second component, we adopt a graph-based algorithm used in <cite>(Liu et al., 2012)</cite> to compute the confidence of each opinion target candidate, and the candidates with higher confidence than the threshold will be extracted as the opinion targets. Here, opinion words are regarded as the important indicators. We assume that two target candidates are likely to belong to the similar category, if they are modified by similar opinion words.",
  "y": "uses"
 },
 {
  "id": "304773c64de1f0906f0246f2aa0d29_10",
  "x": "M is the matrix of word associations, where M i,j denotes the association between the opinion target candidate i and the potential opinion word j. And I is defined as the prior confidence of each candidate for opinion target. Similar to <cite>(Liu et al., 2012)</cite> , we set each item in , where tf (v) is the term frequency of v in the corpus, and df (v) is computed by using the Google n-gram corpus 2 .",
  "y": "uses"
 },
 {
  "id": "304773c64de1f0906f0246f2aa0d29_11",
  "x": "**DATASETS AND EVALUATION METRICS** In this section, to answer the questions mentioned in the first section, we collect a large collection named as LARGE, which includes reviews from three different domains and different languages. This collection was also used in <cite>(Liu et al., 2012)</cite> .",
  "y": "similarities"
 },
 {
  "id": "304773c64de1f0906f0246f2aa0d29_12",
  "x": "From the results, we still see that PSWAM outperforms WAM in all datasets on precision when size of corpus is smaller than 5 \u00d7 10 4 . To further prove the effectiveness of our combination, we compare PSWAM with some state-of-the-art methods, including Hu (Hu and Liu, 2004a) , which extracted frequent opinion target words based on association mining rules, DP (Qiu et al., 2011) , which extracted opinion targets through syntactic patterns, and LIU <cite>(Liu et al., 2012)</cite> , which fulfilled this task by using unsupervised WAM. The parameter settings in these baselines are the same as the settings in the original papers.",
  "y": "uses"
 },
 {
  "id": "304773c64de1f0906f0246f2aa0d29_13",
  "x": "From the results, we still see that PSWAM outperforms WAM in all datasets on precision when size of corpus is smaller than 5 \u00d7 10 4 . To further prove the effectiveness of our combination, we compare PSWAM with some state-of-the-art methods, including Hu (Hu and Liu, 2004a) , which extracted frequent opinion target words based on association mining rules, DP (Qiu et al., 2011) , which extracted opinion targets through syntactic patterns, and LIU <cite>(Liu et al., 2012)</cite> , which fulfilled this task by using unsupervised WAM. The parameter settings in these baselines are the same as the settings in the original papers.",
  "y": "uses"
 },
 {
  "id": "30718e751f18432c2478442530267e_0",
  "x": "Question answering systems deteriorate dramatically in the presence of adversarial sentences in articles. According to<cite> Jia and Liang (2017)</cite> , the single BiDAF system (Seo et al., 2016) only achieves an F1 score of 4.8 on the ADDANY adversarial dataset. In this paper, we present a method to tackle this problem via answer sentence selection.",
  "y": "background"
 },
 {
  "id": "30718e751f18432c2478442530267e_1",
  "x": "Question answering systems deteriorate dramatically in the presence of adversarial sentences in articles. According to<cite> Jia and Liang (2017)</cite> , the single BiDAF system (Seo et al., 2016) only achieves an F1 score of 4.8 on the ADDANY adversarial dataset. In this paper, we present a method to tackle this problem via answer sentence selection.",
  "y": "motivation"
 },
 {
  "id": "30718e751f18432c2478442530267e_2",
  "x": "However,<cite> Jia and Liang (2017)</cite> show that these systems are very vulnerable to paragraphs with adversarial sentences. For instance, the single BiDAF system (Seo et al., 2016) , which achieves an F1 of 75.5 on Standford Question Answering Dataset (SQuAD), deteriorates significantly to an F1 of 4.8 on the ADDANY adversarial dataset. Besides the single BiDAF, the single Match LSTM, the ensemble Match LSTM, and the ensemble BiDAF achieve an F1 of 7.6, 11.7, and 2.7 respectively in question answering on ADDANY adversarial dataset<cite> (Jia and Liang, 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "30718e751f18432c2478442530267e_3",
  "x": "For instance, the single BiDAF system (Seo et al., 2016) , which achieves an F1 of 75.5 on Standford Question Answering Dataset (SQuAD), deteriorates significantly to an F1 of 4.8 on the ADDANY adversarial dataset. Besides the single BiDAF, the single Match LSTM, the ensemble Match LSTM, and the ensemble BiDAF achieve an F1 of 7.6, 11.7, and 2.7 respectively in question answering on ADDANY adversarial dataset<cite> (Jia and Liang, 2017)</cite> . Therefore, question answering with adversarial sentences in paragraphs is a prominent issue and is the focus of this study.",
  "y": "background"
 },
 {
  "id": "30718e751f18432c2478442530267e_4",
  "x": "For instance, the single BiDAF system (Seo et al., 2016) , which achieves an F1 of 75.5 on Standford Question Answering Dataset (SQuAD), deteriorates significantly to an F1 of 4.8 on the ADDANY adversarial dataset. Besides the single BiDAF, the single Match LSTM, the ensemble Match LSTM, and the ensemble BiDAF achieve an F1 of 7.6, 11.7, and 2.7 respectively in question answering on ADDANY adversarial dataset<cite> (Jia and Liang, 2017)</cite> . Therefore, question answering with adversarial sentences in paragraphs is a prominent issue and is the focus of this study.",
  "y": "motivation background"
 },
 {
  "id": "30718e751f18432c2478442530267e_5",
  "x": "Dataset for Testing. Our test set is<cite> Jia and Liang (2017)</cite>'s ADDANY adversarial dataset. It includes 1,000 paragraphs and each paragraph refers to only one query, i.e., 1,000 (C, Q) pairs.",
  "y": "uses"
 },
 {
  "id": "30718e751f18432c2478442530267e_6",
  "x": "---------------------------------- **RESULTS** The performance of question answering is evaluated by the Macro-averaged F1 score (Rajpurkar <cite>Jia and Liang, 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "30718e751f18432c2478442530267e_7",
  "x": "To efficiently tackle question answering for long documents, Choi et al. (2017) propose a method based on answer sentence selection to first narrow down a document and then use RNN to generate an answer. However, following the idea of adversarial examples in image recognition (Goodfellow et al., 2014; Kurakin et al., 2016; Papernot et al., 2016) ,<cite> Jia and Liang (2017)</cite> point out the unreliability of existing question answering models in the presence of adversarial sentences. In this study, we propose a method to tackle this problem through answer sentence selection.",
  "y": "background"
 },
 {
  "id": "30718e751f18432c2478442530267e_8",
  "x": "However, following the idea of adversarial examples in image recognition (Goodfellow et al., 2014; Kurakin et al., 2016; Papernot et al., 2016) ,<cite> Jia and Liang (2017)</cite> point out the unreliability of existing question answering models in the presence of adversarial sentences. In this study, we propose a method to tackle this problem through answer sentence selection. The main component of our system is Tree-LSTM which is a powerful variant of Tree-RNN.",
  "y": "motivation background"
 },
 {
  "id": "30718e751f18432c2478442530267e_9",
  "x": "Experiments show the F1 score has been largely improved from 4.8 to 52.3. To the best of our knowledge, we are the first to apply TreeLSTMs in answer sentence selection and the first to tackle question answering with adversarial examples on ADDANY adversarial dataset. However,<cite> Jia and Liang (2017)</cite> also present the deterioration of QA systems on another dataset, ADDSENT adversarial dataset.",
  "y": "similarities"
 },
 {
  "id": "311b238406da4891c09cb9c3c0334d_0",
  "x": "However, in stance detection, systems are to determine author's favorability towards a given target and the target even may not be explicitly mentioned in the text. Moreover, the text may express positive opinion about an entity contained in the text, but one can also infer that the author is against the defined target (an entity or a topic). This makes the task more difficult, compared to the sentiment analysis, but it can often bring complementary information <cite>[3]</cite> .",
  "y": "background"
 },
 {
  "id": "311b238406da4891c09cb9c3c0334d_1",
  "x": "**THE APPROACH OVERVIEW** We preprocessed the Czech commentaries by the same rules as in the original system <cite>[3]</cite> (for example: all urls were replaced by keyword URL, links to images are replaced by IMGURL, only letters are preserved, the rest of the characters is removed, \u2026). Moreover, we stemmed the texts by HPS -High Precision Stemmer [2] .",
  "y": "uses"
 },
 {
  "id": "311b238406da4891c09cb9c3c0334d_2",
  "x": "The original system <cite>[3]</cite> used more features, which could not be easily applied on Czech commentaries. We do not work with tweets, so we could not use a set of features generated from hashtags. We have not analyzed the influence of part-of-speech (POS) tags yet.",
  "y": "differences"
 },
 {
  "id": "311b238406da4891c09cb9c3c0334d_3",
  "x": "We do not work with tweets, so we could not use a set of features generated from hashtags. We have not analyzed the influence of part-of-speech (POS) tags yet. We did not identify strong candidates to build a domain specific dictionary as in <cite>[3]</cite> .",
  "y": "differences"
 },
 {
  "id": "3188ee1583a9c711cf147fc596768d_0",
  "x": "**ABSTRACT** This paper evaluates two semi-supervised techniques for the adaptation of a parse selection model to Wikipedia domains. The techniques examined are Structural Correspondence Learning (SCL)<cite> (Blitzer et al., 2006)</cite> and Self-training (Abney, 2007; McClosky et al., 2006) .",
  "y": "background"
 },
 {
  "id": "3188ee1583a9c711cf147fc596768d_1",
  "x": "It is a more realistic situation, but at the same time also considerably more difficult. In this paper we evaluate two semi-supervised approaches to domain adaptation of a discriminative parse selection model. We examine Structural Correspondence Learning (SCL)<cite> (Blitzer et al., 2006)</cite> for this task, and compare it to several variants of Self-training (Abney, 2007; McClosky et al., 2006) .",
  "y": "similarities"
 },
 {
  "id": "3188ee1583a9c711cf147fc596768d_2",
  "x": "**PREVIOUS WORK** So far, Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis<cite> (Blitzer et al., 2006</cite>; ). An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007) .",
  "y": "background"
 },
 {
  "id": "3188ee1583a9c711cf147fc596768d_3",
  "x": "**STRUCTURAL CORRESPONDENCE LEARNING** Structural Correspondence Learning<cite> (Blitzer et al., 2006)</cite> exploits unlabeled data from both source and target domain to find correspondences among features from different domains. These correspondences are then integrated as new features in the labeled data of the source domain.",
  "y": "background"
 },
 {
  "id": "3188ee1583a9c711cf147fc596768d_4",
  "x": "The outline of SCL is given in Algorithm 1. The key to SCL is to exploit pivot features to automatically identify feature correspondences. Pivots are features occurring frequently and behaving similarly in both domains<cite> (Blitzer et al., 2006)</cite> .",
  "y": "background"
 },
 {
  "id": "3188ee1583a9c711cf147fc596768d_5",
  "x": "Non-pivots that correlate with many of the same pivots are assumed to correspond. These pivot predictor weight vectors thus implicitly align non-pivot features from source and target domain. Intuitively, if we are able to find good correspondences through 'linking' pivots, then the augmented source data should transfer better to a target domain<cite> (Blitzer et al., 2006)</cite> .",
  "y": "similarities"
 },
 {
  "id": "3188ee1583a9c711cf147fc596768d_6",
  "x": "---------------------------------- **SCL FOR DISCRIMINATIVE PARSE SELECTION** So far, pivot features on the word level were used<cite> (Blitzer et al., 2006</cite>; .",
  "y": "background"
 },
 {
  "id": "3188ee1583a9c711cf147fc596768d_7",
  "x": "In our empirical setup, we follow<cite> Blitzer et al. (2006)</cite> and balance the size of source and target data. Thus, depending on the size of the resulting target domain dataset, and the \"broadness\" of the categories involved in creating it, we might wish to filter out certain pages. We implemented a filter mechanism that excludes pages of a certain category (e.g. a supercategory that is hypothesized to be \"too broad\").",
  "y": "similarities uses"
 },
 {
  "id": "3188ee1583a9c711cf147fc596768d_8",
  "x": "The paper compares Structural Correspondence Learning<cite> (Blitzer et al., 2006)</cite> with (various instances of) self-training (Abney, 2007; McClosky et al., 2006) for the adaptation of a parse selection model to Wikipedia domains. The empirical findings show that none of the evaluated self-training variants (delible/indelible, single versus multiple iterations, various selection techniques) achieves a significant improvement over the baseline. The more 'indirect' exploitation of unlabeled data through SCL is more fruitful than pure self-training.",
  "y": "similarities"
 },
 {
  "id": "31b06dfc081149e1e436f0bb5e0904_0",
  "x": "As a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate Koo and Collins, 2010; Bohnet and Nivre, 2012; Martins et al., 2009<cite> Martins et al., , 2013</cite> . The same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree. Indeed, the best performing systems in last year shared task on broad-coverage semantic dependency parsing follow this principle (Oepen et al., 2014) .",
  "y": "background motivation"
 },
 {
  "id": "31b06dfc081149e1e436f0bb5e0904_1",
  "x": "Our second-order model looks at some pairs of arcs: arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two. Martins and Almeida (2014) for further details. The parser was built as an extension of a recent dependency parser, TurboParser (Martins et al., 2010<cite> (Martins et al., , 2013</cite> , with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD).",
  "y": "uses"
 },
 {
  "id": "31b06dfc081149e1e436f0bb5e0904_2",
  "x": "The overall set of parts used by our parser is illustrated in Figure 1 ; note that by using only a subset of the parts (predicate, arc, labeled arc, and sibling parts), the semantic parser decodes each predicate frame independently from other predicates; it is the co-parent and grandparent parts that have the effect of creating inter-dependence among predicates; we will analyze the effect of these dependencies in the experimental section ( \u00a73). For each part in our model (shown in Figure 1 ), we computed binary features based on various combination of lexical forms, lemmas, POS tags and syntactic dependency relations of words related to the corresponding predicates and arguments. Most of these features were taken from TurboParser <cite>(Martins et al., 2013)</cite> , and others were inspired by the semantic parser of Johansson and Nugues (2008) .",
  "y": "uses"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_0",
  "x": "As can be seen for verhindern/prevent in (5) and (6), the same shifter can even affect both positive and negative expressions. We present a reproduction and extension to the work of <cite>Schulder et al. (2017)</cite> , <cite>which</cite> introduced a lexicon of verbal polarity shifters, as well as methods to increase the size of this lexicon through bootstrapping. The lexicon lists verb lemmas and assigns a binary label (shifter or no shifter) to each.",
  "y": "extends uses"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_1",
  "x": "The <cite>original approach</cite> was developed on English. We apply <cite>it</cite> to German, validating the generality of <cite>the approach</cite> and creating a new resource, a German lexicon of 677 verbal polarity shifters. We also improve the bootstrapping process by adding features that leverage polarity shifter resources across languages.",
  "y": "extends"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_2",
  "x": "However, while there has been significant research on negation in sentiment analysis (Wiegand et al., 2010) , current classifiers fail to handle polarity shifters adequately (<cite>Schulder et al., 2017</cite>) . This is in part due to the lack of lexical resources for polarity shifters. Unlike negation words (no, not, never, etc.) , of which there are only a few dozen in a language, polarity shifters are far more numerous.",
  "y": "background"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_3",
  "x": "Unlike negation words (no, not, never, etc.) , of which there are only a few dozen in a language, polarity shifters are far more numerous. Among verbs alone there are many hundreds (<cite>Schulder et al., 2017</cite>) . Comprehensive shifter lexicons are, therefore, considerably more expensive to create.",
  "y": "background"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_4",
  "x": "Once available, they can be used to improve the aforementioned tasks, as has already been shown for the case of English polarity classification (<cite>Schulder et al., 2017</cite>) . To reduce the cost of creating such polarity shifter lexicons, <cite>Schulder et al. (2017)</cite> introduced methods to automatically generate a labeled list of words using either a limited amount of labeled training data or no labeled data at all. <cite>Their approach</cite> includes both features that rely on semantic resources and datadriven ones.",
  "y": "background"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_5",
  "x": "Comprehensive shifter lexicons are, therefore, considerably more expensive to create. Once available, they can be used to improve the aforementioned tasks, as has already been shown for the case of English polarity classification (<cite>Schulder et al., 2017</cite>) . To reduce the cost of creating such polarity shifter lexicons, <cite>Schulder et al. (2017)</cite> introduced methods to automatically generate a labeled list of words using either a limited amount of labeled training data or no labeled data at all.",
  "y": "background"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_6",
  "x": "To reduce the cost of creating such polarity shifter lexicons, <cite>Schulder et al. (2017)</cite> introduced methods to automatically generate a labeled list of words using either a limited amount of labeled training data or no labeled data at all. <cite>Their approach</cite> includes both features that rely on semantic resources and datadriven ones. <cite>They</cite> limited <cite>their work</cite> to English verbs, but expressed the expectation that <cite>their methods</cite> should also work for other languages.",
  "y": "background"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_7",
  "x": "<cite>Their approach</cite> includes both features that rely on semantic resources and datadriven ones. <cite>They</cite> limited <cite>their work</cite> to English verbs, but expressed the expectation that <cite>their methods</cite> should also work for other languages. To verify that expectation, we apply <cite>their approach</cite> to German, for which all resources required to reproduce <cite>their experiments</cite> are available.",
  "y": "motivation differences future_work"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_8",
  "x": "<cite>They</cite> limited <cite>their work</cite> to English verbs, but expressed the expectation that <cite>their methods</cite> should also work for other languages. To verify that expectation, we apply <cite>their approach</cite> to German, for which all resources required to reproduce <cite>their experiments</cite> are available. Keeping in mind that this is not the case for many other languages, we focus our evaluation on differentiating between features that rely on unstructured data and those requiring rare semantic resources.",
  "y": "extends"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_9",
  "x": "While polarity shifters are not restricted to a particular part of speech -shifter nouns (e.g. downfall), adjectives (devoid) and adverbs (barely) also exist -we limit ourselves to verbs. Verbs and nouns are the most important minimal semantic units (Schneider et al., 2016) and verbs are usually the main syntactic predicates of clauses, projecting far-reaching scopes. Focusing on verbs also allows us a closer comparison with <cite>Schulder et al. (2017)</cite> and to investigate cross-lingual similarities between verbal shifters.",
  "y": "similarities"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_10",
  "x": "(i) we introduce a German lexicon of verbal polarity shifters; (ii) we reproduce and adapt the approach of <cite>Schulder et al. (2017)</cite> to German to extend our lexicon; (iii) we introduce additional methods that take advantage of the existence of the English verbal polarity shifter lexicon and improve upon the current state of the art. The focus of our work is the binary classification of verbal polarity shifters in German. The resulting German lexicon of 677 verbal polarity shifters is made publicly available.",
  "y": "extends uses"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_11",
  "x": "One reason for this is the lack of lexicons and corpora that cover other forms of polarity shifters. Even the most complex negation lexicon for English sentiment analysis (Wilson et al., 2005) includes a mere 12 verbal shifters. So far the only larger resources for polarity shifters are the English-language verbal shifter lexicons recently introduced by <cite>Schulder et al. (2017)</cite> and Schulder et al. (2018) .",
  "y": "background"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_12",
  "x": "<cite>Schulder et al. (2017)</cite> automatically bootstrap a lexicon which covers 980 verbal shifters at the lemma level, while Schulder et al. (2018) manually annotate word senses of verbs, creating a lexicon of 2131 shifter senses across 1220 verbs. As we reproduce and extend the work of <cite>Schulder et al. (2017)</cite> , all further use of and comparison to an English shifter lexicon refers to their bootstrapped lexicon as well. To create shifter lexicons at a large scale, automation and bootstrapping techniques are required.",
  "y": "background"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_13",
  "x": "So far the only larger resources for polarity shifters are the English-language verbal shifter lexicons recently introduced by <cite>Schulder et al. (2017)</cite> and Schulder et al. (2018) . <cite>Schulder et al. (2017)</cite> automatically bootstrap a lexicon which covers 980 verbal shifters at the lemma level, while Schulder et al. (2018) manually annotate word senses of verbs, creating a lexicon of 2131 shifter senses across 1220 verbs. As we reproduce and extend the work of <cite>Schulder et al. (2017)</cite> , all further use of and comparison to an English shifter lexicon refers to their bootstrapped lexicon as well.",
  "y": "extends uses"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_14",
  "x": "Danescu-Niculescu-Mizil et al. (2009) propose using negative polarity items (NPIs) to extract downwardentailing operators, which are closely related to polarity shifters. <cite>Schulder et al. (2017)</cite> also make use of NPIs in addition to a number of other features. Rather than using lexicons, another approach would be to learn polarity shifters from labelled corpora.",
  "y": "background"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_15",
  "x": "Polarity classifiers trained on such corpora, such as the state-of-the-art Recursive Neural Tensor Network tagger (Socher et al., 2013) , fail to detect many instances of polarity shifting. <cite>Schulder et al. (2017)</cite> show that the explicit knowledge provided by a shifter lexicon can improve polarity classification in such cases. ----------------------------------",
  "y": "background"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_16",
  "x": "**DATA** We create a gold standard for German verbal shifters, following the approach <cite>Schulder et al. (2017)</cite> used for <cite>their</cite> English gold standard. An expert annotator, who is a native speaker of German, labeled 2000 verbs, randomly sampled from GermaNet (Hamp and Feldweg, 1997) , a German wordnet resource.",
  "y": "uses"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_17",
  "x": "In this section we introduce the features that we will use to bootstrap our German verbal shifter lexicon in \u00a75.3. We start by outlining the features proposed by <cite>Schulder et al. (2017)</cite> and how we adapt them for use with German ( \u00a74.1). We further separate them into data-driven features ( \u00a74.1.1) and resource-driven features ( \u00a74.1.2) to highlight their requirements when applied to a new language.",
  "y": "extends"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_18",
  "x": "In this section we briefly describe how we adapt the features of <cite>Schulder et al. (2017)</cite> to German language data. We distinguish between features that mainly rely on text data from a corpus ( \u00a74.1.1) and those that require complex semantic resources ( \u00a74.1.2). When working with languages with scarcer resources, it can be expected that the former will be more readily available than the latter.",
  "y": "extends"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_19",
  "x": "3 Distributional Similarity (SIM): The distributional similarity feature assumes that words that are semantically similar to negation words are also likely to be polarity shifters. Semantic similarity is modeled as cosine similarity in a word embedding space. The word embeddings are created using Word2Vec (Mikolov et al., 2013) on the German web corpus DeWaC (Baroni et al., 2009) , using the same hyperparameters as <cite>Schulder et al. (2017)</cite> and German translations of their negation seeds.",
  "y": "similarities"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_20",
  "x": "<cite>Schulder et al. (2017)</cite> hypothesize that this phenomenon correlates with shifting, which can be seen as producing a new (negative) end state. Therefore, <cite>they</cite> collect particle verbs containing relevant English particles, such as away, down and out. For our German data we chose the following particles associated with negative end states: ab, aus, entgegen, fort, herunter, hinunter, weg and wider.",
  "y": "background"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_21",
  "x": "<cite>Schulder et al. (2017)</cite> hypothesize that this phenomenon correlates with shifting, which can be seen as producing a new (negative) end state. Therefore, <cite>they</cite> collect particle verbs containing relevant English particles, such as away, down and out. For our German data we chose the following particles associated with negative end states: ab, aus, entgegen, fort, herunter, hinunter, weg and wider.",
  "y": "motivation"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_22",
  "x": "Heuristic using 'jeglich' (ANY): Negative polarity items (NPIs) are known to occur in the context of negation (Giannakidou, 2008) . <cite>Schulder et al. (2017)</cite> showed that the English NPI any co-occurs with shifters, so its presence in a verb phrase can indicate the presence of a verbal shifter. We expect the same for the German NPI jeglich, as seen in (8).",
  "y": "similarities"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_23",
  "x": "GermaNet: Wordnets are large lexical ontologies providing various kinds of semantic information and relations. <cite>Schulder et al. (2017)</cite> used glosses, hypernyms and supersenses taken from the English WordNet (Miller et al., 1990) as features in <cite>their work</cite>. We use GermaNet (Hamp and Feldweg, 1997) , a German wordnet resource that provides all these features.",
  "y": "similarities"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_24",
  "x": "Salsa FrameNet: Framenets provide semantic frames that group words with similar semantic behavior. <cite>Schulder et al. (2017)</cite> use the frame memberships of verbs as a feature, hypothesizing that verbal shifters will be found in the same frames. We reproduce this feature using frames from the German FrameNet project Salsa (Burchardt et al., 2006) .",
  "y": "similarities uses"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_25",
  "x": "The motivation behind the work of <cite>Schulder et al. (2017)</cite> was to introduce a large lexicon of verbal polarity shifters. Now that such a lexicon exists for English, it is an obvious resource to use when creating verbal shifter lexicons for other languages. We hypothesize that a verb with the same meaning as an English verbal shifter will also function as a shifter in its own language.",
  "y": "motivation"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_26",
  "x": "We hypothesize that a verb with the same meaning as an English verbal shifter will also function as a shifter in its own language. All that is required is a mapping from English verbs to, in our case, German verbs. We choose to use the bootstrapped lexicon of <cite>Schulder et al. (2017)</cite> , rather than the manually created one of Schulder et al. (2018) , to show that bootstrapping is sufficient for all stages of the learning process.",
  "y": "uses"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_27",
  "x": "<cite>Schulder et al. (2017)</cite> also observe in <cite>their</cite> error analysis that some verbs act as shifters in only some of their word senses. As different word senses often do not translate into the same foreign word, indiscriminate translation may introduce non-shifting senses of English shifter words as false positives. Evaluating the dictionary mapping as a feature will allow us to judge its usefulness for high-precision lexicon induction in future works.",
  "y": "future_work"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_28",
  "x": "**CLASSIFIER EVALUATION** We start our evaluation by reproducing the classifier evaluation of <cite>Schulder et al. (2017)</cite> . The task is the classification of all verbs from the given gold standard in a 10-fold cross validation.",
  "y": "uses"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_29",
  "x": "Analogous to <cite>Schulder et al. (2017)</cite> we evaluate a supervised SVM classifier as well as a graph-based label propagation (LP) classifier that requires no labeled training data. In addition, we evaluate our crosslingual word embedding classifier ( \u00a74.2.2) and our dictionary classifier ( \u00a74.2.1), which both make use of the pre-existing English lexicon, but require no additional labeled German data. For an overview of the classifiers and their data requirements, see Table 4 .",
  "y": "similarities"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_30",
  "x": "7 As in <cite>Schulder et al. (2017)</cite> , accuracy proves to be a problematic measure, as it has a strong majority label bias. The no shifter label makes up 88.8% of our gold annotation (Table 2) , which explains the strong performance of the majority baseline on this metric. Table 6 : Features included in SVM feature groups in Table 5 .",
  "y": "similarities"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_31",
  "x": "All features in data and resource were also used in <cite>Schulder et al. (2017)</cite> . for English (<cite>Schulder et al., 2017</cite>) . Cross-lingual embeddings and dictionaries as stand-alone classifiers both outperform the label propagation approach due to their better recall coverage of shifters.",
  "y": "similarities"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_33",
  "x": "Adding both cross-lingual features to the SVM classifier improves performance further. This shows that they are not only complementary to the existing features, but also to each other, as using only one cross-lingual feature does not improve performance as much. The most feature-rich SVM configuration, SVM data+resource+dict+embed , provides a significant improvement over SVM data+resource , the best classifier of <cite>Schulder et al. (2017)</cite> .",
  "y": "differences"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_34",
  "x": "SVM data+resource represents the previously best classifier (<cite>Schulder et al., 2017</cite>) . covered. For many other languages, finding a publicly available dictionary of comparable size may pose a challenge.",
  "y": "differences"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_35",
  "x": "Using only the 500 most frequent English words leaves a German coverage of 33%. Figure 2 shows the performance of the differently sized dictionaries as stand-alone classifiers, while Figure 3 shows how much they can improve the best classifier of <cite>Schulder et al. (2017)</cite> , i.e. SVM data+resource . In both cases we see that while even smaller dictionaries can still provide acceptable performance, using cross-lingual embeddings is preferable to using a dictionary of insufficient size.",
  "y": "differences"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_36",
  "x": "**BOOTSTRAPPING** In their intrinsic evaluation <cite>Schulder et al. (2017)</cite> showed that explicit knowledge of a large number of polarity shifters can improve sentiment analysis. To increase the size of our lexicon, we bootstrap additional shifters following <cite>their approach.</cite>",
  "y": "extends uses"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_37",
  "x": "Limiting our annotation effort to predicted shifters and discarding all others reduces the cost of annotation by 92%. Table 7 shows the classifier precision at different confidence intervals. Like <cite>Schulder et al. (2017)</cite> , we see very high performance for the first quartile, matching <cite>their</cite> observation that manual confirmation is not strictly necessary for high confidence labels.",
  "y": "similarities"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_38",
  "x": "In reproducing the work of <cite>Schulder et al. (2017)</cite> , we limited ourselves to verbs. In the future, we would like to investigate methods to extend the shifter lexicon to also cover nouns and adjectives. While we have shown that the <cite>same approach</cite> for classifying verbal shifters works for German and English, future work will expand the number of languages, especially to verify that these methods can also be applied to non-Indo-European languages, such as Chinese, Japanese or Arabic.",
  "y": "uses"
 },
 {
  "id": "31e8c524f05495fdd87bfac6fbecc8_39",
  "x": "In reproducing the work of <cite>Schulder et al. (2017)</cite> , we limited ourselves to verbs. In the future, we would like to investigate methods to extend the shifter lexicon to also cover nouns and adjectives. While we have shown that the <cite>same approach</cite> for classifying verbal shifters works for German and English, future work will expand the number of languages, especially to verify that these methods can also be applied to non-Indo-European languages, such as Chinese, Japanese or Arabic.",
  "y": "similarities future_work"
 },
 {
  "id": "32ca78f6e01b7732a4bf573b91fbfe_0",
  "x": "With the development of neural networks and deep learning, it is possible to learn the representations of concepts from unlabeled text corpus automatically. These representations can be treated as concept features for classification. An important advance in this area is the development of the word2vec technique [4] , which has proved to be an effective approach in Twitter sentiment classification<cite> [5]</cite> .",
  "y": "background"
 },
 {
  "id": "32ca78f6e01b7732a4bf573b91fbfe_1",
  "x": "The idea of word2vec (word embeddings) originated from the concept of distributed representation of words [6] . The common method to derive the vectors is using neural probabilistic language model [7] . Word embeddings proved to be effective representations in the tasks of sentiment analysis<cite> [5,</cite> 8, 9 ] and text classification [10] .",
  "y": "background"
 },
 {
  "id": "32ca78f6e01b7732a4bf573b91fbfe_2",
  "x": "Word embeddings proved to be effective representations in the tasks of sentiment analysis<cite> [5,</cite> 8, 9 ] and text classification [10] . In this work, I are aiming at evaluating word embeddings for sentiment analysis of citations.",
  "y": "uses background"
 },
 {
  "id": "32ca78f6e01b7732a4bf573b91fbfe_3",
  "x": "**POLARITY-SPECIFIC WORD REPRESENTATION TRAINING** To improve sentiment citation classification results, I trained polarity specific word embeddings (PS-Embeddings), which were inspired by the Sentiment-Specific Word Embedding<cite> [5]</cite> . After obtaining the PS-Embeddings, I used the same scheme to average the vectors in one sentence according to the sent2vec model.",
  "y": "uses"
 },
 {
  "id": "32ca78f6e01b7732a4bf573b91fbfe_6",
  "x": "The macro-F score 0.8<cite>5</cite> and the weighted-F score 0.86 proved that word2vec is effective on classifying positive and negative citations. However, unlike the outcomes in the paper of<cite> [5]</cite> , where they concluded that sentiment specific word embeddings performed best, integrating polarity information did not improve the result in this experiment. Table 4 .",
  "y": "differences"
 },
 {
  "id": "32e860cdf03df7f6cb58b7f9e85ac0_1",
  "x": "Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015;<cite> Kiela and Clark, 2015</cite>; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings. While Lopopolo and van Miltenburg (2015) show preliminary results on using sound to learn distributional representations,<cite> Kiela and Clark (2015)</cite> build on ideas from Bruni et al. (2014) to learn word embeddings that respect both linguistic and auditory relationships by optimizing a joint objective. Further, they propose various fusion strategies to combine knowledge from both the modalities.",
  "y": "background"
 },
 {
  "id": "32e860cdf03df7f6cb58b7f9e85ac0_2",
  "x": "Audio and Word Embeddings. Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015;<cite> Kiela and Clark, 2015</cite>; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings. While Lopopolo and van Miltenburg (2015) show preliminary results on using sound to learn distributional representations,<cite> Kiela and Clark (2015)</cite> build on ideas from Bruni et al. (2014) to learn word embeddings that respect both linguistic and auditory relationships by optimizing a joint objective.",
  "y": "background"
 },
 {
  "id": "32e860cdf03df7f6cb58b7f9e85ac0_3",
  "x": "**DATASETS** Freesound. We use the freesound database (Font et al., 2013) , also used in prior work<cite> (Kiela and Clark, 2015</cite>; Lopopolo and van Miltenburg, 2015) to learn the proposed sound-word2vec embeddings.",
  "y": "uses"
 },
 {
  "id": "32e860cdf03df7f6cb58b7f9e85ac0_4",
  "x": "For example, the description to produce a foley \"driving on gravel\" sound is to record the \"crunching sound of plastic or polyethene bags\". AMEN and ASLex. AMEN and ASLex<cite> (Kiela and Clark, 2015)</cite> are subsets of the standard MEN (Bruni et al., 2014) and SimLex (Hill et al., 2015) word similarity datasets consisting of word-pairs that \"can be associated with a distinctive associated sound\".",
  "y": "background"
 },
 {
  "id": "32e860cdf03df7f6cb58b7f9e85ac0_5",
  "x": "AMEN and ASLex<cite> (Kiela and Clark, 2015)</cite> are subsets of the standard MEN (Bruni et al., 2014) and SimLex (Hill et al., 2015) word similarity datasets consisting of word-pairs that \"can be associated with a distinctive associated sound\". We evaluate on this dataset for completeness to benchmark our approach against previous work. However, we are primarily interested in the slightly different problem of relating words with similar auditory instantions that may or may not be semantically related as opposed to relating semantically similar words that can be associated with some common auditory signal.",
  "y": "uses"
 },
 {
  "id": "32e860cdf03df7f6cb58b7f9e85ac0_6",
  "x": "Indeed, we find that word2vec pre-training helps improve performance (Sec. 5.3). Our use of language embeddings as an initialization to fine-tune (specialize) from, as opposed to formulating a joint objective with language and audio context<cite> (Kiela and Clark, 2015)</cite> is driven by the fact that we are interested in embeddings for words grounded in sounds, and not better generic word similarity. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "32e860cdf03df7f6cb58b7f9e85ac0_8",
  "x": "We compare against previous works Lopopolo and van Miltenburg (2015) and<cite> Kiela and Clark (2015)</cite> . While the former uses a standard bag of words and SVD pipeline to arrive at distributional representations for words, the latter trains under a joint objective that respects both linguistic and auditory similarity. We use the openly available implementation for Lopopolo and van Miltenburg (2015) and re-implement<cite> Kiela and Clark (2015)</cite> and train them on our dataset for a fair comparison of the methods.",
  "y": "uses"
 },
 {
  "id": "32e860cdf03df7f6cb58b7f9e85ac0_10",
  "x": "We see that specializing the embeddings for sound using our two-stage training outperforms prior work <cite>(Kiela and Clark (2015)</cite> and Lopopolo and van Miltenburg (2015) ), which did not do specialization. Among our approaches, tag-word2vec performs second best -this is intuitive since the tag distributions implicitly capture auditory relatedness (a sound may have tags cat and meow), while word2vec and sound-word2vec(r) have the lowest performance. (Kiela and Clark, 2015) (higher is better).",
  "y": "differences"
 },
 {
  "id": "32e860cdf03df7f6cb58b7f9e85ac0_11",
  "x": "Our approach performs better than<cite> Kiela and Clark (2015)</cite> . ---------------------------------- **FOLEY SOUND DISCOVERY**",
  "y": "differences"
 },
 {
  "id": "32e860cdf03df7f6cb58b7f9e85ac0_12",
  "x": "We find that Sound-word2vec performs the best with a mean rank of 34.6 compared to other baselines tag-word2vec (38.9), soundword2vec(r) (114.3) and word2vec (189.45). As observed previously, the second best performing approach is tag-word2vec. Lopopolo and van Miltenburg (2015) and<cite> Kiela and Clark (2015)</cite> perform worse than tag-word2vec with a mean rank of 48.4 and 42.1 respectively.",
  "y": "differences"
 },
 {
  "id": "32e860cdf03df7f6cb58b7f9e85ac0_13",
  "x": "AMEN and ASLex<cite> (Kiela and Clark, 2015)</cite> are subsets of the MEN and SimLex-999 datasets for word relatedness grounded in sound. From Table 2, we can see that our embeddings outperform<cite> (Kiela and Clark, 2015)</cite> on both AMEN and ASLex. These datasets were curated by annotating concepts related by sound; however we observe that relatedness is often confounded.",
  "y": "background"
 },
 {
  "id": "32e860cdf03df7f6cb58b7f9e85ac0_14",
  "x": "From Table 2, we can see that our embeddings outperform<cite> (Kiela and Clark, 2015)</cite> on both AMEN and ASLex. These datasets were curated by annotating concepts related by sound; however we observe that relatedness is often confounded. For example, (river, water), (automobile, car) are marked as aurally related however they do not stand out as aurally-related examples as they are already semantically related.",
  "y": "differences"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_0",
  "x": "The issue is an open research area in computer vision and machine learning [1, 2, 3, <cite>4,</cite> 5, 6] . In recent years, recurrent neural networks (RNNs) implemented by long short-term memory (LSTM) especially show good performances in sequence data processing and they are widely used as decoders to generate a natural language description from an image in many methods [3, <cite>4,</cite> 5, 6, 7] . High-performance approaches on convolutional neural networks (CNNs) have been proposed [8, 9] , which are employed to represent the input image with a feature vector for the caption generation [3, <cite>4,</cite> 5] .",
  "y": "background"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_1",
  "x": "The issue is an open research area in computer vision and machine learning [1, 2, 3, <cite>4,</cite> 5, 6] . In recent years, recurrent neural networks (RNNs) implemented by long short-term memory (LSTM) especially show good performances in sequence data processing and they are widely used as decoders to generate a natural language description from an image in many methods [3, <cite>4,</cite> 5, 6, 7] . High-performance approaches on convolutional neural networks (CNNs) have been proposed [8, 9] , which are employed to represent the input image with a feature vector for the caption generation [3, <cite>4,</cite> 5] .",
  "y": "background"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_2",
  "x": "High-level semantic concepts of the image are effective to describe a unique situation and a relation between objects in an image<cite> [4,</cite> 10] . Extracting specific arXiv:1807.09434v1 [cs.CV] 25 Jul 2018 semantic concepts encoded in an image, and applying them into RNN network has improved the performance significantly<cite> [4]</cite> . Detecting semantic attributes are a critical part because the high-level semantic information has a considerable effect on the performance.",
  "y": "background"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_3",
  "x": "**RELATED WORK** Combinations of CNNs and RNNs have been widely used for the image captioning networks [1, 2, 3, <cite>4,</cite> 12, 13] . An end-to-end neural network consisting of a vision CNN followed by a language generating RNN was proposed [1] .",
  "y": "background"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_4",
  "x": "They predicted attributes by treating the problem as a multi-label classification. The CNN framework was used, and outputs from different proposal sub-regions are aggregated. Gan et al.<cite> [4]</cite> proposed Semantic Concept Network (SCN) integrating semantic concept to a LSTM network.",
  "y": "background"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_5",
  "x": "Gan et al.<cite> [4]</cite> proposed Semantic Concept Network (SCN) integrating semantic concept to a LSTM network. SCN factorized each weight matrix of the attribute integrated the LSTM model to reduce the number of parameters. We employed SCN-LSTM as a language generator to verify the effectiveness of our method.",
  "y": "uses"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_6",
  "x": "The model will be described in Section 3.3. After getting distinctive-attribute from images, we apply these attributes to an caption generation network to verify their effect. We used SCN-LSTM<cite> [4]</cite> as a decoder which is a tag integrated network.",
  "y": "uses"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_7",
  "x": "where D p indicates distinctive-attribute predicted by the proposed model described in Section 3.3. Similar to<cite> [4,</cite> 13, 18] , the objective function is composed of the conditional log-likelihood on the image feature and the attribute as where I n , f (\u00b7), and X indicates the nth image, an image feature extraction function, and the caption, respectively.",
  "y": "similarities"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_8",
  "x": "**SEMANTIC INFORMATION EXTRACTION BY TF-IDF** Most of the previous methods constituted semantic information, that was a ground truth attribute, as a binary form<cite> [4,</cite> 12, 13, 19] . They first determined vocabulary using K most common words in the training captions.",
  "y": "uses"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_9",
  "x": "The vocabulary should contain enough particular words to represent each image. At the same time, the semantic information should be trained well for prediction accuracy. In the perspective of vocabulary size, Gan<cite> [4]</cite> and Fang [12] selected 1000 words and Wu [13] selected 256 words, respectively.",
  "y": "background"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_10",
  "x": "In the perspective of vocabulary size, Gan<cite> [4]</cite> and Fang [12] selected 1000 words and Wu [13] selected 256 words, respectively. They all selected vocabulary among nouns, verbs, and adjectives. We determine the words to be included in the vocabulary based on the IDF scores. We do not distinguish between verbs, nouns, adjectives, and other parts of speech.",
  "y": "differences"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_11",
  "x": "We note that our network does not contain softmax as a final layer, different from other attribute predictors described in previous papers<cite> [4,</cite> 13] . Hence, we use the output of an activation function of the fourth FC layer as the final predictive score D p,i . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_12",
  "x": "After semantic information vectors are extracted, they are fed into the prediction model in pairs with images. The training results with the different vectors will be reported in Sec 4.4. SCN-LSTM training procedure generally follows<cite> [4]</cite> except for the dimension of the input attribute vector.",
  "y": "similarities"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_13",
  "x": "We use the public implementation [30] of this method opened by Gan who is the author of the published paper<cite> [4]</cite> . For an image feature, we take out the output of the 2048-way pool5 layer from ResNet-152 which is pre-trained on the ImageNet dataset [31] . Word embedding vectors are initialized with the word2vec vectors proposed by [32] .",
  "y": "uses"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_14",
  "x": "We average inferred probability for 5 identical SCN-LSTM model as<cite> [4]</cite> did. ---------------------------------- **EVALUATION PROCEDURES**",
  "y": "uses"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_15",
  "x": "We use the macro-average F1 metric to compare the performance of the proposed distinctive-attribute prediction model. The output attribute of previous methods<cite> [4,</cite> 12, 13, 19] represent probabilities, on the other hand, that of the proposed method are the distinctiveness score itself. We evaluate the prediction considering it as a multi-label and multi-class classification problem. .",
  "y": "differences"
 },
 {
  "id": "33096f1e855d23046cb4cbfe95eef0_16",
  "x": "**SUPPLEMENTARY MATERIAL** In the experiment, we compared our method with SCN<cite> [4,</cite> 30] that uses extracted tags according to their semantic concept detection method. To evaluate the proposed method with more pictures, we compare the predicted semantic attributes by using SCN and the proposed scheme.",
  "y": "uses"
 },
 {
  "id": "332e252e09d28763deb1ded2171c90_0",
  "x": "**ABSTRACT** Tree-based approaches to alignment model translation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other. The trees may be learned directly from parallel corpora<cite> (Wu, 1997</cite>), or provided by a parser trained on hand-annotated treebanks (Yamada and Knight, 2001) .",
  "y": "background"
 },
 {
  "id": "332e252e09d28763deb1ded2171c90_1",
  "x": "Tree-based approaches to alignment model translation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other. The trees may be learned directly from parallel corpora<cite> (Wu, 1997</cite>), or provided by a parser trained on hand-annotated treebanks (Yamada and Knight, 2001) . In this paper, we compare these approaches on Chinese-English and French-English datasets, and find that automatically derived trees result in better agreement with human-annotated word-level alignments for unseen test data.",
  "y": "uses"
 },
 {
  "id": "332e252e09d28763deb1ded2171c90_2",
  "x": "The tree-based approach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences in word order in the grammars of the two languages. Furthermore, the tree structure allows us to make probabilistic independence assumptions that result in polynomial time algorithms for estimating a translation model from parallel training data, and for finding the highest probability translation given a new sentence. <cite>Wu (1997)</cite> modeled the reordering process with binary branching trees, where each production could be either in the same or in reverse order going from source to target language.",
  "y": "background"
 },
 {
  "id": "332e252e09d28763deb1ded2171c90_3",
  "x": "Yamada and Knight (2001) present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures. This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in <cite>Wu (1997)</cite> , but the specific bracketing of the parse tree provided. In this paper, we make a direct comparison of a syntactically unsupervised alignment model, based on <cite>Wu (1997)</cite> , with a syntactically supervised model, based on Yamada and Knight (2001) .",
  "y": "background"
 },
 {
  "id": "332e252e09d28763deb1ded2171c90_4",
  "x": "This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in <cite>Wu (1997)</cite> , but the specific bracketing of the parse tree provided. In this paper, we make a direct comparison of a syntactically unsupervised alignment model, based on <cite>Wu (1997)</cite> , with a syntactically supervised model, based on Yamada and Knight (2001) . We use the term syntactically supervised to indicate that the syntactic structure in one language is given to the training procedure.",
  "y": "uses"
 },
 {
  "id": "332e252e09d28763deb1ded2171c90_5",
  "x": "---------------------------------- **THE INVERSION TRANSDUCTION GRAMMAR** The Inversion Transduction Grammar of <cite>Wu (1997)</cite> can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context-free grammar productions.",
  "y": "background"
 },
 {
  "id": "332e252e09d28763deb1ded2171c90_6",
  "x": "In our experiments we use a grammar with a start symbol S, a single preterminal C, and two nonterminals A and B used to ensure that only one parse can generate any given word-level alignment (ignoring insertions and deletions)<cite> (Wu, 1997</cite>; Zens and Ney, 2003) . The individual lexical translations produced by the grammar may include a NULL word on either side, in order to represent insertions and deletions. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "332e252e09d28763deb1ded2171c90_7",
  "x": "\"Inversion Transduction Grammar\" (ITG) is the model of <cite>Wu (1997)</cite> , \"Tree-to-String\" is the model of Yamada and Knight (2001) , and \"Tree-to-String, Clone\" allows the node cloning operation described above. Our tree-based models were initialized from uniform distributions for both the lexical translation probabilities and the tree reordering operations, and were trained until AER began to rise on our held-out cross-validation data, which turned out to be four iterations for the tree-to-string models and three for the Inversion Transduction Grammar. French-English results are shown in Table 2 .",
  "y": "background"
 },
 {
  "id": "332e252e09d28763deb1ded2171c90_8",
  "x": "We used the GIZA++ package, including the HMM model of Och and Ney (2000) . We ran Model 1 for three iterations, then the HMM model for three iterations, and finally Model 4 for two iterations, training each model until AER began to increase on our held-out cross validation data. \"Inversion Transduction Grammar\" (ITG) is the model of <cite>Wu (1997)</cite> , \"Tree-to-String\" is the model of Yamada and Knight (2001) , and \"Tree-to-String, Clone\" allows the node cloning operation described above.",
  "y": "uses background"
 },
 {
  "id": "332e252e09d28763deb1ded2171c90_9",
  "x": "They train the models of Wu (1993) . Decoding, meaning exact computation of the highest probability translation given a foreign sentence, is not possible in polynomial time for the IBM models, and in practice decoders search through the space of hypothesis translations using a set of additional, hard alignment constraints. Zens and Ney (2003) compute the viterbi alignments for German-English and French-English sentences pairs using IBM Model 5, and then measure how many of the resulting alignments fall within the hard constraints of both <cite>Wu (1997)</cite> and Berger et al. (1996) .",
  "y": "background"
 },
 {
  "id": "3351b13fc0c9d4d2de16d897c78aee_0",
  "x": "In recent years, methods based on string kernels have demonstrated remarkable performance in various text classification tasks ranging from authorship identification (Popescu and Grozea, 2012) and sentiment analysis (Gim\u00e9nez-P\u00e9rez et al., 2017; to native language identification (Popescu and Ionescu, 2013; Ionescu et al., 2014 Ionescu et al., , 2016 , dialect identification (Ionescu and Popescu, 2016b;<cite> Ionescu and Butnaru, 2017</cite>; and automatic essay scoring (Cozma et al., 2018) . As long as a labeled training set is available, string kernels can reach state-of-the-art results in various languages including English (Ionescu et al., 2014; Gim\u00e9nez-P\u00e9rez et al., 2017; Cozma et al., 2018) , Arabic (Ionescu, 2015; Ionescu et al., 2016;<cite> Ionescu and Butnaru, 2017</cite>; , Chinese and Norwegian (Ionescu et al., 2016) . Different from all these recent approaches, we use unlabeled data from the test set to significantly increase the performance of string kernels.",
  "y": "background"
 },
 {
  "id": "3351b13fc0c9d4d2de16d897c78aee_1",
  "x": "In recent years, methods based on string kernels have demonstrated remarkable performance in various text classification tasks ranging from authorship identification (Popescu and Grozea, 2012) and sentiment analysis (Gim\u00e9nez-P\u00e9rez et al., 2017; to native language identification (Popescu and Ionescu, 2013; Ionescu et al., 2014 Ionescu et al., , 2016 , dialect identification (Ionescu and Popescu, 2016b;<cite> Ionescu and Butnaru, 2017</cite>; and automatic essay scoring (Cozma et al., 2018) . As long as a labeled training set is available, string kernels can reach state-of-the-art results in various languages including English (Ionescu et al., 2014; Gim\u00e9nez-P\u00e9rez et al., 2017; Cozma et al., 2018) , Arabic (Ionescu, 2015; Ionescu et al., 2016;<cite> Ionescu and Butnaru, 2017</cite>; , Chinese and Norwegian (Ionescu et al., 2016) . Different from all these recent approaches, we use unlabeled data from the test set to significantly increase the performance of string kernels.",
  "y": "background"
 },
 {
  "id": "3351b13fc0c9d4d2de16d897c78aee_2",
  "x": "As any other transductive learning method, the main disadvantage of the proposed framework is that the unlabeled test samples from the target domain need to be used in the training stage. Nevertheless, we present empirical results indicating that our approach can obtain significantly better accuracy rates in cross-domain polarity classification and Arabic dialect identification compared to state-of-the-art methods based on string kernels (Gim\u00e9nez-P\u00e9rez et al., 2017;<cite> Ionescu and Butnaru, 2017)</cite> . We also report better results than other domain adaptation methods (Pan et al., 2010; Bollegala et al., 2013; Franco-Salvador et al., 2015; Sun et al., 2016; Huang et al., 2017) .",
  "y": "differences"
 },
 {
  "id": "3351b13fc0c9d4d2de16d897c78aee_3",
  "x": "The data set was used in the ADI Shared Task of the 2017 VarDial Evaluation Campaign . Baseline. We choose as baseline the approach of Ionescu and<cite> Butnaru (2017)</cite> , which is based on string kernels and multiple kernel learning.",
  "y": "uses"
 },
 {
  "id": "3351b13fc0c9d4d2de16d897c78aee_4",
  "x": "In addition, we also compare with the second-best approach (Meta-classifier) . Evaluation procedure and parameters. Ionescu and<cite> Butnaru (2017)</cite> combined four kernels into a sum, and used Kernel Ridge Regression for training.",
  "y": "background"
 },
 {
  "id": "3351b13fc0c9d4d2de16d897c78aee_5",
  "x": "The fourth kernel is an RBF kernel (K ivec ) based on the i-vectors provided with the ADI data set (Ali et al., 2016) . In our experiments, we employ the exact same kernels as Ionescu and<cite> Butnaru (2017)</cite> to ensure an unbiased comparison with their ap- <cite>Butnaru, 2017)</cite> and the first runner up . The best accuracy rates are highlighted in bold.",
  "y": "similarities uses"
 },
 {
  "id": "3351b13fc0c9d4d2de16d897c78aee_6",
  "x": "The marker * indicates that the performance is significantly better than (Ionescu and <cite>Butnaru, 2017)</cite> according to a paired McNemar's test performed at a significance level of 0.01. proach. As in the polarity classification experiments, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training the transductive classifier, and we use Kernel Ridge Regression with a regularization of 10 \u22125 in all our ADI experiments.",
  "y": "differences"
 },
 {
  "id": "3351b13fc0c9d4d2de16d897c78aee_7",
  "x": "Results. The results for the cross-domain Arabic dialect identification experiments on both the development and the test sets are presented in Table 3 . The domain-adapted sum of kernels obtains improvements above 0.8% over the stateof-the-art sum of kernels (Ionescu and <cite>Butnaru, 2017)</cite> .",
  "y": "differences"
 },
 {
  "id": "3356313ee5cdf186816cd6fecfce84_0",
  "x": "Conventional hybrid approaches to KWS first divide their audio signal in time frames to extract features, e.g., Mel Frequency Cepstral Coefficients (MFCC). A neural net then estimates phoneme or state posteriors of the keyword Hidden Markov Model in order to calculate the keyword probability using a Viterbi search. In recent years, end-to-end architectures gained traction that directly classify keyword posterior probabilites based on the previously extracted features, e.g., [1,<cite> 2,</cite> 3, 4, 5] .",
  "y": "background"
 },
 {
  "id": "3356313ee5cdf186816cd6fecfce84_2",
  "x": "To the best of our knowledge, we are the first to apply this method to the task of KWS. The first convolutional layer of our model is inspired by SincNet and we combine it with DSCconv. DSCconvs have first been introduced in the domain of Image Processing [8, 13] and have been applied to other domains since: Zhang et al. applied DSCconv to KWS <cite>[2]</cite> .",
  "y": "background"
 },
 {
  "id": "3356313ee5cdf186816cd6fecfce84_4",
  "x": "DSConv have been successfully applied to the domain of computer vision [8, 13] , neural translation [7] and KWS <cite>[2]</cite> . Fig. 3 provides an overview of the steps from a regular convolution to the GDSConv. The number of parameters of one DSConv layer amounts to N DSConv = k \u00b7 c in + c in \u00b7 c out with the kernel size k and the number of input and output channels c in and c out respectively; the first summand is determined by the depthwise convolution, the second summand by the pointwise convolution [7] .",
  "y": "background"
 },
 {
  "id": "3356313ee5cdf186816cd6fecfce84_5",
  "x": "Compared to the DSConv network in <cite>[2]</cite> , our network is more efficient in terms of accuracy for a given parameter count. Their biggest model has a 1.2% lower accuracy than our base model while having about 4 times the parameters. Choi et al. [4] has the most competitive results while we are still able to improve upon their accuracy for a given number of parameters.",
  "y": "differences"
 },
 {
  "id": "3395c9ed8ad9f2d048bf8ebf950d16_0",
  "x": "We often draw pragmatic inferences about a speaker's intentions from what they choose to say, but also from what they choose not to say in context. This suggests that pragmatic reasoning might allow modern neural network models to more efficiently learn on grounded language data from cooperative reference games. As a motivating case, consider an instance of the color reference task from<cite> Monroe et al. (2017)</cite> shown in the first row of Table 1 . In this task, a speaker communicates a target color to a listener in a context containing two distractor colors; the listener picks out the target based on what the speaker says. In the first instance from Table 1 , the speaker utters \"dark blue\" to describe the target. Whereas \"dark\" and \"blue\" also apply to the target, they lose their informativity in the presence of the distractors, and so the speaker pragmatically opts for \"dark blue\". A listener who is learning the language from such examples might draw several inferences from the speaker's utterance. First, under the assumption that the speaker is informative, a \"literal\" learner might infer that \"dark blue\" applies to the target shade more than the distractors. Second, a \"pragmatic\" learner might consider the cheaper alternatives-\"dark\" and \"blue\"-that have occurred in the presence of the same target in prior contexts, and infer that these alternative utterances must also apply to the distractors given the speaker's failure to use them. The pragmatic learner might thus gain more semantic knowledge from the same training instances than the literal learner: pragmatic reasoning can reduce the data complexity of learning.",
  "y": "background"
 },
 {
  "id": "3395c9ed8ad9f2d048bf8ebf950d16_1",
  "x": "We compare pragmatic and non-pragmatic models at training and at test, while varying conditions on the training data to test hypotheses regarding the utility of pragmatic inference for learning. In particular, we show that incorporating pragmatic reasoning at training time yields improved, state-of-the-art accuracy for listener models on the color reference task from<cite> Monroe et al. (2017)</cite> , and the effect demonstrated by this improvement is especially large under small training data sizes. We further introduce a new color-grid reference task and data set consisting of higher dimensional objects and more complex speaker language; we find that the effect of pragmatic listener training is even larger in this setting.",
  "y": "differences background"
 },
 {
  "id": "3395c9ed8ad9f2d048bf8ebf950d16_2",
  "x": "**RELATED WORK** Prior work has shown that neural network models trained to capture the meanings of utterances can be improved using pragmatic reasoning at test time via the RSA framework (Andreas and Klein, 2016;<cite> Monroe et al., 2017</cite>; Goodman and Frank, 2016; Frank and Goodman, 2012) . For instance,<cite> Monroe et al. (2017)</cite> train context-agnostic (i.e. non-pragmatic) neural network models to learn the meanings of color utterances using a corpus of examples of the form shown in the first line of Table 1 .",
  "y": "background"
 },
 {
  "id": "3395c9ed8ad9f2d048bf8ebf950d16_3",
  "x": "For instance,<cite> Monroe et al. (2017)</cite> train context-agnostic (i.e. non-pragmatic) neural network models to learn the meanings of color utterances using a corpus of examples of the form shown in the first line of Table 1 . At evaluation, they add an RSA layer on top of the trained model to draw pragmatic, context-sensitive inferences about intended color referents. Other related work explores additional approaches to create context-aware models that generate color descriptions (Meo et al., 2014) , image captions (Vedantam et al., 2017) , spatial references (Golland et al., 2010) , and utterances in simple reference games (Andreas and Klein, 2016) .",
  "y": "background"
 },
 {
  "id": "3395c9ed8ad9f2d048bf8ebf950d16_4",
  "x": "We compare neural nets trained pragmatically and non-pragmatically on a new color-grid reference game corpus as well as the color reference corpus from<cite> Monroe et al. (2017)</cite> . In this section, we describe our tasks and models. ----------------------------------",
  "y": "background"
 },
 {
  "id": "3395c9ed8ad9f2d048bf8ebf950d16_5",
  "x": "The color reference game from<cite> Monroe et al. (2017)</cite> consists of rounds played between a speaker and a listener. Each round has a context of two distractors and a target color (Figure 1a ). Only the speaker knows the target, and must communicate it to the listener-who must pick out the target based on the speaker's English utterance. Similarly, each round of our new color-grid reference game contains target and distractor color-grid objects, and the speaker must communicate the target grid to the listener (Figure 1b) . We train neural network models to play the listener role in these games.",
  "y": "background"
 },
 {
  "id": "3395c9ed8ad9f2d048bf8ebf950d16_6",
  "x": "The language model is pre-trained over speaker utterances paired with targets, but the support of the distribution encoded by this LSTM is too large for the s 1 normalization term within the RSA listener to be computed efficiently. Similar to<cite> Monroe et al. (2017)</cite>, we resolve this issue by taking a small set of samples from the pre-trained LSTM applied to each object in a context, to approximate p(U | O), each time l 1 is computed during training and evaluation. ----------------------------------",
  "y": "similarities background"
 },
 {
  "id": "3395c9ed8ad9f2d048bf8ebf950d16_7",
  "x": "for j = 1 to k do for each object in a round 28: Sample<cite> Monroe et al. (2017)</cite> ). Both architectures apply a tanh layer to an input object o (a grid or color), and use the result as the initial hidden state of an LSTM layer.",
  "y": "uses"
 },
 {
  "id": "3395c9ed8ad9f2d048bf8ebf950d16_8",
  "x": "For the color reference task, we use the data collected by<cite> Monroe et al. (2017)</cite> from human play on the color reference task through Amazon Mechanical Turk using the framework of Hawkins (2015). Each game consists of 50 rounds played by a human speaker and listener. In each round, the speaker describes a target color surrounded by a context of two other distractor colors, and a listener clicks on the targets based on the speaker's description (see Figure 1a) . The resulting data consists of 46, 994 rounds across 948 games, where the colors of some rounds are sampled to be more likely to require pragmatic reasoning than others. In particular, 15, 516 trials are close with both distractors within a small distance to the target color in RGB space, 15, 782 are far with both distractors far from the target, and 15, 693 are split with one distractor near the target and one far from the target.",
  "y": "uses background"
 },
 {
  "id": "3395c9ed8ad9f2d048bf8ebf950d16_9",
  "x": "For model development, we use the train/dev/test split from<cite> Monroe et al. (2017)</cite> with 15, 665 training, 15, 670 dev, and 15, 659 test rounds. Within our models, we represent color objects using a 3-dimensional CIELAB color spacenormalized so that the values of each dimension are in [\u22121, 1]. Our use of the CIELAB color space departs from prior work on the color data which used a 54-dimensional Fourier space <cite>(Monroe et al., 2017</cite> (Monroe et al., , 2016 Zhang and Lu, 2002) .",
  "y": "extends background"
 },
 {
  "id": "3395c9ed8ad9f2d048bf8ebf950d16_10",
  "x": "Following<cite> Monroe et al. (2017)</cite> , we preprocess the tokens by lowercasing, splitting off punctuation, and replacing tokens that appear only once with [unk] . In the color data, we also follow the prior work and split off -er, -est, and -ish, suffixes. Whereas the prior work concatenated speaker messages into a single utterance without limit, we limit the full sequence length to 40 tokens for efficiency.",
  "y": "extends"
 },
 {
  "id": "3395c9ed8ad9f2d048bf8ebf950d16_11",
  "x": "We follow<cite> Monroe et al. (2017)</cite> for language model hyper-parameters, with embedding and LSTM layers of size 100. Also following this prior work, we use a learning rate of 0.004, batch size 128, and apply 0.5 dropout to each layer. We train for 7, 000 iterations, evaluating the model's accuracy on the dev set every 100 iterations.",
  "y": "uses"
 },
 {
  "id": "3395c9ed8ad9f2d048bf8ebf950d16_12",
  "x": "We generally use speaker rationality \u03b1 = 8.0 based on dev set tuning, and we follow<cite> Monroe et al. (2017)</cite> for other hyper-parameters-with embedding size of 100 and LSTM size of 100 in our meaning functions. Also following this prior work, we allow the LSTM to be bidirectional with learning rate of 0.005, batch size 128, and gradient clipping at 5.0. We train listeners for 10, 000 iterations on the color data and 15, 000 iterations on grid data, evaluating dev set accuracy every 500 iterations. We pick the model with the best accuracy from those evaluated at 500 iteration intervals.",
  "y": "uses"
 },
 {
  "id": "3395c9ed8ad9f2d048bf8ebf950d16_13",
  "x": "These results provide evidence that literal meanings estimated through pragmatic training are better calibrated for pragmatic usage than meanings estimated through non-pragmatic l 0 training. Furthermore, relative to state-of-the-art in<cite> Monroe et al. (2017)</cite> , Table 2 shows that our pragmatically trained model yields improved accuracy over their best \"blended\" pragmatic L e model which computed predictions based on the product of two separate non-pragmatically trained models. The effect sizes are small for the pragmatic to non-pragmatic comparisons when training on the full color data (though approaching the ceiling 0.9108 human accuracy), but we hypothesized that the effect of pragmatic training would increase for training with smaller data sizes (as motivated by arguments in the introduction).",
  "y": "differences background"
 },
 {
  "id": "34b73a56bd9b80dc415ca2c5608596_0",
  "x": "Recent work using word embeddings-low-dimensional vector representations of words trained on large datasets to capture key semantic informationhas demonstrated that language encodes several gender, racial, and other common contemporary biases that correlate with both implicit biases (Caliskan et al., 2017) and macro-scale historical trends <cite>(Garg et al., 2018)</cite> . For example, the historical biases presented in <cite>(Garg et al., 2018)</cite> are computed using decade-specific word embeddings produced by training different Word2Vec (Mikolov et al., 2013 ) models on a large corpus of historical text from that decade.",
  "y": "background"
 },
 {
  "id": "34b73a56bd9b80dc415ca2c5608596_1",
  "x": "To validate our model, we compare our results to those produced via the decade-by-decade models trained in <cite>(Garg et al., 2018)</cite> using the Corpus of Historical American English (Davies, 2010) . In particular, we compute linguistic bias scores for two analyses presented in <cite>(Garg et al., 2018)</cite> : the extent to which female versus male words are semantically similar to occupation-related words, and the extent to which Asian vs. White last names are semantically similar to the same, from 1910 through 1990.",
  "y": "similarities uses"
 },
 {
  "id": "34b73a56bd9b80dc415ca2c5608596_2",
  "x": "We then compute correlations between changes in these scores and the actual changes in female and Asian workforce participation rates (relative to men and Whites, respectively) over the same time period. Figure 2 depicts these results. The correlation between our scores and changes in workforce participation rates are similar to the correlation between the scores from <cite>(Garg et al., 2018)</cite> and the same (r = 0.8, p = 0.01 and r = 0.81, p < 0.01, respectively, for gender occupation bias; r = 0.84, p < 0.01 and r = 0.79, p = 0.01, respectively, for Asian/White occupation bias).",
  "y": "similarities"
 },
 {
  "id": "34b73a56bd9b80dc415ca2c5608596_3",
  "x": "Figure 2 depicts these results. The correlation between our scores and changes in workforce participation rates are similar to the correlation between the scores from <cite>(Garg et al., 2018)</cite> and the same (r = 0.8, p = 0.01 and r = 0.81, p < 0.01, respectively, for gender occupation bias; r = 0.84, p < 0.01 and r = 0.79, p = 0.01, respectively, for Asian/White occupation bias). Qualitative inspection of Figure 2 suggests that our model also produces smoother decade-by-decade scores, suggesting that it not only identifies attribute- <cite>(Garg et al., 2018)</cite> and our model (blue dotted and green dashed lines, respectively) compared to actual workforce participation rates (solid lines) for gender (top) and Asian/White (bottom) linguistic biases.",
  "y": "differences"
 },
 {
  "id": "34b73a56bd9b80dc415ca2c5608596_4",
  "x": "Finally, we define bias towards refugees similar to how the authors of <cite>(Garg et al., 2018)</cite> define bias against Asians during the 20th century, measuring to what extent radio shows associate \"outsider\" adjectives like \"aggressive\", \"frightening\", \"illegal\", etc. To compute refugee bias scores with respect to the attribute set A, we use the relative norm distance metric from <cite>(Garg et al., 2018)</cite> :",
  "y": "similarities uses"
 },
 {
  "id": "34b73a56bd9b80dc415ca2c5608596_5",
  "x": "From qualitative inspection, the day-byday scores produced by the non-dynamic model appear much less smooth, and hence, fail to show the relative shift in discourse that likely occurred in response to a major refugee-related news event. One possible reason for this is that the median number of words for each day in the talk radio corpus is 4 million-over 5x fewer than a median of 22 million words per decade used to train each decade-specific model in <cite>(Garg et al., 2018)</cite> . These results suggest that using our dynamic embedding approach is particularly valuable when data is sparse for any given attribute.",
  "y": "differences"
 },
 {
  "id": "34b73a56bd9b80dc415ca2c5608596_6",
  "x": "We validated our model by replicating gender and ethnic stereotypes produced in <cite>(Garg et al., 2018)</cite> by training multiple word embedding models and applied it to a novel corpus of talk radio data to analyze how perceptions of refugees as \"outsiders\" vary by geography and over time. Our results illustrate that dynamic word embeddings capture salient shifts in public discourse around specific topics, suggesting their potential usefulness as a tool for obtaining a granular understanding of how the media and members of the public perceive different issues, especially when data is sparse. Opportunities for future work include a) comparing the results of our model to other existing dynamic embedding models, particularly when the attribute of interest is temporal in nature, b) exploring embeddings defined with respect to other attributes of interest, perhaps in combination with other contextual embedding models like (Peters et al., 2018) , c) exploring alternative definitions of bias towards refugees and other groups, and d) learning a dynamic embedding model for continuous attributes in order to limit the need to impose (perhaps arbitrary) discretizations.",
  "y": "similarities uses"
 },
 {
  "id": "35233406ffd78d87743478454432d5_0",
  "x": "From those 4 QA pairs discussing a Microsoft mouse, we know that the Microsoft mouse is compatible with \"Microsoft Surface Pro 3\" and \"Windows 10\" but incompatible with \"iPad\". Furthermore, we have no idea whether \"Samsung Galaxy Tab 2 10.0\" is compatible or not with this mouse. Similar to our previous work in product reviews<cite> (Xu et al., 2016)</cite> , we call the mouse target entity and those 4 products complementary entities of the target entity.",
  "y": "similarities background"
 },
 {
  "id": "35233406ffd78d87743478454432d5_1",
  "x": "This is because customers tend to ask specific questions in PCQA. We leave the work of mining compatible/incompatible products on open questions to future work. Given the structure of a QA pair, our method naturally has a twostage framework: Complementary Entity Recognition (CER)<cite> (Xu et al., 2016)</cite> and yes/no answer classification.",
  "y": "uses"
 },
 {
  "id": "35233406ffd78d87743478454432d5_2",
  "x": "We leave the work of mining compatible/incompatible products on open questions to future work. Given the structure of a QA pair, our method naturally has a twostage framework: Complementary Entity Recognition (CER)<cite> (Xu et al., 2016)</cite> and yes/no answer classification. For the first stage, we employ a similar approach as in<cite> (Xu et al., 2016)</cite> ; for the second stage, it is reduced to a yes/no answer classification problem (McAuley and Yang, 2016) .",
  "y": "uses similarities"
 },
 {
  "id": "35233406ffd78d87743478454432d5_3",
  "x": "**RELATED WORKS** The problem of Complementary Entity Recognition (CER) is first proposed by Xu et. al.<cite> (Xu et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "35233406ffd78d87743478454432d5_4",
  "x": "The problem of Complementary Entity Recognition (CER) is first proposed by Xu et. al.<cite> (Xu et al., 2016)</cite> . However, our previous work focuses on product reviews and consider CER as a special kind of aspect extraction problem (Liu, 2015) . Determining the polarities of compatibility is reduced to a traditional sentiment classification problem. This paper focuses on yes/no QAs in PCQA and the polarities of compatibility is a yes/no answer classification problem.",
  "y": "motivation"
 },
 {
  "id": "35233406ffd78d87743478454432d5_5",
  "x": "The major differences are that many complementary entities are not named entities and CER heavily relies on the context of an entity (e.g., \"iPhone\" in \"I like my iPhone\" is not a complementary entity). Complementary entities are also studied as a social network problem in recommender systems (Zheng et al., 2009; McAuley et al., 2015) . We discussed the benefit of CER over social network problem in <cite>(Xu et al., 2016</cite> ) so we omit here but keep a performance comparison in Section 5.",
  "y": "background"
 },
 {
  "id": "35233406ffd78d87743478454432d5_6",
  "x": "We also bring out the test dataset used in<cite> (Xu et al., 2016)</cite> for a comparison (Section 5). We notice that PCQA addresses compatibility issues in a different perspective compared to product reviews. PCQA tends to be specific on compatibility issues; reviews are free to talk about their experiences (e.g, opinions on features/aspects).",
  "y": "uses"
 },
 {
  "id": "35233406ffd78d87743478454432d5_7",
  "x": "Then we briefly introduce the method for CER in<cite> (Xu et al., 2016)</cite> . ---------------------------------- **TWO-STAGE FRAMEWORK**",
  "y": "uses"
 },
 {
  "id": "35233406ffd78d87743478454432d5_8",
  "x": "Since complementary entities are mentioned in yes/no questions and their polarities of compatibility information are in answers, the proposed method naturally has a two-stage framework: Complementary Entity Recognition: we extract complementary entities from questions using dependency paths almost the same as in<cite> (Xu et al., 2016)</cite> . It utilizes a large amount of unlabeled reviews under the same category as the target entity to expand knowledge about domain-specific verbs. Identifying Polarities of Yes/No Answers: then we determine the polarity (yes, no or neutral) of yes/no answers for each question with complementary entity and assign a compatibility label (compatible, incompatible or unknown) to it.",
  "y": "uses"
 },
 {
  "id": "35233406ffd78d87743478454432d5_9",
  "x": "We briefly introduce the method used in<cite> (Xu et al., 2016)</cite> and how the dependency paths can be used in questions of PCQA (details of dependency paths can be found in the original paper). The basic idea is to use dependency paths to identify the context of complementary relations around complementary entities. Dependency paths can match dependency relations parsed through dependency parsing 1 , which parses a sentence into a set of dependency relations.",
  "y": "uses"
 },
 {
  "id": "35233406ffd78d87743478454432d5_10",
  "x": "**COMPLEMENTARY ENTITY RECOGNITION** We briefly introduce the method used in<cite> (Xu et al., 2016)</cite> and how the dependency paths can be used in questions of PCQA (details of dependency paths can be found in the original paper). The basic idea is to use dependency paths to identify the context of complementary relations around complementary entities.",
  "y": "extends"
 },
 {
  "id": "35233406ffd78d87743478454432d5_11",
  "x": "The initial agreement is 93%. Then disagreements are discussed and final agreements are reached among all annotators. To obtain knowledge about domainspecific verbs, we use 6000 reviews for each product similar as in<cite> (Xu et al., 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "35233406ffd78d87743478454432d5_12",
  "x": "CER6K: This method is the method proposed in<cite> (Xu et al., 2016)</cite> . Specifically, it uses 6000 reviews to expand domain-specific verbs. Next, we perform a separate evaluation on yes/no answer classification.",
  "y": "uses"
 },
 {
  "id": "35522a080b41f716723d2a619f59c4_0",
  "x": "To reduce this kind of error introduced by the translator, Wan in <cite>(Wan, 2009</cite> ) applied a co-training scheme. In this setting classifiers are trained in both languages and the two classifiers teach each other for the unlabeled examples. The co-training approach manages to boost the performance as it allows the text similarity in the target language to compete with the \"fake\" similarity from the translated texts.",
  "y": "background"
 },
 {
  "id": "35522a080b41f716723d2a619f59c4_1",
  "x": "The source domain correspond to English reviews and the target domain for Chinese ones. The full feature vector is (x s , x t ). The difficulty we are facing is, due to noise in the translations, the conditional probabilities p(y|x s ) and the one in the translated texts p(y|x s ) may be quite different. Consider the following two straightforward strategies of using automatic machine translations: one can translate the original English labeled data (y, x s ) into (y, x t ) in Chinese and train a classifier, or one can train a classifier on (y, x s ) and translate x t in Chinese into x s in English so as to use the classifier. But as the conditional distribution can be quite different for the original language and the pseudo language produced by the machine translators, these two strategies give poor performance as reported in<cite> (Wan, 2009)</cite> .",
  "y": "similarities"
 },
 {
  "id": "35522a080b41f716723d2a619f59c4_2",
  "x": "**DATA SET** For comparsion, we use the same data set in<cite> (Wan, 2009)</cite> : Test Set(Labeled Chinese Reviews): The data set contains a total of 886 labeled product reviews in Chinese (451 positive reviews and 435 negative ones).",
  "y": "uses"
 },
 {
  "id": "35522a080b41f716723d2a619f59c4_3",
  "x": "They are of the same domain as the test set. We translate each English review into Chinese and vice versus through the public Google Translation service. Also following the setting in<cite> (Wan, 2009)</cite> , we only use the Chinese unlabeled data and English training sets for our SCL training procedures.",
  "y": "uses"
 },
 {
  "id": "35522a080b41f716723d2a619f59c4_4",
  "x": "Also following the setting in<cite> (Wan, 2009)</cite> , we only use the Chinese unlabeled data and English training sets for our SCL training procedures. The test set is blind to the training stage. The features we used are bigrams and unigrams in the two languages as in<cite> (Wan, 2009)</cite> .",
  "y": "uses"
 },
 {
  "id": "35522a080b41f716723d2a619f59c4_5",
  "x": "We compare our procedure with the co-training scheme reported in<cite> (Wan, 2009)</cite> : The method with the best performance in<cite> (Wan, 2009)</cite> . Two standard SVMs are trained using the co-training scheme for the Chinese views and the English views. And the results of the two SVMs are combined to give the final output.",
  "y": "uses"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_0",
  "x": "The problem has been addressed recently by researchers working on large knowledge bases such as Reverb (Fader et al., 2011) and Freebase (Bollacker et al., 2008) . The creation of question answering (QA) benchmarks for these knowledge bases (KB) has a significant impact on the domain, as shown by the number of QA systems recently proposed in the literature (Berant and Liang, 2014; Berant et al., 2013; Bordes et al., 2014a; <cite>Bordes et al., 2014b</cite>; Fader et al., 2013; Fader et al., 2014; Yao and Van Durme, 2014; Yih et al., 2014; Dong et al., 2015) . We identify two types of approaches for KBcentric QA systems: parsing-based approaches and information retrieval (IR) based approaches.",
  "y": "background"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_1",
  "x": "Parsing-based approaches (Yih et al., 2014; Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014) answer factoid questions by learning a structured representation for the sentences, called logical form. This logical form is then used to query the knowledge base and retrieve the answer. IR-based approaches try to identify the best possible match between the knowledge base and the question (Bordes et al., 2014a; <cite>Bordes et al., 2014b</cite>; Yao and Van Durme, 2014; Dong et al., 2015) .",
  "y": "background"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_2",
  "x": "On Wikianswers, the underlying semantics is very simple (just one single triple). However, the task remains challenging due to the large variety of lexicalizations for the same semantics. We follow the approach of <cite>Bordes et .al (2014b)</cite> which learns the embeddings of words and KB elements.",
  "y": "uses"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_3",
  "x": "We follow the approach of <cite>Bordes et .al (2014b)</cite> which learns the embeddings of words and KB elements. <cite>They</cite> model the semantics of natural language sentences and KB triples as the sum of the embeddings of the associated words and KB elements respectively. Despite its simplicity, this model performs surprisingly well in practice.",
  "y": "background"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_4",
  "x": "<cite>They</cite> model the semantics of natural language sentences and KB triples as the sum of the embeddings of the associated words and KB elements respectively. Despite its simplicity, this model performs surprisingly well in practice. Something even more interesting (<cite>Bordes et al., 2014b</cite>) is that the system can have a good performance even without using a paraphrase corpus.",
  "y": "background"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_5",
  "x": "The orthogonality constraint in the context of question answering is new, although it has been successfully used in other contexts . Like (<cite>Bordes et al., 2014b</cite>) , we use al-most no linguistic features such as POS tagging, parsing, etc. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_6",
  "x": "These queries are then ranked using a scoring function. <cite>Bordes et al. (2014b)</cite> introduce a linguistically leaner IR-based approach which identifies the KB triple most similar to the input NL question. In <cite>their</cite> approach, KB triples and NL questions are represented as sums of embeddings of KB symbols and words respectively.",
  "y": "background"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_7",
  "x": "In <cite>their</cite> approach, KB triples and NL questions are represented as sums of embeddings of KB symbols and words respectively. The similarity between a triple and a question is then simply the dot product of their embeddings. Interestingly, <cite>Bordes' (2014b)</cite> system performs relatively well (MAP score 0.34) on the Wikianswers dataset even without using the paraphrase corpus.",
  "y": "background"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_8",
  "x": "In <cite>their</cite> approach, KB triples and NL questions are represented as sums of embeddings of KB symbols and words respectively. The similarity between a triple and a question is then simply the dot product of their embeddings. Interestingly, <cite>Bordes' (2014b)</cite> system performs relatively well (MAP score 0.34) on the Wikianswers dataset even without using the paraphrase corpus.",
  "y": "background"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_9",
  "x": "Interestingly, <cite>Bordes' (2014b)</cite> system performs relatively well (MAP score 0.34) on the Wikianswers dataset even without using the paraphrase corpus. Our work continues this direction by further separating relations with entities.",
  "y": "extends"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_10",
  "x": "Word embeddings are generally learned (Deerwester et al., 1990; Mikolov et al., 2013; Lebret and Collobert, 2015; Faruqui et al., 2014) such that words with similar context will naturally share similar embeddings as measured for instance by cosine similarity. The embeddings learned in (<cite>Bordes et al., 2014b</cite> ) also encode context information. They link the embedding of words with the whole triple-answer in their scoring function.",
  "y": "background"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_11",
  "x": "which is the dot product between the embedding of the sentence and the embedding of the triple. The model is introduced in (<cite>Bordes et al., 2014b</cite>) and we use the same scoring function. Note that the model actually sums up each word embedding to form the embedding of the sentence.",
  "y": "uses"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_12",
  "x": "**TRAINING** Originally in (<cite>Bordes et al., 2014b</cite>) , given a question to be answered, training is performed by imposing a margin-constraint between the correct answer and negative ones. More precisely, note a a negative answer to the question q (the correct answer to q being a).",
  "y": "background"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_13",
  "x": "In (<cite>Bordes et al., 2014b</cite>) , the embedding of the semantics is then calculated as e + r for this very simple case. Now suppose that \u2200e = e, ||e \u2212 e || 2 \u2265 (i.e John is different from Mary with margin ) and that the same kind of constraints also holds for relations. However, even when these constraints are satisfied, it is not guaranteed that ||e + r \u2212 e \u2212 r || 2 \u2265 , which means that the model may still get confused on the whole semantics even if each part is clear.",
  "y": "background"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_14",
  "x": "That is, two sentences whose semantic representations involve two distinct entities and/or relations will have different values. In real problems, however, posing a hard orthogonality constraint largely reduces the model's What is the religious celebration of christians ? (christian.e be-all-about.r original-sin.e) (easter.e be-most-important-holiday.r christian.e) What do cassava come from ? (cassava.e be-source-of.r security.e) (cassava.e be-grow-in.r africa.e) Table 1 : Some examples for which our system differs from ( (<cite>Bordes et al., 2014b</cite>) ).",
  "y": "differences"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_15",
  "x": "We compare the model (<cite>Bordes et al., 2014b</cite>) with the model where we enforce E and R (and also \"E\" and \"R\") to be orthogonal. This means that words or KB symbols in fact live in an embedding space of dimension 10. At test time, for a given sentence \"e i r j \", a set of (e, r) pairs is ranked and we compute the proportion of cases where the first ranked pair is correct.",
  "y": "uses"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_16",
  "x": "This is the \"reranking\" setting used in (<cite>Bordes et al., 2014b</cite>) . Table 3 compares different systems in this setting. The Embedding scores are taken from (<cite>Bordes et al., 2014b</cite>) Table 3 shows that our technique improves the performance also on the larger, non-synthetic, dataset provided by Fader (2013) over the <cite>Bordes (2014b)</cite>'s method.",
  "y": "uses"
 },
 {
  "id": "357667e192057a48dff60edad86bf0_17",
  "x": "Table 3 compares different systems in this setting. The Embedding scores are taken from (<cite>Bordes et al., 2014b</cite>) Table 3 shows that our technique improves the performance also on the larger, non-synthetic, dataset provided by Fader (2013) over the <cite>Bordes (2014b)</cite>'s method. In addition, Table 1 shows some examples where the two systems differ and where the orthogonality regularized embeddings seem to better support the identification of similar relations.",
  "y": "uses"
 },
 {
  "id": "36436e1d8a3f1d65fcc369649341f2_0",
  "x": "Our intuitive explanation for this result is that lazy learning techniques keep all training items, whereas greedy approaches lose useful information by forgetting low-frequency or exceptional instances of the task, not covered by the extracted rules or models (Daelemans, 1996) . Apart from the empirical work in Tilburg and Antwerp, a number of recent studies on statistical natural language processing (e.g. Dagan & Lee, 1997; <cite>Collins & Brooks, 1995)</cite> also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low-frequency events, is harmful to generalization accuracy. After reviewing this empirical work briefly, I will report on new results (work in progress in collaboration with van den Bosch and Zavrel), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks: tagging, grapheme-to-phoneme conversion, and pp-attachment.",
  "y": "background"
 },
 {
  "id": "36436e1d8a3f1d65fcc369649341f2_1",
  "x": "Apart from the empirical work in Tilburg and Antwerp, a number of recent studies on statistical natural language processing (e.g. Dagan & Lee, 1997; <cite>Collins & Brooks, 1995)</cite> also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low-frequency events, is harmful to generalization accuracy. After reviewing this empirical work briefly, I will report on new results (work in progress in collaboration with van den Bosch and Zavrel), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks: tagging, grapheme-to-phoneme conversion, and pp-attachment. The results show that forgetting individual training items, however \"improbable' they may be, is indeed harmful.",
  "y": "similarities"
 },
 {
  "id": "36436e1d8a3f1d65fcc369649341f2_2",
  "x": "Lazy Learning with a simple similarity metric based on information entropy (IB I-IG, Daelemarts & van den Bosch, 1992 consistently outperforms abstracting (greedy) learning techniques such as C5.0 or backprop learning on a broad selection of natural language processing tasks ranging from phonology to semantics. Our intuitive explanation for this result is that lazy learning techniques keep all training items, whereas greedy approaches lose useful information by forgetting low-frequency or exceptional instances of the task, not covered by the extracted rules or models (Daelemans, 1996) . Apart from the empirical work in Tilburg and Antwerp, a number of recent studies on statistical natural language processing (e.g. Dagan & Lee, 1997; <cite>Collins & Brooks, 1995)</cite> also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low-frequency events, is harmful to generalization accuracy.",
  "y": "background"
 },
 {
  "id": "36436e1d8a3f1d65fcc369649341f2_3",
  "x": "Our intuitive explanation for this result is that lazy learning techniques keep all training items, whereas greedy approaches lose useful information by forgetting low-frequency or exceptional instances of the task, not covered by the extracted rules or models (Daelemans, 1996) . Apart from the empirical work in Tilburg and Antwerp, a number of recent studies on statistical natural language processing (e.g. Dagan & Lee, 1997; <cite>Collins & Brooks, 1995)</cite> also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low-frequency events, is harmful to generalization accuracy. After reviewing this empirical work briefly, I will report on new results (work in progress in collaboration with van den Bosch and Zavrel), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks: tagging, grapheme-to-phoneme conversion, and pp-attachment.",
  "y": "extends"
 },
 {
  "id": "370da04cbb2a6ab807428f7e058110_0",
  "x": "Since most of the syntactic relations occur between adjacent words, co-occurrence networks can be seen as simplified versions of syntactic networks [18] . Several patterns have been identified in co-occurrence networks formed from large corpora, such as the power-law regimes for degrees distribution [2] and core-periphery structure [20] resulting from the complex organization of the lexicon. The overall structure and dynamics of networks representing texts have been modeled to describe their mechanism of growth and attachment [21, 22] , while nuances in the topology of real networks were exploited in practical problems, including natural language processing [23, 24,<cite> 25]</cite> .",
  "y": "background"
 },
 {
  "id": "370da04cbb2a6ab807428f7e058110_1",
  "x": "In this study, we use the co-occurrence representation to probe how the variation of network topology along a text is able to identify an author's style. Writing style is more subjective than other text characteristics (e.g. topic), making authorship recognition one of the most challenging text mining tasks [26, 27] . It is crucial for practical applications such as text classification<cite> [25]</cite> , copyright resolution [28] , identification of terrorist messages [29] and of plagiarism [26] .",
  "y": "background"
 },
 {
  "id": "370da04cbb2a6ab807428f7e058110_2",
  "x": "Methods from statistical physics have also been used for authorship recognition [31, 32] , which in recent years included text modeling with co-occurrence networks [33, 34, 35, 36, 37, 38] . The adequacy of co-occurrence networks for the task was confirmed for the first time with the correlation between network topology and authors' styles<cite> [25]</cite> . Despite this relative success, some issues concerning the applicability of network methods remain unsolved.",
  "y": "background"
 },
 {
  "id": "370da04cbb2a6ab807428f7e058110_3",
  "x": "The adequacy of co-occurrence networks for the task was confirmed for the first time with the correlation between network topology and authors' styles<cite> [25]</cite> . Despite this relative success, some issues concerning the applicability of network methods remain unsolved. A major issue in network representation is that regular patterns among concepts only emerge when large pieces of texts are available.",
  "y": "motivation background"
 },
 {
  "id": "370da04cbb2a6ab807428f7e058110_4",
  "x": "The co-occurrence networks were constructed with each distinct word becoming a node and two words being linked if they were adjacent in the pre-processed text<cite> [25]</cite> . The link is directed from the word appearing first to the second word and is weighted by the number of times the pair is found in the text. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "370da04cbb2a6ab807428f7e058110_5",
  "x": "Each partition is described by the following topological network measurements: clustering coefficient C, which gives the fraction of possible triangles that exist for a particular node; network diameter D, which is the largest of all shortest paths (max{S ij }); network radius R, which is the smallest of all shortest paths (min{S ij }); number of cliques (complete subgraphs) C q ; load centrality L, similar to betweenness centrality but considering weights on edges; network transitivity T, which measures the fraction of all connected triples which are in fact triangles, T = 3 \u00d7 triangles/triads; betweenness centrality B, which measures how many shortest paths pass through a given node; shortest path length S, which is the shortest number of edges between two nodes; degree K or connectivity (number of edges) of a node; intermittency I, which measures how periodically a word is repeated [43] ; total number of nodes N (i.e. vocabulary size); and total number of edges E. Even though intermittency is not a traditional network measurement, we considered it because of its strong relationship with the concept of cycle length in networks. Moreover, this measurement has been proven useful for analyzing text styles<cite> [25]</cite> . The metrics D, R, C q , T, N and E are scalar values for a network, while the other measurements are computed for each node individually.",
  "y": "uses background"
 },
 {
  "id": "370da04cbb2a6ab807428f7e058110_6",
  "x": "One should highlight the betweenness centrality B, which was extensively used by KNN, NB and RBFN even though its mean value (i.e. first moment, and the leftmost column under the B label on figure 3(d) ) was not used by these algorithms. Two last combinations of attributes were constructed. The first moments \u00b5 1 represent the static metrics previously studied (see e.g. <cite>[25,</cite> 51] ) and define a subset of 12 attributes.",
  "y": "uses"
 },
 {
  "id": "370da04cbb2a6ab807428f7e058110_7",
  "x": "Even though a direct comparison to related works requires using the same text collection, two examples using collections with similar characteristics which use static network metrics are worth mentioning. A similar study for the same task<cite> [25]</cite> analyzed 40 texts from 8 authors in English reaching a success score of 65%. In another work, 36 Persian books from 5 authors were classified with an accuracy rate of 77.8% [51] .",
  "y": "similarities"
 },
 {
  "id": "3881903212a2d0fea039c8967ab553_0",
  "x": "**MOTIVATION** The motivation for this paper stems from prior work done by the first author in collaboration with other researchers (Prabhakaran et al., 2013a; <cite>Prabhakaran et al., 2013b)</cite> . Prabhakaran et al. (2013a) introduced the notion of power in the domain of presidential debates, and<cite> Prabhakaran et al. (2013b)</cite> followed it up with an automatic power ranker system based on interactions within the debates.",
  "y": "motivation"
 },
 {
  "id": "3881903212a2d0fea039c8967ab553_1",
  "x": "**MOTIVATION** The motivation for this paper stems from prior work done by the first author in collaboration with other researchers (Prabhakaran et al., 2013a; <cite>Prabhakaran et al., 2013b)</cite> . Prabhakaran et al. (2013a) introduced the notion of power in the domain of presidential debates, and<cite> Prabhakaran et al. (2013b)</cite> followed it up with an automatic power ranker system based on interactions within the debates.",
  "y": "background"
 },
 {
  "id": "3881903212a2d0fea039c8967ab553_2",
  "x": "We also find that this fluctuation in topic shifting tendencies is significantly correlated with the candidates' power. As an additional contribution of this paper, we demonstrate the utility of our topic shift features extracted using both types of SITS-based analyses in improving the performance of the automatic power ranker system presented in<cite> (Prabhakaran et al., 2013b)</cite> . We also investigated the utility of topic shifting features described in (Prabhakaran et al., 2014) extracted using LDA based topic modeling.",
  "y": "differences"
 },
 {
  "id": "3881903212a2d0fea039c8967ab553_3",
  "x": "For our experiments, we use the SVM rank based supervised learned power ranker presented in that work to estimate this ranking function. As we do in<cite> (Prabhakaran et al., 2013b)</cite> , we here report Kendall's Tau and Normalized Discounted Cumulative Gain values (NDCG and NDCG@3) on 5-fold cross validation (at the debate level). All three metrics are based on the number of rank inversions between original and predicted ranking.",
  "y": "uses"
 },
 {
  "id": "3881903212a2d0fea039c8967ab553_4",
  "x": "NDCG@3 focuses only on the top 3 positions in the ranked list. We use the best performing feature set of<cite> (Prabhakaran et al., 2013b)</cite> posted the overall best system obtaining a Tau of 0.60, NDCG of 0.970, and NDCG@3 of 0.937. These results demonstrates the utility of topic shift features in the power ranking problem, especially using the SITS var formulation.",
  "y": "uses"
 },
 {
  "id": "38ad38f25a2823c64cd16bc9f2af93_0",
  "x": "This transcription bottleneck problem can be handled by translating into a widely spoken language to ensure subsequent interpretability of the collected recordings, and such parallel corpora have been recently created by aligning the collected audio with translations in a well-resourced language (Adda et al., 2016;<cite> Godard et al., 2017</cite>; Boito et al., 2018) . Moreover, some linguists suggested that more than one translation should be collected to capture deeper layers of meaning (Evans and Sasse, 2004) . This work is a contribution to the Computational Language Documentation (CLD) research field, that aims to replace part of the manual steps performed by linguists during language documentation initiatives by automatic approaches.",
  "y": "background"
 },
 {
  "id": "38ad38f25a2823c64cd16bc9f2af93_1",
  "x": "**METHODOLOGY** The Multilingual Mboshi Parallel Corpus: In this work we extend the bilingual Mboshi-French parallel corpus <cite>(Godard et al., 2017)</cite> , fruit of the documentation process of Mboshi (Bantu C25), an endangered language spoken in Congo-Brazzaville. The corpus contains 5,130 utterances, for which it provides audio, transcriptions and translations in French.",
  "y": "extends"
 },
 {
  "id": "38ad38f25a2823c64cd16bc9f2af93_2",
  "x": "Nous traduisons un corpus parall\u00e8le bilingue Mboshi-Fran\u00e7ais <cite>(Godard et al., 2017)</cite> dans quatre autres langues, et \u00e9valuons l'impact de la langue de traduction sur une t\u00e2che de segmentation en mots non supervis\u00e9e. Nos r\u00e9sultats sugg\u00e8rent que la langue de traduction peut influencer l\u00e9g\u00e8rement la qualit\u00e9 de segmentation. Cependant, combiner l'information apprise par diff\u00e9rents mod\u00e8les bilingues nous permet d'am\u00e9liorer ces r\u00e9sultats de mani\u00e8re marginale.",
  "y": "extends"
 },
 {
  "id": "38ad38f25a2823c64cd16bc9f2af93_3",
  "x": "Recently, collecting aligned translations in well-resourced languages became a popular solution for ensuring posterior interpretability of the recordings (Adda et al., 2016) . In this paper we investigate language-related impact in automatic approaches for computational language documentation. We translate the bilingual Mboshi-French parallel corpus <cite>(Godard et al., 2017)</cite> into four other languages, and we perform bilingual-rooted unsupervised word discovery.",
  "y": "extends"
 },
 {
  "id": "392cbe849c1b8a69aae9923ade41aa_0",
  "x": "While most of them assign grammatical functions on top of constituency trees (Blaheta and Charniak, 2000; Jijkoun and de Rijke, 2004; Chrupa\u0142a and van Genabith, 2006; Klenner, 2007; Seeker et al., 2010) , less work has tried to predict GF labels for unlabelled dependency trees. One of them is McDonald et al. (2006) who first generate the unlabelled trees using a graph-based parser, and then model the assignment of dependency labels as a sequence labelling task. Another approach has been proposed by <cite>Zhang et al. (2017)</cite> who present a simple, yet efficient and accurate parsing model that generates unlabelled trees by identifying the most probable head for each token in the input.",
  "y": "background"
 },
 {
  "id": "392cbe849c1b8a69aae9923ade41aa_1",
  "x": "Dependency Parsing as Head Selection Our labelling model is an extension of the parsing model of <cite>Zhang et al. (2017)</cite> . We use our own implementation of the head-selection parser and focus on the grammatical function labelling part. The parser uses a bidirectional LSTM to extract a dense, positional representation a i of the word w i at position i in a sentence:",
  "y": "extends"
 },
 {
  "id": "392cbe849c1b8a69aae9923ade41aa_2",
  "x": "Then, in a post-processing step, they assign labels to each head-dependent pair, using a two-layer rectifier network. Dependency Parsing as Head Selection Our labelling model is an extension of the parsing model of <cite>Zhang et al. (2017)</cite> . We use our own implementation of the head-selection parser and focus on the grammatical function labelling part.",
  "y": "extends uses"
 },
 {
  "id": "392cbe849c1b8a69aae9923ade41aa_3",
  "x": "An additional classifier with two rectified hidden layers is used to predict dependency labels, and is trained separately from the unlabeled parsing component, in a pipeline architecture. The classifier predictions are based on the representations of the head and the dependent, b j and b i , which are the concatenation of the input and the bidirectional LSTM-based representations: Despite its simplicity and the lack of global optimisation, <cite>Zhang et al. (2017)</cite> report competitive results for English, Czech, and German.",
  "y": "background"
 },
 {
  "id": "392cbe849c1b8a69aae9923ade41aa_4",
  "x": "**LABELING DEPENDENCIES WITH HISTORY** Although the labelling approach in <cite>Zhang et al. (2017)</cite> is simple and efficient, looking at head and dependent only when assigning the labels comes with some disadvantages. First, some labels are easier to predict when we also take context into account, e.g. the parent and grandparent nodes or the siblings of the head or dependent.",
  "y": "background motivation"
 },
 {
  "id": "392cbe849c1b8a69aae9923ade41aa_5",
  "x": "Although the labelling approach in <cite>Zhang et al. (2017)</cite> is simple and efficient, looking at head and dependent only when assigning the labels comes with some disadvantages. To address this issue, we propose an extended labelling model that incorporates a decision history.",
  "y": "motivation"
 },
 {
  "id": "392cbe849c1b8a69aae9923ade41aa_6",
  "x": "Our interest is focussed on German, but to put our work in context, we follow <cite>Zhang et al. (2017)</cite> and report results also for English, which has a configurational word order, and for Czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than German. For English, we use the Penn Treebank (PTB) (Marcus et al., 1993) with standard training/dev/test splits. The POS tags are assigned using the Stanford POS tagger (Toutanova et al., 2003) with ten-way jackknifing, and constituency trees are converted to Stanford basic dependencies (De Marneffe et al., 2006) .",
  "y": "uses"
 },
 {
  "id": "392cbe849c1b8a69aae9923ade41aa_7",
  "x": "The POS tags are assigned using the Stanford POS tagger (Toutanova et al., 2003) with ten-way jackknifing, and constituency trees are converted to Stanford basic dependencies (De Marneffe et al., 2006) . The German and Czech data come from the CoNLL-X shared task (Buchholz and Marsi, 2006) and our data split follows <cite>Zhang et al. (2017)</cite> . As the CoNLL-X testsets are rather small (\u223c 360 sentences), we also",
  "y": "uses"
 },
 {
  "id": "392cbe849c1b8a69aae9923ade41aa_8",
  "x": "We test different labelling models on top of the unlabelled trees produced by our re-implementation of the parsing as head selection model ( \u00a72). We first train the unlabelled parsing models for the three languages. Unless stated otherwise, all parameters are set according to <cite>Zhang et al. (2017)</cite> , and tag embedding size was set to 40 for all languages.",
  "y": "uses"
 },
 {
  "id": "392cbe849c1b8a69aae9923ade41aa_9",
  "x": "Please note that we do not use pre-trained embeddings in our experiments. In the next step, we train four different labelling models: the labeller of <cite>Zhang et al. (2017)</cite> that uses a rectifier neural network with two hidden layers (baseline), two bidirectional LSTM models (BILSTM(L) and BILSTM(B)), and one tree LSTM model (TREELSTM) ( \u00a73). The hidden layer dimension in all LSTM models was set to 200.",
  "y": "uses"
 },
 {
  "id": "392cbe849c1b8a69aae9923ade41aa_10",
  "x": "All history-based labelling models perform significantly better than the local baseline model, 1 but for English the improvements are smaller (0.3%) than for the nonconfigurational languages (\u223c0.7%). While we tried to reimplement the model of <cite>Zhang et al. (2017)</cite> following the details in the paper, our reimplemented model yields higher scores for German, compared to the results in the paper. The scores for English are slightly lower since, in contrast to <cite>Zhang et al. (2017)</cite> , we do not use pre-trained embeddings.",
  "y": "differences"
 },
 {
  "id": "392cbe849c1b8a69aae9923ade41aa_11",
  "x": "All history-based labelling models perform significantly better than the local baseline model, 1 but for English the improvements are smaller (0.3%) than for the nonconfigurational languages (\u223c0.7%). While we tried to reimplement the model of <cite>Zhang et al. (2017)</cite> following the details in the paper, our reimplemented model yields higher scores for German, compared to the results in the paper. The scores for English are slightly lower since, in contrast to <cite>Zhang et al. (2017)</cite> , we do not use pre-trained embeddings.",
  "y": "differences"
 },
 {
  "id": "392cbe849c1b8a69aae9923ade41aa_12",
  "x": "On the SPMRL 2014 shared task data, our results are only 0.3% lower than the ones of the winning system (Bj\u00f6rkelund et al., 2014) fectiveness of our models, we also ran our labeller on the unlabelled output of the SPMRL 2014 winning system and on unlabelled gold trees. On the output of the blended system LAS slightly improves from 88.62% to 88.76% (TREELSTM). 3 When applied to unlabelled gold trees, the distance between our models and the baseline becomes larger and the best of our history-based models (BILSTM(B), 97.38%) outperforms the original labeller of <cite>Zhang et al. (2017)</cite> (96.15%) by more than 1%.",
  "y": "differences"
 },
 {
  "id": "392cbe849c1b8a69aae9923ade41aa_13",
  "x": "**CONCLUSIONS** We have shown that GF labelling, which is of crucial importance for languages like German, can be improved by combining LSTM models with a decision history. All our models outperform the original labeller of <cite>Zhang et al. (2017)</cite> and give results in the same range as the best system from the SPMRL-2014 shared task (without the reranker), but with a much simpler model.",
  "y": "differences"
 },
 {
  "id": "397e593f8f282d4951402d83036c12_0",
  "x": "---------------------------------- **INTRODUCTION** End-to-end neural machine translation (NMT) (Sutskever et al., 2014; <cite>Bahdanau et al., 2015)</cite> has gained increasing popularity in the machine translation community.",
  "y": "background"
 },
 {
  "id": "397e593f8f282d4951402d83036c12_1",
  "x": "Capable of capturing longdistance dependencies with gating (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) and attention<cite> (Bahdanau et al., 2015)</cite> mechanisms, NMT has proven to outperform conventional statistical machine translation systematically across a variety of language pairs (Junczys-Dowmunt et al., 2016) . This paper introduces THUMT, an open-source toolkit developed by the Tsinghua Natural Language Processing Group. On top of Theano (Bergstra et al., 2010) , THUMT implements the standard attention-based encoder-decoder framework for NMT<cite> (Bahdanau et al., 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "397e593f8f282d4951402d83036c12_2",
  "x": "On top of Theano (Bergstra et al., 2010) , THUMT implements the standard attention-based encoder-decoder framework for NMT<cite> (Bahdanau et al., 2015)</cite> . It sup- * Corresponding author: Yang Liu. ports three training criteria: maximum likelihood estimation<cite> (Bahdanau et al., 2015)</cite> , minimum risk training , and semisupervised training .",
  "y": "uses"
 },
 {
  "id": "397e593f8f282d4951402d83036c12_4",
  "x": "On top of Theano (Bergstra et al., 2010) , THUMT implements the standard attention-based encoder-decoder framework for NMT<cite> (Bahdanau et al., 2015)</cite> . It sup- * Corresponding author: Yang Liu. ports three training criteria: maximum likelihood estimation<cite> (Bahdanau et al., 2015)</cite> , minimum risk training , and semisupervised training .",
  "y": "uses"
 },
 {
  "id": "397e593f8f282d4951402d83036c12_5",
  "x": "We compare THUMT with the state-of-the-art opensource toolkit GroundHog<cite> (Bahdanau et al., 2015)</cite> and achieve significant improvements on ChineseEnglish translation tasks by introducing new training criteria and optimizers. 2 The Toolkit 2.1 Model THUMT implements the standard attention-based NMT model<cite> (Bahdanau et al., 2015)</cite> on top of Theano (Bergstra et al., 2010) . Please refer to<cite> (Bahdanau et al., 2015)</cite> for more details.",
  "y": "differences"
 },
 {
  "id": "397e593f8f282d4951402d83036c12_6",
  "x": "We compare THUMT with the state-of-the-art opensource toolkit GroundHog<cite> (Bahdanau et al., 2015)</cite> and achieve significant improvements on ChineseEnglish translation tasks by introducing new training criteria and optimizers. 2 The Toolkit 2.1 Model THUMT implements the standard attention-based NMT model<cite> (Bahdanau et al., 2015)</cite> on top of Theano (Bergstra et al., 2010) . Please refer to<cite> (Bahdanau et al., 2015)</cite> for more details.",
  "y": "uses"
 },
 {
  "id": "397e593f8f282d4951402d83036c12_7",
  "x": "2 The Toolkit 2.1 Model THUMT implements the standard attention-based NMT model<cite> (Bahdanau et al., 2015)</cite> on top of Theano (Bergstra et al., 2010) . Please refer to<cite> (Bahdanau et al., 2015)</cite> for more details. ----------------------------------",
  "y": "background"
 },
 {
  "id": "397e593f8f282d4951402d83036c12_8",
  "x": "THUMT supports three training criteria: 1. Maximum likelihood estimation (MLE)<cite> (Bahdanau et al., 2015)</cite> : the default training criterion in THUMT, which aims to find a set of model parameters that maximizes the likelihood of training data. 2. Minimum Risk Training (MRT) : the recommended training criterion in THUMT, which aims to find a set of model parameters that minimizes the risk (i.e., expected loss measured by evaluation metrics) on training data.",
  "y": "uses"
 },
 {
  "id": "397e593f8f282d4951402d83036c12_9",
  "x": "We follow Luong et al. (2015) to address unknown words. In our implementation, we use FastAlign (Dyer et al., 2013) (Papineni et al., 2002) score. Our baseline system is GroundHog<cite> (Bahdanau et al., 2015)</cite> , a state-of-the-art open-source NMT toolkit.",
  "y": "uses"
 },
 {
  "id": "3a7625a0f38424fe922ad095e07e68_0",
  "x": "The authors' argued that because large quantities of real data are available, models generalize properly without augmentation. Although training using augmented text data is rare, generating new questions about images has been studied. The COCO-QA dataset<cite> (Ren et al., 2015)</cite> for VQA was created by parsing COCO captions with a syntactic parser, and then used this to create QA pairs for four kinds of questions using hand-crafted rules.",
  "y": "background"
 },
 {
  "id": "3a7625a0f38424fe922ad095e07e68_1",
  "x": "We conduct experiments on two of the most popular VQA datasets: 'The VQA Dataset' (Antol et al., 2015) and COCO-QA<cite> (Ren et al., 2015)</cite> . 'The VQA Dataset' is currently the most popular VQA dataset and it contains both synthetic and real-world images. The real-world images are from the COCO dataset (Lin et al., 2014) .",
  "y": "uses"
 },
 {
  "id": "3a7625a0f38424fe922ad095e07e68_2",
  "x": "COCO-QA<cite> (Ren et al., 2015)</cite> also uses images from COCO, with the questions generated by an NLP algorithm that uses COCO's captions. All questions belong to four categories: object, number, color, and location. Many algorithms have been proposed for VQA.",
  "y": "background"
 },
 {
  "id": "3a7625a0f38424fe922ad095e07e68_3",
  "x": "The authors' argued that because large quantities of real data are available, models generalize properly without augmentation. Although training using augmented text data is rare, generating new questions about images has been studied. The COCO-QA dataset<cite> (Ren et al., 2015)</cite> for VQA was created by parsing COCO captions with a syntactic parser, and then used this to create QA pairs for four kinds of questions using hand-crafted rules.",
  "y": "background"
 },
 {
  "id": "3a7625a0f38424fe922ad095e07e68_4",
  "x": "Although training using augmented text data is rare, generating new questions about images has been studied. The COCO-QA dataset<cite> (Ren et al., 2015)</cite> for VQA was created by parsing COCO captions with a syntactic parser, and then used this to create QA pairs for four kinds of questions using hand-crafted rules. However, due to inability of the algorithm to cope with complex sentence structures, a significant portion of COCO-QA questions have grammatical errors or are oddly phrased.",
  "y": "motivation"
 },
 {
  "id": "3a7625a0f38424fe922ad095e07e68_5",
  "x": "Detailed reviews of existing methods can be found in Kafle and Kanan (2017) and Wu et al. (2016) . However, simpler models such as linear classifiers and multilayer perceptrons (MLPs) perform only slightly worse on many VQA datasets. These baseline methods predict the answer using a vector of image features concatenated to a vector of question features<cite> (Ren et al., 2015</cite>; Zhou et al., 2015; Kafle and Kanan, 2016) .",
  "y": "background"
 },
 {
  "id": "3a7625a0f38424fe922ad095e07e68_6",
  "x": "CNN features from ResNet-152 and the skip-thought vectors<cite> (Kiros et al., 2015)</cite> are used as image and question features respectively. We evaluate the MLP model on COCO-VQA and COCO-QA datasets. For COCO-QA, we excluded all the augmented QA pairs derived from COCO's validation images during training, as the test portion of COCO-QA contains questions for these images.",
  "y": "background"
 },
 {
  "id": "3a7f65a63e875db3e6d722a695aa5a_0",
  "x": "The current system is backed by the EasyCCG parser <cite>(Lewis and Steedman, 2014)</cite> , slightly modified to allow for incorporating constraints, and other CCG parsers could be plugged in with similar modifications. To do this, the annotator clicks on the category and changes it, as shown in the figure. When they hit enter or click somewhere else, the sentence is automatically parsed again in the background, this time with the lexical category constraint that go has category (S[b]\\ NP)/ PP.",
  "y": "extends"
 },
 {
  "id": "3a7f65a63e875db3e6d722a695aa5a_1",
  "x": "Span Constraints Although constraining lexical categories is often enough to determine the entire CCG derivation (cf. Bangalore and Joshi, 1999;<cite> Lewis and Steedman, 2014)</cite> , this is not always the case. For example, consider the sentence I want to be a millionaire like my dad. Assuming that like my dad is a verb phrase modifier (category (S \\ NP)\\(S \\ NP)), it could attach to either to be or want, giving very different meanings (cf. Zimmer, 2013). We therefore implemented one other type of edit operation/constraint: span constraints.",
  "y": "background"
 },
 {
  "id": "3b0a82129333203eca96a7473095f3_0",
  "x": "In a recent paper advocating a corpus-based and probabilistic approach to grammar development, <cite>Black, Lafferty, and Roukos (1992)</cite> argue that \"the current state of the art is far from being able to produce a robust parser of general English\" and advocate \"steady and quantifiable,\" empirically corpus-driven grammar development and testing. <cite>Black et al.</cite> are addressing a community in which armchair introspection has been and still is the dominant methodology in many quarters, but in some parts of Europe, corpus linguistics never died. For nearly two decades, the Nijmegen group led by Jan Aarts have been undertaking corpus analyses that, although motivated primarily by the desire to study language variation using corpus data, are particularly relevant to the issue of broad-coverage grammar development.",
  "y": "background motivation"
 },
 {
  "id": "3b0a82129333203eca96a7473095f3_1",
  "x": "**ABSTRACT** In a recent paper advocating a corpus-based and probabilistic approach to grammar development, <cite>Black, Lafferty, and Roukos (1992)</cite> argue that \"the current state of the art is far from being able to produce a robust parser of general English\" and advocate \"steady and quantifiable,\" empirically corpus-driven grammar development and testing. <cite>Black et al.</cite> are addressing a community in which armchair introspection has been and still is the dominant methodology in many quarters, but in some parts of Europe, corpus linguistics never died.",
  "y": "background"
 },
 {
  "id": "3b0a82129333203eca96a7473095f3_2",
  "x": "**ABSTRACT** In a recent paper advocating a corpus-based and probabilistic approach to grammar development, <cite>Black, Lafferty, and Roukos (1992)</cite> argue that \"the current state of the art is far from being able to produce a robust parser of general English\" and advocate \"steady and quantifiable,\" empirically corpus-driven grammar development and testing. <cite>Black et al.</cite> are addressing a community in which armchair introspection has been and still is the dominant methodology in many quarters, but in some parts of Europe, corpus linguistics never died.",
  "y": "background"
 },
 {
  "id": "3b0a82129333203eca96a7473095f3_3",
  "x": "This task forms a significant component of the wider Tools for Syntactic Corpus Analysis (TOSCA) project being undertaken at Nijmegen. Oostdijk's work provides an excellent example of the strengths and weaknesses of the approach advocated by <cite>Black et al.</cite> In addition, she discusses issues such as sampling and tokenization of corpus material, as well as the exploitation of the analyzed corpus in studies of language variation. However, in this review I will concentrate on the central core of her book: the development of the grammar and performance of the associated parser, since this is the part that is most relevant to computational linguistics.",
  "y": "background"
 },
 {
  "id": "3b0a82129333203eca96a7473095f3_4",
  "x": "Parse selection is done interactively by guiding the parser manually; Oostdijk justifies this approach by arguing that it ensures a high level of accuracy and guarantees parsing efficiency by pre-empting unnecessary Computational Linguistics Volume 19, Number 1 search. An approach in which intervention is limited to selection between predefined legitimate analyses is an improvement on one in which the analyst is able to create new descriptions at will (e.g., Leech and Garside 1991) in that the resulting database of analyses will be consistent and intervention will be simpler and faster. However, other approaches are possible, such as the use of probabilities to guide parse selection, if not grammar induction (e.g., <cite>Black et al. 1992</cite> ).",
  "y": "background"
 },
 {
  "id": "3b0a82129333203eca96a7473095f3_5",
  "x": "More discussion is devoted to comparison with the approach to corpus analysis taken by the Lancaster group (Garside et al. 1987) ; Oostdijk argues that because their espousal of probabilistic methods and rejection of a rule-based generative approach is not founded on sound empirical evidence, it is impossible to develop a comprehensive generative grammar for a corpus. While I am sympathetic to Oostdijk's position and think that the grammar she goes on to present is impressive enough to bias us towards the opposite conclusion, it is a mistake to accept the assumption that the two approaches are incompatible, as much recent work (including that of <cite>Black et al. 1992</cite>) has demonstrated the usefulness of combining statistical techniques with rule-based systems. The core of the book is a description of the grammar developed and analyses adopted for notoriously difficult phenomena, such as nonconstituent coordination, gapping, apposition, partitives, other noun phrase premodifier syntax, and so forth.",
  "y": "background"
 },
 {
  "id": "3bc48bea420e4977027832240450ec_0",
  "x": "Our primary and secondary contributions therefore, are to carry out the first study that reports results across all three different dataset classes, and to release a open source code framework implementing three complementary groups of TDSA methods. In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (Mitchell et al., 2013; Zhang et al., 2016; Liu and Zhang, 2017;<cite> Wang et al., 2017)</cite> but other papers have not released code (Wang et al., 2016; Tay et al., 2017) . In some cases, the code was initially made available, then removed, and is now back online (Tang et al., 2016a) .",
  "y": "background"
 },
 {
  "id": "3bc48bea420e4977027832240450ec_1",
  "x": "In some cases, when code has been released, it is difficult to use which could explain why the results were not reproduced. Of course, we would not expect researchers to produce industrial strength code, or provide continuing free ongoing support for multiple years after publication, but the situation is clearly problematic for the development of the new field in general. In this paper, we therefore reproduce three papers chosen as they employ widely differing methods: Neural Pooling (NP) , NP with dependency parsing<cite> (Wang et al., 2017)</cite> , and RNN (Tang et al., 2016a) , as well as having been applied largely to different datasets.",
  "y": "motivation"
 },
 {
  "id": "3bc48bea420e4977027832240450ec_2",
  "x": "Turning now to the focus of our investigations, Target Dependent sentiment analysis (TDSA) research (Nasukawa and Yi, 2003) arose as an extension to the coarse grained analysis of document level sentiment analysis (Pang et al., 2002; Turney, 2002) . Since its inception, papers have applied different methods such as feature based (Kiritchenko et al., 2014) , Recursive Neural Networks (RecNN) (Dong et al., 2014) , Recurrent Neural Networks (RNN) (Tang et al., 2016a) , attention applied to RNN (Wang et al., 2016; Chen et al., 2017; Tay et al., 2017) , Neural Pooling (NP)<cite> Wang et al., 2017)</cite> , RNN combined with NP (Zhang et al., 2016) , and attention based neural networks (Tang et al., 2016b) . Others have tackled TDSA as a joint task with target extraction, thus treating it as a sequence labelling problem.",
  "y": "background"
 },
 {
  "id": "3bc48bea420e4977027832240450ec_3",
  "x": "<cite>Wang et al. (2017)</cite> extended the work of by using the dependency linked words from the target. Dong et al. (2014) used the dependency tree to create a Recursive Neural Network (RecNN) inspired by Socher et al. (2013) but compared to Socher et al. (2013) they also utilised the dependency tags to create an Adaptive RecNN (ARecNN). Critically, the methods reported above have not been applied to the same datasets, therefore a true comparative evaluation between the different methods is somewhat difficult.",
  "y": "background"
 },
 {
  "id": "3bc48bea420e4977027832240450ec_6",
  "x": "For each of the experiments below we used the following configurations unless otherwise stated: we performed 5 fold stratified cross validation, features are scaled using Max Min scaling before inputting into the SVM, and used the respective C-Values for the SVM stated in the paper for each of the models. One major difficulty with the description of the method in the paper and re-implementation is handling the same target multiple appearances issue as originally raised by <cite>Wang et al. (2017)</cite> . As the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use.",
  "y": "background"
 },
 {
  "id": "3bc48bea420e4977027832240450ec_7",
  "x": "As the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use. We therefore took the approach of <cite>Wang et al. (2017)</cite> and found all of the features for each appearance and performed median pooling over features. This change could explain the subtle differences between the results we report and those of the original paper.",
  "y": "uses"
 },
 {
  "id": "3bc48bea420e4977027832240450ec_9",
  "x": "---------------------------------- **REPRODUCTION OF WANG ET AL. (2017)** <cite>Wang et al. (2017)</cite> extended the NP work of and instead of using the full tweet/sentence/text contexts they used the full dependency graph of the target word.",
  "y": "background"
 },
 {
  "id": "3bc48bea420e4977027832240450ec_10",
  "x": "The experiments are performed on the Dong et al. (2014) and <cite>Wang et al. (2017)</cite> Twitter datasets where we train and test on the previously specified train and test splits. We also scale our features using Max Min scaling before inputting into the SVM. We used all three sentiment lexicons as in the original paper, and we found the C-Value by performing 5 fold stratified cross validation on the training datasets.",
  "y": "uses"
 },
 {
  "id": "3c4c0875593ed0f196f5295fbaeb37_1",
  "x": "The use of such metrics is, however, only sensible if they are known to be sufficiently correlated with human preferences, which is not the case, as we show in the most complete study to date, across metrics, systems, datasets and domains. We evaluate three end-to-end NLG systems: RNNLG<cite> (Wen et al., 2015)</cite> , TGen (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2015) and LOLS (Lampouras and Vlachos, 2016) , using a large number of 21 automated metrics.",
  "y": "uses"
 },
 {
  "id": "3c4c0875593ed0f196f5295fbaeb37_2",
  "x": "However, these metrics can be easily manipulated with grammatically correct and easily readable output that is unrelated to the input. Our study clearly demonstrates the need for more advanced metrics, as used in related fields, e.g. MT (Specia et al., 2010). Recent advances in corpus-based NLG (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2015; <cite>Wen et al., 2015</cite>; Mei et al., 2016; Wen et al., 2016; Du\u0161ek and Jur\u010d\u00ed\u010dek, 2016; Lampouras and Vlachos, 2016) require costly training data, consisting of meaning representations (MRs) paired with corresponding NL texts.",
  "y": "motivation"
 },
 {
  "id": "3c4c0875593ed0f196f5295fbaeb37_3",
  "x": "Our study clearly demonstrates the need for more advanced metrics, as used in related fields, e.g. MT (Specia et al., 2010). Recent advances in corpus-based NLG (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2015; <cite>Wen et al., 2015</cite>; Mei et al., 2016; Wen et al., 2016; Du\u0161ek and Jur\u010d\u00ed\u010dek, 2016; Lampouras and Vlachos, 2016) require costly training data, consisting of meaning representations (MRs) paired with corresponding NL texts. In our work, we propose a novel framework for crowdsourcing high quality NLG training data, using automatic quality control measures and evaluating two types of MRs, pictorial and textual, used to elicit data (Novikova and Rieser, 2016) .",
  "y": "motivation differences"
 },
 {
  "id": "3c4c0875593ed0f196f5295fbaeb37_4",
  "x": "To address (1), we filter the crowdsourced data using a combination of automatic and semimanual validation procedures, as described in (Novikova and Rieser, 2016) . We validate the data by selecting native English participants, allowing only well formed English sentences to be submitted, and measuring the semantic similarity of a collected NL utterance and an associated MR. Using this framework, we collected a dataset of 50k instances in the restaurant domain, which is 10 times bigger than datasets currently used for NLG training, e.g. SFRest and SFHot<cite> (Wen et al., 2015)</cite> or Bagel (Mairesse et al., 2010) .",
  "y": "differences"
 },
 {
  "id": "3c4c0875593ed0f196f5295fbaeb37_6",
  "x": "We evaluate three end-to-end NLG systems: RNNLG<cite> (Wen et al., 2015)</cite> , TGen (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2015) and LOLS (Lampouras and Vlachos, 2016) , using a large number of 21 automated metrics. The metrics are divided into groups of word-based metrics (WBMs, such as TER (Snover et al., 2006) , BLEU (Papineni et al., 2002) , ROUGE (Lin, 2004) , semantic similarity (Han et al., 2013) reliability, we calculate the Spearman correlation between the metrics and human ratings for the same natural language (NL) utterances, the accuracy of relative rankings and conduct a detailed error analysis.",
  "y": "uses"
 },
 {
  "id": "3c4c0875593ed0f196f5295fbaeb37_7",
  "x": "However, these metrics can be easily manipulated with grammatically correct and easily readable output that is unrelated to the input. Our study clearly demonstrates the need for more advanced metrics, as used in related fields, e.g. MT (Specia et al., 2010) . Recent advances in corpus-based NLG (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2015; <cite>Wen et al., 2015</cite>; Mei et al., 2016; Wen et al., 2016; Du\u0161ek and Jur\u010d\u00ed\u010dek, 2016; Lampouras and Vlachos, 2016) require costly training data, consisting of meaning representations (MRs) paired with corresponding NL texts.",
  "y": "motivation"
 },
 {
  "id": "3c4c0875593ed0f196f5295fbaeb37_8",
  "x": "Our study clearly demonstrates the need for more advanced metrics, as used in related fields, e.g. MT (Specia et al., 2010) . Recent advances in corpus-based NLG (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2015; <cite>Wen et al., 2015</cite>; Mei et al., 2016; Wen et al., 2016; Du\u0161ek and Jur\u010d\u00ed\u010dek, 2016; Lampouras and Vlachos, 2016) require costly training data, consisting of meaning representations (MRs) paired with corresponding NL texts. In our work, we propose a novel framework for crowdsourcing high quality NLG training data, using automatic quality control measures and evaluating two types of MRs, pictorial and textual, used to elicit data (Novikova and Rieser, 2016) .",
  "y": "motivation differences"
 },
 {
  "id": "3c4c0875593ed0f196f5295fbaeb37_9",
  "x": "We validate the data by selecting native English participants, allowing only well formed English sentences to be submitted, and measuring the semantic similarity of a collected NL utterance and an associated MR. Using this framework, we collected a dataset of 50k instances in the restaurant domain, which is 10 times bigger than datasets currently used for NLG training, e.g. SFRest and SFHot<cite> (Wen et al., 2015)</cite> or Bagel (Mairesse et al., 2010) . To evaluate the quality of the collected corpus, we analyse the data with regards to lexical richness and syntactic variation and compare our results to other popular datasets in similar domains, i.e. SFRest, SFHot and BAGEL.We use the Lexical Complexity Analyser (Lu, 2012) to measure various dimensions of lexical richness and variation, such as mean segmental type-token ratio (MSTTR) and lexical sophistication (LS).",
  "y": "differences"
 },
 {
  "id": "3c74f66c209335ea33dda38c203199_0",
  "x": "Do multilingual models mostly benefit from a better modeling of lexical entries or do they also learn to share more abstract linguistic categories? We focus on the case of language models (LM) trained on two languages, one of which (L1) is over-resourced with respect to the other (L2), and investigate whether the syntactic knowledge learned for L1 is transferred to L2. To this end we use the long-distance agreement benchmark recently introduced by <cite>Gulordava et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "3c74f66c209335ea33dda38c203199_1",
  "x": "A practical benefit of training models multilingually is to transfer knowledge from high-resource languages to lowresource ones and improve task performance in the latter. Here we aim at understanding how linguistic knowledge is transferred among languages, specifically at the syntactic level, which to our knowledge has not been studied so far. Assessing the syntactic abilities of monolingual neural LMs trained without explicit supervision has been the focus of several recent studies: Linzen et al. (2016) analyzed the performance of LSTM LMs at an English subject-verb agreement task, while <cite>Gulordava et al. (2018)</cite> extended the analysis to various long-range agreement patterns in different languages.",
  "y": "background"
 },
 {
  "id": "3c74f66c209335ea33dda38c203199_2",
  "x": "This is the least optimistic scenario for linguistic transfer but also the most controlled one. In future experiments we plan to study how transfer is affected by varying degrees of vocabulary overlap. Following the setup of <cite>Gulordava et al. (2018)</cite> , we train 2-layer LSTM models with embedding and hidden layers of 650 dimensions for 40 epochs.",
  "y": "uses"
 },
 {
  "id": "3c74f66c209335ea33dda38c203199_3",
  "x": "The trained models are evaluated on the Italian section of the syntactic benchmark provided by <cite>Gulordava et al. (2018)</cite> , which includes various non-trivial number agreement constructions. 2 Note that all models are trained on a regular corpus likelihood objective and do not receive any specific supervision for the syntactic tasks. Table 1 shows the results of our preliminary experiments.",
  "y": "uses"
 },
 {
  "id": "3d1f3980190048625ec93517ebffdc_0",
  "x": "We use the confidence of the classifier, a value in the range [0, 1], to group articles into bins. We call this value the propaganda index, since it reflects the probability for an article to have a propagandistic intent. We use four families of features: Word n-gram features We use tf.idf -weighted word [1, 3]-grams<cite> (Rashkin et al. 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "3d1f3980190048625ec93517ebffdc_1",
  "x": "NELA. We also integrate the NEws LAndscape (NELA) features (Horne, Khedr, and Adal 2018) : 130 content-based features collected from the existing literature that measure different aspects of a news article (e.g., sentiment, bias, morality, complexity). We evaluated proppy on data from<cite> Rashkin et al. (2017)</cite> in a binary setup of distinguishing propaganda vs nonpropaganda.",
  "y": "uses"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_0",
  "x": "First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the <cite>WinoBias probing corpus</cite>. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on <cite>WinoBias</cite> can be eliminated.",
  "y": "uses"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_1",
  "x": "Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on <cite>WinoBias</cite> can be eliminated. ---------------------------------- **INTRODUCTION**",
  "y": "differences"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_2",
  "x": "Our results show that ELMo embeddings perform unequally on male and female pronouns: male entities can be predicted from occupation words 14% more accurately than female entities. In addition, we examine how gender bias in ELMo propagates to the downstream applications. Specifically, we evaluate a state-of-the-art coreference resolution system ) that makes use of ELMo's contextual embeddings on <cite>WinoBias</cite> (<cite>Zhao et al., 2018a</cite>) , a coreference diagnostic <cite>dataset that</cite> evaluates whether systems behave differently on decisions involving male and female entities of stereotyped or anti-stereotyped occupations.",
  "y": "uses"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_3",
  "x": "We explore two different strategies: (1) a training-time data augmentation technique (<cite>Zhao et al., 2018a</cite>) , where we augment the corpus for training the coreference system with its genderswapped variant (female entities are swapped to male entities and vice versa) and, afterwards, retrain the coreference system; and (2) ---------------------------------- **RELATED WORK**",
  "y": "uses"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_4",
  "x": "For example, <cite>Zhao et al. (2018a)</cite> and Rudinger et al. (2018) show that coreference resolution systems relying on word embeddings encode such occupational stereotypes. In concurrent work, May et al. (2019) measure gender bias in sentence embeddings, but their evaluation is on the aggregation of word representations. In contrast, we analyze bias in contextualized word representations and its effect on a downstream task.",
  "y": "background"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_5",
  "x": "For example, <cite>Zhao et al. (2018a)</cite> and Rudinger et al. (2018) show that coreference resolution systems relying on word embeddings encode such occupational stereotypes. In contrast, we analyze bias in contextualized word representations and its effect on a downstream task.",
  "y": "differences"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_6",
  "x": "Table 1 lists the data analysis on the One Billion Word Benchmark (Chelba et al., 2013) corpus, the training corpus for ELMo. We show counts for the number of occurrences of male pronouns (he, his and him) and female pronouns (she and her) in the corpus as well as the co-occurrence of occupation words with those pronouns. We use the set of occupation words defined in the <cite>WinoBias corpus</cite> and their assignments as prototypically male or female (<cite>Zhao et al., 2018a</cite>) .",
  "y": "uses"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_7",
  "x": "We show counts for the number of occurrences of male pronouns (he, his and him) and female pronouns (she and her) in the corpus as well as the co-occurrence of occupation words with those pronouns. We use the set of occupation words defined in the <cite>WinoBias corpus</cite> and their assignments as prototypically male or female (<cite>Zhao et al., 2018a</cite>) . The analysis shows that the <cite>Billion Word corpus</cite> contains a significant skew with respect to gender: (1) male pronouns occur three times more than female pronouns and (2) male pronouns co-occur more frequently with occupation words, irrespective of whether they are prototypically male or female.",
  "y": "motivation"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_8",
  "x": "To visualize the gender subspace, we pick a few sentence pairs from <cite>WinoBias</cite> (<cite>Zhao et al., 2018a</cite>) . Each sentence in the corpus contains one gendered pronoun and two occupation words, such as \"The developer corrected the secretary because she made a mistake\" and also the same sentence with the opposite pronoun (he). In Figure 1 on the right, we project the ELMo embeddings of occupation words that are co-referent with the pronoun (e.g. secretary in the above example) for when the pronoun is male (blue dots) and female (orange dots) on the two principal components from the PCA analysis.",
  "y": "uses"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_9",
  "x": "Then we split all such instances into training and test, with 539 and 62 instances, respectively and augment these sentences by swapping all the gendered words with words of the opposite gender such that the numbers of male 1 We use the list collected in (<cite>Zhao et al., 2018a</cite>) and female entities are balanced. We first test if ELMo embedding vectors carry gender information. We train an SVM classifier with an RBF kernel 2 to predict the gender of a mention (i.e., an occupation word) based on its ELMo embedding.",
  "y": "uses"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_10",
  "x": "We evaluate bias with respect to the <cite>WinoBias</cite> dataset (<cite>Zhao et al., 2018a</cite>) , a benchmark of paired male and female coreference resolution examples following the Winograd format (Hirst, 1981; Rahman and Ng, 2012; Peng et al., 2015) . <cite>It</cite> contains two different subsets, pro-stereotype, where pronouns are associated with occupations predominately associated with the gender of the pronoun, or anti-stereotype, when the opposite relation is true. Table 2 : F1 on OntoNotes and <cite>WinoBias</cite> development sets.",
  "y": "uses"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_11",
  "x": "We evaluate bias with respect to the <cite>WinoBias</cite> dataset (<cite>Zhao et al., 2018a</cite>) , a benchmark of paired male and female coreference resolution examples following the Winograd format (Hirst, 1981; Rahman and Ng, 2012; Peng et al., 2015) . <cite>It</cite> contains two different subsets, pro-stereotype, where pronouns are associated with occupations predominately associated with the gender of the pronoun, or anti-stereotype, when the opposite relation is true. Table 2 : F1 on OntoNotes and <cite>WinoBias</cite> development sets.",
  "y": "background"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_12",
  "x": "<cite>It</cite> contains two different subsets, pro-stereotype, where pronouns are associated with occupations predominately associated with the gender of the pronoun, or anti-stereotype, when the opposite relation is true. Table 2 : F1 on OntoNotes and <cite>WinoBias</cite> development sets. <cite>WinoBias dataset</cite> is split Semantics Only and w/ Syntactic Cues subsets.",
  "y": "uses"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_13",
  "x": "Table 2 : F1 on OntoNotes and <cite>WinoBias</cite> development sets. <cite>WinoBias dataset</cite> is split Semantics Only and w/ Syntactic Cues subsets. ELMo improves the performance on the OntoNotes dataset by 5% but shows stronger bias on the <cite>WinoBias dataset.</cite>",
  "y": "background"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_14",
  "x": "ELMo improves the performance on the OntoNotes dataset by 5% but shows stronger bias on the <cite>WinoBias dataset.</cite> Avg. stands for averaged F1 score on the pro-and anti-stereotype subsets while \"Diff.\" is the absolute difference between these two subsets. * indicates the difference between pro/anti stereotypical conditions is significant (p < .05) under an approximate randomized test (Graham et al., 2014) .",
  "y": "uses"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_15",
  "x": "Gender bias is measured by taking the difference of the performance in pro-and antistereotypical subsets. Previous work (<cite>Zhao et al., 2018a</cite>) evaluated the systems based on GloVe embeddings but here we evaluate a state-of-the-art system that trained on the OntoNotes corpus with ELMo embeddings . ----------------------------------",
  "y": "background motivation"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_16",
  "x": "Next, we describe two methods for mitigating bias in ELMo for the purpose of coreference resolution: (1) a train-time data augmentation approach and (2) a test-time neutralization approach. <cite>Zhao et al. (2018a)</cite> propose a method to reduce gender bias in coreference resolution by augmenting the training corpus for this task.",
  "y": "background"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_17",
  "x": "In addition, <cite>they</cite> find it useful to also mitigate bias in supporting resources and therefore replace standard GloVe embeddings with bias mitigated word embeddings from Bolukbasi et al. (2016) . We evaluate the performance of both aspects of <cite>this approach</cite>. ----------------------------------",
  "y": "background"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_18",
  "x": "In addition, <cite>they</cite> find it useful to also mitigate bias in supporting resources and therefore replace standard GloVe embeddings with bias mitigated word embeddings from Bolukbasi et al. (2016) . We evaluate the performance of both aspects of <cite>this approach</cite>. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_19",
  "x": "Instead of augmenting training corpus by swapping gender words, we generate a gender-swapped version of the test instances. We then apply ELMo to obtain contextualized word representations of the original and the gender-swapped sentences and use their average as the final representations. Table 2 summarizes our results on <cite>WinoBias</cite>.",
  "y": "uses"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_20",
  "x": "---------------------------------- **RESULTS** ELMo Bias Transfers to Coreference Row 3 in Table 2 summarizes performance of the ELMo based coreference system on <cite>WinoBias</cite>.",
  "y": "uses"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_21",
  "x": "While ELMo helps to boost the coreference resolution F1 score (OntoNotes) it also propagates bias to the task. It exhibits large differences between pro-and anti-stereotyped sets (|Diff|) on both semantic and syntactic examples in <cite>WinoBias</cite>. Bias Mitigation Rows 4-6 in Table 2 summarize the effectiveness of the two bias mitigation approaches we consider.",
  "y": "uses"
 },
 {
  "id": "3d6df70136820f74ce76f60a59cc42_22",
  "x": "Bias Mitigation Rows 4-6 in Table 2 summarize the effectiveness of the two bias mitigation approaches we consider. Data augmentation is largely effective at mitigating bias in the coreference resolution system with ELMo (reducing |Diff | to insignificant levels) but requires retraining the system. Neutralization is less effective than augmentation and cannot fully remove gender bias on the Semantics Only portion of <cite>WinoBias</cite>, indicating it is effective only for simpler cases.",
  "y": "motivation"
 },
 {
  "id": "3d84cf97f48dad66b4d8de0baf79b1_0",
  "x": "Motivated by these issues, Silberer (2017) constructed multimodal semantic models from text and image data, with the goal of grounding word meaning using visual attributes. More recently, Derby et al. (2018) built similar models with the added constraint of sparsity, demonstrating that sparse multimodal vectors provide a more faithful representation of human semantic representations. Finally, the work that most resembles ours is that of<cite> Fagarasan et al. (2015)</cite> , who use Partial Least Squares Regression (PLSR) to learn a mapping from a word embedding model onto specific conceptual properties.",
  "y": "uses"
 },
 {
  "id": "3d84cf97f48dad66b4d8de0baf79b1_1",
  "x": "Finally, the work that most resembles ours is that of<cite> Fagarasan et al. (2015)</cite> , who use Partial Least Squares Regression (PLSR) to learn a mapping from a word embedding model onto specific conceptual properties. Concurrent work recently undertaken by Li and Summers-Stay (2019) replaces the PLSR model with a feedforward neural network. In our work, we instead map property knowledge directly into vector space models of word meaning, rather than learning a supervised predictive function from concept embedding dimensions to feature terms.",
  "y": "extends differences"
 },
 {
  "id": "3d84cf97f48dad66b4d8de0baf79b1_2",
  "x": "We make primary comparison with the work of<cite> Fagarasan et al. (2015)</cite> , although their approach differs from ours in that they map from an embedding space onto the feature space, while we learn a mapping from the feature domain onto the embedding space. We outline both methods below. ----------------------------------",
  "y": "differences uses"
 },
 {
  "id": "3d84cf97f48dad66b4d8de0baf79b1_3",
  "x": "The fitted regression model thus provides a framework for predicting vectors in the feature space from vectors in the embedding space. In this work, we use the PLSR approach as a baseline for our model. In implementing PLSR, we set the intermediate dimension size to 50, following<cite> Fagarasan et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "3d84cf97f48dad66b4d8de0baf79b1_4",
  "x": "We first evaluate how well the baseline PLSR model performs on the feature vector reconstruction task used by<cite> Fagarasan et al. (2015)</cite> . In this evaluation, the feature vector for a test concept is predicted and we test whether the real concept vector is within the top N most similar neighbours of the predicted vector. We report results over both 50 (as in<cite> Fagarasan et al. (2015)</cite> ) and 120 dimensions for a range of values of N (Table 1) .",
  "y": "uses"
 },
 {
  "id": "3d84cf97f48dad66b4d8de0baf79b1_5",
  "x": "We first evaluate how well the baseline PLSR model performs on the feature vector reconstruction task used by<cite> Fagarasan et al. (2015)</cite> . In this evaluation, the feature vector for a test concept is predicted and we test whether the real concept vector is within the top N most similar neighbours of the predicted vector. We report results over both 50 (as in<cite> Fagarasan et al. (2015)</cite> ) and 120 dimensions for a range of values of N (Table 1) .",
  "y": "similarities"
 },
 {
  "id": "3d84cf97f48dad66b4d8de0baf79b1_6",
  "x": "As property norms do not represent an exhaustive listing of property knowledge, this is not surprising, and predicted properties not in the norms are not necessarily errors (Devereux et al., 2009; <cite>Fagarasan et al., 2015)</cite> . Moreover, the set of features used within the norms are dependent on the concepts that were presented to the human participants. It is therefore notable that the conceptual representations predicted by our model for the two outof-norms concept words are particularly plausible, even though the attributes were never intended to conceptually represent these words.",
  "y": "background"
 },
 {
  "id": "3d84cf97f48dad66b4d8de0baf79b1_7",
  "x": "**CONCLUSION** We proposed a method for constructing distributional semantic vectors for human property norms from a pretrained vector space model of word meaning, which outperforms previous methods for predicting concept features on two property norm datasets. As discussed by<cite> Fagarasan et al. (2015)</cite> and others, it is clear that property norm datasets provide only a semi-complete picture of human conceptual knowledge, and more extensive surveys may provide additional useful property knowledge information.",
  "y": "motivation"
 },
 {
  "id": "3e2fb3d4c1e224c084117c22a5db78_0",
  "x": "This gap in the literature recently started to be addressed with studies on Spanish (Arag\u00f3n et al., 2018) , Hindi (Kumar et al., 2018) , and German (Wiegand et al., 2018) , to name a few. In this paper we contribute in this direction presenting the first Greek annotated dataset for offensive language identification: the Offensive Greek Tweet Dataset (OGTD). OGTD uses a working definition of offensive language inspired by the OLID dataset for English (<cite>Zampieri et al., 2019a</cite>) used in the recent OffensEval (SemEval-2019 Task 6) (Zampieri et al., 2019b) .",
  "y": "uses"
 },
 {
  "id": "3e2fb3d4c1e224c084117c22a5db78_1",
  "x": "OGTD considers a more general definition of offensiveness inspired by the first layer of the hierarchical annotation model described in (<cite>Zampieri et al., 2019a</cite>) . (<cite>Zampieri et al., 2019a</cite>) model distinguishes targeted from general profanity, and considers the target of offensive posts as indicators of potential hate speech posts (insults targeted at groups) and cyberbulling posts (insults targeted at individuals). Offensive Language: Previous work presented a dataset with sentences labelled as flame (i.e. attacking or containing abusive words) or okay (Razavi et al., 2010) with a Na\u00efve Bayes hybrid classifier and a user offensiveness estimation using an offensive lexicon and sentence syntactic structures (Chen et al., 2012) .",
  "y": "uses"
 },
 {
  "id": "3e2fb3d4c1e224c084117c22a5db78_2",
  "x": "(<cite>Zampieri et al., 2019a</cite>) model distinguishes targeted from general profanity, and considers the target of offensive posts as indicators of potential hate speech posts (insults targeted at groups) and cyberbulling posts (insults targeted at individuals). Offensive Language: Previous work presented a dataset with sentences labelled as flame (i.e. attacking or containing abusive words) or okay (Razavi et al., 2010) with a Na\u00efve Bayes hybrid classifier and a user offensiveness estimation using an offensive lexicon and sentence syntactic structures (Chen et al., 2012) . A dataset of 3.3M comments from the Yahoo Finance and News website, labelled as abusive or clean, was utilized in several experiments using ngrams, linguistic and syntactic features, combined with different types of word and comment embeddings as distributional semantics features (Nobata et al., 2016) .",
  "y": "background"
 },
 {
  "id": "3e2fb3d4c1e224c084117c22a5db78_3",
  "x": "The bulk of work on detecting abusive posts online addressed particular types of such language like textual attacks and hate speech (Malmasi and Zampieri, 2017) , ag-gression (Kumar et al., 2018) , and others. OGTD considers a more general definition of offensiveness inspired by the first layer of the hierarchical annotation model described in (<cite>Zampieri et al., 2019a</cite>) . (<cite>Zampieri et al., 2019a</cite>) model distinguishes targeted from general profanity, and considers the target of offensive posts as indicators of potential hate speech posts (insults targeted at groups) and cyberbulling posts (insults targeted at individuals).",
  "y": "uses"
 },
 {
  "id": "3e2fb3d4c1e224c084117c22a5db78_4",
  "x": "**RELATED WORK** The bulk of work on detecting abusive posts online addressed particular types of such language like textual attacks and hate speech (Malmasi and Zampieri, 2017) , ag-gression (Kumar et al., 2018) , and others. OGTD considers a more general definition of offensiveness inspired by the first layer of the hierarchical annotation model described in (<cite>Zampieri et al., 2019a</cite>) .",
  "y": "similarities"
 },
 {
  "id": "3e2fb3d4c1e224c084117c22a5db78_5",
  "x": "The most recent project expanded on existing ideas for defining offensive language and presented the OLID (Offensive Language Identification Dataset), a corpus of Twitter posts hierarchically annotated on three levels, whether they contain offensive language or not, whether the offense is targeted and finally, the target of the offense (<cite>Zampieri et al., 2019a</cite>) . A CNN (Convolutional neural network) deep learning approach outperformed every model trained, with pre-trained FastText embeddings and updateable embeddings learned by the model as features. In OffensEval (SemEval-2019 Task 6), participants had the opportunity to use the OLID to train their own systems, with the top teams outperforming the original models trained on the dataset.",
  "y": "background"
 },
 {
  "id": "3e2fb3d4c1e224c084117c22a5db78_6",
  "x": "The intuition behind this approach is that Twitter as a microblogging service often gathers complaints and profane comments on widely viewed television and politics, and as such, this period was a good opportunity for data collection. Following the methodology described in (<cite>Zampieri et al., 2019a</cite>) and others, including a recent comparable Danish dataset (Sigurbergsson and Derczynski, 2020) , we collected tweets using keywords such as sensitive or obscene language. Queries for tweets containing common curse words and expressions usually found in offensive messages in Greek as keywords (such as the well-known word for \"asshole\", \"\u03bc\u03b1\u03bb\u03ac\u03ba\u03b1\u03c2\" (malakas) or \"go to hell\", \"\u03c3\u03c4\u03bf \u03b4\u03b9\u03ac\u03bf\u03bb\u03bf\" (sto diaolo), etc.) returned a large number of tweets.",
  "y": "uses"
 },
 {
  "id": "3e2fb3d4c1e224c084117c22a5db78_7",
  "x": "The challenge is to recognize between ironic and insulting uses of these swear words, a common phenomenon in Greek. The final query for data collection was for tweets containing \"\u03b5\u03af\u03c3\u03b1\u03b9\" (eisai, \"you are\") as a keyword, inspired by (<cite>Zampieri et al., 2019a</cite>) . This particular keyword is considered a stop word as it is quite common and frequent in languages but was suspected to prove helpful for building the dataset for this particular project, as offensive language often follows the following structure: auxiliary verb (be) + noun/adjective.",
  "y": "uses"
 },
 {
  "id": "3e2fb3d4c1e224c084117c22a5db78_8",
  "x": "**PRE-PROCESSING AND ANNOTATION** We collected a set of 49,154 tweets. URLs, Emojis and Emoticons were removed, while usernames and user mentions were filtered as @USER following the same methodology described in OLID (<cite>Zampieri et al., 2019a</cite>) .",
  "y": "uses"
 },
 {
  "id": "3e2fb3d4c1e224c084117c22a5db78_9",
  "x": "We used the same guidelines used in the annotation of the English OLID dataset (<cite>Zampieri et al., 2019a</cite>) . Finally, we run several machine learning and deep learning classifiers and the best results were achieved by a LSTM and GRU with Attention model. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "3e344c590b4d5270a29054ac15efa5_0",
  "x": "Chowdhury [1] experimented with the effectiveness of word embedding in authorship attribution for Bangla language for various architectures. Another type of embedding, which we tried to analyze in this paper is character embedding. Character CNN was first introduced by Zhang <cite>[2]</cite> for the text classification task.",
  "y": "background"
 },
 {
  "id": "3e344c590b4d5270a29054ac15efa5_1",
  "x": "Character embeddings have been employed in POS tagging [23] , language modelling [23] and dependency parsing [24] . Character-RNN were used for machine translation, for representing words [25] or to generate character level translations [26] . Pure Character level classification was first explored using CNN architecture <cite>[2]</cite> .",
  "y": "background"
 },
 {
  "id": "3e344c590b4d5270a29054ac15efa5_2",
  "x": "**IV. METHODOLOGY A. PROPOSED ARCHITECTURE** Character-level CNN can sufficiently replace words for classifications <cite>[2]</cite> . This means CNN does not require the syntactic or semantic structure of a language, which makes such approaches effectively independent of language as the number of characters is limited.",
  "y": "background"
 },
 {
  "id": "3e344c590b4d5270a29054ac15efa5_3",
  "x": "Character-level CNN can sufficiently replace words for classifications <cite>[2]</cite> . This means CNN does not require the syntactic or semantic structure of a language, which makes such approaches effectively independent of language as the number of characters is limited. To this end, CNN was used in this paper to perform the task of author attribution.",
  "y": "uses background"
 },
 {
  "id": "3e344c590b4d5270a29054ac15efa5_4",
  "x": "When the number of authors increased, the number of samples per author decreased making it difficult for the character-level model to collect enough information. With larger datasets, this model will be able to perform significantly better <cite>[2]</cite> . This can be illustrated from Figure 1 that with a larger number of samples, the Char-CNN model raises steeply and performs competitively with the other models.",
  "y": "background"
 },
 {
  "id": "3e344c590b4d5270a29054ac15efa5_5",
  "x": "When the number of authors increased, the number of samples per author decreased making it difficult for the character-level model to collect enough information. With larger datasets, this model will be able to perform significantly better <cite>[2]</cite> . This can be illustrated from Figure 1 that with a larger number of samples, the Char-CNN model raises steeply and performs competitively with the other models.",
  "y": "similarities"
 },
 {
  "id": "3e344c590b4d5270a29054ac15efa5_6",
  "x": "This effect of training time become largely magnified on largescale cases, making the word-level model unfit for light-weight devices. As stated in the paper <cite>[2]</cite> , ConvNets with character embedding can completely replace words and work even without any semantic meanings. Which means that convolutional layers can extract whatever information necessary for author attribution, given enough data.",
  "y": "background"
 },
 {
  "id": "3e344c590b4d5270a29054ac15efa5_7",
  "x": "We attempt to fill this gap and compare character embeddings with word embeddings showing that character embeddings perform almost as good as the best word embedding model. But besides accuracy, character level classification has a greater hand in terms of memory, time and number of parameters. Considering the small size of our datasets, we hope to have improved performance with larger datasets, as is the case for character level ConvNets <cite>[2]</cite> .",
  "y": "similarities"
 },
 {
  "id": "3e5c070a6966361b54f069248438ec_0",
  "x": "Although greedy parsers are fast, accuracies of these parsers are typically much lower than graph-based parsers. Conversely, beam-search parsers achieve accuracies comparable to graph-based parsers (Zhang and Nivre, 2011) but are much slower than their greedy counterparts. Recently, <cite>Chen and Manning (2014)</cite> have showed that fast and accurate parsing can be achieved using neural network based parsers.",
  "y": "background"
 },
 {
  "id": "3e5c070a6966361b54f069248438ec_1",
  "x": "We present a neural network based shift-reduce CCG parser, the first neural network based parser for CCG. We first adapt <cite>Chen and Manning (2014)</cite> 's shift-reduce dependency parser for CCG parsing. We then develop a structured neural network model based on , in order to explore the impact of a beamsearch on the parser.",
  "y": "extends"
 },
 {
  "id": "3e5c070a6966361b54f069248438ec_2",
  "x": "There has been some work on neural networks for constituent based parsing (Collobert, 2011; Socher et al., 2013; Watanabe and Sumita, 2015) . <cite>Chen and Manning (2014)</cite> developed a neural network architecture for dependency parsing. This parser was fast and accurate, parsing around 1000 sentences per second and achieving an unlabeled attachment score of 92.0% on the standard Penn Treebank test data for English.",
  "y": "background"
 },
 {
  "id": "3e5c070a6966361b54f069248438ec_3",
  "x": "<cite>Chen and Manning (2014)</cite> 's parser used a feed forward neural network. Several improvements were made to this architecture in terms of using Long Short-Term Memory (LSTM) networks (Dyer et al., 2015) , deep neural networks and structured neural networks Zhou et al., 2015; . ----------------------------------",
  "y": "background"
 },
 {
  "id": "3e5c070a6966361b54f069248438ec_4",
  "x": "---------------------------------- **OUR NEURAL NETWORK PARSER (NNPAR)** The architecture of our neural network based shift-reduce CCG parser is similar to that of <cite>Chen and Manning (2014)</cite> .",
  "y": "similarities"
 },
 {
  "id": "3e5c070a6966361b54f069248438ec_5",
  "x": "pertags from the parser configuration. For each of these discrete features we obtain a continuous vector representation in the form of their corresponding embeddings and use them in the input layer. Following <cite>Chen and Manning (2014)</cite>, we use a cube activation function and softmax for output layer.",
  "y": "uses"
 },
 {
  "id": "3e5c070a6966361b54f069248438ec_6",
  "x": "We use 200 hidden units in the the hidden layer. For the output layer we compute softmax probabilities only for the actions which are possible in a particular parser configuration instead of all the actions. We use the training settings of <cite>Chen and Manning (2014)</cite> for our parser.",
  "y": "uses"
 },
 {
  "id": "3e5c070a6966361b54f069248438ec_7",
  "x": "**STRUCTURED NEURAL NETWORK** <cite>Chen and Manning (2014)</cite>'s parser is a greedy parser and it is not straight forward to add a beam during training into their parser. As a way of introducing a beam, presented a structured perceptron training for the neural network parser.",
  "y": "background"
 },
 {
  "id": "3e5c070a6966361b54f069248438ec_8",
  "x": "In addition to using a softmax for the output layer, we also applied this structured neural network approach for our experiments using a beam. Unlike 's neural network architecture, which consists of two hidden layers with 2048 hidden units each, we use the <cite>Chen and Manning (2014)</cite> style architecture described in the previous sections. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "3e5c070a6966361b54f069248438ec_9",
  "x": "**COMPARISON TO CHEN AND MANNING (2014)** Our neural network parser differs from <cite>Chen and Manning (2014)</cite> in a number of respects. We use CCG supertags in the input layer rather than dependency labels.",
  "y": "differences"
 },
 {
  "id": "3ea1f4acd7e2812e68eca54600fc5c_0",
  "x": "**INTRODUCTION** Dependency parsing is a topic that has engendered increasing interest in recent years. One promising approach is based on exact search and structural learning<cite> (McDonald et al., 2005</cite>; McDonald and Pereira, 2006) .",
  "y": "background"
 },
 {
  "id": "3ea1f4acd7e2812e68eca54600fc5c_2",
  "x": "---------------------------------- **PARSING ALGORITHM** In our approach, we adopt Eisner (1996) 's bottomup chart-parsing algorithm in <cite>McDonald et al. (2005)</cite> 's formulation, which finds the best projective dependency tree for an input string",
  "y": "extends"
 },
 {
  "id": "3ea1f4acd7e2812e68eca54600fc5c_3",
  "x": "In deriving features, we used all information given in the treebanks, i.e. words (w), fine-grained POS tags (fp), combinations of lemmas and coarsegrained POS tags (lcp), and whether two tokens agree 1 (agr = yes, no, don't know). We essentially employ the same set of features as <cite>McDonald et al. (2005)</cite> : ----------------------------------",
  "y": "uses"
 },
 {
  "id": "3ea1f4acd7e2812e68eca54600fc5c_4",
  "x": "Having a closed-form solution, OPAL is easier to implement and more efficient than the MIRA algorithm used by <cite>McDonald et al. (2005)</cite> , although it achieves a performance comparable to MIRA's on many problems (Crammer et al., 2006) . ---------------------------------- **LEARNING LABELS FOR DEPENDENCY RELATIONS**",
  "y": "differences"
 },
 {
  "id": "3ea1f4acd7e2812e68eca54600fc5c_5",
  "x": "So far, the presented system, which follows closely the approach of <cite>McDonald et al. (2005)</cite> , only predicts unlabelled dependency trees. To derive a labeling, we departed from their approach: We split each feature along the deprel label dimension, so that each deprel U is associated with its own feature vector (cf. eq. (4), where V is the tensor product and In parsing, we only consider the best deprel label.",
  "y": "differences motivation"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_0",
  "x": "Sequence labeling models have been used for fundamental NLP tasks such as POS tagging, chunking and named entity recognition (NER). Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015) with handcrafted features and task-specific resources. With advances in deep learning, neural models have given state-of-the-art results on many sequence labeling tasks (Ling et al., 2015; <cite>Lample et al., 2016</cite>; Ma and Hovy, 2016) .",
  "y": "background"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_1",
  "x": "With advances in deep learning, neural models have given state-of-the-art results on many sequence labeling tasks (Ling et al., 2015; <cite>Lample et al., 2016</cite>; Ma and Hovy, 2016) . Words and characters are encoded in distributed representations (Mikolov et al., 2013) and sentence-level features are learned automatically during end-to-end training. Many existing state-of-the-art neural sequence labeling models utilize word-level Long Short-Term Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels <cite>Lample et al., 2016</cite>; Ma and Hovy, 2016; Peters et al., 2017) .",
  "y": "background"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_2",
  "x": "Liu et al. (2018) report lower average F-scores on NER when reproducing the structure of <cite>Lample et al. (2016)</cite> , and on POS tagging when reproducing Ma and Hovy (2016) . Most literature compares results with others by citing the scores directly <cite>Lample et al., 2016</cite>) without re-implementing them under the same setting, resulting in less persuasiveness on the advantage of their models. In addition, conclusions from different reports can be contradictory.",
  "y": "background"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_3",
  "x": "For example, most work observes that stochastic gradient descent (SGD) gives best performance on NER task (Chiu and Nichols, 2016; <cite>Lample et al., 2016</cite>; Ma and Hovy, 2016) , while Reimers and Gurevych (2017b) report that SGD is the worst optimizer on the same datasets. The comparison between different deep neural models is challenging due to sensitivity on experimental settings. We list six inconsistent configurations in literature, which lead to difficulties for fair comparison.",
  "y": "background"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_4",
  "x": "Ling et al. (2015) give results only on POS dataset, while some papers (Chiu and Nichols, 2016; <cite>Lample et al., 2016</cite>; Strubell et al., 2017) report results on the NER dataset only. dos Santos et al. (2015) conducts experiments on NER for Portuguese and Spanish. Most work uses the development set to select hyperparameters (<cite>Lample et al., 2016</cite>; Ma and Hovy, 2016) , while others add development set into training set (Chiu and Nichols, 2016; Peters et al., 2017) .",
  "y": "background"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_5",
  "x": "Ling et al. (2015) give results only on POS dataset, while some papers (Chiu and Nichols, 2016; <cite>Lample et al., 2016</cite>; Strubell et al., 2017) report results on the NER dataset only. dos Santos et al. (2015) conducts experiments on NER for Portuguese and Spanish. Most work uses the development set to select hyperparameters (<cite>Lample et al., 2016</cite>; Ma and Hovy, 2016) , while others add development set into training set (Chiu and Nichols, 2016; Peters et al., 2017) .",
  "y": "background"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_6",
  "x": "Liu et al. (2018) and Hashimoto et al. (2017) use different development sets for chunking. \u2022 Preprocessing. A typical data preprocessing step is to normize digit characters (Chiu and Nichols, 2016; <cite>Lample et al., 2016</cite>; Yang et al., 2016; Strubell et al., 2017) .",
  "y": "background"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_7",
  "x": "Besides, <cite>Lample et al. (2016)</cite> and Ma and Hovy (2016) use end-to-end structure without handcrafted features. \u2022 Hyperparameters including learning rate, dropout rate (Srivastava et al., 2014) , number of layers, hidden size etc. can strongly affect the model performance. Chiu and Nichols (2016) search for the hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters.",
  "y": "background"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_8",
  "x": "Strubell et al. (2017) built a deeper dilated CNN architecture to capture larger local features. Hammerton (2003) was the first to exploit LSTM for sequence labeling. built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (<cite>Lample et al., 2016</cite>; Liu et al., 2018) , GRU (Yang et al., 2016) , and CNN (Chiu and Nichols, 2016; Ma and Hovy, 2016) features.",
  "y": "extends"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_9",
  "x": "3) Our findings are more consistent with most previous work on configurations such as usefulness of character information (<cite>Lample et al., 2016</cite>; Ma and Hovy, 2016) , optimizer (Chiu and Nichols, 2016; <cite>Lample et al., 2016</cite>; Ma and Hovy, 2016) and tag scheme (Ratinov and Roth, 2009; Dai et al., 2015) . In contrast, many results of Reimers and Gurevych (2017b) contradict existing reports. 4) We conduct a wider range of comparison for word sequence representations, including all combinations of character CNN/LSTM and word CNN/LSTM structures, while Reimers and Gurevych (2017b) studied the word LSTM models only.",
  "y": "similarities"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_10",
  "x": "Our neural sequence labeling framework contains three layers, i.e., a character sequence representation layer, a word sequence representation layer and an inference layer, as shown in Figure 1 . Character information has been proven to be critical for sequence labeling tasks (Chiu and Nichols, 2016; <cite>Lample et al., 2016</cite>; Ma and Hovy, 2016) , with LSTM and CNN being used to model character sequence information (\"Char Rep.\"). Similarly, on the word level, LSTM or CNN structures can be leveraged to capture long-term information or local features (\"Word Rep.\"), respectively.",
  "y": "motivation background"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_11",
  "x": "Character features such as prefix, suffix and capitalization can be represented with embeddings through a feature-based lookup table (Collobert et al., 2011; Strubell et al., 2017) , or neural networks without human-defined features (<cite>Lample et al., 2016</cite>; Ma and Hovy, 2016) . In this work, we focus on neural character sequence representations without hand-engineered features. Character CNN.",
  "y": "background"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_12",
  "x": "Liu et al. (2018) applied one bidirectional LSTM for the character sequence over a sentence rather than each word individually. We examined both structures and found that they give comparable accuracies on sequence labeling tasks. We choose <cite>Lample et al. (2016)</cite> 's structure as its character LSTMs can be calculated in parallel, making the system more efficient.",
  "y": "uses"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_13",
  "x": "**WORD SEQUENCE REPRESENTATIONS** Similar to character sequences in words, we can model word sequence information through LSTM or CNN structures. LSTM has been widely used in sequence labeling (<cite>Lample et al., 2016</cite>; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018) .",
  "y": "similarities background"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_14",
  "x": "We re-implement the structure of several reports (Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017) , which take the CCNN+WLSTM+CRF architecture. Our reproduced models give slightly better performances. The results of <cite>Lample et al. (2016)</cite> can be reproduced by our CLSTM+WLSTM+CRF.",
  "y": "similarities"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_15",
  "x": "The results of the POS tagging task is shown in Table 6 . The results of <cite>Lample et al. (2016)</cite> , Ma and Hovy (2016) and Yang et al. (2017b) can be reproduced by our CLSTM+WLSTM+CRF and CCNN+WLSTM+CRF models. Our WLSTM based models give better results than WLSTM+CRF based models, this is consistent with the fact that Ling et al. (2015) take CLSTM+WLSTM without CRF layer but achieve the best POS accuracy.",
  "y": "similarities"
 },
 {
  "id": "3fd7a249a8fa7a71a4c6aa2e79fecf_16",
  "x": "Our observation is consistent with most literature (Chiu and Nichols, 2016; <cite>Lample et al., 2016</cite>; Ma and Hovy, 2016) . ---------------------------------- **ANALYSIS**",
  "y": "similarities"
 },
 {
  "id": "3ff58556ab973a9dde640a2b74c37b_0",
  "x": "The two existing systems that use function labels sucessfully, either inherit Collins' modelling of the notion of complement (Gabbard, Kulick and Marcus, 2006) or model function labels directly <cite>(Musillo and Merlo, 2005)</cite> . Furthermore, our results indicate that the proposed models are robust. To model our task accurately, additional parameters must be estimated.",
  "y": "background"
 },
 {
  "id": "3ff58556ab973a9dde640a2b74c37b_1",
  "x": "The two existing systems that use function labels sucessfully, either inherit Collins' modelling of the notion of complement (Gabbard, Kulick and Marcus, 2006) or model function labels directly <cite>(Musillo and Merlo, 2005)</cite> . Furthermore, our results indicate that the proposed models are robust. To model our task accurately, additional parameters must be estimated.",
  "y": "uses background"
 },
 {
  "id": "3ff58556ab973a9dde640a2b74c37b_2",
  "x": "In our experiments, the set D of earlier history representations is modified to yield a model that is sensitive to regularities in structurally defined sequences of nodes bearing semantic role labels, within and across constituents. For more information on this technique to capture structural domains, see <cite>(Musillo and Merlo, 2005)</cite> where the technique was applied to function parsing. Given the hidden history representation h(d 1 , \u00b7 \u00b7 \u00b7 , d i\u22121 ) of a derivation, a normalized exponential output function is computed by the SSNs to estimate a probability distribution over the possible next derivation moves d i .",
  "y": "uses background"
 },
 {
  "id": "3ff58556ab973a9dde640a2b74c37b_3",
  "x": "Given the hidden history representation h(d 1 , \u00b7 \u00b7 \u00b7 , d i\u22121 ) of a derivation, a normalized exponential output function is computed by the SSNs to estimate a probability distribution over the possible next derivation moves d i . To exploit the intuition that semantic role labels are predictive of syntactic structure, we must pro-vide semantic role information as early as possible to the parser. Extending a technique presented in (Klein and Manning, 2003 ) and adopted in<cite> (Merlo and Musillo, 2005)</cite> for function labels with stateof-the-art results, we split some part-of-speech tags into tags marked with AM-X semantic role labels.",
  "y": "extends"
 },
 {
  "id": "412c2daf6d060f520850d187c6eb36_0",
  "x": "Previous corpus-based sense disambiguation methods require substantial amounts of sense-tagged training data (Kelly and Stone, 1975; Black, 1988 and Hearst, 1991) or aligned bilingual corpora (Brown et al., 1991; Dagan, 1991 and Gale et al. 1992 ). <cite>Yarowsky (1992)</cite> introduces a thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without the need for sense-tagged training data. By collecting statistical data of word occurrences in the context of different thesaurus categories from a relatively large corpus (10 million words), the system can identify salient words for each category.",
  "y": "background"
 },
 {
  "id": "412c2daf6d060f520850d187c6eb36_1",
  "x": "Like the thesaurusbased approach of <cite>Yarowsky (1992)</cite> , our approach relies on the dilution of this noise by their distribution through all the 1792 defining concepts. Different words in the corpus have different numbers of senses and different senses have definitions of varying lengths. The principle adopted in collecting co-occurrence data is that every pair of content words which co-occur in a sentence should have equal contribution to the conceptual cooccurrence data regardless of the number of definitions (senses) of the words and the lengths of the definitions.",
  "y": "similarities"
 },
 {
  "id": "412c2daf6d060f520850d187c6eb36_2",
  "x": "Our system is tested on the twelve words discussed in <cite>Yarowsky (1992)</cite> and previous publications on sense disambiguation. Results are shown in Table 1 . Our system achieves an average accuracy of 77% on a mean 3-way sense distinction over the twelve words.",
  "y": "uses"
 },
 {
  "id": "412c2daf6d060f520850d187c6eb36_3",
  "x": "Our system achieves an average accuracy of 77% on a mean 3-way sense distinction over the twelve words. Numerically, the result is not as good as the 92% as reported in <cite>Yarowsky (1992)</cite> . However, direct comparison between the numerical results can be misleading since the experiments are carried out on two very different corpora both in size and genre.",
  "y": "differences"
 },
 {
  "id": "412c2daf6d060f520850d187c6eb36_4",
  "x": "s The subject has not read through the whole corpus. approaches in a large proportion of cases. 9 The average sentence length in the Brown corpus is 19.41\u00b0 words which is 5 times smaller than the 100 word window used in Gale et al. (1992) and <cite>Yarowsky (1992)</cite> .",
  "y": "differences"
 },
 {
  "id": "412c2daf6d060f520850d187c6eb36_5",
  "x": "DBCC (Defmition-Bascd Conceptual Cooccurrence) and Human mark the columns with the results of our system and the human subject in disambiguating the occurrences of the 12 words in the Brown corpus, respectively. Thes. (thesaurus) marks the column with the results of <cite>Yarowsky (1992)</cite> tested on the Grolier's Encyclopedia.",
  "y": "uses"
 },
 {
  "id": "412c2daf6d060f520850d187c6eb36_6",
  "x": "3. The senses marked with * are used in <cite>Yarowsky (1992)</cite> but no corresponding sense is found in LDOCE. 4. The sense marked with ** is defined in LDOCE but not used in <cite>Yarowsky (1992)</cite> . 6. In our experiment, the words are disambiguated between all the senses listed except the ones marked with 7.",
  "y": "uses"
 },
 {
  "id": "412c2daf6d060f520850d187c6eb36_7",
  "x": "3. The senses marked with * are used in <cite>Yarowsky (1992)</cite> but no corresponding sense is found in LDOCE. 4. The sense marked with ** is defined in LDOCE but not used in <cite>Yarowsky (1992)</cite> . 6. In our experiment, the words are disambiguated between all the senses listed except the ones marked with 7.",
  "y": "differences"
 },
 {
  "id": "412c2daf6d060f520850d187c6eb36_8",
  "x": "The rare senses listed in LDOCE are not listed here. For some of the words, more than one sense listed in LDOCE corresponds to a sense as used in <cite>Yarowsky (1992)</cite> . In these cases, the senses used by Yarowsky are adopted for easier comparison.",
  "y": "similarities"
 },
 {
  "id": "412c2daf6d060f520850d187c6eb36_9",
  "x": "Although only 1792 defining concepts are used, the set of all possible combinations (a power set of the defining concepts) is so huge that it is very unlikely two word senses will have the same combination of defining concepts unless they are almost identical in meaning. On the other hand, the thesaurus-based method of <cite>Yarowsky (1992)</cite> may suffer from loss of information (since it is semi-class-based) as well as data sparseness (since H Classes used in Resnik (1992) are based on the WordNet taxonomy while classes of Brown et al. (1992) and Pereira et al. (1993) are derived from statistical data collected from corpora. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "412c2daf6d060f520850d187c6eb36_10",
  "x": "This knowledge has been successfully acquired from corpora in manual or semi-automatic approaches such as that described in Hearst (1991) . However, fully automatic lexically based approaches 3 Hatzivassiloglou (1994) shows that the introduction of linguistic cues improves the performance of a statistical semantic knowledge acquisition system in the context of word grouping. such as that described in <cite>Yarowsky (1992)</cite> are very unlikely to be capable of acquiring this finer knowledge because the problem of data sparseness becomes even more serious with the introduction of syntactic constraints.",
  "y": "differences"
 },
 {
  "id": "418016e4df12f80205cadc41119244_0",
  "x": "Actually we find Inspired by <cite>[2]</cite> , we formulate BioCS as a sequence tagging problem. We propose a new deep sequence tagging framework to achieve the goal. Experiments on a data set of 141M sentences from PubMed paper abstracts show that the biological knowledge graph we constructed provide a good understanding of biological statements (https://scikg.github.io).",
  "y": "uses"
 },
 {
  "id": "418e03aa7ba304c4774111e9f300ad_0",
  "x": "The SIGMORPHON shared task for 2018 (Cotterell et al., 2018) provided three data scenarios consisting of high (10000), medium (1000), and low (100) examples. This paper described the three systems that we submitted to the inflection track in the SIGMORPHON shared task. All our models are based on encoder-decoder model introduced by <cite>Faruqui et al. (2016)</cite> for the morphological inflection task.",
  "y": "uses"
 },
 {
  "id": "418e03aa7ba304c4774111e9f300ad_1",
  "x": "The morphological (re)inflection task has been studied mainly in last two SIGMORPHON shared tasks (Cotterell et al., 2016 (Cotterell et al., , 2017 . Most of the morphological inflection models are variants of sequence to sequence models applied by <cite>Faruqui et al. (2016)</cite> to morphological reinflection. The input to the model is the source word prepended with relevant morphological tags, the output of the model is the target word for the inflection task.",
  "y": "background"
 },
 {
  "id": "418e03aa7ba304c4774111e9f300ad_2",
  "x": "The methods used mostly work well for re-inflection task, since the re-inflection task is symmetric, and one can invert the source and target forms. In the subsequent year's shared task for 2017 (Cotterell et al., 2017) , multiple authors explored new data enhancement techniques Bergmanis et al., 2017; Silfverberg et al., 2017) to improve the performance of the seq2seq models in medium and low resource scenarios. The work presented in this paper is based on the work of the simple encoder-decoder system of <cite>Faruqui et al. (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "41d56f3f962fa3b0e0563544a6de40_0",
  "x": "Left-to-right (LR) decoding<cite> (Watanabe et al., 2006</cite> ) is promising decoding algorithm for hierarchical phrase-based translation (Hiero) that visits input spans in arbitrary order producing the output translation in left to right order. This leads to far fewer language model calls, but while LR decoding is more efficient than CKY decoding, it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result. This paper introduces two improvements to LR decoding that make it comparable in translation quality to CKY-based Hiero.",
  "y": "background"
 },
 {
  "id": "41d56f3f962fa3b0e0563544a6de40_1",
  "x": "LR-decoding algorithms exist for phrasebased (Koehn, 2004; Galley and Manning, 2010) and syntax-based (Huang and Mi, 2010; Feng et al., 2012 ) models and also for hierarchical phrasebased models<cite> (Watanabe et al., 2006</cite>; Siahbani et al., 2013) , which is our focus in this paper. <cite>Watanabe et al. (2006)</cite> first proposed left-toright (LR) decoding for Hiero (LR-Hiero henceforth) which uses beam search and runs in O(n 2 b) in practice where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010) . To simplify target generation, SCFG rules are constrained to be prefix-lexicalized on target side aka Griebach Normal Form (GNF).",
  "y": "background"
 },
 {
  "id": "41d56f3f962fa3b0e0563544a6de40_2",
  "x": "LR-decoding algorithms exist for phrasebased (Koehn, 2004; Galley and Manning, 2010) and syntax-based (Huang and Mi, 2010; Feng et al., 2012 ) models and also for hierarchical phrasebased models<cite> (Watanabe et al., 2006</cite>; Siahbani et al., 2013) , which is our focus in this paper. <cite>Watanabe et al. (2006)</cite> first proposed left-toright (LR) decoding for Hiero (LR-Hiero henceforth) which uses beam search and runs in O(n 2 b) in practice where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010) . To simplify target generation, SCFG rules are constrained to be prefix-lexicalized on target side aka Griebach Normal Form (GNF).",
  "y": "background"
 },
 {
  "id": "41d56f3f962fa3b0e0563544a6de40_3",
  "x": "LR-Hiero uses a constrained lexicalized SCFG which we call a GNF grammar: X \u2192 \u03b3,b \u03b2 where \u03b3 is a string of non-terminal and terminal symbols,b is a string of terminal symbols and \u03b2 is a possibly empty sequence of non-terminals. This ensures that as each rule is used in a derivation, Add h to hypList 29: return hypList the target string is generated from left to right. The rules are obtained from a word and phrase aligned bitext using the rule extraction algorithm in<cite> (Watanabe et al., 2006)</cite> .",
  "y": "uses"
 },
 {
  "id": "41d56f3f962fa3b0e0563544a6de40_4",
  "x": "to the economy /s Figure 2 : The process of translating the Chinese sentence in Figure 3 (b) in LR-Hiero. Left side shows the rules used in the derivation (G indicates glue rules as defined in<cite> (Watanabe et al., 2006)</cite> ). The hypotheses column shows the translation prefix and the ordered list of yet-to-be-covered spans.",
  "y": "uses"
 },
 {
  "id": "41d56f3f962fa3b0e0563544a6de40_5",
  "x": "We use 3 baselines: (i) our implementation of<cite> (Watanabe et al., 2006)</cite> : LR-Hiero with beam search (LR-Hiero) and (ii) LR-Hiero with cube pruning (Siahbani et al., 2013) : (LR-Hiero+CP); and (iii) Kriya, an open-source implementation of Hiero in Python, which performs comparably to other open-source Hiero systems (Sankaran et al., 2012) . Table 3 shows model sizes for LR-Hiero (GNF) and Hiero (SCFG). Typical Hiero rule extraction excludes phrase-pairs with unaligned words on boundaries (loose phrases).",
  "y": "uses"
 },
 {
  "id": "41d56f3f962fa3b0e0563544a6de40_6",
  "x": "In (Siahbani et al., 2013) we discuss that LR-Hiero with beam search<cite> (Watanabe et al., 2006)</cite> does not perform at the same level of state-of-the-art Hiero (more LM calls and less translation quality). As we can see in this figure, adding new modified rules slightly increases the number of language model queries on Cs-En and De-En so that LR-Hiero+CP still works 2 to 3 times faster than Hiero. On Zh-En, LR-Hiero+CP applies queue diversity (QD=15) which reduces search errors and improves translation quality but increases the number of hypothesis generation as well.",
  "y": "background"
 },
 {
  "id": "425148e63eb84bba50326e362cc5b8_0",
  "x": "Recently, <cite>Agrawal et al. [1]</cite> published a paper titled \"What is wrong with topic modeling? And how to fix it using search-based software engineering\", where <cite>they</cite> claimed that the instability of topics is one major shortcoming of this technique. Indeed, studies could result in wrong conclusions if the results are based on instable topics. <cite>They</cite> proposed using a differential evolution search algorithm to find the input parameters which maximize the topic model stability measured as the similarity of topics between multiple runs.",
  "y": "background"
 },
 {
  "id": "425148e63eb84bba50326e362cc5b8_1",
  "x": "---------------------------------- **INTRODUCTION** Latent Dirichlet Allocation (LDA) is a topic modeling technique for textual data [5] that is widely applied in software engineering <cite>[1</cite>-4, 6, 10, 11, 14-16, 19, 24, 25] for different tasks such as requirements engineering [15] , software architecture [10] , source code analysis [9] , defect reports [16] , testing [14] and to bibliometric analysis of software engineering literature [11, 22] .",
  "y": "background"
 },
 {
  "id": "425148e63eb84bba50326e362cc5b8_2",
  "x": "Latent Dirichlet Allocation (LDA) is a topic modeling technique for textual data [5] that is widely applied in software engineering <cite>[1</cite>-4, 6, 10, 11, 14-16, 19, 24, 25] for different tasks such as requirements engineering [15] , software architecture [10] , source code analysis [9] , defect reports [16] , testing [14] and to bibliometric analysis of software engineering literature [11, 22] . A survey on topic modelling in software engineering has been conducted [24] and a book titled \"The art and science of analyzing software data\" [4] devoted a chapter for LDA analysis [6] . Many sources give methodological guidance on how to apply LDA topic modeling in software engineering <cite>[1</cite>, 3, 19] .",
  "y": "background"
 },
 {
  "id": "425148e63eb84bba50326e362cc5b8_3",
  "x": "The quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [13] , perplexity of measure in the test data [13] , or Silhouette coefficient of resulting topics [19] . Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs <cite>[1]</cite> . Recently, <cite>Agrawal et al. [1]</cite> published a paper titled \"What is wrong with topic modeling? And how to fix it using search-based software engineering\", where <cite>they</cite> claimed that the instability of topics is one major shortcoming of this technique.",
  "y": "background"
 },
 {
  "id": "425148e63eb84bba50326e362cc5b8_4",
  "x": "The quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [13] , perplexity of measure in the test data [13] , or Silhouette coefficient of resulting topics [19] . Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs <cite>[1]</cite> . Recently, <cite>Agrawal et al. [1]</cite> published a paper titled \"What is wrong with topic modeling? And how to fix it using search-based software engineering\", where <cite>they</cite> claimed that the instability of topics is one major shortcoming of this technique.",
  "y": "background"
 },
 {
  "id": "425148e63eb84bba50326e362cc5b8_5",
  "x": "Past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [19] or differential evolution <cite>[1]</cite> . As pointed out in Section 1, what is optimal can be measured with many metrics such as perplexity [13] , stability <cite>[1]</cite> , or coherence [23] . The stability of a topic model can be defined as the model's ability to replicate its solutions [8] .",
  "y": "background"
 },
 {
  "id": "425148e63eb84bba50326e362cc5b8_6",
  "x": "Before topic generation, LDA requires that we set the input parameters such as the number of topics k, and hyper priors \u03b1 and \u03b2. Past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [19] or differential evolution <cite>[1]</cite> . As pointed out in Section 1, what is optimal can be measured with many metrics such as perplexity [13] , stability <cite>[1]</cite> , or coherence [23] .",
  "y": "background"
 },
 {
  "id": "425148e63eb84bba50326e362cc5b8_7",
  "x": "The stability of a topic model can be defined as the model's ability to replicate its solutions [8] . Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm <cite>[1]</cite> . Past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model <cite>[1</cite>, 8, 12] .",
  "y": "background"
 },
 {
  "id": "425148e63eb84bba50326e362cc5b8_8",
  "x": "Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm <cite>[1]</cite> . Past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model <cite>[1</cite>, 8, 12] . We think using the results of a single LDA run, whether optimized for stability or not, is dangerous as perfect stability is impossible to reach.",
  "y": "background"
 },
 {
  "id": "425148e63eb84bba50326e362cc5b8_9",
  "x": "Another anomaly is that for two topics with no intersecting top ten words, we would get a better Spearman correlation value than -1 (-0.86). Third, we can measure Jaccard similarity between the top words of any two topics. Extended Jaccard measures have been used in LDA stability task optimization <cite>[1,</cite> 12] .",
  "y": "background"
 },
 {
  "id": "425148e63eb84bba50326e362cc5b8_10",
  "x": "Past work in software engineering <cite>[1]</cite> and machine learning [12] point out that LDA instability may lead to incorrect conclusions and proposes input parameter optimization to alleviate the problem. This paper suggests performing replicated runs, clustering the results and measuring the topic stability. These approach are not alternative but additive.",
  "y": "background"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_0",
  "x": "<cite>Transformer</cite> with self-attention has achieved great success in the area of nature language processing. Recently, there have been a few studies on <cite>transformer</cite> for end-to-end speech recognition, while its application for hybrid acoustic model is still very limited. In this paper, we revisit the <cite>transformer</cite>-based hybrid acoustic model, and propose a model structure with interleaved self-attention and 1D convolution, which is proven to have faster convergence and higher recognition accuracy.",
  "y": "background"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_1",
  "x": "Recently, there have been a few studies on <cite>transformer</cite> for end-to-end speech recognition, while its application for hybrid acoustic model is still very limited. In this paper, we revisit the <cite>transformer</cite>-based hybrid acoustic model, and propose a model structure with interleaved self-attention and 1D convolution, which is proven to have faster convergence and higher recognition accuracy. We also study several aspects of the <cite>transformer model</cite>, including the impact of the positional encoding feature, dropout regularization, as well as training with and without time restriction.",
  "y": "motivation"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_2",
  "x": "Recently, there have been a few studies on <cite>transformer</cite> for end-to-end speech recognition, while its application for hybrid acoustic model is still very limited. In this paper, we revisit the <cite>transformer</cite>-based hybrid acoustic model, and propose a model structure with interleaved self-attention and 1D convolution, which is proven to have faster convergence and higher recognition accuracy. We also study several aspects of the <cite>transformer model</cite>, including the impact of the positional encoding feature, dropout regularization, as well as training with and without time restriction.",
  "y": "extends"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_3",
  "x": "We also study several aspects of the <cite>transformer model</cite>, including the impact of the positional encoding feature, dropout regularization, as well as training with and without time restriction. We show competitive recognition results on the public Librispeech dataset when compared to the Kaldi baseline at both cross entropy training and sequence training stages. For reproducible research, we release our source code and recipe within the PyKaldi2 toolbox.",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_4",
  "x": "However, a well-known weakness of RNNs is the gradient vanishing or explosion problem due to BPTT, and the recurrent connections in RNNs make it challenging to parallelize the computations in both training and inference stages. <cite>Transformer</cite> <cite>[8]</cite> , which relies solely on self-attention to capture the temporal correlations in sequential signals, is a new type of neural network structure for sequence modeling, which has achieved excellent results in machine translation <cite>[8]</cite> , language modeling [9] , as well as end-to-end speech recognition [10, 11] . Self-attention is appealing for sequence modeling in the sense that it can learn long-term correlations by one step of attention operation, while for RNNs, it would take multiple steps in the time space for both forward and backward computation, and noise may accumulate during the process.",
  "y": "background"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_5",
  "x": "Self-attention is appealing for sequence modeling in the sense that it can learn long-term correlations by one step of attention operation, while for RNNs, it would take multiple steps in the time space for both forward and backward computation, and noise may accumulate during the process. CNNs, on the other hand, require multiple layers to capture the correlations between the two features which are very distant in the time space, although dilation that uses large strides can reduce the number of layers that is required. While there have been many studies on end-to-end speech recognition using <cite>transformers</cite> [10, 11, 12, 13, 14] , their applications for hybrid acoustic models are less well understood.",
  "y": "motivation"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_6",
  "x": "CNNs, on the other hand, require multiple layers to capture the correlations between the two features which are very distant in the time space, although dilation that uses large strides can reduce the number of layers that is required. While there have been many studies on end-to-end speech recognition using <cite>transformers</cite> [10, 11, 12, 13, 14] , their applications for hybrid acoustic models are less well understood. In this paper, we study the more standard <cite>transformer</cite> for speech recognition within the hybrid framework, and provide further insight to this model through experiments on the Librispeech public dataset.",
  "y": "extends"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_7",
  "x": "**RELATED WORKS** There have been a few studies on <cite>transformers</cite> for end-to-end speech recognition, particularly for sequence-to-sequence with attention model [10, 11, 12] , as well as transducer [13] and CTC models [14] . In [10] , the authors compared RNNs with <cite>transformers</cite> for various speech recognition and synthesis tasks, and obtained competitive or even better results with <cite>transformers</cite>.",
  "y": "background"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_8",
  "x": "There have been a few studies on <cite>transformers</cite> for end-to-end speech recognition, particularly for sequence-to-sequence with attention model [10, 11, 12] , as well as transducer [13] and CTC models [14] . In [10] , the authors compared RNNs with <cite>transformers</cite> for various speech recognition and synthesis tasks, and obtained competitive or even better results with <cite>transformers</cite>. However, the key challenge for <cite>transformer-based sequence-to-sequence model</cite> is to perform online streaming speech recognition, as there is no clear boundary for chunk-wise self-attention.",
  "y": "background"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_9",
  "x": "There have been a few studies on <cite>transformers</cite> for end-to-end speech recognition, particularly for sequence-to-sequence with attention model [10, 11, 12] , as well as transducer [13] and CTC models [14] . In [10] , the authors compared RNNs with <cite>transformers</cite> for various speech recognition and synthesis tasks, and obtained competitive or even better results with <cite>transformers</cite>. However, the key challenge for <cite>transformer-based sequence-to-sequence model</cite> is to perform online streaming speech recognition, as there is no clear boundary for chunk-wise self-attention.",
  "y": "motivation"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_10",
  "x": "In [10] , the authors compared RNNs with <cite>transformers</cite> for various speech recognition and synthesis tasks, and obtained competitive or even better results with <cite>transformers</cite>. However, the key challenge for <cite>transformer-based sequence-to-sequence model</cite> is to perform online streaming speech recognition, as there is no clear boundary for chunk-wise self-attention. <cite>Transformer</cite> based transducer [13] and CTC model [14] do not have the issue for online speech recognition, however, the results presented in the two studies are not competitive compared the hybrid baseline system from Kaldi [15] .",
  "y": "motivation"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_11",
  "x": "In this work, we look at a few other aspects of transformerbased hybrid acoustic models that have not been studied previously. In [16, 17, 18] , self-attention is only applied in a chunk of acoustic input restricted by a time window, which makes the <cite>transformer model</cite> easier to train as it does not need to consider very long term correlations. While whole sequence-level self-attention has been applied in sequence-to-sequence models [10] , hybrid model is different in the sense that it is required to maintain strict frame-level alignments before performing predictions, which may be challenging for a <cite>transformer</cite> with multiple layers of self-attention as it may reorder the sequence.",
  "y": "background"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_12",
  "x": "In this work, we look at a few other aspects of transformerbased hybrid acoustic models that have not been studied previously. In [16, 17, 18] , self-attention is only applied in a chunk of acoustic input restricted by a time window, which makes the <cite>transformer model</cite> easier to train as it does not need to consider very long term correlations. While whole sequence-level self-attention has been applied in sequence-to-sequence models [10] , hybrid model is different in the sense that it is required to maintain strict frame-level alignments before performing predictions, which may be challenging for a <cite>transformer</cite> with multiple layers of self-attention as it may reorder the sequence.",
  "y": "background"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_13",
  "x": "In [16, 17, 18] , self-attention is only applied in a chunk of acoustic input restricted by a time window, which makes the <cite>transformer model</cite> easier to train as it does not need to consider very long term correlations. While whole sequence-level self-attention has been applied in sequence-to-sequence models [10] , hybrid model is different in the sense that it is required to maintain strict frame-level alignments before performing predictions, which may be challenging for a <cite>transformer</cite> with multiple layers of self-attention as it may reorder the sequence. Furthermore, lower sampling rates are usually used for <cite>transformer</cite>-based acoustic models, which makes it easier for sequence-level self-attention as the input sequences are much shorter.",
  "y": "background"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_14",
  "x": "We propose an interleaved self-attention and convolution structure for <cite>transformer model</cite>, with the motivation that convolution can learn local feature correlations and maintain the ordering information of the sequence while self-attention can capture longterm correlations. We show that the model can achieved competitive recognition results when trained with or without time-restriction. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_16",
  "x": "**<cite>TRANSFORMER</cite>** In this section, we review each component in the standard <cite>transformer model</cite>, and discuss a model structure that is mainly investigated for speech recognition in this work. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_17",
  "x": "---------------------------------- **<cite>TRANSFORMER</cite>** In this section, we review each component in the standard <cite>transformer model</cite>, and discuss a model structure that is mainly investigated for speech recognition in this work.",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_18",
  "x": "---------------------------------- **SELF-ATTENTION WITH MULTIPLE HEADS** The attention mechanism in <cite>transformer</cite> is technically the same as in the original RNN-based attention model [19] .",
  "y": "background"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_19",
  "x": "In <cite>[8]</cite> , the authors used the dot-production attention [20] rather than the conventional additive attention [19] in favor of the low computational complexity, which is rewritten here as: where Q, K, V are referred to the query, key and value according to <cite>[8]</cite> . In <cite>transformer</cite>, both Q and K are from the source sequence, while in the conventional RNN-based attention model [19] , Q is from the decoder hidden state, and K is from the encoder hidden state.",
  "y": "background"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_20",
  "x": "The key difference is that the query used to compute the attention probability is also from the source sequence, instead of using the decoder hidden state as in the RNN-based attention model [19] . In <cite>[8]</cite> , the authors used the dot-production attention [20] rather than the conventional additive attention [19] in favor of the low computational complexity, which is rewritten here as: where Q, K, V are referred to the query, key and value according to <cite>[8]</cite> .",
  "y": "background"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_21",
  "x": "where Q, K, V are referred to the query, key and value according to <cite>[8]</cite> . In <cite>transformer</cite>, both Q and K are from the source sequence, while in the conventional RNN-based attention model [19] , Q is from the decoder hidden state, and K is from the encoder hidden state. In Eq (1), d k is the dimension of the model, and it is used to scale the dot-product between Q and K in order to smooth the probability distribution returned by the Softmax operation.",
  "y": "differences"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_22",
  "x": "In Eq (1), d k is the dimension of the model, and it is used to scale the dot-product between Q and K in order to smooth the probability distribution returned by the Softmax operation. This is to avoid placing most of the attention probability to a single frame as a result of the dot-product attention, while additive attention does not require such a scaling factor from our experience. Another key idea from the <cite>transformer paper</cite> <cite>[8]</cite> is the <cite>multihead attention mechanism</cite>, which performs multiple attention operations in parallel using different model parameters.",
  "y": "background"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_23",
  "x": "---------------------------------- **INTERLEAVED SELF-ATTENTION AND CONVOLUTION** For hybrid models, our preliminary experiments show that <cite>transformers</cite> with multiple self-attention layers alone is hard to train without time restriction, and it can easily diverge after a few epochs.",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_24",
  "x": "The positional encoding along may not be able to provide sufficient information to maintain the sequential information in the acoustic sequence (cf. section 4.1). In this paper, we propose a <cite>transformer</cite> model with interleaved 1D convolution and self-attention, with the motivation that the convolution layer can maintain the sequential information of the input sequence, while at the same time, it can learn the local correlations. Self-attention, on the other hand, is expected to capture the global information as the attention is performed as the entire sequence level.",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_25",
  "x": "Same as the standard <cite>transformer</cite> <cite>[8]</cite> , we also insert the feedforward layer after the multi-head attention. The final model structure is shown in Figure 1 . It is possible that the feedforward layer is redundant, or its size could be reduced given the 1D convolution layer.",
  "y": "similarities"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_26",
  "x": "It is possible that the feedforward layer is redundant, or its size could be reduced given the 1D convolution layer. We will investigate this aspect in our future work. Table 1 shows the number of parameters in each component of the <cite>transformer</cite> model studied in this paper.",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_27",
  "x": "To constrain our research scope, we fixed the depth of the <cite>transformer models</cite> to be 6 layers, and the dimension of the model d k in Eq (1) to be 512. The number of hidden units in the feedforward layer is 2048. The kernel size for each convolution layer is 3 without stride.",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_28",
  "x": "We did not train deeper <cite>transformer models</cite> due to the memory constraint. In our experiments, we used a high frame rate as 100 Hz, i.e., extracting one acoustic frame in every 10 millisecond. This led to long acoustic sequences.",
  "y": "differences"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_29",
  "x": "In our experiments, we used a high frame rate as 100 Hz, i.e., extracting one acoustic frame in every 10 millisecond. This led to long acoustic sequences. As the memory cost of self-attention is in the order of O(T 2 ), where T is the length of the acoustic sequence, lower frame rate would significantly cut down the memory cost, and enable the training of much deeper <cite>transformer models</cite> that will studied in our future work.",
  "y": "future_work"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_30",
  "x": "Instead, we only applied the utterance-level mean and variance normalization. We used a 4-gram language model for decoding that is released as the part of the corpus, and we used Kaldi [15] to build a Gaussian mixture model (GMM) system for bootstrapping. Our <cite>transformer</cite> acoustic models were trained using the PyKaldi2 toolbox [22] , which is built on top of Kaldi and PyTorch through the PyKaldi [23] wrapper.",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_31",
  "x": "We used the Adam optimizer [24] cross entropy (CE) training, and the same learning rate scheduler as in <cite>[8]</cite> . For sequence training, we used the vanilla stochastic gradient decent (SGD) with fixed learning rate. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_32",
  "x": "---------------------------------- **RESULTS OF POSITIONAL ENCODING AND DROPOUT** We first evaluated the positional encoding discussed in section 3.2 and dropout training for the <cite>transformer model</cite>.",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_33",
  "x": "We first evaluated the positional encoding discussed in section 3.2 and dropout training for the <cite>transformer model</cite>. Results are given in Table 2 . Unlike the observations in the area of machine translation, positional encoding did not make a big difference in terms of recognition accuracies for our <cite>transformer models</cite>.",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_34",
  "x": "We performed a sanity check by removing the positional encoding when evaluating a <cite>transformer model</cite> trained with positional encoding, and obtained results which are only around 0.1% worse absolute. In the future, we shall investigate if it would make a difference after scaling the positional encoding features to the same dynamic range as the acoustic feature after the linear projection layer. As for dropout training, it was pointed out in <cite>[8]</cite> that <cite>transformer model</cite> for sequence to-sequence ASR may suffer from overfitting easily, and regularization such as dropout is important to address such kind of issue.",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_35",
  "x": "As for dropout training, it was pointed out in <cite>[8]</cite> that <cite>transformer model</cite> for sequence to-sequence ASR may suffer from overfitting easily, and regularization such as dropout is important to address such kind of issue. In our experiments, we also observed the overfitting problem, and our models were usually trained for about 8 -10 epochs. However, dropout is not effective for our model, and it only slightly improved the recognition accuracy.",
  "y": "background"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_36",
  "x": "**1D CONVOLUTION AND LAYER NORMALIZATION** We then evaluated the impact of the 1D convolution layers in our <cite>transformer</cite> model by removing all the convolution layers. This corresponds to a vanilla <cite>transformer</cite> as in <cite>[8]</cite> .",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_37",
  "x": "**1D CONVOLUTION AND LAYER NORMALIZATION** We then evaluated the impact of the 1D convolution layers in our <cite>transformer</cite> model by removing all the convolution layers. This corresponds to a vanilla <cite>transformer</cite> as in <cite>[8]</cite> .",
  "y": "similarities"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_38",
  "x": "We still added the positional encoding feature to the inputs since the sequential information from the convolution layers is no longer available. We also have to insert another layer normalization to the output of the <cite>transformer</cite> before the output linear layer to stabilize the training. Otherwise, the training diverges quickly after one or two epochs in our experiments.",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_39",
  "x": "However, this is very challenging for sequenceto-sequence model based on <cite>transformer</cite> as the boundary for each output token is unclear. For hybrid models, the latency is controllable by adjusting the size of the time restriction window. In our implementation, we still take the whole sequence as the input, but mask out the frames which are outside the time restriction window, i.e., the attention probabilities of those frames are set to be zero during training.",
  "y": "motivation"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_40",
  "x": "In addition, the numbers in Table 4 refer to the accumulated attention window. If the time window for each self-attention layer is [\u2212\u221e, 2], then the total accumulated time window for 6 self-attention layers would be [\u2212\u221e , 12] . For faster convergence, we used the <cite>transformer models</cite> with one more layer normalization as in section 4.2, although the offline results on the dev-other and test-other evaluation sets are slightly worse.",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_41",
  "x": "The table shows that when we limited the future context information for the <cite>transformer model</cite>, we obtained slightly worse results. However, contrary to our expectations, when we increased the right context size, we did not achieve higher recognition accuracy, although the CE losses were significantly reduced, e.g., from \u223c0.78 for the model with the attention window of [\u2212\u221e, 0] to \u223c0.70 for the model with the attention window of [\u2212\u221e, 24] from our setups. Although the convolution layers have already looked ahead 6 frame in total, we believe the future context information should still be helpful.",
  "y": "differences"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_42",
  "x": "---------------------------------- **SEQUENCE TRAINING RESULTS** In Table 5 , we show the sequence training results of the <cite>transformer</cite> model trained with the maximum mutual information (MMI) criterion.",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_43",
  "x": "---------------------------------- **CONCLUSION** While <cite>transformer</cite> has been very successful in the area of nature language processing, its application to speech recognition is mostly within the end-to-end architecture.",
  "y": "motivation"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_44",
  "x": "While <cite>transformer</cite> has been very successful in the area of nature language processing, its application to speech recognition is mostly within the end-to-end architecture. We are more interested in <cite>transformers</cite> for hybrid acoustic models as there is no theoretical issues for online streaming speech recognition. In this paper, we have presented a <cite>transformer</cite> model with interleaved self-attention and convolution for hybrid acoustic modeling, although this structure may be also applicable to end-to-end models.",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_45",
  "x": "In this paper, we have presented a <cite>transformer</cite> model with interleaved self-attention and convolution for hybrid acoustic modeling, although this structure may be also applicable to end-to-end models. We have showed that the convolutional layers can improve the recognition accuracy with faster convergence compared to the model with self-attention layers only. We have also investigated several other aspects of the model including the impact of the positional encoding feature, dropout regularization as well training with and without the time restriction.",
  "y": "uses"
 },
 {
  "id": "42854f204c8d2a62822e12f731ad08_46",
  "x": [
   "We have showed that the convolutional layers can improve the recognition accuracy with faster convergence compared to the model with self-attention layers only. We have also investigated several other aspects of the model including the impact of the positional encoding feature, dropout regularization as well training with and without the time restriction. Our work is an addition to the current study of self-attention for hybrid models with a sequential TDNN and self-attention architecture trained with time restriction only."
  ],
  "y": "future_work"
 },
 {
  "id": "42ca932eaa96c174cdfb815bee82cb_0",
  "x": "The principle of dependency length minimization makes conditional predictions, which do not imply that placing heads at the center is optimal in general (Ferreri-Cancho, 2015) . In the context of just one head and at least one dependent, the principle predicts that the central placement of the head is optimal when there are at least two dependents but that placement is irrelevant if there is only one dependent. In the context of an initial verb followed by two arguments (e.g., subject and object), the principle predicts that the heads of the verbal arguments are placed first with respect to their dependents, e.g., articles or adjectives follow the head noun, whereas, for a final verb, the prediction is that the heads of the arguments are placed last with respect to their dependents, e.g. articles or adjectives precede the head noun<cite> (Ferrer-i-Cancho, 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "42ca932eaa96c174cdfb815bee82cb_1",
  "x": "In the context of an initial verb followed by two arguments (e.g., subject and object), the principle predicts that the heads of the verbal arguments are placed first with respect to their dependents, e.g., articles or adjectives follow the head noun, whereas, for a final verb, the prediction is that the heads of the arguments are placed last with respect to their dependents, e.g. articles or adjectives precede the head noun<cite> (Ferrer-i-Cancho, 2015)</cite> . The principle of dependency length minimization predicts consistent branching, to some extent, for verb-first or verb-last placements. 1 If there is one head, the principle predicts a rather symmetric head placement but various heads can lead to an anti-symmetric placement <cite>(Ferrer-i-Cancho, 2015</cite> , 2008 .",
  "y": "background"
 },
 {
  "id": "42ca932eaa96c174cdfb815bee82cb_2",
  "x": "An alternative solution is considering the placement of the verb as a parameter. From that single parameter we have shown that it is possible to infer head-final placement within verb arguments in SOV, head-first placement within verb arguments in SVO, head-first placement within verb arguments of VSO/VOS<cite> (Ferrer-i-Cancho, 2015)</cite> and also head first for the placement of inflected auxiliaries in SOV and head-final for their placement in VSO (Ferrer-i-Cancho, 2008 ). However, the predictive power of the position of the verb does not imply that this is a fundamental parameter in a universal grammar sense: verb position might be determined by evolutionary time (e.g., verb-last being more likely in early stages of evolution) or sentence length (e.g., SVO being more likely in languages where speakers produce more elaborate sentences or simply longer sentences) as explained by Ferrer-i-Cancho (2014) .",
  "y": "background motivation"
 },
 {
  "id": "42ca932eaa96c174cdfb815bee82cb_3",
  "x": "Fourth, an explanation for the relative placement of verbal auxiliaries given the placement of the main verb <cite>(Ferrer-i-Cancho, 2015</cite> , 2008 would be lost. A theory covering all the phenomena reached originally by the principle of online memory minimization would be heavier. 2 The monotonic dependency between cognitive cost and distance<cite> Ferrer-i-Cancho (2015)</cite> assumes that the cognitive cost of a dependency is a strictly monotonic function of its length.",
  "y": "background"
 },
 {
  "id": "42ca932eaa96c174cdfb815bee82cb_4",
  "x": "A theory covering all the phenomena reached originally by the principle of online memory minimization would be heavier. 2 The monotonic dependency between cognitive cost and distance<cite> Ferrer-i-Cancho (2015)</cite> assumes that the cognitive cost of a dependency is a strictly monotonic function of its length. Alday (2015) considers that this assumption may not be valid.",
  "y": "background"
 },
 {
  "id": "42ca932eaa96c174cdfb815bee82cb_5",
  "x": "Third, notice that<cite> Ferrer-i-Cancho (2015)</cite> assumes that the cost of a dependency of length d is g(d). In that setup, the identity of the dependents is irrelevant. However, we may consider a more general definition of the cost of a dependency between two units u and v: g(u, v, d).",
  "y": "background"
 },
 {
  "id": "42ca932eaa96c174cdfb815bee82cb_6",
  "x": "3 The unit of measurement of dependency length A limitation of<cite> Ferrer-i-Cancho (2015)</cite> is that dependency length is measured in words. However, it could be measured in other linguistic units: syllables, morphemes, phonemes, etc. A higher level of precision might illuminate inconsistencies between the dominant order according to the criteria in Dryer (2013) and other orders that appear recurrently in particular circumstances.",
  "y": "background motivation"
 },
 {
  "id": "43422cf92d8cbb280c0b4c590632f1_0",
  "x": "A strategy to combat uncertainty about neuronal activity is to materialize information by relative values instead of absolute ones. For instance, this is the way that sparse associative memories based on neural cliques [11] are recovered through the Winner-Takes-All (WTA) mechanism which has shown high biological plausibility [12] . Furthermore, the WTA principle is applied in several studies [13, 14,<cite> 15]</cite> for increase activation sparseness in neural networks.",
  "y": "background"
 },
 {
  "id": "43422cf92d8cbb280c0b4c590632f1_1",
  "x": "Starting from the biological perspective, by the comprehension of brain access schemes, it can be infered that the combination of binary synaptic weights, sparsely encoded memory patterns and local learning rules is one way of producing good representation [17, 18] . And as fast information retrieval remains a critical point to the real world applications, such a method proves again its power as the computation made is basically a vector-matrix product (in the binary case this can still be considered just counting), followed by an operation of threshold [19] . Recent work has started to address as well the problematic of memory footprint when learning word embeddings [20,<cite> 15]</cite> .",
  "y": "background motivation"
 },
 {
  "id": "43422cf92d8cbb280c0b4c590632f1_2",
  "x": "Recent work has started to address as well the problematic of memory footprint when learning word embeddings [20,<cite> 15]</cite> . In order to mitigate such an issue, <cite>[15]</cite> proposed a method that learns multi-codebook. Therefore, each word is now represented by a hash code of discrete numbers, that have to be defined in a way that similar words will have similar representations as well as a factor of difference, to capture nuances.",
  "y": "future_work background"
 },
 {
  "id": "43422cf92d8cbb280c0b4c590632f1_3",
  "x": "Therefore, each word is now represented by a hash code of discrete numbers, that have to be defined in a way that similar words will have similar representations as well as a factor of difference, to capture nuances. Presenting a different use for the previous work done by the field of codebooks compression based source coding, known as product quantization (PQ) [21] and additive quantization [22] , they show that by minimizing the squared distance between both distributions (baseline and composed embeddings), and using a direct learning approach for the codes in an end-to-end neural network, with a Gumbel-softmax layer [23] to encourage the discreteness <cite>[15]</cite> , it is possible to construct the word embeddings radically reducing the number of parameters without hurting performance. These word codebooks are learned using a local WTA rule in the hidden layer of an autoencoder neural network.",
  "y": "background"
 },
 {
  "id": "43422cf92d8cbb280c0b4c590632f1_4",
  "x": "In the second step, a deep autoencoder neural network model is optimized to reduce the mean squared error of reconstructed contextual features extracted from first step. This model implements the Gumbel Softmax reparametrization function [23] to obtain discrete activations in the last intermediate layer. For the sake of simplicity, we adopted the same architecture as <cite>[15]</cite> , as shown in Fig. 2 .",
  "y": "uses"
 },
 {
  "id": "43422cf92d8cbb280c0b4c590632f1_5",
  "x": "Our framework is an extension of <cite>[15]</cite> work to deal with latent representations of contextual pretrained LM. For future work, we intend to explore other LM recent architectures such as BERT [2] , and also aggregate information from knowledge-based corpus such as WordNet [29] . ----------------------------------",
  "y": "extends"
 },
 {
  "id": "43a52325987ea035136a6a718389d9_0",
  "x": "However, to date their exploitation has been limited because for most languages, no Levin style classification is available. Since manual classification is costly (Kipper et al., 2008) automatic approaches have been proposed recently which could be used to learn novel classifications in a cost-effective manner (Joanis et al., 2008; Li and Brew, 2008; \u00d3 S\u00e9aghdha and Copestake, 2008; Vlachos et al., 2009;<cite> Sun and Korhonen, 2009</cite> ). However, most work on Levin type classification has focussed on English.",
  "y": "background"
 },
 {
  "id": "43a52325987ea035136a6a718389d9_1",
  "x": "We take a recent verb clustering approach developed for English <cite>(Sun and Korhonen, 2009</cite> ) and apply it to French -a major language for which no such experiment has been conducted yet. Basic NLP resources (corpora, taggers, parsers and subcategorization acquisition systems) are now sufficiently developed for this language for the application of a state-ofthe-art verb clustering approach to be realistic. Our investigation reveals similarities between the English and French classifications, supporting the linguistic hypothesis (Jackendoff, 1990) and the earlier result of Merlo et al. (2002) that Levin classes have a strong cross-linguistic basis.",
  "y": "uses"
 },
 {
  "id": "43a52325987ea035136a6a718389d9_2",
  "x": "This large subcategorization lexicon provides SCF frequency information for 3,297 French verbs. It was acquired fully automatically from Le Monde newspaper corpus (200M words from years 1991-2000) using ASSCI -a recent subcategorization acquisition system for French (Messiant, 2008) . Systems similar to ASSCI have been used in recent verb classification works e.g. (Schulte im Walde, 2006; Li and Brew, 2008;<cite> Sun and Korhonen, 2009</cite> ).",
  "y": "similarities"
 },
 {
  "id": "43a52325987ea035136a6a718389d9_3",
  "x": "We adopt a fully unsupervised approach to SP acquisition using the method of<cite> Sun and Korhonen (2009)</cite> , with the difference that we determine the optimal number of SP clusters automatically following Zelnik-Manor and Perona (2004) . The method is introduced in the following section. The approach involves (i) taking the GRs (SUBJ, OBJ, IOBJ) associated with verbs, (ii) extracting all the argument heads in these GRs, and (iii) clustering the resulting N most frequent argument heads into M classes.",
  "y": "extends differences"
 },
 {
  "id": "43a52325987ea035136a6a718389d9_4",
  "x": "---------------------------------- **CLUSTERING METHODS** Spectral clustering (SPEC) has proved promising in previous verb clustering experiments (Brew and Schulte im Walde, 2002;<cite> Sun and Korhonen, 2009</cite> ) and other similar NLP tasks involving high dimensional feature space (Chen et al., 2006) .",
  "y": "background"
 },
 {
  "id": "43a52325987ea035136a6a718389d9_5",
  "x": "Following<cite> Sun and Korhonen (2009)</cite> we used the MNCut spectral clustering (Meila and Shi, 2001 ) which has a wide applicability and a clear probabilistic interpretation (von Luxburg, 2007; Verma and Meila, 2005) . However, we extended the method to determine the optimal number of clusters automatically using the technique proposed by (Zelnik-Manor and Perona, 2004) . Clustering groups a given set of verbs V = {v n } N n=1 into a disjoint partition of K classes.",
  "y": "similarities uses"
 },
 {
  "id": "43a52325987ea035136a6a718389d9_6",
  "x": "We employ the same measures for evaluation as previously employed e.g. by\u00d3 S\u00e9aghdha and Copestake (2008) and<cite> Sun and Korhonen (2009)</cite> . The first measure is modified purity (mPUR) -a global measure which evaluates the mean precision of clusters. Each cluster is associated with its prevalent class.",
  "y": "similarities uses"
 },
 {
  "id": "43a52325987ea035136a6a718389d9_7",
  "x": "Table 2 shows F-measure results for all the features. The 4th column of the table shows, for comparison, the results of<cite> Sun and Korhonen (2009)</cite> obtained for English when they used the same features as us, clustered them using SPEC, and evaluated them against the English version of our gold standard, also using F-measure 2 . As expected, SPEC (the 2nd column) outperforms K-Means (the 3rd column).",
  "y": "similarities"
 },
 {
  "id": "43a52325987ea035136a6a718389d9_8",
  "x": "Although the results at different thresholds are not comparable due to the different number of verbs and classes (see columns 2-3), the results for features at the same threshold are. Those results suggest that when 2000 or more occurrences per verb are used, most features perform like they performed for English in the experiment of<cite> Sun and Korhonen (2009)</cite> which is not typical to many other classes. Interestingly, Levin classes 29.2, 36.1, 37.3, and 37.7 were among the best performing classes also in the supervised verb classification experiment of Sun et al. (2008) because these classes have distinctive characteristics also in English.",
  "y": "similarities"
 },
 {
  "id": "43a52325987ea035136a6a718389d9_9",
  "x": "When the best features are used, many individual Levin classes have similar performance in the two languages. Due to differences in data sets direct comparison of performance figures for English and French is not possible. When considering the general level of performance, our best performance for French (65.4 F) is lower than the best performance for English in the experiment of<cite> Sun and Korhonen (2009)</cite> .",
  "y": "differences"
 },
 {
  "id": "43a52325987ea035136a6a718389d9_10",
  "x": "However, parser and feature extraction performance can also play a big role in overall accuracy, and should therefore be investigated further <cite>(Sun and Korhonen, 2009</cite> ). The relatively low performance of basic LP features in French suggests that at least some of the current errors are due to parsing. Future research should investigate the source of error at different stages of processing.",
  "y": "future_work"
 },
 {
  "id": "44916cd85311c78666839a3376ccc6_0",
  "x": "One crucial part of the AM pipeline is to segment written text into argumentative and nonargumentative units. Recent research in the area of unit segmentation (Eger et al., 2017;<cite> Ajjour et al., 2017)</cite> has lead to promising results with F1-scores of up to 0.90 for in-domain segmentation (Eger et al., 2017) . Nevertheless, there is still a need for more robust approaches.",
  "y": "background"
 },
 {
  "id": "44916cd85311c78666839a3376ccc6_1",
  "x": "Further, <cite>Ajjour et al. (2017)</cite> proposed a setup with three bidirectional LSTMs (Bi-LSTMs) (Schuster and Paliwal, 1997) in total as their best solution. While the first two of them are fully connected and work on word embeddings and task-specific features respectively, the intention for the third is to take the output of the first two as input and learn to correct their errors. Even though the third Bi-LSTM did not improve on the F1-score metric, it did succeed in resolving some of the wrong consecutive token predictions, without worsening the final results.",
  "y": "background"
 },
 {
  "id": "44916cd85311c78666839a3376ccc6_2",
  "x": "This framework has been applied previously for the same task (Stab, 2017; Eger et al., 2017;<cite> Ajjour et al., 2017)</cite> . The architectures proposed in this section build on <cite>Ajjour et al. (2017)</cite> , omitting the second Bi-LSTM, which was used to process features other than word embeddings (see section 3.1). They are further being modified by adding attention layers at different positions.",
  "y": "similarities"
 },
 {
  "id": "44916cd85311c78666839a3376ccc6_3",
  "x": "A (B) label denotes that the token is at the beginning of an argumentative unit, an (I) label that it lies inside a unit and an (O) label that the token is not part of a unit. This framework has been applied previously for the same task (Stab, 2017; Eger et al., 2017;<cite> Ajjour et al., 2017)</cite> . The architectures proposed in this section build on <cite>Ajjour et al. (2017)</cite> , omitting the second Bi-LSTM, which was used to process features other than word embeddings (see section 3.1).",
  "y": "extends differences"
 },
 {
  "id": "44916cd85311c78666839a3376ccc6_4",
  "x": "This is supposed to allow the head to focus on specific features of the input. In this case, the self-attention layers use additive attention, while the multi-head attention layers use scaled dot-product attention, with the latter following the implementation of Vaswani et al. (2017) . Baseline re-implementation The baseline model from <cite>Ajjour et al. (2017)</cite> uses a total of three Bi-LSTMs (two of them fully connected) to assign labels to tokens (see Figure 1a) .",
  "y": "similarities uses"
 },
 {
  "id": "44916cd85311c78666839a3376ccc6_5",
  "x": "This model will be referred to as baseline +input . The second variation adds the attention layer after the first and before the second Bi-LSTM, which will be called baseline +error . According to <cite>Ajjour et al. (2017)</cite> , the latter Bi-LSTM is used to correct the errors of the first one.",
  "y": "similarities"
 },
 {
  "id": "44916cd85311c78666839a3376ccc6_6",
  "x": "As a performance measure, we report the weighted F1-score instead of the macro F1-score, since it takes the imbalance of the samples per label into account. For our re-implementation of the baseline, we are able to approximately reproduce the results reported by <cite>Ajjour et al. (2017)</cite> . Additionally, we can verify that there is no major change in the performance when adding a second Bi-LSTM to the network (compare results for bilstm and baseline in Table 1 ).",
  "y": "similarities uses"
 },
 {
  "id": "45238fe9b493ccdf5921c8f5284097_0",
  "x": "Research in automatic summarization has made headway over the years with single document summarization as the front-runner due to the availability of large datasets (Sandhaus, 2008; Hermann et al., 2015;<cite> Narayan et al., 2018b)</cite> which has enabled the development of novel methods, many of them employing recent advances in neural networks (See et al., 2017; Narayan et al., 2018c; , inter alia). * The work was primarily done while Shashi was still at School of Informatics, University of Edinburgh. 1 Our dataset and code are available at https:// github.com/sheffieldnlp/highres Figure 1 : Highlight-based evaluation of a summary.",
  "y": "background"
 },
 {
  "id": "45238fe9b493ccdf5921c8f5284097_1",
  "x": "For the above reasons manual evaluation is considered necessary for measuring progress in summarization. However, the intrinsic difficulty of the task has led to research without manual evaluation or only fluency being assessed manually. Those that conduct manual assessment of the content, typically use a single reference summary, either directly (Celikyilmaz et al., 2018; Tan et al., 2017) or through questions <cite>(Narayan et al., 2018b</cite>,c) and thus are also likely to exhibit reference bias.",
  "y": "background"
 },
 {
  "id": "45238fe9b493ccdf5921c8f5284097_2",
  "x": "Those that conduct manual assessment of the content, typically use a single reference summary, either directly (Celikyilmaz et al., 2018; Tan et al., 2017) or through questions <cite>(Narayan et al., 2018b</cite>,c) and thus are also likely to exhibit reference bias. In this paper we propose a novel approach for manual evaluation, HIGHlight-based Referenceless Evaluation of document Summarization (HIGHRES), in which a summary is assessed against the source document via manually highlighted salient content in the latter (see Figure 1 for an example). Our approach avoids reference bias, as the multiple highlights obtained help consider more content than what is contained in a single reference.",
  "y": "differences"
 },
 {
  "id": "45238fe9b493ccdf5921c8f5284097_3",
  "x": "Furthermore, we propose to evaluate the clarity of a summary separately from its fluency, as they are different dimensions. Finally, HIGHRES provides absolute instead of ranked evaluation, thus the assessment of a system can be conducted and interpreted without reference to other systems. To validate our proposed approach we use the recently introduced eXtreme SUMmarization dataset (XSUM,<cite> Narayan et al., 2018b)</cite> to evaluate two state-of-the-art abstractive summarization methods, Pointer Generator Networks (See et al., 2017) and Topic-aware Convolutional Networks<cite> (Narayan et al., 2018b)</cite> , using crowd-sourcing for both highlight annotation and quality judgments.",
  "y": "uses"
 },
 {
  "id": "45238fe9b493ccdf5921c8f5284097_4",
  "x": "In what follows, we discuss previously proposed approaches along three axes: evaluation metrics, relative vs. absolute, and the choice of reference. Evaluation Metrics Despite differences in the exact definitions, the majority (e.g., Hsu et al., 2018; Celikyilmaz et al., 2018;<cite> Narayan et al., 2018b</cite>; Chen and Bansal, 2018; Peyrard and Gurevych, 2018) agree on both or either one of two broad quality definitions: coverage determines how much of the salient content of the source document is captured in the summary, and informativeness, how much of the content captured in the summary is salient with regards to the original document. These measures correspond to \"recall\" and \"precision\" metrics respectively in Table 1 : Overview of manual evaluations conducted in recent summarization systems.",
  "y": "background"
 },
 {
  "id": "45238fe9b493ccdf5921c8f5284097_5",
  "x": "Clarke and Lapata (2010) proposed a question-answering based approach to improve the agreement among human evaluations for the quality of summary content, which was recently employed by<cite> Narayan et al. (2018b)</cite> and Narayan et al. (2018c) (QA in Table 1 ). In this approach, questions were created first from the reference summary and then the system summaries were judged with regards to whether they enabled humans to answer those questions correctly. ShafieiBavani et al. (2018) , on the other hand, used the \"Pyramid\" method (Nenkova and Passonneau, 2004 ) which requires summaries to be annotated by experts for salient information.",
  "y": "background"
 },
 {
  "id": "45238fe9b493ccdf5921c8f5284097_6",
  "x": "Most recent work uses a single human judgment to capture all linguistic qualities of the summary (Hsu et al., 2018; Kry\u015bci\u0144ski et al., 2018;<cite> Narayan et al., 2018b</cite>; Song et al., 2018; Guo et al., 2018) ; we group them under \"Fluency\" in Table 1 with an exception of \"Clarity\" which was evaluated in the DUC evaluation campaigns (Dang, 2005) . The \"Clarity\" metric puts emphasis in easy identification of noun and pronoun phrases in the summary which is a different dimension than \"Fluency\", as a summary may be fluent but difficult to be understood due to poor clarity. Absolute vs Relative Summary Ranking.",
  "y": "uses"
 },
 {
  "id": "45238fe9b493ccdf5921c8f5284097_7",
  "x": "The relative assessment is often done using the paired comparison (Thurstone, 1994) or the best-worst scaling (Woodworth and G, 1991; Louviere et al., 2015) , to improve inter-annotator agreement. On the other hand, absolute assessment of summarization (Li et al., 2018b; Song et al., 2018; Kry\u015bci\u0144ski et al., 2018; Hsu et al., 2018; Hardy and Vlachos, 2018 ) is often done using the Likert rating scale (Likert, 1932) where a summary is assessed on a numerical scale. Absolute assessment was also employed in combination with the question answering approach for content evaluation <cite>(Narayan et al., 2018b</cite>; Mendes et al., 2019) .",
  "y": "uses"
 },
 {
  "id": "45238fe9b493ccdf5921c8f5284097_8",
  "x": "The question answering approach of Narayan et al. (2018b,c) also falls in this category, as the questions were written using the reference summary. However, summarization datasets are limited to a single reference summary per document (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018;<cite> Narayan et al., 2018b)</cite> thus evaluations using them is prone to reference bias (Louis and Nenkova, 2013) , also a known issue in machine translation evaluation (Fomicheva and Specia, 2016) . A circumvention for this issue is to evaluate it against the source document (Song et al., 2018; Narayan et al., 2018a; Hsu et al., 2018; Kry\u015bci\u0144ski et al., 2018) , asking judges to assess the summary after reading the source document.",
  "y": "motivation"
 },
 {
  "id": "45238fe9b493ccdf5921c8f5284097_9",
  "x": "---------------------------------- **SUMMARIZATION DATASET AND MODELS** We use the extreme summarization dataset (XSUM,<cite> Narayan et al., 2018b)</cite> 2 which comprises BBC articles paired with their singlesentence summaries, provided by the journalists writing the articles.",
  "y": "uses"
 },
 {
  "id": "45238fe9b493ccdf5921c8f5284097_10",
  "x": "Following<cite> Narayan et al. (2018b)</cite> , we didn't use the whole test set portion, but sampled 50 articles from it for our highlight-based evaluation. We assessed summaries from two state-ofthe-art abstractive summarization systems using our highlight-based evaluation: (i) the PointerGenerator model (PTGEN) introduced by See et al. (2017) is an RNN-based abstractive systems which allows to copy words from the source text, and (ii) the Topic-aware Convolutional Sequence to Sequence model (TCONVS2S) introduced by<cite> Narayan et al. (2018b)</cite> is an abstractive model which is conditioned on the article's topics and based entirely on Convolutional Neural Networks. We used the pre-trained models 3 provided by the authors to obtain summaries from both systems for the documents in our test set.",
  "y": "uses"
 },
 {
  "id": "45238fe9b493ccdf5921c8f5284097_11",
  "x": "The summary in the XSUM dataset demonstrates a larger number of novel ngrams compared to other popular datasets such as CNN/DailyMail (Hermann et al., 2015) or NY Times (Sandhaus, 2008) as such it is suitable to be used for our experiment since the more abstractive nature of the summary renders automatic methods such as ROUGE less accurate as they rely on string matching, and thus calls for human evaluation for more accurate system comparisons. Following<cite> Narayan et al. (2018b)</cite> , we didn't use the whole test set portion, but sampled 50 articles from it for our highlight-based evaluation. We assessed summaries from two state-ofthe-art abstractive summarization systems using our highlight-based evaluation: (i) the PointerGenerator model (PTGEN) introduced by See et al. (2017) is an RNN-based abstractive systems which allows to copy words from the source text, and (ii) the Topic-aware Convolutional Sequence to Sequence model (TCONVS2S) introduced by<cite> Narayan et al. (2018b)</cite> is an abstractive model which is conditioned on the article's topics and based entirely on Convolutional Neural Networks.",
  "y": "uses"
 },
 {
  "id": "45238fe9b493ccdf5921c8f5284097_12",
  "x": "The superiority of TCONVS2S is expected; TCONVS2S is better than PTGEN for recognizing pertinent content and generating informative summaries due to its ability to represent high-level document knowledge in terms of topics and long-range dependencies<cite> (Narayan et al., 2018b)</cite> . We further measured the agreement among the judges using the coefficient of variation (Everitt, 2006) from the aggregated results. It is defined as the ratio between the sample standard deviation and sample mean.",
  "y": "similarities"
 },
 {
  "id": "45238fe9b493ccdf5921c8f5284097_13",
  "x": "When comparing the reference summaries against the original documents, both ROUGE and HROUGE confirm that the reference summaries are rather abstractive as reported by<cite> Narayan et al. (2018b)</cite> , and they in fact score below the system summaries. Recall scores are very low in all cases which is expected, since the 10 highlights obtained per document or the documents themselves, taken together, are much longer than any of the summaries. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "45551e674210bb9bbb56c8778d2f8c_0",
  "x": "To model homophily, recent research in abusive language detection on Twitter<cite> (Mishra et al., 2018a)</cite> incorporates embeddings for authors (i.e., users who have composed tweets) that encode the structure of their surrounding communities. The embeddings (called author profiles) are generated by applying a node embedding framework to an undirected unlabeled community graph where nodes denote the authors and edges the follower-following relationships amongst them on Twitter. However, these profiles do not capture the linguistic behavior of the authors and their communities and do not convey whether their tweets tend to be abusive or not.",
  "y": "background"
 },
 {
  "id": "45551e674210bb9bbb56c8778d2f8c_1",
  "x": "Pavlopoulos et al. (2017b) showed that including randomly-initialized user embeddings improved the performance of their RNN methods. Qian et al. (2018) Following previous work<cite> (Mishra et al., 2018a)</cite> , we experiment with a subset of the Twitter dataset compiled by Waseem and Hovy (2016 ----------------------------------",
  "y": "uses"
 },
 {
  "id": "45551e674210bb9bbb56c8778d2f8c_2",
  "x": "**REPRESENTING ONLINE COMMUNITIES** We create two different graphs: the first one is identical to the community graph of <cite>Mishra et al. (2018a)</cite> (referred to as the community graph). It contains 1, 875 nodes representing each of the authors in the dataset.",
  "y": "uses similarities"
 },
 {
  "id": "45551e674210bb9bbb56c8778d2f8c_3",
  "x": "---------------------------------- **GENERATING AUTHOR PROFILES** We first describe the approach of <cite>Mishra et al. (2018a)</cite> that learns author embeddings using node2vec (Grover and Leskovec, 2016) ; this serves as our baseline.",
  "y": "uses background"
 },
 {
  "id": "45551e674210bb9bbb56c8778d2f8c_4",
  "x": "The former captures the structural role of nodes, while the latter captures the local neighborhood around them. Two hyper-parameters control the overall contribution of each of these strategies. Following <cite>Mishra et al. (2018a)</cite> , we initialize these parameters to their default value of 1 and set the embedding size and number of iterations to 200 and 25 respectively.",
  "y": "uses"
 },
 {
  "id": "45551e674210bb9bbb56c8778d2f8c_5",
  "x": "Character n-grams have been shown to be highly effective for abuse detection due to their robustness to spelling variations. LR + AUTH. This is the state of the art method<cite> (Mishra et al., 2018a)</cite> for the dataset we are using.",
  "y": "background"
 },
 {
  "id": "45551e674210bb9bbb56c8778d2f8c_6",
  "x": "GCN on its own achieves a high performance, particularly on the sexism class where its performance is typical of a community-based profiling approach, i.e., high recall at the expense of precision. However, on the racism class, its recall is hindered by the same factor that <cite>Mishra et al. (2018a)</cite> highlighted for their node2vec-only method, i.e., that racist tweets come from 5 unique authors only who have also contributed sexist or clean tweets. The racist activity of these authors is therefore eclipsed, leading to misclassifications of their tweets.",
  "y": "similarities"
 },
 {
  "id": "45551e674210bb9bbb56c8778d2f8c_7",
  "x": "---------------------------------- **CONCLUSIONS** In this paper, we built on the work of <cite>Mishra et al. (2018a)</cite> that introduces community-based profiling of authors for abusive language detection.",
  "y": "extends"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_0",
  "x": "We propose a variant of a well-known machine translation (MT) evaluation metric, HyTER <cite>(Dreyer and Marcu, 2012)</cite> , which exploits reference translations enriched with meaning equivalent expressions. The original <cite>HyTER metric</cite> relied on hand-crafted paraphrase networks which restricted its applicability to new data. We test, for the first time, <cite>HyTER</cite> with automatically built paraphrase lattices.",
  "y": "extends"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_1",
  "x": "We propose a variant of a well-known machine translation (MT) evaluation metric, HyTER <cite>(Dreyer and Marcu, 2012)</cite> , which exploits reference translations enriched with meaning equivalent expressions. We test, for the first time, <cite>HyTER</cite> with automatically built paraphrase lattices.",
  "y": "uses"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_2",
  "x": "We propose a variant of a well-known machine translation (MT) evaluation metric, HyTER <cite>(Dreyer and Marcu, 2012)</cite> , which exploits reference translations enriched with meaning equivalent expressions. The original <cite>HyTER metric</cite> relied on hand-crafted paraphrase networks which restricted its applicability to new data. We test, for the first time, <cite>HyTER</cite> with automatically built paraphrase lattices.",
  "y": "motivation extends"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_3",
  "x": "We propose a variant of a well-known machine translation (MT) evaluation metric, HyTER <cite>(Dreyer and Marcu, 2012)</cite> , which exploits reference translations enriched with meaning equivalent expressions. We show that although <cite>the metric</cite> obtains good results on small and carefully curated data with both manually and automatically selected substitutes, it achieves medium performance on much larger and noisier datasets, demonstrating the limits of <cite>the metric</cite> for tuning and evaluation of current MT systems.",
  "y": "motivation extends"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_4",
  "x": "The HyTER metric <cite>(Dreyer and Marcu, 2012 )</cite> relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and <cite>HyTER</cite> rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. <cite>HyTER</cite> spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data.",
  "y": "background"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_5",
  "x": "The HyTER metric <cite>(Dreyer and Marcu, 2012 )</cite> relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and <cite>HyTER</cite> rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. <cite>HyTER</cite> spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data.",
  "y": "background"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_6",
  "x": "The HyTER metric <cite>(Dreyer and Marcu, 2012 )</cite> relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. <cite>HyTER</cite> spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data.",
  "y": "motivation"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_7",
  "x": "<cite>HyTER</cite> spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We propose to use an embedding-based lexical substitution model (Melamud et al., 2015) for building this type of reference networks and test, for the first time, the metric with automatically generated lattices (hereafter HyTERA). We show that HyTERA strongly correlates with <cite>HyTER</cite> with hand-crafted lattices, and approximates the hTER score (Snover et al., 2006) as measured using post-edits made by human annotators.",
  "y": "similarities uses"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_8",
  "x": "<cite>HyTER</cite> spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We propose to use an embedding-based lexical substitution model (Melamud et al., 2015) for building this type of reference networks and test, for the first time, the metric with automatically generated lattices (hereafter HyTERA). Furthermore, we generate lattices for standard datasets from a recent WMT Metrics Shared Task and perform the first evaluation of <cite>HyTER</cite> on large and noisier datasets.",
  "y": "uses"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_9",
  "x": "**THE ORIGINAL HYTER METRIC** The HyTER metric <cite>(Dreyer and Marcu, 2012)</cite> computes the similarity between a translation hypothesis and a reference lattice that compactly encodes millions of meaning-equivalent translations. Formally <cite>HyTER</cite> is defined as:",
  "y": "background"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_10",
  "x": "The HyTER metric <cite>(Dreyer and Marcu, 2012)</cite> computes the similarity between a translation hypothesis and a reference lattice that compactly encodes millions of meaning-equivalent translations. Formally <cite>HyTER</cite> is defined as: where Y is a set of references that can be encoded as a finite state automaton such as the one represented in Figure 1 , x is a translation hypothesis and LS is the standard Levenshtein distance, defined as the minimum number of substitutions, deletions and insertions required to transform x into y. We use, in all our experiments, our own im-plementation of <cite>HyTER</cite> 1 that relies on the Open-FST framework (Allauzen et al., 2007) .",
  "y": "background"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_11",
  "x": "Formally <cite>HyTER</cite> is defined as: where Y is a set of references that can be encoded as a finite state automaton such as the one represented in Figure 1 , x is a translation hypothesis and LS is the standard Levenshtein distance, defined as the minimum number of substitutions, deletions and insertions required to transform x into y. We use, in all our experiments, our own im-plementation of <cite>HyTER</cite> 1 that relies on the Open-FST framework (Allauzen et al., 2007) . Contrary to the original <cite>HyTER</cite> implementation, we do not consider permutations when transforming x into y as previous results (cf.",
  "y": "background"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_12",
  "x": "Contrary to the original <cite>HyTER</cite> implementation, we do not consider permutations when transforming x into y as previous results (cf. Table 3 in <cite>(Dreyer and Marcu, 2012)</cite> ) have shown that permutations have only very little impact while significantly increasing the computational complexity of <cite>HyTER</cite> computation. 2 We also use an exact search rather than a A * search to minimize Equation (1).",
  "y": "extends differences"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_13",
  "x": "Table 3 in <cite>(Dreyer and Marcu, 2012)</cite> ) have shown that permutations have only very little impact while significantly increasing the computational complexity of <cite>HyTER</cite> computation. 2 We also use an exact search rather than a A * search to minimize Equation (1). <cite>The HyTER metric</cite> has already been successfully used in MT evaluation but only with handcrafted lattices.",
  "y": "differences background"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_14",
  "x": "We first use the setting of <cite>Dreyer and Marcu (2012)</cite> , in Section \u00a7 5.1, to compare the score estimated by <cite>HyTER</cite> and HyTERA to hTER scores. In Section \u00a7 5.2, we explore whether HyTERA can reliably predict human translation quality scores from the WMT16 Metrics Shared Task. 5 Comparing HyTER and HyTERA",
  "y": "uses differences"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_15",
  "x": "**OPEN MT NIST EVALUATION** To evaluate the performance of HyTER, <cite>Dreyer and Marcu (2012)</cite> examine whether it can approximate the hTER score (Snover et al., 2006) that measures the number of edits required to change a system output into its post-edition. hTER scores are a good estimate of translation quality and usefulness, but require each translation hypothesis to be corrected by a human annotator.",
  "y": "background"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_16",
  "x": "hTER scores are a good estimate of translation quality and usefulness, but require each translation hypothesis to be corrected by a human annotator. <cite>Dreyer and Marcu (2012)</cite> show that it can be closely approximated by <cite>HyTER</cite> scores. In this section, we reproduce their experiments with HyTERA to see whether it is possible to use automatically-built rather than hand-crafted references to approximate hTER scores.",
  "y": "background"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_17",
  "x": "We propose to use an embedding-based lexical substitution model (Melamud et al., 2015) for building this type of reference networks and test, for the first time, the metric with automatically generated lattices (hereafter HyTERA). 9 In all cases, there is a high correlation between <cite>HyTER</cite>, HyTERA and hTER, significantly higher than the correlation between BLEU and hTER.",
  "y": "similarities"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_18",
  "x": "This observation shows that replacing the handcrafted lattices with automatically built ones has only a moderate impact on the H<cite>yTER</cite> metric quality: automatic lattices result in a small drop of the correlation when evaluating hypotheses translated from Chinese, and slightly improve it for the Arabic to English condition. Overall HyTERA scores are highly correlated with <cite>HyTER</cite> scores (\u03c1 = 0.766 for Arabic and \u03c1 = 0.756 for Chinese). More importantly, considering the filtered lattices allows to significantly reduce computation time compared to the allPars ones without hurting the quality estimation capacity of the metric.",
  "y": "extends similarities"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_19",
  "x": "We propose to use an embedding-based lexical substitution model (Melamud et al., 2015) for building this type of reference networks and test, for the first time, the metric with automatically generated lattices (hereafter HyTERA). Overall HyTERA scores are highly correlated with <cite>HyTER</cite> scores (\u03c1 = 0.766 for Arabic and \u03c1 = 0.756 for Chinese).",
  "y": "similarities"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_20",
  "x": "9 In all cases, there is a high correlation between <cite>HyTER</cite>, HyTERA and hTER, significantly higher than the correlation between BLEU and hTER. This observation shows that replacing the handcrafted lattices with automatically built ones has only a moderate impact on the H<cite>yTER</cite> metric quality: automatic lattices result in a small drop of the correlation when evaluating hypotheses translated from Chinese, and slightly improve it for the Arabic to English condition. Overall HyTERA scores are highly correlated with <cite>HyTER</cite> scores (\u03c1 = 0.766 for Arabic and \u03c1 = 0.756 for Chinese).",
  "y": "background"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_21",
  "x": "9 In all cases, there is a high correlation between <cite>HyTER</cite>, HyTERA and hTER, significantly higher than the correlation between BLEU and hTER. This observation shows that replacing the handcrafted lattices with automatically built ones has only a moderate impact on the H<cite>yTER</cite> metric quality: automatic lattices result in a small drop of the correlation when evaluating hypotheses translated from Chinese, and slightly improve it for the Arabic to English condition. Overall HyTERA scores are highly correlated with <cite>HyTER</cite> scores (\u03c1 = 0.766 for Arabic and \u03c1 = 0.756 for Chinese).",
  "y": "background"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_22",
  "x": "Note that the tested systems were selected by NIST to cover a variety of system architectures (statistical, rule-based, hybrid) and performances <cite>(Dreyer and Marcu, 2012)</cite> , which makes distinction between them an easy task for all metrics. The benefits of using a metric like <cite>HyTER</cite>, which focuses on the word level, are much clearer in the sentence-based evaluation (Table 2) . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_23",
  "x": "All metrics rank the systems in the same order, except from <cite>HyTER</cite> with allParsFiltered that only inverts two systems. Note that the tested systems were selected by NIST to cover a variety of system architectures (statistical, rule-based, hybrid) and performances <cite>(Dreyer and Marcu, 2012)</cite> , which makes distinction between them an easy task for all metrics. The benefits of using a metric like <cite>HyTER</cite>, which focuses on the word level, are much clearer in the sentence-based evaluation (Table 2) .",
  "y": "similarities"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_24",
  "x": "All metrics rank the systems in the same order, except from <cite>HyTER</cite> with allParsFiltered that only inverts two systems. Note that the tested systems were selected by NIST to cover a variety of system architectures (statistical, rule-based, hybrid) and performances <cite>(Dreyer and Marcu, 2012)</cite> , which makes distinction between them an easy task for all metrics. The benefits of using a metric like <cite>HyTER</cite>, which focuses on the word level, are much clearer in the sentence-based evaluation (Table 2) .",
  "y": "motivation"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_25",
  "x": "Table 3 reports the results achieved by HyTERA on the six language pairs of the WMT16 Shared Task and its rank among the other metrics tested in the competition. HyTERA obtains medium performance on the WMT16 dataset, which is much larger and noisier than the dataset used for evaluation in <cite>(Dreyer and Marcu, 2012)</cite> : it is made, for each language, of 560 translations sampled from outputs of all systems taking part in the WMT15 campaign. It is important to note that the hTER scores used in the initial <cite>HyTER</cite> evaluation were produced by experienced LDC annotators, while the WMT16 Direct Assessment (DA) adequacy judgments were collected from non-experts through crowd-sourcing (Bojar et al., 2016) .",
  "y": "differences"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_26",
  "x": "It is important to note that the hTER scores used in the initial <cite>HyTER</cite> evaluation were produced by experienced LDC annotators, while the WMT16 Direct Assessment (DA) adequacy judgments were collected from non-experts through crowd-sourcing (Bojar et al., 2016) . HyTERA achieves higher performance than the SENTBLEU baseline in four language pairs (cs/de/ru/tr-en). It obtains slightly lower correlation than SENTBLEU for fi-en and ro-en, the language pairs in which correlation was lower for all metrics.",
  "y": "background"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_27",
  "x": "It is important to note that HyTERA needs no tuning, is straightforward to use and very fast to compute, especially with filtered lattices (on average 6s). The lower performance of the metric on this dataset is also due to the different nature of the MT systems tested. While in the <cite>(Dreyer and Marcu, 2012)</cite> evaluation, the systems came from the 2010 Open MT NIST evaluation and were selected to cover a variety of architectures and performances, the systems that participated in WMT15 are, for the large part, neural MT systems (Bojar et al., 2015) .",
  "y": "background"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_28",
  "x": "The lower performance of the metric on this dataset is also due to the different nature of the MT systems tested. While in the <cite>(Dreyer and Marcu, 2012)</cite> evaluation, the systems came from the 2010 Open MT NIST evaluation and were selected to cover a variety of architectures and performances, the systems that participated in WMT15 are, for the large part, neural MT systems (Bojar et al., 2015) . As reported by Bentivogli et al. (2016) , Neural MT systems make at least 17% fewer lexical choice errors than phrase-based systems, which limits the potential of HyTERA, primarily focused on capturing correct lexical choice.",
  "y": "differences"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_29",
  "x": "---------------------------------- **CONCLUSION** We have proposed a method for automatic paraphrase lattice creation which makes the H<cite>yTER metric</cite> applicable to new datasets.",
  "y": "extends"
 },
 {
  "id": "45ba2841e91a2fd62f0534aeaf7491_30",
  "x": "**CONCLUSION** We have proposed a method for automatic paraphrase lattice creation which makes the H<cite>yTER metric</cite> applicable to new datasets. We provide the first evaluation of <cite>HyTER</cite> on data from a recent WMT Metrics Shared task.",
  "y": "uses"
 },
 {
  "id": "45c4e9bc90d28cd1a5a61393625062_0",
  "x": "On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words across languages. These approaches seem to have reached a plateau in terms of performance. More recently, and departing from such traditional approaches, we have proposed in<cite> (Li and Gaussier, 2010)</cite> an approach based on improving the comparability of the corpus under consideration, prior to extracting bilingual lexicons.",
  "y": "background"
 },
 {
  "id": "45c4e9bc90d28cd1a5a61393625062_1",
  "x": "**THE COMPARABILITY MEASURE** In order to measure the degree of comparability of bilingual corpora, we make use of the measure M developed in<cite> (Li and Gaussier, 2010)</cite> : Given a comparable corpus P consisting of an English part P e and a French part P f , the degree of comparability of P is defined as the expectation of finding the translation of any given source/target word in the target/source corpus vocabulary. Let \u03c3 be a function indicating whether a translation from the translation set T w of the word w is found in the vocabulary P v of a corpus P, i.e.:",
  "y": "uses"
 },
 {
  "id": "45c4e9bc90d28cd1a5a61393625062_2",
  "x": "In our experiments, we use the method described in this paper, as well as the one in<cite> (Li and Gaussier, 2010)</cite> which is the only alternative method to enhance corpus comparability. ---------------------------------- **IMPROVING CORPUS QUALITY**",
  "y": "uses"
 },
 {
  "id": "45c4e9bc90d28cd1a5a61393625062_3",
  "x": "After the clustering process, we obtain the resulting corpora P 1 (with the external corpus P 1 T ) and P 2 (with P 2 T ). As mentioned before, we also used the method described in<cite> (Li and Gaussier, 2010)</cite> on the same data, producing resulting corpora P 1 (with P 1 T ) and P 2 (with P 2 T ) from P 0 . In terms of lexical coverage, P 1 (resp. P 2 ) covers 97.9% (resp.",
  "y": "uses"
 },
 {
  "id": "45c4e9bc90d28cd1a5a61393625062_4",
  "x": "In a first series of experiments, bilingual lexicons were extracted from the corpora obtained by our approach (P 1 and P 2 ), the corpora obtained by the approach described in <cite>(Li and Gaussier, 2010</cite> ) (P 1 and P 2 ) and the original corpus P 0 , with the fixed N value set to 20. Table 1 displays the results obtained. Each of the last two columns \"P 1 > P 0 \" and \"P 2 > P 0 \" contains the absolute and the relative difference (in %) w.r.t.",
  "y": "uses"
 },
 {
  "id": "45c4e9bc90d28cd1a5a61393625062_5",
  "x": "Some works like (Munteanu et al., 2004) and (Munteanu and Marcu, 2006) propose methods to extract parallel fragments from comparable corpora. However, their approach only focuses on a very small part of the original corpus, whereas our work aims at preserving most of the vocabulary of the original corpus. We have followed here the general approach in<cite> (Li and Gaussier, 2010)</cite> which consists in enhancing the quality of a comparable corpus prior to extracting information from it.",
  "y": "uses"
 },
 {
  "id": "45d4d6f0ac4a4f3bf7b2ac70fbcf7f_0",
  "x": "We term our problem Concurrent Dialogue Acts (CDA) Recognition. Forum discussion is a type of dialogue that breeds CDAs. The sample in Figure 1 from MSDialog-Intent corpus <cite>[14]</cite> discusses a technical issue with Microsoft Bingo among two users (U 1 & U 2) and two agents (A1 & A2).",
  "y": "background"
 },
 {
  "id": "45d4d6f0ac4a4f3bf7b2ac70fbcf7f_1",
  "x": "Qu et al. <cite>[14]</cite> apply a CNN-based text classifier proposed by Kim [8] using a fixed window to represent the context. Although capable of classifying utterances with CDAs, Qu et al. <cite>[14]</cite> 's model only concerns a strictly-local context range and thus cannot include distant information. In this paper, we present a novel neural model that is adapted from Convolutional Recurrent Neural Network (CRNN) to both incorporate the interaction between distant utterances and generalize the DA recognition task to accommodate CDA.",
  "y": "background"
 },
 {
  "id": "45d4d6f0ac4a4f3bf7b2ac70fbcf7f_2",
  "x": "Qu et al. <cite>[14]</cite> apply a CNN-based text classifier proposed by Kim [8] using a fixed window to represent the context. Although capable of classifying utterances with CDAs, Qu et al. <cite>[14]</cite> 's model only concerns a strictly-local context range and thus cannot include distant information. In this paper, we present a novel neural model that is adapted from Convolutional Recurrent Neural Network (CRNN) to both incorporate the interaction between distant utterances and generalize the DA recognition task to accommodate CDA.",
  "y": "motivation"
 },
 {
  "id": "45d4d6f0ac4a4f3bf7b2ac70fbcf7f_3",
  "x": "**DATASET** We use the MSDialog-Intent dataset <cite>[14]</cite> to conduct experiments. In the dataset, each of the 10,020 utterances is annotated with a subset of 12 DAs.",
  "y": "uses"
 },
 {
  "id": "45d4d6f0ac4a4f3bf7b2ac70fbcf7f_4",
  "x": "For generalizability, our model only incorporates textual content of the dialogues. Besides, unlike Qu et al. <cite>[14]</cite> , we keep all the DA annotations in the dataset to preserve the meaningful DA structures within and across utterances. 1",
  "y": "differences"
 },
 {
  "id": "45d4d6f0ac4a4f3bf7b2ac70fbcf7f_7",
  "x": "Following previous work <cite>[14]</cite> on multi-label classification, we adopt label-based accuracy (i.e., Hamming score) and micro-F 1 score as our main evaluation metrics. Micro-precision and micro-recall are also reported to assist the analysis. Among all, accuracy is the only metrics that is on a per utterance basis.",
  "y": "uses"
 },
 {
  "id": "460b820360d51d99b4a7ae3f0755bc_0",
  "x": "Firstly WSD tasks before 2013 generally relied on only a lexicon, such as WordNet (Fellbaum, 1998) or an alternative equivalent, whereas SemEval 2013 Task 12 WSD and this task <cite>(Moro and Navigli, 2015)</cite> included Entity Linking (EL) using the encyclopaedia Wikipedia via BabelNet (Navigli and Ponzetto, 2012) . Secondly, as shown by Manion and Sainudiin (2014) with a simple linear regression, the iterative approach increases WSD performance for documents that have a higher degree of document monosemy -the percentage of unique monosemous lemmas in a document. As seen in Figures 1(a) to (i) on the previous page, named entities (or unique rather than common nouns) are more monosemous compared to other parts of speech, especially for more technical domains.",
  "y": "background"
 },
 {
  "id": "460b820360d51d99b4a7ae3f0755bc_1",
  "x": "While the iterative approach achieved reasonably competitive results in English, this success did not translate as well to Spanish and Italian. The Italian Biomedical domain had the highest document monosemy, observable in Figure 1 (g ), yet this did not help the iterative Run2 and Run3. Yet it is worth noting the results of the task paper <cite>(Moro and Navigli, 2015)</cite> report that SUDOKU Run2 and Run3 achieved very low F-Scores for named entity disambiguation (<28.6) in Spanish and Italian.",
  "y": "background"
 },
 {
  "id": "460b820360d51d99b4a7ae3f0755bc_2",
  "x": "The author could not improve on their superior results achieved in English, however for Spanish and Italian the BabelNet First Sense (BFS) baseline was much lower since it often resorted to lexicographic sorting in the absence of WordNet synsets -see (Navigli et al., 2013) . The author's baseline-independent submissions were unaffected by this, which on reviewing results in <cite>(Moro and Navigli, 2015)</cite> appears to have helped SUDOKU do best for these languages. Table 3 : F1 scores for each domain/language for SUDOKU and LIMSI.",
  "y": "background"
 },
 {
  "id": "46a23364b7bc51493d83f874a824ad_0",
  "x": "parse tree, either as co-indexed \"traces\", such as in the Penn Treebank, as illustrated in Figure 1 , or as arcs as in a dependency representation. In practice, current statistical parsers do not encode LDD directly, as illustrated in Figure 2 , and leave it to post-processing procedures to recover the LDD relation (Johnson, 2002;<cite> Nivre et al., 2010)</cite> . These approaches exploit the very strong constraints that govern long-distance relations syntactically, and ignore the full or partial recovery of the semantic roles entirely.",
  "y": "background motivation"
 },
 {
  "id": "46a23364b7bc51493d83f874a824ad_1",
  "x": "This figure illustrates the Stanford dependency representation that was used in Rimmel et al. (2009), and<cite> Nivre et al. (2010)</cite> , indicating below the sentence the long distance dependency that needs to be recovered, but that is not in the representation. The first tree encodes the subject relation between ac- Figure 3 : LDDs represented as a syntactic dependency tree above the sentence (in blue) and argument structure labels under the sentence (in green). The label A0 stands for AGENT and A1 stands for THEME.",
  "y": "background"
 },
 {
  "id": "46a23364b7bc51493d83f874a824ad_2",
  "x": "These sentences cover seven types of long-distance relations, illustrated in Figure 4 : object extraction from relative clauses (ORC) in (3) or from reduced relative clauses (ORed) in (4), subject extraction from relative clauses (SRC) in (5) or from an embedded clause (SEmb) in (6), free relatives (Free) in (7), object-oriented questions (OQ) in (8), and right node raising constructions (RNR) in (9). Compared to the other statistical dependency parsers, questions (OQ) are not well represented in our training data, since they do not include the additional QB data<cite> (Nivre et al., 2010)</cite> used to improve the performance of MSTParser and MaltParser. ----------------------------------",
  "y": "differences background"
 },
 {
  "id": "46a23364b7bc51493d83f874a824ad_3",
  "x": "---------------------------------- **PARSING SET UP** Like the dependency parser in<cite> Nivre et al. (2010)</cite> , the parser was not trained on the same data or tree representations as those used in the test data.",
  "y": "similarities"
 },
 {
  "id": "46a23364b7bc51493d83f874a824ad_4",
  "x": "Unlike<cite> Nivre et al. (2010)</cite> , we did not use an external part-of-speech tagger to annotate the data of the development set. To minimize preprocessing of the data, we choose to have part-ofspeech tagging as an internal part of the parsing model, which therefore, takes raw input. In order for our results to be comparable to those reported in previous evaluations (Rimell et al., 2009;<cite> Nivre et al., 2010)</cite> , we ran the parser \"out of the box\" directly on the test sentences, without using the development sentences to finetune.",
  "y": "differences"
 },
 {
  "id": "46a23364b7bc51493d83f874a824ad_6",
  "x": "Like in previous papers (Rimell et al., 2009;<cite> Nivre et al., 2010)</cite> , we evaluate the parser on its ability to recover LDDs. Two evaluations were done. The first one was semi-automatic, performed with a modified version of the evaluation script developed in Rimell et al. (2009) . An independent manual evaluation was also performed.",
  "y": "similarities"
 },
 {
  "id": "46a23364b7bc51493d83f874a824ad_7",
  "x": "However, in this evaluation the representations vary across models and exact matches would not allow a fair assessment. Both previous evaluation exercises (Rimell et al., 2009;<cite> Nivre et al., 2010)</cite> suggest some avenues to relax the matching conditions, and define equivalence classes of representations. ----------------------------------",
  "y": "background"
 },
 {
  "id": "46a23364b7bc51493d83f874a824ad_8",
  "x": "**EQUIVALENCE CLASSES OF ARCS** To relax the requirement of exact match on the definition of arc, a set of equivalence classes between single arcs and paths connecting two nodes indirectly is precisely defined in the post-processing scheme of<cite> Nivre et al. (2010)</cite> , which applies to the Stanford labelling scheme. In<cite> Nivre et al. (2010)</cite> , the encoding of long-distance dependencies in a dependency parser is categorised as simple, complex, and indirect.",
  "y": "background"
 },
 {
  "id": "46a23364b7bc51493d83f874a824ad_9",
  "x": "In<cite> Nivre et al. (2010)</cite> , the encoding of long-distance dependencies in a dependency parser is categorised as simple, complex, and indirect. In the simple case, the LDD coincides with an arc in a tree. In the complex case, the LDD is represented by a path of arcs. In the indirect case, the dependency is not directly encoded in a path in the tree, but it must be inferred from a larger portion of the tree using heuristics. The two last cases require post-processing of the tree.",
  "y": "background"
 },
 {
  "id": "46a23364b7bc51493d83f874a824ad_10",
  "x": "Following<cite> Nivre et al. (2010)</cite> , we define a longdistance dependency as simple or complex. In the simple case, the LDD coincides with an arc in a tree. A complex dependency is defined as a path of at most two simple dependencies.",
  "y": "similarities"
 },
 {
  "id": "46a23364b7bc51493d83f874a824ad_11",
  "x": "Automatic and manual results (percent recall) are shown in Table 1 , where we compare our results to the relevant ones of those reported in previous evaluations (Rimell et al., 2009; <cite>Nivre et al., 2010</cite>; Nguyen et al., 2012) . 8 These papers compare several statistical parsers. Some parsers like Nguyen, the C&C parser (Clark and Curran, 2007) and Enju (Miyao and Tsujii, 2005) are based on rich grammatical formalisms, and others others are representative of statistical dependency parsers (MST, MALT, (McDonald, 2006; Nivre et al., 2006) These last two parsers constitute the relevant comparison for our approach.",
  "y": "background"
 },
 {
  "id": "46a23364b7bc51493d83f874a824ad_12",
  "x": "9 Like the other parsers discussed in Rimell et al. (2009) and<cite> Nivre et al. (2010)</cite> , the overall performance on these long-distance constructions is much lower than the overall scores for this parser. However, the parser recovers long-distance dependencies at least as well as standard statistical dependency parsers that use a post-processing step, and better than standard statistical parsers. 10 The differences in recall between manual and automatic evaluation in Table 1 show that the automatic evaluation is sometimes too strict and sometimes too lenient.",
  "y": "similarities differences"
 },
 {
  "id": "46a23364b7bc51493d83f874a824ad_13",
  "x": "We classify the errors made by our parser on the development set based on<cite> Nivre et al. (2010)</cite> one which occurs when the parser fails to assign the correct functional relation (e.g., subject, object), while a Sem error is one in which the parser fails to assign the correct semantic relation (e.g., A1, A2). Nivre et al's Link error is one where the parser fails to find a dependency by coordination in the case of right node raising. Our restrictive modifications follow the constraints indicated above on what counts as a correct dependency.",
  "y": "uses"
 },
 {
  "id": "46a23364b7bc51493d83f874a824ad_14",
  "x": "Global errors are most frequent for OQ, ORC and SRC. Questions (OQ) are not well represented in our training data, since they do not include the additional QB data<cite> (Nivre et al., 2010)</cite> used to improve the performance of MSTParser and MaltParser (see Table 4 for comparison of number of errors for each parser). With respect to ORC and SRC, most Global errors are related to part-ofspeech tagging errors and wrong head assignment of complex NPs which are modified by the relevant relative clause.",
  "y": "differences"
 },
 {
  "id": "46b9079fb1dd6b4626f20819ccfa07_0",
  "x": "We describe experiments on the Switchboard speech recognition task. The syntactic features provide an additional 0.3% reduction in test-set error rate beyond the model of (Roark et al., 2004a;<cite> Roark et al., 2004b</cite> ) (significant at p < 0.001), which makes use of a discriminatively trained n-gram model, giving a total reduction of 1.2% over the baseline Switchboard system. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "46b9079fb1dd6b4626f20819ccfa07_1",
  "x": "See (Chen and Goodman, 1998) for an overview of estimation techniques for n-gram models. This paper describes a method for incorporating syntactic features into the language model, using discriminative parameter estimation techniques. We build on the work in Roark et al. (2004a;<cite> 2004b)</cite> , which was summarized and extended in Roark et al. (2005) .",
  "y": "extends"
 },
 {
  "id": "46b9079fb1dd6b4626f20819ccfa07_2",
  "x": "A second contrast between our work and previous work, including that of Rosenfeld et al. (2001) , is in the use of discriminative parameter estimation techniques. The criterion we use to optimize the parameter vector\u1fb1 is closely related to the end goal in speech recognition, i.e., word error rate. Previous work (Roark et al., 2004a;<cite> Roark et al., 2004b)</cite> has shown that discriminative methods within an ngram approach can lead to significant reductions in WER, in spite of the features being of the same type as the original language model.",
  "y": "similarities"
 },
 {
  "id": "46b9079fb1dd6b4626f20819ccfa07_3",
  "x": "---------------------------------- **GLOBAL LINEAR MODELS** We follow the framework of Collins (2002; 2004) , recently applied to language modeling in Roark et al. (2004a;<cite> 2004b)</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "46b9079fb1dd6b4626f20819ccfa07_4",
  "x": "As one example, the language modeling features might take into account n-grams, for example through definitions such as \u03a6 2 (a, w) = Count of the the in w Previous work (Roark et al., 2004a;<cite> Roark et al., 2004b</cite> ) considered features of this type. In this paper, we introduce syntactic features, which may be sensitive to the parse tree for w, for example where S \u2192 NP VP is a context-free rule production.",
  "y": "background"
 },
 {
  "id": "46b9079fb1dd6b4626f20819ccfa07_5",
  "x": "We briefly review the two training algorithms described in<cite> Roark et al. (2004b)</cite> , the perceptron algorithm and global conditional log-linear models (GCLMs). Figure 1 shows the perceptron algorithm. It is an online algorithm, which makes several passes over the training set, updating the parameter vector after each training example.",
  "y": "similarities uses"
 },
 {
  "id": "46b9079fb1dd6b4626f20819ccfa07_6",
  "x": "A second parameter estimation method, which was used in <cite>(Roark et al., 2004b)</cite> , is to optimize the log-likelihood under a log-linear model. Similar approaches have been described in Johnson et al. (1999) and Lafferty et al. (2001) . The objective function used in optimizing the parameters is",
  "y": "extends differences"
 },
 {
  "id": "46b9079fb1dd6b4626f20819ccfa07_7",
  "x": "We refer to these models as global conditional loglinear models (GCLMs). Each of these algorithms has advantages. A number of results-e.g., in Sha and Pereira (2003) and<cite> Roark et al. (2004b)</cite> -suggest that the GCLM approach leads to slightly higher accuracy than the perceptron training method.",
  "y": "background"
 },
 {
  "id": "46b9079fb1dd6b4626f20819ccfa07_8",
  "x": "---------------------------------- **EXPERIMENTS** The experimental set-up we use is very similar to that of Roark et al. (2004a;<cite> 2004b)</cite> , and the extensions to that work in Roark et al. (2005) .",
  "y": "similarities uses"
 },
 {
  "id": "46b9079fb1dd6b4626f20819ccfa07_9",
  "x": [
   "The results presented in this paper are a first step in examining the potential utility of syntactic features for discriminative language modeling for speech recognition. We tried two possible sets of features derived from the full annotation, as well as a variety of possible feature sets derived from shallow parse and POS tag sequences, the best of which gave a small but significant improvement beyond what was provided by the n-gram features. Future work will include a further investigation of parserderived features."
  ],
  "y": "future_work"
 },
 {
  "id": "46faad9d86cda118df5eb9c1e7df65_0",
  "x": "This motivated us to explore the data programming approach that exploits expert linguistic knowledge in a more compact and consistent rule based form. Given our interest in the analysis of multi-party dialogues, we used the STAC corpus of multiparty chats, an initial version of which is described in (Afantenos et al., 2015;<cite> Perret et al., 2016)</cite> . In all versions of this corpus, dialogue structures are directed acyclical graphs (DAGs) formed according to SDRT 2 (Asher and Lascarides, 2003; .",
  "y": "uses"
 },
 {
  "id": "46faad9d86cda118df5eb9c1e7df65_1",
  "x": "To compare our approach to earlier efforts, we also used the corpus from<cite> (Perret et al., 2016)</cite> . This corpus was also useful to check for over fitting of our Generative model developed on the multi-modal data. The corpus from<cite> (Perret et al., 2016)</cite> is an early version of a \"linguistic only\" version of the STAC corpus. It contains no nonlinguistic DUs, unlike the STAC multimodal corpus. 3 It also contains quite a few errors; for example, about 60 stories in the<cite> (Perret et al., 2016)</cite> dataset have no discourse structure in them at all and consist of only one DU.",
  "y": "uses"
 },
 {
  "id": "46faad9d86cda118df5eb9c1e7df65_2",
  "x": "3 It also contains quite a few errors; for example, about 60 stories in the<cite> (Perret et al., 2016)</cite> dataset have no discourse structure in them at all and consist of only one DU. We eliminated these from the Perret 2016 data set that we used in our comparative experiments below, as these sto-3 There is also on the STAC website an updated linguistic only version of the STAC corpus. It has 1,091 dialogues, 11,961 linguistic only DUs and 10,191 semantic relations.",
  "y": "differences"
 },
 {
  "id": "46faad9d86cda118df5eb9c1e7df65_3",
  "x": "The dataset from<cite> (Perret et al., 2016)</cite> is similar to our linguistic only STAC corpus but is still substantially different and degraded in quality. report significant error rates in annotation on the earlier versions of the STAC corpus and that the current linguistic only corpus of STAC offers an improvement over the<cite> (Perret et al., 2016)</cite> corpus. ries were obviously not a correct representation of what was going on in the game at the relevant point.",
  "y": "similarities"
 },
 {
  "id": "46faad9d86cda118df5eb9c1e7df65_4",
  "x": "report significant error rates in annotation on the earlier versions of the STAC corpus and that the current linguistic only corpus of STAC offers an improvement over the<cite> (Perret et al., 2016)</cite> corpus. ries were obviously not a correct representation of what was going on in the game at the relevant point. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "46faad9d86cda118df5eb9c1e7df65_5",
  "x": "2. 98% of the discourse relations in our development corpus span 10 DUs or less. To reduce class imbalance, we restricted the relations we consider to a distance of \u2264 10. 3. Following (Muller et al., 2012;<cite> Perret et al., 2016)</cite> we \"flatten\" CDUs by connecting all relations incoming or outgoing from a CDU to the \"head\" of the CDU, or its first DU.",
  "y": "uses"
 },
 {
  "id": "46faad9d86cda118df5eb9c1e7df65_6",
  "x": "We also performed these operations on our version of the linguistic only corpus used by<cite> (Perret et al., 2016)</cite> . ---------------------------------- **DATA PROGRAMMING EXPERIMENTS**",
  "y": "uses"
 },
 {
  "id": "46faad9d86cda118df5eb9c1e7df65_7",
  "x": "An attachment decision concerning two DUs is tightly linked to the type of relation relating the DUs: when an annotator decides that two DUs are attached, he or she does so with some knowledge of what type of relation attaches them. Figure 2 shows a sample LFs used for attachment prediction with the Result relation in mind. LFs also exploit information about the DUs' linguistic or non-linguistic status, the dialogue acts they express, their lexical content, grammatical category and speaker, and the distance between them-features also used in supervised learning methods <cite>(Perret et al., 2016</cite>; Afantenos et al., 2015; Muller et al., 2012) .",
  "y": "background"
 },
 {
  "id": "46faad9d86cda118df5eb9c1e7df65_8",
  "x": "A set of highly accurate predictions for individual candidates does not necessarily lead to accurate discourse structures; for instance, without global structural constraints, GEN and local models may not yield the directed acyclic graphs (DAGs), required by SDRT. As in previous work (Muller et al., 2012; Afantenos et al., 2015;<cite> Perret et al., 2016)</cite> , we use the Maximum Spanning Tree (MST) algorithm, and a variation thereof, to ensure that the dialogue structures predicted conform to some more general structural principle. We implemented the Chu-Liu-Edmonds algorithm (Chu, 1965; Edmonds, 1967) , an efficient method of finding the highest-scoring non-projective tree in a directed graph, as described in Jurafsky and Martin 5 .",
  "y": "uses"
 },
 {
  "id": "46faad9d86cda118df5eb9c1e7df65_9",
  "x": "Since SDRT structures can contain nodes with multiple incoming relations, i.e. are not always tree-like, we altered the MST algorithm in the manner of (Muller et al., 2012; Afantenos et al., 2015;<cite> Perret et al., 2016)</cite> , forcing the MST to include all high-probability incoming relations which do not create cycles. This produces MS-DAG structures which are in principle more faithful to SDRT. In addition, since discourse attachments in general follow an inverse power law (many short-distance attachments and fewer long-distance attachments), we implemented two MST/MS-DAG variants that always choose the shortest relation among multiple high-probability relations (MST/short and MS-DAG/short).",
  "y": "extends"
 },
 {
  "id": "46faad9d86cda118df5eb9c1e7df65_10",
  "x": "Table 2 shows that LogReg* was the best supervised learning method on the STAC data in terms of producing local models. This is evidence that hand crafted features capturing non local information about a DU's contexts do better than all purpose contextual encodings from neural nets at least on this task. We also implemented BERT+LogReg*, a learning algorithm that uses BERT's encodings together with a Logistic Regression classifier trained on STAC's gold data with handcrafted features from (Afantenos et al., 2015) and used in<cite> (Perret et al., 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "46faad9d86cda118df5eb9c1e7df65_11",
  "x": "The general inverse power law distribution of discourse attachments explains the good performance of the MST shortest link variant. GEN + \"MST-short\" has the highest attachment score of all approaches to the problem of attachment in the literature (Morey et al., 2018) , though we are cautious in comparing scores for systems applied to different corpora. Finally, we wanted to see how GEN and our other models fared on our version of the (Perret<cite> (Perret et al., 2016)</cite> data set, GEN has higher scores than LogReg*'s local model; but with a decoding mechanism similar to that reported in<cite> (Perret et al., 2016)</cite> , Lo-gReg*'s global model significantly improves over the GEN's.",
  "y": "uses"
 },
 {
  "id": "46faad9d86cda118df5eb9c1e7df65_12",
  "x": "The only reason GEN did not beat LogReg* is that it did not get a sufficient boost from decoding. We think that this happened because our LFs already contain a lot of global information about the discourse structure, which meant that MST had less of an effect. Note, however, that even on the<cite> (Perret et al., 2016)</cite> data set, the MST decoding mechanism provided LogReg* only a boost of 12 F1 points, as seen in Table 4 , which is significantly lower than what is reported in<cite> (Perret et al., 2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "472b7c9f53b3130d7e5bba772e5b88_0",
  "x": "**RELATED WORK** Unsupervised speech representation learning [2, 3, 4, 5, <cite>6,</cite> 7, 8, 9, 10] is effective in extracting high-level properties from speech. SLP downstream tasks can be improved through speech representations because surface features such as log Mel-spectrograms or waveform can poorly reveal the abundant information within speech.",
  "y": "background"
 },
 {
  "id": "472b7c9f53b3130d7e5bba772e5b88_1",
  "x": "Contrastive Predictive Coding (CPC) [5] and wav2vec [7] use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task. Autoregressive Predictive Coding (APC)<cite> [6]</cite> uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss. Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, <cite>6,</cite> 7] .",
  "y": "background"
 },
 {
  "id": "472b7c9f53b3130d7e5bba772e5b88_2",
  "x": "Contrastive Predictive Coding (CPC) [5] and wav2vec [7] use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task. Autoregressive Predictive Coding (APC)<cite> [6]</cite> uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss. Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, <cite>6,</cite> 7] .",
  "y": "background"
 },
 {
  "id": "472b7c9f53b3130d7e5bba772e5b88_3",
  "x": "Unlike previous left-to-right unidirectional approaches that only consider past sequences to predict information about future frames, the proposed method allows us to train a bidirectional speech representation model, alleviating the unidirectionality constraint of previous methods. As a result, the Mockingjay model obtains substantial improvements in several SLP tasks. Moreover, as previous approaches restrict the power of the pre-trained models to representation extraction only [5, <cite>6,</cite> 7, 8] , the proposed method is robust and can be fine-tuned easily on downstream tasks.",
  "y": "differences"
 },
 {
  "id": "472b7c9f53b3130d7e5bba772e5b88_4",
  "x": "**EXPERIMENT** Following previous works [2, 3, 4, 5, <cite>6,</cite> 7, 8] , we evaluate different features and representations on downstream tasks, including: phoneme classification, speaker recognition, and sentiment classification on spoken content. For a fair comparison, each downstream task uses an identical model architecture and hyperparameters despite different input features.",
  "y": "uses"
 },
 {
  "id": "472b7c9f53b3130d7e5bba772e5b88_5",
  "x": "**COMPARING WITH OTHER REPRESENTATIONS** The proposed approaches are mainly compared with APC<cite> [6]</cite> representations, as they also experiment on phone classification and speaker verification. As reported in<cite> [6]</cite> , the APC approach outperformed CPC representations [5, 7, 9] in both two tasks, which makes APC suitable as a strong baseline.",
  "y": "uses"
 },
 {
  "id": "472b7c9f53b3130d7e5bba772e5b88_6",
  "x": "The proposed approaches are mainly compared with APC<cite> [6]</cite> representations, as they also experiment on phone classification and speaker verification. As reported in<cite> [6]</cite> , the APC approach outperformed CPC representations [5, 7, 9] in both two tasks, which makes APC suitable as a strong baseline. APC uses an unidirectional autoregressive model.",
  "y": "uses background"
 },
 {
  "id": "485464efe36b1ca1f6dc4e1cf8d474_0",
  "x": "This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (Saon and Chien, 2012; Chan et al., 2016) to document summarization (Chang and Chien, 2009 ), text classification (Blei et al., 2003; Zhang et al., 2015) , text segmentation (Chien and Chueh, 2012) , information extraction (<cite>Narasimhan et al., 2016</cite>) , image caption generation (Vinyals et al., 2015; Xu et al., 2015) , sentence generation (Li et al., 2016b) , dialogue control (Zhao and Eskenazi, 2016; Li et al., 2016a) , sentiment classification, recommendation system, question answering (Sukhbaatar et al., 2015) and machine translation , to name a few. Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model. The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.",
  "y": "uses background"
 },
 {
  "id": "48def208400142f043a07be5d83713_0",
  "x": "We improve our RTM models<cite> (Bi\u00e7ici and Way, 2014)</cite>: \u2022 by using improved ParFDA instance selection model allowing better language models (LM) in which similarity judgments are made to be built with improved optimization and selection of the LM data, \u2022 by selecting TreeF features over source and translation data jointly instead of taking their intersection, \u2022 with extended learning models including bayesian ridge regression (Tan et al., 2015) , which did not obtain better performance than support vector regression in training results (Section 2.2).",
  "y": "differences background motivation"
 },
 {
  "id": "48def208400142f043a07be5d83713_1",
  "x": "\u2022 by selecting TreeF features over source and translation data jointly instead of taking their intersection, \u2022 with extended learning models including bayesian ridge regression (Tan et al., 2015) , which did not obtain better performance than support vector regression in training results (Section 2.2). We present top results with Referential Translation Machines (Bi\u00e7ici, 2015;<cite> Bi\u00e7ici and Way, 2014)</cite> at quality estimation task (QET15) in WMT15 (Bojar et al., 2015) .",
  "y": "uses background"
 },
 {
  "id": "48def208400142f043a07be5d83713_2",
  "x": "We improve our RTM models<cite> (Bi\u00e7ici and Way, 2014</cite> ): \u2022 by using improved ParFDA instance selection model allowing better language models (LM) in which similarity judgments are made to be built with improved optimization and selection of the LM data, \u2022 by selecting TreeF features over source and translation data jointly instead of taking their intersection, \u2022 with extended learning models including bayesian ridge regression (Tan et al., 2015) , which did not obtain better performance than support vector regression in training results (Section 2.2).",
  "y": "differences background motivation"
 },
 {
  "id": "48def208400142f043a07be5d83713_3",
  "x": "\u2022 with extended learning models including bayesian ridge regression (Tan et al., 2015) , which did not obtain better performance than support vector regression in training results (Section 2.2). We present top results with Referential Translation Machines (Bi\u00e7ici, 2015;<cite> Bi\u00e7ici and Way, 2014)</cite> at quality estimation task (QET15) in WMT15 (Bojar et al., 2015) . RTMs pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bi\u00e7ici and Yuret, 2015) as interpretants for reaching shared semantics.",
  "y": "uses background"
 },
 {
  "id": "48def208400142f043a07be5d83713_4",
  "x": "We present results using support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Sch\u00f6lkopf, 2004) for sentence and document translation prediction tasks and Global Linear Models (GLM) (Collins, 2002) with dynamic learning (GLMd) (Bi\u00e7ici, 2013;<cite> Bi\u00e7ici and Way, 2014)</cite> for word-level translation performance prediction. We also use these learning models after a feature subset selection (FS) with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009 ), or PLS after FS (FS+PLS). GLM relies on Viterbi decoding, perceptron learning, and flexible feature definitions.",
  "y": "uses background"
 },
 {
  "id": "48def208400142f043a07be5d83713_5",
  "x": "RTMs with FS followed by PLS and learning with SVR is able to achieve the top rank in this task. Task 2: Prediction of Word-level Translation Quality Task 2 is about binary classification of word-level quality. We develop individual RTM models for each subtask and use GLMd model (Bi\u00e7ici, 2013;<cite> Bi\u00e7ici and Way, 2014)</cite> , for predicting the quality at the word-level.",
  "y": "uses"
 },
 {
  "id": "48def208400142f043a07be5d83713_6",
  "x": "We compare the difficulty of tasks according to MRAER levels achieved. In Table 6 , we list the RTM test results for tasks and subtasks that predict HTER or METEOR from QET15, QET14<cite> (Bi\u00e7ici and Way, 2014)</cite> , and QET13 (Bi\u00e7ici, 2013) . The best results when predicting HTER are obtained this year.",
  "y": "uses"
 },
 {
  "id": "4924d721b50abe1c8d883a7efd0205_0",
  "x": "UKB is a collection of programs which was first released for performing graph-based Word Sense Disambiguation using a preexisting knowledge base such as WordNet, and attained state-of-the-art results among knowledge-based systems when evaluated on standard benchmarks<cite> Agirre et al., 2014)</cite> . In addition, UKB has been extended to perform disambiguation of medical entities (Agirre et al., 2010) , named-entities (Erbs et al., 2012; , word similarity and to create knowledge-based word embeddings (Goikoetxea et al., 2015) . All programs are open source 2 ,3 and are accompanied by the resources and instructions necessary to reproduce the results.",
  "y": "background"
 },
 {
  "id": "4924d721b50abe1c8d883a7efd0205_1",
  "x": "When UKB was released, the papers specified the optimal parameters for WSD<cite> Agirre et al., 2014)</cite> , as well as other key issues like the underlying knowledge-base version, specific set of relations to be used, and method to pre-process the input text. At the time, we assumed that future researchers would use the optimal parameters and settings specified in the papers, and that they would contact the authors if in doubt. The default parameters of the software were not optimal, and the other issues were left under the users responsibility.",
  "y": "background"
 },
 {
  "id": "4924d721b50abe1c8d883a7efd0205_2",
  "x": "**UKB PARAMETERS AND SETTING FOR WSD** When using UKB for WSD, the main parameters and settings can be classified in five main categories. For each of those we mention the best options and the associated UKB parameter when relevant (in italics), as taken from <cite>Agirre et al., 2014</cite> ):",
  "y": "uses"
 },
 {
  "id": "4924d721b50abe1c8d883a7efd0205_3",
  "x": "The frequencies are smoothed adding one to all counts (dict weight smooth). The sense frequency is used when initializing context words, and is also used to produce the final sense weights as a linear combination of sense frequencies and graph-based sense probabilities. The use of sense frequencies with UKB was introduced in<cite> (Agirre et al., 2014)</cite> .",
  "y": "background"
 },
 {
  "id": "4924d721b50abe1c8d883a7efd0205_4",
  "x": "5 This decrease could be caused by the fact that Raganato et al. (2017a) did not use sense frequencies. In addition to UKB, the table also reports the 5 Note that the UKB results for S2, S3 and S07 (62.6, 63.0 and 48.6 respectively) are different from those in<cite> (Agirre et al., 2014)</cite> , which is to be expected, as the new datasets have been converted to WordNet 3.0 (we confirmed experimentally that this is the sole difference between the two experiments). best performing knowledge-based systems on this dataset.",
  "y": "differences"
 },
 {
  "id": "4924d721b50abe1c8d883a7efd0205_5",
  "x": "In addition to the results of UKB using the setting in<cite> Agirre et al., 2014)</cite> as specified in Section 3, we checked whether some reasonable settings would obtain better results. Table 3 shows the results when applying the three algorithms described in Section 3, both with and without sense frequencies, as well as using a single sentence for context or extended context. The table shows that the key factor is the use of sense frequencies, and systems that do not use them (those with a nf subscript) suffer a loss between 7 and 8 percentage points in F1.",
  "y": "extends"
 },
 {
  "id": "497b717bc4ff6b9e2160ee823f6b42_0",
  "x": "Latest results are based on neural network experience, but are far more simple: various versions of Word2Vec, Skip-gram and CBOW models [8] , which currently show the State-of-the-Art results and have proven success with morphologically complex languages like Russian [1] ,<cite> [10]</cite> . These are corpus-based approaches, where one computes or trains the model from a large corpus. They usually consider some word context, like in bag-ofwords, where model is simple count of how often can some word be seen in context of a word being described.",
  "y": "background"
 },
 {
  "id": "497b717bc4ff6b9e2160ee823f6b42_1",
  "x": "Traditional corpora for the word semantic similarity task are News, Wikipedia, electronic libraries and others (e.g. RUSSE workshop<cite> [10]</cite> ). It was shown that type of corpus used for training affects the resulting accuracy. Twitter is not usually considered, and intuition behind this is that probably every-day language is too simple and too occasional to produce good results.",
  "y": "background"
 },
 {
  "id": "497b717bc4ff6b9e2160ee823f6b42_2",
  "x": "The inter-annotator agreement is measured and the mean value is put into dataset. Until recent days there was no such dataset for Russian language. To mitigate this the \"RUSSE: The First Workshop on Russian Semantic Similarity\"<cite> [10]</cite> was conducted, producing RUSSE Human-Judgements evaluation dataset (we will refer to it as HJ-dataset).",
  "y": "background"
 },
 {
  "id": "497b717bc4ff6b9e2160ee823f6b42_3",
  "x": "The RUSSE contest was followed by paper from its organizers<cite> [10]</cite> and several participators [1] , [5] , thus filling the gap in word semantic similarity task for Russian language. In this paper we evaluate a Word2Vec model, trained on Russian Twitter corpus against RUSSE HJ-dataset, and show results comparable to top results of other RUSSE competitors. ----------------------------------",
  "y": "background"
 },
 {
  "id": "497b717bc4ff6b9e2160ee823f6b42_4",
  "x": "To compute correlation we use Spearman coefficient, since it was used as accuracy measure in RUSSE<cite> [10]</cite> . ---------------------------------- **PROPERTIES OF THE DATA**",
  "y": "similarities uses"
 },
 {
  "id": "497b717bc4ff6b9e2160ee823f6b42_5",
  "x": "For convenience we replicate results for these corpora, originally presented in<cite> [10]</cite> , alongside with our result in Table 5 . Given these considerations we conclude that with size of Twitter corpus of 500M one can achieve reasonably good results on task of word semantic similarity. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "497b717bc4ff6b9e2160ee823f6b42_6",
  "x": "In this paper we investigated the use of Twitter corpus for training Word2Vec model for task of word semantic similarity. We described a method to obtain stream of Twitter messages and prepare them for training. We use HJ-dataset, which was created for RUSSE contest<cite> [10]</cite> to measure correlation between similarity of word vectors and human judgements on word pairs similarity.",
  "y": "uses"
 },
 {
  "id": "499580e888a1598681a8d877b07866_0",
  "x": "Kreutzer et al. (2015) apply neural networks using pre-trained alignments and word lookup-table to word-level QE, which achieve the excellent performance by using the combination of baseline features at word level. However, these are not 'pure' neural approaches for QE. <cite>Kim and Lee (2016)</cite> apply neural machine translation (NMT) models, based on recurrent neural network, to sentence-level QE.",
  "y": "background"
 },
 {
  "id": "499580e888a1598681a8d877b07866_1",
  "x": "**RECURRENT NEURAL NETWORK BASED QUALITY ESTIMATION MODEL** Recurrent neural network (RNN) based quality estimation model<cite> (Kim and Lee, 2016)</cite> consists of two parts: two bidirectional RNNs on the source and target sentences in the first part and another RNN for predicting the quality in the second part. In the first part (Figure 1 ), modified RNN-based NMT model generates quality vectors, which indicate a sequence of vectors about target words' translation qualities.",
  "y": "background"
 },
 {
  "id": "499580e888a1598681a8d877b07866_2",
  "x": "In this paper, we extend the recurrent neural network based quality estimation model to word and phrase level. Recurrent neural network (RNN) based quality estimation model<cite> (Kim and Lee, 2016)</cite> consists of two parts: two bidirectional RNNs on the source and target sentences in the first part and another RNN for predicting the quality in the second part.",
  "y": "extends uses"
 },
 {
  "id": "499580e888a1598681a8d877b07866_3",
  "x": "Each quality vector is generated by decomposing the probability of each target word from the modified NMT model. 1 <cite>Kim and Lee (2016)</cite> modify the NMT model to 1) use source and target A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output Figure 1: First part of recurrent neural network based quality estimation model for generating quality vectors<cite> (Kim and Lee, 2016)</cite> sentences as inputs, 2 2) apply bidirectional RNNs both on source and target sentences, which enable to fully utilize the bidirectional quality information, and 3) generate quality vectors for target words as outputs.",
  "y": "background"
 },
 {
  "id": "499580e888a1598681a8d877b07866_4",
  "x": "1 <cite>Kim and Lee (2016)</cite> modify the NMT model to 1) use source and target A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output Figure 1: First part of recurrent neural network based quality estimation model for generating quality vectors<cite> (Kim and Lee, 2016)</cite> sentences as inputs, 2 2) apply bidirectional RNNs both on source and target sentences, which enable to fully utilize the bidirectional quality information, and 3) generate quality vectors for target words as outputs. In the second part ( Figure 2 , 3 and 4), the final quality of translation at various level (sentencelevel/word-level/phrase-level) is predicted by using the quality vectors as inputs.",
  "y": "background"
 },
 {
  "id": "499580e888a1598681a8d877b07866_5",
  "x": "In the second part ( Figure 2 , 3 and 4), the final quality of translation at various level (sentencelevel/word-level/phrase-level) is predicted by using the quality vectors as inputs. <cite>Kim and Lee (2016)</cite> apply RNN based model to sentence-level QE and we extend this model to word and phraselevel QE. In subsection 2.1, 2.2 and 2.3, we describe the RNN based 3 (second part) sentence, word and phrase-level QE models.",
  "y": "extends uses"
 },
 {
  "id": "499580e888a1598681a8d877b07866_6",
  "x": "In RNN based sentence-level QE model (Figure 2) , HTER (human-targeted translation edit rate) (Snover et al., 2006) in [0,1] for target sentence is predicted by using a logistic sigmoid func-A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output<cite> (Kim and Lee, 2016)</cite> tion such that (1) W s is the weight matrix of sigmoid function 5 at sentence-level QE.",
  "y": "background"
 },
 {
  "id": "499580e888a1598681a8d877b07866_7",
  "x": "s is a summary unit of the sequential quality vectors and is fixed to the last hidden state 6 h T y of RNN. The hidden state h j is computed by where f is the activation function of RNN<cite> (Kim and Lee, 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "4a19f17c00e904a595c1703ab9318d_0",
  "x": "**INTRODUCTION** Thanks to recent advances in deep learning over the last two decades, machine translation (MT) has shown steady improvements (Bahdanau et al., 2014; Gehring et al., 2017;<cite> Vaswani et al., 2017)</cite> . Nevertheless, even the state-of-the-art MT systems are still often far from perfect.",
  "y": "background"
 },
 {
  "id": "4a19f17c00e904a595c1703ab9318d_1",
  "x": "Given the underlying assumption of the APE task in which the source text (src), the corresponding output of the unknown MT system (mt), and its human postedited sentences (pe) are the only available resources, many studies have attempted to leverage information from both src and mt by suggesting multi-source architectures (Chatterjee et al., 2016; Libovick\u00fd et al., 2016) . Furthermore, as the newly proposed architecture called the Transformer network<cite> (Vaswani et al., 2017)</cite> has shown to be effective in various sequence-to-sequence problems, various multi-source adaptations of the Transformer (Junczys-Dowmunt and Grundkiewicz, 2018; Shin and Lee, 2018; Tebbifakhr et al., 2018) have been applied to APE. In this work, we propose a new multi-source APE model by extending Transformer.",
  "y": "background"
 },
 {
  "id": "4a19f17c00e904a595c1703ab9318d_2",
  "x": "The model architecture, as shown in Figure 1 , is an extension of Transformer. As described in<cite> Vaswani et al. (2017)</cite> , each stacked layer is composed of multi-head attention networks and position-wise fully connected feed-forward networks. Unlike the original Transformer, to construct joint representations for two inputs, the encoder consists of two sub networks: src encoder (x) and mt encoder (x , y) , shown on the left and right side of the encoder in Figure 1 .",
  "y": "background"
 },
 {
  "id": "4a19f17c00e904a595c1703ab9318d_3",
  "x": "\uf0b7 (x , y) accepts both mt embeddings and the output of (x) as inputs and returns the final output of the encoder e = ( , \u2026 , ) in which each mt is jointly encoded with its corresponding src context through the attention layer. \uf0b7 Finally, the decoder, shown on the far right of Figure 1 , generates an output sequence z which maximizes (1) at each step by attending to relevant parts of the output of the encoder in an auto-regressive manner (Graves, 2013) . In addition to shared embedding described in<cite> Vaswani et al. (2017)</cite> , we also utilize weight sharing across the embedding and output layers in a manner similar to Junczys-Dowmunt and Grundkiewicz (2018).",
  "y": "uses"
 },
 {
  "id": "4a19f17c00e904a595c1703ab9318d_4",
  "x": "The multi-head attention layers marked with dashed lines in Figure 1 play an important role in constructing joint representations. As described in<cite> Vaswani et al. (2017)</cite> , we utilize the same multihead attention with h-heads based on scaled dotproduct attention to get matrix C composed of context vectors as follows: where",
  "y": "uses similarities"
 },
 {
  "id": "4a19f17c00e904a595c1703ab9318d_5",
  "x": "We trained our model for ~14K update steps with the Adam optimizer (Kingma and Ba, 2014), warm up learning rates<cite> (Vaswani et al., 2017</cite> ) with a size of 12,000, and batch size of approximately 17,000 tokens for each triplet. Other detailed settings were the same asVaswani et al. (2017) . To obtain a single trained model, it consumed ~14K update steps until convergence on the development set.",
  "y": "uses"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_0",
  "x": "Neural networks have long been criticized for lacking compositionality, leading critics to argue they are inappropriate for modeling language and thought [8, 23, 24] . Nonetheless neural architectures have continued to advance and make important contributions in natural language processing (NLP) [19] . Recent work has revisited these classic critiques through studies of modern neural architectures [10,<cite> 15,</cite> 3, 20, 22, 2, 6] , with a focus on the sequence-to-sequence (seq2seq) models used successfully in machine translation and other NLP tasks [32, 4, 36] .",
  "y": "background"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_1",
  "x": "Recent work has revisited these classic critiques through studies of modern neural architectures [10,<cite> 15,</cite> 3, 20, 22, 2, 6] , with a focus on the sequence-to-sequence (seq2seq) models used successfully in machine translation and other NLP tasks [32, 4, 36] . These studies show that powerful seq2seq approaches still have substantial difficulties with compositional generalization, especially when combining a new concept (\"to Facebook\") with previous concepts (\"slowly\" or \"eagerly\")<cite> [15,</cite> 3, 20] . New benchmarks have been proposed to encourage progress [10,<cite> 15,</cite> 2] , including the SCAN dataset for compositional learning <cite>[15]</cite> .",
  "y": "background"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_2",
  "x": "After training, the aim is to execute, zero-shot, novel instructions such as \"walk around right after look twice.\" Previous studies show that seq2seq recurrent neural networks (RNN) generalize well when the training and test sets are similar, but fail catastrophically when generalization requires systematic compositionality<cite> [15,</cite> 3, 20] . For instance, models often fail to understand how to \"jump twice\" after learning how to \"run twice,\" \"walk twice,\" and how to \"jump.\" Developing neural architectures with these compositional abilities remains an open problem. ----------------------------------",
  "y": "background"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_3",
  "x": "**EXPERIMENTS** 4.1 Architecture and training parameters I use a common architecture and training procedure for all experiments in this paper. The meta seq2seq architecture builds upon the seq2seq architecture from <cite>[15]</cite> that performed best across a range of SCAN evaluations.",
  "y": "extends"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_4",
  "x": "A greedy decoder is used since it is effective on SCAN's deterministic outputs <cite>[15]</cite> . In each experiment, the network is meta-trained for 10,000 episodes with the ADAM optimizer [14] . Halfway through training, the learning rate is reduced from 0.001 to 0.0001.",
  "y": "uses"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_5",
  "x": "This experiment applies meta seq2seq learning to the SCAN task of adding a new primitive <cite>[15]</cite> . Models are trained to generalize compositionally by decomposing the original SCAN seq2seq task into a series of related seq2seq sub-tasks. The goal is to learn a new primitive instruction and use it compositionally, operationalized in SCAN as the \"add jump\" split <cite>[15]</cite> .",
  "y": "uses"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_6",
  "x": "The goal is to learn a new primitive instruction and use it compositionally, operationalized in SCAN as the \"add jump\" split <cite>[15]</cite> . Models learn a new primitive \"jump\" and aim to use it compositionally in other instructions, resembling the \"to Facebook\" example introduced earlier in this paper. First, the original seq2seq problem from <cite>[15]</cite> is described.",
  "y": "uses"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_7",
  "x": "Also during training, the models are exposed to all primitive and composed instructions for the other actions (e.g., \"walk\", \"walk twice\", \"look around right and walk twice\", etc.) along with the correct output sequences, which is about 13,000 unique instructions. Following <cite>[15]</cite> , the critical \"jump\" demonstration is overrepresented in training to ensure it is learned. During test, models are evaluated on all of the composed instructions that use the \"jump\" primitive, examining the ability to integrate new primitives and use them productively.",
  "y": "uses"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_8",
  "x": "On the \"add jump\" test set <cite>[15]</cite> , standard seq2seq modeling completely fails to generalize compositionally, reaching an average performance of only 0.03% correct (SD = 0.02). It fails even while achieving near perfect performance on the training set (>99% on average). This replicates the results from <cite>[15]</cite> which trained many seq2seq models, finding the best network performed at only 1.2% accuracy.",
  "y": "similarities"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_9",
  "x": "Results. The results are summarized in Table 2 . On the \"add jump\" test set <cite>[15]</cite> , standard seq2seq modeling completely fails to generalize compositionally, reaching an average performance of only 0.03% correct (SD = 0.02).",
  "y": "background"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_10",
  "x": "The slight decline in performance compared to Experiment 4.3 is not statistically significant with five runs. The standard seq2seq learner takes advantage of the augmented training to generalize better than in standard SCAN training (Experiment 4.3 and <cite>[15]</cite> ), achieving 12.26% accuracy (SD = 8.33) on the test instructions (with >99% accuracy during training). The augmented task provides 23 fully compositional primitives during training, compared to the three in the original task.",
  "y": "differences"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_11",
  "x": "**EXPERIMENT: GENERALIZING TO LONGER INSTRUCTIONS THROUGH META-TRAINING** The final experiment examines whether the meta seq2seq approach can learn to generalize to longer sequences, even when the test sequences are longer than any experienced during meta-training. This experiment uses the SCAN \"length\" split <cite>[15]</cite> .",
  "y": "uses"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_12",
  "x": "After learning how to \"dax,\" people understand how to \"dax twice,\" \"dax slowly,\" or even \"dax like there is no tomorrow.\" These abilities are central to language and thought yet they are conspicuously lacking in modern neural networks<cite> [15,</cite> 3, 20, 22, 2] . In this paper, I introduced a meta sequence-to-sequence (meta seq2seq) approach for learning to generalize compositionally, exploiting the algebraic structure of a domain to help understand novel utterances. In contrast to standard seq2seq, meta seq2seq learners can abstract away the surface patterns and operate closer to rule space.",
  "y": "background"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_13",
  "x": "After learning how to \"dax,\" people understand how to \"dax twice,\" \"dax slowly,\" or even \"dax like there is no tomorrow.\" These abilities are central to language and thought yet they are conspicuously lacking in modern neural networks<cite> [15,</cite> 3, 20, 22, 2] . In this paper, I introduced a meta sequence-to-sequence (meta seq2seq) approach for learning to generalize compositionally, exploiting the algebraic structure of a domain to help understand novel utterances. In contrast to standard seq2seq, meta seq2seq learners can abstract away the surface patterns and operate closer to rule space.",
  "y": "motivation"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_14",
  "x": "After learning how to \"dax,\" people understand how to \"dax twice,\" \"dax slowly,\" or even \"dax like there is no tomorrow.\" These abilities are central to language and thought yet they are conspicuously lacking in modern neural networks<cite> [15,</cite> 3, 20, 22, 2] . In this paper, I introduced a meta sequence-to-sequence (meta seq2seq) approach for learning to generalize compositionally, exploiting the algebraic structure of a domain to help understand novel utterances. In contrast to standard seq2seq, meta seq2seq learners can abstract away the surface patterns and operate closer to rule space.",
  "y": "differences"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_16",
  "x": "Hybrid models could also address the challenge of generalizing to longer output sequences, a problem that continues to vex neural networks<cite> [15,</cite> 3, 28] including meta seq2seq learning. The meta seq2seq approach could be applied to a wide range of tasks including low resource machine translation [12] or to graph traversal problems [11] . For traditional seq2seq tasks like machine translation, standard seq2seq training could be augmented with hybrid training that alternates between standard training and meta-training to encourage compositional generalization.",
  "y": "background"
 },
 {
  "id": "4a28a289ffc730fea4114f6c71bd06_17",
  "x": "In future work, I intend to explore adding more symbolic machinery to the architecture [27] with the goal of handling genuinely new symbols. Hybrid models could also address the challenge of generalizing to longer output sequences, a problem that continues to vex neural networks<cite> [15,</cite> 3, 28] including meta seq2seq learning. The meta seq2seq approach could be applied to a wide range of tasks including low resource machine translation [12] or to graph traversal problems [11] .",
  "y": "future_work"
 },
 {
  "id": "4a7fecf3b80c274739e9c83be9a36b_0",
  "x": "However, these systems heavily depend on many hand-crafted linguistic features. In this paper, we propose a discourse context-aware self-attention neural network model for fine-grained IS classification. On the ISNotes corpus<cite> (Markert et al., 2012)</cite> , our model with the contextually-encoded word representations (BERT) (Devlin et al., 2018) achieves new state-of-the-art performances on fine-grained IS classification, obtaining a 4.1% absolute overall accuracy improvement compared to Hou et al. (2013a) .",
  "y": "uses"
 },
 {
  "id": "4a7fecf3b80c274739e9c83be9a36b_1",
  "x": "Specifically, information status (IS henceforth) reflects the accessibility of a discourse entity based on the evolving discourse context and the speaker's assumption about the hearer's knowledge and beliefs. For instance, according to <cite>Markert et al. (2012)</cite> , old mentions 1 refer to entities that have been referred to previously; mediated men-tions have not been mentioned before but are accessible to the hearer by reference to another old mention or to prior world knowledge; and new mentions refer to entities that are introduced to the discourse for the first time and are not known to the hearer before. In this paper, we follow the IS scheme proposed by <cite>Markert et al. (2012)</cite> and focus on learning finegrained IS on written texts.",
  "y": "background"
 },
 {
  "id": "4a7fecf3b80c274739e9c83be9a36b_2",
  "x": "Specifically, information status (IS henceforth) reflects the accessibility of a discourse entity based on the evolving discourse context and the speaker's assumption about the hearer's knowledge and beliefs. For instance, according to <cite>Markert et al. (2012)</cite> , old mentions 1 refer to entities that have been referred to previously; mediated men-tions have not been mentioned before but are accessible to the hearer by reference to another old mention or to prior world knowledge; and new mentions refer to entities that are introduced to the discourse for the first time and are not known to the hearer before. In this paper, we follow the IS scheme proposed by <cite>Markert et al. (2012)</cite> and focus on learning finegrained IS on written texts.",
  "y": "uses"
 },
 {
  "id": "4a7fecf3b80c274739e9c83be9a36b_3",
  "x": "We find that the sentence containing the target mention as well as the lexical overlap information between the target mention and the preceding mentions are the most important discourse context when assigning IS for a mention. With self-attention, our model can capture important signals within a mention and the interactions between the mention and its context. On the ISNotes corpus<cite> (Markert et al., 2012)</cite> , our model with the contextually-encoded word representations (BERT) (Devlin et al., 2018) achieves new state-of-the-art performances on fine-grained IS classification, obtaining a 4.1% absolute overall accuracy improvement compared to Hou et al. (2013a) .",
  "y": "uses"
 },
 {
  "id": "4a7fecf3b80c274739e9c83be9a36b_5",
  "x": "Bridging resolution (Hou et al., 2014 contains two sub tasks: identifying bridging anaphors<cite> (Markert et al., 2012</cite>; Hou et al., 2013a; Hou, 2016) and finding the correct antecedent among candidates (Hou et al., 2013b; Hou, 2018a,b) . Previous work handle bridging anaphora recognition as part of IS classification problem. <cite>Markert et al. (2012)</cite> et al. (2013a) regarding the overall IS classificiation accuracy but the result on bridging anaphora recognition is much worse than Hou et al. (2013a) .",
  "y": "background"
 },
 {
  "id": "4a7fecf3b80c274739e9c83be9a36b_6",
  "x": "The IS scheme proposed by <cite>Markert et al. (2012)</cite> adopts three major IS categories (old, new and mediated) from Nissim et al. (2004) and distinguishes six subcategories for mediated. Table 1 lists the definitions for these IS categories and summarizes the main affecting factors for each IS class. As described in Section 1, a mention's internal syntactic and semantic properties can signal its IS.",
  "y": "background"
 },
 {
  "id": "4a7fecf3b80c274739e9c83be9a36b_7",
  "x": "We perform experiments on the ISNotes corpus<cite> (Markert et al., 2012)</cite> , which contains 10,980 mentions annotated for information status in 50 news texts. Following Hou et al. (2013a) , all experiments are performed via 10-fold cross-validation on documents. We report overall accuracy as well as precision, recall and F-measure per IS class.",
  "y": "uses"
 },
 {
  "id": "4adcc28c6d1906d74874b8fca371dc_0",
  "x": "An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem (Bernardi et al., 2016; Mao et al., 2015; Vinyals et al., 2015; Fang et al., 2015) . There has also been a great deal of work on sentence-based image search or cross-modal retrieval where the objective is to learn a joint space for images and text (Hodosh et al., 2013; Frome et al., 2013;<cite> Kiros et al., 2015</cite>; Socher et al., 2014; Donahue et al., 2015) . Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets.",
  "y": "background"
 },
 {
  "id": "4adcc28c6d1906d74874b8fca371dc_1",
  "x": "Most work on learning a joint space for images and their descriptions is based on Canonical Correlation Analysis (CCA) or neural variants of CCA over representations of image and its descriptions (Hodosh et al., 2013; Andrew et al., 2013; Yan and Mikolajczyk, 2015; Gong et al., 2014; . Besides CCA, a few others learn a visual-semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function<cite> (Kiros et al., 2015</cite>; Socher et al., 2014; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description Plummer et al., 2015) in a common space. Recently Lin and Parikh (2016) have leveraged visual question answering models to encode images and descriptions into the same space.",
  "y": "background"
 },
 {
  "id": "4adcc28c6d1906d74874b8fca371dc_2",
  "x": "where k stands for each language. This loss function encourages the similarity S(c k , i) between gold-standard description c k and image i to be greater than any other irrelevant description c k by a margin \u03b1. A similar loss function is useful for learning multimodal embeddings in a single language<cite> (Kiros et al., 2015)</cite> .",
  "y": "motivation"
 },
 {
  "id": "4adcc28c6d1906d74874b8fca371dc_3",
  "x": "We used the L2 norm to mitigate over-fitting<cite> (Kiros et al., 2015)</cite> . Specifically, we use Visual Semantic Embeddings (VSE) of<cite> Kiros et al. (2015)</cite> and Order Embeddings (OE) of Vendrov et al. (2016) .",
  "y": "similarities"
 },
 {
  "id": "4adcc28c6d1906d74874b8fca371dc_5",
  "x": "We report results for both English and German descriptions. Note that we have one single model for both languages. In Tables 1 and 2 we present the ranking results of the baseline models of<cite> Kiros et al. (2015)</cite> and Vendrov et al. (2016) and our proposed PIVOT and PARALLEL models.",
  "y": "background"
 },
 {
  "id": "4adcc28c6d1906d74874b8fca371dc_6",
  "x": "In the semantic textual similarity task (STS), we use the textual embeddings from our model to compute the similarity between a pair of sen- (Wieting et al., 2017) \u2212 83.7 84.5 85.0 MLMME (Calixto et al., 2017) VGG19 \u2212 72.7 79.7 VSE<cite> (Kiros et al., 2015)</cite> VGG19 80.6 82.7 89.6 OE (Vendrov et al., 2016) VGG19 82. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014 , STS-2015 (Agirre et al. 2012 , Agirre et al. 2014 , Agirre et al. 2015 .",
  "y": "similarities"
 },
 {
  "id": "4adcc28c6d1906d74874b8fca371dc_7",
  "x": "The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS-2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010) . In Table 4 , we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017) , paraphrastic sentence embeddings (Wieting et al., 2017) , visual semantic embeddings<cite> (Kiros et al., 2015)</cite> , and order embeddings (Vendrov et al., 2016) .",
  "y": "similarities background"
 },
 {
  "id": "4b08797852539f464793dd23cf8999_0",
  "x": "**RELATED WORK** If we consider traditional cluster encoded word representation, e.g., Brown clusters (Brown et al., 1992) , it only uses a small number of bits to track the path on a hierarchical tree of word clusters to represent each word. In fact, word embedding generalized the idea of discrete clustering representation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and generalization ability (Bengio et al., 2003; Mikolov et al., 2013a; <cite>Mikolov et al., 2013b)</cite> .",
  "y": "background motivation"
 },
 {
  "id": "4b08797852539f464793dd23cf8999_1",
  "x": "When the memory for training word embedding is also limited, we need to modify the training algorithms by introducing new data structures to reduce the bits used to encode the values. In practice, we found that in the stochastic gradient descent (SGD) iteration in word2vec algorithms (Mikolov et al., 2013a; <cite>Mikolov et al., 2013b)</cite> , the updating vector's values are often very small numbers (e.g., < 10 \u22125 ). In this case, if we directly apply the rounding method to certain precisions (e.g., 8 bits), the update of word vectors will always be zero. For example, the 8-bit precision is 2 \u22127 = 0.0078, so 10 \u22125 is not significant enough to update the vector with 8-bit values.",
  "y": "background motivation"
 },
 {
  "id": "4b08797852539f464793dd23cf8999_2",
  "x": "In this section, we describe a comprehensive study on tasks that have been used for evaluating word embeddings. We train the word embedding algorithms, word2vec (Mikolov et al., 2013a; <cite>Mikolov et al., 2013b)</cite> , based on the Oct. 2013 Wikipedia dump. 1 We first compare levels of truncation of word2vec embeddings, and then evaluate the stochastic rounding and the auxiliary vectors based methods for training word2vec vectors.",
  "y": "uses"
 },
 {
  "id": "4b08797852539f464793dd23cf8999_3",
  "x": "We ran both CBOW and skipgram with negative sampling (Mikolov et al., 2013a; <cite>Mikolov et al., 2013b)</cite> on the Wikipedia dump data, and set the window size of context to be five. Then we performed value truncation with 4 bits, 6 bits, and 8 bits. The results are shown in Fig. 1 , and the numbers of the averaged results are shown in Table 1 .",
  "y": "uses"
 },
 {
  "id": "4b0aab00a99547791bff0597aabc06_0",
  "x": "Our method leverages <cite>Cotterell et al. (2017)</cite> 's formulation of Mikolov et al. (2013) 's popular skip-gram model as exponential family principal component analysis (EPCA) and tensor factorization. This paper's primary contributions are: (i) enriching learned word embeddings with multiple, automatically obtained frames from large, disparate corpora; and (ii) demonstrating these enriched embeddings better capture SPR-based properties. In so doing, we also generalize <cite>Cotterell et al.</cite>'s method to arbitrary tensor dimensions.",
  "y": "uses"
 },
 {
  "id": "4b0aab00a99547791bff0597aabc06_1",
  "x": "This paper's primary contributions are: (i) enriching learned word embeddings with multiple, automatically obtained frames from large, disparate corpora; and (ii) demonstrating these enriched embeddings better capture SPR-based properties. In so doing, we also generalize <cite>Cotterell et al.</cite>'s method to arbitrary tensor dimensions. This allows us to include an arbitrary amount of semantic information when learning embeddings.",
  "y": "extends"
 },
 {
  "id": "4b0aab00a99547791bff0597aabc06_2",
  "x": "<cite>Cotterell et al. (2017)</cite> showed that SG is a form of exponential family PCA that factorizes the matrix of word/context cooccurrence counts (rather than shifted positive PMI values). With this interpretation, <cite>they</cite> generalize SG from matrix to tensor factorization, and provide a theoretical basis for modeling higher-order SG (or additional context, such as morphological features of words) within a word embeddings framework. Specifically, <cite>Cotterell et al.</cite> recast higher-order SG as maximizing the log-likelihood",
  "y": "background"
 },
 {
  "id": "4b0aab00a99547791bff0597aabc06_3",
  "x": "We also follow <cite>Cotterell et al. (2017)</cite> and augment the above with the signed number of tokens separating w i and w j , e.g., recording that w i appeared two to the left of w j ; these counts form a 3-tensor. To turn semantic parses into tensor counts, we first identify relevant information from the parses. We consider all parses that are triggered by the target word w j (seen \u2265 T times) and that have at least one role filled by some word in the sentence.",
  "y": "uses"
 },
 {
  "id": "4b0aab00a99547791bff0597aabc06_6",
  "x": "The 0 line represents a plain word2vec baseline and the dashed line represents the 3-tensor baseline of <cite>Cotterell et al. (2017)</cite> . Both of <cite>these baselines</cite> are windowed: <cite>they</cite> are restricted to a local context and cannot take advantage of frames or any lexical signal that can be derived from frames. Overall, we notice that we obtain large improvements from models trained on lexical signals that have been derived from frame output (sep and none), even if the model itself does not incorporate any frame labels.",
  "y": "uses"
 },
 {
  "id": "4b0aab00a99547791bff0597aabc06_7",
  "x": "The embeddings that predict the role filling lexical items (the green triangles) correlate higher with SPR oracles than the embeddings that predict PropBank frames and roles (red circles). Examining Fig. 2a , we see that both model types outperform both the word2vec and <cite>Cotterell et al. (2017)</cite> baselines in nearly all model configurations and ablations. We see the highest improvement when predicting role fillers given the frame trigger and the number of tokens separating the two (the green triangles in the sep rows).",
  "y": "differences"
 },
 {
  "id": "4b0aab00a99547791bff0597aabc06_8",
  "x": "**RELATED WORK** The recent popularity of word embeddings have inspired others to consider leveraging linguistic annotations and resources to learn embeddings. Both <cite>Cotterell et al. (2017)</cite> and Levy and Goldberg (2014a) incorporate additional syntactic and morphological information in their word embeddings.",
  "y": "background"
 },
 {
  "id": "4b0aab00a99547791bff0597aabc06_9",
  "x": "**RELATED WORK** The recent popularity of word embeddings have inspired others to consider leveraging linguistic annotations and resources to learn embeddings. Both <cite>Cotterell et al. (2017)</cite> and Levy and Goldberg (2014a) incorporate additional syntactic and morphological information in their word embeddings.",
  "y": "motivation"
 },
 {
  "id": "4b0aab00a99547791bff0597aabc06_10",
  "x": "We also presented a QVEC evaluation for semantic proto-roles. As demonstrated by our experiments, our extension of <cite>Cotterell et al. (2017)</cite> 's tensor factorization enriches word embeddings by including syntacticsemantic information not often captured, resulting in consistently higher SPR-based correlations. The implementation is available at <cite>https: //github.com/fmof/tensor-factorization</cite>.",
  "y": "extends"
 },
 {
  "id": "4b17b24ec0203263e581cbeeaa9fc7_0",
  "x": "There has been considerable recent interest in the use of statistical methods for grouping words in large on-line corpora into categories which capture some of our intuitions about the reference of the words we use and the relationships between them (e.g.<cite> Brown et al., 1992</cite>; Schiitze, 1993) . Although they have received most attention from within computational linguistics, such approaches are also of interest from the point of view of psychology. The huge task of developing concepts of word meanings is one that human beings readily achieve; we are all generally aware of the similarities and differences between the meanings of words, despite the fact that in many cases these meanings are not amenable to rigourous definition.",
  "y": "background"
 },
 {
  "id": "4b17b24ec0203263e581cbeeaa9fc7_1",
  "x": "**APPROACHES TO SEMANTIC CLUSTERING** A number of analyses were carried out on text corpora to examine the sorts of semantic groupings that can be achieved using simple statistical methods. Using an approach similar to that of <cite>Brown et al. (1992)</cite> , each 'target word '1 wi in the corpus was represented as a vector in which each component j is the probability that any one word position in a 'context window' will be occupied by a 'context word' wj, given that the window is centred on word wi.",
  "y": "similarities"
 },
 {
  "id": "4b17b24ec0203263e581cbeeaa9fc7_2",
  "x": "The members of some of these groups were selected following inspection of the relevant dendrograms and are listed in (1992), the distance metric used was the Spearman Rank Correlation coefficient. The approach described here differs from that of <cite>Brown et al. (1992)</cite> in that context words both preceding and following the target word are considered (although information about the ordering of the context was not used), and in that Euclidean distance, rather than average mutual information, is used for clustering. Each of the methods described here represents each target word in the same manner, regardless of the syntactic or semantic designation which might conventionally be assigned to it.",
  "y": "uses"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_0",
  "x": "At heart, these systems use phrase tables and thus rely primarily on small contexts during the translation process. Lately, there have been a number of attempts to develop a purely neural machine translation system (NMT) [12, 5, 2,<cite> 22]</cite> . NTM systems should eventually outperform the best standard systems because neural networks scale well with larger models and generalize to word sequences that do not appear in the training set.",
  "y": "background"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_1",
  "x": "In addition, NMT systems are easy to train with backpropagation and their decoder is easy to implement, unlike the highly intricate decoders used by the phrase-based systems [13] . NMT systems also use minimal domain knowledge, which makes them applicable to any other problem that can be formulated as mapping a sequence to another sequence<cite> [22]</cite> . A major limitation of existing NMTs is their use of a fixed modest-sized vocabulary.",
  "y": "background"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_2",
  "x": "Empirically, both Sutskever et al.<cite> [22]</cite> and Bahdanau et al. [2] have observed that sentences with many rare words tend to be translated much more poorly than sentences containing mainly frequent words. Standard phrase-based systems, on the other hand, suffer less from the rare word problem because they can afford a much larger vocabulary, and because of their use of explicit alignments and phrase counts allows them to memorize the translations of even extremely rare words. Motivated by the strengths of the standard phrase-based system, we propose and implement a simple approach to address the rare word problem of NMTs.",
  "y": "background motivation"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_3",
  "x": "There are many ways to parameterize these conditional distributions. For example, Kalchbrenner et al. [12] used a combination of a convolutional neural network and a recurrent neural network, Sutskever et al.<cite> [22]</cite> used a large and deep Long Short-Term Memory (LSTM) model, Cho et al. [5] used an architecture similar to the LSTM, and Bahdanau et al. [2] used a more elaborate neural network architecture that uses an attentional mechanism over the input sequence, similarly to Graves [9] and Graves et al. [10] . In this work, we use the exact model of Sutskever et al.<cite> [22]</cite> , which has a large deep LSTM to encodue the input sequence and a separate deep LSTM to produce a translation from the input sequence.",
  "y": "background"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_4",
  "x": "There are many ways to parameterize these conditional distributions. For example, Kalchbrenner et al. [12] used a combination of a convolutional neural network and a recurrent neural network, Sutskever et al.<cite> [22]</cite> used a large and deep Long Short-Term Memory (LSTM) model, Cho et al. [5] used an architecture similar to the LSTM, and Bahdanau et al. [2] used a more elaborate neural network architecture that uses an attentional mechanism over the input sequence, similarly to Graves [9] and Graves et al. [10] . In this work, we use the exact model of Sutskever et al.<cite> [22]</cite> , which has a large deep LSTM to encodue the input sequence and a separate deep LSTM to produce a translation from the input sequence.",
  "y": "uses"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_5",
  "x": "We present three annotation strategies that can easily be applied to any NMT system. We treat the NMT system [12, <cite>22,</cite> 5] as a black box and train it on a dataset annotated with alignment information specified by one of the models below. Such alignment data can be obtained from a parallel corpus using an unsupervised aligner.",
  "y": "uses"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_6",
  "x": "**TRAINING DATA** To be comparable with the results reported by previous work on neural machine translation systems<cite> [22,</cite> 5, 2] , we train our models on the same training data of 12M parallel sentences (348M French and 304M English words). The 12M subset was selected from the full WMT'14 parallel corpora using the method proposed in [1] .",
  "y": "uses"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_7",
  "x": "2 Due to the computationally intensive nature of the naive softmax in the target language, we limit the French vocabulary to the 40K most frequent French words (note that<cite> [22]</cite> used a vocabulary of 80k French words). On the source side, however, we can afford a much larger vocabulary, so we use the 200K most frequent English words. The model treats all other words as unknowns.",
  "y": "differences"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_8",
  "x": "---------------------------------- **TRAINING DETAILS** Our training procedure and hyperparameter choices are similar to those used by Sutskever et al.<cite> [22]</cite> .",
  "y": "similarities"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_9",
  "x": "Our training procedure and hyperparameter choices are similar to those used by Sutskever et al.<cite> [22]</cite> . In more details, we train multi-layer deep LSTMs, each of which has 1000 cells, with 1000 dimensional embeddings. Like Sutskever et al.<cite> [22]</cite> , we reverse the words in the source sentences which has been shown to improve LSTM memory utilization and results in better translations of long sentences.",
  "y": "uses"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_10",
  "x": "Like Sutskever et al.<cite> [22]</cite> , we reverse the words in the source sentences which has been shown to improve LSTM memory utilization and results in better translations of long sentences. Our hyperparameters can be summarized as follows: (a) the parameters are initialized uniformly in [-0.08, 0.08], (b) SGD has a fixed learning rate of 0.7, (c) we train for 8 epochs (after 5 epochs, we begin to halve the learning rate every 0.5 epoch), (d) the size of the mini-batch is 128, and (e) we rescale the normalized gradient to ensure that its norm does not exceed 5 [18] . We also follow the GPU parallelization scheme proposed in<cite> [22]</cite> , allowing us to reach a training speed of 9.0K words per second ([22] achieved 6.3K words per second with a larger vocabulary of 80K; our target vocabulary has 40K words).",
  "y": "uses"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_11",
  "x": "This numerical score is based on detokenized translations. However, all other systems that we compared against have been evaluated on the tokenized translations using the multi-bleu.pl script, which is consistent with previous work [5, 2, 19,<cite> 22]</cite> . Thus, to make it possible to compare our system against the system of Durrani et al. [7] , we evaluated its tokenized predictions (which can be downloaded from statmt.org [7] ) on the test set (newstest2014) and arrived at the BLEU score of 37.0 points<cite> [22]</cite> .",
  "y": "similarities"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_12",
  "x": "Thus, to make it possible to compare our system against the system of Durrani et al. [7] , we evaluated its tokenized predictions (which can be downloaded from statmt.org [7] ) on the test set (newstest2014) and arrived at the BLEU score of 37.0 points<cite> [22]</cite> . ---------------------------------- **MAIN RESULTS**",
  "y": "similarities"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_13",
  "x": "For the nonensemble models, we report the performance of models with 4 and 6 layers (see Section 5.2 for more analysis). For the ensemble setting, we use a combination of 5 depth-4 models and 3 depth-6 models. Our best result (36.9 BLEU) outperforms all other NMT systems by a large margin, and in particular, it outperforms the current best NMT system<cite> [22]</cite> by 2.1 BLEU points (we even outperform Sutskever et al.<cite> [22]</cite> when they rerank the n-best list of a phrase-based baseline<cite> [22]</cite> ).",
  "y": "differences"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_14",
  "x": "**RARE WORD ANALYSIS** To analyze the effect of rare words on translation quality, we follow Sutskever et al.<cite> [22]</cite> and sort the sentences in newstest2014 by the average frequency rank of their words. We split the test sentences into groups where the sentences within each group have a comparable number of rare words and evaluate each group independently.",
  "y": "uses"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_15",
  "x": "We evaluate our systems before and after translating the OOV words and compare with the standard MT systems -we use the state-of-the-art (SOTA) system from WMT'14 [7] , and neural MT systems -we use the ensemble system described in<cite> [22]</cite> (See Section 4). Rare word translation is challenging for neural machine translation systems as shown in Figure 5 . The translation quality of our model before applying the unknown word translations is shown by the green star line, and the current best NMT system<cite> [22]</cite> is the purple diamond line.",
  "y": "uses"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_16",
  "x": "We evaluate our systems before and after translating the OOV words and compare with the standard MT systems -we use the state-of-the-art (SOTA) system from WMT'14 [7] , and neural MT systems -we use the ensemble system described in<cite> [22]</cite> (See Section 4). Rare word translation is challenging for neural machine translation systems as shown in Figure 5 . The translation quality of our model before applying the unknown word translations is shown by the green star line, and the current best NMT system<cite> [22]</cite> is the purple diamond line.",
  "y": "uses"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_17",
  "x": "While<cite> [22]</cite> produces excellent translations of sentences with frequent words (the left part of the graph), they are worse than SOTA system (red triangle line) on sentences with many rare words (the right side of the graph). When applying our unknown word translation technique (blue square line), we significantly improve the translation quality of our NMT: in for the last group of 500 sentences which have the greatest proportion of OOV words in the test set, we increase the BLEU score of our system by 6.5 BLEU points. Overall, our rare word translation model interpolates between the SOTA system and the system of Sutskever et al.<cite> [22]</cite> , which allows us to outperform SOTA on sentences that consist predominantly of frequent words and approach its performance on sentences with many OOV words.",
  "y": "differences"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_18",
  "x": "Overall, our rare word translation model interpolates between the SOTA system and the system of Sutskever et al.<cite> [22]</cite> , which allows us to outperform SOTA on sentences that consist predominantly of frequent words and approach its performance on sentences with many OOV words. On the x-axis, we order newstest2014 sentences by their average frequency rank and divide the sentences into groups, where each group consists of sentences with a comparable prevalence of rare words. We compute the BLEU score of each group independently.",
  "y": "uses"
 },
 {
  "id": "4b34c36a93a049b8fd637fee768438_19",
  "x": "We have shown that a simple alignment-based technique can mitigate and even overcome one of the main weaknesses of current NMT systems, which is their inability to translate words that are not in their vocabulary. A key advantage of our technique is the fact that it is applicable to any NMT system and not only to the deep LSTM model of Sutskever et al.<cite> [22]</cite> . A technique like ours is likely necessary if an NMT system is to achieve state-of-the-art performance on machine translation.",
  "y": "differences"
 },
 {
  "id": "4bb290aba7ee7843280a8b0e88e5a0_1",
  "x": "We find that these two systems are on par with each other, with no significant differences in terms of accuracy between them. A major distinction between the work of Berant et al. (2013) and<cite> Yao and Van Durme (2014)</cite> is the ability of the former to represent, and compose, aggregation operators (such as argmax, or count), as well as integrate disparate pieces of information. This representational capability was important in previous, closed-domain tasks such as GeoQuery.",
  "y": "background"
 },
 {
  "id": "4bb290aba7ee7843280a8b0e88e5a0_2",
  "x": "**IE AND SP SYSTEMS** jacana-freebase 2 <cite>(Yao and Van Durme, 2014)</cite> treats QA from a KB as a binary classification problem. Freebase is a gigantic graph with millions of nodes (topics) and billions of edges (relations).",
  "y": "background"
 },
 {
  "id": "4bb290aba7ee7843280a8b0e88e5a0_3",
  "x": "Both Berant et al. (2013) and<cite> Yao and Van Durme (2014)</cite> tested their systems on the WEBQUESTIONS dataset, which contains 3778 training questions and 2032 test questions collected from the Google Suggest API. Each question came with a standard answer from Freebase annotated by Amazon Mechanical Turk. Berant et al. (2013) reported a score of 31.4% in terms of accuracy (with partial credit if inexact match) on the test set and later in Berant and Liang (2014) revised it to 35.7%.",
  "y": "background"
 },
 {
  "id": "4be5a47b5fd900c3578330b352b24c_0",
  "x": "In this paper we develop a sentiment detection algorithm for social media that classifies the polarity of sentence phrases as positive, negative, or neutral and test its performance in Twitter through the participation in the expression level task (subtask A) of the SemEval-2013 Task 2: Sentiment Analysis in Twitter (Wilson et al., 2013) which the authors helped organize. To do so, we build on previous work on sentiment detection algorithms for the more formal news genre, notably the work of<cite> Agarwal et al (2009)</cite> , but adapt it for the language of social media, in particular Twitter. We show that exploiting lexical-stylistic features and dictionaries geared toward social media are useful in detecting sentiment.",
  "y": "extends"
 },
 {
  "id": "4be5a47b5fd900c3578330b352b24c_1",
  "x": "To do so, we build on previous work on sentiment detection algorithms for the more formal news genre, notably the work of<cite> Agarwal et al (2009)</cite> , but adapt it for the language of social media, in particular Twitter. We show that exploiting lexical-stylistic features and dictionaries geared toward social media are useful in detecting sentiment. In this rest of this paper, we discuss related work, including the state of the art sentiment system<cite> (Agarwal et al., 2009)</cite> our method is based on, the lexicons we used, our method, and experiments and results.",
  "y": "extends"
 },
 {
  "id": "4be5a47b5fd900c3578330b352b24c_2",
  "x": "**LEXICONS** Several lexicons are used in our system. We use the DAL and expand it with WordNet, as it was used in the original work<cite> (Agarwal et al., 2009)</cite> , and expand it further to use Wiktionary and an emoticon lexicon.",
  "y": "similarities extends uses"
 },
 {
  "id": "4be5a47b5fd900c3578330b352b24c_3",
  "x": "It therefore covers a broad set of words. Each word is given three scores (pleasantness -also called evaluation (ee), activeness (aa), and imagery (ii)) on a scale of 1 (low) to 3 (high). We compute the polarity of a chunk in the same manner as the original work<cite> (Agarwal et al., 2009)</cite> , using the sum of the AE Space Score's (| \u221a ee 2 + aa 2 |) of each word within the chunk.",
  "y": "similarities uses"
 },
 {
  "id": "4be5a47b5fd900c3578330b352b24c_4",
  "x": "We experimented with different amounts of n-grams and found that more than 500 n-grams reduced performance. The DAL and other dictionaries are used along with a negation state machine<cite> (Agarwal et al., 2009)</cite> to determine the polarity for each word in the sentence. We include all the features described in the original system<cite> (Agarwal et al., 2009</cite> ).",
  "y": "uses"
 },
 {
  "id": "4be5a47b5fd900c3578330b352b24c_5",
  "x": "We include all the features described in the original system<cite> (Agarwal et al., 2009</cite> ). ---------------------------------- **LEXICAL-STYLISTIC FEATURES**",
  "y": "similarities uses"
 },
 {
  "id": "4be5a47b5fd900c3578330b352b24c_6",
  "x": "We include POS tags and the top 500 n-gram features<cite> (Agarwal et al., 2009)</cite> . We experimented with different amounts of n-grams and found that more than 500 n-grams reduced performance. The DAL and other dictionaries are used along with a negation state machine<cite> (Agarwal et al., 2009)</cite> to determine the polarity for each word in the sentence.",
  "y": "uses"
 },
 {
  "id": "4c615cb5a496c1e225a8457a4f55d4_0",
  "x": "For automatic evaluation of a simplification output, it is common practice to use machine translation (MT) metrics (e.g. BLEU (Papineni et al., 2002) ), simplicity metrics (e.g. SARI <cite>(Xu et al., 2016)</cite> ), and readability metrics (e.g. FKGL (Kincaid et al., 1975) ). Most of these metrics are available in individual code repositories, with particular software requirements that sometimes differ even in programming language (e.g. corpus-level SARI is implemented in Java, whilst sentence-level SARI is available in both Java and Python). Other metrics (e.g. SAMSA (Sulem et al., 2018b) ) suffer from insufficient documentation or require executing multiple scripts with hard-coded paths, which prevents researchers from using them.",
  "y": "background"
 },
 {
  "id": "4c615cb5a496c1e225a8457a4f55d4_1",
  "x": "BLEU is a precision-oriented metric that relies on the proportion of n-gram matches between a system's output and reference(s). Previous work <cite>(Xu et al., 2016)</cite> has shown that BLEU correlates fairly well with human judgements of grammaticality and meaning preservation. EASSE uses SACREBLEU (Post, 2018) 1 to calculate BLEU.",
  "y": "background"
 },
 {
  "id": "4c615cb5a496c1e225a8457a4f55d4_2",
  "x": "In this version, for each operation (ope \u2208 {add, del, keep}) and n-gram order, precision p ope (n), recall r ope (n) and F1 f ope (n) scores are calculated. These are then averaged over the n-gram order to get the overall operation F1 score Although<cite> Xu et al. (2016)</cite> indicate that only precision should be considered for the deletion operation, we follow the Java implementation that uses F1 score for all operations in corpus-level SARI.",
  "y": "differences"
 },
 {
  "id": "4c615cb5a496c1e225a8457a4f55d4_3",
  "x": "EASSE provides access to three publicly available datasets for automatic SS evaluation (Table 1): PWKP (Zhu et al., 2010) , TurkCorpus <cite>(Xu et al., 2016)</cite> , and HSplit (Sulem et al., 2018a) . All of them consist of the data from the original datasets, which are sentences extracted from English Wikipedia (EW) articles. It is important to highlight that EASSE can also evaluate system's outputs in other datasets provided by the user.",
  "y": "background"
 },
 {
  "id": "4c615cb5a496c1e225a8457a4f55d4_4",
  "x": "The latter correspond to instances of sentence splitting. Since this dataset has only one reference for each original sentence, 5 https://github.com/facebookresearch/ text-simplification-evaluation 6 The lexical complexity score of a simplified sentence is computed by taking the log-ranks of each word in the frequency it is not ideal for calculating automatic metrics that rely on multiple references, such as SARI. TurkCorpus<cite> Xu et al. (2016)</cite> asked crowdworkers to simplify 2,359 original sentences extracted from PWKP to collect eight simplification references for each one.",
  "y": "background"
 },
 {
  "id": "4c615cb5a496c1e225a8457a4f55d4_5",
  "x": "EASSE provides access to various SS system outputs that follow different approaches for the task. For instance, we included those that rely on phrase-based statistical MT, either by itself (e.g. PBSMT-R (Wubben et al., 2012) ), or coupled with semantic analysis, (e.g. Hybrid (Narayan and Gardent, 2014) ). We also included SBSMT-SARI <cite>(Xu et al., 2016)</cite> , which relies on syntaxbased statistical MT; DRESS-LS (Zhang and Lapata, 2017 ), a neural model using the standard encoder-decoder architecture with attention combined with reinforcement learning; and DMASS-DCSS (Zhao et al., 2018) , the current state-of-theart in the TurkCorpus, which is based on the Transformer architecture (Vaswani et al., 2017) .",
  "y": "uses"
 },
 {
  "id": "4c615cb5a496c1e225a8457a4f55d4_6",
  "x": "The origin of the TurkCorpus set itself could explain some of these observations. According to<cite> Xu et al. (2016)</cite> , the annotators in TurkCorpus were instructed to mainly produce paraphrases, i.e. mostly replacements with virtually no deletions. As such, copying words is also a significant transformation, so systems that are good at performing it better mimic the characteristics of the human simplifications in this dataset.",
  "y": "background"
 },
 {
  "id": "4c71ac789203d50e3e428458c2f88c_0",
  "x": "To demonstrate the efficacy of our approach, we transform the publicly available WEATHERGOV dataset (Liang et al., 2009) into fixed-schema tables, which is then used for our experiments. Our proposed mixed hierarchi- Table Summarization with fixed schema tables as input cal attention model provides an improvement of around 18 BLEU (around 30%) over the current state-of-the-art result by<cite> Mei et al. (2016)</cite> . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "4c71ac789203d50e3e428458c2f88c_1",
  "x": "Dynamic Record attention for Decoder: Our decoder is a GRU based decoder with dynamic attention mechanism similar to <cite>(Mei et al., 2016)</cite> with modifications to modulate attention weights at each time step using static record attentions. At each time step t attention weights are calculated using 8, 9, 10, where \u03b3 r t is the aggregated attention weight of record r at time step t. We use the soft attention over input encoder sequences c r to calculate the weighted average, which is passed to the GRU. GRU hidden state s t is used to calculate output probabilities p t by using a softmax as described by equation 11, 12, 13, which is then used to get output word y t .",
  "y": "extends"
 },
 {
  "id": "4c71ac789203d50e3e428458c2f88c_2",
  "x": "Dataset and methodology: To evaluate our model we have used WEATHERGOV dataset (Liang et al., 2009) which is the standard benchmark dataset to evaluate tabular data summarization techniques. We compared the performance of our model against the state-of-the-art work of MBW <cite>(Mei et al., 2016)</cite> , as well as two other baseline models KL (Konstas and Lapata, 2013) and ALK (Angeli et al., 2010 '00010000000000' and '00001000000000' respectively. Similar works for directions, for example 'NW', 'NNE' and 'NE' are encoded as '00000100000000', '00000011000000' and '00000001000000' resp.",
  "y": "uses"
 },
 {
  "id": "4c71ac789203d50e3e428458c2f88c_3",
  "x": "Evaluation metrics: To evaluate our models we employed BLEU and Rouge-L scores. In addition to the standard BLEU (sBleu) (Papineni et al., 2002) , a customized BLEU (cBleu) <cite>(Mei et al., 2016)</cite> has also been reported. cBleu does not penalize numbers which differ by at most five; hence 20 and 18 will be considered same.",
  "y": "uses"
 },
 {
  "id": "4c8e83eb213879e68285e9cd09be47_0",
  "x": "We present results on five languages, for both seen and unseen words and for various amounts of training data. The soft attention model performs surprisingly poorly on seen words, so that its overall performance is worse than the na\u00efve baseline and several earlier models<cite> (Pettersson et al., 2014)</cite> . However, on unseen words (which we argue are what matters), both neural models do well.",
  "y": "background motivation"
 },
 {
  "id": "4c8e83eb213879e68285e9cd09be47_1",
  "x": "Over the years, researchers have proposed normalization methods based on rules and/or edit distances (Baron and Rayson, 2008; Bollmann, 2012; Hauser and Schulz, 2007; Bollmann et al., 2011; Pettersson et al., 2013a; Mitankin et al., 2014;<cite> Pettersson et al., 2014)</cite> , statistical machine translation (Pettersson et al., 2013b; Scherrer and Erjavec, 2013) , and most recently neural network models (Bollmann and S\u00f8gaard, 2016; Bollmann et al., 2017; Korchagina, 2017) . However, most of these systems have been developed and tested on a single language (or even a single corpus), and many have not been compared to the na\u00efve but strong baseline that only changes words seen in the training data, normalizing each to its most frequent modern form observed during training. 1 These issues make it hard to tell which methods generalize across languages and corpora, and how they compare to each other.",
  "y": "background motivation"
 },
 {
  "id": "4c8e83eb213879e68285e9cd09be47_2",
  "x": "Since historical spelling normalization is typically a low-resource task, systems should also ideally be tested with varying amounts of training data to assess how much annotation might be required for a new corpus <cite>(Pettersson et al., 2014</cite>; Bollmann and S\u00f8gaard, 2016; Korchagina, 2017) . Finally, since these systems may be deployed on corpora other than those they were trained on, and used as preprocessing for other tasks, we advocate reporting performance on a downstream task and/or different corpus. To our knowledge the only previous supervised learning system to do so is Pettersson et al. (2013b) .",
  "y": "background"
 },
 {
  "id": "4c8e83eb213879e68285e9cd09be47_4",
  "x": "---------------------------------- **EXPERIMENTS** We use the same datasets as<cite> Pettersson et al. (2014)</cite> , with data from five languages over a range of historical periods.",
  "y": "uses"
 },
 {
  "id": "4c8e83eb213879e68285e9cd09be47_5",
  "x": "**EXPERIMENTS** We use the same datasets as<cite> Pettersson et al. (2014)</cite> , with data from five languages over a range of historical periods. 6 We use the same train/dev/test splits as Pettersson; dataset statistics are shown in Table  1 . Because we do no hyperparameter tuning, we do not use the development sets, and all results are reported on the test sets.",
  "y": "extends"
 },
 {
  "id": "4d25b647a261a415d769532386265a_0",
  "x": "Recently, transformer networks have been shown to perform well for neural machine translation <cite>[11]</cite> and many other NLP tasks [12] . A Transformer layer distinguishes itself from a regular recurrent network by entirely relying on a key-value \"self\"-attention mechanism for learning relationships between distant concepts, rather than relying on recurrent connections and memory cells to preserve information, as in LSTMs, that can fade over time steps. Transformer layers can be seen as bagof-concept layers because they don't preserve location information in the weighted sum self-attention operation.",
  "y": "background"
 },
 {
  "id": "4d25b647a261a415d769532386265a_1",
  "x": "Transformer layers can be seen as bagof-concept layers because they don't preserve location information in the weighted sum self-attention operation. To model word order, sinusoidal positional embeddings are used <cite>[11]</cite> . There has been recent research interest in using transformer networks for end-to-end ASR both with CTC loss [13] and in an encoder-decoder framework [14, 15] with modest performance compared to baseline systems.",
  "y": "uses"
 },
 {
  "id": "4d25b647a261a415d769532386265a_2",
  "x": "Transformer layers <cite>[11]</cite> have the ability to learn long range relationships for many sequential classification tasks [12] . Multi- The dot product between keys and queries is scaled by the inverse square root of the key dimension. This self-attention operation is done h times in parallel, for the case of h attention heads, with different projection matrices from dinput to d k , d k , and dv. The final output is a concatenation of h vectors each with dimension dv which is in turn linearly projected to the desired output dimension of the self-attention layer. On top of the self-attention component, transformer layers have multiple operations applied on each time step; dropout, residual connection, layer norm, two fully connected layers with a ReLU layer in between, another residual and Layer norm operations.",
  "y": "background"
 },
 {
  "id": "4d25b647a261a415d769532386265a_3",
  "x": "The final output is a concatenation of h vectors each with dimension dv which is in turn linearly projected to the desired output dimension of the self-attention layer. On top of the self-attention component, transformer layers have multiple operations applied on each time step; dropout, residual connection, layer norm, two fully connected layers with a ReLU layer in between, another residual and Layer norm operations. Figure(1) -left show the details of one transformer layer as proposed by <cite>[11]</cite> .",
  "y": "uses background"
 },
 {
  "id": "4d25b647a261a415d769532386265a_4",
  "x": "In table (1) we show the WER of the proposed transformer encoder-decoder model with convolutional context using the canonical configuration in the first row. Replacing the 1-D convolutional context in the decoder with sinusoidal positional embedding, as proposed in the baseline machine translation transformers <cite>[11]</cite> and adopted in [13, 15] , shows inferior WER performance. By combining sinusoidal and convolutional position embedding (rows 1+2), we don't observe any gains. This supports our intuition that the relative convolutional positional information provides sufficient signal for the transformer layers to recreate more global word order.",
  "y": "uses background"
 },
 {
  "id": "4dbf6e2bfa30e96816b7f6d9c6e069_0",
  "x": "B: Well twenty five Euro, I mean that's um that's about like eighteen pounds or something. D: This is this gonna to be like the premium product kinda thing or B: So I don't know how how good a remote control that would get you. Um. Abstractive summary: The project manager talked about the project finances and selling prices. To aggregate the information from multiple utterances, we adapt an existing <cite>integer linear programming (ILP)</cite> based fusion technique <cite>[1]</cite> .",
  "y": "uses"
 },
 {
  "id": "4dbf6e2bfa30e96816b7f6d9c6e069_1",
  "x": "The fusion technique is based on the idea of merging dependency parse trees of the utterances. The trees are merged on the common nodes that are represented by the word and parts-ofspeech (POS) combination. Each edge of the merged structure is represented as a variable in the <cite>ILP</cite> objective function, and the solution will decide whether the edge has to be preserved or discarded.",
  "y": "uses"
 },
 {
  "id": "4dbf6e2bfa30e96816b7f6d9c6e069_2",
  "x": "We modify the technique by introducing an anaphora resolution step and also an ambiguity resolver that takes the context of words into account. Further, to solve the <cite>ILP</cite>, we introduce several constraints, such as desired length of the output, etc. To the best of our knowledge, our work is the first to address the problems of readability, grammaticality and content selection jointly for meeting summary generation without employing a templatebased approach.",
  "y": "motivation"
 },
 {
  "id": "4dbf6e2bfa30e96816b7f6d9c6e069_4",
  "x": "All the sentences generated from each meeting transcript are concatenated to produce the final abstractive summary. We need to maximize the information content of the generated sentence, keeping it grammatical. We model the problem as an <cite>integer linear programming (ILP)</cite> formulation, similar to the dependency graph fusion as proposed by <cite>Fillipova and Strube</cite> <cite>[1]</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "4dbf6e2bfa30e96816b7f6d9c6e069_5",
  "x": "In order to solve the above <cite>ILP</cite> problem, we impose a number of constraints. Some of the constraints have been directly adapted from the original <cite>ILP</cite> formulation <cite>[1]</cite> . For example, we use the same constraints for restricting one incoming edge per node, as well as we impose the connectivity constraint to ensure a connected graph structure.",
  "y": "uses"
 },
 {
  "id": "4dbf6e2bfa30e96816b7f6d9c6e069_6",
  "x": "In order to solve the above <cite>ILP</cite> problem, we impose a number of constraints. Some of the constraints have been directly adapted from the original <cite>ILP</cite> formulation <cite>[1]</cite> . For example, we use the same constraints for restricting one incoming edge per node, as well as we impose the connectivity constraint to ensure a connected graph structure.",
  "y": "extends"
 },
 {
  "id": "4dbf6e2bfa30e96816b7f6d9c6e069_7",
  "x": "In order to take this fact into account, we introduce the term px N , where N and px denote the total number of extracted utterances in a segment and the position of the utterance (the edge x belongs to) in the set of N utterances, respectively. In order to solve the above <cite>ILP</cite> problem, we impose a number of constraints. Some of the constraints have been directly adapted from the original <cite>ILP</cite> formulation <cite>[1]</cite> .",
  "y": "extends"
 },
 {
  "id": "4e7ee576b07a8a21a42472bf921291_0",
  "x": "However, the non-convexity and stochastic training of neural IR models raises questions about their consistency compared to heuristic and learning-to-rank models that use discrete representations and simpler methods of combining evidence. Consistent behavior under slightly different conditions is essential to reproducible research and deployment in industry. This paper studies the stability of K-NRM, a recent state-of-the-art neural ranking model<cite> [10]</cite> .",
  "y": "background"
 },
 {
  "id": "4e7ee576b07a8a21a42472bf921291_1",
  "x": "Request permissions from permissions@acm.org. SIGIR '18, July [8] [9]<cite> [10]</cite> [11] [12] 2018 word embeddings tailored for search tasks and kernels that group matches into bins of different quality. Its parameter space is large, the solution space is non-convex, and training is stochastic.",
  "y": "background"
 },
 {
  "id": "4e7ee576b07a8a21a42472bf921291_2",
  "x": "Recent neural IR methods can be categorized as representationbased and interaction-based [2] . Representation-based models use distributed representations of the query and document that are matched in the representation space [4, 8] . Interaction-based models use local interactions between the query and document words and neural networks that learn matching patterns [2, <cite>10]</cite> .",
  "y": "background"
 },
 {
  "id": "4e7ee576b07a8a21a42472bf921291_3",
  "x": "K-NRM<cite> [10]</cite> is an interaction-based model that uses kernel pooling to summarize word-word interactions. It builds a word-word similarity matrix from word embeddings, and uses kernel pooling to 'count' the soft matches at multiple similarity levels using Gaussian kernels. A linear learning-to-rank layer combines the kernel features.",
  "y": "background"
 },
 {
  "id": "4e7ee576b07a8a21a42472bf921291_4",
  "x": "**EXPERIMENTAL SETUP** Our experiments followed the original K-NRM work<cite> [10]</cite> and used its open-source implementation 1 . We used the same click log data from Sogou.com, a Chinese web search engine.",
  "y": "similarities uses"
 },
 {
  "id": "4e7ee576b07a8a21a42472bf921291_5",
  "x": "Documents were represented by titles. Xiong, et al.<cite> [10]</cite> built the vocabulary from queries and titles, but we built it from the queries, titles and URLs for better term coverage. Training Labels: The relevance labels for training were generated by the DCTR [1] click model from user clicks in the training sessions.",
  "y": "extends differences"
 },
 {
  "id": "4e7ee576b07a8a21a42472bf921291_6",
  "x": "Training Labels: The relevance labels for training were generated by the DCTR [1] click model from user clicks in the training sessions. DCTR uses the clickthrough rate for each query-document pair as the relevance score. Testing Labels: Following Xiong et al.<cite> [10]</cite> , three testing conditions were used.",
  "y": "similarities uses"
 },
 {
  "id": "4e7ee576b07a8a21a42472bf921291_7",
  "x": "Model Configuration: We adopted the same default hyperparameter configuration and 11 Gaussian kernels as in prior work<cite> [10]</cite> . The first kernel had \u00b5 = 1, \u03c3 = 10 \u22123 to cover exact matches. The other 10 kernels were equally split in the cosine value range [\u22121, 1]: \u00b5 1 = 0.9, \u00b5 2 = 0.7, ..., \u00b5 10 = \u22120.9; \u03c3 was set to 0.1.",
  "y": "similarities uses"
 },
 {
  "id": "4e7ee576b07a8a21a42472bf921291_8",
  "x": "Performance is stable across most trials. Table 1 also shows results reported by Xiong et al<cite> [10]</cite> . Their model performance falls in the lower end of our trials, probably due to different vocabularies and stopping conditions.",
  "y": "differences"
 },
 {
  "id": "4f5a25d7a961e7e61c2caef81418e0_0",
  "x": "Both the decoder and encoder are jointly trained on a data set consisting of sentence-summary pairs. Our model can be seen as an extension of the recently proposed model for the same problem by<cite> Rush et al. (2015)</cite> . While they use a feed-forward neural language model for generation, we use a recurrent neural network.",
  "y": "extends differences"
 },
 {
  "id": "4f5a25d7a961e7e61c2caef81418e0_1",
  "x": "Empirically we show that our model beats the state-of-the-art systems of<cite> Rush et al. (2015)</cite> on multiple data sets. Particularly notable is the fact that even with a simple generation module, which does not use any extractive feature tuning, our model manages to significantly outperform their ABS+ system on the Gigaword data set and is comparable on the DUC-2004 task. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "4f5a25d7a961e7e61c2caef81418e0_2",
  "x": "Later, along the lines of Banko et al. (2000) , MOSES was used directly as a method for text simplification by Wubben et al. (2012) . Other works which have recently been proposed for the problem of sentence summarization include (Galanis and Androutsopoulos, 2010; Napoles et al., 2011; Cohn and Lapata, 2013) . Very recently<cite> Rush et al. (2015)</cite> proposed a neural attention model for this problem using a new data set for training and showing state-of-the-art performance on the DUC tasks.",
  "y": "background"
 },
 {
  "id": "4f5a25d7a961e7e61c2caef81418e0_3",
  "x": "Our models are trained on the annotated version of the Gigaword corpus (Graff et al., 2003; Napoles et al., 2012) and we use only the annotations for tokenization and sentence separation while discarding other annotations such as tags and parses. We pair the first sentence of each article with its headline to form sentence-summary pairs. The data is pre-processed in the same way as<cite> Rush et al. (2015)</cite> and we use the same splits for training, validation, and testing.",
  "y": "similarities uses"
 },
 {
  "id": "4f5a25d7a961e7e61c2caef81418e0_4",
  "x": "For Gigaword we report results on the same randomly held-out test set of 2000 sentence-summary pairs as <cite>(Rush et al., 2015)</cite> . 1 We also evaluate our models on the DUC-2004 evaluation data set comprising 500 pairs (Over et al., 2007) . Our evaluation is based on three variants of ROUGE (Lin, 2004) , namely, ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest-common substring).",
  "y": "similarities uses"
 },
 {
  "id": "4f5a25d7a961e7e61c2caef81418e0_5",
  "x": "We then pick the model with best perplexity on the held-out set and use it to compute the F1-score of ROUGE-1, ROUGE-2, and ROUGE-L on the test sets, all of which we report. For the DUC corpus however, inline with the standard, we report the recall-only ROUGE. As baseline we use the state-of-the-art attention-based system (ABS) of<cite> Rush et al. (2015)</cite> which relies on a feed-forward network decoder.",
  "y": "similarities uses"
 },
 {
  "id": "4f5a25d7a961e7e61c2caef81418e0_6",
  "x": "As baseline we use the state-of-the-art attention-based system (ABS) of<cite> Rush et al. (2015)</cite> which relies on a feed-forward network decoder. Additionally, we compare to an enhanced version of their system (ABS+), which relies on a range of separate extractive summarization features that are added as log-linear features in a secondary learning step with minimum error rate training (Och, 2003) . Table 1 shows that both our RAS-Elman and RAS-LSTM models achieve lower perplexity than ABS as well as other models reported in<cite> Rush et al. (2015)</cite> .",
  "y": "differences"
 },
 {
  "id": "4f5a25d7a961e7e61c2caef81418e0_7",
  "x": "---------------------------------- **CONCLUSION** We extend the state-of-the-art model for abstractive sentence summarization <cite>(Rush et al., 2015)</cite> to a recurrent neural network architecture.",
  "y": "extends differences"
 },
 {
  "id": "518b281a4bc04b3504d3b385a5dc62_0",
  "x": "Most previous work focuses on segmenting surface forms into their constituent morphs (e.g., taking: tak +ing), but surface form segmentation does not solve the sparse data problem as the analyses of take and taking are not connected to each other. We extend the MorphoChains system <cite>(Narasimhan et al., 2015)</cite> to provide morphological analyses that can abstract over spelling differences in functionally similar morphs. These analyses are not required to use all the orthographic material of a word (stopping: stop +ing), nor are they limited to only that material (acidified: acid +ify +ed).",
  "y": "extends"
 },
 {
  "id": "518b281a4bc04b3504d3b385a5dc62_1",
  "x": "The importance of identifying underlying morphemes rather than surface morphs is widely recognized, for example by the MorphoChallenge organizers, who in later years provided datasets and evaluation measures to encourage this deeper level of analysis (Kurimo et al., 2010) . Nevertheless, only a few systems have attempted this task (Goldwater and Johnson, 2004; Naradowsky and Goldwater, 2009) , and as far as we know, only one, the rule-based MORSEL (Lignos et al., 2009; Lignos, 2010) , has come close to the level of performance achieved by segmentation systems such as Morfessor (Virpioja et al., 2013) . We present a system that adapts the unsupervised MorphoChains segmentation system <cite>(Narasimhan et al., 2015)</cite> to provide morphological analyses that aim to abstract over spelling differences in functionally similar morphemes.",
  "y": "extends"
 },
 {
  "id": "518b281a4bc04b3504d3b385a5dc62_2",
  "x": "We base our work on the MorphoChains segmentation system <cite>(Narasimhan et al., 2015)</cite> , 1 which defines a morphological chain as a sequence of child-parent pairs. Each pair consists of two morphologically related words where the child must be longer than the parent. To analyse a word we want to find the best parent for that word; we do so recursively until we conclude that the stop condition is met (i.e. a word doesn't have a morphological parent).",
  "y": "uses"
 },
 {
  "id": "518b281a4bc04b3504d3b385a5dc62_3",
  "x": "---------------------------------- **MODEL** We predict child-parent pairs using a log-linear model, following<cite> Narasimhan et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "518b281a4bc04b3504d3b385a5dc62_4",
  "x": "Following<cite> Narasimhan et al. (2015)</cite>, we do so using Contrastive Estimation (CE) (Smith and Eisner, 2005) . In CE every training example w \u2208 W serves as both a positive example and a set of implied negative examples-strings that are similar to w but don't occur in the corpus. The negative examples are the source of the probability mass allocated to the positive examples.",
  "y": "uses"
 },
 {
  "id": "518b281a4bc04b3504d3b385a5dc62_5",
  "x": "The word w and its negative examples constitute the neighbourhood N (w) of w. Given the list of words W and their neighbourhoods, the CE likelihood is defined as: We use the same neighbourhood functions as<cite> Narasimhan et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "518b281a4bc04b3504d3b385a5dc62_6",
  "x": "Semantic similarity was an important feature in MorphoChains:<cite> Narasimhan et al. (2015)</cite> concluded that up to 25 percent of their model's precision was due to the semantic similarity feature. We use the same feature here (COS). For a child-parent pair (w A , w B ) with word embeddings v w A and v w B respectively we compute semantic similarity as:",
  "y": "background"
 },
 {
  "id": "518b281a4bc04b3504d3b385a5dc62_7",
  "x": "Semantic Similarity Morphologically related words exhibit semantic similarity among their word embeddings (Schone and Jurafsky, 2000; Baroni et al., 2002) . Semantic similarity was an important feature in MorphoChains:<cite> Narasimhan et al. (2015)</cite> concluded that up to 25 percent of their model's precision was due to the semantic similarity feature. We use the same feature here (COS).",
  "y": "uses"
 },
 {
  "id": "518b281a4bc04b3504d3b385a5dc62_8",
  "x": "Affixes Candidate pairs where the child contains a frequently occurring affix are more likely to be correct. To identify possible affixes to use as features,<cite> Narasimhan et al. (2015)</cite> counted the number of words that end (or start) with each substring Table 3 : Examples illustrating which of the binary features in the model are active for various potential child-parent pairs. Not shown here is the real-valued semantic similarity feature COS, used in all examples except 9 and 10, where it is replaced by the binary feature STOPCOS=y, for y in increments of 0.1.",
  "y": "background"
 },
 {
  "id": "518b281a4bc04b3504d3b385a5dc62_9",
  "x": "Semantic similarity was an important feature in MorphoChains:<cite> Narasimhan et al. (2015)</cite> concluded that up to 25 percent of their model's precision was due to the semantic similarity feature. We use the same feature here (COS). To identify possible affixes to use as features,<cite> Narasimhan et al. (2015)</cite> counted the number of words that end (or start) with each substring Table 3 : Examples illustrating which of the binary features in the model are active for various potential child-parent pairs.",
  "y": "uses"
 },
 {
  "id": "518b281a4bc04b3504d3b385a5dc62_10",
  "x": "Stop Condition To identify words with no parents we use two types of binary features suggested by<cite> Narasimhan et al. (2015)</cite> . STOPCOS=y is the maximum cosine similarity between the word and any of its parent candidates (using bins of size 0.1), and STOPLEN=x is instantiated for all possible word lengths x in the training data. For illustration, if we are considering whether decided is a word with no parents (Table 3 Example 10), the binary features STOPLEN=7 and STOPCOS=0.5 become active.",
  "y": "uses"
 },
 {
  "id": "518b281a4bc04b3504d3b385a5dc62_11",
  "x": "The system that performed best on those languages was then used unmodified on the test languages. Hyperparameters In addition to threshold values described above, we use the same \u03bb = 1 (Equation 3) as<cite> Narasimhan et al. (2015)</cite> . To control for under segmentation we downscale weights of the stop features by a factor of 0.8.",
  "y": "uses"
 },
 {
  "id": "518b281a4bc04b3504d3b385a5dc62_12",
  "x": "Baselines We compare our model to three other systems: Morfessor 2.0 (Virpioja et al., 2013) , MORSEL (Lignos et al., 2009; Lignos, 2010) and MorphoChains <cite>(Narasimhan et al., 2015)</cite> . We also use a trivial baseline Words which outputs the input word. Morfessor (Morf.2.0) is a family of probabilistic algorithms that perform unsupervised word segmentation into morphs.",
  "y": "uses"
 },
 {
  "id": "518b281a4bc04b3504d3b385a5dc62_13",
  "x": "Results show the importance of the LSE-based affix features (Model-A). Using these features gives gains of +1.0%, 3.8% and 0.6% F-1 absolute on English, German and Turkish respectively over using the raw affix occurrence frequencies as used by<cite> Narasimhan et al. (2015)</cite> . We can see that our data selection scheme (Model-D) is important for English (+3.5%) and German (+1.1%).",
  "y": "uses"
 },
 {
  "id": "518d8a8395e38d9971bd51344cf1b8_0",
  "x": "However, we apply the model to correct errors in Egyptian Arabic dialect text, following a conventional orthography standard, CODA (Habash et al., 2012) . Using the described approach, we demonstrate a word-error-rate (WER) reduction of 65% over a do-nothing input baseline, and we improve over a state-of-the-art system<cite> (Eskander et al., 2013)</cite> which relies heavily on language-specific and manually-selected constraints. We present a detailed analysis of mistakes and demonstrate that the proposed model indeed learns to correct a wider variety of errors.",
  "y": "differences background motivation"
 },
 {
  "id": "518d8a8395e38d9971bd51344cf1b8_1",
  "x": "Discriminative models have been proposed at the word-level for error correction and for error detection (Habash and Roth, 2011) . In addition, there has been growing work on lexical normalization of social media data, a somewhat related problem to that considered in this paper (Han and Baldwin, 2011; Han et al., 2013; Subramaniam et al., 2009; Ling et al., 2013) . The work of<cite> Eskander et al. (2013)</cite> is the most relevant to the present study: it presents a character-edit classification model (CEC) using the same dataset we use in this paper.",
  "y": "similarities background"
 },
 {
  "id": "518d8a8395e38d9971bd51344cf1b8_2",
  "x": "2<cite> Eskander et al. (2013)</cite> analyzed the data to identify the seven most common types of errors. They developed seven classifiers and applied them to the data in succession. This makes the approach tailored to the specific data set in use and limited to a specific set of errors.",
  "y": "uses similarities"
 },
 {
  "id": "518d8a8395e38d9971bd51344cf1b8_3",
  "x": "In this work, a single model is considered for all types of errors. The model considers every character in the input text for a possible spelling error, as opposed to looking only at certain input characters and contexts in which they appear. Moreover, in contrast to<cite> Eskander et al. (2013)</cite> , it looks beyond the boundary of the current word.",
  "y": "differences"
 },
 {
  "id": "518d8a8395e38d9971bd51344cf1b8_4",
  "x": "\u2022 delete: e i is deleted. \u2022 insert(c): A character c is inserted bef ore e i . To address errors occurring at the end 2<cite> Eskander et al. (2013)</cite> also considered a slower, more expensive, and more language-specific method using a morphological tagger that outperformed the CEC model; however, we do not compare to it in this paper.",
  "y": "differences background"
 },
 {
  "id": "518d8a8395e38d9971bd51344cf1b8_5",
  "x": "The Delete label is a single label that comprises all deletion actions. Labels modeled by<cite> Eskander et al. (2013)</cite> are marked with E , and EP for cases modeled partially, for example, the Insert{A} would only be applied at certain positions such as the end of the word. spellings of the same word.",
  "y": "uses"
 },
 {
  "id": "518d8a8395e38d9971bd51344cf1b8_6",
  "x": "spellings of the same word. The CODA orthography was proposed by Habash et al. (2012) in an attempt to standardize dialectal writing, and we use it as a reference of correct text for spelling correction following the previous work by<cite> Eskander et al. (2013)</cite> . We use the same corpus (labeled \"ARZ\") and experimental setup splits used by them.",
  "y": "uses"
 },
 {
  "id": "518d8a8395e38d9971bd51344cf1b8_7",
  "x": "3 We group the actions into: Substitute, Insert, Delete, and Complex, and also list common transformations within each group. We further distinguish between the phenomena modeled by our system and by<cite> Eskander et al. (2013)</cite> . At least 10% of all generated action labels are not handled by<cite> Eskander et al. (2013)</cite> .",
  "y": "differences"
 },
 {
  "id": "518d8a8395e38d9971bd51344cf1b8_8",
  "x": "We include a set of basic features inspired by<cite> Eskander et al. (2013)</cite> in their CEC system and additional features for further improvement. Basic features We use a set of nine basic features: the given character, the preceding and following two characters, and the first two and last two characters in the word. These are the same features used by CEC, except that CEC does not include characters beyond the word boundary, while we consider space characters as well as characters from the previous and next words.",
  "y": "uses background motivation"
 },
 {
  "id": "518d8a8395e38d9971bd51344cf1b8_9",
  "x": "A speech effect handling step was applied as a preprocessing step to all models. This step removes redundant repetitions of characters in sequence, e.g., ktyyyyyr 'veeeeery'. The same speech effect handling was applied by<cite> Eskander et al. (2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "518d8a8395e38d9971bd51344cf1b8_10",
  "x": "Finally, the accuracy Acc metric, used by<cite> Eskander et al. (2013)</cite> , is a simple string matching metric which enforces a word alignment that pairs words in the reference to those of the output. It is calculated by dividing the number of correct output words by the number of words in the input. This metric assumes no split errors in the data (a word incorrectly split into two words), which is the case in the data we are working with.",
  "y": "background"
 },
 {
  "id": "518d8a8395e38d9971bd51344cf1b8_12",
  "x": "Table 5 presents the results of our best model (GSEC+4grams), and best model+MLE. The latter achieves a 92.1% Acc score. The Acc score reported by<cite> Eskander et al. (2013)</cite> for CEC+MLE is 91.3% .",
  "y": "background"
 },
 {
  "id": "520437f612e678dcd4ec9c043cf701_0",
  "x": "This paper investigates syntactic stylometry for deception detection, adding a somewhat unconventional angle to prior literature. Over four different datasets spanning from the product review to the essay domain, we demonstrate that features driven from Context Free Grammar (CFG) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico-syntactic features. Our results improve the best published result on the hotel review data (<cite>Ott et al., 2011</cite>) reaching 91.2% accuracy with 14% error reduction.",
  "y": "differences"
 },
 {
  "id": "520437f612e678dcd4ec9c043cf701_1",
  "x": "Most are based on dictionarybased word counting using LIWC (Pennebaker et al., 2007 ) (e.g., Hancock et al. (2007) , Vrij et al. (2007) ), while some recent ones explored the use of machine learning techniques using simple lexico-syntactic patterns, such as n-grams and part-of-speech (POS) tags (Mihalcea and Strapparava (2009) , <cite>Ott et al. (2011)</cite> ). <cite>These</cite> previous studies unveil interesting correlations between certain lexical items or categories with deception that may not be readily apparent to human judges. For instance, the work of <cite>Ott et al. (2011)</cite> in the hotel review domain results in very insightful observations that deceptive reviewers tend to use verbs and personal pronouns (e.g., \"I\", \"my\") more often, while truthful reviewers tend to use more of nouns, adjectives, prepositions.",
  "y": "background"
 },
 {
  "id": "520437f612e678dcd4ec9c043cf701_2",
  "x": "Most are based on dictionarybased word counting using LIWC (Pennebaker et al., 2007 ) (e.g., Hancock et al. (2007) , Vrij et al. (2007) ), while some recent ones explored the use of machine learning techniques using simple lexico-syntactic patterns, such as n-grams and part-of-speech (POS) tags (Mihalcea and Strapparava (2009) , <cite>Ott et al. (2011)</cite> ). <cite>These</cite> previous studies unveil interesting correlations between certain lexical items or categories with deception that may not be readily apparent to human judges. For instance, the work of <cite>Ott et al. (2011)</cite> in the hotel review domain results in very insightful observations that deceptive reviewers tend to use verbs and personal pronouns (e.g., \"I\", \"my\") more often, while truthful reviewers tend to use more of nouns, adjectives, prepositions.",
  "y": "background"
 },
 {
  "id": "520437f612e678dcd4ec9c043cf701_3",
  "x": "<cite>These</cite> previous studies unveil interesting correlations between certain lexical items or categories with deception that may not be readily apparent to human judges. For instance, the work of <cite>Ott et al. (2011)</cite> in the hotel review domain results in very insightful observations that deceptive reviewers tend to use verbs and personal pronouns (e.g., \"I\", \"my\") more often, while truthful reviewers tend to use more of nouns, adjectives, prepositions. In parallel to these shallow lexical patterns, might there be deep syntactic structures that are lurking in deceptive writing?",
  "y": "background"
 },
 {
  "id": "520437f612e678dcd4ec9c043cf701_4",
  "x": "Our results improve the best published result on the hotel review data of <cite>Ott et al. (2011)</cite> reaching 91.2% accuracy with 14% error reduction. We also achieve substantial improvement over the essay data of Mihalcea and Strapparava (2009) , obtaining upto 85.0% accuracy. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "520437f612e678dcd4ec9c043cf701_5",
  "x": "To explore different types of deceptive writing, we consider the following four datasets spanning from the product review to the essay domain: I. TripAdvisor-Gold: Introduced in <cite>Ott et al. (2011)</cite> , this dataset contains 400 truthful reviews obtained from www.tripadviser.com and 400 deceptive reviews gathered using Amazon Mechanical Turk, evenly distributed across 20 Chicago hotels. II. TripAdvisor-Heuristic: This dataset contains 400 truthful and 400 deceptive reviews harvested from www.tripadviser.com, based on fake review detection heuristics introduced in Feng et al. (2012) .",
  "y": "uses"
 },
 {
  "id": "520437f612e678dcd4ec9c043cf701_6",
  "x": "**FEATURE ENCODING** Words Previous work has shown that bag-ofwords are effective in detecting domain-specific deception (<cite>Ott et al., 2011</cite>; Mihalcea and Strapparava, 2009 ). We consider unigram, bigram, and the union of the two as features.",
  "y": "background"
 },
 {
  "id": "520437f612e678dcd4ec9c043cf701_7",
  "x": "Note that <cite>Ott et al. (2011)</cite> found that even though POS tags are effective in detecting fake product reviews, they are not as effective as words. Therefore, we strengthen POS features with unigram features. Deep syntax We experiment with four different encodings of production rules based on the Probabilistic Context Free Grammar (PCFG) parse trees as follows:",
  "y": "background"
 },
 {
  "id": "520437f612e678dcd4ec9c043cf701_8",
  "x": "Note that <cite>Ott et al. (2011)</cite> found that even though POS tags are effective in detecting fake product reviews, they are not as effective as words. Therefore, we strengthen POS features with unigram features. Deep syntax We experiment with four different encodings of production rules based on the Probabilistic Context Free Grammar (PCFG) parse trees as follows:",
  "y": "motivation"
 },
 {
  "id": "520437f612e678dcd4ec9c043cf701_9",
  "x": "We first discuss the results for the TripAdvisorGold dataset shown in Table 2 . As reported in <cite>Ott et al. (2011)</cite> , bag-of-words features achieve surprisingly high performance, reaching upto 89.6% accuracy. Deep syntactic features, encoded asr * slightly improves this performance, achieving 90.4% accuracy.",
  "y": "background"
 },
 {
  "id": "520437f612e678dcd4ec9c043cf701_10",
  "x": "We first discuss the results for the TripAdvisorGold dataset shown in Table 2 . As reported in <cite>Ott et al. (2011)</cite> , bag-of-words features achieve surprisingly high performance, reaching upto 89.6% accuracy. Deep syntactic features, encoded asr * slightly improves this performance, achieving 90.4% accuracy.",
  "y": "similarities"
 },
 {
  "id": "520437f612e678dcd4ec9c043cf701_11",
  "x": "We first discuss the results for the TripAdvisorGold dataset shown in Table 2 . As reported in <cite>Ott et al. (2011)</cite> , bag-of-words features achieve surprisingly high performance, reaching upto 89.6% accuracy. Deep syntactic features, encoded asr * slightly improves this performance, achieving 90.4% accuracy.",
  "y": "differences"
 },
 {
  "id": "520437f612e678dcd4ec9c043cf701_12",
  "x": "Deep syntactic features, encoded asr * slightly improves this performance, achieving 90.4% accuracy. When these syntactic features are combined with unigram features, we attain the best performance of 91.2% accuracy, 2 We use LIBLINEAR (Fan et al., 2008) with L2-regulization, parameter optimized over the 80% training data (3 folds for training, 1 fold for testing). 3 Numbers in italic are classification results reported in <cite>Ott et al. (2011)</cite> and Mihalcea and Strapparava (2009).",
  "y": "background"
 },
 {
  "id": "520437f612e678dcd4ec9c043cf701_13",
  "x": "4 The Yelp data we explored in this work shares a similar spirit in that gold standard labels are harvested from existing meta data, which are not guaranteed to align well with true hidden labels as to deceptive v.s. truthful reviews. Two previous work obtained more precise gold standard labels by hiring Amazon turkers to write deceptive articles (e.g., Mihalcea and Strapparava (2009), <cite>Ott et al. (2011)</cite> ), both of which have been examined in this study with respect to their syntactic characteristics. Although we are not aware of any prior work that dealt with syntactic cues in deceptive writing directly, prior work on hedge detection (e.g., Greene and Resnik (2009), Li et al. (2010) ) relates to our findings.",
  "y": "uses background"
 },
 {
  "id": "52b5ed7a0753402ad4bceb83a2b495_0",
  "x": "Most of the presented works study the interrelationship between words in a text snip- pet<cite> (Hill et al., 2016</cite>; Kiros et al., 2015; Le and Mikolov, 2014) in an unsupervised fashion. Other methods build a task specific representation (Kim, 2014; Collobert et al., 2011) . In this paper we propose to use the covariance matrix of the word vectors in some document to define a novel descriptor for a document.",
  "y": "background"
 },
 {
  "id": "52b5ed7a0753402ad4bceb83a2b495_1",
  "x": "We call our representation DoCoV descriptor. Our descriptor obtains a fixed-length representation of the paragraph which captures the interrelationship between the dimensions of the word embedding via the covariance matrix elements. This makes our work distinguished from to the work of (Le and Mikolov, 2014; <cite>Hill et al., 2016</cite>; Kiros et al., 2015) where they study the interrelationship of words in the text snippet.",
  "y": "differences"
 },
 {
  "id": "52b5ed7a0753402ad4bceb83a2b495_2",
  "x": "(1) Some neural-based paragraph representations such as paragraph vectors (Le and Mikolov, 2014) , FastSent<cite> (Hill et al., 2016)</cite> use a shared space between the words and paragraphs. This is counter intuitive, as the paragraph is a different entity other than the words. Figure 1 illustrates that point, we do not see a clear interpretation of why the paragraph vectors (Le and Mikolov, 2014) are positioned in the space as in figure 1 .",
  "y": "background"
 },
 {
  "id": "52b5ed7a0753402ad4bceb83a2b495_3",
  "x": "**MOTIVATION AND CONTRIBUTIONS** Below we describe our motivation towards the proposal of our novel representation: (1) Some neural-based paragraph representations such as paragraph vectors (Le and Mikolov, 2014) , FastSent<cite> (Hill et al., 2016)</cite> use a shared space between the words and paragraphs.",
  "y": "motivation background"
 },
 {
  "id": "52b5ed7a0753402ad4bceb83a2b495_4",
  "x": "Moreover, there is no inference steps involved while computing the covariance matrix given its observations. This is an advantage compared to existing methods for generating paragraph vectors, such as (Le and Mikolov, 2014;<cite> Hill et al., 2016)</cite> . Our contribution in this work is two-fold: (1) We propose the Document-Covariance descriptor (DoCoV) to represent every document as the covariance of the word embedding of its words.",
  "y": "differences"
 },
 {
  "id": "52b5ed7a0753402ad4bceb83a2b495_5",
  "x": "Recently other neural-based sentence and paragraph level representations appeared to provide a fixed length representation like Skip-Thought Vectors (Kiros et al., 2015) and FastSent<cite> (Hill et al., 2016)</cite> . Some efforts focused on defining a Word Mover Distance(WMD) based on word level representation (Kusner et al., 2015) . Prior to this work, we proposed earlier trials for using covariance features in community question answering (Malhas et al., 2016b,a; Torki et al., 2017) .",
  "y": "background"
 },
 {
  "id": "52b5ed7a0753402ad4bceb83a2b495_6",
  "x": "We follow the setup used in<cite> (Hill et al., 2016)</cite> . ---------------------------------- **DATASETS AND BASELINES**",
  "y": "uses"
 },
 {
  "id": "52b5ed7a0753402ad4bceb83a2b495_7",
  "x": "We contrast our results against the methods reported in<cite> (Hill et al., 2016)</cite> . The competing methods are the paragraph vectors (Le and Mikolov, 2014) , skip-thought vectors (Kiros et al., 2015) , Fastsent<cite> (Hill et al., 2016)</cite> , Sequential (Denoising) Autoencoders (SDAE)<cite> (Hill et al., 2016)</cite> . The Mean vector baseline is also implemented.",
  "y": "differences"
 },
 {
  "id": "52b5ed7a0753402ad4bceb83a2b495_8",
  "x": "**DATASETS AND BASELINES** We contrast our results against the methods reported in<cite> (Hill et al., 2016)</cite> . The competing methods are the paragraph vectors (Le and Mikolov, 2014) , skip-thought vectors (Kiros et al., 2015) , Fastsent<cite> (Hill et al., 2016)</cite> , Sequential (Denoising) Autoencoders (SDAE)<cite> (Hill et al., 2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "52b5ed7a0753402ad4bceb83a2b495_9",
  "x": "Also, we use the sum of the similarities generated by the DoCoV and the mean vectors. All of our results are reported using the freely available Gnews word2vec of dim = 300. We use same evaluation measures<cite> (Hill et al., 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "52b5ed7a0753402ad4bceb83a2b495_10",
  "x": "Other models such as skipthought vectors (Kiros et al., 2015) and SDAE<cite> (Hill et al., 2016)</cite> requires building an encoder-decoder model which takes time 3 to learn. For other models like paragraph vectors (Le and Mikolov, 2014) and Fastsent vectors<cite> (Hill et al., 2016)</cite> , they require a gradient descent inference step to compute the paragraph/sentence vectors. Using the DoCoV, we just require a pre-trained word embedding model and we do not need any additional training like encoder-decoder models or inference steps via gradient descent.",
  "y": "background"
 },
 {
  "id": "52b5ed7a0753402ad4bceb83a2b495_11",
  "x": "Other models such as skipthought vectors (Kiros et al., 2015) and SDAE<cite> (Hill et al., 2016)</cite> requires building an encoder-decoder model which takes time 3 to learn. For other models like paragraph vectors (Le and Mikolov, 2014) and Fastsent vectors<cite> (Hill et al., 2016)</cite> , they require a gradient descent inference step to compute the paragraph/sentence vectors. Using the DoCoV, we just require a pre-trained word embedding model and we do not need any additional training like encoder-decoder models or inference steps via gradient descent.",
  "y": "background"
 },
 {
  "id": "52b5ed7a0753402ad4bceb83a2b495_12",
  "x": "Other models such as skipthought vectors (Kiros et al., 2015) and SDAE<cite> (Hill et al., 2016)</cite> requires building an encoder-decoder model which takes time 3 to learn. For other models like paragraph vectors (Le and Mikolov, 2014) and Fastsent vectors<cite> (Hill et al., 2016)</cite> , they require a gradient descent inference step to compute the paragraph/sentence vectors. Using the DoCoV, we just require a pre-trained word embedding model and we do not need any additional training like encoder-decoder models or inference steps via gradient descent.",
  "y": "differences background"
 },
 {
  "id": "52b5ed7a0753402ad4bceb83a2b495_13",
  "x": "We use linear<cite> (Hill et al., 2016)</cite> . (Kiros et al., 2015) 75.5 79.3 91.4 92.1 84.58 bi-skip (Kiros et al., 2015) 73.9 77.9 89.4 92.5 84.43 comb-skip (Kiros et al., 2015) 76.5 80.1 92.2 93.6 85.6 FastSent<cite> (Hill et al., 2016)</cite> 70.8 78.4 76.8 88.7 78.68 FastSentAE<cite> (Hill et al., 2016)</cite> 71.8 76.7 80.4 88.8 79.43 SAE<cite> (Hill et al., 2016)</cite> 62.6 68 80.2 86.1 74.23 SAE+embs<cite> (Hill et al., 2016)</cite> 73.2 75.3 80.4 89.8 79.68 SDAE<cite> (Hill et al., 2016)</cite> 67.6 74 77.6 89.3 77.13 SDAE+embs<cite> (Hill et al., 2016)</cite> 74 SVM for all the tasks. All of our results are reported using the freely available Gnews word2vec of dim = 300.",
  "y": "uses"
 },
 {
  "id": "52b5ed7a0753402ad4bceb83a2b495_15",
  "x": "All of our results are reported using the freely available Gnews word2vec of dim = 300. We use classification accuracy as the evaluation measure for this experiment as<cite> (Hill et al., 2016)</cite> . The subsets used in comparative benchmark evaluation are: Movie Reviews MR (Pang and Lee, 2005) , Subjectivity Subj (Pang and Lee, 2004) ,Customer Reviews CR (Hu and Liu, 2004) and TREC Question TREC (Li and Roth, 2002) .",
  "y": "uses"
 },
 {
  "id": "52b5ed7a0753402ad4bceb83a2b495_16",
  "x": "We further observe that DoCoV is consistently better than the paragraph vectors (Le and Mikolov, 2014) , Fastsent and SDAE<cite> (Hill et al., 2016)</cite> . The overall accuracy of DoCoV is highlighted and it outperforms other methods on the text classification benchmark. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "53bc4206427e95d600f787e0531df1_0",
  "x": "Next, we will describe the detail of our method of PMI-\u03b2 priors. However, just consider the frequency of bi-term in corpus-level will generate the topics which contain too many common words. To solve this problem, we consider the <cite>Pointwise Mutual Information (PMI)</cite> <cite>[9]</cite> .",
  "y": "uses"
 },
 {
  "id": "53bc4206427e95d600f787e0531df1_1",
  "x": "To solve this problem, we consider the <cite>Pointwise Mutual Information (PMI)</cite> <cite>[9]</cite> . Since the <cite>PMI</cite> score not only considers the co-occurrence frequency of the two words, but also normalizes by the single word frequency. Thus, we want to apply <cite>PMI</cite> score in the original BTM.",
  "y": "background"
 },
 {
  "id": "53bc4206427e95d600f787e0531df1_2",
  "x": "To solve this problem, we consider the <cite>Pointwise Mutual Information (PMI)</cite> <cite>[9]</cite> . Thus, we want to apply <cite>PMI</cite> score in the original BTM.",
  "y": "uses"
 },
 {
  "id": "53bc4206427e95d600f787e0531df1_3",
  "x": "To solve this problem, we consider the <cite>Pointwise Mutual Information (PMI)</cite> <cite>[9]</cite> . A suitable way to apply <cite>PMI</cite> scores is modifying the priors in the BTM.",
  "y": "uses"
 },
 {
  "id": "53bc4206427e95d600f787e0531df1_4",
  "x": "To solve this problem, we consider the <cite>Pointwise Mutual Information (PMI)</cite> <cite>[9]</cite> . Applying the <cite>PMI</cite> score to the \u03b2-priors is the only one choice because we can adjust the degree of the word co-occurrence by modifying the distributions in the \u03b2-priors.",
  "y": "uses"
 },
 {
  "id": "53cd860c539e20874ada4343ab788f_0",
  "x": "2 In NLP, on the other hand, what is important is having a relatively full set of features for the particular group of languages you are working on. This mismatch of needs has motivated various proposals to reconstruct missing entries, in WALS and other databases, from known entries (Daum\u00e9 III and Campbell, 2007; Daum\u00e9 III, 2009;<cite> Coke et al., 2016</cite>; Littell et al., 2017) . In this study, we examine whether we can tackle the problem of inferring linguistic typology from parallel corpora, specifically by training a massively multi-lingual neural machine translation (NMT) system and using the learned representations to infer typological features for each language.",
  "y": "background"
 },
 {
  "id": "53cd860c539e20874ada4343ab788f_1",
  "x": "Baseline Feature Vectors: Several previous methods take advantage of typological implicature, the fact that some typological traits correlate strongly with others, to use known features of a language to help infer other unknown features of the language (Daum\u00e9 III and Campbell, 2007; Takamura et al., 2016;<cite> Coke et al., 2016)</cite> . As an alternative that does not necessarily require pre-existing knowledge of the typological features in the language at hand, Littell et al. (2017) have proposed a method for inferring typological features directly from the language's k nearest neighbors (k-NN) according to geodesic distance (distance on the Earth's surface) and genetic distance (distance according to a phylogenetic family tree). In our experiments, our baseline uses this method by taking the 3-NN for each language according to normalized geodesic+genetic distance, and calculating an average feature vector of these three neighbors.",
  "y": "background"
 },
 {
  "id": "53cd860c539e20874ada4343ab788f_2",
  "x": "In contrast to LMVEC, we hypothesize that the alignments to an identical sentence in English, the model will have a stronger signal allowing it to more accurately learn vectors that reflect the syntactic, phonetic, or semantic consistencies of various languages. This has been demonstrated to some extent in previous work that has used specifically engineered alignment-based models (Lewis and Xia, 2008; \u00d6stling, 2015;<cite> Coke et al., 2016)</cite> , and we examine whether these results apply to neural network feature extractors and expand beyond word order and syntax to other types of typology as well. Table 1 : Accuracy of syntactic, phonological, and inventory features using LM language vectors (LMVEC), MT language vectors (MTVEC), MT encoder cell averages (MTCELL) or both MT feature vectors (MTBOTH).",
  "y": "extends"
 },
 {
  "id": "53cd860c539e20874ada4343ab788f_4",
  "x": "We also noted that the accuracies of certain features decreased from NONE-AUX to MTBOTH-AUX, particularly gender markers, case suffix and negative affix, but these decreases were to a lesser extent in magnitude than the improvements. Interestingly, and in contrast to previous methods for inferring typology from raw text, which have been specifically designed for inducing word order or other syntactic features (Lewis and Xia, Table 2 : Top 5 improvements from \"NONE -Aux\" to \"MTBOTH -Aux\" in the syntax (\"S \"), phonology (\"P \"), and inventory (\"I \") classes. 2008; \u00d6stling, 2015;<cite> Coke et al., 2016)</cite> , our proposed method is also able to infer information about phonological or phonetic inventory features.",
  "y": "differences"
 },
 {
  "id": "547551e556d8aa919f731da99424c9_0",
  "x": "Some recent work has addressed this by learning general-purpose sentence representations Wieting et al., 2015; Hill et al., 2016;<cite> Conneau et al., 2017</cite>; McCann et al., 2017; Jernite et al., 2017; Nie et al., 2017; Pagliardini et al., 2017) . However, there exists no clear consensus yet on what training objective or methodology is best suited to this goal. Understanding the inductive biases of distinct neural models is important for guiding progress in representation learning.",
  "y": "background"
 },
 {
  "id": "547551e556d8aa919f731da99424c9_1",
  "x": "Arora et al. (2016) , however, demonstrate that simple word embedding averages are comparable to more complicated models like skip-thoughts. More recently, <cite>Conneau et al. (2017)</cite> show that a completely supervised approach to learning sentence representations from natural language inference data outperforms all previous approaches on transfer learning benchmarks. Here we use the terms \"transfer learning performance\" on \"transfer tasks\" to mean the performance of sentence representations evaluated on tasks unseen during training.",
  "y": "background"
 },
 {
  "id": "547551e556d8aa919f731da99424c9_2",
  "x": "This is the same classification strategy adopted by <cite>Conneau et al. (2017)</cite> . We train on a collection of about 1 million sentence pairs from the SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017) corpora. Dong et al. (2015) use periodic task alternations with equal training ratios for every task.",
  "y": "uses"
 },
 {
  "id": "547551e556d8aa919f731da99424c9_3",
  "x": "**EVALUATION STRATEGY** We follow a similar evaluation protocol to those presented in ; Hill et al. (2016) ; <cite>Conneau et al. (2017)</cite> which is to use our learned representations as features for a low complexity classifier (typically linear) on a novel supervised task/domain unseen during training without updating the parameters of our sentence representation model. We also consider such a transfer learning evaluation in an artificially constructed low-resource setting.",
  "y": "similarities"
 },
 {
  "id": "547551e556d8aa919f731da99424c9_4",
  "x": "We also consider such a transfer learning evaluation in an artificially constructed low-resource setting. In addition, we also evaluate the quality of our learned individual word representations using standard benchmarks (Faruqui & Dyer, 2014; Tsvetkov et al., 2015) . The choice of transfer tasks and evaluation framework 3 are borrowed largely from <cite>Conneau et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "547551e556d8aa919f731da99424c9_5",
  "x": "Unlike <cite>Conneau et al. (2017)</cite> , who use pretrained GloVe word embeddings, we learn our word embeddings from scratch. Somewhat surprisingly, in Table 3 we observe that the learned word embeddings are competitive with popular methods such as GloVe, word2vec, and fasttext (Bojanowski et al., 2016) on the benchmarks presented by Faruqui & Dyer (2014) and Tsvetkov et al. (2015) . In Table 5 , we probe our sentence representations to determine if certain sentence characteristics and syntactic properties can be inferred following work by Adi et al. (2016) and Shi et al. (2016) .",
  "y": "differences"
 },
 {
  "id": "547551e556d8aa919f731da99424c9_6",
  "x": "We also report QVEC benchmarks (Tsvetkov et al., 2015) Model Accuracy 1k 5k 10k 25k All (400k) Table 4 : Supervised & low-resource classification accuracies on the Quora duplicate question dataset. Accuracies are reported corresponding to the number of training examples used. The first 6 rows are taken from , the next 4 are from Tomar et al. (2017) , the next 5 from Shen et al. (2017) and The last 4 rows are our experiments using Infersent<cite> (Conneau et al., 2017)</cite> and our models.",
  "y": "uses"
 },
 {
  "id": "547551e556d8aa919f731da99424c9_7",
  "x": "For natural language inference, the same encoder is used to encode both the premise and hypothesis and a concatenation of their representations along with the absolute difference and hadamard product (as described in <cite>Conneau et al. (2017)</cite> ) are given to a single layer MLP with a dropout (Srivastava et al., 2014 ) rate of 0.3. All models use word embeddings of 512 dimensions and GRUs with either 1500 or 2048 hidden units. We used minibatches of 48 examples and the Adam Kingma & Ba (2014) optimizer with a learning rate of 0.002.",
  "y": "uses"
 },
 {
  "id": "547551e556d8aa919f731da99424c9_9",
  "x": "In addition to performing 10-fold cross-validation to determine the L2 regularization penalty on the logistic regression models, we also tune the way in which our sentence representations are generated from the hidden states corresponding to words in a sentence. For example, use the last hidden state while <cite>Conneau et al. (2017)</cite> perform max-pooling across all of the hidden states. We consider both of these approaches and pick the one with better performance on the validation set.",
  "y": "differences uses"
 },
 {
  "id": "547551e556d8aa919f731da99424c9_10",
  "x": "In tables 3 and 5 we do not concatenate the representations of multiple models. and <cite>Conneau et al. (2017)</cite> provide a detailed description of tasks that are typically used to evaluate sentence representations. We provide a condensed summary and refer readers to their work for a more thorough description. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "547551e556d8aa919f731da99424c9_11",
  "x": "In addition to the above tasks which were considered by <cite>Conneau et al. (2017)</cite> , we also evaluate on the recently published Quora duplicate question dataset 6 since it is an order of magnitude larger than the others (approximately 400,000 question pairs). The task is to correctly identify question pairs that are duplicates of one another, which we formulate as a binary classification problem. We use the same data splits as in .",
  "y": "extends"
 },
 {
  "id": "547551e556d8aa919f731da99424c9_12",
  "x": "Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF, GloVe + WR (U) and all supervised numbers were taken from Arora et al. (2016) and Wieting et al. (2015) and Charagram-phrase numbers were taken from Wieting et al. (2016) . Other numbers were obtained from the evaluation suite provided by <cite>Conneau et al. (2017)</cite>",
  "y": "uses"
 },
 {
  "id": "5503b8571748ea900340aead22743b_0",
  "x": "Several ensemble models have been proposed for the parsing of syntactic dependencies. These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008;<cite> Attardi and Dell'Orletta, 2009)</cite> , and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; Hall et al., 2007; <cite>Attardi and Dell'Orletta, 2009</cite> ). In the latter case, the correctness of the final dependency tree is ensured by: (a) selecting entire trees proposed by the base parsers (Henderson and Brill, 1999) ; or (b) re-parsing the pool of dependencies proposed by the base models (Sagae and Lavie, 2006) .",
  "y": "background"
 },
 {
  "id": "5503b8571748ea900340aead22743b_1",
  "x": "One such algorithm was proposed by<cite> Attardi and Dell'Orletta (2009)</cite> . The algorithm, which has a runtime complexity of O(n), builds dependency trees using a greedy top-down strategy, i.e., it starts by selecting the highest-scoring root node, then the highest-scoring children, etc. We compare these algorithms against the word-by-word voting scheme in Table 5 .",
  "y": "background"
 },
 {
  "id": "5503b8571748ea900340aead22743b_2",
  "x": "---------------------------------- **ON PARSER INTEGRATION AT LEARNING TIME** Recent work has shown that the combination of base parsers at learning time, e.g., through stacking, yields considerable benefits (Nivre and McDonald, 2008; <cite>Attardi and Dell'Orletta, 2009</cite> ).",
  "y": "background"
 },
 {
  "id": "55f67c918001335974608200a87cfc_0",
  "x": "**INTRODUCTION** Fusing structured knowledge from knowledge graphs into deep models using Graph Neural Networks (GNN) [23, 30, 29] is shown to improve their performance on tasks such as visual question answering [12] , object detection [9] , natural language inference [2] , neural machine translation [11] , and opendomain question answering <cite>[17]</cite> . This particularly helps question answering neural models such as Memory Networks [22] and Key-Value Memory Networks [10] by providing them with wellstructured knowledge on specific and open domains [28] .",
  "y": "background"
 },
 {
  "id": "55f67c918001335974608200a87cfc_1",
  "x": "Most models, however, answer questions using a single information source, usually either a text corpus, or a single knowledge graph. Text corpora have high coverage but extracting information from them is challenging whereas knowledge graphs are incomplete but are easier to extract answers from <cite>[17]</cite> . In this paper, we propose a relational GNN for open-domain question answering that learns contextual knowledge graph embeddings by jointly updating the embeddings from a knowledge graph and a set of linked documents.",
  "y": "motivation"
 },
 {
  "id": "55f67c918001335974608200a87cfc_2",
  "x": "Pioneer works such as Trans-E [1] , Complex-E [18] , Hole-E [13] , and DistMul [25] use unsupervised and mostly linear models to learn such pre-trained representations. A few recent works, on the other hand, use GNNs to compute the knowledge graph representation [24, 20, 27] . These high-level knowledge graph representations are particularly important for question answering task [22, 10, <cite>17]</cite> .",
  "y": "background"
 },
 {
  "id": "55f67c918001335974608200a87cfc_3",
  "x": "A few recent works, on the other hand, use GNNs to compute the knowledge graph representation [24, 20, 27] . These high-level knowledge graph representations are particularly important for question answering task [22, 10, <cite>17]</cite> . We use pre-trained representations to initialize the model and then update them using a relational and bi-directional GNN model.",
  "y": "extends motivation"
 },
 {
  "id": "55f67c918001335974608200a87cfc_4",
  "x": "Our model links the knowledge graph and documents through document-contextualized edges and also links entities with their positions in the corpus. This linking is used in GRAFT-Net as well which also performs question answering through fusing learned knowledge graph and linked document representations <cite>[17]</cite> . Unlike GRAFT-Net, our model uses variants of differential pooling [26] and bi-directional graph attention [19] for more powerful message passing.",
  "y": "background"
 },
 {
  "id": "55f67c918001335974608200a87cfc_5",
  "x": "A query can have zero or multiple answers and hence the task is reduced to binary node classification on G (i.e., binary cross entropy loss). Following <cite>[17]</cite> , we first extract a subgraph G q \u2282 G which contains v aq with high probability. This is done by linking the query entities to G and expanding their neighborhood using Personalized PageRank (PPR) method.",
  "y": "uses"
 },
 {
  "id": "55f67c918001335974608200a87cfc_6",
  "x": "To distinguish inward edges from outward edges, we negate h r . This is distinct from previous approaches which only process incoming nodes [<cite>17]</cite> . The next step is aggregating the embeddings of the edges connecting to the node, i.e., h l eout and h l ein .",
  "y": "differences"
 },
 {
  "id": "55f67c918001335974608200a87cfc_7",
  "x": "We also apply batch-normalization [6] and dropout [16] . We evaluated our model on the WebQuestionsSP dataset consisting of 4,737 natural language questions (i.e., 3,098 training, 250 validation, and 1,639 test questions) posed over Freebase entities [8] . Following <cite>[17]</cite> , we apply the same pre-processing and report average F 1 and Hits@1, as well as micro-average, and macro-average F 1 scores.",
  "y": "uses"
 },
 {
  "id": "55f67c918001335974608200a87cfc_8",
  "x": "Table 1 shows the performance of our model compared to other models that also feature early fusion of the knowledge graph and text. These include Key-Value Memory Networks (KVMN) [3] and GRAFT-Net <cite>[17]</cite> . The results suggest that our model outperforms GRAFT-Net with an absolute increase in all metrics.",
  "y": "differences"
 },
 {
  "id": "563476cdb64cdf7d47bc5e8e1c32c3_0",
  "x": "We interpret the problem of transliterating English named entities into Hindi or Japanese Katakana as a variant of the letter-to-phoneme (L2P) subtask of textto-speech processing. Therefore, we apply a re-implementation of a state-of-the-art, discriminative L2P system <cite>(Jiampojamarn et al., 2008)</cite> to the problem, without further modification. In doing so, we hope to provide a baseline for the NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009) , indicating how much can be achieved without transliteration-specific technology.",
  "y": "uses"
 },
 {
  "id": "563476cdb64cdf7d47bc5e8e1c32c3_1",
  "x": "However, this task's focus on generation has isolated the character-level component, which makes L2P technology a nearideal match. For our submission, we re-implement the L2P approach described by<cite> Jiampojamarn et al. (2008)</cite> as faithfully as possible, and apply it unmodified to the transliteration shared task for the English-to-Hindi (Kumaran and Kellner, 2007) and English-to-Japanese Katakana 1 tests. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "563476cdb64cdf7d47bc5e8e1c32c3_2",
  "x": "This algorithm is used to conduct a search for a max-weight derivation according to a linear model with indicator features. A sample derivation is shown in Figure 1 . There are two main categories of features: context and transition features, which follow the first two feature templates described by<cite> Jiampojamarn et al. (2008)</cite> .",
  "y": "uses"
 },
 {
  "id": "563476cdb64cdf7d47bc5e8e1c32c3_3",
  "x": "Our system made two alternate design decisions (we do not claim improvements) over those made by <cite>(Jiampojamarn et al., 2008)</cite> , mostly based on the availability of software. First, we employed a beam of 40 candidates in our decoder, to enable efficient use of large language model contexts. This is put to good use in the Hindi task, where we found n-gram indicators of length up to n = 6 provided optimal development performance.",
  "y": "differences"
 },
 {
  "id": "56fa13128027f7c37d504d97cfcc45_0",
  "x": "They also show that using SynData directly degrades the performance. Recent work seeks to derive valuable information from SynData while filtering noise, via domain adaptation (Braud and Denis, 2014; , classifying connectives (Rutherford and Xue, 2015) or multi-task learning (Lan et al., 2013;<cite> Liu et al., 2016)</cite> , and shows promising results. society reckon existence youth problems, but many young people think themselves no problems.",
  "y": "background"
 },
 {
  "id": "56fa13128027f7c37d504d97cfcc45_1",
  "x": "We find that fine-tuning word embeddings during training leads to severe overfitting in our experiments. Following <cite>Liu et al. (2016)</cite> , we alternately use two tasks to train the model, one task per epoch. For tasks on both the PDTB and CDTB, we use the same hyper-parameters.",
  "y": "uses"
 },
 {
  "id": "56fa13128027f7c37d504d97cfcc45_2",
  "x": "<cite>Liu et al. (2016)</cite> use a multi-task model with three auxiliary tasks: 1) conn: connective classification on explicit instances, 2) exp: relation classification on the labeled explicit instances in the PDTB, and 3) rst: relation classification on the labeled RST corpus (William and Thompson, 1988), which defines different discourse relations with that in the PDTB. The results are shown in Table 3. Although <cite>Liu et al. (2016)</cite> achieve the stateof-the-art performance (Line 5), they use two additional labeled corpora.",
  "y": "background"
 },
 {
  "id": "56fa13128027f7c37d504d97cfcc45_3",
  "x": "The results are shown in Table 3. Although <cite>Liu et al. (2016)</cite> achieve the stateof-the-art performance (Line 5), they use two additional labeled corpora. We can find that M T N bi (Line 6) yields better results than those systems incorporating SynData (Line 1, 2 and 3), or even the labeled RST (Line 4).",
  "y": "motivation background"
 },
 {
  "id": "56fa13128027f7c37d504d97cfcc45_4",
  "x": "Although <cite>Liu et al. (2016)</cite> achieve the stateof-the-art performance (Line 5), they use two additional labeled corpora. We can find that M T N bi (Line 6) yields better results than those systems incorporating SynData (Line 1, 2 and 3), or even the labeled RST (Line 4). These results confirm that BiSynData can indeed alleviate the disadvantages of SynData effectively.",
  "y": "differences"
 },
 {
  "id": "56fa13128027f7c37d504d97cfcc45_5",
  "x": "More recently, Braud and Denis (2014) and consider explicit data from a different domain, and use domain adaptation methods to explore the effect of them. Rutherford and Xue (2015) propose to gather weakly labeled data from explicit instances via connective classification, which are used as additional training data directly. Lan et al. (2013) and <cite>Liu et al. (2016)</cite> combine explicit and implicit data using multi-task learning models and gain improvements.",
  "y": "background"
 },
 {
  "id": "56fa13128027f7c37d504d97cfcc45_6",
  "x": "Lan et al. (2013) and <cite>Liu et al. (2016)</cite> combine explicit and implicit data using multi-task learning models and gain improvements. Different from all the above work, we construct additional training data from a bilingual corpus. Multi-task neural networks have been successfully used for many NLP tasks.",
  "y": "differences"
 },
 {
  "id": "574ab9a51f3414e6587da7dfca2ff8_0",
  "x": "****NARRATIVE COMPOSITION: ACHIEVING THE PERCEIVED LINEARITY OF NARRATIVE**** **ABSTRACT** The last few years have seen an increased interest in narrative within the field of Natural Language Generation <cite>(Reiter et al., 2008</cite>; Elson and McKeown, 2010; Siddharthan et al., 2012; Lester, 2012) .",
  "y": "background"
 },
 {
  "id": "574ab9a51f3414e6587da7dfca2ff8_1",
  "x": "**** The last few years have seen an increased interest in narrative within the field of Natural Language Generation <cite>(Reiter et al., 2008</cite>; Elson and McKeown, 2010; Siddharthan et al., 2012; Lester, 2012) . Narrative is generally acknowledged as a fundamental mode of presenting and communicating information between humans, with different manifestations across media but with a very significant presence in textual form.",
  "y": "background"
 },
 {
  "id": "57ef27eefdf272bead22212863a8a8_0",
  "x": "Similarly, the lexical predictability ratio (LPR) of<cite> Brooke et al. (2015)</cite> is an association measure applicable to any possible syntactic pattern, which is calculated by discounting syntactic predictability from the overall conditional probability for each word given the other words in the phrase. Though most association measures involve only usage statistics of the phrase and its subparts, the DRUID measure (Riedl and Biemann, 2015) is an exception which uses distributional semantics around the phrase to identify how easily an n-gram could be replaced by a single word. Typically multiword lexicons are created by ranking n-grams according to an association measure and applying a threshold.",
  "y": "background"
 },
 {
  "id": "57ef27eefdf272bead22212863a8a8_1",
  "x": "Lexical bundles have been applied, in particular, to learner language: for example, Chen and Baker (2010) show that non-native student writers use a severely restricted range of lexical bundle types, and tend to overuse those types, while Granger and Bestgen (2014) investigate the role of proficiency, demonstrating that intermediate learners underuse lower-frequency bigrams and overuse high-frequency bigrams relative to advanced learners. Sakaguchi et al. (2016) demonstrate that improving fluency (closely linked to the use of linguistic formulas) is more important than improving strict grammaticality with respect to native speaker judgments of non-native productions;<cite> Brooke et al. (2015)</cite> explicitly argue for FS lexicons as a way to identify, track, and improve learner proficiency. ----------------------------------",
  "y": "background"
 },
 {
  "id": "57ef27eefdf272bead22212863a8a8_2",
  "x": "Our approach to FS identification involves optimization of the total explanatory power of a lattice, where each node corresponds to an n-gram type. The explanatory power of the whole lattice is defined simply as a product of the explainedness of the individual nodes. Each node can be considered either \"on\" (is an FS) or \"off\" (is not an FS). The basis of the calculation of explainedness is the syntax-sensitive LPR association measure of<cite> Brooke et al. (2015)</cite> , but it is calculated differently depending on the on/off status of the node as well as the status of the nodes in its vicinity.",
  "y": "differences uses"
 },
 {
  "id": "57ef27eefdf272bead22212863a8a8_3",
  "x": "**COLLECTING STATISTICS** The first step in the process is to derive a set of ngrams and related statistics from a large, unlabeled corpus of text. Since our primary association measure is an adaption of LPR, our approach in this section mostly follows<cite> Brooke et al. (2015)</cite> up until the last stage.",
  "y": "similarities extends"
 },
 {
  "id": "57ef27eefdf272bead22212863a8a8_4",
  "x": "Since our primary association measure is an adaption of LPR, our approach in this section mostly follows<cite> Brooke et al. (2015)</cite> up until the last stage. An initial requirement of any such method is an n-gram frequency threshold, which we set to 1 instance per 10 million words, following<cite> Brooke et al. (2015)</cite> . 3 We include gapped or non-contiguous n-grams in our analysis, in acknowledgment of the fact that many languages have MWEs where the components can be \"separated\", including verb particle constructions in English (Deh\u00e9, 2002) , and noun-verb idioms in Japanese .",
  "y": "similarities uses"
 },
 {
  "id": "57ef27eefdf272bead22212863a8a8_5",
  "x": "We use the same equation for gapped n-grams, with the caveat that quantities involving sequences which include the location where the gap occurs are derived from special gapped n-gram statistics. Note that the identification of the best ratio across all possible choices of context, not just the largest, is important for longer FS, where the entire POS context alone might uniquely identify the phrase, resulting in the minimum LPR of 1 even for entirely formulaic sequences-an undesirable result. In the segmentation approach of<cite> Brooke et al. (2015)</cite> , LPR for an entire span is calculated as a product of the individual LPRs, but here we will use the minimum LPR across the words in the sequence:",
  "y": "differences"
 },
 {
  "id": "57ef27eefdf272bead22212863a8a8_6",
  "x": "This is done as a preprocessing step, and greatly improves the tractability of the iterative optimization of the lattice. Of course, a threshold for hard covering must be chosen: during development we found that a ratio of 2/3 (corresponding to a significant majority of the counts of a lower node corresponding to the higher node) worked well. We also use the concept of hard covering to address the issue of pronouns, based on the observation that specific pronouns often have high LPR values due to pragmatic biases <cite>(Brooke et al., 2015)</cite> ; for instance, private state verbs like feel tend to have first person singular subjects.",
  "y": "motivation"
 },
 {
  "id": "57ef27eefdf272bead22212863a8a8_7",
  "x": "In English, we follow<cite> Brooke et al. (2015)</cite> in using a 890M token filtered portion of the ICWSM blog corpus (Burton et al., 2009 ) tagged with the Tree Tagger (Schmid, 1995) . To facilitate a comparison with Newman et al. (2012) , which does not scale up to a corpus as large as the ICWSM, we also build a lexicon using the 100M token British National Corpus (Burnard, 2000) , using the standard CLAWS-derived POS tags for the corpus. Lemmatization included removing all inflectional marking from both words and POS tags.",
  "y": "uses"
 },
 {
  "id": "57ef27eefdf272bead22212863a8a8_8",
  "x": "Lemmatization included removing all inflectional marking from both words and POS tags. For English, gaps are identified using the same POS regex used in<cite> Brooke et al. (2015)</cite> , which includes simple nouns and portions thereof, up to a maximum of 4 words. The other two languages we include in our evaluation are Croatian and Japanese.",
  "y": "similarities uses"
 },
 {
  "id": "57ef27eefdf272bead22212863a8a8_9",
  "x": "Benefits of this style of evaluation include replicability, the diversity of FS, and the ability to calculate a true F-score. We use the annotation of 2000 n-grams in the ICWSM corpus from that earlier work, and applied the same annotation methodology to the other three corpora: after training and based on written guidelines derived from the definitions of Wray (2008), three native-speaker, educated annotators judged 500 contiguous n-grams and another 500 gapped n-grams for each corpus. Other than the inclusion of new languages, our test sets differ from<cite> Brooke et al. (2015)</cite> in two ways.",
  "y": "differences"
 },
 {
  "id": "57ef27eefdf272bead22212863a8a8_11",
  "x": "The LPRseg method of<cite> Brooke et al. (2015)</cite> consistently outperforms simple ranking, and the lattice method proposed here does better still, with a margin that is fairly consistent across the languages. Generally, clearing and overlap node interactions provide a relatively large increase in precision at the cost of a smaller drop in recall, though the change is fairly symmetrical in Croatian. When only covering is used, the results are fairly similar to<cite> Brooke et al. (2015)</cite> , which is unsurprising given the extent to which decomposition and covering are related.",
  "y": "differences"
 },
 {
  "id": "57ef27eefdf272bead22212863a8a8_12",
  "x": "The LPRseg method of<cite> Brooke et al. (2015)</cite> consistently outperforms simple ranking, and the lattice method proposed here does better still, with a margin that is fairly consistent across the languages. Generally, clearing and overlap node interactions provide a relatively large increase in precision at the cost of a smaller drop in recall, though the change is fairly symmetrical in Croatian. When only covering is used, the results are fairly similar to<cite> Brooke et al. (2015)</cite> , which is unsurprising given the extent to which decomposition and covering are related.",
  "y": "similarities"
 },
 {
  "id": "57ef27eefdf272bead22212863a8a8_14",
  "x": "Although the optimization of the lattice is several orders of magnitude more complex than the decomposition heuristics of<cite> Brooke et al. (2015)</cite> , the time needed to build and optimize the lattice is a fraction of the time required to collect the statistics for LPR calculation, and so the end-to-end runtimes of the two methods are comparable. In the BNC, the full lattice method was much faster than LocalMaxs and DP-Seg, though direct runtime comparisons to these methods are of modest value due to differences in both scope and implementation. Finally, though the model was designed specifically for FS extraction, we note that it could be useful for related tasks such as unsupervised learning of morphological lexicons, particularly for agglutinative languages.",
  "y": "differences"
 },
 {
  "id": "58b423c4ea2a3530d0c469fc0f5528_0",
  "x": "Furthermore, such technologies can ultimately scale to translate into other formal representations, such as program scripts (Raza et al., 2015) . Prior work has demonstrated the feasibility of this task. <cite>Kushman and Barzilay (2013)</cite> proposed a model that learns to perform the task from a parallel corpus of regular expressions and the text descriptions.",
  "y": "background"
 },
 {
  "id": "58b423c4ea2a3530d0c469fc0f5528_1",
  "x": "Since their model relies heavily on this component, it cannot be readily applied to other formal representations where such semantic equivalence calculations are not possible. In this paper, we reexamine the need for such specialized domain knowledge for this task. Given the same parallel corpus used in <cite>Kushman and Barzilay (2013)</cite> , we use an LSTM-based sequence to sequence neural network to perform the mapping.",
  "y": "uses"
 },
 {
  "id": "58b423c4ea2a3530d0c469fc0f5528_2",
  "x": "Our work, however, is closest to <cite>Kushman and Barzilay (2013)</cite> . They learned a semantic parsing translation model from a parallel dataset of natural language and regular expressions. Their model used a regular expressionspecific semantic unification technique to disambiguate the meaning of the natural language descriptions.",
  "y": "similarities"
 },
 {
  "id": "58b423c4ea2a3530d0c469fc0f5528_3",
  "x": "In the generate step, we generate regular expression representations from a small manually-crafted grammar (Table 1) . Our grammar includes 15 nonterminal derivations and 6 terminals and of both basic and high-level operations. We identify these via frequency analysis of smaller datasets from previous work<cite> (Kushman and Barzilay, 2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "58b423c4ea2a3530d0c469fc0f5528_4",
  "x": "**EXPERIMENTS** Datasets We split the 10,000 regexp and description pairs in NL-RX into 65% train, 10% dev, and 25% test sets. In addition, we also evaluate our model on the dataset used by <cite>Kushman and Barzilay (2013)</cite> (KB13), although it contains far fewer data points (824).",
  "y": "uses"
 },
 {
  "id": "58b423c4ea2a3530d0c469fc0f5528_5",
  "x": "We report DFA-Equal accuracy as our model's evaluation metric, using <cite>Kushman and Barzilay (2013)</cite> 's implementation to directly compare our results. ---------------------------------- **EVALUATION METRIC**",
  "y": "uses"
 },
 {
  "id": "58b423c4ea2a3530d0c469fc0f5528_6",
  "x": "For a given test example, it finds the closest cosinesimilar neighbor from the training set and uses the regexp from that example for its prediction. Semantic-Unify: Our second baseline, SemanticUnify, is the previous state-of-the-art model from<cite> (Kushman and Barzilay, 2013)</cite> , explained above. 2",
  "y": "uses"
 },
 {
  "id": "591e2873606d6171e48fd34a731fc7_0",
  "x": "Previous work includes three different approaches to discretizing continuous values into location labels (see also Section 2): 1.) Geodesic grids are the most straightforward, but do not \"lead to a natural representation of the administrative, population-based or language boundaries in the region\"<cite> (Han et al., 2012)</cite> . 2.) Clustering coordinates prevents the identification of (nearly) empty locations and keeps points which are geographically close together in one location.",
  "y": "background motivation"
 },
 {
  "id": "591e2873606d6171e48fd34a731fc7_1",
  "x": "Previous work includes three different approaches to discretizing continuous values into location labels (see also Section 2): 1.) Geodesic grids are the most straightforward, but do not \"lead to a natural representation of the administrative, population-based or language boundaries in the region\"<cite> (Han et al., 2012)</cite> . 2.) Clustering coordinates prevents the identification of (nearly) empty locations and keeps points which are geographically close together in one location.",
  "y": "background"
 },
 {
  "id": "591e2873606d6171e48fd34a731fc7_2",
  "x": "3.) Predefined administrative regions, like cities, can provide homogeneous interpretable areas. However, mapping coordinates to the closest city can be ambiguous. Previous work typically considered cities with a population of at least 100K <cite>(Han et al., 2012</cite> (Han et al., , 2014 .",
  "y": "background"
 },
 {
  "id": "591e2873606d6171e48fd34a731fc7_3",
  "x": "Previous work typically considered cities with a population of at least 100K <cite>(Han et al., 2012</cite> (Han et al., , 2014 . This approach has the opposite problem of clustering: different linguistic areas might be contained within a single administrative region. Here, we propose Point-To-City (P2C), a new method mapping continuous coordinates to locations.",
  "y": "motivation"
 },
 {
  "id": "591e2873606d6171e48fd34a731fc7_4",
  "x": "Rahimi et al. (2018) proposes a Graph-Convolutional neural network, though the text features are represented by a bag-of-words, while we rely on word embeddings. The ability of the labels to reflect real anthropological areas, however, affects primarily the models which rely on linguistic data. This is the case of the studies of<cite> Han et al. (2012)</cite> and Han et al. (2014) who based their predictions on the so-called Location-Indicative Words (LIW).",
  "y": "motivation"
 },
 {
  "id": "591e2873606d6171e48fd34a731fc7_5",
  "x": "---------------------------------- **METHODS** Data sets We apply our method to two widely used data sets for geolocation: TWITTER-US (Roller et al., 2012) , and TWITTER-WORLD<cite> (Han et al., 2012)</cite> .",
  "y": "uses"
 },
 {
  "id": "591e2873606d6171e48fd34a731fc7_6",
  "x": "1. for each point, apply k-d trees to find clusters of points where each pair has a reciprocal distance less than d; 5. assign the points which fall into more than one cluster to the one with the nearest centroid; 6. substitute each instance's coordinates with the centroid coordinates of the corresponding cluster. The algorithm converges when the final number of points cannot be further reduced, since they all are farther apart from each other than the maximum distance d. After assigning each instance its new coordinates, we follow<cite> Han et al. (2012</cite> Han et al. ( , 2014 in using the GeoNames data set to associate clusters with cities, by substituting the instance coordinates with those of the closest town center.",
  "y": "uses"
 },
 {
  "id": "591e2873606d6171e48fd34a731fc7_7",
  "x": "The benefit is in terms of noise reduction, for the selective removal of geographically ambiguous words, and computational affordability. Following<cite> Han et al. (2012)</cite> , we further filter the vocabulary via Information Gain Ratio (IGR), selecting the terms with the highest values until we reach a computationally feasible vocabulary size: here, 750K and 460K for TWITTER-US and TWITTER-WORLD. Attention-based CNN For classification, we use an attention-based convolutional neural model.",
  "y": "uses"
 },
 {
  "id": "59b6eaca400342159b867d018d4042_0",
  "x": "For example, a set of facts like sideEffect(meloxicam, stomachBleeding), interacts-With(meloxicam, ibuprofen), etc are matched against a corpus, and the matching sentences are then used to generate training data consisting of labeled relation mentions. Distant supervision is less expensive to obtain than directly supervised labels, but produces noisy training data whenever matching errors occur. Hence distant supervision is often coupled with learning methods that allow for this sort of noise, e.g., by introducing latent variables for each entity mention<cite> (Hoffmann et al., 2011</cite>; Riedel et al., 2010; Surdeanu et al., 2012) ; by carefully selecting the entity mentions from contexts likely to include specific KB facts (Wu and Weld, 2010) ; or by careful filtering of the KB strings used as seeds (Movshovitz-Attias and Cohen, 2012) .",
  "y": "background"
 },
 {
  "id": "59b6eaca400342159b867d018d4042_1",
  "x": "Hence distant supervision is often coupled with learning methods that allow for this sort of noise, e.g., by introducing latent variables for each entity mention<cite> (Hoffmann et al., 2011</cite>; Riedel et al., 2010; Surdeanu et al., 2012) ; by carefully selecting the entity mentions from contexts likely to include specific KB facts (Wu and Weld, 2010) ; or by careful filtering of the KB strings used as seeds (Movshovitz-Attias and Cohen, 2012) . Another recently-introduced approach to reducing the noise in distant supervision is to combine distant labeling with label propagation (LP) (Bing et al., 2015;<cite> Bing et al., 2016)</cite> . Label propagation is a family of graph-based semi-supervised learning (SSL) methods in which instances that are \"nearby\" in the graph are encouraged to have similar labels.",
  "y": "background"
 },
 {
  "id": "59b6eaca400342159b867d018d4042_2",
  "x": "Label propagation is a family of graph-based semi-supervised learning (SSL) methods in which instances that are \"nearby\" in the graph are encouraged to have similar labels. Depending on the LP method used, agreement with seed labels can be imposed as a hard constraint (Zhu et al., 2003) or a soft constraint<cite> (Lin and Cohen, 2010</cite>; Talukdar and Cohen, 2014) . When seed-label agreement is a soft constraint, then LP can be viewed as a way of smoothing the seed labels, so that labels for groups of \"similar\" instances (i.e., instances nearby in the graph) are upweighted if they agree, and downweighted if they disagree.",
  "y": "background"
 },
 {
  "id": "59b6eaca400342159b867d018d4042_3",
  "x": "This approach was shown to improve performance in recognizing instances of certain medical noun-phrase (NP) categories, such as drug names and disease names. An extension of this approach <cite>(Bing et al., 2016)</cite> learned to classify NP pairs as relations, using a more complex graph structure. Figure 1 : A structured document in WebMD describing the drug meloxicam.",
  "y": "background"
 },
 {
  "id": "59b6eaca400342159b867d018d4042_4",
  "x": "In <cite>(Bing et al., 2016)</cite> , relation extraction was performed on an \"entity centric\" corpus, where each document is primarily concerned with a particular \"title entity\", and the first argument of each relation is always the title entity: hence relation extraction can be viewed as classification, where an entity mention is labeled with its slot filling role, i.e., its relation to the title entity. The intuition behind combining concept extraction and relation extraction is that relation arguments are often constrained to be of a particular type; for example, the sideEffect of a drug is necessarily of the type symptom. The second contribution is a novel use of document structure; in particular, we exploit the fact that in some small, well-structured corpora, sections can be identified that correspond fairly accurately to relation arguments.",
  "y": "background"
 },
 {
  "id": "59b6eaca400342159b867d018d4042_5",
  "x": "We use an existing multi-class label propagation method, namely, MultiRankWalk (MRW)<cite> (Lin and Cohen, 2010)</cite> , which is a graph-based SSL method related to personalized PageRank (PPR) (Haveliwala et al., 2003) (aka random walk with restart (Tong et al., 2006) ). MRW can be viewed simply as computing a personalized PageRank vector for each class, each of which is computed using a personalization vector that is initially uniform over the seeds, and finally assigning to each node the class associ-ated with its highest-scoring vector. MRW's final scores depend on the centrality of nodes, as well as their proximity to seeds.",
  "y": "uses"
 },
 {
  "id": "59b6eaca400342159b867d018d4042_6",
  "x": "For lists, the dependency features are computed relative to the head of the list. We use SVMs (Chang and<cite> Lin, 2001)</cite> and discard singleton features, as well as the most frequent 5% of features (as a stop-wording variant). Specifically, binary classifiers are trained with examples of one relation as the positives, and examples of the other classes as negatives.",
  "y": "uses"
 },
 {
  "id": "59b6eaca400342159b867d018d4042_7",
  "x": "Our evaluation dataset contains 20 manually labeled pages, 10 pages each from the disease corpus WikiDisease and the drug corpus DailyMed. This data was originally generated in <cite>(Bing et al., 2016)</cite> . The annotated text fragments are manually chunked NPs which are the second argument values of any of the eight relations considered here, with the title drug or disease entity of the corresponding document as the relation subject.",
  "y": "uses"
 },
 {
  "id": "59b6eaca400342159b867d018d4042_8",
  "x": "We also compare against two latent variable learners. The first is MultiR<cite> (Hoffmann et al., 2011</cite>) which models each relation mention separately and aggregates their labels using a deterministic OR. The second one is MIML- RE (Surdeanu et al., 2012) which has a similar structure to MultiR, but uses a classifier to aggregate the mention level predictions into an entity pair prediction.",
  "y": "uses"
 },
 {
  "id": "59b6eaca400342159b867d018d4042_9",
  "x": "We found that the performance of these methods varies significantly with the number of negative examples used during training, and hence we tuned these and other parameters 4 directly on the evaluation data, and report their best performance. Another distantsupervision baseline we compare to is the Mintz++ model from (Surdeanu et al., 2012) , which improves on the original model from (Mintz et al., 2009 ) by training multiple classifiers, and allowing multiple labels per entity pair. We also compare with DIEBOLDS <cite>(Bing et al., 2016)</cite> , which uses LP on a graph containing entity mention pairs.",
  "y": "uses"
 },
 {
  "id": "59b6eaca400342159b867d018d4042_10",
  "x": "**RESULTS ON LABELED PAGES** The results for precision, recall and F1 measure are given in Table 1 . The results for DIEBOLDS are from <cite>(Bing et al., 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "59b6eaca400342159b867d018d4042_11",
  "x": "**RELATED WORK** To overcome the noise in distantly-labeled examples, (Riedel et al., 2010) introduced an \"at least one\" heuristic, where instead of taking all mentions for a pair as correct examples only at least one of them is assumed to express that relation. MultiR<cite> (Hoffmann et al., 2011) and</cite> MIML-RE (Surdeanu et al., 2012) extend this approach to support multi- ple relations expressed by different sentences in a bag.",
  "y": "background"
 },
 {
  "id": "59b6eaca400342159b867d018d4042_12",
  "x": "To overcome the noise in distantly-labeled examples, (Riedel et al., 2010) introduced an \"at least one\" heuristic, where instead of taking all mentions for a pair as correct examples only at least one of them is assumed to express that relation. MultiR<cite> (Hoffmann et al., 2011) and</cite> MIML-RE (Surdeanu et al., 2012) extend this approach to support multi- ple relations expressed by different sentences in a bag. Unlike these approaches, DIEJOB improves the quality of training data with a bootstrapping step before feeding the noisy examples into a learner, by using the confident examples from a structured corpus as seeds.",
  "y": "background"
 },
 {
  "id": "59b6eaca400342159b867d018d4042_13",
  "x": "First, it distills the distantly-labeled examples by propagating labels from good seed examples, and downweights the noisy ones. Second, the propagation will walk to more relation examples in the concept mention set that cannot be distantly labeled with triples from knowledge bases. Document structure was previously explored by <cite>(Bing et al., 2016)</cite> , which used the structure to enrich an LP graph by adding coupling edges between mentions in the same section of particular documents.",
  "y": "background"
 },
 {
  "id": "59b6eaca400342159b867d018d4042_14",
  "x": "Accordingly, their propagation seeds are different: DIEJOB uses confident examples as seeds (labeled from particular sections of a structured cor-pus) to propagate labels to more examples via feature similarity, while DIEBOLDS directly uses Freebase triples as seeds and propagates labels through edges built from coordinate lists and sections. In the classic bootstrap learning scheme (Riloff and Jones, 1999; Agichtein and Gravano, 2000;<cite> Bunescu and Mooney, 2007)</cite> , a small number of seed instances are used to extract new patterns from a large corpus, which are then used to extract more instances. Then in an iterative fashion, new instances are used to extract more patterns.",
  "y": "background"
 },
 {
  "id": "5a039a2af7e07cffff76d3470f32f1_0",
  "x": "Many techniques have been proposed to learn such embeddings (Pennington et al., 2014;<cite> Mikolov et al., 2013</cite>; Mnih and Kavukcuoglu, 2013) . While most of the work has focused on English word embeddings, few attempts have been carried out to learn word embeddings for other languages, mostly using the above mentioned techniques. In this paper, we focus on Arabic word embeddings.",
  "y": "background"
 },
 {
  "id": "5a039a2af7e07cffff76d3470f32f1_1",
  "x": "**RELATED WORK** There is a wealth of research on evaluating unsupervised word embeddings, which can be can be broadly divided into intrinsic and extrinsic evalu- <cite>(Mikolov et al., 2013</cite>; Gao et al., 2014; Schnabel et al., 2015) . Extrinsic evaluations assess the quality of the embeddings as features in models for other tasks, such as semantic role labeling and part-of-speech tagging (Collobert et al., 2011) , or noun-phrase chunking and sentiment analysis (Schnabel et al., 2015) .",
  "y": "background"
 },
 {
  "id": "5a039a2af7e07cffff76d3470f32f1_2",
  "x": "To the best of our knowledge, only a handful of recent studies attempted evaluating Arabic word embeddings. Zahran et al. (Zahran et al., 2015) translated the English benchmark in<cite> (Mikolov et al., 2013)</cite> and used it to evaluate different embedding techniques when applied on a large Arabic corpus. However, as the authors themselves point out, translating an English benchmark is not the best strategy to evaluate Arabic embeddings.",
  "y": "background"
 },
 {
  "id": "5a039a2af7e07cffff76d3470f32f1_3",
  "x": "Once tuples have been generated, they can be used as word analogy questions to evaluate different word embeddings as defined by Mikolov et al.<cite> (Mikolov et al., 2013)</cite> . A word analogy question for a tuple consisting of two word pairs (a, b) and (c, d) can be formulated as follows: \"a to b is like c to ?\". Each such question will then be answered by calculating a target vector t = b \u2212 a + c. We then calculate the cosine similarity between the target vector t and the vector representation of each word w in a given word embeddings V .",
  "y": "uses"
 },
 {
  "id": "5a039a2af7e07cffff76d3470f32f1_4",
  "x": "This provides a more accurate representation of a relation as mentioned in<cite> (Mikolov et al., 2013)</cite> . For each relation, we generate a question per word pair consisting of the word pair plus 10 random word pairs from the same relation. Thus, each question would consist of 11 word pairs (a i , b i ) where 1 \u2264 i \u2264 11.",
  "y": "similarities"
 },
 {
  "id": "5a039a2af7e07cffff76d3470f32f1_5",
  "x": "The first three are based on a large corpus of Arabic documents constructed by Zahran et al. (Zahran et al., 2015) , which consists of 2,340,895 words. Using this corpus, the authors generated three different word embeddings using three different techniques, namely the Continuous Bagof-Words (CBOW) model<cite> (Mikolov et al., 2013)</cite> , the Skip-gram model<cite> (Mikolov et al., 2013)</cite> and GloVe (Pennington et al., 2014) . The fourth word embeddings we evaluate in this paper is the Arabic part of the Polyglot word embeddings, which was trained on the Arabic Wikipedia by Al-Rfou et al and consists of over 100,000 words (Al-Rfou et al., 2013) .",
  "y": "uses"
 },
 {
  "id": "5a2cd80d7c57e06a51457e53169b49_0",
  "x": "Recent research has attempted to induce unsupervised bilingual lexicons by aligning monolingual word vector spaces (Zhang et al., 2017a;<cite> Conneau et al., 2018</cite>; Aldarmaki et al., 2018; Artetxe et al., 2018a; Alvarez-Melis and Jaakkola, 2018; Mukherjee et al., 2018) . Given a pair of languages, their word alignment is inherently a bi-directional problem (e.g. EnglishItalian vs Italian-English). However, most existing research considers mapping from one language to another without making use of symmetry.",
  "y": "background motivation"
 },
 {
  "id": "5a2cd80d7c57e06a51457e53169b49_1",
  "x": "As shown in Figure 1a , when the model of<cite> Conneau et al. (2018)</cite> is applied to English and Italian, the primal model maps the word \"three\" to the Italian word \"tre\", but the dual model maps \"tre\" to \"two\" instead of \"three\". We propose to address this issue by exploiting duality, encouraging forward and backward mappings to form a closed loop (Figure 1b ). In particular, we extend the model of<cite> Conneau et al. (2018)</cite> by using a cycle consistency loss (Zhou et al., 2016) to regularize two models in opposite directions.",
  "y": "background motivation"
 },
 {
  "id": "5a2cd80d7c57e06a51457e53169b49_2",
  "x": "In particular, we extend the model of<cite> Conneau et al. (2018)</cite> by using a cycle consistency loss (Zhou et al., 2016) to regularize two models in opposite directions. Our model significantly outperforms competitive baselines, obtaining the best published results.",
  "y": "motivation differences similarities"
 },
 {
  "id": "5a2cd80d7c57e06a51457e53169b49_3",
  "x": "**RELATED WORK** UBLI. A typical line of work uses adversarial training (Miceli Barone, 2016; Zhang et al., 2017a,b;<cite> Conneau et al., 2018)</cite> , matching the distributions of source and target word embeddings through generative adversarial networks (Goodfellow et al., 2014) .",
  "y": "background"
 },
 {
  "id": "5a2cd80d7c57e06a51457e53169b49_4",
  "x": "In this paper, we choose<cite> Conneau et al. (2018)</cite> as our baseline as it is theoretically attractive and gives strong results on large-scale datasets. Cycle Consistency. Forward-backward consistency has been used to discover the correspondence between unpaired images (Zhu et al., 2017; Kim et al., 2017) .",
  "y": "similarities"
 },
 {
  "id": "5a2cd80d7c57e06a51457e53169b49_5",
  "x": "We take<cite> Conneau et al. (2018)</cite> as our baseline, introducing a novel regularizer to enforce cycle consistency. Let X = {x 1 , ..., x n } and Y = {y 1 , ..., y m } be two sets of n and m word embeddings for a source and a target language, respectively. The primal UBLI task aims to learn a linear mapping F : X \u2192 Y such that for each x i , F(x i ) corresponds to its translation in Y . Similarly, a linear mapping G : Y \u2192 X is defined for the dual task. In addition, we introduce two language discriminators D x and D y , which are trained to discriminate between the mapped word embeddings and the original word embeddings.",
  "y": "extends uses"
 },
 {
  "id": "5a2cd80d7c57e06a51457e53169b49_6",
  "x": [
   "In addition, we introduce two language discriminators D x and D y , which are trained to discriminate between the mapped word embeddings and the original word embeddings. Conneau et al. (2018) align two word embedding spaces through generative adversarial networks, in which two networks are trained simultaneously. Specifically, take the primal UBLI task as an example, the linear mapping F tries to generate \"fake\" word embeddings F(x) that look similar to word embeddings from Y , while the discriminator D y aims to distinguish between \"fake\" and real word embeddings from Y ."
  ],
  "y": "background"
 },
 {
  "id": "5a2cd80d7c57e06a51457e53169b49_7",
  "x": "**MODEL SELECTION** We follow<cite> Conneau et al. (2018)</cite> , using an unsupervised criterion to perform model selection. In preliminary experiments, we find in adversarial training that the single-direction criterion S(F, X, Y ) by<cite> Conneau et al. (2018)</cite> does not always work well.",
  "y": "uses"
 },
 {
  "id": "5a2cd80d7c57e06a51457e53169b49_8",
  "x": "In preliminary experiments, we find in adversarial training that the single-direction criterion S(F, X, Y ) by<cite> Conneau et al. (2018)</cite> does not always work well. To address this, we make a simple extension by calculating the weighted average of forward and backward scores: Where \u03bb is a hyperparameter to control the importance of the two objectives. 1 Here S first generates bilingual lexicons by learned mappings, and then computes the average cosine similarity of these translations.",
  "y": "motivation extends"
 },
 {
  "id": "5a2cd80d7c57e06a51457e53169b49_9",
  "x": "Our datasets includes: (i) The Multilingual Unsupervised and Supervised Embeddings (MUSE) dataset released by<cite> Conneau et al. (2018)</cite> . (ii) the more challenging Vecmap dataset from Dinu et al. (2015) and the extensions of Artetxe et al. (2017) . We follow the evaluation setups of<cite> Conneau et al. (2018)</cite> , utilizing cross-domain similarity local scaling (CSLS) for retrieving the translation of given source words.",
  "y": "uses"
 },
 {
  "id": "5a2cd80d7c57e06a51457e53169b49_10",
  "x": "(ii) the more challenging Vecmap dataset from Dinu et al. (2015) and the extensions of Artetxe et al. (2017) . We follow the evaluation setups of<cite> Conneau et al. (2018)</cite> , utilizing cross-domain similarity local scaling (CSLS) for retrieving the translation of given source words. Following a standard evaluation practice (Vuli\u0107 and Moens, 2013; Mikolov et al., 2013;<cite> Conneau et al., 2018)</cite> , we report precision at 1 scores (P@1).",
  "y": "uses"
 },
 {
  "id": "5a2cd80d7c57e06a51457e53169b49_11",
  "x": "(ii) the more challenging Vecmap dataset from Dinu et al. (2015) and the extensions of Artetxe et al. (2017) . We follow the evaluation setups of<cite> Conneau et al. (2018)</cite> , utilizing cross-domain similarity local scaling (CSLS) for retrieving the translation of given source words. Following a standard evaluation practice (Vuli\u0107 and Moens, 2013; Mikolov et al., 2013;<cite> Conneau et al., 2018)</cite> , we report precision at 1 scores (P@1).",
  "y": "uses"
 },
 {
  "id": "5a2cd80d7c57e06a51457e53169b49_13",
  "x": "**COMPARISON WITH THE STATE-OF-THE-ART** In this section, we compare our model with state-of-the-art systems, including those with different degrees of supervision. The baselines include: (1) Procrustes<cite> (Conneau et al., 2018)</cite> , which learns a linear mapping through Procrustes Analysis (Sch\u00f6nemann, 1966) .",
  "y": "background"
 },
 {
  "id": "5a2cd80d7c57e06a51457e53169b49_14",
  "x": "(3) GeoMM (Jawanpuria et al., 2018), a geometric approach which learn a Mahalanobis metric to refine the notion of similarity. (4) GeoMM semi , iterative GeoMM with weak supervision. (5) Adv-C-Procrustes<cite> (Conneau et al., 2018)</cite> , which refines the mapping learned by Adv-C with iterative Procrustes, which learns the new mapping matrix by constructing a bilingual lexicon iteratively. (6) Unsup-SL (Artetxe et al., 2018a) , which integrates a weak unsupervised mapping with a robust selflearning.",
  "y": "background"
 },
 {
  "id": "5ad1e8b75cc6f5b627f770cced8e0f_0",
  "x": "Although these methods generally show good results, they typically allow no manual inspection of why a specific judgment is made and are thus ill-suited for applications in the humanities. Approaches Based on Machine Translation (MT) Evaluation Metrics: <cite>Madnani et al. (2012)</cite> conduct a study on the usefulness of automated MT evaluation metrics (e.g., BLEU, NIST and Meteor) for the task of paraphrase identification. They train an ensemble of different classifiers using scores of MT metrics as features.",
  "y": "background"
 },
 {
  "id": "5ad1e8b75cc6f5b627f770cced8e0f_1",
  "x": "From the Gold corpus, also the source text (numbered in the repository with 1, see <cite>Madnani et al. (2012)</cite> ) serves as reference, and the paraphrastic reuse of it (numbered with 2), provides the system output. Reference Methods: Often, machine translation metrics are based on simple edit distance measures. Unlike simple word error rate (WER; Su et al. (1992) ), which depends on a strict word order, the positionindependent error rate (PER; Tillmann et al. (1997) ) uses a bag-of-words approach.",
  "y": "uses background"
 },
 {
  "id": "5ad1e8b75cc6f5b627f770cced8e0f_2",
  "x": "Similar to <cite>Madnani et al. (2012)</cite> we use these MT scores separately in a classification task to predict paraphrasticality where the respective MT score is fed into a MaxEnt classifier as only feature. Table 4 : Accuracy in solving our three tasks. Detecting Paraphrases (RQ1): Using the relative operation count from the alignment as features in a classification task, we determine the classification accuracy of our approach on the gold corpus.",
  "y": "similarities"
 },
 {
  "id": "5aeb64701a6b7d9878ea5e14a87b4e_1",
  "x": "Some are \"lexical sample\" datasets, that is, only occurrences of some selected lemmas are annotated (McCarthy and Navigli, 2009; Biemann, 2013) , and some are \"all-words\", providing substitutes for all content words in the given sentences (Sinha and Mihalcea, 2014;<cite> Kremer et al., 2014)</cite> . In addition, there is a cross-lingual lexical substitution dataset (McCarthy et al., 2013) , where annotators provided Spanish substitutes for English target words in English sentence context. Lexical substitution is a method for characterizing word meaning in context that has several attractive properties.",
  "y": "background"
 },
 {
  "id": "5aeb64701a6b7d9878ea5e14a87b4e_2",
  "x": "Lexical substitution makes it possible to describe word meaning without having to rely on any particular dictionary. In addi- Table 2 : Analysis of lexical substitution data: Relation of the substitute to the target, in percentages by part of speech (from Kremer et al. (2014)) tion, providing substitutes is a task that seems to be well doable by untrained annotators: Both Biemann (2013) and our recent annotation<cite> (Kremer et al., 2014)</cite> used crowdsourcing to collect the substitutes. 1",
  "y": "background"
 },
 {
  "id": "5aeb64701a6b7d9878ea5e14a87b4e_3",
  "x": "---------------------------------- **ANALYZING LEXICAL SUBSTITUTES** In a recent lexical substitution annotation effort<cite> (Kremer et al., 2014)</cite> , we collected lexical substitution annotation for all nouns, verbs, and adjectives in a mixed news and fiction corpus, using untrained annotators via crowdsourcing.",
  "y": "background"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_0",
  "x": "Zhang et al. (2017a,b) first reported encouraging results with adversarial training. <cite>Conneau et al. (2018)</cite> improved this approach with <cite>post-mapping refinements</cite>, showing impressive results for several language pairs. <cite>Their</cite> learned mapping was then successfully used to train a fully unsupervised neural machine translation system (Lample et al., 2018a,b) .",
  "y": "background"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_1",
  "x": "Zhang et al. (2017a,b) first reported encouraging results with adversarial training. <cite>Conneau et al. (2018)</cite> improved this approach with <cite>post-mapping refinements</cite>, showing impressive results for several language pairs. <cite>Their</cite> learned mapping was then successfully used to train a fully unsupervised neural machine translation system (Lample et al., 2018a,b) .",
  "y": "background"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_2",
  "x": "Although successful, adversarial training has been criticized for not being stable and failing to converge, inspiring researchers to propose nonadversarial methods more recently (Xu et al., 2018a; Hoshen and Wolf, 2018; Alvarez-Melis and Jaakkola, 2018; Artetxe et al., 2018b) . In particular, Artetxe et al. (2018b) show that the adversarial methods of <cite>Conneau et al. (2018)</cite> and Zhang et al. (2017a,b) fail for many language pairs. In this paper, we revisit adversarial training and propose a number of key improvements that yield more robust training and improved mappings.",
  "y": "background motivation"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_3",
  "x": "Although successful, adversarial training has been criticized for not being stable and failing to converge, inspiring researchers to propose nonadversarial methods more recently (Xu et al., 2018a; Hoshen and Wolf, 2018; Alvarez-Melis and Jaakkola, 2018; Artetxe et al., 2018b) . In particular, Artetxe et al. (2018b) show that the adversarial methods of <cite>Conneau et al. (2018)</cite> and Zhang et al. (2017a,b) fail for many language pairs. In this paper, we revisit adversarial training and propose a number of key improvements that yield more robust training and improved mappings.",
  "y": "motivation extends uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_4",
  "x": "In particular, Artetxe et al. (2018b) show that the adversarial methods of <cite>Conneau et al. (2018)</cite> and Zhang et al. (2017a,b) fail for many language pairs. In this paper, we revisit adversarial training and propose a number of key improvements that yield more robust training and improved mappings. Our main idea is to learn the cross-lingual mapping in a projected latent space and add more constraints to guide the unsupervised mapping in this space.",
  "y": "extends"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_5",
  "x": "Although successful, adversarial training has been criticized for not being stable and failing to converge, inspiring researchers to propose nonadversarial methods more recently (Xu et al., 2018a; Hoshen and Wolf, 2018; Alvarez-Melis and Jaakkola, 2018; Artetxe et al., 2018b) . In particular, Artetxe et al. (2018b) show that the adversarial methods of <cite>Conneau et al. (2018)</cite> and Zhang et al. (2017a,b) fail for many language pairs. In this paper, we revisit adversarial training and propose a number of key improvements that yield more robust training and improved mappings.",
  "y": "background motivation"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_6",
  "x": "This grounding step forces the model to retain word semantics during the mapping process. We conduct a series of experiments with six different language pairs (in both directions) comprising European, non-European, and low-resource languages from two different datasets. Our results show that our model is more robust and yields significant gains over <cite>Conneau et al. (2018)</cite> for all translation tasks in all evaluation measures.",
  "y": "differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_7",
  "x": "In the first step, they use the dictionary (starting with the seed) to learn a linear mapping, which is then used in the second step to induce a new dictionary. A more recent line of research attempts to eliminate the seed dictionary totally and learn the map-ping in a purely unsupervised way. This was first proposed by Miceli Barone (2016) , who initially used an adversarial network similar to <cite>Conneau et al. (2018)</cite> , and found that the mapper (which is also the encoder) translates everything to a single embedding, known commonly as the mode collapse issue (Goodfellow, 2017) .",
  "y": "background"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_8",
  "x": "For selecting the best model, they rely on sharp drops of the discriminator accuracy. In their follow-up work (Zhang et al., 2017b) , they minimize Earth-Mover's distance between the distribution of the transformed source embeddings and the distribution of the target embeddings. <cite>Conneau et al. (2018)</cite> show impressive results with adversarial training and refinement with the Procrustes solution.",
  "y": "background"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_9",
  "x": "<cite>Conneau et al. (2018)</cite> show impressive results with adversarial training and refinement with the Procrustes solution. However, while <cite>all these methods</cite> learn the mapping in the original embedding space, our approach learns it in the latent code space considering both the mapper and the target encoder as adversary.",
  "y": "differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_10",
  "x": "Let X = {x 1 , . . . , x n } and Y = {y 1 , . . . , y m } be two sets consisting of n and m word embeddings of d-dimensions for a source and a target language, respectively. We assume that X and Y are trained independently from monolingual corpora. Our aim is to learn a mapping f (x) in an unsupervised way (i.e., no bilingual dictionary given) such that for every x i , f (x) corresponds to its translation in Y. Our overall approach follows the same sequence of steps as <cite>Conneau et al. (2018)</cite>: 1.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_11",
  "x": "We use adversarial training to find a mapping between q(z x |x) and q(z y |y). This is in contrast with <cite>most existing methods</cite> (e.g., <cite>Conneau et al. (2018)</cite> ; Artetxe et al. (2017) ) that directly map the distribution of the source word embeddings p(x) to the distribution of the target p(y). As S\u00f8gaard et al. (2018) pointed out, the isomorphism does not hold in general between the word embedding spaces of two languages.",
  "y": "differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_12",
  "x": "We use adversarial training to find a mapping between q(z x |x) and q(z y |y). This is in contrast with <cite>most existing methods</cite> (e.g., <cite>Conneau et al. (2018)</cite> ; Artetxe et al. (2017) ) that directly map the distribution of the source word embeddings p(x) to the distribution of the target p(y). As S\u00f8gaard et al. (2018) pointed out, the isomorphism does not hold in general between the word embedding spaces of two languages.",
  "y": "differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_13",
  "x": "Our discriminators have the same architecture as <cite>Conneau et al. (2018)</cite> . <cite>It is</cite> a feed-forward network with two hidden layers of size 2048 and Leaky-ReLU activations. We apply dropout with a rate of 0.1 on the input to the discriminators.",
  "y": "uses similarities"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_14",
  "x": "where P L X (src|z) is the probability according to L X to distinguish whether z is coming from the source encoder (src = 1) or from the target-tosource mapper Our discriminators have the same architecture as <cite>Conneau et al. (2018)</cite> . <cite>It is</cite> a feed-forward network with two hidden layers of size 2048 and Leaky-ReLU activations.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_15",
  "x": "We also apply the orthogonalization update to the mappers following <cite>Conneau et al. (2018)</cite> with \u03b2 = 0.01. Our training setting is similar to <cite>Conneau et al. (2018)</cite> , and we apply the same pre-and postprocessing steps. We use stochastic gradient descent (SGD) with a batch size of 32, a learning rate of 0.1, and a decay of 0.98.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_16",
  "x": "The autoencoders (encoder-decoder) in this stage get updated only on the post-cycle reconstruction loss. We also apply the orthogonalization update to the mappers following <cite>Conneau et al. (2018)</cite> with \u03b2 = 0.01. Our training setting is similar to <cite>Conneau et al. (2018)</cite> , and we apply the same pre-and postprocessing steps.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_17",
  "x": "For selecting the best model, we use the unsupervised validation criterion proposed by <cite>Conneau et al. (2018)</cite> , which correlates highly with the mapping quality. <cite>In this criterion</cite>, 10, 000 most frequent source words along with their nearest neighbors in the target space are considered. The average cosine similarity between these pseudo translations is considered as the validation metric.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_18",
  "x": "For selecting the best model, we use the unsupervised validation criterion proposed by <cite>Conneau et al. (2018)</cite> , which correlates highly with the mapping quality. <cite>In this criterion</cite>, 10, 000 most frequent source words along with their nearest neighbors in the target space are considered. The average cosine similarity between these pseudo translations is considered as the validation metric.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_19",
  "x": "The average cosine similarity between these pseudo translations is considered as the validation metric. The initial bilingual dictionary induced by adversarial training (or any other unsupervised method) is generally of lower quality than what could be achieved by a supervised method. <cite>Conneau et al. (2018)</cite> and Artetxe et al. (2018b) propose fine-tuning methods to refine the initial mappings.",
  "y": "background"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_20",
  "x": "<cite>Conneau et al. (2018)</cite> and Artetxe et al. (2018b) propose fine-tuning methods to refine the initial mappings. Similar to <cite>Conneau et al. (2018)</cite> ), we finetune our initial mappings (G and F ) by iteratively solving the Procrustes problem and applying a dictionary induction step. <cite>This method</cite> uses singular value decomposition or SVD of Z T y Z x to find the optimal mappings G (similarly SVD(Z T x Z y ) for F ) given the approximate alignment of words from the previous step.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_21",
  "x": "The initial bilingual dictionary induced by adversarial training (or any other unsupervised method) is generally of lower quality than what could be achieved by a supervised method. <cite>Conneau et al. (2018)</cite> and Artetxe et al. (2018b) propose fine-tuning methods to refine the initial mappings. Similar to <cite>Conneau et al. (2018)</cite> ), we finetune our initial mappings (G and F ) by iteratively solving the Procrustes problem and applying a dictionary induction step.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_22",
  "x": "Similar to <cite>Conneau et al. (2018)</cite> ), we finetune our initial mappings (G and F ) by iteratively solving the Procrustes problem and applying a dictionary induction step. <cite>This method</cite> uses singular value decomposition or SVD of Z T y Z x to find the optimal mappings G (similarly SVD(Z T x Z y ) for F ) given the approximate alignment of words from the previous step. For generating synthetic dictionary in each iteration, we only consider the translation pairs that are mutual nearest neighbors.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_23",
  "x": "In our fine-tuning, we run five iterations of this process. For finding the nearest neighbors, we use the Cross-domain Similarity Local Scaling (CSLS) which works better in mitigating the hubness problem (<cite>Conneau et al., 2018</cite>) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_24",
  "x": "**DATASETS** We evaluate our model on two different datasets. The first one is from <cite>Conneau et al. (2018)</cite> , <cite>which</cite> consists of FastText monolingual embeddings of (d =) 300 dimensions (Bojanowski et al., 2017) trained on Wikipedia monolingual corpus and gold dictionaries for 110 language pairs.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_25",
  "x": "---------------------------------- **BASELINES** We compare our method with the unsupervised models of <cite>Conneau et al. (2018)</cite> , Artetxe et al. (2018b) , Alvarez-Melis and Jaakkola (2018) , Xu et al. (2018a) , and Hoshen and Wolf (2018) .",
  "y": "similarities differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_26",
  "x": "To evaluate how our unsupervised method compares with methods that rely on a bilingual seed dictionary, we follow <cite>Conneau et al. (2018)</cite> , and compute a supervised baseline that uses the Procrustes solution directly on the seed dictionary (5000 pairs) to learn the mapping function, and then uses CSLS to do the nearest neighbor search. We also compare with the supervised approaches of Artetxe et al. (2017 Artetxe et al. ( , 2018a , which to our knowledge are the state-of-the-art supervised systems. For some of the <cite>baselines</cite>, results are reported from their papers, while for the rest we report results by running the publicly available codes on our machine.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_27",
  "x": "We compare our method with the unsupervised models of <cite>Conneau et al. (2018)</cite> , Artetxe et al. (2018b) , Alvarez-Melis and Jaakkola (2018) , Xu et al. (2018a) , and Hoshen and Wolf (2018) . For some of the <cite>baselines</cite>, results are reported from their papers, while for the rest we report results by running the publicly available codes on our machine.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_28",
  "x": "We present our results on European languages on the datasets of <cite>Conneau et al. (2018)</cite> and Dinu et al. (2015) in Tables 1 and 3 , while the results on non-European languages are shown in Table 2 . Through experiments, our goal is to assess: 1. Does the unsupervised mapping method based on our proposed adversarial autoencoder model improve over the best existing adversarial method of <cite>Conneau et al. (2018)</cite> in terms of mapping accuracy and convergence (Section 5.1)?",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_29",
  "x": "Through experiments, our goal is to assess: 1. Does the unsupervised mapping method based on our proposed adversarial autoencoder model improve over the best existing adversarial method of <cite>Conneau et al. (2018)</cite> in terms of mapping accuracy and convergence (Section 5.1)? 2. How does our unsupervised mapping method compare with other unsupervised and supervised approaches (Section 5.2)? 3.",
  "y": "motivation"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_31",
  "x": "Since our approach follows the same steps as <cite>Conneau et al. (2018),</cite> we first compare our proposed model with <cite>their model</cite> on European (Table 1) , non-European and low-resource languages (Table  2 ) on <cite>their dataset</cite>. In the tables, we present the numbers that <cite>they</cite> reported in <cite>their paper</cite> (<cite>Conneau et al. (2018)</cite> (paper)) as well as the results that we get by running <cite>their code</cite> on our machine (<cite>Conneau et al. (2018)</cite> (code)). For a fair comparison with respect to the quality of the learned mappings (or induced seed dictionary), here we only consider the results of our approach that use the refinement procedure of <cite>Conneau et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_32",
  "x": "**COMPARISON WITH <cite>CONNEAU ET AL. (2018)</cite>** Since our approach follows the same steps as <cite>Conneau et al. (2018),</cite> we first compare our proposed model with <cite>their model</cite> on European (Table 1) , non-European and low-resource languages (Table  2 ) on <cite>their dataset</cite>. In the tables, we present the numbers that <cite>they</cite> reported in <cite>their paper</cite> (<cite>Conneau et al. (2018)</cite> (paper)) as well as the results that we get by running <cite>their code</cite> on our machine (<cite>Conneau et al. (2018)</cite> (code)).",
  "y": "similarities differences uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_33",
  "x": "Since our approach follows the same steps as <cite>Conneau et al. (2018),</cite> we first compare our proposed model with <cite>their model</cite> on European (Table 1) , non-European and low-resource languages (Table  2 ) on <cite>their dataset</cite>. In the tables, we present the numbers that <cite>they</cite> reported in <cite>their paper</cite> (<cite>Conneau et al. (2018)</cite> (paper)) as well as the results that we get by running <cite>their code</cite> on our machine (<cite>Conneau et al. (2018)</cite> (code)). For a fair comparison with respect to the quality of the learned mappings (or induced seed dictionary), here we only consider the results of our approach that use the refinement procedure of <cite>Conneau et al. (2018)</cite> .",
  "y": "differences uses similarities"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_34",
  "x": "In the tables, we present the numbers that <cite>they</cite> reported in <cite>their paper</cite> (<cite>Conneau et al. (2018)</cite> (paper)) as well as the results that we get by running <cite>their code</cite> on our machine (<cite>Conneau et al. (2018)</cite> (code)). For a fair comparison with respect to the quality of the learned mappings (or induced seed dictionary), here we only consider the results of our approach that use the refinement procedure of <cite>Conneau et al. (2018)</cite> . In Table 1 , we see that our Adversarial autoencoder + <cite>Conneau et al. (2018)</cite> Refinement outperforms <cite>Conneau et al. (2018)</cite> in all the six translation tasks involving European language pairs, yielding gains in the range 0.3 -1.3%.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_35",
  "x": "In the tables, we present the numbers that <cite>they</cite> reported in <cite>their paper</cite> (<cite>Conneau et al. (2018)</cite> (paper)) as well as the results that we get by running <cite>their code</cite> on our machine (<cite>Conneau et al. (2018)</cite> (code)). For a fair comparison with respect to the quality of the learned mappings (or induced seed dictionary), here we only consider the results of our approach that use the refinement procedure of <cite>Conneau et al. (2018)</cite> . In Table 1 , we see that our Adversarial autoencoder + <cite>Conneau et al. (2018)</cite> Refinement outperforms <cite>Conneau et al. (2018)</cite> in all the six translation tasks involving European language pairs, yielding gains in the range 0.3 -1.3%.",
  "y": "differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_36",
  "x": "In Table 1 , we see that our Adversarial autoencoder + <cite>Conneau et al. (2018)</cite> Refinement outperforms <cite>Conneau et al. (2018)</cite> in all the six translation tasks involving European language pairs, yielding gains in the range 0.3 -1.3%. Our method is also superior to <cite>theirs</cite> for the non-European and low-resource language pairs in Table 2 . Here our method gives more gains ranging from 1.8 to 4.3%.",
  "y": "differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_37",
  "x": "In Table 1 , we see that our Adversarial autoencoder + <cite>Conneau et al. (2018)</cite> Refinement outperforms <cite>Conneau et al. (2018)</cite> in all the six translation tasks involving European language pairs, yielding gains in the range 0.3 -1.3%. We found <cite>their model</cite> to be very fragile for En from/to Ms, and does not converge at all for Ms\u2192En.",
  "y": "differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_38",
  "x": "In Table 1 , we see that our Adversarial autoencoder + <cite>Conneau et al. (2018)</cite> Refinement outperforms <cite>Conneau et al. (2018)</cite> in all the six translation tasks involving European language pairs, yielding gains in the range 0.3 -1.3%. We ran <cite>their code</cite> 10 times for Ms\u2192En but failed every time.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_39",
  "x": "Our method is also superior to <cite>theirs</cite> for the non-European and low-resource language pairs in Table 2 . We ran <cite>their code</cite> 10 times for Ms\u2192En but failed every time. Compared to <cite>that</cite>, our method is more robust and converged most of the time we ran.",
  "y": "differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_40",
  "x": "Compared to <cite>that</cite>, our method is more robust and converged most of the time we ran. If we compare our method with the method of <cite>Conneau et al. (2018)</cite> on the more challenging Dinu-Artexe dataset in Table 3 , we see here also our method performs better than <cite>their method</cite> in all the four translation tasks involving European language pairs. In this dataset, our method shows more robustness compared to <cite>their method</cite>.",
  "y": "differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_41",
  "x": "If we compare our method with the method of <cite>Conneau et al. (2018)</cite> on the more challenging Dinu-Artexe dataset in Table 3 , we see here also our method performs better than <cite>their method</cite> in all the four translation tasks involving European language pairs. In this dataset, our method shows more robustness compared to <cite>their method</cite>. For example, <cite>their method</cite> had difficulties in converging for En from/to Es translations; for En\u2192Es, it converges only 2 times out of 10 attempts, while for Es\u2192En it did not converge a single time in 10 attempts.",
  "y": "differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_42",
  "x": "If we compare our method with the method of <cite>Conneau et al. (2018)</cite> on the more challenging Dinu-Artexe dataset in Table 3 , we see here also our method performs better than <cite>their method</cite> in all the four translation tasks involving European language pairs. For example, <cite>their method</cite> had difficulties in converging for En from/to Es translations; for En\u2192Es, it converges only 2 times out of 10 attempts, while for Es\u2192En it did not converge a single time in 10 attempts.",
  "y": "differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_43",
  "x": "If we compare our method with the method of <cite>Conneau et al. (2018)</cite> on the more challenging Dinu-Artexe dataset in Table 3 , we see here also our method performs better than <cite>their method</cite> in all the four translation tasks involving European language pairs. For example, <cite>their method</cite> had difficulties in converging for En from/to Es translations; for En\u2192Es, it converges only 2 times out of 10 attempts, while for Es\u2192En it did not converge a single time in 10 attempts. Compared to <cite>that</cite>, our method was more robust, converging 4 times out of 10 attempts.",
  "y": "differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_44",
  "x": "In Section 5.3, we compare our model with <cite>Conneau et al. (2018)</cite> more rigorously by evaluating them with and without fine-tuning and measuring their performance on P@1, P@5, and P@10. ---------------------------------- **COMPARISON WITH OTHER METHODS**",
  "y": "differences uses similarities"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_45",
  "x": "In this section, we compare our model with other state-of-the-art methods that do not follow the same procedure as us and <cite>Conneau et al. (2018)</cite> . For example, Artetxe et al. (2018b) do the initial mapping in the similarity space, then they apply a different self-learning method to fine-tune the embeddings, and perform a final refinement with symmetric re-weighting. Instead of mapping from source to target, they map both source and target embeddings to a common space.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_46",
  "x": "Let us first consider the results for European language pairs on the dataset of <cite>Conneau et al. (2018)</cite> in Table 1 . Our Adversarial autoencoder + <cite>Conneau et al. (2018)</cite> Refinement performs better than most of the other methods on <cite>this dataset</cite>, achieving the highest accuracy for 4 out of 6 translation tasks. For De\u2192En, our result is very close to the best system of Artetxe et al. (2018b) with only 0.2% difference.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_48",
  "x": "We notice that the method of Artetxe et al. (2018b) gives better results than other baselines, even in some translation tasks they achieve the highest accuracy. To understand whether the improvements of their method are due to a better initial mapping or better post-processing, we conducted two additional experiments. In our first experiment, we use their method to induce the initial seed dictionary and then apply iterative Procrustes solution (same refinement procedure of <cite>Conneau et al. (2018)</cite> ) for refinement.",
  "y": "uses"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_49",
  "x": "This setup allows us to compare our model directly with the adversarial model of <cite>Conneau et al. (2018)</cite> , putting the effect of finetuning aside. Table 5 presents the ablation results for En-Es, En-De, and En-It in both directions. The first row presents the results of <cite>Conneau et al. (2018)</cite> <cite>that</cite> uses adversarial training to map the word embeddings.",
  "y": "differences uses similarities"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_50",
  "x": "This setup allows us to compare our model directly with the adversarial model of <cite>Conneau et al. (2018)</cite> , putting the effect of finetuning aside. Table 5 presents the ablation results for En-Es, En-De, and En-It in both directions. The first row presents the results of <cite>Conneau et al. (2018)</cite> <cite>that</cite> uses adversarial training to map the word embeddings.",
  "y": "uses similarities differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_51",
  "x": "However, it is important Table 5 : Ablation study of our adversarial autoencoder model on the dataset of <cite>Conneau et al. (2018)</cite>. to note that in contrast to <cite>Conneau et al. (2018)</cite> , our mapping is performed at the code space. As we compare our full model with the model of <cite>Conneau et al. (2018)</cite> in the without fine-tuning setting, we notice large improvements in all measures across all datasets: 5.1 -7.3% in En\u2192Es, 3 -6% in Es\u2192En, 3.4 -4.3% in En\u2192De, 1 -3% in De\u2192En, 3.4 -4.3% in En\u2192It, and 0.3 -3.7% in It\u2192En.",
  "y": "differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_52",
  "x": "to note that in contrast to <cite>Conneau et al. (2018)</cite> , our mapping is performed at the code space. As we compare our full model with the model of <cite>Conneau et al. (2018)</cite> in the without fine-tuning setting, we notice large improvements in all measures across all datasets: 5.1 -7.3% in En\u2192Es, 3 -6% in Es\u2192En, 3.4 -4.3% in En\u2192De, 1 -3% in De\u2192En, 3.4 -4.3% in En\u2192It, and 0.3 -3.7% in It\u2192En. These improvements demonstrate that our model finds a better mapping compared to <cite>Conneau et al. (2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_53",
  "x": "As we compare our full model with the model of <cite>Conneau et al. (2018)</cite> in the without fine-tuning setting, we notice large improvements in all measures across all datasets: 5.1 -7.3% in En\u2192Es, 3 -6% in Es\u2192En, 3.4 -4.3% in En\u2192De, 1 -3% in De\u2192En, 3.4 -4.3% in En\u2192It, and 0.3 -3.7% in It\u2192En. These improvements demonstrate that our model finds a better mapping compared to <cite>Conneau et al. (2018)</cite> . Among the three components, the cycle consistency is the most influential one across all languages.",
  "y": "differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_54",
  "x": "Training the target encoder adversarially also gives a significant boost. The reconstruction has less impact. If we compare the results of ---Cycle with <cite>Conneau-18,</cite> we see sizeable gains for En-Es in both directions.",
  "y": "differences"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_55",
  "x": "Now let us turn our attention to the results with fine-tuning. Here also we see gains across all datasets for our model, although the gains are not as verbose as before (about 1% on average). However, this is not surprising as it has been shown that iterative fine-tuning with Procrustes solution is a robust method that can recover many errors made in the initial mapping (<cite>Conneau et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "5b14e259a557aa3cbfcbd6265f04c8_56",
  "x": "In our adversarial training, both the mapper and the target encoder are treated as generators that act jointly to fool the discriminator. To guide the mapping further, we include constraints for cycle consistency and post-cycle reconstruction. Through extensive experimentations on six different language pairs comprising European, nonEuropean and low-resource languages from two different data sources, we demonstrate that our method outperforms the method of <cite>Conneau et al. (2018)</cite> for all translation tasks in all measures (P@{1,5,10}) across all settings (with and without fine-tuning).",
  "y": "differences"
 },
 {
  "id": "5b9a6590d2e7c49f9a9788abe6dc1b_0",
  "x": "**INTRODUCTION** Recent work on neural constituency parsing<cite> (Dyer et al., 2016</cite>; Choe and Charniak, 2016) has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler. Let A be a parser that we want to parse with (here one of the generative models), and let B be a base parser that we use to propose candidate parses which are then scored by the less-tractable parser A. We denote this cross-scoring setup by B \u2192 A. The papers above repeatedly saw that the cross-scoring setup B \u2192 A under which their generative models were applied outperformed the standard singleparser setup B \u2192 B. We term this a cross-scoring gain.",
  "y": "background"
 },
 {
  "id": "5b9a6590d2e7c49f9a9788abe6dc1b_1",
  "x": "It might even be the case that B is a better parser overall (i.e. B \u2192 B outperforms A \u2192 A). Of course, many real hybrids will exhibit both reranking and model combination gains. In this paper, we present experiments to isolate the degree to which each gain occurs for each of two state-of-the-art generative neural parsing models: the Recurrent Neural Network Grammar generative parser (RG) of <cite>Dyer et al. (2016)</cite> , and the LSTM language modeling generative parser (LM) of Choe and Charniak (2016) .",
  "y": "uses"
 },
 {
  "id": "5b9a6590d2e7c49f9a9788abe6dc1b_2",
  "x": "In this paper, we present experiments to isolate the degree to which each gain occurs for each of two state-of-the-art generative neural parsing models: the Recurrent Neural Network Grammar generative parser (RG) of <cite>Dyer et al. (2016)</cite> , and the LSTM language modeling generative parser (LM) of Choe and Charniak (2016) . In particular, we present and use a beam-based search procedure with an augmented state space that can search directly in the generative models, allowing us to explore A \u2192 A for these generative parsers A independent of any base parsers. Our findings suggest the presence of model combination effects in both generative parsers: when parses found by searching directly in the generative parser are added to a list of candidates from a strong base parser (the RNNG discriminative parser, RD<cite> (Dyer et al., 2016)</cite> ), performance decreases when compared to using just candidates from the base parser, i.e., B \u222a A \u2192 A has lower evaluation performance than B \u2192 A (Section 3.1).",
  "y": "uses"
 },
 {
  "id": "5b9a6590d2e7c49f9a9788abe6dc1b_3",
  "x": "**DECODING IN GENERATIVE NEURAL MODELS** All of the parsers we investigate in this work (the discriminative parser RD, and the two generative parsers RG and LM, see Section 1) produce parse trees in a depth-first, left-to-right traversal, using the same basic actions: NT(X), which opens a new constituent with the non-terminal symbol X; SHIFT / GEN(w), which adds a word; and RE-DUCE, which closes the current constituent. We refer to <cite>Dyer et al. (2016)</cite> for a complete description of these actions, and the constraints on them necessary to ensure valid parse trees.",
  "y": "uses"
 },
 {
  "id": "5b9a6590d2e7c49f9a9788abe6dc1b_4",
  "x": "Past work on discriminative neural constituency parsers has shown the effectiveness of beam search with a small beam (Vinyals et al., 2015) or even greedy search, as in the case of RD<cite> (Dyer et al., 2016)</cite> . The standard beam search procedure, which we refer to as action-synchronous, maintains a beam of K partially-completed parses that all have the same number of actions taken. At each stage, a pool of successors is constructed by extending each candidate in the beam with each of its possible next actions.",
  "y": "background"
 },
 {
  "id": "5b9a6590d2e7c49f9a9788abe6dc1b_5",
  "x": "For the LSTM generative model (LM), we use the pre-trained model released by Choe and Charniak (2016) . We train RNNG discriminative (RD) and generative (RG) models, following <cite>Dyer et al. (2016)</cite> by using the same hyperparameter settings, and using pretrained word embeddings from Ling et al. (2015) for the discriminative model. The automaticallypredicted part-of-speech tags we use as input for RD are the same as those used by Cross and Huang (2016) .",
  "y": "uses"
 },
 {
  "id": "5c13e64d468b8a1c403072f213c992_0",
  "x": "First, we justify the application of a new measure for the automatic extraction of paraphrase corpora. Second, we discuss the work done by<cite> (Barzilay & Lee, 2003)</cite> who use clustering of paraphrases to induce rewriting rules. We will see, through classical visualization methodologies (Kruskal & Wish, 1977) and exhaustive experiments, that clustering may not be the best approach for automatic pattern identification.",
  "y": "background motivation"
 },
 {
  "id": "5c13e64d468b8a1c403072f213c992_1",
  "x": "**INTRODUCTION** Sentence Compression can be seen as the removal of redundant words or phrases from an input sentence by creating a new sentence in which the gist of the original meaning of the sentence remains unchanged. Sentence Compression takes an important place for Natural Language Processing (NLP) tasks where specific constraints must be satisfied, such as length in summarization (Barzilay & Lee, 2002; Knight & Marcu, 2002; Shinyama et al., 2002;<cite> Barzilay & Lee, 2003</cite>; Le Nguyen & Ho, 2004; Unno et al., 2006) , style in text simplification (Marsi & Krahmer, 2005) or sentence simplification for subtitling (Daelemans et al., 2004) .",
  "y": "background"
 },
 {
  "id": "5c13e64d468b8a1c403072f213c992_2",
  "x": "In particular, we will first justify the application of a new measure for the automatic extraction of paraphrase corpora. Second, we will discuss the work done by<cite> (Barzilay & Lee, 2003)</cite> who use clustering of paraphrases to induce rewriting rules. We will see, through classical visualization methodologies (Kruskal & Wish, 1977) and exhaustive experiments, that clustering may not be the best approach for automatic pattern identification.",
  "y": "background motivation"
 },
 {
  "id": "5c13e64d468b8a1c403072f213c992_3",
  "x": "Two different approaches have been proposed for Sentence Compression: purely statistical methodologies<cite> (Barzilay & Lee, 2003</cite>; Le Nguyen & Ho, 2004) and hybrid linguistic/statistic methodologies (Knight & Marcu, 2002; Shinyama et al., 2002; Daelemans et al., 2004; Marsi & Krahmer, 2005; Unno et al., 2006) . As our work is based on the first paradigm, we will focus on the works proposed by<cite> (Barzilay & Lee, 2003)</cite> and (Le Nguyen & Ho, 2004) . (Barzilay & Lee, 2003 ) present a knowledge-lean algorithm that uses multiple-sequence alignment to learn generate sentence-level paraphrases essentially from unannotated corpus data alone. In contrast to (Barzilay & Lee, 2002) , they need neither parallel data nor explicit information about sentence semantics. Rather, they use two comparable corpora. Their approach has three main steps. First, working on each of the comparable corpora separately, they compute lattices compact graph-based representations to find commonalities within groups of structurally similar sentences. Next, they identify pairs of lattices from the two different corpora that are paraphrases of each other. Finally, given an input sentence to be paraphrased, they match it to a lattice and use a paraphrase from the matched lattices mate to generate an output sentence.",
  "y": "background"
 },
 {
  "id": "5c13e64d468b8a1c403072f213c992_4",
  "x": "In particular, (Le Nguyen & Ho, 2004) do not propose any methodology to automatically extract paraphrases. Instead, they collect a corpus by performing the decomposition program using news and their summaries. Comparatively,<cite> (Barzilay & Lee, 2003)</cite> propose to use the N-gram Overlap metric to capture similarities between sentences and automatically create paraphrase corpora. However, this choice is arbitrary and mainly leads to the extraction of quasi-exact or exact matching pairs.",
  "y": "background motivation"
 },
 {
  "id": "5c13e64d468b8a1c403072f213c992_5",
  "x": "For that purpose, we introduce a new metric, the Sumo-Metric. Unlike (Le Nguyen & Ho, 2004) , one interesting idea proposed by<cite> (Barzilay & Lee, 2003</cite> ) is to cluster similar pairs of paraphrases to apply multiplesequence alignment. However, once again, this choice is not justified and we will see by classical visualization methodologies (Kruskal & Wish, 1977) and exhaustive experiments by applying different clustering algorithms, that clustering may not be the best approach for automatic pattern identification. As a consequence, we will study global and local biology based sequence alignments compared to multi-sequence alignment that may lead to better results for the induction of rewriting rules.",
  "y": "differences background motivation"
 },
 {
  "id": "5c13e64d468b8a1c403072f213c992_6",
  "x": "A few unsupervised metrics have been applied to automatic paraphrase identification and extraction<cite> (Barzilay & Lee, 2003</cite>; Dolan & Brockett, 2004) . However, these unsupervised methodologies show a major drawback by extracting quasi-exact 2 or even exact match pairs of sentences as they rely on classical string similarity measures such as the Edit Distance in the case of (Dolan & Brockett, 2004) and word N-gram overlap for<cite> (Barzilay & Lee, 2003)</cite> . Such pairs are clearly useless.",
  "y": "background motivation"
 },
 {
  "id": "5c13e64d468b8a1c403072f213c992_7",
  "x": "More recently, (Anonymous, 2007) proposed a new metric, the Sumo-Metric specially designed for asymmetrical entailed pairs identification, and proved better performance over previous established metrics, even in the specific case when tested with the Microsoft Paraphrase Research Corpus (Dolan & Brockett, 2004) . In particular, it shows systematically better F-Measure and Accuracy measures over all other metrics showing an improvement of (1) at least 2.86% in terms of F-Measure and 3.96% in terms of Accuracy and (2) at most 6.61% in terms of FMeasure and 6.74% in terms of Accuracy compared to the second best metric which is also systematically the word N-gram overlap similarity measure used by<cite> (Barzilay & Lee, 2003)</cite> .",
  "y": "background"
 },
 {
  "id": "5c13e64d468b8a1c403072f213c992_8",
  "x": "Literature shows that there are two main reasons to apply clustering for paraphrase extraction. On one hand, as<cite> (Barzilay & Lee, 2003)</cite> evidence, clusters of paraphrases can lead to better learning of text-totext rewriting rules compared to just pairs of paraphrases. On the other hand, clustering algorithms may lead to better performance than stand-alone similarity measures as they may take advantage of the different structures of sentences in the cluster to detect a new similar sentence.",
  "y": "background"
 },
 {
  "id": "5c13e64d468b8a1c403072f213c992_9",
  "x": "However, as<cite> (Barzilay & Lee, 2003)</cite> do not propose any evaluation of which clustering algorithm should be used, we experiment a set of clustering algorithms and present the comparative results. Contrarily to what expected, we will see that clustering is not a worthy effort. Instead of extracting only sentence pairs from corpora 3 , one may consider the extraction of paraphrase sentence clusters.",
  "y": "motivation differences"
 },
 {
  "id": "5c13e64d468b8a1c403072f213c992_11",
  "x": "First, clustering seems not to be a natural way to manage 4 The limitation to 500 data is due to computation costs since MDS requires the diagonalization of the square similarity or distance matrix. such data. Then, according to the clustering method used, several types of clusters can be expected: very small clusters which contain \"satellite\" data (pretty relevant) or large clusters with part of the main central class (pretty irrelevant). These results confirm the observed figures in the previous subsection and reinforce the sight that clustering is a worthless effort for automatic paraphrase corpora construction, contrarily to what<cite> (Barzilay & Lee, 2003)</cite> suggest.",
  "y": "differences"
 },
 {
  "id": "5c13e64d468b8a1c403072f213c992_12",
  "x": "Experiments, by using 4 algorithms and through visualization techniques, revealed that clustering is a worthless effort for paraphrase corpora construction, contrary to the literature claims<cite> (Barzilay & Lee, 2003)</cite> . Therefore simple paraphrase pair extraction is suggested and by using a recent and more reliable metric (Sumo-Metric) (Anonymous, 2007) designed for asymmetrical entailed pairs. We also propose a dynamic choosing of the alignment algorithm and a word scoring function for the alignment algorithms.",
  "y": "differences"
 },
 {
  "id": "5c5abc2773143af41d49087e17310e_0",
  "x": "In this paper, we present a sentence selection objective for extractive summarization in which sentences are penalized for containing content that is specific to the documents they were extracted from. We modify an existing system, HIER-SUM<cite> (Haghighi & Vanderwende, 2009</cite>) , to use our objective, which significantly outperforms the original HIERSUM in pairwise user evaluation. Additionally, our ROUGE scores advance the current state-of-the-art for both supervised and unsupervised systems with statistical significance.",
  "y": "extends"
 },
 {
  "id": "5c5abc2773143af41d49087e17310e_1",
  "x": "Summaries can be evaluated manually, or with automatic metrics such as ROUGE (Lin, 2004) . The use of structured probabilistic topic models has made it possible to represent document set content with increasing complexity (Daum\u00e9 & Marcu, 2006; Tang et al., 2009; Celikyilmaz & HakkaniTur, 2010) . <cite>Haghighi and Vanderwende (2009)</cite> demonstrated that these models can improve the quality of generic multi-document summaries over simpler surface models.",
  "y": "background"
 },
 {
  "id": "5c5abc2773143af41d49087e17310e_2",
  "x": "We re-implement the HIERSUM system from <cite>Haghighi and Vanderwende (2009)</cite> , and show that using our objective dramatically improves the content of extracted summaries. ---------------------------------- **MODELING CONTENT**",
  "y": "extends"
 },
 {
  "id": "5c5abc2773143af41d49087e17310e_4",
  "x": "The highest frequency words (after removing stop words) have a high likelihood of appearing in human-authored summaries (Nenkova & Vanderwende, 2005) . However, the raw<cite> (Haghighi & Vanderwende, 2009).</cite> unigram distribution may contain words that appear frequently in one document, but do not reflect the content of the document set as a whole.",
  "y": "background"
 },
 {
  "id": "5c5abc2773143af41d49087e17310e_5",
  "x": "This idea was first presented by Daum\u00e9 and Marcu (2006) for their BAYESUM system for query-focused summarization, and later adapted for non-query summarization in the TOPICSUM system by <cite>Haghighi and Vanderwende (2009)</cite> . 1 In these systems, each word from the original documents is drawn from one of three vocabulary distributions. The first, \u03c6 b , is the background distribution of general English words.",
  "y": "background"
 },
 {
  "id": "5c5abc2773143af41d49087e17310e_6",
  "x": "The second, \u03c6 d , contains vocabulary that is specific to that one document. And the third, \u03c6 c , is the distribution of content words for that document set, and contains relevant words that should appear in the generated summary. HIERSUM<cite> (Haghighi & Vanderwende, 2009</cite> ) adds more structure to TOPICSUM by further splitting the content distribution into multiple sub-topics. The content words in each sentence can be generated by either the general content topic or the content sub-topic for that sentence, and the words from the general content distribution are considered when building the summary.",
  "y": "background"
 },
 {
  "id": "5c5abc2773143af41d49087e17310e_7",
  "x": "This quantity is used for summary sentence selection in several systems including Lerman and McDonald (2009) and <cite>Haghighi and Vanderwende (2009)</cite> , and was used as a feature in the discrimitive sentence ranking of Daum\u00e9 and Marcu (2006) . TOPICSUM and HIERSUM use the following KL objective, which finds S * , the summary that minimizes the KL-divergence between the estimated content distribution \u03c6 c and the summary word distribution P S : A greedy approximation is used to find S * .",
  "y": "background"
 },
 {
  "id": "5c5abc2773143af41d49087e17310e_8",
  "x": "Users were given the two summaries to compare, plus a human-generated reference summary. The order that the summaries appeared in was random. We asked users to select which summary was better for the following ques-5 <cite>Haghighi and Vanderwende (2009)</cite> Q1 Which was better in terms of overall content?",
  "y": "uses"
 },
 {
  "id": "5cb56f6bf8123a9949a7f7c4ebc85c_0",
  "x": "**ABSTRACT** One of the tasks in aspect-based sentiment analysis is to extract aspect and opinion terms from review text. Our study focuses on evaluating transfer learning using BERT <cite>(Devlin et al., 2019)</cite> to classify tokens from hotel reviews in bahasa Indonesia.",
  "y": "uses"
 },
 {
  "id": "5cb56f6bf8123a9949a7f7c4ebc85c_1",
  "x": "While some work has been done in this task Fernando et al., 2019; Xue and Li, 2018) , we have not seen a transfer learning approach (Ruder, 2019) employed, which should need much less training effort. Using transfer learning is especially helpful for low-resource languages (Kocmi and Bojar, 2018) , such as bahasa Indonesia. Our main contribution in this study is evaluating BERT <cite>(Devlin et al., 2019)</cite> as a pretrained transformer model on this token classification task on hotel reviews in bahasa Indonesia.",
  "y": "uses"
 },
 {
  "id": "5cb56f6bf8123a9949a7f7c4ebc85c_2",
  "x": "We proposed to use transfer learning from pretrained BERT-Base, Multilingual Cased <cite>(Devlin et al., 2019)</cite> for this token classification problem. We used the implementation in PyTorch by Hugging Face (2019) 3 . We found out that the multilingual cased tokenizer of BERT does not recognize some common terms in our dataset, such as \"kamar\" (room), \"kendala\" (issue), \"wifi\", \"koneksi\" (connection), \"bagus\" (good), \"bersih\" (clean).",
  "y": "uses"
 },
 {
  "id": "5cb56f6bf8123a9949a7f7c4ebc85c_3",
  "x": "In their paper,<cite> Devlin et al. (2019)</cite> show that they can achieve state-of-the-art performance not only on sentence-level, but also on token-level tasks, such as for named entity recognition (NER). This motivates us to explore BERT in our study. This way, we do not need to use dependency parsers or any feature engineering.",
  "y": "background"
 },
 {
  "id": "5cb56f6bf8123a9949a7f7c4ebc85c_4",
  "x": "In their paper,<cite> Devlin et al. (2019)</cite> show that they can achieve state-of-the-art performance not only on sentence-level, but also on token-level tasks, such as for named entity recognition (NER). This motivates us to explore BERT in our study. This way, we do not need to use dependency parsers or any feature engineering.",
  "y": "motivation"
 },
 {
  "id": "5d68c07f716cd3c9861921d7e515ea_0",
  "x": "We follow the step-by-step approach to neural data-to-text generation we proposed in <cite>Moryossef et al. (2019)</cite> , in which the generation process is divided into a text-planning stage followed by a plan-realization stage. We suggest four extensions to that framework: (1) we introduce a trainable neural planning component that can generate effective plans several orders of magnitude faster than <cite>the original planner</cite>; (2) we incorporate typing hints that improve <cite>the model</cite>'s ability to deal with unseen relations and entities; (3) we introduce a verification-by-reranking stage that substantially improves the faithfulness of the resulting texts; (4) we incorporate a simple but effective referring expression generation module. These extensions result in a generation process that is faster, more fluent, and more accurate.",
  "y": "uses"
 },
 {
  "id": "5d68c07f716cd3c9861921d7e515ea_1",
  "x": "We follow the step-by-step approach to neural data-to-text generation we proposed in <cite>Moryossef et al. (2019)</cite> , in which the generation process is divided into a text-planning stage followed by a plan-realization stage. We suggest four extensions to that framework: (1) we introduce a trainable neural planning component that can generate effective plans several orders of magnitude faster than <cite>the original planner</cite>; (2) we incorporate typing hints that improve <cite>the model</cite>'s ability to deal with unseen relations and entities; (3) we introduce a verification-by-reranking stage that substantially improves the faithfulness of the resulting texts; (4) we incorporate a simple but effective referring expression generation module. These extensions result in a generation process that is faster, more fluent, and more accurate.",
  "y": "extends"
 },
 {
  "id": "5d68c07f716cd3c9861921d7e515ea_2",
  "x": "In recent work <cite>(Moryossef et al., 2019)</cite> <cite>we</cite> proposed to adopt ideas from \"traditional\" language generation approaches (i.e. Reiter and Dale (2000) ; Walker et al. (2007) ; Gatt and Krahmer (2017) ) that separate the generation into a planning stage that determines the order and structure of the expressed facts, and a realization stage that maps the plan to natural language text. <cite>We</cite> show that by breaking the task this way, one can achieve the same fluency of neural generation systems while being able to better control the form of the generated text and to improve its correctness by reducing missing facts and \"hallucinations\", common in neural systems. In this work we adopt the step-by-step framework of <cite>Moryossef et al. (2019)</cite> and propose four independent extensions that improve aspects of our original system: we suggest a new plan generation mechanism, based on a trainable-yetverifiable neural decoder, that is orders of magnitude faster than the original one ( \u00a73); we use knowledge of the plan structure to add typing information to plan elements.",
  "y": "background"
 },
 {
  "id": "5d68c07f716cd3c9861921d7e515ea_3",
  "x": "In recent work <cite>(Moryossef et al., 2019)</cite> <cite>we</cite> proposed to adopt ideas from \"traditional\" language generation approaches (i.e. Reiter and Dale (2000) ; Walker et al. (2007) ; Gatt and Krahmer (2017) ) that separate the generation into a planning stage that determines the order and structure of the expressed facts, and a realization stage that maps the plan to natural language text. <cite>We</cite> show that by breaking the task this way, one can achieve the same fluency of neural generation systems while being able to better control the form of the generated text and to improve its correctness by reducing missing facts and \"hallucinations\", common in neural systems. In this work we adopt the step-by-step framework of <cite>Moryossef et al. (2019)</cite> and propose four independent extensions that improve aspects of our original system: we suggest a new plan generation mechanism, based on a trainable-yetverifiable neural decoder, that is orders of magnitude faster than the original one ( \u00a73); we use knowledge of the plan structure to add typing information to plan elements.",
  "y": "background"
 },
 {
  "id": "5d68c07f716cd3c9861921d7e515ea_4",
  "x": "In recent work <cite>(Moryossef et al., 2019)</cite> <cite>we</cite> proposed to adopt ideas from \"traditional\" language generation approaches (i.e. Reiter and Dale (2000) ; Walker et al. (2007) ; Gatt and Krahmer (2017) ) that separate the generation into a planning stage that determines the order and structure of the expressed facts, and a realization stage that maps the plan to natural language text. <cite>We</cite> show that by breaking the task this way, one can achieve the same fluency of neural generation systems while being able to better control the form of the generated text and to improve its correctness by reducing missing facts and \"hallucinations\", common in neural systems. In this work we adopt the step-by-step framework of <cite>Moryossef et al. (2019)</cite> and propose four independent extensions that improve aspects of our original system: we suggest a new plan generation mechanism, based on a trainable-yetverifiable neural decoder, that is orders of magnitude faster than the original one ( \u00a73); we use knowledge of the plan structure to add typing information to plan elements.",
  "y": "extends uses"
 },
 {
  "id": "5d68c07f716cd3c9861921d7e515ea_6",
  "x": "The data-to-plan component in <cite>Moryossef et al. (2019)</cite> exhaustively generates all possible plans, scores them using a heuristic, and chooses the highest scoring one for realization. While this is feasible with the small input graphs in the WebNLG challenge (Colin et al., 2016) , it is also very computationally intensive, growing exponentially with the input size. We propose an alternative planner which works in linear time in the size of the graph and remains verifiable: generated plans are guaranteed to represent the input faithfully.",
  "y": "background"
 },
 {
  "id": "5d68c07f716cd3c9861921d7e515ea_7",
  "x": "Then, all edges visited in the traversal are removed from the input graph, and the process repeats (performing another truncated DFS) until no more edges remain. Each truncated DFS traversal corresponds to a sentence plan, following the DFS-to-plan procedure of <cite>Moryossef et al. (2019)</cite> : the linearized plan is generated incrementally at each step of the traversal. This process is linear in the number of edges in the graph.",
  "y": "uses"
 },
 {
  "id": "5d68c07f716cd3c9861921d7e515ea_8",
  "x": "Speed On a 7 edges graph, <cite>the planner</cite> of <cite>Moryossef et al. (2019)</cite> takes an average of 250 seconds to generate a plan, while our planner takes 0.0025 seconds, 5 orders of magnitude faster. ---------------------------------- **INCORPORATING TYPING INFORMATION FOR UNSEEN ENTITIES AND RELATIONS**",
  "y": "differences"
 },
 {
  "id": "5d68c07f716cd3c9861921d7e515ea_9",
  "x": "**INCORPORATING TYPING INFORMATION FOR UNSEEN ENTITIES AND RELATIONS** In <cite>Moryossef et al. (2019)</cite> , the sentence plan trees were linearized into strings that were then fed to a neural machine translation decoder (Open-NMT) (Klein et al., 2017) with a copy mecha-nism. This linearization process is lossy, in the sense that the linearized strings do not explicitly distinguish between symbols that represent entities (e.g., BARACK OBAMA) and symbols that represent relations (e.g., works-for).",
  "y": "background"
 },
 {
  "id": "5d68c07f716cd3c9861921d7e515ea_10",
  "x": "While the plan generation stage is guaranteed to be faithful to the input, the translation process from plans to text is based on a neural seq2seq model and may suffer from known issues with such models: hallucinating facts that do not exist in the input, repeating facts, or dropping facts. While the clear mapping between plans and text helps to reduce these issues greatly, <cite>the system</cite> in <cite>Moryossef et al. (2019)</cite> still has 2% errors of these kinds. Existing approaches: soft encouragement via neural modules.",
  "y": "background"
 },
 {
  "id": "5d68c07f716cd3c9861921d7e515ea_12",
  "x": "However, the generated referring expressions are sometimes incorrect. <cite>Moryossef et al. (2019)</cite> suggests the possibility of handling this with a post-processing referring-expression generation step (REG). Here, we propose a concrete REG module and demonstrate its effectiveness.",
  "y": "background"
 },
 {
  "id": "5d68c07f716cd3c9861921d7e515ea_15",
  "x": "**EFFECT OF TYPE INFORMATION** We repeat the coverage experiment in <cite>(Moryossef et al., 2019)</cite> , counting the number of output texts that contain all the entities in the input graph, and, of these text, counting the ones in which the entities appear in the exact same order as the plan. Incorporating typing information reduced the number of texts not containing all entities by 18% for the seen part of the test set, and 16% for the unseen part.",
  "y": "uses"
 },
 {
  "id": "5d68c07f716cd3c9861921d7e515ea_16",
  "x": "We thus performed manual analysis, following the procedure in <cite>Moryossef et al. (2019)</cite> . We manually inspect 148 samples from the seen part of the test set, containing 440 relations, counting expressed, omitted, wrong and over-generated (hallucinated) facts. 5 We compare to the <cite>StrongNeural</cite> and <cite>BestPlan</cite> systems from <cite>Moryossef et al. (2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "5d68c07f716cd3c9861921d7e515ea_18",
  "x": "---------------------------------- **CONCLUSIONS** We adopt the planning-based neural generation framework of <cite>Moryossef et al. (2019)</cite> and extend it to be orders of magnitude faster and produce more correct and more fluent text.",
  "y": "extends uses"
 },
 {
  "id": "5d68c07f716cd3c9861921d7e515ea_19",
  "x": "We adopt the planning-based neural generation framework of <cite>Moryossef et al. (2019)</cite> and extend it to be orders of magnitude faster and produce more correct and more fluent text. We conclude that these extensions not only improve <cite>the system</cite> of <cite>Moryossef et al. (2019)</cite> but also highlight the flexibility and advantages of the step-by-step framework for text generation. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_0",
  "x": "To address this drawback, new VQA datasets<cite> [44,</cite> 8, 37] have been recently proposed with questions that explicitly require understanding and reasoning about text in the image, which is referred to as the TextVQA task. Figure 1 . Compared to previous work (e.g. <cite>[44]</cite> ) on the TextVQA task, our model, accompanied by rich features for image text, handles all modalities with a multimodal transformer over a joint embedding space instead of pairwise fusion mechanisms between modalities.",
  "y": "background"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_1",
  "x": "Figure 1 . Compared to previous work (e.g. <cite>[44]</cite> ) on the TextVQA task, our model, accompanied by rich features for image text, handles all modalities with a multimodal transformer over a joint embedding space instead of pairwise fusion mechanisms between modalities. Furthermore, answers are predicted through iterative decoding with pointers instead of one-step classification over a fixed vocabulary or copying single text token from the image.",
  "y": "differences"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_2",
  "x": "Several approaches<cite> [44,</cite> 8, 37, 7] have been proposed for the TextVQA task, based on OCR results of the image. In particular, LoRRA <cite>[44]</cite> extends previous VQA models [43] with an OCR attention branch and adds OCR tokens as a dynamic vocabulary to the answer classifier, allowing copying a single OCR token from the image as the answer. Similarly in [37] , OCR tokens are grouped into blocks and added to the output space of a VQA model.",
  "y": "background"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_3",
  "x": "In particular, LoRRA <cite>[44]</cite> extends previous VQA models [43] with an OCR attention branch and adds OCR tokens as a dynamic vocabulary to the answer classifier, allowing copying a single OCR token from the image as the answer. Similarly in [37] , OCR tokens are grouped into blocks and added to the output space of a VQA model. While these approaches enable reading text in images to some extent, they typically rely on custom pairwise multimodal fusion mechanisms between two modalities (such as single-hop attention over image regions and text tokens, conditioned on the input question), which limit the types of possible interactions between modalities. Furthermore, they treat answer prediction as a single-step classification problem -either selecting an answer from the training set answers or copying a text token from the image -making it difficult to generate complex answers such as book titles or signboard names with multiple words, or answers with both common words and specific image text tokens, such as McDonald's burger where McDonald's is from text in the image and burger is from the model's own vocabulary. In addition, the word embedding based image text features in previous work have limited representation power and miss important cues such as the appearance (e.g. font and color) and the location of text tokens in images. For example, tokens that have different fonts and are spatially apart from each other usually do not belong to the same street sign. In this paper, we address the above limitations with our novel Multimodal Multi-Copy Mesh (M4C) model for the TextVQA task, based on the transformer [48] architecture accompanied by iterative answer decoding through dynamic pointers, as shown in Figure 1 .",
  "y": "motivation"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_4",
  "x": "3) We adopt a rich feature representation for text tokens in images and show that it is better than features based only on word embedding in previous work. 4) Our model significantly outperforms previous work on three challenging datasets for the TextVQA task: TextVQA <cite>[44]</cite> (+25% relative), ST-VQA [8] (+65% relative), and OCR-VQA [37] (+32% relative). ----------------------------------",
  "y": "differences"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_5",
  "x": "**RELATED WORK** VQA based on reading and understanding image text. Recently, a few datasets and methods<cite> [44,</cite> 8, 37, 7] have been proposed for visual question answering based on text in images (referred to as the TextVQA task).",
  "y": "background"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_6",
  "x": "Recently, a few datasets and methods<cite> [44,</cite> 8, 37, 7] have been proposed for visual question answering based on text in images (referred to as the TextVQA task). LoRRA <cite>[44]</cite> , a prominent prior work on this task, extends the Pythia [43] framework for VQA and allows it to copy a single OCR token from the image as the answer, by applying a single attention hop (conditioned on the question) over the OCR tokens and including the OCR token indices in the answer classifier's output space. A conceptually similar model is proposed in [37] , where OCR tokens are grouped into blocks and added to both the input features and the output answer space of a VQA model.",
  "y": "background"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_7",
  "x": "As it is intractable to have every possible text token in the answer vocabulary, copying text from the image would often be an easier option for answer prediction. Prior work has explored dynamically copying the inputs in different tasks such as text summarization [42] , knowledge retrieval [52] , and image captioning [35] based on Pointer Networks [50] and its variants. For the TextVQA task, recent works<cite> [44,</cite> 37] have proposed to copy OCR tokens by adding their indices to classifier outputs.",
  "y": "background"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_8",
  "x": "For the TextVQA task, recent works<cite> [44,</cite> 37] have proposed to copy OCR tokens by adding their indices to classifier outputs. However, apart from their limitation of copying only a single token (or block), one drawback of these approaches is that they require a pre-defined number of OCR tokens (since the classifier has a fixed output dimension) and their output is dependent on the ordering of the tokens. In this work, we overcome this drawback using a permutation-invariant pointer network together with our multimodal transformer.",
  "y": "extends"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_9",
  "x": "Embedding of detected objects. Given an image, we obtain a set of M visual objects through a pretrained detector (Faster R-CNN [41] in our case). Following prior work [3, 43,<cite> 44]</cite> , we extract appearance feature x fr m using the detector's output from the m-th object (where m = 1, \u00b7 \u00b7 \u00b7 , M ).",
  "y": "uses"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_10",
  "x": "We follow this intuition in our model and use a rich OCR representation consisting of four types of features, which is shown in our experiments to be significantly better than word embedding (such as FastText) alone in prior work <cite>[44]</cite> . After obtaining a set of N OCR tokens in an image through external OCR systems, from the n-th token (where n = 1, \u00b7 \u00b7 \u00b7 , N ) we extract 1) a 300-dimensional FastText [9] vector x ft n , which is a word embedding with sub-word information, 2) an appearance feature x fr n from the same Faster R-CNN detector in the object detection above, extracted via RoI-Pooling on the OCR token's bounding box, 3) a 604-dimensional Pyramidal Histogram of Characters (PHOC) [2] vector x p n , capturing what characters are present in the token -this is more robust to OCR errors and can be seen as a coarse character model, and 4) a 4-dimensional location feature x b n based on the OCR token's relative bounding box coordinates [x min /W im , y min /H im , x max /W im , y max /H im ]. We linearly project each feature into d-dimensional space, and sum them up (after layer normalization) as the final OCR token embedding {x ocr n } as below",
  "y": "differences"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_11",
  "x": "**EXPERIMENTS** We evaluate our model on three challenging datasets for the TextVQA task, including the TextVQA dataset <cite>[44]</cite> , the ST-VQA dataset [8] , and the OCR-VQA dataset [37] . Our model outperforms previous work by a significant margin on all the three datasets.",
  "y": "uses"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_12",
  "x": "**EVALUATION ON THE TEXTVQA DATASET** The TextVQA dataset <cite>[44]</cite> contains 28,408 images from the Open Images dataset [27] , with human-written questions asking to reason about text in the image. Similar to VQAv2 [17] , each question in the TextVQA dataset has 10 human annotated answers, and the final accuracy is measured via soft voting of the 10 answers.",
  "y": "background"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_13",
  "x": "Similar to VQAv2 [17] , each question in the TextVQA dataset has 10 human annotated answers, and the final accuracy is measured via soft voting of the 10 answers. 2 We use d = 768 as the dimensionality of the joint embedding space and extract question word features with BERT-BASE using the 768-dimensional outputs from its first three layers, which are fine-tuned during training. For visual objects, following Pythia [43] and LoRRA <cite>[44]</cite> , we detect objects with a Faster R-CNN detector [41] pretrained on the Visual Genome dataset [26] , and keeps 100 top-scoring objects per image.",
  "y": "uses"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_14",
  "x": "Finally, we extract text tokens on each image using the Rosetta OCR system [10] . Unlike the prior work LoRRA <cite>[44]</cite> that uses a multilingual Rosetta version, in our model we use an English-only version of Rosetta that we find has higher recall. We refer to these two versions as Rosettaml and Rosetta-en, respectively.",
  "y": "differences"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_15",
  "x": "As a notable prior work on this dataset, we show a stepby-step comparison with the LoRRA model <cite>[44]</cite> . Ablations on pretrained question encoding and OCR systems. We first experiment with a restricted version of our model using the multimodal transformer architecture but without iterative decoding in answer prediction, i.e. M4C (w/o dec.) in Table 1 .",
  "y": "uses"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_16",
  "x": "Here, we see that our model in line 8 still outperforms LoRRA (line 1) by as much as 9.5% (absolute) when using the same OCR system as LoRRA and even fewer pretrained components. We also analyze the performance of our model with respect to the maximum decoding steps, shown in Figure 3 , where decoding for multiple steps greatly improves the performance compared with a single step. Figure 4 shows qualitative examples (more examples in appendix) of our M4C model on the TextVQA dataset in comparison to LoRRA <cite>[44]</cite> , where our model is capable of selecting multiple OCR tokens and combining them with its fixed vocabulary in predicted answers.",
  "y": "uses"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_17",
  "x": "We also compare to the winning entries in the TextVQA Challenge 2019. 4 We compare are from fixed answer vocabulary). Compared to the previous work LoRRA <cite>[44]</cite> which selects one answer from training set or copies only a single OCR token, our model can copy multiple OCR tokens and combine them with its fixed vocabulary through iterative decoding.",
  "y": "differences"
 },
 {
  "id": "5e0b1b085a7a10b1e1c17286f7048e_18",
  "x": "However, we note that even without fixed answer vocabulary, our restricted model (M4C w/o fixed vocabulary in Table 5 ) still outperforms the previous work LoRRA <cite>[44]</cite> , suggesting that it is particularly important to learn to copy multiple OCR tokens to form an answer (a key feature in our model but not in LoRRA). ---------------------------------- **# METHOD**",
  "y": "differences"
 },
 {
  "id": "5e85f66e9971497e5e21af6893418d_0",
  "x": "Besides CCA, a few others learn a visual-semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function (Kiros et al., 2015; Socher et al., 2014; <cite>Vendrov et al., 2016)</cite> or by aligning image regions (objects) and segments of the description Plummer et al., 2015) in a common space. Recently Lin and Parikh (2016) have leveraged visual question answering models to encode images and descriptions into the same space. However, all of this work is targeted at monolingual descriptions, i.e., mapping images and descriptions in a single language onto a joint embedding space.",
  "y": "background"
 },
 {
  "id": "5e85f66e9971497e5e21af6893418d_1",
  "x": "Experiment Setup We sampled minibatches of size 64 images and their descriptions, and drew all negative samples from the minibatch. We trained using the Adam optimizer with learning rate 0.001, and early stopping on the validation set. Following<cite> Vendrov et al. (2016)</cite> we set the dimensionality of the embedding space and the GRU hidden layer N to 1024 for both English and German.",
  "y": "similarities uses"
 },
 {
  "id": "5e85f66e9971497e5e21af6893418d_2",
  "x": "For the former we use cosine similarity and for the latter we use the metric of<cite> Vendrov et al. (2016)</cite> which is useful for learning embeddings that maintain an order, e.g., dog and cat are more closer to pet than animal while being distinct. Such ordering is shown to be useful in building effective multimodal space of images and texts. An analogy in our setting would be two descriptions of an image are closer to the image while at the same time preserving the identity of each (which is useful when sentences describe two different aspects of the image).",
  "y": "uses similarities"
 },
 {
  "id": "5e85f66e9971497e5e21af6893418d_3",
  "x": "3 Baselines As baselines we use monolingual models, i.e., models trained on each language separately. Specifically, we use Visual Semantic Embeddings (VSE) of Kiros et al. (2015) and Order Embeddings (OE) of<cite> Vendrov et al. (2016)</cite> . We System Text to Image Image to Text R@1 R@5 R@10 Mr R@1 R@5 R@10 Mr VSE (Kiros et al., 2015) 23.3 53.6 65.8 5 31.6 60.4 72.7 3 OE<cite> (Vendrov et al., 2016)</cite> (Kiros et al., 2015) 20.3 47.2 60.1 6 29.3 58.1 71.8 4 OE<cite> (Vendrov et al., 2016)</cite> at an outdoor market , a small group of people stoop to buy potatoes from a street vendor , who has his goods laid out on the ground 24 2 2 Table 3 : The rank of the gold-standard image when using each German and English descriptions as a query on models trained using asymmetric similarity.",
  "y": "uses"
 },
 {
  "id": "5e85f66e9971497e5e21af6893418d_5",
  "x": "Note that we have one single model for both languages. In Tables 1 and 2 we present the ranking results of the baseline models of Kiros et al. (2015) and<cite> Vendrov et al. (2016)</cite> and our proposed PIVOT and PARALLEL models. We do not compare our image-description ranking results with Calixto et al. (2017) since they report results on half of validation set of Multi30k whereas our results are on the publicly available test set of Multi30k.",
  "y": "background"
 },
 {
  "id": "5e85f66e9971497e5e21af6893418d_6",
  "x": "In the semantic textual similarity task (STS), we use the textual embeddings from our model to compute the similarity between a pair of sen- (Wieting et al., 2017) \u2212 83.7 84.5 85.0 MLMME (Calixto et al., 2017) VGG19 \u2212 72.7 79.7 VSE (Kiros et al., 2015) VGG19 80.6 82.7 89.6 OE<cite> (Vendrov et al., 2016)</cite> VGG19 82. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014 , STS-2015 (Agirre et al. 2012 , Agirre et al. 2014 , Agirre et al. 2015 .",
  "y": "similarities"
 },
 {
  "id": "5e85f66e9971497e5e21af6893418d_7",
  "x": "We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017) , paraphrastic sentence embeddings (Wieting et al., 2017) , visual semantic embeddings (Kiros et al., 2015) , and order embeddings<cite> (Vendrov et al., 2016)</cite> . The shared task baseline is computed based on word overlap and is high for both the 2014 and the 2015 dataset, indicating that there is substantial lexical overlap between the STS image description datasets. Our models outperform both the baseline system and the best system submitted to the shared task.",
  "y": "similarities"
 },
 {
  "id": "5f86a4791bee14e0b1053e9b9a6fff_0",
  "x": "General bitext mapping algorithms are a recent invention. So far, most researchers interested in co-occurrence of mutual translations have relied on bitexts where sentence boundaries (or other text unit boundaries) were easy to find (e.g. Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1995;<cite> Melamed, 1995)</cite> . Aligned text segments suggest a boundary-based model of cooccurrence, illustrated in Figure 2 .",
  "y": "background"
 },
 {
  "id": "5f86a4791bee14e0b1053e9b9a6fff_1",
  "x": "It is somewhat surprising that this is a question at all, and most authors ignore it. However, when authors specify their algorithms in sufficient detail to answer this question, the most common answer (given, e.g., by Brown et al., 1993; Dagan et al., 1993; Kupiec, 1993;<cite> Melamed, 1995)</cite> turns out to be unsound. The problem is easiest to illustrate under the boundary-based model of co-occurrence.",
  "y": "background"
 },
 {
  "id": "5f86a4791bee14e0b1053e9b9a6fff_2",
  "x": "Co-occurrence is a universal precondition for translational equivalence among word tokens in bitexts. Other preconditions may be imposed if certain language-specific resources are available <cite>(Melamed, 1995)</cite> . For example, parts of speech tend to be preserved in translation (Papageorgiou et al., 1994) .",
  "y": "background"
 },
 {
  "id": "5f86a4791bee14e0b1053e9b9a6fff_3",
  "x": "Without loss of generality, whenever translation models refer to cooccurrence counts, they can refer to co-occurrence counts that have been filtered using whatever language-specific resources happen to be available. It does not matter if there are dependencies among the different knowledge sources, as long as each is used as a simple filter on the co-occurrence relation <cite>(Melamed, 1995)</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "5f97682d8a1b78f08fd3623ee81703_0",
  "x": "Open Information Extraction (Etzioni et al., 2008) systems output relational tuples from text without a pre-specified relational vocabulary by identifying relation phrases present in text. Early work on Open IE focused on verb-mediated relations that could be expressed using a handful of patterns and still covered substantial information in text. Subsequent research has focused on increasing recall -a noteworthy approach (OLLIE) uses bootstrapping for learning general language patterns <cite>(Mausam et al., 2012)</cite> .",
  "y": "background"
 },
 {
  "id": "5f97682d8a1b78f08fd3623ee81703_1",
  "x": "Various extensions improve on the amount of linguistic knowledge in the systems -EXEMPLAR (de S\u00e1 Mesquita et al., 2013) improves the set of rules on top of dependency parses; Open IE 4.0 1 uses carefully designed rules over 1 https://github.com/knowitall/openie semantic role labeling systems ; several works attempt clause identification or sentence restructuring, thus identifying sentence components and applying extraction rules on top of these components (Schmidek and Barbosa, 2014; Corro and Gemulla, 2013; Bast and Haussmann, 2013) . Other approaches include use of lexicosyntactic qualia-based patterns (Xavier et al., 2015) , simple sentence-specific inference (Bast and Haussmann, 2014) , and a supervised approach using tree kernels (Xu et al., 2013) . While the focus on verbs continues to be common in these Open IE systems, some works have directed attention on noun-mediated relations such as OL-LIE <cite>(Mausam et al., 2012)</cite> , RENOUN (Yahya et al., 2014) , and RELNOUN.",
  "y": "background"
 },
 {
  "id": "5f97682d8a1b78f08fd3623ee81703_2",
  "x": "**BACKGROUND ON NOMINAL OPEN IE** Probably the earliest work on Nominal Open IE is OLLIE, which is a pattern learning approach based on a bootstrapped training data using high precision verb-based extractions <cite>(Mausam et al., 2012)</cite> . It identified that nominal IE can't be completely syntactic, and, at the least, a list of relational nouns (e.g, mother, director, CEO, capital) is needed for high precision extraction.",
  "y": "background"
 },
 {
  "id": "5f97682d8a1b78f08fd3623ee81703_3",
  "x": "Note that while precision and yield (number of correct extractions) can be naturally computed by tagging extractions, estimating recall is challenging, as it requires annotators to tag all possible extractions from these sentences. Following previous work <cite>(Mausam et al., 2012)</cite> , we report yield, since recall is proportional to yield and suffices for system comparisons. Table 3 reports the results.",
  "y": "uses"
 },
 {
  "id": "5fe12a1a43957faded5722f698eb41_0",
  "x": "Approaches to lexical simplification generally follow a standard pipeline consisting of two main steps: generation and ranking. In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (Devlin and Tait, 1998) , learning substitution rules from sentence-aligned parallel corpora of complex-simple texts (Horn et al., 2014;<cite> Paetzold and Specia, 2017)</cite> , and learning word embeddings from a large corpora to obtain similar words of the complex word (Glava\u0161 and\u0160tajner, 2015; Kim et al., 2016; Specia, 2016a, 2017) . In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness.",
  "y": "background"
 },
 {
  "id": "5fe12a1a43957faded5722f698eb41_1",
  "x": "Given a dataset of tar-get words, their sentential contexts and substitution candidates for the target words, the goal is to train a model that accurately ranks the candidates based on their simplicity and semantic matching. For generating substitution candidates, we utilize the method proposed by<cite> Paetzold and Specia (2017)</cite> , which was recently shown to be the state-of-art method for generating substitution candidates. They exploit a hybrid substitution generation approach where candidates are first extracted from 550,644 simple-complex aligned sentences from the Newsela corpus (Xu et al., 2015) .",
  "y": "uses"
 },
 {
  "id": "5fe12a1a43957faded5722f698eb41_2",
  "x": "They exploit a hybrid substitution generation approach where candidates are first extracted from 550,644 simple-complex aligned sentences from the Newsela corpus (Xu et al., 2015) . Then, these candidates are complemented with candidates generated with a retrofitted word embedding model. The word embedding model is retrofitted over WordNet's synonym pairs (for details, please refer to<cite> Paetzold and Specia (2017)</cite> ).",
  "y": "uses background"
 },
 {
  "id": "5fe12a1a43957faded5722f698eb41_3",
  "x": "As baseline features, we use the same n-gram probability features as in<cite> Paetzold and Specia (2017)</cite> , who also employ a neural network to rank substitution candidates. As in<cite> Paetzold and Specia (2017)</cite> , the features were extracted using the SubIMDB corpus (Paetzold and Specia, 2015) . We also experiment with additional features that have been reported as useful in this task.",
  "y": "uses"
 },
 {
  "id": "5fe12a1a43957faded5722f698eb41_4",
  "x": "**FEATURES FOR DSSM** As baseline features, we use the same n-gram probability features as in<cite> Paetzold and Specia (2017)</cite> , who also employ a neural network to rank substitution candidates. As in<cite> Paetzold and Specia (2017)</cite> , the features were extracted using the SubIMDB corpus (Paetzold and Specia, 2015) .",
  "y": "uses"
 },
 {
  "id": "5fe12a1a43957faded5722f698eb41_5",
  "x": "**IMPLEMENTATION DETAILS AND TRAINING PROCEDURE OF THE DSSM** Following previous works that used supervised machine learning for ranking in lexical simplification (Horn et al., 2014;<cite> Paetzold and Specia, 2017)</cite> , we train the DSSM using the LexMTurk dataset (Horn et al., 2014) , which contains 500 instances composed of a sentence, a target word and substitution candidates ranked by simplicity <cite>(Paetzold and Specia, 2017)</cite> . In order to learn the parameters W t and W s (Figure 1 ) of the DSSM, we use the standard backpropagation algorithm (Rumelhart et al., 1988) .",
  "y": "uses"
 },
 {
  "id": "5fe12a1a43957faded5722f698eb41_6",
  "x": "**DATASETS AND EVALUATION METRICS** To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (Paetzold and Specia, 2016b) , which contains 929 instances, and NNSEval (Paetzold and Specia, 2016a) , which contains 239 instances. Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity <cite>(Paetzold and Specia, 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "5fe12a1a43957faded5722f698eb41_7",
  "x": "We compared the proposed model (DSSM Ranking) to two state-of-the-art approaches to ranking in lexical simplification that exploit supervised machine learning-based methods. The first baseline is the Neural Substitution Ranking (NSR) approach described in <cite>(Paetzold and Specia, 2017)</cite> , which employs a multi-layer perceptron neural network. We reimplement their model as part of the LEXenstein toolkit (Paetzold and Specia, 2015) .",
  "y": "uses"
 },
 {
  "id": "5fe12a1a43957faded5722f698eb41_9",
  "x": "All the three models employ the n-gram probability features extracted from the SubIMDB corpus (Paetzold and Specia, 2015) , as described in <cite>(Paetzold and Specia, 2017)</cite> , and are trained using the LexMTurk dataset. ---------------------------------- **RESULTS**",
  "y": "uses"
 },
 {
  "id": "5fe12a1a43957faded5722f698eb41_10",
  "x": "We also tried running all ranking systems on selected candidates that best replace the target word in the input sentence. We follow the Unsupervised Boundary Ranking Substitution Selection method described in<cite> Paetzold and Specia (2017)</cite> , which ranks candidates according to how well they fit the context of the target word, and discards 50% of the worst ranking candidates. The bottom part of the table 1 (Selection Step + Substitution Candidates Ranking) summarizes the results of all ranking systems after performing the selection step on generated substitution candidates.",
  "y": "uses"
 },
 {
  "id": "5fe12a1a43957faded5722f698eb41_11",
  "x": "We follow the Unsupervised Boundary Ranking Substitution Selection method described in<cite> Paetzold and Specia (2017)</cite> , which ranks candidates according to how well they fit the context of the target word, and discards 50% of the worst ranking candidates. We obtain similar tendency in the results, with the DSSM Ranking outperforming both baselines.",
  "y": "similarities differences"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_0",
  "x": "The rising influence of user-generated online reviews (Cone, 2011) has led to growing incentive for businesses to solicit and manufacture DECEPTIVE OPINION SPAM-fictitious reviews that have been deliberately written to sound authentic and deceive the reader. Recently, <cite>Ott et al. (2011)</cite> have introduced an opinion spam dataset containing gold standard deceptive positive hotel reviews. However, the complementary problem of negative deceptive opinion spam, intended to slander competitive offerings, remains largely unstudied.",
  "y": "motivation"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_1",
  "x": "Recently, <cite>Ott et al. (2011)</cite> have introduced an opinion spam dataset containing gold standard deceptive positive hotel reviews. However, the complementary problem of negative deceptive opinion spam, intended to slander competitive offerings, remains largely unstudied. Following an approach similar to <cite>Ott et al. (2011)</cite> , in this work we create and study the first dataset of deceptive opinion spam with negative sentiment reviews.",
  "y": "similarities"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_2",
  "x": "**INTRODUCTION** Consumer's purchase decisions are increasingly influenced by user-generated online reviews of products and services (Cone, 2011) . Accordingly, there is a growing incentive for businesses to solicit and manufacture DECEPTIVE OPINION SPAMfictitious reviews that have been deliberately written to sound authentic and deceive the reader <cite>(Ott et al., 2011)</cite> .",
  "y": "motivation"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_3",
  "x": "In particular, reviews intended to promote or hype an offering, and which therefore express a positive sentiment towards the offering, are called positive deceptive opinion spam. In contrast, reviews intended to disparage or slander competitive offerings, and which therefore express a negative sentiment towards the offering, are called negative deceptive opinion spam. While previous related work <cite>(Ott et al., 2011</cite>; Ott et al., 2012) has explored characteristics of positive deceptive opinion spam, the complementary problem of negative deceptive opinion spam remains largely unstudied.",
  "y": "motivation"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_4",
  "x": "Following the framework of <cite>Ott et al. (2011)</cite> , we use Amazon's Mechanical Turk service to produce the first publicly available 1 dataset of negative deceptive opinion spam, containing 400 gold standard deceptive negative reviews of 20 popular Chicago hotels. To validate the credibility of our deceptive reviews, we show that human deception detection performance on the negative reviews is low, in agreement with decades of traditional deception detection research (Bond and DePaulo, 2006) . We then show that standard n-gram text categorization techniques can be used to detect negative deceptive opinion spam with approximately 86% accuracy -far surpassing that of the human judges.",
  "y": "similarities uses"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_5",
  "x": "In conjunction with <cite>Ott et al. (2011)</cite> 's positive deceptive opinion spam dataset, we then explore the interaction between sentiment and deception with respect to three types of language features: (1) changes in first-person singular use, often attributed to psychological distancing (Newman et al., 2003) , (2) decreased spatial awareness and more narrative form, consistent with theories of reality monitoring (Johnson and Raye, 1981) and imaginative writing (Biber et al., 1999; Rayson et al., 2001) , and (3) increased negative emotion terms, often attributed to leakage cues (Ekman and Friesen, 1969) , but perhaps better explained in our case as an exaggeration of the underlying review sentiment. ---------------------------------- **DATASET**",
  "y": "uses"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_6",
  "x": "**DATASET** One of the biggest challenges facing studies of deception is obtaining labeled data. Recently, <cite>Ott et al. (2011)</cite> have proposed an approach for generating positive deceptive opinion spam using Amazon's popular Mechanical Turk crowdsourcing service.",
  "y": "motivation"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_7",
  "x": "In this section we discuss our efforts to extend <cite>Ott et al. (2011)</cite> 's dataset to additionally include negative deceptive opinion spam. ---------------------------------- **DECEPTIVE REVIEWS FROM MECHANICAL TURK**",
  "y": "extends"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_8",
  "x": "Deceptive negative reviews are gathered from Mechanical Turk using the same procedure as <cite>Ott et al. (2011)</cite> . In particular, we create and divide 400 HITs evenly across the 20 most popular hotels in Chicago, such that we obtain 20 reviews for each hotel. We allow workers to complete only a single HIT each, so that each review is written by a unique worker.",
  "y": "uses"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_9",
  "x": "5 The average accepted review length was 178 words, higher than for the positive reviews gathered by <cite>Ott et al. (2011)</cite> , who report an average review length of 116 words. ---------------------------------- **TRUTHFUL REVIEWS FROM THE WEB**",
  "y": "differences"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_10",
  "x": "Following <cite>Ott et al. (2011)</cite> , we sample a subset of the available truthful reviews so that we retain an equal number of truthful and deceptive reviews (20 each) for each hotel. However, because the truthful reviews are on average longer than our deceptive reviews, we sample the truthful reviews according to a log-normal distribution fit to the lengths of our deceptive reviews, similarly to <cite>Ott et al. (2011)</cite> Table 1 : Deception detection performance, incl. (P)recision, (R)ecall, and (F)1-score, for three human judges and two meta-judges on a set of 160 negative reviews.",
  "y": "similarities"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_11",
  "x": "However, because the truthful reviews are on average longer than our deceptive reviews, we sample the truthful reviews according to a log-normal distribution fit to the lengths of our deceptive reviews, similarly to <cite>Ott et al. (2011)</cite> Table 1 : Deception detection performance, incl. (P)recision, (R)ecall, and (F)1-score, for three human judges and two meta-judges on a set of 160 negative reviews. The largest value in each column is indicated with boldface.",
  "y": "similarities"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_12",
  "x": "Indeed, <cite>Ott et al. (2011)</cite> found that two out of three human judges were unable to perform statistically significantly better than chance (at the p < 0.05 level) at detecting positive deceptive opinion spam. Nevertheless, it is important to subject our reviews to human judgments to validate their convincingness. In particular, if human detection performance is found to be very high, then it would cast doubt on the usefulness of the Mechanical Turk approach for soliciting gold standard deceptive opinion spam.",
  "y": "background"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_13",
  "x": "Following <cite>Ott et al. (2011)</cite> , we asked three volunteer undergraduate university students to read and make assessments on a subset of the negative review dataset described in Section 2. Specifically, we randomized all 40 deceptive and truthful reviews from each of four hotels (160 reviews total). We then asked the volunteers to read each review and mark whether they believed it to be truthful or deceptive.",
  "y": "similarities uses"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_14",
  "x": "Standard n-gram-based text categorization techniques have been shown to be effective at detecting deception in text (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009; <cite>Ott et al., 2011</cite>; Feng et al., 2012) . Following <cite>Ott et al. (2011)</cite> , we evaluate the performance of linear Support Vector Machine (SVM) classifiers trained with unigram and bigram term-frequency features on our novel negative deceptive opinion spam dataset. We employ the same 5-fold stratified cross-validation (CV) procedure as <cite>Ott et al. (2011)</cite> , whereby for each cross-validation iteration we train our model on all reviews for 16 hotels, and test our model on all reviews for the remaining 4 hotels.",
  "y": "background"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_15",
  "x": "Standard n-gram-based text categorization techniques have been shown to be effective at detecting deception in text (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009; <cite>Ott et al., 2011</cite>; Feng et al., 2012) . Following <cite>Ott et al. (2011)</cite> , we evaluate the performance of linear Support Vector Machine (SVM) classifiers trained with unigram and bigram term-frequency features on our novel negative deceptive opinion spam dataset. We employ the same 5-fold stratified cross-validation (CV) procedure as <cite>Ott et al. (2011)</cite> , whereby for each cross-validation iteration we train our model on all reviews for 16 hotels, and test our model on all reviews for the remaining 4 hotels.",
  "y": "uses"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_16",
  "x": "Following <cite>Ott et al. (2011)</cite> , we evaluate the performance of linear Support Vector Machine (SVM) classifiers trained with unigram and bigram term-frequency features on our novel negative deceptive opinion spam dataset. We employ the same 5-fold stratified cross-validation (CV) procedure as <cite>Ott et al. (2011)</cite> , whereby for each cross-validation iteration we train our model on all reviews for 16 hotels, and test our model on all reviews for the remaining 4 hotels. The SVM cost parameter, C, is tuned by nested cross-validation on the training data.",
  "y": "uses"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_18",
  "x": "For example, fake positive reviews included less spatial language (e.g., floor, small, location, etc.) because individuals who had not actually experienced the hotel simply had less spatial detail available for their review (Johnson and Raye, 1981) . This was also the case for our negative reviews, with less spatial language observed for fake negative reviews relative to truthful. Likewise, our fake negative reviews had more verbs relative to nouns than truthful, suggesting a more narrative style that is indicative of imaginative writing (Biber et al., 1999; Rayson et al., 2001 ), a pattern also observed by <cite>Ott et al. (2011)</cite> .",
  "y": "similarities"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_19",
  "x": "Second, the effect of deception on the pattern of pronoun frequency was not the same across positive and negative reviews. In particular, while first person singular pronouns were produced more frequently in fake reviews than truthful, consistent with the case for positive reviews, the increase was diminished in the negative reviews examined here. In the positive reviews reported by <cite>Ott et al. (2011)</cite> , the rate of first person singular in fake reviews (M=4.36%, SD=2.96%) was twice the rate observed in truthful reviews (M=2.18%, SD=2.04%).",
  "y": "background"
 },
 {
  "id": "5fe872d8e15ac38f845bc244f7bf5f_20",
  "x": "We have created the first publicly-available corpus of gold standard negative deceptive opinion spam, containing 400 reviews of 20 Chicago hotels, which we have used to compare the deception detection capabilities of untrained human judges and standard n-gram-based Support Vector Machine classifiers. Our results demonstrate that while human deception detection performance is greater for negative rather than positive deceptive opinion spam, the best detection performance is still achieved through automated classifiers, with approximately 86% accuracy. We have additionally explored, albeit briefly, the relationship between sentiment and deception by utilizing <cite>Ott et al. (2011)</cite> 's positive deceptive opinion spam dataset in conjunction with our own.",
  "y": "uses"
 },
 {
  "id": "603f49fc6ecf90da67a9a55986f217_0",
  "x": "---------------------------------- **INTRODUCTION** Distributed word representations, also known as word embeddings, are low-dimensional vector representations for words that capture semantic aspects (Bengio et al., 2003; Pennington et al., 2014; <cite>Mikolov et al., 2013a)</cite> .",
  "y": "background"
 },
 {
  "id": "603f49fc6ecf90da67a9a55986f217_1",
  "x": "These methods were used to learn entity and relation embeddings from a large collection of relation triplets for the task of knowledge base completion. Since these methods did not use any co-occurrence information from a text corpus, all entities were required to appear at least once in the training data, ruling out generalization to unseen entities 1 . More recently, Xu et al. (2014) combined the training objective of SKIP-GRAM<cite> (Mikolov et al., 2013a)</cite> with the training objective of (Bordes et al., 2013) to incorporate lexical 1 There exists work on relation extraction and knowledgebase completion that combines structured relation triplets and logical rules with unstructured text using various forms of latent variable models (Riedel et al., 2013; Chang et al., 2014; Toutanova et al., 2015; Rockt\u00e4schel et al., 2015) .",
  "y": "background"
 },
 {
  "id": "603f49fc6ecf90da67a9a55986f217_2",
  "x": "We demonstrate effectiveness of the learned word representations on the tasks of knowledge-base completion and word analogy. 2 Subspace-regularized word embedding Although our proposed framework for relational modeling is general enough to use with any existing word embedding method, we work with Word2Vec model<cite> (Mikolov et al., 2013a)</cite> in this paper for illustrating our ideas and later for empirical evaluations. Word2Vec is a neural network model trained on sequence of words and its hidden layer activations can be read out as the word representations.",
  "y": "uses"
 },
 {
  "id": "603f49fc6ecf90da67a9a55986f217_3",
  "x": "Two variants were proposed in<cite> (Mikolov et al., 2013a</cite> ) -SKIP-GRAM, which maximizes the log likelihood of the local context words given the target word, and CBOW, which maximizes the log likelihood of the target word given its local context. More specifically, CBOW maximizes the objective where w t+c t\u2212c represents the words (or tokens) in the local context window around the t'th word (or token) and v t = \u2212c\u2264i\u2264c,i =0 w t+i can be seen as the average context vector.",
  "y": "background"
 },
 {
  "id": "603f49fc6ecf90da67a9a55986f217_4",
  "x": "Two variants were proposed in<cite> (Mikolov et al., 2013a</cite> ) -SKIP-GRAM, which maximizes the log likelihood of the local context words given the target word, and CBOW, which maximizes the log likelihood of the target word given its local context. We report empirical results with CBOW since it was computationally faster than SKIP-GRAM while giving similar results in our early explorations.",
  "y": "uses"
 },
 {
  "id": "603f49fc6ecf90da67a9a55986f217_5",
  "x": "Google word analogy data<cite> (Mikolov et al., 2013a)</cite> contains 19544 analogy relations (14 relation types -5 semantic, 9 syntactic) of the form a:b::c:d constructed from 550 unique relation triplets. We use this data only for evaluation (test phase). WordRep contains a large collection of relation triplets (44584 triplets in total, 25 relation types -18 semantic, 7 syntactic) extracted from WordNet, Wikipedia and Dictionary.",
  "y": "background"
 },
 {
  "id": "603f49fc6ecf90da67a9a55986f217_6",
  "x": "Google word analogy data<cite> (Mikolov et al., 2013a)</cite> contains 19544 analogy relations (14 relation types -5 semantic, 9 syntactic) of the form a:b::c:d constructed from 550 unique relation triplets. We use this data only for evaluation (test phase). WordRep contains a large collection of relation triplets (44584 triplets in total, 25 relation types -18 semantic, 7 syntactic) extracted from WordNet, Wikipedia and Dictionary.",
  "y": "uses"
 },
 {
  "id": "603f49fc6ecf90da67a9a55986f217_7",
  "x": "We compare the proposed RELSUB model with two methods: (i) CBOW<cite> (Mikolov et al., 2013a)</cite> , and (ii) RELCONST which is based on constant translation model for relations which was originally proposed in (Bordes et al., 2013) for embedding knowledge-bases and was recently used by Table 2 : Google analogy data: Accuracy on word analogy task (Xu et al., 2014) for learning word embeddings. Our objective for RELCONST is same as Eq. 4 except that {\u03b1 k } m k=1 are set equal to the vector of all 1's and norm constraint on u k are removed. This enables us to directly test the merit of the proposed rank-1 subspace relational model over that of constant translational model in the same regularization framework.",
  "y": "uses"
 },
 {
  "id": "603f49fc6ecf90da67a9a55986f217_8",
  "x": "Relaxing the constant translation to rank-1 subspace assumption results in significant improvements on this task. In the analogy task, we want to predict the missing word in an analogy tuple a:b::c:?. We use the Google word-analogy data<cite> (Mikolov et al., 2013a)</cite> for this evaluation.",
  "y": "uses"
 },
 {
  "id": "603f49fc6ecf90da67a9a55986f217_9",
  "x": "We compare the proposed RELSUB model with two methods: (i) CBOW<cite> (Mikolov et al., 2013a)</cite> , and (ii) RELCONST which is based on constant translation model for relations which was originally proposed in (Bordes et al., 2013) for embedding knowledge-bases and was recently used by Table 2 : Google analogy data: Accuracy on word analogy task (Xu et al., 2014) for learning word embeddings. We observe considerable gains with RELSUB over CBOW for semantic categories.",
  "y": "differences"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_0",
  "x": "Deep neural network approaches have recently been successfully developed for several educational applications, including automated essay assessment. In several cases, neural network approaches exceeded the previous state of the art on essay scoring<cite> (Taghipour and Ng, 2016)</cite> . The task of automated essay scoring (AES) is generally different from the task of automated short answer scoring (SAS).",
  "y": "background"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_1",
  "x": "Nevertheless, deep learning approaches to AES have thus far demonstrated strong performance with minimal inputs consisting of unigrams and word embeddings. For example,<cite> Taghipour and Ng (2016)</cite> explore simple LSTM and CNN-based architectures with regression and evaluate on the ASAP-AES data. Alikaniotis et al. (2016) train score-specific word embeddings with several LSTM architectures.",
  "y": "background"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_2",
  "x": "On the other hand, recurrent neural networks may derive some of their predictive power in AES from more redundant signals in longer input sequences (as sketched by<cite> Taghipour and Ng (2016)</cite> ). As a result, the shorter responses in SAS may hinder the ability of recurrent networks to achieve state-of-the-art results. To explore the effectiveness of neural network architectures on SAS, we use the basic architecture and parameters of<cite> Taghipour and Ng (2016)</cite> on three publicly available short answer datasets: ASAP-SAS (Shermis, 2015), Powergrading (Basu et al., 2013) , and SRA (Dzikovska et al., 2016 (Dzikovska et al., , 2013 .",
  "y": "background"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_3",
  "x": "On the other hand, recurrent neural networks may derive some of their predictive power in AES from more redundant signals in longer input sequences (as sketched by<cite> Taghipour and Ng (2016)</cite> ). As a result, the shorter responses in SAS may hinder the ability of recurrent networks to achieve state-of-the-art results. To explore the effectiveness of neural network architectures on SAS, we use the basic architecture and parameters of<cite> Taghipour and Ng (2016)</cite> on three publicly available short answer datasets: ASAP-SAS (Shermis, 2015), Powergrading (Basu et al., 2013) , and SRA (Dzikovska et al., 2016 (Dzikovska et al., , 2013 .",
  "y": "motivation"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_4",
  "x": "As a result, the shorter responses in SAS may hinder the ability of recurrent networks to achieve state-of-the-art results. To explore the effectiveness of neural network architectures on SAS, we use the basic architecture and parameters of<cite> Taghipour and Ng (2016)</cite> on three publicly available short answer datasets: ASAP-SAS (Shermis, 2015), Powergrading (Basu et al., 2013) , and SRA (Dzikovska et al., 2016 (Dzikovska et al., , 2013 . While these datasets differ with respect to the length and complexity of student responses, all prompts in the datasets focus on content accuracy.",
  "y": "uses"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_5",
  "x": "To explore the effectiveness of neural network architectures on SAS, we use the basic architecture and parameters of<cite> Taghipour and Ng (2016)</cite> on three publicly available short answer datasets: ASAP-SAS (Shermis, 2015), Powergrading (Basu et al., 2013) , and SRA (Dzikovska et al., 2016 (Dzikovska et al., , 2013 . While these datasets differ with respect to the length and complexity of student responses, all prompts in the datasets focus on content accuracy. We explore how well the optimal parameters for AES from<cite> Taghipour and Ng (2016)</cite> fare on these datasets, and whether different architectures and parameters perform better on the SAS task.",
  "y": "uses"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_6",
  "x": "We carried out a series of experiments across datasets to discern the effect of specific parameters in the SAS setting. We took the best parameter set from<cite> Taghipour and Ng (2016)</cite> as our reference since it performed best on the AES data. We looked at the effect of varying several important parameters to discern the effectiveness of each for SAS:",
  "y": "uses"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_8",
  "x": "We work with the basic neural network architecture explored by<cite> Taghipour and Ng (2016)</cite> (Figure  4 ). 3 First, the word tokens of each response are converted to embeddings. Optionally, features are extracted from the embeddings by a convolutional network layer.",
  "y": "uses"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_9",
  "x": "The MoT layer simply averages the hidden states of the LSTM across the input. We use the same attention mechanism employed in<cite> Taghipour and Ng (2016)</cite> , which involves taking the dot product of each LSTM hidden state and a vector that is trained with the network. The aggregation layer output is a single vector, which is input to a fully connected layer.",
  "y": "uses"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_10",
  "x": "The text is lightly preprocessed as input to the neural networks following<cite> Taghipour and Ng (2016)</cite> . The text is tokenized with the standard NLTK tokenizer and lowercased. All numbers are mapped to a single <num> symbol.",
  "y": "uses"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_11",
  "x": "Following<cite> Taghipour and Ng (2016)</cite> , for our parameter exploration experiments on the development set, we report the best performance across epochs. When we train final models on the combined training and development set and evaluate on the test set, we report the results from the last epoch. During development, we observed that even after employing best practices for ensuring repro-ducibility of results 5 , there was still some small variation between runs of the same parameter settings.",
  "y": "uses"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_12",
  "x": "**PARAMETER EXPLORATION RESULTS** Our focus in this section is comparing different architecture and parameter choices for the neural networks with the best parameters from<cite> Taghipour and Ng (2016)</cite> . Table 2 shows the results of our experiments on the development set for ASAP-SAS and Powergrading, and Table 3 shows the corresponding results for SRA.",
  "y": "uses"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_13",
  "x": "The mean-over-time layer performs relatively well across datasets, but achieves the best results only on the SRA-SEB dataset. We hypothesized that the mean-over-time layer is helpful when the input consists of longer responses (as was the case for the essay data in<cite> Taghipour and Ng (2016)</cite> ). We computed the Pearson's correlation on the ASAP-SAS data between the difference on each prompt of the two conditions and the mean response length in the development set.",
  "y": "background"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_14",
  "x": "We hypothesized that the mean-over-time layer is helpful when the input consists of longer responses (as was the case for the essay data in<cite> Taghipour and Ng (2016)</cite> ). We computed the Pearson's correlation on the ASAP-SAS data between the difference on each prompt of the two conditions and the mean response length in the development set. However, the correlation was modest at 0.437.",
  "y": "differences"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_15",
  "x": "\"wF1\" is the weighted F1 score. \"Baseline\" is the baseline non-neural system. \"T&N best\" is the best-performing parameter set in <cite>Taghipour & Ng (2016)</cite> : tuned embeddings (here, GLOVE 100 dimensions), 300-dimensional LSTM, unidirectional, mean-over-time layer. Scores are bolded if they outperform the score for the \"T&N best\" parameter setting.",
  "y": "background"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_16",
  "x": "\"Baseline\" is the baseline non-neural system. \"T&N best\" is the best-performing parameter set in<cite> Taghipour and Ng (2016)</cite> : tuned embeddings (here, GLOVE 100 dimensions), 300-dimensional LSTM, unidirectional, mean-over-time layer. Scores are bolded if they outperform the score for the \"T&N best\" parameter setting. \u2022 Powergrading: CNN features with window length 5, 150-dimensional bidirectional LSTM, attention mechanism",
  "y": "background"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_17",
  "x": "Mean-over-time produced competitive results on many prompts, but contrary to<cite> Taghipour and Ng (2016)</cite> , bidirectional LSTMS and attention produced some of the best results, which is consistent with results for neural models on other text classification tasks (e.g., Longpre et al. (2016) ). Research is needed to explain these emerging differences in effective neural architectures for AES vs. SAS, including model-specific factors such as the interaction of an LSTM's integration of features over time and the redundancy of predictive signals in essays vs. short answers, along with data-specific factors such as the consistency of human scoring, the demands of different rubrics, and the homogeneity or diversity of prompts in each setting. At the same time, different from the AES task, the family of neural architectures explored here needs further augmenting to achieve state-of-the-art results on the SAS task.",
  "y": "differences"
 },
 {
  "id": "60b0b54af27a6b04a6708a60834952_18",
  "x": "Mean-over-time produced competitive results on many prompts, but contrary to<cite> Taghipour and Ng (2016)</cite> , bidirectional LSTMS and attention produced some of the best results, which is consistent with results for neural models on other text classification tasks (e.g., Longpre et al. (2016) ). Research is needed to explain these emerging differences in effective neural architectures for AES vs. SAS, including model-specific factors such as the interaction of an LSTM's integration of features over time and the redundancy of predictive signals in essays vs. short answers, along with data-specific factors such as the consistency of human scoring, the demands of different rubrics, and the homogeneity or diversity of prompts in each setting. At the same time, different from the AES task, the family of neural architectures explored here needs further augmenting to achieve state-of-the-art results on the SAS task.",
  "y": "future_work"
 },
 {
  "id": "60cc075e5351a756de8f9919d5a84e_1",
  "x": "A packed chart is used to efficiently represent all of the possible analyses for a sentence, and the CKY chart parsing algorithm described in Steedman (2000) is used to build the chart. <cite>Clark and Curran (2004)</cite> evaluate a number of log-linear parsing models for CCG. In this paper we use the normal-form model, which defines probabilities with the conditional log-linear form in (1), where y is a derivation and x is a sentence.",
  "y": "background"
 },
 {
  "id": "60cc075e5351a756de8f9919d5a84e_2",
  "x": "The feature set we use is from the best performing normal-form model in <cite>Clark and Curran (2004)</cite> . For a given sentence the output of the parser is a dependency structure corresponding to the most probable derivation, which can be found using the Viterbi algorithm. The dependency relations are defined in terms of the argument slots of CCG lexical categories.",
  "y": "uses"
 },
 {
  "id": "60cc075e5351a756de8f9919d5a84e_4",
  "x": "**MODEL ESTIMATION** In <cite>Clark and Curran (2004)</cite> we describe a discriminative method for estimating the parameters of a log-linear parsing model. The estimation method maximises the following objective function:",
  "y": "background"
 },
 {
  "id": "60cc075e5351a756de8f9919d5a84e_5",
  "x": "In <cite>Clark and Curran (2004)</cite> we describe efficient methods for performing the calculations using packed charts. However, a very large amount of memory is still needed to store the packed charts for the complete training data even though the representation is very compact; in we report a memory usage of 30 GB. To handle this we have developed a parallel implementation of the estimation algorithm which runs on a Beowulf cluster.",
  "y": "motivation"
 },
 {
  "id": "60cc075e5351a756de8f9919d5a84e_6",
  "x": "In <cite>Clark and Curran (2004)</cite> we show that the parsing model resulting from training data generated in this way produces state-of-the-art CCG dependency recovery: 84.6 F-score over labelled dependencies. The final row corresponds to a more restrictive setting on the supertagger, in which a value of \u03b2 = 0.05 is used initially and \u03b2 = 0.1 is used if the node limit is exceeded. The two types of normalform constraints are also used.",
  "y": "background"
 },
 {
  "id": "60cc075e5351a756de8f9919d5a84e_7",
  "x": "In <cite>Clark and Curran (2004)</cite> we show that using this more restrictive setting has a small negative impact on the accuracy of the resulting parser (about 0.6 F-score over labelled dependencies). However, the memory requirement for training the model is now only 4 GB, a reduction of 87% compared with the original approach. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "60cc075e5351a756de8f9919d5a84e_8",
  "x": "---------------------------------- **RESULTS (PARSE TIMES)** The results in this section are all using the best performing normal-form model in <cite>Clark and Curran (2004)</cite> , which corresponds to row 3 in Table 3 .",
  "y": "uses"
 },
 {
  "id": "60cc075e5351a756de8f9919d5a84e_9",
  "x": "This configuration of the system would be useful for obtaining data for lexical knowledge acquisition, for example, for which large amounts of data are required. The oracle row shows the parser speed when it is provided with only the correct lexical categories. The parser is extremely fast, and in <cite>Clark and Curran (2004)</cite> we show that the F-score for labelled dependencies is almost 98%.",
  "y": "background"
 },
 {
  "id": "60d39eec9573e42d7fb306b0f696c7_0",
  "x": "With state-of-the-art empirical results, most regard BiLSTM-CNN as a robust core module for sequence-labeling NER [1,<cite> 2,</cite> 3, 4, 5] . However, each direction of BiLSTM only sees and encodes half of a sequence at each time step. For each token, the forward LSTM only encodes past context; the backward LSTM only encodes future context.",
  "y": "background"
 },
 {
  "id": "60d39eec9573e42d7fb306b0f696c7_1",
  "x": "With state-of-the-art empirical results, most regard BiLSTM-CNN as a robust core module for sequence-labeling NER [1,<cite> 2,</cite> 3, 4, 5] . However, each direction of BiLSTM only sees and encodes half of a sequence at each time step. This paper explores two types of cross-structures to help cope with the problem: Cross-BiLSTM-CNN and Att-BiLSTM-CNN.",
  "y": "background motivation"
 },
 {
  "id": "60d39eec9573e42d7fb306b0f696c7_2",
  "x": "**(BASELINE-)BILSTM-CNN** For Baseline <cite>[2]</cite> , a CNN is used to compute character-level word features alongside word embedding and multi-layer BiLSTM is used to capture the future and the past for each time step: The probability of each token class is given by affine-softmax.",
  "y": "uses"
 },
 {
  "id": "60d39eec9573e42d7fb306b0f696c7_3",
  "x": "For Baseline <cite>[2]</cite> , a CNN is used to compute character-level word features alongside word embedding and multi-layer BiLSTM is used to capture the future and the past for each time step: The probability of each token class is given by affine-softmax. Using OSBIE sequential labels <cite>[2]</cite> , when there are P entity types, the number of token classes d p = P \u00d7 4 + 1.",
  "y": "uses"
 },
 {
  "id": "60d39eec9573e42d7fb306b0f696c7_4",
  "x": "Table 1 shows overall results. Besides Baseline-, Cross-, and Att-BiLSTM-CNN, results of bare-bone BiLSTM-CNN <cite>[2]</cite> , CRF-BiLSTM(-BiLSTM) [11, 12] , and CRF-IDCNN [11] from the literature are also listed. The models proposed in this paper surpassed previous reported bare-bone models by 1.4% on OntoNotes and 4.6% on WNUT.",
  "y": "uses"
 },
 {
  "id": "60f4e3a8c8cae1b1ba2620b11ae6b0_0",
  "x": "As these models become a staple component of many NLP tasks, it is crucial to understand what kind of knowledge they learn, and why and when they perform well. To that end, researchers have investigated the linguistic knowledge that these models learn by analyzing BERT (Goldberg, 2018; Lin et al., 2019) directly or training probing classifiers on the contextualized embeddings or attention heads of BERT (Tenney et al., 2019b,a;<cite> Hewitt and Manning, 2019)</cite> . BERT and RoBERTa, as Transformer models (Vaswani et al., 2017) , compute the hidden representation of all the attention heads at each layer for each token by attending to all the token representations in the preceding layer.",
  "y": "background"
 },
 {
  "id": "60f4e3a8c8cae1b1ba2620b11ae6b0_1",
  "x": "Goldberg (2018) analyzes the contextualized embeddings of BERT by computing language model surprisal on subject-verb agreement and shows that BERT learns significant knowledge of syntax. Tenney et al. (2019b) introduce a probing classifier for evaluating syntactic knowledge in BERT and show that BERT encodes syntax more than semantics. <cite>Hewitt and Manning (2019)</cite> train a structural probing model that maps the hidden representations of each token to an inner-product space that corresponds to syntax tree distance.",
  "y": "background"
 },
 {
  "id": "60f4e3a8c8cae1b1ba2620b11ae6b0_2",
  "x": "As in Raganato and Tiedemann, we take the root of the gold dependency tree as the starting node and apply the Chu-Liu-Edmonds algorithm (Chu, 1965; Edmonds, 1967) to compute the maximum spanning tree. (Using the gold root as the starting point in MST may artificially improve our results slightly, but this bias is applied evenly across all the models we compare.) The resulting tree is a valid directed dependency tree, though we follow <cite>Hewitt and Manning (2019)</cite> in evaluating it as undirected, for easier comparison with our MAX method. Following Voita et al. (2019) , we exclude the sentence demarcation tokens ([CLS], [SEP], <s>, </s>) from the attention matrices.",
  "y": "uses"
 },
 {
  "id": "60f4e3a8c8cae1b1ba2620b11ae6b0_3",
  "x": "We then apply either of the two extraction methods to the attention matrix. During evaluation when we compare the gold dependencies, to handle the subtokens within the merged tokens, we set all subtokens except for the first to depend on the first subtoken. This approach is largely similar to that in <cite>Hewitt and Manning (2019)</cite> .",
  "y": "similarities"
 },
 {
  "id": "60f4e3a8c8cae1b1ba2620b11ae6b0_4",
  "x": "---------------------------------- **RESULTS** Overall, the results of both analysis methods suggest that, although some attention heads of BERT capture specific dependency relation types, they do not reflect the full extent of the significant amount of syntactic knowledge BERT and RoBERTa are known to learn as shown in previous syntactic probing work (Tenney et al., 2019b;<cite> Hewitt and Manning, 2019)</cite> .",
  "y": "differences"
 },
 {
  "id": "60f54cf8f510affe214f63f8e23e19_0",
  "x": "This problem is particularly relevant for verbal MWEs, which present high morphological and syntactic variability . Our goal is to study the impact of word representations on verbal MWE (VMWE) identification, comparing lemmas, surface forms, traditional word embeddings and subword representations. We compare the performance of an off-the-shelf MWE identification system based on neural sequence tagging <cite>(Zampieri et al., 2018)</cite> using lemmas and surface forms as input features, encoded in the form of classical pre-initialised word2vec embeddings (Mikolov et al., 2013) or, alternatively, using new-generation FastText embeddings built from character n-grams (Bojanowski et al., 2017) .",
  "y": "uses"
 },
 {
  "id": "60f54cf8f510affe214f63f8e23e19_1",
  "x": "In other tasks such as named entity recognition, character convolution layers have been successfully applied (Ma and Hovy, 2016) . The use of pre-trained vs. randomly initialised embeddings has been analysed in some PARSEME shared task papers (Ehren et al., 2018;<cite> Zampieri et al., 2018)</cite> . The closest works to ours are the Veyn <cite>(Zampieri et al., 2018)</cite> and SHOMA (Taslimipoor and Rohanian, 2018) systems, submitted to the PARSEME shared task 1.1.",
  "y": "uses"
 },
 {
  "id": "60f54cf8f510affe214f63f8e23e19_2",
  "x": "The use of pre-trained vs. randomly initialised embeddings has been analysed in some PARSEME shared task papers (Ehren et al., 2018;<cite> Zampieri et al., 2018)</cite> . The closest works to ours are the Veyn <cite>(Zampieri et al., 2018)</cite> and SHOMA (Taslimipoor and Rohanian, 2018) systems, submitted to the PARSEME shared task 1.1. Veyn is used as our off-the-shelf base system, so most of its architecture is identical to ours.",
  "y": "similarities"
 },
 {
  "id": "60f54cf8f510affe214f63f8e23e19_3",
  "x": "**MWE IDENTIFICATION SYSTEM** We use our inhouse MWE identification system Veyn <cite>(Zampieri et al., 2018)</cite> , based on sequence tagging using recurrent neural networks. 5 The system takes as input the concatenation of the embeddings of the words' features (e.g. lemmas and POS).",
  "y": "uses"
 },
 {
  "id": "616e8732490f0fa87d35998f769196_0",
  "x": "In addition, in the semantics domain, the use of a new TAG operation, flexible composition, is used to perform certain semantic operations that seemingly cannot be modeled with TL-MCTAG alone<cite> (Chiang and Scheffler, 2008)</cite> and in work in synchronous TAG semantics, constructions such as nested quantifiers require a set-local MCTAG (SL-MCTAG) analysis (Nesson and Shieber, 2006) . In this paper we suggest a shift in focus from constraining locality and complexity through restrictions that all trees in a tree set must adjoin within a single tree or tree set to constraining locality and complexity through restrictions on the derivational distance between trees in the same tree set in a valid derivation. We examine three formalisms, two of them introduced in this work for the first time, that use derivational distance to constrain locality and demonstrate by construction of parsers their relationship to TL-MCTAG in both expressivity and complexity.",
  "y": "background"
 },
 {
  "id": "616e8732490f0fa87d35998f769196_1",
  "x": "In Section 7 we recall the delayed TL-MCTAG formalism introduced by<cite> Chiang and Scheffler (2008)</cite> and define a CKY-style parser for it as well. In Section 8 we explore the complexity of all three parsers and the relationship between the formalisms. In Section 9 we discuss the linguistic applications of these formalisms and show that they permit analyses of some of the hard cases that have led researchers to look beyond TL-MCTAG.",
  "y": "uses"
 },
 {
  "id": "616e8732490f0fa87d35998f769196_2",
  "x": "For most linguistic applications of TAG, this requirement seems natural and is strictly obeyed. There are a few applications, including flexible composition and scrambling in free-word order languages that benefit from TAG-based grammars that drop the simultaneity requirement<cite> (Chiang and Scheffler, 2008</cite>; Rambow, 1994) . From a complexity perspective, however, checking the simultaneity requirement is expensive (Kallmeyer, 2007) .",
  "y": "background"
 },
 {
  "id": "616e8732490f0fa87d35998f769196_3",
  "x": "The side conditions for the Adjoin non-base rule enforce that the dominance constraints are satisfied and that the derivational distance from the base of a tree vector to its currently highest adjoined tree is maintained accurately. We note that in order to allow a non-total ordering of the trees in a vector we would simply have to record all trees in a tree vector in the histories as is done in the delayed TL-MCTAG parser. 7 Delayed TL-MCTAG<cite> Chiang and Scheffler (2008)</cite> introduce the delayed TL-MCTAG formalism which makes use of a derivational distance restriction in a somewhat different way.",
  "y": "background"
 },
 {
  "id": "616e8732490f0fa87d35998f769196_4",
  "x": "Borrowing directly from<cite> Chiang and Scheffler (2008)</cite> , Figure 7 gives two examples. Parsing for delayed TL-MCTAG is not discussed by<cite> Chiang and Scheffler (2008)</cite> but can be accomplished using a similar CKY-style strategy as in the two parsers above. We present a parser in Figure 6 .",
  "y": "uses"
 },
 {
  "id": "616e8732490f0fa87d35998f769196_5",
  "x": "Parsing for delayed TL-MCTAG is not discussed by<cite> Chiang and Scheffler (2008)</cite> but can be accomplished using a similar CKY-style strategy as in the two parsers above. We present a parser in Figure 6 . Rather than keeping histories that record derivational distance, we keep an active delay list for each item that records the delays that are active (by recording the identities of the trees that have adjoined) for the tree of which the current node is a part.",
  "y": "differences"
 },
 {
  "id": "6293d300ab46a6d6135ed256005403_0",
  "x": "Zhang et al. (2014) proposed a triple-based document enrichment framework which uses triples of SPO as background knowledge. They first proposed a search enginebased method to evaluate the relatedness between every pair of triples, and then an iterative propagation algorithm was introduced to select the most relevant triples to a given source document (see Section 2), which achieved a good performance. However, to evaluate the semantic relatedness between two triples,<cite> Zhang et al. (2014)</cite> primarily relied on the text of triples and used search engines, which makes their method difficult to re-implement and in turn limits its application in practice.",
  "y": "background"
 },
 {
  "id": "6293d300ab46a6d6135ed256005403_1",
  "x": "However, to evaluate the semantic relatedness between two triples,<cite> Zhang et al. (2014)</cite> primarily relied on the text of triples and used search engines, which makes their method difficult to re-implement and in turn limits its application in practice. Moreover, they did not carry out any task-based evaluation, which makes it uncertain whether their method will be helpful in real applications. Therefore, we instead use topic models, especially Latent Dirichlet Allocation (LDA), to encode distributional semantics of words and convert every triple into a real-valued vector, which is then used to evaluate the relatedness between a pair of triples. We then incorporate these triples into the given source document and represent them together as a graph of triples. Then a modified iterative propagation is carried out over the entire graph to select the most relevant triples of background knowledge to the given source document.",
  "y": "differences background motivation"
 },
 {
  "id": "6293d300ab46a6d6135ed256005403_2",
  "x": "**BACKGROUND** The most closely related work in this area is our own <cite>(Zhang et al., 2014)</cite> , which used the triples of SPO as background knowledge. In that work, we first proposed a triple graph to represent the source document and then used a search enginebased iterative algorithm to rank all the triples.",
  "y": "background"
 },
 {
  "id": "6293d300ab46a6d6135ed256005403_3",
  "x": "Triple graph<cite> Zhang et al. (2014)</cite> proposed the triple graph as a document representation, where the triples of SPO serve as nodes, and the edges between nodes indicate their semantic relatedness. There are two kinds of nodes in the triple graph: (1) source document nodes (sd-nodes), which are triples extracted from source documents, and (2) background knowledge nodes (bk-nodes), which are triples extracted from external sources. Both of them are extracted automatically with Reverb, a well-known Open Information Extraction system (Etzioni et al., 2011) . There are also two kinds of edges: (1) an edge between a pair of sd-nodes, and (2) an edge between one sd-node and another bk-node, both of which are unidirectional. In the original representation, there are no edges between two bk-nodes because they treat the bk-nodes as recipients of relevance weight only. In this paper, we modify this setup and connect every pair of bknodes with an edge, so the bk-nodes serve as intermediate nodes during the iterative propagation process and contribute to the final performance too as shown in our experiments (see Section 5.1).",
  "y": "differences background"
 },
 {
  "id": "6293d300ab46a6d6135ed256005403_4",
  "x": "Relevance evaluation To compute the weight of a edge,<cite> Zhang et al. (2014)</cite> evaluate the semantic relatedness between two nodes with a search engine-based method. They first convert every node, which is a triple of SPO, into a query by combining the text of Subject and Object together. Then for every pair of nodes t i and t j , they construct three queries: p, q, and p \u2229 q, which correspond to the queries of t i , t j , and t j \u2229 t j , the combination of t i and t j . All these queries are put into a search engine to get H(p), H(q), and H(p \u2229 q), the numbers of returned pages for query p, p, and p \u2229 q. Then the WebJaccard Coefficient (Bollegala et al., 2007 ) is used to evaluate r(i, j), the relatedness between t i and t j , according to Formula 1. otherwise.",
  "y": "background"
 },
 {
  "id": "6293d300ab46a6d6135ed256005403_5",
  "x": "Using r(i, j),<cite> Zhang et al. (2014)</cite> further define p(i, j), the probability of t i and t j propagating to each other, as shown in Formula 2. Here N is the set of all nodes, and \u03b4 (i, j) denotes whether an edge exists between two nodes or not. Iterative propagation Considering that the source document D is represented as a graph of sd-nodes, so the relevance of background knowledge t b to D is naturally converted into that of t b to the graph of sd-nodes.",
  "y": "background"
 },
 {
  "id": "6293d300ab46a6d6135ed256005403_6",
  "x": "Zhang et al. (2014) evaluate this relevance by propagating relevance weight from sd-nodes to t b iteratively. After convergence, the relevance weight of t b will be treated as the final relevance to D. There are in total n \u00d7 n pairs of nodes, and their p(i, j) are stored in a matrix P.<cite> Zhang et al. (2014)</cite> use W = (w 1 , w 2 , . . . , w n ) to denote the relevance weights of nodes, where w i indicates the relevance of t i to D. At the beginning, each w i of bk-nodes is initialized to 0, and each that of sd-nodes is initialized to its importance to D. Then W is updated to W after every iteration according to Formula 3. They keep updating the weights of both sd-nodes and bk-nodes until convergence and do not distinguish them explicitly. ----------------------------------",
  "y": "background"
 },
 {
  "id": "6293d300ab46a6d6135ed256005403_7",
  "x": "**MODIFIED ITERATIVE PROPAGATION** In this part, we propose a modified iterative propagation based ranking model to select the mostrelevant triples of background knowledge. There are three primary modifications to the original model of<cite> Zhang et al. (2014)</cite> , all of which are shown more powerful in our experiments.",
  "y": "background"
 },
 {
  "id": "6293d300ab46a6d6135ed256005403_8",
  "x": "First of all, the original model <cite>(Zhang et al., 2014)</cite> does not reset the relevance weight of sdnodes after every iteration. This results in a continued decrease of the relevance weight of sd-nodes, which weakens the effect of sd-nodes during the iterative propagation and in turn affects the final performance. To tackle this problem, we decrease the relevance weight of bk-nodes and increase that of sd-nodes according to a fixed ratio after every iteration, so as to ensure that the total weight of sd-nodes is always higher than that of bk-nodes.",
  "y": "motivation differences background"
 },
 {
  "id": "6293d300ab46a6d6135ed256005403_9",
  "x": [
   "We also modify the definition of p(i, j), the probability of two nodes t i and t j propagating to each other. Zhang et al. (2014) compute this probability according to Formula 2, which highlights the number of neighbors, but weakens the relatedness between nodes, due to the normalization. We modify this setup by removing the normalization process and computing p(i, j) as the relatedness between t i and t j directly, which is evaluated according to Formula 1 ."
  ],
  "y": "motivation differences"
 },
 {
  "id": "6293d300ab46a6d6135ed256005403_10",
  "x": "Baseline systems As<cite> Zhang et al. (2014)</cite> argued, it is difficult to use the methods in traditional ranking tasks, such as information retrieval (Manning et al., 2008) and entity linking (Han et al., 2011; Sen, 2012) , as baselines in this task, because our model takes triples as basic input and thus lacks some crucial information such as link structure. For better comparison, we implement three methods as baselines, which have been proved effective in relevance evaluation: (1) Vector Space Model (VSM), (2) Word Embedding (WE), and (3) Latent Dirichlet Allocation (LDA). Note that our model captures the distributional semantics of triples with LDA, while WE serves as a baseline only, where the word embeddings are acquired over the same corpus mentioned previously with 2 http://www.sogou.com/labs/dl/c.html the publicly available tool word2vec 3 .",
  "y": "motivation differences"
 },
 {
  "id": "6293d300ab46a6d6135ed256005403_11",
  "x": "Then we compute w i by accumulating all the cosine-similarities between t i and the triples extracted from D. For all the baselines, we rank the triples of background knowledge according to w i , their relevance to D. Experimental setup Previous research relies on manual annotation to evaluate the ranking performance <cite>(Zhang et al., 2014)</cite> , which costs a lot, and in which it is difficult to get high consistency. In this paper, we carry out an automatic evaluation.",
  "y": "differences background"
 },
 {
  "id": "6293d300ab46a6d6135ed256005403_12",
  "x": "This study encodes distributional semantics into the triple-based background knowledge ranking model <cite>(Zhang et al., 2014)</cite> for better document enrichment. We first use LDA to represent every triple as a real-valued vector, which is used to evaluate the relatedness between triples, and then propose a modified iterative propagation model to rank all the triples of background knowledge. For evaluation, we conduct two series of experiments: (1) evaluation as ranking problem, and (2) taskbased evaluation, especially for document classification. In the first set of experiments, our model outperforms multiple strong baselines based on VSM, LDA, and WE. In the second set of experiments, our full model with background knowledge outperforms the state-of-the-art systems significantly. Moreover, we also explore the impact of knowledge quality and show its importance.",
  "y": "motivation background"
 },
 {
  "id": "6330c615d4c62b30933dac3057c9d6_0",
  "x": "The discriminator decoder takes the hidden state at the last time step of a sequence concatenated with both the max-pooled and mean-pooled representation of the hidden states <cite>[20]</cite> and outputs a number in the range [0, 1]. The difficulty of using GANs in text generation comes from the discrete nature of text, making the model non-differentiable hence, we update parameters for the generator model with policy gradients as described in Yu [16] . We utilize AWD-LSTM [21] and TransformerXL [22] based language models.",
  "y": "uses"
 },
 {
  "id": "6330c615d4c62b30933dac3057c9d6_1",
  "x": "We use Adam optimizer [23] with \u03b21 = 0.7 and \u03b22 = 0.8 similar to <cite>[20]</cite> and use a batch size of 50. Other practices for LM training were the same as [22] and [21] for Transformer-XL and AWD-LSTM respectively. We refer to our proposed GAN as Creative-GAN and compare it to a baseline (a language model equivalent to our pre-trained generator) and a GumbelGAN model [15] across all proposed datasets.",
  "y": "similarities"
 },
 {
  "id": "6330c615d4c62b30933dac3057c9d6_2",
  "x": "We use the same pre-processing as in earlier work <cite>[20,</cite> 24] . We reserve 10% of our data for test set and another 10% for our validation set. We first pre-train our generator on the Gutenberg dataset [25] for 20 epochs and then fine-tune <cite>[20]</cite> them to our target datasets with a language modeling objective.",
  "y": "uses"
 },
 {
  "id": "6330c615d4c62b30933dac3057c9d6_3",
  "x": "We use the same pre-processing as in earlier work <cite>[20,</cite> 24] . We reserve 10% of our data for test set and another 10% for our validation set. We first pre-train our generator on the Gutenberg dataset [25] for 20 epochs and then fine-tune <cite>[20]</cite> them to our target datasets with a language modeling objective.",
  "y": "uses"
 },
 {
  "id": "649eff228a47b484d01872a980e58f_0",
  "x": "These conflicting system requirements make KWS an active area of research ever since its inception over 50 years ago [4] . Recently, with the renaissance of artificial neural networks in the form of deep learning algorithms, neural network (NN) based KWS has become very popular [5, 6,<cite> 7,</cite> 8] . Low power consumption requirement for keyword spotting systems make microcontrollers an obvious choice for deploying KWS in an always-on system.",
  "y": "background"
 },
 {
  "id": "649eff228a47b484d01872a980e58f_1",
  "x": "\u2022 We first train the popular KWS neural net models from the literature [5, 6,<cite> 7,</cite> 8] on Google speech commands dataset [9] and compare them in terms of accuracy, memory footprint and number of operations per inference. \u2022 In addition, we implement a new KWS model using depth-wise separable convolutions and point-wise convolutions, inspired by the success of resource-efficient MobileNet [10] in computer vision. This model outperforms the other prior models in all aspects of accuracy, model size and number of operations.",
  "y": "similarities uses"
 },
 {
  "id": "649eff228a47b484d01872a980e58f_2",
  "x": "Combining the strengths of CNNs and RNNs, convolutional recurrent neural network based KWS is investigated in <cite>[7]</cite> and demonstrate the robustness of the model to noise. While all the prior KWS neural networks are trained with cross entropy loss function, a max-pooling based loss function for training KWS model with long short-term memory (LSTM) is proposed in [8] , which achieves better accuracy than the DNNs and LSTMs trained with cross entropy loss. Although many neural network models for KWS are presented in literature, it is difficult to make a fair comparison between them as they are all trained and evaluated on different proprietary datasets (e.g. \"TalkType\" dataset in <cite>[7]</cite> , \"Alexa\" dataset in [8] , etc.) with different input speech features and audio duration.",
  "y": "background"
 },
 {
  "id": "649eff228a47b484d01872a980e58f_3",
  "x": "Although many neural network models for KWS are presented in literature, it is difficult to make a fair comparison between them as they are all trained and evaluated on different proprietary datasets (e.g. \"TalkType\" dataset in <cite>[7]</cite> , \"Alexa\" dataset in [8] , etc.) with different input speech features and audio duration. Also, the primary focus of prior research has been to maximize the accuracy with a small memory footprint model, without explicit constraints of underlying hardware, such as limits on number of operations per inference. In contrast, this work is more hardware-centric and targeted towards neural network architectures that maximize accuracy on microcontroller devices.",
  "y": "background"
 },
 {
  "id": "649eff228a47b484d01872a980e58f_4",
  "x": "Convolution recurrent neural network <cite>[7]</cite> is a hybrid of CNN and RNN, which takes advantages of both. It exploits the local temporal/spatial correlation using convolution layers and global temporal dependencies in the speech features using recurrent layers. As shown in Fig. 3 , a CRNN model starts with a convolution layer, followed by an RNN to encode the signal and a dense fully-connected layer to map the information.",
  "y": "background"
 },
 {
  "id": "649eff228a47b484d01872a980e58f_5",
  "x": "The training data is augmented with background noise and random time shift of up to 100ms. The trained models are evaluated based on the classification accuracy on the test set. Table 2 summarizes the accuracy, memory requirement and operations per inference for the network architectures for KWS from literature [5, 6,<cite> 7,</cite> 8] trained on Google speech commands dataset [9] .",
  "y": "similarities uses"
 },
 {
  "id": "649eff228a47b484d01872a980e58f_7",
  "x": "Figure 5 shows the number of operations per inference, memory requirement and test accuracy of neural network models from prior work [5, 6,<cite> 7,</cite> 8] trained on Google speech commands dataset overlayed with the memory and compute bounding boxes for the neural network classes from section 4. ---------------------------------- **RESOURCE CONSTRAINED NEURAL NETWORK ARCHITECTURE EXPLORATION**",
  "y": "background"
 },
 {
  "id": "649eff228a47b484d01872a980e58f_8",
  "x": "[5, 6,<cite> 7,</cite> 8] trained on the speech commands dataset [9] . As shown in Fig. 1 , from each input speech signal, T \u00d7 F features are extracted and the number of these features impact the model size, number of operations and accuracy. The key parameters in the feature extraction step that impact the model size, number of operations and accuracy are (1) number of MFCC features per frame (F) and (2) the frame stride (S).",
  "y": "background"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_0",
  "x": "Named entity recognition is an important task in NLP. High performance approaches have been dominated by applying CRF, SVM, or perceptron models to hand-crafted features (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015) . However,<cite> Collobert et al. (2011b)</cite> proposed an effective neural network model that requires little feature engineering and instead learns important features from word embeddings trained on large quantities of unlabelled text -an approach made possible by recent advancements in unsupervised learning of word embeddings on massive amounts of data (Collobert and Weston, 2008; Mikolov et al., 2013) and neural network training algorithms permitting deep architectures (Rumelhart et al., 1986) .",
  "y": "background"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_1",
  "x": "However,<cite> Collobert et al. (2011b)</cite> proposed an effective neural network model that requires little feature engineering and instead learns important features from word embeddings trained on large quantities of unlabelled text -an approach made possible by recent advancements in unsupervised learning of word embeddings on massive amounts of data (Collobert and Weston, 2008; Mikolov et al., 2013) and neural network training algorithms permitting deep architectures (Rumelhart et al., 1986) . Unfortunately there are many limitations to the model proposed by<cite> Collobert et al. (2011b)</cite> . First, it uses a simple feed-forward neural network, which restricts the use of context to a fixed sized window around each word -an approach that discards useful long-distance relations between words.",
  "y": "background"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_2",
  "x": "Unfortunately there are many limitations to the model proposed by<cite> Collobert et al. (2011b)</cite> . First, it uses a simple feed-forward neural network, which restricts the use of context to a fixed sized window around each word -an approach that discards useful long-distance relations between words. Second, by depending solely on word embeddings, it is unable to exploit explicit character level features such as prefix and suffix, which could be useful especially with rare words where word embeddings are poorly trained. We seek to address these issues by proposing a more powerful neural network model.",
  "y": "motivation"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_3",
  "x": "Furthermore, as lexicons are crucial to NER performance, we propose a new lexicon encoding scheme and matching algorithm that can make use of partial matches, and we compare it to the simpler approach of<cite> Collobert et al. (2011b)</cite> . Extensive evaluation shows that our proposed method establishes a new state of the art on both the CoNLL-2003 NER shared task and the OntoNotes 5.0 datasets. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_4",
  "x": "We present a hybrid model of bi-directional LSTMs and CNNs that learns both character-and word-level features, presenting the first evaluation of such an architecture on well-established English language evaluation datasets. Furthermore, as lexicons are crucial to NER performance, we propose a new lexicon encoding scheme and matching algorithm that can make use of partial matches, and we compare it to the simpler approach of<cite> Collobert et al. (2011b)</cite> . Extensive evaluation shows that our proposed method establishes a new state of the art on both the CoNLL-2003 NER shared task and the OntoNotes 5.0 datasets.",
  "y": "differences"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_5",
  "x": "---------------------------------- **MODEL** Our neural network is inspired by the work of<cite> Collobert et al. (2011b)</cite> , where lookup tables transform discrete features such as words and characters into continuous vector representations, which are then concatenated and fed into a neural network.",
  "y": "motivation"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_6",
  "x": "Our best model uses the publicly available 50dimensional word embeddings released by<cite> Collobert et al. (2011b)</cite> 2 , which were trained on Wikipedia and the Reuters RCV-1 corpus. We also experimented with two other sets of published embeddings, namely Stanford's GloVe embeddings 3 trained on 6 billion words from Wikipedia and Web text (Pennington et al., 2014) and Google's word2vec embeddings 4 trained on 100 billion words from Google News (Mikolov et al., 2013) . In addition, as we hypothesized that word embeddings trained on in-domain text may perform better, we also used the publicly available GloVe (Pennington et al., 2014) program and an in-house re-implementation 5 of the word2vec (Mikolov et al., 2013) program to train word embeddings on Wikipedia and Reuters RCV1 datasets as well.",
  "y": "uses"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_7",
  "x": "6 Following<cite> Collobert et al. (2011b)</cite> , all words are lower-cased before passing through the lookup table  Text Hayao Tada , commander of the Japanese North China Area Army to convert to their corresponding embeddings. The pre-trained embeddings are allowed to be modified during training. 7",
  "y": "uses"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_8",
  "x": "**CAPITALIZATION FEATURE** As capitalization information is erased during lookup of the word embedding, we evaluate Collobert's method of using a separate lookup table to add a capitalization feature with the following options: allCaps, upperInitial, lowercase, mixedCaps, noinfo<cite> (Collobert et al., 2011b)</cite> . This method is compared with the character type feature (Section 2.5) and character-level CNNs.",
  "y": "uses"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_9",
  "x": "For each token in the match, the feature is en- coded in BIOES annotation (Begin, Inside, Outside, End, Single), indicating the position of the token in the matched entry. In other words, B will not appear in a suffix-only partial match, and E will not appear in a prefix-only partial match. As we will see in Section 4.5, we found that this more sophisticated method outperforms the method presented by<cite> Collobert et al. (2011b)</cite> , which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches, and marks tokens with YES/ NO.",
  "y": "differences"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_10",
  "x": "In addition, since<cite> Collobert et al. (2011b)</cite> released their lexicon with their SENNA system, we also applied their lexicon to our model for comparison and investigated using both lexicons simultaneously as distinct features. We found that the two lexicons complement each other and improve performance on the CoNLL-2003 dataset. Our best model uses the SENNA lexicon with exact matching and our DBpedia lexicon with partial matching, with BIOES annotation in both cases.",
  "y": "uses"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_11",
  "x": "As we will see in Section 4.5, we found that this more sophisticated method outperforms the method presented by<cite> Collobert et al. (2011b)</cite> , which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches, and marks tokens with YES/ NO. In addition, since<cite> Collobert et al. (2011b)</cite> released their lexicon with their SENNA system, we also applied their lexicon to our model for comparison and investigated using both lexicons simultaneously as distinct features. We found that the two lexicons complement each other and improve performance on the CoNLL-2003 dataset.",
  "y": "similarities"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_12",
  "x": "We train our network to maximize the sentencelevel log-likelihood from<cite> Collobert et al. (2011b)</cite> . 17 First, we define a tag-transition matrix A where A i,j represents the score of jumping from tag i to tag j in successive tokens, and A 0,i as the score for starting with tag i. This matrix of parameters are also learned. Define \u03b8 as the set of parameters for the neural network, and \u03b8 = \u03b8 \u222a {A i,j \u2200i, j} as the set of all parameters to be trained.",
  "y": "uses"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_13",
  "x": "This objective function and its gradients can be efficiently computed by dynamic programming<cite> (Collobert et al., 2011b)</cite> . At inference time, given neural network outputs [f \u03b8 ] i,t we use the Viterbi algorithm to find the tag sequence [i] T 1 that maximizes the score The output tags are annotated with BIOES (which stand for Begin, Inside, Outside, End, Single, indicating the position of the token in the 18 OntoNotes results taken from (Durrett and Klein, 2014) 19 Evaluation on OntoNotes 5.0 done by Pradhan et al. (2013) 20 Not directly comparable as they evaluated on an earlier version of the corpus with a different data split.",
  "y": "background"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_14",
  "x": "We re-implemented the FFNN model of<cite> Collobert et al. (2011b)</cite> as a baseline for comparison. Table 5 shows that while performing reasonably well on CoNLL-2003, FFNNs are clearly inadequate for OntoNotes, which has a larger domain, showing that LSTM models are essential for NER. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_15",
  "x": "**WORD EMBEDDINGS** One possible reason that Collobert embeddings perform better than other publicly available embeddings on CoNLL-2003 is that they are trained on the Reuters RCV-1 corpus, the source of the CoNLL-2003 dataset, whereas the other embeddings are not 28 . On the other hand, we suspect that Google's embeddings perform poorly because of vocabulary mismatch -in particular, Google's embeddings were trained in a case-sensitive manner, and embeddings for many common punctuations and 27 Wilcoxon rank sum test, p < 0.001 28 To make a direct comparison to<cite> Collobert et al. (2011b)</cite> , we do not exclude the CoNLL-2003 NER task test data from the word vector training data.",
  "y": "uses"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_16",
  "x": "Table 9 shows the results. In this weakened model, both lexicons contribute significant 33 improvements over the baseline. Compared to the SENNA lexicon, our DBpedia lexicon is noisier but has broader coverage, which explains why when applying it using the same method as<cite> Collobert et al. (2011b)</cite> , it performs worse on CoNLL-2003 but better on OntoNotesa dataset containing many more obscure named entities.",
  "y": "uses"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_18",
  "x": "Much later, with the advent of neural word embeddings,<cite> Collobert et al. (2011b)</cite> presented SENNA, which employs a deep FFNN and word embeddings to achieve near state of the art results on POS tagging, chunking, NER, and SRL. We build on their approach, sharing the word embeddings, feature encoding method, and objective functions. Recently, Santos et al. (2015) presented their CharWNN network, which augments the neural network of<cite> Collobert et al. (2011b)</cite> with character level CNNs, and they reported improved performance on Spanish and Portuguese NER.",
  "y": "background"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_19",
  "x": "Much later, with the advent of neural word embeddings,<cite> Collobert et al. (2011b)</cite> presented SENNA, which employs a deep FFNN and word embeddings to achieve near state of the art results on POS tagging, chunking, NER, and SRL. We build on their approach, sharing the word embeddings, feature encoding method, and objective functions. Recently, Santos et al. (2015) presented their CharWNN network, which augments the neural network of<cite> Collobert et al. (2011b)</cite> with character level CNNs, and they reported improved performance on Spanish and Portuguese NER.",
  "y": "extends"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_20",
  "x": "Much later, with the advent of neural word embeddings,<cite> Collobert et al. (2011b)</cite> presented SENNA, which employs a deep FFNN and word embeddings to achieve near state of the art results on POS tagging, chunking, NER, and SRL. We build on their approach, sharing the word embeddings, feature encoding method, and objective functions. Recently, Santos et al. (2015) presented their CharWNN network, which augments the neural network of<cite> Collobert et al. (2011b)</cite> with character level CNNs, and they reported improved performance on Spanish and Portuguese NER.",
  "y": "background"
 },
 {
  "id": "64b344bf8ec9b6a113bf6b3f638528_21",
  "x": "We build on their approach, sharing the word embeddings, feature encoding method, and objective functions. Recently, Santos et al. (2015) presented their CharWNN network, which augments the neural network of<cite> Collobert et al. (2011b)</cite> with character level CNNs, and they reported improved performance on Spanish and Portuguese NER. We have successfully incorporated character-level CNNs into our model.",
  "y": "similarities"
 },
 {
  "id": "652534f801dbff0c009c4a39fdef4d_0",
  "x": "This is the key problem addressed by research on ASR quality estimation C. de Souza et al., 2015;<cite> Jalalvand et al., 2015b)</cite> , and the task for which TranscRater, the tool described in this paper, has been designed. The work on ASR quality estimation (ASR QE) has several motivations. First, the steady increase of applications involving automatic speech recognition (e.g. video/TV programs subtitling, voice search engines, voice question answering, spoken dialog systems, meeting and broadcast news transcriptions) calls for an accurate method to estimate ASR output quality at run-time.",
  "y": "background"
 },
 {
  "id": "652534f801dbff0c009c4a39fdef4d_1",
  "x": "As a consequence, also the evaluation metrics will change. Precision/recall/F1 (or other metrics, such as balanced accuracy, in case of very unbalanced distributions) will be used for classification while, similar to MT QE, the mean absolute error (MAE) or similar metrics will be used for regression. A variant of the basic ASR QE task is to consider it as a QE-based ranking problem<cite> (Jalalvand et al., 2015b)</cite> , in which each utterance is captured by multiple microphones or transcribed by multiple ASR systems.",
  "y": "background"
 },
 {
  "id": "652534f801dbff0c009c4a39fdef4d_2",
  "x": "For regression-based tasks (WER prediction), TranscRater includes an interface to the Scikitlearn package (Pedregosa et al., 2011) , a Python machine learning library that contains a large set of classification and regression algorithms. Based on the empirical results reported in C. de Souza et al., 2015;<cite> Jalalvand et al., 2015b)</cite> , which indicate that Extremely Randomized Trees (XRT (Geurts et al., 2006) ) is a very competitive algorithm in several WER prediction tasks, the current version of the tool exploits XRT. However, adapting the interface to apply other algorithms is an easy task and one of the future extension directions.",
  "y": "background"
 },
 {
  "id": "652534f801dbff0c009c4a39fdef4d_3",
  "x": "This can be done either indirectly, by exploiting the predicted WER labels in a \"ranking by regression\" approach (RR) or directly, by exploiting machinelearned ranking methods (MLR). To train and test MLR models, TranscRater exploits RankLib 8 , a library of learning-to-rank algorithms. The current version of the tool includes an interface to the Random Forest algorithm (RF (Breiman, 2001) ), the same used in<cite> (Jalalvand et al., 2015b)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "652534f801dbff0c009c4a39fdef4d_4",
  "x": "**BENCHMARKING** The features and algorithms contained in TranscRater have been successfully used in previous works C. de Souza et al., 2015;<cite> Jalalvand et al., 2015b</cite>; Jalalvand et al., 2015a) . To further investigate their effectiveness, in this section we provide new results, both in WER prediction (MAE) and transcription ranking (NDCG), together with some efficiency analysis (Time in seconds 9 ).",
  "y": "background"
 },
 {
  "id": "65f7546e2abfd74c0daa43c25ca63f_0",
  "x": "A linguistics constraint-driven generation approach such as equivalent constraint <cite>[6,</cite> 7] is not restrictive to languages with distinctive grammar structure. In this paper, we propose a novel language-agnostic method to learn how to generate code-switching sentences by using a pointer-generator network [8] . The model is trained from concatenated sequences of parallel sentences to generate code-switching sentences, constrained by codeswitching texts.",
  "y": "background"
 },
 {
  "id": "65f7546e2abfd74c0daa43c25ca63f_1",
  "x": "This idea is also in line with code-mixing by borrowing words from the embedded language [9] and intuitively, the copying mechanism can be seen as an end-to-end approach to translate, align, and reorder the given words into a grammatical code-switching sentence. This approach is the unification of all components in the work of<cite> [6]</cite> into a single computational model. A code-switching language model learned in this way is able to capture the patterns and constraints of the switches and mitigate the out-of-vocabulary (OOV) issue during sequence generation.",
  "y": "uses"
 },
 {
  "id": "65f7546e2abfd74c0daa43c25ca63f_2",
  "x": "**RELATED WORK** The synthetic code-switching generation approach was introduced by adapting equivalence constraint on monolingual sentence pairs during the decoding step on an automatic speech recognition (ASR) model<cite> [6]</cite> . [11] explored Functional Head Constraint, which was found to be more restrictive than the Equivalence Constraint, but complex to be implemented, by using a lattice parser with a weighted finitestate transducer.",
  "y": "background"
 },
 {
  "id": "65f7546e2abfd74c0daa43c25ca63f_3",
  "x": "As our baseline, we compare our proposed method with three other models: (1) We use Seq2Seq with attention; (2) We generate sequences that satisfy Equivalence Constraint<cite> [6]</cite> . The constraint doesn't allow any switch within a crossing of two word alignments. We use FastAlign [19] as the word aligner 1 ; (3) We also form sentences using the alignments without any constraint.",
  "y": "uses"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_0",
  "x": "Furthermore, they do not provide an explicit explanation during the learning process and often rely on other analytical methods to provide an interpretation for their predictions. As a result, it is often hard to trust the predictions made arXiv:2003.06050v1 [cs. LG] 12 Mar 2020 by embedding-based methods. Recent advances in the area of deep reinforcement learning (DRL) have inspired reinforcement learning (RL) based solutions for the KG completion problem [21, 3, 30, <cite>13</cite>, 19, 22, 14, 29] .",
  "y": "background"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_1",
  "x": "<cite>RL-based methods</cite> formulate the task of KG completion as a sequential decision-making process in which the goal is to train an RL agent to walk over the graph by taking a sequence of actions (i.e., choosing the next entity) that connects the source to the target entity. The sequences of entities and relations can be directly used as a logical reasoning path for interpreting model predictions. For example, in order to answer the query (Reggie Miller, plays sport, ?), the agent may find the following reasoning path in the KG: Reggie Miller These RL solutions demonstrate competitive accuracy with other deep learning methods while boasting improved interpretability of the reasoning process.",
  "y": "background"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_2",
  "x": "<cite>Existing RL-based methods</cite> for KG completion do not capture the entity's neighborhood information. Previous studies on one-shot fact prediction have shown that the local neighborhood structure improves the fact prediction performance for long-tailed relations [31, 37] . We propose a graph neural network (GNN) [9] to encode the neighborhood information of the entities and leverage the state representation with the type and neighborhood information of the entities.",
  "y": "background"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_3",
  "x": "<cite>Existing RL-based methods</cite> for KG completion do not capture the entity's neighborhood information. Previous studies on one-shot fact prediction have shown that the local neighborhood structure improves the fact prediction performance for long-tailed relations [31, 37] . We propose a graph neural network (GNN) [9] to encode the neighborhood information of the entities and leverage the state representation with the type and neighborhood information of the entities.",
  "y": "motivation"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_4",
  "x": "With the recent success of deep reinforcement learning in AlphaGO [25] researchers began to adopt RL to solve a variety of problems that were conventionally addressed by deep learning methods, such as ad recommendation [38, 15, 2] , dialogue systems [20, 18] , and question answering [30, 3] . As a result, more recent methods proposed using RL to solve the multi-hop reasoning problem in knowledge graphs by framing it as a sequential decision-making process [3, 23, 30, 22, 12, <cite>13</cite>] . Deeppath [30] was the first method that used RL to find relation paths between two entities in KGs.",
  "y": "background"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_5",
  "x": "MARLPaR [12] uses a multi-agent RL approach where two agents are used to perform the relation selection and entity selection iteratively. <cite>Lin et al. [13]</cite> implement reward shaping to address the problem of the sparse reward signal and action dropout to reduce the effect of incorrect paths. Xian et al. [29] use KG reasoning for recommender systems and designed both a multi-hop scoring function and a user-conditioned action pruning strategy to improve the efficiency of RL-based recommendation.",
  "y": "background"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_6",
  "x": "<cite>Lin et al. [13]</cite> implement reward shaping to address the problem of the sparse reward signal and action dropout to reduce the effect of incorrect paths. Xian et al. [29] use KG reasoning for recommender systems and designed both a multi-hop scoring function and a user-conditioned action pruning strategy to improve the efficiency of RL-based recommendation. Because these <cite>RL models</cite> treat the KG completion problem as a path reasoning problem instead of a link prediction problem, they are able to overcome both drawbacks of embedding methods that are outlined above.",
  "y": "background"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_7",
  "x": "Because these <cite>RL models</cite> treat the KG completion problem as a path reasoning problem instead of a link prediction problem, they are able to overcome both drawbacks of embedding methods that are outlined above. However, the <cite>RL models</cite> have drawbacks of their own, the most notable of which are computational cost and predictive accuracy. Many of <cite>these RL methods</cite> have tried to combine the representational power of embeddings and reasoning power of RL by training an agent to navigate an embedding space.",
  "y": "background"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_8",
  "x": "Many of <cite>these RL methods</cite> have tried to combine the representational power of embeddings and reasoning power of RL by training an agent to navigate an embedding space. For example, the authors of [<cite>13</cite>] build an agent-based model on top of pre-trained embeddings generated by ComplEx [27] or ConvE [4] . While we take a <cite>similar modular approach</cite>, our solution enriches the embedding space with additional information about entity types and local neighborhood information.",
  "y": "background"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_9",
  "x": "Many of <cite>these RL methods</cite> have tried to combine the representational power of embeddings and reasoning power of RL by training an agent to navigate an embedding space. For example, the authors of [<cite>13</cite>] build an agent-based model on top of pre-trained embeddings generated by ComplEx [27] or ConvE [4] . While we take a <cite>similar modular approach</cite>, our solution enriches the embedding space with additional information about entity types and local neighborhood information.",
  "y": "background"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_10",
  "x": "Many of <cite>these RL methods</cite> have tried to combine the representational power of embeddings and reasoning power of RL by training an agent to navigate an embedding space. For example, the authors of [<cite>13</cite>] build an agent-based model on top of pre-trained embeddings generated by ComplEx [27] or ConvE [4] . While we take a <cite>similar modular approach</cite>, our solution enriches the embedding space with additional information about entity types and local neighborhood information.",
  "y": "similarities extends"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_11",
  "x": "**A REINFORCEMENT LEARNING SOLUTION** Similar to [3, <cite>13</cite>, 30] , we formulate this problem as a Markov Decision Process (MDP), in which the goal is to train a policy gradient agent (using REIN- FORCE [28] ) to learn an optimal reasoning path to answer a given query (e s , r, ?). We express the RL framework as a set of states, actions, rewards, and transitions.",
  "y": "similarities"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_12",
  "x": "For performance reasons, many RL models are forced to cap the size of the action space and do so via a pre-computed heuristic. For example, [<cite>13</cite>] pre-computes PageRank scores for each node, and narrows the action space to a fixed number of highest-ranking neighbors. In this work, we use entity type information to limit the search to the entities with best-matching types, given the previous actions.",
  "y": "background"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_13",
  "x": "However, since knowledge graphs are incomplete, a binary reward cannot model the potentially missing facts. As a result, the agent receives low-quality rewards as it explores the environment. Inspired by [<cite>13</cite>] , we use pre-trained KG embeddings based on existing KG embedding methods to design a soft reward function for the terminal state s T based on [17] :",
  "y": "motivation"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_14",
  "x": "Inspired by [<cite>13</cite>] , we use pre-trained KG embeddings based on existing KG embedding methods to design a soft reward function for the terminal state s T based on [17] : Where f (e s , r, e T ) is a similarity measure calculated based on pre-trained KG embedding approach [<cite>13</cite>] . We use different embedding methods depending on different datasets.",
  "y": "uses"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_15",
  "x": "Following [<cite>13</cite>] , we use an LST M to encode the history h t = {e t\u2212k , r t\u2212k+1 , ..., e t\u22121 , r t } of the past k steps taken by the agent in solving the query. The history embedding for h t is represented as: We define the policy network \u03c0 with weight parameters \u03b8 as follows:",
  "y": "uses"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_16",
  "x": "The transition to a new state is thus given by: To reduce the potential impact of argmax leading to the overuse of incorrect paths, we utilize random action dropout as described in [<cite>13</cite>] . The policy network is trained using stochastic gradient descent to maximize the expected reward:",
  "y": "uses"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_17",
  "x": "**EXPERIMENTS** In this section, we describe and discuss the experimental results of our proposed approach. We compare against several baseline methods: ConvE (embeddingbased) [4] , ComplEx (embedding-based) [27] , MINERVA (agent-based) [3] , and MultiHopKG (agent-based) using both ConvE and ComplEx for pre-trained embeddings [<cite>13</cite>] .",
  "y": "uses"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_18",
  "x": "|{(e s , r, e d ) \u2208 Q : rank(e d , F(e s , r)) \u2264 k}| |Q| \u00d7 100 (8) where Q = Queries and rank(e d ,\u00ca d ) is a function that returns the position of entity e d in the set of ordered predictions\u00ca d . MRR is a related metric, defined as the multiplicative inverse of the rank of the correct answer, i.e.: Because none of these models generalize to unknown entities, followed by previous work [3, <cite>13</cite>] , we measure Hits@k and MRR only for queries for which both e s and e d have already been seen at least once by the model during training.",
  "y": "uses"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_19",
  "x": "---------------------------------- **PARAMETER SELECTION** For NELL-995, utilize the same hyperparameters described in [<cite>13</cite>] when training ConvE, ComplEx, Distmult, and <cite>Lin et al [13]</cite> baselines.",
  "y": "uses"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_20",
  "x": "For the two Amazon datasets, we perform a grid search for our method and all <cite>baselines</cite> and report the best performance for each. For all datasets, we train the KG embedding models (ConvE and ComplEx) for 1000 epochs each. These embeddings are then used to make predictions directly but also serve as pre-trained inputs for the RL agent, which we train for 30 epochs per experiment for all datasets.",
  "y": "uses"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_21",
  "x": "For NELL-995 data, We quote the results reported in [3, <cite>13</cite>] . Embedding-based methods show an overall higher performance compared to the <cite>RL-based methods</cite>. We can see that in all three datasets, our results outperform both RL baselines (<cite>Lin et al. [13]</cite> and MINERVA [3] ).",
  "y": "uses"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_22",
  "x": "For NELL-995 data, We quote the results reported in [3, <cite>13</cite>] . Embedding-based methods show an overall higher performance compared to the <cite>RL-based methods</cite>. We can see that in all three datasets, our results outperform both RL baselines (<cite>Lin et al. [13]</cite> and MINERVA [3] ).",
  "y": "differences"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_23",
  "x": "We can see that in all three datasets, our results outperform both RL baselines (<cite>Lin et al. [13]</cite> and MINERVA [3] ). Amazon datasets, on the other hand, are far more challenging. We notice that even the embedding based methods are struggling with low performance on these datasets.",
  "y": "differences"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_24",
  "x": "Our method results in a 4% improvement in MRR (and 5.43% in Hits@1) over the <cite>best RL baseline</cite> on Amazon Cellphones and a 3.9% improvement in MRR (and 5.5% in Hits@1) on Amazon Beauty. On the NELL-995 dataset, our method results in 2.8% improvement in MRR and 4% improvement in Hits@1 over the best performing <cite>baseline</cite>. We also performed ablations studies to analyze the effect of each module in our model.",
  "y": "differences"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_25",
  "x": "Our method results in a 4% improvement in MRR (and 5.43% in Hits@1) over the <cite>best RL baseline</cite> on Amazon Cellphones and a 3.9% improvement in MRR (and 5.5% in Hits@1) on Amazon Beauty. On the NELL-995 dataset, our method results in 2.8% improvement in MRR and 4% improvement in Hits@1 over the best performing <cite>baseline</cite>. We also performed ablations studies to analyze the effect of each module in our model.",
  "y": "differences"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_26",
  "x": "We believe due to the sparsity of these two knowledge graphs, type information was more effective for action space pruning than entity page ranks, as done in [<cite>13</cite>] . Note that, there are only 5 entity types in the Amazon datasets. As a result, the number of entities that will be discarded (due to type mismatch) is higher which assists the agent to discover a better path.",
  "y": "uses"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_27",
  "x": "Figure 2 shows that path diversity (top row) improves across all models as the model performance (bottom row) improves. For this analysis, we compare our ablation models (Ours (-N) and Ours (-T)) with the best performing RL baseline by <cite>Lin et al. [13]</cite> . Our method is more successful in discovering novel paths and obtains a better hit ratio on the development set.",
  "y": "uses"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_28",
  "x": "Figure 2 shows that path diversity (top row) improves across all models as the model performance (bottom row) improves. For this analysis, we compare our ablation models (Ours (-N) and Ours (-T)) with the best performing RL baseline by <cite>Lin et al. [13]</cite> . Our method is more successful in discovering novel paths and obtains a better hit ratio on the development set.",
  "y": "differences"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_29",
  "x": "We compare the ablation models along with the <cite>best RL baseline</cite> performance on seen and unseen queries. Note that, percentage of unseen queries is much lower in the Amazon datasets compared to the NELL-995 dataset. Table 4 shows that our proposed method performs better on both seen and unseen queries.",
  "y": "uses"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_30",
  "x": "We compare the ablation models along with the <cite>best RL baseline</cite> performance on seen and unseen queries. Table 4 shows that our proposed method performs better on both seen and unseen queries.",
  "y": "differences"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_31",
  "x": "We evaluate our proposed model on different relation types and compare our results with the <cite>best performing RL baseline</cite>. We take a similar approach as [<cite>13</cite>] to extract to-many and to-one relations. A relation r is considered to-many if queries containing relation r can have more than 1 correct answer, otherwise, it is considered a to-one relation.",
  "y": "uses"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_32",
  "x": "We evaluate our proposed model on different relation types and compare our results with the <cite>best performing RL baseline</cite>. We take a similar approach as [<cite>13</cite>] to extract to-many and to-one relations. A relation r is considered to-many if queries containing relation r can have more than 1 correct answer, otherwise, it is considered a to-one relation.",
  "y": "similarities"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_33",
  "x": "Our proposed model consistently shows a better performance than <cite>Lin et al.</cite>, except for the NELL-995 dataset where the improvement is marginal. Both to-one and to-many are more sensitive to removing the neighbor-encoder rather than removing the type information. However, for to-one relations MRR does not drop in any dataset after removing the type information.",
  "y": "differences"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_34",
  "x": "Our method discovers the path: New York (City) . Again, other <cite>RL baselines</cite> struggle with finding the next best step after entity New York. Our method uses the location information to find the answer Michael Bloomberg.",
  "y": "background"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_35",
  "x": "Again, other <cite>RL baselines</cite> struggle with finding the next best step after entity New York. Our method uses the location information to find the answer Michael Bloomberg. In the Amazon datasets, there are fewer entity and relation types.",
  "y": "differences"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_37",
  "x": "In the Amazon datasets, there are fewer entity and relation types. As a result, we observe many frequent patterns that <cite>all RL baselines</cite> are able to discover. Therefore, we focus on the diversity of the relations used in our method and the best performing baseline [<cite>13</cite>] for the discovered paths in the development set.",
  "y": "uses"
 },
 {
  "id": "666cc3c936358c5e9b2f7d0eb8d0e4_38",
  "x": "Our results show that incorporating the heterogeneous context and the local neighborhood information results in a better performance for the query answering task. Our analysis shows that the type information is important for faster convergence and finding more diverse paths, and the neighborhood information improves the performance on unseen queries. In the future, we plan to explore more efficient strategies for action-space pruning to improve the scalability of <cite>existing RL solutions</cite>.",
  "y": "future_work"
 },
 {
  "id": "6683d7b77f536b93416d985414afeb_0",
  "x": "The combination of visual and textual content poses new challenges for information systems which need not only to deal with the semantics of text but also that of images. Recent work <cite>(Barbieri et al., 2017)</cite> has shown that textual information can be used to predict emojis associated to text. In this paper we show that in the current context of multimodal communication where texts and images are combined in social networks, visual information should be combined with texts in order to obtain more accurate emojiprediction models.",
  "y": "background motivation"
 },
 {
  "id": "6683d7b77f536b93416d985414afeb_1",
  "x": "We explore the use of emojis in the social media platform Instagram. We put forward a multimodal approach to predict the emojis associated to an Instagram post, given its picture and text 1 . Our task and experimental framework are similar to <cite>(Barbieri et al., 2017)</cite> , however, we use different data (Instagram instead of Twitter) and, in addition, we rely on images to improve the selection of the most likely emojis to associate to a post.",
  "y": "extends differences"
 },
 {
  "id": "6683d7b77f536b93416d985414afeb_2",
  "x": "Moreover, as done by<cite> Barbieri et al. (2017)</cite> , we considered only the posts which include one and only one of the 20 most frequent emojis (the most frequent emojis are shown in Table 3 ). Our dataset is composed of 299,809 posts, each containing a picture, the text associated to it and only one emoji. In the experiments we also considered the subsets of the 10 (238,646 posts) and 5 most frequent emojis (184,044 posts) (similarly to the approach followed by<cite> Barbieri et al. (2017)</cite> ).",
  "y": "extends differences"
 },
 {
  "id": "6683d7b77f536b93416d985414afeb_3",
  "x": "Moreover, as done by<cite> Barbieri et al. (2017)</cite> , we considered only the posts which include one and only one of the 20 most frequent emojis (the most frequent emojis are shown in Table 3 ). Our dataset is composed of 299,809 posts, each containing a picture, the text associated to it and only one emoji. In the experiments we also considered the subsets of the 10 (238,646 posts) and 5 most frequent emojis (184,044 posts) (similarly to the approach followed by<cite> Barbieri et al. (2017)</cite> ).",
  "y": "similarities uses"
 },
 {
  "id": "6683d7b77f536b93416d985414afeb_4",
  "x": "**TASK:** We extend the experimental scheme of<cite> Barbieri et al. (2017)</cite> , by considering also visual information when modeling posts. We cast the emoji prediction problem as a classification task: given an image or a text (or both inputs in the multimodal scenario) we select the most likely emoji that could be added to (thus used to label) such contents.",
  "y": "extends differences"
 },
 {
  "id": "6683d7b77f536b93416d985414afeb_5",
  "x": "In the first experiment (Section 4.2) we compare the FastText model with the state of the art on emoji classification (B-LSTM) by<cite> Barbieri et al. (2017)</cite> . Our second experiment (Section 4.3) evaluates the visual (ResNet) and textual (FastText) models on the emoji prediction task. Moreover, we evaluate a multimodal combination of both models respectively based on visual and<cite> Barbieri et al. (2017)</cite> , using the same Twitter dataset.",
  "y": "similarities"
 },
 {
  "id": "6683d7b77f536b93416d985414afeb_6",
  "x": "Moreover, we evaluate a multimodal combination of both models respectively based on visual and<cite> Barbieri et al. (2017)</cite> , using the same Twitter dataset. textual inputs. Finally we discuss the contribution of each modality to the prediction task.",
  "y": "similarities uses"
 },
 {
  "id": "6683d7b77f536b93416d985414afeb_7",
  "x": "To compare the FastText model with the word and character based B-LSTMs presented by<cite> Barbieri et al. (2017)</cite> , we consider the same three emoji prediction tasks they proposed: top-5, top-10 and top-20 emojis most frequently used in their Tweet datasets. In this comparison we used the same Twitter datasets. As we can see in Table 1 FastText model is competitive, and it is also able to outperform the character based B-LSTM in one of the emoji prediction tasks (top-20 emojis Table 2 : Prediction results of top-5, top-10 and top-20 most frequent emojis in the Instagram dataset: Precision (P), Recall (R), F-measure (F1).",
  "y": "uses similarities"
 },
 {
  "id": "6683d7b77f536b93416d985414afeb_8",
  "x": "\"%\" indicates the percentage of the class in the test set dard emoji is the second one or is often mispredicted by wrongly selecting or . Another relevant confusion scenario related to emoji prediction has been spotted by<cite> Barbieri et al. (2017)</cite> : relying on Twitter textual data they showed that the emoji was hard to predict as it was used similarly to . Instead when we consider Instagram data, the emoji is easier to predict (0.23), even if it is often confused with .",
  "y": "similarities"
 },
 {
  "id": "67b6d87aa2a943a854251fada6e183_0",
  "x": "In this paper, we discuss an implementation of a global city-level geolocation prediction system for English Twitter users. The system utilises both tweet text and public profile metadata for modeling and inference. Specifically, we train multinomial Bayes classifiers based on location indicative words (LIWs) in tweets<cite> (Han et al., 2012)</cite> , and user-declared location and time zone metadata.",
  "y": "uses"
 },
 {
  "id": "67b6d87aa2a943a854251fada6e183_1",
  "x": "One drawback to the uniformsized cell representation is that it introduces class imbalance: urban areas tend to contain far more tweets than rural areas. Based on this observation, Roller et al. (2012) introduced an adaptive grid representation in which cells contain approximately the same number of users, based on a KDtree partition. Given that most tweets are from urban areas, <cite>Han et al. (2012)</cite> consider a citybased class division, and explore different feature selection methods to extract \"location indicative words\", which they show to improve prediction accuracy.",
  "y": "background"
 },
 {
  "id": "67b6d87aa2a943a854251fada6e183_2",
  "x": "When designing a practical geolocation system, simple models such as naive Bayes and nearest prototype methods (e.g., based on KL divergence) have clear advantages in terms of training and classification throughput, given the size of the class set (often numbering in the thousands of classes) and sheer volume of training data (potentially in the terabytes of data). This is particularly important for online systems and downstream applications that require timely predictions. As such, we build off the text-based naive Bayes-based geolocation system of <cite>Han et al. (2012)</cite> , which our experiments have shown to have a good balance of tractability and accuracy.",
  "y": "extends"
 },
 {
  "id": "67b6d87aa2a943a854251fada6e183_3",
  "x": "**METHODOLOGY** In this study, we adopt the same city-based representation and multinomial naive Bayes learner as <cite>Han et al. (2012)</cite> . The city-based representation consists of 3,709 cities throughout the world, and is obtained by aggregating smaller cities with the largest nearby city.",
  "y": "similarities uses"
 },
 {
  "id": "67b6d87aa2a943a854251fada6e183_4",
  "x": "<cite>Han et al. (2012)</cite> found that using feature selection to identify \"location indicative words\" led to improvements in geolocation performance. We use the same feature selection technique that they did. Specifically, feature selection is based on information gain ratio (IGR) (Quinlan, 1993) over the city-based label set for each word.",
  "y": "uses similarities background motivation"
 },
 {
  "id": "67b6d87aa2a943a854251fada6e183_5",
  "x": "In the original research of <cite>Han et al. (2012)</cite> , only the text of Twitter messages was used, and training was based exclusively on geotagged tweets, despite these accounting for only around 1% of the total public data on Twitter. In this research, we include additional non-geotagged tweets (e.g., posted from a non-GPS enabled device) for those users who have geotagged tweets (allowing us to determine a home location for the user).",
  "y": "background"
 },
 {
  "id": "67b6d87aa2a943a854251fada6e183_6",
  "x": "We base our evaluation on the publicly-available WORLD dataset of <cite>Han et al. (2012)</cite> . The dataset contains 1.4M users whose tweets are primarily identified as English based on the output of the langid.py language identification tool (Lui and Baldwin, 2012) , and who have posted at least 10 geotagged tweets. The city-level home location for a geotagged user is determined as follows.",
  "y": "uses"
 },
 {
  "id": "67b6d87aa2a943a854251fada6e183_7",
  "x": "To benchmark our method, we reimplement two recently-published state-of-the-art methods: (1) the KL-divergence nearest prototype method of Roller et al. (2012) based on KD-tree partitioned grid cells, which we denote as KL; and (2) the multinomial naive Bayes city-level geolocation model of <cite>Han et al. (2012)</cite> , which we denote as MB. Because of the different class representations, Acc numbers are not comparable between the benchmarks.",
  "y": "uses"
 },
 {
  "id": "67b6d87aa2a943a854251fada6e183_8",
  "x": "Of the two original models, we can see that MB is comparable to KL, in line with the findings of <cite>Han et al. (2012)</cite> . The MB-LOC results are by far the highest of all the base classifiers. Contrary to the suggestion of Cheng et al. (2010) that userdeclared locations are too unreliable to use for user geolocation, we find evidence indicating that they are indeed a valuable source of information for this task.",
  "y": "similarities"
 },
 {
  "id": "684a637d08e8dbabddc1f1982f5393_0",
  "x": "In this talk, we evaluate two recent approaches to information presentation in SDS: (1) the Refiner approach (Polifroni et al., 2003) which generates summaries by clustering the options to maximize coverage of the domain, and (2) the <cite>user-model based summarize and refine (UMSR)</cite> approach (<cite>Demberg and Moore, 2006</cite>) which clusters options to maximize utility with respect to a user model, and uses linguistic devices (e.g., discourse cues, adverbials) to highlight the trade-offs among the presented items. To evaluate these strategies, we go beyond the typical \"overhearer\" evaluation methodology, in which participants read or listen to pre-prepared dialogues, which limits the evaluation criteria to users' perceptions (e.g., informativeness, ease of comprehension). Using a Wizard-of-Oz methodology to evaluate the approaches in an interactive setting, we show that in addition to being preferred by users, the <cite>UMSR</cite> approach is superior to the Refiner approach in terms of both task success and dialogue efficiency, even when the user is performing a demanding secondary task.",
  "y": "uses"
 },
 {
  "id": "684a637d08e8dbabddc1f1982f5393_1",
  "x": "Finally, we hypothesize that <cite>UMSR</cite> is more effective because <cite>it</cite> uses linguistic devices to highlight relations (e.g., trade-offs) between options and attributes. We report the results of two studies which show that the discourse cues in <cite>UMSR</cite> summaries help users compare different options and choose between options, even though they do not improve verbatim recall. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "684a637d08e8dbabddc1f1982f5393_2",
  "x": "Finally, we hypothesize that <cite>UMSR</cite> is more effective because <cite>it</cite> uses linguistic devices to highlight relations (e.g., trade-offs) between options and attributes. We report the results of two studies which show that the discourse cues in <cite>UMSR</cite> summaries help users compare different options and choose between options, even though they do not improve verbatim recall. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "684a637d08e8dbabddc1f1982f5393_3",
  "x": "To evaluate these strategies, we go beyond the typical \"overhearer\" evaluation methodology, in which participants read or listen to pre-prepared dialogues, which limits the evaluation criteria to users' perceptions (e.g., informativeness, ease of comprehension). Using a Wizard-of-Oz methodology to evaluate the approaches in an interactive setting, we show that in addition to being preferred by users, the <cite>UMSR</cite> approach is superior to the Refiner approach in terms of both task success and dialogue efficiency, even when the user is performing a demanding secondary task. Finally, we hypothesize that <cite>UMSR</cite> is more effective because <cite>it</cite> uses linguistic devices to highlight relations (e.g., trade-offs) between options and attributes.",
  "y": "differences"
 },
 {
  "id": "684a637d08e8dbabddc1f1982f5393_4",
  "x": "In this talk, we evaluate two recent approaches to information presentation in SDS: (1) the Refiner approach (Polifroni et al., 2003) which generates summaries by clustering the options to maximize coverage of the domain, and (2) the <cite>user-model based summarize and refine (UMSR)</cite> approach (<cite>Demberg and Moore, 2006</cite>) which clusters options to maximize utility with respect to a user model, and uses linguistic devices (e.g., discourse cues, adverbials) to highlight the trade-offs among the presented items. Using a Wizard-of-Oz methodology to evaluate the approaches in an interactive setting, we show that in addition to being preferred by users, the <cite>UMSR</cite> approach is superior to the Refiner approach in terms of both task success and dialogue efficiency, even when the user is performing a demanding secondary task.",
  "y": "uses"
 },
 {
  "id": "684a637d08e8dbabddc1f1982f5393_5",
  "x": "In this talk, we evaluate two recent approaches to information presentation in SDS: (1) the Refiner approach (Polifroni et al., 2003) which generates summaries by clustering the options to maximize coverage of the domain, and (2) the <cite>user-model based summarize and refine (UMSR)</cite> approach (<cite>Demberg and Moore, 2006</cite>) which clusters options to maximize utility with respect to a user model, and uses linguistic devices (e.g., discourse cues, adverbials) to highlight the trade-offs among the presented items. Finally, we hypothesize that <cite>UMSR</cite> is more effective because <cite>it</cite> uses linguistic devices to highlight relations (e.g., trade-offs) between options and attributes.",
  "y": "uses"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_0",
  "x": "Challenges here arise from the level of logical compositionality expressed by questions, as well as the domain openness. Our approach is weakly supervised, trained on question-answer-table triples without requiring intermediate strong supervision. It performs two phases: first, machine understandable logical forms (programs) are generated from natural language questions following the work of<cite> [Pasupat and Liang, 2015]</cite> .",
  "y": "uses"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_1",
  "x": "Our best single model achieves 34.8% accuracy on the WikiTableQuestions dataset, while the best ensemble of our models pushes the state-of-the-art score on this task to 38.7%, thus slightly surpassing both the engineered feature scoring baseline, as well as the Neural Programmer model of<cite> [Neelakantan et al., 2016]</cite> . ---------------------------------- **INTRODUCTION**",
  "y": "differences"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_2",
  "x": "Question-Answering (QA) systems are often faced with a trade-off between the openness of the domain and the depth of logical compositionality hidden in questions. One example are systems able to answer complex questions about a specific topic (e.g.<cite> [Wang et al., 2015]</cite> ). Unsurprisingly, these systems often struggle to generalize to other, more open domains.",
  "y": "background"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_3",
  "x": "Here, we propose a novel weakly supervised model for natural language interfaces operating on semi-structured tables. Our deep learning approach eliminates the need for expensive feature engineering in the candidate scoring phase, while being able to generalize well to never-seen before data. Each natural language question is translated into a set of computer understandable candidate representations, called logical forms, based on the work of<cite> [Pasupat and Liang, 2015]</cite> .",
  "y": "uses"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_4",
  "x": "Our method uses only weak-supervision from questionanswer-table input triples, without requiring expensive annotations of logical forms or latent operation sequences. We empirically confirm our approach on a series of experiments on WikiTableQuestions<cite> [Pasupat and Liang, 2015]</cite> , a real-world dataset containing 22,033 pairs of questions and their corresponding manually retrieved answers with about 2,108 randomly selected Wikipedia tables. The inherent chal-lenges of this dataset include i) the small number of training examples, ii) the complexity of questions that generally require compositionality over multiple simpler operations, iii) generalization to completely unseen tables and domains at test time, and iv) lack of strong supervision.",
  "y": "uses"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_5",
  "x": "Semantic parsing-based methods perform a functional parse of the question that is further converted to a machine understandable program and executed on a knowledgebase or database. A big obstacle for semantic parsers is the need for annotated logical forms when dealing with new domains. To tackle this problem, our method follows recent work of <cite>[Reddy et al., 2014</cite>; Kwiatkowski et al., 2013] that relies solely on weak-supervision through question-answertable triples.",
  "y": "uses"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_6",
  "x": "A big obstacle for semantic parsers is the need for annotated logical forms when dealing with new domains. To tackle this problem, our method follows recent work of <cite>[Reddy et al., 2014</cite>; Kwiatkowski et al., 2013] that relies solely on weak-supervision through question-answertable triples. In the context of QA for semi-structured tables and dealing with multi-compositional queries,<cite> [Pasupat and Liang, 2015]</cite> generate and rank candidate logical forms with a log-linear model trained on question-answer pairs.",
  "y": "background"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_7",
  "x": "The logical form with the highest model probability is then considered as the correct interpretation and is executed. In this work, we generate logical form candidates in the same way as<cite> [Pasupat and Liang, 2015]</cite> . While they resort to hand-crafted features to determine the relevance of the candidates for a question, we use automatic learning of representations, thus benefiting from generalization and flexibility.",
  "y": "uses"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_8",
  "x": "While they resort to hand-crafted features to determine the relevance of the candidates for a question, we use automatic learning of representations, thus benefiting from generalization and flexibility. To do so, we embed each question and the paraphrases of the respective candidate logical forms into the same vector space, making use of similarity metrics for scoring. Paraphrases have been successfully used to facilitate semantic parsers<cite> [Wang et al., 2015</cite>; .",
  "y": "background"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_9",
  "x": "While [Berant and Liang, 2014] is suited for factoid questions with a modest amount of compositionality,<cite> [Wang et al., 2015]</cite> targets more complicated questions. Both of these paraphrase-driven QA systems differ from our work as their scoring relies on hand-crafted features. [<cite> Neelakantan et al., 2016]</cite> also focus on compositional questions, but instead of generating and ranking multiple logical forms, they propose a model that directly constructs a logical form from an embedding of the question.",
  "y": "background"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_10",
  "x": "Paraphrases have been successfully used to facilitate semantic parsers<cite> [Wang et al., 2015</cite>; . While [Berant and Liang, 2014] is suited for factoid questions with a modest amount of compositionality,<cite> [Wang et al., 2015]</cite> targets more complicated questions. Both of these paraphrase-driven QA systems differ from our work as their scoring relies on hand-crafted features.",
  "y": "differences background"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_11",
  "x": "Both of these paraphrase-driven QA systems differ from our work as their scoring relies on hand-crafted features. [<cite> Neelakantan et al., 2016]</cite> also focus on compositional questions, but instead of generating and ranking multiple logical forms, they propose a model that directly constructs a logical form from an embedding of the question. A list of discrete operations are manually defined and each operation is parametrized by a real-valued vector that is learned during training.",
  "y": "background"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_12",
  "x": "Recently,<cite> [Yin et al., 2015]</cite> propose Neural Enquirer, a fully neural, end-to-end differentiable network that executes queries across multiple tables. They use a synthetic dataset to demonstrate the abilities of the model to deal with compositionality in the questions. Embedding-based methods represent the question and the answer as semantic vectors.",
  "y": "background"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_13",
  "x": "However, whereas these models are applied on datasets that require little compositional reasoning, our work targets questions whose answers ask for multi-step complex deductions and operate on semi-structured tables instead of structured knowledgebases. Representation learning using deep learning architectures has been widely explored in other domains, e.g. in the context of sentiment classification, [Kim, 2014;<cite> Socher et al., 2013]</cite> , or for image-hashtag prediction [Denton et al., 2015] . QA systems also differ in the knowledge structure they reason on, which can impose additional challenges.",
  "y": "background"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_14",
  "x": "Representation learning using deep learning architectures has been widely explored in other domains, e.g. in the context of sentiment classification, [Kim, 2014;<cite> Socher et al., 2013]</cite> , or for image-hashtag prediction [Denton et al., 2015] . QA systems also differ in the knowledge structure they reason on, which can impose additional challenges. Systems vary from operating on structured knowledge bases [Bordes et al., 2014b] ; [Bordes et al., 2014a ] to semi-structured tables<cite> [Pasupat and Liang, 2015]</cite> ,<cite> [Neelakantan et al., 2016]</cite> , [Jauhar et al., 2016] and completely unstructured text, which is related to information extraction [Clark et al., 2016] .",
  "y": "background"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_15",
  "x": "Systems vary from operating on structured knowledge bases [Bordes et al., 2014b] ; [Bordes et al., 2014a ] to semi-structured tables<cite> [Pasupat and Liang, 2015]</cite> ,<cite> [Neelakantan et al., 2016]</cite> , [Jauhar et al., 2016] and completely unstructured text, which is related to information extraction [Clark et al., 2016] . We focus on semi-structured tables that face the trade-off between degree of structure and ubiquity. ----------------------------------",
  "y": "uses background"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_16",
  "x": "We now proceed with the detailed description of our QA system. In a nutshell, our model runs through the following stages. For every question q : i) a set of candidate logical forms {z i } i\u2208Iq is generated using the method of<cite> [Pasupat and Liang, 2015]</cite> ; ii) each such candidate program z i is paraphrased in a textual representation t i that offers accuracy gain, interpretability and comprehensibility ; iii) all textual forms t i are scored against the input question q using a neural network model; iv) the logical form z * i corresponding to the highest ranked t * i is selected as the machine-understandable translation of question q; v) z * i is executed on the input table and its answer is returned to the user.",
  "y": "uses"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_17",
  "x": "In a nutshell, our model runs through the following stages. For every question q : i) a set of candidate logical forms {z i } i\u2208Iq is generated using the method of<cite> [Pasupat and Liang, 2015]</cite> ; ii) each such candidate program z i is paraphrased in a textual representation t i that offers accuracy gain, interpretability and comprehensibility ; iii) all textual forms t i are scored against the input question q using a neural network model; iv) the logical form z * i corresponding to the highest ranked t * i is selected as the machine-understandable translation of question q; v) z * i is executed on the input table and its answer is returned to the user. Our contributions are the novel models that perform the steps ii) and iii), while for i), iv) and v) we rely on the work of<cite> [Pasupat and Liang, 2015]</cite> (henceforth: PL2015).",
  "y": "uses"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_18",
  "x": "Next, Algorithm 1 Recursive paraphrasing of a Lambda DCS logical form. The + operation is defined as string concatenation with spaces. Details about Lambda DCS language can be found in<cite> [Liang, 2013]</cite> return t t is the textual paraphrase of the Lambda DCS logical form 18: end procedure information from the KG facilitates the process of parsing a question into a set of candidate logical forms.",
  "y": "background"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_19",
  "x": "Details about Lambda DCS language can be found in<cite> [Liang, 2013]</cite> return t t is the textual paraphrase of the Lambda DCS logical form 18: end procedure information from the KG facilitates the process of parsing a question into a set of candidate logical forms. This is done using a semantic parser that recursively builds up logical forms by repeatedly applying deduction rules. Each candidate logical form is represented in Lambda DCS form<cite> [Liang, 2013]</cite> and can be transformed into a SPARQL query, whose execution against the KG yields an answer.",
  "y": "uses"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_20",
  "x": "During training we ignore questions for which a single matching pair (q, t) is not present. The percentage of questions for which a candidate logical form exists that evaluates to the correct answer is called oracle score. PL2015 report an oracle score of 76.7%, but a manual annotation by<cite> [Pasupat and Liang, 2015]</cite> reveals that PL2015 can answer only 53.5% of the questions correctly.",
  "y": "background"
 },
 {
  "id": "68b5e39365b153dd2bef32845617f2_21",
  "x": "We evaluate the model every 500 steps on the validation set, and choose the best performing model after reaching 50,000 training steps using the early stopping procedure. Each model variant is trained eight times and the best one of each variant is eventually run against the test set. Table 1 shows the performance of our models compared to Neural Programmer<cite> [Neelakantan et al., 2016]</cite> and PL2015<cite> [Pasupat and Liang, 2015]</cite> baselines.",
  "y": "uses"
 },
 {
  "id": "69857bcd5ba67cb7ca0b4344a3a85f_0",
  "x": "Those that use manually written rules model only the most visible discourse constraints (e.g., the discourse connective \"although\" marks a CONCESSION relation), while being oblivious to fine-grained lexical indicators. And the methods that utilize manually annotated corpora (Carlson et al., 2003; Karamanis et al., 2004) and supervised learning algorithms have high costs associated with the annotation procedure, and cannot be easily adapted to different domains and genres. In contrast, more recent research has focused on stochastic approaches that model discourse coherence at the local lexical (Lapata, 2003) and global levels<cite> (Barzilay and Lee, 2004)</cite> , while preserving regularities recognized by classic discourse theories (Barzilay and Lapata, 2005) .",
  "y": "background"
 },
 {
  "id": "69857bcd5ba67cb7ca0b4344a3a85f_1",
  "x": "As each of these stochastic models captures different aspects of coherence, an important question is whether we can combine them in a model capable of exploiting all coherence indicators. A frequently used testbed for coherence models is the discourse ordering problem, which occurs often in text generation, complex question answering, and multi-document summarization: given discourse units, what is the most coherent ordering of them (Marcu, 1996; Lapata, 2003;<cite> Barzilay and Lee, 2004</cite>; Barzilay and Lapata, 2005) ? Because the problem is NP-complete (Althaus et al., 2005) , it is critical how coherence model evaluation is intertwined with search: if the search for the best ordering is greedy and has many errors, one is not able to properly evaluate whether a model is better than another. If the search is exhaustive, the ordering procedure may take too long to be useful.",
  "y": "motivation"
 },
 {
  "id": "69857bcd5ba67cb7ca0b4344a3a85f_2",
  "x": "One of the most frequently used metrics for the automatic evaluation of document coherence is Kendall's (Lapata, 2003;<cite> Barzilay and Lee, 2004)</cite> . TAU measures the minimum number of adjacent transpositions needed to transform a proposed order into a reference order. The range of the TAU metric is between -1 (the worst) to 1 (the best).",
  "y": "background"
 },
 {
  "id": "69857bcd5ba67cb7ca0b4344a3a85f_3",
  "x": "---------------------------------- **EVALUATION SETTING** The task on which we conduct our evaluation is information ordering (Lapata, 2003;<cite> Barzilay and Lee, 2004</cite>; Barzilay and Lapata, 2005) .",
  "y": "uses"
 },
 {
  "id": "69857bcd5ba67cb7ca0b4344a3a85f_4",
  "x": "We evaluated the performance of several search algorithms across four stochastic models of document coherence: the IBM \u00a3 and IBM \u00a3 coherence models, the content model of<cite> Barzilay and Lee (2004)</cite> (CM) , and the entity-based model of Barzilay and Lapata (2005) (EB) (Section 2). We measure search performance using an Estimated Search Error (ESE) figure, which reports the percentage of times when the search algorithm proposes a sentence order which scores lower than Overall performance TAU QUAKES ACCID. Lapata (2003) 0.48 0.07 <cite>Barzilay & Lee (2004)</cite> 0.81 0.44 Barzilay & Lee (reproduced) 0.39 0.36 Barzilay & Lapata (2005) 0 the original sentence order (OSO).",
  "y": "uses"
 },
 {
  "id": "69857bcd5ba67cb7ca0b4344a3a85f_6",
  "x": "We evaluated the performance of several search algorithms across four stochastic models of document coherence: the IBM \u00a3 and IBM \u00a3 coherence models, the content model of<cite> Barzilay and Lee (2004)</cite> (CM) , and the entity-based model of Barzilay and Lapata (2005) (EB) (Section 2). We measure search performance using an Estimated Search Error (ESE) figure, which reports the percentage of times when the search algorithm proposes a sentence order which scores lower than Overall performance TAU QUAKES ACCID. Lapata (2003) 0.48 0.07 <cite>Barzilay & Lee (2004)</cite> 0.81 0.44 Barzilay & Lee (reproduced) 0.39 0.36 Barzilay & Lapata (2005) 0 the original sentence order (OSO).",
  "y": "uses"
 },
 {
  "id": "69857bcd5ba67cb7ca0b4344a3a85f_7",
  "x": "The last comparison we provide is between the performance provided by our framework and previously-reported performance results (Table 3) . We are able to provide this comparison based on the TAU figures reported in<cite> (Barzilay and Lee, 2004)</cite> . The training and test data for both genres is the same, and therefore the figures can be directly compared.",
  "y": "uses"
 },
 {
  "id": "69857bcd5ba67cb7ca0b4344a3a85f_8",
  "x": "The training and test data for both genres is the same, and therefore the figures can be directly compared. These figures account for combined model and search performance. We first note that, unfortunately, we failed to accurately reproduce the model of<cite> Barzilay and Lee (2004)</cite> .",
  "y": "differences"
 },
 {
  "id": "69857bcd5ba67cb7ca0b4344a3a85f_9",
  "x": "The large difference on the EARTHQUAKES corpus between the performance of<cite> Barzilay and Lee (2004)</cite> and our reproduction of their model is responsible for the overall lower performance (0.47) of our log-linear \u00a9 model and IDL-CH-HB \u00a3 V r V search algorithm, which is nevertheless higher than that of its component model CM (0.39). On the other hand, we achieve the highest accuracy figure (0.50) on the ACCIDENTS corpus, outperforming the previous-highest figure (0.44) of<cite> Barzilay and Lee (2004)</cite> . These result empirically show that utility-trained log-linear models of discourse coherence outperform each of the individual coherence models considered.",
  "y": "differences"
 },
 {
  "id": "69857bcd5ba67cb7ca0b4344a3a85f_10",
  "x": "On the other hand, we achieve the highest accuracy figure (0.50) on the ACCIDENTS corpus, outperforming the previous-highest figure (0.44) of<cite> Barzilay and Lee (2004)</cite> . These result empirically show that utility-trained log-linear models of discourse coherence outperform each of the individual coherence models considered. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "69857bcd5ba67cb7ca0b4344a3a85f_11",
  "x": "The genetic algorithms of Mellish et al. (1998) and Karamanis and Manarung (2002) , as well as the greedy algorithm of Lapata (2003) , provide no theoretical guarantees on the optimality of the solutions they propose. At the other end of the spectrum, the exhaustive search of<cite> Barzilay and Lee (2004)</cite> , while ensuring optimal solutions, is prohibitively expensive, and cannot be used to perform utility-based training. The linear programming algorithm of Althaus et al. (2005) is the only proposal that achieves both good speed and accuracy.",
  "y": "background"
 },
 {
  "id": "69857bcd5ba67cb7ca0b4344a3a85f_12",
  "x": "Our generation algorithms are fundamentally different from previously-proposed algorithms for discourse generation. At the other end of the spectrum, the exhaustive search of<cite> Barzilay and Lee (2004)</cite> , while ensuring optimal solutions, is prohibitively expensive, and cannot be used to perform utility-based training.",
  "y": "differences"
 },
 {
  "id": "6a054953660e465151e4d8a2223a76_0",
  "x": "An alternative way of obtaining a vector space with few dimensions, usually with just 100-500, is the use of SVD as a part of Latent Semantic Analysis (Dumais, 2004) or another models such as SGNS (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) . However, these models take more time to instantiate in comparison to weighting of a co-occurrence matrix, bring more parameters to explore and produce vector spaces with uninterpretable dimensions (vector space dimension interpretation is used by some lexical mod-els, for example, McGregor et al. (2015) , and the passage from formal semantics to tensor models relies on it (Coecke et al., 2010) ). In this work we focus on vector spaces that directly weight a co-occurrence matrix and report results for SVD, GloVe and SGNS from the study of<cite> Levy et al. (2015)</cite> for comparison.",
  "y": "uses"
 },
 {
  "id": "6a054953660e465151e4d8a2223a76_1",
  "x": "The mismatch of recent experiments with nondense models in vector dimensionality between lexical and compositional tasks gives rise to a number of questions: Can the findings of<cite> Levy et al. (2015)</cite> be directly applied to models with a few thousand dimensions?",
  "y": "motivation"
 },
 {
  "id": "6a054953660e465151e4d8a2223a76_2",
  "x": "Many approaches use only positive PMI values, as negative PMI values may not positively contribute to model performance and sparser matrices are more computationally tractable (Turney and Pantel, 2010) . This can be generalised to an additional cutoff parameter k (neg) following<cite> Levy et al. (2015)</cite> , giving our third PMI variant (abbreviated as SPMI): 2 When k = 1 SPMI is equivalent to positive PMI.",
  "y": "uses"
 },
 {
  "id": "6a054953660e465151e4d8a2223a76_3",
  "x": "---------------------------------- **FREQUENCY WEIGHTING (FREQ)** Another issue with PMI is its bias towards rare events <cite>(Levy et al., 2015)</cite> ; one way of solving this issue is to weight the value by the co-occurrence frequency (Evert, 2005) :",
  "y": "motivation"
 },
 {
  "id": "6a054953660e465151e4d8a2223a76_4",
  "x": "We evaluate these heuristics by comparing the performance they give on SimLex-999 against that obtained using the best possible parameter selections (determined via an exhaustive search at each dimensionality setting). We also compare them to the best scores reported by<cite> Levy et al. (2015)</cite> for their model (PMI and SVD), word2vec-SGNS (Mikolov et al., 2013) and GloVe (Pennington et al., 2014 )-see Figure 3a , where only the betterperforming SPMI and SCPMI are shown. For lognPMI and lognCPMI, our heuristics pick the best possible models.",
  "y": "uses"
 },
 {
  "id": "6a054953660e465151e4d8a2223a76_5",
  "x": "For 1SPMI and nSPMI the difference is higher. With lognSCPMI and 1SCPMI, the heuristics follow<cite> Levy et al. (2015)</cite> . We also give our best score, SVD, SGNS and GloVe numbers from that study for comparison.",
  "y": "uses"
 },
 {
  "id": "6a054953660e465151e4d8a2223a76_6",
  "x": "With lognSCPMI and 1SCPMI, the heuristics follow<cite> Levy et al. (2015)</cite> . We also give our best score, SVD, SGNS and GloVe numbers from that study for comparison. On the right, our heuristic in comparison to the best and average results together with the models selected using the recommendations presented in<cite> Levy et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "6a054953660e465151e4d8a2223a76_7",
  "x": "SPMI and SCPMI should be preferred over CPMI and PMI. As Figure 3b shows, our heuristics give performance close to the optimum for any dimensionality, with a large improvement over both an average parameter setting and the parameters suggested by<cite> Levy et al. (2015)</cite> in a high-dimensional setting. 4 Finally, to see whether the heuristics transfer robustly, we repeat this comparison on the MEN dataset (see Figures 3c, 3d) .",
  "y": "differences"
 },
 {
  "id": "6a054953660e465151e4d8a2223a76_8",
  "x": "This paper presents a systematic study of cooccurrence quantification focusing on the selection of parameters presented in<cite> Levy et al. (2015)</cite> . We replicate their recommendation for high-dimensional vector spaces, and show that with appropriate parameter selection it is possible to achieve comparable performance with spaces of dimensionality of 1K to 50K, and propose a set of model selection heuristics that maximizes performance. We foresee the results of the paper are generalisable to other experiments, since model selection was performed on a similarity dataset, and was additionally tested on a relatedness dataset.",
  "y": "uses"
 },
 {
  "id": "6a054953660e465151e4d8a2223a76_10",
  "x": "**CONCLUSION** This paper presents a systematic study of cooccurrence quantification focusing on the selection of parameters presented in<cite> Levy et al. (2015)</cite> . We replicate their recommendation for high-dimensional vector spaces, and show that with appropriate parameter selection it is possible to achieve comparable performance with spaces of dimensionality of 1K to 50K, and propose a set of model selection heuristics that maximizes performance.",
  "y": "extends"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_0",
  "x": "We describe DCU's LFG dependencybased metric submitted to the shared evaluation task of WMT-MetricsMATR 2010. The metric is built on the LFG F-structurebased approach presented in <cite>(Owczarzak et al., 2007)</cite> . We explore the following improvements on the original metric: 1) we replace the in-house LFG parser with an open source dependency parser that directly parses strings into LFG dependencies; 2) we add a stemming module and unigram paraphrases to strengthen the aligner; 3) we introduce a chunk penalty following the practice of METEOR to reward continuous matches; and 4) we introduce and tune parameters to maximize the correlation with human judgement.",
  "y": "extends"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_1",
  "x": "Owczarzak et al. (2007) extended this line of research with the use of a term-based encoding of Lexical Functional Grammar (LFG: (Kaplan and Bresnan, 1982) ) labelled dependency graphs into unordered sets of dependency triples, and calculating precision, recall, and F-score on the triple sets corresponding to the translation and reference sentences. With the addition of partial matching and n-best parses,<cite> Owczarzak et al. (2007)</cite> 's method considerably outperforms Liu and Gildea's (2005) w.r.t. correlation with human judgement.",
  "y": "background"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_2",
  "x": "With the addition of partial matching and n-best parses,<cite> Owczarzak et al. (2007)</cite> 's method considerably outperforms Liu and Gildea's (2005) w.r.t. correlation with human judgement. The EDPM metric (Kahn et al., 2010) improves this line of research by using arc labels derived from a Probabilistic Context-Free Grammar (PCFG) parse to replace the LFG labels, showing that a PCFG parser is sufficient for preprocessing, compared to a dependency parser in (Liu and Gildea, 2005) and <cite>(Owczarzak et al., 2007)</cite> .",
  "y": "background"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_3",
  "x": "Besides the metrics that rely solely on the dependency structures, information from the dependency parser is a component of some other metrics that use more diverse resources, such as the textual entailment-based metric of (Pado et al., 2009) . In this paper we extend the work of <cite>(Owczarzak et al., 2007)</cite> in a different manner: we use an adapted version of the Malt parser (Nivre et al., 2006) to produce 1-best LFG dependencies and allow triple matches where the dependency labels are different. We incorporate stemming, synonym and paraphrase information as in (Kahn et al., 2010) , and at the same time introduce a chunk penalty in the spirit of METEOR to penalize discontinuous matches.",
  "y": "extends"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_4",
  "x": "---------------------------------- **THE DEPENDENCY-BASED METRIC** In this section, we briefly review the metric presented in <cite>(Owczarzak et al., 2007)</cite> .",
  "y": "uses"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_5",
  "x": "---------------------------------- **MT EVALUATION AS DEPENDENCY TRIPLE MATCHING** The basic method of <cite>(Owczarzak et al., 2007)</cite> can be illustrated by the example in Table 1 .",
  "y": "uses"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_6",
  "x": "The basic method of <cite>(Owczarzak et al., 2007)</cite> can be illustrated by the example in Table 1 . The metric in <cite>(Owczarzak et al., 2007)</cite> performs triple matching over the Hyp-and Ref-Triples and calculates the metric score using the F-score of matching precision and recall. Let m be the number of matches, h be the number of triples in the hypothesis and e be the number of triples in the reference.",
  "y": "background"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_7",
  "x": "The score of the hypothesis in <cite>(Owczarzak et al., 2007)</cite> is the Fscore based on the precision and recall of matching as in (1):<cite> Owczarzak et al., 2007)</cite> uses several techniques to facilitate triple matching. First of all, considering that the MT-generated hypotheses have variable quality and are sometimes ungrammatical, the metric will search the 50-best parses of both the hypothesis and reference and use the pair that has the highest F-score to compensate for parser noise. Secondly, the metric performs complete or partial matching according to the dependency labels, so the metric will find more matches on dependency structures that are presumably more informative.",
  "y": "background"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_8",
  "x": "The metric described in <cite>(Owczarzak et al., 2007)</cite> uses the DCU LFG parser (Cahill et al., 2004) to produce LFG dependency triples. The parser uses a Penn treebank-trained parser to produce c-structures (constituency trees) and an LFG fstructure annotation algorithm on the c-structure to obtain f-structures. In <cite>(Owczarzak et al., 2007)</cite> , triple matching on f-structures produced by this paradigm correlates well with human judgement, but this paradigm is not adequate for the WMTMetricsMatr evaluation in two respects: 1) the inhouse LFG annotation algorithm is not publicly available and 2) the speed of this paradigm is not satisfactory.",
  "y": "background"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_9",
  "x": "The parser uses a Penn treebank-trained parser to produce c-structures (constituency trees) and an LFG fstructure annotation algorithm on the c-structure to obtain f-structures. In <cite>(Owczarzak et al., 2007)</cite> , triple matching on f-structures produced by this paradigm correlates well with human judgement, but this paradigm is not adequate for the WMTMetricsMatr evaluation in two respects: 1) the inhouse LFG annotation algorithm is not publicly available and 2) the speed of this paradigm is not satisfactory. We instead use the Malt Parser 1 (Nivre et al., 2006 ) with a parsing model trained on LFG dependencies to produce the f-structure triples.",
  "y": "background"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_10",
  "x": "The parser uses a Penn treebank-trained parser to produce c-structures (constituency trees) and an LFG fstructure annotation algorithm on the c-structure to obtain f-structures. In <cite>(Owczarzak et al., 2007)</cite> , triple matching on f-structures produced by this paradigm correlates well with human judgement, but this paradigm is not adequate for the WMTMetricsMatr evaluation in two respects: 1) the inhouse LFG annotation algorithm is not publicly available and 2) the speed of this paradigm is not satisfactory. We instead use the Malt Parser 1 (Nivre et al., 2006 ) with a parsing model trained on LFG dependencies to produce the f-structure triples.",
  "y": "differences"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_11",
  "x": "Currently our parser produces only the 1-best outputs. Compared to the 50-best parses in <cite>(Owczarzak et al., 2007)</cite> , the 1-best parse limits the number of triple matches that can be found. To compensate for this, we allow triple matches that have the same Head and Modifier to constitute a match, even if their dependency labels are different.",
  "y": "differences"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_12",
  "x": "---------------------------------- **CAPTURING VARIATIONS IN LANGUAGE** In <cite>(Owczarzak et al., 2007)</cite> , lexical variations at the word-level are captured by WordNet.",
  "y": "background"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_13",
  "x": "**CAPTURING VARIATIONS IN LANGUAGE** In <cite>(Owczarzak et al., 2007)</cite> , lexical variations at the word-level are captured by WordNet. We use a Porter stemmer and a unigram paraphrase database to allow more lexical variations.",
  "y": "extends"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_14",
  "x": "The metric described in <cite>(Owczarzak et al., 2007)</cite> does not explicitly consider word order and fluency. METEOR, on the other hand, utilizes this information through a chunk penalty. We introduce a chunk penalty to our dependency-based metric following METEOR's string-based approach.",
  "y": "background"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_15",
  "x": "The metric described in <cite>(Owczarzak et al., 2007)</cite> does not explicitly consider word order and fluency. METEOR, on the other hand, utilizes this information through a chunk penalty. We introduce a chunk penalty to our dependency-based metric following METEOR's string-based approach.",
  "y": "differences"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_16",
  "x": "We experiment with four settings of the metric: HARD, SOFT, SOFTALL and WEIGHTED in order to validate our enhancements. The first two settings compare the effect of allowing/not allowing soft matches, but only uses WordNet as in <cite>(Owczarzak et al., 2007)</cite> . The third setting applies our additional linguistic features and the final setting tunes parameter weights for higher correlation with human judgement.",
  "y": "uses"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_17",
  "x": "In this paper we describe DCU's dependencybased MT evaluation metric submitted to WMTMetricsMATR 2010. Building upon the LFGbased metric described in <cite>(Owczarzak et al., 2007)</cite> , we use a publicly available parser instead of an in-house parser to produce dependency labels, so that the metric can run on a third party machine. We improve the metric by allowing more lexical variations and weighting dependency triple matches depending on their importance according to correlation with human judgement.",
  "y": "extends"
 },
 {
  "id": "6b11cfba6ee73c1f67941cf73506be_18",
  "x": "In this paper we describe DCU's dependencybased MT evaluation metric submitted to WMTMetricsMATR 2010. Building upon the LFGbased metric described in <cite>(Owczarzak et al., 2007)</cite> , we use a publicly available parser instead of an in-house parser to produce dependency labels, so that the metric can run on a third party machine. We improve the metric by allowing more lexical variations and weighting dependency triple matches depending on their importance according to correlation with human judgement.",
  "y": "differences"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_0",
  "x": "We focus on character n-grams based on research in the field of word embedding construction<cite> (Wieting et al. 2016)</cite> . Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103.",
  "y": "uses"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_1",
  "x": "**INTRODUCTION** Neural language models have played a crucial role in recent advances of neural network based methods in natural language processing (NLP). For example, neural encoderdecoder models, which are becoming the de facto standard for various natural language generation tasks including machine translation (Sutskever, Vinyals, and Le 2014) , summarization (Rush, Chopra, and Weston 2015) , dialogue <cite>(Wen et al. 2015)</cite> , and caption generation<cite> (Vinyals et al. 2015)</cite> can be interpreted as conditional neural language models.",
  "y": "background"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_2",
  "x": "This implies that better neural language models improve the performance of application tasks. In general, neural language models require word embeddings as an input<cite> (Zaremba, Sutskever, and Vinyals 2014)</cite>. However, as described by<cite> (Verwimp et al. 2017)</cite> , this approach cannot make use of the internal structure of words although the internal structure is often an effective clue for considering the meaning of a word.",
  "y": "background"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_3",
  "x": "In general, neural language models require word embeddings as an input<cite> (Zaremba, Sutskever, and Vinyals 2014)</cite>. However, as described by<cite> (Verwimp et al. 2017)</cite> , this approach cannot make use of the internal structure of words although the internal structure is often an effective clue for considering the meaning of a word. For example, we can comprehend that the word 'causal' is related to 'cause' immediately because both words include the same character sequence 'caus'.",
  "y": "background"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_4",
  "x": "In general, neural language models require word embeddings as an input<cite> (Zaremba, Sutskever, and Vinyals 2014)</cite>. However, as described by<cite> (Verwimp et al. 2017)</cite> , this approach cannot make use of the internal structure of words although the internal structure is often an effective clue for considering the meaning of a word. For example, we can comprehend that the word 'causal' is related to 'cause' immediately because both words include the same character sequence 'caus'. Thus, if we incorporate a method that handles the internal structure such as character information, we can improve the quality of neural language models and probably make them robust to infrequent words.",
  "y": "motivation"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_5",
  "x": "Thus, if we incorporate a method that handles the internal structure such as character information, we can improve the quality of neural language models and probably make them robust to infrequent words. To incorporate the internal structure,<cite> (Verwimp et al. 2017)</cite> concatenated character embeddings with an input word embedding. They demonstrated that incorporating character embeddings improved the performance of RNN language models.",
  "y": "background"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_6",
  "x": "On the other hand, in the field of word embedding construction, some previous researchers found that character n-grams are more useful than single characters<cite> (Wieting et al. 2016</cite>; Bojanowski et al. 2017) . In particular,<cite> (Wieting et al. 2016)</cite> demonstrated that constructing word embeddings from character n-gram embeddings outperformed the methods that construct word embeddings from character embeddings by using CNN or a Long Short-Term Memory (LSTM). Based on their reports, in this paper, we propose a neural language model that utilizes character n-gram embeddings.",
  "y": "background"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_7",
  "x": "On the other hand, in the field of word embedding construction, some previous researchers found that character n-grams are more useful than single characters<cite> (Wieting et al. 2016</cite>; Bojanowski et al. 2017) . In particular,<cite> (Wieting et al. 2016)</cite> demonstrated that constructing word embeddings from character n-gram embeddings outperformed the methods that construct word embeddings from character embeddings by using CNN or a Long Short-Term Memory (LSTM). Based on their reports, in this paper, we propose a neural language model that utilizes character n-gram embeddings.",
  "y": "background"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_8",
  "x": "On the other hand, in the field of word embedding construction, some previous researchers found that character n-grams are more useful than single characters<cite> (Wieting et al. 2016</cite>; Bojanowski et al. 2017) . In particular,<cite> (Wieting et al. 2016)</cite> demonstrated that constructing word embeddings from character n-gram embeddings outperformed the methods that construct word embeddings from character embeddings by using CNN or a Long Short-Term Memory (LSTM). Based on their reports, in this paper, we propose a neural language model that utilizes character n-gram embeddings.",
  "y": "uses"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_9",
  "x": "**INCORPORATING CHARACTER N-GRAM EMBEDDINGS** We incorporate charn-MS-vec, which is an embedding constructed from character n-gram embeddings, into RNN language models since, as discussed earlier, previous studies revealed that we can construct better word embeddings by using character n-gram embeddings<cite> (Wieting et al. 2016</cite>; Bojanowski et al. 2017 ). In particular, we expect charn-MSvec to help represent infrequent words by taking advantage of the internal structure.",
  "y": "uses"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_10",
  "x": "To compute c t , we apply an encoder to character n-gram embeddings. Previous studies demonstrated that additive composition, which computes the (weighted) sum of embeddings, is a suitable method for embedding construction<cite> Wieting et al. 2016</cite> the number of character n-grams extracted from the word, and let S be the matrix whose i-th column corresponds to s i , that is, S = [s 1 , ..., s I ]. The multi-dimensional self-attention constructs the word embedding c t by the following equations:",
  "y": "background"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_11",
  "x": "Tables 6, 7 , and 8, respectively, show perplexities of the proposed method and previous studies on PTB, WT2, and WT103 6 . Since AWD-LSTM-MoS<cite> (Yang et al. 2018</cite> ) and AWD-LSTM-DOC<cite> (Takase, Suzuki, and Nagata 2018)</cite> achieved the stateof-the-art scores on PTB and WT2, we combined char3-MSvec with them. These tables show that the proposed method improved the performance of the base model and outperformed the state-of-the-art scores on all datasets.",
  "y": "uses"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_12",
  "x": "(Melis, Dyer, and Blunsom 2018) demonstrated that the standard LSTM can achieve superior performance by selecting appropriate hyperparameters. Finally, (Merity, Keskar, and Socher 2018b) introduced DropConnect<cite> (Wan et al. 2013</cite> ) and averaged SGD (Polyak and Juditsky 1992) into the LSTM language model and achieved state-of-the-art perplexities on PTB and WT2. For WT103, (Merity, Keskar, and Socher 2018a) Table 10 : ROUGE F1 scores on the headline generation test sets provided by (Zhou et al. 2017) and (Kiyono et al. 2017) .",
  "y": "uses"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_13",
  "x": "( Kim et al. 2016 ) introduced character information into RNN language models. They applied CNN to character embeddings for word embedding construction. Their proposed method achieved perplexity competitive with the basic LSTM language model<cite> (Zaremba, Sutskever, and Vinyals 2014)</cite> even though its parameter size is small.",
  "y": "background"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_14",
  "x": "(Luong, Socher, and Manning 2013) applied Recursive Neural Networks to construct word embeddings from morphemic embeddings. (Ling et al. 2015) applied bidirectional LSTMs to character embeddings for word embedding construction. On the other hand, (Bojanowski et al. 2017 ) and<cite> (Wieting et al. 2016)</cite> focused on character n-gram.",
  "y": "background"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_15",
  "x": "In addition,<cite> (Wieting et al. 2016)</cite> found that the sum of character n-gram embeddings also outperformed word embeddings constructed from character embeddings with CNN and LSTM. As an encoder, previous studies argued that additive composition, which computes the (weighted) sum of embeddings, is a suitable method theoretically (Tian, Okazaki, and Inui 2016) and empirically (Muraoka et al. 2014; . In this paper, we used multidimensional self-attention to construct word embeddings because it can be interpreted as an element-wise weighted sum.",
  "y": "background"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_16",
  "x": "In addition,<cite> (Wieting et al. 2016)</cite> found that the sum of character n-gram embeddings also outperformed word embeddings constructed from character embeddings with CNN and LSTM. As an encoder, previous studies argued that additive composition, which computes the (weighted) sum of embeddings, is a suitable method theoretically (Tian, Okazaki, and Inui 2016) and empirically (Muraoka et al. 2014; . In this paper, we used multidimensional self-attention to construct word embeddings because it can be interpreted as an element-wise weighted sum.",
  "y": "motivation"
 },
 {
  "id": "6b1432f4aac35e6acd8ca8770fe484_17",
  "x": "Based on the research in the field of word embedding construction<cite> (Wieting et al. 2016)</cite> , we focused on character n-gram embeddings to construct word embeddings. We used multi-dimensional self-attention (Shen et al. 2018 ) to encode character n-gram embeddings. Our proposed charn-MS-vec improved the performance of stateof-the-art RNN language models and achieved the best perplexities on Penn Treebank, WikiText-2, and WikiText-103.",
  "y": "uses"
 },
 {
  "id": "6bb7d5f16861470214626c1cc497bb_0",
  "x": "There has been work on factors leading to speaker commitment in theoretical linguistics (i.a., Karttunen (1971) ; Simons et al. (2010) ) and computational linguistics (i.a., Diab et al. (2009) ; Saur\u00ed and Pustejovsky (2012) ; Prabhakaran et al. (2015) ), but mostly on constructed or newswire examples, which may simplify the task by failing to reflect the lexical and syntactic diversity of naturally occurring utterances. de Marneffe et al. (2019) introduced the CommitmentBank, a dataset of naturally occurring sentences annotated with speaker commitment towards the content of complements of clause-embedding verbs under canceling-entailment environments (negation, modal, question and conditional), to study the linguistic correlates of speaker commitment. In this paper, we use it to evaluate two state-of-the-art (SoA) models of speaker commitment:<cite> Stanovsky et al. (2017)</cite> and .",
  "y": "uses"
 },
 {
  "id": "6bb7d5f16861470214626c1cc497bb_1",
  "x": "We evaluate the performance of two speaker commitment models on the CommitmentBank: a rulebased model<cite> (Stanovsky et al., 2017</cite> ) and a neuralbased one . Rule-based model<cite> Stanovsky et al. (2017)</cite> proposed a rule-based model based on a deterministic algorithm based on TruthTeller (Lotan et al., 2013) , which uses a top-down approach on a de- pendency tree and predicts speaker commitment score in [\u22123, 3] according to the implicative signatures (Karttunen, 2012) of the predicates, and whether the predicates are under the scope of negation and uncertainty modifiers. For example, refuse p entails \u00acp, so the factuality of its complement p gets flipped if encountered.",
  "y": "uses"
 },
 {
  "id": "6bb7d5f16861470214626c1cc497bb_2",
  "x": "Rule-based model<cite> Stanovsky et al. (2017)</cite> proposed a rule-based model based on a deterministic algorithm based on TruthTeller (Lotan et al., 2013) , which uses a top-down approach on a de- pendency tree and predicts speaker commitment score in [\u22123, 3] according to the implicative signatures (Karttunen, 2012) of the predicates, and whether the predicates are under the scope of negation and uncertainty modifiers. For example, refuse p entails \u00acp, so the factuality of its complement p gets flipped if encountered. Neural-based model introduced three neural models for speaker commitment: a linear biLSTM, a dependency tree biL-STM, a hybrid model that ensembles the two. also proposed a multitask training scheme in which a model is trained on four factuality datasets: FactBank (Saur\u00ed and Pustejovsky, 2009) , UW (Lee et al., 2015) , MEAN-TIME (Minard et al., 2016) and UDS , all with annotations on a [\u22123, 3] scale.",
  "y": "background"
 },
 {
  "id": "6bb7d5f16861470214626c1cc497bb_3",
  "x": "**MODELS OF SPEAKER COMMITMENT** We evaluate the performance of two speaker commitment models on the CommitmentBank: a rulebased model<cite> (Stanovsky et al., 2017</cite> ) and a neuralbased one . Rule-based model<cite> Stanovsky et al. (2017)</cite> proposed a rule-based model based on a deterministic algorithm based on TruthTeller (Lotan et al., 2013) , which uses a top-down approach on a de- pendency tree and predicts speaker commitment score in [\u22123, 3] according to the implicative signatures (Karttunen, 2012) of the predicates, and whether the predicates are under the scope of negation and uncertainty modifiers.",
  "y": "uses background"
 },
 {
  "id": "6bb7d5f16861470214626c1cc497bb_4",
  "x": "The score on UW with MAE was obtained by<cite> Stanovsky et al. (2017)</cite> , while the other scores were obtained by . and Pearson's r correlation, measuring how well the model captures variability in the data. Pearson's r is considered more informative than MAE because the reference sets are biased towards +3.",
  "y": "uses"
 },
 {
  "id": "6bb7d5f16861470214626c1cc497bb_5",
  "x": "**ANALYSIS** Focusing on the restricted set, we perform detailed error analysis of the outputs of the rule-based and hybrid biLSTM models, which achieved the best Figure 3: Pearson r correlation and Mean Absolute Error (MAE) on All -2.0 baseline, Rule-based annotator<cite> (Stanovsky et al., 2017)</cite> , and three biLSTM models in . Pearson r is undefined for All -2.0.",
  "y": "uses"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_0",
  "x": "Lee et al. (2017; <cite>Lee et al. (2018)</cite> first introduced a neural mention detector as a part of their end-to-end coreference system; however, the system does not output intermediate mentions, hence the mention detector cannot be used by other coreference systems directly. To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016) , they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system (Lee et al., 2013) .",
  "y": "background"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_1",
  "x": "In this paper, we compare three neural architectures for MD. The first system is a slightly modified version of the mention detection part of the <cite>Lee et al. (2018)</cite> system. The second system employs a bi-directional LSTM on the sentence level and uses biaffine attention (Dozat and Manning, 2017) over the LSTM outputs to predict the mentions.",
  "y": "uses"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_2",
  "x": "Thirdly, by using better mentions from our mention detector, we can improve the end-to-end <cite>Lee et al. (2018)</cite> system and the Clark and Manning (2016a) pipeline system by up to 0.7% and 1.7% respectively. ---------------------------------- **RELATED WORK**",
  "y": "extends"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_3",
  "x": "This move proved very effective; however, as a result the mention detection part of their system needs to be trained jointly with the coreference resolution part, hence can not be used separately. The system has been later extended by Zhang et al. (2018) and <cite>Lee et al. (2018)</cite> . Zhang et al. (2018) added biaffine attention to the coreference part of the Lee et al. (2017) system, improving the system by 0.6%.",
  "y": "background"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_4",
  "x": "The HIGH RECALL mode, on the other hand, predicts as many mentions as possible, which is more appropriate for preprocessing for a coreference system since mentions can be further filtered by the system during coreference resolution. In HIGH F1 mode we output mentions whose probability p m (i) is larger then a threshold \u03b2 such as 0.5. In HIGH RECALL mode we output mentions based on a fixed mention/word ratio \u03bb; this is the same method used by <cite>Lee et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_5",
  "x": "---------------------------------- **LEE MD** Our first system is based on the mention detection part of the <cite>Lee et al. (2018)</cite> system.",
  "y": "uses"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_6",
  "x": "**BASELINE SYSTEM** For the mention detection evaluation we use the <cite>Lee et al. (2018)</cite> system as baseline. The baseline is trained end-toend on the coreference task and we use as baseline the mentions predicted by the system before carrying out coreference resolution.",
  "y": "uses"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_7",
  "x": "For the coreference evaluation we use the state-of-the-art <cite>Lee et al. (2018)</cite> system as our baseline for the end-to-end system, and the Clark and Manning (2016a) system as our baseline for the pipeline system. During the evaluation, we slightly modified the <cite>Lee et al. (2018)</cite> system to allow the system to take the mentions predicted by our model instead of its internal mention detector. Other than that we keep the system unchanged.",
  "y": "uses"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_8",
  "x": "The baseline is trained end-toend on the coreference task and we use as baseline the mentions predicted by the system before carrying out coreference resolution. For the coreference evaluation we use the state-of-the-art <cite>Lee et al. (2018)</cite> system as our baseline for the end-to-end system, and the Clark and Manning (2016a) system as our baseline for the pipeline system. During the evaluation, we slightly modified the <cite>Lee et al. (2018)</cite> system to allow the system to take the mentions predicted by our model instead of its internal mention detector.",
  "y": "extends"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_9",
  "x": "---------------------------------- **HYPERPARAMETERS** For our first model (LEE MD) we use the default settings of <cite>Lee et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_10",
  "x": "For mention detection on the CONLL data set, we first take the best model from <cite>Lee et al. (2018)</cite> and use its default mention/token ratio (\u03bb = 0.4) to output predicted mentions before coreference resolution. We use this as our baseline for the HIGH RE-CALL setting. We then evaluate all three proposed models with the same \u03bb as that of the baseline.",
  "y": "uses"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_11",
  "x": "Evaluation on the CRAC data set 3 For the CRAC data set, we train the <cite>Lee et al. (2018)</cite> system end-to-end on the reduced corpus with singleton mentions removed and extract mentions from the system by set \u03bb = 0.4. We then train our models with the same \u03bb but on the full corpus, since our mention detectors naturally support both mention 3 As the <cite>Lee et al. (2018)</cite> system does not predict singleton mentions, the results on CRAC data set in Table 2 are evaluated without singleton mentions. While the results reported in Table 3 are evaluated with singleton mentions included.",
  "y": "uses"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_12",
  "x": "We then train our models with the same \u03bb but on the full corpus, since our mention detectors naturally support both mention 3 As the <cite>Lee et al. (2018)</cite> system does not predict singleton mentions, the results on CRAC data set in Table 2 are evaluated without singleton mentions. While the results reported in Table 3 are evaluated with singleton mentions included. 88.0 89.7 89.1 Table 3 : Comparison between our BIAFFINE MD and the top performing systems on the mention detection task using the CONLL and CRAC data sets.",
  "y": "differences"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_13",
  "x": "We first evaluate our BIAFFINE MD in combination with the end-to-end <cite>Lee et al. (2018)</cite> system. We slightly modified the system to feed the system mentions predicted by our mention detector. As a result, the original mention selection function is switched off, we keep all the other settings (include the mention scoring function) unchanged.",
  "y": "uses"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_14",
  "x": "We first evaluate our BIAFFINE MD in combination with the end-to-end <cite>Lee et al. (2018)</cite> system. We slightly modified the system to feed the system mentions predicted by our mention detector. As a result, the original mention selection function is switched off, we keep all the other settings (include the mention scoring function) unchanged.",
  "y": "differences"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_15",
  "x": "The <cite>Lee et al. (2018)</cite> system is an extended version of the Lee et al. (2017) system, hence they share most of the network architecture. The Lee et al. (2017) has a lower performance on mention detection (93.5% recall when \u03bb = 0.4), which creates a large (4%) difference when compared with the recall of our BIAFFINE MD. We train the system without the joint learning, and the newly trained model achieved an average F1 of 67.7% and this is 0.5 better than the original end-to-end Lee et al. (2017) system (see table 4 ).",
  "y": "background"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_16",
  "x": "For our second experiment, we used the Lee et al. (2017) instead. The <cite>Lee et al. (2018)</cite> system is an extended version of the Lee et al. (2017) system, hence they share most of the network architecture. The Lee et al. (2017) has a lower performance on mention detection (93.5% recall when \u03bb = 0.4), which creates a large (4%) difference when compared with the recall of our BIAFFINE MD.",
  "y": "uses"
 },
 {
  "id": "6c4264bedb6683e909c1e530f22262_17",
  "x": "We further evaluated the <cite>Lee et al. (2018)</cite> system on the CRAC data set. We first train the original <cite>Lee et al. (2018)</cite> on the reduced version (with singletons removed) of the CRAC data set to create a baseline. As we can see from Table 4, the baseline system has an average F1 score of 68.4%.",
  "y": "uses"
 },
 {
  "id": "6c872be6b2fbe83890e28ddc1098a3_0",
  "x": "IRT provides a well-studied methodology for modeling item difficulty as opposed to more heuristic-based difficulty estimates such as sentence length. IRT was previously used to build a new test set for the NLI task<cite> (Lalor et al., 2016)</cite> and show that model performance is dependent on test set difficulty. In this work we use IRT to probe specific items to try to analyze model performance at a more finegrained level, and expand the analysis to include the task of SA.",
  "y": "background"
 },
 {
  "id": "6c872be6b2fbe83890e28ddc1098a3_1",
  "x": "IRT was previously used to build a new test set for the NLI task<cite> (Lalor et al., 2016)</cite> and show that model performance is dependent on test set difficulty. In this work we use IRT to probe specific items to try to analyze model performance at a more finegrained level, and expand the analysis to include the task of SA. We train three DNNs models with varying training set sizes to compare performance on two NLP tasks: NLI and Sentiment Analysis (SA).",
  "y": "extends"
 },
 {
  "id": "6c872be6b2fbe83890e28ddc1098a3_2",
  "x": "**ESTIMATING ITEM DIFFICULTY** To model item difficulty we use the Three Parameter Logistic (3PL) model from IRT (Baker, 2001; Baker and Kim, 2004; <cite>Lalor et al., 2016)</cite> . The 3PL model in IRT models an individual's latent ability (\u03b8) on a task as a function of three item characteristics: discrimination ability (a), difficulty (b), and guessing (c).",
  "y": "uses"
 },
 {
  "id": "6c872be6b2fbe83890e28ddc1098a3_3",
  "x": "---------------------------------- **DATA** To estimate item difficulties for NLI, we used the pre-trained IRT models of<cite> Lalor et al. (2016)</cite> and extracted the difficulty item parameters.",
  "y": "uses"
 },
 {
  "id": "6c872be6b2fbe83890e28ddc1098a3_4",
  "x": "For SA, we collected a new data set of labels for 134 examples randomly selected from the Stanford Sentiment Treebank (SSTB) (Socher et al., 2013) , using a similar AMT setup as<cite> Lalor et al. (2016)</cite> . For each randomly selected example, we had 1000 Turkers label the sentence as very negative, negative, neutral, positive, or very positive. We converted these responses to binary positive/negative labels and fit a new IRT 3PL model ( \u00a72.1) using the mirt R package (Chalmers et al., 2015) .",
  "y": "similarities"
 },
 {
  "id": "6c872be6b2fbe83890e28ddc1098a3_5",
  "x": "Inter-rater reliability scores for the collected annotations are showin in Table 3 . Scores for the NLI annotations were calculated when the original dataset was collected and are reproduced here<cite> (Lalor et al., 2016)</cite> . Human annotations for the SA annotations were converted to binary before calculating the agreement.",
  "y": "uses"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_0",
  "x": "**ABSTRACT** It is shown that many published models for the Stanford Question Answering Dataset (Rajpurkar et al., 2016) lack robustness, suffering an over 50% decrease in F1 score during adversarial evaluation based on the <cite>AddSent</cite> <cite>(Jia and Liang, 2017)</cite> algorithm. It has also been shown that retraining models on data generated by <cite>AddSent</cite> has limited effect on their robustness.",
  "y": "motivation"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_1",
  "x": "It is shown that many published models for the Stanford Question Answering Dataset (Rajpurkar et al., 2016) lack robustness, suffering an over 50% decrease in F1 score during adversarial evaluation based on the <cite>AddSent</cite> <cite>(Jia and Liang, 2017)</cite> algorithm. It has also been shown that retraining models on data generated by <cite>AddSent</cite> has limited effect on their robustness. We propose a novel alternative adversary-generation algorithm, AddSentDiverse, that significantly increases the variance within the adversarial training data by providing effective examples that punish the model for making certain superficial assumptions.",
  "y": "motivation"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_2",
  "x": "It has also been shown that retraining models on data generated by <cite>AddSent</cite> has limited effect on their robustness. We propose a novel alternative adversary-generation algorithm, AddSentDiverse, that significantly increases the variance within the adversarial training data by providing effective examples that punish the model for making certain superficial assumptions. Further, in order to improve robustness to <cite>AddSent</cite>'s semantic perturbations (e.g., antonyms), we jointly improve the model's semantic-relationship learning capabilities in addition to our AddSentDiversebased adversarial training data augmentation.",
  "y": "extends"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_3",
  "x": "Significant progress has been made with deep end to end neural-attention models, with some achieving above human level performance on the test set (Wang and Jiang, 2017; Seo et al., 2017; Huang et al., 2018; Peters et al., 2018) . However, as shown recently by <cite>Jia and Liang (2017)</cite> , these models are very fragile when presented with adversarially generated data. <cite>They</cite> proposed <cite>AddSent</cite>, which creates a semantically-irrelevant sentence containing a fake answer that resembles the question syntactically, and appends it to the context.",
  "y": "motivation"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_4",
  "x": "Significant progress has been made with deep end to end neural-attention models, with some achieving above human level performance on the test set (Wang and Jiang, 2017; Seo et al., 2017; Huang et al., 2018; Peters et al., 2018) . However, as shown recently by <cite>Jia and Liang (2017)</cite> , these models are very fragile when presented with adversarially generated data. <cite>They</cite> proposed <cite>AddSent</cite>, which creates a semantically-irrelevant sentence containing a fake answer that resembles the question syntactically, and appends it to the context.",
  "y": "background"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_5",
  "x": "Many state-ofthe-art models exhibit a nearly 50% reduction in F1 score on <cite>AddSent</cite>, showing their over-reliance on syntactic similarity and limited semantic understanding. Importantly, this is in part due to the nature of the SQuAD dataset. Most questions in the dataset have answer spans embedded in sentences that are syntactically similar to the question.",
  "y": "background"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_6",
  "x": "Importantly, this is in part due to the nature of the SQuAD dataset. Most questions in the dataset have answer spans embedded in sentences that are syntactically similar to the question. Thus during training, the model is rarely punished for answering questions based on syntactic similarity, and learns it as a reliable approach to Q&A. This correlation between syntactic similarity and correctness is of course not true in general: the adversaries generated by <cite>AddSent</cite> <cite>(Jia and Liang, 2017)</cite> are syntactically similar to the question but do not answer them.",
  "y": "background"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_7",
  "x": "The models' failures on <cite>AddSent</cite> demonstrates their ignorance of this aspect of the task. <cite>Jia and Liang (2017)</cite> presented some initial attempts to fix this problem by retraining the BiDAF model (Seo et al., 2017) with adversaries generated with <cite>AddSent</cite>. But <cite>they</cite> showed that the method is not very effective, as slight modifications (e.g., different positioning of the distractor sentence in the paragraph and different fake answer set) to the adversary generation algorithm at test time have drastic impact on the retrained model's performance. In this paper, we show that their method of adversarial training failed because the specificity of the <cite>AddSent</cite> algorithm along with the lack of naturally-occurring counterexamples allow models to learn superficial clues regarding what is a 'distractor' and subsequently ignore it; thus significantly limiting their robustness.",
  "y": "background"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_8",
  "x": "<cite>Jia and Liang (2017)</cite> presented some initial attempts to fix this problem by retraining the BiDAF model (Seo et al., 2017) with adversaries generated with <cite>AddSent</cite>. But <cite>they</cite> showed that the method is not very effective, as slight modifications (e.g., different positioning of the distractor sentence in the paragraph and different fake answer set) to the adversary generation algorithm at test time have drastic impact on the retrained model's performance. In this paper, we show that their method of adversarial training failed because the specificity of the <cite>AddSent</cite> algorithm along with the lack of naturally-occurring counterexamples allow models to learn superficial clues regarding what is a 'distractor' and subsequently ignore it; thus significantly limiting their robustness. Instead, we first introduce a novel algorithm, AddSentDiverse, for generating adversarial examples with signifi-cantly higher variance (by varying the locations where the distractors are placed and expanding the set of fake answers), so that the model is punished during training time for making these superficial assumptions about the distractor.",
  "y": "background"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_9",
  "x": "<cite>Jia and Liang (2017)</cite> presented some initial attempts to fix this problem by retraining the BiDAF model (Seo et al., 2017) with adversaries generated with <cite>AddSent</cite>. But <cite>they</cite> showed that the method is not very effective, as slight modifications (e.g., different positioning of the distractor sentence in the paragraph and different fake answer set) to the adversary generation algorithm at test time have drastic impact on the retrained model's performance. In this paper, we show that their method of adversarial training failed because the specificity of the <cite>AddSent</cite> algorithm along with the lack of naturally-occurring counterexamples allow models to learn superficial clues regarding what is a 'distractor' and subsequently ignore it; thus significantly limiting their robustness. Instead, we first introduce a novel algorithm, AddSentDiverse, for generating adversarial examples with signifi-cantly higher variance (by varying the locations where the distractors are placed and expanding the set of fake answers), so that the model is punished during training time for making these superficial assumptions about the distractor.",
  "y": "motivation"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_10",
  "x": "Instead, we first introduce a novel algorithm, AddSentDiverse, for generating adversarial examples with signifi-cantly higher variance (by varying the locations where the distractors are placed and expanding the set of fake answers), so that the model is punished during training time for making these superficial assumptions about the distractor. We show that an AddSentDiverse-based adversariallytrained model beats an <cite>AddSent</cite>-trained model across 3 different adversarial test sets, showing an average improvement of 24.22% in F1 score, demonstrating a general increase in robustness. However, even with our diversified adversarial training data, the model is still not fully resilient to <cite>AddSent</cite>-style attacks, e.g., its antonymy-style semantic perturbations.",
  "y": "differences"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_11",
  "x": "Instead, we first introduce a novel algorithm, AddSentDiverse, for generating adversarial examples with signifi-cantly higher variance (by varying the locations where the distractors are placed and expanding the set of fake answers), so that the model is punished during training time for making these superficial assumptions about the distractor. We show that an AddSentDiverse-based adversariallytrained model beats an <cite>AddSent</cite>-trained model across 3 different adversarial test sets, showing an average improvement of 24.22% in F1 score, demonstrating a general increase in robustness. However, even with our diversified adversarial training data, the model is still not fully resilient to <cite>AddSent</cite>-style attacks, e.g., its antonymy-style semantic perturbations.",
  "y": "motivation"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_12",
  "x": "Interestingly, we see that these additions only increase model robustness when trained adversarially, because intuitively in the non-adversarially-trained setup, there are not enough negative (adversarial) examples for the model to learn how to use its semantic features. Overall, we demonstrate that with our adversarial training method and model improvement, we can increase the performance of a state-of-theart model by 36.46% on the <cite>AddSent</cite> evaluation set. Although we focused on the <cite>AddSent</cite> adversary <cite>(Jia and Liang, 2017)</cite> , our method of effective adversarial training by eliminating superficial statistical correlations (with joint model capability improvements) are generalizable to other similar insertion-based adversaries for Q&A tasks.",
  "y": "differences"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_13",
  "x": "Although we focused on the <cite>AddSent</cite> adversary <cite>(Jia and Liang, 2017)</cite> , our method of effective adversarial training by eliminating superficial statistical correlations (with joint model capability improvements) are generalizable to other similar insertion-based adversaries for Q&A tasks. 1 ----------------------------------",
  "y": "extends"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_14",
  "x": "Adversarial Evaluation In computer vision, adversarial examples are frequently used to punish model oversensitivity, where semantic-preserving perturbations (usually in the form of small noise vectors) are added to an image to fool the classifier into giving it a different label (Szegedy et al., 2014; Goodfellow et al., 2015) . In the field of Q&A, <cite>Jia and Liang (2017)</cite> introduced the <cite>AddSent</cite> algorithm, which generates adversaries that punish model failure in the other direction: overstability, or the inability to detect semantic-altering noise. It does so by generating distractor sentences that only resemble the questions syntactically and appending them to the context paragraphs (detailed description included in Sec. 3).",
  "y": "background"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_15",
  "x": "In the field of Q&A, <cite>Jia and Liang (2017)</cite> introduced the <cite>AddSent</cite> algorithm, which generates adversaries that punish model failure in the other direction: overstability, or the inability to detect semantic-altering noise. It does so by generating distractor sentences that only resemble the questions syntactically and appending them to the context paragraphs (detailed description included in Sec. 3). When tested on these adversarial examples, <cite>Jia and Liang (2017)</cite> showed that even the most 'robust' amongst published models (the Mnemonic Reader (Hu et al., 2017) ) only achieved 46.6% F1 (compared to 79.6% F1 on the regular task).",
  "y": "motivation"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_16",
  "x": "We show, however, that one can make a pre-existing model significantly more robust by simply retraining it with better, higher variance adversarial training data, and improve it further with minor semantic feature additions to its inputs. Adversarial Training It has been shown in the field of image classification that training with adversarial examples produces more robust and error-resistant models (Goodfellow et al., 2015; Kurakin et al., 2017) . In the field of Q&A, <cite>Jia and Liang (2017)</cite> attempted to retrain the BiDAF (Seo et al., 2017) model with data generated with <cite>AddSent</cite> algorithm.",
  "y": "background"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_17",
  "x": "In the field of Q&A, <cite>Jia and Liang (2017)</cite> attempted to retrain the BiDAF (Seo et al., 2017) model with data generated with <cite>AddSent</cite> algorithm. Despite performing well when evaluated on <cite>AddSent</cite>, the retrained model suffers a more than 30% decrease in F1 performance when tested on a slightly different adversarial dataset generated by AddSentMod (which differs from <cite>AddSent</cite> in two superficial ways: using a different set of fake answers and prepending instead of appending the distractor sentence to the context). We show that using <cite>AddSent</cite> to generate adversarial training data introduces new superficial trends for a model to exploit; and instead we propose the AddSentDiverse algorithm that generates highly varied data for adversarial training, resulting in more robust models.",
  "y": "differences"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_18",
  "x": "Despite performing well when evaluated on <cite>AddSent</cite>, the retrained model suffers a more than 30% decrease in F1 performance when tested on a slightly different adversarial dataset generated by AddSentMod (which differs from <cite>AddSent</cite> in two superficial ways: using a different set of fake answers and prepending instead of appending the distractor sentence to the context). We show that using <cite>AddSent</cite> to generate adversarial training data introduces new superficial trends for a model to exploit; and instead we propose the AddSentDiverse algorithm that generates highly varied data for adversarial training, resulting in more robust models. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_19",
  "x": "Our 'AddSentDiverse' algorithm is a modified version of <cite>AddSent</cite> <cite>(Jia and Liang, 2017)</cite> , aimed at producing good adversarial examples for robust training purposes. For each {context, question, answer} triple, <cite>AddSent</cite> does the following: (1) Several antonym and named-entity based semantic altering perturbations (swapping) are applied to the question; (2) A fake answer is generated that matches the 'type' of the original answer (e.g., Prague \u2192 Chicago, etc.); (3) The fake answer and the altered question are combined into a distractor statement based on a set of manually defined rules; (4) Errors in grammar are fixed by crowd-workers; (5) The finalized distractor is appended to the end of the context. The specificity of the algorithm creates new superficial cues that a model can learn and use during training and never get punished for: (1) a model can learn that it is unlikely for the last sentence to contain the real answer; (2) a model can learn that the fixed set of fake answers should not be picked.",
  "y": "extends"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_20",
  "x": "Our 'AddSentDiverse' algorithm is a modified version of <cite>AddSent</cite> <cite>(Jia and Liang, 2017)</cite> , aimed at producing good adversarial examples for robust training purposes. For each {context, question, answer} triple, <cite>AddSent</cite> does the following: (1) Several antonym and named-entity based semantic altering perturbations (swapping) are applied to the question; (2) A fake answer is generated that matches the 'type' of the original answer (e.g., Prague \u2192 Chicago, etc.); (3) The fake answer and the altered question are combined into a distractor statement based on a set of manually defined rules; (4) Errors in grammar are fixed by crowd-workers; (5) The finalized distractor is appended to the end of the context. The specificity of the algorithm creates new superficial cues that a model can learn and use during training and never get punished for: (1) a model can learn that it is unlikely for the last sentence to contain the real answer; (2) a model can learn that the fixed set of fake answers should not be picked.",
  "y": "background"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_21",
  "x": "The specificity of the algorithm creates new superficial cues that a model can learn and use during training and never get punished for: (1) a model can learn that it is unlikely for the last sentence to contain the real answer; (2) a model can learn that the fixed set of fake answers should not be picked. These nullify the effectiveness of the distractors as the model will learn to simply ignore them. We thus introduce the AddSentDiverse algorithm, which adds two modifications to <cite>AddSent</cite> that allows for generating higher-variance adversarial examples.",
  "y": "extends"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_22",
  "x": "We thus introduce the AddSentDiverse algorithm, which adds two modifications to <cite>AddSent</cite> that allows for generating higher-variance adversarial examples. Namely, we randomize the distractor placement (Sec. 3.1) and we diversity the set of fake answers used (Sec. 3.2). Lastly, to address the antonymstyle semantic perturbations used in <cite>AddSent,</cite> we show that we need to improve model capabilities by adding indicator features for semantic relationships (but only when) in tandem with the addition of diverse adversarial data (Sec. 3.3).",
  "y": "motivation"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_23",
  "x": "During training done by <cite>Jia and Liang (2017)</cite> , the distractor is always added as the last sentence, creating a very skewed distribution for Y . This resulted in the model learning to ignore the last sentence, as it was never punished for doing so. This, in turn, caused the retrained model to fail on AddSentMod, where the distractor is inserted to the front instead of the back of the context paragraph (this is shown by our experiments as well).",
  "y": "background"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_24",
  "x": "**DYNAMIC FAKE ANSWER GENERATION** To prevent the model from superficially deciding what is a distractor based on certain specific words, we dynamically generate the fake answers instead of using <cite>AddSent</cite>'s pre-defined set. Let S be the set that contains all the answers in the SQuAD training data, tagged by their type (e.g., person, location, etc.).",
  "y": "differences"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_25",
  "x": "For each answer a, we generate the fake answer dynamically by randomly selecting another answer a = a from S that has the same type as a, as opposed to <cite>AddSent</cite> <cite>(Jia and Liang, 2017)</cite> , which uses a pre-defined fake answer for each type (e.g., \"Chicago\" for any location). This creates a much larger set of fake answers, thus decreasing the correlation between any text and its likelihood of being a part of a distractor, forcing the model to become more robust. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_26",
  "x": "In previous sections, we prevented the model from identifying distractors based on superficial clues such as location and fake answer identity by eliminating these correlations within the training data. But even if we force the model to learn some deeper methods for identifying/discarding the distractors, it only has limited ability in recognizing semantic differences because its current inputs do not capture crucial aspects of lexical semantics such as antonymy (which were inserted by <cite>Jia and Liang (2017)</cite> when generating the <cite>AddSent</cite> adversaries; see Sec. 3). Most current models use pretrained word embeddings (e.g., GloVE (Pennington et al., 2014) and ELMo (Peters et al., 2018) ) as input, which are usually calculated based on the distributional hypothesis (Harris, 1954) , and do not capture lexical semantic relations such as antonymy (Geffet and Dagan, 2005) .",
  "y": "differences"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_27",
  "x": "These shortcomings are reflected by our results in Sec. 4.6, where we see that we can't resolve all <cite>AddSent</cite>- style adversaries by diversifying the training data alone. For the model to be robust to semanticsbased (e.g., antonym-style) attacks, it needs extra knowledge of lexical semantic relations. Hence, we augment the input of each word in the question/context with two indicator features indicating the existence of its synonym and antonym (using WordNet (Fellbaum, 1998) ) in the context/question, allowing the model to use lexical semantics directly instead of learned statistical correlations of the word embeddings.",
  "y": "motivation"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_28",
  "x": "Models are evaluated on the original SQuAD dev set and 4 adversarial datasets: <cite>AddSent</cite>, the adversarial evaluation set by <cite>Jia and Liang (2017)</cite> , and 3 variations of <cite>AddSent</cite>: AddSentPrepend, where the distractor is prepended to the context, AddSentRandom, where the distractor is randomly inserted into the context, 4 and AddSentMod <cite>(Jia and Liang, 2017)</cite> , where a different set of fake answers is used and the distractor is prepended to the context. Experiments measure the soft F1 score and all of the adversarial evaluations are modeldependent, following the style of <cite>AddSent</cite>, where multiple adversaries are generated for each exam-ple in the evaluation set and the model's worst performance among the variants is recorded. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_29",
  "x": "**EVALUATION DETAILS** Models are evaluated on the original SQuAD dev set and 4 adversarial datasets: <cite>AddSent</cite>, the adversarial evaluation set by <cite>Jia and Liang (2017)</cite> , and 3 variations of <cite>AddSent</cite>: AddSentPrepend, where the distractor is prepended to the context, AddSentRandom, where the distractor is randomly inserted into the context, 4 and AddSentMod <cite>(Jia and Liang, 2017)</cite> , where a different set of fake answers is used and the distractor is prepended to the context. Experiments measure the soft F1 score and all of the adversarial evaluations are modeldependent, following the style of <cite>AddSent</cite>, where multiple adversaries are generated for each exam-ple in the evaluation set and the model's worst performance among the variants is recorded.",
  "y": "uses"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_30",
  "x": "In our main experiment, we compare the BSAE model's performance on different test sets when trained with three different training sets: the original SQuAD data (Original-SQuAD), SQuAD data augmented with <cite>AddSent</cite> generated adversaries (similar to adversarial training conducted by <cite>Jia and Liang (2017)</cite>), and SQuAD data augmented with our AddSentDiverse generated adversaries. For the latter two, we run the respective adversarial generation algorithms on the training set, and add randomly selected adversarial examples such that they make up 20% of the total training data. The results are shown in Table 1 .",
  "y": "uses"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_31",
  "x": "First, as shown, the <cite>AddSent</cite>-trained model is not able to perform well on test sets where the distractors are not inserted at the end, e.g., the AddSentRandom adversarial test set. 5 On the other hand, it can be seen that retraining with AddSentDiverse boosts performance of the model significantly across all adversarial datasets, indicating a general increase in robustness. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_32",
  "x": "We also conducted experiments studying the effect of different distractor placement strategies on the trained models' robustness. The BSAE model was trained on 4 variations of AddSentDiverseaugmented training set, with the only difference between them being the location of the distractor within the context: InsFirst, where the distractor is prepended, InsLast, where the distractor is appended, InsMid, where the distractor is inserted in the middle and InsRandom, where the distractor is randomly placed. The retrained models are tested on <cite>AddSent</cite> and AddSentPrepend, whose only difference is where the distractor is located.",
  "y": "differences uses"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_33",
  "x": "Finally, we examined the errors of our final adversarially-trained BSAE+SA model on the <cite>AddSent</cite> dataset and found that out of the 21.09% remaining errors (Table 4) , 33.3% (46 cases) of these erroneous predictions occurred within the inserted distractor, and 63.7% (88 cases) occurred on questions that the model got wrong in the original SQuAD dev set (without the inserted distractors). The former errors are mainly occurring within distractors created with named-entity replacements (which we haven't addressed directly in the current paper) or malformed distractors (that in fact do answer the question). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "6ca6283ae23bbd6d0827d8f5f2947a_34",
  "x": "We see that under random insertion, the distribution is very close to uniform. Note that if we were to aggregate n and plot P sa for n \u2264 3, 5 and 7, as shown in Fig. 3 , the distributions of P sa created by inserting in the middle and inserting randomly are very similar, but the distribution of inserting in the middle is skewed against the beginnings and ends of the paragraphs. This explains why in our experiment studying the effect of distractor placement strategies (see Table 2), InsMid's performance was not skewed towards either <cite>AddSent</cite> or AddSentPrepend, but was worse on both when compared to InsRandom.",
  "y": "uses"
 },
 {
  "id": "6ca7c7782c33f51e9bdb2e8613c24d_0",
  "x": "Natural language dialog has been used in many areas, such as for call-center/routing application (Carpenter & Chu-Carroll 1998) , email routing (Walker, Fromer & Narayanan 1998), information retrieval and database access <cite>(Androutsopoulos & Ritchie 1995)</cite> , and for telephony banking (Zadrozny et al. 1998) . In this demonstration, we present a natural language dialog interface to online shopping. Our user studies show natural language dialog to be a very effective means for negotiating user's requests and intentions in this domain.",
  "y": "background"
 },
 {
  "id": "6ca7c7782c33f51e9bdb2e8613c24d_1",
  "x": "**INTRODUCTION** Natural language dialog has been used in many areas, such as for call-center/routing application (Carpenter & Chu-Carroll 1998) , email routing (Walker, Fromer & Narayanan 1998), information retrieval and database access <cite>(Androutsopoulos & Ritchie 1995)</cite> , and for telephony banking (Zadrozny et al. 1998) . In this demonstration, we present a natural language dialog interface to online shopping.",
  "y": "extends"
 },
 {
  "id": "6cb86d91918743b0e4ff27e9d2351b_0",
  "x": "However, manual annotation is tedious and error-prone, especially if many annotators are involved. It is therefore desirable to have automatic means for detecting errors and inconsistencies in the annotation. Automatic methods for error detection in treebanks have been developed in the DECCA project 1 for several different annotation types, for example part-of-speech<cite> (Dickinson and Meurers, 2003a)</cite> , constituency syntax (Dickinson and Meurers, 2003b) , and dependency syntax (Boyd et al., 2008) .",
  "y": "background"
 },
 {
  "id": "6cb86d91918743b0e4ff27e9d2351b_1",
  "x": "**THE ERROR DETECTION ALGORITHM** The algorithm, described in <cite>Dickinson and Meurers (2003a)</cite> for POS tags, works by starting from individual tokens (the nuclei) by recording their assigned part-of-speech over an entire treebank. From there, it iteratively increases the context for each instance by extending the string to both sides to include adjacent tokens.",
  "y": "background"
 },
 {
  "id": "6cb86d91918743b0e4ff27e9d2351b_2",
  "x": "---------------------------------- **THE LABEL DISTRIBUTION VIEW** In addition to the output of the algorithm by <cite>Dickinson and Meurers (2003a)</cite>, the tool also provides a second view, which displays tag distributions of word forms to the user (see Figure 2) .",
  "y": "extends"
 },
 {
  "id": "6cb86d91918743b0e4ff27e9d2351b_3",
  "x": "These results are in line with findings by <cite>Dickinson and Meurers (2003a)</cite> for the Penn Treebank. They show that even manually annotated corpora contain errors and an automatic error mining tool can be a big help in finding them. Furthermore, it can help annotators to improve their annotation guidelines by pointing out phenomena that are not covered by the guidelines, because these phenomena will be more likely to show variation.",
  "y": "similarities"
 },
 {
  "id": "6cb86d91918743b0e4ff27e9d2351b_4",
  "x": "Automatically marking potential annotation errors and inconsistencies are one way of supporting annotators in their work. We presented a tool that provides a graphical interface for annotators to find and evaluate annotation errors in treebanks. It implements the error detection algorithms by <cite>Dickinson and Meurers (2003a)</cite> and Boyd et al. (2008) .",
  "y": "uses"
 },
 {
  "id": "6cbc59d4cb2d3246b3efa1ee612270_0",
  "x": "They also cleaned United Nations material and post-edited general-domain data that was previously filtered as indomain following the \"invitation model\" <cite>(Hoang and Sima'an, 2014)</cite> . For the other language pairs, the input material was 30,000 post-edited segments. The main part of the training corpora (approximately 75%) was part of Pangeanic's own repository harvested through web crawling and also OpenSubtitles (Tiedemann, 2012) .",
  "y": "background"
 },
 {
  "id": "6cbc59d4cb2d3246b3efa1ee612270_1",
  "x": "Therefore, 2 translators were contracted as part of the project to create 30,000 segments of in-domain data, translating public administrations websites. They also cleaned United Nations material and post-edited general-domain data that was previously filtered as indomain following the \"invitation model\" <cite>(Hoang and Sima'an, 2014)</cite> . For the other language pairs, the input material was 30,000 post-edited segments.",
  "y": "background"
 },
 {
  "id": "6cc36fef99fb1f25370175452f30b0_0",
  "x": "There has been much recent work in attempting to convert native parser output into alternative representations for evaluation purposes, e.g. (Clark and Curran, 2007; <cite>Matsuzaki and Tsujii, 2008</cite> ). The conclusion is that such conversions are surprisingly difficult. Clark and Curran (2007) shows that converting gold-standard CCG derivations into the GRs in DepBank resulted in an Fscore of only 85%; hence the upper bound on the performance of the CCG parser, using this evaluation scheme, was only 85%.",
  "y": "background"
 },
 {
  "id": "6cc36fef99fb1f25370175452f30b0_1",
  "x": "There has been much recent work in attempting to convert native parser output into alternative representations for evaluation purposes, e.g. (Clark and Curran, 2007; <cite>Matsuzaki and Tsujii, 2008</cite> ). The conclusion is that such conversions are surprisingly difficult. Clark and Curran (2007) shows that converting gold-standard CCG derivations into the GRs in DepBank resulted in an Fscore of only 85%; hence the upper bound on the performance of the CCG parser, using this evaluation scheme, was only 85%.",
  "y": "background motivation"
 },
 {
  "id": "6cc36fef99fb1f25370175452f30b0_2",
  "x": "---------------------------------- **CONVERSION SCHEMAS** There are three types of conversion schema: schemas which introduce nodes for lexical items; schemas which insert or elide PTB nodes for unary 3 Another possible approach has been taken by<cite> Matsuzaki and Tsujii (2008)</cite> , who convert HPSG analyses from a grammar automatically extracted from the PTB back into the PTB.",
  "y": "background"
 },
 {
  "id": "6cc36fef99fb1f25370175452f30b0_3",
  "x": "Even an upper bound of around 98%, which is achieved by<cite> Matsuzaki and Tsujii (2008)</cite> , is not sufficient, since this guarantees a loss of at least 2%. 4 ----------------------------------",
  "y": "background"
 },
 {
  "id": "6cc36fef99fb1f25370175452f30b0_4",
  "x": "The fact that the upper bound here is less than 95% shows that it is not possible to fairly evaluate the CCG parser on the complete test set. Even an upper bound of around 98%, which is achieved by<cite> Matsuzaki and Tsujii (2008)</cite> , is not sufficient, since this guarantees a loss of at least 2%. 4",
  "y": "differences"
 },
 {
  "id": "6cc36fef99fb1f25370175452f30b0_5",
  "x": "One question that is often asked of the CCG parsing work is \"Why not convert back into the PTB representation and perform a Parseval evaluation?\" By showing how difficult the conversion is, we believe that we have finally answered this question, as well as demonstrating comparable performance with the Berkeley parser. In addition, we have thrown further doubt on the possible use of the PTB for cross-framework parser evaluation, as recently suggested by<cite> Matsuzaki and Tsujii (2008)</cite> . Even the smallest loss due to mapping across representations is significant when a few tenths of a percentage point matter.",
  "y": "similarities"
 },
 {
  "id": "6cd4235e66a6e6e9768250c3db7fc6_0",
  "x": "For development and evaluation, there are two Chinese temporally annotated corpora, the ACE 2005 training corpus and TempEval-2 (c.f. Section 3). Table 1 lists approaches to Chinese temporal tagging with some further information. The most recent work is the learningbased language-independent discriminative parsing approach for normalizing temporal expressions by <cite>Angeli and Uszkoreit (2013</cite> There are also (semi-)automatic approaches to port a temporal tagger from one language to another.",
  "y": "background"
 },
 {
  "id": "6cd4235e66a6e6e9768250c3db7fc6_1",
  "x": "In contrast, the TempEval-2 Chinese data sets (Verhagen et al., 2010) contain TIMEX3 annotations with extent and normalization information. However, no TempEval-2 participants addressed Chinese and only<cite> Angeli and Uszkoreit (2013)</cite> report evaluation results on this corpus. Since HeidelTime is TIMEX3-compliant, and we address the extraction and normalization subtasks, we use the TempEval-2 corpus in our work.",
  "y": "background"
 },
 {
  "id": "6cd4235e66a6e6e9768250c3db7fc6_2",
  "x": "The Chinese training and test sets consist of 44 and 15 documents with 746 and 190 temporal expressions, respectively. However, several expressions have no normalized value information (85 in the training and 47 in the test set), others no type. 1 This issue was also reported by<cite> Angeli and Uszkoreit (2013)</cite> .",
  "y": "background"
 },
 {
  "id": "6cd4235e66a6e6e9768250c3db7fc6_4",
  "x": "---------------------------------- **EVALUATION SETUP** Corpus: We use three versions of the TempEval-2 training and test sets: (i) the original versions, (ii) the improved versions described in Section 3.3, and (iii) the cleaned versions also used by<cite> Angeli and Uszkoreit (2013)</cite> in which temporal expressions without value information are removed.",
  "y": "uses similarities"
 },
 {
  "id": "6cd4235e66a6e6e9768250c3db7fc6_7",
  "x": "However, since the accuracy measure used by the TempEval-2 script calculates the ratio of correctly normalized expressions to all extracted expressions and not to all expressions in the gold standard, we additionally present the raw numbers of correctly normalized expressions for the two systems. Table 4 shows the comparison between our approach and the one by<cite> Angeli and Uszkoreit (2013)</cite> . We outperform their approach not only with respect to the accuracy but also with respect to the numbers of correctly normalized expressions (574 vs. 484 5 and 121 vs. 86 5 on the training and test sets, respectively) -despite the fact that we perform the full task of temporal tagging and not only the normalization.",
  "y": "differences"
 },
 {
  "id": "6d8612cfb4bf05322fed1c02f4885a_0",
  "x": "Nguyen et al. (2013) analyzed the relationship between language use and age, modelled as a continuous variable. They found similar language usage trends for both genders, with increasing word and tweet length with age, and an increasing tendency to write more grammatically correct, standardized text. Such findings encourage further research in the area of measuring readability, which not only facilitates adjusting the text to the reader (Danescu-Niculescu-Mizil et al., 2011) , but can also play an important role in identifying authorial style <cite>(Pitler and Nenkova, 2008)</cite> .",
  "y": "background"
 },
 {
  "id": "6d8612cfb4bf05322fed1c02f4885a_1",
  "x": "We group these features into: Surface We measure the length of tweets in words and characters, and the length of words. As shorter words are considered more readable (Gunning, 1969;<cite> Pitler and Nenkova, 2008)</cite> , we also measure the ratio of words longer than five letters.",
  "y": "uses background"
 },
 {
  "id": "6d8612cfb4bf05322fed1c02f4885a_2",
  "x": "Readability After filtering tweets to contain only words, we use the most prominent readability measures per user: the Automatic Readability Index (Senter and Smith, 1967) , the FleschKincaid Grade Level (Kincaid et al., 1975) , the Coleman-Liau Index (Coleman and Liau, 1975) , the Flesch Reading Ease (Flesch, 1948) , the LIX Index (Anderson, 1983) , the SMOG grade (McLaughlin, 1969 ) and the Gunning-Fog Index (Gunning, 1969) . The majority of those are computed using the average word and sentence lengths and number of syllables per sentence, combined with weights. Syntax Researchers argue about longer sentences not necessarily being more complex in terms of syntax (Feng et al., 2009;<cite> Pitler and Nenkova, 2008)</cite> .",
  "y": "background"
 },
 {
  "id": "6d8612cfb4bf05322fed1c02f4885a_3",
  "x": "Using Stanford Named Entity Recognizer (Finkel et al., 2005) , we measure the proportion of named entities (3-classed) to words, as their presence potentially decreases readability (Beinborn et al., 2012) , and netspeak aspects such as the proportion of elongations (wooow) and words with numbers (good n8). We quantify the number of hedges (Hyland, 2005) and abstract words 1 used, and the ratio of standalone numbers stated per user as these are indicators of specificity (Pennebaker et al., 2003;<cite> Pitler and Nenkova, 2008)</cite> . We also capture the ratio of hapax legomena, and of superlatives and plurals using Stanford POS Tagger 1 www.englishbanana.com (Toutanova et al., 2003 ) using the Twitter model.",
  "y": "uses background"
 },
 {
  "id": "6edf517d79f7fd2a0653a3d5fb543d_0",
  "x": "These methods work by independently training word embeddings in different languages, and mapping them to a shared space through linear transformations. While early methods required a training dictionary to find the initial alignment (Mikolov et al., 2013) , fully unsupervised methods have managed to obtain comparable results based on either adversarial training or selflearning<cite> (Artetxe et al., 2018b)</cite> . A prominent application of these methods is Bilingual Lexicon Induction (BLI), that is, using the resulting cross-lingual embeddings to build a bilingual dictionary.",
  "y": "background"
 },
 {
  "id": "6edf517d79f7fd2a0653a3d5fb543d_1",
  "x": "While early methods required a training dictionary to find the initial alignment (Mikolov et al., 2013) , fully unsupervised methods have managed to obtain comparable results based on either adversarial training or selflearning<cite> (Artetxe et al., 2018b)</cite> . A prominent application of these methods is Bilingual Lexicon Induction (BLI), that is, using the resulting cross-lingual embeddings to build a bilingual dictionary. In this paper, we go one step further and, rather than directly inducing the bilingual dictionary from the cross-lingual word embeddings, we use them to build an unsupervised machine translation system, and extract a bilingual dictionary from a synthetic parallel corpus generated with it.",
  "y": "extends background"
 },
 {
  "id": "6edf517d79f7fd2a0653a3d5fb543d_2",
  "x": "The input of our method is a set of cross-lingual word embeddings and the monolingual corpora used to train them. In our experiments, we use fastText embeddings (Bojanowski et al., 2017) mapped through VecMap<cite> (Artetxe et al., 2018b</cite> ), but the algorithm described next can also work with any other word embedding and cross-lingual mapping method. The general idea of our method is to to build an unsupervised phrase-based statistical machine translation system Artetxe et al., 2018c Artetxe et al., , 2019 , and use it to generate a synthetic parallel corpus from which to extract a bilingual dictionary.",
  "y": "uses"
 },
 {
  "id": "6edf517d79f7fd2a0653a3d5fb543d_3",
  "x": "the MUSE dataset were trained using these exact same settings, so our embeddings only differ in the Wikipedia dump used to extract the training corpus and the pre-processing applied to it, which is not documented in the original dataset. Having done that, we map these word embeddings to a cross-lingual space using the unsupervised mode in VecMap<cite> (Artetxe et al., 2018b)</cite> , which builds an initial solution based on the intralingual similarity distribution of the embeddings and iteratively improves it through self-learning. Finally, we induce a bilingual dictionary using our proposed method and evaluate it in comparison to previous retrieval methods (standard nearest neighbor, inverted nearest neighbor, inverted softmax 5 and CSLS).",
  "y": "uses"
 },
 {
  "id": "6edf517d79f7fd2a0653a3d5fb543d_4",
  "x": "The resulting cross-lingual embeddings are then used to induce the translations of words that were missing in the training dictionary by taking their nearest neighbor in the target language. The amount of required supervision was later reduced through self-learning methods (Artetxe et al., 2017) , and then completely eliminated through adversarial training (Zhang et al., 2017a; or more robust iterative approaches combined with initialization heuristics<cite> (Artetxe et al., 2018b</cite>; Hoshen and Wolf, 2018) . At the same time, several recent methods have formulated embedding mappings as an optimal transport problem (Zhang et al., 2017b; .",
  "y": "uses"
 },
 {
  "id": "6f4dc72277119f0df3d4a7155c61fc_0",
  "x": "**INTRODUCTION** Recurrent neural networks (RNNs) yield high-quality results in many applications [1, 4, 18, 21] but often overfit due to overparametrization. In many practical problems, RNNs can be compressed orders of times with only slight quality drop or even with quality improvement<cite> [2,</cite> 15, 20] .",
  "y": "background"
 },
 {
  "id": "6f4dc72277119f0df3d4a7155c61fc_1",
  "x": "Recurrent neural networks (RNNs) yield high-quality results in many applications [1, 4, 18, 21] but often overfit due to overparametrization. In many practical problems, RNNs can be compressed orders of times with only slight quality drop or even with quality improvement<cite> [2,</cite> 15, 20] . Methods for RNN compression can be divided into three groups: based on matrix factorization [6, 19] , quantization [7] or sparsification<cite> [2,</cite> 15, 20] .",
  "y": "background"
 },
 {
  "id": "6f4dc72277119f0df3d4a7155c61fc_2",
  "x": "Methods for RNN compression can be divided into three groups: based on matrix factorization [6, 19] , quantization [7] or sparsification<cite> [2,</cite> 15, 20] . We focus on RNNs sparsification. Two main groups of approaches for sparsification are pruning and Bayesian sparsification.",
  "y": "uses"
 },
 {
  "id": "6f4dc72277119f0df3d4a7155c61fc_3",
  "x": "In pruning [15, 20] , weights with absolute values less than a predefined threshold are set to zero. Such methods imply a lot of hyperparameters (thresholds, pruning schedule etc). Bayesian sparsification techniques [14, 16, 8, 9,<cite> 2]</cite> treat weights of an RNN as random variables and approximate posterior distribution over them given sparsity-inducing prior distribution.",
  "y": "background"
 },
 {
  "id": "6f4dc72277119f0df3d4a7155c61fc_4",
  "x": "It is achieved by multiplying such a variable on a learnable weight, finding posterior over it and setting the weight to zero if the corresponding signal-to-noise ratio is small. In this work, we investigate the last mentioned property for gated architectures, particularly for LSTM. Following<cite> [2,</cite> 14] , we sparsify individual weights of the RNN.",
  "y": "uses"
 },
 {
  "id": "6f4dc72277119f0df3d4a7155c61fc_5",
  "x": "To find parameters of the approximate posterior distribution, evidence lower bound (ELBO) is optimized: Because of the log-uniform prior, for the majority of weights signal-to-noise ratio m 2 ij /\u03c3 2 ij \u2192 0 and these weights do not affect network's output. In <cite>[2]</cite> SparseVD is adapted to RNNs.",
  "y": "background"
 },
 {
  "id": "6f4dc72277119f0df3d4a7155c61fc_6",
  "x": "**PROPOSED METHOD** To sparsify individual weights, we apply SparseVD [14] to all weights of the RNN, taking into account recurrent specifics underlined in <cite>[2]</cite> . To compress layers and remove neurons, we follow [8] and introduce group variables for the neurons of all layers (excluding output predictions), and specifically, z",
  "y": "uses"
 },
 {
  "id": "6f4dc72277119f0df3d4a7155c61fc_7",
  "x": "In <cite>[2]</cite> SparseVD is adapted to RNNs. To sparsify individual weights, we apply SparseVD [14] to all weights of the RNN, taking into account recurrent specifics underlined in <cite>[2]</cite> .",
  "y": "similarities"
 },
 {
  "id": "6f4dc72277119f0df3d4a7155c61fc_8",
  "x": "For language modeling, we use networks with one We compare four models in terms of quality and sparsity: baseline model without any regularization, standard SparseVD model for weights sparsification only (W), SparseVD model with group variables for neurons sparsification (W+N) and SparseVD model with group variables for gates and neurons sparsification (W+G+N). In all SparseVD models, we sparsify weights matrices of all layers. Since in text classification tasks usually only a small number of input words are important, we use additional multiplicative weights to sparsify the input vocabulary in case of group sparsification (W+N, W+G+N) following <cite>[2]</cite> .",
  "y": "uses"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_0",
  "x": "It has been more recently that graph-based methods for knowledge-based WSD have gained much attention in the NLP community ( (Sinha and Mihalcea, 2007) , (Navigli and Lapata, 2007) , (Agirre and Soroa, 2008) ,<cite> (Agirre and Soroa, 2009)</cite> ). In these methods a graph representation for senses (nodes) and relation (edges) is first built. Then graph-based techniques that are sensible to the structural properties of the graph are used to find the best senses for words in the incoming contexts.",
  "y": "background"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_1",
  "x": "Thus the concepts with highest ranks are assigned to the corresponding words. In <cite>(Agirre and Soroa, 2009</cite> ), a comparative analysis of different graph-based models over two well known WSD benchmarks is reported. In the paper two variants of the random surfer model as defined by PageRank model (Brin and Page, 1998) are analyzed.",
  "y": "background"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_2",
  "x": "In particular, a variant called Personalized PageRank (P P R) is proposed <cite>(Agirre and Soroa, 2009</cite> ) that tries to trade-off between the amount of the employed lexical information and the overall efficiency. In synthesis, along the ideas of the Topic sensitive PageRank (Haveliwala, 2002) , P P R suggests that a proper initialization of the teleporting vector p suitably captures the context information useful to drive the random surfer PageRank model over the graph to converge towards the proper senses in fewer steps. The basic idea behind the adoption of P P R is to impose a personalized vector that expresses the contexts of all words targeted by the disambiguation.",
  "y": "background"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_3",
  "x": "This method improves on the complexity of the previously presented methods (e.g. (Agirre and Soroa, 2008) ) as it allows to contextualize the behaviors of PageRank over a sentence, without asking for a different graph: in this way the WordNet graph is always adopted, in a word or sentence oriented fashion. Moreover, it is possible to avoid to rebuild a graph for each target word, as the entire sentence can be coded into the personalization vector. In <cite>(Agirre and Soroa, 2009</cite> ), a possible, and more accurate alternative, is also presented called PPR word2word (P P Rw2w) where a different personalization vector is used for each word in a sentence.",
  "y": "background"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_4",
  "x": "In <cite>(Agirre and Soroa, 2009</cite> ), a possible, and more accurate alternative, is also presented called PPR word2word (P P Rw2w) where a different personalization vector is used for each word in a sentence. Although clearly less efficient in terms of time complexity, this approach guarantees the best accuracy, so that it can be considered the state-ofthe art in unsupervised WSD. In this paper a different approach to personalization of the PageRank is presented, aiming at preserving the suitable efficiency of the sentence oriented PPR algorithm for WSD but achieving an accuracy at least as high as the P P Rw2w one.",
  "y": "motivation"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_5",
  "x": "We propose to use distributional evidence that can be automatically acquired from a corpus to define the topical information encoded by the personalization vector, in order to amplify the bias on the resulting P P R and improve the performance of the sentence oriented version. The intuition is that distributional evidence is able to cover the gap between word oriented usages of the P P R as for the P P Rw2w defined in<cite> (Agirre and Soroa, 2009)</cite> , and its sentence oriented counterpart. In this way we can preserve higher accuracy levels while limiting the number of PageRank runs, i.e. increasing efficiency.",
  "y": "differences"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_6",
  "x": "It models the amount of likelihood that a generic Web surfer, standing at a vertex, randomly follows a link from this vertex toward any other vertex in the graph: the uniform probability p i = 1 N \u2200i, is assigned to each one of the N vertices in G. While it guarantees the convergence of the algorithm, it expresses the trade-off between the probability of following links provided by the Web graph and the freedom to violate them. An interesting aspect of the ranking process is the initial state. Many algorithms (as well as the one proposed by<cite> (Agirre and Soroa, 2009)</cite> ) initialize the ranks of the vertex at a uniform value (usually 1/N for a graph with N vertices).",
  "y": "background"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_7",
  "x": "How the matrix M can be made as much reusable as possible? \u2022 How to encode in Eq. 1 the incoming context in order to properly address the different words in the sentence \u03c3? In order to address the above problems, in line with the notion of topic-sensitive PageRank, a personalized PageRank approach has been recently devised <cite>(Agirre and Soroa, 2009</cite> ) as discussed in the next section.",
  "y": "background"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_8",
  "x": "In <cite>(Agirre and Soroa, 2009</cite> ), a novel use of PageRank for word sense disambiguation is presented. It aims to present an optimized version of the algorithm previously discussed in (Agirre and Soroa, 2008) . The main difference concerns the method used to initialize and use the graph G for disambiguating a sentence with respect to the overall graph (hereafter GKB) that represents the complete lexicon.",
  "y": "background"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_9",
  "x": "GD is a subgraph of the original GKB, obtained by computing the shortest paths between the concepts of the words co-occurring in the context. These are expected to capture most of the information relevant to the disambiguation (i.e. sense ranking) step. The alternative proposed in <cite>(Agirre and Soroa, 2009</cite> ) allows a more static use of the full LKB.",
  "y": "background"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_10",
  "x": "This method reduces the number of invocations of PageRank thus lowering the average disambiguation time. A word oriented version of the algorithm is also proposed in <cite>(Agirre and Soroa, 2009</cite> ). It defines different initializations for the different words w i \u2208 \u03c3: these are obtained by setting the initial probability mass in p to 0 for all the senses Sense(w i ) of the targeted w i .",
  "y": "background"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_11",
  "x": "It defines different initializations for the different words w i \u2208 \u03c3: these are obtained by setting the initial probability mass in p to 0 for all the senses Sense(w i ) of the targeted w i . In this way, only the context words and not the target are used for the personalization step 1 . This approach to the personalized PageRank is termed word-by-word or P P Rw2w version in<cite> (Agirre and Soroa, 2009)</cite> .",
  "y": "background"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_12",
  "x": "The key idea in <cite>(Agirre and Soroa, 2009</cite> ) is to adapt the matrix initialization step in order to exploit the available contextual evidence. Notice that personalization in Word Sense Disambiguation is inspired by the topic-sensitive PageRank approach, proposed in (Haveliwala, 2002) , for Web search tasks. It exploits a context dependent definition of the vector p in Eq. 1 to influence the linkbased sense ranking achievable over a sentence.",
  "y": "background"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_13",
  "x": "In order to compare the quality of the proposed approach, the results of the personalized PageRank proposed in <cite>(Agirre and Soroa, 2009</cite> ) over the same dataset are reported in Table 1 (The * systems, denoted by UKB). As also suggested in<cite> (Agirre and Soroa, 2009)</cite> Table 1 : Official Results over the Semeval'07 dataset. The * systems was presented in <cite>(Agirre and Soroa, 2009</cite> ).",
  "y": "uses"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_15",
  "x": "As also suggested in<cite> (Agirre and Soroa, 2009)</cite> Table 1 : Official Results over the Semeval'07 dataset. The * systems was presented in <cite>(Agirre and Soroa, 2009</cite> ). The LSA UKB 1.7 and LSA UKB 3.0 show the rank of the model proposed in this paper.",
  "y": "uses"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_16",
  "x": "In line with<cite> (Agirre and Soroa, 2009)</cite> , different types of WordNet graphs are employed in our experiments: WN17 all hyponymy links between synsets of the WN1.7 dictionary are considered; WN17x all hyponymy links as well as the extended 1.7 version of WordNet, whereas the syntactically parsed glosses, are semantically disambiguated and connected to the corresponding synsets;",
  "y": "similarities"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_17",
  "x": "We used the Senseval'02 and Senseval'03 datasets to fine tune parameters of our LSA model, that are: (1) the dimensionality cut k to derive the LSA space; (2) the threshold \u03c4 to determine the expansion dictionary in the LSA space for every POS tag (e.g. noun or adjectives), that may require different values; (3) the damping factor \u03b1 and (4) the number of iteration over the graph. In <cite>(Agirre and Soroa, 2009</cite> ) the suggested parameters are \u03b1 = 0.85 as the damping factor and 30 as the upper limit to the PageRank iterations. We always adopted this setting to estimate the performances of the standard P P R and P P Rw2w algorithms referred through U KB.",
  "y": "uses"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_18",
  "x": "The best F1 scores between any pair are emphasized in bold, to comparatively asses the results. As a confirmation of the outcome in <cite>(Agirre and Soroa, 2009</cite> ), different lexical resources achieve different results. In general by adopting the graph derived from WN3.0 (i.e. WN30 and WN30g) lower performance can be achieved.",
  "y": "similarities"
 },
 {
  "id": "70c786a0affcc9f206cc4252112cd2_19",
  "x": "This confirms that the impact of the topical information provided by the LSA expansion of the sentence is beneficial for a better use of the lexical graph. An even more interesting outcome is that the improvement implied by the proposed LSA method on the sentence oriented model (i.e. the standard PPR method of<cite> (Agirre and Soroa, 2009)</cite> ) is higher, so that the difference between the performances of the P P Rw2w model are no longer strikingly better than the P P R one. For example, on the simple WN1.7 hyponymy network the P P R \u2212 LSA100 3 method abolishes the gap of about 4% previously observed for the PPR-UKB model.",
  "y": "differences"
 },
 {
  "id": "715cba53c376e50b76a0966ff16a6a_0",
  "x": "**INTRODUCTION** In recent years, sentiment analysis has received considerable attentions in Natural Language Processing (NLP) community (Blitzer et al., 2007; Dasgupta and Ng, 2009; Pang et al., 2002) . Polarity classification, which determine whether the sentiment expressed in a document is positive or negative, is one of the most popular tasks of sentiment analysis<cite> (Dasgupta and Ng, 2009</cite> ).",
  "y": "background"
 },
 {
  "id": "715cba53c376e50b76a0966ff16a6a_1",
  "x": "**INTRODUCTION** In recent years, sentiment analysis has received considerable attentions in Natural Language Processing (NLP) community (Blitzer et al., 2007; Dasgupta and Ng, 2009; Pang et al., 2002) . Polarity classification, which determine whether the sentiment expressed in a document is positive or negative, is one of the most popular tasks of sentiment analysis<cite> (Dasgupta and Ng, 2009</cite> ).",
  "y": "background"
 },
 {
  "id": "715cba53c376e50b76a0966ff16a6a_2",
  "x": "For example, although the sentence \"The thief tries to protect his excellent reputation\" contains the word \"excellent\", it tells us nothing about the author's opinion and in fact could be well embedded in a negative review. Second, sentiment classification systems are typically domain-specific, which makes the expensive process of annotating a large amount of data for each domain and is a bottleneck in building high quality systems<cite> (Dasgupta and Ng, 2009</cite> ). This motivates the task of learning robust sentiment models from minimal supervision (Li, et al., 2009) .",
  "y": "background motivation"
 },
 {
  "id": "715cba53c376e50b76a0966ff16a6a_3",
  "x": "Recently, semi-supervised learning, which uses large amount of unlabeled data together with labeled data to build better learners (Raina et al., 2007; Zhu, 2007) , has drawn more attention in sentiment analysis<cite> (Dasgupta and Ng, 2009</cite>; Li, et al., 2009) . As argued by several researchers (Bengio, 2007; Salakhutdinov and Hinton, 2007) , deep architecture, composed of multiple levels of non-linear operations (Hinton et al., 2006) , is expected to perform well in semi-supervised learning because of its capability of modeling hard artificial intelligent tasks. Deep Belief Networks (DBN) is a representative deep learning algorithm achieving notable success for semi-supervised learning (Hinton, et al., 2006) .",
  "y": "background"
 },
 {
  "id": "715cba53c376e50b76a0966ff16a6a_4",
  "x": "However, active learning choose the training data actively, which reduce the needs of labeled data (Tong and Koller, 2002) . Recently, active learning had been applied in sentiment classification<cite> (Dasgupta and Ng, 2009)</cite> . Inspired by the study of semi-supervised learning, active learning and deep architecture, this paper proposes a novel semi-supervised polarity classification algorithm called Active Deep Networks (ADN) that is based on a representative deep learning algorithm Deep Belief Networks (DBN) (Hinton, et al., 2006) and active learning (Tong and Koller, 2002) .",
  "y": "background motivation"
 },
 {
  "id": "715cba53c376e50b76a0966ff16a6a_5",
  "x": "Supervised sentiment classification systems are domain-specific and annotating a large scale corpus for each domain is very expensive<cite> (Dasgupta and Ng, 2009</cite> ). There are several solutions for this corpus annotation bottleneck. The first type of solution is using old domain labeled examples to new domain sentiment clas-sification. The second type of solution is semisupervised sentiment classification. The third type of solution is unsupervised sentiment classification.",
  "y": "background motivation"
 },
 {
  "id": "715cba53c376e50b76a0966ff16a6a_6",
  "x": "However, unsupervised learning of sentiment is difficult, partially because of the prevalence of sentimentally ambiguous reviews<cite> (Dasgupta and Ng, 2009</cite> ). Using multi-domain sentiment corpus to sentiment classification is also hard to apply, because each domain has a very limited amount of training data, due to annotating a large corpus is difficult and time-consuming (Li and Zong, 2008) . So in this paper we focus on semi-supervised approach to sentiment classification.",
  "y": "background motivation"
 },
 {
  "id": "715cba53c376e50b76a0966ff16a6a_7",
  "x": "There are many review documents in the dataset. We preprocess these reviews to be classified, which is similar with<cite> Dasgupta and Ng (2009)</cite> . Each review is represented as a vector of unigrams, using binary weight equal to 1 for terms present in a vector.",
  "y": "similarities"
 },
 {
  "id": "715cba53c376e50b76a0966ff16a6a_8",
  "x": "The main issue for an active learner is the choosing of next unlabeled instance to query. In this paper, we choose the reviews whose labels are most uncertain for the classifier. Following previous work on active learning for SVMs<cite> (Dasgupta and Ng, 2009</cite>; Tong and Koller, 2002) , we define the uncertainty of a review as its distance from the separating hyperplane. In other words, reviews that are near the separating hyperplane are chosen as the labeled training data.",
  "y": "similarities background"
 },
 {
  "id": "715cba53c376e50b76a0966ff16a6a_9",
  "x": "The experimental setting is similar with<cite> Dasgupta & Ng (2009)</cite> . We perform active learning for five iterations and select twenty of the most uncertainty reviews to be queried each time. Then the ADN is re-trained on all of labeled and unlabeled reviews so far with semisupervised learning. At last, we can decide the label of reviews x according to the output h N (x) of the ADN architecture as below:",
  "y": "extends"
 },
 {
  "id": "715cba53c376e50b76a0966ff16a6a_10",
  "x": "Similar with<cite> Dasgupta and Ng (2009)</cite>, we divide the 2,000 reviews into ten equal-sized folds randomly and test all the algorithms with crossvalidation. In each folds, 100 reviews are random selected as training data and the remaining 100 data are used for test. Only the reviews in the training data set are used for the selection of labeled data by active learning. The ADN architecture has different number of hidden units for each hidden layer.",
  "y": "extends"
 },
 {
  "id": "715cba53c376e50b76a0966ff16a6a_11",
  "x": "We compare the classification performance of ADN with five representative classifiers, i.e., Semi-supervised spectral learning (Spectral) (Kamvar et al., 2003) , Transductive SVM (TSVM), Active learning (Active) (Tong and Koller, 2002) , Mine the Easy Classify the Hard (MECH)<cite> (Dasgupta and Ng, 2009)</cite> , and Deep Belief Networks (DBN) (Hinton, et al., 2006) . Spectral learning, TSVM, and Active learning method are three baseline methods for sentiment classification. MECH is a new semi-supervised method for sentiment classification<cite> (Dasgupta and Ng, 2009)</cite> .",
  "y": "background"
 },
 {
  "id": "715cba53c376e50b76a0966ff16a6a_12",
  "x": "Spectral learning, TSVM, and Active learning method are three baseline methods for sentiment classification. MECH is a new semi-supervised method for sentiment classification<cite> (Dasgupta and Ng, 2009)</cite> . DBN (Hinton, et al., 2006) is the classical deep learning method proposed recently.",
  "y": "background"
 },
 {
  "id": "715cba53c376e50b76a0966ff16a6a_13",
  "x": "The classification accuracies on test data in cross validation for five datasets and six methods are shown in Table 1 . The results of previous four methods are reported by<cite> Dasgupta and Ng (2009)</cite> . For ADN method, the initial two labeled data are selected randomly, so we repeat thirty times for each fold and the results are averaged.",
  "y": "background"
 },
 {
  "id": "7176d3dd72e781dca42f8c146d062d_0",
  "x": "In this paper we present an extension of a successful simple and effective method for extracting parallel sentences from comparable corpora and we apply it to an Arabic/English NIST system. We experiment with a new TERp filter, along with WER and TER filters. We also report a comparison of our approach with that of<cite> (Munteanu and Marcu, 2005)</cite> using exactly the same corpora and show performance gain by using much lesser data.",
  "y": "similarities differences"
 },
 {
  "id": "7176d3dd72e781dca42f8c146d062d_1",
  "x": "The ease of availability of these comparable corpora and the potential for parallel corpus as well as dictionary creation has sparked an interest in trying to make maximum use of these comparable resources, some of these works include dictionary learning and identifying word translations (Rapp, 1995) , named entity recognition (Sproat et al., 2006) , word sense disambiguation (Kaji, 2003) , improving SMT performance using extracted parallel sentences<cite> (Munteanu and Marcu, 2005)</cite> , (Rauf and Schwenk, 2009 ). There has been considerable amount of work on bilingual comparable corpora to learn word translations as well as discovering parallel sentences. Yang and Lee (2003) use an approach based on dynamic programming to identify potential parallel sentences in title pairs.",
  "y": "background"
 },
 {
  "id": "7176d3dd72e781dca42f8c146d062d_2",
  "x": "Using the extracted sentences they learn a dictionary and iterate over with more sentence pairs. Recent work by <cite>Munteanu and Marcu (2005)</cite> uses a bilingual lexicon to translate some of the words of the source sentence. These translations are then used to query the database to find matching translations using information retrieval (IR) techniques.",
  "y": "background"
 },
 {
  "id": "7176d3dd72e781dca42f8c146d062d_3",
  "x": "TERp has been tried encouraged by the outperformance of TER in our previous study on French-English. We have applied our technique on a different language pair Arabic-English, versus French-English that we reported the technique earlier on. Our use of full SMT sentences, gives us an added advantage of being able to detect one of the major errors of these approaches, also identified by<cite> (Munteanu and Marcu, 2005)</cite> , i.e, the cases where the initial sentences are identical but the retrieved sentence has a tail of extra words at sentence end.",
  "y": "similarities"
 },
 {
  "id": "7176d3dd72e781dca42f8c146d062d_4",
  "x": "We also perform a comparison of the data extracted by our approach and that by<cite> (Munteanu and Marcu, 2005)</cite> and report the results in Section 5.3. This paper is organized as follows. In the next section we first describe the baseline SMT system trained on human-provided translations only.",
  "y": "uses"
 },
 {
  "id": "7176d3dd72e781dca42f8c146d062d_5",
  "x": "LDC provides extracted parallel texts extracted with the algorithm published by<cite> (Munteanu and Marcu, 2005)</cite> . This corpus contains 1.1M sentence pairs (about 35M words) which were automatically extracted and aligned from the monolingual Arabic and English Gigaword corpora, a confidence score being provided for each sentence pair. We also applied our approach on data provided by LDC, but on a different subset.",
  "y": "similarities"
 },
 {
  "id": "7176d3dd72e781dca42f8c146d062d_6",
  "x": "Contrary to the previous approaches as in<cite> (Munteanu and Marcu, 2005)</cite> which used small amounts of in-domain parallel corpus as an initial resource, our system exploits the target language side of the comparable corpus to attain the same goal, thus the comparable corpus itself helps to better extract possible parallel sentences. We have also presented a comparison with their approach and found our bitexts to achieve nice improvements using much less words. The LDC comparable corpora were used in this paper, but the same approach can be extended to extract parallel sentences from huge amounts of corpora available on the web by identifying comparable articles using techniques such as (Yang and Li, 2003) and (Resnik and Y, 2003) .We have successfully applied our approach to French-English and ArabicEnglish language pairs.",
  "y": "differences"
 },
 {
  "id": "7176d3dd72e781dca42f8c146d062d_7",
  "x": "Bootstrapping is used and the size of the learned bilingual dictionary is increased over iterations to get better results. Our technique is similar to that of<cite> (Munteanu and Marcu, 2005)</cite> but we bypass the need of the bilingual dictionary by using proper SMT translations and instead of a maximum entropy classifier we use simple measures like the word error rate (WER) and the translation edit rate (TER) to decide whether sentences are parallel or not. We also report an extension of our work (Rauf and Schwenk, 2009 ) by experimenting with an additional filter TERp, and building a named entity noun dictionary using the unknown words from the SMT (section 5.2).",
  "y": "similarities differences"
 },
 {
  "id": "71a72cfca17b0b15938ed590f9c868_0",
  "x": "**ABSTRACT** Previous work on classifying information status (Nissim, 2006;<cite> Rahman and Ng, 2011</cite>) is restricted to coarse-grained classification and focuses on conversational dialogue. We here introduce the task of classifying finegrained information status and work on written text.",
  "y": "motivation"
 },
 {
  "id": "71a72cfca17b0b15938ed590f9c868_1",
  "x": "Previous work on learning IS (Nissim, 2006;<cite> Rahman and Ng, 2011</cite> ) is restricted in several ways. It deals with conversational dialogue, in particular with the corpus annotated by Nissim et al. (2004) . However, many applications that can profit from IS concentrate on written texts, such as summarization.",
  "y": "motivation"
 },
 {
  "id": "71a72cfca17b0b15938ed590f9c868_2",
  "x": "We reimplement Nissim's (2006) and <cite>Rahman and Ng's (2011)</cite> approaches as baselines and show that our approach outperforms these by a large margin for both coarse-and finegrained IS classification. ---------------------------------- **RELATED WORK**",
  "y": "differences uses"
 },
 {
  "id": "71a72cfca17b0b15938ed590f9c868_3",
  "x": "As their approach is restricted to definites, they only analyse a subset of the mentions we consider carrying IS. Siddharthan et al. (2011) also concentrate on a subproblem of IS only, namely the hearer-old/hearer-new distinctions for person proper names. Nissim (2006) and<cite> Rahman and Ng (2011)</cite> both present algorithms for IS detection on Nissim et al.'s (2004) Switchboard corpus.",
  "y": "background"
 },
 {
  "id": "71a72cfca17b0b15938ed590f9c868_4",
  "x": "Nissim (2006) and<cite> Rahman and Ng (2011)</cite> both present algorithms for IS detection on Nissim et al.'s (2004) Switchboard corpus. Both papers treat IS classification as a local classification problem whereas we look at dependencies between the IS status of different mentions, leading to collective classification. In addition, they only distinguish the three main categories old, mediated and new.",
  "y": "differences"
 },
 {
  "id": "71a72cfca17b0b15938ed590f9c868_5",
  "x": "**FEATURES FOR LOCAL CLASSIFICATION** We use the following local features, including the features in Nissim (2006) and<cite> Rahman and Ng (2011)</cite> to be able to gauge how their systems fare on our corpus and as a comparison point for our novel collective classification approach. The features developed by Nissim (2006) are shown in Table 4 .",
  "y": "uses"
 },
 {
  "id": "71a72cfca17b0b15938ed590f9c868_6",
  "x": "Both Nissim (2006) and<cite> Rahman and Ng (2011)</cite> classify each mention individually in a standard supervised ML setting, not considering potential dependencies between the IS categories of different mentions. However, collective or joint classification has made substantial impact in other NLP tasks, such as opinion mining (Pang and Lee, 2004; Somasundaran et al., 2009 ), text categorization (Yang et al., 2002; Taskar et al., 2002) and the related task of coreference resolution (Denis and Baldridge, 2007) . We investigate two types of relations between mentions that might impact on IS classification.",
  "y": "background"
 },
 {
  "id": "71a72cfca17b0b15938ed590f9c868_7",
  "x": "Using such a relational feature catches two birds with one stone: firstly, it integrates the internal structure of a mention into the algorithm, which<cite> Rahman and Ng (2011)</cite> ignore; secondly, it captures dependencies between parent and child classification, which would not be possible if we integrated the internal structure via flat features or additional tree kernels. We hypothesise that the higher syntactic complexity of our news genre (14.5% of all mentions are mediated/synt) will make this feature highly effective in distinguishing between new and mediated categories. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "71a72cfca17b0b15938ed590f9c868_8",
  "x": "**EXPERIMENTAL SETUP** We use our gold standard corpus (see Section 3.3) via 10-fold cross-validation on documents for all experiments. Following Nissim (2006) and<cite> Rahman and Ng (2011)</cite> , we perform all experiments on gold standard mentions and use the human WSJ syntactic annotation for feature extraction, when necessary.",
  "y": "uses"
 },
 {
  "id": "71a72cfca17b0b15938ed590f9c868_9",
  "x": "**LOCAL CLASSIFIERS** We reimplemented the algorithms in Nissim (2006) and<cite> Rahman and Ng (2011)</cite> as comparison baselines, using their feature and algorithm choices. Algorithm Nissim is therefore a decision tree J48 with standard settings in WEKA with the features in Table 4.",
  "y": "uses"
 },
 {
  "id": "71a72cfca17b0b15938ed590f9c868_10",
  "x": "We use NetKit (Macskassy and Provost, 2007) with its standard ICA settings for collective inference, as it allows direct comparison between local and collective classification. The relational classifiers are always exactly the same classifiers as the local ones with the relational features added: thus, if the local classifier is a tree kernel SVM so is the relational one. One problem when using the SVM Tree kernel as relational classifier is that it allows only for binary classification so that we need to train several binary networks in a one-vs-all paradigm (see also<cite> (Rahman and Ng, 2011)</cite> ), which will not be able to use the multiclass dependencies of the relational features to optimum effect.",
  "y": "background"
 },
 {
  "id": "71a72cfca17b0b15938ed590f9c868_11",
  "x": "Based on linguistic intuition, we define features for classifying mentions collectively. We show that our collective classification approach outperforms the state-of-the-art in coarse-grained IS classification by about 10% (Nissim, 2006) and 5%<cite> (Rahman and Ng, 2011)</cite> accuracy. The gain is almost entirely due to improvements in distinguishing between new and mediated mentions.",
  "y": "differences"
 },
 {
  "id": "71a72cfca17b0b15938ed590f9c868_12",
  "x": "For the latter, we also report the -to our knowledge -first finegrained IS classification results. Since the work reported in this paper relied -following Nissim (2006) and<cite> Rahman and Ng (2011)</cite> -on gold standard mentions and syntactic annotations, we plan to perform experiments with predicted mentions as well. We also have to improve the recognition of bridging, ideally combining recognition and antecedent selection for a complete resolution component.",
  "y": "uses"
 },
 {
  "id": "71bcd87091c4f5e2b7bc564b7bd9dd_0",
  "x": "**PROPOSED MODEL** Here, we describe briefly the underlying framework, called RNN Encoder-Decoder, proposed by<cite> (Cho et al., 2014b)</cite> and (Sutskever et al., 2014) upon which we build a machine transliteration model that learns to transliterate end-to-end. The enoder is a character-based recurrent neural network that learns a highly nonlinear mapping from a spelling to the phonetic of the input sequence.",
  "y": "extends background"
 },
 {
  "id": "71bcd87091c4f5e2b7bc564b7bd9dd_1",
  "x": "**EXPERIMENTS** We conducted a set of experiments to show the effectiveness of RNN Encoder-Decoder model<cite> (Cho et al., 2014b</cite>; Sutskever et al., 2014) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task . Table 1 shows different datasets in our experiments.",
  "y": "uses"
 },
 {
  "id": "71bcd87091c4f5e2b7bc564b7bd9dd_2",
  "x": "'TaskID' is a unique identifier in the following experiments. We leveraged a character-based encoderdecoder model (Bojanowski et al., 2015; Chung et al., 2016) with soft attention mechanism<cite> (Cho et al., 2014b)</cite> . In this model, input sequences in both source and target languages have been represented as characters.",
  "y": "uses"
 },
 {
  "id": "71bcd87091c4f5e2b7bc564b7bd9dd_3",
  "x": "The encoder has 128 hidden units for each direction (forward and backward), and the decoder has 128 hidden units with soft attention mechanism<cite> (Cho et al., 2014b)</cite> . We train the model using stochastic gradient descent with Adam (Kingma and Ba, 2014). Each update is computed using a minibatch of 128 sequence pairs.",
  "y": "background"
 },
 {
  "id": "7202fd7fe7e776362b126f7cbf0bf3_0",
  "x": "End-to-end speech models often have millions of parameters [7, 8,<cite> 9,</cite> 10, 11] . However, data augmentation and dropout have not been extensively studied or applied to them. We investigate the effectiveness of data augmentation and dropout for regularizing end-to-end speech models.",
  "y": "background"
 },
 {
  "id": "7202fd7fe7e776362b126f7cbf0bf3_1",
  "x": "End-to-end speech models often have millions of parameters [7, 8,<cite> 9,</cite> 10, 11] . However, data augmentation and dropout have not been extensively studied or applied to them. We investigate the effectiveness of data augmentation and dropout for regularizing end-to-end speech models.",
  "y": "motivation background"
 },
 {
  "id": "7202fd7fe7e776362b126f7cbf0bf3_2",
  "x": "Feature level augmentation has also demonstrated effectiveness [5, 3, 4] . Ko et al. [2] performed audio level speed perturbation that also lead to performance improvements. Background noise is used for augmentation in [8,<cite> 9]</cite> to improve performance on noisy speech.",
  "y": "background"
 },
 {
  "id": "7202fd7fe7e776362b126f7cbf0bf3_3",
  "x": "Background noise is used for augmentation in [8,<cite> 9]</cite> to improve performance on noisy speech. Apart from adding noise, our data augmentation also modifies the tempo, pitch, volume and temporal alignment of the audio. Dropout has been applied to speech recognition before.",
  "y": "similarities background"
 },
 {
  "id": "7202fd7fe7e776362b126f7cbf0bf3_4",
  "x": "---------------------------------- **MODEL ARCHITECTURE** The end-to-end model structure used in this work is very similar to the model architecture of Deep Speech 2 (DS2) <cite>[9]</cite> .",
  "y": "similarities"
 },
 {
  "id": "7202fd7fe7e776362b126f7cbf0bf3_6",
  "x": "Our results on both WSJ and LibriSpeech are competitive to existing methods. We would like to note that our model achieved comparable results with Amodei et al. <cite>[9]</cite> on LibriSpeech dataset, although our model is only trained only on the provided training set. This demonstrates the effectiveness of the proposed regularization methods for training end-to-end speech models.",
  "y": "similarities"
 },
 {
  "id": "7293ab5db16d3fe1fee48d45154697_0",
  "x": "The systems participating in the task are required to apply the labels BAD and OK, either to words or phrases. In this paper we describe the approach behind the submissions of the Universitat d'Alacant team to these sub-tasks. For our word-level submissions we have applied the approach proposed by<cite> Espl\u00e0-Gomis et al. (2015)</cite> , where we used black-box bilingual on-line resources.",
  "y": "extends differences"
 },
 {
  "id": "7293ab5db16d3fe1fee48d45154697_1",
  "x": "The new task tackles MTQE for translating English into German. For this task we have combined two on-line-available MT systems, 1 Lucy LT KWIK Translator 2 and Google Translate, 3 and the bilingual concordancer Reverso Context 4 to spot sub-segment correspondences between a sentence S in the source language (SL) and a given translation hypothesis T in the target language (TL). As described by<cite> Espl\u00e0-Gomis et al. (2015)</cite> , a collection of features is obtained from these correspondences and then used by a binary classifier to determine the final word-level MTQE labels.",
  "y": "extends differences"
 },
 {
  "id": "7293ab5db16d3fe1fee48d45154697_2",
  "x": "The paper ends with some concluding remarks. 2 Sources of bilingual information for machine translation quality estimation at the word and phrase levels The method used to produce the word-level MTQE submissions is the same than that used by the UAlacant team in the last edition of the shared task of MTQE at WMT 2015 <cite>(Espl\u00e0-Gomis et al., 2015)</cite> , which uses binary classification based on a collection of information.",
  "y": "background"
 },
 {
  "id": "7293ab5db16d3fe1fee48d45154697_3",
  "x": "A complete description of the features used for word-level MTQE can be found in Section 2 of the paper by<cite> Espl\u00e0-Gomis et al. (2015)</cite> . Following the approach by<cite> Espl\u00e0-Gomis et al. (2015)</cite> , the perceptron was built with a single hidden layer containing the same number of nodes as the number of features; this was the best performing architecture in the preliminary experiments.",
  "y": "background"
 },
 {
  "id": "7293ab5db16d3fe1fee48d45154697_4",
  "x": "**BINARY CLASSIFIER** A multilayer perceptron (Duda et al., 2000 , Section 6) was used for classification, as implemented in Weka 3.7 (Hall et al., 2009) . Following the approach by<cite> Espl\u00e0-Gomis et al. (2015)</cite> , the perceptron was built with a single hidden layer containing the same number of nodes as the number of features; this was the best performing architecture in the preliminary experiments.",
  "y": "extends differences"
 },
 {
  "id": "7293ab5db16d3fe1fee48d45154697_5",
  "x": "dcs.shef.ac.uk/wmt16_files_qe/task2_ en-de_test.tar.gz 6 The list of features can be found in the file features list in the package http://www.quest. dcs.shef.ac.uk/wmt16_files_qe/task2p_ en-de_test.tar.gz 7 The rest of parameters of the classifiers were also kept as in the approach by<cite> Espl\u00e0-Gomis et al. (2015)</cite> . provided by the organisation were used to train the binary classifiers, both for word and phrase levels, while the development sets were used as validation sets on which the training error was computed, in order to minimise the risk of overfitting.",
  "y": "uses similarities"
 },
 {
  "id": "7293ab5db16d3fe1fee48d45154697_6",
  "x": "The table also includes the results obtained with a binary classifier trained only on the baseline features (baseline), in order to estimate the contribution of the features described in this work on the performance of the system. Incidentally, and in spite of the changes in languages and machine translation systems, the results obtained for word-level MTQE are very similar to those obtained by<cite> Espl\u00e0-Gomis et al. (2015)</cite> for the translation from English into Spanish. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "7293ab5db16d3fe1fee48d45154697_7",
  "x": "The approach employed is aimed at being system-independent, since it only uses resources produced by external systems, which makes the addition of new sources of bilingual information straightforward. In fact, one of the sources of bilingual information used in the previous edition of the shared task, Apertium, has been replaced by a new one: Lucy LT. The results obtained confirm the conclusion by<cite> Espl\u00e0-Gomis et al. (2015)</cite> that combining the baseline features with those obtained from external sources of bilingual information provide a noticeable improvement, in this case, not only for word-level MTQE, but also for phrase-level MTQE.",
  "y": "extends differences"
 },
 {
  "id": "737a452057be3e254b35bd8df492be_0",
  "x": "<cite>[10]</cite> released one of the initial data sets from Twitter with the goal of identifying what constitutes racism and sexism. [9] in their work pointed out that hate speech is different from offensive language and released a data set of 25k tweets with the goal of distinguishing hate speech from offensive language. Stop saying dumb blondes with pretty faces as you need a pretty face to pull them off !!! #mkr In Islam women must be locked in their houses and Muslims claim this is treating them well Table 1 : Tweets from <cite>[10]</cite> data set demonstrating online abuse <cite>They</cite> find that racist and homophobic tweets are more likely to be classified as hate speech but sexist tweets are generally classified as offensive.",
  "y": "background"
 },
 {
  "id": "737a452057be3e254b35bd8df492be_1",
  "x": "Stop saying dumb blondes with pretty faces as you need a pretty face to pull them off !!! #mkr In Islam women must be locked in their houses and Muslims claim this is treating them well Table 1 : Tweets from <cite>[10]</cite> data set demonstrating online abuse <cite>They</cite> find that racist and homophobic tweets are more likely to be classified as hate speech but sexist tweets are generally classified as offensive. [4] introduced a large, hand-coded corpus of online harassment data for studying the nature of harassing comments and the culture of trolling. Keeping these motivations in mind, we make the following salient contributions:",
  "y": "uses"
 },
 {
  "id": "737a452057be3e254b35bd8df492be_2",
  "x": "2) Deep Learning models which learn feature representations on their own. <cite>[10]</cite> released the popular data set of 16k tweets annotated as belonging to sexism, racism or none class 1 , and provided a feature engineered model for detection of abuse in their corpus. [9] use a similar handcrafted feature engineered model to identify offensive language and distinguish it from hate speech. [2] in their work, experiment with multiple deep learning architectures for the task of hate speech detection on Twitter using the same data set by <cite>[10]</cite> .",
  "y": "background"
 },
 {
  "id": "737a452057be3e254b35bd8df492be_3",
  "x": "2) Deep Learning models which learn feature representations on their own. <cite>[10]</cite> released the popular data set of 16k tweets annotated as belonging to sexism, racism or none class 1 , and provided a feature engineered model for detection of abuse in their corpus. [9] use a similar handcrafted feature engineered model to identify offensive language and distinguish it from hate speech. [2] in their work, experiment with multiple deep learning architectures for the task of hate speech detection on Twitter using the same data set by <cite>[10]</cite> .",
  "y": "background"
 },
 {
  "id": "737a452057be3e254b35bd8df492be_4",
  "x": "On the data set released by <cite>[10]</cite> , [5] experiment with a two-step approach of detecting abusive language first and then classifying them into specific types i.e. racist, sexist or none. They achieve best results using a Hybrid Convolution Neural Network (CNN) with the intuition that character level input would counter the purposely or mistakenly misspelled words and made-up vocabularies. [6] in their work ran experiments on the Gazetta dataset and the DETOX system ( [12] ) and show that a Recurrent Neural Network (RNN) coupled with deep, classification-specific attention outperforms the previous state of the art in abusive comment moderation.",
  "y": "background"
 },
 {
  "id": "737a452057be3e254b35bd8df492be_5",
  "x": "Once u i is obtained we calculate the importance of the word as the similarity Data Set Tweets Count <cite>[10]</cite> 15,844 [9] 25,112 [4] 20,362 Table 2 : Data sets and their total tweets count of u i with u c and get a normalized importance weight \u03b1 i through a softmax function. The context vector u c can be seen as a tool which filters which word is more important over all the words like that used in the LSTM. Figure 2 shows the high-level architecture of this model.",
  "y": "uses"
 },
 {
  "id": "737a452057be3e254b35bd8df492be_6",
  "x": "**DATA SETS** We have used the 3 benchmark data sets for abusive content detection on Twitter. At the time of the experiment, the <cite>[10]</cite> data set had a total of 15,844 tweets out of which 1,924 were labelled as belonging to racism, 3,058 as sexism and 10,862 as none.",
  "y": "uses"
 },
 {
  "id": "737a452057be3e254b35bd8df492be_7",
  "x": "We call <cite>[10]</cite> data set as <cite>D1</cite> , [9] data set as D2 and [4] as D3 For tweet tokenization, we use Ekphrasis which is a text processing tool built specially from social platforms such as Twitter. [3] use a big collection of Twitter messages (330M) to generate word embeddings, with a vocabulary size of 660K words, using GloVe ( [8] ). We use these pre-trained word embeddings for initializing the first layer (embedding layer) of our neural networks.",
  "y": "uses"
 },
 {
  "id": "737a452057be3e254b35bd8df492be_8",
  "x": "**RESULTS** The network is trained at a learning rate of 0.001 for 10 epochs, with a dropout of 0.2 to prevent over-fitting. The results are averaged over 10-fold cross-validations for <cite>D1</cite> and D3 and 5 fold cross-validations for D2 because [9] reported results using 5 fold CV.",
  "y": "uses"
 },
 {
  "id": "737a452057be3e254b35bd8df492be_9",
  "x": "We also share some examples from the three data sets in Figure  2 which our BiLSTM attention model could not classify correctly. On closer investigation we find that most cases where our model fails are instances where annotation is either noisy or the difference between classes are very blurred and subtle. The first tweet is a tweet from <cite>[10]</cite> , the second tweet is a tweet from from [9] data set and the third from the [4]",
  "y": "uses"
 },
 {
  "id": "737a452057be3e254b35bd8df492be_10",
  "x": "The first tweet is a sexist tweet from <cite>[10]</cite> where as the second tweet is an example of racist tweet from the same datset . The third tweet is from [9] data set labelled as offensive language. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "73d4518e44f14a725a28e86de96963_0",
  "x": "Information related to affect has been also exploited (Reyes et al., 2013;<cite> Barbieri et al., 2014</cite>; Hern\u00e1ndez Far\u00edas et al., 2015) . Other scholars proposed methods exploiting the context surrounding an ironic utterance (Wallace et al., 2015; Karoui et al., 2015) . Recently, also deep learning techniques have been applied (Nozza et al., 2016; Poria et al., 2016) .",
  "y": "background"
 },
 {
  "id": "73d4518e44f14a725a28e86de96963_1",
  "x": "Attempting to take advantage of the emotionally-laden characteristics of ironic expressions, we relied on emotIDM, an irony detection model that, taking advantage of several affective resources available for English (Nissim and Patti, 2016) , exploits various facets of affective information from sentiment to finer-grained emotions for characterizing the presence of irony in Twitter . In ) the robustness of emotIDM was assessed over different Twitter state-of-the-art corpora for irony detection (Reyes et al., 2013;<cite> Barbieri et al., 2014</cite>; Mohammad et al., 2015; Pt\u00e1\u010dek et al., 2014; Riloff et al., 2013) . The obtained results outperform those in the previous works confirming the significance of affective features for irony detection.",
  "y": "background"
 },
 {
  "id": "73d4518e44f14a725a28e86de96963_2",
  "x": "From these tweets, 265 were belonging to the ironic class, while 592 were labeled as non-ironic. Notice that, in (Hern\u00e1ndez-Farias et al., 2014) , the authors found a similar behavior regarding URL information in the dataset provided by the organizers of SentiPOLC-2014 (Basile et al., 2014) . Furthermore,<cite> Barbieri et al. (2014)</cite> exploited a feature for alerting the existence of an URL in a tweet; such feature was ranked among the most discriminative ones according to an information gain analysis.",
  "y": "background"
 },
 {
  "id": "73d4518e44f14a725a28e86de96963_3",
  "x": "We took advantage of a set of corpora previously used in the state of the art in irony detection. We exploited data from a set of corpora collected exploiting different approaches: self-tagging or manual annotation or crowd-sourcing 3 . We exploited the corpora developed by (Reyes et al., 2013) , <cite>(Barbieri et al., 2014)</cite> , (Mohammad et al., 2015) , (Pt\u00e1\u010dek et al., 2014) , (Riloff et al., 2013) , (Ghosh et al., 2015) , (Karoui et al., 2017) , and (Sulis et al., 2016) .",
  "y": "uses"
 },
 {
  "id": "73d4518e44f14a725a28e86de96963_4",
  "x": "Distinguishing between different kinds of ironic devices is still a controversial issue. In computational linguistics, only few research works have attempted to address such a difficult task (Wang, 2013;<cite> Barbieri et al., 2014</cite>; Sulis et al., 2016; Van Hee et al., 2016) . We are interested in assessing the performance of emotIDM when it deals with different types of irony, in order to test if a wide variety of affective features can help in discriminating also in the finer-grained classification task here proposed.",
  "y": "background motivation"
 },
 {
  "id": "73d7831596bfe6d6861f360042048f_0",
  "x": "Constructing such grammars is time consuming and error-prone and requires extensive linguistic knowledge and programming proficiency. Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field<cite> (Sproat and Jaitly, 2016)</cite> . In this paper, we present our approach to nonstandard text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively.",
  "y": "background"
 },
 {
  "id": "73d7831596bfe6d6861f360042048f_1",
  "x": "Constructing such grammars is time consuming and error-prone and requires extensive linguistic knowledge and programming proficiency. Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field<cite> (Sproat and Jaitly, 2016)</cite> . In this paper, we present our approach to nonstandard text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively.",
  "y": "motivation"
 },
 {
  "id": "73d7831596bfe6d6861f360042048f_2",
  "x": "---------------------------------- **DATA-DRIVEN APPROACHES** Recently, methods based on neural networks have been applied to TN and ITN<cite> (Sproat and Jaitly, 2016</cite>; Pusateri et al., 2017; Yolchuyeva et al., 2018) .",
  "y": "background"
 },
 {
  "id": "73d7831596bfe6d6861f360042048f_3",
  "x": "Recently, methods based on neural networks have been applied to TN and ITN<cite> (Sproat and Jaitly, 2016</cite>; Pusateri et al., 2017; Yolchuyeva et al., 2018) . To overcome one of the biggest problems -a lack of supervision, WFSTs have been used to transform large amounts of written-form text to its spoken form. Researchers hope a vast amount of such data can counteract the errors inherited in WFST-based models.",
  "y": "motivation"
 },
 {
  "id": "73d7831596bfe6d6861f360042048f_4",
  "x": "Researchers hope a vast amount of such data can counteract the errors inherited in WFST-based models. Recent data-driven approaches examine window-based sequence-to-sequence (seq2seq) models and convolutional neural networks (CNN) to normalize a central piece of text with the help of context<cite> (Sproat and Jaitly, 2016</cite>; Yolchuyeva et al., 2018) . Window-based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special <self> token.",
  "y": "background"
 },
 {
  "id": "73d7831596bfe6d6861f360042048f_5",
  "x": "---------------------------------- **BASELINE MODELS** Following <cite>Sproat and Jaitly (2016)</cite>, we implement a seq2seq model trained on window-based data.",
  "y": "uses"
 },
 {
  "id": "73d7831596bfe6d6861f360042048f_6",
  "x": "Table 1 illustrates the window-based model's training examples corresponding to one sentence \"wake me up at 8 AM .\" which is broken down into 6 pairs. <n> and </n> indicate the center of the window. A window center might contain 1 or more words (e.g., \"8 AM\") and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as TIME, DATE, ORDINAL<cite> (Sproat and Jaitly, 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "73d7831596bfe6d6861f360042048f_7",
  "x": "The data for the window-based seq2seq model and full sentence seq2seq were generated from the publicly available release of parallel written/speech formatted text from <cite>Sproat and Jaitly (2016)</cite> . The set consists of Wikipedia text which was processed through Google TTS's Kestrel text normalization system relying primarily on handcrafted rules to produce speech-formatted text. Although a large parallel dataset is available for English, we consider the feasibility of developing neural models for other languages which may not have text normalization systems in place.",
  "y": "uses"
 },
 {
  "id": "73d7831596bfe6d6861f360042048f_8",
  "x": "Therefore, we choose to scale the training data size to a limited set of text which could be generated by annotators in a reasonable time frame. As summarized in Table 2 , both window-based and sentencebased models are trained with 500K training instances. Our datasets were randomly sampled from a set of 4.9M sentences in the training data portion of the <cite>Sproat and Jaitly (2016)</cite> data release and split into training, validation, and test data.",
  "y": "uses"
 },
 {
  "id": "73d7831596bfe6d6861f360042048f_9",
  "x": "We follow <cite>Sproat and Jaitly (2016)</cite> in down-sampling window-based training data to constrain the proportion of \"<self>\" tokens to 10% of the data. For training sentence-based models, the source sentence is segmented into characters while the target sentence is broken into tokens. For the subword model, both the source and target sentences are segmented into subword sequences.",
  "y": "uses"
 },
 {
  "id": "73d7831596bfe6d6861f360042048f_10",
  "x": "---------------------------------- **BASELINE MODEL SETUP** Our first approach replicates the window-based seq2seq model of <cite>Sproat and Jaitly (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "73d7831596bfe6d6861f360042048f_11",
  "x": "As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with <cite>Sproat and Jaitly (2016)</cite> , considering our training set is much smaller. There are 16 different edit labels shown. Data with TELEPHONE labels were not included in the initial analysis of <cite>Sproat and Jaitly (2016)</cite> , but were made available in the dataset release.",
  "y": "similarities differences"
 },
 {
  "id": "73d7831596bfe6d6861f360042048f_12",
  "x": "As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with <cite>Sproat and Jaitly (2016)</cite> , considering our training set is much smaller. There are 16 different edit labels shown. Data with TELEPHONE labels were not included in the initial analysis of <cite>Sproat and Jaitly (2016)</cite> , but were made available in the dataset release.",
  "y": "differences"
 },
 {
  "id": "73d7831596bfe6d6861f360042048f_13",
  "x": "* TELEPHONE is not reported in <cite>Sproat and Jaitly (2016)</cite> but included in the dataset; ** we removed ELECTRONIC category. As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with <cite>Sproat and Jaitly (2016)</cite> , considering our training set is much smaller. There are 16 different edit labels shown.",
  "y": "extends differences"
 },
 {
  "id": "73d7831596bfe6d6861f360042048f_14",
  "x": "Our labels are generated directly from the Google FST<cite> (Sproat and Jaitly, 2016)</cite> . Each type of feature is represented by a one-hot encoding. To combine linguistic features with subword units, one can add or concatenate each subword's embedding with its corresponding linguistic feature embedding and feed a combined embedding to the bi-LSTM encoder.",
  "y": "uses"
 },
 {
  "id": "73eaa7d5a54b2d60bd8128e0270683_0",
  "x": "Deep neural networks have been proven to be a powerful framework for natural language processing, and have demonstrated strong performance on a number of challenging tasks, ranging from machine translation (Cho et al., 2014b,a) , to text categorisation (Zhang et al., 2015; Joulin et al., 2017; Liu et al., 2018b) . Not only do such deep models outperform traditional machine learning methods, they also come with the benefit of not requiring difficult feature engineering. For instance, both Lample et al. (2016) and <cite>Ma and Hovy (2016)</cite> propose end-to-end models for sequence labelling task and achieve state-of-the-art results.",
  "y": "background"
 },
 {
  "id": "73eaa7d5a54b2d60bd8128e0270683_1",
  "x": "Of particular interest to this paper is the work by <cite>Ma and Hovy (2016)</cite> introduce a strong end-to-end model combining a bi-directional Long Short-Term Memory (Bi-LSTM) network with Convolutional Neural Network (CNN) character encoding in a Conditional Random Field (CRF). Their model is highly capable of capturing not only word-but also characterlevel features. We extend this model by integrating an auto-encoder loss, allowing the model to take hand-crafted features as input and re-construct them as output, and show that, even with such a highly competitive model, incorporating linguistic features is still beneficial.",
  "y": "background"
 },
 {
  "id": "73eaa7d5a54b2d60bd8128e0270683_2",
  "x": "**MODEL ARCHITECTURE** We build on a highly competitive sequence labelling model, namely Bi-LSTM-CNN-CRF, first introduced by <cite>Ma and Hovy (2016)</cite> . Given an input sequence of x = {x 1 , x 2 , . . . , x T } of length T , the model is capable of tagging each input with a predicted label\u0177, resulting in a sequence of\u0177 = {\u0177 1 ,\u0177 2 , . . . ,\u0177 T } closely matching the gold label sequence y = {y 1 , y 2 , . . . , y T }.",
  "y": "similarities uses"
 },
 {
  "id": "73eaa7d5a54b2d60bd8128e0270683_3",
  "x": "An illustration of the model architecture is presented in Figure 1 . Zadrozny, 2014; Chiu and Nichols, 2016;<cite> Ma and Hovy, 2016)</cite> have demonstrated that CNNs are highly capable of capturing character-level features. Here, our character-level CNN is similar to that used in <cite>Ma and Hovy (2016)</cite> but differs in that we use a ReLU activation (Nair and Hinton, 2010) .",
  "y": "motivation background"
 },
 {
  "id": "73eaa7d5a54b2d60bd8128e0270683_4",
  "x": "Zadrozny, 2014; Chiu and Nichols, 2016;<cite> Ma and Hovy, 2016)</cite> have demonstrated that CNNs are highly capable of capturing character-level features. Here, our character-level CNN is similar to that used in <cite>Ma and Hovy (2016)</cite> but differs in that we use a ReLU activation (Nair and Hinton, 2010) . 1",
  "y": "extends differences"
 },
 {
  "id": "73eaa7d5a54b2d60bd8128e0270683_5",
  "x": "We use the IOBES tagging scheme, as previous study have shown that this scheme provides a modest improvement to the model performance (Ratinov and Roth, 2009; Chiu and Nichols, 2016; Lample et al., 2016;<cite> Ma and Hovy, 2016)</cite> . Model configuration. Following the work of <cite>Ma and Hovy (2016)</cite> , we initialise word embeddings with GloVe (Pennington et al., 2014 ) (300-dimensional, trained on a 6B-token corpus).",
  "y": "similarities uses"
 },
 {
  "id": "73eaa7d5a54b2d60bd8128e0270683_6",
  "x": "We use the IOBES tagging scheme, as previous study have shown that this scheme provides a modest improvement to the model performance (Ratinov and Roth, 2009; Chiu and Nichols, 2016; Lample et al., 2016;<cite> Ma and Hovy, 2016)</cite> . Model configuration. Following the work of <cite>Ma and Hovy (2016)</cite> , we initialise word embeddings with GloVe (Pennington et al., 2014 ) (300-dimensional, trained on a 6B-token corpus).",
  "y": "similarities"
 },
 {
  "id": "73eaa7d5a54b2d60bd8128e0270683_7",
  "x": "Baseline. In addition to reporting a number of prior results of competitive baseline models, as listed in Table 2 , we also re-implement the Bi-LSTM-CNN-CRF model by <cite>Ma and Hovy (2016)</cite> (referred to as Neural-CRF in Table 2 ) and report its average performance. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "740db031e3fc086dfdb2477caeac66_0",
  "x": "When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder (Bowman et al., 2016; <cite>Xu and Durrett, 2018</cite>; Dieng et al., 2019) . While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Various efforts have been made to alleviate the latent variable collapse issue.",
  "y": "background"
 },
 {
  "id": "740db031e3fc086dfdb2477caeac66_1",
  "x": "Yang et al. (2017) discovered that there is a trade-off between the contextual capacity of the decoder and effective use of encoding information, and developed a dilated CNN as decoder which can vary the amount of conditioning context. They also introduced a loss clipping strategy in order to make the model more robust. <cite>Xu and Durrett (2018)</cite> addressed the problem by replacing the standard normal distribution for the prior with the von Mises-Fisher (vMF) distribution.",
  "y": "background"
 },
 {
  "id": "740db031e3fc086dfdb2477caeac66_2",
  "x": "Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss (Bowman et al., 2016; , or resort to designing more sophisticated model structures (Yang et al., 2017; <cite>Xu and Durrett, 2018</cite>; Dieng et al., 2019) . In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse. In contrast to existing VAE-RNN models for text modelling which merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation for all hidden states of the RNN encoder.",
  "y": "background motivation"
 },
 {
  "id": "740db031e3fc086dfdb2477caeac66_3",
  "x": "Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures. We evaluate our model against several strong baselines which apply VAE for text modelling (Bowman et al., 2016; Yang et al., 2017; <cite>Xu and Durrett, 2018</cite>) . We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation dataset (Novikova et al., 2017) .",
  "y": "uses"
 },
 {
  "id": "740db031e3fc086dfdb2477caeac66_4",
  "x": "In this section, we discuss the technical details of the proposed holistic regularisation VAE (HR-VAE) model, a general architecture which can effectively mitigate the KL vanishing phenomenon. Our model design is motivated by one noticeable defect shared by the VAE-RNN based models in previous works (Bowman et al., 2016; Yang et al., 2017; <cite>Xu and Durrett, 2018</cite>; Dieng et al., 2019) . That is, all these models, as shown in Figure 1a , only impose a standard normal distribution prior on the last hidden state of the RNN encoder, which potentially leads to learning a suboptimal representation of the latent variable and results in model vulnerable to KL loss vanishing.",
  "y": "motivation"
 },
 {
  "id": "740db031e3fc086dfdb2477caeac66_5",
  "x": "---------------------------------- **DATASETS** We evaluate our model on two public datasets, namely, Penn Treebank (PTB) (Marcus and Marcinkiewicz, 1993) and the end-to-end (E2E) text generation corpus (Novikova et al., 2017) , which have been used in a number of previous works for text generation (Bowman et al., 2016; <cite>Xu and Durrett, 2018</cite>; Wiseman et al., 2018; Su et al., 2018) .",
  "y": "uses"
 },
 {
  "id": "740db031e3fc086dfdb2477caeac66_6",
  "x": "For the PTB dataset, we used the train-test split following (Bowman et al., 2016; <cite>Xu and Durrett, 2018</cite>) . For the E2E dataset, we used the train-test split from the original dataset (Novikova et al., 2017) and indexed the words with a frequency higher than 3. We represent input data with 512-dimensional word2vec embeddings (Mikolov et al., 2013) .",
  "y": "uses"
 },
 {
  "id": "740db031e3fc086dfdb2477caeac66_7",
  "x": "**BASELINES** We compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base 3 : A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue (Bowman et al., 2016) ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder (Yang et al., 2017) ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (<cite>Xu and Durrett, 2018</cite>) .",
  "y": "uses"
 },
 {
  "id": "74420437db295ca874d5c946891f69_0",
  "x": "As multiword expressions (MWEs) exhibit a range of idiosyncrasies, their automatic detection warrants the use of many different features. <cite>Tsvetkov and Wintner (2014)</cite> proposed a Bayesian network model that combines linguistically motivated features and also models their interactions. In this paper, we extend their model with new features and apply it to Croatian, a morphologically complex and a relatively free word order language, achieving a satisfactory performance of 0.823 F1-score.",
  "y": "background"
 },
 {
  "id": "74420437db295ca874d5c946891f69_1",
  "x": "<cite>Tsvetkov and Wintner (2014)</cite> proposed a Bayesian network model that combines linguistically motivated features and also models their interactions. In this paper, we extend their model with new features and apply it to Croatian, a morphologically complex and a relatively free word order language, achieving a satisfactory performance of 0.823 F1-score. Furthermore, by comparing against (semi)na\u00efve Bayes models, we demonstrate that manually modeling feature interactions is indeed important.",
  "y": "extends"
 },
 {
  "id": "74420437db295ca874d5c946891f69_2",
  "x": "Recently, <cite>Tsvetkov and Wintner (2014)</cite> proposed an approach for the detection of MWE candidates that combines a number of statistical and linguistic features. The most interesting aspect of their work is that they explicitly model the linguistically motivated interactions between the features using a Bayesian network (BN). The advantages of BNs lie in their interpretability and the possibility to encode linguistic knowledge in the form of the network structure.",
  "y": "background"
 },
 {
  "id": "74420437db295ca874d5c946891f69_3",
  "x": "In this paper, we address the task of MWE detection (type-level MWE classification) for Croatian, a South Slavic language with a rich morphology and a relatively free word order. The starting point of our work is the model of <cite>Tsvetkov and Wintner (2014)</cite> , which we extend with a number of features, including language-specific ones that account for the relatively free word order. Our main research question is whether modeling the interactions between features is important, and whether these can be learned automatically.",
  "y": "extends"
 },
 {
  "id": "74420437db295ca874d5c946891f69_4",
  "x": "<cite>Tsvetkov and Wintner (2014)</cite> showed that a manually-designed BN substantially outperforms the one whose structure is learned automatically, hypothesizing that the cause for this might be the increased model complexity. We conduct a similar experiment using a structurelearning algorithm, but also model the interactions using a simpler, semi-naive Bayes classifier, for which the number of parameters is restricted. Finally, we compare these models against a structurefree counterpart, a na\u00efve Bayes classifier.",
  "y": "background"
 },
 {
  "id": "74420437db295ca874d5c946891f69_5",
  "x": "<cite>Tsvetkov and Wintner (2014)</cite> showed that a manually-designed BN substantially outperforms the one whose structure is learned automatically, hypothesizing that the cause for this might be the increased model complexity. We conduct a similar experiment using a structurelearning algorithm, but also model the interactions using a simpler, semi-naive Bayes classifier, for which the number of parameters is restricted. Finally, we compare these models against a structurefree counterpart, a na\u00efve Bayes classifier.",
  "y": "similarities"
 },
 {
  "id": "74420437db295ca874d5c946891f69_6",
  "x": "For the experiments, we compile a new manually annotated dataset of Croatian MWEs. Unlike <cite>Tsvetkov and Wintner (2014)</cite> , who only consider bigrams, we consider MWEs of up to five words in length. We make the dataset freely available, along with all feature sets needed to replicate the experiments.",
  "y": "differences"
 },
 {
  "id": "74420437db295ca874d5c946891f69_7",
  "x": "---------------------------------- **MODEL** We adopt the BN model of <cite>Tsvetkov and Wintner (2014)</cite> , but extend it with language-specific as well as semantically motivated features.",
  "y": "extends"
 },
 {
  "id": "74420437db295ca874d5c946891f69_8",
  "x": "The model of <cite>Tsvetkov and Wintner (2014)</cite> uses nine statistically and linguistically motivated features, computed for each MWE candidate and designed to discriminate between MWEs and ordinary word sequences. We adopted eight of these features: 1 (1) capitalization (indicating which MWE constituents are capitalized), (2) hyphenation (which constituents are hyphenated), (3) fossil word (whether constituents also occur outside of the MWE), (4) frozen form (whether the MWE is morphologically frozen), (5) partial morphological inflection (whether MWE admits only limited inflection), (6) syntactic pattern (the MWE's part-of-speech pattern), (7) semantic context, and (8) association measure. The values of statistical features were computed from hrWaC, a 1.2B-token Croatian web corpus compiled by Ljube\u0161i\u0107 and Erjavec (2011) .",
  "y": "background"
 },
 {
  "id": "74420437db295ca874d5c946891f69_9",
  "x": "Original features. The model of <cite>Tsvetkov and Wintner (2014)</cite> uses nine statistically and linguistically motivated features, computed for each MWE candidate and designed to discriminate between MWEs and ordinary word sequences. We adopted eight of these features: 1 (1) capitalization (indicating which MWE constituents are capitalized), (2) hyphenation (which constituents are hyphenated), (3) fossil word (whether constituents also occur outside of the MWE), (4) frozen form (whether the MWE is morphologically frozen), (5) partial morphological inflection (whether MWE admits only limited inflection), (6) syntactic pattern (the MWE's part-of-speech pattern), (7) semantic context, and (8) association measure.",
  "y": "uses"
 },
 {
  "id": "74420437db295ca874d5c946891f69_10",
  "x": "**CONCLUSION** We described the experiments on using a combination of linguistically motivated features for MWE detection in Croatian. We adopted the Bayesian network model of <cite>Tsvetkov and Wintner (2014)</cite> and extended it with new features and manually-designed feature interactions, inspired by an analysis of Croatian MWEs.",
  "y": "extends"
 },
 {
  "id": "74568758fe5fef3727d94e7597f305_0",
  "x": "Topic segmentation likewise has multiple educational applications, such as question answering, detecting student initiative, and assessing student answers. There have been essentially two approaches to topic segmentation in the past. The first of these, lexical cohesion, may be used for either linear segmentation (Morris and Hirst, 1991; <cite>Hearst, 1997)</cite> or hierarchical segmentation (Yarri, 1997; Choi, 2000) .",
  "y": "background"
 },
 {
  "id": "74568758fe5fef3727d94e7597f305_1",
  "x": "In this study we apply the methods of Foltz et al. (1998) , Hearst (1994<cite> Hearst ( , 1997</cite> , and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue. All three are vector space methods that measure lexical cohesion to determine topic shifts. Our results show that the new using an orthonormal basis significantly outperforms the other methods.",
  "y": "uses"
 },
 {
  "id": "74568758fe5fef3727d94e7597f305_2",
  "x": "Both Hearst (1994<cite> Hearst ( , 1997</cite> and Foltz et al. (1998) use vector space methods discussed below to represent and compare units of text. The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text. However, Hearst (1994<cite> Hearst ( , 1997</cite> and Foltz et al. (1998) differ on how text units are defined and on how to interpret the results of a comparison.",
  "y": "background"
 },
 {
  "id": "74568758fe5fef3727d94e7597f305_3",
  "x": "Both Hearst (1994<cite> Hearst ( , 1997</cite> and Foltz et al. (1998) use vector space methods discussed below to represent and compare units of text. The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text. However, Hearst (1994<cite> Hearst ( , 1997</cite> and Foltz et al. (1998) differ on how text units are defined and on how to interpret the results of a comparison.",
  "y": "background"
 },
 {
  "id": "74568758fe5fef3727d94e7597f305_4",
  "x": "However, Hearst (1994<cite> Hearst ( , 1997</cite> and Foltz et al. (1998) differ on how text units are defined and on how to interpret the results of a comparison. The text unit's definition in Hearst (1994<cite> Hearst ( , 1997</cite> and Foltz et al. (1998) is generally task dependent, depending on what size gives the best results. For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level.",
  "y": "background"
 },
 {
  "id": "74568758fe5fef3727d94e7597f305_5",
  "x": "Using LSA and this criterion, Foltz et al. (1998) detected chapter boundaries with an F-measure of .33 (see Manning and Sch\u00fctze (1999) for a definition of Fmeasure). Hearst (1994<cite> Hearst ( , 1997</cite> in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores. A depth score is computed as the difference between a given vector comparison and its surrounding peaks, i.e. the local maxima of vector comparisons on either side of the given vector comparison.",
  "y": "background"
 },
 {
  "id": "74568758fe5fef3727d94e7597f305_6",
  "x": "Foltz et al. (1998) , noting mean cosines of .16 for boundaries and .43 for non-boundaries, choose a threshold criterion of .15, which is two standard deviations below the boundary mean of .43. Using LSA and this criterion, Foltz et al. (1998) detected chapter boundaries with an F-measure of .33 (see Manning and Sch\u00fctze (1999) for a definition of Fmeasure). Hearst (1994<cite> Hearst ( , 1997</cite> in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores.",
  "y": "differences background"
 },
 {
  "id": "74568758fe5fef3727d94e7597f305_7",
  "x": "Once all the depth scores are calculated for a text, those that are higher than one standard deviation below the mean are taken as topic boundaries. Using a vector space method without singular value decomposition,<cite> Hearst (1997)</cite> reports an F-measure of .70 when detecting topic shifts between paragraphs. Thus previous work suggests that<cite> the Hearst (1997)</cite> method is superior to that of Foltz et al. (1998) , having roughly twice the accuracy indicated by F-measure.",
  "y": "background"
 },
 {
  "id": "74568758fe5fef3727d94e7597f305_8",
  "x": "Using a vector space method without singular value decomposition,<cite> Hearst (1997)</cite> reports an F-measure of .70 when detecting topic shifts between paragraphs. Thus previous work suggests that<cite> the Hearst (1997)</cite> method is superior to that of Foltz et al. (1998) , having roughly twice the accuracy indicated by F-measure. Although these two results used different data sets and are therefore not directly comparable, one would predict based on this limited evidence that the Hearst algorithm would outperform the Foltz algorithm on other topic segmentation tasks.",
  "y": "background"
 },
 {
  "id": "74568758fe5fef3727d94e7597f305_9",
  "x": "To replicate Foltz et al. (1998) , software was written in Java that created a moving window of varying sizes on the input text, and the software retrieved the LSA vector and calculated the cosine of each window. Hearst (1994<cite> Hearst ( , 1997</cite> was replicated using the JTextTile (Choi, 1999 ) Java software. A variant of Hearst (1994<cite> Hearst ( , 1997</cite> was created by using LSA instead of the standard vector space method.",
  "y": "uses"
 },
 {
  "id": "74568758fe5fef3727d94e7597f305_10",
  "x": "Using the development corpus's average topic length of 16 utterances as a reference point, F-measures were calculated for the combinations of window size and text unit size in Table 1 . On the test set, this combination of parameters yielded an F-measure of .14 as opposed to the Fmeasure for monologue reported by<cite> Hearst (1997)</cite> , .70.",
  "y": "differences"
 },
 {
  "id": "74568758fe5fef3727d94e7597f305_11",
  "x": "Recall that in monologue,<cite> Hearst (1997)</cite> reports a much larger F-measure than Foltz et al. (1998) , .70 vs. .33, albeit on different data sets. In the present dialogue corpus, these roles are reversed, .14 vs. .52. Possible reasons for this reversal are the segmentation criterion, the vector space method, or the fact that Foltz has been trained on similar data via regression and Hearst has not.",
  "y": "background"
 },
 {
  "id": "74568758fe5fef3727d94e7597f305_12",
  "x": "Recall that in monologue,<cite> Hearst (1997)</cite> reports a much larger F-measure than Foltz et al. (1998) , .70 vs. .33, albeit on different data sets. In the present dialogue corpus, these roles are reversed, .14 vs. .52. Possible reasons for this reversal are the segmentation criterion, the vector space method, or the fact that Foltz has been trained on similar data via regression and Hearst has not.",
  "y": "differences"
 },
 {
  "id": "74568758fe5fef3727d94e7597f305_13",
  "x": "It may be that Hearst (1994<cite> Hearst ( , 1997</cite> )'s segmentation criterion, i.e. depth scores, do not translate well to dialogue. Perhaps the assignment of segment boundaries based on the relative difference between a candidate score and its surrounding peaks is highly sensitive to cohesion gaps created by conversational implicatures. On the other hand the differences between these two methods may be entirely attributable to the amount of training they received.",
  "y": "background"
 },
 {
  "id": "74e7b114ae968e196ea87f529f5eff_0",
  "x": "Distinguishing hypernyms (e.g. dog-animal) from cohyponyms (e.g. dog-cat) and, in turn, discriminating them from random words (e.g. dog-fruit) is a fundamental task in Natural Language Processing (NLP). Hypernymy in fact represents a key organization principle of semantic memory (Murphy, 2002) , the backbone of taxonomies and ontologies, and one of the crucial inferences supporting lexical entailment (Geffet and Dagan, 2005) . Cohyponymy (or coordination), on the other hand, is the relation held by words sharing a close hypernym, which are therefore attributionally similar <cite>(Weeds et al., 2014)</cite> .",
  "y": "background"
 },
 {
  "id": "74e7b114ae968e196ea87f529f5eff_1",
  "x": "The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphrasing, textual entailment, sentiment analysis and so on <cite>(Weeds et al., 2014)</cite> . For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results <cite>(Weeds et al., 2014</cite>; Rimmel, 2014; Geffet and Dagan, 2005) . Both supervised and unsupervised approaches have been investigated.",
  "y": "background"
 },
 {
  "id": "74e7b114ae968e196ea87f529f5eff_2",
  "x": "The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphrasing, textual entailment, sentiment analysis and so on <cite>(Weeds et al., 2014)</cite> . For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results <cite>(Weeds et al., 2014</cite>; Rimmel, 2014; Geffet and Dagan, 2005) . Both supervised and unsupervised approaches have been investigated.",
  "y": "background"
 },
 {
  "id": "74e7b114ae968e196ea87f529f5eff_3",
  "x": "For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results <cite>(Weeds et al., 2014</cite>; Rimmel, 2014; Geffet and Dagan, 2005) . Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in <cite>Weeds et al. (2014)</cite> , even though Levy et al. (2015) have recently claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x.",
  "y": "background"
 },
 {
  "id": "74e7b114ae968e196ea87f529f5eff_4",
  "x": "In this paper, we propose a supervised method, based on a Random Forest algorithm and 13 corpus-based features. In our evaluation, carried out using the 10-fold cross validation on 9,600 pairs, we achieved an accuracy of 88.3% when the three classes are present, and of 92.3% and 97.3% when only two classes are present. Such results are competitive with the state-of-the-art <cite>(Weeds et al., 2014)</cite> .",
  "y": "uses differences"
 },
 {
  "id": "74e7b114ae968e196ea87f529f5eff_5",
  "x": "In this paper, we have described ROOT13, a classifier for hypernyms, co-hyponyms and random words. The classifier, based on the Random Forest algorithm, uses only 13 unsupervised corpus-based features, which have been described and their contribution reported. Our classifier is competitive with the state-of-the-art <cite>(Weeds et al., 2014)</cite> .",
  "y": "uses differences"
 },
 {
  "id": "7516b533aafa8b41e7e554fa54e39c_0",
  "x": "In the last a couple of years the NLU field has been revolutionized with the advent of models based on the Transformer architecture, which are pretrained on massive amounts of unsupervised data and then fine-tuned for various supervised learning NLU tasks. Transformer models have come to dominate a wide variety of leader-boards in the NLU field; in the area of MRC, the current state-of-the-art model on the DREAM dataset (see<cite> [Sun et al., 2019]</cite> ) fine tunes Albert, a large pretrained Transformer-based model, and additionally combines it with an extra layer of multi-head attention between context and question-answer [Zhu et al., 2020] . The purpose of this note is to document a new state-of-the-art result in the DREAM task, which is accomplished by, additionally, performing multi-task learning on two MRC multi-choice reading comprehension tasks (RACE and DREAM).",
  "y": "background"
 },
 {
  "id": "7516b533aafa8b41e7e554fa54e39c_1",
  "x": "The context could be in the form of text passages, or in the form of dialogues. The questions could be open-formed (e.g. HotPotQA [Yang et al., 2018] ), asking the system to either extract the answers as spans from the context or external knowledge, or abstract and summarize the answers; the questions could also be in the form of asking the system to choose the best answer from multiple choices. In this note we will focus on the multi-choice MRC tasks, more specifically, the DREAM task <cite>[Sun et al., 2019</cite> ].",
  "y": "uses"
 },
 {
  "id": "7516b533aafa8b41e7e554fa54e39c_2",
  "x": "DREAM<cite> [Sun et al., 2019]</cite> is a much smaller reading comprehension dataset with more than 6,000 dialogues and over 10,000 questions. The average dialogue length is 86 words. Each question provides 3 answer options to choose from.",
  "y": "background"
 },
 {
  "id": "7516b533aafa8b41e7e554fa54e39c_3",
  "x": "**RELATED WORK** Early works on the DREAM task include feature-based GBDT<cite> [Sun et al., 2019]</cite> , and FTLM [Radford et al., 2018] which is based on the Transformer [Vaswani et al., 2017] architecture. The top system accuracy on the DREAM leaderboard has been advanced gradually to above 90 percent, since the break-through of the text encoder in the form of large pretrained Transformerbased models (BERT [Devlin et al., 2019] , XLNet [Yang et al., 2019a] , RoBERTa [Liu et al., 2019] , Albert [Lan et al., 2020] ).",
  "y": "background"
 },
 {
  "id": "754ceac25ff3a711ec3737e7eb860b_0",
  "x": "**OBJECTIVE** Following <cite>Klementiev et al. (2012)</cite> we split our objective into two sub-objectives, a bilingual objective minimizing the transfer errors and a monolingual objective minimizing the monolingual errors for l 1 and l 2 . We formalize the loss over the whole training set as",
  "y": "uses"
 },
 {
  "id": "754ceac25ff3a711ec3737e7eb860b_1",
  "x": "Crosslingual representations are induced to represent words, phrases, or documents for more than one language, where the representations are constrained to preserve representational similarity or can be transformed between languages<cite> (Klementiev et al., 2012</cite>; Hermann & Blunsom, 2014) . In particular, crosslingual representations can be helpful for tasks such as translation or to leverage training data in a source language when little or no training data is available for a target language. Examples of such transfer learning tasks are crosslingual sentiment analysis (Wan, 2009) and crosslingual document classification<cite> (Klementiev et al., 2012)</cite> .",
  "y": "background"
 },
 {
  "id": "754ceac25ff3a711ec3737e7eb860b_2",
  "x": "Examples of such transfer learning tasks are crosslingual sentiment analysis (Wan, 2009) and crosslingual document classification<cite> (Klementiev et al., 2012)</cite> . induced language-specific word representations, learned a linear mapping between the language-specific representations using bilingual word pairs and evaluated their approach for single word translation. <cite>Klementiev et al. (2012)</cite> used automatically aligned sentences and words to constrain word representations across languages based on the number of times a given word in one language was aligned to a word in another language.",
  "y": "background"
 },
 {
  "id": "754ceac25ff3a711ec3737e7eb860b_3",
  "x": "induced language-specific word representations, learned a linear mapping between the language-specific representations using bilingual word pairs and evaluated their approach for single word translation. <cite>Klementiev et al. (2012)</cite> used automatically aligned sentences and words to constrain word representations across languages based on the number of times a given word in one language was aligned to a word in another language. They also introduced a dataset for crosslingual document classification and evaluated their work on this task.",
  "y": "background"
 },
 {
  "id": "754ceac25ff3a711ec3737e7eb860b_4",
  "x": "However, these previous methods all suffer from one or more of three short-comings. <cite>Klementiev et al. (2012)</cite> ; ; Gouws et al. (2014) all learn their representations using a word-level monolingual objective. This effectively means that compositionality is not encouraged by the monolingual objective, which may be problematic when composing word representations for a phrase or document-level task.",
  "y": "motivation"
 },
 {
  "id": "754ceac25ff3a711ec3737e7eb860b_5",
  "x": "**CREATING REPRESENTATIONS FOR PHRASES AND DOCUMENTS** Following the work of <cite>Klementiev et al. (2012)</cite> ; Hermann & Blunsom (2014) ; Gouws et al. (2014) we represent each word as a vector and use separate word representations for each language. Like Hermann & Blunsom (2014) , we look up the vector representations for all words of a given sentence in the corresponding lookup table and apply a composition function to transform these word vectors into a sentence representation.",
  "y": "similarities"
 },
 {
  "id": "754ceac25ff3a711ec3737e7eb860b_6",
  "x": "The choice of the monolingual objective greatly influences the generality of models for crosslingual word representations. <cite>Klementiev et al. (2012)</cite> use a neural language model to leverage monolingual data. However, this does not explicitly encourage compositionality of the word representations.",
  "y": "motivation background"
 },
 {
  "id": "754ceac25ff3a711ec3737e7eb860b_7",
  "x": "However, their approach can only be trained using sentence aligned data, which makes it difficult to extend to leverage unannotated monolingual data. Gouws et al. (2014) introduced BilBOWA combining a bilingual objective with the Skip-Gram model proposed by which predicts the context of a word given the word itself. They achieve high accuracy on the German \u2192 English sub-task of the crosslingual document classification task introduced by <cite>Klementiev et al. (2012)</cite> .",
  "y": "background"
 },
 {
  "id": "754ceac25ff3a711ec3737e7eb860b_8",
  "x": "The crosslingual word and document representations induced using the approach proposed in this work present an intuitive way to tackle crosslingual document classification. Like previous work, we evaluate our method on the crosslingual document classification task introduced by <cite>Klementiev et al. (2012)</cite> . The goal is to correctly classify news articles taken from the English and German sections of the RCV1 and RCV2 corpus (Lewis et al., 2004) into one of four (Collins, 2002) for 10 iterations on representations of documents in one language (English/German) and evaluate its performance on representations of documents in the corresponding other language (German/English).",
  "y": "uses background"
 },
 {
  "id": "754ceac25ff3a711ec3737e7eb860b_9",
  "x": "We use the original data and the original implementation of the averaged perceptron used by <cite>Klementiev et al. (2012)</cite> to evaluate the document representations created by our method. There are different versions of the training set of varying sizes, ranging from 100 to 10,000 documents, and the test sets for both languages contain 5,000 documents. Most related work only reports results using the 1,000 documents sized training set. Following previous work, we tune the hyperparameters of our model on held out documents in the same language that the model was trained on.",
  "y": "uses"
 },
 {
  "id": "754ceac25ff3a711ec3737e7eb860b_10",
  "x": "In addition, our model allows the representations to draw upon monolingual data from either or both languages. Like <cite>Klementiev et al. (2012)</cite> we choose EuroParl v7 (Koehn, 2005) as our bilingual corpus and leverage the English and German parts of the RCV1 and RCV2 corpora as monolingual resources. To avoid a testing bias, we exclude all documents that are part of the crosslingual classification task.",
  "y": "similarities uses"
 },
 {
  "id": "754ceac25ff3a711ec3737e7eb860b_11",
  "x": "Table 2 shows results for all these configurations. The result table includes previous work as well as the Glossed, the machine translation and the majority class baselines from <cite>Klementiev et al. (2012)</cite> . Our method achieves results that are comparable or improve upon the previous state of the art for all dataset configurations.",
  "y": "background"
 },
 {
  "id": "754ceac25ff3a711ec3737e7eb860b_12",
  "x": "We hypothesize that the key cause of this effect is domain adaptation. From this observation it is also worth pointing out that our method is on par with the previous state of the art for the DE \u2192 EN sub-task using no monolingual training data and would improve upon it using as little as 5% of the monolingual data. To show that our method achieves high accuracy even with a reduced vocabulary, we discard representations for infrequent terms and report results using our best setup with the same vocabulary size as <cite>Klementiev et al. (2012)</cite> .",
  "y": "similarities"
 },
 {
  "id": "754ceac25ff3a711ec3737e7eb860b_13",
  "x": "The method is agnostic to the choice of composition function, enabling more complex (e.g. preserving word order information) ways to compose phrase representations from word representations. Depending on the amount of training data available the accuracy achieved with our models is comparable or greatly improves upon previously reported results for the crosslingual document classification task introduced by <cite>Klementiev et al. (2012)</cite> . To increase the expressiveness of our method we plan to investigate more complex composition functions, possibly based on convolution or other ways to preserve word order information.",
  "y": "similarities"
 },
 {
  "id": "759c1c892361f62ad8f2c46e569e8a_0",
  "x": "It has been widely used in the context of dialog policy learning (Fatemi et al., 2016; Dhingra et al., 2017;<cite> Casanueva et al., 2017)</cite> . However according to a recent comparison<cite> (Casanueva et al., 2017)</cite> in the context of dialog policy learning, it performed worse than other RL methods such as Gaussian Process in many testing conditions. Recently, several advances in deep RL such as distributional RL (Bellemare et al., 2017) , dueling network architectures (Wang et al., 2016) and their combination (Hessel et al., 2018 )a Rainbow agent -have been shown to be promising for further improvements of deep RL agents in benchmark environments, e.g. Atari 2600.",
  "y": "background"
 },
 {
  "id": "759c1c892361f62ad8f2c46e569e8a_1",
  "x": "However according to a recent comparison<cite> (Casanueva et al., 2017)</cite> in the context of dialog policy learning, it performed worse than other RL methods such as Gaussian Process in many testing conditions. Recently, several advances in deep RL such as distributional RL (Bellemare et al., 2017) , dueling network architectures (Wang et al., 2016) and their combination (Hessel et al., 2018 )a Rainbow agent -have been shown to be promising for further improvements of deep RL agents in benchmark environments, e.g. Atari 2600. However, it is still unclear whether these methods could advance dialog policies.",
  "y": "background"
 },
 {
  "id": "759c1c892361f62ad8f2c46e569e8a_4",
  "x": "The ontologies used for the benchmarks in this paper together with their properties are listed in table 2. CR  3  9  268  SFR  6  11  636  LAP  11  21  257   Table 2 : Benchmark domains with #slots the user can provide or #request from the system as well as #values of each requestable slot<cite> (Casanueva et al., 2017)</cite> . <cite>Casanueva et al. (2017)</cite> propose six different environmental models, varying in user friendliness, simulated input channel noise and the presence or absence of action masks, which, when enabled, simplify learning by masking some of the possible actions.",
  "y": "background"
 },
 {
  "id": "759c1c892361f62ad8f2c46e569e8a_5",
  "x": "<cite>Casanueva et al. (2017)</cite> propose six different environmental models, varying in user friendliness, simulated input channel noise and the presence or absence of action masks, which, when enabled, simplify learning by masking some of the possible actions. An overview of all these environmental configurations and their assignment to tasks is given in Table 1 . Evaluation results in <cite>Casanueva et al. (2017)</cite> with several dialog policy types, e.g. a handcrafted policy and the best reported policies serve as baselines in our experiments.",
  "y": "uses"
 },
 {
  "id": "759c1c892361f62ad8f2c46e569e8a_6",
  "x": "Training and evaluation with the PyDial user simulator follows the PyDial benchmarking tasks<cite> (Casanueva et al., 2017)</cite> , where each task (see Table 1) is trained on 10000 dialogs split into ten training iterations of 1000 dialogs each. We evaluate policies after each training iteration on 1000 test dialogs. All of the following results were obtained by averaging over the outcome of ten different random seeds using the parameters described in appendix A.",
  "y": "uses"
 },
 {
  "id": "759c1c892361f62ad8f2c46e569e8a_7",
  "x": "The first row of Table 3 and 4 show the results of the highest scoring policy from the PyDial benchmark<cite> (Casanueva et al., 2017)</cite> to serve as baselines. Evaluations of the handcrafted policies follow in the last line. The results show that Rainbow agent outperforms reward of the best PyDial agents in all 18 conditions and success rate in 16 out of 18 setting.",
  "y": "uses"
 },
 {
  "id": "759c1c892361f62ad8f2c46e569e8a_8",
  "x": "Following the PyDial benchmarking process, we leave all hyperparameters constant across all environments and dialog domains<cite> (Casanueva et al., 2017)</cite> , thus also evaluating the generalization capabilities of the agents. ---------------------------------- **B EXAMPLE DIALOGS**",
  "y": "uses"
 },
 {
  "id": "7616f6f8c1c188b32cd3a8374b61dd_0",
  "x": "In this paper we present the development process of NLP-QT, a question treebank that will be used for data-driven parsing in the context of a domain-specific QA system for querying NLP resource metadata. We motivate the need to build NLP-QT as a resource in its own right, by comparing the Penn Treebank-style annotation scheme used for QuestionBank (Judge et al., 2006) with the modified NP annotation for the Penn Treebank introduced by <cite>Vadas and Curran (2007)</cite> . We argue that this modified annotation scheme provides a better interface representation for semantic interpretation and show how it can be incorporated into the NLP-QT resource, without significant loss in parser performance.",
  "y": "motivation"
 },
 {
  "id": "7616f6f8c1c188b32cd3a8374b61dd_1",
  "x": "However, for reasons explained in more detail in sections 2 and 3, we will adopt annotation guidelines for questions that differ from the Penn Treebank-style annotation used in QuestionBank. Rather, we will follow a more hierarchical annotation style for NPs that has been proposed by <cite>Vadas and Curran (2007)</cite> and that provides an easier interface for semantic interpretation. Section 3 will introduce the <cite>Vadas and Curran (2007)</cite> annotation style and will motivate why it is appropriate for the QA system envisaged here.",
  "y": "uses similarities"
 },
 {
  "id": "7616f6f8c1c188b32cd3a8374b61dd_2",
  "x": "Section 3 will introduce the <cite>Vadas and Curran (2007)</cite> annotation style and will motivate why it is appropriate for the QA system envisaged here. Section 4 will present a set of parsing experiments for the Berkeley parser trained on different combinations of treebank data discussed in sections 2 and 3. The final section summarizes the main results of this paper and discusses directions for future research.",
  "y": "similarities"
 },
 {
  "id": "7616f6f8c1c188b32cd3a8374b61dd_3",
  "x": "It is precisely this type of shortcoming that led <cite>Vadas and Curran (2007)</cite> to revise the Penn Treebank annotation style for NPs along the following lines: \u2022 If the intended scope of a base NP leads to a strictly right-branching structure, then the Penn Treebank annotation remains unchanged. \u2022 If the intended scope is partially or completely left-branching, then an extra node is introduced into the tree for each leftbranching structure.",
  "y": "background"
 },
 {
  "id": "7616f6f8c1c188b32cd3a8374b61dd_4",
  "x": "The resulting annotation for the compound noun second language acquisition materials is shown in the right column of Figure 1 . From the point of view of semantic interpretation, the more contoured <cite>Vadas and Curran (2007)</cite> annotation style is to be preferred since it reflects the type of answer that is required, namely materials for second language acquisition, but not for example acquisition materials for second language, or the second (batch) of language acquisition materials. It is precisely for this reason that we adopt the annotation style of <cite>Vadas and Curran (2007)</cite> for the NLP Resource Metadata Questions Treebank (henceforth abbreviated as NLP-QT).",
  "y": "background"
 },
 {
  "id": "7616f6f8c1c188b32cd3a8374b61dd_5",
  "x": "The resulting annotation for the compound noun second language acquisition materials is shown in the right column of Figure 1 . From the point of view of semantic interpretation, the more contoured <cite>Vadas and Curran (2007)</cite> annotation style is to be preferred since it reflects the type of answer that is required, namely materials for second language acquisition, but not for example acquisition materials for second language, or the second (batch) of language acquisition materials. It is precisely for this reason that we adopt the annotation style of <cite>Vadas and Curran (2007)</cite> for the NLP Resource Metadata Questions Treebank (henceforth abbreviated as NLP-QT).",
  "y": "similarities uses"
 },
 {
  "id": "7616f6f8c1c188b32cd3a8374b61dd_6",
  "x": "**EXPERIMENTAL RESULTS** This section summarizes the set of experiments that we have conducted with the <cite>Vadas and Curran (2007)</cite> annotation style for NPs and in particular with the NLP-QT data set. We discuss two types of experiments:",
  "y": "similarities"
 },
 {
  "id": "7616f6f8c1c188b32cd3a8374b61dd_7",
  "x": "Using Bikel (2004)'s parser, <cite>Vadas and Curran (2007)</cite> report that the parsing results slightly decrease when the parser is trained on the Penn Treebank with the modified annotation style for NPs. As Table 2 shows, we obtain a similar result when testing on section 23 of the Penn Treebank, using the Berkeley parser trained on sections 02-21 of the same treebank: there is minor drop in F-score from 90.43 to 89.96. We also confirm Gildea's finding that testing a parser on test sets from a different domain than the training sets results in a significant loss of performance: when using the same models that we used for the Penn Treebank experiments, the average F-score for test data from the Question Bank in a 10-fold cross-validation experiment is 79.944 for the model trained on the original Penn Treebank and 77.607 for the model trained on the modified Penn Treebank.",
  "y": "background"
 },
 {
  "id": "7616f6f8c1c188b32cd3a8374b61dd_8",
  "x": "But since our primary interest is in parsing questions as accurately as possible, we conducted a second set of experiments, summarized in the lower half of Table 2. Here additional training data from the Question Bank was added to both the original and the modified Penn Treebank training data. The decrease in performance caused by adding the QuestionBank training data together with the modified NP annotation on section 23 is comparable to the one caused by adding the modified NP annotation alone (a decrease from 90.263 to 90.04, whereas for the original Penn Treebank data the F-score decreased from 90.43 to 89.96), but this slight decrease is more than offset by the increase in semantic information obtained from the <cite>Vadas and Curran (2007)</cite> annotation for complex base NPs.",
  "y": "differences"
 },
 {
  "id": "7616f6f8c1c188b32cd3a8374b61dd_9",
  "x": "To this end, we performed a set of parsing experiments, again using the Berkeley parser, where the test data are taken both from the QuestionBank and a seed set of 500 manually annotated questions from the NLP-QT. The results are shown in Table 3 . As in the experiments shown in the previous subsection, the performance with a model trained purely on Penn Treebank data (with NPs annotated in the <cite>Vadas and Curran (2007)</cite> style) serves as a baseline (the model is called np-wsj in the table).",
  "y": "similarities"
 },
 {
  "id": "7616f6f8c1c188b32cd3a8374b61dd_10",
  "x": "**CONCLUSION AND FUTURE WORK** In this paper we have presented the development process of the NLP-QT resource that will be used for data-driven parsing in the context of a domainspecific QA system for querying NLP resource metadata. We have motivated the need to build NLP-QT as a resource in its own right by comparing the Penn Treebank-style annotation scheme used for QuestionBank with the modified NP annotation for the Penn Treebank introduced by <cite>Vadas and Curran (2007)</cite> .",
  "y": "motivation"
 },
 {
  "id": "7666d5a8e05e79f68ec60e47cddecd_0",
  "x": "In (Chambers and Jurafsky, 2009 ) narrative schemas are induced from corpora using coreference relations between participants in texts. Transformation-based learning is used in (Saggion, 2011) to induce templates and rules for non-extractive summary generation. Paraphrase templates containing concepts and typical strings were induced from comparable sentences in <cite>(Barzilay and Lee, 2003)</cite> using multisentence alignment to discover \"variable\" and fixed structures.",
  "y": "background"
 },
 {
  "id": "7666d5a8e05e79f68ec60e47cddecd_1",
  "x": "Our algorithm has a reasonable computational complexity, unlike alignment-based or clustering-based approaches <cite>(Barzilay and Lee, 2003)</cite> , which are computationally expensive. ---------------------------------- **CONCLUSIONS AND OUTLOOK**",
  "y": "differences"
 },
 {
  "id": "76fd2709a325366be6154d2a84b29b_0",
  "x": "The techniques for solving this problem can be applied to a variety of NLP tasks, such as query expansion, word sense disambiguation, machine translation, information extraction and question answering. Previous work addressing the problem can be roughly classified into three categories: (1) learning word embeddings from large collections of text using variants of neural networks (Mikolov et al. (2013a) ; Mikolov et al. (2013b) ; Mikolov et al. (2013c) ;<cite> Levy and Goldberg (2014)</cite> ) or global matrix factorization (Deerwester et al. (1990) ; Turney (2012) ); (2) extracting knowledge from existing semantic networks, such as WordNet (Yang and Powers (2005) ; Alvarez and Lim (2007) ; Hughes and Ramage (2007) ) and ConceptNet (Boteanu and Chernova (2015) ); (3) combining the above two models by various ways (Agirre et al. (2009) ; Zhila et al. (2013) ; Iacobacci et al. (2015) ; Summers-Stay et al. (2016) ). The empirical evidence shows that the word representations learned from neural network models do an especially good job in capturing not only attributional similarities between words but also similarities between pairs of words (Mikolov et al. (2013c) ).",
  "y": "background"
 },
 {
  "id": "76fd2709a325366be6154d2a84b29b_1",
  "x": "Embeddings produced from different kinds of contexts can induce different word similarities. The original skip-gram embeddings can yield broad topical similarities, while the dependency-based word embeddings can capture more functional similarities <cite>(Levy et al., 2014)</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "770368eff3410f3c2ab18b14b42243_0",
  "x": "Researchers have mainly explored two types of Seq2Seq models. The first are generative models, which are usually trained with cross-entropy to generate responses word-by-word conditioned on a dialogue context [Ritter et al., 2011 , Vinyals and Le, 2015 , Sordoni et al., 2015 , Shang et al., 2015<cite> , Li et al., 2016a</cite> , Serban et al., 2016b . The second are discriminative models, which are trained to select an appropriate response from a set of candidate responses [Lowe et al., 2015 , Bordes and Weston, 2016 , Inaba and Takahashi, 2016 , Yu et al., 2016 .",
  "y": "background"
 },
 {
  "id": "770368eff3410f3c2ab18b14b42243_1",
  "x": "One weakness of current generative models is their limited ability to incorporate rich dialogue context and to generate meaningful and diverse responses [Serban et al., 2016b<cite> , Li et al., 2016a</cite> . To overcome this challenge, we propose new generative models that are better able to incorporate long-term dialogue history, to model uncertainty and ambiguity in dialogue, and to generate responses with high-level compositional structure. Our experiments demonstrate the importance of the model architecture and the related inductive biases in achieving this improved performance.",
  "y": "motivation"
 },
 {
  "id": "770368eff3410f3c2ab18b14b42243_2",
  "x": "We also present results on Twitter in the Appendix. This task has been studied extensively in the recent literature [Ritter et al., 2011 , Sordoni et al., 2015<cite> , Li et al., 2016a</cite> . Corpus: The Ubuntu Dialogue Corpus consists of about half a million dialogues extracted from the #Ubuntu Internet Relayed Chat (IRC) channel.",
  "y": "background"
 },
 {
  "id": "770368eff3410f3c2ab18b14b42243_3",
  "x": "In another line of work, researchers have started proposing alternative training and response selection criteria [Weston, 2016] . <cite>Li et al. [2016a]</cite> propose ranking candidate responses according to a mutual information criterion, in order to incorporate dialogue context efficiently and retrieve on-topic responses. Li et al. [2016b] further propose a model trained using reinforcement learning to optimize a hand-crafted reward function.",
  "y": "background"
 },
 {
  "id": "772cdd4263cf8979a54cc5196e5853_0",
  "x": "Both phrase-structure and dependency parsers have developed a lot in the last decade (Nivre et al., 2004; McDonald et al., 2005; Charniak and Johnson, 2005; <cite>Huang, 2008)</cite> . Different approaches have been proved to be effective for these two parsing tasks which has implicated a divergence between techniques used (and a growing gap between researcher communities). In this work, we exploit this divergence and show the added value of features extracted from automatic dependency parses of sentences for a discriminative phrase-structure parser.",
  "y": "background motivation"
 },
 {
  "id": "772cdd4263cf8979a54cc5196e5853_1",
  "x": "The most successful supervised phrase-structure parsers are feature-rich discriminative parsers which heavily depend on an underlying PCFG (Charniak and Johnson, 2005; <cite>Huang, 2008)</cite> . These approaches consists of two stages. At the first stage they apply a PCFG to extract possible parses. The full set of possible parses cannot be iterated through in practice, and is usually pruned as a consequence. The n-best list parsers keep just the 50-100 best parses according to the PCFG. Other methods remove nodes and hyperedges whose posterior probability is under a predefined threshold from the forest (chart). The task of the second stage is to select the best parse from the set of possible parses (i.e. rerank this set).",
  "y": "background"
 },
 {
  "id": "772cdd4263cf8979a54cc5196e5853_2",
  "x": "Our oracle extraction method is an extension of<cite> Huang (2008)</cite>'s dynamic programing procedure which takes into consideration POS tag and grammatical function matches as well and selects hyperedges with higher posterior probability for tie-breaking. For a detailed description of the training and supporting algorithms please refer to Charniak and Johnson (2005) and<cite> Huang (2008)</cite> . ----------------------------------",
  "y": "extends"
 },
 {
  "id": "772cdd4263cf8979a54cc5196e5853_3",
  "x": "Learning is guided by the so-called oracle parse which is the full parse in the set of possible parses most similar to the gold standard tree. Our oracle extraction method is an extension of<cite> Huang (2008)</cite>'s dynamic programing procedure which takes into consideration POS tag and grammatical function matches as well and selects hyperedges with higher posterior probability for tie-breaking. For a detailed description of the training and supporting algorithms please refer to Charniak and Johnson (2005) and<cite> Huang (2008)</cite> .",
  "y": "background"
 },
 {
  "id": "772cdd4263cf8979a54cc5196e5853_4",
  "x": "Regarding the two discriminative approaches, our findings are similar to<cite> Huang (2008)</cite> , i.e. the packed forest-based and n-best list procedures achieved similar results by using only local features. We found that the improvements by applying the dependency features are similar at the two evaluation metrics (with and without grammatical functions). ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "7895613ddd09696bbee4143c4359b0_0",
  "x": "Recent work has shown that we can learn better visually grounded representations of sentences by training image-sentence ranking models on multiple languages (Gella et al., 2017; <cite>K\u00e1d\u00e1r et al., 2018)</cite> . This line of research has focused on training models on datasets where the same images are annotated with sentences in multiple languages. In this paper, we consider the problem of training an image-sentence ranking model using imagecaption collections in different languages with nonoverlapping images drawn from different sources.",
  "y": "background motivation"
 },
 {
  "id": "7895613ddd09696bbee4143c4359b0_1",
  "x": [
   "K\u00e1d\u00e1r et al. (2018) claim that a multilingual image-sentence ranking model trained on disjoint datasets performs on-par with a model trained on aligned data. However, the disjoint datasets in their paper are artificial because they were formed by randomly splitting the Multi30K dataset into two halves. We examine whether the ranking model can benefit from multilingual supervision when it is trained using disjoint datasets drawn from different sources."
  ],
  "y": "background motivation"
 },
 {
  "id": "7895613ddd09696bbee4143c4359b0_2",
  "x": "We adopt the model architecture and training procedure of<cite> K\u00e1d\u00e1r et al. (2018)</cite> for the task of matching images with sentences. This task is defined as learning to rank the sentences associated with an image higher than other sentences in the data set, and vice-versa (Hodosh et al., 2013) . The model is comprised of a recurrent neural network language model and a convolutional neural network image encoder. The parameters of the language encoder are randomly initialized, while the image encoder is pre-trained, frozen during training and followed by a linear layer which is tuned for the task. The model is trained to make true pairs < a, b > similar to each other, and contrastive pairs <\u00e2, b > and < a,b > dissimilar from each other in a joint embedding space by minimizing the max-violation loss function (Faghri et al., 2017) :",
  "y": "uses background"
 },
 {
  "id": "7895613ddd09696bbee4143c4359b0_3",
  "x": "The model is trained to make true pairs < a, b > similar to each other, and contrastive pairs <\u00e2, b > and < a,b > dissimilar from each other in a joint embedding space by minimizing the max-violation loss function (Faghri et al., 2017) : In our experiments, the < a, b > pairs are either image-caption pairs < i, c > or caption-caption pairs < c a , c b > (following Gella et al. (2017) ;<cite> K\u00e1d\u00e1r et al. (2018)</cite> ). When we train on < i, c > pairs, we sample a batch from an image-caption data set with uniform probability, encode the images and the sentences, and perform an update of the model parameters.",
  "y": "background"
 },
 {
  "id": "7895613ddd09696bbee4143c4359b0_4",
  "x": "For the caption-caption objective, we follow<cite> K\u00e1d\u00e1r et al. (2018)</cite> and generate a sentence pair data set by taking all pairs of sentences that belong to the same image and are written in different languages: 5 English and 5 German captions result in 25 English-German pairs. The sentences are encoded and we perform an update of the model parameters using the same loss. When training with both the image-caption and caption-caption (c2c) ranking tasks, we randomly select the task to perform with probability p=0.5.",
  "y": "uses"
 },
 {
  "id": "7895613ddd09696bbee4143c4359b0_5",
  "x": "---------------------------------- **MODEL** Our implementation, training protocol and parameter settings are based on the existing codebase of<cite> K\u00e1d\u00e1r et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "7895613ddd09696bbee4143c4359b0_6",
  "x": "Tables 1 and 2 present the result for English and German, respectively. The Sum-of-recall scores for both languages show that the best approach is the bilingual model with the c2c loss (En+De+c2c, and De+En+c2c). These results reproduce the findings of<cite> K\u00e1d\u00e1r et al. (2018)</cite> .",
  "y": "similarities"
 },
 {
  "id": "7895613ddd09696bbee4143c4359b0_7",
  "x": "We find that a model trained in the aligned setting (De+En) is better than a model trained in the disjoint setting (De+COCO), as shown in Table  2 . This finding contradicts the conclusion of<cite> K\u00e1d\u00e1r et al. (2018)</cite> , who claimed that the aligned and disjoint conditions lead to comparable performance. This is most likely because the disjoint setting in<cite> K\u00e1d\u00e1r et al. (2018)</cite> is artificial, in the sense that they used different 50% subsets of M30K. In our experiments the disjoint image-caption sets are real, in the sense that we trained the models on the two different datasets.",
  "y": "differences background"
 },
 {
  "id": "7895613ddd09696bbee4143c4359b0_8",
  "x": "Summary First we reproduced the findings of<cite> K\u00e1d\u00e1r et al. (2018)</cite> showing that bilingual joint training improves over monolingual and using c2c loss further improves performance. Furthermore, we have found that adding the COCO as additional training data both when only training on German, and training on both German-English from M30K improves performance even if the model is trained on data drawn from a different dataset. ----------------------------------",
  "y": "differences background"
 },
 {
  "id": "7895613ddd09696bbee4143c4359b0_9",
  "x": "More recently, there has been a focus on solving this task using multilingual data (Gella et al., 2017; <cite>K\u00e1d\u00e1r et al., 2018)</cite> in the Multi30K dataset ; an extension of the popular Flickr30K dataset into German, French, and Czech. These works take a multi-view learning perspective in which images and their descriptions in multiple languages are different views of the same concepts. The assumption is that common representations of multiple languages and perceptual stimuli can potentially exploit complementary information between views to learn better representations.",
  "y": "background"
 },
 {
  "id": "7895613ddd09696bbee4143c4359b0_10",
  "x": "Their results were improved by the approach presented in<cite> K\u00e1d\u00e1r et al. (2018)</cite> , who has also shown that the multilingual models outperform bilingual models, and that image-caption retrieval performance in languages with less resources can be improved with data from higher-resource languages. We largely follow<cite> K\u00e1d\u00e1r et al. (2018)</cite> , however, our main interest lies in learning multimodal and bilingual representations in the scenario where the images do not come from the same data set i.e.: the data is presented is two sets of image-caption tuples rather than image-caption-caption triples. Taking a broader perspective, images have been used as pivots in multilingual multimodal language processing.",
  "y": "background"
 },
 {
  "id": "7895613ddd09696bbee4143c4359b0_11",
  "x": "We largely follow<cite> K\u00e1d\u00e1r et al. (2018)</cite> , however, our main interest lies in learning multimodal and bilingual representations in the scenario where the images do not come from the same data set i.e.: the data is presented is two sets of image-caption tuples rather than image-caption-caption triples. Taking a broader perspective, images have been used as pivots in multilingual multimodal language processing. On the word level this intuition is applied to visually grounded bilingual lexicon induction, which aims to learn cross-lingual word representations without aligned text using images as pivots (Bergsma and Van Durme, 2011; Kiela et al., 2015; Vuli\u0107 et al., 2016; Hartmann and S\u00f8gaard, 2017; Hewitt et al., 2018) . Images have been used as pivots to learn translation models only from image-caption data sets, without parallel text (Hitschler et al., 2016; Nakayama and Nishida, 2017; Lee et al., 2017; Chen et al., 2018) .",
  "y": "differences background"
 },
 {
  "id": "7895613ddd09696bbee4143c4359b0_12",
  "x": "**CONCLUSIONS** Previous work has demonstrated improved imagesentence ranking performance when training models jointly on multiple languages (Gella et al., 2017; <cite>K\u00e1d\u00e1r et al., 2018)</cite> . Here we presented a study on learning multimodal and multilingual representations in the disjoint setting, where images between languages do not overlap.",
  "y": "background"
 },
 {
  "id": "78a7ca27c5ca032116db12205af939_0",
  "x": "Wang et al. [17] address both disease classification and medical image report generation problems in the same model. They introduce a novel Text-Image Embedding network (TieNet), which integrates self-attention LSTM using tex- tual report data and visual attention CNN using image data. TieNet is capable of extracting an informative embedding to represent the paired medical image and report, which significantly improves the disease classification performance compared to <cite>[16]</cite> .",
  "y": "background motivation"
 },
 {
  "id": "78a7ca27c5ca032116db12205af939_1",
  "x": "Our contributions are in four-fold: (1) we describe an integrated image interpretation framework for disease annotation and medical report generation, (2) we transfer knowledge from large image data sets (ChestX-ray8 <cite>[16]</cite> and ImageNet) to enhance medical image interpretation using a small number of reports for training (IU X-ray [2] ), (3) we evaluate suitability of the NLP evaluation metrics for medical report generation, and (4) we demonstrate the functionality of localizing the key finding in an X-ray with a heatmap. ---------------------------------- **METHOD**",
  "y": "uses background"
 },
 {
  "id": "78a7ca27c5ca032116db12205af939_2",
  "x": "Similar to <cite>[16]</cite> , we apply a thresholding based bounding box (B-Box) generation method. The B-Box bounds pixels whose heatmap intensity is above 90% of the maximum intensity. The resulting region of interest is then cropped for next level modeling. Fig. 2b illustrates the process of report generation. If there is no active thoracic disease found in an X-ray, a report will be directly generated by an attentive LSTM based on the original X-ray as shown in the green dashed box. Otherwise (as shown in the red dashed box), the cropped subimage with localized disease from the classification module (Fig. 2a) is used to generate description of abnormalities whereas the original X-ray is used to generate description of normalities in the report.",
  "y": "uses"
 },
 {
  "id": "78a7ca27c5ca032116db12205af939_3",
  "x": "The MeSH terms are used as labels for disease classification [17] as well as the follow-up report generation with abnormality and normality descriptions. We convert all the words to lower-case, remove all non-alphanumeric tokens, replace single-occurrence tokens with a special token and use another special token to separate sentences. We filter out images and reports that are non-relevant to the eight common thoracic diseases included in both ChestX-ray8 <cite>[16]</cite> and IU X-ray datasets [2] , resulting in a dataset with 2225 pairs of X-ray image and report.",
  "y": "uses"
 },
 {
  "id": "7936967a70c44890f3a61f6625c59d_0",
  "x": "Efforts in agent-human negotiations involving free-form natural language as a means of communication are rather sparse. Researchers (He et al., 2018) recently studied natural language negotiations in buyer-seller bargaining setup, which is comparatively less restricted than previously studied game environments (Asher et al., 2016;<cite> Lewis et al., 2017)</cite> . Lack of a well-defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario.",
  "y": "background"
 },
 {
  "id": "7936967a70c44890f3a61f6625c59d_1",
  "x": "Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by He et al. (2018) . Instead of focusing on the previously studied game environments (Asher et al., 2016;<cite> Lewis et al., 2017)</cite> , the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist 1 . The dataset consists of 6682 dialogues between a buyer and a seller who converse in natural language to negotiate the price of a given product (sample in Table 1 ).",
  "y": "differences"
 },
 {
  "id": "7936967a70c44890f3a61f6625c59d_2",
  "x": "This can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning <cite>(Lewis et al., 2017</cite>; He et al., 2018) . ---------------------------------- **CONCLUSION**",
  "y": "uses"
 },
 {
  "id": "7936967a70c44890f3a61f6625c59d_3",
  "x": "With the capability to incorporate cues from natural language, such a framework can be used in the future to get negotiation feedback, in order to guide the planning of a negotiating agent. This can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning <cite>(Lewis et al., 2017</cite>; He et al., 2018) . ----------------------------------",
  "y": "future_work"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_0",
  "x": "Deep energy-based models are powerful, but pose challenges for learning and inference (Belanger and McCallum, 2016) . <cite>Tu and Gimpel (2018)</cite> developed an efficient framework for energy-based models by training \"inference networks\" to approximate structured inference instead of using gradient descent. However, <cite>their alternating optimization approach</cite> suffers from instabilities during training, requiring additional loss terms and careful hyperparameter tuning.",
  "y": "motivation"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_1",
  "x": "To stabilize training, <cite>Tu and Gimpel (2018)</cite> experimented with several additional terms in the training objectives, finding performance to be dependent on their inclusion. Also, when using the approach of <cite>Tu and Gimpel (2018)</cite> , there is a mismatch between the training and test-time uses of the trained inference network. During training with hinge loss, the inference network is actually trained to do \"costaugmented\" inference.",
  "y": "background"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_2",
  "x": "Also, when using the approach of <cite>Tu and Gimpel (2018)</cite> , there is a mismatch between the training and test-time uses of the trained inference network. During training with hinge loss, the inference network is actually trained to do \"costaugmented\" inference. However, at test time, the goal is to simply minimize the energy without any cost term.",
  "y": "background"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_3",
  "x": "During training with hinge loss, the inference network is actually trained to do \"costaugmented\" inference. However, at test time, the goal is to simply minimize the energy without any cost term. <cite>Tu and Gimpel (2018)</cite> fine-tuned the cost-augmented network to match the test-time criterion, but found only minimal change from this fine-tuning.",
  "y": "background motivation"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_4",
  "x": "While SPENs have been used for multiple NLP tasks, including multi-label classification (Belanger and McCallum, 2016) , part-of-speech tagging <cite>(Tu and Gimpel, 2018)</cite> , and semantic role labeling (Belanger et al., 2017) , they are not widely used in NLP. Structured prediction is extremely common in NLP, but is typically approached using methods that are more limited than SPENs (such as conditional random fields) or models that suffer from a train/test mismatch (such as most auto-regressive models). SPENs offer a maximally expressive framework for structured prediction while avoiding the train/test mismatch and therefore offer great potential for NLP.",
  "y": "background"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_5",
  "x": "<cite>Tu and Gimpel (2018)</cite> propose an alternative that replaces gradient descent with a neural network trained to do inference, i.e., to mimic the function performed in equation (1). This \"inference network\" A \u03a8 : X \u2192 Y R is parameterized by \u03a8 and trained with the goal that When training the energy function parameters \u0398, <cite>Tu and Gimpel (2018)</cite> replaced the cost-augmented inference step in the structured hinge loss from Belanger and McCallum (2016) with a costaugmented inference network F \u03a6 and trained the energy function and inference network parameters jointly:",
  "y": "background"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_6",
  "x": "This \"inference network\" A \u03a8 : X \u2192 Y R is parameterized by \u03a8 and trained with the goal that When training the energy function parameters \u0398, <cite>Tu and Gimpel (2018)</cite> replaced the cost-augmented inference step in the structured hinge loss from Belanger and McCallum (2016) with a costaugmented inference network F \u03a6 and trained the energy function and inference network parameters jointly: where D is the set of training pairs, [h] + = max(0, h), and is a structured cost function that computes the distance between its two arguments.",
  "y": "background"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_7",
  "x": "where D is the set of training pairs, [h] + = max(0, h), and is a structured cost function that computes the distance between its two arguments. <cite>Tu and Gimpel (2018)</cite> alternatively optimized \u0398 and \u03a6, which is similar to training in generative adversarial networks (Goodfellow et al., 2014) . As alternating optimization can be difficult in practice (Salimans et al., 2016) , <cite>Tu & Gimpel</cite> experimented with including several additional terms in the above objective to stabilize training.",
  "y": "background"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_8",
  "x": "where D is the set of training pairs, [h] + = max(0, h), and is a structured cost function that computes the distance between its two arguments. <cite>Tu and Gimpel (2018)</cite> alternatively optimized \u0398 and \u03a6, which is similar to training in generative adversarial networks (Goodfellow et al., 2014) . As alternating optimization can be difficult in practice (Salimans et al., 2016) , <cite>Tu & Gimpel</cite> experimented with including several additional terms in the above objective to stabilize training.",
  "y": "background"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_9",
  "x": "As alternating optimization can be difficult in practice (Salimans et al., 2016) , <cite>Tu & Gimpel</cite> experimented with including several additional terms in the above objective to stabilize training. We adopt the same learning framework as <cite>Tu & Gimpel</cite> of jointly learning the energy function and inference network, but we propose a novel objective function that jointly trains a cost-augmented inference network, a test-time inference network, and the energy function. The energy functions we use for our sequence labeling tasks are taken from <cite>Tu and Gimpel (2018)</cite> and are described in detail in the appendix.",
  "y": "differences uses similarities extends"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_10",
  "x": "As alternating optimization can be difficult in practice (Salimans et al., 2016) , <cite>Tu & Gimpel</cite> experimented with including several additional terms in the above objective to stabilize training. We adopt the same learning framework as <cite>Tu & Gimpel</cite> of jointly learning the energy function and inference network, but we propose a novel objective function that jointly trains a cost-augmented inference network, a test-time inference network, and the energy function. The energy functions we use for our sequence labeling tasks are taken from <cite>Tu and Gimpel (2018)</cite> and are described in detail in the appendix.",
  "y": "uses"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_11",
  "x": "Thus, it is not well-aligned with the test-time inference problem. <cite>Tu and Gimpel (2018)</cite> used the same inference network for solving both problems, which led <cite>them</cite> to fine-tune the network at test-time with a different objective. We avoid this issue by training two inference networks, A \u03a8 for test-time inference and F \u03a6 for cost-augmented inference:",
  "y": "motivation"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_12",
  "x": "We avoid this issue by training two inference networks, A \u03a8 for test-time inference and F \u03a6 for cost-augmented inference: As indicated, this loss can be viewed as the sum of the margin-rescaled and perceptron losses for SPEN training with inference networks. We treat this optimization problem as a minmax game and find a saddle point for the game similar to <cite>Tu and Gimpel (2018)</cite> and Goodfellow et al. (2014) .",
  "y": "similarities"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_13",
  "x": "Joint Parameterizations. If we were to train independent inference networks A \u03a8 and F \u03a6 , this new objective could be much slower than the original approach of <cite>(Tu and Gimpel, 2018)</cite> . However, the compound objective offers several natural options for defining joint parameterizations of the two inference networks. We consider three options which are visualized in Figure 1 and described below:",
  "y": "differences"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_14",
  "x": "However, <cite>Tu and Gimpel (2018)</cite> found that the trained cost-augmented network was barely affected by fine-tuning for the test-time inference objective. This suggests that the cost-augmented network was mostly acting as a test-time inference network by the time of convergence. With the stacked parameterization, however, we explicitly provide the gold standard y to the cost-augmented network, permitting it to learn to change the predictions of the test-time network in appropriate ways to improve the energy function.",
  "y": "background"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_15",
  "x": "<cite>Tu and Gimpel (2018)</cite> used the following objective for the cost-augmented inference network (maximizing it with respect to \u03a6): where [h] + = max(0, h). However, there are two potential reasons why l will equal zero and therefore trigger no gradient update.",
  "y": "background"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_16",
  "x": "Then, when using zero truncation, the gradient of the inference network parameters will be 0. This is likely why <cite>Tu and Gimpel (2018)</cite> found it important to add several stabilization terms to the l 0 objective. We find that by instead removing the truncation, learning stabilizes and becomes less dependent on these additional terms.",
  "y": "differences"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_17",
  "x": "<cite>Tu and Gimpel (2018)</cite> pretrained their tag language model (TLM) on a large, automatically-tagged corpus and fixed its parameters when optimizing \u0398. We instead do not pretrain the TLM and learn its parameters when training \u0398. We also propose new global energy terms. Define y t = h(y 0 , . . . , y t\u22121 ) where h is an LSTM TLM that takes a sequence of labels as input and returns a distribution over next labels.",
  "y": "differences"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_18",
  "x": "The global energy is therefore Here \u03b3 is a hyperparameter that is tuned. We experiment with three settings for the global energy: GE(a): forward TLM as in <cite>Tu and Gimpel (2018)</cite> ; GE(b): forward and backward TLMs (\u03b3 = 0); GE(c): all four TLMs in Eq. (7).",
  "y": "similarities"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_19",
  "x": "Removing truncation makes learning succeed and leads to effective models even without using CE. However, when using the local CE term, truncation has little effect on performance. The importance of CE in prior work <cite>(Tu and Gimpel, 2018)</cite> is likely due to the fact that truncation was being used.",
  "y": "background"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_20",
  "x": "Across all tasks, the shared and stacked parameterizations are more accurate than the previous objectives. For the separated parameterization, the performance Table 3 : Top: differences in accuracy/F1 between test-time inference networks A \u03a8 and cost-augmented networks F \u03a6 (on development sets). The \"marginrescaled\" row uses a SPEN with the local CE term and without zero truncation, where A \u03a8 is obtained by finetuning F \u03a6 as done by <cite>Tu and Gimpel (2018)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_21",
  "x": "The results are shown in Table  4 . Adding the backward (b) and word-augmented TLMs (c) improves over only using the forward TLM from <cite>Tu and Gimpel (2018)</cite> . With the global energies, our performance is comparable to several strong results (cf. 90.94 of Lample et al., 2016 and 91.37 of Ma and Hovy, 2016) .",
  "y": "uses"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_22",
  "x": "In the relaxed output space Y R (x), y t,j can be interpreted as the probability of the tth position being labeled with label j. We then use the following energy for sequence labeling <cite>(Tu and Gimpel, 2018)</cite> : where U j \u2208 R d is a parameter vector for label j and the parameter matrix W \u2208 R L\u00d7L contains label pair parameters. Also, b(x, t) \u2208 R d denotes the \"input feature vector\" for position t. We define it to be the d-dimensional BiLSTM (Hochreiter and Schmidhuber, 1997) hidden vector at t. The full set of energy parameters \u0398 includes the U j vectors, W , and the parameters of the BiLSTM.",
  "y": "uses"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_23",
  "x": "Also, b(x, t) \u2208 R d denotes the \"input feature vector\" for position t. We define it to be the d-dimensional BiLSTM (Hochreiter and Schmidhuber, 1997) hidden vector at t. The full set of energy parameters \u0398 includes the U j vectors, W , and the parameters of the BiLSTM. <cite>Tu and Gimpel (2018)</cite> also added a global energy term that they referred to as a \"tag language model\" (TLM). We use h to denote an LSTM TLM that takes a sequence of labels as input and returns a distribution over next labels.",
  "y": "similarities"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_24",
  "x": "where y 0 is the start-of-sequence symbol and y T +1 is the end-of-sequence symbol. This energy returns the negative log-likelihood under the TLM of the candidate output y. For inference networks, we use architectures similar to those used by <cite>(Tu and Gimpel, 2018)</cite> .",
  "y": "similarities"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_25",
  "x": "Like <cite>Tu and Gimpel (2018)</cite> , we use a BiLSTM to compute the input feature vector for each position, using hidden dimension of size 100. We also use BiL-STMs for the inference networks. The output of the inference network is a softmax function, so the inference network will produce a distribution over labels at each position.",
  "y": "similarities uses"
 },
 {
  "id": "79b3933e51c5fd412d00829815a958_26",
  "x": [
   "We use Adam (Kingma and Ba, 2014) and do early stopping on the development set. The hyperparameter k (the number of I steps) is tuned over the set {1, 2, 5, 10, 50}. \u03b3 is tuned over the set {0, 0.5, 1}. 4 We find that Adam works better than SGD when training the inference network without the local cross entropy term. The three curves for each setting correspond to different random seeds."
  ],
  "y": "background"
 },
 {
  "id": "79e36756354f61b087655bc6afede8_0",
  "x": "BLE can be used to address the accuracy problem of SMT, which estimates comparable features for the translation pairs in the translation model <cite>(Klementiev et al., 2012)</cite> . BLE also can be used to address the coverage problem of SMT, which mines translations for the unknown words or phrases in the translation model from comparable corpora (Daume III and Jagarlamudi, 2011; Irvine et al., 2013) . Moreover, studies have been conducted to address the accuracy and coverage problems of SMT simultaneously with BLE (Irvine and Callison-Burch, 2013a) .",
  "y": "background"
 },
 {
  "id": "79e36756354f61b087655bc6afede8_1",
  "x": "BLE can be used to address the accuracy problem of SMT, which estimates comparable features for the translation pairs in the translation model <cite>(Klementiev et al., 2012)</cite> . Our study focuses on addressing the accuracy problem of SMT with BLE.",
  "y": "background motivation"
 },
 {
  "id": "79e36756354f61b087655bc6afede8_2",
  "x": "However, this method is not scalable for large data sets. In this paper, we estimate topical feature in a scalable way following <cite>(Klementiev et al., 2012)</cite> . We treat an article pair aligned by interlanguage links in Wikipedia as a topic aligned pair.",
  "y": "uses"
 },
 {
  "id": "79e36756354f61b087655bc6afede8_3",
  "x": "The intuition of temporal similarity is that news stories across languages tend to discuss the same world events on the same day, and the occurrences of a translated phrase pair over time tend to spike on the same dates (Klementiev and Roth, 2006;<cite> Klementiev et al., 2012)</cite> . We estimate temporal feature following (Klementiev and Roth, 2006;<cite> Klementiev et al., 2012)</cite> . For a phrase pair, we build source and target temporal occurrence vectors by counting their occurrences in equally sized temporal bins, which are sorted from the set of time-stamped documents in the comparable corpus.",
  "y": "background"
 },
 {
  "id": "79e36756354f61b087655bc6afede8_4",
  "x": "The intuition of temporal similarity is that news stories across languages tend to discuss the same world events on the same day, and the occurrences of a translated phrase pair over time tend to spike on the same dates (Klementiev and Roth, 2006;<cite> Klementiev et al., 2012)</cite> . We estimate temporal feature following (Klementiev and Roth, 2006;<cite> Klementiev et al., 2012)</cite> . For a phrase pair, we build source and target temporal occurrence vectors by counting their occurrences in equally sized temporal bins, which are sorted from the set of time-stamped documents in the comparable corpus.",
  "y": "uses"
 },
 {
  "id": "79e36756354f61b087655bc6afede8_5",
  "x": "In our experiments, we compared our proposed method with <cite>(Klementiev et al., 2012)</cite> . We estimated comparable features from comparable corpora using the method of <cite>(Klementiev et al., 2012)</cite> and our proposed method respectively. We appended the comparable features to the phrase table, and evaluated the two methods in the perspective of SMT performance.",
  "y": "uses"
 },
 {
  "id": "79e36756354f61b087655bc6afede8_6",
  "x": "**EXPERIMENTS** In our experiments, we compared our proposed method with <cite>(Klementiev et al., 2012)</cite> . We estimated comparable features from comparable corpora using the method of <cite>(Klementiev et al., 2012)</cite> and our proposed method respectively.",
  "y": "uses"
 },
 {
  "id": "79e36756354f61b087655bc6afede8_7",
  "x": "6 The \"NIST\" column of Table 4 shows the statistics of this parallel corpus. For decoding, we used the state-of-theart PBSMT toolkit Moses (Koehn et al., 2007) with default options, except for the phrase length limit (7\u21923) following <cite>(Klementiev et al., 2012)</cite> . We trained a 5-gram language model on the English side of the parallel corpus using the SRILM toolkit 7 with interpolated Kneser-Ney discounting, and used it for all the experiments.",
  "y": "uses"
 },
 {
  "id": "79e36756354f61b087655bc6afede8_8",
  "x": "We treated the two sides of the parallel corpus as independent monolingual corpora, following (Haghighi et al., 2008;<cite> Klementiev et al., 2012</cite> We used an open-source Python script 13 to extract and clean the text from the dumps. We aligned the articles on the same topic in Chinese-English Wikipedia via the interlanguage links. We estimated comparable features for the unique phrase pairs used for tuning and testing.",
  "y": "uses"
 },
 {
  "id": "79e36756354f61b087655bc6afede8_9",
  "x": "\"Baseline\" denotes the baseline system that does not use comparable features. \"Klementiev+\" denotes the system that appends the comparable features estimated following <cite>(Klementiev et al., 2012)</cite> to the phrase table. \"Proposed\" denotes the system that uses the comparable features estimated by our proposed method. \"+Contex-tual\", \"+Topical\" and \"+Temporal\" denote the systems that append contextual, topical and temporal features respectively. \"+All\" denotes the system that appends all the three types of features.",
  "y": "uses"
 },
 {
  "id": "79e36756354f61b087655bc6afede8_10",
  "x": "The reason for this is that the comparable features estimated by <cite>(Klementiev et al., 2012)</cite> are inaccurate. \"Proposed\" performs significantly better than both \"Baseline\" and \"Klementiev+\". The reason for this is that \"Proposed\" deals with the data sparseness problem of BLE for comparable feature estimation, making the features more accurate thus improve the SMT performance. As for different comparable features of \"Proposed\", \"+Contextual\", \"+Topical\" and \"+Temporal\" are all helpful, and combining them can be more effective.",
  "y": "differences"
 },
 {
  "id": "79e36756354f61b087655bc6afede8_11",
  "x": "As for different comparable features of \"Proposed\", \"+Contextual\", \"+Topical\" and \"+Temporal\" are all helpful, and combining them can be more effective. The results verify the effectiveness of our proposed method for the accuracy problem of PBSMT. We also investigated the comparable features estimated by the method of <cite>(Klementiev et al., 2012)</cite> and our proposed method.",
  "y": "uses"
 },
 {
  "id": "79e36756354f61b087655bc6afede8_12",
  "x": "The results verify the effectiveness of our proposed method for the accuracy problem of PBSMT. We also investigated the comparable features estimated by the method of <cite>(Klementiev et al., 2012)</cite> and our proposed method. Based on our investigation, most comparable features estimated by our proposed method are more accurate than the ones estimated by the method of (Klementiev et al., 2012 Table 8 : Examples of comparable feature scores estimated by the method of <cite>(Klementiev et al., 2012)</cite> (above the bold line) and our proposed method (below the bold line) for the phrase pairs shown in Table 1 (\"con\", \"top\" and \"tem\" denote phrasal contextual, topical and temporal features respectively, \"con lex\", \"top lex\" and \"tem lex\" denote lexical contextual, topical and temporal features respectively).",
  "y": "differences"
 },
 {
  "id": "79e36756354f61b087655bc6afede8_13",
  "x": "Based on our investigation, most comparable features estimated by our proposed method are more accurate than the ones estimated by the method of (Klementiev et al., 2012 Table 8 : Examples of comparable feature scores estimated by the method of <cite>(Klementiev et al., 2012)</cite> (above the bold line) and our proposed method (below the bold line) for the phrase pairs shown in Table 1 (\"con\", \"top\" and \"tem\" denote phrasal contextual, topical and temporal features respectively, \"con lex\", \"top lex\" and \"tem lex\" denote lexical contextual, topical and temporal features respectively). ture scores estimated for the phrase pairs shown in Table 1 . Table 8 shows the comparable feature scores estimated by the method of <cite>(Klementiev et al., 2012)</cite> (above the bold line) and our proposed method (below the bold line).",
  "y": "uses"
 },
 {
  "id": "79e36756354f61b087655bc6afede8_14",
  "x": "We can see that the method of <cite>(Klementiev et al., 2012)</cite> suffers from the data sparseness problem. Many of the feature scores are 1e \u2212 07, and many of the feature scores for the correct translations (\"unemployment figures\" and \"number of unemployed\") are lower than the incorrect ones (\". unemployment was\" and \"unemployment and bringing\"). Our proposed method addresses the data sparseness problem by using paraphrases for vector smoothing.",
  "y": "uses"
 },
 {
  "id": "79e36756354f61b087655bc6afede8_15",
  "x": "We can see that the method of <cite>(Klementiev et al., 2012)</cite> suffers from the data sparseness problem. Our proposed method addresses the data sparseness problem by using paraphrases for vector smoothing.",
  "y": "differences"
 },
 {
  "id": "79e96060492c3978dc5a7a0d5f293f_0",
  "x": "Of direct importance to the discussion in this paper are results from domain adaptation in polarity detection. One of the earlier successful approaches (Blitzer et al. 2006<cite> (Blitzer et al. , 2007</cite> involved Structural Correspondence Learning (SCL). SCL identifies \"pivot\" features that are both highly discriminative in the labeled source domain data and also frequent in the unlabeled target domain data.",
  "y": "background"
 },
 {
  "id": "79e96060492c3978dc5a7a0d5f293f_1",
  "x": "Instead of using a single, general, feature set for source and target, three distinct feature sets are created: the general set of features, a source-domain specific version of the feature set, and a target-specific version of the feature set. Li and Zong (NLP-KE 2008) explore a classifier combination technique they call \"MultipleLabel Consensus Training\" which results in better accuracy than non-adapted models on the data sets used in <cite>Blitzer et al. (2007)</cite> . They also addressed the multi-domain sentiment analysis problem using feature -level fusion and classifier-level fusion approaches in Li and Zong (ACL 2008) .",
  "y": "background"
 },
 {
  "id": "79e96060492c3978dc5a7a0d5f293f_2",
  "x": "Their MultiDomain Regularization (MDR) framework seeks to learn domain specific parameters guided by the shared parameter across domains. Samdani and Yih (2011) propose an ensemble learner that consists of classifiers trained on different feature groups. The feature groups are Chen et al. (2011) use a specific co-training algorithm for domain adaptation on the <cite>Blitzer et al. (2007)</cite> data set.",
  "y": "background"
 },
 {
  "id": "79e96060492c3978dc5a7a0d5f293f_3",
  "x": "The feature groups are Chen et al. (2011) use a specific co-training algorithm for domain adaptation on the <cite>Blitzer et al. (2007)</cite> data set. In averaged pair-wise comparisons they establish gains over a source-plustarget logistic regression baseline. Glorot et al. (2011) investigate a deep learning approach to domain adaptation and report increased accuracy across domains both on the <cite>Blitzer et al. (2007)</cite> 4-domain data set and the larger Amazon review data set (25 domains) also made available in that release.",
  "y": "background"
 },
 {
  "id": "79e96060492c3978dc5a7a0d5f293f_4",
  "x": "3. The domain adaptation approach proposed in Daum\u00e9 (2007) . 4. We also compared the results of approaches 1 and 2 to published results on Structural Correspondence Learning (SCL) by using the same datasets as in <cite>Blitzer et al. (2007)</cite> .",
  "y": "uses"
 },
 {
  "id": "79e96060492c3978dc5a7a0d5f293f_5",
  "x": "<cite>Blitzer et al. (2007)</cite> employ the Structural Correspondence Learning (SCL) algorithm for sentiment domain adaptation. Blitzer et al. evaluate the SCL domain adaptation on four publicly released datasets from Amazon product reviews: books, DVDs, electronics and kitchen appliances. In these four datasets, reviews with rating > 3 were labeled positive, those with rating < 3 were labeled negative, and the rest discarded because their polarity was ambiguous.",
  "y": "background"
 },
 {
  "id": "79e96060492c3978dc5a7a0d5f293f_6",
  "x": "Each labeled dataset was split into 1600 instances for training and 400 instances for testing. The baseline in <cite>Blitzer et al. (2007)</cite> is a linear classifier trained without adaptation, while their ceiling reference is the same as ours, which is the in-domain classifier trained and tested on the same domain. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "79e96060492c3978dc5a7a0d5f293f_7",
  "x": "In these experiments, we compare the results of our all-in-one classifier and the ensemble classifier trained and tested on the four datasets to the results of SCL and its variation SCL-MI domain adaptation as reported by <cite>Blitzer et al. (2007)</cite> baseline and ceiling in-domain classifiers for the four domains. ---------------------------------- **RESULTS & DISCUSSION**",
  "y": "uses"
 },
 {
  "id": "79e96060492c3978dc5a7a0d5f293f_8",
  "x": "---------------------------------- **STRUCTURAL CORRESPONDENCE LEARNING (SCL)** We employed the four domains datasets used in <cite>Blitzer et al. (2007)</cite> to train and test the all-in one and the ensemble classifiers.",
  "y": "uses"
 },
 {
  "id": "79e96060492c3978dc5a7a0d5f293f_9",
  "x": "We employed the four domains datasets used in <cite>Blitzer et al. (2007)</cite> to train and test the all-in one and the ensemble classifiers. We also replicated the in-domain results of these four datasets using our maximum entropy classifier. We compare the results of the all-in-one and the ensemble classifier to the SCL and its variation SCL-MI adaptation techniques using the four datasets used to evaluate SCL and SCL-MI in <cite>Blitzer et al. (2007)</cite> .",
  "y": "uses"
 },
 {
  "id": "79e96060492c3978dc5a7a0d5f293f_10",
  "x": "The results in the previous section indicate that both the all-in-one and the ensemble approaches exceed both Daum\u00e9's domain adaptation technique on the 27 datasets (given our current implementation of Daum\u00e9's approach) and SCL on the four datasets in <cite>Blitzer et al. (2007)</cite> and that the all-in-one approach achieves comparable results in terms of transfer ratio to Glorot et al. (2011) . The ensemble approach exceeds the all-in-one in some domains like apparel and automotive. They both are very close in some domains like When comparing the all-in-one and the ensemble approaches on the four datasets in <cite>Blitzer et al. (2007)</cite> , the all-in-one exceeds the ensemble only in the DVD domain.",
  "y": "differences"
 },
 {
  "id": "79e96060492c3978dc5a7a0d5f293f_11",
  "x": "The results in the previous section indicate that both the all-in-one and the ensemble approaches exceed both Daum\u00e9's domain adaptation technique on the 27 datasets (given our current implementation of Daum\u00e9's approach) and SCL on the four datasets in <cite>Blitzer et al. (2007)</cite> and that the all-in-one approach achieves comparable results in terms of transfer ratio to Glorot et al. (2011) . The ensemble approach exceeds the all-in-one in some domains like apparel and automotive. They both are very close in some domains like When comparing the all-in-one and the ensemble approaches on the four datasets in <cite>Blitzer et al. (2007)</cite> , the all-in-one exceeds the ensemble only in the DVD domain.",
  "y": "uses"
 },
 {
  "id": "7a1a1593a9480b6ee246ff4248668e_0",
  "x": "**MODELS** First, we discuss our baseline model which is similar to the machine translation encoder-alignerdecoder model of<cite> Luong et al. (2015)</cite> , and presented by Chopra et al. (2016) . Next, we introduce our multi-task learning approach of sharing the parameters between abstractive summarization and entailment generation models.",
  "y": "similarities"
 },
 {
  "id": "7a1a1593a9480b6ee246ff4248668e_1",
  "x": "---------------------------------- **BASELINE MODEL** Our baseline model is a strong, multi-layered encoder-attention-decoder model with bilinear attention, similar to<cite> Luong et al. (2015)</cite> and following the details in Chopra et al. (2016) .",
  "y": "similarities"
 },
 {
  "id": "7a1a1593a9480b6ee246ff4248668e_2",
  "x": "The word probability distribution at time step t of the decoder is defined as follows: where g is a non-linear function and c t and s t are the context vector and LSTM-RNN decoder hidden state at time step t, respectively. The context vector c t = \u03b1 t,i h i is a weighted combination of encoder hidden states h i , where the attention weights are learned through the bilinear attention mechanism proposed in<cite> Luong et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "7a1a1593a9480b6ee246ff4248668e_3",
  "x": "---------------------------------- **MULTI-TASK LEARNING** Multi-task learning helps in sharing knowledge between related tasks across domains <cite>(Luong et al., 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "7a1a1593a9480b6ee246ff4248668e_4",
  "x": "**MULTI-TASK LEARNING** Multi-task learning helps in sharing knowledge between related tasks across domains <cite>(Luong et al., 2015)</cite> . In this work, we show improvements on the task of abstractive summarization by sharing its parameters with the task of entailment generation.",
  "y": "differences"
 },
 {
  "id": "7a1a1593a9480b6ee246ff4248668e_5",
  "x": "---------------------------------- **SUMMARIZATION RESULTS: GIGAWORD** Baseline Results and Previous Work Our baseline is a strong encoder-attention-decoder model based on<cite> Luong et al. (2015)</cite> and presented by Chopra et al. (2016) .",
  "y": "uses"
 },
 {
  "id": "7a1a1593a9480b6ee246ff4248668e_6",
  "x": "Here, we directly use the Gigaword-trained model to test on the DUC-2004 dataset (see tuning discussion in Sec. 4.1). In Table 2 , we again see that et al. (2015) 28.18 8.49 23.81 Chopra et al. (2016) 28.97 8.26 24.06 Nallapati et al. (2016) our<cite> Luong et al. (2015)</cite> baseline model achieves competitive performance with previous work, esp. on Rouge-2 and Rouge-L. Next, we show promising multi-task improvements over this baseline of around 0.4% across all metrics, despite being a test-only setting and also with the mismatch between the summarization and entailment domains.",
  "y": "similarities"
 },
 {
  "id": "7a437574a9a7fff56a480801e47711_0",
  "x": "The primary shortcoming of this presentation lies in perpetuating the false dichotomy between \"grammar-based\" and \"data-driven\" approaches to language modeling for speech recognition, which motivates the final chapter of the book. In fact, the authors' approach is both grammar-based and data-driven, given the corpus-based grammar specialization and PCFG estimation, which the authors themselves demonstrate to be indispensable. Robust grammar-based language modeling is a topic that has received a fair bit of attention over the past decade (Chelba and Jelinek 2000; Charniak 2001; <cite>Roark 2001</cite>; Wang, Stolcke, and Harper 2004, among others) , and while this line of research has not focused on the use of manually built, narrow-domain feature grammars, there is enough similarity between the approach described in this book and the cited papers that the papers would seem to be better comparison points than the class-based language models that are chosen to represent robust approaches in the comparison.",
  "y": "background"
 },
 {
  "id": "7a5ebe06eebebabf4340f1cf583d86_0",
  "x": "The lexicon can be considered the most dynamic part of all linguistic knowledge sources over time. There are two innovative change strategies typical for lexical systems: the creation of entirely new lexical items, commonly reflecting the emergence of novel ideas, technologies or artifacts, on the one hand, and, on the other hand, shifts in the meaning of already existing lexical items, a process which usually takes place over larger periods of time. Tracing semantic changes of the latter type is the main focus of our research. Meaning shift has recently been investigated with emphasis on neural language models (Kim et al., 2014;<cite> Kulkarni et al., 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "7a5ebe06eebebabf4340f1cf583d86_1",
  "x": "Neural language models, originating from the word2vec algorithm (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c) , are currently considered as state-of-the-art solutions for implementing this assumption (Schnabel et al., 2015) . Within this approach, changes in similarity relations between lexical items at two different points of time are interpreted as a signal for meaning shift. Accordingly, lexical items which are very similar to the lexical item under scrutiny can be considered as approximating its meaning at a given point in time. Both techniques were already combined in prior work to show, e.g., the increasing association of the lexical item \"gay\" with the meaning dimension of \"homosexuality\" (Kim et al., 2014;<cite> Kulkarni et al., 2015)</cite> . We here investigate the accuracy and reliability of such similarity judgments derived from different training protocols dependent on word frequency, word ambiguity and the number of training epochs (i.e., iterations over all training material).",
  "y": "background motivation"
 },
 {
  "id": "7a5ebe06eebebabf4340f1cf583d86_2",
  "x": "**RELATED WORK** Neural language models for tracking semantic changes over time typically distinguish between two different training protocols-continuous training of models (Kim et al., 2014) where the model for each time span is initialized with the embeddings of its predecessor, and, alternatively, independent training with a mapping between models for different points in time<cite> (Kulkarni et al., 2015)</cite> . A comparison between these two protocols, such as the one proposed in this paper, has not been carried out before.",
  "y": "background motivation"
 },
 {
  "id": "7a5ebe06eebebabf4340f1cf583d86_3",
  "x": "---------------------------------- **EXPERIMENTAL SET-UP** For comparability with earlier studies (Kim et al., 2014;<cite> Kulkarni et al., 2015)</cite> , we use the fiction part of the GOOGLE BOOKS NGRAM corpus (Michel et al., 2011; Lin et al., 2012) .",
  "y": "uses"
 },
 {
  "id": "7a5ebe06eebebabf4340f1cf583d86_4",
  "x": "We thus concentrate on two experimental protocols-the one described by Kim et al. (2014) (referred to as Kim protocol) and the one from <cite>Kulkarni et al. (2015)</cite> (referred to as Kulkarni protocol), including close variations thereof. Kulkarni's protocol operates on all 5-grams occurring during five consecutive years (e.g., [1900] [1901] [1902] [1903] [1904] and trains models independently of each other. Kim's protocol operates on uniformly sized samples of 10M 5-grams for each year from 1850 onwards in a continuous fashion (years before 1900 are used for initialization only).",
  "y": "uses background"
 },
 {
  "id": "7a5ebe06eebebabf4340f1cf583d86_5",
  "x": "Our investigation in the performance of two common protocols for training neural language models on historical text data led to several hitherto unknown results. We could show that negative sampling outperforms hierarchical softmax both in terms of accuracy and reliability, especially 4 <cite>Kulkarni et al. (2015)</cite> compiled the following list based on prior work (Wijaya and Yeniterzi, 2011; Gulordava and Baroni, 2011; Jatowt and Duh, 2014; Kim et al., 2014): card, sleep, parent, address, gay, mouse, king, checked, check, actually, supposed, guess, cell, headed, ass, mail, toilet, cock, bloody, nice and guy. 5 We used WORDNET 3.0 and the API provided by the Natural Language Toolkit (NLTK): www.nltk.org Even the most reliable system often identifies widely different words as most similar.",
  "y": "differences"
 },
 {
  "id": "7ac01a84ab696e7fa9d0ce336a393e_0",
  "x": "Multimodal corpora such as Flickr30k [1] or MSCOCO [2] containing images along with natural language captions were made available for research. They were soon extended with speech modality: speech recordings for the captions of Flickr8k were collected by [3] via crowdsourcing; spoken captions for MSCOCO were generated using Google Text-To-Speech (TTS) by<cite> [4]</cite> and using Voxygen TTS by [5] ; extensions of these corpora to other languages than English, such as Japanese, were also introduced by [6] . These corpora, as well as deep learning models, lead to contributions in multilingual language grounding and learning of shared and multimodal representations with neural networks<cite> [4,</cite> 7, 8, 9, 10, 11, 12, 13] .",
  "y": "background"
 },
 {
  "id": "7ac01a84ab696e7fa9d0ce336a393e_1",
  "x": "Multimodal corpora such as Flickr30k [1] or MSCOCO [2] containing images along with natural language captions were made available for research. They were soon extended with speech modality: speech recordings for the captions of Flickr8k were collected by [3] via crowdsourcing; spoken captions for MSCOCO were generated using Google Text-To-Speech (TTS) by<cite> [4]</cite> and using Voxygen TTS by [5] ; extensions of these corpora to other languages than English, such as Japanese, were also introduced by [6] . These corpora, as well as deep learning models, lead to contributions in multilingual language grounding and learning of shared and multimodal representations with neural networks<cite> [4,</cite> 7, 8, 9, 10, 11, 12, 13] .",
  "y": "background"
 },
 {
  "id": "7ac01a84ab696e7fa9d0ce336a393e_2",
  "x": "They were soon extended with speech modality: speech recordings for the captions of Flickr8k were collected by [3] via crowdsourcing; spoken captions for MSCOCO were generated using Google Text-To-Speech (TTS) by<cite> [4]</cite> and using Voxygen TTS by [5] ; extensions of these corpora to other languages than English, such as Japanese, were also introduced by [6] . These corpora, as well as deep learning models, lead to contributions in multilingual language grounding and learning of shared and multimodal representations with neural networks<cite> [4,</cite> 7, 8, 9, 10, 11, 12, 13] . This paper focuses on computational models of visually grounded speech that were introduced by [14,<cite> 4]</cite> .",
  "y": "background"
 },
 {
  "id": "7ac01a84ab696e7fa9d0ce336a393e_3",
  "x": "This paper focuses on computational models of visually grounded speech that were introduced by [14,<cite> 4]</cite> . Learned representations of such models were analyzed by [11, 7,<cite> 4]</cite> : [11] introduced novel methods for interpreting the activation patterns of recurrent neural networks (RNN) in a model of visually grounded meaning representation from textual and visual input and showed that RNN pay attention to word tokens belonging to specific lexical categories. [4] found that final layers tend to encode semantic information whereas lower layers tend to encode form-related information.",
  "y": "background"
 },
 {
  "id": "7ac01a84ab696e7fa9d0ce336a393e_4",
  "x": "Such computational models can be used to emulate child language acquisition and could shed light on the inner cognitive pro-This work was supported by grants from NeuroCoG IDEX UGA as part of of the \"Investissements d'avenir\" program (ANR-15-IDEX-02) cesses at work in humans as suggested by [15] . While [11, 7,<cite> 4]</cite> focused on analyzing speech representations learnt by speech-image neural models from a phonological and semantic point of view, the present work focuses on lexical acquisition and the way speech utterances are segmented into lexical units and processed by a computational model of visually grounded speech. We analyze a key component of the neural model -the attention mechanism -and we observe its behaviour and draw parallels between artificial neural attention and human attention.",
  "y": "differences"
 },
 {
  "id": "7ac01a84ab696e7fa9d0ce336a393e_5",
  "x": "The model we use for our experiments is based on that of<cite> [4]</cite> . It is trained to solve an image retrieval task: given a spoken description it retrieves the closest image that matches the description. To do so, the model projects an image and its spoken description in a common representation space, so that matching image/utterance pairs lie near while mismatching image/utterance pairs lie apart.",
  "y": "uses"
 },
 {
  "id": "7ac01a84ab696e7fa9d0ce336a393e_6",
  "x": "It only consists of a dense layer that learns how to shrink the<cite> 4</cite>096 dimensional VGG-16 input vector to a 512 dimensional vector, which is then L2 normalised. The speech encoder (input is 13 MFCC vectors instead of raw speech) consists of a convolutional layer followed by 5 stacked recurrent layers. Contrary to the original model (<cite> [4]</cite> ), we used GRU units instead of RHN units.",
  "y": "differences"
 },
 {
  "id": "7ac01a84ab696e7fa9d0ce336a393e_7",
  "x": "In the original architecture (<cite> [4]</cite> ), attention follows the last recurrent layer. To have more insight on the representation learnt by the network, we added an attention mechanism after the first recurrent layer. Final vector produced by the speech encoder is a dot product of the vectors produced by both attentions.",
  "y": "extends"
 },
 {
  "id": "7ac01a84ab696e7fa9d0ce336a393e_8",
  "x": "MSCOCO and STAIR are thus comparable corpora. We trained our model on extended versions of MSCOCO and STAIR. Spoken COCO dataset was introduced by<cite> [4]</cite> for English.",
  "y": "uses"
 },
 {
  "id": "7ac01a84ab696e7fa9d0ce336a393e_9",
  "x": "We trained our model on extended versions of MSCOCO and STAIR. Spoken COCO dataset was introduced by<cite> [4]</cite> for English. We followed the same methodology as<cite> [4]</cite> and generated synthetic speech for each caption in the Japanese STAIR dataset.",
  "y": "uses"
 },
 {
  "id": "7ac01a84ab696e7fa9d0ce336a393e_10",
  "x": "We created the spoken STAIR dataset so it would follow the exact same train/val/test 5 split as<cite> [4]</cite> . We thus have two comparable corpora: one featuring images and spoken captions in English, and another one featuring the same images and spoken captions in Japanese. This allowed us to compare the behaviour of the same architecture on two typologically different languages.",
  "y": "uses"
 },
 {
  "id": "7ac01a84ab696e7fa9d0ce336a393e_11",
  "x": "Original implementation by<cite> [4]</cite> with RHN reports median rank r = 13 on English dataset. Chance for median rank r is 2500.5. ----------------------------------",
  "y": "background"
 },
 {
  "id": "7adc4bb66b9173ccee2adc4b64c945_0",
  "x": "The shifting hypothesis (Taboada et al., 2011) , however, assumes that negators change sentiment values by a constant amount. In this paper, we refer to a negation word as the negator (e.g., isn't), a text span being modified by and composed with a negator as the argument (e.g., very good), and entire phrase (e.g., isn't very good) as the negated phrase. The recently available Stanford Sentiment Treebank <cite>(Socher et al., 2013)</cite> renders manually annotated, real-valued sentiment scores for all phrases in parse trees.",
  "y": "background"
 },
 {
  "id": "7adc4bb66b9173ccee2adc4b64c945_1",
  "x": "We first evaluate the modeling capabilities of two influential heuristics and show that they capture only very limited regularity of negators' effect. We then extend the models to be dependent on the negators and demonstrate that such a simple extension can significantly improve the performance of fitting to the human annotated data. Next, we evaluate a recently proposed composition model<cite> (Socher, 2013)</cite> that relies on both the negator and the argument.",
  "y": "uses"
 },
 {
  "id": "7adc4bb66b9173ccee2adc4b64c945_2",
  "x": "For example, in the work of (Kennedy and Inkpen, 2006 ), a feature not good will be created if the word good is encountered within a predefined range after a negator. There exist different ways of incorporating more complicated syntactic and semantic information. Much recent work considers sentiment analysis from a semantic-composition perspective (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2012;<cite> Socher et al., 2013)</cite> , which achieved the state-of-the-art performance.",
  "y": "background"
 },
 {
  "id": "7adc4bb66b9173ccee2adc4b64c945_3",
  "x": "Moilanen and Pulman (2007) used a collection of hand-written compositional rules to assign sentiment values to different granularities of text spans. Choi and Cardie (2008) proposed a learning-based framework. The more recent work of (Socher et al., 2012;<cite> Socher et al., 2013)</cite> proposed models based on recursive neural networks that do not rely on any heuristic rules.",
  "y": "background"
 },
 {
  "id": "7adc4bb66b9173ccee2adc4b64c945_4",
  "x": "The approach leverages a principled method, the forward and backward propagation, to learn a vector representation to optimize the system performance. In principle neural network is able to fit very complicated functions (Mitchell, 1997) , and in this paper, we adapt the state-of-the-art approach described in <cite>(Socher et al., 2013)</cite> to help understand the behavior of negators specifically. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7adc4bb66b9173ccee2adc4b64c945_5",
  "x": "That is, the non-uniform information shown in Figure 1 is not directly modeled. The other takes into account s( w) too. For the former, we adopt the recursive neural tensor network (RNTN) proposed recently by<cite> Socher et al. (2013)</cite> , which has showed to achieve the state-of-the-art performance in sentiment analysis.",
  "y": "uses"
 },
 {
  "id": "7adc4bb66b9173ccee2adc4b64c945_6",
  "x": "More details can be found in <cite>(Socher et al., 2013)</cite> . As shown in the black portion of Figure 2 , each instance of RNTN corresponds to a binary parse tree of a given sentence. Each node of the parse tree is a fixed-length vector that encodes compositional semantics and syntax, which can be used to predict the sentiment of this node.",
  "y": "background"
 },
 {
  "id": "7adc4bb66b9173ccee2adc4b64c945_7",
  "x": "**INFERENCE AND LEARNING** Inference and learning in PSTN follow a forwardbackward propagation process similar to that in <cite>(Socher et al., 2013)</cite> , and for completeness, we depict the details as follows. To train the model, one first needs to calculate the predicted sentiment distribution for each node:",
  "y": "uses"
 },
 {
  "id": "7adc4bb66b9173ccee2adc4b64c945_8",
  "x": "To train the model, one first needs to calculate the predicted sentiment distribution for each node: and then compute the posterior probability over the m labels: During learning, following the method used by the RNTN model in <cite>(Socher et al., 2013)</cite> , PSTN also aims to minimize the cross-entropy error between the predicted distribution y i \u2208 R m\u00d71 at node i and the target distribution t i \u2208 R m\u00d71 at that node.",
  "y": "uses"
 },
 {
  "id": "7adc4bb66b9173ccee2adc4b64c945_9",
  "x": "We will discuss the gradient computation for the V sen and W sen in detail next. Note that the gradient calculations for the V, W, W label , L are the same as that of presented in <cite>(Socher et al., 2013)</cite> . In the backpropogation process of the training, each node (except the root node) in the tree carries two kinds of errors: the local softmax error and the error passing down from its parent node.",
  "y": "uses"
 },
 {
  "id": "7adc4bb66b9173ccee2adc4b64c945_10",
  "x": "Finally, we can finish the above equations with the following formula for computing \u03b4 p 2 ,down : After the models are trained, they are applied to predict the sentiment of the test data. The original RNTN and the PSTN predict 5-class sentiment for each negated phrase; we map the output to real-valued scores based on the scale that<cite> Socher et al. (2013)</cite> used to map real-valued sentiment scores to sentiment categories.",
  "y": "uses"
 },
 {
  "id": "7adc4bb66b9173ccee2adc4b64c945_11",
  "x": "---------------------------------- **EXPERIMENT SET-UP** Data As described earlier, the Stanford Sentiment Treebank <cite>(Socher et al., 2013)</cite> has manually annotated, real-valued sentiment values for all phrases in parse trees.",
  "y": "background"
 },
 {
  "id": "7adc4bb66b9173ccee2adc4b64c945_12",
  "x": "**EXPERIMENT SET-UP** Data As described earlier, the Stanford Sentiment Treebank <cite>(Socher et al., 2013)</cite> has manually annotated, real-valued sentiment values for all phrases in parse trees. This provides us with the training and evaluation data to study the effect of negators with syntax and semantics of different complexity in a natural setting.",
  "y": "uses"
 },
 {
  "id": "7adc4bb66b9173ccee2adc4b64c945_13",
  "x": "Each occurrence of a negator and the phrase it is directly composed with in the treebank, i.e., w n , w , is considered a data point in our study. In total, we collected 2,261 pairs, including 1,845 training and 416 test cases. The split of training and test data is same as specified in <cite>(Socher et al., 2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "7b5ca6526f460139f273484bd276bc_0",
  "x": "DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer. Experimental results show that DCR achieves stateof-the-art exact match and F1 scores on the SQuAD dataset<cite> (Rajpurkar et al. 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "7b5ca6526f460139f273484bd276bc_1",
  "x": "This is because RCQA can exploit the textual evidences to ensure increased answer coverage, which is particularly helpful for non-factoid answers. However, it is also challenging for RCQA to identify answer in arbitrary position in the passage with arbitrary length, especially for nonfactoid answers which might be clauses or sentences. As a result, apart from a few exceptions<cite> (Rajpurkar et al. 2016</cite>; Wang and Jiang 2016) , this research direction has not been fully explored yet.",
  "y": "background motivation"
 },
 {
  "id": "7b5ca6526f460139f273484bd276bc_2",
  "x": "The rule-based chunking approach suffered from low coverage (\u2248 70% recall of answer chunks) that cannot be improved during training; and candidate ranking performance depends greatly on the quality of the hand-crafted features. More recently, Wang and Jiang (2016) proposed two endto-end neural network models, one of which chunks a candidate answer by predicting the answer's two boundary indices and the other classifies each passage word into answer/notanswer. Both models improved significantly over the method proposed by <cite>Rajpurkar et al. (2016)</cite> .",
  "y": "background"
 },
 {
  "id": "7b5ca6526f460139f273484bd276bc_3",
  "x": "First, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in<cite> (Rajpurkar et al. 2016)</cite> . Second, it represents answer candidates as chunks, as in<cite> (Rajpurkar et al. 2016</cite> ), instead of word-level representations (Wang and Jiang 2016) , to make the model aware of the subtle differences among candidates (importantly, overlapping candidates). The contributions of this paper are three-fold.",
  "y": "differences"
 },
 {
  "id": "7b5ca6526f460139f273484bd276bc_4",
  "x": "Our proposed model, called dynamic chunk reader (DCR), not only significantly differs from both the above systems in the way that answer candidates are generated and ranked, but also shares merits with both works. First, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in<cite> (Rajpurkar et al. 2016)</cite> . Second, it represents answer candidates as chunks, as in<cite> (Rajpurkar et al. 2016</cite> ), instead of word-level representations (Wang and Jiang 2016) , to make the model aware of the subtle differences among candidates (importantly, overlapping candidates).",
  "y": "similarities"
 },
 {
  "id": "7b5ca6526f460139f273484bd276bc_5",
  "x": "The experiments on the Stanford Question Answering Dataset (SQuAD)<cite> (Rajpurkar et al. 2016)</cite> , which contains a variety of human-generated factoid and non-factoid questions, have shown the effectiveness of above three contributions. Our paper is organized as follows. We formally define the RCQA problem first.",
  "y": "uses"
 },
 {
  "id": "7b5ca6526f460139f273484bd276bc_6",
  "x": "For the answer selection task this paper focuses on, several datasets exist, e.g. TREC-QA for factoid answer extraction from multiple given passages, bAbI (Weston, Chopra, and Bordes 2014) designed for inference purpose, and the SQuAD dataset<cite> (Rajpurkar et al. 2016)</cite> used in this paper. To the best of our knowledge, the SQuAD dataset is the only one for both factoid and nonfactoid answer extraction with a question distribution more close to real-world applications. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "7b5ca6526f460139f273484bd276bc_7",
  "x": "In this section we modified a state-of-the-art RC system for cloze-style tasks for our answer extraction purpose, to see how much gap we have for the two type of tasks, and to inspire our end-to-end system in the next section. In order to make the cloze-style RC system to make chunk-level decision, we use the RC model to generate features for chunks, which are further used in a feature-based ranker like in<cite> (Rajpurkar et al. 2016)</cite> . As a result, this baseline can be viewed as a deep learning based counterpart of the system in<cite> (Rajpurkar et al. 2016</cite> ).",
  "y": "similarities"
 },
 {
  "id": "7b5ca6526f460139f273484bd276bc_8",
  "x": "Answer Chunking To reduce the errors generated by the rule-based chunker in<cite> (Rajpurkar et al. 2016)</cite> , first, we capture the part-of-speech (POS) pattern of all answer subsequences in the training dataset to form a POS pattern trie tree, and then apply the answer POS patterns to passage P i to acquire a collection of all subsequences (chunk candidates) C i whose POS patterns can be matched to the POS pattern trie. This is equivalent to putting an constraint subj(m, n, P i ) to candidate answer chunk generation process that only choose the chunk with a POS pattern seen for answers in the training data. Then the sub-sequences C i are used as answer candidates for P i .",
  "y": "uses"
 },
 {
  "id": "7b5ca6526f460139f273484bd276bc_9",
  "x": "Dataset We used the Stanford Question Answering Dataset (SQuAD)<cite> (Rajpurkar et al. 2016)</cite> for the experiment. SQuAD came into our sight because it is a mix of factoid and non-factoid questions, a real-world data (crowd-sourced), and of large scale (over 100K question-answer pairs collected from 536 Wikipedia articles). Answers range from single words to long, variable-length phrase/clauses.",
  "y": "uses"
 },
 {
  "id": "7b5ca6526f460139f273484bd276bc_10",
  "x": "We also studied how each component in our model contributes to the overall performance. Table 3 shows the details as well as the results of the baseline ranker. As the first row of Table 3 shows, our baseline system improves 10% (EM) over <cite>Rajpurkar et al. (2016)</cite> (Table 2 , row 1), the feature-based ranking system.",
  "y": "differences"
 },
 {
  "id": "7c5c5f13205c40a27d2629727df840_0",
  "x": "semantics. Our meta logic (a.k.a. glue logic) is system F with many base types t, e i (instead of simply typed \u03bb-calculus with t, and e) Our logic for semantic representations is many-sorted higher-order logic (e i instead of a single sort e). For representing quantification, we actually prefer to use Hilbert's \u01eb and \u03c4 -terms constructed with two constants \u01eb, \u03c4 : \u039b\u03b1. (\u03b1 \u2192 t) \u2192 \u03b1 and one for generic elements <cite>[8]</cite> .",
  "y": "uses"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_0",
  "x": "The major distinction between these methods is in the contrast between the approaches based exclusively on the information contained in the text to be segmented, such as lexical repetition (e.g., Choi 2000; Hearst 1997; Heinonen 1998; Kehagias, Pavlina, and Petridis 2003; Utiyama and Isahara 2001) , and those approaches that rest on complementary semantic knowledge extracted from dictionaries and thesauruses (e.g., Kozima 1993; Lin et al. 2004; Morris and Hirst 1991) , or from collocations collected in large corpora (Bolshakov and Gelbukh 2001; Brants, Chen, and Tsochantaridis 2002; <cite>Choi et al. 2001</cite>; Ferret 2002; Kaufmann 1999; Ponte and Croft 1997) . According to their authors, methods that use additional knowledge allow for a solution to problems encountered when sentences belonging to a unique topic do not share common words due to the use of hyperonyms or synonyms and allow words that are semantically related to be taken as positive evidence for topic continuity. Empirical arguments in favor of these methods have been provided recently by <cite>Choi et al. (2001)</cite> in a study using Latent Semantic Analysis (Latent Semantic Indexing, Deerwester et al. 1990 ) to extract a semantic space from a corpus allowing determination of the similarity of meanings of words, sentences, or paragraphs.",
  "y": "background"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_1",
  "x": "According to their authors, methods that use additional knowledge allow for a solution to problems encountered when sentences belonging to a unique topic do not share common words due to the use of hyperonyms or synonyms and allow words that are semantically related to be taken as positive evidence for topic continuity. Empirical arguments in favor of these methods have been provided recently by <cite>Choi et al. (2001)</cite> in a study using Latent Semantic Analysis (Latent Semantic Indexing, Deerwester et al. 1990 ) to extract a semantic space from a corpus allowing determination of the similarity of meanings of words, sentences, or paragraphs. By comparing the accuracy of the very same algorithm according to whether or not it takes into account complementary semantic knowledge, <cite>they</cite> were able to show the benefit derived from such knowledge.",
  "y": "background"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_2",
  "x": "Empirical arguments in favor of these methods have been provided recently by <cite>Choi et al. (2001)</cite> in a study using Latent Semantic Analysis (Latent Semantic Indexing, Deerwester et al. 1990 ) to extract a semantic space from a corpus allowing determination of the similarity of meanings of words, sentences, or paragraphs. By comparing the accuracy of the very same algorithm according to whether or not it takes into account complementary semantic knowledge, <cite>they</cite> were able to show the benefit derived from such knowledge. However, implications of <cite>Choi et al.'s study</cite> for text segmentation and for the use of LSA in natural language processing are unclear due to the methodology employed.",
  "y": "background"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_3",
  "x": "However, implications of <cite>Choi et al.'s study</cite> for text segmentation and for the use of LSA in natural language processing are unclear due to the methodology employed. In <cite>their experiments</cite>, semantic knowledge was acquired from a corpus containing the materials to be segmented in the test phase. One could speculate whether the largest part of the benefit obtained thanks to the addition of semantic knowledge was not due to this hyper-specificity of the LSA corpus (i.e., the inclusion of the test materials).",
  "y": "background"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_4",
  "x": "If this were the case, it would call into question the possibility of using LSA to acquire generic semantic knowledge that can be used to segment new texts. A priori, the problem does not seem serious for at least two reasons. First, <cite>Choi et al.'s</cite> segmentation procedure does not rely on supervised learning in which a system learns how to efficiently segment a text from training data.",
  "y": "background"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_5",
  "x": "Second, <cite>Choi et al.</cite> employed a large number of small test samples to evaluate their algorithm, each making up-on average-0.15% of the LSA corpus. The present study shows, however, that the presence of the test materials in the LSA corpus has an important effect, but also that the generic semantic knowledge derived from large corpora clearly improves the segmentation accuracy. This conclusion is drawn from two experiments in which the presence or absence of the test materials in the LSA corpus is manipulated.",
  "y": "background"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_6",
  "x": "First, <cite>Choi et al.'s</cite> segmentation procedure does not rely on supervised learning in which a system learns how to efficiently segment a text from training data. The LSA corpus only intervenes in an indirect manner by allowing the extraction of semantic proximities between words that are then used to compute similarities between parts of the text to segment (see Section 2 for details). Second, <cite>Choi et al.</cite> employed a large number of small test samples to evaluate their algorithm, each making up-on average-0.15% of the LSA corpus. The present study shows, however, that the presence of the test materials in the LSA corpus has an important effect, but also that the generic semantic knowledge derived from large corpora clearly improves the segmentation accuracy.",
  "y": "differences"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_7",
  "x": "The first experiment is based on the original materials from <cite>Choi et al.</cite>, which consisted of a small corpus (1,000,000 words). The second experiment is based on a much larger corpus (25,000,000 words). Before reporting these experiments, Choi's algorithm and the use of LSA within this framework are described.",
  "y": "uses"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_8",
  "x": "The first experiment is based on the original materials from <cite>Choi et al.</cite>, which consisted of a small corpus (1,000,000 words). The second experiment is based on a much larger corpus (25,000,000 words). Before reporting these experiments, Choi's algorithm and the use of LSA within this framework are described.",
  "y": "differences"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_9",
  "x": "According to the vector space model, each sentence is represented by a vector of word frequency count, and the similarity between two sentences is calculated by means of the cosine measure between the corresponding vectors. In a first evaluation based on the procedure described below, Choi showed that its algorithm outperforms several other approaches such as TextTiling (Hearst 1997) and Segmenter (Kan, Klavans, and McKeown 1998) . <cite>Choi et al. (2001)</cite> claimed that it was possible to improve the inter-sentence similarities index by taking into account the semantic proximities between words estimated on the basis of Latent Semantic Analysis (LSA).",
  "y": "background"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_10",
  "x": "This makes it possible to measure the semantic proximity between any two words by using, for instance, the cosine measure between the corresponding vectors. Proximity between any two sentences (or any other textual units), even if these sentences were not present in the original corpus, can be estimated by computing a vector for each of these units-which corresponds to the weighted sum of the vectors of the words that compose it-and then by computing the cosine between these vectors (Deerwester et al. 1990 ). <cite>Choi et al. (2001)</cite> have shown that using this procedure to compute the inter-sentence similarities results in the previous version of the algorithm (based solely on word repetition) being outperformed.",
  "y": "background"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_11",
  "x": "<cite>Choi et al. (2001)</cite> have shown that using this procedure to compute the inter-sentence similarities results in the previous version of the algorithm (based solely on word repetition) being outperformed. The aim of this experiment is to determine the impact of the presence of the test materials in the LSA corpus on the results obtained by <cite>Choi et al. (2001)</cite> .",
  "y": "motivation"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_12",
  "x": "One Within semantic space, which corresponds to the one used by <cite>Choi et al.</cite>, was built using the entire Brown corpus as the LSA corpus. Four hundred different Without spaces were built, one for each test sample, by each time removing from the Brown corpus only the sentences that make this sample. To extract the LSA space and to apply the segmentation algorithm, a series of parameters had to be set.",
  "y": "background"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_13",
  "x": "Four hundred different Without spaces were built, one for each test sample, by each time removing from the Brown corpus only the sentences that make this sample. To extract the LSA space and to apply the segmentation algorithm, a series of parameters had to be set. First of all, paragraphs were used as documents for building the lexical tables because <cite>Choi et al.</cite> observed that such middle-sized units were more effective than shorter units (i.e., sentences).",
  "y": "uses background"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_14",
  "x": "Words were not stemmed, as in <cite>Choi et al. (2001)</cite> . To build the LSA space, the singular value decomposition was realized using the program SVDPACKC (Berry 1992; Berry et al. 1993) , and the first 300 singular vectors were retained. Concerning the segmentation algorithm, I used the version in which the number of boundaries to be found is imposed, and thus fixed at nine.",
  "y": "uses"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_15",
  "x": "**RESULTS** The segmentation accuracy was evaluated by means of the index reported by <cite>Choi et al. (2001)</cite> : the Pk measure of segmentation inaccuracy (Beeferman, Berger, and Lafferty 1999) , which gives the proportion of sentences that are wrongly predicted to belong to the same segment or wrongly predicted to belong to different segments. I also report, for potential future comparison, Pevzner and Hearst's (2002) WindowDiff index, which remedies several problems in the Pk measure.",
  "y": "uses"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_16",
  "x": "1 Compared with the Within condition, the performance in the Without condition is definitely worse, as confirmed by t tests for paired sample (each test sample being used as an observation) that are significant for an alpha smaller than 0.0001. The C99 algorithm, which does not employ LSA to estimate the similarities between the sentences, produces a Pk of 0.13 (<cite>Choi et al. 2001</cite>, Table 3, line 3: No stemming) . It appears that although the Without condition is still better than C99, the benefit is very small.",
  "y": "uses"
 },
 {
  "id": "7c8ec9e38bf4c0c458d60af014e102_17",
  "x": "---------------------------------- **EXPERIMENT 2** Experiment 1 was conducted on the <cite>Choi et al. (2001)</cite> LSA corpus, a 1,000,000-word collection of texts from very different genres and with varied themes.",
  "y": "uses"
 },
 {
  "id": "7cb7cfed8b7e7bf2f0a810e02e6cbc_0",
  "x": "While AMR is suitable for information aggregation, it ignores aspects of language such as tense, grammatical number, etc., which are important for the natural language generation (NLG) stage that normally occurs in the end of the summarization process. Due to the lack of such information, approaches for NLG from AMR typically infer it from regularities in the training data (Pourdamghani et al., 2016; Konstas et al., 2017; Song et al., 2016; Flanigan et al., 2016) , which however is not suitable in the context of summarization. Consequently, the main previous work on AMR-based abstractive summarization <cite>(Liu et al., 2015)</cite> only generated bag-of-words from the summary AMR graph.",
  "y": "motivation"
 },
 {
  "id": "7cb7cfed8b7e7bf2f0a810e02e6cbc_1",
  "x": "Abstractive Summarization using AMR: In<cite> Liu et al. (2015)</cite> work, the source document's sentences were parsed into AMR graphs, which were then combined through merging, collapsing and graph expansion into a single AMR graph representing the source document. Following this, a summary AMR graph was extracted, from which a bag of concept words was obtained without attempting to form fluent text. Vilca and Cabezudo (2017) performed a summary AMR graph extraction augmented with discourse-level information and the PageRank (Page et al., 1998) algorithm.",
  "y": "background"
 },
 {
  "id": "7cb7cfed8b7e7bf2f0a810e02e6cbc_2",
  "x": "---------------------------------- **GUIDING NLG FOR AMR-BASED SUMMARIZATION** We first briefly describe the AMR-based summarization method of<cite> Liu et al. (2015)</cite> and then our guided NLG approach.",
  "y": "background"
 },
 {
  "id": "7cb7cfed8b7e7bf2f0a810e02e6cbc_3",
  "x": "---------------------------------- **AMR-BASED SUMMARIZATION** In<cite> Liu et al. (2015)</cite> 's work, each of the sentence of the source document was parsed into an AMR graph, and combined into a source graph, G = (V, E) where v \u2208 V and e \u2208 E are the unique concepts and the relations between pairs of concepts.",
  "y": "background"
 },
 {
  "id": "7cb7cfed8b7e7bf2f0a810e02e6cbc_4",
  "x": "**UNGUIDED NLG FROM AMR** Our baseline is a standard (unguided) seq2seq model with attention (Luong et al., 2015) which consists of an encoder and a decoder. The encoder computes the hidden representation of the input, {z 1 , z 2 , . . . , z k }, which is the linearized summary AMR graph, G \u2032 from<cite> Liu et al. (2015)</cite> , following Van Noord and Bos (2017)'s preprocessing steps.",
  "y": "background"
 },
 {
  "id": "7cb7cfed8b7e7bf2f0a810e02e6cbc_5",
  "x": "Table 2 : BLEU and ROUGE results for guided and unguided models using test dataset. Guided NLG for full summarization In this experiment we combine our guided NLG model with<cite> Liu et al. (2015)</cite> 's work in order to generate fluent texts from their summary AMR graphs using the hyper-parameters tuned in the previous paragraph. Liu et al. (2015) used parses from both the manual annotation of the Proxy dataset as well as those obtained using the JAMR parser (Flanigan et al., 2014) .",
  "y": "similarities uses"
 },
 {
  "id": "7cb7cfed8b7e7bf2f0a810e02e6cbc_6",
  "x": "Instead of JAMR we use the RIGA parser (Barzdins and Gosko, 2016) which had the highest accuracy in the SemEval 2016 Task 8 (May, 2016). We compare our result against<cite> Liu et al. (2015)</cite> 's bag of words 1 , the unguided AMR-to-text model from \u00a73.2, and a seq2seq summarization model (Open-NMT BRNN) 2,3 which summarizes directly from the source document to summary sentence without using AMR as an interlingua and is trained on CNN/DM corpus (Hermann et al., 2015) using the same settings as See et al. (2017) . In Table 3 , we can see that our approach results in improvements over both the unguided AMR-totext and the standard seq2seq summarization.",
  "y": "similarities"
 },
 {
  "id": "7cb7cfed8b7e7bf2f0a810e02e6cbc_7",
  "x": "In Table 3 , we can see that our approach results in improvements over both the unguided AMR-totext and the standard seq2seq summarization. One interesting note is that using the RIGA parses result in higher ROUGE scores than the gold parses for the guided model in our experiment. This phenomenon was also observed in<cite> Liu et al. (2015)</cite> 's experiment where the summary graphs extracted from automatic parses had higher accuracy than those extracted from manual parses.",
  "y": "similarities"
 },
 {
  "id": "7cbe7dc02bbb53fe06d9215ed88fe0_0",
  "x": "For P-S level and S-Ph level, we regard the paragraph of P-S as a long sentence, and the phrase of SPh as a short sentence. Then we use various types of text similarity features including string features, knowledge based features, corpus based features, syntactic features, machine translation based features, multi-level text features and so on, to capture the semantic similarity between two texts. Some of these features are borrowed from our previous system in the Semantic Textual Similarity (STS) task in * SEM Shared Task 2013 <cite>(Zhu and Lan, 2013)</cite> .",
  "y": "uses"
 },
 {
  "id": "7cbe7dc02bbb53fe06d9215ed88fe0_1",
  "x": "We extract 13 string based features in consideration of the common sequence shared by two texts. We chose the Longest Common Sequence (LCS) feature <cite>(Zhu and Lan, 2013)</cite> , the Ngram Overlap feature (n=1,2,3) and the Weighted Word Overlap feature (\u0160aric et al., 2012) . All these features are computed from original text and from the processed text after lemmatization as well.",
  "y": "uses"
 },
 {
  "id": "7cbe7dc02bbb53fe06d9215ed88fe0_2",
  "x": "In this work we used the knowledge based features in our previous work <cite>(Zhu and Lan, 2013)</cite> , which include four word similarity metrics based on WordNet: Path similarity (Banea et al., 2012) , WUP similarity (Wu and Palmer, 1994) , LCH similarity (Leacock and Chodorow, 1998) and Lin similarity (Lin, 1998) . Then two strategies, i.e., the best alignment strategy and the aggregation strategy, are employed to propagate the word similarity to the text similarity. Totally we get 8 knowledge based features.",
  "y": "uses"
 },
 {
  "id": "7cbe7dc02bbb53fe06d9215ed88fe0_3",
  "x": "Two corpora are used to compute the LSA vector of words: New York Times Annotated Corpus (NYT) and Wikipedia. Besides, in consideration of different weights for different words, they also calculated the weighted LSA vector for each word. In addition, we use the Co-occurrence Retrieval Model (CRM) feature from our previous work <cite>(Zhu and Lan, 2013)</cite> as another corpus-based feature.",
  "y": "uses"
 },
 {
  "id": "7cbe7dc02bbb53fe06d9215ed88fe0_4",
  "x": "Dependency relations of sentences often contain semantic information. In this work we follow two syntactic dependency similarity features presented in our previous work <cite>(Zhu and Lan, 2013)</cite>, i.e., Simple Dependency Overlap and Special Dependency Overlap. The Simple Dependency Overlap measures all dependency relations while the Special Dependency Overlap fea-ture only focuses on the primary roles extracted from several special dependency relations, i.e., subject, object and predict.",
  "y": "uses"
 },
 {
  "id": "7cbe7dc02bbb53fe06d9215ed88fe0_5",
  "x": "Machine Translation based features. Machine translation (MT) evaluation metrics are designed to assess whether the output of a MT system is semantically equivalent to a set of reference translations. This type of feature has been proved to be effective in our previous work <cite>(Zhu and Lan, 2013)</cite> .",
  "y": "background"
 },
 {
  "id": "7cbe7dc02bbb53fe06d9215ed88fe0_6",
  "x": "This type of feature has been proved to be effective in our previous work <cite>(Zhu and Lan, 2013)</cite> . As a result, we extend the original 6 lexical level MT metrics to 10 metrics, i.e., WER, TER, PER, BLEU, NIST, ROUGE-L, GTM-1,GTM-2, GTM-3 and METEOR-ex. All these metrics are calculated using the Asiya Open Toolkit for Automatic Machine Translation (Meta-) Evaluation 4 .",
  "y": "extends"
 },
 {
  "id": "7ce7cfb0f918a46a1000e25b2e9eea_0",
  "x": "The COCO-QA dataset<cite> (Ren et al., 2015)</cite> for VQA was created by parsing COCO captions with a syntactic parser, and then used this to create QA pairs for four kinds of questions using hand-crafted rules. However, due to inability of the algorithm to cope with complex sentence structures, a significant portion of COCO-QA questions have grammatical errors or are oddly phrased. Visual question generation was also studied in (Mostafazadeh et al., 2016) , with an emphasis on generating questions about images that are beyond the literal visual content of the image.",
  "y": "motivation"
 },
 {
  "id": "7ce7cfb0f918a46a1000e25b2e9eea_1",
  "x": "---------------------------------- **DATASETS AND ALGORITHMS FOR VQA** We conduct experiments on two of the most popular VQA datasets: 'The VQA Dataset' (Antol et al., 2015) and COCO-QA<cite> (Ren et al., 2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "7ce7cfb0f918a46a1000e25b2e9eea_2",
  "x": "COCO-QA<cite> (Ren et al., 2015)</cite> also uses images from COCO, with the questions generated by an NLP algorithm that uses COCO's captions. All questions belong to four categories: object, number, color, and location. Many algorithms have been proposed for VQA.",
  "y": "background"
 },
 {
  "id": "7ce7cfb0f918a46a1000e25b2e9eea_3",
  "x": "Detailed reviews of existing methods can be found in Kafle and Kanan (2017) and Wu et al. (2016) . However, simpler models such as linear classifiers and multilayer perceptrons (MLPs) perform only slightly worse on many VQA datasets. These baseline methods predict the answer using a vector of image features concatenated to a vector of question features<cite> (Ren et al., 2015</cite>; Zhou et al., 2015; Kafle and Kanan, 2016) .",
  "y": "background"
 },
 {
  "id": "7ce7cfb0f918a46a1000e25b2e9eea_4",
  "x": "First, we use the simple MLP baseline model used in Kafle and Kanan (2016) to assess the two data augmentation methods. The MLP model treats VQA as a classification problem with concatenated image and question features given to the model as features and answers as categories. CNN features from ResNet-152 and the skip-thought vectors<cite> (Kiros et al., 2015)</cite> are used as image and question features respectively.",
  "y": "uses"
 },
 {
  "id": "7d29a7d7c19d097758481b466360b1_0",
  "x": "---------------------------------- **INTRODUCTION** Since the first approach [Wright and Wrigley 1991] of combining a probabilistic method into the GLR technique was published, Some probabilistic GLR parsers also have been implemented in which probabilities are assigned to actions of LR parsing tables by using lookaheads or LR states as simple context information of [Briscoe and Carroll 1993] , [Kentaro et al. 1998 ], and <cite>[Ruland, 2000]</cite> which does not use the stack information of the GLR parser effectively, because of highly complex internal GLR stack.",
  "y": "background"
 },
 {
  "id": "7d29a7d7c19d097758481b466360b1_1",
  "x": "Since the first approach [Wright and Wrigley 1991] of combining a probabilistic method into the GLR technique was published, Some probabilistic GLR parsers also have been implemented in which probabilities are assigned to actions of LR parsing tables by using lookaheads or LR states as simple context information of [Briscoe and Carroll 1993] , [Kentaro et al. 1998 ], and <cite>[Ruland, 2000]</cite> which does not use the stack information of the GLR parser effectively, because of highly complex internal GLR stack. As a result, they have used relatively limited contextual information for disambiguation. [Kwak et al., 2001] have proposed a conditional action model that uses the partially constructed parse represented by the graph-structured stack as the additional context. However, this method inappropriately defined sub-tree structure. Our proposed model uses Surface Phrasal Types representing the structural characteristics of the sub-trees for its additional contextual information.",
  "y": "motivation"
 },
 {
  "id": "7d7895690c84fb1af46c30f858470e_0",
  "x": "Current state-of-the-art question answering models reason over an entire passage, not incrementally. As we will show, naive approaches to incremental reading, such as restriction to unidirectional language models in the model, perform poorly. We present extensions to the DocQA <cite>[2]</cite> model to allow incremental reading without loss of accuracy. The model also jointly learns to provide the best answer given the text that is seen so far and predict whether this best-so-far answer is sufficient.",
  "y": "extends"
 },
 {
  "id": "7d7895690c84fb1af46c30f858470e_1",
  "x": "However, on tasks like Question Answering, in all the existing well-performing models, RNNs are employed in a bidirectional way, or a self-attention mechanism is employed [11,<cite> 2,</cite> 4, 8] . This means these models need to processes the whole input sequence to compute the final answer. This is a reasonable approach if the input sequence is as short as a sentence, but it becomes less effective and efficient as the length of the input sequence increases.",
  "y": "background"
 },
 {
  "id": "7d7895690c84fb1af46c30f858470e_2",
  "x": "However, on tasks like Question Answering, in all the existing well-performing models, RNNs are employed in a bidirectional way, or a self-attention mechanism is employed [11,<cite> 2,</cite> 4, 8] . This means these models need to processes the whole input sequence to compute the final answer. This is a reasonable approach if the input sequence is as short as a sentence, but it becomes less effective and efficient as the length of the input sequence increases.",
  "y": "motivation background"
 },
 {
  "id": "7d7895690c84fb1af46c30f858470e_3",
  "x": "This means these models need to processes the whole input sequence to compute the final answer. This is a reasonable approach if the input sequence is as short as a sentence, but it becomes less effective and efficient as the length of the input sequence increases. We introduce a new incremental model based on DocQA <cite>[2]</cite> , which is an RNN based model proposed for QA.",
  "y": "extends"
 },
 {
  "id": "7d7895690c84fb1af46c30f858470e_4",
  "x": "**ARCHITECTURE OF THE MODELS** We use DocQA as the baseline model <cite>[2]</cite> . The architecture of DocQA is illustrated in Figure 4a .",
  "y": "uses"
 },
 {
  "id": "7d7895690c84fb1af46c30f858470e_5",
  "x": "As a testbed for our approach, we have chosen the question answering task. We aim to build a model that can learn incrementally from text, where the learning goal is to answer a given question. In standard question answering, we do not care how the context is presented to the model, and for the models that achieve state of the art results, e.g. [11,<cite> 2]</cite> , they process the full context before making any decisions.",
  "y": "differences"
 },
 {
  "id": "7d7895690c84fb1af46c30f858470e_6",
  "x": "We aim to build a model that can learn incrementally from text, where the learning goal is to answer a given question. In standard question answering, we do not care how the context is presented to the model, and for the models that achieve state of the art results, e.g. [11,<cite> 2]</cite> , they process the full context before making any decisions. We show that it is possible to modify these models to be incremental while achieving similar performance.",
  "y": "extends differences"
 },
 {
  "id": "7d89a96743d9db5667d90cbd3ebd30_0",
  "x": "This task is often considered as one of the simplest in NLP because basic machine learning techniques can yield strong baselines <cite>(Wang & Manning, 2012)</cite> , often beating much more intricate approaches (Socher et al., 2011) . In the simplest settings, this task can be seen as a binary classification between positive and negative sentiment. However, there are several challenges towards achieving the best possible accuracy.",
  "y": "background"
 },
 {
  "id": "7d89a96743d9db5667d90cbd3ebd30_1",
  "x": "However, there are several challenges towards achieving the best possible accuracy. It is not obvious how to represent variable length documents beyond simple bag of words approaches that lose word order information. One can use advanced machine learning techniques such as recurrent neural networks and their variations (Mikolov et al., 2010; Socher et al., 2011) , however it is not clear if these provide any significant gain over simple bag-of-words and bag-of-ngram techniques (Pang & Lee, 2008;<cite> Wang & Manning, 2012)</cite> .",
  "y": "background"
 },
 {
  "id": "7d89a96743d9db5667d90cbd3ebd30_2",
  "x": "The large pool of diverse models is a) simple to implement (in line with previous work by Wang and Manning <cite>(Wang & Manning, 2012)</cite> ) and b) it yields state of the art performance on one of the largest publicly available benchmarks of movie reviews, the Stanford IMDB dataset of reviews. Code to reproduce our experiments is available at https://github.com/mesnilgr/iclr15. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "7d89a96743d9db5667d90cbd3ebd30_3",
  "x": "In our ensemble, we used a supervised reweighing of the counts as in the Naive Bayes Support Vector Machine (NB-SVM) approach <cite>(Wang & Manning, 2012)</cite> . This approach computes a log-ratio vector between the average word counts extracted from positive documents and the average word counts extracted from negative documents. The input to the logistic regression classifier corresponds to the log-ratio vector multiplied by the binary pattern for each word in the document vector.",
  "y": "similarities uses"
 },
 {
  "id": "7d89a96743d9db5667d90cbd3ebd30_4",
  "x": "The input to the logistic regression classifier corresponds to the log-ratio vector multiplied by the binary pattern for each word in the document vector. Note that the logictic regression can be replaced by a linear SVM. Our implementation 1 slightly improved the performance reported in <cite>(Wang & Manning, 2012)</cite> by adding tri-grams (improvement of +0.6%), as shown in Table 1 .",
  "y": "differences"
 },
 {
  "id": "7d89a96743d9db5667d90cbd3ebd30_6",
  "x": "Favoring simplicity and reproducibility of our performance, all results reported in this paper were produced by a linear classifier. Finally, Table 3 reports the results of combining the previous models into an ensemble. When we interpolate the scores of RNN, sentence vectors and NB-SVM, we achieve a new state-of-the-art performance of 92.57%, to be compared to 91.22% reported by <cite>(Wang & Manning, 2012)</cite> .",
  "y": "differences"
 },
 {
  "id": "7e380d496bb253885218465b778cc1_0",
  "x": "For every language pair and aligned sentence in the dataset, a soft-alignment probability matrix is generated. We use Average Normalized Entropy (ANE) <cite>(Boito et al., 2019a)</cite> computed over these matrices for selecting the most confident one for segmenting each phoneme sequence. This exploits the idea that models trained on different language pairs will have language-related behavior, thus differing on the resulting alignment and segmentation over the same phoneme sequence.",
  "y": "uses"
 },
 {
  "id": "7e380d496bb253885218465b778cc1_1",
  "x": "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from <cite>Boito et al. (2019a)</cite> . Table 2 presents the results for bilingual UWS and multilingual leveraging. For the former, we reach our best result by using as aligned information the French, the original aligned language for this dataset.",
  "y": "uses"
 },
 {
  "id": "7e380d496bb253885218465b778cc1_2",
  "x": "Lastly, following the methodology from <cite>Boito et al. (2019a)</cite> , we extract the most confident alignments (in terms of ANE) discovered by the bilingual models. Table 3 presents the top 10 most confident (discovered type, translation) pairs. 3 Looking at the pairs the bilingual models are most confident about, we observe there are some types discovered by all the bilingual models (e.g. Mboshi word itua, and the concatenation obo\u00e1+ng\u00e1).",
  "y": "uses"
 },
 {
  "id": "7f234ecfb4cf880502faa8b89cd07b_0",
  "x": "Our model is empirically justified and deals with the dirtiness [16] of loosely related tasks. We show that it is a generalization of various multi-task learning algorithms such as hard parameter sharing [7] , low supervision<cite> [25]</cite> , and cross-stitch networks [21] , as well as transfer learning algorithms such as frustratingly easy domain adaptation [9] . Moreover, we study what task properties predict gains, and what properties correlate with learning certain types of sharing, as well as the inductive bias of the resulting architecture.",
  "y": "uses"
 },
 {
  "id": "7f234ecfb4cf880502faa8b89cd07b_1",
  "x": "L K . We assume that all the deep networks have the same hyper-parameters at the outset. With loosely related tasks, one task may be better modeled with one hidden layer; another one with two<cite> [25]</cite> .",
  "y": "background"
 },
 {
  "id": "7f234ecfb4cf880502faa8b89cd07b_2",
  "x": "Recall that our architecture is partly motivated by the observation that for loosely related tasks, only certain features in specific layers should be shared, while many of the layers and subspaces may remain more taskspecific<cite> [25]</cite> . We want to learn what to share while inducing models for the different tasks. For simplicity, we ignore subspaces at first and assume only two tasks A and B. The outputs h A,k,t and h B,k,t of the k-th layer for time step t for task A and B respectively interact through what [21] refer to as cross-stitch units \u03b1 (see Figure  1 ).",
  "y": "motivation"
 },
 {
  "id": "7f234ecfb4cf880502faa8b89cd07b_3",
  "x": "---------------------------------- **LEARNING MIXTURES MANY TASKS HAVE AN IMPLICIT HIERARCHY THAT INFORMS THEIR INTERACTION. RATHER THAN** predefining it <cite>[25,</cite> 11] , we enable our model to learn hierarchical relations by associating different tasks with different layers if this is beneficial for learning.",
  "y": "differences"
 },
 {
  "id": "7f234ecfb4cf880502faa8b89cd07b_4",
  "x": "**PREVIOUS PROPOSALS AS INSTANTIATIONS OF SLUICE NETWORKS** The architecture is very flexible and can be seen as a generalization over several existing algorithms for transfer and multi-task learning, including [7, 9,<cite> 25,</cite> 21] . We show how to derive each of these below.",
  "y": "uses similarities"
 },
 {
  "id": "7f234ecfb4cf880502faa8b89cd07b_5",
  "x": "We split the space in two, leading to three subspaces, if we only share one half across the two networks. Low Supervision<cite> [25]</cite> propose a model where only the inner layers of two deep recurrent works are shared. This is obtained using heavy mean-constrained L0 regularization over the first layer Li,1, e.g., \u2126(W ) = K i ||Li,1||0 with i \u03bbiL(i) < 1, while for the auxiliary task, only the first layer \u03b2 parameter is set to 1.",
  "y": "background"
 },
 {
  "id": "7f234ecfb4cf880502faa8b89cd07b_6",
  "x": "2 Tasks In multi-task learning, one task is usually considered the main task, while other tasks are used as auxiliary tasks to improve performance on the main task. As main tasks, we use chunking (CHUNK), named entity recognition (NER), and a simplified version of semantic role labeling (SRL) where we only identify headwords, and pair them with part-of-speech tagging (POS) as an auxiliary task, following<cite> [25]</cite> . Example annotations for each task can be found in Table 2 .",
  "y": "uses"
 },
 {
  "id": "7f234ecfb4cf880502faa8b89cd07b_7",
  "x": "We use the same hyperparameters for all comparison models across all domains. We train our models on each domain and evaluate them both on the in-domain test set as well as on the test sets of all other domains to evaluate their out-of-domain generalization ability. Baseline Models As baselines, we compare against i) a single-task model only trained on chunking; ii) the low supervision model by<cite> [25]</cite> , which predicts the auxiliary task at the first layer; iii) an MTL model based on hard parameter sharing [6] ; and iv) cross-stitch networks [21] .",
  "y": "uses"
 },
 {
  "id": "7f234ecfb4cf880502faa8b89cd07b_8",
  "x": "Figure 2 presents the final \u03b1 weights in the sluice networks for Chunking, NER, and SRL, trained with newswire as training data. We see that a) for the low-level simplified SRL, there is more sharing at inner layers, which is in line with<cite> [25]</cite> , while Chunking and NER also rely on the outer layer, and b) more information is shared from the more complex target tasks than vice versa. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "7f234ecfb4cf880502faa8b89cd07b_9",
  "x": "We verify this by computing a learning curve for random relabelings of 200 sentences annotated with syntactic chunking brackets, as well as 100 gold standard POS-annotated sentences. The figure in 3 shows that hard parameter sharing, while learning faster because of the smoother loss surface in multi-task learning, is a good regularizer, confirming the findings in<cite> [25]</cite> , whereas the sluice network is even better at fitting noise than the single-task models. While ability to fit noise is not necessarily a problem [32] , this means that it can be beneficial to add inductive bias to the regularizer, especially when working with small amounts of data.",
  "y": "similarities"
 },
 {
  "id": "7f2383426952ddde6fd527cf0d63bd_0",
  "x": "Having a shared vectorspace multilingual translation system ( Fig.2 and Fig.3 ) able to operate on unsegmented streams of text have a number of novel applications. The most straightforward novel application is the possibility to embed the documents of all project languages into the same shared semantics vectorspace and compute document semantic similarity <cite>(Hill, Cho & Korhonen, 2016)</cite> irrespective of the document language. The sliding-window translation approach allows to view the document as a sequence (trace) of vectors corresponding to every slidingwindow step while translating the document.",
  "y": "background"
 },
 {
  "id": "7f2383426952ddde6fd527cf0d63bd_1",
  "x": "It is still an open issue which vectorspace projections yield the semantically best clusters <cite>(Hill, Cho & Korhonen, 2016)</cite> and further experiments are needed. Particularly for storyline (Rissen et al., 2013) clustering the signals for the stories belonging to the same storyline might be not so much the semantic similarity of the articles (they might report various developments of the storyline from differing viewpoints), but rather the matching time and location as well as same organizations and people being involved -the information typically supplied by Named Entity Linking (NEL) tools. The tradeoffs between semantic clustering quality and computational complexity are likely to be crucial.",
  "y": "future_work"
 },
 {
  "id": "8084b5077b2a8db755b1bbd0f6fe60_0",
  "x": "There are some interesting problems around written language identification that have attracted some attention recently, as native language identification (NLI, Tetreault et al., 2013) , the identification of the country of origin or the discrimination between similar or closely related languages (DSL, <cite>Tiedemann and Ljube\u0161i\u0107, 2012)</cite> . LI has reached a great success in discriminating between languages with unique character sets and languages belonging to different language groups or typologically distant. However, according to Zampieri (2013) , multilingualism, noisy or non-standard features in text and discrimination between similar languages, varieties or dialects remain as the major known bottlenecks in language identification.",
  "y": "background"
 },
 {
  "id": "8084b5077b2a8db755b1bbd0f6fe60_1",
  "x": "According to<cite> Tiedemann and Ljube\u0161i\u0107 (2012)</cite> , character-based n-gram methods fail for languages with a high lexical overlap, since the more shared words between two languages, the more similar will their n-gram character frequency profiles be. Table 1 : Macro-averaged Precision, Recall and F 1 -score on the DSL training dataset resulting from 10-fold cross-validation using the best model for each group of languages o varieties. Model has a letter code indicating the kind of elements considered: C (characters), T (tokens), L (tokens from the list of the 10,000 most frequent tokens), and a number indicating how many consecutive elements have been taken in a feature: 1 (unigrams), 1-2 (unigrams and bigrams), 1-5 (sequences of length one to five).",
  "y": "background"
 },
 {
  "id": "8084b5077b2a8db755b1bbd0f6fe60_2",
  "x": "Grefenstette (1995) compared this approach to Ingle (1978) , based on the frequency of short words. The interested reader is referred to Zampieri (2013) for a review of some statistical and machine learning proposals and to both Baldwin and Lui (2010) and Lui and Baldwin (2011) for an overview of some linguistically motivated models. As Baldwin and Lui (2010) or<cite> Tiedemann and Ljube\u0161i\u0107 (2012)</cite> point out, language identification is erroneously considered an easy and solved problem 2 , in part because of some general purpose systems being available, notably TextCat 3 , Xerox Language Identifier 4 and, more recently, langid.py (Lui and Baldwin, 2012) .",
  "y": "background"
 },
 {
  "id": "8084b5077b2a8db755b1bbd0f6fe60_3",
  "x": "Texts classified as Malay or Indonesian are subsequently scanned for some linguistic features (format of numbers and exclusive words), yielding a more precise performance than TextCat. Ljube\u0161i\u0107 et al. (2007) also propose a cascaded identifier that relies on 'black lists' to discard nonBalkan languages and a second order Markov model on n-grams to discriminate among them, augmented with a 'black list' component that raises accuracy up to 0.99 when dealing with the most difficult pair (Croatian and Serbian). This work is followed up in<cite> Tiedemann and Ljube\u0161i\u0107 (2012)</cite> where 9% of improvement over standard approaches is reported and where support for Bosnian discrimination is included.",
  "y": "background"
 },
 {
  "id": "8084b5077b2a8db755b1bbd0f6fe60_4",
  "x": "We will also refer to the lists of the 10,000 most frequent words as 'white list', which have a complementary role to the 'black lists' of<cite> Tiedemann and Ljube\u0161i\u0107 (2012)</cite> . To determine which features are best suited to each group, we measured their performance using tenfold cross-validation on the training dataset and using the development dataset for testing. For group A, best results were obtained using bag of features consisting of variable length character n-grams ranging from one to five (C1-5).",
  "y": "similarities"
 },
 {
  "id": "8084b5077b2a8db755b1bbd0f6fe60_5",
  "x": "There is, however, a Portuguese-specific issue: some texts obey the 1990 Orthographic Agreement 7 which blurs the orthographic distinctions regarding diacritics or consonant clusters; in fact, one sentence contains words following both standards (perspectiva and reprodu\u00e7\u00e3o). It remains unexplained why word bigrams did not capture the Brazilian preference for passive voice (foram rebaixados), auxiliary + gerund chunks (estamos utilizando) or clitic dropping (lembro). Despite findings by<cite> Tiedemann and Ljube\u0161i\u0107 (2012)</cite> , character n-grams performed better during tenfold cross-validation on the training dataset for different feature settings on the DSL dataset for group A (Bosnian, Croatian and Serbian).",
  "y": "differences"
 },
 {
  "id": "8084b5077b2a8db755b1bbd0f6fe60_6",
  "x": "Focusing on Spanish language, we plan to geographically expand the classifier to deal with all national varieties, a much harder task as both Baldwin and Lui (2010) and remark. Moreover, the classifier could be used, as<cite> Tiedemann and Ljube\u0161i\u0107 (2012)</cite> suggest, to learn varieties discriminators to label texts beyond national classes (e.g. both Caribbean and Andean Spanish cross-cut national borders and, conversely, nations involved are known not to be dialectally uniform). Given that error analysis showed that word bigrams fail to capture certain syntactical idiosyncrasies, a model with longer n-grams and/or knowledge-richer features such as POS sequences could also be explored, although report lower performance than knowledge-poor features.",
  "y": "similarities"
 },
 {
  "id": "8084b5077b2a8db755b1bbd0f6fe60_7",
  "x": [
   "A diachronic expansion, such as Trieschnigg et al. (2012) , is also in mind. Medieval Castilian coexisted with other Romance varieties such as Leonese or Aragonese whose features permeated Castilian texts. Researchers are in need of a tool to properly classify diachronic texts to accurately describe older stages of Spanish."
  ],
  "y": "uses"
 },
 {
  "id": "809ad258132199e3eae8add5d1bfdf_1",
  "x": "Unlike image captioning, answering questions requires the ability to identify specific details in the image (e.g. color of an object, or activity of a person). There are several recently proposed VQA datasets on real images e.g.<cite> [2,</cite> 24, 25, 14, 29] , as well as on abstract scenes <cite>[2]</cite> . The latter allows research on semantic reasoning without first requiring the development of highly accurate detectors.",
  "y": "background"
 },
 {
  "id": "809ad258132199e3eae8add5d1bfdf_2",
  "x": "For example, in the VQA dataset (with images from MS COCO) <cite>[2]</cite> , the most common sport answer \"tennis\" is the correct answer for 41% of the questions starting with \"What sport is\". Similarly, \"white\" alone is the correct answer for 23% of the questions starting with \"What color are the\". Almost half of all questions in the VQA datatset <cite>[2]</cite> can be answered correctly by a neural network that ignores the image completely and uses the question alone, relying on systematic regularities in the kinds of questions that are asked and what answers they tend to have.",
  "y": "background"
 },
 {
  "id": "809ad258132199e3eae8add5d1bfdf_3",
  "x": "Second, binary questions are easier to evaluate than open-ended questions. Although our approach of visual verification is applicable to real images (more discussion in Sec. 6), we choose to use abstract images<cite> [2,</cite> 3, 39, 38, 40] as a test bed because abstract scene images allow us to focus on high-level semantic reasoning. They also allow us to balance the dataset by making changes to the images, something that would be difficult or impossible with real images.",
  "y": "uses"
 },
 {
  "id": "809ad258132199e3eae8add5d1bfdf_4",
  "x": "Our main contributions are as follows: (1) We balance the existing abstract binary VQA dataset <cite>[2]</cite> by creating complementary scenes so that all questions 1 have an answer of \"yes\" for one scene and an answer of \"no\" for another closely related scene. We show that a languageonly approach performs significantly worse on this balanced dataset. (2) We propose an approach that summarizes the content of the question in a tuple form which concisely describes the visual concept whose existence is to be verified in the scene.",
  "y": "uses"
 },
 {
  "id": "809ad258132199e3eae8add5d1bfdf_5",
  "x": "**RELATED WORK** Visual question answering. Recent work has proposed several datasets and methods to promote research on the task of visual question answering [15, 4, 33, 24,<cite> 2,</cite> 25, 14, 29] , ranging from constrained settings [15, 24, 29] to freeform natural language questions and answers [4, 33,<cite> 2,</cite> 25, 14] .",
  "y": "background"
 },
 {
  "id": "809ad258132199e3eae8add5d1bfdf_6",
  "x": "A number of recent papers<cite> [2,</cite> 14, 25, 29] proposed neural network models for VQA composing LSTMs (for questions) and CNNs (for images). <cite>[2]</cite> introduced a large-scale dataset for free-form and open-ended VQA, along with several natural VQA models. [4] uses crowdsourced workers to answer questions about visual content asked by visually-impaired users.",
  "y": "background"
 },
 {
  "id": "809ad258132199e3eae8add5d1bfdf_7",
  "x": "We first describe the VQA dataset for abstract scenes collected by <cite>[2]</cite> . We then describe how we balance this dataset by collecting more scenes. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "809ad258132199e3eae8add5d1bfdf_8",
  "x": "**DATASETS** We first describe the VQA dataset for abstract scenes collected by <cite>[2]</cite> . We then describe how we balance this dataset by collecting more scenes.",
  "y": "extends"
 },
 {
  "id": "809ad258132199e3eae8add5d1bfdf_12",
  "x": "We have extracted PRS tuples and aligned PS to the clipart objects in the image, we can now compute a score indicating the strength of visual evidence for the concept inquired in the question. Our scoring function measures compatibility between image and text features (described in Sec. 4.4). Our model is an ensemble of two similar models-Q-model and Tuple-model, whose common architecture is inspired from a recently proposed VQA approach <cite>[2]</cite> .",
  "y": "uses"
 },
 {
  "id": "809ad258132199e3eae8add5d1bfdf_13",
  "x": "SOTA Q+Tuple+H-IMG: This VQA model has a similar architecture as our approach, except that it uses holistic image features (H-IMG) that describe the entire scene layout, instead of focusing on specific regions in the scene as determined by P and S. This model is analogous to the state-ofthe-art models presented in<cite> [2,</cite> 25, 29, 14] , except applied to abstract scenes. These holistic features include a bag-of-words for clipart objects occurrence (150-dim), human expressions (8-dim), and human poses (7-dim). The 7 human poses refer to 7 clusters obtained by clustering all the human pose vectors (concatenation of (x, y) locations and global angles of all 15 deformable parts of human body) in the training set.",
  "y": "similarities"
 },
 {
  "id": "809ad258132199e3eae8add5d1bfdf_15",
  "x": "Note that the accuracy is higher than 50% because this is not binary classification accuracy but the VQA accuracy <cite>[2]</cite> , which provides partial credit when there is inter-human disagreement in the ground-truth answers. Attention helps. When trained on balanced dataset (where language biases are absent), our model Q+Tuple+A-IMG is able to outperform all baselines by a significant margin.",
  "y": "uses"
 },
 {
  "id": "809ad258132199e3eae8add5d1bfdf_16",
  "x": "When trained on balanced dataset (where language biases are absent), our model Q+Tuple+A-IMG is able to outperform all baselines by a significant margin. Specifically, our model gives improvement in performance relative to the state-of-the-art VQA model from <cite>[2]</cite> (Q+Tuple+H-IMG), showing that attending to relevant regions and describing them in detail helps, as also seen in Sec. 5.2. Role of balancing.",
  "y": "differences"
 },
 {
  "id": "809ad258132199e3eae8add5d1bfdf_17",
  "x": "The results of the baselines and our model, trained on balanced and unbalanced datasets, are shown in Table 3 . We observe that our model trained on the balanced dataset performs the best. And again, our model that focuses on relevant regions in the image to answer the question outperforms the state-of-the-art approach of <cite>[2]</cite> (Q+Tuple+H-IMG) that does not model attention. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "80e3aec943c37927050f97459360b4_0",
  "x": "What the acoustic modeling cares is the output of word or phoneme sequence, instead of the frame-by-frame labeling which the traditional CE training criterion optimizes. Hence, the Connectionist Temporal Classification (CTC) approach [15] [16] <cite>[17]</cite> [18] was introduced to map the speech input frames into an output label sequence. The building network is still a LSTM-RNN, but the training objective function is changed from CE to CTC.",
  "y": "background"
 },
 {
  "id": "80e3aec943c37927050f97459360b4_1",
  "x": "The most attractive characteristics of CTC is that it provides a path to end-to-end optimization of acoustic models. In the deep speech [19] [20] and EESEN [21] [22] work, the end-to-end speech recognition system was explored to directly predict characters instead of phonemes, hence removing the need of using lexicons and decision trees which are the building blocks in <cite>[17]</cite> [18] . This is one step toward removing expert knowledge when building an ASR system.",
  "y": "background"
 },
 {
  "id": "80e3aec943c37927050f97459360b4_2",
  "x": "This is one step toward removing expert knowledge when building an ASR system. As the goal of ASR is to generate a word sequence from the speech acoustic, word unit is the most natural output unit for network modeling. In <cite>[17]</cite> , the CTC with up to 27 thousand (k) word output targets was explored but the ASR accuracy is not very good, partially due to the high out-of-vocabulary (OOV) rate when using only around 3k hours training data.",
  "y": "background"
 },
 {
  "id": "80e3aec943c37927050f97459360b4_3",
  "x": "There are two challenges to the word-based CTC. The first one is the OOV issue. In <cite>[17]</cite> [23] [24] , only frequent words in the training set are used as the targets and the remaining words are just tagged as the OOV.",
  "y": "background"
 },
 {
  "id": "80e3aec943c37927050f97459360b4_4",
  "x": "In this study, we focus on how to improve the CTC with word output units. There are two challenges to the word-based CTC. The first one is the OOV issue. In <cite>[17]</cite> [23] [24] , only frequent words in the training set are used as the targets and the remaining words are just tagged as the OOV.",
  "y": "motivation"
 },
 {
  "id": "80e3aec943c37927050f97459360b4_5",
  "x": "**CTC MODELING** The CTC criterion [15] was introduced to map the speech input frames into an output label sequence [16] <cite>[17]</cite> [18] . To deal with the issue that the number of output labels is smaller than that of input speech frames, CTC introduces a special blank label and allows the repetition of labels to map the label sequence into a CTC path, which forces the output and input sequences to have the same length.",
  "y": "background"
 },
 {
  "id": "80e3aec943c37927050f97459360b4_6",
  "x": "With the conditional independent assumption, ( | ) can be decomposed into a product of posterior from each frame as The calculation of ( | ) is done via the forward-backward process in [15] . The CTC output labels can be phonemes [16] <cite>[17]</cite> [18] , characters [19] [20] [21] [22] [36] or even words <cite>[17]</cite> [23] [24] .",
  "y": "background"
 },
 {
  "id": "80e3aec943c37927050f97459360b4_7",
  "x": "Then, we built a phoneme-based LSTM model trained with the CTC criterion, modeling around 6000 tied contextdependent phonemes. It has the same 5-layer LSTM structure with projection layer as the previous LSTM-CE model. Eight frames of 80-dim log Mel-filter-bank features are stacked together as the input, and the time step shift is three frames as in <cite>[17]</cite> .",
  "y": "uses"
 },
 {
  "id": "80e3aec943c37927050f97459360b4_8",
  "x": "The WER gap is consistent with what has been observed in <cite>[17]</cite> [24] . All the CTC models except the phoneme-based CTC model in this study purely rely on the network score to generate outputs without using LM. We use the structure in Figure 1 to build hybrid CTC models.",
  "y": "similarities"
 },
 {
  "id": "81499fd759b958a0c02d9ed9d72a46_0",
  "x": "We propose several algorithms to map bilingual word embeddings to the same vector space, either during training or during post-processing. We apply a linear transformation to map the English side of each pretrained crosslingual word embedding to the same space. We also extend <cite>Duong et al. (2016)</cite> , which used a lexicon to learn bilingual word embeddings.",
  "y": "extends"
 },
 {
  "id": "81499fd759b958a0c02d9ed9d72a46_1",
  "x": "As a simple solution, we propose a simple post hoc method by mapping the English parts of each bilingual word embedding to each other. In this way, the mapping is always exact and one-to-one. <cite>Duong et al. (2016)</cite> constructed bilingual word embeddings based on monolingual data and PanLex.",
  "y": "background"
 },
 {
  "id": "81499fd759b958a0c02d9ed9d72a46_2",
  "x": "<cite>Duong et al. (2016)</cite> constructed bilingual word embeddings based on monolingual data and PanLex. In this way, <cite>their</cite> approach can be applied to more languages as PanLex covers more than a thousand languages. <cite>They</cite> solve the polysemy problem by integrating an EM algorithm for selecting a lexicon. Relative to many previous crosslingual word embeddings, <cite>their</cite> joint training algorithm achieved state-of-the-art performance for the bilingual lexicon induction task, performing significantly better on monolingual similarity and achieving a competitive result on cross lingual document classification.",
  "y": "background"
 },
 {
  "id": "81499fd759b958a0c02d9ed9d72a46_3",
  "x": "<cite>Duong et al. (2016)</cite> constructed bilingual word embeddings based on monolingual data and PanLex. Here we also adopt <cite>their</cite> approach, and extend it to multilingual embeddings.",
  "y": "extends uses"
 },
 {
  "id": "81bdddc7d6b04c88407537f57c0580_0",
  "x": "In this paper, we study the critical yet under-addressed Answer Triggering<cite> (Yang et al., 2015)</cite> problem: Given a question and a set of answer candidates, determine whether the candidate set contains any correct answer, and if so, select a correct answer as system output. The answer triggering problem can be logically divided into two sub-problems: P 1 : Build an individual-level model to rank answer candidates so that a correct one (if it exists) gets the highest score. P 2 : Make a group-level binary prediction on the existence of correct answers within the candidate set.",
  "y": "uses"
 },
 {
  "id": "81bdddc7d6b04c88407537f57c0580_1",
  "x": "The answer triggering problem can be logically divided into two sub-problems: P 1 : Build an individual-level model to rank answer candidates so that a correct one (if it exists) gets the highest score. P 2 : Make a group-level binary prediction on the existence of correct answers within the candidate set. Previous work <cite>(Yang et al., 2015</cite>; Jurczyk et al., 2016) attack the problem via a pipeline approach: First solve P 1 as a ranking task and then solve P 2 by choosing an optimal threshold upon the previous step's highest ranking score.",
  "y": "background"
 },
 {
  "id": "81bdddc7d6b04c88407537f57c0580_2",
  "x": "Previous work <cite>(Yang et al., 2015</cite>; Jurczyk et al., 2016) attack the problem via a pipeline approach: First solve P 1 as a ranking task and then solve P 2 by choosing an optimal threshold upon the previous step's highest ranking score. We propose Group-level Answer Triggering (GAT), an end-to-end framework for jointly optimizing P 1 and P 2 .",
  "y": "differences"
 },
 {
  "id": "81bdddc7d6b04c88407537f57c0580_3",
  "x": "Our key contribution in GAT is a novel group-level objective function, which aggregates individual-level information and penalizes three potential error types in answer triggering as a group-level task. By optimizing this objective function, we can directly back-propagate the final answer triggering errors to the entire framework and learn all the parameters simultaneously. We conduct evaluation using the same dataset and measure as in previous work <cite>(Yang et al., 2015</cite>; Jurczyk et al., 2016) , and our framework improves the F 1 score by 6.6% (from 36.65% to 43.27%), compared with the state of the art.",
  "y": "uses"
 },
 {
  "id": "81bdddc7d6b04c88407537f57c0580_4",
  "x": "---------------------------------- **EXPERIMENTS 3.1 DATASET** We use the WIKIQA dataset<cite> (Yang et al., 2015)</cite> for evaluation.",
  "y": "uses"
 },
 {
  "id": "81bdddc7d6b04c88407537f57c0580_5",
  "x": "The results are summarized in Table 1 . We can see that GAT combined with Cnt features improves the F 1 score from<cite> Yang et al. (2015)</cite> and Jurczyk et al. (2016) by around 11.1% and 6.6% (from 32.17 and 36.65 to 43.27), which shows the effectiveness of our framework. We denote this configuration as our full framework.",
  "y": "uses"
 },
 {
  "id": "81bdddc7d6b04c88407537f57c0580_6",
  "x": "We denote this configuration as our full framework. Through the comparison between Naive and GAT, Model Prec Rec F1<cite> (Yang et al., 2015)</cite> 27.96 37.86 32.17 (Jurczyk et al., 2016) we can see that our proposed objective function has a great advantage over the Naive one which does not model the complexity of answer triggering for positive candidate sets. Different from<cite> Yang et al. (2015)</cite>'s results, combining with the QLen feature does not further improve the performance in our case, possibly because we choose Bi-RNN as our encoder, which may capture some question characteristics better than a length feature.",
  "y": "differences"
 },
 {
  "id": "81bdddc7d6b04c88407537f57c0580_7",
  "x": "Through the comparison between Naive and GAT, Model Prec Rec F1<cite> (Yang et al., 2015)</cite> 27.96 37.86 32.17 (Jurczyk et al., 2016) we can see that our proposed objective function has a great advantage over the Naive one which does not model the complexity of answer triggering for positive candidate sets. Different from<cite> Yang et al. (2015)</cite>'s results, combining with the QLen feature does not further improve the performance in our case, possibly because we choose Bi-RNN as our encoder, which may capture some question characteristics better than a length feature. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "81bdddc7d6b04c88407537f57c0580_8",
  "x": "Since the code from<cite> (Yang et al., 2015)</cite> is available, we use it (rather than (Jurczyk et al., 2016) ) to assist our analysis. We first test a variant of our full framework by replacing the Encoder and QA Matching component with the CNN based model from<cite> (Yang et al., 2015)</cite> 2 , denoted as GAT w/ CNN, and train it with our objective. From the first two rows in Table 2 , we observe that: (1) Using our current design Bi-RNN and feed-foward NN improves from 35.03% to 43.27%, in comparison with the CNN based model, partly because their CNN only consists of one convolution layer and one average pooling layer.",
  "y": "uses"
 },
 {
  "id": "81bdddc7d6b04c88407537f57c0580_9",
  "x": "Now we conduct further analysis in order to better understand the contribution of each component in our full framework. Since the code from<cite> (Yang et al., 2015)</cite> is available, we use it (rather than (Jurczyk et al., 2016) ) to assist our analysis. We first test a variant of our full framework by replacing the Encoder and QA Matching component with the CNN based model from<cite> (Yang et al., 2015)</cite> 2 , denoted as GAT w/ CNN, and train it with our objective.",
  "y": "uses"
 },
 {
  "id": "81bdddc7d6b04c88407537f57c0580_10",
  "x": "We first test a variant of our full framework by replacing the Encoder and QA Matching component with the CNN based model from<cite> (Yang et al., 2015)</cite> 2 , denoted as GAT w/ CNN, and train it with our objective. From the first two rows in Table 2 , we observe that: (1) Using our current design Bi-RNN and feed-foward NN improves from 35.03% to 43.27%, in comparison with the CNN based model, partly because their CNN only consists of one convolution layer and one average pooling layer. However, we leave more advanced encoder and QA matching design for future work, and anticipate that more complex CNN based models can achieve similar or better results than our current design, as in many other QA-related work (Hu et al., 2014; . (2) Compared with the best result from<cite> (Yang et al., 2015)</cite> in Table 1 , training the CNN based model end-to-end using our objective improves from 32.17% to 35.03%.",
  "y": "differences"
 },
 {
  "id": "81bdddc7d6b04c88407537f57c0580_11",
  "x": "However, we leave more advanced encoder and QA matching design for future work, and anticipate that more complex CNN based models can achieve similar or better results than our current design, as in many other QA-related work (Hu et al., 2014; . (2) Compared with the best result from<cite> (Yang et al., 2015)</cite> in Table 1 , training the CNN based model end-to-end using our objective improves from 32.17% to 35.03%. This directly shows an end-to-end learning strategy works better than the pipeline approach in<cite> (Yang et al., 2015)</cite> . Now we detach the Encoder component ENC 2 Where the QA matching score is obtained first through CNN encoding and then a bilinear model.",
  "y": "differences"
 },
 {
  "id": "81bdddc7d6b04c88407537f57c0580_12",
  "x": "from our end-to-end full framework. To obtain semantic vectors of questions and candidate answers as input to the subsequent QA Matching component, we leverage<cite> Yang et al.(2015)</cite> 's released code to train the Encoder component (with CNN) through their well-tuned individual-level optimization, and use their learnt semantic vectors. Then our framework without ENC, i.e., -ENC, is trained and tested as before.",
  "y": "uses"
 },
 {
  "id": "81bdddc7d6b04c88407537f57c0580_13",
  "x": "Then our framework without ENC, i.e., -ENC, is trained and tested as before. We further detach the QA matching component QAM in a similar way: We directly use the matching score between a question and a candidate answer obtained by<cite> Yang et al. (2015)</cite> , and concatenate it with Cnt features as input to the Softmax layer, which is our framework without ENC or QAM, denoted as -ENC -QAM, and trained by our group-level objective. By comparing them with our end-to-end frameworks on both dev and test sets, we can see that it is beneficial to jointly train the entire framework.",
  "y": "uses"
 },
 {
  "id": "81dd7a27479f0cec3a01337c57ca95_0",
  "x": "**INTRODUCTION** Text compression can be applied to various tasks such as Summarization, Text Editing and even Data Augmentation where the compressed text can be employed as additional training examples. However, almost all existing approaches require either parallel data, hand-crafted rules, or extra syntactic information such as dependency labels or part-of-speech tag features trees (McDonald, 2006; Filippova and Strube, 2008;<cite> Zhao et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "81dd7a27479f0cec3a01337c57ca95_1",
  "x": "Google sentence compression dataset (Filippova and Altun, 2013) (2015), we used the first 1, 000 sentences of evaluation set as our test set. The Gigaword dataset (Napoles et al., 2012) with 1.02 million examples, where the first 200 are labeled for extractive Sentence Compression by two annotators <cite>(Zhao et al., 2018)</cite> . 5 Automatic and Human Evaluation Following Wang et al. (2017) , we use the ground truth compressed sentences to compute F1 scores.",
  "y": "uses"
 },
 {
  "id": "81dd7a27479f0cec3a01337c57ca95_2",
  "x": "Although F1 can be indicative of how well a compression model performs, we note that their could be multiple viable compressions for the same sentence, which single-reference ground-truth cannot cover (Handler et al., 2019) . Thus to faithfully evaluate the quality of the compressions generated by our model, we follow<cite> Zhao et al. (2018)</cite> and conducted human studies on the Gigaword dataset with Amazon Mechanical Turk (MTurk). 6 Specifically, we sample 150 examples from the test set, and put our model output side-by-side with the two compressions created by the two annotators.",
  "y": "uses"
 },
 {
  "id": "81dd7a27479f0cec3a01337c57ca95_3",
  "x": "We hire annotators who have an approval rate of at least 98% and 10, 000 approved HITs. Following <cite>(Zhao et al., 2018)</cite> , we employed Readability and Informativeness as criteria on a five-point Likert scale. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "81dd7a27479f0cec3a01337c57ca95_4",
  "x": "**RESULTS AND ANALYSIS** Automatic Evaluation We report F1 scores on both Google and Gigaword dataset. Since each of the 200 sentences from Giga test set has two references (by Annotator 1 and 2, respectively), we report two F1's following<cite> Zhao et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "81dd7a27479f0cec3a01337c57ca95_5",
  "x": "Similar to our AvgPPL,<cite> Zhao et al. (2018)</cite> also employed average Perplexity (though without our length correction terms) as the reward to a policy network trained with reinforcement learning. Another characteristic of our model is that we obtain a sequence of sentences which are all valid compressions of the original one, while other models usually generate only one compression. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "820fa732cc4cedf2d5d94b2afb90fc_0",
  "x": "The Transformer neural sequence model <cite>[Vaswani et al., 2017]</cite> has emerged as a popular alternative to recurrent sequence models. Transformer relies on attention layers to communicate information between and across sequences. One major challenge with Transformer is the speed of incremental inference.",
  "y": "background"
 },
 {
  "id": "820fa732cc4cedf2d5d94b2afb90fc_1",
  "x": "---------------------------------- **MULTI-HEAD ATTENTION** The \"Transformer\" seuqence-to-sequence model <cite>[Vaswani et al., 2017]</cite> uses h different attention layers (heads) in parallel, which the authors refer to as \"Multi-head attention\".",
  "y": "background"
 },
 {
  "id": "820fa732cc4cedf2d5d94b2afb90fc_2",
  "x": "einsum ( \" hv , hdv\u2212>d \" , o , P_o) r e t u r n y Note: <cite>[Vaswani et al., 2017]</cite> include a constant scaling factor on the logits. We omit this in our code, as it can be folded into the linear projections P q or P k . ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "820fa732cc4cedf2d5d94b2afb90fc_3",
  "x": "In addition, we process a batch of b different non-interacting sequences at once. Following <cite>[Vaswani et al., 2017]</cite> , in an autoregressive model, we can prevent backward-information-flow by adding a \"mask\" to the logits containing the value \u2212\u221e in the illegal positions. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "820fa732cc4cedf2d5d94b2afb90fc_4",
  "x": "To simplify the performance analysis, we will make several simplifying assumptions: h , as suggested by <cite>[Vaswani et al., 2017]</cite> The total number of arithmetic operations is \u0398(bnd 2 ).",
  "y": "uses similarities"
 },
 {
  "id": "820fa732cc4cedf2d5d94b2afb90fc_5",
  "x": "**MULTIHEAD ATTENTION (INCREMENTAL)** In some settings, data dependencies make it is impossible to process queries from multiple positions in parallel. An example is a self-attention layer in an autoregressive language model such as Transformer <cite>[Vaswani et al., 2017]</cite> .",
  "y": "similarities"
 },
 {
  "id": "820fa732cc4cedf2d5d94b2afb90fc_6",
  "x": "We introduce multi-query Attention as a variation of multi-head attention as described in <cite>[Vaswani et al., 2017]</cite> . Multi-head attention consists of multiple attention layers (heads) in parallel with different linear transformations on the queries, keys, values and outputs. Multi-query attention is identical except that the different heads share a single set of keys and values.",
  "y": "similarities uses"
 },
 {
  "id": "820fa732cc4cedf2d5d94b2afb90fc_7",
  "x": "**EXPERIMENTAL SETUP** Following <cite>[Vaswani et al., 2017]</cite> , we evaluate on the WMT 2014 English-German translation task. As a baseline, we use an encoder-decoder Transformer model with 6 layers, using d model = 1024 d f f = 4096, h = 8, d k = d v = 128, learned positional embeddings, and weight-sharing between the token-embedding and output layers.",
  "y": "uses similarities"
 },
 {
  "id": "822b2010b07d3e1103f904fa45388a_0",
  "x": "**ABSTRACT** Neural Machine Translation (NMT) is known to outperform Phrase Based Statistical Machine Translation (PBSMT) for resource rich language pairs but not for resource poor ones. Transfer Learning <cite>(Zoph et al., 2016 )</cite> is a simple approach in which we can simply initialize an NMT model (child model) for a resource poor language pair using a previously trained model (parent model) for a resource rich language pair where the target languages are the same.",
  "y": "motivation"
 },
 {
  "id": "822b2010b07d3e1103f904fa45388a_1",
  "x": "One of the most attractive features of Neural Machine Translation (NMT) (Bahdanau et al., 2015; Cho et al., 2014; Sutskever et al., 2014) is that it is possible to train an end to end system without the need to deal with word alignments, phrase tables and complicated decoding algorithms which are a characteristic of Phrase Based Statistical Machine Translation (PBSMT) systems (Koehn et al., 2003) . It is reported that NMT works better than PBSMT only when there is an abundance of parallel corpora. In the case of low resource languages like Hausa, vanilla NMT is either worse than or comparable to PBSMT <cite>(Zoph et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "822b2010b07d3e1103f904fa45388a_2",
  "x": "However, it is possible to use a previously trained X-Y model (parent model; X-Y being the resource rich language pair where X and Y represent the source and target languages respectively) to initialize the parameters of a Z-Y model (child model; Z-Y being the resource poor language pair) leading to significant improvements <cite>(Zoph et al., 2016)</cite> for the latter. This paper is about an empirical study of transfer learning for NMT for low resource languages. Our main focus is on translation to English for the following low resource languages: Hausa, Uzbek, Marathi, Malayalam, Punjabi, Malayalam, Kazakh, Luxembourgish, Javanese and Sundanese.",
  "y": "background"
 },
 {
  "id": "822b2010b07d3e1103f904fa45388a_3",
  "x": "---------------------------------- **RELATED WORK** Transfer learning for NMT <cite>(Zoph et al., 2016)</cite> is an approach where previously trained NMT models for French and German to English (resource rich pairs) were used to initialize models for Hausa, Uzbek, Spanish to English (resource poor pairs).",
  "y": "background"
 },
 {
  "id": "822b2010b07d3e1103f904fa45388a_4",
  "x": "It is essentially the same as described in <cite>(Zoph et al., 2016)</cite> where we learn a model (parent model) for a resource rich language pair (Hindi-English) and use it to initialize the model (child model) for the resource poor pair (Marathi-English). Henceforth the source languages of the parent model and 282 child models will be known as parent and child languages respectively and the corresponding language pairs will be known as the parent and child language pairs respectively. The target language vocabulary (English) should be the same for both the parent and the child models.",
  "y": "similarities"
 },
 {
  "id": "822b2010b07d3e1103f904fa45388a_5",
  "x": "Refer to Figure 1 for an overview of the method. It is essentially the same as described in <cite>(Zoph et al., 2016)</cite> where we learn a model (parent model) for a resource rich language pair (Hindi-English) and use it to initialize the model (child model) for the resource poor pair (Marathi-English). Henceforth the source languages of the parent model and 282 child models will be known as parent and child languages respectively and the corresponding language pairs will be known as the parent and child language pairs respectively.",
  "y": "similarities uses"
 },
 {
  "id": "822b2010b07d3e1103f904fa45388a_6",
  "x": "All of our experiments were performed using an encoder-decoder NMT system with attention for the various baselines and transfer learning experiments. We used an in house NMT system developed using the Tensorflow (Abadi et al., 2015) framework so as to exploit multiple GPUs to speed up training. To ensure replicability we use the same NMT model design as in <cite>the original work</cite> <cite>(Zoph et al., 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "822b2010b07d3e1103f904fa45388a_7",
  "x": "Our choice of languages was influenced by two factors: \u2022 a. We wanted to replicate the basic transfer learning results <cite>(Zoph et al., 2016)</cite> and hence chose French, German for Hausa and Uzbek. \u2022 b. We wanted to compare the effects of using parent languages belonging to the same lan",
  "y": "motivation"
 },
 {
  "id": "822b2010b07d3e1103f904fa45388a_8",
  "x": "\u2022 Opportunistic experimentation on 4 child languages (Kazakh, Javanese, Sundanese and Luxembourgish) by using 3 parent languages out of which one is from the same language family and the other two are from another language family. Turkish being the related language for Kazakh, German for Luxembourgish and Indonesian for Javanese and Sundanese. The model and training details are the same as that in <cite>the original work</cite> <cite>(Zoph et al., 2016)</cite> Note that the target language (English) vocabulary is same for all settings and the WPM is learned on the English side of the French-English corpus since it is the largest one amongst all our pairs.",
  "y": "similarities"
 },
 {
  "id": "822b2010b07d3e1103f904fa45388a_9",
  "x": "The model and training details are the same as that in <cite>the original work</cite> <cite>(Zoph et al., 2016)</cite> Note that the target language (English) vocabulary is same for all settings and the WPM is learned on the English side of the French-English corpus since it is the largest one amongst all our pairs. We deliberately chose this since we wished to maintain the same target side vocabulary for all our experiments (both baseline and transfer) for fair comparison. The parent source vocabulary (and hence embeddings) is randomly mapped to child source vocabulary since it was shown that NMT is less sensitive to it <cite>(Zoph et al., 2016</cite>",
  "y": "motivation"
 },
 {
  "id": "822b2010b07d3e1103f904fa45388a_10",
  "x": "Transfer learning for NMT <cite>(Zoph et al., 2016)</cite> is an approach where previously trained NMT models for French and German to English (resource rich pairs) were used to initialize models for Hausa, Uzbek, Spanish to English (resource poor pairs). They showed that French-English as a parent model was better than German-English when trying to improve the Spanish-English translation quality (since Spanish is linguistically closer to French than German) but they did not conduct an exhaustive investigation for multiple language pairs. In this paper we extend this work to explore how language relatedness impacts transfer learning.",
  "y": "background"
 },
 {
  "id": "822b2010b07d3e1103f904fa45388a_11",
  "x": "Transfer learning for NMT <cite>(Zoph et al., 2016)</cite> is an approach where previously trained NMT models for French and German to English (resource rich pairs) were used to initialize models for Hausa, Uzbek, Spanish to English (resource poor pairs). They showed that French-English as a parent model was better than German-English when trying to improve the Spanish-English translation quality (since Spanish is linguistically closer to French than German) but they did not conduct an exhaustive investigation for multiple language pairs. In this paper we extend this work to explore how language relatedness impacts transfer learning.",
  "y": "extends"
 },
 {
  "id": "836992d035c4be0c8eacdd419f151e_0",
  "x": "These approaches typically focus on aggregating information from a mix of sources, including long-range information from the textual context or other linked entities. While this approach is suitable for entity linking settings such as newswire (Bentivogli, 2010) and Wikipedia (Ratinov et al., 2011) , we cannot always rely on this information in other settings like Twitter (Guo et al., 2013; Fang and Chang, 2014; Huang et al., 2014; Dredze et al., 2016) , Snapchat (Moon et al., 2018) , other web platforms<cite> (Eshel et al., 2017)</cite> , or dialogue systems (Bowden et al., 2018) . We need models that can make effective use of limited context windows in noisy settings.",
  "y": "background"
 },
 {
  "id": "836992d035c4be0c8eacdd419f151e_1",
  "x": "We need models that can make effective use of limited context windows in noisy settings. In this work, we investigate this problem of effectively using context in the setting of the WikilinksNED dataset from <cite>Eshel et al. (2017)</cite> . The examples in this dataset, which consists of 3.2 million entity disambiguation examples derived from Wikilinks (Singh et al., 2012) , have at most 20 words of context on either side and usually no other mentions of the entity being disambiguated.",
  "y": "similarities uses"
 },
 {
  "id": "836992d035c4be0c8eacdd419f151e_2",
  "x": "We build off a state-of-the-art attentive LSTM model from prior work<cite> (Eshel et al., 2017)</cite> and show that despite its good performance, it fails to resolve some examples that human readers would find trivial. For example, disambiguating the identity of the song Down in Figure 1 is easy if we can recognize the nearby string Jay Sean in the context, but the model sometimes fails to do this. We explore the performance of a standard attention mechanism as well as two modifications.",
  "y": "similarities uses"
 },
 {
  "id": "836992d035c4be0c8eacdd419f151e_3",
  "x": "We denote the mention text (i.e., anchor text of the hyperlink) by m, and denote the left and right context of the mention by c l and c r respectively; these are at most 20 words. For this dataset, we can assume that the possible linked titles for a mention have been seen in training, and the main task is instead to disambiguate between them and identify the gold title t * . We therefore follow prior work<cite> (Eshel et al., 2017)</cite> and take as candidates all gold entities in the training set whose mention was m rather than relying on a separate candidate generation scheme.",
  "y": "similarities uses"
 },
 {
  "id": "836992d035c4be0c8eacdd419f151e_4",
  "x": "We therefore follow prior work<cite> (Eshel et al., 2017)</cite> and take as candidates all gold entities in the training set whose mention was m rather than relying on a separate candidate generation scheme. Our model places a distribution over titles P (t|m, c l , c r ), where t takes values in the set of candidate Wikipedia titles for that mention. This model, depicted in Figure 1 , roughly follows that of <cite>Eshel et al. (2017)</cite> , with some key differences, as we discuss in the rest of this section.",
  "y": "extends differences"
 },
 {
  "id": "836992d035c4be0c8eacdd419f151e_5",
  "x": "Our word embeddings are trained over Wikipedia as described in the following paragraph. Embedding entities We follow the method of <cite>Eshel et al. (2017)</cite> for generating entity embeddings, using word2vecf (Levy and Goldberg, 2014) to jointly train word and entity embeddings simultaneously using Wikipedia article text. Each title t is associated in turn with each content word w in the article, yielding a set of (w, t) pairs that are consumed by the training procedure.",
  "y": "similarities uses"
 },
 {
  "id": "836992d035c4be0c8eacdd419f151e_6",
  "x": "Unlike <cite>Eshel et al. (2017)</cite>, we structure training as a multiclass decision among these titles rather than a binary prediction problem over each title as gold or not. We run our model over the candidates t \u2208 T to produce the distribution P (t|m, c l , c r ) and train to maximize the log probablity log P (t * |m, c l , c r ) of the gold title. Results The model set forth in this section is the basis for the remaining models in this paper; we call it the GRU model as that is the only context encoding mechanism it uses.",
  "y": "differences"
 },
 {
  "id": "836992d035c4be0c8eacdd419f151e_7",
  "x": "In the next section, we explore techniques for using the context in a more sophisticated way to improve further on this result. <cite>Eshel et al. (2017)</cite> , allowing the model to weight the importance of the outputs of the GRU at each time step. Each context (left and right) has its own attention weights.",
  "y": "background"
 },
 {
  "id": "836992d035c4be0c8eacdd419f151e_8",
  "x": "These vectors are then fed forward through the model as the final continuous representation of the left or right context, l or r respectively. Results In Table 1 , we see that our model with attention (GRU+ATTN) outperforms our basic GRU model by around 1% absolute. It also outperforms the roughly similar model of <cite>Eshel et al. (2017)</cite> on the test set: this gain is due to a combination of factors including the improved training procedure and some small modeling changes.",
  "y": "differences"
 },
 {
  "id": "854679aec9cf4f53e97936f865f49c_0",
  "x": "It is, however, affected by domain variation - <cite>Foster et al. (2007)</cite> report that its f-score drops by approximately 8 percentage points when applied to the BNC domain. Our training size is 500,000 sentences. We conduct two experiments: the first, in which 500,000 sentences are extracted randomly from the BNC (minus the test set sentences), and the second in which only shorter sentences, of length \u2264 20 words, are chosen as training material.",
  "y": "background"
 },
 {
  "id": "854679aec9cf4f53e97936f865f49c_1",
  "x": "The new system is trained on f-structureannotated parser output trees, and the performance of Charniak and Johnson's parser degrades when applied to BNC data<cite> (Foster et al., 2007)</cite> . The second reason has been suggested by Gildea (2001) : WSJ data is easier to learn than the more varied data in the Brown Corpus or BNC. Perhaps even if gold standard BNC parse trees were available for training, the system would not behave as well as it does for WSJ material.",
  "y": "background"
 },
 {
  "id": "854679aec9cf4f53e97936f865f49c_2",
  "x": "Encouragingly, we have also shown that domain-specific training material produced by a parser can be used to claw back a significant portion of this performance degradation. Our method is general and could be applied to other WSJ-trained generators (e.g. (Nakanishi et , 2007) ). We intend to continue this research by training our generator on parse trees produced by a BNC-self-trained version of the Charniak and Johnson reranking parser<cite> (Foster et al., 2007)</cite> .",
  "y": "extends"
 },
 {
  "id": "8622616ffd4db96058a9b8aff54212_0",
  "x": "The approach taken to the keyword extraction task is that of supervised machine learning. This means that a set of documents with known keywords is used to train a model, which in turn is applied to select keywords to and from previously unseen documents. The keyword extraction discussed in this paper is based on work presented in <cite>Hulth (2003a)</cite> and Hulth (2003b) .",
  "y": "uses"
 },
 {
  "id": "8622616ffd4db96058a9b8aff54212_1",
  "x": "This means that a set of documents with known keywords is used to train a model, which in turn is applied to select keywords to and from previously unseen documents. The keyword extraction discussed in this paper is based on work presented in <cite>Hulth (2003a)</cite> and Hulth (2003b) . In <cite>Hulth (2003a)</cite> an evaluation of three different methods to extract candidate terms from documents is presented.",
  "y": "background"
 },
 {
  "id": "8622616ffd4db96058a9b8aff54212_2",
  "x": "For these experiments, the same machine learning system-RDS-is used as for the experiments presented by <cite>Hulth (2003a)</cite> . Also the same data are used to train the models and to tune the parameters. The results of the experiments are presented in Tables 1-5 , which show: the average number of keywords assigned per document (Assign.); the average number of correct keywords per document (Corr.); precision (P); recall (R); and F-measure (F).",
  "y": "uses similarities"
 },
 {
  "id": "8622616ffd4db96058a9b8aff54212_3",
  "x": "---------------------------------- **USING A GENERAL CORPUS** In the experiments presented in <cite>Hulth (2003a)</cite> , only the documents present in the training, validation, and test set respectively are used for calculating the collection frequency.",
  "y": "background"
 },
 {
  "id": "8622616ffd4db96058a9b8aff54212_5",
  "x": "---------------------------------- **REGRESSION VS. CLASSIFICATION** In the experiments presented in <cite>Hulth (2003a)</cite> , the automatic keyword indexing task is treated as a binary classification task, where each candidate term is classified either as a keyword or a non-keyword.",
  "y": "background"
 },
 {
  "id": "86af8f2cc08b00821a7a83abdfd964_0",
  "x": "In this paper, we study direct transfer methods for multilingual named entity recognition. Specifically, we extend the method recently proposed by<cite> T\u00e4ckstr\u00f6m et al. (2012)</cite> , which is based on cross-lingual word cluster features. First, we show that by using multiple source languages, combined with self-training for target language adaptation, we can achieve significant improvements compared to using only single source direct transfer.",
  "y": "extends differences"
 },
 {
  "id": "86af8f2cc08b00821a7a83abdfd964_1",
  "x": "Although semi-supervised approaches have been shown to reduce the need for manual annotation (Freitag, 2004; Miller et al., 2004; Ando and Zhang, 2005; Suzuki and Isozaki, 2008; Lin and Wu, 2009; Turian et al., 2010; Dhillon et al., 2011; <cite>T\u00e4ckstr\u00f6m et al., 2012)</cite> , these methods still require a substantial amount of manual annotation for each target language. Manually creating a sufficient amount of annotated resources for all entity types in all languages thus seems like an Herculean task. In this study, we turn to direct transfer methods <cite>T\u00e4ckstr\u00f6m et al., 2012)</cite> as a way to combat the need for annotated resources in all languages.",
  "y": "background"
 },
 {
  "id": "86af8f2cc08b00821a7a83abdfd964_2",
  "x": "Although semi-supervised approaches have been shown to reduce the need for manual annotation (Freitag, 2004; Miller et al., 2004; Ando and Zhang, 2005; Suzuki and Isozaki, 2008; Lin and Wu, 2009; Turian et al., 2010; Dhillon et al., 2011; <cite>T\u00e4ckstr\u00f6m et al., 2012)</cite> , these methods still require a substantial amount of manual annotation for each target language. Manually creating a sufficient amount of annotated resources for all entity types in all languages thus seems like an Herculean task. In this study, we turn to direct transfer methods <cite>T\u00e4ckstr\u00f6m et al., 2012)</cite> as a way to combat the need for annotated resources in all languages.",
  "y": "similarities uses"
 },
 {
  "id": "86af8f2cc08b00821a7a83abdfd964_3",
  "x": "These methods allow one to train a system for a target language, using only annotations in some source language, as long as all source language features also have support in the target languages. Specifically, we extend the direct transfer method proposed by<cite> T\u00e4ckstr\u00f6m et al. (2012)</cite> in two ways. First, in \u00a73, we use multiple source languages for training.",
  "y": "extends differences"
 },
 {
  "id": "86af8f2cc08b00821a7a83abdfd964_4",
  "x": "Rather than starting from scratch when creating systems that predict linguistic structure in one language, we should be able to take advantage of any corresponding annotations that are available in other languages. This idea is at the heart of both direct transfer methods <cite>T\u00e4ckstr\u00f6m et al., 2012)</cite> and of annotation projection methods (Yarowsky et al., 2001; Diab and Resnik, 2002; Hwa et al., 2005) . While the aim of the latter is to transfer annotations across languages, direct transfer methods instead aim to transfer systems, trained on some source language, directly to other languages.",
  "y": "background"
 },
 {
  "id": "86af8f2cc08b00821a7a83abdfd964_5",
  "x": "However, showed that a language independent dependency parser can indeed be created by training on a delexicalized treebank and by only incorporating features defined on universal part-of-speech tags (Das and Petrov, 2011) . Recently,<cite> T\u00e4ckstr\u00f6m et al. (2012)</cite> developed an algorithm for inducing cross-lingual word clusters and proposed to use these clusters to enrich the feature space of direct transfer systems. The richer set of cross-lingual features was shown to substantially improve on direct transfer of both dependency parsing and NER from English to other languages.",
  "y": "background"
 },
 {
  "id": "86af8f2cc08b00821a7a83abdfd964_6",
  "x": "**MULTI-SOURCE DIRECT TRANSFER** Learning from multiple languages have been shown to be of benefit both in unsupervised learning of syntax and part-of-speech (Snyder et al., 2009; BergKirkpatrick and Klein, 2010) and in transfer learning of dependency syntax (Cohen et al., 2011; 256 cross-lingual word clusters and the same feature templates as<cite> T\u00e4ckstr\u00f6m et al. (2012)</cite> , with the exception that the transition factors are not conditioned on the input. 3 The features used are similar to those used by Turian et al. (2010) , but include cross-lingual rather than monolingual word clusters.",
  "y": "differences"
 },
 {
  "id": "86af8f2cc08b00821a7a83abdfd964_7",
  "x": [
   "By simply mixing annotated source language data with target language data, we can significantly reduce the annotation burden required to reach a given level of performance in the target language, even with up to 64,000 tokens annotated in the target language. We hypothesize that more elaborate domain adaptation techniques, such as that proposed by Chen et al. (2011) , can lead to further improvements in these scenarios. Our use of cross-lingual word clusters is orthogonal to several other approaches discussed in this paper."
  ],
  "y": "background"
 },
 {
  "id": "874a8d4f847aff2895deb7c7560c56_0",
  "x": "---------------------------------- **DATASETS** We used the dataset provided by <cite>[9]</cite> to extract the othering lexicon content.",
  "y": "similarities uses"
 },
 {
  "id": "874a8d4f847aff2895deb7c7560c56_1",
  "x": "The dataset was provided by <cite>[9]</cite> . An example of the previous method for the tweet 'we want to send them all home' contains two-sided pronouns and the verb 'send', which grammatically relates to the pronoun 'them' as a subjective clause. Even though the verb 'send' is not offensive, we include it in the lexicon.",
  "y": "uses"
 },
 {
  "id": "874a8d4f847aff2895deb7c7560c56_2",
  "x": "In general, lexical and syntactic features are useful when they are applied directly for automatic categorisation of annoying behaviours or topic detection [40] , whereas in cyberhate detection we need a deep understanding of the posted text, which will be the focus of the current work. According to [49] , while part-of-speech tagging does not significantly improve classifier performance, we apply POS tagging for extracting specific content (verbs, nouns and adjectives) for lexicon building. <cite>[9]</cite> prepared their data by stemming and then creating unigram, bigram and trigram features, each of which was weighted by its TF-IDF metric.",
  "y": "background"
 },
 {
  "id": "874a8d4f847aff2895deb7c7560c56_3",
  "x": "In addition to that, we implemented semi-supervised classification by training in the positive samples of the <cite>[9]</cite> dataset and training in only the lexicon as negative samples. The model was evaluated by testing it on the four datasets: religion, disability, racism and sexual-orientation. Two classifiers were applied: Multilayer Perception (MLP) and Logistic Regression (LR).",
  "y": "similarities uses"
 },
 {
  "id": "874a8d4f847aff2895deb7c7560c56_4",
  "x": "The lexicon assumes that two-sided pronouns appear with a specific pattern and offensive words in each row. This row is then converted to vector space using the paragraph2vec algorithm: <Othering Terms Permutation><Othering Pattern><Hateful Words> Each part was extracted as following: <All two-sided Pronouns><dependency(nsubj,dobj,det,compound)><POS (VB, NN, JJ)> Our lexicon content was extracted from negative samples of an annotated dataset <cite>[9]</cite> . From each instance in the negative dataset we constitute one row in the lexicon, with each row in the lexicon containing three columns: (1) all combinations of two-sided English pronouns, (2) the othering pronoun-related patterns which were extracted using typed dependency parser, and(3)the negative words which were extracted by POS 3 tagging <cite>[9]</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "874a8d4f847aff2895deb7c7560c56_5",
  "x": "For semi-supervised learning, when we tested our lexicon on unseen datasets, we trained the embedding algorithm in our othering lexicon as negative and the positive samples of the <cite>[9]</cite> dataset (see Figure 9 ). Then we labelled each instance individually, assigning the positive and negative labels as 0 and 1 respectively. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "874a8d4f847aff2895deb7c7560c56_6",
  "x": "For Religion data set, this data set collected after Woolwich event, which contains language about Muslim and African man (religion and race), according to <cite>[9]</cite> Twitter users use this type of language in their everyday communications which could be contained in the benign tweets. ---------------------------------- **STUDY LIMITATIONS**",
  "y": "background"
 },
 {
  "id": "877a0b5b5d25b3849ca44ed42b8d6d_0",
  "x": "The use of various synchronous grammar based formalisms has been a trend for statistical machine translation (SMT) (Wu, 1997; Eisner, 2003; Galley et al., 2006; Chiang, 2007;<cite> Zhang et al., 2008)</cite> . The grammar formalism determines the intrinsic capacities and computational efficiency of the SMT systems. To evaluate the capacity of a grammar formalism, two factors, i.e. generative power and expressive power are usually considered (Su and Chang, 1990) .",
  "y": "background"
 },
 {
  "id": "877a0b5b5d25b3849ca44ed42b8d6d_1",
  "x": "For the current synchronous grammars based SMT, to some extent, the generalization ability of the grammar rules (the usability of the rules for the new sentences) can be considered as a kind of the generative power of the grammar and the disambiguition ability to the rule candidates can be considered as an embodiment of expressive power. However, the generalization ability and the disambiguition ability often contradict each other in practice such that various grammar formalisms in SMT are actually different trade-off between them. For instance, in our investigations for SMT (Section 3.1), the Formally SCFG based hierarchical phrase-based model (hereinafter FSCFG) (Chiang, 2007) has a better generalization capability than a Linguistically motivated STSSG based model (hereinafter LSTSSG) <cite>(Zhang et al., 2008)</cite> , with 5% rules of the former matched by NIST05 test set while only 3.5% rules of the latter matched by the same test set.",
  "y": "differences"
 },
 {
  "id": "877a0b5b5d25b3849ca44ed42b8d6d_2",
  "x": "Through this formalization, we can see that FSCFG rules and LSTSSG rules are both included. However, we should point out that the rules with mixture of X non-terminals and syntactic non-terminals are not included in our current implementation despite that they are legal under the proposed formalism. The rule extraction in current implementation can be considered as a combination of the ones in (Chiang, 2007) and <cite>(Zhang et al., 2008)</cite> .",
  "y": "uses"
 },
 {
  "id": "877a0b5b5d25b3849ca44ed42b8d6d_3",
  "x": "For example, some features related with structure richness and grammar consistency 1 of a derivation should be designed to distinguish the derivations involved various heterogeneous rule applications. For the page limit and the fair comparison, we only adopt the conventional features as in <cite>(Zhang et al., 2008)</cite> in our current implementation. Figure 2 : Some synthetic synchronous grammar rules can be extracted from the sentence pair in Figure  1 .",
  "y": "uses"
 },
 {
  "id": "877a0b5b5d25b3849ca44ed42b8d6d_4",
  "x": "For comparisons, we used the following three baseline systems: LSTSSG An in-house implementation of linguistically motivated STSSG based model similar to <cite>(Zhang et al., 2008)</cite> . FSCFG An in-house implementation of purely formally SCFG based model similar to (Chiang, 2007) . MBR We use an in-house combination system which is an implementation of a classic sentence level combination method based on the Minimum Bayes Risk (MBR) decoding (Kumar and Byrne, 2004) .",
  "y": "uses"
 },
 {
  "id": "878c6cf1c47c86f36a7ff3f04e2998_0",
  "x": "Recently, <cite>Thorne et al. (2018)</cite> proposed a public dataset to explore the complete process of the large-scale fact-checking. It is designed not only to verify claims but also to extract sets of related evidence. Nevertheless, the pipeline solution proposed in that paper suffers from following problems: 1) The overall performance (30.88% accuracy) still needs further improvement to be applicable to the evidence selection and classification, which also highlights the challenging nature of this task.",
  "y": "background"
 },
 {
  "id": "878c6cf1c47c86f36a7ff3f04e2998_1",
  "x": "Instead of selecting the sentences by recomputing sentence-level TF-IDF features between claim and document text as in <cite>Thorne et al. (2018)</cite> , we propose a neural ranker using decomposable attention (DA) model (Parikh et al., 2016) to perform evidence selection. DA model does not require the input text to be parsed syntactically, nor is an ensemble, and it is faster without any recurrent structure. In general, using neural methods is better for the following reasons: 1) The TF-IDF may have limited ability to capture semantics compared to word representation learning 2) Faster inference time compared to TF-IDF methods that need real-time reconstruction.",
  "y": "differences"
 },
 {
  "id": "878c6cf1c47c86f36a7ff3f04e2998_2",
  "x": "Same as <cite>Thorne et al. (2018)</cite> , we use the decomposable attention (DA) between the claim and the evidence for RTE. DA model decomposes the RTE problem into subproblems, which can be considered as bi-direction wordlevel attention features. Note that the DA model is utilized over other models such as as Chen et al. (2017b) ; Glockner et al. (2018) , because it is a simple but effective model.",
  "y": "similarities"
 },
 {
  "id": "878c6cf1c47c86f36a7ff3f04e2998_3",
  "x": "However, NEI claims have no annotated evidence, thus cannot be used to train RTE. To overcome this issue, same as<cite> (Thorne et al., 2018)</cite> , the most probable NEI evidence are simulated by sampling sentences from the nearest page to the claim using the document retrieval module. MLP DA rte DA rte +NER Accuracy (%) 63.2 78.4 79.9 Table 4 : Oracle RTE classification accuracy in the test set using gold evidence.",
  "y": "similarities"
 },
 {
  "id": "878c6cf1c47c86f36a7ff3f04e2998_4",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** Dataset: FEVER dataset<cite> (Thorne et al., 2018</cite> ) is a relatively large-scale dataset compared to other previous fact extraction and verification works, with around 5.4M Wikipedia documents and 185k samples.",
  "y": "uses"
 },
 {
  "id": "878c6cf1c47c86f36a7ff3f04e2998_5",
  "x": "In all the datasets, we tuned the hyper-parameters with grid-search over the validation set. Evaluation: For each module, we independently measure oracle performance, where we assume gold standard documents and set of evidence are provided (oracle evaluation). For the final fullpipeline, we compare to and follow the metric defined in <cite>Thorne et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "878c6cf1c47c86f36a7ff3f04e2998_6",
  "x": "The MLP is a simple multi-layer perceptron using TF and TF-IDF cosine similarity between the claim and evidence as features as shown in <cite>Thorne et al. (2018)</cite> . The highest accuracy achieved is 79.9% using DA rte with NER information, thus, we further evaluate the full-pipeline accuracy on this setting. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "878c6cf1c47c86f36a7ff3f04e2998_7",
  "x": "Prior work (Vlachos and Riedel, 2014; Ciampaglia et al., 2015) have proposed fact-checking through entailment from knowledge bases. Some works have investigated fact verification using PolitiFact data (Wang, 2017; Rashkin et al., 2017) or FakeNews challenge (Pomerleau and Rao) . Most closely related to our work, <cite>Thorne et al. (2018)</cite> addresses large-scale fact extraction and verification task using a pipeline approach.",
  "y": "similarities"
 },
 {
  "id": "887864e173d7f7c164fe9f9d940727_0",
  "x": "We propose a novel Aspect-based Rating Prediction Model (AspeRa) for aspect-based representation learning for items by encoding word-occurrence statistics into word embeddings and applying dimensionality reduction to extract the most important aspects that are used for the user-item rating estimation. We investigate how and in what settings such neural autoencoders can be applied to contentbased recommendations for text items. ---------------------------------- **ASPERA MODEL** The AspeRa model combines the advantages of deep learning (end-to-end learning, spatial text representation) and topic modeling (interpretable topics) for text-based recommendation systems. Fig. 1 shows the overall architecture of AspeRa. The model receives as input two reviews at once, treating both identically. Each review is embedded with self-attention to produce two vectors, one for author (user) features and the other for item features. These two vectors are used to predict a rating corresponding to the review. All vectors are forced to belong to the same feature space. The embedding is produced by the Neural Attention-Based Aspect Extraction Model (ABAE) <cite>[7]</cite> .",
  "y": "uses"
 },
 {
  "id": "887864e173d7f7c164fe9f9d940727_1",
  "x": "Following ABAE <cite>[7]</cite> , we set the aspects matrix ortho-regularization coefficient equal to 0.1. Since this model utilizes an aspect embedding matrix to approximate aspect words in the vocabulary, initialization of aspect embeddings is crucial. The work [8] used k-means clustering-based initialization [17, 18, 36] , where the aspect embedding matrix is initialized with centroids of the resulting clusters of word embeddings.",
  "y": "similarities uses"
 },
 {
  "id": "887864e173d7f7c164fe9f9d940727_2",
  "x": "Qualitative analysis shows that some aspects describe what could be called a topic (a set of words diverse by part of speech and function describing a certain domain), some encode sentiment (top words are adjectives showing attitude to certain objects discussed in the text), and some encode names (actors, directors, etc.). We also found similar patterns in the output of the basic ABAE model <cite>[7]</cite> . Thus, most aspects are clearly coherent, but there is room for improvement.",
  "y": "similarities"
 },
 {
  "id": "887864e173d7f7c164fe9f9d940727_3",
  "x": "Recent advances in distributed word representations have made it a cornerstone of modern natural language processing [6] , with neural networks recently used to learn text representations. He et al. <cite>[7]</cite> proposed an unsupervised neural attention-based aspect extraction (ABAE) approach that encodes word-occurrence statistics into word embeddings and applies an attention mechanism to remove irrelevant words, learning a set of aspect embeddings. Several recent works, including DeepCoNN [43] , propose a completely different approach.",
  "y": "background"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_0",
  "x": "Some recent attempts [5, 8, 11, 12, 15] also have been made to model the dynamics of language in terms of word senses. One of the studies in this area has been presented by <cite>Mitra et al. [19]</cite> where <cite>the authors show</cite> that at earlier times, the sense of the word 'sick' was mostly associated to some form of illness; however, over the years, a new sense associating the same word to something that is 'cool' or 'crazy' has emerged. <cite>Their study</cite> is based on a unique network representation of the corpus called a distributional thesauri (DT) network built using Google books syntactic n-grams. <cite>They have used</cite> unsupervised clustering techniques to induce a sense of a word and then compared the induced senses of two time periods to get the new sense for a particular target word.",
  "y": "background"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_1",
  "x": "---------------------------------- **LIMITATIONS OF THE RECENT APPROACHES** While <cite>Mitra et al. [19]</cite> reported a precision close to 0.6 over a random sample of 49 words, we take another random sample of 100 words separately and repeat manual evaluation.",
  "y": "extends"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_2",
  "x": "We then explore another unsupervised approach presented in Lau et al. [16] over the same Google books corpus 1 , apply topic modeling for sense induction and directly adapt their similarity measure to get the new senses. Using a set intersecting with the 100 random samples for <cite>Mitra et al. [19]</cite> , we obtain the precision values of 0.21 and 0.28, respectively. Clearly, none of the precision values are good enough for reliable novel sense detection.",
  "y": "uses"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_3",
  "x": "---------------------------------- **OUR PROPOSAL AND THE ENCOURAGING RESULTS** We propose a method based on the network features to reduce the number of false positives and thereby, increase the overall precision of the method proposed by <cite>Mitra et al. [19]</cite> .",
  "y": "extends"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_4",
  "x": "Researchers also attempt to apply dynamic word embeddings as well to detect language evolution [23, 30] , analyze temporal word analogy [24] . We now describe the two baselines that are relevant for our work. Baseline 1: <cite>Mitra et al. [19]</cite> <cite>The authors proposed</cite> an unsupervised method to identify word sense changes automatically for nouns.",
  "y": "background"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_5",
  "x": "We now describe the two baselines that are relevant for our work. Baseline 1: <cite>Mitra et al. [19]</cite> <cite>The authors proposed</cite> an unsupervised method to identify word sense changes automatically for nouns. Datasets and graph construction: <cite>The authors used</cite> the Google books corpus, consisting of texts from over 3.4 million digitized English books published between 1520 and 2008. <cite>The authors constructed</cite> distributional thesauri (DT) networks from the Google books syntactic n-grams data [9] .",
  "y": "background motivation"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_6",
  "x": "The clusters for a target word 'float' is shown in the right image of Figure 1 . <cite>The authors then compare</cite> the sense clusters extracted across two different time points to obtain the suitable signals of sense change. Specifically, for a candidate word w, a sense cluster in the later time period is called as a 'birth' cluster if at least 80% words of this cluster do not appear in any of the sense clusters from the previous time period.",
  "y": "background"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_7",
  "x": "<cite>The authors then apply</cite> multi-stage filtering in order to obtain meaningful candidate words. Baseline 2: Lau et al. [16] : The authors proposed an unsupervised approach based on topic modeling for sense induction, and showed novel sense identification as one of its applications. For a candidate word, Hierarchical Dirichlet Process [26] is run over a corpus to induce topics.",
  "y": "background"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_8",
  "x": "Separately, we also apply the proposed SVM classification model as a filtering step to obtain 'filtered birth' cases. This helps in designing a comparative evaluation of these algorithms as follows. From both the time point pairs (T 1 and T 2 ), we take 100 random samples from the birth cases reported by <cite>Mitra et al. [19]</cite> and get these manually evaluated.",
  "y": "uses"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_9",
  "x": "To further evaluate the proposed algorithm, we perform two more evaluations. First, we take 60 random samples from each time point pair for computing precision of the 'filtered birth' cases. Secondly, we also take 100 random samples for each time point pair Word Word for computing precision of our approach independently of <cite>Mitra et al. [19]</cite> , i.e., the proposed approach is not informed of the 'birth' cluster reported by <cite>Mitra et al. [19]</cite> , instead all the clusters in old and new time point are shown.",
  "y": "extends"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_10",
  "x": "In this process of manual annotation, we obtain an inter-annotator agreement (Fleiss' kappa [7] ) of 0.745, which is substantial [28] . Table 3 shows three example words from T 1 , their 'birth' clusters as reported in <cite>Mitra et al. [19]</cite> and the manual evaluation result. The first two cases belong to computer related sense of 'searches' and 'logging', which were absent from time point 1909-1953.",
  "y": "uses"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_11",
  "x": "Since the reported novel sense cluster can in principle be different from the 'birth' sense reported by the method of <cite>Mitra et al. [19]</cite> for the same word, we get the novel sense cases manually evaluated by 3 annotators (42 and 28 cases for the two time periods, respectively). Note that for these 100 random samples (that are all marked 'true' by <cite>Mitra et al. [19]</cite> ), it is possible to find an upper bound on the recall of Lau et al. [16] 's approach automatically. While the low recall might be justified because this is a different approach, even the precision is found to be in the same range as that of <cite>Mitra et al. [19]</cite> .",
  "y": "differences"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_12",
  "x": "Since the reported novel sense cluster can in principle be different from the 'birth' sense reported by the method of <cite>Mitra et al. [19]</cite> for the same word, we get the novel sense cases manually evaluated by 3 annotators (42 and 28 cases for the two time periods, respectively). Note that for these 100 random samples (that are all marked 'true' by <cite>Mitra et al. [19]</cite> ), it is possible to find an upper bound on the recall of Lau et al. [16] 's approach automatically. While the low recall might be justified because this is a different approach, even the precision is found to be in the same range as that of <cite>Mitra et al. [19]</cite> .",
  "y": "uses"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_13",
  "x": "Note that for these 100 random samples (that are all marked 'true' by <cite>Mitra et al. [19]</cite> ), it is possible to find an upper bound on the recall of Lau et al. [16] 's approach automatically. While the low recall might be justified because this is a different approach, even the precision is found to be in the same range as that of <cite>Mitra et al. [19]</cite> . Table 6 presents the evaluation results for the same set of 100 random samples after using the proposed SVM filtering.",
  "y": "similarities"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_14",
  "x": "Table 6 presents the evaluation results for the same set of 100 random samples after using the proposed SVM filtering. We see that the filtering using SVM classification improves the precision for both the time point pairs (T 1 and T 2 ) significantly, boosting it from the range of 0.23-0.32 to 0.74-0.86. Note that, as per our calculations, indeed the recall of <cite>Mitra et al. [19]</cite> would be 100% (as we are taking random samples for annotation from the set of reported 'birth' cases by <cite>Mitra et al. [19]</cite> only). Even then <cite>Mitra et al. [19]</cite> 's F-measure ranges from 0.37-0.48 while ours is 0.67-0.68.",
  "y": "uses"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_15",
  "x": "Table 6 presents the evaluation results for the same set of 100 random samples after using the proposed SVM filtering. We see that the filtering using SVM classification improves the precision for both the time point pairs (T 1 and T 2 ) significantly, boosting it from the range of 0.23-0.32 to 0.74-0.86. Note that, as per our calculations, indeed the recall of <cite>Mitra et al. [19]</cite> would be 100% (as we are taking random samples for annotation from the set of reported 'birth' cases by <cite>Mitra et al. [19]</cite> only). Even then <cite>Mitra et al. [19]</cite> 's F-measure ranges from 0.37-0.48 while ours is 0.67-0.68.",
  "y": "differences"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_16",
  "x": "We see that the filtering using SVM classification improves the precision for both the time point pairs (T 1 and T 2 ) significantly, boosting it from the range of 0.23-0.32 to 0.74-0.86. Note that, as per our calculations, indeed the recall of <cite>Mitra et al. [19]</cite> would be 100% (as we are taking random samples for annotation from the set of reported 'birth' cases by <cite>Mitra et al. [19]</cite> only). Even then <cite>Mitra et al. [19]</cite> 's F-measure ranges from 0.37-0.48 while ours is 0.67-0.68. Table 7 represents some of the examples which were declared as 'birth' by <cite>Mitra et al. [19]</cite> but SVM filtering correctly flagged them as 'false birth'.",
  "y": "uses"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_17",
  "x": "Considering the small training set, the results are highly encouraging. We also obtain decent recall values for the two time point pairs, giving an overall F-measure of 0.67-0.68. Further, we check if we can meaningfully combine the results reported by both the methods of <cite>Mitra et al. [19]</cite> and Lau et al. [16] for more accurate sense detection; and how does this compare with Table 8 ; both the senses look quite similar.",
  "y": "uses"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_18",
  "x": "Extensive evaluation of the proposed approach: We first take 60 random samples each from the 'birth' cases reported by the SVM filtering for the two time point pairs, T 1 (from 318 cases) and T 2 (from 329 cases). The precision values of this evaluation are found to be 0.87 (52/60) and 0.75 (45/60) respectively, quite consistent with those reported in Table 6 . We did another experiment in order to estimate the performance of our model for detecting novel sense, independent of the method of <cite>Mitra et al. [19]</cite> .",
  "y": "differences"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_19",
  "x": "We find that in most of such cases, the sense cluster reported as 'birth' contained many new terms (and therefore, the network properties have undergone change) but the implied sense was already present in one of the previous clusters with very few common words (and therefore, the new cluster contained > 80% new words, and is being reported as 'birth' in <cite>Mitra et al. [19]</cite> ). Two such examples are given in Table 13 . The <cite>split-join algorithm</cite> proposed in <cite>Mitra et al. [19]</cite> needs to be adapted for such cases.",
  "y": "differences"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_20",
  "x": "We find that in most of such cases, the sense cluster reported as 'birth' contained many new terms (and therefore, the network properties have undergone change) but the implied sense was already present in one of the previous clusters with very few common words (and therefore, the new cluster contained > 80% new words, and is being reported as 'birth' in <cite>Mitra et al. [19]</cite> ). Two such examples are given in Table 13 . The <cite>split-join algorithm</cite> proposed in <cite>Mitra et al. [19]</cite> needs to be adapted for such cases.",
  "y": "motivation future_work"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_21",
  "x": "Each of the clusters represents a particular sense of the target. We now compare the sense clusters extracted across two different time points to obtain the suitable signals of sense change following the approach proposed in Mitra et al. <cite>Mitra et al. [19]</cite> . After getting the novel sense clusters, we pick up 50 random samples, of which 25 cases are flagged as 'true birth' and the rest 25 cases are flagged as 'false birth' by manual evaluation.",
  "y": "uses"
 },
 {
  "id": "8a1d4802c170fa8a71504533437e8f_22",
  "x": "For 'gay', since there is no sense cluster in the older time period with 'gay' being a noun, cluster comparison does not even detect the 'birth' cluster of 'gay'. The 'birth' sense clusters for 'guy', 'bush' in the new time period, as detected by <cite>split-join algorithm</cite> contain general terms like \"someone, anyone, men, woman, mother, son\" and \"cloud, air, sky, sunlight\" respectively. As the network around these words did not change much over time, our method found it difficult to detect.",
  "y": "differences"
 },
 {
  "id": "8ace0627a085efd0cf0ccc211c556f_0",
  "x": "---------------------------------- **APPROACH** We use XLNet <cite>[11]</cite> to attempt to capture long range language dependencies.",
  "y": "uses"
 },
 {
  "id": "8ace0627a085efd0cf0ccc211c556f_1",
  "x": "XLNet is a generalized auto-regressive model that can be used for language modeling based on the transformer-XL architecture <cite>[11]</cite> . This means that the outputs of XLNet depend strictly on the previous outputs. This is different from other state-of-the-art language models like BERT (Bidirectional Encoder Representations from Transformers) which rely on conditioning the probabilities given surrounding words.",
  "y": "background"
 },
 {
  "id": "8ace0627a085efd0cf0ccc211c556f_2",
  "x": "During training, a certain percentage of words are masked for use in prediction. If both \"San\" and \"Francisco\" were masked, BERT would not be able to use information when decoding one of the words to help in decoding the other. masking the input introduces a few disadvantages mentioned in <cite>[11]</cite> .",
  "y": "motivation"
 },
 {
  "id": "8ace0627a085efd0cf0ccc211c556f_3",
  "x": "During training, a certain percentage of words are masked for use in prediction. masking the input introduces a few disadvantages mentioned in <cite>[11]</cite> .",
  "y": "motivation"
 },
 {
  "id": "8ace0627a085efd0cf0ccc211c556f_4",
  "x": "We use a library that contains a pre-trained version of XLNet, an implementation of the transformer-XL architecture [14] . It was previously trained on BooksCorpus [15] and English Wikipedia which have 13GB of plain text combined <cite>[11]</cite> .",
  "y": "uses"
 },
 {
  "id": "8ace0627a085efd0cf0ccc211c556f_5",
  "x": "The RNNLM still gives a much better score. We suspect that this is due to a few things: firstly, the XLNet is 110M parameters and was trained on approximately 13GB of text compared to 25MB worth of text for TED-LIUM. Given the size of the model, and the fact that it was pre-trained on 512 TPUs <cite>[11]</cite> , we expect that training for 20 epochs on TED-LIUM's text is not enough to overcome the differences between written text and conversational speech.",
  "y": "uses"
 },
 {
  "id": "8ad5f7a658a8bb5377981ba6b098dc_0",
  "x": "In this paper we demonstrate one way that this can be achieved. We build on the paraphrasing approach of <cite>[1]</cite> in that we use a fixed set of templates to generate a set of candidate logical forms to answer a given query and map each logical form to a natural language expression, its canonical utterance. Instead of using a complex paraphrasing model however, we use tensor kernels to find relationships between terms occuring in the query and in the canonical utterance.",
  "y": "similarities uses"
 },
 {
  "id": "8ad5f7a658a8bb5377981ba6b098dc_1",
  "x": "There are two datasets of queries for this database: Free917 consisting of 917 questions annotated with logical forms [9] , and WebQuestions which consists of 5,810 question-answer pairs, with no logical forms [10] . Approaches to this task include schema matching [9] , inducing latent logical forms [10] , application of paraphrasing techniques<cite> [1,</cite><cite> 1</cite>1] , information extraction [12] , learning low dimensional embeddings of words and knowledge base constituents [13] and application of logical reasoning in conjunction with statistical techniques [11] . Note that most of these approaches do not require annotated logical forms, and either induce logical forms when training using the given answers, or bypass them altogether.",
  "y": "similarities uses"
 },
 {
  "id": "8ad5f7a658a8bb5377981ba6b098dc_2",
  "x": "**SEMANTIC PARSING VIA PARAPHRASING** The ParaSempre system of <cite>[1]</cite> is based on the idea of generating a set of candidate logical forms from the query using a set of templates. For example, the query Who did Brad Pitt play in Troy? would generate the logical form as well as many incorrect logical forms.",
  "y": "background"
 },
 {
  "id": "8ad5f7a658a8bb5377981ba6b098dc_3",
  "x": "One way of doing this to consider a fixed number of logical forms for each query sentence, and train a classifier to choose the best logical form given a sentence <cite>[1]</cite> . In order to ues this approach, we need a single feature vector for each pair of queries and logical forms. Our proposal is to extract features for each query and logical form indepdendently, and to take their tensor product as the combined vector.",
  "y": "motivation"
 },
 {
  "id": "8ad5f7a658a8bb5377981ba6b098dc_4",
  "x": "We built our implementation on top of the ParaSempre system <cite>[1]</cite> , and so our evaluation exactly matches theirs. Our implementation is freely available online. 1 We substituted the paraphrase system of ParaSempre with our tensor kernel-based system (i.e. we excluded features from both the association and vector space models), but we included the ParaSempre features derived from logical forms.",
  "y": "similarities uses"
 },
 {
  "id": "8ad5f7a658a8bb5377981ba6b098dc_5",
  "x": "Evaluating using ParaSempre on the development set took 22h31m; using the tensor kernel took<cite> 1</cite>4h44m on a comparable machine. Since we have adopted the logical form templates of ParaSempre, our upper bound or oracle F1 score is the same, 63% <cite>[1]</cite> . This is the score that would be obtained if we knew which was the best logical form out of all those generated.",
  "y": "similarities uses"
 },
 {
  "id": "8ad5f7a658a8bb5377981ba6b098dc_6",
  "x": "---------------------------------- **DISCUSSION** Average F1 score Sempre [10] 35.7 ParaSempre <cite>[1]</cite> 39.9 Facebook [13] 41.8 DeepQA [11] 45.3 Tensor kernel with unigrams 40.1 In development, we found that ordering the training alphabetically by the text of the query lead to a large reduction in accuracy.",
  "y": "extends differences"
 },
 {
  "id": "8ae44e74146d3f40845741fac4dff9_0",
  "x": "Handcoded knowledge has proved useful for both metaphor identification, i.e. distinguishing between literal and metaphorical language in text (Fass, 1991; Martin, 1990; Krishnakumaran and Zhu, 2007; Gedigian et al., 2006) and metaphor interpretation, i.e. identifying the intended literal meaning of a metaphorical expression (Fass, 1991; Martin, 1990; Narayanan, 1997; Barnden and Lee, 2002) . However, to be applicable in a real-world setting a metaphor processing system needs to be able to identify and interpret metaphorical expressions in unrestricted text. The recent metaphor paraphrasing approach of <cite>Shutova (2010)</cite> was designed with this requirement in mind and used statistical methods, but still relied on the WordNet (Fellbaum, 1998) database to generate the initial set of paraphrases.",
  "y": "motivation"
 },
 {
  "id": "8ae44e74146d3f40845741fac4dff9_1",
  "x": "However, to be applicable in a real-world setting a metaphor processing system needs to be able to identify and interpret metaphorical expressions in unrestricted text. The recent metaphor paraphrasing approach of <cite>Shutova (2010)</cite> was designed with this requirement in mind and used statistical methods, but still relied on the WordNet (Fellbaum, 1998) database to generate the initial set of paraphrases. In this paper, we take the metaphor paraphrasing task a step further and present a fully unsupervised approach to this problem.",
  "y": "extends"
 },
 {
  "id": "8ae44e74146d3f40845741fac4dff9_2",
  "x": "In comparison to lexical substitution, metaphor paraphrasing presents an additional challenge, namely that of discriminating between literal and metaphorical substitutes. <cite>Shutova (2010)</cite> used a selectional preference-based model for this purpose, obtaining encouraging results in a supervised setting. We evaluate the capacity of our vector space model to discriminate between literal and figurative paraphrases on its own, as well as integrating it with a selectional preference-based model similar to that of <cite>Shutova (2010)</cite> and thus evaluating the latter in an unsupervised setting.",
  "y": "background"
 },
 {
  "id": "8ae44e74146d3f40845741fac4dff9_3",
  "x": "In comparison to lexical substitution, metaphor paraphrasing presents an additional challenge, namely that of discriminating between literal and metaphorical substitutes. <cite>Shutova (2010)</cite> used a selectional preference-based model for this purpose, obtaining encouraging results in a supervised setting. We evaluate the capacity of our vector space model to discriminate between literal and figurative paraphrases on its own, as well as integrating it with a selectional preference-based model similar to that of <cite>Shutova (2010)</cite> and thus evaluating the latter in an unsupervised setting.",
  "y": "similarities"
 },
 {
  "id": "8ae44e74146d3f40845741fac4dff9_4",
  "x": "It first computes candidate paraphrases according to a latent model of semantic similarity based on the context of the metaphorically used word, and then measures the literalness of the candidates using a selectional preference model. We focus on paraphrasing metaphorical verbs and evaluate our system using the dataset of <cite>Shutova (2010)</cite> especially designed for this task. The comparison against a paraphrasing gold standard provided by <cite>Shutova (2010)</cite> is complemented by an evaluation against direct human judgements of system output.",
  "y": "uses"
 },
 {
  "id": "8ae44e74146d3f40845741fac4dff9_5",
  "x": "It first computes candidate paraphrases according to a latent model of semantic similarity based on the context of the metaphorically used word, and then measures the literalness of the candidates using a selectional preference model. We focus on paraphrasing metaphorical verbs and evaluate our system using the dataset of <cite>Shutova (2010)</cite> especially designed for this task. The comparison against a paraphrasing gold standard provided by <cite>Shutova (2010)</cite> is complemented by an evaluation against direct human judgements of system output.",
  "y": "uses"
 },
 {
  "id": "8ae44e74146d3f40845741fac4dff9_6",
  "x": "As the task is to identify the literal interpretation, this ranking still needs to be refined. Following <cite>Shutova (2010)</cite> , we use a selectional preference model to discriminate between literally and metaphorically used substitutes. Verbs used metaphorically are likely to demonstrate semantic preference for the source domain, e.g. speed up would select for MACHINES, or VEHICLES, rather than CHANGE (the target domain), whereas the ones used literally for the target domain, e.g. facilitate would select for PROCESSES (including CHANGE).",
  "y": "uses"
 },
 {
  "id": "8ae44e74146d3f40845741fac4dff9_7",
  "x": "**EVALUATION AND DISCUSSION** We compared the rankings of the initial candidate generation by the vector space model (VS) and the selectional preference-based reranking (SP) to that of an unsupervised paraphrasing baseline. We thus evaluated the ability of VS on its own to detect literal paraphrases, as well as the effectiveness of the SP model of <cite>Shutova (2010)</cite> in an unsupervised setting and in combination with VS.",
  "y": "uses"
 },
 {
  "id": "8ae44e74146d3f40845741fac4dff9_8",
  "x": "**DATASET** To our knowledge, the only metaphor paraphrasing dataset and gold standard available to date is that of <cite>Shutova (2010)</cite> . We used this dataset to develop and test our system.",
  "y": "background"
 },
 {
  "id": "8ae44e74146d3f40845741fac4dff9_9",
  "x": "**DATASET** To our knowledge, the only metaphor paraphrasing dataset and gold standard available to date is that of <cite>Shutova (2010)</cite> . We used this dataset to develop and test our system.",
  "y": "uses"
 },
 {
  "id": "8ae44e74146d3f40845741fac4dff9_10",
  "x": "<cite>Shutova (2010)</cite> annotated metaphorical expressions in a subset of the BNC sampling various genres: literature, newspaper/journal articles, essays on politics, international relations and sociology, radio broadcast (transcribed speech). The dataset consists of 62 phrases that include a metaphorical verb and either a subject or a direct object. subject-verb constructions) .",
  "y": "background"
 },
 {
  "id": "8ae44e74146d3f40845741fac4dff9_11",
  "x": "We then also evaluated VS, SP and baseline rankings against a human-constructed paraphrasing gold standard. The gold standard was created by <cite>Shutova (2010)</cite> as follows. Five independent annotators were presented with a set of sentences containing metaphorical However, given that the metaphor paraphrasing task is open-ended, it is hard to construct a comprehensive gold standard.",
  "y": "uses"
 },
 {
  "id": "8ae44e74146d3f40845741fac4dff9_12",
  "x": "**DISCUSSION** Our system consistently produces better results than the baseline, with an improvement of 12% in precision on our human evaluation (SP) and an improvement of 4% MAP on the gold standard (SP). At first sight, these improvements of our unsupervised system may not seem very high, in particular when compared to the results of the supervised system of <cite>Shutova (2010)</cite> .",
  "y": "uses"
 },
 {
  "id": "8ae44e74146d3f40845741fac4dff9_13",
  "x": "Our method identifies literal paraphrases for metaphorical expressions with a precision of 0.52 measured at top-ranked paraphrases. Given the unsupervised nature of our system and considering the state-of-the-art in unsupervised lexical substitution, we consider this a promising result. Following <cite>Shutova (2010)</cite> , the current experimental design and test set focuses on subject-verb and verb-object metaphors only, but we expect the method to be equally applicable to other parts of speech and a wider range of syntactic constructions.",
  "y": "uses"
 },
 {
  "id": "8b2bb6753dc72a048ec28958e943fb_0",
  "x": "There are various approaches proposed for morphological disambiguation based on lexical rules or statistical models. Rule based methods apply hand-crafted rules in order to select the correct morphological analyses or eliminate incorrect ones (Oflazer and Kuru\u00f6z 1994; Oflazer and Tur 1996; Daybelge and Cicekli 2007) . <cite>Y\u00fcret and T\u00fcre (2006)</cite> proposed a decision list learning algorithm for extraction of Finally, hybrid models which combine statistical and rule based approaches are also proposed (Oflazer and Tur 1996; Kutlu and Cicekli 2013) .",
  "y": "background"
 },
 {
  "id": "8b2bb6753dc72a048ec28958e943fb_1",
  "x": "However, morphological disambiguation is a much harder problem in general due to the fact that it requires the classification of both roots, suffixes and the corresponding labels. Moreover, compared to an agglutinative language such as Turkish, English words can take on a limited number of word forms and part-of-speech tags. <cite>Y\u00fcret and T\u00fcre (2006)</cite> observe that more than ten thousand tag types exists in a corpus comprised of a million Turkish words.",
  "y": "background"
 },
 {
  "id": "8b2bb6753dc72a048ec28958e943fb_2",
  "x": "They integrate their generative model to NLP applications such as language modeling, word alignment and morphological disambiguation and obtain state-of-the-art results for Russian morphological disambiguation. <cite>Y\u00fcret and T\u00fcre (2006)</cite> extract Turkish morphological disambiguation rules using a decision list learner, Greedy Prepend Algorithm (GPA), and they achieve 95.8% accuracy on manually disambiguated data consisting of around 1K words. Megyesi (1999) adapt a transformation based syntactic rule learner (Brill 1995) for Hungarian and Hajic (1998) extend his work for Czech and five other languages.",
  "y": "background"
 },
 {
  "id": "8b2bb6753dc72a048ec28958e943fb_3",
  "x": "**EXPERIMENTS** For Turkish, we used a semi-automatically disambiguated corpus containing 1M tokens<cite> (Y\u00fcret and T\u00fcre 2006)</cite> . Since this dataset is annotated semi-automatically, it also contains noise.",
  "y": "uses"
 },
 {
  "id": "8b2bb6753dc72a048ec28958e943fb_5",
  "x": "As discussed before, the available data for Turkish morphological disambiguation task contains some systematic errors. <cite>Y\u00fcret and T\u00fcre (2006)</cite> report that the accuracy of the training data is below 95%. According to our observation there is a major confusion between noun and adjective POS tags in training data which affects the decision of the morphological disambiguation systems.",
  "y": "background"
 },
 {
  "id": "8bd97eb118175c9fd2147b6456421c_0",
  "x": "Recent advances have shown that E2E models can outperform the state-of-the-art conventional system when trained on thousands of hours of data [5, <cite>6]</cite> . In previous work [7] , it has been shown that contextual information (i.e., phrases relevant to recognition in the current context such as contact names, geographic place names, songs, etc.) can improve ASR accuracy. Such phrases are often foreign words, or are rarely seen in training.",
  "y": "background"
 },
 {
  "id": "8bd97eb118175c9fd2147b6456421c_1",
  "x": "beam search. To bring more relevant words for biasing in E2E models, [11] proposes to push biasing weights to each subword unit and deals with over-biasing. Further improvements such as biasing before beam pruning, and wordpiece-based biasing have been proposed to achieve state-of-the-art biasing results [12,<cite> 6,</cite> 13] .",
  "y": "background"
 },
 {
  "id": "8bd97eb118175c9fd2147b6456421c_2",
  "x": "Shallow fusion has been used in E2E models for decoding [10] and contextual biasing<cite> [6]</cite> . Biasing phrases are first represented as n-gram WFST in the word level (G), and then left composed with a \"speller\" FST (S) to produce a contextual LM: C = min(det(S \u2022 G)). The speller transduces a sequence of subword units to corresponding words.",
  "y": "background"
 },
 {
  "id": "8bd97eb118175c9fd2147b6456421c_3",
  "x": "In [13] , wordpieces have been shown to outperform graphemes in biasing since they create a sparser match of biasing units. All these improvements lead to significantly better biasing which is comparable to the state-of-the-art conventional model<cite> [6]</cite> . To avoid over-biasing, [13] also proposed to only activate biasing phrases when they are proceeded by a set of prefixes.",
  "y": "background"
 },
 {
  "id": "8bd97eb118175c9fd2147b6456421c_4",
  "x": "We use a pronunciation lexicon to obtain phoneme sequences of words. Since phonemes show strength in recognizing rare words [1<cite>6]</cite> , we want to present these words as phonemes more often. In a target sentence, we decide to randomly present the i th word as phonemes with a probability",
  "y": "background"
 },
 {
  "id": "8bd97eb118175c9fd2147b6456421c_5",
  "x": "For words that appear more than T times, the more frequent they are, the less likely they are presented as phonemes 2 . Note that the decision of whether to use wordpieces or phonemes is made randomly at each gradient iteration, and thus a given sentence could have different target sequences at different epochs. We use context-independent phonemes as in [1<cite>6]</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "8bd97eb118175c9fd2147b6456421c_6",
  "x": "To generate words as outputs, we search through a decoding graph similar to [1<cite>6]</cite> but accept both phonemes and wordpieces. An example is shown in Figure 2 . The decoding FST has wordpiece loops around state 0 (we show only a few for simplicity), but also has a pronunciation section (states 1 through 14) .",
  "y": "extends differences"
 },
 {
  "id": "8bd97eb118175c9fd2147b6456421c_7",
  "x": "Based on [1<cite>6]</cite> , we add two improvements to the decoding strategy. First, during decoding we consume as many input epsilon arcs as possible thus guaranteeing that all wordpieces in word are produced when all corresponding phonemes are seen in the input. Second, we merge paths that have the same output symbols.",
  "y": "similarities uses"
 },
 {
  "id": "8bd97eb118175c9fd2147b6456421c_8",
  "x": "Similarly to<cite> [6]</cite> , an input utterance is divided to 25-ms frames, windowed and shifted at a rate of 10 ms. A 80-dimensional logMel feature is extracted at each frame, and the current frame and two frames to the left are concatenated to produce a 240-dimensional log-Mel feature. These features are then downsampled at a rate of 30 ms.",
  "y": "similarities uses"
 },
 {
  "id": "8bd97eb118175c9fd2147b6456421c_9",
  "x": "These features are then downsampled at a rate of 30 ms. We use RNN-T as the sequenceto-sequence model. Similar to<cite> [6]</cite> , the encoder of the RNN-T consists of 8 Long Short-Term Memory (LSTM) [21] layers and the prediction network contains 2 layers.",
  "y": "similarities uses"
 },
 {
  "id": "8bd97eb118175c9fd2147b6456421c_10",
  "x": "Comparing different biasing strategies, we find that the wordpiece-phoneme model performs the best: 16% relatively better than the grapheme model, and 8.3% better than the wordpiece model. We attribute the superior per- formance of the wordpiece-phoneme model to the robustness of phonemes to OOV words, as observed in [1<cite>6]</cite> . Since the wordpiece-phoneme model contains both wordpieces and phonemes as modeling units, we can further perform wordpiece biasing in addition to phoneme-based biasing by building a wordpiece FST in parallel to the phoneme FST.",
  "y": "similarities uses"
 },
 {
  "id": "8bd97eb118175c9fd2147b6456421c_11",
  "x": "As shown in the last column of Table 1 , the wordpiece model performs better than the grapheme model as in<cite> [6]</cite> . However, we note that the regression is significantly smaller than the all-phoneme model in [1<cite>6]</cite> .",
  "y": "differences"
 },
 {
  "id": "8bd97eb118175c9fd2147b6456421c_12",
  "x": "One potential approach to improve regression is to incorporate an English external language model for phonemes in rescoring, similarly to the wordpiece-based rescoring in [10] . However, we note that the regression is significantly smaller than the all-phoneme model in [1<cite>6]</cite> . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "8bd97eb118175c9fd2147b6456421c_13",
  "x": [
   "Biasing at the phoneme level enables us to avoid the OOV problem in the wordpiece model. Evaluating on a test set containing navigation queries to French place names, we show the proposed approach performs significantly better than a state-of-the-art grapheme and wordpiece model, by 16% and 8%, respectively in terms of relative WER reductions. Wordpiece biasing is complimentary to phoneme biasing and adds a further 2% reduction."
  ],
  "y": "future_work"
 },
 {
  "id": "8bdfc9e82e474413f29ee92f81467e_0",
  "x": "Our work is able to ground the system's textual response with language and images by learning the semantic correspondence between them while modelling long-term dialogue context. Lu et al., 2016) . In contrast to standard sequenceto-sequence models <cite>(Cho et al., 2014</cite>; Sutskever et al., 2014; Bahdanau et al., 2015) , HREDs model the dialogue context by introducing a context Recurrent Neural Network (RNN) over the encoder RNN, thus forming a hierarchical encoder.",
  "y": "differences"
 },
 {
  "id": "8bdfc9e82e474413f29ee92f81467e_2",
  "x": "For each m = 1, . . . , M n , we have hidden states of each module defined as: where f text \u03b8 ,f cxt \u03b8 and f dec \u03b8 are GRU cells<cite> (Cho et al., 2014)</cite> . \u03b8 represent model parameters, w n,m is the m-th word in the n-th utterance and g enc \u03b8 is a Convolutional Neural Network (CNN); here we use VGGnet (Simonyan and Zisserman, 2014) .",
  "y": "uses"
 },
 {
  "id": "8bdfc9e82e474413f29ee92f81467e_3",
  "x": "2 We used 512 as the word embedding size as well as hidden dimension for all the RNNs using GRUs<cite> (Cho et al., 2014)</cite> with tied embeddings for the (bidirectional) encoder and decoder. The decoder uses Luong-style attention mechanism (Luong et al., 2015) with input feeding. We trained our model with the Adam optimizer (Kingma and Ba, 2015) , with a learning rate of 0.0004 and clipping gradient norm over 5.",
  "y": "uses"
 },
 {
  "id": "8c26fb4c81c121103c1d5851edb41e_0",
  "x": "We use a modified cost function to mitigate inconsistencies in predictions associated with nonparametric ordinal regression. We also leverage several regularization techniques for deep neural networks to further improve model performance, such as residual con-nection (He et al., 2016) and batch normalization (Ioffe and Szegedy, 2015) . We conduct our experiments on Trafficking-10k<cite> (Tong et al., 2017)</cite> , a dataset of escort ads for which anti-trafficking experts assigned each sample one of seven ordered labels ranging from \"1: Very Unlikely (to come from traffickers)\" to \"7: Very Likely\".",
  "y": "uses background"
 },
 {
  "id": "8c26fb4c81c121103c1d5851edb41e_1",
  "x": "We conduct our experiments on Trafficking-10k<cite> (Tong et al., 2017)</cite> , a dataset of escort ads for which anti-trafficking experts assigned each sample one of seven ordered labels ranging from \"1: Very Unlikely (to come from traffickers)\" to \"7: Very Likely\". Our proposed model significantly outperforms previously published models<cite> (Tong et al., 2017)</cite> on Trafficking-10k as well as a variety of baseline ordinal regression models. In addition, we analyze the emojis used in escort ads with Word2Vec and t-SNE (van der Maaten and Hinton, 2008) , and we show that the lexicon of trafficking-related emojis can be subsequently expanded.",
  "y": "differences background"
 },
 {
  "id": "8c26fb4c81c121103c1d5851edb41e_2",
  "x": "Closest to our work is the Human Trafficking Deep Network (HTDN)<cite> (Tong et al., 2017)</cite> . HTDN has three main components: a language network that uses pretrained word embeddings and a long shortterm memory network (LSTM) to process text input; a vision network that uses a convolutional network to process image input; and another convolutional network to combine the output of the previous two networks and produce a binary classification. Compared to the language network in HTDN, our model replaces LSTM with a gatedfeedback recurrent neural network, adopts certain regularizations, and uses an ordinal regression layer on top. It significantly improves HTDN's benchmark despite only using text input.",
  "y": "differences background"
 },
 {
  "id": "8c26fb4c81c121103c1d5851edb41e_3",
  "x": "As in the work of E. <cite>Tong et al. (2017)</cite> , we pre-train word embeddings using a skip-gram model (Mikolov et al., 2013b) applied to unlabeled data from escort ads, however, we go further by analyzing the emojis' embeddings and thereby expand the trafficking lexicon. Ordinal regression: We briefly review ordinal regression before introducing the proposed methodology. We assume that the training data are",
  "y": "extends differences background"
 },
 {
  "id": "8c26fb4c81c121103c1d5851edb41e_4",
  "x": "Vector representations of words, also known as word embeddings, can be obtained through unsupervised learning on a large text corpus so that certain linguistic regularities and patterns are encoded. Unfortunately, the escort ads contain a plethora of emojis, acronyms, and (sometimes deliberate) typographical errors that are not encountered in more standard text data, which suggests that it is likely better to learn word embeddings from scratch on a large collection of escort ads instead of using previously published embeddings<cite> (Tong et al., 2017)</cite> .",
  "y": "background motivation"
 },
 {
  "id": "8c26fb4c81c121103c1d5851edb41e_5",
  "x": "Then we present a detailed comparison of our proposed model with commonly used ordinal regression models as well as the previous state-of-the-art classification model by E. <cite>Tong et al. (2017)</cite> . To assess the effect of each component in our model, we perform an ablation test where the components are swapped by their more standard alternatives one at a time. Next, we perform a qualitative analysis on the model predictions on the raw data, which are scraped from a different escort website than the one that provides the labeled training data.",
  "y": "uses"
 },
 {
  "id": "8c26fb4c81c121103c1d5851edb41e_6",
  "x": "**DATASETS** We use raw texts scraped from Backpage and TNABoard to pre-train the word embeddings, and use the same labeled texts E. <cite>Tong et al. (2017)</cite> used to conduct model comparisons. The raw text dataset consists of 44,105 ads from TNABoard and 124,220 ads from Backpage.",
  "y": "uses background"
 },
 {
  "id": "8c26fb4c81c121103c1d5851edb41e_7",
  "x": "The labeled dataset is called Trafficking-10k. It consists of 12,350 ads from Backpage labeled by experts in human trafficking detection 6<cite> (Tong et al., 2017)</cite> . Each label is one of seven ordered levels of likelihood that the corresponding ad comes from a human trafficker. Descriptions and sample proportions of the labels are in Table 1 . The original Trafficking-10K includes both texts and images, but as mentioned in Section 1, only the texts are used in our case. We apply the same preprocessing to Trafficking-10k as we do to raw data.",
  "y": "uses background"
 },
 {
  "id": "8c26fb4c81c121103c1d5851edb41e_8",
  "x": "To compare our model with the previous stateof-the-art classification model for escort ads, the Human Trafficking Deep Network (HTDN)<cite> (Tong et al., 2017)</cite> , we also polarize the true and predicted labels into two classes, \"1-4: Unlikely\" and \"5-7: Likely\"; then we compute the binary classification accuracy (Acc.) as well as the weighted binary classification accuracy (Wt. Acc.) The text data need to be vectorized before they can be fed into the baseline models (whereas vectorization is built into ORNN). The standard practice is to tokenize the texts using n-grams and then create weighted term frequency vectors using the term frequency (TF)-inverse document frequency (IDF) scheme (Beel et al., 2016; Manning et al., 2009) . The specific variation we use is the recommended unigram + sublinear TF + smooth IDF (Manning et al., 2009; Pedregosa et al., 2011) .",
  "y": "uses similarities"
 },
 {
  "id": "8c26fb4c81c121103c1d5851edb41e_9",
  "x": "Dimension reduction techniques such as Latent Semantic Analysis (Dumais, 2004) can be optionally applied to the frequency vectors, but B. Schuller et al. (2015) concluded from their experiments that dimension reduction on frequency vectors actually hurts model performance, which our preliminary experiments agree with. All models are trained and evaluated using the same (w.r.t. data shuffle and split) 10-fold crossvalidation (CV) on Trafficking-10k, except for HTDN, whose result is read from the original paper<cite> (Tong et al., 2017)</cite> 7 .",
  "y": "background"
 },
 {
  "id": "8c26fb4c81c121103c1d5851edb41e_10",
  "x": "Traffickers often avoid using explicit keywords when advertising victims, but instead use acronyms, intentional typos, and emojis<cite> (Tong et al., 2017)</cite> . Law enforcement maintains a lexicon of trafficking flags mapping certain emojis to their potential true meanings (e.g., the cherry emoji can indicate an underaged victim), but compiling such a lexicon manually is expensive, requires frequent updating, and relies on domain expertise that is hard to obtain (e.g., insider information from traffickers or their victims). To make matters worse, traffickers change their dictionaries over time and regularly switch to new emojis to replace certain keywords<cite> (Tong et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "8c26fb4c81c121103c1d5851edb41e_11",
  "x": "To make matters worse, traffickers change their dictionaries over time and regularly switch to new emojis to replace certain keywords<cite> (Tong et al., 2017)</cite> . In such a dynamic and adversarial environment, the need for a data-driven approach in updating the existing lexicon is evident. As mentioned in Section 3.1, training a skipgram model on a text corpus can map words (including emojis) used in similar contexts to similar numeric vectors.",
  "y": "background motivation"
 },
 {
  "id": "8c26fb4c81c121103c1d5851edb41e_12",
  "x": "the norm for sex traffickers to use escort websites to openly advertise their victims. We designed an ordinal regression neural network (ORNN) to predict the likelihood that an escort ad comes from a trafficker, which can drastically narrow down the set of possible leads for law enforcement. Our ORNN achieved the state-of-the-art performance on Trafficking-10K<cite> (Tong et al., 2017)</cite> , outperforming all baseline ordinal regression models as well as improving the classification accuracy over the Human Trafficking Deep Network<cite> (Tong et al., 2017)</cite> .",
  "y": "differences background"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_0",
  "x": "The reader can also require the text to focus on entities they have a particular interest in. Finally, we let the reader choose the style of the summary based on their favorite source of information, e.g., the writing style of a particular news source. Our work builds on sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) , which have been extensively applied to the task of abstractive summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; <cite>Paulus et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_1",
  "x": "These models are neural language models conditioned on an input document. The encoder module builds a representation of the input document and the decoder generates a summary by attending to the the source representation (Bahdanau et al., 2015) . Recent summarization models build upon pointer networks (Nallapati et al., 2016; See et al., 2017; <cite>Paulus et al., 2017)</cite> and have a few main architectural differences.",
  "y": "background"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_2",
  "x": "Recent summarization models build upon pointer networks (Nallapati et al., 2016; See et al., 2017; <cite>Paulus et al., 2017)</cite> and have a few main architectural differences. For instance, (See et al., 2017) pairs attention with a coverage mechanism to avoid repetition and<cite> (Paulus et al., 2017)</cite> relies on intra-decoder attention to enable generating coherent multi-sentence summaries. We introduce a controllable summarization model that provides a mechanism for users to specify high level attributes such as length, style, or entities of interest.",
  "y": "background"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_3",
  "x": "In contrast to previous work, we use multi-hop attention, such that the attention is added at each layer of the decoder. In addition to attending over encoder states (Bahdanau et al., 2015) , we also use intra-attention in the decoder to enable the model to refer back to previously generated words at any time scale. The mechanism allows the decoder to keep track of its progress and dissuades the decoder from generating repeated information (Vaswani et al., 2017; <cite>Paulus et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_4",
  "x": "Automatic summarization has been an active field of research for close to 60 years (Luhn, 1958) . Efforts in both extractive and abstractive methods have followed advances in the field of natural language processing, pattern recognition, and machine learning (Nenkova et al., 2011) . Recently, sequence-to-sequence neural networks (Sutskever et al., 2014) have been applied to abstractive summarization (Nallapati et al., 2016; See et al., 2017; <cite>Paulus et al., 2017)</cite> following their success in both machine translation (Bahdanau et al., 2015; Luong et al., 2015b) , parsing (Luong et al., 2015a) and image captioning (Vinyals et al., 2015b) .",
  "y": "background"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_5",
  "x": "Research in abstractive summarization with sequence-to-sequence models focuses on neural architectures (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; <cite>Paulus et al., 2017)</cite> and learning objectives<cite> (Paulus et al., 2017)</cite> . Summarization models have benefited from architectural advances in machine translation and related fields. For instance, attention mechanisms (Bahdanau et al., 2015) enable generation to focus on a targeted part of the source document.",
  "y": "background"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_6",
  "x": "Pointer mechanisms have been useful for abstractive summarization where copying entities and other rare words from the input is highly advantageous (See et al., 2017; <cite>Paulus et al., 2017)</cite> . Summarization also has distinct challenges. For instance, the generation of multi-sentence summaries differs from individual sentence translation: when doing left-to-right decoding, the decoder needs to be aware of its previous generation at a larger time scale, otherwise the network tends to produce repeated text.",
  "y": "background"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_7",
  "x": "Summarization also has distinct challenges. For instance, the generation of multi-sentence summaries differs from individual sentence translation: when doing left-to-right decoding, the decoder needs to be aware of its previous generation at a larger time scale, otherwise the network tends to produce repeated text. To address this impediment, (See et al., 2017) introduce coverage modeling,<cite> (Paulus et al., 2017)</cite> propose intra-decoder attention, and (Suzuki and Nagata, 2017) equip the decoder with an estimator of unigram frequency.",
  "y": "background"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_8",
  "x": "To address this impediment, (See et al., 2017) introduce coverage modeling,<cite> (Paulus et al., 2017)</cite> propose intra-decoder attention, and (Suzuki and Nagata, 2017) equip the decoder with an estimator of unigram frequency. Regarding learning objectives,<cite> (Paulus et al., 2017)</cite> investigates the potential improvement from replacing maximum likelihood training with reinforcement learning to optimize ROUGE, the most common automatic metric to assess summarization. They find that a combination of both losses to perform best according to human evaluations, as training with reinforcement learning alone tends to produce non-grammatical text.",
  "y": "background"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_9",
  "x": "Regarding learning objectives,<cite> (Paulus et al., 2017)</cite> investigates the potential improvement from replacing maximum likelihood training with reinforcement learning to optimize ROUGE, the most common automatic metric to assess summarization. Our model builds upon this previous work.",
  "y": "extends background"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_10",
  "x": "Our model builds upon this previous work. Following (Gehring et al., 2017) , we rely on convolutional networks, in contrast to previous work using recurrent networks (Nallapati et al., 2016; See et al., 2017; <cite>Paulus et al., 2017)</cite> . The convolutional networks enable faster training time and better batching.",
  "y": "differences"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_11",
  "x": "Like<cite> (Paulus et al., 2017)</cite> , we rely on intra-attention for generating multi-sentence text. We introduce multi-hop intra-attention inspired by multi-hop source attention from (Gehring et al., 2017) . This allows more complex attention patterns and improves accuracy.",
  "y": "similarities"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_12",
  "x": "Like<cite> (Paulus et al., 2017)</cite> , we share the word embeddings in the encoder and decoder lookup tables with the embeddings from the output layer of the decoder. This facilitates better copying of source entities. Contrary to<cite> (Paulus et al., 2017</cite>; See et al., 2017; Nallapati et al., 2016) , we rely on Byte-Pair Encoding (BPE) (Sennrich et al., 2016) to improve the model copy mechanism instead of using an additional pointer mechanism.",
  "y": "similarities"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_13",
  "x": "Like<cite> (Paulus et al., 2017)</cite> , we share the word embeddings in the encoder and decoder lookup tables with the embeddings from the output layer of the decoder. This facilitates better copying of source entities. Contrary to<cite> (Paulus et al., 2017</cite>; See et al., 2017; Nallapati et al., 2016) , we rely on Byte-Pair Encoding (BPE) (Sennrich et al., 2016) to improve the model copy mechanism instead of using an additional pointer mechanism.",
  "y": "differences"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_14",
  "x": "This facilitates better copying of source entities. Contrary to<cite> (Paulus et al., 2017</cite>; See et al., 2017; Nallapati et al., 2016) , we rely on Byte-Pair Encoding (BPE) (Sennrich et al., 2016) to improve the model copy mechanism instead of using an additional pointer mechanism. Contrary to<cite> (Paulus et al., 2017)</cite> , we did not explore training objectives and our training procedure aims at maximizing the likelihood of the training summaries given the source document.",
  "y": "differences"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_15",
  "x": "The statistics of the dataset are reported in Table 1 after limiting the length of the train documents to 400, as suggested by (See et al., 2017) . We evaluate on two versions of this dataset, the entity anonymized version (Hermann et al., 2015; Nallapati et al., 2016; <cite>Paulus et al., 2017)</cite> and the full text version (See et al., 2017) 1 . We use BPE with 30K types (Sennrich et al., 2016) .",
  "y": "uses"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_16",
  "x": "To avoid repetition, we prevent the decoder from generating the same trigram more than once during test, following<cite> (Paulus et al., 2017)</cite> . ---------------------------------- **EVALUATION**",
  "y": "uses"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_17",
  "x": "We evaluate using the standard ROUGE metric (Lin, 2004) and report the F1 scores for ROUGE-1, ROUGE-2, and ROUGE-L. We compare to existing abstractive baselines (Nallapati et al., 2016; See et al., 2017; <cite>Paulus et al., 2017)</cite> and report results on the Lead-3 extraction baseline which simply selects the first three sentences of the input article as its summary. ---------------------------------- **RESULTS**",
  "y": "uses"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_18",
  "x": "Table 3 : Summarization when setting the control variables to the ground truth, as if simulating true user preferences Our summarizer allows the user to control the length of the generated summary, the entities on which it focuses on, as well as the source style it imitates (see Section 2). First, we evaluate the effect of providing the true reference variables at decoding time. This simulates a user which expresses preferences through specifying values of Model ROUGE-1 ROUGE-2 ROUGE-L Lead-3 (Nallapati et al., 2017) 39.2 15.7 35.5 ML words-lvt2k-temp-att (Nallapati et al., 2016) 35.46 13.30 32.65 ML, no intra-attention<cite> (Paulus et al., 2017)</cite> 37.86 14.69 34.99 ML, with intra-attention<cite> (Paulus et al., 2017)</cite> 38 the control variables.",
  "y": "uses"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_19",
  "x": "For length and sourcestyle control, we set each variable to a constant value which maximizes ROUGE on the validation set. For entity control, we randomly sample an entity that appears in the first three sentences of the input document and provide it as the entity of interest. Table 4 shows results on the entity-anonymized version of the dataset used by (Nallapati et al., 2016; <cite>Paulus et al., 2017)</cite> and Table 5 reports results on the original version of the dataset used by (See et al., 2017) .",
  "y": "uses"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_20",
  "x": "In both cases, our method achieves a slight advantage over alternatives. On the original text, we report 39.75 F1-ROUGE1 as opposed to 39.53 for (See et al., 2017) . On the entity-anonymized text, we report 38.68 F1-ROUGE1 as opposed to 38.30 for the best maximum likelihood training setting of<cite> (Paulus et al., 2017)</cite> .",
  "y": "differences"
 },
 {
  "id": "8c57f595f1acecd03c32181cad444b_21",
  "x": "On the original text, we report 39.75 F1-ROUGE1 as opposed to 39.53 for (See et al., 2017) . On the entity-anonymized text, we report 38.68 F1-ROUGE1 as opposed to 38.30 for the best maximum likelihood training setting of<cite> (Paulus et al., 2017)</cite> . Our model does not outperform the reinforcement learning model of<cite> (Paulus et al., 2017)</cite> which optimizes ROUGE.",
  "y": "differences"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_0",
  "x": "On the other hand, we can impose limited architectural constraints in the form of selective rationalization<cite> (Lei et al., 2016</cite>; Li et al., 2016b; Chen et al., 2018a,b) where the goal is to only expose the portion of the text relevant for prediction. The selection is done by a separately trained model called rationale generator. The resulting text selection can be subsequently used as an input to an unconstrained, complex predictor, i.e., architectures used in the absence of any rationalization.",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_1",
  "x": "On the other hand, we can impose limited architectural constraints in the form of selective rationalization<cite> (Lei et al., 2016</cite>; Li et al., 2016b; Chen et al., 2018a,b) where the goal is to only expose the portion of the text relevant for prediction. In this paper, we build on and extend selective rationalization.",
  "y": "extends"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_2",
  "x": "This ensures that the selected rationale must contain all/most of the information about the target label, leaving out irrelevant parts, within size constraints imposed on the rationales. We also theoretically show that the equilibrium of the three-player game guarantees good properties for the extracted rationales. Moreover, we empirically show that (1) the three-player framework on its own helps cooperative games such as<cite> (Lei et al., 2016)</cite> to improve both predictive accuracy and rationale quality; (2) by combining the two solutions -introspective generator and the three player game -we can achieve high predictive accuracy and non-degenerate rationales.",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_3",
  "x": "Many previous works<cite> (Lei et al., 2016</cite>; Chen et al., 2018a) follows the above definition of rationales. In this work, we further define the complement of rationale, denoted as r c (X), as For notational ease, define",
  "y": "similarities background"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_4",
  "x": "plement predictor that predicts the probability of Y based on R c . Figure 1 (b) illustrates the basic three-player model. Compared with<cite> (Lei et al., 2016)</cite> , as shown in Figure 1(a) , the three-player model introduces an additional complement predictor, which plays a minimax game in addition to the cooperative game in<cite> (Lei et al., 2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_5",
  "x": "Without the complement predictor, and thus the loss Lg, the framework reduces to the method in<cite> (Lei et al., 2016)</cite> . Training During training, the three players perform gradient descent steps with respect to their own losses. For the generator, Since z(X) is a set of binary variables, we cannot apply the regular gradient descent algorithm.",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_6",
  "x": "Table 2 gives examples of the above tasks. Finally, as suggested by an anonymous reviewer, we evaluate on the text matching benchmark AskUbuntu, following<cite> Lei et al. (2016)</cite> . The experimental setting and results are reported in Appendix F.",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_7",
  "x": "Multi-aspect beer review This is the same data used in<cite> (Lei et al., 2016)</cite> . Each review evaluates multiple aspects of a kind of beer, including appearance, smell, palate, and an overall score. For each aspect, a rating \u2208 [0,1] is labeled.",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_8",
  "x": "---------------------------------- **MULTI-ASPECT BEER REVIEW** When only 10% of the words are used,<cite> Lei et al. (2016)</cite> has a significant performance downgrade compared to the accuracy when using the whole passage (82.05 v.s. 87.59).",
  "y": "differences"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_9",
  "x": "On the other hand, our introspection models are able to maintain higher predictive accuracy (86.16 v.s. 82.05) compared to<cite> (Lei et al., 2016)</cite> , while only sacrificing a little loss on highlighting precision (0.47% drop). Similar observations are made when 20% of the words required to highlight with one exception. Comparing the model of<cite> (Lei et al., 2016)</cite> with and without the proposed mini-max module, there is a huge gap of more than 5% on recall of generated rationales.",
  "y": "differences"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_10",
  "x": "Comparing the model of<cite> (Lei et al., 2016)</cite> with and without the proposed mini-max module, there is a huge gap of more than 5% on recall of generated rationales. This confirms the motivation that the original cooperative game tends to generate less comprehensive rationales, where the three-player framework controls the unselected words to be less informative so the recall is significantly improved. It is worth mentioning that when a classifier is trained with randomly highlighted rationales (i.e. random dropout (Srivastava et al., 2014) on the inputs), it performs significantly worse on both predictive accuracy and highlighting qualities.",
  "y": "differences"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_11",
  "x": "Moreover, our reimplementation of<cite> (Lei et al., 2016)</cite> for the original regression task achieves 90.1% precision when highlighting 10% words, which suggest that rationalization for binary classification is more challenging compared to the regression where finer supervision is available. In summary, our three-player framework consistently improves the quality of extracted rationales on both of the original<cite> (Lei et al., 2016)</cite> and the introspective framework. Particularly, without controlling the unselected words, the introspection model experiences a serious degeneration problem as expected.",
  "y": "extends"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_12",
  "x": "Moreover, our reimplementation of<cite> (Lei et al., 2016)</cite> for the original regression task achieves 90.1% precision when highlighting 10% words, which suggest that rationalization for binary classification is more challenging compared to the regression where finer supervision is available. In summary, our three-player framework consistently improves the quality of extracted rationales on both of the original<cite> (Lei et al., 2016)</cite> and the introspective framework. Particularly, without controlling the unselected words, the introspection model experiences a serious degeneration problem as expected.",
  "y": "differences"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_13",
  "x": "We report two classification results, which are the accuracy of the predictor and complement predictor. For both cooperative methods, i.e.<cite> (Lei et al., 2016)</cite> with and without introspection, we train an independent extra predictor on the unselected words from the generator, which does not affect the training of the generator-predictor framework. From the left part of Table 4 , we observe that the original model of<cite> (Lei et al., 2016</cite> ) has a hard time maintaining the accuracy compared to the classifier trained with full texts.",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_14",
  "x": "For both cooperative methods, i.e.<cite> (Lei et al., 2016)</cite> with and without introspection, we train an independent extra predictor on the unselected words from the generator, which does not affect the training of the generator-predictor framework. From the left part of Table 4 , we observe that the original model of<cite> (Lei et al., 2016</cite> ) has a hard time maintaining the accuracy compared to the classifier trained with full texts. Transforming it into a three-player game helps to improve the performance of the evaluation set while lower the accuracy of the complement predictor.",
  "y": "differences"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_15",
  "x": "---------------------------------- **HUMAN EVALUATION** We further conduct subjective evaluations by comparing the original model of<cite> (Lei et al., 2016)</cite> with our introspective threeplayer model.",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_16",
  "x": "To further demonstrate the quality of generated rationales, we provide some illustrative examples. Since there is a rich form of su- pervised signal, i.e., the number of class labels is large, the chance of any visible degeneration of the<cite> Lei et al. (2016)</cite> 's model should be low. However, we still spot quite a few cases.",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_17",
  "x": "However, we still spot quite a few cases. In the first example,<cite> Lei et al. (2016)</cite> fails to highlight the second entity while ours does. In the second example, the introspective three-player model selects more words than<cite> (Lei et al., 2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_18",
  "x": "However, we still spot quite a few cases. In the first example,<cite> Lei et al. (2016)</cite> fails to highlight the second entity while ours does. In the second example, the introspective three-player model selects more words than<cite> (Lei et al., 2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_19",
  "x": "According to Lemma 1.1, Lg can be rewritten as Table 1 and Degeneration Cases of<cite> (Lei et al., 2016)</cite> This section provides the details to obtain the results in Table 1 in the introduction section, where the method of<cite> (Lei et al., 2016)</cite> generates degenerated rationales. The method of<cite> (Lei et al., 2016)</cite> works well in many applications. However, as discussed in Section 1 and 2.2, all the cooperative rationalization approaches may suffer from the problem of degeneration.",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_20",
  "x": "Proof. According to Lemma 1.1, Lg can be rewritten as Table 1 and Degeneration Cases of<cite> (Lei et al., 2016)</cite> This section provides the details to obtain the results in Table 1 in the introduction section, where the method of<cite> (Lei et al., 2016)</cite> generates degenerated rationales. The method of<cite> (Lei et al., 2016)</cite> works well in many applications.",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_21",
  "x": "In this section, we design an experiment to confirm the existence of the problem in the original<cite> (Lei et al., 2016)</cite> model. We use the same single-beer review constructed from (McAuley et al., 2012) , as will be described in Appendix C. Instead of constructing a balanced binary classification task, we set the samples with scores higher than 0.5 as positive examples. On such a task, the prediction model with full inputs achieves 82.3% accuracy on the development set.",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_22",
  "x": "---------------------------------- **B EXPERIMENTAL SETUP OF EXAMPLES IN** During the training of<cite> (Lei et al., 2016)</cite> , we stipulate that the generated rationales are very concise: we punish it when the rationales have more than 3 pieces or more than 20% of the words are generated (both with hinge losses).",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_23",
  "x": "**B EXPERIMENTAL SETUP OF EXAMPLES IN** During the training of<cite> (Lei et al., 2016)</cite> , we stipulate that the generated rationales are very concise: we punish it when the rationales have more than 3 pieces or more than 20% of the words are generated (both with hinge losses). From the results, we can see that<cite> Lei et al. (2016)</cite> tends to predict color words, like dark-brown, yellow, as rationales.",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_24",
  "x": "Rationale from<cite> (Lei et al., 2016)</cite> (Acc: 76.4%): [\"dark-brown/black color\"] Rationale from our method (Acc: 80.4%): [\"huge tan\", \"thick lacing\"]",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_25",
  "x": "Rationale from<cite> (Lei et al., 2016)</cite> (Acc: 76.4%): [\"dark-brown/black color\"] Rationale from our method (Acc: 80.4%): [\"huge tan\", \"thick lacing\"]",
  "y": "differences"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_26",
  "x": "Original Text (negative): really cloudy , lots of sediment , washed out yellow color . looks pretty gross , actually , like swamp water . no head , no lacing . Rationale from<cite> (Lei et al., 2016)</cite> (Acc: 76.4%):",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_27",
  "x": "no head , no lacing . Rationale from<cite> (Lei et al., 2016)</cite> (Acc: 76.4%): [\"really cloudy lots\", \"yellow\", \"no\", \"no\"] Rationale from our method (Acc: 80.4%):",
  "y": "differences"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_28",
  "x": "This section explains how we designed the human study. The goal is to evaluate the unpredictable rates of the input texts after the rationales are removed. To this end, we mask the original texts with the rationales generated by<cite> (Lei et al., 2016)</cite> and our method.",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_29",
  "x": "---------------------------------- **F ADDITIONAL EXPERIMENTS ON ASKUBUNTU** Setting Following the suggestion from the reviews, we evaluate the proposed method on the question retrieval task on AskUbuntu<cite> (Lei et al., 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_30",
  "x": "AskUbuntu is a non-factoid question retrieval benchmark. The goal is to retrieve the most relevant questions from an input question. We use the same data split provided by<cite> (Lei et al., 2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_31",
  "x": "The former summarizes a problem from using Ubuntu, while the latter contains the detailed descriptions. In our experiments, we follow the same setting from<cite> (Lei et al., 2016)</cite> by only using the question bodies. Different from their work, we do not pretrain an encoder by predicting a question title using the corresponding question body.",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_32",
  "x": "The former summarizes a problem from using Ubuntu, while the latter contains the detailed descriptions. In our experiments, we follow the same setting from<cite> (Lei et al., 2016)</cite> by only using the question bodies. Different from their work, we do not pretrain an encoder by predicting a question title using the corresponding question body.",
  "y": "differences uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_33",
  "x": "This pipeline significantly stabilizes the training and provides better performances. 8 We use the same word embeddings as released by<cite> (Lei et al., 2016)</cite> . Results Table 9 summarizes the results.",
  "y": "uses"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_34",
  "x": "The original model from<cite> (Lei et al., 2016)</cite> fails to maintain the performance compared to the model trained with full texts. Adding the proposed minimax game helps both the<cite> (Lei et al., 2016</cite> ) and the introspection model to generate more informative texts as the rationales, which improves the MAP of the prediction while lowering the complement MAP. Compared to the other tasks, the complement MAPs on AskUbuntu are relatively large.",
  "y": "differences"
 },
 {
  "id": "8c7722ecab0d6a21e15ce63b8a47f5_35",
  "x": "We observe similar patterns as in previous datasets. The original model from<cite> (Lei et al., 2016)</cite> fails to maintain the performance compared to the model trained with full texts. Adding the proposed minimax game helps both the<cite> (Lei et al., 2016</cite> ) and the introspection model to generate more informative texts as the rationales, which improves the MAP of the prediction while lowering the complement MAP.",
  "y": "extends"
 },
 {
  "id": "8d3a20b4e50f81c94e884a0b978575_0",
  "x": "A combined case can allowed the user to add their own option to the list if needed [13] . Tasks can ask the user to refer to the whole image or to only certain parts or objects, and sometimes require them to annotate the regions of some relevant objects. The images can be natural scenes taken by real people, or artificial scenes created with clip arts like in <cite>[1]</cite> .",
  "y": "similarities"
 },
 {
  "id": "8dd8c0e61010d97d0ddae6d81a9067_0",
  "x": "Sentence generation, or surface realisation, is the task of generating meaningful, grammatically correct and fluent text from some abstract semantic or syntactic representation of the sentence. It is an important and growing field of natural language processing with applications in areas such as transferbased machine translation (Riezler and Maxwell, 2006) and sentence condensation (Riezler et al., 2003) . While recent work on generation in restricted domains, such as (Belz, 2007) , has shown promising results there remains much room for improvement particularly for broad coverage and robust generators, like those of Nakanishi et al. (2005) and<cite> Cahill and van Genabith (2006)</cite> , which do not rely on handcrafted grammars and thus can easily be ported to new languages.",
  "y": "background"
 },
 {
  "id": "8dd8c0e61010d97d0ddae6d81a9067_1",
  "x": "We are not aware of any similar work on generation. In the LFG-based generation algorithm presented by<cite> Cahill and van Genabith (2006)</cite> complex named entities (i.e. those consisting of more than one word token) and other multi-word units can be fragmented in the surface realization. We show that the identification of such units may be used as a simple measure to constrain the generation model's output.",
  "y": "background"
 },
 {
  "id": "8dd8c0e61010d97d0ddae6d81a9067_2",
  "x": "We are not aware of any similar work on generation. In the LFG-based generation algorithm presented by<cite> Cahill and van Genabith (2006)</cite> complex named entities (i.e. those consisting of more than one word token) and other multi-word units can be fragmented in the surface realization. We show that the identification of such units may be used as a simple measure to constrain the generation model's output.",
  "y": "motivation"
 },
 {
  "id": "8dd8c0e61010d97d0ddae6d81a9067_3",
  "x": "We take the generator of<cite> (Cahill and van Genabith, 2006)</cite> as our baseline generator. When tested on f-structures for all sentences from Section 23 of the Penn Wall Street Journal (WSJ) treebank (Mar-cus et al., 1993) , the techniques described in this paper improve BLEU score from 66.52 to 68.82. In addition, coverage is increased from 98.18% to almost 100% (99.96%).",
  "y": "uses"
 },
 {
  "id": "8dd8c0e61010d97d0ddae6d81a9067_4",
  "x": "**RELATED WORK ON STATISTICAL GENERATION** In (statistical) generators, sentences are generated from an abstract linguistic encoding via the application of grammar rules. These rules can be handcrafted grammar rules, such as those of (LangkildeGeary, 2002; Carroll and Oepen, 2005) , created semi-automatically (Belz, 2007) or, alternatively, extracted fully automatically from treebanks (Bangalore and Rambow, 2000; Nakanishi et al., 2005;<cite> Cahill and van Genabith, 2006)</cite> .",
  "y": "background"
 },
 {
  "id": "8dd8c0e61010d97d0ddae6d81a9067_5",
  "x": "T ree best := argmax Tree P (Tree|F-Str) (1) The generation model of<cite> (Cahill and van Genabith, 2006)</cite> maximises the probability of a tree given an f-structure (Eqn. 1), and the string generated is the yield of the highest probability tree. The generation process is guided by purely local information in the input f-structure: f-structure annotated CFG rules (LHS \u2192 RHS) are conditioned on their LHSs and on the set of features/attributes Feats = {a i |\u2203v j \u03c6(LHS)a i = v j } 3 \u03c6-linked to the LHS (Eqn.",
  "y": "background"
 },
 {
  "id": "8dd8c0e61010d97d0ddae6d81a9067_6",
  "x": "Grammar Rules Prob Given the input f-structure (for She accepted) in Figure 3 , (and assuming suitable generation rules for intransitive VPs and accepted) the model would produce the inappropriate highest probability tree of Figure 4 with an incorrect case for the pronoun in subject position. Figure 2: C-and f-structures with \u03c6 links for the sentence She hired her. To solve the problem,<cite> Cahill and van Genabith (2006)</cite> apply an automatic generation grammar transformation to their training data: they automatically label CFG nodes with additional case information and the model now learns the new improved generation rules of Tables 4 and 5 . Note how the additional case labelling subverts the problematic independence assumptions of the probability model and communicates the fact that a subject NP has to be realised as nominative case from the S \u2192 NP-nom VP production, via the intermediate NP-nom \u2192 PRP-nom, down to the lexical production PRP-nom \u2192 she.",
  "y": "background"
 },
 {
  "id": "8dd8c0e61010d97d0ddae6d81a9067_7",
  "x": "---------------------------------- **A HISTORY-BASED GENERATION MODEL** The automatic generation grammar transform presented in<cite> (Cahill and van Genabith, 2006)</cite> provides a solution to coarse-grained and (in fact) inappropriate independence assumptions in the basic generation model.",
  "y": "background"
 },
 {
  "id": "8dd8c0e61010d97d0ddae6d81a9067_8",
  "x": "However, the required information can easily be incorporated into the generation model by uniformly conditioning generation rules on their parent (mother) grammatical function, in addition to the local \u03c6-linked feature set. This additional conditioning has the effect of making the choice of generation rules sensitive to the history of the generation process, and, we argue, provides a simpler, more uniform, general, intuitive and natural probabilistic generation model obviating the need for CFG-grammar transforms in the original proposal of<cite> (Cahill and van Genabith, 2006)</cite> . In the new model, each generation rule is now conditioned on the LHS rule CFG category, the set of features \u03c6-linked to LHS and the parent grammatical function of the f-structure \u03c6-linked to LHS.",
  "y": "differences"
 },
 {
  "id": "8dd8c0e61010d97d0ddae6d81a9067_9",
  "x": "The mother grammatical function of the f-structure f 2 associated with node NP(\u2191SUBJ=\u2193) and its daughter NNP(\u2191=\u2193) (via the \u2191=\u2193 functional annotation) is SUBJ, as (\u03c6(M(n 2 ))SUBJ) = \u03c6(n 2 ), or equivalently (f 1 SUBJ) = f 2 . Given Figures 1 and 2 as training set, the improved model learns the generation rules (the mother grammatical function of the outermost f-structure is assumed to be a dummy TOP grammatical function) of Tables 6 and 7. F-Struct Feats Grammar Rules Note, that for our example the effect of the uniform additional conditioning on mother grammatical function has the same effect as the generation grammar transform of <cite>(Cahill and van Genabith, 2006</cite> ), but without the need for the gram- mar transform.",
  "y": "similarities differences"
 },
 {
  "id": "8dd8c0e61010d97d0ddae6d81a9067_10",
  "x": "Given the input f-structure in Figure 3 , the model will generate the correct string she accepted. In addition, uniform conditioning on mother grammatical function is more general than the case-phenomena specific generation grammar transform of<cite> (Cahill and van Genabith, 2006)</cite> , in that it applies to each and every sub-part of a recursive input f-structure driving generation, making available relevant generation history (context) to guide local generation decisions. The new history-based probabilistic generation model is defined as:",
  "y": "differences"
 },
 {
  "id": "8dd8c0e61010d97d0ddae6d81a9067_11",
  "x": "For the first experiments (type 1), the WSJ treebank training and test data were altered so that multi-word units are concatenated into single words (for example, New York becomes New York). As in<cite> (Cahill and van Genabith, 2006)</cite> fstructures are generated from the (now altered) treebank and from this data, along with the treebank trees, the PCFG-based grammar, which is used for training the generation model, is extracted. Similarly, the f-structures for the test and development sets are created from Penn Treebank trees which have been modified so that multi-word units form single units.",
  "y": "similarities uses"
 },
 {
  "id": "8dd8c0e61010d97d0ddae6d81a9067_12",
  "x": "In Table 10 , means significant at level 0.005. > means significant at level 0.05. In Table 10 , Baseline gives the results of the generation algorithm of<cite> (Cahill and van Genabith, 2006)</cite> .",
  "y": "uses"
 },
 {
  "id": "8e59c2c48e27b2abd5f63d6b4ce23d_0",
  "x": "Moreover, they are typically deployed as a pre-processing step before training the NMT model, hence, the predicted set of subword units are essentially not optimized for the translation task. Recently, <cite>(Cherry et al., 2018)</cite> extended the approach of NMT based on subword units to implement the translation model directly at the level of characters, which could reach comparable performance to the subword-based model, although this would require much larger networks which may be more difficult to train. The major reason to this requirement may lie behind the fact that treating the characters as individual tokens at the same level and processing the input sequences in linear time increases the difficulty of the learning task, where translation would then be modeled as a mapping between the characters in two languages.",
  "y": "background"
 },
 {
  "id": "8e59c2c48e27b2abd5f63d6b4ce23d_1",
  "x": "This can lead to morphological errors at different levels, and cause loss of semantic or syntactic information (Ataman et al., 2017) , due to the ambiguity in subword embeddings. In fact, recent studies have shown that the same approach can be extended to implement the NMT model directly at the level of characters, which could alleviate potential morphological errors due to subword segmentation. Although character-level NMT models have shown the potential to obtain comparable performance with subwordbased NMT models, this would require increasing the computational cost of the model, defined by the network parameters (Kreutzer and Sokolov, 2018;<cite> Cherry et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "8e59c2c48e27b2abd5f63d6b4ce23d_2",
  "x": "We evaluate decoding architectures using different levels of granularity in the vocabulary units and the attention mechanism, including the standard decoding architecture implemented either with subword (Sennrich et al., 2016) or fully character-level <cite>(Cherry et al., 2018)</cite> units, which constitute the baseline approaches, and the hierarchical decoding architecture, by implementing all in Pytorch (Paszke et al., 2017) within the OpenNMT-py framework (Klein et al., 2017) . In order to evaluate how each generative method performs in languages with different morphological typology, we model the machine translation task from English into five languages from different language families and exhibiting distinct morphological typology: Arabic (templatic), Czech (mostly fusional, partially agglutinative), German (fusional), Italian (fusional) and Turkish (agglutinative). We use the TED Talks corpora (Cettolo, 2012) for training the NMT models, which range from 110K to 240K sentences, and the official development and test sets from IWSLT 1 (Cettolo et al., 2017) .",
  "y": "uses"
 },
 {
  "id": "8e59c2c48e27b2abd5f63d6b4ce23d_3",
  "x": "This suggests that in case of high lexical sparsity, learning to translate based on representations of characters might aid in reducing contextual sparsity, allowing to learn better distributed representations. As the training data size increases, one would expect the likelihood of observing rare words to decrease, especially in languages with low morphological complexity, along with the significance of representing rare and unseen words <cite>(Cherry et al., 2018)</cite> . Our results support this hypothesis, where decreasing lexical sparsity, either in the form of the training data size, or the morphological complexity of the target language, eliminates the advantage of character-level translation.",
  "y": "background"
 },
 {
  "id": "8e59c2c48e27b2abd5f63d6b4ce23d_4",
  "x": "As the training data size increases, one would expect the likelihood of observing rare words to decrease, especially in languages with low morphological complexity, along with the significance of representing rare and unseen words <cite>(Cherry et al., 2018)</cite> . Our results support this hypothesis, where decreasing lexical sparsity, either in the form of the training data size, or the morphological complexity of the target language, eliminates the advantage of character-level translation. In Arabic and Italian, where the training data is almost twice as large as the other languages, using the hierarchical model provides improvements of 2.83 and 2.31 BLEU points over the character-level NMT model.",
  "y": "similarities"
 },
 {
  "id": "8e59c2c48e27b2abd5f63d6b4ce23d_5",
  "x": "In the experiments conducted in the English-to-German translation direction, the results of which are given in Table 3 , accuracy obtained with the hierarchical and subword-based NMT decoders significantly increase with the extension of the training data, where the subword-based model obtains the best accuracy, followed by the hierarchical model, and the character-level NMT model obtains significantly lower accuracy compared to both approaches. Studies have shown that character-level NMT models could potentially reach the same performance with the subword-based NMT models <cite>(Cherry et al., 2018)</cite> , although this might require increasing the capacity of the network. On the other hand, the consistency in Since solely relying on BLEU scores may not be sufficient in understanding the generative properties of different NMT models, we perform an additional evaluation in order to assess the capacity of models in learning syntactic or morphological dependencies using the Morpheval test suites, which consist of sentence pairs that differ by one morphological contrast, and each output accuracy is measured in terms of the percentage of translations that could convey the morphological contrast in the target language.",
  "y": "background"
 },
 {
  "id": "8e59c2c48e27b2abd5f63d6b4ce23d_6",
  "x": "In this paper, we limit the evaluation to recurrent architectures for comparison to previous work, including (Luong and Manning, 2016) , (Sennrich et al., 2016) and <cite>(Cherry et al., 2018)</cite> , and leave implementation of hierarchical decoding with feed-forward architectures to future work. ---------------------------------- **CONCLUSION**",
  "y": "uses"
 },
 {
  "id": "8e738a8f52e5931a92c9e4577a1ad3_0",
  "x": "Lexical simplification is the task to find and substitute a complex word or phrase in a sentence with its simpler synonymous expression. This task is a rather easier task which prepare a pair of complex and simple representations than a challenging task which changes the substitute pair in a given context<cite> (Specia et al., 2012</cite>; Kajiwara and Yamamoto, 2015) .",
  "y": "background"
 },
 {
  "id": "8e738a8f52e5931a92c9e4577a1ad3_1",
  "x": "**RELATED WORK** The evaluation dataset for the English Lexical Simplification task<cite> (Specia et al., 2012)</cite> Figure 1: A part of the dataset of Kajiwara and Yamamoto (2015) . notated on top of the evaluation dataset for English lexical substitution (McCarthy and Navigli, 2007) .",
  "y": "background"
 },
 {
  "id": "8e738a8f52e5931a92c9e4577a1ad3_2",
  "x": "In addition, De Belder and Moens (2012) built an evaluation dataset for English lexical simplification based on that developed by McCarthy and Navigli (2007) . Unlike the dataset of<cite> Specia et al. (2012)</cite> , sentences in their dataset contain at least one complex word, but they might contain more than one complex word.",
  "y": "background"
 },
 {
  "id": "8e738a8f52e5931a92c9e4577a1ad3_3",
  "x": "Again, it is not adequate for the automatic evaluation of lexical simplification because the human ranking of the resulting simplification might be affected by the context containing complex words. Furthermore, De Belder and Moens' (2012) dataset is too small to be used for achieving a reliable evaluation of lexical simplification systems. 3 Problems in previous datasets for Japanese lexical simplification Kajiwara and Yamamoto (2015) followed<cite> Specia et al. (2012)</cite> to construct an evaluation dataset for Japanese lexical simplification.",
  "y": "background"
 },
 {
  "id": "8e738a8f52e5931a92c9e4577a1ad3_4",
  "x": "These procedures were the same as for De Belder and Moens (2012) . Spearman's score of this work was lower than that of<cite> Specia et al. (2012)</cite> by 0.064.",
  "y": "differences"
 },
 {
  "id": "8e738a8f52e5931a92c9e4577a1ad3_5",
  "x": "3 Problems in previous datasets for Japanese lexical simplification Kajiwara and Yamamoto (2015) followed<cite> Specia et al. (2012)</cite> to construct an evaluation dataset for Japanese lexical simplification. Namely, they split the data creation process into two steps: substitute extraction and simplification ranking. These procedures were the same as for De Belder and Moens (2012) . Spearman's score of this work was lower than that of<cite> Specia et al. (2012)</cite> by 0.064.",
  "y": "background"
 },
 {
  "id": "8e738a8f52e5931a92c9e4577a1ad3_6",
  "x": "Because Kajiwara and Yamamoto (2015) extracted sentences from a newswire corpus, their dataset has a poor variety of expression. English lexical simplification datasets<cite> (Specia et al., 2012</cite>; De Belder and Moens, 2012) do not have this problem because both of them use a balanced corpus of English (Sharoff, 2006) . Complex words might exist in context.",
  "y": "background"
 },
 {
  "id": "8e738a8f52e5931a92c9e4577a1ad3_7",
  "x": "Therefore, in this work, we extract sentences containing only one complex word. Ties are not permitted in simplification ranking. When each annotator assigns a simplification ranking to a substitution list, a tie cannot be assigned in previous datasets<cite> (Specia et al., 2012</cite>; Kajiwara and Yamamoto, 2015) .",
  "y": "uses"
 },
 {
  "id": "8e738a8f52e5931a92c9e4577a1ad3_8",
  "x": "When each annotator assigns a simplification ranking to a substitution list, a tie cannot be assigned in previous datasets<cite> (Specia et al., 2012</cite>; Kajiwara and Yamamoto, 2015) . This deteriorates ranking consistency if some substitutes have a similar simplicity. De Belder and Moens (2012) allow ties in simplification ranking and report considerably higher agreement among annotators than<cite> Specia et al. (2012)</cite> .",
  "y": "background"
 },
 {
  "id": "8e738a8f52e5931a92c9e4577a1ad3_9",
  "x": "De Belder and Moens (2012) allow ties in simplification ranking and report considerably higher agreement among annotators than<cite> Specia et al. (2012)</cite> . The method of ranking integration is na\u00efve. Kajiwara and Yamamoto (2015) and<cite> Specia et al. (2012)</cite> use an average score to integrate rankings, but it might be biased by outliers.",
  "y": "background motivation"
 },
 {
  "id": "8e738a8f52e5931a92c9e4577a1ad3_10",
  "x": "Table 1 shows the characteristics of our dataset. It is about the same size as previous work<cite> (Specia et al., 2012</cite>; Kajiwara and Yamamoto, 2015) . Our dataset has two advantages: (1) improved correlation with human judgment by making a controlled and balanced dataset, and (2) enhanced consistency by allowing ties in ranking and removing outlier annotators.",
  "y": "similarities"
 },
 {
  "id": "8e738a8f52e5931a92c9e4577a1ad3_11",
  "x": "Table 2 shows the results of the ranking integration. Our method achieved better accuracy in ranking integration than previous methods<cite> (Specia et al., 2012</cite>; Kajiwara and Yamamoto, 2015) and is similar to the results from De Belder and Moens (2012) . This shows that the reliability score can be used for improving the quality.",
  "y": "differences"
 },
 {
  "id": "8fc0d25eb177ea876c2b69096f0145_0",
  "x": "**INTRODUCTION** Despite the existence of various Indonesian pretrained word embeddings, there are no publicly available Indonesian analogy task datasets on which to evaluate these embeddings. Consequently, it is unknown if Indonesian word embeddings introduced in, e.g.,<cite> (Al-Rfou et al., 2013)</cite> and (Grave et al., 2018) , capture syntactic or semantic information as measured by analogy tasks.",
  "y": "motivation"
 },
 {
  "id": "8fc0d25eb177ea876c2b69096f0145_1",
  "x": "Despite the existence of various Indonesian pretrained word embeddings, there are no publicly available Indonesian analogy task datasets on which to evaluate these embeddings. Consequently, it is unknown if Indonesian word embeddings introduced in, e.g.,<cite> (Al-Rfou et al., 2013)</cite> and (Grave et al., 2018) , capture syntactic or semantic information as measured by analogy tasks. Also, such embeddings are usually trained on Indonesian Wikipedia <cite>(Al-Rfou et al., 2013</cite>; Bojanowski et al., 2017) whose size is relatively small, approximately 60M tokens.",
  "y": "motivation background"
 },
 {
  "id": "8fc0d25eb177ea876c2b69096f0145_2",
  "x": "We refer to them as Wiki/fastText and CC/fastText hereinafter. We also used another two pretrained embeddings: polyglot embedding trained on Indonesian Wikipedia<cite> (Al-Rfou et al., 2013)</cite> and NLPL embedding trained on the Indonesian portion of CoNLL 2017 corpus (Fares et al., 2017) . For training our word embeddings, we used online news corpus obtained from Tempo.",
  "y": "uses"
 },
 {
  "id": "8ff1560ac0241a763b4b0d93718b40_0",
  "x": "The production of knowledge bases and the need to answer questions over such resources received researchers attentions to propose different models to find the answer of questions from the knowledge bases, known as KBQA 1 . Answering factoid questions with one relation, also known as simple question answering, has been widely studied in recent years (Dai et al., 2016;<cite> Yin et al., 2016</cite>; He and Golub, 2016; Yu et al., 2017) . A common approach that has been used in most of the researches is utilizing a two-component system, including an entity linker and a relation extractor. In this paper, we focus on the relation extraction component, which is also treated as a classification problem.",
  "y": "background"
 },
 {
  "id": "8ff1560ac0241a763b4b0d93718b40_1",
  "x": "Having a large number of relations in a knowledge base, however, this simple question relation extraction is not a solved problem yet. Classifying questions to predefined set of relations is one of the main approaches for this task (Mohammed et al., 2018) . Moreover, matching question content with relations has also been proposed and shown promising results<cite> (Yin et al., 2016</cite>; Yu et al., 2017) .",
  "y": "background"
 },
 {
  "id": "8ff1560ac0241a763b4b0d93718b40_2",
  "x": "One paradigm in proposed approaches for relation extraction in KBQA is based on semantic parsing in which questions were parsed and turned into logical forms in order to query the knowledge base (Berant et al., 2013; Berant and Liang, 2014) . From another point of view, two mainstreams for extracting relations in KBQA are studied: (a) using a classifier which chooses the most probable relation among all (Mohammed et al., 2018) ; (b) matching questions and relations through learning of an embedding space for representing all relations and question words (Bordes et al., 2015; Dai et al., 2016;<cite> Yin et al., 2016</cite>; He and Golub, 2016; Yu et al., 2017) , in which each relation is considered either as a meaningful sequence of words or as a unique entity.",
  "y": "background"
 },
 {
  "id": "8ff1560ac0241a763b4b0d93718b40_3",
  "x": "In this regard, following <cite>Yin et al. (2016)</cite> , we first extract the entity mentions out of question words and put a symbol (e.g. < e >) in its place, so that we will have a question pool in which each question is labeled with its relation that can be considered as a paraphrase of that question. For each new question (Q'), we find the most resemble question (Q) in our question pool, and assign its corresponding relation as the relation for Q'. To do so, we need a model to compute the resemblance of each pair of questions and find the most similar one to Q'.",
  "y": "uses"
 },
 {
  "id": "8ff1560ac0241a763b4b0d93718b40_4",
  "x": "**DATASET** Following the previous works by <cite>Yin et al. (2016)</cite> and Yu et al. (2017) , we use the common benchmark dataset of the simple question answering, namely SimpleQuestions, which was originally introduced by Bordes et al. (2015) . This dataset contains 108442 questions gathered with the help of English-speaking annotators.",
  "y": "uses"
 },
 {
  "id": "8ff1560ac0241a763b4b0d93718b40_5",
  "x": "This dataset contains 108442 questions gathered with the help of English-speaking annotators. <cite>Yin et al. (2016)</cite> proposed a new benchmark for evaluating relation extraction task on SimpleQuestion. In this benchmark, every question, whose entity is replaced by a unique token, is labeled with its ground truth relation as its positive label, and all other relation of the gold entity that is mentioned in the question are considered as negative labels.",
  "y": "background"
 },
 {
  "id": "8ff1560ac0241a763b4b0d93718b40_6",
  "x": "<cite>Yin et al. (2016)</cite> proposed a new benchmark for evaluating relation extraction task on SimpleQuestion. We use the same dataset which contains 72239, 10310 and 20610 question samples as train, validation, and test sets respectively.",
  "y": "uses"
 },
 {
  "id": "8ff1560ac0241a763b4b0d93718b40_7",
  "x": "In this table, AMPCNN<cite> (Yin et al., 2016)</cite> is an attentive max-pooling CNN for matching a question with all relations. APCNN (dos Santos et al., 2016) and ABCNN<cite> (Yin et al., 2016)</cite> both employ an attentive pooling mechanism. These two models Model Accuracy (%) AMPCNN<cite> (Yin et al., 2016)</cite> 91.3 OWA-ABCNN<cite> (Yin et al., 2016)</cite> 90.2 Proposed Q'-Q + Q'-R model 93.41 are not originally evaluated on relation prediction of simple questions.",
  "y": "differences"
 },
 {
  "id": "8ff1560ac0241a763b4b0d93718b40_8",
  "x": "Hier-Res-BiLSTM (HR-BiLSTM) (Yu et al., 2017) 93.3 Proposed Q'-Q + Q'-R model 93.41 are not originally evaluated on relation prediction of simple questions. In fact, the authors of AMPCNN<cite> (Yin et al., 2016)</cite> , conducted the corresponding experiments on a one-way-attention adaptation of these two models to compare them with the available methods in this task.",
  "y": "background"
 },
 {
  "id": "90522b5ac99d1657bf9af9d165c36e_0",
  "x": "This problem has lately raised severe concerns in the word embedding community (e.g., Hellrich and Hahn (2016b) ;<cite> Antoniak and Mimno (2018)</cite> ; Wendlandt et al. (2018) ) and is also of interest to the wider machine learning community due to the influence of probabilistic-and thus unstablemethods on experimental results (Reimers and Gurevych, 2017; Henderson et al., 2018) , as well as replicability and reproducibility (Ivie and Thain, 2018, pp. 63:3-4) . Stability is critical for studies examining the underlying semantic space as a more advanced form of corpus linguistics, e.g., tracking lexical change (Kim et al., 2014; Kulkarni et al., 2015; Hellrich et al., 2018) .",
  "y": "motivation"
 },
 {
  "id": "90522b5ac99d1657bf9af9d165c36e_1",
  "x": "Our analysis is based on three corpora of different sizes and investigates effects on both accuracy and stability. The inclusion of accuracy measurements and the larger size of our training corpora exceed prior work. We show how the choice of down-sampling strategies, a seemingly minor detail, leads to major differences in the characterization of SVD PPMI in recent studies (Hellrich and Hahn, 2017;<cite> Antoniak and Mimno, 2018)</cite> .",
  "y": "motivation"
 },
 {
  "id": "90522b5ac99d1657bf9af9d165c36e_2",
  "x": "Measuring word embedding stability can be linked to older research comparing distributional thesauri (Salton and Lesk, 1971) by the most similar words they contain for particular anchor words (Weeds et al., 2004; Padr\u00f3 et al., 2014) . Most stability experiments focused on repeatedly training the same algorithm on one corpus Hahn, 2016a,b, 2017;<cite> Antoniak and Mimno, 2018</cite>; Pierrejean and Tanguy, 2018; Chugh et al., 2018) , whereas Wendlandt et al. (2018) quantified stability by comparing word similarity for models trained with different algorithms. We follow the former approach, since we deem it more relevant for ensuring that study results can be replicated or reproduced.",
  "y": "background"
 },
 {
  "id": "90522b5ac99d1657bf9af9d165c36e_3",
  "x": "Reasonable metrical choices are, e.g., the Jaccard coefficient (Jaccard, 1912) between these sets<cite> (Antoniak and Mimno, 2018</cite>; Chugh et al., 2018) , or a percentage based coefficient (Hellrich and Hahn, 2016a,b; Wendlandt et al., 2018; Pierrejean and Tanguy, 2018) . We here use j@n, i.e., the Jaccard coefficient for the n most similar words. It depends on a set M of word embedding models, m, for which the n most similar words (by cosine) from a set A of anchor words, a, as provided by the 'most similar words' function msw(a, n, m), are compared:",
  "y": "background"
 },
 {
  "id": "90522b5ac99d1657bf9af9d165c36e_4",
  "x": "**CORPORA** The corpora used in most stability studies are relatively small. For instance, the largest corpus in<cite> Antoniak and Mimno (2018)</cite> contains 15M tokens, whereas the corpus used by Hellrich and Hahn (2017) and the largest corpus from Wendlandt et al. (2018) each contain about 60M tokens.",
  "y": "background"
 },
 {
  "id": "90522b5ac99d1657bf9af9d165c36e_5",
  "x": "The latter two corpora are far larger than common in stability studies, making our study the largest-scale evaluation of embedding stability we are aware of. All three corpora were tokenized, transformed to lower case and cleaned from punctuation. We used both the corpora as-is, as well as independently drawn random subsamples (see also Hellrich and Hahn (2016a) ;<cite> Antoniak and Mimno (2018)</cite> ) to simulate the arbitrary content selection in most corpora-texts could be removed or replaced with similar ones without changing the overall nature of a corpus, e.g., Wikipedia articles are continuously edited.",
  "y": "similarities uses"
 },
 {
  "id": "90522b5ac99d1657bf9af9d165c36e_6",
  "x": "As proposed by<cite> Antoniak and Mimno (2018)</cite> , we further modified our SVD PPMI implementation to use random numbers generated with a non-fixed seed for probabilistic down-sampling. A fixed seed would benefit reliability, but also act as a bias during all analyses-seed choice has been shown to cause significant differences in experimental results (Henderson et al., 2018) . Down-sampling strategies for df and ff can be chosen independently of each other, e.g., using probabilistic down-sampling for df together with weighted down-sampling for ff .",
  "y": "extends differences"
 },
 {
  "id": "90522b5ac99d1657bf9af9d165c36e_7",
  "x": "Our results show SVD WPPMI to be both highly reliable and accurate, especially on COHA, which has a size common in both stability studies and corpus linguistic applications. Diverging reports on SVD PPMI stability-described as perfectly reliable in Hellrich and Hahn (2017) , yet not in<cite> Antoniak and Mimno (2018)</cite> -can thus be explained by their difference in down-sampling options, i.e., no down-sampling or probabilistic down-sampling. GLOVE's high stability in other studies<cite> (Antoniak and Mimno, 2018</cite>; Wendlandt et al., 2018) seems to be counterbalanced by its low accuracy and also appears to be limited to training on small corpora.",
  "y": "background"
 },
 {
  "id": "90522b5ac99d1657bf9af9d165c36e_8",
  "x": "Our results show SVD WPPMI to be both highly reliable and accurate, especially on COHA, which has a size common in both stability studies and corpus linguistic applications. Diverging reports on SVD PPMI stability-described as perfectly reliable in Hellrich and Hahn (2017) , yet not in<cite> Antoniak and Mimno (2018)</cite> -can thus be explained by their difference in down-sampling options, i.e., no down-sampling or probabilistic down-sampling. GLOVE's high stability in other studies<cite> (Antoniak and Mimno, 2018</cite>; Wendlandt et al., 2018) seems to be counterbalanced by its low accuracy and also appears to be limited to training on small corpora.",
  "y": "background"
 },
 {
  "id": "90962438b8efa0f7a14fd050365310_0",
  "x": "The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples. A breakthrough has come in the form of research by McClosky et al. (2006a; 2006b ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model <cite>(Charniak and Johnson, 2005)</cite> . Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003) .",
  "y": "background"
 },
 {
  "id": "90962438b8efa0f7a14fd050365310_1",
  "x": "In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a; 2006b) . We parse the BNC (Burnard, 2000) in its entirety using the reranking parser of<cite> Charniak and Johnson (2005)</cite> . 1,000 BNC sentences are manually annotated for constituent structure, resulting in the first gold standard set for this corpus.",
  "y": "uses"
 },
 {
  "id": "90962438b8efa0f7a14fd050365310_2",
  "x": "Written text comprises 90% of the BNC: 75% non-fictional and 25% fictional. To facilitate parsing with a WSJ-trained parser, some reversible transformations were applied to the BNC data, e.g. British English spellings were converted to American English and neutral quotes disambiguated. The reranking parser of<cite> Charniak and Johnson (2005)</cite> was used to parse the BNC.",
  "y": "uses"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_0",
  "x": "As a result, infrequent yet semantically important construction types could be parsed with accuracies far below what one might expect. This shortcoming of aggregate parsing metrics was highlighted in a recent study by <cite>Rimell et al. (2009)</cite> , introducing a new parser evaluation corpus containing around 700 sentences annotated with unbounded dependencies in seven different grammatical constructions. This corpus was used to evaluate five state-of-the-art parsers for English, focusing on grammar-based and statistical phrase structure parsers.",
  "y": "background"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_1",
  "x": "When averaging over the seven constructions in the corpus, none of the parsers had an accuracy higher than 61%. In this paper, we extend the evaluation of <cite>Rimell et al. (2009)</cite> to two dependency parsers, MSTParser (McDonald, 2006) and MaltParser (Nivre et al., 2006a) , trained on data from the PTB, converted to Stanford typed dependencies (de Marneffe et al., 2006) , and combined with a simple post-processor to extract unbounded dependencies from the basic dependency tree. Extending the evaluation to dependency parsers is of interest because it sheds light on whether highly tuned grammars or computationally expensive parsing formalisms are necessary for extracting complex linguistic phenomena in practice.",
  "y": "extends"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_2",
  "x": "In this paper, we extend the evaluation of <cite>Rimell et al. (2009)</cite> to two dependency parsers, MSTParser (McDonald, 2006) and MaltParser (Nivre et al., 2006a) , trained on data from the PTB, converted to Stanford typed dependencies (de Marneffe et al., 2006) , and combined with a simple post-processor to extract unbounded dependencies from the basic dependency tree. Extending the evaluation to dependency parsers is of interest because it sheds light on whether highly tuned grammars or computationally expensive parsing formalisms are necessary for extracting complex linguistic phenomena in practice. Unlike the best performing grammar-based parsers studied in <cite>Rimell et al. (2009)</cite> , neither MSTParser nor MaltParser was developed specifically as a parser for English, and neither has any special mechanism for dealing with unbounded dependencies.",
  "y": "differences"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_3",
  "x": "Our evaluation ultimately shows that the recall of MSTParser and MaltParser on unbounded dependencies is much lower than the average (un)labeled attachment score for each system. Nevertheless, the two dependency parsers are found to perform only slightly worse than the best grammar-based parsers evaluated in <cite>Rimell et al. (2009)</cite> and considerably better than the other statistical parsers in that evaluation. Interestingly, though the two systems have similar accuracies overall, there is a clear distinction between the kinds of errors each system makes, which we argue is consistent with observations by McDonald and Nivre (2007) .",
  "y": "differences"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_4",
  "x": "An unbounded dependency involves a word or phrase interpreted at a distance from its surface position, where an unlimited number of clause boundaries may in principle intervene. The unbounded dependency corpus of <cite>Rimell et al. (2009)</cite> includes seven grammatical constructions: object extraction from a relative clause (ObRC), object extraction from a reduced relative clause (ObRed), subject extraction from a relative clause (SbRC), free relatives (Free), object questions (ObQ), right node raising (RNR), and subject extraction from an embedded clause (SbEm), all chosen for being relatively frequent and easy to identify in PTB trees. Examples of the constructions can be seen in Figure 1 .",
  "y": "background"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_5",
  "x": "Moreover, <cite>Rimell et al. (2009)</cite> show that, although individual types of unbounded dependencies may be rare, the unbounded dependency types in the corpus, considered as a class, occur in as many as 10% of sentences in the PTB. In <cite>Rimell et al. (2009)</cite> , five state-of-the-art parsers were evaluated for their recall on the goldstandard dependencies. Three of the parsers were based on grammars automatically extracted from the PTB: the C&C CCG parser (Clark and Curran, 2007) , the Enju HPSG parser (Miyao and Tsujii, 2005) , and the Stanford parser (Klein and Manning, 2003) .",
  "y": "background"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_6",
  "x": "---------------------------------- **DEPENDENCY PARSERS** In this paper we repeat the study of <cite>Rimell et al. (2009)</cite> for two dependency parsers, with the goal of evaluating how parsers based on dependency grammars perform on unbounded dependencies.",
  "y": "uses"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_7",
  "x": "**PARSER TRAINING** One important difference between MSTParser and MaltParser, on the one hand, and the best performing parsers evaluated in <cite>Rimell et al. (2009)</cite> , on the other, is that the former were never developed specifically as parsers for English. Instead, they are best understood as data-driven parser generators, that is, tools for generating a parser given a training set of sentences annotated with dependency structures.",
  "y": "differences"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_8",
  "x": "The Stanford scheme comes in several varieties, but because both parsers require the dependency structure for each sentence to be a tree, we had to use the so-called basic variety (de Marneffe et al., 2006) . It is well known that questions are very rare in the WSJ data, and <cite>Rimell et al. (2009)</cite> found that parsers trained only on WSJ data generally performed badly on the questions included in the evaluation corpus, while the C&C parser equipped with a model trained on a combination of WSJ and question data had much better performance. To investigate whether the performance of MSTParser and MaltParser on questions could also be improved by adding more questions to the training data, we trained one variant of each parser using data that was extended with 3924 questions taken from QuestionBank (QB) (Judge et al., 2006) .",
  "y": "background"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_9",
  "x": "In comparison to the five parsers evaluated in <cite>Rimell et al. (2009)</cite> , it is worth noting that MSTParser and MaltParser were trained on the same basic data as four of the five, but with a different kind of syntactic representation -dependency trees instead of phrase structure trees or theoryspecific representations from CCG and HPSG. It is especially interesting to compare MSTParser and MaltParser to the Stanford parser, which essentially produces the same kind of dependency structures as output but uses the original phrase structure trees from the PTB as input to training. For our experiments we used MSTParser with the same parsing algorithms and features as reported in .",
  "y": "differences uses"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_10",
  "x": "---------------------------------- **POST-PROCESSING** All the development and test sets in the corpus of <cite>Rimell et al. (2009)</cite> were parsed using MSTParser and MaltParser after part-of-speech tagging the input using SVMTool (Gim\u00e9nez and M\u00e0rquez, 2004 ) trained on section 2-21 of the WSJ data in Stanford basic dependency format.",
  "y": "uses"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_11",
  "x": "The evaluation was performed using the same criteria as in <cite>Rimell et al. (2009)</cite> . A dependency was considered correctly recovered if the goldstandard head and dependent were correct and the label was an \"acceptable match\" to the goldstandard label, indicating the grammatical function of the extracted element at least to the level of subject, passive subject, object, or adjunct. The evaluation in <cite>Rimell et al. (2009)</cite> took into account a wide variety of parser output formats, some of which differed significantly from the gold-standard.",
  "y": "uses"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_12",
  "x": "The evaluation in <cite>Rimell et al. (2009)</cite> took into account a wide variety of parser output formats, some of which differed significantly from the gold-standard. Since MSTParser and MaltParser produced Stanford dependencies for this experiment, evaluation required less manual examination than for some of the other parsers, as was also the case for the output of the Stanford parser in the original evaluation. However, a manual evaluation was still performed in order to resolve questionable cases.",
  "y": "background"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_13",
  "x": "The evaluation was performed using the same criteria as in <cite>Rimell et al. (2009)</cite> . The evaluation in <cite>Rimell et al. (2009)</cite> took into account a wide variety of parser output formats, some of which differed significantly from the gold-standard.",
  "y": "uses background"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_14",
  "x": "The ObQ score for C&C, MSTParser, and MaltParser is for a model trained with additional questions (without this C&C scored 27.5; MSTParser and MaltParser as in Table 1 ). a weighted macroaverage, where the constructions are weighted proportionally to their relative frequency in the PTB. WAvg excludes ObQ sentences, since frequency statistics were not available for this construction in <cite>Rimell et al. (2009)</cite> .",
  "y": "background"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_15",
  "x": "On the weighted average MaltParser scores 3.5 points higher, because the constructions on which it outperforms MSTParser are more frequent in the PTB, and because WAvg excludes ObQ, where MSTParser is more accurate. Table 2 shows the results for MSTParser and MaltParser in the context of the other parsers evaluated in <cite>Rimell et al. (2009)</cite> . 5 For the parsers 5 The average scores reported differ slightly from those in which have a model trained on questions, namely C&C, MSTParser, and MaltParser, the figure shown for ObQ sentences is that of the question model.",
  "y": "uses"
 },
 {
  "id": "9158f716efae0d1f38510dd0847c45_16",
  "x": "This is a pattern that has been observed in previous evaluations of the parsers and can be explained by the global learning and inference strategy of MSTParser and the richer feature space of MaltParser (McDonald and Nivre, 2007) . Perhaps more interestingly, the accuracies of MSTParser and MaltParser are only slightly below the best performing systems in <cite>Rimell et al. (2009)</cite> -C&C and Enju . This is true even though MSTParser and MaltParser have not been engineered specifically for English and lack special mechanisms for handling unbounded dependencies, beyond the simple post-processing heuristic used to extract them from the output trees.",
  "y": "differences"
 },
 {
  "id": "91c1a4ab0347fb8b11ff213a97e864_0",
  "x": "However, perceptron training with inexact search is less studied for bottom-up parsing and, more generally, inference over hypergraphs. In this paper, we generalize the violation-fixing perceptron of Huang et al. (2012) to hypergraphs and apply it to the cube-pruning parser of <cite>Zhang and McDonald (2012)</cite> . This results in the highest reported scores on WSJ evaluation set (UAS 93.50% and LAS 92.41% respectively) without the aid of additional resources.",
  "y": "uses"
 },
 {
  "id": "91c1a4ab0347fb8b11ff213a97e864_1",
  "x": "This idea was adapted to graph-based dependency parsers by <cite>Zhang and McDonald (2012)</cite> and shown to outperform left-to-right beam search. Both these examples, bottom-up approximate dependency and constituency parsing, can be viewed as specific instances of inexact hypergraph search. Typically, the approximation is accomplished by cube-pruning throughout the hypergraph (Chiang, 2007) .",
  "y": "background"
 },
 {
  "id": "91c1a4ab0347fb8b11ff213a97e864_2",
  "x": "Unlike sequential search, the impact on learning of approximate hypergraph search -as well as methods to mitigate any ill effects -has not been studied. Motivated by this, we develop online learning algorithms for inexact hypergraph search by generalizing the violation-fixing percepron of Huang et al. (2012) . We empirically validate the benefit of this approach within the cube-pruning dependency parser of <cite>Zhang and McDonald (2012)</cite> .",
  "y": "uses"
 },
 {
  "id": "91c1a4ab0347fb8b11ff213a97e864_3",
  "x": "The second baseline is the skip update, which also always updates at the root nodes but skips any non-violations. This is the strategy used by <cite>Zhang and McDonald (2012)</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "91c1a4ab0347fb8b11ff213a97e864_4",
  "x": "In our experiments, we compare the performance of the two violation-fixing update strategies against two baselines. This is the strategy used by <cite>Zhang and McDonald (2012)</cite> .",
  "y": "uses"
 },
 {
  "id": "91c1a4ab0347fb8b11ff213a97e864_5",
  "x": "We ran a number of experiments on the cubepruning dependency parser of <cite>Zhang and McDonald (2012)</cite> , <cite>whose</cite> search space can be represented as a hypergraph in which the nodes are the complete and incomplete states and the hyperedges are the instantiations of the two parsing rules in the Eisner algorithm (Eisner, 1996) . The feature templates we used are a superset of <cite>Zhang and McDonald (2012)</cite> . These features include first-, second-, and third-order features and their labeled counterparts, as well as valency features.",
  "y": "uses"
 },
 {
  "id": "91c1a4ab0347fb8b11ff213a97e864_6",
  "x": "We ran a number of experiments on the cubepruning dependency parser of <cite>Zhang and McDonald (2012)</cite> , <cite>whose</cite> search space can be represented as a hypergraph in which the nodes are the complete and incomplete states and the hyperedges are the instantiations of the two parsing rules in the Eisner algorithm (Eisner, 1996) . The feature templates we used are a superset of <cite>Zhang and McDonald (2012)</cite> . These features include first-, second-, and third-order features and their labeled counterparts, as well as valency features.",
  "y": "uses"
 },
 {
  "id": "91c1a4ab0347fb8b11ff213a97e864_7",
  "x": "This template examines the leftmost child and the rightmost child of a modifier simultaneously. All other highorder features of <cite>Zhang and McDonald (2012)</cite> only look at arcs on the same side of their head. We trained the parser with hamming-loss-augmented MIRA (Crammer et al., 2006) , following Martins et al. (2010) .",
  "y": "background"
 },
 {
  "id": "91c1a4ab0347fb8b11ff213a97e864_8",
  "x": "Table 1 displays the results. Our improved cube-pruned parser represents a significant improvement over the feature-rich transition-based parser of Zhang and Nivre (2011) with a large beam size. It also improves over the baseline cube-pruning parser without max-violation update strategies (<cite>Zhang and McDonald, 2012</cite>) , showing the importance of update strategies in inexact hypergraph search.",
  "y": "differences"
 },
 {
  "id": "926e7df3c367ae29da574ba465504f_0",
  "x": "In recent works, short phrases (Lau et al., 2011) or images (<cite>Aletras and Stevenson, 2013</cite>) have been used as alternatives. Particularly, images offer a language independent representation of the topic which can also be complementary to textual labels. Aletras et al. (2015) showed that the visual representation of a topic is as effective as the textual labels on retrieving information using a topic browser while it can be understood quickly by the users.",
  "y": "background"
 },
 {
  "id": "926e7df3c367ae29da574ba465504f_1",
  "x": "Aletras et al. (2015) showed that the visual representation of a topic is as effective as the textual labels on retrieving information using a topic browser while it can be understood quickly by the users. The method presented by <cite>Aletras and Stevenson (2013)</cite> selects an image from a small set of candidates by re-ranking them using an unsupervised graph-based method. <cite>It is</cite> an iterative method that has a runtime complexity of O(n 2 ) which makes it infeasible to run over large number of images.",
  "y": "background"
 },
 {
  "id": "926e7df3c367ae29da574ba465504f_2",
  "x": "Aletras et al. (2015) showed that the visual representation of a topic is as effective as the textual labels on retrieving information using a topic browser while it can be understood quickly by the users. The method presented by <cite>Aletras and Stevenson (2013)</cite> selects an image from a small set of candidates by re-ranking them using an unsupervised graph-based method. <cite>It is</cite> an iterative method that has a runtime complexity of O(n 2 ) which makes it infeasible to run over large number of images.",
  "y": "background"
 },
 {
  "id": "926e7df3c367ae29da574ba465504f_3",
  "x": "The method presented by <cite>Aletras and Stevenson (2013)</cite> selects an image from a small set of candidates by re-ranking them using an unsupervised graph-based method. Thus the scope of <cite>this method</cite> gets limited to solving a local problem of re-ordering a small set of candidate images for a given topic.",
  "y": "background"
 },
 {
  "id": "926e7df3c367ae29da574ba465504f_4",
  "x": "The method presented by <cite>Aletras and Stevenson (2013)</cite> selects an image from a small set of candidates by re-ranking them using an unsupervised graph-based method. Furthermore, <cite>its</cite> accuracy is limited by the recall of the information retrieval engine.",
  "y": "background"
 },
 {
  "id": "926e7df3c367ae29da574ba465504f_5",
  "x": "Data. We evaluate our model on the publicly available data set provided by <cite>Aletras and Stevenson (2013)</cite> . It consists of 300 topics generated using Wikipedia articles and news articles taken from the New York Times.",
  "y": "uses"
 },
 {
  "id": "926e7df3c367ae29da574ba465504f_6",
  "x": "Negative examples sampling. The 20 candidate image labels per topic are collected by <cite>Aletras and Stevenson (2013)</cite> using an information retrieval engine (Google). Hence most of them are expected to be relevant to the topic.",
  "y": "background"
 },
 {
  "id": "926e7df3c367ae29da574ba465504f_7",
  "x": "Evaluation Metrics. Our evaluation follows prior work (Lau et al., 2011; <cite>Aletras and Stevenson, 2013</cite>) using two metrics. The Top-1 average rating is the average human rating assigned to the top-ranked label proposed by the topic labeling method.",
  "y": "uses"
 },
 {
  "id": "926e7df3c367ae29da574ba465504f_8",
  "x": "---------------------------------- **RESULTS AND DISCUSSION** We compare our approach to the state-of-the-art method that uses Personalized PageRank (<cite>Aletras and Stevenson, 2013</cite>) to re-rank image candidates.",
  "y": "uses"
 },
 {
  "id": "926e7df3c367ae29da574ba465504f_9",
  "x": "We also test a relevant approach originally proposed for image annotation that learns a joint model of text and image features (Weston et al., 2010) . Finally, we test two versions of our own DNN using only either the caption (DNN (Topic+Caption)) or the visual information of the image (DNN (Topic+VGG)). We adapt the original method of <cite>Aletras and Stevenson (2013)</cite> to compute the PageRank scores of all the available images in the test set of each fold for each topic (Global PPR).",
  "y": "extends"
 },
 {
  "id": "926e7df3c367ae29da574ba465504f_10",
  "x": "**MODEL** Top-1 aver. rating nDCG-1 nDCG-3 nDCG-5 <cite>Global PPR</cite> (<cite>Aletras and Stevenson, 2013</cite>) 1 (<cite>Aletras and Stevenson, 2013</cite>) 2.24 --- The DNN (Topic+Caption) model that uses only textual information, obtains a Top-1 Average performance of 1.94. Incorporating visual information (VGG) improves it to 2.12 (DNN (Topic+Caption+VGG)).",
  "y": "differences"
 },
 {
  "id": "926e7df3c367ae29da574ba465504f_11",
  "x": [
   "**CONCLUSION** We presented a deep neural network that jointly models textual and visual information for the task of topic labeling with images. Our model is generic and works for any unseen pair of topic and image."
  ],
  "y": "differences"
 },
 {
  "id": "9272b3f7e628a156caed328d475d0c_0",
  "x": "It is only very recently that some groups of researchers looked into readability assessment in Bengali. They observed that English readability formulas did not work well on Bengali texts [11] , <cite>[21]</cite> . This observation is not surprising, because Bengali is very different than English.",
  "y": "background"
 },
 {
  "id": "9272b3f7e628a156caed328d475d0c_1",
  "x": "Two different lines of work focused on children and adult readability formulas. Recently Lahiri et al. showed moderate correlation between readability indices and formality score ( [10] ) in four different domains [14] . <cite>Sinha et al.</cite> classified English readability formulas into three broad categories -traditional methods, cognitively motivated methods, and machine learning methods <cite>[21]</cite> .",
  "y": "background"
 },
 {
  "id": "9272b3f7e628a156caed328d475d0c_2",
  "x": "We found only three lines of work that specifically looked into Bengali readability [6] , [11] , <cite>[21]</cite> . Das and Roychoudhury worked with a miniature model of two parameters in their pioneering study [6] . They found that the two-parameter model was a better predictor of readability than the one-parameter model.",
  "y": "background motivation"
 },
 {
  "id": "9272b3f7e628a156caed328d475d0c_3",
  "x": "Note, however, that Das and Roychoudhury's corpus was small (only seven documents), thereby calling into question the validity of their results. <cite>Sinha et al.</cite> alleviated these problems by considering six parameters instead of just two <cite>[21]</cite> . <cite>They</cite> further showed that English readability indices were inadequate for Bengali, and built <cite>their</cite> own readability model on 16 texts.",
  "y": "background"
 },
 {
  "id": "9272b3f7e628a156caed328d475d0c_4",
  "x": "An important limitation of our study is the small corpus size. We only have 30 annotated passages at our disposal, whereas Islam et al. [11] had around 300. But Islam et al.'s dataset is not annotated in as fine-grained a fashion as ours. Note also that our dataset is larger than both <cite>Sinha et al.'s</cite> 16document dataset <cite>[21]</cite> , and Das and Roychoudhury's seven document dataset [6] .",
  "y": "differences"
 },
 {
  "id": "92e43071b2a9b05b5d96277c1aa250_0",
  "x": "Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009;<cite> Jans et al., 2012</cite>; Pichotta and Mooney, 2014) . These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed.",
  "y": "background"
 },
 {
  "id": "92e43071b2a9b05b5d96277c1aa250_1",
  "x": "For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem-plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001) , Chambers and Jurafsky (2008) propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers and Jurafsky, 2009;<cite> Jans et al., 2012</cite>; Pichotta and Mooney, 2014) .",
  "y": "background"
 },
 {
  "id": "92e43071b2a9b05b5d96277c1aa250_2",
  "x": "They also introduce a competitive \"unigram model\" as a baseline for the narrative cloze task. To learn the restaurant script from our dataset, we implement the models of Chambers and Jurafsky (2008) and<cite> Jans et al. (2012)</cite> , as well as the unigram baseline of Pichotta and Mooney (2014) . To evaluate our success in learning the restaurant script, we perform a modified version of the narrative cloze task, predicting only verbs that we annotate as \"restaurant script-relevant\" and comparing the performance of each model.",
  "y": "similarities uses"
 },
 {
  "id": "92e43071b2a9b05b5d96277c1aa250_3",
  "x": "This section provides an overview of each of the different methods and parameter settings we employ to learn narrative chains from the Dinners from Hell corpus, starting with the original model (Chambers and Jurafsky, 2008) and extending to the modifications of<cite> Jans et al. (2012)</cite> . As part of this work, we are releasing a program called NaChos, our integrated Python implementation of each of the methods for learning narrative chains described in this section. 1",
  "y": "extends differences"
 },
 {
  "id": "92e43071b2a9b05b5d96277c1aa250_4",
  "x": "Skip N-gram By default, C(e 1 , e 2 ) is incremented if e 1 and e 2 occur anywhere within the same chain of events derived from a single coreference chain (skip-all); we also implement an option to restrict the distance between e 1 and e 2 to 0 though 5 intervening events (skip-0 through skip-5). <cite>(Jans et al., 2012)</cite> Coreference Chain Length The original model counts co-occurrences in all coreference chains; we include<cite> Jans et al. (2012)</cite> 's option to count over only the longest chains in each document, or to count only over chains of length 5 or greater (long). Count Threshold Because PMI favors low-count events, we add an option to set C(e 1 , e 2 ) to zero for any e 1 , e 2 for which C(e 1 , e 2 ) is below some threshold, T , up to 5.",
  "y": "extends differences"
 },
 {
  "id": "92e43071b2a9b05b5d96277c1aa250_5",
  "x": "e n , at insertion point k. The original model, proposed by Chambers and Jurafsky (2008) , predicts the event that maximizes unordered pmi, where V is the set of all observed events (the vocabulary) and C(e 1 , e 2 ) is symmetric. Two additional models are introduced by<cite> Jans et al. (2012)</cite> and we use them here, as well.",
  "y": "future_work"
 },
 {
  "id": "92e43071b2a9b05b5d96277c1aa250_6",
  "x": "In this way, we generate a total of 2,273 cloze tests. Scoring We employ three different scoring metrics: average rank (Chambers and Jurafsky, 2008) , mean reciprocal rank, and recall at 50 <cite>(Jans et al., 2012)</cite> . Baseline The baseline we use for the narrative cloze task is to rank events by frequency.",
  "y": "similarities"
 },
 {
  "id": "92e43071b2a9b05b5d96277c1aa250_7",
  "x": "The fact that several model settings outperform an informed baseline on average rank and mean reciprocal rank indicates that these methods may in general be applicable to smaller, domain-specific corpora. Furthermore, it is apparent from the results that the bigram probability models perform better overall than PMI-based models, a finding also reported in<cite> Jans et al. (2012)</cite> . This replication is futher evidence that these methods do in fact transfer.",
  "y": "differences"
 },
 {
  "id": "932a13e179da50c9189bd0c612cb9c_0",
  "x": "**INTRODUCTION** The paper presents a simple sentence-level translation pair extraction algorithm from comparable monolingual news data. It differs from similar algorithms that select translation correspondences explicitly at the document level (Fung and Cheung, 2004; Resnik and Smith, 2003; Snover et al., 2008; <cite>Munteanu and Marcu, 2005</cite>; Quirk et al., 2007; Utiyama and Isahara, 2003) .",
  "y": "differences"
 },
 {
  "id": "932a13e179da50c9189bd0c612cb9c_1",
  "x": "As a result, the early-stopping step fires earlier and becomes more effective. \u2022 Sentence-level filter: The word-overlap filter in<cite> (Munteanu and Marcu, 2005)</cite> has been implemented: for a sentence pair (S, T ) to be considered parallel the ratio of the lengths of the two sentences has to be smaller than two. Additionally, at least half of the words in each sentence have to have a translation in the other sentence based on the word-based lexicon.",
  "y": "uses"
 },
 {
  "id": "932a13e179da50c9189bd0c612cb9c_2",
  "x": "Here, the size of the target candidate set \u0398 is 61 736 sentences. All the techniques presented result in some improvement. The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions<cite> (Munteanu and Marcu, 2005)</cite> .",
  "y": "uses"
 },
 {
  "id": "932a13e179da50c9189bd0c612cb9c_3",
  "x": "In this paper, we have presented a novel sentencelevel pair extraction algorithm for comparable data. We use a simple symmetrized scoring function based on the Model-1 translation probability. With the help of an efficient implementation, it avoids any translation candidate selection at the document level (Resnik and Smith, 2003; Smith, 2002; Snover et al., 2008; Utiyama and Isahara, 2003; <cite>Munteanu and Marcu, 2005</cite>; Fung and Cheung, 2004) .",
  "y": "uses"
 },
 {
  "id": "932a13e179da50c9189bd0c612cb9c_4",
  "x": "Its usefulness for extracting parallel sentences is demonstrated on news data for two language pairs. Currently, we are working on a feature-rich approach<cite> (Munteanu and Marcu, 2005)</cite> to improve the sentence-pair selection accuracy. Feature functions will be 'light-weight' such that they can be computed efficiently in an incremental way at the sentence-level.",
  "y": "uses"
 },
 {
  "id": "939274ae40a68acc322b34d8f91f7e_0",
  "x": "Semantic relationships between named entities often span across multiple sentences. In order to extract inter-sentence relations, most approaches utilise distant supervision to automatically generate document-level corpora (Peng et al., 2017; Song et al., 2018) . Recently, <cite>Verga et al. (2018)</cite> introduced multi-instance learning (MIL) (Riedel et al., 2010; Surdeanu et al., 2012) to treat multiple mentions of target entities in a document.",
  "y": "background"
 },
 {
  "id": "939274ae40a68acc322b34d8f91f7e_1",
  "x": "The documentlevel graph is formed by connecting words with local dependencies from syntactic parsing and sequential information, as well as non-local dependencies from coreference resolution and other semantic dependencies (Peng et al., 2017) . We infer relations between entities using MIL-based bi-affine pairwise scoring function<cite> (Verga et al., 2018)</cite> on the entity node representations. Our contribution is threefold.",
  "y": "similarities uses"
 },
 {
  "id": "939274ae40a68acc322b34d8f91f7e_2",
  "x": "It then constructs a graph structure with words as nodes and labelled edges that correspond to local and non-local dependencies. Next, it encodes the graph structure using a stacked GCNN layer and classifies the relation between the target entities by applying MIL<cite> (Verga et al., 2018)</cite> to aggregate all 1 The dataset is publicly available at http://nactem. ac.uk/CHR/. mention pair representations.",
  "y": "uses similarities"
 },
 {
  "id": "939274ae40a68acc322b34d8f91f7e_3",
  "x": "Since each target entity can have multiple mentions in a document, we employ a multi-instance learning (MIL)-based classification scheme to aggregate the predictions of all target mention pairs using bi-affine pairwise scoring<cite> (Verga et al., 2018)</cite> . As shown in Figure 2 , each word i is firstly projected into two separate latent spaces using two-layered feed-forward neural networks (FFNN), which correspond to the first (head) or second (tail) argument of the target pair. , where x K i corresponds to the representation of the i-th word after |K| blocks of GCNN encoding, W (0) , W (1) are the parameters of two FFNNs for head and tail respectively and x head i , x tail i \u2208 R d are the resulted head/tail representations for the ith word.",
  "y": "similarities uses"
 },
 {
  "id": "939274ae40a68acc322b34d8f91f7e_4",
  "x": "For the CDR dataset, we performed hypernym filtering similar to Gu et al. (2017) and <cite>Verga et al. (2018)</cite> . In the CHR dataset, both directions were generated for each candidate chemical pair as chemicals can be either a reactant (first argument) or a product (second argument) in an interaction. ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "939274ae40a68acc322b34d8f91f7e_5",
  "x": "For the CDR dataset, we compare with five stateof-the-art models: SVM , ensemble of feature-based and neural-based models (Zhou et al., 2016a) , CNN and Maximum Entropy (Gu et al., 2017) , Piece-wise CNN (Li et al., 2018) and Transformer<cite> (Verga et al., 2018)</cite> . We additionally prepare and evaluate the following models: CNN-RE, a re-implementation from Kim (2014) and Zhou et al. (2016a) and RNN-RE, a reimplementation from Sahu and Anand (2018) . In all models we use bi-affine pairwise scoring to detect relations.",
  "y": "similarities uses"
 },
 {
  "id": "939274ae40a68acc322b34d8f91f7e_6",
  "x": "Unlike <cite>Verga et al. (2018)</cite>, we used the pre-trained word embeddings in place of sub-word embeddings to align with our word graphs. Due to the size of the CDR dataset, we merged the training and development sets to train the models, similarly to Xu et al. (2016a) and Gu et al. (2017) . We report the performance as the average of five runs with different parameter initialisation seeds in terms of precision (P), recall (R) and F1-score.",
  "y": "extends differences"
 },
 {
  "id": "939274ae40a68acc322b34d8f91f7e_7",
  "x": "They restricted the relation candidates in up to two-span sentences. <cite>Verga et al. (2018)</cite> considered multi-instance learning for document-level RE. Our work is different from <cite>Verga et al. (2018)</cite> in that we replace Transformer with a GCNN model for full-abstract encoding using non-local dependencies such as entity coreference.",
  "y": "extends differences"
 },
 {
  "id": "939274ae40a68acc322b34d8f91f7e_8",
  "x": "<cite>Verga et al. (2018)</cite> considered multi-instance learning for document-level RE. Our work is different from <cite>Verga et al. (2018)</cite> in that we replace Transformer with a GCNN model for full-abstract encoding using non-local dependencies such as entity coreference. GCNN was firstly proposed by Kipf and Welling (2017) and applied on citation networks and knowledge graph datasets.",
  "y": "background"
 },
 {
  "id": "93cf4d4fd9cd875e8e148d1bbd8e2c_1",
  "x": "First, we design and test multiple game-inspired novel interfaces for collecting human attention maps of where humans choose to look to answer questions from the large-scale VQA dataset (Antol et al., 2015) ; this VQA-HAT (Human ATtention) dataset will be released publicly. Second, we perform qualitative and quantitative comparison of the maps generated by state-of-the-art attention-based VQA models (Yang et al., 2015;<cite> Lu et al., 2016</cite> ) and a task-independent saliency baseline (Judd et al., 2009 ) against our human attention maps through visualizations and rank-order correlation. We find that machine-generated attention maps from the most accurate VQA model have a mean rank-correlation of 0.26 with human attention maps, which is worse than task-independent saliency maps that have a mean rank-correlation of 0.49.",
  "y": "background motivation"
 },
 {
  "id": "93cf4d4fd9cd875e8e148d1bbd8e2c_2",
  "x": "Attention-based models for VQA typically use convolutional neural networks to highlight relevant regions of image given a question. Stacked Attention Networks (SAN) proposed in (Yang et al., 2015) use LSTM encodings of question words to produce a spatial attention distribution over the convolutional layer features of the image. Hierarchical Co-Attention Network<cite> (Lu et al., 2016)</cite> generates multiple levels of image attention based on words, phrases and complete questions, and is the top entry on the VQA Challenge 1 as of the time of this submission.",
  "y": "background"
 },
 {
  "id": "93cf4d4fd9cd875e8e148d1bbd8e2c_3",
  "x": "Figure 6: Random samples of human attention (column 2) v/s machine-generated attention (columns 3-5). \u2022 Stacked Attention Network (SAN) (Yang et al., 2015) with two attention layers (SAN-2) 2 . \u2022 Hierarchical Co-Attention Network (HieCoAtt)<cite> (Lu et al., 2016)</cite> with word-level (HieCoAtt-W), phrase-level (HieCoAtt-P) and question-level (HieCoAtt-Q) attention maps; we evaluate all three maps 3 .",
  "y": "motivation"
 },
 {
  "id": "93cf4d4fd9cd875e8e148d1bbd8e2c_4",
  "x": "HieCoAtt-W<cite> (Lu et al., 2016)</cite> 0.246 \u00b1 0.004 HieCoAtt-P<cite> (Lu et al., 2016)</cite> 0.256 \u00b1 0.004 HieCoAtt-Q<cite> (Lu et al., 2016)</cite> 0.264 \u00b1 0.004 Table 2 : Mean rank-correlation coefficients (higher is better); error bars show standard error of means. HieCoAtt-W<cite> (Lu et al., 2016)</cite> 0.062 \u00b1 0.012 HieCoAtt-P<cite> (Lu et al., 2016)</cite> 0.048 \u00b1 0.010 HieCoAtt-Q<cite> (Lu et al., 2016)</cite> 0 Table 3 : Mean rank-correlation coefficients (higher is better) on the reduced set without center bias; error bars show standard error of means.",
  "y": "uses similarities"
 },
 {
  "id": "950263323d351bcb483be7cdf15a7e_0",
  "x": "Narayanan et al. (2009) performed sentiment analysis on conditional sentences. Our objective however is inclined towards wish and suggestion detection, rather than sentiment analysis. Wish Detection: <cite>Goldberg et al. (2009)</cite> performed wish detection on datasets obtained from political discussion forums and product reviews.",
  "y": "background"
 },
 {
  "id": "950263323d351bcb483be7cdf15a7e_1",
  "x": "Wish Detection: <cite>Goldberg et al. (2009)</cite> performed wish detection on datasets obtained from political discussion forums and product reviews. <cite>They automatically extracted</cite> sentence templates from a corpus of new year wishes, and used them as features with a statistical classifier. Suggestion Detection: Ramanand et al. (2010) pointed out that wish is a broader category, which might not bear suggestions every time.",
  "y": "background"
 },
 {
  "id": "950263323d351bcb483be7cdf15a7e_2",
  "x": "Wish Detection: <cite>Goldberg et al. (2009)</cite> performed wish detection on datasets obtained from political discussion forums and product reviews. <cite>They automatically extracted</cite> sentence templates from a corpus of new year wishes, and used them as features with a statistical classifier. None of these works aligned the problem of wish and suggestion detection with subjunctive mood, or identified features related to it. Wish and suggestion detection remain young problems, and our work contributes towards the same.",
  "y": "motivation"
 },
 {
  "id": "950263323d351bcb483be7cdf15a7e_3",
  "x": "\u2022 Wish Detection Oxford dictionary defines the noun wish as, A desire or hope for something to happen. <cite>Goldberg et al. (2009)</cite> follow this definition of wish and provide <cite>manually annotated datasets</cite>, where each sentence is labelled as wish or non-wish. <cite>Following two datasets</cite> are made available: a. <cite>Political Discussions</cite>: 6379 sentences, out of which 34% are annotated wishes.",
  "y": "background"
 },
 {
  "id": "950263323d351bcb483be7cdf15a7e_4",
  "x": "<cite>Goldberg et al. (2009)</cite> follow this definition of wish and provide <cite>manually annotated datasets</cite>, where each sentence is labelled as wish or non-wish. <cite>Following two datasets</cite> are made available: a. <cite>Political Discussions</cite>: 6379 sentences, out of which 34% are annotated wishes. b. <cite>Product Reviews</cite>: 1235 sentences, out of which 12% are annotated as wishes. Table 1 presents some examples from <cite>these datasets</cite>.",
  "y": "background"
 },
 {
  "id": "950263323d351bcb483be7cdf15a7e_5",
  "x": "Table 1 presents some examples from <cite>these datasets</cite>. Ramanand et al. (2010) worked on <cite>product review dataset</cite> of the <cite>wish corpus</cite>, with an objective to extract suggestions for improvements. They considered suggestions as a subset of wishes, and thus retained the labels of only suggestion bearing wishes.",
  "y": "background"
 },
 {
  "id": "950263323d351bcb483be7cdf15a7e_6",
  "x": "They considered suggestions as a subset of wishes, and thus retained the labels of only suggestion bearing wishes. They also annotated additional product reviews, but their data is not available for open research. \u2022 Suggestion Detection Product reviews (new): We re-annotated the <cite>product review dataset</cite> from <cite>Goldberg et al. (2009)</cite> , for suggestions.",
  "y": "extends"
 },
 {
  "id": "950263323d351bcb483be7cdf15a7e_7",
  "x": "\u2022 Suggestion Detection Product reviews (new): We re-annotated the <cite>product review dataset</cite> from <cite>Goldberg et al. (2009)</cite> , for suggestions. This also includes wishes for improvements and new features. Out of 1235 sentences, 6% are annotated as suggestions.",
  "y": "extends"
 },
 {
  "id": "950263323d351bcb483be7cdf15a7e_9",
  "x": "5 Subjunctive Feature Evaluation <cite>Goldberg et al. (2009)</cite> evaluated <cite>their approach</cite> using a 10 fold cross validation on <cite>their datasets</cite>. In order to compare subjunctive features against <cite>their wish template features</cite>, we also perform 10 fold cross validation on <cite>their wish datasets</cite> (politics and products). The evaluation metrics include Precision, Recall, and Area Under Curve (AUC) for the positive class.",
  "y": "background"
 },
 {
  "id": "950263323d351bcb483be7cdf15a7e_10",
  "x": "the size of feature vector is not more than 1000. 5 Subjunctive Feature Evaluation <cite>Goldberg et al. (2009)</cite> evaluated <cite>their approach</cite> using a 10 fold cross validation on <cite>their datasets</cite>. In order to compare subjunctive features against <cite>their wish template features</cite>, we also perform 10 fold cross validation on <cite>their wish datasets</cite> (politics and products).",
  "y": "uses"
 },
 {
  "id": "950263323d351bcb483be7cdf15a7e_11",
  "x": "The evaluation metrics include Precision, Recall, and Area Under Curve (AUC) for the positive class. AUC was also used by <cite>Goldberg et al. (2009)</cite> . To the best of our knowledge, statistical classification based approach have not yet been employed to detect suggestions in reviews.",
  "y": "background"
 },
 {
  "id": "950263323d351bcb483be7cdf15a7e_12",
  "x": "Table 2 compares the AUC values obtained with unigrams, subjunctive features, a combination of both, and the results from <cite>Goldberg et al. (2009)</cite> for wish detection. Table 3 compares the AUC values obtained with unigrams, subjunctive features, and a combination of both for suggestion detection. Table 4 presents some of the top features used by the classifier.",
  "y": "similarities differences"
 },
 {
  "id": "950263323d351bcb483be7cdf15a7e_13",
  "x": "Wish Detection: Unigrams vs Subjunctive: One probable reason for the better performance of subjunctive features over unigrams in the case of <cite>product dataset</cite>, could be the small size of the dataset. In the case of <cite>politics dataset</cite>, similar reason (big dataset) can be attributed for the better performance of unigrams over subjunctive features. <cite>Goldberg et al. (2009)</cite> perform better than our subjunctive features for the politics data.",
  "y": "differences"
 },
 {
  "id": "950263323d351bcb483be7cdf15a7e_14",
  "x": "**RESULTS AND DISCUSSION** Wish Detection: Unigrams vs Subjunctive: One probable reason for the better performance of subjunctive features over unigrams in the case of <cite>product dataset</cite>, could be the small size of the dataset. In the case of <cite>politics dataset</cite>, similar reason (big dataset) can be attributed for the better performance of unigrams over subjunctive features.",
  "y": "differences"
 },
 {
  "id": "950263323d351bcb483be7cdf15a7e_15",
  "x": "In the case of <cite>politics dataset</cite>, similar reason (big dataset) can be attributed for the better performance of unigrams over subjunctive features. <cite>Goldberg et al. (2009)</cite> perform better than our subjunctive features for the politics data. However, subjunctive features perform much better with product data as compared to the wish templates (Table 3 ).",
  "y": "differences"
 },
 {
  "id": "950263323d351bcb483be7cdf15a7e_16",
  "x": "In the case of <cite>politics dataset</cite>, similar reason (big dataset) can be attributed for the better performance of unigrams over subjunctive features. <cite>Goldberg et al. (2009)</cite> perform better than our subjunctive features for the politics data. However, subjunctive features perform much better with product data as compared to the wish templates (Table 3 ).",
  "y": "differences"
 },
 {
  "id": "950cc4a7fa2db3aa6786cc0ae802b5_0",
  "x": "The ability of users to vote for their favourite answer makes these sites a valuable source of training data for open-domain non-factoid QA systems. In this paper, we present a neural approach to open-domain non-factoid QA, focusing on the subtask of answer reranking, i.e. given a list of candidate answers to a question, order the answers according to their relevance to the question. We test our approach on the Yahoo! Answers dataset of manner or How questions introduced by<cite> Jansen et al. (2014)</cite> , who describe answer reranking experiments on this dataset using a diverse range of features incorporating syntax, lexical semantics and discourse.",
  "y": "uses"
 },
 {
  "id": "950cc4a7fa2db3aa6786cc0ae802b5_1",
  "x": "The best performance on this dataset -33.01 P@1 and 53.96 MRR -is reported by Fried et al. (2015) who improve on the lexical semantic models of<cite> Jansen et al. (2014)</cite> by exploiting indirect associations between words using higher-order models. In contrast, our approach is very simple and requires no feature engineering. Question-answer pairs are represented by concatenated distributed representation vectors and a multilayer perceptron is used to compute the score for an answer (the probability of an answer being the best answer to the question).",
  "y": "background"
 },
 {
  "id": "950cc4a7fa2db3aa6786cc0ae802b5_2",
  "x": "For comparison with recent work in answer reranking<cite> (Jansen et al., 2014</cite>; Sharp et al., 2015) , we also evaluate the averaged word embedding vectors obtained with the skip-gram model (Mikolov et al., 2013 ) (henceforth referred to as the SkipAvg model). ---------------------------------- **EXPERIMENTS**",
  "y": "uses background"
 },
 {
  "id": "950cc4a7fa2db3aa6786cc0ae802b5_3",
  "x": "---------------------------------- **DATA** In order to be able to compare our work with previous research, we use the Yahoo! Answers dataset that was first introduced by<cite> Jansen et al. (2014)</cite> and was later used by Sharp et al. (2015) and Fried et al. (2015) .",
  "y": "uses"
 },
 {
  "id": "950cc4a7fa2db3aa6786cc0ae802b5_4",
  "x": "Further information about the dataset can be found in<cite> Jansen et al. (2014)</cite> . Our approach requires unlabelled data for unsupervised pre-training of the word and paragraph vectors. For these purposes we use the L6 Yahoo! Answers Comprehensive Questions and Answers corpus obtained via Webscope.",
  "y": "background"
 },
 {
  "id": "950cc4a7fa2db3aa6786cc0ae802b5_5",
  "x": "In order to be able to compare our work with previous research, we use the Yahoo! Answers dataset that was first introduced by<cite> Jansen et al. (2014)</cite> and was later used by Sharp et al. (2015) and Fried et al. (2015) . Further information about the dataset can be found in<cite> Jansen et al. (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "950cc4a7fa2db3aa6786cc0ae802b5_6",
  "x": "5 The Yahoo! Answers manner question dataset prepared by<cite> Jansen et al. (2014)</cite> and described in the previous paragraph, was initially sampled from this larger dataset. We want to emphasize that the L6 dataset is only used for unsupervised pretraining -no meta-information is used in our experiments. We also experiment with the English Gigaword corpus, 6 which contains data from several English newswire sources.",
  "y": "background"
 },
 {
  "id": "950cc4a7fa2db3aa6786cc0ae802b5_7",
  "x": "For these purposes we use the L6 Yahoo! Answers Comprehensive Questions and Answers corpus obtained via Webscope. 5 The Yahoo! Answers manner question dataset prepared by<cite> Jansen et al. (2014)</cite> and described in the previous paragraph, was initially sampled from this larger dataset.",
  "y": "uses"
 },
 {
  "id": "950cc4a7fa2db3aa6786cc0ae802b5_8",
  "x": "Following<cite> Jansen et al. (2014)</cite> and Fried et al. (2015) , we implement two baselines: the baseline that selects an answer randomly and the candidate retrieval (CR) baseline. The CR baseline uses the same scoring as in<cite> Jansen et al. (2014)</cite> : the questions and the candidate answers are represented using tf-idf (Salton, 1991) over lemmas; the candidate answers are ranked according to their cosine similarity to the respective question. We use the gensim 7 implementation of the DBOW and DM paragraph vector models.",
  "y": "uses"
 },
 {
  "id": "950cc4a7fa2db3aa6786cc0ae802b5_9",
  "x": "**EXPERIMENTAL SETUP** Following<cite> Jansen et al. (2014)</cite> and Fried et al. (2015) , we implement two baselines: the baseline that selects an answer randomly and the candidate retrieval (CR) baseline. The CR baseline uses the same scoring as in<cite> Jansen et al. (2014)</cite> : the questions and the candidate answers are represented using tf-idf (Salton, 1991) over lemmas; the candidate answers are ranked according to their cosine similarity to the respective question.",
  "y": "uses"
 },
 {
  "id": "950cc4a7fa2db3aa6786cc0ae802b5_10",
  "x": "**RESULTS** In Table 1 , we report best development P@1 and MRR of the multilayer perceptron trained on Yahoo! Answers<cite> (Jansen et al., 2014)</cite> 200 outperforming the rest by a small margin. The multilayer perceptron with combinations of different distributed representations reach slightly higher P@1 and MRR on the development set.",
  "y": "uses"
 },
 {
  "id": "950cc4a7fa2db3aa6786cc0ae802b5_11",
  "x": "Table 2 presents the results on the test set. We only evaluate the 200-dimension DBOW model and its combinations with other models, comparing these to the baselines and the previous results on the same dataset (we use the same train/dev/test split as<cite> Jansen et al. (2014)</cite> ). The DBOW outperforms the baselines by a statistically significant margin.",
  "y": "uses"
 },
 {
  "id": "95883b369c4b019fa98493a728c1a0_0",
  "x": "We find that there are significant caveats to the simple interpretation of these metrics as proposed, and that some applications of these metrics in well-cited works may be erroneous. Specifically, we find that the bias metric proposed by <cite>(Bolukbasi et al. 2016 )</cite> is highly sensitive to embedding hyperparameter selection, and that in many cases, the variance due to the selection of some hyper-parameters, notably the embedding space dimensionality, is greater than the variance in the metric due to corpus selection, while in fewer cases, even the relative rankings of the bias measured in the embedding spaces of various corpora varies with hyper-parameter selection. In light of these observations, it may be the case that bias estimates should not be thought to directly reflect the properties of the underlying corpus, but rather the properties of the specific embedding spaces in question, particularly in the context of hyper-parameter selections used to generate them.",
  "y": "differences"
 },
 {
  "id": "95883b369c4b019fa98493a728c1a0_1",
  "x": "Initial attempts have been made to develop metrics which seek to describe the geometric properties of the embedding space with respect to various axes of interest which are empirically determined to correspond to our intuitions of the hypothetical biases under study to quantify the degrees to which various biases exist within the embedding space, and presumably, the underlying text corpus (<cite>Bolukbasi et al. 2016</cite>; Caliskan, Bryson, and Narayanan 2017; Garg et al. 2018) . For instance, using such a bias measure, <cite>(Bolukbasi et al. 2016)</cite> concluded that \"word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent\". To the end of quantifying these biases, (Caliskan, Bryson, and Narayanan 2017) proposed and used a bias measure to report that \"text corpora contain recoverable and accurate imprints of our historic biases\".",
  "y": "background"
 },
 {
  "id": "95883b369c4b019fa98493a728c1a0_2",
  "x": "Qualitatively, when inspecting term-analogical relationships as is done in (Garg et al. 2018) , it is difficult to deny the existence of these biases, but the task of quantitatively capturing them in a canonical measure has been a topic of recent study. Initial attempts have been made to develop metrics which seek to describe the geometric properties of the embedding space with respect to various axes of interest which are empirically determined to correspond to our intuitions of the hypothetical biases under study to quantify the degrees to which various biases exist within the embedding space, and presumably, the underlying text corpus (<cite>Bolukbasi et al. 2016</cite>; Caliskan, Bryson, and Narayanan 2017; Garg et al. 2018) . For instance, using such a bias measure, <cite>(Bolukbasi et al. 2016)</cite> concluded that \"word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent\".",
  "y": "background"
 },
 {
  "id": "95883b369c4b019fa98493a728c1a0_3",
  "x": "---------------------------------- **PROPERTIES OF THE BIAS MEASUREMENT TECHNIQUE** In this section, we evaluate the stability of the bias measure developed in <cite>(Bolukbasi et al. 2016)</cite> which is claimed to measure societal biases in word embeddings.",
  "y": "background motivation"
 },
 {
  "id": "95883b369c4b019fa98493a728c1a0_4",
  "x": "For the sake of com-pleteness, we repeat the definition of <cite>(Bolukbasi et al. 2016 )</cite>'s bias metric: Given two groups of words (e.g., gender words) first groups' subspace direction g is identified as the first principal component of the vectors",
  "y": "background"
 },
 {
  "id": "95883b369c4b019fa98493a728c1a0_5",
  "x": "Figure 1 illustrates the results of <cite>(Bolukbasi et al. 2016 )</cite> bias detection algorithm for a list of profession names with word embedding vectors of dimension 256 trained using the Skipgram algorithm (Mikolov et al. 2013 ) on a sample of 23k Wikipedia articles with 50k term vocabulary. As one can observe, this bias measure identifies some profession names such as commander and nurse, which historically were predominantly male and female jobs, respectively. Moreover, we trained word embeddings with differing dimension on the same sampled Wikipedia corpus using Skip-gram algorithm, and calculated Kendall tau rank correlation coefficients for the bias metrics corresponding to terms in the above list of professions. As illustrated in Figure 2 , we found that though rankings for low dimensional embeddings were unstable, for larger dimensions (\u2265 128) rankings of the biases of the profession terms achieve superior Kendall tau scores.",
  "y": "differences"
 },
 {
  "id": "95883b369c4b019fa98493a728c1a0_6",
  "x": "Next, we evaluated the stability of the direct bias measure proposed in <cite>(Bolukbasi et al. 2016)</cite> . <cite>The authors</cite> assert that the direct bias measure can be used as a metric to conclude how much an embedding is biased. For instance, for word embeddings trained on Google News articles, <cite>they</cite> reported that direct gender bias on 327 profession names is 0.08 and thus <cite>they</cite> concluded that this embedding is biased. However, as we have illustrated in Figure 3a , this bias score is not stable, i.e., direct bias measure decays exponentially with increasing word embedding dimension.",
  "y": "differences"
 },
 {
  "id": "95883b369c4b019fa98493a728c1a0_8",
  "x": "---------------------------------- **CONCLUDING REMARKS** We conclude that while meta analyses of the bias metrics proposed by <cite>(Bolukbasi et al. 2016)</cite> indicate that the metrics capture and somewhat quantify sociologically meaningful biases present in learned embedding spaces, the metrics are highly sensitive to the hyperparameter configurations of the algorithms used to learn them.",
  "y": "differences"
 },
 {
  "id": "95883b369c4b019fa98493a728c1a0_9",
  "x": "We conclude that while meta analyses of the bias metrics proposed by <cite>(Bolukbasi et al. 2016)</cite> indicate that the metrics capture and somewhat quantify sociologically meaningful biases present in learned embedding spaces, the metrics are highly sensitive to the hyperparameter configurations of the algorithms used to learn them. We specifically find that the average magnitude of the quantified bias is particularly sensitive to the embedding dimension hyper-parameter selected, as well as the sample of bias-axis-inducing terms used to construct the various projections employed by the metric. While it is the case that the bias metrics in <cite>(Bolukbasi et al. 2016 )</cite> may provide meaningful rankings of corpora when controlling for model hyper-parameter configuration, publishing the average absolute value of the metric without a complete account for model configuration is suspect.",
  "y": "differences"
 },
 {
  "id": "9655fb9abfb1c30b39f3261680fafc_0",
  "x": "Data-to-text generation, a classic task of natural language generation is to convert the structured input data (i.e., a table) into descriptions that adequately and fluently describes the data (Kukich, 1983; Reiter and Dale, 1997; Barzilay and Lapata, 2005; Angeli et al., 2010; Kim and Mooney, 2010; Perez-Beltrachini and Gardent, 2017) . datato-document generation is a slightly more challenging setting in which a system generates multisentence summaries based on input data<cite> (Wiseman et al., 2017)</cite> . It is traditionally divided into two subtasks: content selection and the surface realization.",
  "y": "background"
 },
 {
  "id": "9655fb9abfb1c30b39f3261680fafc_1",
  "x": "Although neural network models are capable of generating fluent texts, they tend to make mistakes in the generated sentences, containing sentences that do not conform to the input structured data. As shown in Figure 1 , the neural model produces the wrong rebound number. Previous work<cite> (Wiseman et al., 2017)</cite> notices the problem and proposes an extractive metric to evaluate the consistency between the generation and its input structured data.",
  "y": "background"
 },
 {
  "id": "9655fb9abfb1c30b39f3261680fafc_2",
  "x": "Previous work<cite> (Wiseman et al., 2017)</cite> notices the problem and proposes an extractive metric to evaluate the consistency between the generation and its input structured data. However, such consistency is not involved in the training process to guide model learning. In this paper, we propose to use a new training framework that directly incorporates consistency verification to guide the generation during the training process.",
  "y": "differences"
 },
 {
  "id": "9655fb9abfb1c30b39f3261680fafc_3",
  "x": "Since only a subset of generated text are related to the reference and the input data, we design two novel word-level reward signals based on the consistency with the reference text and with the input data respectively. The non-differentiable consistency reward signals are incorporated into training procedure via a reinforcement learning approach. We evaluate our proposed method on the RO-TOWIRE dataset<cite> (Wiseman et al., 2017)</cite> , which targets at generating multi-sentence game summaries.",
  "y": "uses"
 },
 {
  "id": "9655fb9abfb1c30b39f3261680fafc_4",
  "x": "---------------------------------- **INFORMATION EXTRACTION** To extract information describing the input data from the generated texts, we apply a simple information extraction system similar to<cite> (Wiseman et al., 2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "9655fb9abfb1c30b39f3261680fafc_5",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** Data: We use ROTOWIRE dataset<cite> (Wiseman et al., 2017)</cite> , which is a collection of articles summarizing NBA basketball games, paired with their corresponding box-and line-score tables.",
  "y": "uses"
 },
 {
  "id": "9655fb9abfb1c30b39f3261680fafc_6",
  "x": "We subtract mean reward as baseline for content reward and bound both type of rewards to [-2, 1] . For relation extractor model, we use an ensemble of CNNs and LSTMs relation classification models<cite> (Wiseman et al., 2017)</cite> , which achieves the precision of 94.7% and recall of 75.3% given the reference. Evaluation: We use automatic evaluation metric BLEU-4 (Papineni et al., 2002) and the extractive evaluation metrics proposed by<cite> (Wiseman et al., 2017)</cite> , which contains three criteria: content selection (CS), relation generation (RG), content ordering (CO).",
  "y": "uses"
 },
 {
  "id": "9655fb9abfb1c30b39f3261680fafc_7",
  "x": "We subtract mean reward as baseline for content reward and bound both type of rewards to [-2, 1] . For relation extractor model, we use an ensemble of CNNs and LSTMs relation classification models<cite> (Wiseman et al., 2017)</cite> , which achieves the precision of 94.7% and recall of 75.3% given the reference. Evaluation: We use automatic evaluation metric BLEU-4 (Papineni et al., 2002) and the extractive evaluation metrics proposed by<cite> (Wiseman et al., 2017)</cite> , which contains three criteria: content selection (CS), relation generation (RG), content ordering (CO).",
  "y": "uses"
 },
 {
  "id": "9655fb9abfb1c30b39f3261680fafc_9",
  "x": "Below: First three sentences produced by baseline and our proposed model. blue words denotes facts consistency with input, red words denotes facts contradicting with input and italic words denote facts not mentioned in input MLE training on our baseline model and achieve comparable results on ROTOWIRE dataset w.r.t. the previous work<cite> (Wiseman et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "9655fb9abfb1c30b39f3261680fafc_10",
  "x": "blue words denotes facts consistency with input, red words denotes facts contradicting with input and italic words denote facts not mentioned in input MLE training on our baseline model and achieve comparable results on ROTOWIRE dataset w.r.t. the previous work<cite> (Wiseman et al., 2017)</cite> . The differences between our method and<cite> (Wiseman et al., 2017</cite> ) is that we adopt a LSTM for the encoder, while<cite> (Wiseman et al., 2017</cite> ) uses a table encoder similar to (Yang et al., 2017) .",
  "y": "differences"
 },
 {
  "id": "9655fb9abfb1c30b39f3261680fafc_11",
  "x": "Qualitative analysis: Table 3 shows a validation set game statistics with generation results by original Seq2Seq model and our proposed method 4 . The original Seq2Seq model is more likely to produce incorrect facts (e.g., wrong score points and in their paper and rerun this baseline with the author's code. 3 Hand crafted templates from<cite> (Wiseman et al., 2017</cite> ), e.g. <player> scored <pts> points (<fgm>-<fga> FG, <tpm>-<tpa> 3PT, <ftm>-<fta> FT) 4 The whole game input data and the summaries are relatively lengthy, we presents the first three sentences in the summary and its corresponding game statistics for brevity.",
  "y": "uses"
 },
 {
  "id": "9731b4cea1405b7cbf3792aed5b1e4_0",
  "x": "Therefore, our setting resembles the established task of entity recognition (Finkel et al., 2005; <cite>Ratinov and Roth, 2009</cite>) , with the difference being that we focus on un-named entities. Contribution. One of the factors impeding progress in common sense information extraction is the lack of training data.",
  "y": "differences background"
 },
 {
  "id": "9731b4cea1405b7cbf3792aed5b1e4_1",
  "x": "In this paper, we focus on mentions of audible (sound) and olfactible (smell) concepts. We treat sense recognition in text as a sequence labeling task where each sentence is a sequence of tokens labeled using the BIO tagging scheme <cite>(Ratinov and Roth, 2009 )</cite>. The BIO labels denote tokens at the beginning, inside, and outside of a relevant mention, respectively.",
  "y": "uses"
 },
 {
  "id": "9731b4cea1405b7cbf3792aed5b1e4_2",
  "x": "to words, and y refers to BIO labels. Conditional Random Fields (CRFs) (Lafferty et al., 2001 ) have been widely used named entity recognition <cite>(Ratinov and Roth, 2009</cite>; Finkel et al., 2005) , a task similar to our own. While the CRF models performed reasonably well on our task, we sought to obtain improvements by proposing and training variations of Long Short Memory (LSTM) recurrent neural networks (Hochreiter and Schmidhuber, 1997).",
  "y": "background"
 },
 {
  "id": "9731b4cea1405b7cbf3792aed5b1e4_3",
  "x": "The abbreviations denote the following: LSTM refers to a vanilla LSTM model, using only word embeddings as features, + OR refers to the LSTM plus the output recurrence, + CHAR refers to the LSTM plus the character embeddings as features. + OR + CHAR refers to the LSTM plus the output recurrence and character embeddings as features. For the CRF, we use the commonly used features for named entity recognition: words, prefix/suffices, and part-of-speech tag <cite>(Ratinov and Roth, 2009 )</cite>.",
  "y": "uses"
 },
 {
  "id": "9731b4cea1405b7cbf3792aed5b1e4_4",
  "x": "Entity recognition systems are traditionally based on a sequential model, for example a CRF, and involve feature engineering (Lafferty et al., 2001; <cite>Ratinov and Roth, 2009 )</cite>. More recently, neural approaches have been used for named entity recognition (Hammerton, 2003; Collobert et al., 2011; dos Santos and Guimar\u00e3es, 2015; Chiu and Nichols, 2016; Shimaoka et al., 2016) . Like other neural approaches, our approach does not require feature engineering, the only features we use are word and character embeddings.",
  "y": "background"
 },
 {
  "id": "975413dd6b3d3df9c5d111d94e8eb7_0",
  "x": "Textual relation (Bunescu and Mooney, 2005) , defined as the shortest path between two entities in the dependency parse tree of a sentence, has been widely shown to be the main bearer of relational information in text and proved effective in relation extraction tasks (Xu et al., 2015;<cite> Su et al., 2018)</cite> . If we can learn a general-purpose embedding for textual relations, it may facilitate many downstream relational understanding tasks by providing general relational knowledge. Similar to language modeling for learning general-purpose word embeddings, distant supervision (Mintz et al., 2009 ) is a promising way to acquire supervision, at no cost, for training general-purpose embedding of textual relations.",
  "y": "background"
 },
 {
  "id": "975413dd6b3d3df9c5d111d94e8eb7_1",
  "x": "If we can learn a general-purpose embedding for textual relations, it may facilitate many downstream relational understanding tasks by providing general relational knowledge. Similar to language modeling for learning general-purpose word embeddings, distant supervision (Mintz et al., 2009 ) is a promising way to acquire supervision, at no cost, for training general-purpose embedding of textual relations. Recently<cite> Su et al. (2018)</cite> propose to leverage global co-occurrence statistics of textual and KB relations to learn embeddings of textual relations, and show that it can effectively combat the wrong labeling problem of distant supervision (see Figure 1 for example).",
  "y": "background"
 },
 {
  "id": "975413dd6b3d3df9c5d111d94e8eb7_3",
  "x": "Distant supervision methods (Mintz et al., 2009) for relation extraction have been studied by a number of works (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Ji et al., 2017; Wu et al., 2017) . <cite>(Su et al., 2018)</cite> use global co-occurrence statistics of 1 https://github.com/czyssrs/GloREPlus textual and KB relations to effectively combat the wrong labeling problem. But the global statistics in their work is limited to NYT dataset, capturing domain-specific distributions. Another line of research that relates to ours is the universal schema (Riedel et al., 2013) for relation extraction, KB completion, as well as its extensions (Toutanova et al., 2015; Verga et al., 2016) .",
  "y": "background"
 },
 {
  "id": "975413dd6b3d3df9c5d111d94e8eb7_4",
  "x": "Following <cite>(Su et al., 2018)</cite> , we then collect the global co-occurrence statistics of textual and KB relations. More specifically, for a relational triple (e 1 , t, e 2 ) with textual relation t, if (e 1 , r, e 2 ) with KB relation r exists in the KB, then we count it as a co-occurrence of t and r. We count the total number of co-occurrences of each pair of textual and KB relation across the entire corpus. We then normalize the global co-occurrence statistics such that each textual relation has a valid probability distribution over all the KB relations, which presumably captures the semantics of the textual relation.",
  "y": "uses"
 },
 {
  "id": "975413dd6b3d3df9c5d111d94e8eb7_5",
  "x": "We train a maximum number of 200 epochs and take the checkpoint with minimum validation loss for the result. We also compare with using vanilla RNN in GloRE <cite>(Su et al., 2018)</cite> . Denote the embedding trained with Tranformer as GloRE++, standing for both new data and different model, and with RNN as GloRE+, standing for new data.",
  "y": "uses"
 },
 {
  "id": "975413dd6b3d3df9c5d111d94e8eb7_6",
  "x": "**RELATION EXTRACTION** We experiment on the popular New York Times (NYT) relation extraction dataset (Riedel et al., 2010) . Following GloRE <cite>(Su et al., 2018)</cite> , we aim at augmenting existing relation extractors with the textual relation embeddings.",
  "y": "uses"
 },
 {
  "id": "975413dd6b3d3df9c5d111d94e8eb7_7",
  "x": "We then construct an ensemble model by a weighted combination of predictions from the base model and the textual relation embedding. Same as <cite>(Su et al., 2018)</cite> , we use PCNN+ATT (Lin et al., 2016 ) as our base model. GloRE++ improves its best F 1 -score from 42.7% to 45.2%, slightly outperforming the previous state-of-theart (GloRE, 44.7%).",
  "y": "similarities uses"
 },
 {
  "id": "975413dd6b3d3df9c5d111d94e8eb7_8",
  "x": "GloRE++ improves its best F 1 -score from 42.7% to 45.2%, slightly outperforming the previous state-of-theart (GloRE, 44.7%). As shown in previous work <cite>(Su et al., 2018)</cite> , on NYT dataset, due to a significant amount of false negatives, the PR curve on the held-out set may not be an accurate measure of performance. Therefore, we mainly employ manual evaluation.",
  "y": "motivation"
 },
 {
  "id": "976e5cf02dc5430c0b1393d3cb38e5_0",
  "x": "Although the syntax-based translation model in<cite> Galley et al. (2006)</cite> falls under the string-to-tree category, I wonder why hierarchical phrasebased SMT, or Hiero (Chiang 2007) , is not explicitly put under the string-to-string category, since Hiero also uses \"unlabeled hierarchical phrases where there is no representation of linguistic categories.\" Chapter 2 focuses on how the statistical framework of a syntax-based SMT approach learns its model from a word-aligned and parsed parallel text. The first section explains how phrase pairs are extracted as translation rules from a word-aligned sentence pair in phrase-based SMT (Koehn, Och, and Marcu 2003) , highlighting the definition of a phrase as a sequence of words and the alignment-consistency property of a phrase pair as defined in Och and Ney (2004) .",
  "y": "background"
 },
 {
  "id": "976e5cf02dc5430c0b1393d3cb38e5_1",
  "x": "The remainder of the chapter introduces three predominant instantiations of syntax-based models: hierarchical phrase-based SMT (Hiero) (Chiang 2007) , which is a non-labeled syntax-based SMT approach arising from the phrase-based approach; syntax-augmented machine translation (SAMT), which introduces the notion of soft labels while keeping the nonlinguistic phrase notion; and GHKM <cite>(Galley et al. 2004</cite> ), which only extracts translation rules consistent with constituency parse subtrees. This chapter is nicely organized and it is easy to follow the gradual evolution from phrase-based SMT to GHKM. Chapter 3 introduces the decoding formalism in the form of a directed hypergraph, defined as a set of vertices and a set of directed hyperedges.",
  "y": "background"
 },
 {
  "id": "97852048a123350455f398728d6d34_0",
  "x": "Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a domain mismatch case<cite> (Plank and Moschitti, 2013)</cite> . To alleviate this performance loss, we need to resort to domain adaptation (DA) techniques to adapt a system trained on some source domain to perform well on new target domains.",
  "y": "background"
 },
 {
  "id": "97852048a123350455f398728d6d34_1",
  "x": "We here focus on the unsupervised domain adaptation (i.e., no labeled target data) and singlesystem DA (Petrov and McDonald, 2012;<cite> Plank and Moschitti, 2013)</cite> , i.e., building a single system that is able to cope with different, yet related target domains. While DA has been investigated extensively in the last decade for various natural language processing (NLP) tasks, the examination of DA for RE is only very recent. To the best of our knowledge, there have been only three studies on DA for RE <cite>(Plank and Moschitti, 2013</cite>; Nguyen and Grishman, 2014; .",
  "y": "uses"
 },
 {
  "id": "97852048a123350455f398728d6d34_2",
  "x": "While DA has been investigated extensively in the last decade for various natural language processing (NLP) tasks, the examination of DA for RE is only very recent. To the best of our knowledge, there have been only three studies on DA for RE <cite>(Plank and Moschitti, 2013</cite>; Nguyen and Grishman, 2014; . Of these, follow the supervised DA paradigm and assume some labeled data in the target domains.",
  "y": "background"
 },
 {
  "id": "97852048a123350455f398728d6d34_3",
  "x": "In contrast,<cite> Plank and Moschitti (2013)</cite> and Nguyen and Grishman (2014) work on the unsupervised DA. In our view, unsupervised DA is more challenging, but more realistic and practical for RE as we usually do not know which target domains we need to work on in advance, thus cannot expect to possess labeled data of the target domains. Our current work therefore focuses on the single-system unsupervised DA.",
  "y": "background"
 },
 {
  "id": "97852048a123350455f398728d6d34_4",
  "x": "It is unclear at the first glance how to encode word embeddings into the tree kernels effectively so that word embeddings could help to improve the generalization performance of RE. One way is to use word embeddings to compute similarities between words and embed these similarity scores into the kernel functions, e.g., by resembling the method of<cite> Plank and Moschitti (2013)</cite> that exploited LSA (in the semantic syntactic tree kernel (SSTK), cf. \u00a72.1).",
  "y": "uses background"
 },
 {
  "id": "97852048a123350455f398728d6d34_5",
  "x": "\u00a72.1). We explore various methods to apply word embeddings to generate the semantic representations for DA of RE and demonstrate that semantic representations are very effective to significantly improve the portability of the relation extractors based on the tree kernels, bringing us to the second question: (ii) Between the feature-based method in Nguyen and Grishman (2014) and the SSTK method in<cite> Plank and Moschitti (2013)</cite> , which method is better for DA of RE, given the recent discovery of word embeddings for both methods?",
  "y": "motivation background"
 },
 {
  "id": "97852048a123350455f398728d6d34_6",
  "x": "In particular, while<cite> Plank and Moschitti (2013)</cite> only use the path-enclosed trees induced from the constituent parse trees as the representation for relation mentions, Nguyen and Grishman (2014) include a rich set of features extracted from multiple resources such as constituent trees, dependency trees, gazetteers, semantic resources in the representation. Besides,<cite> Plank and Moschitti (2013)</cite> consider the direction of relations in their evaluation (i.e, distinguishing between relation classes and their inverses) but Nguyen and Grishman (2014) disregard this relation direction. Finally, we note that although both studies evaluate their systems on the ACE 2005 dataset, they actually have different dataset partitions.",
  "y": "background"
 },
 {
  "id": "97852048a123350455f398728d6d34_7",
  "x": "It is worth noting that besides the approach difference, these two works employ rather different resources and settings in their evaluation, making it impossible to directly compare their performance. In particular, while<cite> Plank and Moschitti (2013)</cite> only use the path-enclosed trees induced from the constituent parse trees as the representation for relation mentions, Nguyen and Grishman (2014) include a rich set of features extracted from multiple resources such as constituent trees, dependency trees, gazetteers, semantic resources in the representation. Besides,<cite> Plank and Moschitti (2013)</cite> consider the direction of relations in their evaluation (i.e, distinguishing between relation classes and their inverses) but Nguyen and Grishman (2014) disregard this relation direction.",
  "y": "background"
 },
 {
  "id": "97852048a123350455f398728d6d34_8",
  "x": "Finally, we note that although both studies evaluate their systems on the ACE 2005 dataset, they actually have different dataset partitions. In order to overcome this limitation, we conduct an evaluation in which the two methods are directed to use the same resources and settings, and are thus compared in a compatible manner to achieve an insight on their effectiveness for DA of RE. In fact, the problem of incompatible comparison is unfortunately very common in the RE literature (Wang, 2008;<cite> Plank and Moschitti, 2013)</cite> and we believe there is a need to tackle this increasing confusion in this line of research.",
  "y": "motivation"
 },
 {
  "id": "97852048a123350455f398728d6d34_9",
  "x": "---------------------------------- **TREE KERNEL-BASED METHOD** In the tree kernel-based method (Moschitti, 2006; Moschitti, 2008;<cite> Plank and Moschitti, 2013)</cite> , a relation mention (the two entity mentions and the sentence containing them) is represented by the path-enclosed tree (PET), the smallest constituency-based subtree including the two target entity mentions (Zhang et al., 2006) .",
  "y": "background"
 },
 {
  "id": "97852048a123350455f398728d6d34_10",
  "x": "The major limitation of STK is its inability to match two trees that share the same substructure, but involve different though semantically related terminal nodes (words). This is caused by the hard matches between words, and consequently between sequences containing them. For instance, in the following example taken from<cite> Plank and Moschitti (2013)</cite> , the two fragments \"governor from Texas\" and \"head of Mary-land\" would not match in STK although they have very similar syntactic structures and basically convey the same relationship.",
  "y": "background"
 },
 {
  "id": "97852048a123350455f398728d6d34_11",
  "x": "Which method is better for DA of RE? In order to ensure the two methods <cite>(Plank and Moschitti, 2013</cite>; Nguyen and Grishman, 2014 ) are compared compatibly on the same resources, we make sure the two systems have access to the same amount of information. Thus, we follow<cite> Plank and Moschitti (2013)</cite> and use the PET trees (beside word clusters and word embeddings) as the only resource the two methods can exploit.",
  "y": "uses"
 },
 {
  "id": "97852048a123350455f398728d6d34_12",
  "x": "In order to ensure the two methods <cite>(Plank and Moschitti, 2013</cite>; Nguyen and Grishman, 2014 ) are compared compatibly on the same resources, we make sure the two systems have access to the same amount of information. Thus, we follow<cite> Plank and Moschitti (2013)</cite> and use the PET trees (beside word clusters and word embeddings) as the only resource the two methods can exploit. For the feature-based method, we utilize all the features extractable from the PET trees that are standardly used in the state-of-the-art featurebased systems for DA of RE (Nguyen and Grishman, 2014) .",
  "y": "uses"
 },
 {
  "id": "97852048a123350455f398728d6d34_13",
  "x": "**WORD EMBEDDINGS & TREE KERNELS** In this section, we first give the intuition that guides us in designing the proposed methods. In particular, one limitation of the syntactic semantic tree kernel presented in<cite> Plank and Moschitti (2013)</cite> ( \u00a72.1) is that semantics is highly tied to syntax (the PET trees) in the kernel computation, limiting the generalization capacity of semantics to the extent of syntactic matches.",
  "y": "motivation background"
 },
 {
  "id": "97852048a123350455f398728d6d34_14",
  "x": "Both PHRASE and TREE have d dimensions. It is also interesting to examine combinations of these three representations (cf., Table 1 ). SIM: Finally, for completeness, we experiment with a more obvious way to introduce word embeddings into tree kernels, resembling more closely the approach of<cite> Plank and Moschitti (2013)</cite> .",
  "y": "similarities"
 },
 {
  "id": "97852048a123350455f398728d6d34_15",
  "x": "---------------------------------- **DATASET, RESOURCES AND PARAMETERS** We use the word clusters trained by<cite> Plank and Moschitti (2013)</cite> on the ukWaC corpus (Baroni et al., 2009 ) with 2 billion words, and the C&W word embeddings from Turian el al. (2010) 2 with 50 dimensions following Nguyen and Grishman (2014) .",
  "y": "uses"
 },
 {
  "id": "97852048a123350455f398728d6d34_16",
  "x": "We use the word clusters trained by<cite> Plank and Moschitti (2013)</cite> on the ukWaC corpus (Baroni et al., 2009 ) with 2 billion words, and the C&W word embeddings from Turian el al. (2010) 2 with 50 dimensions following Nguyen and Grishman (2014) . In order to make the comparisons compatible, we introduce word embeddings into the tree kernel by extending the package provided by<cite> Plank and Moschitti (2013)</cite> , which uses the Charniak parser to obtain the constituent trees, the SVM-light-TK for the SSTK kernel in SVM, the directional relation classes, etc. We utilize the default vector kernel in the SVM-light-TK package (d=3).",
  "y": "extends"
 },
 {
  "id": "97852048a123350455f398728d6d34_17",
  "x": "We take half of bc as the only target development set, and use the remaining data and domains for testing. The dataset partition is exactly the same as in<cite> Plank and Moschitti (2013)</cite> . As described in their paper, the target domains quite differ from the source domain in the relation distributions and vocabulary.",
  "y": "uses"
 },
 {
  "id": "97852048a123350455f398728d6d34_18",
  "x": "In particular, we take the systems using the PET trees, word clusters and LSA in<cite> Plank and Moschitti (2013)</cite> as the baselines and augment them with the embeddings WED = HEAD+PHRASE. We report the performance of these augmented systems in Table 2 for the two scenarios: (i) in-domain: both training and testing are performed on the source domain via 5-fold cross validation and (ii) out-of-domain: models are trained on the source domain but evaluated on the three target domains. To summarize, we find:",
  "y": "extends"
 },
 {
  "id": "97852048a123350455f398728d6d34_19",
  "x": "Third and most importantly, for all the systems in<cite> Plank and Moschitti (2013)</cite> (the baselines) and for all the target domains, whether word clusters and LSA are utilized or not, we consistently witness the performance improvement of the baselines when combined with word embedding (comparing systems X and X+WED where X is some baseline system). The best out-of-domain performance is achieved when word embeddings are employed in conjunction with the composite kernels (PET+PET WC+PET LSA for the target domains bc and wl, and PET+PET WC for the target domain cts). To be more concrete, the best system with word embeddings (row 12 in Table 2 ) significantly outperforms the best system in<cite> Plank and Moschitti (2013)</cite> with p < 0.05, an improvement of 3.7%, 1.1% and 2.7% on the target domains bc, cts and wl respectively, demonstrating the benefit of word embeddings for DA of RE in the tree kernel-based method.",
  "y": "differences"
 },
 {
  "id": "97852048a123350455f398728d6d34_20",
  "x": "To be more concrete, the best system with word embeddings (row 12 in Table 2 ) significantly outperforms the best system in<cite> Plank and Moschitti (2013)</cite> with p < 0.05, an improvement of 3.7%, 1.1% and 2.7% on the target domains bc, cts and wl respectively, demonstrating the benefit of word embeddings for DA of RE in the tree kernel-based method. ---------------------------------- **TREE KERNEL-BASED VS FEATURE-BASED DA OF RE**",
  "y": "differences"
 },
 {
  "id": "97852048a123350455f398728d6d34_21",
  "x": "This section aims to compare the tree kernel-based method in<cite> Plank and Moschitti (2013)</cite> and the feature-based method in Nguyen and Grishman (2014) for DA of RE on the same settings (i.e, same dataset partition, the same pre-processing Table 3 : Tree kernel-based in<cite> Plank and Moschitti (2013)</cite> vs feature-based in Nguyen and Grishman (2014) . All the comparisons between the tree kernel-based method and the feature-based method in this table are significant with p < 0.05. procedure, the same model of directional relation classes, the same PET trees for tree kernels and feature extraction, the same word clusters and the same word embeddings).",
  "y": "uses"
 },
 {
  "id": "97852048a123350455f398728d6d34_22",
  "x": "This section analyzes the output of the systems to gain more insights into their operation. Word Embeddings for the Tree-kernel based Method We focus on the comparison of the best model in<cite> Plank and Moschitti (2013)</cite> (row 11 in Table 2 ) (called P) with the same model but augmented with the embedding WED (row 12 in Tabel 2) (called P+WED). One of the most interesting insights is that the embedding WED helps to semantically generalize the phrases connecting the two target entity mentions beyond the syntactic constraints.",
  "y": "extends"
 },
 {
  "id": "97852048a123350455f398728d6d34_23",
  "x": "However, as shown by<cite> Plank and Moschitti (2013)</cite> , instance weighting is not very useful for DA of RE. ---------------------------------- **CONCLUSION**",
  "y": "background"
 },
 {
  "id": "97852048a123350455f398728d6d34_24",
  "x": "**CONCLUSION** In order to improve the generalization of relation extractors, we propose to augment the semantic syntactic tree kernels with the semantic representation of relation mentions, generated from the word embeddings of the context words. The method demonstrates strong promise for the DA of RE, i.e, it significantly improves the best system of<cite> Plank and Moschitti (2013)</cite> (up to 7% relative improvement).",
  "y": "differences"
 },
 {
  "id": "97852048a123350455f398728d6d34_25",
  "x": "The method demonstrates strong promise for the DA of RE, i.e, it significantly improves the best system of<cite> Plank and Moschitti (2013)</cite> (up to 7% relative improvement). Moreover, we perform a compatible comparison between the tree kernel-based method and the feature-based method on the same settings and resources, which suggests that the tree kernel-based method<cite> (Plank and Moschitti, 2013)</cite> is better than the feature-based method (Nguyen and Grishman, 2014) for DA of RE. An error analysis is conducted to get a deeper comprehension of the systems.",
  "y": "uses"
 },
 {
  "id": "983ef31a44646d8e6276ee1933e41d_0",
  "x": "Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017) . And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015) . introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. <cite>Aharoni and Goldberg (2018)</cite> improve WebSplit by reducing overlap in the data splits, and * Both authors contributed equally.",
  "y": "background"
 },
 {
  "id": "983ef31a44646d8e6276ee1933e41d_1",
  "x": "\u2022 By incorporating WikiSplit into training, we more than double (30.5 to 62.4) the BLEU score obtained on WebSplit by <cite>Aharoni and Goldberg (2018)</cite> . Figure 1 . ----------------------------------",
  "y": "similarities uses"
 },
 {
  "id": "983ef31a44646d8e6276ee1933e41d_2",
  "x": "**CORPUS STATISTICS AND QUALITY** Our extraction heuristic is imperfect, so we manually assess corpus quality using the same categorization schema proposed by <cite>Aharoni and Goldberg (2018)</cite> ; see Table 1 for examples of correct, unsupported and missing sentences in splits extracted from Wikipedia. We do this for 100 randomly selected examples using three different thresholds of \u03b4.",
  "y": "similarities uses"
 },
 {
  "id": "983ef31a44646d8e6276ee1933e41d_3",
  "x": "We compare three training configurations: WEBSPLIT only, WIKISPLIT only, and BOTH, which is simply their concatenation. Text-to-text training instances are defined as all the unique pairs of (C, S), where C is a complex sentence and S is its simplification into multiple simple sentences<cite> Aharoni and Goldberg, 2018)</cite> . For training, we delimit the simple sentences with a special symbol.",
  "y": "similarities"
 },
 {
  "id": "983ef31a44646d8e6276ee1933e41d_4",
  "x": "We use the same sequence-to-sequence architecture that produced the top result for <cite>Aharoni and Goldberg (2018)</cite> , \"Copy512\", which is a one-layer, bi-directional LSTM (cell size 512) with attention (Bahdanau et al., 2014 ) and a copying mechanism (See et al., 2017 ) that dynamically interpolates the standard word distribution with a distribution over the words in the input sentence. Training details are as described in the Appendix of <cite>Aharoni and Goldberg (2018)</cite> using the OpenNMT-py framework (Klein et al., 2017) . 6",
  "y": "similarities uses"
 },
 {
  "id": "983ef31a44646d8e6276ee1933e41d_5",
  "x": "5 We also report lengthbased statistics to quantify splitting. We use the same sequence-to-sequence architecture that produced the top result for <cite>Aharoni and Goldberg (2018)</cite> , \"Copy512\", which is a one-layer, bi-directional LSTM (cell size 512) with attention (Bahdanau et al., 2014 ) and a copying mechanism (See et al., 2017 ) that dynamically interpolates the standard word distribution with a distribution over the words in the input sentence. Training details are as described in the Appendix of <cite>Aharoni and Goldberg (2018)</cite> using the OpenNMT-py framework (Klein et al., 2017) .",
  "y": "similarities uses"
 },
 {
  "id": "983ef31a44646d8e6276ee1933e41d_6",
  "x": "5 Past work on WebSplit<cite> Aharoni and Goldberg, 2018)</cite> reported macro-averaged sentence-level BLEU, calculated without smoothing precision values of zero. We found this ill-defined case occurred often for low-quality output. 6 github.com/OpenNMT/OpenNMT-py, 0ecec8b Table 5 : Results on the WebSplit v1.0 test set when varying the training data while holding model architecture fixed: corpus-level BLEU, sentence-level BLEU (to match past work), simple sentences per complex sentence, and tokens per simple sentence (micro-average).",
  "y": "differences"
 },
 {
  "id": "983ef31a44646d8e6276ee1933e41d_7",
  "x": "AG18 is the previous best model by <cite>Aharoni and Goldberg (2018)</cite> , which used the full WebSplit training set, whereas we downsampled it. dient\", \"publisher\") and generally fails to produce coherent sentences. In contrast, the WIKISPLIT model achieves 59.4 BLEU on the WebSplit validation set, without observing any in-domain data.",
  "y": "extends differences"
 },
 {
  "id": "983ef31a44646d8e6276ee1933e41d_8",
  "x": "As shown in Table 6, the BOTH model produced the most accurate output (95% correct simple sentences), with the lowest incidence of missed or unsupported statements. Our manual evaluation includes the corresponding outputs from <cite>Aharoni and Goldberg (2018)</cite> (AG18), which were 22% accurate. The examples in Table 7 demonstrate that the WIKISPLIT and BOTH models produce much more coherent output which faithfully rephrases the input.",
  "y": "uses"
 },
 {
  "id": "983ef31a44646d8e6276ee1933e41d_9",
  "x": "In Example 1, the combined model (BOTH) produces three fluent sentences, overcoming the strong bias toward two-sentence output inherent in the majority of its training examples. We relate our approach to prior work on WebSplit v1.0 by reporting scores on its test set in Table 5. Our best performance in BLEU is again obtained by combining the proposed WikiSplit dataset with the downsampled WebSplit, yielding <cite>Aharoni and Goldberg (2018)</cite> , while the other outputs are from our models trained on the corresponding data.",
  "y": "extends differences"
 },
 {
  "id": "9aa9fa6b94aa24939b50effa0e575b_0",
  "x": "We introduce a very simple change to the loss function used in the original formulation by<cite> Kiros et al. (2014)</cite> , which leads to drastic improvements in the retrieval performance. In particular, the original paper uses the rank loss which computes the sum of violations across the negative training examples. Instead, we penalize the model according to the hardest negative examples.",
  "y": "extends"
 },
 {
  "id": "9aa9fa6b94aa24939b50effa0e575b_1",
  "x": "Our results outperform the state-of-the-art results by 8.8% in caption retrieval and 11.3% in image retrieval at R@1. On Flickr30K, we more than double R@1 as reported by<cite> Kiros et al. (2014)</cite> in both image and caption retrieval, and achieve near state-of-the-art performance. We further show that similar improvements also apply to the Order-embeddings by Vendrov et al. (2015) which builds on a similar loss function.",
  "y": "differences"
 },
 {
  "id": "9aa9fa6b94aa24939b50effa0e575b_2",
  "x": "Several different approaches have been proposed, most sharing the core idea of embedding images and language in a common space. This allows us to easily search for semantically meaningful neighbors in either modality. Learning such embeddings using powerful neural networks has led to significant advancements in image-caption retrieval and generation<cite> Kiros et al. (2014)</cite> ; Karpathy & Fei-Fei (2015) , video-to-text alignment Zhu et al. (2015) , and question-answering Malinowski et al. (2015) .",
  "y": "background"
 },
 {
  "id": "9aa9fa6b94aa24939b50effa0e575b_3",
  "x": "This paper investigates the visual-semantic embeddings (VSE) of<cite> Kiros et al. (2014)</cite> for imagecaption retrieval. We propose a set of simple modifications to the original formulation that prove to be extremely effective. In particular, we change the rank loss used in the original formulation to penalize the model according to the hardest negative training exemplars instead of averaging the individual violations across the negatives.",
  "y": "uses"
 },
 {
  "id": "9aa9fa6b94aa24939b50effa0e575b_4",
  "x": "In such an approach, we learn two mappings, one for images and another for captions, that embed the two modalities in a joint space. Given a similarity measure in this space, the task of image retrieval can be formulated as a nearest neighbor search problem. Works such as<cite> Kiros et al. (2014)</cite> , Karpathy & Fei-Fei (2015) , Zhu et al. (2015) , Socher et al. (2014) use a rank loss to learn the joint visual-semantic embedding.",
  "y": "background"
 },
 {
  "id": "9aa9fa6b94aa24939b50effa0e575b_5",
  "x": "An attention mechanism on both image and caption is used by Nam et al. (2016) , where the authors sequentially and selectively focus on a subset of words and image regions to compute the similarity. In Huang et al. (2016) , the authors use a multi-modal context-modulated attention mechanism to compute the matching score between an image and a caption. Our work builds upon the work by<cite> Kiros et al. (2014)</cite> , in which the authors use a rank loss to optimize the embedding.",
  "y": "extends"
 },
 {
  "id": "9aa9fa6b94aa24939b50effa0e575b_6",
  "x": "Our work builds on Visual-Semantic Embeddings<cite> Kiros et al. (2014)</cite> . In what follows, we first define the task of image-caption retrieval and summarize the original model and its loss function. Then we introduce our new loss.",
  "y": "extends"
 },
 {
  "id": "9aa9fa6b94aa24939b50effa0e575b_7",
  "x": "We define the similarity measure s(i, c) in the joint embedding space following<cite> Kiros et al. (2014)</cite> . Let \u03c6(i; \u03b8 \u03c6 ) \u2208 R D \u03c6 be the representation of the image (e.g. the representation before logits in VGG19 Simonyan & Zisserman (2014) or ResNet152 He et al. (2016) ). Similarly, let \u03c8(c; \u03b8 \u03c8 ) \u2208 R D \u03c8 be the embedding of a caption c in a caption embedding space (e.g. a GRU-based text encoder).",
  "y": "uses"
 },
 {
  "id": "9aa9fa6b94aa24939b50effa0e575b_9",
  "x": "We define e(\u03b8, S) = 1 N N n=1 (i n , c n ) to be the empirical loss of the model, parametrized by \u03b8, over the training samples S = {(i n , c n )} N n=1 , where (i n , c n ) is a loss of a single example. The rank loss used in<cite> Kiros et al. (2014)</cite> , Socher et al. (2014) , and Karpathy & Fei-Fei (2015) is defined as follows: where \u03b1 represents the margin,\u0109 denotes a negative caption for the query image i, and\u00ee a negative image for the query caption c. Here, we used a shorthand notation [x] + \u2261 max(x, 0).",
  "y": "uses"
 },
 {
  "id": "9aa9fa6b94aa24939b50effa0e575b_10",
  "x": "Note that since we use SGD to learn our parameters, we take the max over each mini-batch (this is similar to<cite> Kiros et al. (2014)</cite> , while empty circles are negative samples for the query i. The dashed circles on the two sides are drawn at the same radii. The example on the left has a higher loss with the sum loss compared to the example on the right. The max loss assigns a higher loss to the example on the right.",
  "y": "similarities"
 },
 {
  "id": "9aa9fa6b94aa24939b50effa0e575b_11",
  "x": "**EXPERIMENTS** We perform experiments with our VSE++ and compare it to the original formulation of<cite> Kiros et al. (2014)</cite> (referred to as VSE), as well as state-of-the-art approaches. We re-implemented VSE with the help of the authors' open-source code 2 .",
  "y": "uses"
 },
 {
  "id": "9aa9fa6b94aa24939b50effa0e575b_12",
  "x": "We also try training using random crops which we denote by RC. For RC, we have the full VGG19 model and extract features over a single randomly chosen cropped patch on the fly as opposed to pre-computing the image features once and reusing them. For the caption encoder, we use a GRU similar to the one used in<cite> Kiros et al. (2014)</cite> .",
  "y": "similarities"
 },
 {
  "id": "9aa9fa6b94aa24939b50effa0e575b_13",
  "x": "We set the dimensionality of the GRU, D \u03c8 , and the joint embedding space, D, to 1024. The dimensionality of the word embeddings that are input to the GRU is set to 300. We further note that in<cite> Kiros et al. (2014)</cite> , the caption embedding is normalized, while the image embedding is not.",
  "y": "background"
 },
 {
  "id": "9aa9fa6b94aa24939b50effa0e575b_14",
  "x": "We further note that in<cite> Kiros et al. (2014)</cite> , the caption embedding is normalized, while the image embedding is not. In VSE++ we normalize both vectors.",
  "y": "differences"
 },
 {
  "id": "9aa9fa6b94aa24939b50effa0e575b_15",
  "x": "In this paper, we focused on the task of image-caption retrieval and investigated visual-semantic embeddings. We have shown that a new loss function that uses only violation incurred by hard negatives drastically improves performance over the typical loss that sums the violations across the negatives, typically used in previous work <cite>(Kiros et al. (2014)</cite> ; Vendrov et al. (2015) ). We performed experiments on the MS-COCO and Flickr30K datasets.",
  "y": "differences"
 },
 {
  "id": "9ac581130218d6c68ca785e3d5ba99_0",
  "x": "In-Degree Centrality as implemented in (Manion and Sainudiin, 2014) observes F-Score improvement (F + \u2206F) by applying the iterative approach. The author found in the investigations of his thesis (Manion, 2014) that the iterative approach performed best on the SemEval 2013 Multilingual WSD Task <cite>(Navigli et al., 2013)</cite> , as opposed to earlier tasks such as SensEval 2004 English All Words WSD Task (Snyder and Palmer, 2004) and the SemEval 2010 All Words WSD task on a Specific Domain (Agirre et al., 2010) . While these earlier tasks also experienced improvement, F-Scores remained lower overall.",
  "y": "background"
 },
 {
  "id": "9ac581130218d6c68ca785e3d5ba99_1",
  "x": "Then for disambiguation the graph centrality measure PageRank (Brin and Page, 1998 ) is used in conjunction with a surfing vector that biases probability mass to certain sense nodes in the semantic subgraph. This idea is taken from Personalised PageRank (PPR) (Agirre and Soroa, 2009) , which applies the method put forward by Haveliwala (2003) to the field of WSD. In the previous SemEval WSD task <cite>(Navigli et al., 2013)</cite> team UMCC DLSI (Gutierrez et al., 2013) implemented this method and achieved the best performance by biasing probability mass based on SemCor (Miller et al., 1993 ) sense frequencies.",
  "y": "background"
 },
 {
  "id": "9ac581130218d6c68ca785e3d5ba99_2",
  "x": "The author could not improve on their superior results achieved in English, however for Spanish and Italian the BabelNet First Sense (BFS) baseline was much lower since it often resorted to lexicographic sorting in the absence of WordNet synsets -see <cite>(Navigli et al., 2013)</cite> . The author's baseline-independent submissions were unaffected by this, which on reviewing results in (Moro and Navigli, 2015) appears to have helped SUDOKU do best for these languages. Table 3 : F1 scores for each domain/language for SUDOKU and LIMSI.",
  "y": "background"
 },
 {
  "id": "9af4a895dd4b45bb3827c74bdc7f05_0",
  "x": "Very recently, however, Somasundaran and Chodorow (2014) and <cite>Somasundaran et al. (2015)</cite> Even if these results were extremely promising, they leave a number of questions unanswered. First, they were obtained by studying short oral responses. Can they be generalized to longer written texts, a situation that allows the learner to spend much more time on its production? Then one can wonder whether the use of MI is sufficient, or if additional benefits can be obtained by taking into account other associational measures for collocations.",
  "y": "motivation"
 },
 {
  "id": "9af4a895dd4b45bb3827c74bdc7f05_1",
  "x": "Fisher's exact test (Pedersen et al., 1996) , which corresponds to the probability of observing, under the null hypothesis of independence, at least as many collocations as the number actually observed, 5. Mutual rank ratio (mrr, Dean, 2005), a nonparametric measure that has been successful in detecting collocation errors in EFL texts (Futagi et al., 2008), 6 . logDice (Rychly, 2008), a logarithmic transformation of the Dice coefficient used in the Sketch Engine (Kilgarriff et al., 2014) . In order to extract more information from the distribution of the ASs in each text than the mean or the median, Durrant and Schmitt (2009) and <cite>Somasundaran et al. (2015)</cite> used a standard procedure in descriptive statistics and automatic information processing known as discretization, binning or quantization (Garcia et al., 2013) .",
  "y": "background"
 },
 {
  "id": "9af4a895dd4b45bb3827c74bdc7f05_2",
  "x": "As in Yannakoudakis et al. (2011) , the 1141 texts from the year 2000 were used for training, while the 97 texts from the year 2001 were used for testing. Collocational Features: The global statistical features in <cite>Somasundaran et al. (2015)</cite> and were used: the mean, the median, the maximum and the minimum of the ASs, and the proportion of bigrams that are present in the learner text but absent from the reference corpus. Because the best number of bins for discretizing the distributions was not known, the following ones were compared: 3, 5, 8, 10, 15, 20, 25, 33, 50, 75 and 100. To get all these features, each learner text was tokenized and POS-tagged by means of CLAWS7 2 and all bigrams were extracted.",
  "y": "uses"
 },
 {
  "id": "9af4a895dd4b45bb3827c74bdc7f05_3",
  "x": "It is noteworthy that all the correlations reported in table 1 are much larger that the correlation of a baseline system based purely on length (r = 0.27). To determine if the automatic procedure for discretizing the ASs is at least as effective as the bin boundaries manually set by <cite>Somasundaran et al. (2015)</cite> , I used them instead of the automatic bins for the model with eight bins based on MI. The correlation obtained was 0.60, a value slightly lower than that reported in Table 1 (0.61).",
  "y": "uses"
 },
 {
  "id": "9af4a895dd4b45bb3827c74bdc7f05_4",
  "x": "It is also necessary to determine whether the collocational features can improve not only the baseline used here, but also a predictive model that includes many other features known for their effectiveness. Further developments are worth mentioning. Unlike <cite>Somasundaran et al. (2015)</cite> , I only used bigrams' collocational features.",
  "y": "differences"
 },
 {
  "id": "9b203bfa690c4a79c1324360a4b8dc_0",
  "x": "Recent attempts to solve this task deal with proposing similarity measures based on distributional semantic models (Roller et al., 2014; Weeds et al., 2014;<cite> Santus et al., 2016</cite>; Shwartz et al., 2017; Roller and Erk, 2016) . For hypernymy detection, several works use distributional inclusion hypothesis (Geffet and Dagan, 2005) , entropy-based distributional measure (Santus et al., 2014) as well as several embedding schemes (Fu et al., 2014; Yu et al., 2015; Nguyen et al., 2017) . Image generality for lexical entailment detection (Kiela et al., 2015) has also been tried out for the same purpose.",
  "y": "background"
 },
 {
  "id": "9b203bfa690c4a79c1324360a4b8dc_1",
  "x": "One such attempt is made by Weeds et al. (2014) , where they proposed a supervised framework and used several vector operations as features for the classification of hypernymy and co-hyponymy. <cite>Santus et al. (2016)</cite> proposed a supervised method based on a Random Forest algorithm to learn taxonomical semantic relations and they have shown that the model performs well for co-hyponymy detection. In another attempt, Jana and Goyal (2018b) proposed various complex network measures which can be used as features to build a supervised classifier model for co-hyponymy detection, and showed improvements over other baseline approaches.",
  "y": "background"
 },
 {
  "id": "9b203bfa690c4a79c1324360a4b8dc_2",
  "x": "Evaluation results: We evaluate the usefulness of DT embeddings against three benchmark datasets for cohyponymy detection (Weeds et al., 2014;<cite> Santus et al., 2016</cite>; Jana and Goyal, 2018b) , following their experimental setup. We show that the network embeddings outperform the baselines by a huge margin throughout all the experiments, except for co-hyponyms vs. random pairs, where the baselines already have very high accuracy and network embeddings are able to match the results. arXiv:2002.11506v1 [cs.CL] 24 Feb 2020",
  "y": "uses"
 },
 {
  "id": "9b203bfa690c4a79c1324360a4b8dc_3",
  "x": "Classification model: To distinguish the word pairs having co-hyponymy relation from the word pairs having hypernymy or meronymy relation, or from any random pair of words, we combine the network embeddings of the two words by concatenation (CC) and addition (ADD) operations to provide as features to train classifiers like Support Vector Machine (SVM) and Random Forest (RF). Evaluation results: We evaluate the usefulness of DT embeddings against three benchmark datasets for cohyponymy detection (Weeds et al., 2014;<cite> Santus et al., 2016</cite>; Jana and Goyal, 2018b) , following their experimental setup. We show that the network embeddings outperform the baselines by a huge margin throughout all the experiments, except for co-hyponyms vs. random pairs, where the baselines already have very high accuracy and network embeddings are able to match the results.",
  "y": "differences"
 },
 {
  "id": "9b203bfa690c4a79c1324360a4b8dc_4",
  "x": "---------------------------------- **EXPERIMENTAL RESULTS AND ANALYSIS** We perform experiments using three benchmark datasets for co-hyponymy detection (Weeds et al., 2014;<cite> Santus et al., 2016</cite>; Jana and Goyal, 2018b) .",
  "y": "uses"
 },
 {
  "id": "9b203bfa690c4a79c1324360a4b8dc_5",
  "x": "For each of these, we follow the same experimental setup as discussed by the authors and compare our method with the method proposed by the author as well as the state-of-the-art models by Jana and Goyal (2018b) . We perform the analysis of three datasets to investigate the extent of overlap present in these publicly available benchmark datasets and find out that 45.7% word pairs of dataset prepared by Weeds et al. (2014) are present in dataset ROOT9 prepared by <cite>Santus et al. (2016)</cite> . This intersection set comprises 27.8% of the ROOT9 dataset.",
  "y": "uses"
 },
 {
  "id": "9b203bfa690c4a79c1324360a4b8dc_7",
  "x": "**CO-HYP VS RANDOM** Co-Hyp vs Hyper<cite> (Santus et al., 2016)</cite> 97.8 95.7 (Jana and Goyal, 2018b) 99 Table 4 : Percentage F1 scores on a ten-fold cross validation of our models along with the best models described in<cite> (Santus et al., 2016)</cite> and (Jana and Goyal, 2018b) for ROOT9 dataset a set of baseline methodologies, the descriptions of which are presented in Table 1 . Following the same experimental setup, we report the accuracy measure for ten-fold cross validation and compare our models with the baselines in proposed by Weeds et al. (2014) .",
  "y": "uses"
 },
 {
  "id": "9b203bfa690c4a79c1324360a4b8dc_8",
  "x": "Here, the best model proposed by Jana and Goyal (2018b) uses SVM classifier which is fed with structural similarity of the words in the given word pair from the distributional thesaurus network. We see that all the 4 proposed methods perform at par or better than the baselines, and using RF CC gives a 15.4% improvement over the best results reported. 3.2. Experiment-2<cite> (Santus et al., 2016)</cite> In the second experiment, we use ROOT9 dataset prepared by <cite>Santus et al. (2016)</cite> , containing 9,600 labeled pairs extracted from three datasets: EVALution (Santus et al., 2015) , Lenci/Benotto (?) and BLESS (Baroni and Lenci, 2011) .",
  "y": "uses"
 },
 {
  "id": "9b203bfa690c4a79c1324360a4b8dc_9",
  "x": "3.2. Experiment-2<cite> (Santus et al., 2016)</cite> In the second experiment, we use ROOT9 dataset prepared by <cite>Santus et al. (2016)</cite> , containing 9,600 labeled pairs extracted from three datasets: EVALution (Santus et al., 2015) , Lenci/Benotto (?) and BLESS (Baroni and Lenci, 2011) . There is an even distribution of the three classes (hypernyms, co-hyponyms and random) in the dataset. Following the same experimental setup as<cite> (Santus et al., 2016)</cite> , we report percentage F1 scores on a ten-fold cross validation for binary classification of co-hyponyms vs random pairs, as well as co-hyponyms vs. hypernyms using both SVM and Random Forest classifiers.",
  "y": "uses"
 },
 {
  "id": "9b203bfa690c4a79c1324360a4b8dc_10",
  "x": "Following the same experimental setup as<cite> (Santus et al., 2016)</cite> , we report percentage F1 scores on a ten-fold cross validation for binary classification of co-hyponyms vs random pairs, as well as co-hyponyms vs. hypernyms using both SVM and Random Forest classifiers. Table 4 represents the performance comparison of our models with the best stateof-the-art models reported in<cite> (Santus et al., 2016)</cite> and (Jana and Goyal, 2018b) . Here, the best model proposed by <cite>Santus et al. (2016)</cite> uses Random Forest classifier which is fed with nine corpus based features like frequency of words, co-occurrence frequency etc., and the best model proposed by Jana and Goyal (2018b) use Random Forest classifier which is fed with five complex network features like structural similarity, shortest path etc.",
  "y": "uses"
 },
 {
  "id": "9b203bfa690c4a79c1324360a4b8dc_11",
  "x": "Here, the best model proposed by <cite>Santus et al. (2016)</cite> uses Random Forest classifier which is fed with nine corpus based features like frequency of words, co-occurrence frequency etc., and the best model proposed by Jana and Goyal (2018b) use Random Forest classifier which is fed with five complex network features like structural similarity, shortest path etc. computed from the distributional thesaurus network. The results in Table 4 shows that, for the binary classification task of co-hyponymy vs random pairs, we achieve percentage F1 score of 99.0 with RF CC which is at par with the state-of-the-art models.",
  "y": "background"
 },
 {
  "id": "9b203bfa690c4a79c1324360a4b8dc_12",
  "x": "Here, the best model proposed by <cite>Santus et al. (2016)</cite> uses Random Forest classifier which is fed with nine corpus based features like frequency of words, co-occurrence frequency etc., and the best model proposed by Jana and Goyal (2018b) use Random Forest classifier which is fed with five complex network features like structural similarity, shortest path etc. The results in Table 4 shows that, for the binary classification task of co-hyponymy vs random pairs, we achieve percentage F1 score of 99.0 with RF CC which is at par with the state-of-the-art models.",
  "y": "similarities"
 },
 {
  "id": "9b594c5b29175fc8ee598f61609c79_0",
  "x": [
   "**COMPRESSION** Each dialogue act has its constituent words scored using tf.idf, and as the user compresses the meeting to a greater degree the browser gradually removes the less important words from each dialogue act, leaving only the most informative material of the meeting. Previous work has explored the eect of lexical cohesion and conversational features on characterizing topic boundaries, following Galley et al.(2003) ."
  ],
  "y": "background"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_0",
  "x": "Recently, sequence-to-sequence models<cite> [Sutskever et al., 2014</cite>; have gain superior performance in machine translation Vaswani et al., 2017 ]. Yet these state-of-the-art neural machine translation (NMT) models still fail to generation target sentences with comparable quality as human translators. To obtain a high-quality translation, there are a few recent works attempt to incorporate human revision instructions into the algorithmic translation process.",
  "y": "background"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_1",
  "x": "**INTRODUCTION** Recently, sequence-to-sequence models<cite> [Sutskever et al., 2014</cite>; have gain superior performance in machine translation Vaswani et al., 2017 ]. Yet these state-of-the-art neural machine translation (NMT) models still fail to generation target sentences with comparable quality as human translators.",
  "y": "motivation background"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_2",
  "x": "To obtain a high-quality translation, there are a few recent works attempt to incorporate human revision instructions into the algorithmic translation process. For example, previous interactive NMT [ Sanchis-Trilles et al., 2014; Alvaro Peris et al., 2016;<cite> Knowles and Koehn, 2016]</cite> proposes to ask human to revise the translation output from the beginning of a sentence to the end (i.e. from the left to right), and regenerates the partial translation on the right side of the * Equal contribution, part of this work was done while Rongxiang Weng was a research intern at ByteDance AI Lab. \u2020 Corresponding author.",
  "y": "background"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_3",
  "x": "Cheng et al. [2016] show that revising critical mistakes first could significantly reduce the number of revisions. However, their intuition could not arXiv:1907.03468v1 [cs.CL] 8 Jul 2019 be directly applied to current uni-directional NMT models, because after revising critical mistakes, uni-directional interactive models cannot correct minor mistakes to the left of the revision, even with advanced decoding algorithm [Hokamp and Liu, 2017; Post and Vilar, 2018;<cite> Hasler et al., 2018]</cite> .",
  "y": "motivation background"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_4",
  "x": "CAMIT performs the interactive NMT from both ends of the sentence, updating the whole sentence after getting a revision. Different from previous work about bi-directional decoder Mou et al., 2016;<cite> Liu et al., 2018]</cite> , our method is designed to take human revisions for interactive NMT. In such case, human can revise the most critical mistake first in an arbitrary position of the sentence; and after that the model will update the whole sentence, fixing minor mistakes automatically.",
  "y": "differences"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_5",
  "x": "**BACKGROUND** Neural machine translation (NMT) is based on a standard Seq2Seq model, which adopts an encoder-decoder architecture for sentence modeling and generation<cite> [Sutskever et al., 2014</cite>; Vaswani et al., 2017] . The encoder summarizes the source sentence into an intermediate representation, and the decoder generates the target sentence from left to right.",
  "y": "background"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_7",
  "x": "Here \u2212 \u2192 f is a recurrent unit or self-attention network [Vaswani et al., 2017] . c j is a vector summarizing relevant source information. It is computed by the attention mechanism<cite> [Luong et al., 2015b</cite>; Vaswani et al., 2017] .",
  "y": "background"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_8",
  "x": "One revision is the process that human translator manually corrects one mistake in a sentence. Commonly, the revision can include several operations: replacement, insertion and deletion. Following previous work<cite> [\u00c1lvaro Peris et al., 2016</cite>;<cite> Knowles and Koehn, 2016</cite>; Cheng et al., 2016] , we focus on the replacement and others can be implemented with several replacement operations.",
  "y": "uses"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_9",
  "x": "In the interactive process, let y = {y 1 , \u00b7 \u00b7 \u00b7 , y j , \u00b7 \u00b7 \u00b7 } be the initial sequence of words, output by the base Seq2Seq model. If human translator revises the word y j , the output is divided into 2 parts: Uni-directional interactive model only updates the right part<cite> [\u00c1lvaro Peris et al., 2016</cite>;<cite> Knowles and Koehn, 2016]</cite> , ignoring potential mistakes in the left part, which means the revised word should always be the left most error, otherwise the errors in the left part will never be corrected.",
  "y": "background motivation"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_10",
  "x": "Uni-directional interactive model only updates the right part<cite> [\u00c1lvaro Peris et al., 2016</cite>;<cite> Knowles and Koehn, 2016]</cite> , ignoring potential mistakes in the left part, which means the revised word should always be the left most error, otherwise the errors in the left part will never be corrected. We purpose a sequential bi-directional decoder for interactive NMT, which can update both parts of the sentence. The proposed model includes two decoders: a forward decoder \u2212 \u2192 f and a backward decoder",
  "y": "differences"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_11",
  "x": "The training stage is similar with multi-task models [Dong et al., 2015; <cite>Luong et al., 2015a]</cite> , both decoders could be trained using cross-entropy as the objective: where L R is computed by: The P (y j |y >j , x) and L L are defined in Equation 1 and Equation 4, respectively.",
  "y": "similarities"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_12",
  "x": "We have shown how to update the sentence after a single revision. However, another challenge for interactive NMT y r k+1 happens when human translator performs several revisions in one round, because the interactive process should regenerate the translation with all the revisions considered. To solve the problem, we propose to combine the grid beam search<cite> [Hokamp and Liu, 2017]</cite> with our bi-directional decoder.",
  "y": "extends"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_13",
  "x": "When generating a word in decoding, the model will read the revision memory, trying to automatically fix mistakes with a copy mechanism<cite> [Gu et al., 2016]</cite> . The final output distribution of word w is computed by distributions from the decoder and from the revision memory: where r j,t is the probability of copying word y r t in current position j. It is calculated by the revision context < s j , c j > of current decoding state and revision contexts of items in the memory.",
  "y": "uses"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_14",
  "x": "Formally, every time we obtain a correct translation pair {x, y } after interaction, we update the whole translation model for one step, according to Equation 4. By learning from the sentence level interaction history, our Seq2Seq model better fits We measure the translation quality with the IBM-BLEU score<cite> [Papineni et al., 2002]</cite> . We implement our interactive NMT model upon NJUNMT-pytorch 4 .",
  "y": "uses"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_15",
  "x": "**IDEAL AND REAL INTERACTIVE ENVIRONMENTS** Following previous work [Cheng et al., 2016;<cite> \u00c1lvaro Peris et al., 2016</cite>;<cite> Hokamp and Liu, 2017]</cite> , we experiment on both the ideal and real environments. Because real-world human interactions are expensive and time-consuming to obtain, we first report results on the ideal environment, which generates simulated human interactions by identifying critical mistakes.",
  "y": "uses"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_16",
  "x": "**RELATED WORK** Interactive machine translation has been widely exploited to improve the translation by using interaction feedback from human users<cite> [Langlais et al., 2000</cite>; Simard et al., 2007; Barrachina et al., 2009; Gonz\u00e1lez-Rubio et al., 2013; Cheng et al., 2016] in statistic machine translation (SMT) [Yamada and Knight, 2001;<cite> Koehn et al., 2003</cite>; Chiang, 2007] . Recently, researchers employ it in neural machine translation (NMT).",
  "y": "background"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_17",
  "x": "Interactive machine translation has been widely exploited to improve the translation by using interaction feedback from human users<cite> [Langlais et al., 2000</cite>; Simard et al., 2007; Barrachina et al., 2009; Gonz\u00e1lez-Rubio et al., 2013; Cheng et al., 2016] in statistic machine translation (SMT) [Yamada and Knight, 2001;<cite> Koehn et al., 2003</cite>; Chiang, 2007] . Recently, researchers employ it in neural machine translation (NMT). Barrachina et al. [2009] , Gonz\u00e1lez-Rubio et al. [2013] and<cite> Knowles and Koehn [2016]</cite> present an interactive NMT model with the uni-directional interaction protocol (UniDiR), in which users can only interact with the model from left to right.",
  "y": "background"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_18",
  "x": "Barrachina et al. [2009] , Gonz\u00e1lez-Rubio et al. [2013] and<cite> Knowles and Koehn [2016]</cite> present an interactive NMT model with the uni-directional interaction protocol (UniDiR), in which users can only interact with the model from left to right. In our proposed model, human can revise the most critical mistake first in arbitrary position of the sentence; and after that the whole sentence will updated, fixing minor mistakes automatically.",
  "y": "differences background"
 },
 {
  "id": "9b6c02d11028b5bf3813a7d61fed28_19",
  "x": "External information, such as word or char, has been verified to be effective in promoting translation [Chen et al., 2018; Zheng et al., 2018] , but these methods can not employ on the interactive process directly. Our proposed revision memory is inspired by the CopyNet<cite> [Gu et al., 2016]</cite> and history cache [Tu et al., 2017] , which only focus on better using source or global context in supervised learning, and is different from our model. ----------------------------------",
  "y": "motivation differences"
 },
 {
  "id": "9c3c35343aeaae0520d92f64e118a2_0",
  "x": "Along with this dataset, the task of targeted aspectbased sentiment analysis is introduced which is different from general aspect-based sentiment analysis, in extracting fine-grained information with respect to targets specified in reviews. At <cite>(Chen et al., 2017)</cite> , a Chinese dataset for aspectbased sentiment analysis from comments about the news with 6365 positive, 9457 neutral and 6839 negative annotated data samples was proposed. ----------------------------------",
  "y": "background"
 },
 {
  "id": "9c3c35343aeaae0520d92f64e118a2_1",
  "x": "\uf0b7 RAM <cite>(Chen et al., 2017)</cite> : This method makes a memory from the input and with using multiple attention mechanism, it extracts important information from memory and for prediction it uses a combination of the extracted features of different attentions non-linearly. \uf0b7 IAN (Dehong et al., 2017) : It learns the attentions inside the document and targets interactively, and originates the representations for targets and the document separately. \uf0b7 ATAE-LSTM (Wang et al., 2016) : It uses attention mechanism along with Long Short-Term Memory Network.",
  "y": "background"
 },
 {
  "id": "9c3c35343aeaae0520d92f64e118a2_2",
  "x": "Table 3 compares the performance of these models on English datasets based on f1 score macro and accuracy metrics. \uf0b7 RAM <cite>(Chen et al., 2017)</cite> : This method makes a memory from the input and with using multiple attention mechanism, it extracts important information from memory and for prediction it uses a combination of the extracted features of different attentions non-linearly.",
  "y": "uses background"
 },
 {
  "id": "9c3c35343aeaae0520d92f64e118a2_3",
  "x": "The authors of RAM <cite>(Chen et al., 2017)</cite> claimed that their model is language insensitive, which mean it can perform on all languages and, compared to TD-LSTM (Tang et al., 2016) which might lose feature if the opinion word is far from the target, they employed the recurrent attention to solving this problem. But by comparing results, it's obvious that TD-LSTM (Tang et al., 2016) outperforms their method in Persian. Pars-ABSA dataset is available through: https://github.com/Titowak/Pars-ABSA",
  "y": "background"
 },
 {
  "id": "9c3c35343aeaae0520d92f64e118a2_4",
  "x": "The authors of RAM <cite>(Chen et al., 2017)</cite> claimed that their model is language insensitive, which mean it can perform on all languages and, compared to TD-LSTM (Tang et al., 2016) which might lose feature if the opinion word is far from the target, they employed the recurrent attention to solving this problem. But by comparing results, it's obvious that TD-LSTM (Tang et al., 2016) outperforms their method in Persian. Pars-ABSA dataset is available through: https://github.com/Titowak/Pars-ABSA",
  "y": "differences"
 },
 {
  "id": "9c5baf669470fe4dd18277591591f1_0",
  "x": "The analyses have focused on wordlevel models, yet character-level models have been shown to outperform word-level models in some NLP tasks, such as text classification (Zhang et al., 2015) , named entity recognition (Kuru et al., 2016) , and time normalization<cite> (Laparra et al., 2018a)</cite> . Thus, there is a need to study pre-trained contextualized character embeddings, to see if they also yield improvements, and if so, to analyze where those benefits are coming from. All of the pre-trained word-level contextual embedding models include some character or subword components in their architecture.",
  "y": "background"
 },
 {
  "id": "9c5baf669470fe4dd18277591591f1_1",
  "x": "The analyses have focused on wordlevel models, yet character-level models have been shown to outperform word-level models in some NLP tasks, such as text classification (Zhang et al., 2015) , named entity recognition (Kuru et al., 2016) , and time normalization<cite> (Laparra et al., 2018a)</cite> . Thus, there is a need to study pre-trained contextualized character embeddings, to see if they also yield improvements, and if so, to analyze where those benefits are coming from. All of the pre-trained word-level contextual embedding models include some character or subword components in their architecture.",
  "y": "motivation"
 },
 {
  "id": "9c5baf669470fe4dd18277591591f1_2",
  "x": "We focus on the task of parsing time normalizations (Laparra et al., 2018b) , where large gains of character-level models over word-level models have been observed<cite> (Laparra et al., 2018a)</cite> . This task involves finding and composing pieces of a time expression to infer time intervals, so for example, the expression 3 days ago could be normalized to the interval [2019-03-01, 2019-03-02) . We first take a state-of-the-art neural network for parsing time normalizations<cite> (Laparra et al., 2018a)</cite> and replace its randomly initialized character embeddings with pre-trained contextual character embeddings.",
  "y": "uses background"
 },
 {
  "id": "9c5baf669470fe4dd18277591591f1_3",
  "x": "We first take a state-of-the-art neural network for parsing time normalizations<cite> (Laparra et al., 2018a)</cite> and replace its randomly initialized character embeddings with pre-trained contextual character embeddings. After showing that this yields major performance improvements, we analyze the improvements to understand why pre-trained contextual character embeddings are so useful. Our contributions are:",
  "y": "extends"
 },
 {
  "id": "9c5baf669470fe4dd18277591591f1_4",
  "x": "\u2022 We derive pre-trained contextual character embeddings from Flair (Akbik et al., 2018) , apply them to a state-of-the art time normalizer<cite> (Laparra et al., 2018a)</cite> , and obtain major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% error reduction in clinical notes. \u2022 We demonstrate that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and crossdomain changes. \u2022 We quantify the amount of context leveraged by pre-trained contextual character embeddings.",
  "y": "uses"
 },
 {
  "id": "9c5baf669470fe4dd18277591591f1_6",
  "x": "In this paper, We focus on the character-level time entity identifier that is the foundation of <cite>Laparra et al. (2018a)</cite> 's model. The sequence tagger is a multi-output RNN with three different input features, shown in Figure 1 . Features are mapped through an embedding layer, then fed into stacked bidirectional Gated Recurrent Units (bi-GRUs), and followed by a softmax layer.",
  "y": "uses"
 },
 {
  "id": "9c5baf669470fe4dd18277591591f1_7",
  "x": "Features are mapped through an embedding layer, then fed into stacked bidirectional Gated Recurrent Units (bi-GRUs), and followed by a softmax layer. There are three types of outputs per <cite>Laparra et al. (2018a)</cite> 's encoding of the SCATE schema, so there is a separate stack of bi-GRUs and a softmax for each output type. We keep the original neural architecture and parameter settings in <cite>Laparra et al. (2018a)</cite> , and experiment with the following embedding layers: Rand(128): the original setting of <cite>Laparra et al. (2018a)</cite> , where 128-dimensional character embeddings are randomly initialized.",
  "y": "background"
 },
 {
  "id": "9c5baf669470fe4dd18277591591f1_8",
  "x": "We keep the original neural architecture and parameter settings in <cite>Laparra et al. (2018a)</cite> , and experiment with the following embedding layers: Rand(128): the original setting of <cite>Laparra et al. (2018a)</cite> , where 128-dimensional character embeddings are randomly initialized. Rand (4096) ning Flair forward-backward character-level LM Flair's forward and backward character-level language models over the text, and concatenating the hidden states from forward and backward character-level LMs for each character . We evaluate in the clinical and news domains, the former being more than 9 times larger and the latter having a more diverse set of labels.",
  "y": "extends"
 },
 {
  "id": "9d12c4d6aea96ff3e1b93faf1eb961_0",
  "x": "---------------------------------- **TUTORIAL DESCRIPTION** This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (Saon and Chien, 2012; Chan et al., 2016) to document summarization (Chang and Chien, 2009 ), text classification (Blei et al., 2003; Zhang et al., 2015) , text segmentation (Chien and Chueh, 2012) , information extraction (Narasimhan et al., 2016) , image caption generation (Vinyals et al., 2015; Xu et al., 2015) , sentence generation (Li et al., 2016b) , dialogue control <cite>(Zhao and Eskenazi, 2016</cite>; Li et al., 2016a) , sentiment classification, recommendation system, question answering (Sukhbaatar et al., 2015) and machine translation , to name a few.",
  "y": "background"
 },
 {
  "id": "9dd9ac975c6f55797615f0e52aa296_0",
  "x": "In order to make it compatible with the previous work, we follow the procedure in<cite> (Nguyen and Grishman, 2015b)</cite> to process the trigger candidates for CNN. In particular, we limit the context of the trigger candidates to a fixed window size by trimming longer sentences and padding shorter sentences with a special token when necessary. Let 2n + 1 be the fixed window size, and W = [w 0 , w 1 , . . . , w n , . . . , w 2n 1 , w 2n ] be some trigger candidate where the current token is positioned in the middle of the window (token w n ).",
  "y": "uses"
 },
 {
  "id": "9dd9ac975c6f55797615f0e52aa296_1",
  "x": "The position embedding table is initialized randomly. 3. The real-valued embedding vector for the entity type of w i : This vector is generated by looking up the entity type embedding table (initialized randomly) for the entity type of w i . Note that we employ the BIO annotation schema to assign entity type labels to each token in the sentences using the entity mention heads as in<cite> (Nguyen and Grishman, 2015b)</cite> . The transformation from the token w i to the vector x i (x i 2 R d ) essentially converts the input candidate W into a sequence of real-valued vectors X = (x 0 , x 1 , . . . , x 2n ).",
  "y": "uses"
 },
 {
  "id": "9dd9ac975c6f55797615f0e52aa296_2",
  "x": "We apply the same parameters and resources as<cite> (Nguyen and Grishman, 2015b)</cite> to ensure the compatible comparison. Specifically, we employ the window sizes in the set {2, 3, 4, 5} for the convolution operation with 150 filters for each window size. The window size of the trigger candidate is 31 while the dimensionality of the position embeddings and entity type embeddings is 50.",
  "y": "uses"
 },
 {
  "id": "9dd9ac975c6f55797615f0e52aa296_3",
  "x": "We use word2vec from (Mikolov et al., 2013b) as the pretrained word embeddings. The other parameters include the dropout rate \u21e2 = 0.5, the mini-batch size = 50, the predefined threshold for the l 2 norms = 3. Following the previous studies (Li et al., 2013; Chen et al., 2015;<cite> Nguyen and Grishman, 2015b)</cite> , we evaluate the models on the ACE 2005 corpus with 33 event subtypes.",
  "y": "uses"
 },
 {
  "id": "9dd9ac975c6f55797615f0e52aa296_4",
  "x": "Following the previous studies (Li et al., 2013; Chen et al., 2015;<cite> Nguyen and Grishman, 2015b)</cite> , we evaluate the models on the ACE 2005 corpus with 33 event subtypes. In order to make it compatible, we use the same test set with 40 newswire articles, the same development set with 30 other documents and the same training set with the remaining 529 documents. All the data preprocessing and evaluation criteria follow those in<cite> (Nguyen and Grishman, 2015b)</cite>.",
  "y": "uses"
 },
 {
  "id": "9dd9ac975c6f55797615f0e52aa296_6",
  "x": "We compares the non-consecutive CNN model (NC-CNN) with the state-of-the-art systems on the ACE 2005 dataset in Table 1 . These systems include: 2) The neural network models, i.e, the CNN model in<cite> (Nguyen and Grishman, 2015b)</cite> (CNN), the dynamic multi-pooling CNN model (DM-CNN) in (Chen et al., 2015) and the bidirectional recurrent neural networks (B-RNN) in (Nguyen et al., 2016a) .",
  "y": "uses"
 },
 {
  "id": "9dd9ac975c6f55797615f0e52aa296_7",
  "x": "Finally, comparing NC-CNN and the CNN model in<cite> (Nguyen and Grishman, 2015b)</cite>, we see that the non-consecutive mechanism significantly improves the performance of the traditional CNN model for ED (up to 2.3% in absolute Fmeasures with p < 0.05). ---------------------------------- **THE DOMAIN ADAPTATION EXPERIMENTS**",
  "y": "differences"
 },
 {
  "id": "9dd9ac975c6f55797615f0e52aa296_8",
  "x": "The best reported system in the DA setting for ED is<cite> (Nguyen and Grishman, 2015b)</cite> , which demonstrated that the CNN model outperformed the feature-based models in the cross-domain setting. In this section, we compare NC-CNN with the CNN model in<cite> (Nguyen and Grishman, 2015b)</cite> (as well as the other models above) in the DA setting to further investigate their effectiveness. ----------------------------------",
  "y": "background"
 },
 {
  "id": "9dd9ac975c6f55797615f0e52aa296_9",
  "x": "In this section, we compare NC-CNN with the CNN model in<cite> (Nguyen and Grishman, 2015b)</cite> (as well as the other models above) in the DA setting to further investigate their effectiveness. ---------------------------------- **DATASET**",
  "y": "uses"
 },
 {
  "id": "9dd9ac975c6f55797615f0e52aa296_10",
  "x": "This section also uses the ACE 2005 dataset but focuses more on the difference between domains. The ACE 2005 corpus includes 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and webblogs (wl). Following<cite> (Nguyen and Grishman, 2015b)</cite>, we use news (the union of bn and nw) as the source domain and bc, cts, wl and un as four different target domains 3 .",
  "y": "uses"
 },
 {
  "id": "9dd9ac975c6f55797615f0e52aa296_11",
  "x": "**PERFORMANCE** We emphasize that the performance of the systems MaxEnt, Joint+Local, Joint+Local+Global, B-RNN, and CNN is obtained from the actual systems in the original work (Li et al., 2013; <cite>Nguyen and Grishman, 2015b</cite>; Nguyen et al., 2016a) . The performance of DM-CNN, on the other hand, is from our re-implementation of the system in (Chen et al., 2015) using the same hyper-parameters and resources as CNN and NC-CNN for a fair comparison.",
  "y": "uses"
 },
 {
  "id": "9e0a44722390d0508fbe56785701e6_0",
  "x": "The work of <cite>Shwartz et al. (2016)</cite> , that we closely follow, is also using both semantic and syntactic features, by combining the dependency paths between entities, with word embedding representations of both the entities and the lemmas in the dependency paths. Another related area is relation extraction for Open Information Extraction (OpenIE). Some of the more representative projects in the area, like Reverb (Fader et al., 2011) and more recently ClauseIE (Del Corro and Gemulla, 2013) use syntactic information (PoS tagging / chunking, and dependency parsing respectively) to extract entity and relation phrases.",
  "y": "uses similarities"
 },
 {
  "id": "9e0a44722390d0508fbe56785701e6_1",
  "x": "As these approaches require multiple sources of weak supervision, we examine another line of projects which works by aggregating the support sentences 1 for each entity pair (Riedel et al., 2010; Hoffmann et al., 2011) . This is the approach that <cite>Shwartz et al. (2016)</cite> and the current work follow. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "9e0a44722390d0508fbe56785701e6_2",
  "x": "---------------------------------- **HYPENET** A recent paper<cite> (Shwartz et al., 2016)</cite> proposed HypeNET, a new method for RE that integrated dependency path information with distributional semantic vector representation of the entities.",
  "y": "background"
 },
 {
  "id": "9e0a44722390d0508fbe56785701e6_3",
  "x": "The bottom half of Figure 1 presents the distant supervision generation process adapted from <cite>Shwartz et al. (2016)</cite> to work with our data. In the original work, the text is processed to split and tokenise the sentences, tag the parts of speech and separate the noun phrases (NPs) -these are the candidate entities. They then construct the dependency path between each possible pair of entities.",
  "y": "extends"
 },
 {
  "id": "9e0a44722390d0508fbe56785701e6_4",
  "x": "Since for any given pair of entities it is much more likely that they are not going to be related, we only keep a small fraction of the negative instances. Following <cite>Shwartz et al. (2016)</cite> , we use a 4:1 negative to positive ratio. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "9e0a44722390d0508fbe56785701e6_5",
  "x": "---------------------------------- **ISOLATING HYPENET FEATURES** To discover the effectiveness of the approach of <cite>Shwartz et al. (2016)</cite> , we wanted to separate HypeNET's neural architecture from its input features and use those features with different (and simpler) classifiers.",
  "y": "motivation"
 },
 {
  "id": "9e0a44722390d0508fbe56785701e6_6",
  "x": "To discover the effectiveness of the approach of <cite>Shwartz et al. (2016)</cite> , we wanted to separate HypeNET's neural architecture from its input features and use those features with different (and simpler) classifiers. HypeNET's main advantage is that it integrated dependency path features with distributional information about the word lemmas along the path and left and right entities. As our goal was to generate discrete features to be used with more traditional classifiers, we opted for using Brown clusters (Brown et al., 1992) instead of the 50-dimensional GloVe vectors (Pennington et al., 2014) used by <cite>Shwartz et al. (2016)</cite> .",
  "y": "differences"
 },
 {
  "id": "9e0a44722390d0508fbe56785701e6_10",
  "x": "to reduce noise in the distant supervision labels (Hoffmann et al., 2011) . In <cite>Shwartz et al. (2016)</cite> , the grouping was performed by the mean pooling layer; in the case of the fastText-based system, we simply concatenate the feature tokens from all the supports and feed them into the single hidden layer. For each of the three relations, we ran the fastText model with and without grouping each entity pair's supports, using exactly the same features in both cases.",
  "y": "differences"
 },
 {
  "id": "9e0a44722390d0508fbe56785701e6_12",
  "x": "**USING SATELLITE NODES** We also looked at the role of the dependency path satellite nodes (words to the left and right of the entities). This type of features has also been adopted by various systems including Mintz et al. (2009) and <cite>Shwartz et al. (2016)</cite> , and we wanted to establish a basis for its effectiveness across multiple relations.",
  "y": "similarities uses"
 },
 {
  "id": "9e491cad55265802275e9aaea9faae_0",
  "x": "We are particularly interested in the contextaware QA paradigm, where the answer to each question can be obtained by referring to its accompanying context (paragraph or a list of sentences). Under this setting, the two most notable types of supervisions are coarse sentence-level and finegrained span-level. In sentence-level QA, the task is to pick sentences that are most relevant to the question among a list of candidates <cite>(Yang et al., 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "9e491cad55265802275e9aaea9faae_1",
  "x": "For the target datasets, we evaluate on two recent QA datasets, WikiQA <cite>(Yang et al., 2015)</cite> and Se-mEval 2016 (Task 3A) , which possess sufficiently different characteristics from that of SQuAD. Our results show 8% improvement in WikiQA and 1% improevement in Se-mEval. In addition, we report state-of-the-art results on recognizing textual entailment (RTE) in SICK (Marelli et al., 2014) with a similar transfer learning procedure.",
  "y": "uses"
 },
 {
  "id": "9e491cad55265802275e9aaea9faae_2",
  "x": "There have been several QA paradigms in NLP, which can be categorized by the context and supervision used to answer questions. This context can range from structured and confined knowledge bases (Berant et al., 2013) to unstructured and unbounded natural language form (e.g., documents on the web (Voorhees and Tice, 2000)) and unstructured, but restricted in size (e.g., a paragraph or multiple sentences (Hermann et al., 2015) ). The recent advances in neural question answering lead to numerous datasets and successful models in these paradigms (Rajpurkar et al., 2016;<cite> Yang et al., 2015</cite>; Nguyen et al., 2016; Trischler et al., 2016 ).",
  "y": "background"
 },
 {
  "id": "9e491cad55265802275e9aaea9faae_3",
  "x": "This enables us to make a fair comparison between pretraining with span-supervised and sentencesupervised QA datasets. WikiQA <cite>(Yang et al., 2015)</cite> is a sentence-level QA dataset, containing 1.9k/0.3k train/dev answerable examples. Each example consists of a real user's Bing query and a snippet of a Wikipedia article retrieved by Bing, containing 18.6 sentences on average.",
  "y": "background"
 },
 {
  "id": "9e8af6ca401cd74adc9a4137ae05ec_0",
  "x": "Spell checkers, typically developed for native speakers, fail to address many of the types of spelling errors peculiar to non-native speakers, especially those errors influenced by differences in phonology. Our model of pronunciation variation is used to extend a pronouncing dictionary for use in the spelling correction algorithm developed by <cite>Toutanova and Moore (2002)</cite>, which includes models for both orthography and pronunciation. The pronunciation variation modeling is shown to improve performance for misspellings produced by Japanese writers of English.",
  "y": "extends motivation"
 },
 {
  "id": "9e8af6ca401cd74adc9a4137ae05ec_1",
  "x": "We propose a method for modeling pronunciation variation in the context of spell checking for non-native writers of English. Spell checkers, typically developed for native speakers, fail to address many of the types of spelling errors peculiar to non-native speakers, especially those errors influenced by differences in phonology. Our model of pronunciation variation is used to extend a pronouncing dictionary for use in the spelling correction algorithm developed by <cite>Toutanova and Moore (2002)</cite>, which includes models for both orthography and pronunciation.",
  "y": "extends"
 },
 {
  "id": "9e8af6ca401cd74adc9a4137ae05ec_2",
  "x": "Phonological differences result in number of distinctions in English that are not present in Japanese and romazi causes difficulties for JWEFL because the Latin letters correspond to very different sounds in Japanese. We propose a method for creating a model of pronunciation variation from a phonetically untranscribed corpus of read speech recorded by nonnative speakers. The pronunciation variation model is used to generate multiple pronunciations for each canonical pronunciation in a pronouncing dictionary and these variations are used in the spelling correction approach developed by <cite>Toutanova and Moore (2002)</cite> , which uses statistical models of spelling errors that consider both orthography and pronunciation.",
  "y": "background"
 },
 {
  "id": "9e8af6ca401cd74adc9a4137ae05ec_3",
  "x": "Phonological differences result in number of distinctions in English that are not present in Japanese and romazi causes difficulties for JWEFL because the Latin letters correspond to very different sounds in Japanese. We propose a method for creating a model of pronunciation variation from a phonetically untranscribed corpus of read speech recorded by nonnative speakers. The pronunciation variation model is used to generate multiple pronunciations for each canonical pronunciation in a pronouncing dictionary and these variations are used in the spelling correction approach developed by <cite>Toutanova and Moore (2002)</cite> , which uses statistical models of spelling errors that consider both orthography and pronunciation.",
  "y": "motivation"
 },
 {
  "id": "9e8af6ca401cd74adc9a4137ae05ec_4",
  "x": "Each edit operation also has an associated probability that improves the ranking of candidate corrections by modeling how likely particular edits are. Brill and Moore (2000) estimate the probability of each edit from a corpus of spelling errors. <cite>Toutanova and Moore (2002)</cite> extend Brill and Moore (2000) to consider edits over both letter sequences and sequences of phones in the pronunciations of the word and misspelling.",
  "y": "background"
 },
 {
  "id": "9e8af6ca401cd74adc9a4137ae05ec_5",
  "x": "The spelling correction models from Brill and Moore (2000) and <cite>Toutanova and Moore (2002)</cite> use the noisy channel model approach to determine the types and weights of edit operations. The idea behind this approach is that a writer starts out with the intended word w in mind, but as it is being written the word passes through a noisy channel resulting in the observed non-word r. In order to determine how likely a candidate correction is, the spelling correction model determines the probability that the word w was the intended word given the misspelling r: P (w|r). To find the best correction, the word w is found for which P (w|r) is maximized: argmax w P (w|r).",
  "y": "background"
 },
 {
  "id": "9e8af6ca401cd74adc9a4137ae05ec_6",
  "x": "---------------------------------- **EXTENDING TO PRONUNCIATION** <cite>Toutanova and Moore (2002)</cite> describe an extension to Brill and Moore (2000) where the same noisy channel error model is used to model phone sequences instead of letter sequences.",
  "y": "background"
 },
 {
  "id": "9e8af6ca401cd74adc9a4137ae05ec_8",
  "x": "The error model over phone sequences, called P P H , is just like P L shown in Figure 1 except that r and w are replaced with their pronunciations. The model is trained like P L using alignments between phones. Since a spelling correction model needs to rank candidate words rather than candidate pronunciations, <cite>Toutanova and Moore (2002)</cite> derive an error model that determines the probability that a word w was spelled as the non-word r based on their pronunciations.",
  "y": "background"
 },
 {
  "id": "9e8af6ca401cd74adc9a4137ae05ec_9",
  "x": "A letter-to-phone (LTP) model is needed to predict the pronunciation of misspellings for P P HL , since they are not found in a pronouncing dictionary. Like <cite>Toutanova and Moore (2002)</cite> , we use the n-gram LTP model from Fisher (1999) to predict these pronunciations. The n-gram LTP model predicts the pronunciation of each letter in a word considering up to four letters of context to the left and right.",
  "y": "uses"
 },
 {
  "id": "9e8af6ca401cd74adc9a4137ae05ec_10",
  "x": "**METHOD** This section presents our method for modeling pronunciation variation from a phonetically untranscribed corpus of read speech. The pronunciationbased spelling correction approach developed in <cite>Toutanova and Moore (2002)</cite> requires a list of possible pronunciations in order to compare the pronunciation of the misspelling to the pronunciation of correct words.",
  "y": "background"
 },
 {
  "id": "9e8af6ca401cd74adc9a4137ae05ec_11",
  "x": "This section presents our method for modeling pronunciation variation from a phonetically untranscribed corpus of read speech. The pronunciationbased spelling correction approach developed in <cite>Toutanova and Moore (2002)</cite> requires a list of possible pronunciations in order to compare the pronunciation of the misspelling to the pronunciation of correct words. To account for target pronunciations specific to Japanese speakers, we observe the pronunciation variation in the ERJ and generate additional pronunciations for each word in the word list.",
  "y": "uses"
 },
 {
  "id": "9e8af6ca401cd74adc9a4137ae05ec_12",
  "x": "---------------------------------- **RESULTS** In order to evaluate the effect of pronunciation variation in <cite>Toutanova and Moore (2002)</cite> 's spelling correction approach, we compare the performance of the pronunciation model and the combined model with and without pronunciation variation.",
  "y": "uses"
 },
 {
  "id": "9e8af6ca401cd74adc9a4137ae05ec_13",
  "x": "---------------------------------- **SUMMARY OF RESULTS** The noisy channel spelling correction approach developed by Brill and Moore (2000) and <cite>Toutanova and Moore (2002)</cite> appears well-suited for writers of English as a foreign language.",
  "y": "background"
 },
 {
  "id": "9e8af6ca401cd74adc9a4137ae05ec_14",
  "x": "We have presented a method for modeling pronunciation variation from a phonetically untranscribed corpus of read non-native speech by adapting a monophone recognizer initially trained on native speech. This model allows a native pronouncing dictionary to be extended to include non-native pronunciation variations. We incorporated a pronouncing dictionary extended for Japanese writers of English into the spelling correction model developed by <cite>Toutanova and Moore (2002)</cite> , which combines orthography-based and pronunciation-based models.",
  "y": "uses"
 },
 {
  "id": "9e8c386b7ea3b5e4e2843fb8382fd8_0",
  "x": "1 This deficiency can be addressed by summing surprisal measures over the saccade region (see Figure 1) , and the resulting cumulative n-grams have been shown to be more predictive of reading times than the usual non-cumulative n-grams <cite>(van Schijndel and Schuler, 2015)</cite> . However PCFG surprisal, which has a similar deficiency when non-cumulatively modeling reading times, has not previously been found to be predictive when accumulated over saccade regions. This paper uses a reading time corpus to investigate two accumulation techniques (pre-and post-saccade) and finds that both forms of accumulation improve the fit of n-gram surprisal to reading times.",
  "y": "background"
 },
 {
  "id": "9e8c386b7ea3b5e4e2843fb8382fd8_1",
  "x": "In line with previous findings on the Dundee corpus <cite>(van Schijndel and Schuler, 2015)</cite> , cumulative 5-grams provide a significant improvement over basic n-grams (p < 0.001), but unlike previous work, basic n-grams do not improve over cumulative n-grams on this corpus (p > 0.05). The benefit of cumulative n-grams suggests that the lexical processing of words skipped during a saccade has a time cost similar to directly fixated words. ----------------------------------",
  "y": "similarities differences"
 },
 {
  "id": "9e8c386b7ea3b5e4e2843fb8382fd8_2",
  "x": "Accumulated PCFG surprisal (see Equation 4) did not improve reading time fit (p > 0.05), unlike n-gram surprisal, which replicates a previous result using the Dundee corpus <cite>(van Schijndel and Schuler, 2015)</cite> . In fact, not even basic PCFG surprisal was predictive (p > 0.05) over this baseline model in the UCL corpus, whereas it was predictive over this baseline in the Dundee corpus. Posthoc testing on the exploratory data partition revealed that PCFG surprisal becomes predictive on the UCL corpus when the n-gram predictors are removed from the baseline (p < 0.001), which could indicate that PCFG surprisal may simply help predict reading times when the n-gram model is too weak.",
  "y": "similarities"
 },
 {
  "id": "9e8c386b7ea3b5e4e2843fb8382fd8_3",
  "x": "---------------------------------- **DISCUSSION** This work has confirmed previous findings that cumulative n-grams provide a better model of reading times than the typical non-cumulative reading times <cite>(van Schijndel and Schuler, 2015)</cite> .",
  "y": "similarities"
 },
 {
  "id": "9ea14a9fe422451901ad221bee5714_0",
  "x": "Summarily, we can say that a standard off the shelf neural model relies mostly on its capacity to learn distributional information from the large datasets provided as input during training. In this pretext, we revisit the problem of compound type identification in Sanskrit<cite> (Krishna et al., 2016)</cite> and experiment with various neural architectures for solving the task. The process of compounding and the nature of compositionality of the compounds are well studied in the field of NLP.",
  "y": "background"
 },
 {
  "id": "9ea14a9fe422451901ad221bee5714_1",
  "x": "These include productivity and recursiveness of the words involved in the process, presence of implicit relations between the components, and finally, the analysis of a compound relies on its pragmatic or contextual features (Kim and Baldwin, 2005) . Recently, there has been a concerted effort in studying the nature of compositionality in compounds by leveraging on distributional word-representations or word embeddings and then learning function approximators to predict the nature of compositionality of such words (Mitchell and Lapata, 2010; Cordeiro et al., 2016; Salehi et al., 2015; Jana et al., 2019) . In Sanskrit,<cite> Krishna et al. (2016)</cite> have proposed a framework for semantic type classification of compounds in Sanskrit.",
  "y": "background"
 },
 {
  "id": "9ea14a9fe422451901ad221bee5714_2",
  "x": "Our extensive experiments include standard neural models comprising of Multi-Layer Perceptrons (MLP), Convolution Neural Networks (CNN) (Zhang et al., 2015) and Recurrent models such as Long Short Term Memory (LSTM) configurations. Unlike the feature-rich representation of<cite> Krishna et al. (2016)</cite> , we rely on various word embedding approaches, which include character level, sub-word level, and word-level embedding approaches. Using end-to-end training, the pretrained embeddings are fine tuned for making them task specific embeddings.",
  "y": "differences"
 },
 {
  "id": "9ea14a9fe422451901ad221bee5714_3",
  "x": "Using end-to-end training, the pretrained embeddings are fine tuned for making them task specific embeddings. So all the architectures are integrated with end-to-end training (Kim, 2014 ). The best system of ours, an end-to-end LSTM architecture initialised with fasttext embeddings has shown promising results in terms of F-score (0.73) compared to the state of the art classifier from<cite> Krishna et al. (2016)</cite> (0.74) and outperformed it in terms of accuracy (77.68%).",
  "y": "similarities uses"
 },
 {
  "id": "9ea14a9fe422451901ad221bee5714_4",
  "x": "Here, similar to<cite> Krishna et al. (2016)</cite> , we expect the users to provide a compound in its component-wise segmented form as input to the model. But our model relies on distributed representations or embeddings of the input as features, instead of the linguistically involved feature set proposed in<cite> Krishna et al. (2016)</cite> . Approaches for compound analysis have been of great interest in NLP for multiple languages including English, Italian, Dutch and German (S\u00e9aghdha and Copestake, 2013; Tratz and Hovy, 2010; Kim and Baldwin, 2005; Girju et al., 2005; Verhoeven et al., 2014a) .",
  "y": "similarities uses"
 },
 {
  "id": "9ea14a9fe422451901ad221bee5714_5",
  "x": "We treat the task as a supervised multiclass classification problem. Here, similar to<cite> Krishna et al. (2016)</cite> , we expect the users to provide a compound in its component-wise segmented form as input to the model. But our model relies on distributed representations or embeddings of the input as features, instead of the linguistically involved feature set proposed in<cite> Krishna et al. (2016)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "9ea14a9fe422451901ad221bee5714_6",
  "x": "Approaches for compound analysis have been of great interest in NLP for multiple languages including English, Italian, Dutch and German (S\u00e9aghdha and Copestake, 2013; Tratz and Hovy, 2010; Kim and Baldwin, 2005; Girju et al., 2005; Verhoeven et al., 2014a) . These methods primarily rely on lexical networks, distributional information (S\u00e9aghdha and Copestake, 2013) or a combination of both lexical and distributional information (Nastase et al., 2006) . In Sanskrit,<cite> Krishna et al. (2016)</cite> proposed a similar statistical approach which combined lexical and distributional information by using information from the lexical network Amarakos .",
  "y": "similarities"
 },
 {
  "id": "9ea14a9fe422451901ad221bee5714_7",
  "x": "a, Bahuvr\u012bhi and Dvandva (Kumar et al., 2010) . Similar to prior computational approaches in Sanskrit compounding <cite>(Krishna et al., 2016</cite>; Kumar et al., 2010) , we follow this four class coarse level categorization of the semantic classes in compounds. Compounding in Sanskrit is extremely productive, or rather recursive, resulting in compound words with multiple components (Lowe, 2015) .",
  "y": "similarities uses"
 },
 {
  "id": "9ea14a9fe422451901ad221bee5714_8",
  "x": "We have used the same experimental setting as<cite> Krishna et al. (2016)</cite> for the classification task. a class,<cite> Krishna et al. (2016)</cite> down-sampled it to 4,000, which takes it close to the count of the second most highly populated class Bahuvr\u012bhi.",
  "y": "similarities uses"
 },
 {
  "id": "9ee702243b3976ee4261f433d75528_0",
  "x": "**INTRODUCTION** The word order between source and target languages significantly influences the translation quality in statistical machine translation (SMT) (Tillmann, 2004; Hayashi et al., 2013; <cite>Nakagawa, 2015)</cite> . Models that adjust orders of translated phrases in decoding have been proposed to solve this problem (Tillmann, 2004; Koehn et al., 2005; Nagata et al., 2006) .",
  "y": "background"
 },
 {
  "id": "9ee702243b3976ee4261f433d75528_1",
  "x": "In addition, their computational costs are expensive. To address these problems, preordering (Xia and McCord, 2004; Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012; <cite>Nakagawa, 2015</cite>) and postordering (Goto et al., 2012 (Goto et al., , 2013 Hayashi et al., 2013 ) models have been proposed. Preordering reorders source sentences before translation, while post-ordering reorders sentences translated without considering the word order after translation.",
  "y": "background"
 },
 {
  "id": "9ee702243b3976ee4261f433d75528_2",
  "x": "To address these problems, preordering (Xia and McCord, 2004; Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012; <cite>Nakagawa, 2015</cite>) and postordering (Goto et al., 2012 (Goto et al., , 2013 Hayashi et al., 2013 ) models have been proposed. Preordering reorders source sentences before translation, while post-ordering reorders sentences translated without considering the word order after translation. In particular, preordering effectively improves the translation quality because it solves long-distance reordering and computational complexity issues (Jehl et al., 2014; <cite>Nakagawa, 2015</cite>) .",
  "y": "background"
 },
 {
  "id": "9ee702243b3976ee4261f433d75528_3",
  "x": "Rule-based preordering methods either manually create reordering rules (Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012) or extract reordering rules from a corpus (Xia and McCord, 2004; Genzel, 2010) . On the other hand, studies in (Neubig et al., 2012; Lerner and Petrov, 2013; Hoshino et al., 2015; <cite>Nakagawa, 2015</cite>) apply machine learning to the preordering problem. Hoshino et al. (2015) proposed a method that learns whether child nodes should be swapped at each node of a syntax tree.",
  "y": "background"
 },
 {
  "id": "9ee702243b3976ee4261f433d75528_4",
  "x": "On the other hand, studies in (Neubig et al., 2012; Lerner and Petrov, 2013; Hoshino et al., 2015; <cite>Nakagawa, 2015</cite>) apply machine learning to the preordering problem. Hoshino et al. (2015) proposed a method that learns whether child nodes should be swapped at each node of a syntax tree. Neubig et al. (2012) and <cite>Nakagawa (2015)</cite> proposed methods that construct a binary tree and reordering simultaneously from a source sentence.",
  "y": "background"
 },
 {
  "id": "9ee702243b3976ee4261f433d75528_5",
  "x": "We evaluate the proposed method for English-to-Japanese translations using both phrase-based SMT (PBSMT) and neural MT (NMT). The results confirm that the proposed method achieves comparable translation quality to the state-of-the-art preordering method <cite>(Nakagawa, 2015)</cite> that requires a manual feature design. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "9ee702243b3976ee4261f433d75528_6",
  "x": "We used Stanford CoreNLP 2 for tokenization and POS tagging, Enju 3 for parsing of English, and MeCab 4 for tokenization of Japanese. For word alignment, we used MGIZA. 5 Source-totarget and target-to-source word alignments were calculated using IBM model 1 and hidden Markov model, and they were combined with the intersection heuristic following <cite>(Nakagawa, 2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "9ee702243b3976ee4261f433d75528_8",
  "x": "Compared to the plain PBSMT without preordering, both BLEU and RIBES increased significantly with preordering by RvNN and <cite>BTG</cite>. These scores were comparable (statistically insignificant at p < 0.05) between RvNN and <cite>BTG</cite>, 12 indicating that the proposed method achieves a translation quality comparable to <cite>BTG</cite>. In contrast to the case of PB-SMT, NMT without preordering achieved a significantly higher BLEU score than NMT models with preordering by RvNN and <cite>BTG</cite>.",
  "y": "uses"
 },
 {
  "id": "9ee702243b3976ee4261f433d75528_9",
  "x": "The distortion limit of SMT systems trained using preordered sentences by RvNN and <cite>BTG</cite> was set to 0, while that without preordering was set to 6. Compared to the plain PBSMT without preordering, both BLEU and RIBES increased significantly with preordering by RvNN and <cite>BTG</cite>. These scores were comparable (statistically insignificant at p < 0.05) between RvNN and <cite>BTG</cite>, 12 indicating that the proposed method achieves a translation quality comparable to <cite>BTG</cite>.",
  "y": "similarities"
 },
 {
  "id": "9ee702243b3976ee4261f433d75528_10",
  "x": "**GOLD-STANDARD LABELS FOR PREORDERING** We created training data for preordering by labeling whether each node of the source-side syntax tree has reordered child nodes against a targetside sentence. The label is determined based on Kendall's \u03c4 (Kendall, 1938) as in <cite>(Nakagawa, 2015)</cite> , which is calculated by Equation (1).",
  "y": "uses similarities"
 },
 {
  "id": "9ee702243b3976ee4261f433d75528_14",
  "x": "Furthermore, the ratio of high Kendall's \u03c4 by RvNN is more than that of <cite>BTG</cite>, implying that preordering by RvNN is better than that by <cite>BTG</cite>. We also manually investigated the preordering and translation results. We found that our model improved both.",
  "y": "differences"
 },
 {
  "id": "9ee702243b3976ee4261f433d75528_15",
  "x": "In this paper, we proposed a preordering method without a manual feature design for MT. The experiments confirmed that the proposed method achieved a translation quality comparable to the <cite>state-of-the-art preordering method</cite> that requires a manual feature design. As a future work, we plan to develop a model that jointly parses and preorders a source sentence.",
  "y": "similarities"
 },
 {
  "id": "a0af9cf22996a245af9d66cf1d358f_0",
  "x": "A spike in attention directed toward a particular location may signal an important update, such as the need for aid for the location<cite> (Varga et al. 2013)</cite> . While collective attention is often measured with activity metrics such as post volume (Mitra, Wright, Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). and Gilbert 2016), such metrics often focus on an aggregate quantity summary of attention without considering the nuanced content side of attention dynamics.",
  "y": "motivation"
 },
 {
  "id": "a0af9cf22996a245af9d66cf1d358f_1",
  "x": "Breaking news events, especially crisis events, often attract significant collective attention from the general public (Lin et al. 2014) , resulting in bursts of discussion on social media (Leavitt and Clark 2014; Lehmann et al. 2012 ). During such events, public observers often focus on important locations, people or organizations (hereafter \"named entities\") depending on their relevance to the unfolding crisis (Wakamiya et al. 2015) . A spike in attention directed toward a particular location may signal an important update, such as the need for aid for the location<cite> (Varga et al. 2013)</cite> .",
  "y": "background"
 },
 {
  "id": "a0af9cf22996a245af9d66cf1d358f_2",
  "x": "In the immediate aftermath of Hurricane Maria, many news headlines referred to \"San Juan\" without extra context such as \"the capital of Puerto Rico\", largely because they expected their audience had already become familiar with the city due to the recent crisis. To better understand the nuanced dynamics of collective attention, we take a closer look at how people refer to locations of hurricanes during breaking crisis events via their usage of descriptor context phrases with respect to location mentions. Such descriptor phrases provide additional contextual information for named entities (people, organizations and locations)<cite> (Stali\u016bnait\u0117 et al. 2018)</cite> , helping to locate unfamiliar entities and disambiguate names that could have multiple referents.",
  "y": "background"
 },
 {
  "id": "a0af9cf22996a245af9d66cf1d358f_3",
  "x": "Left y-axis (black solid line) indicates the location's daily log frequency, right y-axis (red dotted line) indicates the location's weekly probability of receiving a descriptor phrase like \"Puerto Rico\". Hurricane Maria in 2017, the location \"San Juan\" was less likely to receive a descriptor (e.g., \"San Juan, PR\") following the peak in collective attention volume. While this shift appears to be due to time<cite> (Stali\u016bnait\u0117 et al. 2018)</cite> , the shift in descriptor use may also stem from non-temporal factors as well, such as an author's expectations of their audience (audience design) and additional information such as links to external news articles, included in the same sentence with the location (a micro-level aspect of the discussion).",
  "y": "background"
 },
 {
  "id": "a0af9cf22996a245af9d66cf1d358f_4",
  "x": "\u2022 RQ2b: How does the use of descriptor context for locations change at an individual author level? To address these research questions, we first analyzed posts written on Facebook in public groups concerning Hurricane Maria relief, and found that location mentions receive descriptors more often when the locations are not local to the group of discussion, suggesting that descriptors may be used to help explain new information to audiences. By looking at public posts written on Twitter concerning natural disasters, we found that the aggregate rate of descriptor phrases decreased following the peaks in these locations' collective attention, supporting prior findings in the change in named entity use<cite> (Stali\u016bnait\u0117 et al. 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "a0af9cf22996a245af9d66cf1d358f_5",
  "x": "Collective attention is an important component in the spread of information (Wu and Huberman 2007) , and it can shift either vary rapidly or gradually in response to particular events such as sports games (Lehmann et al. 2012) , natural disasters<cite> (Varga et al. 2013)</cite> , and political controversy (Garimella et al. 2017) . With the wealth of digital data available to researchers today, studies have often quantified collective attention using the volume of posting and sharing activity in social media sites such as Reddit and Twitter (Leavitt and Clark 2014; Mitra, Wright, and Gilbert 2016) . While these kinds of activity metrics provide an aggregate summary of attention dynamics, they largely obscure the nuanced content of collective attention such as how people refer to such particular events via language and how such referring language evolves over time.",
  "y": "background"
 },
 {
  "id": "a0af9cf22996a245af9d66cf1d358f_6",
  "x": "The dependent clause may describe attributes of the entity that are relevant to a specific topic, such as \"San Juan, epicenter of Hurricane Maria relief effort,\" or attributes that are generally relevant, such as \"San Juan, Puerto Rico.\" From a collective perspective, prior work that examined the use of descriptor phrases in news media found that writers tend to drop such phrases as the entities gradually become more and more familiar (i.e., shared knowledge) among discussion participants over time<cite> (Stali\u016bnait\u0117 et al. 2018)</cite> . In addition to relative time, Siddharthan, Nenkova, and McKeown (2011) found that salience or the importance of the named entity, i.e., whether an entity plays a major role in the story or narrative being told, determines the need for a descriptor phrase, since a perceived salient or important entity is likely to be understood as shared knowledge among discussion participants and therefore unlikely to need a descriptor phrase (Prince 1992). From an individual perspective, Galati and Brennan (2010) suggested that audience matters for writers' choice of using a additional descriptive information, since audiences who are familiar with those entities are less likely to require context to read or participate in such discussions.",
  "y": "background"
 },
 {
  "id": "a0af9cf22996a245af9d66cf1d358f_7",
  "x": "Crisis events such as hurricanes present a useful case study for the development of collective attention, due to the large volume of online participation and large uncertainty among event observers towards the situation during the crisis events<cite> (Varga et al. 2013)</cite> . We chose to study the collective attention changes in public discourse related to hurricanes, due to hurricanes' lasting economic impact, their broad coverage in the news, and their relevance to specific geographic regions. We collected social media data related to five recent devastating hurricanes, and we describe the data collection ( \u00a7 3.1), location detection ( \u00a7 3.2), and descriptor detection ( \u00a7 3.3) for the following datasets:",
  "y": "background"
 },
 {
  "id": "a0af9cf22996a245af9d66cf1d358f_8",
  "x": "To extract sentence structure from text, we used dependency parsing, which decomposes a sentence into a directed acyclic graph connecting words and phrases. Following<cite> Stali\u016bnait\u0117 et al. (2018)</cite> , we used a small set of dependencies to capture the \"MODIFIER\" phrase type in a subclause (adjectival clause, appositional modifier, prepositional modifier, numeric modifier) and another set of dependencies to capture the \"COMPOUND\" type in a super-clause (nominal modifier, compound, appositional modifier). A summary of our phrase patterns to capture descriptor phrases is provided in Table 3 .",
  "y": "uses"
 },
 {
  "id": "a0af9cf22996a245af9d66cf1d358f_9",
  "x": "The intuition is that over the course of crisis events more collective attention to a particular location may result in more awareness of the location among discussion participants, therefore reducing the need for context. In addition to the aforementioned explanatory factors used, we incorporated an additional set of variables to capture this temporal dynamics: relative peak time, i.e. whether the location is mentioned during or after the peak in post volume; and time since start, i.e. days since the beginning of the hurricane. Here, the definition of peak in collective attention is critical, because it determines the point at which an entity is expected to become shared knowledge<cite> (Stali\u016bnait\u0117 et al. 2018)</cite> .",
  "y": "uses background"
 },
 {
  "id": "a0af9cf22996a245af9d66cf1d358f_10",
  "x": "Here, the definition of peak in collective attention is critical, because it determines the point at which an entity is expected to become shared knowledge<cite> (Stali\u016bnait\u0117 et al. 2018)</cite> . Following Mitra, Wright, and Gilbert (2016), we defined the time of peak collective attentiont i for each location i as the (24-hour) period during which it is mentioned the most frequently: is the raw frequency of location i at time t (see Figure 1 for peak in \"San Juan\" posts).",
  "y": "background"
 },
 {
  "id": "a0af9cf22996a245af9d66cf1d358f_11",
  "x": "To summarize, we found consistently less descriptor use over the course of crisis events even after controlling for other explanatory factors, supporting prior work in long-term descriptor phrase change <cite>(Stali\u016bnait\u0117 et al. 2018</cite> ). ---------------------------------- **INDIVIDUAL CHANGE IN DESCRIPTOR CONTEXT USE**",
  "y": "similarities"
 },
 {
  "id": "a0af9cf22996a245af9d66cf1d358f_12",
  "x": "In addition, we focused on only a set of specific crisis events due to their representative usages of location mentions and large volume of online discussions. Future work can build upon our work and generalize it to other different types of crisis events. In addition, we are unable to rule out the possibility that another event attracted attention to the locations under discussion before the crises began (e.g. a political news story relevant to the event's region) Lastly, the study focuses exclusively on location names because of their geographic relevance to events, but other types of named entities (people, organizations) are also likely to undergo changes in descriptor use in response to increased attention<cite> (Stali\u016bnait\u0117 et al. 2018)</cite> .",
  "y": "future_work"
 },
 {
  "id": "a0af9cf22996a245af9d66cf1d358f_13",
  "x": "Future work can build upon our work and generalize it to other different types of crisis events. In addition, we are unable to rule out the possibility that another event attracted attention to the locations under discussion before the crises began (e.g. a political news story relevant to the event's region) Lastly, the study focuses exclusively on location names because of their geographic relevance to events, but other types of named entities (people, organizations) are also likely to undergo changes in descriptor use in response to increased attention<cite> (Stali\u016bnait\u0117 et al. 2018)</cite> . To conclude, this study adds a new content-based perspective to the measurement of collective attention, by analyzing how people discuss breaking news events.",
  "y": "future_work background motivation"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_0",
  "x": "A ubiquitous task in processing electronic medical data is the assignment of standardized codes representing diagnoses and/or procedures to free-text documents such as medical reports. This is a difficult natural language processing task that requires parsing long, heterogeneous documents and selecting a set of appropriate codes from tens of thousands of possibilities-many of which have very few positive training samples. We present a deep learning system that advances the state of the art for the MIMIC-III dataset, achieving a new best micro F1-measure of 55.85%, significantly outperforming the previous best result<cite> (Mullenbach et al., 2018)</cite> .",
  "y": "differences"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_1",
  "x": "We present a deep learning system that advances the state of the art for the MIMIC-III dataset, achieving a new best micro F1-measure of 55.85%, significantly outperforming the previous best result<cite> (Mullenbach et al., 2018)</cite> . We achieve this through a number of enhancements, including two major novel contributions: multiview convolutional channels, which effectively learn to adjust kernel sizes throughout the input; and attention regularization, mediated by natural-language code descriptions, which helps overcome sparsity for thousands of uncommon codes. These and other modifications are selected to address difficulties inherent to both automated coding specifically and deep learning generally.",
  "y": "extends differences"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_2",
  "x": "<cite>Mullenbach et al. (2018)</cite> presented a model capable of predicting full codes for both ICD-9 diagnoses and procedures composed of shared embedding and CNN layers between all codes and an individual attention layer for each code. They also proposed adding regularization to this model using code descriptions. Their best model on MIMIC-III reached a micro F1-score of 53.9%, which was achieved by their base model without regularization.",
  "y": "background"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_3",
  "x": "They applied this model to the task of predicting only the most frequent 50 codes in MIMIC-III, which they accomplished with a micro F1-score of 61.9%. Our approach is most similar to the current state-of-the-art model by <cite>Mullenbach et al. (2018)</cite> . As in their study, we use a CNN layer with attention modules by code and approach regularization using code descriptions.",
  "y": "uses similarities"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_4",
  "x": "They applied this model to the task of predicting only the most frequent 50 codes in MIMIC-III, which they accomplished with a micro F1-score of 61.9%. Our approach is most similar to the current state-of-the-art model by <cite>Mullenbach et al. (2018)</cite> . As in their study, we use a CNN layer with attention modules by code and approach regularization using code descriptions.",
  "y": "similarities"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_5",
  "x": "Our approach is most similar to the current state-of-the-art model by <cite>Mullenbach et al. (2018)</cite> . Our model has notable departures from theirs, however: firstly, we use multi-view CNN channels with max pooling across the channels, which in itself leads to improvements over their model (even before attention regularization).",
  "y": "extends differences"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_6",
  "x": "Also, some of the hospital stays do not have discharge summaries; following previous studies for automated coding, we only consider those that do (Perotte et al., 2013; Baumel et al., 2018;<cite> Mullenbach et al., 2018</cite>; Wang et al., 2018) . We have three sets for our experiments: one including only the discharge summaries, which allows us to compare our results with previous studies on this corpus<cite> (Mullenbach et al., 2018</cite>; Perotte et al., 2013) , hereon the Dis set; one on the concatenation of all patient notes, hereon, Full set; and one on another set which includes only discharge summary samples with the 50 most frequent codes, hereon, Dis-50 set, for comparison to previous studies<cite> (Mullenbach et al., 2018</cite>; Wang et al., 2018) . The dataset includes 8,929 unique ICD codes (2,011 procedures, 6,918 diagnoses) for the patients who have discharge summaries.",
  "y": "similarities"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_7",
  "x": "The number of notes varies between different hospital stays. Also, some of the hospital stays do not have discharge summaries; following previous studies for automated coding, we only consider those that do (Perotte et al., 2013; Baumel et al., 2018;<cite> Mullenbach et al., 2018</cite>; Wang et al., 2018) . We have three sets for our experiments: one including only the discharge summaries, which allows us to compare our results with previous studies on this corpus<cite> (Mullenbach et al., 2018</cite>; Perotte et al., 2013) , hereon the Dis set; one on the concatenation of all patient notes, hereon, Full set; and one on another set which includes only discharge summary samples with the 50 most frequent codes, hereon, Dis-50 set, for comparison to previous studies<cite> (Mullenbach et al., 2018</cite>; Wang et al., 2018) .",
  "y": "similarities"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_8",
  "x": "The dataset includes 8,929 unique ICD codes (2,011 procedures, 6,918 diagnoses) for the patients who have discharge summaries. 3 We follow the train, test, and development splits publicly shared by the recent study on this dataset<cite> (Mullenbach et al., 2018)</cite> . These splits are patient independent.",
  "y": "uses"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_9",
  "x": "---------------------------------- **BASELINES** We compare our approach with four baselines: flat and hierarchical SVMs (Perotte et al., 2013), LEAM (Wang et al., 2018) , and CAML<cite> (Mullenbach et al., 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_10",
  "x": "CAML<cite> (Mullenbach et al., 2018)</cite> has achieved the best state-of-the-art results on MIMIC-III. CAML is composed of a stack of an embedding layer, a CNN layer, and label-dependent attention layers. We run their model on the Dis set using their publicly available code.",
  "y": "background"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_11",
  "x": "CAML<cite> (Mullenbach et al., 2018)</cite> has achieved the best state-of-the-art results on MIMIC-III. We run their model on the Dis set using their publicly available code.",
  "y": "uses"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_12",
  "x": "---------------------------------- **EVALUATION METRICS** The most widely used metric for evaluating ICD code prediction is micro F1-score (Perotte et al., 2013; Wang et al., 2018;<cite> Mullenbach et al., 2018</cite>; Kavuluru et al., 2015) .",
  "y": "background"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_13",
  "x": "New studies have reported results on macro F1-score, precision@n, and AUC of ROC as well (Wang et al., 2018;<cite> Mullenbach et al., 2018)</cite> . As evaluation metric we rely on micro F1-score (micro F1), macro F1-score (macro F1) for the top 50 codes, area under the precisionrecall curve (PR AUC) and precision@n (P@8 when evaluating the models on all codes and P@5 when evaluating the models for the top 50 codes). For obtaining the micro F1, the micro precision and recall are calculated by collapsing all classes into one class and considering the task as a single binary classification.",
  "y": "background"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_14",
  "x": "---------------------------------- **EVALUATION RESULTS** We evaluate the baseline models on the Dis set and Dis-50 sets and provide micro F1 scores for diagnosis and procedure codes for comparability with previous studies<cite> (Mullenbach et al., 2018</cite>; Perotte et al., 2013) .",
  "y": "similarities"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_15",
  "x": "Table 3 provides the evaluation of our models (MVC-LDA and MVC-RLDA) and the four baselines. 6 When the models are trained and tested on Dis-50 set, our models outperform the previous studies in terms of micro and macro F1, P@5, and PR AUC. This is due to the architecture differences, such as the use of multi-view CNN and 6 The authors of CAML<cite> (Mullenbach et al., 2018)</cite> reported micro F1-Proc=60.9%, micro F1-Diag=52.4%, micro F1=53.9%, P@8=70.9% and P@15=56.1% on the Dis set.",
  "y": "differences"
 },
 {
  "id": "a0bd41c3653073dd79e19d3ddc8d14_16",
  "x": "Other modifications may yield further improvements. For instance, we trained the model on all ground-truth codes equally, similarly to previous approaches (Baumel et al., 2018; Wang et al., 2018;<cite> Mullenbach et al., 2018</cite>; Perotte et al., 2013) . However, medical codes are ordered according to their importance.",
  "y": "similarities"
 },
 {
  "id": "a0c0076fa8c3be914d93ec1d66d0c1_0",
  "x": "More recently, <cite>Kisselew et al. (2015)</cite> put the task of modeling derivation into the perspective of zero-shot-learning: instead of using cosine similarities they predicted the derived term by learning a mapping function between the base term and the derived term. Once the predicted vector was computed, a nearest neighbor search was applied to validate if the prediction corresponded to the derived term. In zero-shotlearning the task is to predict novel values, i.e., values that were never seen in training.",
  "y": "background"
 },
 {
  "id": "a0c0076fa8c3be914d93ec1d66d0c1_1",
  "x": "It is often applied across vector spaces, such as different domains (Mikolov et al., 2013; . The experiments by <cite>Kisselew et al. (2015)</cite> were performed over six derivational patterns for German (cf. Table 1), including particle verbs (PVs) with two different particle prefixes (an and durch), which were particularly difficult to predict.",
  "y": "background"
 },
 {
  "id": "a0c0076fa8c3be914d93ec1d66d0c1_2",
  "x": "---------------------------------- **PREDICTION EXPERIMENTS** As in <cite>Kisselew et al. (2015)</cite> , we treat every derivation type as a specific learning problem: we take a set of word pairs with a particular derivation pattern (e.g., \"-in\", B\u00e4cker::B\u00e4ckerin), and divide this set into training and test pairs by performing 10-fold cross-validation.",
  "y": "similarities uses"
 },
 {
  "id": "a0c0076fa8c3be914d93ec1d66d0c1_3",
  "x": "---------------------------------- **DERIVATION DATASETS** We created a new collection of German particle verb derivations 1 relying on the same resource as <cite>Kisselew et al. (2015)</cite> , the semiautomatic derivational lexicon for German DErivBase (Zeller et al., 2013) .",
  "y": "similarities"
 },
 {
  "id": "a0c0076fa8c3be914d93ec1d66d0c1_4",
  "x": "In total, our collection contains 1 410 BV-PV combinations across seven particles, cf. Table 2 . In addition, we apply our models to two existing collections for derivational patterns, the German dataset from <cite>Kisselew et al. (2015)</cite> , comprising six derivational patterns with 80 in-stances each (cf.",
  "y": "uses"
 },
 {
  "id": "a0c0076fa8c3be914d93ec1d66d0c1_5",
  "x": "We thus apply a more informed baseline, the same as in <cite>Kisselew et al. (2015)</cite> , and predict the derived term at exactly the same position as the base term. ---------------------------------- **ADDITIVE METHOD (AVGADD)**",
  "y": "similarities uses"
 },
 {
  "id": "a0c0076fa8c3be914d93ec1d66d0c1_6",
  "x": "---------------------------------- **ADDITIVE METHOD (AVGADD)** AvgAdd is a re-implementation of the best method in <cite>Kisselew et al. (2015)</cite> :",
  "y": "background"
 },
 {
  "id": "a26260547114750ba2aa49e5f96136_0",
  "x": "Here we proposed GenNet model, a neural network-based model. In this model first we will generate the answer of the question from the passage and then will matched the generated answer with given answer, the best matched option will be our answer. For answer generation we used S-net <cite>(Tan et al., 2017)</cite> model trained on SQuAD and To evaluate our model we used Large-scale RACE (ReAding Comprehension Dataset From Examinations) (Lai et al., 2017).",
  "y": "uses"
 },
 {
  "id": "a26260547114750ba2aa49e5f96136_1",
  "x": "we first compute a question-aware representation of the passage (which essentially tries to retain portions of the passage which are only relevant to the question). Then we use answer generation using state-of-art S-Net model <cite>(Tan et al., 2017)</cite> which extract and generate answer figure 2. After we have answer generated from the passage now we weight every given candidate option and select the best matched option.",
  "y": "uses"
 },
 {
  "id": "a26260547114750ba2aa49e5f96136_2",
  "x": "First is Answer extraction and Answer Synthesis/Generation and then option selection. Answer extraction and Generation will be done using state-of-art S-NET model <cite>(Tan et al., 2017)</cite> . S-Net first pull out evidence snippets by matching the question and passage respectively, and then generates the answer by filtering the question, passage, and evidence snippets. consider a passage P = [p 1 , p 2 , p 3 , ...p p ] of word length P, Question Q = [Q 1 , Q 2 , Q 3 , ...Q q ] of word length Q, and n options Z n = [z 1 , z 2 , z 3 , ...z k ] where n > 1 and word length k. We first convert the words to their word-level embedding and character-level embedding using GLOVE (Pennington et al., 2014) .The encoding and embedding layers take in a series of tokens and represent it as a series of vectors.",
  "y": "uses"
 },
 {
  "id": "a26260547114750ba2aa49e5f96136_3",
  "x": "On top of the encoder, S-Net uses GRU with attention as the decoder to produce the answer. At each decoding time step t , the GRU reads the previous word embedding w t\u22121 and previous context vector c t\u22121 and finally produced answer. Figure 4 : Answer Synthesis/Generation Model <cite>(Tan et al., 2017)</cite> The produced answer will be stored in Answer vector.",
  "y": "uses"
 },
 {
  "id": "a2b945e18ab6b73b4021a2db8bda4f_0",
  "x": "We use the argumentative structure proposed by Stab and Gurevych (2014a) : argument components (major claims, claims, premises) and argument relations (support, attack). Figure 1 (i) shows an extract from an essay written in response to the above prompt, labeled with a claim and two premises. The advantage of having a simple annotation scheme is two-fold: it allows for more reliable human annotations and it enables better performance for argumentation mining systems designed to automatically identify the argumentative structure <cite>(Stab and Gurevych, 2014b)</cite> .",
  "y": "background"
 },
 {
  "id": "a2b945e18ab6b73b4021a2db8bda4f_1",
  "x": "We discuss what features are correlated with high scoring es- Figure 1 : (i) Essay extract showing a claim and two premises and (ii) the corresponding argumentative structure (i.e., chain). says vs. low scoring essays. Second, we show that the argumentation features extracted based on argumentative structures automatically predicted by a state-of-the-art argumentation mining system <cite>(Stab and Gurevych, 2014b)</cite> are also good predictors of essays scores (Section 4).",
  "y": "uses"
 },
 {
  "id": "a2b945e18ab6b73b4021a2db8bda4f_2",
  "x": "In addition, we notice that attack relations are sparse, as was the case in <cite>Stab and Gurevych (2014b)</cite> dataset and thus the coefficients for attack relations features (#10, #11 in Table 1 ) are negligible. In summary, our findings contribute to research on essay scoring, showing that argumentation features are good predictors of essay scores, besides spelling, grammar, and stylistic properties of text. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "a2b945e18ab6b73b4021a2db8bda4f_3",
  "x": "We use the approach proposed by <cite>Stab and Gurevych (2014b)</cite> . 2 For argument component identification, we categorize clauses to one of the four classes (major claim (M C), claim (C), premise (P ), and N one). For argument relation identification, given a pair of argument clauses Arg 1 and Arg 2 the classifier decides whether the pair holds a support (S) or non-support (N S) relation (binary classification).",
  "y": "uses"
 },
 {
  "id": "a2b945e18ab6b73b4021a2db8bda4f_4",
  "x": "We use two settings for the classification experiments using libSVM (Chang and Lin, 2011) for both argument component and relation identification. In the first setting, we used the dataset of 90 high quality persuasive essays from <cite>(Stab and Gurevych, 2014b )</cite> (<cite>S&G</cite>) as training and use T OEF L arg for testing (out-of-domain setting). In the second setting (in-domain), we randomly split the T OEF L arg into 80% training and 20% for testing (sampled equally from each category (M C, C, P , and N one for argument components; S and N S for relations)).",
  "y": "uses"
 },
 {
  "id": "a2b945e18ab6b73b4021a2db8bda4f_5",
  "x": "Table 3 and 4 present the classification results for identifying ar-2 In future work, we plan to use the authors' improved approach and larger dataset released after the acceptance of this paper (Stab and Gurevych, 2016 Table 4 : F1 for argument components (in-domain setting) gument components in the first and second setting, respectively. We ran experiments for all different features groups and observe that with the exception of the P class, the F1 scores for all the other classes is comparable to the results reported by <cite>Stab and Gurevych (2014b)</cite> . One explanation of having lower performance on the P (premise) category is that the <cite>S&G</cite> dataset used for training has higher quality essays, while 2/3 of our T OEF L arg dataset consists of medium and low scoring essays (the writing style for providing reasons or example can differ between high and low scoring essays).",
  "y": "similarities"
 },
 {
  "id": "a2b945e18ab6b73b4021a2db8bda4f_6",
  "x": "Table 3 and 4 present the classification results for identifying ar-2 In future work, we plan to use the authors' improved approach and larger dataset released after the acceptance of this paper (Stab and Gurevych, 2016 Table 4 : F1 for argument components (in-domain setting) gument components in the first and second setting, respectively. We ran experiments for all different features groups and observe that with the exception of the P class, the F1 scores for all the other classes is comparable to the results reported by <cite>Stab and Gurevych (2014b)</cite> . One explanation of having lower performance on the P (premise) category is that the <cite>S&G</cite> dataset used for training has higher quality essays, while 2/3 of our T OEF L arg dataset consists of medium and low scoring essays (the writing style for providing reasons or example can differ between high and low scoring essays).",
  "y": "differences"
 },
 {
  "id": "a2b945e18ab6b73b4021a2db8bda4f_7",
  "x": "The F1 score of identifying support relations is 84.3% (or 89% using top100), much higher than reported by <cite>Stab and Gurevych (2014b)</cite> . We obtain similar results when training and testing on T OEF L arg . We observe that two specific feature groups, Structural and Lexical, individually achieve high F1 scores and when combined with other features, they assist the classifier in reaching F1 scores in high 80s%.",
  "y": "differences"
 },
 {
  "id": "a2b945e18ab6b73b4021a2db8bda4f_8",
  "x": "We observe that two specific feature groups, Structural and Lexical, individually achieve high F1 scores and when combined with other features, they assist the classifier in reaching F1 scores in high 80s%. There can be two explanations for this: 1) essays in T OEF L arg have multiple short paragraphs where the position features such as position of the arguments in the essay and paragraph (Structural group) are strong indicators for argument relations; and 2) due to short paragraphs, the percentage of N S instances are less than in the <cite>S&G</cite> dataset, hence the Lexical features (i.e., word-pairs between Arg 1 and Arg 2 ) perform very well. Table 6 : Correlation of LR (10 fold CV) with predicted results.",
  "y": "differences"
 },
 {
  "id": "a2b945e18ab6b73b4021a2db8bda4f_9",
  "x": "Based on the automatic identification of the argument components and relations, we generate the argumentation features to see whether they still predict essays scores that are highly correlated with human scores. Since our goal is to compare with the manual annotation setup, we use the first setting, where we train on the <cite>S&G</cite> dataset and test on our T OEF L arg dataset. We select the best system setup (top100 for both tasks; Table 3 and 5).",
  "y": "uses"
 },
 {
  "id": "a301586ed006905275ab42c5e40d88_0",
  "x": "Following<cite> Seo et al. (2019)</cite> , we concatenate both sparse and dense vectors to encode every phrase in Wikipedia and use maximum similarity search to find the closest candidate phrase to answer each question. We only substitute (or augment) the baseline sparse encoding which is entirely based on frequency-based embedding (tf-idf) with our contextualized sparse representation (COSPR). Our empirical results demonstrate its state-of-the-art performance in open-domain QA datasets, SQUAD OPEN and CURATEDTREC.",
  "y": "uses"
 },
 {
  "id": "a301586ed006905275ab42c5e40d88_1",
  "x": "Our empirical results demonstrate its state-of-the-art performance in open-domain QA datasets, SQUAD OPEN and CURATEDTREC. Notably, our method significantly outperforms DENSPI <cite>(Seo et al., 2019)</cite> , the previous end-toend QA model, by more than 4% with negligible drop in inference speed. Moreover, our method achieves up to 2% better accuracy and x97 speedup in inference compared to pipeline (retrievalbased) approaches.",
  "y": "differences"
 },
 {
  "id": "a301586ed006905275ab42c5e40d88_2",
  "x": "To improve the performance of the open-domain question answering, various modifications have been studied to the pipelined models which include improving the retriever-reader interaction (Wang et al., 2018a; Das et al., 2019) , re-ranking paragraphs and/or answers (Wang et al., 2018b; Lin et al., 2018; Lee et al., 2018; Kratzwald et al., 2019) , learning end-to-end models with weak supervision , or simply making a better retriever and a reader model (Yang et al., 2019; Wang et al., 2019) . Due to the pipeline nature, however, these models inevitably suffer error propagation from the retrievers. To mitigate this problem,<cite> Seo et al. (2019)</cite> propose to learn query-agnostic representations of phrases in Wikipedia and retrieve phrases that best answers a question.",
  "y": "background"
 },
 {
  "id": "a301586ed006905275ab42c5e40d88_3",
  "x": "Due to the pipeline nature, however, these models inevitably suffer error propagation from the retrievers. To mitigate this problem,<cite> Seo et al. (2019)</cite> propose to learn query-agnostic representations of phrases in Wikipedia and retrieve phrases that best answers a question. While<cite> Seo et al. (2019)</cite> have shown that encoding both dense and sparse representations for each phrase could keep the lexically important words of a phrase to some extent, their sparse representations are based on static tf-idf vectors which have globally the same weight for each n-gram.",
  "y": "motivation"
 },
 {
  "id": "a301586ed006905275ab42c5e40d88_4",
  "x": "Refer to<cite> Seo et al. (2019)</cite> for details; we mostly reuse its architecture. ---------------------------------- **SPARSE ENCODING OF PHRASES**",
  "y": "background"
 },
 {
  "id": "a301586ed006905275ab42c5e40d88_5",
  "x": "As training phrase encoders on the whole Wikipedia is computationally prohibitive, we use training examples from an extractive question answering dataset (SQuAD) to train our encoders. Given a pair of question q and a golden document x (a paragraph in the case of SQuAD), we first compute the dense logit of each phrase x i:j by l i,j = h i:j \u00b7 h . Unlike<cite> Seo et al. (2019)</cite> , each phrase's sparse embedding is also trained, so it needs to be considered in the loss function.",
  "y": "differences"
 },
 {
  "id": "a301586ed006905275ab42c5e40d88_6",
  "x": "where i * , j * denote the true start and end positions of the answer phrase. While the loss above is an unbiased estimator, in practice, we adopt early loss summation as suggested by<cite> Seo et al. (2019)</cite> for larger gradient signals in early training. Additionally, we also add dense-only loss that omits the sparse logits (i.e. original loss in<cite> Seo et al. (2019)</cite> ) to the final loss, in which case we find that we obtain higher-quality dense phrase representations.",
  "y": "uses"
 },
 {
  "id": "a301586ed006905275ab42c5e40d88_7",
  "x": "While the loss above is an unbiased estimator, in practice, we adopt early loss summation as suggested by<cite> Seo et al. (2019)</cite> for larger gradient signals in early training. Additionally, we also add dense-only loss that omits the sparse logits (i.e. original loss in<cite> Seo et al. (2019)</cite> ) to the final loss, in which case we find that we obtain higher-quality dense phrase representations. Negative Sampling We train our model on SQuAD v1.1 which always has a positive paragraph that contains the answer.",
  "y": "uses"
 },
 {
  "id": "a301586ed006905275ab42c5e40d88_8",
  "x": "To each paragraph x, we concatenate the paragraph x neg which was paired with the question whose dense representation h neg is most similar to the original dense question representation h , following<cite> Seo et al. (2019)</cite> . Note the difference, however, that we concatenate the negative example instead of considering it as an independent example with noanswer option Levy et al. (2017) . During training, we find that adding tf-idf matching scores on the word-level logits of the negative paragraphs further improves the quality of sparse representations as our sparse models have to give stronger attentions to positively related words in this biased setting.",
  "y": "uses"
 },
 {
  "id": "a301586ed006905275ab42c5e40d88_9",
  "x": "Each phrase representation has 2d se + 2F + 1 dimensions. We use the same storage reduction and search techniques by<cite> Seo et al. (2019)</cite> . For storage, the total size of the index is 1.3 TB including unigram and bigram sparse representations.",
  "y": "uses"
 },
 {
  "id": "a301586ed006905275ab42c5e40d88_10",
  "x": "---------------------------------- **RESULTS** We evaluate the effectiveness of COSPR by augmenting DENSPI <cite>(Seo et al., 2019)</cite> with contextualized sparse representations (DENSPI+COSPR).",
  "y": "extends"
 },
 {
  "id": "a301586ed006905275ab42c5e40d88_12",
  "x": "Our sparse question representation also learns meaningful n-gram weights compared to tf-idf vectors. Table 4 shows the outputs of three OpenQA models: DrQA (Chen et al., 2017) , DENSPI <cite>(Seo et al., 2019)</cite> , and our DENSPI+COSPR. DENSPI+COSPR is able to retrieve various correct answers from different documents, and it often correctly answers questions with specific dates or numbers compared to DENSPI showing the effectiveness of learned sparse representations.",
  "y": "uses"
 },
 {
  "id": "a3dbc3362016cdcfc0c4da429b98cc_0",
  "x": "---------------------------------- **GOLD** Figure 1: Generated example on ROTOWIRE by using Conditional Copy (CC) as baseline <cite>(Wiseman et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "a3dbc3362016cdcfc0c4da429b98cc_1",
  "x": [
   "Wiseman et al. (2017) explored two types of copy mechanism and found conditional copy model (Gulcehre et al., 2016) perform better . Puduppully et al. (2019) enhanced content selection ability by explicitly selecting and planning relevant records. Li and Wan (2018) improved the precision of describing data records in the generated texts by generating a template at first and filling in slots via copy mechanism."
  ],
  "y": "background"
 },
 {
  "id": "a3dbc3362016cdcfc0c4da429b98cc_2",
  "x": "The word embeddings for each type of information are trainable and randomly initialized before training following<cite> Wiseman et al. (2017)</cite> . [; ] denotes the vector concatenation. Then, we use a LSTM decoder with attention and conditional copy to model the conditional probability P (y t |y <t , S).",
  "y": "similarities background"
 },
 {
  "id": "a3dbc3362016cdcfc0c4da429b98cc_3",
  "x": "We conducted experiments on ROTOWIRE <cite>(Wiseman et al., 2017)</cite> . For each example, it provides three tables as described in Section 2.1 which consists of 628 records in total with a long game summary. The average length of game summary is 337.1.",
  "y": "uses"
 },
 {
  "id": "a3dbc3362016cdcfc0c4da429b98cc_4",
  "x": "The average length of game summary is 337.1. In this paper, we followed the data split introduced in<cite> Wiseman et al. (2017)</cite> ----------------------------------",
  "y": "uses"
 },
 {
  "id": "a3dbc3362016cdcfc0c4da429b98cc_5",
  "x": "Code of our model can be found at https://github.com/ernestgong/data2text-three-dimensions/. Table 1 displays the automatic evaluation results on both development and test set. We chose Conditional Copy (CC) model as our baseline, which is the best model in<cite> Wiseman et al. (2017)</cite> . We included reported scores with updated IE model by Puduppully et al. (2019) and our implementation's result on CC in this paper.",
  "y": "uses background"
 },
 {
  "id": "a3dbc3362016cdcfc0c4da429b98cc_6",
  "x": "The self-attention (SA) cell we used, as described in Section 3, achieved better overall performance in terms of F1% of CS, CO and BLEU among the hierarchical encoders. Also we implemented a template system same as the one used in<cite> Wiseman et al. (2017)</cite> which outputted eight sentences: an introductory sentence (two teams' points and who win), six top players' statistics (ranked by their points) and a conclusion sentence. We refer the readers to<cite> Wiseman et al. (2017)</cite> 's paper for more detailed information on templates.",
  "y": "similarities uses"
 },
 {
  "id": "a3dbc3362016cdcfc0c4da429b98cc_7",
  "x": "The self-attention (SA) cell we used, as described in Section 3, achieved better overall performance in terms of F1% of CS, CO and BLEU among the hierarchical encoders. Also we implemented a template system same as the one used in<cite> Wiseman et al. (2017)</cite> which outputted eight sentences: an introductory sentence (two teams' points and who win), six top players' statistics (ranked by their points) and a conclusion sentence. We refer the readers to<cite> Wiseman et al. (2017)</cite> 's paper for more detailed information on templates.",
  "y": "background"
 },
 {
  "id": "a3dbc3362016cdcfc0c4da429b98cc_9",
  "x": "Row, column and time dimension information are important to the modeling of tables because subtracting any of them will result in performance Table 2 : Automatic evaluation results on test set. Results were obtained using<cite> Wiseman et al. (2017)</cite> 's trained extractive evaluation models with relexicalization (Li and Wan, 2018) . * We include delayed copy (DEL)'s result in the paper (Li and Wan, 2018) for comparison.",
  "y": "uses background"
 },
 {
  "id": "a3dbc3362016cdcfc0c4da429b98cc_10",
  "x": "In addition, we compare our model with delayed copy model (DEL) (Li and Wan, 2018) along with gold text, template system (TEM), conditional copy (CC) <cite>(Wiseman et al., 2017)</cite> and NCP+CC (NCP) (Puduppully et al., 2019) . Li and Wan (2018) 's model generate a template at first and then fill in the slots with delayed copy mechanism. Since its result in Li and Wan (2018) 's paper was evaluated by IE model trained by<cite> Wiseman et al. (2017)</cite> and \"relexicalization\" by Li and Wan (2018) , we adopted the corresponding IE model and re-implement \"relexicalization\" as suggested by Li and Wan (2018) for fair comparison.",
  "y": "background"
 },
 {
  "id": "a3dbc3362016cdcfc0c4da429b98cc_11",
  "x": "In addition, we compare our model with delayed copy model (DEL) (Li and Wan, 2018) along with gold text, template system (TEM), conditional copy (CC) <cite>(Wiseman et al., 2017)</cite> and NCP+CC (NCP) (Puduppully et al., 2019) . Li and Wan (2018) 's model generate a template at first and then fill in the slots with delayed copy mechanism. Since its result in Li and Wan (2018) 's paper was evaluated by IE model trained by<cite> Wiseman et al. (2017)</cite> and \"relexicalization\" by Li and Wan (2018) , we adopted the corresponding IE model and re-implement \"relexicalization\" as suggested by Li and Wan (2018) for fair comparison.",
  "y": "uses background"
 },
 {
  "id": "a3dbc3362016cdcfc0c4da429b98cc_12",
  "x": "We compared our full model with gold texts, template-based system, CC <cite>(Wiseman et al., 2017)</cite> and NCP+CC (NCP) (Puduppully et al., 2019) . We randomly sampled 30 examples from test set. Then, we randomly sampled 4 sentences from each model's output for each example. We provided the raters of those sampled sentences with the corresponding NBA game statistics. They were asked to count the number of supporting and contradicting facts in each sentence. Each sentence is rated independently. We report the average number of supporting facts (#Sup) and contradicting facts (#Cont) in Table 3 . Unsurprisingly, template-based system includes most supporting facts and least contradicting facts in its texts because the template consists of a large number of facts and all of those facts are extracted from the table. Also, our model produces less contradicting facts than other two neural models.",
  "y": "differences background"
 },
 {
  "id": "a3dbc3362016cdcfc0c4da429b98cc_13",
  "x": [
   "Bao et al. (2018) develops a table-aware encoder-decoder model. Wiseman et al. (2017) introduced a document-scale data-totext dataset, consisting of long text with more redundant records, which requires the model to select important information to generate. We describe recent works in Section 1."
  ],
  "y": "background"
 },
 {
  "id": "a557739447131395f7a76d87a4cd19_0",
  "x": "**INTRODUCTION** Stance detection (also called stance identification or stance classification) is one of the considerably recent research topics in natural language processing (NLP). It is usually defined as a classification problem where for a text and target pair, the stance of the author of the text for that target is expected as a classification output from the set: {Favor, Against, Neither} <cite>[12]</cite> .",
  "y": "background"
 },
 {
  "id": "a557739447131395f7a76d87a4cd19_1",
  "x": "Nevertheless, in sentiment analysis, the sentiment of the author of a piece of text usually as Positive, Negative, and Neutral is explored while in stance detection, the stance of the author of the text for a particular target (an entity, event, etc.) either explicitly or implicitly referred to in the text is considered. Like sentiment analysis, stance detection systems can be valuable components of information retrieval and other text analysis systems <cite>[12]</cite> . Previous work on stance detection include [16] where a stance classifier based on sentiment and arguing features is proposed in addition to an arguing lexicon automatically compiled.",
  "y": "background"
 },
 {
  "id": "a557739447131395f7a76d87a4cd19_2",
  "x": "Among more recent related work, in [1] stance detection for unseen targets is studied and bidirectional conditional encoding is employed. The authors state that their approach achieves state-ofthe art performance rates [1] on SemEval 2016 Twitter Stance Detection corpus <cite>[12]</cite> . In [3] , a stance-community detection approach called SCIFNET is proposed.",
  "y": "background"
 },
 {
  "id": "a557739447131395f7a76d87a4cd19_3",
  "x": "Lastly, in <cite>[12]</cite> , Se-mEval 2016's aforementioned shared task on Twitter Stance Detection is described. Also provided are the results of the evaluations of 19 systems participating in two subtasks (one with training data set provided and the other without an annotated data set) of the shared task <cite>[12]</cite> . In this paper, we present a tweet data set in Turkish annotated with stance information, where the corresponding annotations are made publicly available.",
  "y": "background"
 },
 {
  "id": "a557739447131395f7a76d87a4cd19_4",
  "x": "Lastly, in <cite>[12]</cite> , Se-mEval 2016's aforementioned shared task on Twitter Stance Detection is described. Also provided are the results of the evaluations of 19 systems participating in two subtasks (one with training data set provided and the other without an annotated data set) of the shared task <cite>[12]</cite> . In this paper, we present a tweet data set in Turkish annotated with stance information, where the corresponding annotations are made publicly available.",
  "y": "background"
 },
 {
  "id": "a557739447131395f7a76d87a4cd19_5",
  "x": "**STANCE DETECTION EXPERIMENTS USING SVM CLASSIFIERS** It is emphasized in the related literature that unigram-based methods are reliable for the stance detection task [16] and similarly unigram-based models have been used as baseline models in studies such as <cite>[12]</cite> . In order to be used as a baseline and reference system for further studies on stance detection in Turkish tweets, we have trained two SVM classifiers (one for each target) using unigrams as features.",
  "y": "background"
 },
 {
  "id": "a557739447131395f7a76d87a4cd19_6",
  "x": "The 10-fold cross-validation results of the two classifiers are provided in Table 1 using the metrics of precision, recall, and F-Measure. The evaluation results are quite favorable for both targets and particularly higher for Target-1, considering the fact that they are the initial experiments on the data set. Yet, completely the opposite pattern is observed in stance detection results of baseline systems given in <cite>[12]</cite> , i.e., better F-Measure rates have been obtained for the Against class when compared with the Favor class <cite>[12]</cite> .",
  "y": "differences"
 },
 {
  "id": "a557739447131395f7a76d87a4cd19_7",
  "x": "The same percentage of common terms may not have been observed in tweets during the expression of negative stances towards the targets. Yet, completely the opposite pattern is observed in stance detection results of baseline systems given in <cite>[12]</cite> , i.e., better F-Measure rates have been obtained for the Against class when compared with the Favor class <cite>[12]</cite> . Some of the baseline systems reported in <cite>[12]</cite> are SVM-based systems using unigrams and ngrams as features similar to our study, but their data sets include all three stance classes of Favor, Against, and Neither, while our data set comprises only tweets classified as belonging to Favor or Against classes.",
  "y": "differences"
 },
 {
  "id": "a557739447131395f7a76d87a4cd19_8",
  "x": "Yet, completely the opposite pattern is observed in stance detection results of baseline systems given in <cite>[12]</cite> , i.e., better F-Measure rates have been obtained for the Against class when compared with the Favor class <cite>[12]</cite> . Some of the baseline systems reported in <cite>[12]</cite> are SVM-based systems using unigrams and ngrams as features similar to our study, but their data sets include all three stance classes of Favor, Against, and Neither, while our data set comprises only tweets classified as belonging to Favor or Against classes. Another difference is that the data sets in <cite>[12]</cite> have been divided into training and test sets, while in our study we provide 10-fold cross-validation results on the whole data set.",
  "y": "differences"
 },
 {
  "id": "a557739447131395f7a76d87a4cd19_9",
  "x": "We have also evaluated SVM classifiers which use only bigrams as features, as ngram-based classifiers have been reported to perform better for the stance detection problem <cite>[12]</cite> . However, we have observed that using bigrams as the sole features of the SVM classifiers leads to quite poor results. This observation may be due to the relatively limited size of the tweet data set employed.",
  "y": "background"
 },
 {
  "id": "a557739447131395f7a76d87a4cd19_10",
  "x": "Other classification approaches could also be implemented and tested against our baseline classifiers. Particularly, related methods presented in recent studies such as <cite>[12]</cite> can be tested on our data set. \u2022 Lastly, the SVM classifiers utilized in this study and their prospective versions utilizing other features can be tested on stance data sets in other languages (such as English) for comparison purposes.",
  "y": "future_work"
 },
 {
  "id": "a6564c4b215e6c5ad4f53eeb5dd69c_0",
  "x": "LSTM-based turn-taking models that operate in a similar predictive fashion were proposed by <cite>Skantze</cite> in <cite>[13]</cite> . In <cite>these models</cite> LSTMs are used to make continuous predictions of a person's speech activity at each individual time step of 50ms. <cite>The networks</cite> are trained to predict a vector of probability scores for speech activity in each individual frame within a set future window.",
  "y": "background"
 },
 {
  "id": "a6564c4b215e6c5ad4f53eeb5dd69c_1",
  "x": "Using these predictions we are able to anticipate utterance endpoints, and start our turns accordingly. LSTM-based turn-taking models that operate in a similar predictive fashion were proposed by <cite>Skantze</cite> in <cite>[13]</cite> . In <cite>these models</cite> LSTMs are used to make continuous predictions of a person's speech activity at each individual time step of 50ms. <cite>The networks</cite> are trained to predict a vector of probability scores for speech activity in each individual frame within a set future window. Rather than designing classifiers to make specific decisions, <cite>these continuous models</cite> are able to capture general information about turn-taking in the data that <cite>they</cite> are trained on. <cite>They</cite> can therefore be applied to a wide variety of turn-taking prediction tasks and have been shown to outperform traditional classifiers when applied to HOLD/SHIFT predictions. A downside to the approach in <cite>[13]</cite> is that, since a single LSTM is being used, all input features must be processed at the same rate.",
  "y": "background motivation"
 },
 {
  "id": "a6564c4b215e6c5ad4f53eeb5dd69c_2",
  "x": "In this paper we present significant extensions to the original work of <cite>Skantze</cite> in <cite>[13]</cite> . We investigate the performance of our proposed multiscale architecture on two datasets that contain two different combinations of modalities. We look at the influence of modeling modalities in separate sub-networks and using separate Session 5: Human Behavior ICMI'18, October 16-20, 2018, Boulder, CO, USA timescales.",
  "y": "extends"
 },
 {
  "id": "a6564c4b215e6c5ad4f53eeb5dd69c_3",
  "x": "---------------------------------- **CONTINUOUS TURN-TAKING PREDICTION** The main objective of continuous turn-taking prediction as proposed in <cite>[13]</cite> is to predict the future speech activity annotations of one of the speakers in a dyadic conversation using input speech features from both speakers (x t ).",
  "y": "background"
 },
 {
  "id": "a6564c4b215e6c5ad4f53eeb5dd69c_4",
  "x": "In an effort to simulate the conditions of a real-time system, the linguistic features were not provided to the system until 100ms after the end of the word. Three different temporal rates for the processing of linguistic features are tested in our experiments. In our discussion and results below, <cite>\"Ling 50ms\"</cite> refers to using word features that have been sampled at regular 50ms intervals, as was proposed in <cite>[13]</cite> .",
  "y": "uses"
 },
 {
  "id": "a6564c4b215e6c5ad4f53eeb5dd69c_5",
  "x": "The three network configurations are: \"no subnets\", which corresponds to an early fusion approach in which the modalities are fed directly into a single LSTM; \"one subnet\", which corresponds to the use of only one sub-network LSTM; and \"two subnets\", which corresponds to the use of separate LSTM subnetworks for the individual modalities. We note that combinations such as <cite>\"Ling 50ms\"</cite> with \"Acous 10ms\" are not possible when using the \"no subnets\" and \"one subnet\" configurations since the features are being input into the same LSTM and cannot operate at different temporal resolutions. Grid searches for three hyperparameters (hidden node size, dropout, and L2 regularization) were performed for each network configuration.",
  "y": "background"
 },
 {
  "id": "a6564c4b215e6c5ad4f53eeb5dd69c_6",
  "x": "Using a single subnet as an added layer also does not yield significant differences to the early fusion approach. We conclude that the main advantage in using our multiscale approach on a combination of acoustic and linguistic modalities is its ability to fuse the two modalities when the linguistic features are operating at a slow 50ms timescale and the acoustic features are operating at a fast 10ms timescale. Comparing our results with previously published baselines reported on the same dataset by <cite>Skantze</cite> in [13] , our best result on the PAUSE 500 task of 0.8553 is a large improvement over <cite>his reported score</cite> of 0.762. Looking at the results from the fusion of visual and acoustic modalities shown in in Table 2 , we were able to achieve our best BCE loss using our multiscale approach to fuse acoustic features at a 10ms timescale and visual features at a 58Hz timescale.",
  "y": "differences uses"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_0",
  "x": "The number of such false negative matches even exceeds the number of positive pairs, by 3 to 10 times, leading to a significant problem for training. Previous approaches (Riedel et al., 2010; Hoffmann et al., 2011;<cite> Surdeanu et al., 2012)</cite> bypassed this problem by heavily under-sampling the \"negative\" class. We instead deal with a learning scenario where we only have entity-pair level labels that are either positive or unlabeled.",
  "y": "background"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_1",
  "x": "Previous approaches (Riedel et al., 2010; Hoffmann et al., 2011;<cite> Surdeanu et al., 2012)</cite> bypassed this problem by heavily under-sampling the \"negative\" class. We instead deal with a learning scenario where we only have entity-pair level labels that are either positive or unlabeled. We proposed an extension to<cite> Surdeanu et al. (2012)</cite> that can train on this dataset.",
  "y": "differences"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_2",
  "x": "Previous approaches (Riedel et al., 2010; Hoffmann et al., 2011;<cite> Surdeanu et al., 2012)</cite> bypassed this problem by heavily under-sampling the \"negative\" class. We instead deal with a learning scenario where we only have entity-pair level labels that are either positive or unlabeled. We proposed an extension to<cite> Surdeanu et al. (2012)</cite> that can train on this dataset.",
  "y": "extends"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_3",
  "x": "**RELATED WORK** Distant supervision was first proposed by Craven and Kumlien (1999) in the biomedical domain. Since then, it has gain popularity (Mintz et al., 2009; Bunescu and Mooney, 2007; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011;<cite> Surdeanu et al., 2012</cite>; Nguyen and Moschitti, 2011) .",
  "y": "background"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_4",
  "x": "Since then, it has gain popularity (Mintz et al., 2009; Bunescu and Mooney, 2007; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011;<cite> Surdeanu et al., 2012</cite>; Nguyen and Moschitti, 2011) . To tolerate noisy labels in positive examples, Riedel et al. (2010) use Multiple Instance Learning (MIL), which assumes only at-least-one of the relation mentions in each \"bag\" of mentions sharing a pair of argument entities which bears a relation, indeed expresses the target relation. MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning<cite> (Surdeanu et al., 2012)</cite> further improve it to support multiple relations expressed by different sentences in a bag.",
  "y": "background"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_5",
  "x": "We randomly picked 200 unlabeled bags 5 from each of the two datasets (Riedel et al., 2010;<cite> Surdeanu et al., 2012</cite> ) generated by DS, and we manually annotate all relation mentions in these bags. The result is shown in Table 2 , along with a few examples that indicate a relation holds in the set of false negative matches (bag-level). Both datasets have around 10% false negative matches in the unlabeled set of bags.",
  "y": "uses"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_6",
  "x": "**A SEMI-SUPERVISED MIML ALGORITHM** Our goal is to model the bag-level label noise, caused by the incomplete KB problem, in addition Table 2 : False negative matches on the Riedel (Riedel et al., 2010) and KBP dataset<cite> (Surdeanu et al., 2012)</cite> . All numbers are on bag (pairs of entities) level.",
  "y": "uses"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_7",
  "x": "All numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and 5% in Riedel and KBP dataset, respectively. to modeling the instance-level noise using a 3-layer MIL or MIML model (e.g.,<cite> Surdeanu et al. (2012)</cite> ).",
  "y": "uses"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_8",
  "x": "to modeling the instance-level noise using a 3-layer MIL or MIML model (e.g.,<cite> Surdeanu et al. (2012)</cite> ). We propose a 4-layer model as shown in Figure 1 . The input to the model is a list of n bags with a vector of binary labels, either Positive (P), or Unlabled (U) for each relation r. Our model can be viewed as a semi-supervised 6 framework that extends a state-of-the-art Multi-Instance Multi-Label (MIML) model<cite> (Surdeanu et al., 2012)</cite> .",
  "y": "extends"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_9",
  "x": "\u03b8 is the fraction of the bags that are positive. It is an observed parameter that depends on both the source corpus and the KB used. Similar to<cite> Surdeanu et al. (2012)</cite> , we also define the following parameters and conditional probabilities (details are in<cite> Surdeanu et al. (2012)</cite> ):",
  "y": "similarities"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_10",
  "x": "We use the set of features in<cite> Surdeanu et al. (2012)</cite> . \u2022 w z is the weight vector for the multi-class relation mention-level classifier. \u2022 w r \u2113 is the weight vector for the rth binary toplevel aggregation classifier (from mention labels to bag-level prediction).",
  "y": "uses"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_11",
  "x": "After fixed \u2113, we seek to maximize: which can be solved with an approximate solution in<cite> Surdeanu et al. (2012)</cite> (step 9-11): update z i independently with: More details can be found in<cite> Surdeanu et al. (2012)</cite> .",
  "y": "uses"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_12",
  "x": "More details can be found in<cite> Surdeanu et al. (2012)</cite> . In the M-step, we retrain both of the mentionlevel and the aggregation level classifiers. The full EM algorithm is shown in algorithm 1.",
  "y": "background"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_13",
  "x": "We implement our model on top of the MIML<cite> (Surdeanu et al., 2012)</cite> code base. 8 We use the same mention-level and aggregate-level feature sets as<cite> Surdeanu et al. (2012)</cite> . We adopt the same idea of using cross validation for the E and M steps to avoid overfitting.",
  "y": "uses"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_14",
  "x": "We implement our model on top of the MIML<cite> (Surdeanu et al., 2012)</cite> code base. 8 We use the same mention-level and aggregate-level feature sets as<cite> Surdeanu et al. (2012)</cite> . We adopt the same idea of using cross validation for the E and M steps to avoid overfitting.",
  "y": "uses"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_15",
  "x": "Data set: We use the KBP (Ji et al., 2011) dataset 9 prepared and publicly released by<cite> Surdeanu et al. (2012)</cite> for our experiment since it is 1) large and realistic, 2) publicly available, 3) most importantly, it is the only dataset that has associated human-labeled ground truth. Any KB held-out evaluation without manual assessment will be significantly affected by KB incompleteness. In KBP dataset, the training bags are generated by mapping Wikipedia (http://en.wikipedia.org) infoboxes (after merging similar types following the KBP 2011 task definition) into a large unlabeled corpus (consisting of 1.5M documents from the KBP source corpus and a complete snapshot of Wikipedia).",
  "y": "uses"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_16",
  "x": "We use 40 queries as development dataset (dev), and the rest (160 queries) as evaluation dataset. We set \u03b8 = 0.25 by tuning on the dev set and use it in the experiments. For a fair comparison, we follow<cite> Surdeanu et al. (2012)</cite> and begin by downsampling the \"negative\" class to 5%.",
  "y": "uses"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_17",
  "x": "For a fair comparison, we follow<cite> Surdeanu et al. (2012)</cite> and begin by downsampling the \"negative\" class to 5%. We also set T=8 and use the following noisy-or (for ith bag) of mention-level probability to rank predicted types (r) of pairs and plot the precision-recall curves for all experiments. Evaluation: We compare our algorithm (MIMLsemi) to three algorithms: 1) MIML<cite> (Surdeanu et al., 2012)</cite> , the Multiple-Instance Multiple Label algorithm which labels the bags directly with the KB (y = \u2113).",
  "y": "uses"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_18",
  "x": "2) MultiR (denoted as Hoffmann) (Hoffmann et al., 2011), a Multiple-Instance algorithm that supports overlapping relations. It also imposes y = \u2113. 3) Mintz++<cite> (Surdeanu et al., 2012)</cite> , a variant of the single-instance learning algorithm (section 3). The first two are stat-of-the-art Multi-Instance Multi-Label algorithms.",
  "y": "uses"
 },
 {
  "id": "a672da8cba61074fe4b8ba1a452a47_19",
  "x": "Mintz++ is a strong baseline<cite> (Surdeanu et al., 2012)</cite> and an improved version of Mintz et al. (2009) . Figure 2 shows that our algorithm consistently outperforms all three algorithms at almost all recall levels (with the exception of a very small region in the PR-curve). This demonstrates that by treating unla-beled data set differently and leveraging the missing positive bags, MIML-semi is able to learn a more accurate model for extraction.",
  "y": "uses background"
 },
 {
  "id": "a67f40381e82afaf249f097a208555_0",
  "x": "The techniques for solving this problem can be applied to a variety of NLP tasks, such as query expansion, word sense disambiguation, machine translation, information extraction and question answering. Previous work addressing the problem can be roughly classified into three categories: (1) learning word embeddings from large collections of text using variants of neural networks (Mikolov et al. (2013a) ; Mikolov et al. (2013b) ; Mikolov et al. (2013c) ; Levy and Goldberg (2014) ) or global matrix factorization (Deerwester et al. (1990) ; Turney (2012) ); (2) extracting knowledge from existing semantic networks, such as WordNet (Yang and Powers (2005) ; Alvarez and Lim (2007) ; Hughes and Ramage (2007) ) and ConceptNet (Boteanu and Chernova (2015) ); (3) combining the above two models by various ways (Agirre et al. (2009) ;<cite> Zhila et al. (2013)</cite> ; Iacobacci et al. (2015) ; Summers-Stay et al. (2016) ). The empirical evidence shows that the word representations learned from neural network models do an especially good job in capturing not only attributional similarities between words but also similarities between pairs of words (Mikolov et al. (2013c) ).",
  "y": "background"
 },
 {
  "id": "a67f40381e82afaf249f097a208555_1",
  "x": "(1) In this paper, we use the word2vec model with linear bag-of-words contexts to capture the domain of a word, and the dependency-based word embeddings with syntactic contexts to characterize the function of a word. The broad contexts used in our model can provide richer information for measuring domain similarity (i.e., topic, subject, or field similarity) and function similarity (i.e, role, relationship, or usage similarity) than noun or verb-based patterns for contexts in Turney's (2012) model. (2) The two existing models for measuring relational similarity are: the directional similarity model <cite>(Zhila et al. (2013)</cite> ) and the dual-space model consisting of domain and function space (Turney (2012) ).",
  "y": "background"
 },
 {
  "id": "a67f40381e82afaf249f097a208555_2",
  "x": "(2) The two existing models for measuring relational similarity are: the directional similarity model <cite>(Zhila et al. (2013)</cite> ) and the dual-space model consisting of domain and function space (Turney (2012) ). Both models suffer some drawbacks. The directional similarity model explores the difference of two relationships in multiple topicality dimensions in the vector space.",
  "y": "motivation"
 },
 {
  "id": "a67f40381e82afaf249f097a208555_3",
  "x": "(2) The two existing models for measuring relational similarity are: the directional similarity model <cite>(Zhila et al. (2013)</cite> ) and the dual-space model consisting of domain and function space (Turney (2012) ). Both models suffer some drawbacks. The directional similarity model explores the difference of two relationships in multiple topicality dimensions in the vector space. However, it ignores the spatial distances between word vectors, which can reveal the function similarity of words in function space.",
  "y": "motivation"
 },
 {
  "id": "a67f40381e82afaf249f097a208555_4",
  "x": "(2) The two existing models for measuring relational similarity are: the directional similarity model <cite>(Zhila et al. (2013)</cite> ) and the dual-space model consisting of domain and function space (Turney (2012) ). The directional similarity model explores the difference of two relationships in multiple topicality dimensions in the vector space.",
  "y": "background"
 },
 {
  "id": "a67f40381e82afaf249f097a208555_5",
  "x": "(2) The two existing models for measuring relational similarity are: the directional similarity model <cite>(Zhila et al. (2013)</cite> ) and the dual-space model consisting of domain and function space (Turney (2012) ). The directional similarity model explores the difference of two relationships in multiple topicality dimensions in the vector space. However, it ignores the spatial distances between word vectors, which can reveal the function similarity of words in function space.",
  "y": "motivation"
 },
 {
  "id": "a67f40381e82afaf249f097a208555_6",
  "x": "The directional similarity model <cite>(Zhila et al. (2013)</cite> ) explores the difference of two relationships in multiple topicality dimensions in the vector space. However, it ignores the spatial distance between word vectors, which can reveal the function similarity of words in function space. The dual-space model (Turney (2012) ) can measure both domain similarity and function similarity between words.",
  "y": "background"
 },
 {
  "id": "a67f40381e82afaf249f097a208555_7",
  "x": "**A NEW DUAL-SPACE MODEL: REFINING THE DIRECTIONAL SIMILARITY MODEL AND THE DUALSPACE MODEL** The directional similarity model <cite>(Zhila et al. (2013)</cite> ) explores the difference of two relationships in multiple topicality dimensions in the vector space. However, it ignores the spatial distance between word vectors, which can reveal the function similarity of words in function space.",
  "y": "motivation"
 },
 {
  "id": "a67f40381e82afaf249f097a208555_8",
  "x": "The two embeddings used in our model consider broader contexts than those in the original dual-space model, which can provide richer information for measuring domain similarity and function similarity. A mathematical description of our dual-space model is given as follows: Given two pairs of words A:B and C:D, suppose D) are the vectors of these words in domain space and V DEP (A) , V DEP (B) , V DEP (C) , V DEP (D) are the vectors of these words in function space. Based on the directional similarity model<cite> (Zhila et al., 2013)</cite> , we define the domain similarity of two pairs of words as follows:",
  "y": "uses"
 },
 {
  "id": "a6954db741df61f014cc622c5b8263_0",
  "x": "This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments (Ford et al., 1982; Hindle and Rooth, 1993) . In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (Magerman, 1995; Charniak, 1997;<cite> Collins, 1999</cite>; Charniak, 2000; Charniak, 2001) . However, several results have brought into question how large a role lexicalization plays in such parsers.",
  "y": "background"
 },
 {
  "id": "a6954db741df61f014cc622c5b8263_1",
  "x": "A conviction arose that lexicalized PCFGs (where head words annotate phrasal nodes) were the key tool for high performance PCFG parsing. This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments (Ford et al., 1982; Hindle and Rooth, 1993) . In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (Magerman, 1995; Charniak, 1997;<cite> Collins, 1999</cite>; Charniak, 2000; Charniak, 2001) .",
  "y": "background"
 },
 {
  "id": "a6954db741df61f014cc622c5b8263_2",
  "x": "**2** The best-performing lexicalized PCFGs have increasingly made use of subcategorization 3 of the categories appearing in the Penn treebank. Charniak (2000) shows the value his parser gains from parentannotation of nodes, suggesting that this information is at least partly complementary to information derivable from lexicalization, and <cite>Collins (1999)</cite> uses a range of linguistically motivated and carefully hand-engineered subcategorizations to break down wrong context-freedom assumptions of the naive Penn treebank covering PCFG, such as differentiating \"base NPs\" from noun phrases with phrasal modifiers, and distinguishing sentences with empty subjects from those where there is an overt subject NP.",
  "y": "background"
 },
 {
  "id": "a6954db741df61f014cc622c5b8263_3",
  "x": "We describe several simple, linguistically motivated annotations which do much to close the gap between a vanilla PCFG and state-of-the-art lexicalized models. Specifically, we construct an unlexicalized PCFG which outperforms the lexicalized PCFGs of Magerman (1995) and Collins (1996) (though not more recent models, such as Charniak (1997) or <cite>Collins (1999)</cite> ). One benefit of this result is a much-strengthened lower bound on the capacity of an unlexicalized PCFG.",
  "y": "differences"
 },
 {
  "id": "a6954db741df61f014cc622c5b8263_4",
  "x": "The second basic deficiency is that many rule types have been seen only once (and therefore have their probabilities overestimated), and many rules which occur in test sentences will never have been seen in training (and therefore have their probabilities underestimated -see <cite>Collins (1999)</cite> for analysis). Note that in parsing with the unsplit grammar, not having seen a rule doesn't mean one gets a parse failure, but rather a possibly very weird parse (Charniak, 1996) . One successful method of combating sparsity is to markovize the rules<cite> (Collins, 1999)</cite> .",
  "y": "background"
 },
 {
  "id": "a6954db741df61f014cc622c5b8263_5",
  "x": "One successful method of combating sparsity is to markovize the rules<cite> (Collins, 1999)</cite> . In particular, we follow that work in markovizing out from the head child, despite the grammar being unlexicalized, because this seems the best way to capture the traditional linguistic insight that phrases are organized around a head (Radford, 1988) . Both parent annotation (adding context) and RHS markovization (removing it) can be seen as two instances of the same idea.",
  "y": "background"
 },
 {
  "id": "a6954db741df61f014cc622c5b8263_6",
  "x": "Figure 2 presents a grid of horizontal and vertical markovizations of the grammar. The raw treebank grammar corresponds to v = 1, h = \u221e (the upper right corner), while the parent annotation in (Johnson, 1998) corresponds to v = 2, h = \u221e, and the second-order model in <cite>Collins (1999)</cite> , is broadly a smoothed version of v = 2, h = 2. In addition to exact nth-order models, we tried variable- Figure 3: Size and devset performance of the cumulatively annotated models, starting with the markovized baseline.",
  "y": "similarities"
 },
 {
  "id": "a6954db741df61f014cc622c5b8263_7",
  "x": "Note that this technique of pushing the functional tags down to preterminals might be useful more generally; for example, locative PPs expand roughly the same way as all other PPs (usually as IN NP), but they do tend to have different prepositions below IN. A second kind of information in the original trees is the presence of empty elements. Following <cite>Collins (1999)</cite> , the annotation GAPPED-S marks S nodes which have an empty subject (i.e., raising and control constructions).",
  "y": "uses"
 },
 {
  "id": "a6954db741df61f014cc622c5b8263_8",
  "x": "In the case of two PPs following a NP, with the question of whether the second PP is a second modifier of the leftmost NP or should attach lower, inside the first PP, the important distinction is usually that the lower site is a non-recursive base NP. <cite>Collins (1999)</cite> captures this notion by introducing the notion of a base NP, in which any NP which dominates only preterminals is marked with a -B. Further, if an NP-B does not have a non-base NP parent, it is given one with a unary production. This was helpful, but substantially less effective than marking base NPs without introducing the unary, whose presence actually erased a useful internal indicator -base NPs are more frequent in subject position than object position, for example.",
  "y": "background"
 },
 {
  "id": "a6954db741df61f014cc622c5b8263_9",
  "x": "This is a partial explanation of the utility of verbal distance in <cite>Collins (1999)</cite> . To 13 The inability to encode distance naturally in a naive PCFG is somewhat ironic. In the heart of any PCFG parser, the fundamental table entry or chart item is a label over a span, for example an NP from position 0 to position 5.",
  "y": "background"
 },
 {
  "id": "a7223f2afa1afd5fbfa1257b98ec02_0",
  "x": "In the first half we introduce online learning and describe the Perceptron algorithm (Rosenblatt, 1958) and the passive-aggressive framework (Crammer et al., 2006) . We then discuss in detail an approach for deriving algorithms for complex natural language processing (Crammer, 2004) . In the second half we discuss is detail relevant applications including text classification (Crammer and Singer, 2003) , named entity recognition <cite>(McDonald et al., 2005)</cite> , parsing (McDonald, 2006) , and other tasks.",
  "y": "uses"
 },
 {
  "id": "a7223f2afa1afd5fbfa1257b98ec02_1",
  "x": "In the first half we introduce online learning and describe the Perceptron algorithm (Rosenblatt, 1958) and the passive-aggressive framework (Crammer et al., 2006) . We then discuss in detail an approach for deriving algorithms for complex natural language processing (Crammer, 2004) . In the second half we discuss is detail relevant applications including text classification (Crammer and Singer, 2003) , named entity recognition <cite>(McDonald et al., 2005)</cite> , parsing (McDonald, 2006) , and other tasks.",
  "y": "uses"
 },
 {
  "id": "a774b918013dbf60eb8cc0ad1de2f9_0",
  "x": "Leading recent approaches to domain adaptation in NLP are based on Neural Networks (NNs), and particularly on autoencoders (Glorot et al., 2011; Chen et al., 2012) . These models are believed to extract features that are robust to cross-domain variations. However, while excelling on benchmark domain adaptation tasks such as cross-domain product sentiment classification<cite> (Blitzer et al., 2007)</cite> , the reasons to this success are not entirely understood.",
  "y": "background"
 },
 {
  "id": "a774b918013dbf60eb8cc0ad1de2f9_1",
  "x": "In the pre-NN era, a prominent approach to domain adaptation in NLP, and particularly in sentiment classification, has been structural correspondence learning (SCL) (Blitzer et al., 2006<cite> (Blitzer et al., , 2007</cite> . Following the auxiliary problems approach to semi-supervised learning (Ando and Zhang, 2005) , this method identifies correspondences among features from different domains by modeling their correlations with pivot features: features that are frequent in both domains and are important for the NLP task. Non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, providing a bridge between the domains.",
  "y": "background"
 },
 {
  "id": "a774b918013dbf60eb8cc0ad1de2f9_2",
  "x": "We experiment with the task of cross-domain product sentiment classification of<cite> (Blitzer et al., 2007)</cite> , consisting of 4 domains (12 domain pairs) and further add an additional target domain, consisting of sentences extracted from social media blogs (total of 16 domain pairs). For pivot feature embedding in our advanced model, we employ the word2vec algorithm (Mikolov et al., 2013) . Our models substantially outperform strong baselines: the SCL algorithm, the marginalized stacked denoising autoencoder (MSDA) model (Chen et al., 2012) and the MSDA-DAN model (Ganin et al., 2016) that combines the power of MSDA with a domain adversarial network (DAN).",
  "y": "extends"
 },
 {
  "id": "a774b918013dbf60eb8cc0ad1de2f9_3",
  "x": "Pivot and Non-Pivot Features The definitions of this approach are given in Blitzer et al. (2006<cite> Blitzer et al. ( , 2007</cite> , where SCL is presented in the context of POS tagging and sentiment classification, respectively. Fundamentally, the method divides the shared feature space of both the source and the target domains to the set of pivot features that are frequent in both domains and are prominent in the NLP task, and a complementary set of non-pivot features. In this section we abstract away from the actual feature space and its division to pivot and non-pivot subsets.",
  "y": "background"
 },
 {
  "id": "a774b918013dbf60eb8cc0ad1de2f9_4",
  "x": "In order to learn a robust and compact feature representation for X we will aim to learn a nonlinear prediction function from x np to x p . As discussed in Section 4 the task we experiment with is cross-domain sentiment classification. Following previous work (e.g. (Blitzer et al., 2006<cite> (Blitzer et al., , 2007</cite> Chen et al., 2012) our feature representation consists of binary indicators for the occurrence of word unigrams and bigrams in the represented document.",
  "y": "uses"
 },
 {
  "id": "a774b918013dbf60eb8cc0ad1de2f9_5",
  "x": "**AUTOENCODER SCL WITH SIMILARITY REGULARIZATION (AE-SCL-SR)** An important observation of <cite>Blitzer et al. (2007)</cite> , is that some pivot features are similar to each other to the level that they indicate the same information with respect to the classification task. For example, in sentiment classification with word unigram features, the words (unigrams) great and excellent are likely to serve as pivot features, as the meaning of each of them is preserved across domains.",
  "y": "background"
 },
 {
  "id": "a774b918013dbf60eb8cc0ad1de2f9_6",
  "x": "To facilitate clarity, some details are not given here and instead are provided in the appendices. Cross-domain Sentiment Classification To demonstrate the power of our models for domain adaptation we experiment with the task of crossdomain sentiment classification<cite> (Blitzer et al., 2007)</cite> . The data for this task consist of Amazon product reviews from four product domains: Books (B), DVDs (D), Electronic items (E) and Kitchen appliances (K).",
  "y": "uses"
 },
 {
  "id": "a774b918013dbf60eb8cc0ad1de2f9_7",
  "x": "We aim to select baselines that represent the state-of-the-art in cross-domain sentiment classification in general, and in the two lines of work we focus at: pivot based and autoencoder based representation learning, in particular. The first baseline is SCL with pivot features selected using the mutual information criterion (SCL-MI,<cite> (Blitzer et al., 2007)</cite> ). This is the SCL method where pivot features are frequent in the unlabeled data of both the source and the target do-mains, and among those features are the ones with the highest mutual information with the task (sentiment) label in the source domain labeled data.",
  "y": "uses"
 },
 {
  "id": "a774b918013dbf60eb8cc0ad1de2f9_8",
  "x": "As in the other models, MSDA-DAN utilizes source domain labeled data as well as unlabeled data from both the source and the target domains at training time. We experiment with a 5-fold cross-validation on the source domain<cite> (Blitzer et al., 2007)</cite> : 1600 reviews for training and 400 reviews for development. The test set for each target domain of <cite>Blitzer et al. (2007)</cite> consists of all 2000 labeled reviews of that domain, and for the Blog domain it consists of the 7086 labeled sentences provided with the task dataset.",
  "y": "uses"
 },
 {
  "id": "a774b918013dbf60eb8cc0ad1de2f9_9",
  "x": "As in the other models, MSDA-DAN utilizes source domain labeled data as well as unlabeled data from both the source and the target domains at training time. We experiment with a 5-fold cross-validation on the source domain<cite> (Blitzer et al., 2007)</cite> : 1600 reviews for training and 400 reviews for development. The test set for each target domain of <cite>Blitzer et al. (2007)</cite> consists of all 2000 labeled reviews of that domain, and for the Blog domain it consists of the 7086 labeled sentences provided with the task dataset.",
  "y": "background"
 },
 {
  "id": "a774b918013dbf60eb8cc0ad1de2f9_10",
  "x": "Baselines: For SCL-MI, following<cite> (Blitzer et al., 2007)</cite> we tuned the number of pivot features (Gillick and Cox, 1989; Blitzer et al., 2006) between 500 and 1000 and the SVD dimensions among 50,100 and 150. For MSDA we tuned the number of reconstructed features among {500, 1000, 2000, 5000, 10000}, the number of model layers among {1, 3, 5} and the corruption probability among {0.1, 0.2, . . . , 0.5}. For MSDA-DAN, we followed Ganin et al. (2016) : the \u03bb adaptation parameter is chosen among 9 values between 10 \u22122 and 1 on a logarithmic scale, the hidden layer size l is chosen among {50, 100, 200} and the learning rate \u00b5 is 10 \u22123 . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "a774b918013dbf60eb8cc0ad1de2f9_12",
  "x": "Variants of the Product Review Data There are two releases of the datasets of the <cite>Blitzer et al. (2007)</cite> cross-domain product review task. We use the one from http://www.cs.jhu. edu/\u02dcmdredze/datasets/sentiment/ index2.html where the data is imbalanced, consisting of more positive than negative reviews.",
  "y": "uses"
 },
 {
  "id": "a774b918013dbf60eb8cc0ad1de2f9_13",
  "x": "edu/\u02dcmdredze/datasets/sentiment/ index2.html where the data is imbalanced, consisting of more positive than negative reviews. We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set. Note that <cite>Blitzer et al. (2007)</cite> used the other release where the unlabeled data consists of the same number of positive and negative reviews.",
  "y": "differences"
 },
 {
  "id": "a774b918013dbf60eb8cc0ad1de2f9_14",
  "x": "We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set. Note that <cite>Blitzer et al. (2007)</cite> used the other release where the unlabeled data consists of the same number of positive and negative reviews. Test Set Size While <cite>Blitzer et al. (2007)</cite> used only 400 target domain reviews for test, we use the entire set of 2000 reviews.",
  "y": "differences"
 },
 {
  "id": "a7d6441ad365994edb41209e6405e0_0",
  "x": "Recently, bidirectional long short-term memory networks (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) have been used for language modelling<cite> (Ling et al., 2015)</cite> , POS tagging<cite> (Ling et al., 2015</cite>;<cite> Wang et al., 2015)</cite> , transition-based dependency parsing (Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016) , fine-grained sentiment analysis (Liu et al., 2015) , syntactic chunking (Huang et al., 2015) , and semantic role labeling (Zhou and Xu, 2015) . LSTMs are recurrent neural networks (RNNs) in which layers are designed to prevent vanishing gradients. Bidirectional LSTMs make a backward and forward pass through the sequence before passing on to the next layer.",
  "y": "background"
 },
 {
  "id": "a7d6441ad365994edb41209e6405e0_1",
  "x": "For further details, see (Goldberg, 2015; Cho, 2015) . We consider using bi-LSTMs for POS tagging. Previous work on using deep learning-based methods for POS tagging has focused either on a single language (Collobert et al., 2011;<cite> Wang et al., 2015)</cite> or a small set of languages<cite> (Ling et al., 2015</cite>; Santos and Zadrozny, 2014 ).",
  "y": "background"
 },
 {
  "id": "a7d6441ad365994edb41209e6405e0_2",
  "x": "In addition, we compare performance with representations at different levels of granularity (words, characters, and bytes). These levels of representation were previously introduced in different efforts (Chrupa\u0142a, 2013; Zhang et al., 2015;<cite> Ling et al., 2015</cite>; Santos and Zadrozny, 2014; Gillick et al., 2016; Kim et al., 2015) , but a comparative evaluation was missing. Moreover, deep networks are often said to require large volumes of training data.",
  "y": "background motivation"
 },
 {
  "id": "a7d6441ad365994edb41209e6405e0_3",
  "x": "In addition, we compare performance with representations at different levels of granularity (words, characters, and bytes). These levels of representation were previously introduced in different efforts (Chrupa\u0142a, 2013; Zhang et al., 2015;<cite> Ling et al., 2015</cite>; Santos and Zadrozny, 2014; Gillick et al., 2016; Kim et al., 2015) , but a comparative evaluation was missing. Moreover, deep networks are often said to require large volumes of training data.",
  "y": "background"
 },
 {
  "id": "a7d6441ad365994edb41209e6405e0_4",
  "x": "Our basic bi-LSTM tagging model is a context bi-LSTM taking as input word embeddings w. We incorporate subtoken information using an hierarchical bi-LSTM architecture<cite> (Ling et al., 2015</cite>; Ballesteros et al., 2015 ). We compute subtokenlevel (either characters c or unicode byte b) embeddings of words using a sequence bi-LSTM at the lower level. This representation is then concatenated with the (learned) word embeddings vector w which forms the input to the context bi-LSTM at the next layer.",
  "y": "uses"
 },
 {
  "id": "a7d6441ad365994edb41209e6405e0_5",
  "x": "While on macro average it is on par with bi-LSTM w + c, it obtains the best results on 12/22 languages, and it is successful in predicting POS for OOV tokens (cf. Table 2 OOV ACC columns), especially for languages like Arabic, Farsi, Hebrew, Finnish. We examined simple RNNs and confirm the finding of<cite> Ling et al. (2015)</cite> that they performed worse than their LSTM counterparts.",
  "y": "similarities"
 },
 {
  "id": "a7d6441ad365994edb41209e6405e0_7",
  "x": "Similarly, Labeau et al. (2015) evaluate character embeddings for German. Bi-LSTMs for POS tagging are also reported in<cite> Wang et al. (2015)</cite> , however, they only explore word embeddings, orthographic information and evaluate on WSJ only. A related study is Cheng et al. (2015) who propose a multi-task RNN for named entity recognition by jointly predicting the next token and current token's name label.",
  "y": "differences"
 },
 {
  "id": "a800862f17f7a8c13ed13fc6e9433f_0",
  "x": "Nevertheless, the best performing systems follow the traditional hybrid approach [3] , outperforming attention based encoder-decoder models<cite> [4,</cite> 5, 6, 7] , and when less training data is used, the gap between \"end-to-end\" and hybrid models is more prominent<cite> [4,</cite> 8] . Several methods have been proposed to tackle data sparsity and overfitting problems; a detailed list can be found in Sec. 2. Recently, increasingly complex attention mechanisms have been proposed to improve seq2seq model performance, including stacking self and regular attention layers and using multiple attention heads in the encoder and decoder [5, 9] .",
  "y": "background"
 },
 {
  "id": "a800862f17f7a8c13ed13fc6e9433f_2",
  "x": "Following [25] , sequence noise mixed from up to 4 utterances is injected with 40% probability and 0.3 weight. The filterbank output is mean-and-variance normalized at the speaker level, and first (\u2206) and second (\u2206\u2206) derivatives are also calculated. The final features presented to the network are also processed through a SpecAugment block that uses the SM policy <cite>[4]</cite> with p = 0.3 and no time warping.",
  "y": "uses"
 },
 {
  "id": "a800862f17f7a8c13ed13fc6e9433f_3",
  "x": "We note that this model already outperforms the best published attention based seq2seq model <cite>[4]</cite> , with roughly ---------------------------------- **66% FEWER PARAMETERS.**",
  "y": "differences"
 },
 {
  "id": "a800862f17f7a8c13ed13fc6e9433f_4",
  "x": "**COMPARISON WITH THE LITERATURE** For comparison with results in the literature we refer to the Switchboard-300 results in<cite> [4,</cite> 8, 52, 53] and the Switchboard-2000 results in [51, 52, 54, 55, 56, 57] . Our 300hour model not only outperforms the previous best attention based encoder-decoder model <cite>[4]</cite> by a large margin, it also surpasses the best hybrid systems with multiple LMs [8] .",
  "y": "uses"
 },
 {
  "id": "a800862f17f7a8c13ed13fc6e9433f_5",
  "x": "For comparison with results in the literature we refer to the Switchboard-300 results in<cite> [4,</cite> 8, 52, 53] and the Switchboard-2000 results in [51, 52, 54, 55, 56, 57] . Our 300hour model not only outperforms the previous best attention based encoder-decoder model <cite>[4]</cite> by a large margin, it also surpasses the best hybrid systems with multiple LMs [8] . Our result on Switchboard-2000 is also better than any single system results reported to date, and reaches the performance of the best system combinations.",
  "y": "differences"
 },
 {
  "id": "a869bebe1744e3a7c71cb0f6fed12c_0",
  "x": "For the widely studied problem of polarity prediction in social media (positive vs. negative emotion or evaluation, only; Rosenthal et al. (2017) ), training data is relatively abundant. However, annotating for more complex representations of affective states-such as Basic Emotions (Ekman, 1992) or ValenceArousal-Dominance (Bradley and Lang, 1994 )-seems to be significantly harder in terms of both time consumption and inter-annotator agreement (IAA)<cite> (Strapparava and Mihalcea, 2007)</cite> . Nevertheless, these more complex models of emotion rapidly gained popularity in recent years due to their increased expressiveness (Wang et al., 2016; Buechel and Hahn, 2017; Sedoc et al., 2017) .",
  "y": "motivation background"
 },
 {
  "id": "a869bebe1744e3a7c71cb0f6fed12c_1",
  "x": [
   "Both claims, the need for large amounts of gold data and the lack of affective information in pre-trained word embeddings, may largely impede the feasibility of DL in lowresource scenarios. Yet, in this paper, we provide strong, first-time evidence that both, in actuality, turn out to be misconceptions. (Strapparava and Mihalcea, 2007) ."
  ],
  "y": "background motivation"
 },
 {
  "id": "a869bebe1744e3a7c71cb0f6fed12c_2",
  "x": "SE07: The test set of SemEval 2007 Task 14<cite> (Strapparava and Mihalcea, 2007)</cite> comprises 1000 English news headlines which are annotated according to six Basic Emotions, joy, anger, sadness, fear, disgust, and surprise on a [0; 100]-scale (BE6 annotation format). ANET: The Affective Norms for English Text (Bradley and Lang, 2010) are an adaptation of the popular lexical database ANEW (Bradley and Lang, 1999 ) to short texts. The corpus comprises 120 situation description which are annotated according to Valence, Arousal, and Dominance on a 9-point scale (VAD annotation format).",
  "y": "uses"
 },
 {
  "id": "a869bebe1744e3a7c71cb0f6fed12c_3",
  "x": "In line with that, we found in a supplemental experiment that not using pre-trained embeddings but instead learning them during training significantly reduces performance, e.g., by over 15%-points for the GRU on SE07. We now compare our best performing model against previously reported results for the SE07 corpus. Table 5 provides the performance of the winning system of the original shared task (WIN-NER; Chaumartin (2007) ), the IAA as reported by the organizers<cite> (Strapparava and Mihalcea, 2007)</cite> , the performance by Beck (2017) , the highest one reported for this data set so far (BECK), as well as the results of our GRU from the 10\u00d710-CV.",
  "y": "uses"
 },
 {
  "id": "a869bebe1744e3a7c71cb0f6fed12c_4",
  "x": "As can be seen, the GRU established a new state-of-the-art result and even achieves superhuman performance. This may sound improbable at first glance. However, <cite>Strapparava and Mihalcea (2007)</cite> employ a rather weak notion of human performance which is-broadly speaking-based on the reliability of a single human rater.",
  "y": "motivation differences"
 },
 {
  "id": "a869bebe1744e3a7c71cb0f6fed12c_5",
  "x": "CNN-LSTM on four topologically diverse data sets of sizes ranging between 1000 and only 120 instances. Counterintuitively, we found that all DL approaches performed well under every experimental condition. Our proposed GRU model even established a novel state-of-the-art result on the SemEval 2007 test set<cite> (Strapparava and Mihalcea, 2007)</cite> outperforming human reliability.",
  "y": "uses differences"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_0",
  "x": "Therefore, the ability to predict association strengths between arbitrary word pairs represents the best solution to these coverage issues (Boyd-Graber et al., 2006) . Although the prediction of Evocation ratings has attracted some attention (Boyd-Graber et al., 2006; <cite>Hayashi, 2016</cite>) , to the best of our knowledge this is the first work to focus on the prediction of USF or EAT strengths. As described in Sec-tion 2, USF and EAT have several advantages over Evocation, such as the ability to work with ambiguous words instead of WordNet synsets.",
  "y": "background"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_1",
  "x": "Although the prediction of Evocation ratings has attracted some attention (Boyd-Graber et al., 2006; <cite>Hayashi, 2016</cite>) , to the best of our knowledge this is the first work to focus on the prediction of USF or EAT strengths. As described in Sec-tion 2, USF and EAT have several advantages over Evocation, such as the ability to work with ambiguous words instead of WordNet synsets. Following <cite>Hayashi (2016)'s work</cite> on Evocation prediction, we frame word association prediction as a supervised regression task and introduce several new and modified features, including the first use of Gaussian embeddings (Vilnis and McCallum, 2014) to better capture the asymmetric nature of word associations.",
  "y": "extends"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_2",
  "x": "However, this approach is somewhat limited in that it frames Evocation prediction as a classification task, considering only five Evocation levels. The main drawback of Evocation prediction as a classification task is that it is too coarse-grained to deal with very weak associations, such as those in remote triads (De Deyne et al., 2016a) , or very slight variations in association strength, such as those useful for computational humour (Cattle and Ma, 2016) . To this end, <cite>Hayashi (2016)</cite> framed Evocation prediction as a supervised regression task.",
  "y": "background"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_3",
  "x": "The main drawback of Evocation prediction as a classification task is that it is too coarse-grained to deal with very weak associations, such as those in remote triads (De Deyne et al., 2016a) , or very slight variations in association strength, such as those useful for computational humour (Cattle and Ma, 2016) . To this end, <cite>Hayashi (2016)</cite> framed Evocation prediction as a supervised regression task. <cite>They</cite> employed a combination of WordNet structure-based features, word embedding-based features, and lexical features and found that vector offsets, i.e. the mathematical difference between vectors, were a strong indicator of Evocation ratings.",
  "y": "background"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_4",
  "x": "**SYSTEM DEFINITION** Our word association prediction system extends the method in <cite>Hayashi (2016)</cite> with several modifications to make it better suited to the USF and EAT datasets. First, we modify <cite>Hayashi (2016)'s</cite> lexVector.",
  "y": "extends"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_5",
  "x": "First, we modify <cite>Hayashi (2016)'s</cite> lexVector. <cite>Hayashi (2016)</cite> represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet). Similarly, <cite>they</cite> represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).",
  "y": "extends"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_6",
  "x": "<cite>Hayashi (2016)</cite> represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet). Similarly, <cite>they</cite> represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file). This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response.",
  "y": "background"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_7",
  "x": "Similarly, <cite>they</cite> represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file). This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response. Since words in USF and EAT can be associated with multiple synsets and we want to be able to capture associations related to polysemy, <cite>instead using a one-hot encoding</cite> we employ count vectors specifying the number of synsets from each POS/lexical category each word belongs to.",
  "y": "background"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_8",
  "x": "Since words in USF and EAT can be associated with multiple synsets and we want to be able to capture associations related to polysemy, <cite>instead using a one-hot encoding</cite> we employ count vectors specifying the number of synsets from each POS/lexical category each word belongs to. Second, <cite>instead of computing Wu-Palmer similarity</cite> (WUP, Wu and Palmer, 1994 ) between a single synset pair, we compute it for all cue synset/response synset pairs and record the maximum and average values. Following Boyd-Graber et al. (2006) and Ma (2013), we also explored the use of path and Leacock-Chodorow (Leacock and Chodorow, 1998) similarities but found they did not add any advantage over WUP alone.",
  "y": "differences"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_9",
  "x": "Second, <cite>instead of computing Wu-Palmer similarity</cite> (WUP, Wu and Palmer, 1994 ) between a single synset pair, we compute it for all cue synset/response synset pairs and record the maximum and average values. Following Boyd-Graber et al. (2006) and Ma (2013), we also explored the use of path and Leacock-Chodorow (Leacock and Chodorow, 1998) similarities but found they did not add any advantage over WUP alone. We take a similar approach for adapting load and betweenness centralities (Barthelemy, 2004) as well as AutoExtend (AutoEx, Rothe and Sch\u00fctze, 2015) similarity.",
  "y": "differences"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_10",
  "x": "Third, we extend the notion of dirRel, introduced in <cite>Hayashi (2016)</cite> to leverage the semantic network structure of WordNet. Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. <cite>In the original formula</cite>, s and t are nodes representing a single synset. We <cite>instead consider</cite> a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1.",
  "y": "extends"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_11",
  "x": "We take a similar approach for adapting load and betweenness centralities (Barthelemy, 2004) as well as AutoExtend (AutoEx, Rothe and Sch\u00fctze, 2015) similarity. Third, we extend the notion of dirRel, introduced in <cite>Hayashi (2016)</cite> to leverage the semantic network structure of WordNet. Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. <cite>In the original formula</cite>, s and t are nodes representing a single synset.",
  "y": "background"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_12",
  "x": "Third, we extend the notion of dirRel, introduced in <cite>Hayashi (2016)</cite> to leverage the semantic network structure of WordNet. Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. <cite>In the original formula</cite>, s and t are nodes representing a single synset. We <cite>instead consider</cite> a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1.",
  "y": "differences"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_13",
  "x": "We <cite>instead consider</cite> a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1. This may increase the probability that |nb(S, k)\u2229nb(T, k)| > 0, a <cite>shortcoming of the original dirRel</cite> due to WordNet's \"relatively sparse connective structure\" (<cite>Hayashi, 2016</cite>) . Fourth, in addition to the Word2Vec (w2v) cosine similarity between cue/response pairs calculated using Google's pre-trained 300 dimension Word2Vec embeddings 2 .",
  "y": "differences"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_14",
  "x": "Gaussian embeddings (Vilnis and McCallum, 2014) represent words not as a fixed point in vector space but as \"potential functions\", continuous densities in latent space; therefore, they are more suitable for capturing asymmetric relationships. More specifically, for each cue/response pair, we calculate both the KL-divergence and cosine similarities of their Gaussian embeddings. The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in <cite>Hayashi (2016)</cite> to compute vector offsets are not well suited for our task.",
  "y": "differences"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_15",
  "x": "More specifically, for each cue/response pair, we calculate both the KL-divergence and cosine similarities of their Gaussian embeddings. The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in <cite>Hayashi (2016)</cite> to compute vector offsets are not well suited for our task. <cite>Instead, we</cite> experiment with offsets calculated using w2v, GloVe, and w2g embeddings.",
  "y": "differences"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_16",
  "x": "The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in <cite>Hayashi (2016)</cite> to compute vector offsets are not well suited for our task. <cite>Instead, we</cite> experiment with offsets calculated using w2v, GloVe, and w2g embeddings. Finally, our 300 topic LDA model (Blei et al., 2003) was trained using Gensim 6 on full English Wikipedia instead of the subset of English Wikipedia used in <cite>Hayashi (2016</cite>) .",
  "y": "differences"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_17",
  "x": "Following the setup used in <cite>Hayashi (2016)</cite> , all neural networks are trained using the Chainer 7 Python library with rectified linear units, dropout, and two hidden layers, each with 50% of the number of units in the input layer. All were trained on 80% of their respective dataset, with 20% held out for testing. Mean squared error was used as a loss function and optimization was performed using Adam algorithm (Kingma and Ba, 2014) .",
  "y": "uses"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_18",
  "x": "All were trained on 80% of their respective dataset, with 20% held out for testing. Mean squared error was used as a loss function and optimization was performed using Adam algorithm (Kingma and Ba, 2014) . To act as a baseline, we also reimplemented the system described in <cite>Hayashi (2016)</cite> and trained <cite>it</cite> on the same 80/20 split of Evocation as our system.",
  "y": "uses"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_19",
  "x": "The results of our <cite>Hayashi (2016)</cite> implementation are roughly comparable to those reported in the <cite>original paper</cite> (r = 0.374, \u03c1 = 0.401 compared to r = 0.439, \u03c1 = 0.400). Our slightly lower Pearson's R may be due to differences in way we split our training and test data as well as due to randomness in the training process itself. On Evocation, our system does not perform as well as <cite>Hayashi (2016)</cite> .",
  "y": "similarities"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_20",
  "x": "On Evocation, our system does not perform as well as <cite>Hayashi (2016)</cite> . This is expected as we explicitly ignore any synset information and instead attempt to predict association strengths between word-sense ambiguous words. Despite this, our performance is not appreciably lower, indicating the fitness of our system.",
  "y": "differences"
 },
 {
  "id": "a99a393c83e47393400c72338faf80_21",
  "x": "In this paper we explored the effectiveness of various features for predicting Evocation, USF, and EAT association strengths, finding GloVe and Word2Vec cosine similarities as well as vector offsets to be the most useful features. We also examined the effectiveness of Gaussian embeddings for capturing the asymmetric nature of word embeddings but found it to be less effective than traditional word embeddings. Although we report a lower performance than that in <cite>Hayashi (2016)</cite> , potentially indicating that predicting association strengths in word-sense ambiguous contexts is a harder task, we believe our results are a promising start.",
  "y": "differences"
 },
 {
  "id": "aa87225d7d326adfb4a8b2702b8f25_0",
  "x": "Neural language models, an alternative means to learn word representations, use language data to optimise (latent) features with respect to a language modelling objective. The objective can be to predict either the next word given the initial words of a sentence [4, 14, 8] , or simply a nearby word given a single cue word<cite> [13,</cite> 15] . The representations learned by neural models (sometimes called embeddings) generally outperform those acquired by co-occurrence counting models when applied to NLP tasks [3] .",
  "y": "background"
 },
 {
  "id": "aa87225d7d326adfb4a8b2702b8f25_1",
  "x": "Based on this representation, a probability distribution is computed over the vocabulary, from which the model can sample a guess at the next word. The model weights and embeddings are updated to maximise the probability of correct guesses for all sentences in the training corpus. More recent work has shown that high quality word embeddings can be learned via models with no nonlinear hidden layer<cite> [13,</cite> 15] .",
  "y": "background"
 },
 {
  "id": "aa87225d7d326adfb4a8b2702b8f25_2",
  "x": "Given a single word in the corpus, these models simply predict which other words will occur nearby. For each word w in V , a list of training cases (w, c) : c \u2208 V is extracted from the training corpus. For instance, in the skipgram approach <cite>[13]</cite> , for each 'cue word' w the 'context words' c are sampled from windows either side of tokens of w in the corpus (with c more likely to be sampled if it occurs closer to w).",
  "y": "background"
 },
 {
  "id": "aa87225d7d326adfb4a8b2702b8f25_3",
  "x": "We conducted all experiments with the resulting (English) source embeddings from these models. For comparison, we trained a monolingual skipgram model <cite>[13]</cite> and its Glove variant [15] for the same number of epochs on the English half of the bilingual corpus. We also extracted embeddings from a full-sentence language model [CW, 8] trained for several months on a larger 1bn word corpus.",
  "y": "uses"
 },
 {
  "id": "aa87225d7d326adfb4a8b2702b8f25_4",
  "x": "Lexical analogy questions are an alternative way of evaluating word representations<cite> [13,</cite> 15] . In this task, models must identify the correct answer (girl) when presented with questions such as 'man is to boy as woman is to ...'. For skipgram-style embeddings, it has been shown that if m, b and w are the embeddings for man, boy and woman respectively, the correct answer is often the nearest neighbour in the vocabulary (by cosine distance) to the vector v = w + b \u2212 m <cite>[13]</cite> .",
  "y": "background"
 },
 {
  "id": "aa87225d7d326adfb4a8b2702b8f25_5",
  "x": "In this task, models must identify the correct answer (girl) when presented with questions such as 'man is to boy as woman is to ...'. For skipgram-style embeddings, it has been shown that if m, b and w are the embeddings for man, boy and woman respectively, the correct answer is often the nearest neighbour in the vocabulary (by cosine distance) to the vector v = w + b \u2212 m <cite>[13]</cite> . Monolingual skipgram/Glove models are better at semantic analogies (father, man; mother, woman)",
  "y": "background"
 },
 {
  "id": "aa87225d7d326adfb4a8b2702b8f25_6",
  "x": "Monolingual skipgram/Glove models are better at semantic analogies (father, man; mother, woman) We evaluated the embeddings on this task using the same vector-algebra method as <cite>[13]</cite> . As before we excluded questions containing a word outside the intersection of all model vocabularies, and restricted all answer searches to this reduced vocabulary, leaving 11,166 analogies.",
  "y": "uses"
 },
 {
  "id": "ab08b0f3c8691852b3aab5e3575ebd_0",
  "x": "It thus enables faster wide-scale analysis within and across different domains/industries such as telecom, healthcare, finance etc. SODA is based on an iterative ensemble based adaptation technique<cite> (Bhatt et al., 2015)</cite> which gradually transfers knowledge from the source to the new target collection while being cognizant of similarity between the two collections. It has been extensively evaluated by business professionals in a user-trial and on a benchmark dataset.",
  "y": "uses background"
 },
 {
  "id": "ab08b0f3c8691852b3aab5e3575ebd_1",
  "x": "This is based on the observations from existing literature <cite>(Bhatt et al., 2015</cite>; Blitzer et al., 2007) which suggest that if the source and target collections are similar, the adaptation performance tends to be better than if the two collections are dissimilar. The similarity module in SODA is capable of computing different kinds of lexical, syntactic, and semantic similarities between unlabeled target and labeled source collections. For this demonstration on sentiment categorization from social media data, it measures cosine similarity between the comments in each collection and computes sim as the similarity score.",
  "y": "uses background"
 },
 {
  "id": "ab08b0f3c8691852b3aab5e3575ebd_2",
  "x": "Algorithm 1 summarizes our approach (refer<cite> (Bhatt et al., 2015)</cite> for more details). ---------------------------------- **ACTIVE LEARNING**",
  "y": "uses background"
 },
 {
  "id": "ab5788da3f24e01b0ec40fba0bdbec_0",
  "x": "For that purpose, state-of-the-art approaches rely on either a separately trained unsupervised Statistical Machine Translation (SMT) system, which is used for warmup during the initial back-translation iterations (Marie and Fujita, 2018;<cite> Artetxe et al., 2019)</cite> , or large-scale pre-training through masked denoising, which is used to initialize the weights of the underlying encoder-decoder (Conneau and Lample, 2019; Song et al., 2019; Liu et al., 2020) . In this paper, we aim to understand the role that the initialization mechanism plays in iterative backtranslation. For that purpose, we mimic the experimental settings of <cite>Artetxe et al. (2019)</cite> , and measure the effect of using different initial systems for warmup: the unsupervised SMT system proposed by <cite>Artetxe et al. (2019)</cite> themselves, supervised NMT and SMT systems trained on both small and large parallel corpora, and a commercial Rule-Based Machine Translation (RBMT) system.",
  "y": "background"
 },
 {
  "id": "ab5788da3f24e01b0ec40fba0bdbec_1",
  "x": "For that purpose, we mimic the experimental settings of <cite>Artetxe et al. (2019)</cite> , and measure the effect of using different initial systems for warmup: the unsupervised SMT system proposed by <cite>Artetxe et al. (2019)</cite> themselves, supervised NMT and SMT systems trained on both small and large parallel corpora, and a commercial Rule-Based Machine Translation (RBMT) system. Despite the fundamentally different nature of these systems, our analysis reveals that iterative back-translation has a strong tendency to converge to a similar solution. Given the relatively small impact of the initial system, we conclude that fu-ture research on unsupervised machine translation should focus more on improving the iterative backtranslation mechanism itself.",
  "y": "uses"
 },
 {
  "id": "ab5788da3f24e01b0ec40fba0bdbec_2",
  "x": "We next describe the iterative back-translation implementation used in our experiments, which was proposed by <cite>Artetxe et al. (2019)</cite> . Note, however, that the underlying principles of iterative backtranslation are very general, so our conclusions should be valid beyond this particular implementation. The method in question trains two NMT systems in opposite directions following an iterative process where, at every iteration, each model is updated by performing a single pass over a set of N synthetic parallel sentences generated through back-translation.",
  "y": "uses"
 },
 {
  "id": "ab5788da3f24e01b0ec40fba0bdbec_3",
  "x": "In the latter case, half of the translations use random sampling , which produces more varied translations, whereas the other half are generated through greedy decoding, which produces more fluent and predictable translations. Following <cite>Artetxe et al. (2019)</cite> , we set N = 1, 000, 000 and a = 30, and perform a total of 60 such iterations. Both NMT models use the big transformer implementation from Fairseq 1 , training with a total batch size of 20,000 tokens with the exact same hyperparameters as .",
  "y": "uses"
 },
 {
  "id": "ab5788da3f24e01b0ec40fba0bdbec_4",
  "x": "\u2022 Unsupervised: We use the unsupervised SMT system proposed by <cite>Artetxe et al. (2019)</cite> , which induces an initial phrase-table using cross-lingual word embedding mappings, combines it with an n-gram language model, and further improves the resulting model through unsupervised tuning and joint refinement. For each initial system, we train a separate NMT model through iterative back-translation as described in Section 2. For that purpose, we use the News Crawl 2007-2013 monolingual corpus as distributed in the WMT 2014 shared task.",
  "y": "uses"
 },
 {
  "id": "ab5788da3f24e01b0ec40fba0bdbec_5",
  "x": "While all the previous authors use a fixed system to generate synthetic parallel corpora, Hoang et al. (2018) propose performing a second iteration of back-translation. Iterative back-translation was also explored by Marie and Fujita (2018) and <cite>Artetxe et al. (2019)</cite> in the context of unsupervised machine translation, relying on an unsupervised SMT system (Lample et al., 2018b; Artetxe et al., 2018a) for warmup. Early work in unsupervised NMT also incorporated the idea of on-the-fly backtranslation, which was combined with denoising autoencoding and a shared encoder initialized through unsupervised cross-lingual embeddings (Artetxe et al., 2018b; Lample et al., 2018a) .",
  "y": "background"
 },
 {
  "id": "ab6f114b2ce4e62e6d8a639e8183eb_0",
  "x": "Pronouns are frequently omitted in pro-drop languages, such as Chinese, generally leading to significant challenges with respect to the production of complete translations. Recently, <cite>Wang et al. (2018)</cite> proposed a novel reconstruction-based approach to alleviating dropped pronoun (DP) translation problems for neural machine translation models. In this work, we improve the original model from two perspectives.",
  "y": "background"
 },
 {
  "id": "ab6f114b2ce4e62e6d8a639e8183eb_1",
  "x": "Recently, <cite>Wang et al. (2018)</cite> proposed a novel reconstruction-based approach to alleviating dropped pronoun (DP) translation problems for neural machine translation models. In this work, we improve the original model from two perspectives. First, we employ a shared reconstructor to better exploit encoder and decoder representations.",
  "y": "uses"
 },
 {
  "id": "ab6f114b2ce4e62e6d8a639e8183eb_2",
  "x": "Recently, <cite>Wang et al. (2018)</cite> proposed a novel reconstruction-based approach to alleviating dropped pronoun (DP) translation problems for neural machine translation models. In this work, we improve the original model from two perspectives. First, we employ a shared reconstructor to better exploit encoder and decoder representations.",
  "y": "extends"
 },
 {
  "id": "ab6f114b2ce4e62e6d8a639e8183eb_3",
  "x": "When translating sentences from a pro-drop language into a non-pro-drop language (e.g. Chinese-to-English), translation models generally fail to translate invisible dropped pronouns (DPs). This phenomenon leads to various translation problems in terms of completeness, syntax and even semantics of translations. A number of approaches have been investigated for DP translation (Le Nagard and Koehn, 2010; Xiang et al., 2013; Wang et al., 2016<cite> Wang et al., , 2018</cite> .",
  "y": "background"
 },
 {
  "id": "ab6f114b2ce4e62e6d8a639e8183eb_4",
  "x": "This phenomenon leads to various translation problems in terms of completeness, syntax and even semantics of translations. A number of approaches have been investigated for DP translation (Le Nagard and Koehn, 2010; Xiang et al., 2013; Wang et al., 2016<cite> Wang et al., , 2018</cite> . <cite>Wang et al. (2018)</cite> is a pioneering work to model DP translation for neural machine trans- * Zhaopeng Tu is the corresponding author of the paper.",
  "y": "background"
 },
 {
  "id": "ab6f114b2ce4e62e6d8a639e8183eb_5",
  "x": "In this work, we propose to improve the original model from two perspectives. First, we use a shared reconstructor to read hidden states from both encoder and decoder. Second, we integrate a DP predictor into NMT to jointly learn to translate and predict DPs. Incorporating these as two auxiliary loss terms can guide both the encoder and decoder states to learn critical information relevant to DPs. Experimental results on a largescale Chinese-English subtitle corpus show that the two modifications can accumulatively improve translation performance, and the best result is +1.5 BLEU points better than that reported by <cite>Wang et al. (2018)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "ab6f114b2ce4e62e6d8a639e8183eb_6",
  "x": "As shown in Figure 1 , <cite>Wang et al. (2018)</cite> introduced two independent reconstructors with their own parameters, which reconstruct the DPannotated source sentence from the encoder and decoder hidden states, respectively. The central Table 1 : Evaluation of external models on predicting the positions of DPs (\"DP Position\") and the exact words of DP (\"DP Words\"). idea underpinning their approach is to guide the corresponding hidden states to embed the recalled source-side DP information and subsequently to help the NMT model generate the missing pronouns with these enhanced hidden representations.",
  "y": "background"
 },
 {
  "id": "ab6f114b2ce4e62e6d8a639e8183eb_7",
  "x": "These annotated source sentences can be used to build a neural-based DP predictor, which can be used to annotate test sentences since the target sentence is not available during the testing phase. As shown in Table 1 , Wang et al. (2016<cite> Wang et al. ( , 2018</cite> explored to predict the exact DP words 1 , the accuracy of which is only 66% in F1-score. By analyzing the translation outputs, we found that 16.2% of errors are newly introduced and caused by errors from the DP predictor.",
  "y": "background"
 },
 {
  "id": "ab6f114b2ce4e62e6d8a639e8183eb_8",
  "x": "Different from <cite>Wang et al. (2018)</cite>, we reconstruct DPP-annotated source sentence, which is predicted by an external model. ---------------------------------- **EXPERIMENT**",
  "y": "differences"
 },
 {
  "id": "ab6f114b2ce4e62e6d8a639e8183eb_9",
  "x": "To compare our work with the results reported by previous work<cite> (Wang et al., 2018)</cite> , we conducted experiments on their released Chinese\u21d2English TV Subtitle corpus. 2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively. We used case-insensitive 4-gram NIST BLEU metrics (Papineni et al., 2002) for evaluation, and sign-test (Collins et al., 2005) to test for statistical significance.",
  "y": "uses"
 },
 {
  "id": "ab6f114b2ce4e62e6d8a639e8183eb_10",
  "x": "We used case-insensitive 4-gram NIST BLEU metrics (Papineni et al., 2002) for evaluation, and sign-test (Collins et al., 2005) to test for statistical significance. We implemented our models on the code repository released by <cite>Wang et al. (2018)</cite> . 3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results.",
  "y": "uses"
 },
 {
  "id": "ab6f114b2ce4e62e6d8a639e8183eb_11",
  "x": "We implemented our models on the code repository released by <cite>Wang et al. (2018)</cite> . 3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results. It should be emphasized that we did not use the pre-train strategy as done in <cite>Wang et al. (2018)</cite> , since we found training from scratch achieved a better performance in the shared reconstructor setting.",
  "y": "differences"
 },
 {
  "id": "ab6f114b2ce4e62e6d8a639e8183eb_12",
  "x": "Baselines (Rows 1-4): The three baselines (Rows 1, 2, and 4) differ regarding the training data used. \"Separate-Recs\u21d2(+DPs)\" (Row 3) is the best model reported in <cite>Wang et al. (2018)</cite> , which we employed as another strong baseline. The baseline trained on the DPP-annotated data (\"Baseline (+DPPs)\", Row 4) outperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating DPs.",
  "y": "uses"
 },
 {
  "id": "ab6f114b2ce4e62e6d8a639e8183eb_13",
  "x": "Introducing a joint prediction objective (Row 6) can achieve a further improvement of +0.61 BLEU points. These results verify that shared reconstructor and jointly predicting DPs can accumulatively improve translation performance. Among the variations of shared reconstructors (Rows 6-8), we found that an interaction attention from encoder to decoder (Row 7) achieves the best performance, which is +3.45 BLEU points better than our baseline (Row 4) and +1.45 BLEU points better than the best result reported by <cite>Wang et al. (2018)</cite> (Row 3) .",
  "y": "differences"
 },
 {
  "id": "ab6f114b2ce4e62e6d8a639e8183eb_14",
  "x": "We attribute the superior performance of \"Shared-Rec enc\u2192dec \" to the fact that the attention context over encoder representations embeds useful DP information, which can help to better attend to the representations of the corresponding pronouns in the decoder side. Similar to <cite>Wang et al. (2018)</cite> , the proposed approach improves BLEU scores at the cost of decreased training and decoding speed, which is due to the large number of newly introduced parameters resulting from the incorporation of reconstructors into the NMT model. Table 3 : Evaluation of DP prediction accuracy. \"External\" model is separately trained on DP-annotated data with external neural methods (Wang et al., 2016) , while \"Joint\" model is jointly trained with the NMT model (Section 3.2).",
  "y": "similarities"
 },
 {
  "id": "ab6f114b2ce4e62e6d8a639e8183eb_15",
  "x": "Table 4 lists translation results when the reconstruction model is used in training only. We can see that the proposed model outperforms both the strong baseline and the best model reported in <cite>Wang et al. (2018)</cite> . This is encouraging since no extra resources and computation are introduced to online decoding, which makes the approach highly practical, for example for translation in industry applications.",
  "y": "differences"
 },
 {
  "id": "ab8c43bf5a37c436d166960af459a8_0",
  "x": "It has been recently studied in two distinct settings: (1) <cite>Rao and Tetreault (2018)</cite> addressed the task of Formality Transfer (FT) where given an informal sentence in English, systems are asked to output a formal equivalent, or vice-versa; (2) introduced the task of FormalitySensitive Machine Translation (FSMT), where given a sentence in French and a desired formality level (approximating the intended audience of the translation), systems are asked to produce an English translation of the desired formality level. While FT and FSMT can both be framed as Machine Translation (MT), appropriate training examples are much harder to obtain than for traditional machine translation tasks. FT requires sentence pairs that express the same meaning in two different styles, which rarely occur naturally and are therefore only available in small quantities.",
  "y": "background"
 },
 {
  "id": "ab8c43bf5a37c436d166960af459a8_1",
  "x": "This state of affairs recently changed, with the introduction of the first large scale parallel corpus for formality transfer, GYAFC (Grammarly's Yahoo Answers Formality Corpus). 110K informal sentences were collected from Yahoo Answers and they were rewritten in a formal style via crowd-sourcing, which made it possible to benchmark style transfer systems based on both PBMT and NMT models<cite> (Rao and Tetreault, 2018)</cite> . In this work, we leverage this corpus to enable multi-task FT and FSMT.",
  "y": "background"
 },
 {
  "id": "ab8c43bf5a37c436d166960af459a8_2",
  "x": "This state of affairs recently changed, with the introduction of the first large scale parallel corpus for formality transfer, GYAFC (Grammarly's Yahoo Answers Formality Corpus). 110K informal sentences were collected from Yahoo Answers and they were rewritten in a formal style via crowd-sourcing, which made it possible to benchmark style transfer systems based on both PBMT and NMT models<cite> (Rao and Tetreault, 2018)</cite> . In this work, we leverage this corpus to enable multi-task FT and FSMT.",
  "y": "uses"
 },
 {
  "id": "ab8c43bf5a37c436d166960af459a8_3",
  "x": "<cite>Rao and Tetreault (2018)</cite> used independent neural machine translation models for each formality transfer direction (informal\u2192formal and formal\u2192informal). Inspired by the bi-directional NMT for low-resource languages , we propose a unified model that can handle either direction -we concatenate the parallel data from the two directions of formality transfer and attach a tag to the beginning of each source sentence denoting the desired target formality level i.e. <F> for transferring to formal and <I> for transferring to informal. This enables our FT model to learn to transfer to the correct style via attending to the tag in the source embedding.",
  "y": "background"
 },
 {
  "id": "ab8c43bf5a37c436d166960af459a8_4",
  "x": "**BI-DIRECTIONAL FORMALITY TRANSFER** <cite>Rao and Tetreault (2018)</cite> used independent neural machine translation models for each formality transfer direction (informal\u2192formal and formal\u2192informal). Inspired by the bi-directional NMT for low-resource languages , we propose a unified model that can handle either direction -we concatenate the parallel data from the two directions of formality transfer and attach a tag to the beginning of each source sentence denoting the desired target formality level i.e. <F> for transferring to formal and <I> for transferring to informal.",
  "y": "uses"
 },
 {
  "id": "ab8c43bf5a37c436d166960af459a8_5",
  "x": "---------------------------------- **EXPERIMENTAL SETUP** FT data: We use the GYAFC corpus introduced by <cite>Rao and Tetreault (2018)</cite> as our FT data.",
  "y": "uses"
 },
 {
  "id": "ab8c43bf5a37c436d166960af459a8_6",
  "x": "We report case-sensitive BLEU with standard WMT tokenization. 2 For FT, <cite>Rao and Tetreault (2018)</cite> show that BLEU correlates well with the overall system ranking assigned by humans. For FSMT, BLEU is an imperfect metric as it conflates mismatches due to translation errors and due to correct style variations.",
  "y": "background"
 },
 {
  "id": "ab8c43bf5a37c436d166960af459a8_7",
  "x": "We evaluate both FT and FSMT tasks using BLEU (Papineni et al., 2002) , which compares the model output with four reference target-style rewrites for FT and a single reference translation for FSMT. We report case-sensitive BLEU with standard WMT tokenization. 2 For FT, <cite>Rao and Tetreault (2018)</cite> show that BLEU correlates well with the overall system ranking assigned by humans.",
  "y": "uses"
 },
 {
  "id": "ab8c43bf5a37c436d166960af459a8_8",
  "x": "---------------------------------- **HUMAN EVALUATION** Following <cite>Rao and Tetreault (2018)</cite>, we assess model outputs on three criteria: formality, fluency and meaning preservation.",
  "y": "uses"
 },
 {
  "id": "ab8c43bf5a37c436d166960af459a8_9",
  "x": "Since the goal of our evaluation is to compare models, our evaluation scheme asks workers to compare sentence pairs on these three criteria instead of rating each sentence in isolation. We collect human judgments using CrowdFlower on 300 samples of each model outputs. For FT, we compare the top performing NMT benchmark model in <cite>Rao and Tetreault (2018)</cite> with our best FT model.",
  "y": "uses"
 },
 {
  "id": "ab8c43bf5a37c436d166960af459a8_10",
  "x": "---------------------------------- **INFORMAL\u2192FORMAL FORMAL\u2192INFORMAL** 6 Formality Transfer Experiments 6.1 Baseline Models from <cite>Rao and Tetreault (2018)</cite> PBMT is a phrase-based machine translation model trained on the GYAFC corpus using a training regime consisting of self-training, data sub-selection and a large language model.",
  "y": "uses background"
 },
 {
  "id": "ab8c43bf5a37c436d166960af459a8_11",
  "x": "6 Formality Transfer Experiments 6.1 Baseline Models from <cite>Rao and Tetreault (2018)</cite> PBMT is a phrase-based machine translation model trained on the GYAFC corpus using a training regime consisting of self-training, data sub-selection and a large language model. NMT Baseline uses OpenNMT-py (Klein et al., 2017) . <cite>Rao and Tetreault (2018)</cite> use a pre-processing step to make source informal sentences more formal and source formal sentences more informal by rules such as re-casing.",
  "y": "background"
 },
 {
  "id": "ab8c43bf5a37c436d166960af459a8_12",
  "x": "**RESULTS** Automatic Evaluation. As shown in Table 1 , our NMT baselines yield surprisingly better BLEU scores than those of <cite>Rao and Tetreault (2018)</cite> , even without using rule-processed source training data and pretrained word embeddings.",
  "y": "differences"
 },
 {
  "id": "ab8c43bf5a37c436d166960af459a8_13",
  "x": "---------------------------------- **QUALITATIVE ANALYSIS** We manually inspect 100 randomly selected samples from our evaluation set and compare the target-style output of our best model (MultiTask-tag-style) with that of the best baseline model (NMT-Combined) from <cite>Rao and Tetreault (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "ab8c43bf5a37c436d166960af459a8_14",
  "x": "We explored the use of multi-task learning to jointly perform monolingual FT and bilingual FSMT. Using French-English translation and English style transfer data, we showed that the joint model is able to learn from both style transfer parallel examples and translation parallel examples. On the FT task, the joint model significantly improves the quality of transfer between formal and informal styles in both directions, compared to prior work<cite> (Rao and Tetreault, 2018</cite> ).",
  "y": "differences"
 },
 {
  "id": "ab919bfae9ddf780cadcd491fe0a9b_0",
  "x": "In the first surface realization shared task (Belz et al., 2011, henceforth SR-11) , which aimed to ameliorate these difficulties, attempts to use grammar-based realizers were unsuccessful, as converting shared task inputs to systemnative inputs turned out to be more difficult than anticipated. Subsequently, Narayan & Gardent (2012) demonstrated that grammarbased systems can be substantially improved with error mining techniques, and Gardent and Narayan (2013) showed that augmenting the (shallow) SR-11 representation of coordination to include shared dependencies can benefit grammar-based realizers. <cite>White (2014)</cite> then showed that even better results can be achieved by inducing a grammar (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013) that is directly compatible with (an enhanced version of) the SR-11 inputs.",
  "y": "background"
 },
 {
  "id": "ab919bfae9ddf780cadcd491fe0a9b_1",
  "x": "<cite>White (2014)</cite> then showed that even better results can be achieved by inducing a grammar (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013) that is directly compatible with (an enhanced version of) the SR-11 inputs. However, as explained below, subsequent analysis revealed substantial remaining issues with the data, which this paper takes a step towards addressing. A common thread in work on reversible, constraint-based grammars is emphasis on properly representing unbounded dependencies and coordination.",
  "y": "background motivation"
 },
 {
  "id": "ab919bfae9ddf780cadcd491fe0a9b_2",
  "x": "For parsing, this emphasis has been shown to pay off in improved recall of unbounded dependencies (Rimell et al., 2009; Nguyen et al., 2012; Oepen et al., 2014) . For realization, however, it remains an open question as to whether approaches based on constraintbased grammars can likewise yield an empirical payoff, given the continuing lack of a common input representation that adequately treats unbounded dependencies and coordination, as these grammars require. With this issue in mind, <cite>White (2014)</cite> experimented with a version of the shallow SR-11 inputs (created by Richard Johansson) which included extra dependencies for unbounded dependencies and coordination, yielding dependency graphs extending core dependency trees.",
  "y": "background"
 },
 {
  "id": "ab919bfae9ddf780cadcd491fe0a9b_3",
  "x": "We have adapted and extended<cite> White's (2014)</cite> CCG induction algorithm to work with the augmented UDs that our system produces. White's algorithm assumed CCG phrases are only rarely projected from a dependent rather than a heade.g., where an NP is projected from a determiner, which is a dependent of the head nounand thus could be easily handled by handcrafted lexical entries. Since such cases are very common in UDs, the algorithm needed to be extended to induce such categories automatically.",
  "y": "extends"
 },
 {
  "id": "abfc6373c577980154dbb93190d69b_0",
  "x": "Part-of-speech (POS) tagging has received a great deal of attention as it is a critical component of most natural language processing systems. In particular, there has been growing interest in both multilingual POS induction ) and cross-lingual POS induction via treebank projection (Yarowsky and Ngai, 2001; Xi and Hwa, 2005; <cite>Das and Petrov, 2011)</cite> .",
  "y": "background"
 },
 {
  "id": "abfc6373c577980154dbb93190d69b_1",
  "x": "Underlying these studies is the idea that a set of (coarse) syntactic POS categories exist in similar forms across languages. These categories are often called universals to represent their cross-lingual nature (Carnie, 2002; Newmeyer, 2005) . When corpora with common tagsets are unavailable, a standard approach is to manually define a mapping from language and treebank specific fine-grained tagsets to a predefined universal set. This was the approach taken by<cite> Das and Petrov (2011)</cite> to evaluate their cross-lingual POS projection system for six different languages.",
  "y": "background"
 },
 {
  "id": "abfc6373c577980154dbb93190d69b_2",
  "x": "Second, we combine the cross-lingual projection part-of-speech taggers of<cite> Das and Petrov (2011)</cite> with the grammar induction system of Naseem et al. (2010) -which requires a universal tagset -to produce a completely unsupervised grammar induction system for multiple languages, that does not require gold POS tags in the target language. ---------------------------------- **TAGSET**",
  "y": "extends"
 },
 {
  "id": "abfc6373c577980154dbb93190d69b_3",
  "x": "In our experiments, we did not make use of refined categories, as the POS tags induced by<cite> Das and Petrov (2011)</cite> were all coarse. We present results on the same eight IndoEuropean languages as<cite> Das and Petrov (2011)</cite> , so that we can make use of their automatically projected POS tags. For all languages, we used the treebanks released as a part of the CoNLL-X (Buchholz and Marsi, 2006) shared task.",
  "y": "uses"
 },
 {
  "id": "abfc6373c577980154dbb93190d69b_4",
  "x": "Additionally, they also used refined categories in the form of CoNLL treebank tags. In our experiments, we did not make use of refined categories, as the POS tags induced by<cite> Das and Petrov (2011)</cite> were all coarse. We present results on the same eight IndoEuropean languages as<cite> Das and Petrov (2011)</cite> , so that we can make use of their automatically projected POS tags.",
  "y": "uses"
 },
 {
  "id": "ad770c1b473bc39e6ec989338357e6_0",
  "x": "Bilingual word embeddings have attracted a lot of attention in recent times (Zou et al., 2013; Ko\u010disk\u00fd et al., 2014; Chandar A P et al., 2014; Gouws et al., 2014; Gouws and S\u00f8gaard, 2015; Luong et al., 2015; Wick et al., 2016) . A common approach to obtain them is to train the embeddings in both languages independently and then learn a mapping that minimizes the distances between equivalences listed in a bilingual dictionary. The learned transformation can also be applied to words missing in the dictionary, which can be used to induce new translations with a direct application in machine translation <cite>(Mikolov et al., 2013b</cite>; Zhao et al., 2015) .",
  "y": "background"
 },
 {
  "id": "ad770c1b473bc39e6ec989338357e6_1",
  "x": "The first method to learn bilingual word embedding mappings was proposed by <cite>Mikolov et al. (2013b)</cite> , <cite>who</cite> learn the linear transformation that minimizes the sum of squared Euclidean distances for the dictionary entries. Subsequent work has proposed alternative optimization objectives to learn better mappings. Xing et al. (2015) incorporate length normalization in the training of word embeddings and try to maximize the cosine similarity instead, introducing an orthogonality constraint to preserve the length normalization after the projection.",
  "y": "background"
 },
 {
  "id": "ad770c1b473bc39e6ec989338357e6_2",
  "x": "Beyond linear mappings, Lu et al. (2015) apply deep canonical correlation analysis to learn a nonlinear transformation for each language. Finally, additional techniques have been used to address the hubness problem in <cite>Mikolov et al. (2013b)</cite> , both through the neighbor retrieval method and the training itself . We leave the study of non-linear transformation and other additions for further work.",
  "y": "background"
 },
 {
  "id": "ad770c1b473bc39e6ec989338357e6_3",
  "x": "We leave the study of non-linear transformation and other additions for further work. In this paper, we propose a general framework to learn bilingual word embeddings. We start with a basic optimization objective <cite>(Mikolov et al., 2013b)</cite> and introduce several meaningful and intuitive constraints that are equivalent or closely related to previously proposed methods (Faruqui and Dyer, 2014; Xing et al., 2015) .",
  "y": "extends"
 },
 {
  "id": "ad770c1b473bc39e6ec989338357e6_4",
  "x": "Let X and Z denote the word embedding matrices in two languages for a given bilingual dictionary so that their ith row X i * and Z i * are the word embeddings of the ith entry in the dictionary. Our goal is to find a linear transformation matrix W so that XW best approximates Z, which we formalize minimizing the sum of squared Euclidean distances following <cite>Mikolov et al. (2013b)</cite> : Alternatively, this is equivalent to minimizing the (squared) Frobenius norm of the residual matrix:",
  "y": "similarities"
 },
 {
  "id": "ad770c1b473bc39e6ec989338357e6_5",
  "x": "This last optimization objective coincides with Xing et al. (2015) , but their work was motivated by an hypothetical inconsistency in <cite>Mikolov et al. (2013b)</cite> , where the optimization objective to learn word embeddings uses dot product, the objective to learn mappings uses Euclidean distance and the similarity computations use cosine. However, the fact is that, as long as W is orthogonal, optimizing the squared Euclidean distance of length-normalized embeddings is equivalent to optimizing the cosine, and therefore, the mapping objective proposed by Xing et al. (2015) is equivalent to that used by <cite>Mikolov et al. (2013b)</cite> with orthogonality constraint and unit vectors. In fact, our experiments show that orthogonality is more relevant than length normalization, in contrast to Xing et al. (2015) , who introduce orthogonality only to ensure that unit length is preserved after mapping.",
  "y": "background"
 },
 {
  "id": "ad770c1b473bc39e6ec989338357e6_6",
  "x": "As long as W is orthogonal, this is equivalent to maximizing the sum of cosine similarities for the dictionary entries, which is commonly used for similarity computations: This last optimization objective coincides with Xing et al. (2015) , but their work was motivated by an hypothetical inconsistency in <cite>Mikolov et al. (2013b)</cite> , where the optimization objective to learn word embeddings uses dot product, the objective to learn mappings uses Euclidean distance and the similarity computations use cosine. However, the fact is that, as long as W is orthogonal, optimizing the squared Euclidean distance of length-normalized embeddings is equivalent to optimizing the cosine, and therefore, the mapping objective proposed by Xing et al. (2015) is equivalent to that used by <cite>Mikolov et al. (2013b)</cite> with orthogonality constraint and unit vectors.",
  "y": "background"
 },
 {
  "id": "ad770c1b473bc39e6ec989338357e6_7",
  "x": "**EXPERIMENTS** In this section, we experimentally test the proposed framework and all its variants in comparison with related methods. For that purpose, we use the translation induction task introduced by <cite>Mikolov et al. (2013b)</cite> , which learns a bilingual mapping on a small dictionary and measures its accuracy on predicting the translation of new words.",
  "y": "uses"
 },
 {
  "id": "ad770c1b473bc39e6ec989338357e6_8",
  "x": "Unfortunately, the dataset <cite>they</cite> use is not public. For that reason, we use the English-Italian dataset on the same task provided by 2 . The dataset contains monolingual word embeddings trained with the word2vec toolkit using the CBOW method with negative sampling (Mikolov et al., 2013a) 3 .",
  "y": "uses"
 },
 {
  "id": "ad770c1b473bc39e6ec989338357e6_9",
  "x": "The code for <cite>Mikolov et al. (2013b)</cite> and Xing et al. (2015) is not publicly available, so we implemented and tested them as part of the proposed framework, which only differs from the original systems in the optimization method (exact solution instead of gradient descent) and the length normalization approach in the case of Xing et al. (2015) (postprocessing instead of constrained training). As for the method by Faruqui and Dyer (2014) , we used their original implementation in Python and MAT-LAB 6 , which we extended to cover cases where the dictionary contains more than one entry for the same word. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "ad770c1b473bc39e6ec989338357e6_11",
  "x": "The contribution of length normalization alone is marginal, but when followed by mean centering we obtain further improvements in bilingual performance without hurting monolingual performance. Table 2 shows the results for our best performing configuration in comparison to previous work. As discussed before, <cite>(Mikolov et al., 2013b)</cite> and (Xing et al., 2015) were implemented as part of our framework, so <cite>they</cite> correspond to our uncostrained mapping with no preprocessing and orthogonal mapping with length normalization, respectively.",
  "y": "similarities"
 },
 {
  "id": "ad770c1b473bc39e6ec989338357e6_12",
  "x": "---------------------------------- **COMPARISON TO OTHER WORK** As it can be seen, the method by Xing et al. (2015) performs better than that of <cite>Mikolov et al. (2013b)</cite> in the translation induction task, which is in line with what they report in their paper.",
  "y": "background"
 },
 {
  "id": "ad770c1b473bc39e6ec989338357e6_13",
  "x": "**COMPARISON TO OTHER WORK** As it can be seen, the method by Xing et al. (2015) performs better than that of <cite>Mikolov et al. (2013b)</cite> in the translation induction task, which is in line with what they report in their paper. Moreover, thanks to the orthogonality constraint their monolingual performance in the word analogy task does not degrade, whereas the accuracy of <cite>Mikolov et al. (2013b)</cite> drops by 2.86% in absolute terms with respect to the original embeddings.",
  "y": "background"
 },
 {
  "id": "ad770c1b473bc39e6ec989338357e6_15",
  "x": "Moreover, it does not suffer from any considerable degradation in monolingual quality, with an anecdotal drop of only 0.07% in contrast with 2.86% for <cite>Mikolov et al. (2013b)</cite> and 7.02% for Faruqui and Dyer (2014) . When compared to Xing et al. (2015) , our results in Table 1 reinforce our theoretical interpretation for their method (cf. Section 2.2), as it empirically shows that its improvement with respect to <cite>Mikolov et al. (2013b)</cite> comes solely from the orthogonality constraint, and not from solving any inconsistency. It should be noted that the implementation by Faruqui and Dyer (2014) also length-normalizes the word embeddings in a preprocessing step.",
  "y": "differences"
 },
 {
  "id": "ad770c1b473bc39e6ec989338357e6_16",
  "x": "Moreover, it does not suffer from any considerable degradation in monolingual quality, with an anecdotal drop of only 0.07% in contrast with 2.86% for <cite>Mikolov et al. (2013b)</cite> and 7.02% for Faruqui and Dyer (2014) . When compared to Xing et al. (2015) , our results in Table 1 reinforce our theoretical interpretation for their method (cf. Section 2.2), as it empirically shows that its improvement with respect to <cite>Mikolov et al. (2013b)</cite> comes solely from the orthogonality constraint, and not from solving any inconsistency. It should be noted that the implementation by Faruqui and Dyer (2014) also length-normalizes the word embeddings in a preprocessing step.",
  "y": "differences"
 },
 {
  "id": "ad9b663ac88667c1b88767ca4b2f8f_0",
  "x": "**INTRODUCTION** Target language side dependency structures have been successfully used in statistical machine translation (SMT) by<cite> Shen et al. (2008)</cite> and achieved state-of-the-art results as reported in the NIST 2008 Open MT Evaluation workshop and the NTCIR-9 Chinese-to-English patent translation task (Goto et al., 2011; Ma and Matsoukas, 2011) . A primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions, which arise due to longdistance dependencies or in languages where grammatical relations are often signaled by morphology instead of word order (McDonald and Nivre, 2011) .",
  "y": "background"
 },
 {
  "id": "ad9b663ac88667c1b88767ca4b2f8f_1",
  "x": "A semantic dependency representation of a whole sentence, predicate-argument structures (PASs), are also included in the output trees of (1) a state-of-the-art head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994; Sag et al., 2003) parser, Enju 1 (Miyao and Tsujii, 2008) and (2) a state-of-the-art CCG parser 2 (Clark and Curran, 2007) . The motivation of this paper is to investigate the impact of these non-isomorphic dependency structures to be used for SMT. That is, we would like to provide a comparative evaluation of these dependencies in a string-to-dependency decoder<cite> (Shen et al., 2008)</cite> .",
  "y": "background motivation"
 },
 {
  "id": "ad9b663ac88667c1b88767ca4b2f8f_2",
  "x": "A dependency graph G for sentence s is called a dependency tree when it satisfies, (1) the nodes cover all the words in s besides the ROOT; (2) one node can have one and only one head (word) with a determined syntactic role; and (3) the ROOT of the graph is reachable from all other nodes. For extracting string-to-dependency transfer rules, we use well-formed dependency structures, either fixed or floating, as defined in<cite> (Shen et al., 2008)</cite> . Similarly, we ignore the syntactic roles both during rule extracting and target dependency language model (LM) training.",
  "y": "uses"
 },
 {
  "id": "ad9b663ac88667c1b88767ca4b2f8f_3",
  "x": "For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007) . In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool 3 written by Johansson and Nugues (2007) . The head finding rules 4 are according to Magerman (1995) and Collins (1997) . Similar approach has been originally used by<cite> Shen et al. (2008)</cite> .",
  "y": "similarities background"
 },
 {
  "id": "ad9b663ac88667c1b88767ca4b2f8f_4",
  "x": "We re-implemented the string-to-dependency decoder described in<cite> (Shen et al., 2008</cite> (Pauls and Klein, 2011) , was employed to train (1) a five-gram LM on the Xinhua portion of LDC English Gigaword corpus v3 (LDC2007T07) and (2) a tri-gram dependency LM on the English dependency structures of the training data. We report the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002) . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "ada92083e8c012c328d5b6172b76ad_0",
  "x": "Fine-grained opinion mining aims to detect structured user opinions in text, which has drawn much attention in the natural language processing (NLP) community (Kim and Hovy, 2006; Breck et al., 2007; <cite>Ruppenhofer et al., 2008</cite>; Wilson et al., 2009; Qiu et al., 2011; Cardie, 2013, 2014; Liu et al., 2015; Wiegand et al., 2016) . A structured opinion includes the key arguments of one opinion, such as expressions, holders and targets (Breck et al., 2007; Cardie, 2012, 2013; Katiyar and Cardie, 2016 ). Here we focus on opinion role labeling (ORL) (Marasovi\u0107 and Frank, 2018) , which identifies opinion holders and * Corresponding author.",
  "y": "background"
 },
 {
  "id": "ada92083e8c012c328d5b6172b76ad_1",
  "x": "The focused task behaves very similar with semantic role labeling (SRL) which identifies the core semantic roles for given predicates. Earlier work attempts to exploit a well-trained SRL model to recognize possible semantic roles for a given opinion expression, and then map the semantic roles into opinion roles (Kim and Hovy, 2006;<cite> Ruppenhofer et al., 2008)</cite> . The heuristic approach is unable to obtain high performance for ORL because there are large mismatches between SRL and ORL.",
  "y": "background"
 },
 {
  "id": "ada92083e8c012c328d5b6172b76ad_2",
  "x": "In addition, we compare this method with two other representative methods of SRL integration as well: one uses discrete SRL outputs as features directly for ORL and the other one exploits a multi-tasklearning (MTL) framework to benefit ORL by SRL information. Experiments are conducted on the MPQA 2.0 dataset, which is a standard benchmark for opinion mining. Results show that SRL is highly effective for ORL, which is consistent with previous findings (Kim and Hovy, 2006; <cite>Ruppenhofer et al., 2008</cite>; Marasovi\u0107 and Frank, 2018) .",
  "y": "similarities"
 },
 {
  "id": "ada92083e8c012c328d5b6172b76ad_3",
  "x": "SRL aims to find the core semantic arguments for a given predicate, which is highly correlative with the ORL task. The semantic roles agent (ARG0) and patient (ARG1) are often corresponding to the opinion holder and target, respectively. Several works even directly transfer semantic roles into opinion roles for ORL (Kim and Hovy, 2006;<cite> Ruppenhofer et al., 2008)</cite> , treating opinion expressions as the major predicates.",
  "y": "background"
 },
 {
  "id": "ada92083e8c012c328d5b6172b76ad_4",
  "x": "Several works even directly transfer semantic roles into opinion roles for ORL (Kim and Hovy, 2006;<cite> Ruppenhofer et al., 2008)</cite> , treating opinion expressions as the major predicates. These systems can achieve good performances, indicating that SRL information can be greatly useful for ORL. Here we propose a novel method to encode the SRL information implicitly, enhancing ORL model with semantic-aware word representations from a neural SRL model (SRL-SAWR).",
  "y": "motivation"
 },
 {
  "id": "ada92083e8c012c328d5b6172b76ad_5",
  "x": "The results show that SRL information is very helpful for ORL, which is consistent with previous studies (Kim and Hovy, 2006; <cite>Ruppenhofer et al., 2008</cite>; Marasovi\u0107 and Frank, 2018) . The implicit SRL-SAWR method is highly effective to integrate SRL information into the ORL model. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "ada92083e8c012c328d5b6172b76ad_6",
  "x": "Considering the much larger scale of annotated SRL corpora, SRL can benefit ORL potentially. According to the above findings, we design a simple system by mapping SRL outputs into ORL directly (Kim and Hovy, 2006;<cite> Ruppenhofer et al., 2008)</cite> . We simply convert the semantic role ARG0 into holder, and ARG1 into target.",
  "y": "uses"
 },
 {
  "id": "ae67018df3a74e0fd4ae90522499a3_0",
  "x": "Although a significant number of studies (e.g. (Huang et al., 2013; Ganguly et al., 2015; Zheng and Callan, 2015; Guo et al., 2016; Zamani and Croft, 2016;<cite> Dehghani et al., 2017</cite>; ) try to apply neural networks in IR, there have been few studies reporting the performance that is comparable to state-of-the-art IR models. These approaches rely on the general idea that neural network can provide a low-dimensional and semantics-rich representation for both queries and documents. Such a representation can bridge lexical and semantic gaps in traditional IR models.",
  "y": "motivation"
 },
 {
  "id": "ae67018df3a74e0fd4ae90522499a3_1",
  "x": "Neural networks are hungry for data, a fact which also holds for neural IR tasks. One can find from above discussions that the second category of approaches suffer from the data spareness problem, although there have been recent attempts (Gupta et al., 2017;<cite> Dehghani et al., 2017)</cite> trying to pseudo label query-document pairs automatically with unsupervised retrieval models such as BM25. Using pseudo labels as relevance signals relieves data spareness in terms of quantity but not quality.",
  "y": "background"
 },
 {
  "id": "ae67018df3a74e0fd4ae90522499a3_2",
  "x": "More recent works propose to use unsupervised IR models to pseudo label query-document pairs that provide weak supervision for representation learning. <cite>Dehghani et al. (2017)</cite> use BM25 to obtain relevant documents for a large set of AOL queries (Pass et al., 2006) which are then used as weakly supervised signals for joint embedding and ranking model training. employ similar supervision signals as<cite> (Dehghani et al., 2017)</cite> to train an embedding network similar to Word2vec and use the obtained embeddings for query expansion and query classification.",
  "y": "background"
 },
 {
  "id": "ae67018df3a74e0fd4ae90522499a3_4",
  "x": "The supervised training objective l s on a triplet of query-document pair (q, d 1 , d 2 ) can then be defined as the cross entropy loss, which is: where P (d 1 q d 2 ) is the actual probability that d 1 is ranked higher than d 2 according to annotations (i.e. pseudo-labels of query-document pairs). The actual probability in this paper is estimated in a similar way as in<cite> (Dehghani et al., 2017)</cite> , which is:",
  "y": "similarities"
 },
 {
  "id": "ae67018df3a74e0fd4ae90522499a3_5",
  "x": "**DATA SETS** The IR experiments are carried out against standard TREC collections consisting of one Robust track and one Web track, which represent different sizes and genres of heterogeneous text collections. These collections have been broadly used in recent studies (Zheng and Callan, 2015; Guo et al., 2016;<cite> Dehghani et al., 2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "ae67018df3a74e0fd4ae90522499a3_6",
  "x": "The ClueWeb-09-Cat-B collection (or ClueWeb for short) is filtered to the set of documents with spam scores in the 60-th percentile with Waterloo Fusion spam scores 1 . For all TREC queries, we only make use of the title fields for retrieval. In order to build the labeled query-document pairs for supervised learning, we choose to use the more general methodology in (Gupta et al., 2017) instead of the one in<cite> (Dehghani et al., 2017)</cite> to relieve from data (i.e. AOL queries) only available from industrial labs.",
  "y": "differences"
 },
 {
  "id": "ae67018df3a74e0fd4ae90522499a3_7",
  "x": "We set the hyper-parameters of our model by following similar tasks such as<cite> (Dehghani et al., 2017)</cite> . The size and number of hidden layers are respectively selected from {64, 128, 256, 512, 1024} and {1, 2, 3, 4}. The values of \u03b1, \u03b2 in equation 3 are chosen from {0.001, 0.01, 0.1, 1, 10, 100, 1000}. We select the initial learning rate from {10 \u22123 , 10 \u22124 , 5 * 10 \u22124 , 10 \u22125 , 5 * 10 \u22125 }. The batch size for learning is selected from {64, 128, 256, 512}. These model hyper-parameters are tuned on the validation set (20% of the training queries used for validation).",
  "y": "similarities"
 },
 {
  "id": "ae67018df3a74e0fd4ae90522499a3_8",
  "x": "We compare the retrieval performance of our joint learning retrieval model with two categories of IR models: classic IR models showing state-of-the-art performance, and the recent neural ranking models for IR. Since our model is representation-focused rather than interaction-focused, we do not plan to compare our model with those based on relevance matching (Guo et al., 2016) in this paper. More importantly, since our model learns from weakly supervised signals by BM25, we are more interested in the comparisons to BM25 and similar models using weakly supervised signals, an experimental strategy also employed in<cite> (Dehghani et al., 2017)</cite> .",
  "y": "similarities"
 },
 {
  "id": "ae67018df3a74e0fd4ae90522499a3_9",
  "x": "\u2022 DSSM: It is a representative deep matching model proposed in (Huang et al., 2013) , which is a representation-focused model. The model is framed as a feed forward neural network with a word hashing layer. \u2022 NRMS: It is a weakly-supervised neural IR model learned with automatically annotated querydocument pairs<cite> (Dehghani et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "ae67018df3a74e0fd4ae90522499a3_10",
  "x": "Under such considerations, we perform experiments with the following baselines: \u2022 Classic models: The probabilistic BM25 model and query likelihood (QL) model based on Dirichlet smoothing are highly efficient IR models. \u2022 NRMS: It is a weakly-supervised neural IR model learned with automatically annotated querydocument pairs<cite> (Dehghani et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "aebac57baf260be18945feba38d6a1_0",
  "x": "So, for instance, the flower bouquet can be expressed in German as Blumenstr\u00e4u\u00dfe, made up of Blumen (flower) and str\u00e4u\u00dfe (bouquet), and in Finnish as kukkakimppu, from kukka (flower) and kimppu (bunch, collection). For many language processing tools that rely on lexicons or language models it is very useful to be able to decompose compounds to increase their coverage and reduce out-of-vocabulary terms. Decompounders have been used successfully in Information Retrieval (Braschler and Ripplinger, 2004) , Machine Translation (Brown, 2002; <cite>Koehn and Knight, 2003)</cite> and Speech Recognition (Adda-Decker et al., 2000) .",
  "y": "background"
 },
 {
  "id": "aebac57baf260be18945feba38d6a1_1",
  "x": "Because people do not use capitalization consistently when writing queries, all the query logs are lowercased. By randomly sampling keywords we would get few compounds (as their frequency is small compared to that of non-compounds), so we have proceeded in the following way to ensure that the gold-standards contain a substantial amount of compounds: we started by building a very naive decompounder that splits a word in several parts using a frequency-based compound splitting method<cite> (Koehn and Knight, 2003)</cite> . Using this procedure, we obtain two random samples with possibly repeated words: one with words that are considered non-compounds, and the other with words that are considered compounds by this naive approach.",
  "y": "uses"
 },
 {
  "id": "aebac57baf260be18945feba38d6a1_2",
  "x": "The evaluation is done using the metrics precision, recall and accuracy, defined in the following way<cite> (Koehn and Knight, 2003</cite> ): \u2022 Correct splits: no. of compounds that are split correctly. \u2022 Correct non-splits: no. non-compounds that are not split.",
  "y": "uses"
 },
 {
  "id": "aebac57baf260be18945feba38d6a1_3",
  "x": "If the highest scoring splitting contains just one part, it means that w is not a compound. For the first step (calculating every possible splitting), it is common to take into account that modifiers inside compound words sometimes need linking morphemes. Table 2 lists the ones used in our system (Langer, 1998; Marek, 2006; Krott, 1999 Concerning the second step, there is some work that uses, for scoring, additional information such as rules for cognate recognition (Brown, 2002) or sentence-aligned parallel corpora and a translation model, as in the full system described by<cite> Koehn and Knight (2003)</cite> .",
  "y": "uses"
 },
 {
  "id": "aebac57baf260be18945feba38d6a1_4",
  "x": "For the first step (calculating every possible splitting), it is common to take into account that modifiers inside compound words sometimes need linking morphemes. Table 2 lists the ones used in our system (Langer, 1998; Marek, 2006; Krott, 1999 Concerning the second step, there is some work that uses, for scoring, additional information such as rules for cognate recognition (Brown, 2002) or sentence-aligned parallel corpora and a translation model, as in the full system described by<cite> Koehn and Knight (2003)</cite> . When those resources are not available, the most common methods used for compound splitting are using features such as the geometric mean of the frequencies of compound parts in a corpus, as in<cite> Koehn and Knight (2003)</cite> 's back-off method, or learning a language model from a corpus and estimating the probability of each sequence of possible compound parts (Schiller, 2005; Marek, 2006) .",
  "y": "uses"
 },
 {
  "id": "af0c9e20d34a080bac3304ded1f8d6_1",
  "x": "2) Another line of research casts DA recognition as a multi-label classification problem to accommodate the CDA scenario. Qu et al. <cite>[14]</cite> apply a CNN-based text classifier proposed by Kim [8] using a fixed window to represent the context. Although capable of classifying utterances with CDAs, Qu et al. <cite>[14]</cite> 's model only concerns a strictly-local context range and thus cannot include distant information.",
  "y": "background"
 },
 {
  "id": "af0c9e20d34a080bac3304ded1f8d6_2",
  "x": "Although capable of classifying utterances with CDAs, Qu et al. <cite>[14]</cite> 's model only concerns a strictly-local context range and thus cannot include distant information. In this paper, we present a novel neural model that is adapted from Convolutional Recurrent Neural Network (CRNN) to both incorporate the interaction between distant utterances and generalize the DA recognition task to accommodate CDA. Our contributions can be summarized as follows: 1) In our adapted Convolutional Recurrent Neural Network (CRNN) we use the recurrent layers to gather long-range contextual information that are extracted from utterances by the convolutional layers.",
  "y": "motivation"
 },
 {
  "id": "af0c9e20d34a080bac3304ded1f8d6_3",
  "x": "**DATASET** We use the MSDialog-Intent dataset <cite>[14]</cite> to conduct experiments. In the dataset, each of the 10,020 utterances is annotated with a subset of 12 DAs.",
  "y": "uses"
 },
 {
  "id": "af0c9e20d34a080bac3304ded1f8d6_4",
  "x": "The dataset includes plenty of metadata for each utterance, e.g., answer vote and user affiliation. For generalizability, our model only incorporates textual content of the dialogues. Besides, unlike Qu et al. <cite>[14]</cite> , we keep all the DA annotations in the dataset to preserve the meaningful DA structures within and across utterances.",
  "y": "differences"
 },
 {
  "id": "af0c9e20d34a080bac3304ded1f8d6_7",
  "x": "**METRICS** Following previous work <cite>[14]</cite> on multi-label classification, we adopt label-based accuracy (i.e., Hamming score) and micro-F 1 score as our main evaluation metrics. Micro-precision and micro-recall are also reported to assist the analysis.",
  "y": "uses"
 },
 {
  "id": "af9b884710f8198f008a9687153db6_0",
  "x": "Especially, it has been shown that the combination of LSTMs (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) , convolutional neural networks (CNNs) (LeCun et al., 1989) , and word-level CRF achieves the state-of-the-art performance<cite> (Ma and Hovy, 2016)</cite> . Figure 1 shows an overview of the word-level CRF for NER. However, the word-level neural CRF has two main limitations: (1) it captures only first-order word label dependencies thus it cannot capture segment-level information; (2) it is not easy to incorporate dictionary features directly into a wordlevel model since named entities and syntactic chunks consist of multiple words rather than a single word.",
  "y": "background"
 },
 {
  "id": "af9b884710f8198f008a9687153db6_1",
  "x": "Our experiments on chunking and NER demonstrate that our method outperforms conventional word-level neural CRF. 2 Word-level Neural CRF As a baseline method, we use word-level neural CRF proposed by<cite> (Ma and Hovy, 2016)</cite> since their method achieves state-of-the-art performance on NER. Specifically, they propose Bi-directional LSTM-CNN CRF (BLSTM-CNN-CRF) for sequential tagging.",
  "y": "uses motivation"
 },
 {
  "id": "af9b884710f8198f008a9687153db6_2",
  "x": "i is the j-th element of the vector o i . Then, A \u2208 R |T |\u00d7|T | is a transition score matrix, A y i\u22121 ,y i is a transition 1 While<cite> (Ma and Hovy, 2016)</cite> define \u03d5(yi\u22121, yi, oi) = exp(Wy i\u22121 ,y i oi + Ay i\u22121 ,y i ) as the potential function where W is the weight vector corresponding to label pair (yi\u22121, yi), we use the simple potential function here. score for jumping from tag y i\u22121 to y i , and Y indicates all possible paths.",
  "y": "differences"
 },
 {
  "id": "af9b884710f8198f008a9687153db6_3",
  "x": "We describe how to choose the threshold T in Section 4.3. While it has been shown that the CRF layer is required to achieve the state-ofthe-art performance in<cite> Ma and Hovy (2016)</cite> , we observe that the CRF has no significant effect on the final performance for the lattice construction. Therefore, we use BLSTM-CNN (without CRF) as the word-level tagging model in this paper.",
  "y": "differences"
 },
 {
  "id": "af9b884710f8198f008a9687153db6_4",
  "x": "We evaluate our method on two segment-level sequence tagging tasks: NER and text chunking 3 . For NER, we use CoNLL 2003 English NER shared task (Tjong Kim Sang and De Meulder, 2003) . Following previous work<cite> (Ma and Hovy, 2016)</cite> , we use BIOES tagging scheme in the wordlevel tagging model.",
  "y": "uses"
 },
 {
  "id": "af9b884710f8198f008a9687153db6_5",
  "x": "To generate a segment lattice, we train word-level BLSTM-CNN with the same hyper-parameters used in<cite> Ma and Hovy (2016)</cite> level CNN, and 100 dimentional pre-trained word embedding of GloVe (Pennington et al., 2014) . At input layer and output layer, we apply dropout (Srivastava et al., 2014) with rate at 0.5. In our model, we set 400 filters with window size 3 in CNN for segment vector.",
  "y": "uses similarities"
 },
 {
  "id": "af9b884710f8198f008a9687153db6_6",
  "x": "The results of CoNLL 2003 NER is shown in Table 2. By adding a CRF layer to BLSTM-CNN, it improves the F1 score from 89.72 to 90.96. This result is consistent with the result of <cite>(Ma and Hovy, 2016</cite> In both experiments, it improves the F1 score by using segment-level CRF.",
  "y": "similarities"
 },
 {
  "id": "af9b884710f8198f008a9687153db6_7",
  "x": "**RELATED WORK** Several different neural network methods have been proven to be effective for NER (Collobert et al., 2011; Chiu and Nichols, 2016; Lample et al., 2016;<cite> Ma and Hovy, 2016)</cite> . Ma and Hovy (2016) demonstrate that combining LSTM, CNN and CRF achieves the state-of-the-art performance on NER and chunking tasks.",
  "y": "background"
 },
 {
  "id": "afee292717afe1b0dcd77e155a5121_0",
  "x": "<cite>Dakka and Cucerzan (2008)</cite> explored the use of NB and SVM classifiers for categorising Wikipedia. They expanded each article's bag-of-words representation with disambiguated surface forms, as well as terms extracted from its first paragraph, abstract, and any tables present. They also extracted a small amount of context surrounding links to other Wikipedia articles.",
  "y": "background"
 },
 {
  "id": "afee292717afe1b0dcd77e155a5121_1",
  "x": "<cite>Dakka and Cucerzan (2008)</cite> expanded their set of 800 hand-labelled articles using a semisupervised approach, extracting training samples from Wikipedia \"List\" pages -pages that group other articles by type. For each \"List\" page containing a link to an article from the hand-labelled set they used the hand-labelled article's category to classify other articles on the list. They neglected to report how many training instances this left them with, but noted that they maintained the original class distribution of the hand-labelled data.",
  "y": "background"
 },
 {
  "id": "afee292717afe1b0dcd77e155a5121_2",
  "x": "They neglected to report how many training instances this left them with, but noted that they maintained the original class distribution of the hand-labelled data. They achieved an F -score of 89.7% with an SVM classifier and the category set PER, LOC, ORG, MISC and COM (for common nouns) when classifying their full article set. We experimented with a combination of the classification techniques used by <cite>Dakka and Cucerzan (2008)</cite> and the feature extraction methods used by Nothman et al. (2009) and others (Ponzetto and Strube, 2007; Hu et al., 2008; Biadsy et al., 2008) , focusing on the extraction of features from Wikipedia's rich metadata.",
  "y": "uses"
 },
 {
  "id": "afee292717afe1b0dcd77e155a5121_3",
  "x": "We then experimented with a number of different feature extraction methods, focusing primarily on the document structure for identifying useful features. Tokens in the first paragraph were identified by <cite>Dakka and Cucerzan (2008)</cite> as useful features for a machine learner, an idea stemming from the fact that most human annotators will recognise an article's category after reading just the first paragraph. We extended this idea by also marking the first sentence and title tokens as separate from other tokens, as we found that often the first sentence was all that was required for a human annotator to classify an article.",
  "y": "extends"
 },
 {
  "id": "afee292717afe1b0dcd77e155a5121_5",
  "x": "Given that these classifications cannot be considered correct we marked them as classification errors. There were also a number of complications when comparing our system with the system described by <cite>Dakka and Cucerzan (2008)</cite> : they used a different, and substantially smaller, hand-labelled data set; they did not specify how they handled disambiguation pages; they provided no results for experiments using only hand-labelled data, instead incorporating training data produced via their semi-automated approach into the final results; and they neglected to report the final size of the training data produced by their semi-automated annotation. However, these two systems provided the closest benchmarks for comparison.",
  "y": "differences"
 },
 {
  "id": "b0083488650bc98477fb10a9c5a808_0",
  "x": "In contrast to the attention-based sequence-tosequence model, where the encoder states are not updated and the model is not able to re-interpret the encoder states while decoding, this model enables the computation of the encoding of the observation sequence as a function of the previously generated transcribed words. Our model is similar to an architecture used in machine translation described in <cite>[13]</cite> . We believe that the 2DLSTM is able to capture necessary monotonic alignments as well as retrieve coverage concepts internally by its cell states.",
  "y": "similarities"
 },
 {
  "id": "b0083488650bc98477fb10a9c5a808_1",
  "x": "In handwriting recognition (HWR), 2DLSTM has shown successful results in auto- Fig. 1 : The internal architecture of the standard and the 2DLSTM. The additional connections are marked in blue <cite>[13]</cite> . matic extraction of features from raw 2D-images over convolutional neural networks (CNNs) [14] .",
  "y": "background"
 },
 {
  "id": "b0083488650bc98477fb10a9c5a808_2",
  "x": "Recently, the 2DLSTM layer also has been used for sequence-to-sequence modeling in machine translation <cite>[13]</cite> where it implicitly updates the source representation conditioned on the generated target words. In a similar direction, a 2D CNN-based network has been proposed where the positions of the source and the target words define the 2D grid for translation modeling [21] . Similar to <cite>[13]</cite> , we apply a 2DLSTM layer to combine the acoustic model (the LSTM encoder) and the language model (the decoder) without any attention components.",
  "y": "background"
 },
 {
  "id": "b0083488650bc98477fb10a9c5a808_3",
  "x": "Recently, the 2DLSTM layer also has been used for sequence-to-sequence modeling in machine translation <cite>[13]</cite> where it implicitly updates the source representation conditioned on the generated target words. In a similar direction, a 2D CNN-based network has been proposed where the positions of the source and the target words define the 2D grid for translation modeling [21] . Similar to <cite>[13]</cite> , we apply a 2DLSTM layer to combine the acoustic model (the LSTM encoder) and the language model (the decoder) without any attention components.",
  "y": "similarities"
 },
 {
  "id": "b0083488650bc98477fb10a9c5a808_4",
  "x": "Similar to <cite>[13]</cite> , we apply a 2DLSTM layer to combine the acoustic model (the LSTM encoder) and the language model (the decoder) without any attention components. Compared to <cite>[13]</cite> , our model is much deeper. We use max-pooling to select the most relevant encoder state whereas <cite>[13]</cite> uses the last horizontal state of the 2DLSTM.",
  "y": "extends differences"
 },
 {
  "id": "b0083488650bc98477fb10a9c5a808_5",
  "x": "At time step (t, n), it gets an input x t,n , and its computation relies on both the vertical s t,n\u22121 and the horizontal hidden states s t\u22121,n . Besides the input i t,n , the forget f t,n and the output o t,n gates that are similar to those in the LSTM, the 2DLSTM employs an additional lambda gate. As written in Equation 5 , its activation is computed analogously to the other gates<cite> [13,</cite> 11] .",
  "y": "background"
 },
 {
  "id": "b0083488650bc98477fb10a9c5a808_6",
  "x": "Similar to <cite>[13]</cite> , we then equip the network by a 2DLSTM layer to relate the encoder and the decoder states. At time step (t , n), the 2DLSTM receives both the encoder state h t , and the last target embedding vector w n\u22121 , as inputs. One dimension of the 2DLSTM (horizontal-axis in the figure) sequentially reads the encoder states and another (vertical axis) plays the role of the decoder.",
  "y": "similarities"
 },
 {
  "id": "b0083488650bc98477fb10a9c5a808_8",
  "x": "This algorithm is faster than <cite>[13]</cite> , where at each output step, they recompute all previous states of 2DLSTM from scratch which are not required. Table 3 lists the decoding speed of the models to decode the entire development set using a single GPU. In general, the decoding speed of our model is about 6 times slower than that of a standard attention-based model.",
  "y": "differences"
 },
 {
  "id": "b0701d41baf3b355d864f46821f34a_1",
  "x": "Need to be peer-reviewed corpus have been proposed to enhance the features of text, then break through the performance bottleneck of bag-of-words models in short-text tasks. With combination of Conventional Neural Network (CNN) <cite>(Kalchbrenner et al., 2014)</cite> , Recurrent Neural Network (RNN), Recursive Neural Network (Socher et al., 2013) and Attention, hundreds of models had been proposed to model text for further classification, matching (Fan et al., 2017) or other tasks. However, these models are tested in different settings with various datasets, preprocessing and even evaluation.",
  "y": "background motivation"
 },
 {
  "id": "b124e65938672691a5589fb5cdb21e_0",
  "x": "Some of the recent studies on this topic report performance evaluation results of different classifiers using different feature sets (Mohammad et al., 2016b ) while others present publicly-available stance-annotated data sets (Mohammad et al., 2016a; Sobhani et al., 2017;<cite> K\u00fc\u00e7\u00fck, 2017)</cite> . In this study, we present our experiments of using named entities for the purposes of improved stance detection in tweets. We have used the publicly-available tweet data set in Turkish annotated with stance information, together with the results of the corresponding SVM classifiers using unigrams as features in <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> as the baselines.",
  "y": "background"
 },
 {
  "id": "b124e65938672691a5589fb5cdb21e_1",
  "x": "We have used the publicly-available tweet data set in Turkish annotated with stance information, together with the results of the corresponding SVM classifiers using unigrams as features in <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> as the baselines. We first perform NER on this data set and next use the named entities as additional features during our SVM-based stance detection experiments. Our findings are particularly encouraging as they provide evidence for the contribution of a high-performance NER system to the subsequent stance detection procedure using the extracted named entities.",
  "y": "uses"
 },
 {
  "id": "b124e65938672691a5589fb5cdb21e_2",
  "x": "Yet, as emphasized in the related literature (Ritter et al., 2011) , porting existing NER systems to social media texts results in poor performance. Therefore, related studies usually propose customized systems and/or annotated data sets (to be used during the training of supervised systems) for this new text genre. In this study, we have used the stance-annotated tweet data set in Turkish <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> which includes 700 random tweets related to two sports clubs and these clubs constitute the stance targets.",
  "y": "uses"
 },
 {
  "id": "b124e65938672691a5589fb5cdb21e_3",
  "x": "In this study, we have used the stance-annotated tweet data set in Turkish <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> which includes 700 random tweets related to two sports clubs and these clubs constitute the stance targets. The data set is a balanced one in the sense that 175 tweets are in favor of Target-1 and 175 tweets are against Target-1, while 175 tweets are in favor of and 175 are against Target-2. There are no tweet instances annotated with the class Neither in the data set <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "b124e65938672691a5589fb5cdb21e_4",
  "x": "In this study, we have used the stance-annotated tweet data set in Turkish <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> which includes 700 random tweets related to two sports clubs and these clubs constitute the stance targets. There are no tweet instances annotated with the class Neither in the data set <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "b124e65938672691a5589fb5cdb21e_5",
  "x": "---------------------------------- **USING NAMED ENTITIES FOR STANCE DETECTION IN TWEETS** In the current study, we have used the stance-annotated tweet data set described in <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "b124e65938672691a5589fb5cdb21e_6",
  "x": "In the current study, we have used the stance-annotated tweet data set described in <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> . Also presented in <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> are the results of the following experiments on this data set: \u2022 SVM classifiers using unigrams as features,",
  "y": "background"
 },
 {
  "id": "b124e65938672691a5589fb5cdb21e_7",
  "x": "\u2022 SVM classifiers using unigrams and the existence of hashtags in tweets as features. The corresponding results have indicated that using unigrams as features leads to favorable performance rates and using unigrams together with hashtag features improves these results further, while using bigrams as features of the SVM classifiers results in poor performance <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> . The favorable results corresponding to the former two settings are provided in Table 3 as excerpted from <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> , in order to be used as reference results for comparison purposes.",
  "y": "background"
 },
 {
  "id": "b124e65938672691a5589fb5cdb21e_8",
  "x": "The favorable results corresponding to the former two settings are provided in Table 3 as excerpted from <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> , in order to be used as reference results for comparison purposes. These are 10-fold cross validation results on the data set. As can be observed in Table 3 , using the existence of hashtags as an additional feature improves stance detection performance in terms of average F-Measure for Target-2 although it leads to a slight decrease in F-Measure for Target-1 <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "b124e65938672691a5589fb5cdb21e_9",
  "x": "As can be observed in Table 3 , using the existence of hashtags as an additional feature improves stance detection performance in terms of average F-Measure for Target-2 although it leads to a slight decrease in F-Measure for Target-1 <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> . In this study, we investigate the possible contribution of named entities to stance detection in tweets. We do not consider named entity types as features, but instead we use named entities as additional features for SVM classifiers which have used unigrams as features.",
  "y": "uses"
 },
 {
  "id": "b124e65938672691a5589fb5cdb21e_10",
  "x": "Similar to the settings in <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> , we have used the SVM classifier based on the SMO algorithm (Platt, 1999) , available in the Weka tool (Hall et al., 2009 ), during our stance detection experiments. 10-fold crossvalidation results of the classifiers using the named entities extracted by the employed NER tool from the data set are provided in Table 4 while the corresponding results of the classifiers using the named entities in the manually-annotated version of the data set (i.e., the answer key for the NER procedure) are provided in Table 5 . Based on the results presented in Tables 3, 4 , and 5, the following conclusions can be drawn:",
  "y": "similarities"
 },
 {
  "id": "b124e65938672691a5589fb5cdb21e_11",
  "x": "Hence, further experiments are necessary to make sound conclusions regarding joint utilization of named entities and hastags as features of the SVM classifiers for the stance detection task. \u2022 As has been reported in <cite>(K\u00fc\u00e7\u00fck, 2017)</cite> , the overall evaluation results of the stance detection task are considerably higher for the Favor class when compared with the results for the Against class, in all settings given in Table 4 and 5. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "b13bc55709f5040cf100bd5f466ff2_0",
  "x": "Since Tacotron [1] paved the way for end-to-end Text-To-Speech (TTS) using neural networks, researchers have attempted to generate more naturally sounding speech by conditioning a TTS model via speaker and prosody embedding [2, 3, <cite>4,</cite> 5, 6] . ( We use the term prosody as defined in earlier work<cite> [4]</cite> henceforth.) Because there is no available label for prosody, learning to control prosody in TTS is a difficult problem to tackle. Recent approaches learn to extract prosody embedding from reference speech in an unsupervised manner and use prosody embedding to control the speech style<cite> [4,</cite> 5] .",
  "y": "background"
 },
 {
  "id": "b13bc55709f5040cf100bd5f466ff2_1",
  "x": "( We use the term prosody as defined in earlier work<cite> [4]</cite> henceforth.) Because there is no available label for prosody, learning to control prosody in TTS is a difficult problem to tackle. Recent approaches learn to extract prosody embedding from reference speech in an unsupervised manner and use prosody embedding to control the speech style<cite> [4,</cite> 5] . These models have demonstrated ability to generate speech with expressive styles with Tacotron [1] using prosody embedding.",
  "y": "similarities uses"
 },
 {
  "id": "b13bc55709f5040cf100bd5f466ff2_2",
  "x": "Since Tacotron [1] paved the way for end-to-end Text-To-Speech (TTS) using neural networks, researchers have attempted to generate more naturally sounding speech by conditioning a TTS model via speaker and prosody embedding [2, 3, <cite>4,</cite> 5, 6] . ( We use the term prosody as defined in earlier work<cite> [4]</cite> henceforth.) Because there is no available label for prosody, learning to control prosody in TTS is a difficult problem to tackle. Recent approaches learn to extract prosody embedding from reference speech in an unsupervised manner and use prosody embedding to control the speech style<cite> [4,</cite> 5] .",
  "y": "background"
 },
 {
  "id": "b13bc55709f5040cf100bd5f466ff2_3",
  "x": "Problems were reported about hand annotations, and the cost was high [8] . Skerry-Ryan et al. used convolutional neural networks and a Gated Recurrent Unit (GRU) [9] to compress the prosody of the reference speech<cite> [4]</cite> . The output, denoted by p, is fixed-length prosody embedding.",
  "y": "background"
 },
 {
  "id": "b13bc55709f5040cf100bd5f466ff2_4",
  "x": "Another problem was also reported [5] ; fixed-length prosody embedding worked poorly if the length of the reference speech was shorter than the speech to generate. In addition, variablelength prosody embedding was also implemented using the output of the GRU at every time step<cite> [4]</cite> . However, this method did not draw attention because it could not obtain satisfactory results given that it was not robust with regard to text and speaker perturbations.",
  "y": "differences"
 },
 {
  "id": "b13bc55709f5040cf100bd5f466ff2_5",
  "x": "Wang et al. came up with the global style token (GST) Tacotron to encode different speaking styles [5] . Although they used the same reference encoder architecture used in earlier work<cite> [4]</cite> , they did not use p itself for prosody embedding. Using a content-based attention, they computed the attention weights for style tokens from p. The attention weights represent the contribution of each style token, and the weighted sum of the style tokens is now used for style embedding.",
  "y": "extends differences"
 },
 {
  "id": "b13bc55709f5040cf100bd5f466ff2_6",
  "x": "The one-hot speaker identity is converted into speaker embedding vector s by the embedding lookup layer. Equation 1 describes the base encoder-decoder, where e, p, and d denote the text encoder state, variable-length prosody embedding, and decoder state, respectively. Reference speech is encoded to prosody embedding using the reference encoder<cite> [4]</cite> .",
  "y": "background"
 },
 {
  "id": "b13bc55709f5040cf100bd5f466ff2_7",
  "x": "The proposed models are trained identically to the Tacotron model. The model is trained according to the L1 loss between the target spectrogram and the generated spectrogram, and no other supervision is given for the reference encoder. Unless otherwise stated, we used the same hyperparameter settings used in earlier work<cite> [4]</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "b13bc55709f5040cf100bd5f466ff2_8",
  "x": "**DATASET** Previous works<cite> [4,</cite> 5] used large amounts of data to train the prosodic TTS model (296 hours of data for the multi-speaker model). To ensure a large amount of data, we used multiple datasets, in this case VCTK, CMU ARCTIC, and internal datasets.",
  "y": "uses similarities"
 },
 {
  "id": "b13bc55709f5040cf100bd5f466ff2_9",
  "x": "For the quantitative comparison, we used the Mean Cepstral Distortion (MCD) with the first 13 MFCCs, as proposed in earlier work<cite> [4]</cite> . Table 1 shows that the proposed methods outperform GST Tacotron in terms of MCD13, where a lower MCD is better. In particular, speech-side prosody control, which has the highest temporal resolution of prosody embedding, showed the lowest MCD.",
  "y": "similarities uses"
 },
 {
  "id": "b1df73c14f53fea607c4c8b71740fe_0",
  "x": "This property is particularly interesting for NLP where many tasks are aimed at discovering novel information. Recent work has applied such models to various tasks with promising results, e.g. Teh (2006) and Cohn et al. (2009) . <cite>Vlachos et al. (2009)</cite> applied the basic model of this class, the Dirichlet Process Mixture Model (DPMM), to lexical-semantic verb clustering with encouraging results.",
  "y": "background"
 },
 {
  "id": "b1df73c14f53fea607c4c8b71740fe_1",
  "x": "Furthermore, <cite>Vlachos et al. (2009)</cite> used a constrained version of the DPMM in order to guide clustering towards some prior intuition or considerations relevant to the specific task at hand. This supervision was modelled as pairwise constraints between instances and it informs the model of relations between them that cannot be recovered by the model on the basis of the feature representation used. Like other forms of supervision, these constraints require manual annotation and it is important to maximize the benefits obtained from it.",
  "y": "background"
 },
 {
  "id": "b1df73c14f53fea607c4c8b71740fe_2",
  "x": "All links are assumed to be consistent with each other. In order to incorporate the constraints in the DPMM, the Gibbs sampling scheme is modified so that mustlinked instances are generated by the same component and cannot-linked instances always by different ones. Following <cite>Vlachos et al. (2009)</cite> , for each instance that does not belong to a linked-group, the sampler is restricted to choose components that do not contain instances cannot-linked with it.",
  "y": "uses"
 },
 {
  "id": "b1df73c14f53fea607c4c8b71740fe_3",
  "x": "We evaluate our results using three information theoretic measures: Variation of Information (Meil\u0203, 2007) , V-measure (Rosenberg and Hirschberg, 2007) and V-beta<cite> (Vlachos et al., 2009</cite> ). All three assess the two desirable properties that a clustering should have with respect to a gold standard, homogeneity and completeness. Homogeneity reflects the degree to which each cluster contains instances from a single class and is defined as the conditional entropy of the class distribution of the gold standard given the clustering.",
  "y": "uses"
 },
 {
  "id": "b2392c74f17fb2c0b6a0f19d16bc99_0",
  "x": "In another line of study, researchers devised unsupervised methods to post-process word vectors. Spectral-decomposition methods such as singular value decomposition (SVD) and principal component analysis (PCA) are usually used in this line of research (Caron 2001; Bullinaria and Levy 2012; Turney 2012; Levy and Goldberg 2014; Levy, Goldberg, and Dagan 2015;<cite> Mu and Viswanath 2018)</cite> . The current paper is in line with the second, unsupervised, research direction.",
  "y": "background"
 },
 {
  "id": "b2392c74f17fb2c0b6a0f19d16bc99_1",
  "x": "The current paper is in line with the second, unsupervised, research direction. Among different unsupervised word vector postprocessing schemes, the all-but-the-top approach<cite> (Mu and Viswanath 2018</cite> ) is a prominent example. Empirically studying the latent features encoded by principal components (PCs) of distributional word vectors, <cite>Mu and Viswanath (2018)</cite> found that the variances explained by the leading PCs \"encode the frequency of the word to a significant degree\".",
  "y": "background"
 },
 {
  "id": "b2392c74f17fb2c0b6a0f19d16bc99_2",
  "x": "The current paper is in line with the second, unsupervised, research direction. Among different unsupervised word vector postprocessing schemes, the all-but-the-top approach<cite> (Mu and Viswanath 2018</cite> ) is a prominent example. Empirically studying the latent features encoded by principal components (PCs) of distributional word vectors, <cite>Mu and Viswanath (2018)</cite> found that the variances explained by the leading PCs \"encode the frequency of the word to a significant degree\".",
  "y": "background"
 },
 {
  "id": "b2392c74f17fb2c0b6a0f19d16bc99_3",
  "x": "Empirically studying the latent features encoded by principal components (PCs) of distributional word vectors, <cite>Mu and Viswanath (2018)</cite> found that the variances explained by the leading PCs \"encode the frequency of the word to a significant degree\". Since word frequencies are arguably unrelated to lexical semantics, they recommend removing such leading PCs from word vectors using a PCA reconstruction. The current work advances the findings of <cite>Mu and Viswanath (2018)</cite> and improves their post-processing scheme.",
  "y": "extends"
 },
 {
  "id": "b2392c74f17fb2c0b6a0f19d16bc99_4",
  "x": "Since word frequencies are arguably unrelated to lexical semantics, they recommend removing such leading PCs from word vectors using a PCA reconstruction. The current work advances the findings of <cite>Mu and Viswanath (2018)</cite> and improves their post-processing scheme. Instead of discarding a fixed number of PCs, we softly filter word vectors using matrix conceptors (Jaeger 2014; 2017) , which characterize the linear space of those word vector features having high variances -the features most contaminated by word frequencies according to <cite>Mu and Viswanath (2018)</cite> .",
  "y": "background"
 },
 {
  "id": "b2392c74f17fb2c0b6a0f19d16bc99_5",
  "x": "The rest of the paper is organized as follows. We first briefly review the principal component nulling approach for unsupervised word vector post-processing introduced in<cite> (Mu and Viswanath 2018)</cite> , upon which our work is based. We then introduce our proposed approach, Conceptor Negation (CN).",
  "y": "extends"
 },
 {
  "id": "b2392c74f17fb2c0b6a0f19d16bc99_6",
  "x": "This section is an overview of the all-but-the-top (ABTT) word vector post-processing approach introduced by <cite>Mu and Viswanath (2018)</cite> . In brief, the ABTT approach is based on two key observations of distributional word vectors. First, using a PCA, <cite>Mu and Viswanath (2018)</cite> revealed that word vectors are strongly influenced by a few leading principal components (PCs).",
  "y": "background"
 },
 {
  "id": "b2392c74f17fb2c0b6a0f19d16bc99_7",
  "x": "This section is an overview of the all-but-the-top (ABTT) word vector post-processing approach introduced by <cite>Mu and Viswanath (2018)</cite> . In brief, the ABTT approach is based on two key observations of distributional word vectors. First, using a PCA, <cite>Mu and Viswanath (2018)</cite> revealed that word vectors are strongly influenced by a few leading principal components (PCs).",
  "y": "background"
 },
 {
  "id": "b2392c74f17fb2c0b6a0f19d16bc99_8",
  "x": "Input : (i) {v w \u2208 R n : w \u2208 V }: word vectors with a vocabulary V ; (ii) d: the number of PCs to be removed. 1 Center the word vectors: Letv w := v w \u2212 \u00b5 for all w \u2208 V , where \u00b5 is the mean of the input word vectors. In practice, <cite>Mu and Viswanath (2018)</cite> found that the improvements yielded by ABTT are particularly impressive for word similarity tasks.",
  "y": "background"
 },
 {
  "id": "b2392c74f17fb2c0b6a0f19d16bc99_9",
  "x": "**POST-PROCESSING WORD VECTORS VIA CONCEPTOR NEGATION** Removing the leading PCs of word vectors using the ABTT algorithm described above is effective in practice, as seen in the elaborate experiments conducted by <cite>Mu and Viswanath (2018)</cite> . However, the method comes with a potential limitation: for each latent feature taking form as a PC of the word vectors, ABTT either completely removes the feature or keeps it intact.",
  "y": "background"
 },
 {
  "id": "b2392c74f17fb2c0b6a0f19d16bc99_10",
  "x": "Consider a random variable x taking values on word vectors {v w \u2208 R n : w \u2208 V }. We can estimate a conceptor C that describes the distribution of x using Equation 4. Recall that<cite> (Mu and Viswanath 2018)</cite> found that the directions with which x has the highest variances encode word frequencies, which are unrelated to word semantics.",
  "y": "background"
 },
 {
  "id": "b2392c74f17fb2c0b6a0f19d16bc99_11",
  "x": "We use the publicly available pre-trained Google News Word2Vec (Mikolov et al. 2013 ) 5 and Common Crawl GloVe 6 (Pennington, Socher, and Manning 2014) to perform lexical-level experiments. For CN, we fix \u03b1 = 2 for Word2Vec and GloVe throughout the experiments 7 . For ABTT, we set d = 3 for Word2Vec and d = 2 for GloVe, as what has been suggested by <cite>Mu and Viswanath (2018)</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "b2392c74f17fb2c0b6a0f19d16bc99_12",
  "x": "We see that the proposed CN method consistently outperforms the original word embedding (orig.) and the post-processed word embedding by ABTT for most of the benchmarks. Table 1 : Post-processing results (Spearman's rank correlation coefficient \u00d7 100) under seven word similarity benchmarks. The baseline results (orig. and ABTT) are collected from<cite> (Mu and Viswanath 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "b2392c74f17fb2c0b6a0f19d16bc99_13",
  "x": "Table 3 : Post-processing results (\u00d7100) on the semantic textual similarity tasks. The baseline results (orig. and ABTT) are collected from<cite> (Mu and Viswanath 2018)</cite> . Concept Categorization In the concept categorization task, we used k-means to cluster words into concept categories based on their vector representations (for example, \"bear\" and \"cat\" belong to the concept category of animals).",
  "y": "uses"
 },
 {
  "id": "b2392c74f17fb2c0b6a0f19d16bc99_14",
  "x": "Note that the datasets of ESSLLI, AP, and BM are increasingly challenging for clustering algorithms, due to the increasing numbers of words and categories. Following (Baroni, Dinu, and Kruszewski 2014; Schnabel et al. 2015;<cite> Mu and Viswanath 2018)</cite> , we used \"purity\" of clusters (Manning, Raghavan, and Sch\u00fctze 2008, Section 16.4) as the evaluation criterion. That the results of k-means heavily depend on two hyper-parameters: (i) the number of clusters and (ii) the initial centroids of clusters.",
  "y": "uses"
 },
 {
  "id": "b2392c74f17fb2c0b6a0f19d16bc99_15",
  "x": "We follow previous research (Baroni, Dinu, and Kruszewski 2014; Schnabel et al. 2015;<cite> Mu and Viswanath 2018)</cite> to set k as the ground-truth number of categories. The settings of the initial centroids of clusters, however, are less well-documented in previous work -it is not clear how many initial centroids have been sampled, or if different centroids have been sampled at all. To avoid the influences of initial centroids in k-means (which are particularly undesirable for this case because word vectors live in R 300 ), in this work, we simply fixed the initial centroids as the average of original, ABTT-processed, and CN-processed word vectors respectively from ground-truth categories.",
  "y": "uses"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_0",
  "x": "**ABSTRACT** This paper describes our system for SemEval-2019 Task 4: Hyperpartisan News Detection (Kiesel et al., 2019) . We use pretrained <cite>BERT</cite> <cite>(Devlin et al., 2018)</cite> architecture and investigate the effect of different fine tuning regimes on the final classification task.",
  "y": "uses"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_1",
  "x": "In earlier work, people mainly used the \"bag of words\" approach in algorithms such as Naive Bayes, Decision Tree, and SVM. However, recent studies (Peters et al., 2018; Radford et al., 2018; <cite>Devlin et al., 2018)</cite> showed that contextual word embeddings perform quite better than traditional word embeddings in many different NLP tasks as a result of their superior capacity of meaning representation.",
  "y": "background"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_2",
  "x": "Among those, <cite>BERT</cite> attracts researchers most because of (i) its transformer based architecture enabling faster training and (ii) state of the art results in many different tasks. Though it is quite new, <cite>BERT</cite> has been tried in many different domains than the one proposed in <cite>Devlin et al. (2018)</cite> . However, almost all of these studies have two things in common: they don't start training <cite>BERT</cite> from scratch and the target domain contains very limited data (Zhu et al., 2018; Yang et al., 2019; Alberti et al., 2019) .",
  "y": "background"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_3",
  "x": "Though it is quite new, <cite>BERT</cite> has been tried in many different domains than the one proposed in <cite>Devlin et al. (2018)</cite> . However, almost all of these studies have two things in common: they don't start training <cite>BERT</cite> from scratch and the target domain contains very limited data (Zhu et al., 2018; Yang et al., 2019; Alberti et al., 2019) . In this study, on the other hand, we address (1) the performance of <cite>BERT</cite> by comparing its domain specific pre-trained and fine-tuned performances, and (2) in the setting where the target domain has extensively more data.",
  "y": "motivation"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_4",
  "x": "In this study, on the other hand, we address (1) the performance of <cite>BERT</cite> by comparing its domain specific pre-trained and fine-tuned performances, and (2) in the setting where the target domain has extensively more data. In the following sections, we first summarize the <cite>BERT</cite> architecture, then give details of shared task data set, and then describe experimental setups we used to train <cite>BERT</cite> model. In the results section, we compare the performance of <cite>BERT</cite> under different settings and share our submission results for the shared task.",
  "y": "uses"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_5",
  "x": "<cite>Devlin et al. (2018)</cite> introduced two unsupervised tasks to pretrain this architecture, Next Sentence Prediction and Masked Language Modeling. In Next Sentence Prediction task, the goal is to determine whether the sentence comes after the specified previous sentence or not. It takes two sentences as input, the latter being in its original form 50% of the time, while other times it can be any random sentence from the corpus.",
  "y": "background"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_6",
  "x": "In this study, we use an open source PyTorch implementation 3 of <cite>BERT</cite> architecture. We make use of <cite>BERT-Base</cite> pretrained model provided by <cite>Devlin et al. (2018)</cite> in order to avoid pretraining from scratch. Similar to <cite>Devlin et al. (2018)</cite> , we use the representation obtained from the last layer for the first token (i.e. \"[CLS]\") for the sentence representation and a softmax classifier on top of it for predicting hyperpartisanship.",
  "y": "uses"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_7",
  "x": "We make use of <cite>BERT-Base</cite> pretrained model provided by <cite>Devlin et al. (2018)</cite> in order to avoid pretraining from scratch. Similar to <cite>Devlin et al. (2018)</cite> , we use the representation obtained from the last layer for the first token (i.e. \"[CLS]\") for the sentence representation and a softmax classifier on top of it for predicting hyperpartisanship. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_8",
  "x": "Similar to <cite>Devlin et al. (2018)</cite> , we use the representation obtained from the last layer for the first token (i.e. \"[CLS]\") for the sentence representation and a softmax classifier on top of it for predicting hyperpartisanship. ---------------------------------- **EXPERIMENTS**",
  "y": "similarities"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_9",
  "x": "In this section, we first introduce data provided by the shared task and the data preprocessing step. Then, we give the details of our experiments and results with <cite>BERT</cite> under pretraining and finetuning settings. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_10",
  "x": "---------------------------------- **INPUT REPRESENTATION** <cite>BERT</cite> restricts the input length to a maximum of 512 tokens.",
  "y": "background"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_11",
  "x": "We use the same tokenization method and embeddings as <cite>Devlin et al. (2018)</cite> to represent the words. ---------------------------------- **FINE-TUNING ONLY**",
  "y": "uses"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_12",
  "x": "---------------------------------- **FINE-TUNING ONLY** In order to show how <cite>BERT</cite> performs in news domain, our first attempt was to use the training data to only fine-tune the pretrained model for classification.",
  "y": "uses"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_13",
  "x": "We used <cite>BERT-Base</cite> which consists of 12 transformer blocks on top of each other applying 12 headed attention mechanism, hidden size of 768 and a total of 110 million parameters. We set 16 as our batch size and 2e-5 as our learning rate as recommended by <cite>Devlin et al. (2018)</cite> Table 1 : Classification results on our portal-wise data splits with fine-tuned <cite>BERT</cite>. We performed experiments using 128, 256 and 512 as our maximum sequence lengths and found out that 256 gives us the best test results, as shown in Table 1 .",
  "y": "uses"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_14",
  "x": "In order to show how <cite>BERT</cite> performs in news domain, our first attempt was to use the training data to only fine-tune the pretrained model for classification. We used <cite>BERT-Base</cite> which consists of 12 transformer blocks on top of each other applying 12 headed attention mechanism, hidden size of 768 and a total of 110 million parameters. We set 16 as our batch size and 2e-5 as our learning rate as recommended by <cite>Devlin et al. (2018)</cite> Table 1 : Classification results on our portal-wise data splits with fine-tuned <cite>BERT</cite>.",
  "y": "uses"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_16",
  "x": "Combined Loss <cite>BERT</cite>-Base 3.65 Our Version 1.79 ment, avoiding overlapping between chunks. For Masked LM task, we follow the same approach with <cite>Devlin et al. (2018)</cite> . At the end of pretraining data generation process, we accumulated near 3.5 million samples, only running the process once on our train split, so without any duplication unlike <cite>Devlin et al. (2018)</cite> because of time restrictions.",
  "y": "uses"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_17",
  "x": "For Masked LM task, we follow the same approach with <cite>Devlin et al. (2018)</cite> . At the end of pretraining data generation process, we accumulated near 3.5 million samples, only running the process once on our train split, so without any duplication unlike <cite>Devlin et al. (2018)</cite> because of time restrictions. We also generated a small held-out dataset using our test split to use in evaluation.",
  "y": "differences"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_18",
  "x": "Starting from the pretrained model of <cite>BERT-Base</cite> instead of a cold start, we trained the model with a learning rate of 3e-5 and 256 as the maximum sequence length for 290k iterations. Table 2 presents the combined loss of two unsupervised tasks on the held-out data for original <cite>BERT-Base</cite> and further pretrained model with the generated data. Results show that pretraining <cite>BERT</cite> further with data from an unseen domain greatly increases its representational power.",
  "y": "uses"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_19",
  "x": "Starting from the pretrained model of <cite>BERT-Base</cite> instead of a cold start, we trained the model with a learning rate of 3e-5 and 256 as the maximum sequence length for 290k iterations. Table 2 presents the combined loss of two unsupervised tasks on the held-out data for original <cite>BERT-Base</cite> and further pretrained model with the generated data. Results show that pretraining <cite>BERT</cite> further with data from an unseen domain greatly increases its representational power.",
  "y": "uses"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_20",
  "x": "Results show that pretraining <cite>BERT</cite> further with data from an unseen domain greatly increases its representational power. Table 3 demonstrates that pretraining <cite>BERT</cite> with domain specific data using unsupervised tasks improves the performance of the model on the supervised classificiation task.",
  "y": "extends differences"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_21",
  "x": "Accuracy is used as the official evaluation metric and the deciding test set is an another manually labelled news articles set named \"by-article-test-set\" which was kept hidden from the participants. In our first attempt, we fine-tuned <cite>BERT</cite> with portal-wise train split using development set to get the best model. After this we further train it with 645 manually labeled data (i.e. \"by-article-trainset\"), because it comes from the same sample as test data.",
  "y": "extends"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_22",
  "x": "In our first attempt, we fine-tuned <cite>BERT</cite> with portal-wise train split using development set to get the best model. After this we further train it with 645 manually labeled data (i.e. \"by-article-trainset\"), because it comes from the same sample as test data. In our last attempt, we pretrained <cite>BERT</cite> with our portal-wise train split, and then fine-tune it as described before.",
  "y": "extends"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_23",
  "x": "The results of our two attempts can be seen in Table 4 . The third model in the table is to show the effect of the last fine-tuning step on \"by-article-train-set\". Looking at the results of second and third models on \"by-article-test-set\" shows us, although we fine-tune <cite>BERT</cite> with supervised data for the same classification task, fine-tuning on \"byarticle-train-set\" improves the results drastically.",
  "y": "differences"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_24",
  "x": "Although our experiments ( Table 3) show us that pretraining <cite>BERT</cite> further with data from news domain has a positive effect on overall accuracy, we are not able to observe the similar effect on \"by-article-test-set\". The second model adapts to the publisher domain more than the first model does because of the extensive pretraining before fine-tuning. As the difference between publisher and article is highly notable from the findings before, overfitting to the publisher domain might end up hurting the generalization of the model.",
  "y": "differences"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_25",
  "x": "We presented a <cite>BERT</cite> baseline for the Hyperpartisan News Detection task. We demonstrated that pretraining <cite>BERT</cite> in an unseen domain improves the performance of the model on the domain specific supervised task. We also showed that the difference in news source affects the generalization.",
  "y": "uses"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_26",
  "x": "**CONCLUSION** We presented a <cite>BERT</cite> baseline for the Hyperpartisan News Detection task. We demonstrated that pretraining <cite>BERT</cite> in an unseen domain improves the performance of the model on the domain specific supervised task.",
  "y": "differences"
 },
 {
  "id": "b27150a3506730c61dc78b3034887e_27",
  "x": "From our findings, we believe that domain adaptation is important for the <cite>BERT</cite> architecture and we would like to investigate the effect of from scratch unsupervised pretraining on the supervised task as future work. Beale 4 is a news anchor who decides to commit suicide on live air. Instead, he gives his famous speech about modern American life and convinces American people to scream his words: \"I'm as mad as hell, and I'm not going to take this any more!\". But the media sees his breakdown as an opportunity for huge ratings.",
  "y": "extends future_work"
 },
 {
  "id": "b2a6ec11403fe73b9bae7742c1c5a2_0",
  "x": "These tools typically suggest edits to guide revision, rather than model the editing process after observing revisions. With the long term goal of developing an intelligent revision assistant, this paper presents an approach to modeling student editor roles. Prior natural language processing (NLP) approaches to student revision analysis have focused on identifying revisions during argumentative writing and classifying their purposes and other properties<cite> [12,</cite> 11, 7, 1] .",
  "y": "background"
 },
 {
  "id": "b2a6ec11403fe73b9bae7742c1c5a2_1",
  "x": "**CORPORA** Our work takes advantage of several corpora of multiple drafts of argumentative essays written by both high-school and college students<cite> [12,</cite> 11] , where all data has been annotated for revision using the framework of <cite>[12]</cite> . We divide our data into a Modeling Corpus (185 paired drafts, 3245 revisions) and an Evaluation Corpus (107 paired draft, 2045 revisions), based on whether expert grades are available before (Score1) and after (Score2) essay revision.",
  "y": "uses"
 },
 {
  "id": "b2a6ec11403fe73b9bae7742c1c5a2_2",
  "x": "For all essays and prior to this study, subsequent drafts were manually aligned at the sentence-level based on semantic similarity. Nonidentical aligned sentences were extracted as the revisions, resulting in three types of revision operations -Add, Delete, M odif y. Each extracted revision was manually annotated with a purpose following the revision schema shown in Figure 1 (modified compared to <cite>[12]</cite> by adding the Precision category). For this study, each revision's position was in addition automatically tagged using its paragraph position in the revised essay.",
  "y": "extends"
 },
 {
  "id": "b2a6ec11403fe73b9bae7742c1c5a2_3",
  "x": "A corpus study in <cite>[12]</cite> showed that content changes are correlated with argumentative writing improvement, reaffirming the statement of [4] . Using a similar method, we investigate if our editor roles are related to writing improvement. We calculate partial Pearson correlations between editor roles and Score2 while controlling for Score1 to regress out the effect of the correlation between Score1 and Score2 (Corr.= 0.692, p < 0.001).",
  "y": "background"
 },
 {
  "id": "b2a6ec11403fe73b9bae7742c1c5a2_4",
  "x": "**EDITOR ROLES** A corpus study in <cite>[12]</cite> showed that content changes are correlated with argumentative writing improvement, reaffirming the statement of [4] . Using a similar method, we investigate if our editor roles are related to writing improvement.",
  "y": "similarities"
 },
 {
  "id": "b31acd3535cd740e609d45986fbf33_0",
  "x": "**ABSTRACT** Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair<cite> (Akbik et al., 2018)</cite> , and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes.",
  "y": "uses"
 },
 {
  "id": "b31acd3535cd740e609d45986fbf33_1",
  "x": "---------------------------------- **INTRODUCTION** Pre-trained language models (LMs) such as ELMo (Peters et al., 2018) , ULMFiT (Howard and Ruder, 2018) , OpenAI GPT (Radford et al., 2018) , Flair<cite> (Akbik et al., 2018)</cite> and Bert (Devlin et al., 2018) have shown great improvements in NLP tasks ranging from sentiment analysis to named entity recognition to question answering.",
  "y": "background"
 },
 {
  "id": "b31acd3535cd740e609d45986fbf33_2",
  "x": "All of the pre-trained word-level contextual embedding models include some character or subword components in their architecture. For example, Flair is a forward-backward LM trained over characters using recurrent neural networks (RNNs), that generates pre-trained contextual word embeddings by concatenating the forward LM's hidden state for the word's last character and the backward LM's hidden state for the word's first character. Flair achieves state-of-the-art or competitive results on part-of-speech tagging and named entity tagging<cite> (Akbik et al., 2018)</cite> .",
  "y": "background"
 },
 {
  "id": "b31acd3535cd740e609d45986fbf33_3",
  "x": "Together with other techniques, they achieve state-of-the-art performance on part-of-speech and morphological tagging. However, both<cite> Akbik et al. (2018)</cite> and Bohnet et al. (2018) discard all other contextual character embeddings, and no analyses of the models are performed at the character-level. In the current paper, we derive pre-trained contextual character embeddings from Flair's forwardbackward LM trained on a 1-billion word corpus of English (Chelba et al., 2014) , and observe if these embeddings yield the same large improvements for character-level tasks as yielded by pre-trained contextual word embeddings for word-level tasks.",
  "y": "motivation"
 },
 {
  "id": "b31acd3535cd740e609d45986fbf33_4",
  "x": [
   "In the current paper, we derive pre-trained contextual character embeddings from Flair's forwardbackward LM trained on a 1-billion word corpus of English (Chelba et al., 2014) , and observe if these embeddings yield the same large improvements for character-level tasks as yielded by pre-trained contextual word embeddings for word-level tasks. We aim to analyze where improvements come from (e.g., term variations, low frequency words) and what they depend on (e.g., embedding size, context size). We focus on the task of parsing time normalizations (Laparra et al., 2018b) , where large gains of character-level models over word-level models have been observed (Laparra et al., 2018a) ."
  ],
  "y": "extends"
 },
 {
  "id": "b31acd3535cd740e609d45986fbf33_5",
  "x": "\u2022 We derive pre-trained contextual character embeddings from Flair<cite> (Akbik et al., 2018)</cite> , apply them to a state-of-the art time normalizer (Laparra et al., 2018a) , and obtain major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% error reduction in clinical notes. \u2022 We demonstrate that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and crossdomain changes. \u2022 We quantify the amount of context leveraged by pre-trained contextual character embeddings.",
  "y": "extends"
 },
 {
  "id": "b31acd3535cd740e609d45986fbf33_7",
  "x": "---------------------------------- **CONCLUSION** We derive pre-trained character-level contextual embeddings from Flair<cite> (Akbik et al., 2018)</cite> , a wordlevel embedding model, inject these into a state-ofthe-art time normalization system, and achieve major performance improvements: 51% error reduction in news and 33% in clinical notes.",
  "y": "extends"
 },
 {
  "id": "b335178d833e26190b7056469d3fa7_0",
  "x": "Previous works on multilingual NMT typically trained models with up to 7 languages (Dong et al., 2015; Firat et al., 2016b; Ha et al., 2016; Johnson et al., 2017; Gu et al., 2018) and up to 20 trained directions (Cettolo et al., 2017) simultaneously. One recent exception is<cite> Neubig and Hu (2018)</cite> which trained many-to-one models from 58 languages into English. While utilizing significantly more languages than previous works, their experiments were restricted to many-to-one models in a lowresource setting with up to 214k examples per language-pair and were evaluated only on four translation directions.",
  "y": "background"
 },
 {
  "id": "b335178d833e26190b7056469d3fa7_1",
  "x": "We evaluate the performance of such massively multilingual models while varying factors like model capacity, the number of trained directions (tasks) and low-resource vs. high-resource settings. Our experiments on the publicly available TED talks dataset (Qi et al., 2018) show that massively multilingual many-to-many models with up to 58 languages to-and-from English are very effective in low resource settings, allowing to use high-capacity models while avoiding overfitting and achieving superior results to the current stateof-the-art on this dataset <cite>(Neubig and Hu, 2018</cite>; Wang et al., 2019) when translating into English. We then turn to experiment with models trained on 103 languages in a high-resource setting.",
  "y": "uses"
 },
 {
  "id": "b335178d833e26190b7056469d3fa7_2",
  "x": "Since the dataset is already tokenized we did not apply additional preprocessing other than applying joint subword segmentation (Sennrich et al., 2016) with 32k symbols. Regarding the languages we evaluate on, we begin with the same four languages as<cite> Neubig and Hu (2018)</cite> -Azerbeijani (Az), Belarusian (Be), Galician (Gl) and Slovak (Sk). These languages present an extreme low-resource case, with as few as 4.5k training examples for Belarusian-English.",
  "y": "uses"
 },
 {
  "id": "b335178d833e26190b7056469d3fa7_3",
  "x": "We follow the method of Ha et al. (2016) ; Johnson et al. (2017) and add a target-language prefix token to each source sentence to enable many-to-many translation. These different setups enable us to examine the effect of the number of translation tasks on the translation quality as measured in BLEU (Papineni et al., 2002) . We also compare our massively multilingual models to bilingual baselines and to two recently published results on this dataset<cite> (Neubig and Hu (2018)</cite> ; Wang et al. (2019) ).",
  "y": "uses"
 },
 {
  "id": "b335178d833e26190b7056469d3fa7_4",
  "x": "---------------------------------- **RESULTS** We use tokenized BLEU in order to be comparable with<cite> Neubig and Hu (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "b335178d833e26190b7056469d3fa7_5",
  "x": "Their many-to-one models use similar-languageregularization, i.e. fine-tuning a pre-trained manyto-one model with data from the language pair of interest together with data from a language pair that has a typologically-similar source language and more data (i.e. Russian and Belarusian, Turkish and Azerbaijani). The results under \"Ours\" are our many-to-one and many-to-many models we trained identically in terms of model architecture and hyper-parameters. We first note that our many-to-many model outperforms all other models when translating into English, with 1.82 BLEU improvement (when av-eraged across the four language pairs) over the best fine-tuned many-to-one models of<cite> Neubig and Hu (2018)</cite> and 2.44 BLEU improvement over our many-to-one model when averaged across the four low-resource language pairs (Table 1) .",
  "y": "differences"
 },
 {
  "id": "b335178d833e26190b7056469d3fa7_6",
  "x": "We also note that our many-to-one model is on average 0.75 BLEU behind the best many-to-one models in<cite> Neubig and Hu (2018)</cite> . We attribute this to the fact that their models are fine-tuned using similarlanguage-regularization while our model is not. We find an additional difference between the results on the resource-scarce languages (Table 1) and the higher-resource languages (Table  2) .",
  "y": "differences"
 },
 {
  "id": "b335178d833e26190b7056469d3fa7_7",
  "x": "Regarding massively multilingual models,<cite> Neubig and Hu (2018)</cite> explored methods for rapid adaptation of NMT to new languages by training multilingual models on the 59-language TED Talks corpus and fine-tuning them using data from the new languages. While modeling significantly more languages than previous studies, they only train many-to-one models, which we show are inferior in comparison to our proposed massively multilingual many-to-many models when evaluated into English on this dataset. Tiedemann (2018) trained an English-centric many-to-many model on translations of the bible including 927 languages.",
  "y": "background"
 },
 {
  "id": "b3ef4c176720bdc89d2f73a2560673_0",
  "x": "We focus on buyer-seller negotiations (He et al., 2018) where two individuals negotiate the price of a given product. Leveraging the recent advancements (Vaswani et al., 2017;<cite> Devlin et al., 2019)</cite> in pre-trained language encoders, we attempt to predict negotiation outcomes early on in the conversation, in a completely data-driven manner ( Figure  1 ). Early prediction of outcomes is essential for effective planning of an automatically negotiating agent.",
  "y": "uses"
 },
 {
  "id": "b3ef4c176720bdc89d2f73a2560673_1",
  "x": "**APPROACH** Pre-trained language models, such as BERT (Vaswani et al., 2017; <cite>Devlin et al., 2019</cite> ) have recently gained huge success on a wide range of NLP tasks. However, since our framework deals with various auxiliary pieces (category, price, etc.), we cannot directly leverage these language models, which have only been trained on natural language inputs.",
  "y": "background"
 },
 {
  "id": "b3ef4c176720bdc89d2f73a2560673_2",
  "x": "Pre-trained language models, such as BERT (Vaswani et al., 2017; <cite>Devlin et al., 2019</cite> ) have recently gained huge success on a wide range of NLP tasks. However, since our framework deals with various auxiliary pieces (category, price, etc.), we cannot directly leverage these language models, which have only been trained on natural language inputs. Instead of relying on additional representations along with BERT outputs, we propose a simple, yet effective way to incorporate the auxiliary information into the same embedding space.",
  "y": "differences"
 },
 {
  "id": "b3ef4c176720bdc89d2f73a2560673_3",
  "x": "Training Details: Given the multiple segments in our model input and small data size, we use BERTbase<cite> (Devlin et al., 2019)</cite> , having output dimension of 768. To tackle the variance in product prices across different categories, all prices in the inputs and outputs were normalized by the listing price. The predictions were unnormalized before final evaluations.",
  "y": "uses"
 },
 {
  "id": "b3f7051cbba3344f0aec0f2e80d5e0_0",
  "x": "The research presented in this paper extends our previous work. As we summarize in Section 2, this paper relies on the same data set and evaluation metric as <cite>DeVault et al. (2011)</cite> , which reports results for learned policies based on maximum entropy models. In this paper, we add a comparison to a hand-authored policy (Rules) and a new policy based on relevance models (RM).",
  "y": "similarities uses"
 },
 {
  "id": "b3f7051cbba3344f0aec0f2e80d5e0_1",
  "x": "We refer the reader to <cite>DeVault et al. (2011)</cite> for additional details. We use an existing virtual human scenario designed for Tactical Questioning (TACQ) (Traum et al., 2008) , where military personnel interview individuals for information of military value. TACQ characters are designed to be non-cooperative at times.",
  "y": "background"
 },
 {
  "id": "b3f7051cbba3344f0aec0f2e80d5e0_2",
  "x": "The system also defines a different set of 96 unique SAs (responses) for the Amani character. We perform our experiments and evaluation using an existing set of 19 annotated Amani dialogues<cite> (DeVault et al., 2011)</cite> . The dialogues were collected through teletype-based role play.",
  "y": "similarities uses"
 },
 {
  "id": "b3f7051cbba3344f0aec0f2e80d5e0_3",
  "x": "We call the DM's decision process a dialogue policy. The system builders' intended policy for Amani is detailed in <cite>DeVault et al. (2011)</cite> . Because Amani has only a fixed set of system responses, the policy problem looks like a traditional classification task.",
  "y": "background"
 },
 {
  "id": "b3f7051cbba3344f0aec0f2e80d5e0_4",
  "x": "In our data set, 6 referees independently linked each user utterance to the best system SA response. In Figure 1 , we provide an example in which three different system SAs were selected by the 6 referees. In other cases, up to 6 different system SAs were selected<cite> (DeVault et al., 2011)</cite> .",
  "y": "background"
 },
 {
  "id": "b3f7051cbba3344f0aec0f2e80d5e0_5",
  "x": "We evaluate the dialogue policies learned in each of our experimental conditions through 19-fold cross-validation of our set of 19 dialogues. In each fold, we hold out one dialogue and use the remaining 18 dialogues as training data. To measure the performance of the dialogue policy, we follow the approach of <cite>DeVault et al. (2011)</cite> , which counts an automatically produced system SA as correct if that SA was chosen by at least one referee for that dialogue turn in the data set.",
  "y": "uses similarities"
 },
 {
  "id": "b3f7051cbba3344f0aec0f2e80d5e0_6",
  "x": "(We do not expect that an automatic system would outperform a human referee.) This score is .79; see <cite>DeVault et al. (2011)</cite> for discussion. ---------------------------------- **EXPERIMENTAL SETUP**",
  "y": "background"
 },
 {
  "id": "b3f7051cbba3344f0aec0f2e80d5e0_7",
  "x": "For the MaxEnt policy, a score of .71 is achieved with \"gold\" SAs, and a lower .57 with run-time SAs. Note that .71 is an inferior performance to the .79 achieved with G-SA/Rules, indicating that MaxEnt does not learn a policy as effective as the hand-authored Rules, even if it is trained and evaluated on gold SA labels. As previously reported in <cite>DeVault et al. (2011)</cite> , a performance of .66 is achieved with the MaxEnt policy when trained on text-based features.",
  "y": "background"
 },
 {
  "id": "b49807b058e5e1e50eae524e592401_0",
  "x": "Rather than assuming an entity can be uniquely categorized into a single type, the task has been approached as a multi-label classification problem: e.g., in \"... became a top seller ... Monopoly is played in 114 countries. ...\" (Figure 1 ), \"Monopoly\" is considered both a game as well as a product. The state-of-the-art approach<cite> (Shimaoka et al., 2017)</cite> for fine-grained entity typing employs an attentive neural architecture to learn representations of the entity mention as well as its context.",
  "y": "background"
 },
 {
  "id": "b49807b058e5e1e50eae524e592401_1",
  "x": "As shown in Figure 1 , featurizer \u03d5 in our model contains three encoders which encode entity e and its context x into feature vectors, and we consider both sentence-level context x s and document-level context x d in contrast to prior work which only takes sentence-level context (Gillick et al., 2014;<cite> Shimaoka et al., 2017)</cite> . 1 The output of featurizer \u03d5 is the concatenation of these feature vectors: We define the computation of these feature vectors in the followings.",
  "y": "differences"
 },
 {
  "id": "b49807b058e5e1e50eae524e592401_2",
  "x": "where \u2212 \u2192 f and \u2190 \u2212 f are L-layer stacked LSTMs units (Hochreiter and Schmidhuber, 1997) . This is different from <cite>Shimaoka et al. (2017)</cite> who use two separate bi-directional RNNs for context on each side of the entity mention. Attention: The feature representation for x s is a weighted sum of the hidden states: g s (x s , e) = n i=1 a i h i , where a i is the attention to hidden state h i .",
  "y": "differences"
 },
 {
  "id": "b49807b058e5e1e50eae524e592401_3",
  "x": "It computes attention based on the alignment between the entity and its context: where W a is the weight matrix. The dot-product attention differs from the self attention<cite> (Shimaoka et al., 2017)</cite> which only considers the context.",
  "y": "differences"
 },
 {
  "id": "b49807b058e5e1e50eae524e592401_4",
  "x": "**ADAPTIVE THRESHOLDS** In prior work, a fixed threshold (r t = 0.5) is used for classification of all types (Ling and Weld, 2012;<cite> Shimaoka et al., 2017)</cite> . We instead assign a different threshold to each type that is optimized to maximize the overall strict F 1 on the dev set.",
  "y": "differences"
 },
 {
  "id": "b49807b058e5e1e50eae524e592401_5",
  "x": "Overall, our approach significantly increases the state-of-the-art macro F 1 on both OntoNotes and BBN datasets. On OntoNotes (Table 3) , our approach improves the state of the art across all three metrics. Note that (1) without adaptive thresholds or document-level contexts, our approach still outperforms other approaches on macro F 1 and micro F 1 ; (2) adding hand-crafted features<cite> (Shimaoka et al., 2017)</cite> does not improve the performance.",
  "y": "differences"
 },
 {
  "id": "b49807b058e5e1e50eae524e592401_6",
  "x": "Ma et al. (2016) 49.30 68.23 61.27 AFET (Ren et al., 2016a) 55.10 71.10 64.70 FNET (Abhishek et al., 2017) 52.20 68.50 63.30 NEURAL<cite> (Shimaoka et al., 2017)</cite> This indicates the benefits of our proposed model architecture for learning fine-grained entity typing, which is discussed in detail in Section 3.4; and (3) BINARY and KWASIBIE were trained on a different dataset, so their results are not directly comparable. ---------------------------------- **APPROACH STRICT MACRO MICRO**",
  "y": "differences"
 },
 {
  "id": "b49807b058e5e1e50eae524e592401_7",
  "x": "Strict Macro Micro KWSABIE (Yogatama et al., 2015) N/A N/A 72.25 Attentive (Shimaoka et al., 2016 ) 58.97 77.96 74.94 FNET(Abhishek et al., 2017 65.80 81.20 77.40 Ling and Weld (2012) 52.30 69.90 69.30 PLE (Ren et al., 2016b) 49.44 68.75 64.54 Ma et al. (2016) 53.54 68.06 66.53 AFET (Ren et al., 2016a) 53.30 69.30 66.40 NEURAL<cite> (Shimaoka et al., 2017)</cite> proach still achieves the state-of-the-art strict and micro F 1 . If compared with the ablation variant of the NEURAL approach, i.e., w/o hand-crafted features, our approach gains significant improvement.",
  "y": "differences"
 },
 {
  "id": "b49807b058e5e1e50eae524e592401_8",
  "x": "Compared to hand-crafted features that heavily rely on system or human annotations, attention mechanism requires significantly less supervision, and document-level or paragraph-level contexts are much easier to get. Through experiments, we observe no improvement by encoding type hierarchical information<cite> (Shimaoka et al., 2017)</cite> . 5 To explain this, we compute cosine similarity between each pair of fine-grained types based on the type embeddings learned by our model, i.e., w t in Eq. (1).",
  "y": "extends differences"
 },
 {
  "id": "b4d7e9b7942698ef0678d3b4a0ad7d_0",
  "x": "Computationally analyzing the social context in which language is used has gathered great interest within the NLP community recently. One of the areas that has generated substantial research is the study of how social power relations between people affect and/or are revealed in their interactions with one another. Researchers have proposed systems to detect social power relations between participants of organizational email threads<cite> (Bramsen et al., 2011</cite>; Gilbert, 2012; , online forums (Danescu-NiculescuMizil et al., 2012; Biran et al., 2012; DanescuNiculescu-Mizil et al., 2013) , chats (Strzalkowski et al., 2012) , and off-line interactions such as presidential debates Nguyen et al., 2013) .",
  "y": "background"
 },
 {
  "id": "b4d7e9b7942698ef0678d3b4a0ad7d_1",
  "x": "**MOTIVATION** Early NLP-based approaches such as<cite> Bramsen et al. (2011) and</cite> Gilbert (2012) built systems to predict hierarchical power relations between people in the Enron email corpus using lexical features from all the messages exchanged between them. One limitation of this approach is that it relies solely on lexical cues and hence works best when large collections of messages exchanged between the pairs of people are available.",
  "y": "background"
 },
 {
  "id": "b4d7e9b7942698ef0678d3b4a0ad7d_2",
  "x": "**MOTIVATION** Early NLP-based approaches such as<cite> Bramsen et al. (2011) and</cite> Gilbert (2012) built systems to predict hierarchical power relations between people in the Enron email corpus using lexical features from all the messages exchanged between them. One limitation of this approach is that it relies solely on lexical cues and hence works best when large collections of messages exchanged between the pairs of people are available.",
  "y": "motivation background"
 },
 {
  "id": "b4d7e9b7942698ef0678d3b4a0ad7d_3",
  "x": "For example,<cite> Bramsen et al. (2011)</cite> excluded sender-recipient pairs who exchanged fewer than 500 words from their evaluation set, since they found smaller text samples are harder to classify. By taking the message out of the context of the interaction in which it was exchanged, they fail to utilize cues from the structure of interactions, which complements the lexical cues in detecting power relations, as we showed in . We modeled the problem of detecting power relationships differently in : we predicted whether a participant in an email thread has a certain type of power or not.",
  "y": "background"
 },
 {
  "id": "b4d7e9b7942698ef0678d3b4a0ad7d_4",
  "x": "**MOTIVATION** Early NLP-based approaches such as<cite> Bramsen et al. (2011) and</cite> Gilbert (2012) built systems to predict hierarchical power relations between people in the Enron email corpus using lexical features from all the messages exchanged between them. One limitation of this approach is that it relies solely on lexical cues and hence works best when large collections of messages exchanged between the pairs of people are available. For example,<cite> Bramsen et al. (2011)</cite> excluded sender-recipient pairs who exchanged fewer than 500 words from their evaluation set, since they found smaller text samples are harder to classify.",
  "y": "motivation background"
 },
 {
  "id": "b4d7e9b7942698ef0678d3b4a0ad7d_5",
  "x": "From<cite> (Bramsen et al., 2011)</cite> we retain the idea that we want to predict the power relation between pairs of people. But in contrast to their formulation, we retain the goal from ) that we want to study communication in the context of an interaction, and that we want to be able to make predictions using only the emails exchanged in a single thread. Like , we use features to capture the dialog structure, but we use automatic taggers to generate them and assume no manual annotation at all at training or test time. This allows us to use the entire Enron email corpus for this study.",
  "y": "differences"
 },
 {
  "id": "b4d7e9b7942698ef0678d3b4a0ad7d_6",
  "x": "Given a thread t and a pair of participants (p 1 , p 2 ) \u2208 RIPP t , we want to automatically detect HP (p 1 , p 2 ). This problem formulation is similar to the ones in<cite> (Bramsen et al., 2011)</cite> and (Gilbert, 2012) . However, the difference is that for us an instance is a pair of participants in a single thread of interaction (which may or may not include other people), whereas for them an instance constitutes all messages exchanged between a pair of people in the entire corpus.",
  "y": "similarities"
 },
 {
  "id": "b4d7e9b7942698ef0678d3b4a0ad7d_7",
  "x": "Lexical features have already been shown to be valuable in predicting power relations<cite> (Bramsen et al., 2011</cite>; Gilbert, 2012) . We use another feature set LEX to capture word ngrams, POS (part of speech) ngrams and mixed ngrams. A mixed ngram (Prabhakaran et al., 2012 ) is a special case of word ngram where words belonging to open classes are replaced with their POS tags.",
  "y": "background"
 },
 {
  "id": "b5097b3d901d073bfe06bcd88318ac_0",
  "x": "We compare them with the original model released by <cite>Mikolov</cite>. Both intrinsic and extrinsic (downstream) evaluations, including Named Entity Recognition (NER) and Sentiment Analysis (SA) were carried out. The downstream tasks reveal that the best model is task-specific, high analogy scores don't necessarily correlate positively with F1 scores and the same applies for more data.",
  "y": "uses"
 },
 {
  "id": "b5097b3d901d073bfe06bcd88318ac_1",
  "x": "Besides, using a small corpus, we obtain better human-assigned WordSim scores, corresponding Spearman correlation and better downstream (NER & SA) performance compared to <cite>Mikolov'</cite>s model, trained on 100 billion word corpus. ---------------------------------- **INTRODUCTION**",
  "y": "differences"
 },
 {
  "id": "b5097b3d901d073bfe06bcd88318ac_2",
  "x": "**INTRODUCTION** There have been many implementations of the word2vec model in either of the two architectures it provides: continuous skipgram and continuous bag of words (CBoW) (<cite>Mikolov et al. (2013a)</cite> ). Similar distributed models of word or subword embeddings (or vector representations) find usage in sota, deep neural networks like Bidirectional Encoder Representations from Transformers (BERT) and its successors (Devlin et al. (2018) ; ; Raffel et al. (2019) ).",
  "y": "motivation"
 },
 {
  "id": "b5097b3d901d073bfe06bcd88318ac_3",
  "x": "---------------------------------- **LITERATURE REVIEW** Breaking away from the non-distributed (high-dimensional, sparse) representations of words, typical of traditional bag-of-words or one-hot-encoding (Turian et al. (2010) ), <cite>Mikolov et al. (2013a)</cite> created word2vec.",
  "y": "background"
 },
 {
  "id": "b5097b3d901d073bfe06bcd88318ac_4",
  "x": "A loglinear classifier is used in both architectures (<cite>Mikolov et al. (2013a)</cite> ). In further work, they extended the model to be able to do phrase representations and subsample frequent words (Mikolov et al. (2013b) ). Being a Neural Network Language Model (NNLM), word2vec assigns probabilities to words in a sequence, like other NNLMs such as feedforward networks or recurrent neural networks (Turian et al. (2010) ).",
  "y": "background"
 },
 {
  "id": "b5097b3d901d073bfe06bcd88318ac_5",
  "x": "It's been shown that word vectors are beneficial for NLP tasks (Turian et al. (2010) ), such as sentiment analysis and named entity recognition. Besides, <cite>Mikolov et al. (2013a)</cite> showed with vector space algebra that relationships among words can be evaluated, expressing the quality of vectors produced from the model. The famous, semantic example: vector(\"King\") -vector(\"Man\") + vector(\"Woman\") \u2248 vector(\"Queen\") can be verified using cosine distance.",
  "y": "background"
 },
 {
  "id": "b5097b3d901d073bfe06bcd88318ac_6",
  "x": "Another type of semantic meaning is the relationship between a capital city and its corresponding country. Syntactic relationship examples include plural verbs and past tense, among others. Combination of both syntactic and semantic analyses is possible and provided (totaling over 19,000 questions) as Google analogy test set by <cite>Mikolov et al. (2013a)</cite> .",
  "y": "background"
 },
 {
  "id": "b5097b3d901d073bfe06bcd88318ac_7",
  "x": "<cite>Mikolov et al. (2013a)</cite> tried various hyper-parameters with both architectures of <cite>their</cite> model, ranging from 50 to 1,000 dimensions, 30,000 to 3,000,000 vocabulary sizes, 1 to 3 epochs, among others. In our work, we extended research to 3,000 dimensions. Different observations were noted from the many trials.",
  "y": "background"
 },
 {
  "id": "b5097b3d901d073bfe06bcd88318ac_8",
  "x": "In both tasks, the default pytorch embedding was tested before being replaced by pre-trained embeddings released by <cite>Mikolov et al. (2013a)</cite> and ours. In each case, the dataset was shuffled before training and split in the ratio 70:15:15 for training, validation (dev) and test sets. Batch size of 64 was used.",
  "y": "uses"
 },
 {
  "id": "b5097b3d901d073bfe06bcd88318ac_9",
  "x": "In terms of analogy score, for 10 epochs, w8s0h0 performs best while w8s1h0 performs best in terms of WordSim and corresponding Spearman correlation. Meanwhile, increasing the corpus size to BW, w4s1h0 performs best in terms of analogy score while w8s1h0 maintains its position as the best in terms of WordSim and Spearman correlation. Besides considering quality metrics, it can be observed from table 4 that comparative ratio of values between the models is not commensurate with the results in intrinsic or extrinsic values, especially when we consider the amount of time and energy spent, since more training time results in more energy consumption Information on the length of training time for the released <cite>Mikolov</cite> model is not readily available.",
  "y": "differences"
 },
 {
  "id": "b5097b3d901d073bfe06bcd88318ac_10",
  "x": "Our models are available for confirmation and source codes are available on github. 2 With regards to NER, most pretrained embeddings outperformed the default pytorch embedding, with our BW w4s1h0 model (which is best in BW analogy score) performing best in F1 score and closely followed by <cite>Mikolov et al. (2013a)</cite> model. On the other hand, with regards to SA, pytorch embedding outperformed the pretrained embeddings but was closely followed by our SW w8s0h0 model (which also had the best SW analogy score).",
  "y": "differences"
 },
 {
  "id": "b5097b3d901d073bfe06bcd88318ac_11",
  "x": "2 With regards to NER, most pretrained embeddings outperformed the default pytorch embedding, with our BW w4s1h0 model (which is best in BW analogy score) performing best in F1 score and closely followed by <cite>Mikolov et al. (2013a)</cite> model. On the other hand, with regards to SA, pytorch embedding outperformed the pretrained embeddings but was closely followed by our SW w8s0h0 model (which also had the best SW analogy score). <cite>Mikolov et al. (2013a)</cite> performed second worst of all, despite originating from a very huge corpus.",
  "y": "differences"
 },
 {
  "id": "b624e8d3ad3d351d4cb27ea4b5c616_0",
  "x": "This system was improved into a semisupervised solution (our 2 nd solution [SI]), which uses additional, unsupervised features. These features have been found to be useful in prior information extraction and NER tasks. The semi-supervised approach circumvents the need to include word n-gram features from any tweets, and builds upon the successful usage of word representations (Collobert et al., 2011) , and word clusters (Lin & Wu, 2009; Miller et al., 2004; Ratinov & Roth, 2009;<cite> Turian et al., 2010)</cite> for NER by utilizing large amounts of unlabelled data or models pre-trained on a large vocabulary.",
  "y": "background"
 },
 {
  "id": "b624e8d3ad3d351d4cb27ea4b5c616_1",
  "x": "---------------------------------- **WORD REPRESENTATIONS [WR]** Distributed word representations have been shown to improve the accuracy of NER systems (Collobert et al., 2011;<cite> Turian et al., 2010)</cite> .",
  "y": "background"
 },
 {
  "id": "b624e8d3ad3d351d4cb27ea4b5c616_2",
  "x": "**WORD CLUSTERS [WC]** Word clusters are word groupings that get generated in an unsupervised fashion, and they have been successfully used as features for NER tasks (Lin & Wu, 2009; Miller et al., 2004; Ratinov & Roth, 2009;<cite> Turian et al., 2010)</cite> . One algorithm for creating such sets is Brown clustering (Brown et al., 1992) , which produces a hierarchical cluster of words in the corpus while optimizing the likelihood of a language model based on a Hidden Markov Model (HMM).",
  "y": "background"
 },
 {
  "id": "b624e8d3ad3d351d4cb27ea4b5c616_3",
  "x": "Although it might appear that our classifier has access to the unlabelled test data sequences while learning, it rather is the case that we resemble an online setting where we continuously update our unsupervised features using the new batch of unlabelled test data, and then retrain our model on the original training data (Blum, 1998; Blum & Mitchell, 1998; Carlson et al., 2010; Chapelle et al., 2009; Liang, 2005; <cite>Turian et al., 2010</cite>; Zhu & Goldberg, 2009) . In this case, the unlabelled data prevent the classifier from overfitting to the training data by acting as a regularization factor. An alternative approach would be to train these clusters on a large number of unlabelled tweets that match the time range and search domain of the test tweets.",
  "y": "similarities"
 },
 {
  "id": "b624e8d3ad3d351d4cb27ea4b5c616_4",
  "x": "---------------------------------- **DISCUSSION AND CONCLUSION** Prior work has shown that semi-supervised algorithms can perform decently for NER tasks with sparse labelled data (Blum, 1998; Carlson et al., 2010; Chapelle et al., 2009; Liang, 2005; <cite>Turian et al., 2010</cite>; Zhu & Goldberg, 2009 ).",
  "y": "background"
 },
 {
  "id": "b624e8d3ad3d351d4cb27ea4b5c616_5",
  "x": "**DISCUSSION AND CONCLUSION** Prior work has shown that semi-supervised algorithms can perform decently for NER tasks with sparse labelled data (Blum, 1998; Carlson et al., 2010; Chapelle et al., 2009; Liang, 2005; <cite>Turian et al., 2010</cite>; Zhu & Goldberg, 2009 ). We leverage this fact in our SI model via the use of unsupervised word clusters, word representations, and refined gazetteers; all of which contributed to a cumulative increase in accuracy over our initial submission [ST] by ~11% when using the test data for evaluation.",
  "y": "uses background"
 },
 {
  "id": "b624e8d3ad3d351d4cb27ea4b5c616_6",
  "x": "Additionally, the supervised training of our classifier on features extracted from the unlabelled data, as opposed to lexical token features, reduces the dimensionality of the training data for the classifier and results in increased performance in terms of both accuracy and training time. Furthermore, our model can be adjusted on the arrival of new unlabelled data by updating the underlying learned word clusters and representations, and retraining the model on the existing labelled data. As identified by<cite> Turian et al. (2010)</cite> , the importance of word representations and word clusters increases as the availability of unlabelled data increases.",
  "y": "background"
 },
 {
  "id": "b645eee5044c17502bcdbc95934a01_1",
  "x": "These give rise to the research problem * Work done when the first author was visiting SUTD. of effectively making use of multiple treebanks under heterogeneous annotations for improving output accuracies (Jiang et al., 2015; Johansson, 2013;<cite> Li et al., 2015)</cite> . The task has been tackled using two typical approaches.",
  "y": "motivation"
 },
 {
  "id": "b645eee5044c17502bcdbc95934a01_2",
  "x": "This method has been used for leveraging two different treebanks for word segmentation (Jiang et al., 2009; Sun and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013) . The second approach is based on multi-view learning (Johansson, 2013;<cite> Li et al., 2015)</cite> . The idea is to address both annotation styles simultaneously by sharing common feature representations.",
  "y": "uses"
 },
 {
  "id": "b645eee5044c17502bcdbc95934a01_3",
  "x": "We follow<cite> Li et al. (2015)</cite> , taking POS-tagging for case study, using the methods of Jiang et al. (2009) and<cite> Li et al. (2015)</cite> as the discrete stacking and multi-view training baselines, respectively, and building neural network counterparts to their models for empirical comparison. The base tagger is a neural CRF model Lample et al., 2016) , which gives competitive accuracies to discrete CRF taggers. Results show that neural stacking allows deeper integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training.",
  "y": "uses"
 },
 {
  "id": "b645eee5044c17502bcdbc95934a01_4",
  "x": "First, it is free from the necessity of manual cross-labelset interactive feature engineering, which is far from trivial for representing annotation correspondence <cite>(Li et al., 2015)</cite> . Second, compared to discrete model, parameter sharing in deep neural network eliminates the issue of exponential growth of search space, and allows separated training of each label type, in the same way as multi-task learning (Collobert et al., 2011) . Our neural multi-view learning model achieves not only better accuracy improvements, but also an order of magnitude faster speed compared to its discrete baseline, adding little time cost compared to a neural model trained on a single treebank.",
  "y": "background"
 },
 {
  "id": "b645eee5044c17502bcdbc95934a01_5",
  "x": "We follow<cite> Li et al. (2015)</cite> , taking POS-tagging for case study, using the methods of Jiang et al. (2009) and<cite> Li et al. (2015)</cite> as the discrete stacking and multi-view training baselines, respectively, and building neural network counterparts to their models for empirical comparison. First, it is free from the necessity of manual cross-labelset interactive feature engineering, which is far from trivial for representing annotation correspondence <cite>(Li et al., 2015)</cite> .",
  "y": "uses background"
 },
 {
  "id": "b645eee5044c17502bcdbc95934a01_6",
  "x": "**DISCRETE LABEL COUPLING** As shown in Figure 1(b) , multi-view learning <cite>(Li et al., 2015)</cite> utilizes corpus A and corpus B simultaneously for training. The coupled tagger directly learns the logistic correspondences between both corpora, therefore can lead a more comprehensive usage of corpus A compared with stacking.",
  "y": "uses"
 },
 {
  "id": "b645eee5044c17502bcdbc95934a01_7",
  "x": "To train the neural stacking model, we first train a base tagger using corpus A. Then, we train the stacked tagger with corpus B, where the parameters of the A tagger has been pretrained from corpus A and the B tagger is randomly initialized. For neural multi-view model, we follow<cite> Li et al. (2015)</cite> and take a the corpus-weighting strategy to sample a number of training instances from both corpora for each training iteration, as shown in Algorithm 1. At each epoch, we randomly sample from the two datasets according to a corpus weights ratio, namely the ratio between the number of sentences in each dataset used for training, to form a training set for the epoch.",
  "y": "uses"
 },
 {
  "id": "b645eee5044c17502bcdbc95934a01_8",
  "x": "---------------------------------- **EXPERIMENTAL SETTINGS** We adopt the Penn Chinese Treebank version 5.0 (CTB5) (Xue et al., 2005) as our main corpus, with the standard data split following previous work (Zhang and Clark, 2008;<cite> Li et al., 2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "b645eee5044c17502bcdbc95934a01_9",
  "x": "However, when a 50% dropout rate is used, the initial performances are significantly worse, which implies that the 50% dropout rate can be too large and leads to underfitting. As a result, we choose a dropout rate of 20% for the remaining experiments, which strikes the balance between over-System Accuracy CRF Baseline <cite>(Li et al., 2015)</cite> 94.10 CRF Stacking <cite>(Li et al., 2015)</cite> 94.81 CRF Multi-view <cite>(Li et al., 2015)</cite> 95 Table 2 : Accuracies on CTB-test. fitting and underfitting.",
  "y": "uses"
 },
 {
  "id": "b645eee5044c17502bcdbc95934a01_10",
  "x": "Table 2 shows the final results on the CTB test data. We lists the results of stacking method of Jiang et al. (2009) re-implemented by<cite> Li et al. (2015)</cite> , and CRF multi-view method reported by<cite> Li et al. (2015)</cite> . We adopt pair-wise significance test (Collins et al., 2005) when comparing the results between two different models.",
  "y": "uses"
 },
 {
  "id": "b645eee5044c17502bcdbc95934a01_11",
  "x": "This shows that neural stacking is a preferred choice for stacking. Multi-view training. With respect of the multiview training method, the NN model improves over the NN baseline from 94.24 to 95.40, by a margin of +1.16, which is higher than that of 0.90 brought by discrete method of<cite> Li et al. (2015)</cite> over its baseline, from 94.10 to 95.00.",
  "y": "differences"
 },
 {
  "id": "b645eee5044c17502bcdbc95934a01_12",
  "x": "This is consistent with the observation of<cite> Li et al. (2015)</cite> , who showed that discrete label coupling training gives slightly better improvement compared with discrete stacking. The final accuracies of NN multi-view training is also higher than that of its CRF counterpart, namely 95.00 vs 95.40 at the confidence level p < 10 \u22123 . The difference between the final NN multi-view training result of 95.40 and the final NN stacking results is not significant.",
  "y": "similarities"
 },
 {
  "id": "b645eee5044c17502bcdbc95934a01_13",
  "x": "**SPEED TEST** We compare the efficiencies of neural and discrete multi-view training by running our models and the model of<cite> Li et al. (2015)</cite> 4 with default configurations on the CTB5 training data. The CRF baseline is adapted from<cite> Li et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "b645eee5044c17502bcdbc95934a01_14",
  "x": "The CRF baseline is adapted from<cite> Li et al. (2015)</cite> . All the systems are implemented in C++ running on an Intel E5-1620 CPU. The results are shown in Table 3 .",
  "y": "extends"
 },
 {
  "id": "b6afd492c60af7ab1ba0be3cd654b2_0",
  "x": "It is able to surpass n-gram-based scores achieved previously by <cite>Du\u0161ek and Jur\u010d\u00ed\u010dek (2015)</cite> , offering a simpler setup and more relevant outputs. We introduce the generation setting in Section 2 and describe our generator architecture in Section 3. Section 4 details our experiments, Section 5 analyzes the results.",
  "y": "differences"
 },
 {
  "id": "b6afd492c60af7ab1ba0be3cd654b2_1",
  "x": "The second generator mode joins sentence planning and surface realization into one step, producing natural language sentences directly. Both modes offer their advantages: The twostep mode simplifies generation by abstracting away from complex surface syntax and morphology, which can be handled by a handcrafted, domain-independent module to ensure grammatical correctness at all times<cite> (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2015)</cite> , and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline (Konstas and Lapata, 2013) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "b6afd492c60af7ab1ba0be3cd654b2_2",
  "x": "Our generator operates in two modes, producing either deep syntax trees (Du\u0161ek et al., 2012) or natural language strings (see Fig. 1 ). Both modes offer their advantages: The twostep mode simplifies generation by abstracting away from complex surface syntax and morphology, which can be handled by a handcrafted, domain-independent module to ensure grammatical correctness at all times<cite> (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2015)</cite> , and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline (Konstas and Lapata, 2013) .",
  "y": "uses"
 },
 {
  "id": "b6afd492c60af7ab1ba0be3cd654b2_3",
  "x": "10 The surface realizer works almost flawlessly on this lim-ited domain<cite> (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2015)</cite> , leaving the seq2seq generator as the major error source. The syntax-generating models tend to make different kinds of errors than the string-based models: Some outputs are valid trees but not entirely syntactically fluent; missing, incorrect, or repeated information is more frequent than a confusion of semantically similar items (see Table 2 ). Semantic error rates of greedy and beam-search decoding are lower than for string-based models, partly because confusion of two similar items counts as two errors.",
  "y": "uses"
 },
 {
  "id": "b6afd492c60af7ab1ba0be3cd654b2_4",
  "x": "The best results of both setups surpass the best results on this dataset using training data without manual alignments<cite> (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2015)</cite> in both automatic metrics 12 and the number of semantic errors. ---------------------------------- **RELATED WORK**",
  "y": "differences"
 },
 {
  "id": "b6afd492c60af7ab1ba0be3cd654b2_5",
  "x": "**RELATED WORK** While most recent NLG systems attempt to learn generation from data, the choice of a particular approach -pipeline or joint -is often arbitrary and depends on system architecture or particular generation domain. Works using the pipeline approach in SDS tend to focus on sentence planning, improving a handcrafted generator (Walker et al., 2001; Stent et al., 2004; Paiva and Evans, 2005) or using perceptron-guided A* search<cite> (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "b6afd492c60af7ab1ba0be3cd654b2_6",
  "x": "Works using the pipeline approach in SDS tend to focus on sentence planning, improving a handcrafted generator (Walker et al., 2001; Stent et al., 2004; Paiva and Evans, 2005) or using perceptron-guided A* search<cite> (Du\u0161ek and Jur\u010d\u00ed\u010dek, 2015)</cite> . We extend the previous works by generating deep syntax trees as well as strings and directly comparing pipeline and joint generation.",
  "y": "extends"
 },
 {
  "id": "b6afd492c60af7ab1ba0be3cd654b2_7",
  "x": "We also showed that our generator can learn to produce meaningful utterances using a much smaller amount of training data than what is typically used for RNN-based approaches. The resulting models had virtually no problems with produc-ing fluent, coherent sentences or with generating valid structure of bracketed deep syntax trees. Our generator was able to surpass the best BLEU/NIST scores on the same dataset previously achieved by a perceptron-based generator of <cite>Du\u0161ek and Jur\u010d\u00ed\u010dek (2015)</cite> while reducing the amount of irrelevant information on the output.",
  "y": "differences"
 },
 {
  "id": "b6c33fbb73cbf0af580e8dd14dc59a_0",
  "x": "When it comes to poetry generation using generative text models, Zhang and Lapata [10] , Yi et al. [11] and Wang et al. [12] use language modeling to generate Chinese poems. However, none of these methods provide feedback on the quality of the generated sample and hence, do not address the qualitative objective required for creative decoding. For the task of text generation, MaskGAN [13] uses a Reinforcement Learning signal from the discriminator, FMD-GAN<cite> [14]</cite> uses an optimal transport mechanism as an objective function.",
  "y": "background"
 },
 {
  "id": "b6c33fbb73cbf0af580e8dd14dc59a_1",
  "x": "GANs have shown to be successful in image generation tasks [18] and recently, some progress has been observed in text generation <cite>[14,</cite> 13, 16] . Our generator is a language model trained using backpropagation through time [19] . During the pre-training phase we optimize for MLE and during the GAN training phase, we optimize on the creativity reward from the discriminator.",
  "y": "background"
 },
 {
  "id": "b6efc2f5239a0c5d9210d7da8466ab_0",
  "x": "Assuming access to a small amount * Performed while faculty at Johns Hopkins University of parallel text is realistic, especially considering the recent success of crowdsourcing translations (Zaidan and Callison-Burch, 2011; Ambati, 2011; <cite>Post et al., 2012)</cite> . We frame the shortcomings of SMT models trained on limited amounts of parallel text 1 in terms of accuracy and coverage. In this context, coverage refers to the number of words and phrases that a model has any knowledge of at all, and it is low when the training text is small, which results in a high out-of-vocabulary (OOV) rate.",
  "y": "motivation"
 },
 {
  "id": "b6efc2f5239a0c5d9210d7da8466ab_1",
  "x": "Adding these translations by definition improves the coverage of our MT models. Then, in additional sets of experiments, we 4 GIZA++ intersection alignments over all training data. 5 The<cite> Post et al. (2012)</cite> also induce translations for source language words which are low frequency in the training data and supplement our SMT models with top-k translations, not just the highest ranked.",
  "y": "extends differences"
 },
 {
  "id": "b6efc2f5239a0c5d9210d7da8466ab_2",
  "x": "**EXPERIMENTS 4.1 EXPERIMENTAL SETUP** We use the data splits given by<cite> Post et al. (2012)</cite> and, following that work, include the dictionaries in the training data and report results on the devtest set using case-insensitive BLEU and four references. We use the Moses phrase-based MT framework (Koehn et al., 2007) .",
  "y": "similarities uses"
 },
 {
  "id": "b6efc2f5239a0c5d9210d7da8466ab_3",
  "x": "We use the Moses phrase-based MT framework (Koehn et al., 2007) . For each language, we extract a phrase table with a phrase limit of seven. In order to make our results comparable to those of<cite> Post et al. (2012)</cite> , we follow that work and use Table 3 : Percent of word types in a held out portion of the training data which are translated correctly by our bilingual lexicon induction technique.",
  "y": "similarities uses"
 },
 {
  "id": "b6efc2f5239a0c5d9210d7da8466ab_4",
  "x": "We retrain with all training data for MT experiments. 9<cite> Post et al. (2012)</cite> gathered up to six translations for each source word, so some have multiple correct translations appending the top-k translations for OOV words to our model instead of just the top-1. Table 4 shows our results adding OOV translations, adding features, and then both.",
  "y": "extends differences"
 },
 {
  "id": "b6efc2f5239a0c5d9210d7da8466ab_6",
  "x": "**LEARNING CURVES OVER PARALLEL DATA** In the experiments above, we only evaluated our methods for improving the accuracy and coverage of models trained on small amounts of bitext using the full parallel training corpora released by<cite> Post et al. (2012)</cite> . Here, we apply the same techniques but vary the amount of parallel data in order to generate learning curves.",
  "y": "uses"
 },
 {
  "id": "b6efc2f5239a0c5d9210d7da8466ab_7",
  "x": "As<cite> Post et al. (2012)</cite> showed, it is reasonable to assume a small parallel corpus for training an SMT model even in a low resource setting. We have used comparable corpora to improve the accuracy and coverage of phrase-based MT models built using small bilingual corpora for six low resource languages. We have shown that our methods improve BLEU score performance independently and that their combined impact is nearly additive.",
  "y": "similarities"
 },
 {
  "id": "b7158e8478b14cd8337f2aa8ee6193_0",
  "x": "This work aims at bridging the gap between many of these English-based systems and Pidgin by training an in-domain English-to-pidgin MT system in an unsupervised way. By this means, English-based NLG systems can be locally adapted by translating the output English text into pidgin English. We employ the publicly available parallel data-to-text corpus E2E <cite>(Novikova et al., 2017)</cite> consisting of tabulated data and English descriptions in the restaurant domain.",
  "y": "uses"
 },
 {
  "id": "b7158e8478b14cd8337f2aa8ee6193_1",
  "x": "**EXPERIMENTS AND RESULTS** We conduct experiments on the E2E corpus <cite>(Novikova et al., 2017)</cite> which amounts to roughly 42k samples in the training set. The monolingual Pidgin corpus contains 56,695 sentences and 32,925 unique words.",
  "y": "uses"
 },
 {
  "id": "b7158e8478b14cd8337f2aa8ee6193_2",
  "x": "However, there is an issue of domain mismatch between down-stream NLG tasks and the trained machine translation system. This creates a caveat where the resulting English-to-Pidgin MT systems (trained on the domain of news and the Bible) cannot be directly used to translate out-domain English texts to Pidgin. An example of the English/pidgin text in the restaurant domain <cite>(Novikova et al., 2017)</cite> is displayed in Table 1 .",
  "y": "uses"
 },
 {
  "id": "b71da01fb46900d81162b3a3c3cd41_0",
  "x": "One recently explored approach is to perform online reordering by swapping adjacent words of the input sentence while building the dependency structure. Using this technique, the system of <cite>Nivre (2009)</cite> processes unrestricted non-projective structures with state-ofthe-art accuracy in observed linear time. The normal procedure for training a transitionbased parser is to use an oracle that predicts an optimal transition sequence for every dependency tree in the training set, and then approximate this oracle by a classifier.",
  "y": "background"
 },
 {
  "id": "b71da01fb46900d81162b3a3c3cd41_1",
  "x": "In this paper, we show that the oracle used for training by <cite>Nivre (2009)</cite> is suboptimal because it eagerly swaps words as early as possible and therefore makes a large number of unnecessary transitions, which potentially affects both efficiency and accuracy. We propose an alternative oracle that reduces the number of transitions by building larger structures before swapping, but still handles arbitrary non-projective structures. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "b71da01fb46900d81162b3a3c3cd41_2",
  "x": "This idea is implemented in the transition system proposed by <cite>Nivre (2009)</cite> . The first three transitions of this system (LEFT-ARC, RIGHT-ARC, and SHIFT) are familiar from many systems for transition-based dependency parsing (Nivre, 2008) . The only novelty is the SWAP transition, which permutes two nodes by moving the second-topmost node from the stack back to the input buffer while leaving the top node on the stack.",
  "y": "background"
 },
 {
  "id": "b71da01fb46900d81162b3a3c3cd41_3",
  "x": "There may be more than one such permutation, but <cite>Nivre (2009)</cite> defines the canonical projective order < G for x given G as the order given by an inorder traversal of G that respects the order < between a node and its direct dependents. This is illustrated in Figure 1 , where the words of a sentence with a non-projective tree have been annotated with their positions in the projective order; reading the words in this order gives the permuted string Did you send the letter who to? ----------------------------------",
  "y": "background"
 },
 {
  "id": "b71da01fb46900d81162b3a3c3cd41_5",
  "x": "We now test the hypothesis that the new training oracle can improve both the accuracy and the efficiency of a transition-based dependency parser. Our experiments are based on the same five data sets as <cite>Nivre (2009)</cite> . The training sets vary in size from 28,750 tokens (1,534 sentences) for Slovene to 1,249,408 tokens (72,703 sentences) for Czech, while the test sets all consist of about 5,000 tokens.",
  "y": "uses similarities"
 },
 {
  "id": "b86a5a8ec1f27354057bb45ff27588_0",
  "x": "It differs significantly from various spoken varieties of Arabic (Zaidan and Callison-Burch, 2011; Zaidan and Callison-Burch, 2014;<cite> Elfardy and Diab, 2013)</cite> . Even though these dialects do not originally exist in written form, they are present in social media texts. Recently a dataset of dialectal Arabic has been made available in the form of the Arabic Online Commentary (AOC) set (Zaidan and Callison-Burch, 2011; Zaidan and CallisonBurch, 2014) .",
  "y": "background"
 },
 {
  "id": "b86a5a8ec1f27354057bb45ff27588_1",
  "x": "Similar to<cite> (Elfardy and Diab, 2013)</cite> , we present a sentence-level classifier that is trained in a supervised manner. Some improvements in terms of classification accuracy and 10-fold cross validation under the same data conditions as (Zaidan and Callison-Burch, 2011;<cite> Elfardy and Diab, 2013)</cite> are presented.",
  "y": "similarities"
 },
 {
  "id": "b86a5a8ec1f27354057bb45ff27588_2",
  "x": "Language identification of related languages is also addressed in the DSL (Discriminating Similar Languages) task of the present Vardial workshop at COLING 14 (Tan et al., 2014) . While most of the above work focuses on document-level language classification, recent work on handling Arabic dialect data addresses the problem of sentence-level classification (Zaidan and CallisonBurch, 2011; Zaidan and Callison-Burch, 2014; <cite>Elfardy and Diab, 2013</cite>; Zaidan and Callison-Burch, 2014 ). The work is based on the data collection effort by (Zaidan and Callison-Burch, 2014 ) which crowdsources the annotation task to workers on Amazons Mechanical Turk.",
  "y": "motivation"
 },
 {
  "id": "b86a5a8ec1f27354057bb45ff27588_3",
  "x": "We set the penalty term C = 0.5. For our experiments, we use the data set provided in (Zaidan and Callison-Burch, 2011 ) which also has been used in the experiments in<cite> (Elfardy and Diab, 2013</cite>; Zaidan and Callison-Burch, 2014) . We focus on the binary classification between MSA and ARZ.",
  "y": "similarities uses"
 },
 {
  "id": "b86a5a8ec1f27354057bb45ff27588_4",
  "x": "p MSA (\u00b7) is defined accordingly. In addition, we have implemented a range of so-called 'meta' features similar to the ones defined in<cite> (Elfardy and Diab, 2013)</cite> . For example, we define a feature \u03c6 Excl (t n 1 ) which is equal to the length of the longest consecutive sequence of exclamation marks in the tokenized sentence t n 1 .",
  "y": "similarities"
 },
 {
  "id": "b86a5a8ec1f27354057bb45ff27588_5",
  "x": "Contrary to<cite> (Elfardy and Diab, 2013)</cite> we find that those features do not improve accuracy of our best model in the cross-validation experiments. On the DEV12 set, the use of the meta features results in a significant drop in accuracy. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "b86a5a8ec1f27354057bb45ff27588_6",
  "x": "For the 10-fold cross validation experiments, 10 language models are built and perplexities are computed on 10 different test sets. The resulting (averaged) accuracy is 83.3 % for cross-validation and 82.2 % on the DEV12 tune set. In comparison,<cite> (Elfardy and Diab, 2013)</cite> reports an accuracy of 80.4 % as perplexity-based baseline.",
  "y": "differences"
 },
 {
  "id": "b86a5a8ec1f27354057bb45ff27588_7",
  "x": "The gold-standard PoS tags are given at the word-level, but the training uses a global representation at the sentence level. Similarly, we use linear SVMs to train a classification model at the sentence level without access to sentence length statistics, i.e. our best performing classifier does not compute features like the percentage of punctuation, numbers, or averaged word length as has been proposed previously<cite> (Elfardy and Diab, 2013)</cite> . All of our features are actually computed at the token level (with the exception of a single sentence-level AIDA-based feature).",
  "y": "differences"
 },
 {
  "id": "b87a8d14f1c2016caa7538aa08a33f_0",
  "x": "**INTRODUCTION** Computational approaches to argument mining/understanding have become very popular (Persing & Ng, 2016; Cano-Basave & He, 2016; Wei et al., 2016; Ghosh et al., 2016; Palau & Moens, 2009; Habernal & Gurevych, 2016) . One important avenue in this work is to understand the structure in argumentative text (Persing & Ng, 2016; Peldszus & Stede, 2015;<cite> Stab & Gurevych, 2016</cite>; Nguyen & Litman, 2016) .",
  "y": "uses"
 },
 {
  "id": "b87a8d14f1c2016caa7538aa08a33f_1",
  "x": "There are two key assumptions our work makes going forward. First, we assume subtask 1 has been completed, i.e. ACs have already been identified. Second, we follow previous work that assumes a tree structure for the linking of ACs (Palau & Moens, 2009; Cohen, 1987; Peldszus & Stede, 2015;<cite> Stab & Gurevych, 2016)</cite> Figure 1: An example of argument structure with four ACs.",
  "y": "uses"
 },
 {
  "id": "b87a8d14f1c2016caa7538aa08a33f_2",
  "x": "Figure 1 shows an example that we will use throughout the paper to concretely explain how our approach works. First, the left side of the figure presents the raw text of a paragraph in a persuasive essay<cite> (Stab & Gurevych, 2016)</cite> , with the ACs contained in square brackets. Squiggly verse straight underlining differentiates between claims and premises, respectively.",
  "y": "uses"
 },
 {
  "id": "b87a8d14f1c2016caa7538aa08a33f_3",
  "x": "We also specify the type of AC, with the head AC marked as claim and the remaining ACs marked as premise. Lastly, we note that the order of arguments components can be a strong indicator of how components should related. Linking to the first argument component can provide a competitive baseline heuristic (Peldszus & Stede, 2015;<cite> Stab & Gurevych, 2016)</cite> .",
  "y": "uses background"
 },
 {
  "id": "b87a8d14f1c2016caa7538aa08a33f_4",
  "x": "We evaluate our models on the corpora of <cite>Stab & Gurevych (2016)</cite> and Peldszus (2014) , and compare our results with the results of the aformentioned authors. ---------------------------------- **RELATED WORK**",
  "y": "uses"
 },
 {
  "id": "b87a8d14f1c2016caa7538aa08a33f_5",
  "x": "Peldszus & Stede (2015) have also used classification models for predicting the presence of links. Various authors have also proposed to jointly model link extraction with other subtasks from the argumentation mining pipeline, using either an Integer Linear Programming (ILP) framework (Persing & Ng, 2016;<cite> Stab & Gurevych, 2016)</cite> or directly feeding previous subtask predictions into another model. The former joint approaches are evaluated on annotated corpora of persuasive essays (Stab & Gurevych, 2014a; , and the latter on a corpus of microtexts (Peldszus, 2014) .",
  "y": "background"
 },
 {
  "id": "b87a8d14f1c2016caa7538aa08a33f_6",
  "x": "At each timestep of the decoder, the network takes in the representation of an AC. Each AC is itself a sequence of tokens, similar to the recently proposed Question-Answering dataset (Weston et al., 2015) . We follow the work of <cite>Stab & Gurevych (2016)</cite> and focus on three different types of features to represent our ACs: 1) Bag-of-Words of the AC; 2) Embedding representation based on GloVe embeddings (Pennington et al., 2014) ; 3) Structural features: Whether or not the AC is the first AC in a paragraph, and Whether the AC is in an opening, body, or closing paragraph.",
  "y": "uses"
 },
 {
  "id": "b87a8d14f1c2016caa7538aa08a33f_7",
  "x": "We test the effectiveness of our proposed model on a dataset of persuasive essays<cite> (Stab & Gurevych, 2016)</cite> , as well as a dataset of microtexts (Peldszus, 2014) . The feature space for the persuasive essay corpus has roughly 3,000 dimensions, and the microtext corpus feature space has between 2,500 and 3,000 dimensions, depending on the data split (see below). The persuasive essay corpus contains a total of 402 essays, with a frozen set of 80 essays held out for testing.",
  "y": "uses"
 },
 {
  "id": "b87a8d14f1c2016caa7538aa08a33f_8",
  "x": "Neither of these classifiers enforce structural or global constraints. Conversely, the ILP Joint Model<cite> (Stab & Gurevych, 2016)</cite> provides constrains by sharing prediction information between the base classifier. For example, the model attempts to enforce a tree structure among ACs within a given paragraph, as well as using incoming link predictions to better predict the type class claim.",
  "y": "uses background"
 },
 {
  "id": "b87a8d14f1c2016caa7538aa08a33f_9",
  "x": "In both corpora we compare against the following previously proposed models: Base Classifier (Stab & Gurevych, 2016 ) is feature-rich, task-specific (AC type or link extraction) SVM classifier. Neither of these classifiers enforce structural or global constraints. Conversely, the ILP Joint Model<cite> (Stab & Gurevych, 2016)</cite> provides constrains by sharing prediction information between the base classifier.",
  "y": "motivation uses"
 },
 {
  "id": "b87a8d14f1c2016caa7538aa08a33f_10",
  "x": "When considering type prediction, both BOW and structural features are important, and it is the embedding features that provide the least benefit. The Ablation results also provide an interesting insight into the effectiveness of different 'pooling' strategies for using individual token embeddings to create a multi-word embedding. The popular method of averaging embeddings (which is used by <cite>Stab & Gurevych (2016)</cite> in their system) is in fact the worst method, although its performance is still competitive with the previous state-of-the-art.",
  "y": "motivation"
 },
 {
  "id": "b87a8d14f1c2016caa7538aa08a33f_11",
  "x": "**CONCLUSION** In this paper we have proposed how to use a modified PN (Vinyals et al., 2015b) to extract links between ACs in argumentative text. We evaluate our models on two corpora: a corpus of persuasive essays<cite> (Stab & Gurevych, 2016)</cite> , and a corpus of microtexts (Peldszus, 2014) .",
  "y": "uses"
 },
 {
  "id": "b8d0e66901698d201b9fb1f362b8c6_0",
  "x": "Recently, neural approaches have reached very competitive accuracy levels, improving over the state of the art in a number of settings<cite> (Plank et al., 2016)</cite> . As a complement to annotated training corpora, external lexicons can be a valuable source of information. First, morphosyntactic lexicons provide a large inventory of (word, PoS) pairs.",
  "y": "background"
 },
 {
  "id": "b8d0e66901698d201b9fb1f362b8c6_1",
  "x": "Second, lexical information encoded in vector representations, known as word embeddings, have emerged more recently (Bengio et al., 2003; Collobert and Weston, 2008; Chrupa\u0142a, 2013; Ling et al., 2015; Ballesteros et al., 2015; M\u00fcller and Sch\u00fctze, 2015) . Such representations, often extracted from large amounts of raw text, have proved very useful for numerous tasks including PoS tagging, in particular when used in recurrent neural networks (RNNs) and more specifically in mono-or bi-directional, word-level or characterlevel long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997; Ling et al., 2015; Ballesteros et al., 2015; <cite>Plank et al., 2016)</cite> . Character-level embeddings are of particular interest for PoS tagging as they generate vector representations that result from the internal characterlevel make-up of each word.",
  "y": "background"
 },
 {
  "id": "b8d0e66901698d201b9fb1f362b8c6_2",
  "x": "---------------------------------- **BASELINE BI-LSTM TAGGER** As shown by<cite> Plank et al. (2016)</cite> , state-of-the-art performance can be achieved using a bi-LSTM architecture fed with word representations.",
  "y": "background"
 },
 {
  "id": "b8d0e66901698d201b9fb1f362b8c6_3",
  "x": "Optimal performance is achieved representing words using the concatenation of (i) a word vector w built using a word embedding layer, called its word embedding, and (ii) a representation c of the word's characters, called its character-based embedding built using a character-level bi-LSTM, which is trained jointly with the word-level layers. Further improvements can be obtained on most but not all languages by initialising the word embedding layer with pre-computed word embeddings. We refer to<cite> Plank et al. (2016)</cite> for further details.",
  "y": "motivation"
 },
 {
  "id": "b8d0e66901698d201b9fb1f362b8c6_4",
  "x": "Best improvements are expected for these words. Pre-computed embeddings Whenever available and following<cite> Plank et al. (2016)</cite> , we performed experiments using Polyglot pre-computed embeddings (Al-Rfou et al., 2013) . Languages for which Polyglot embeddings are available are indicated in Table 1 .",
  "y": "uses"
 },
 {
  "id": "b8d0e66901698d201b9fb1f362b8c6_5",
  "x": "Tab. 1) best lexicon w w + c w P + c w + l w + c + l w P + c + l w(+ l) w + c(+ l) w P + c(+ l) Table 3 : Overall results. PoS accuracy scores are given for each language in the baseline configuration (the same as <cite>Plank et al., 2016)</cite> and in the lexicon-enabled configuration.",
  "y": "uses"
 },
 {
  "id": "b8d0e66901698d201b9fb1f362b8c6_6",
  "x": "**EXPERIMENTAL SETUP** We use as a baseline the state-of-the-art bi-LSTM PoS tagger bilty, a freely available 6 and \"significantly refactored version of the code originally used\" by<cite> Plank et al. (2016)</cite> . We use its standard configuration, with one bi-LSTM layer, characterbased embeddings size of 100, word embedding size of 64 (same as Polyglot embeddings), no multitask learning, 7 and 20 iterations for training.",
  "y": "uses"
 },
 {
  "id": "b8f090dadfbd01a17912e006e7ccfc_0",
  "x": "Examples of combinations of distinct strategies are described in [9] , [<cite>19</cite>] and [20]. In sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels. Despite the limitations imposed by the restrict understanding of the mechanisms behind the classification, it is worth noting that the such representation remains entirely generic, being therefore useful to many tasks as well as for analyzing the evolution of languages, cultures and emotional trends.",
  "y": "background"
 },
 {
  "id": "b8f090dadfbd01a17912e006e7ccfc_1",
  "x": "Alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. In (ii), I suggest, for example, the introduction of a hybrid classifier that could consider both linguistic (deeper linguistic processing [18] ) and topological attributes at the same time in a hybrid way. Examples of combinations of distinct strategies are described in [9] , [<cite>19</cite>] and [20] .",
  "y": "background"
 },
 {
  "id": "b9a748ac201b2d8f5d52abd60aa018_0",
  "x": "Several studies have examined listenability for English learners (Kiyokawa 1990;<cite> Kotani et al. 2014</cite>; Kotani & Yoshimi 2016; Yoon et al. 2016) ; however, to the best of our knowledge, no previous studies on listenability for learners of Asian languages such as Chinese, Korean, and Japanese have been conducted. The method of Kiyokawa (1990) measured listenability based on the length of sentences and the difficulty of words. It was hypothesized that the listenability of a sentence decreases as it becomes longer and contains more advanced vocabulary words.",
  "y": "background"
 },
 {
  "id": "b9a748ac201b2d8f5d52abd60aa018_1",
  "x": "<cite>Kotani et al. (2014)</cite> suggested the possibility of using different linguistic elements such as phonological features, and addressed this question by measuring listenability based on various linguistic features, including speech rate and the frequency of phonological modification patterns such as linking. In addition, their method used listening test scores as a learner feature to measure listenability relative to proficiency. This is because sentences with less listenability for learners at the beginner level might be easy for those at the advanced level. However, because that study focused on the accuracy of measurement, the question of discriminability of This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ linguistic and learner features for the measurement of listenability remained.",
  "y": "background motivation"
 },
 {
  "id": "b9a748ac201b2d8f5d52abd60aa018_2",
  "x": "Listenability is measured based on linguistic and learner features. Linguistic features explain the difficulty of a sentence, and learner features explain the proficiency of a learner. The linguistic (Chall 1948; Fang 1966; Kiyokawa 1990; Messerklinger 2006;<cite> Kotani et al. 2014</cite>; Kotani & Yoshimi 2016; Yoon et al. 2016) , and learner features<cite> (Kotani et al. 2014</cite>; Kotani & Yoshimi 2016) used in this study were originally described elsewhere.",
  "y": "uses"
 },
 {
  "id": "b9a748ac201b2d8f5d52abd60aa018_3",
  "x": "Training/test data for a decision tree classification algorithm were constructed using the learner corpus of <cite>Kotani et al. (2014)</cite> , which includes learners' judgment of listenability. Listenability was judged by learners of English as a foreign language using scores on a five-point Likert scale (1: easy, 2: somewhat easy, 3: average, 4: somewhat difficult, or 5: difficult). Scores were judged on a sentenceby-sentence basis where each learner listened to and assigned scores for 80 sentences from four news clips selected from the editorial and special sections for English learners on the Voice of America (VOA) website (http://www.voanews.com).",
  "y": "uses"
 },
 {
  "id": "b9e1a6bb7b6f2d55c6ea3bb2d3897d_0",
  "x": "**INTRODUCTION** The automatic identification of information status (Prince, 1981; 1992) , i.e. categorizing discourse entities into different classes on the given-new scale, has recently been identified as an important issue in natural language processing (Nissim, 2006;<cite> Rahman and Ng, 2011</cite>; . It is widely acknowledged that information status and, more generally, information structure, 1 is reflected in word order, in the form of referring expressions as well as in prosody.",
  "y": "motivation"
 },
 {
  "id": "b9e1a6bb7b6f2d55c6ea3bb2d3897d_1",
  "x": "In the linguistic literature, referring expressions have been distinguished in much more detail, and there is reason to believe that this could also provide useful information for NLP applications. Nissim (2006) and <cite>Rahman and Ng (2011)</cite> developed methods to automatically identify three different classes: OLD, MEDIATED and NEW expressions. This classification, which is described in Nissim et al. (2004) , has been used for annotating the Switchboard dialog corpus (Calhoun et al., 2010) , on which both studies are based.",
  "y": "background"
 },
 {
  "id": "b9e1a6bb7b6f2d55c6ea3bb2d3897d_2",
  "x": "Based on work described in Nissim (2006) , <cite>Rahman and Ng (2011)</cite> develop a machine learning approach to information-status determination. They develop a support vector machine (SVM) model from the annotated Switchboard dialogs in order to predict the three possible classes. In an extension of this work, Rahman and Ng (2012) compare a rule-based system to a classifier with features based on the rules to predict 16 subtypes of the three basic types.",
  "y": "background"
 },
 {
  "id": "b9e1a6bb7b6f2d55c6ea3bb2d3897d_3",
  "x": "In manual annotation practice, it is very often impossible to decide whether an entity is hearerknown, since this depends on who we assume the hearer to be; and even if we agree on a recipient, we may still be mistaken about their knowledge. The fact that <cite>Rahman and Ng (2011)</cite> report the highest confusion rate between NEW and MEDIATED entities may have its roots in this issue.",
  "y": "motivation background"
 },
 {
  "id": "b9e1a6bb7b6f2d55c6ea3bb2d3897d_4",
  "x": "This does not rule out further subclassification (known/unknown) or the possibility of using machine learning techniques to identify this distinction, see Nenkova et al. (2005) . The fact that <cite>Rahman and Ng (2011)</cite> report the highest confusion rate between NEW and MEDIATED entities may have its roots in this issue. New.",
  "y": "motivation background"
 },
 {
  "id": "b9e1a6bb7b6f2d55c6ea3bb2d3897d_5",
  "x": "We also include a basic \"coreference\" feature, similar to the lexical features of <cite>Rahman and Ng (2011)</cite> , that fires if there is some lexical overlap of nouns (or compound nouns) in the preceding 10 sentences. The original label set described in contains 21 labels. Here we work with a subset of maximally 12 labels, but also consider smaller subsets of labels and carry out a mapping to the Nissim (2006) label set (Table 2) .",
  "y": "similarities"
 },
 {
  "id": "b9e9f358ace19da43bfe9e5bc380c5_0",
  "x": "It is also interesting to study the influence of other linguistic characteristics of Chinese, such as zero-pronoun, on its SQL parsing. We investigate parsing Chinese questions to SQL by creating a first dataset, and empirically evaluating a strong baseline model on the dataset. In particular, we translate the Spider<cite> (Yu et al., 2018b)</cite> dataset into Chinese.",
  "y": "extends differences"
 },
 {
  "id": "b9e9f358ace19da43bfe9e5bc380c5_1",
  "x": "Among a wide range of possible semantic representations, SQL offers a standardized interface to knowledge bases across tasks (Astrova, 2009; Xu et al., 2017; Dong and Lapata, 2018; Lee et al., 2011) . Recently, <cite>Yu et al. (2018b)</cite> released a manually labelled dataset for parsing natural language questions into complex SQL, which facilitates related research. <cite>Yu et al. (2018b)</cite> 's dataset is exclusive for English questions.",
  "y": "background"
 },
 {
  "id": "b9e9f358ace19da43bfe9e5bc380c5_2",
  "x": "<cite>Yu et al. (2018b)</cite> 's dataset is exclusive for English questions. Intuitively, the same semantic parsing task can be applied cross-lingual, since SQL is a universal semantic representation and database interface. However, for languages other than English, there can be added difficulties parsing into SQL. Take Chinese for example, the additional challenges can be at least two-fold. First, structures of relational databases, in particular names and column names of DB tables, are typically represented in English. This adds to the challenges to question-to-DB mapping.",
  "y": "motivation"
 },
 {
  "id": "b9e9f358ace19da43bfe9e5bc380c5_3",
  "x": "The first uses logic for semantic representation, including ATIS (Price, 1990; Dahl et al., 1994) and GeroQuery (Zelle and Mooney, 1996) . The second and dominant category of datasets uses SQL, which includes Restaurants (Tang and Mooney, 2001; Popescu et al., 2003) , Academic (Iyer et al., 2017) , Yelp and IMDB (Yaghmazadeh et al., 2017) , Ad-vising (Finegan-Dollak et al., 2018) and the recently proposed WikiSQL (Zhong et al., 2017) and Spider<cite> (Yu et al., 2018b)</cite> . One salient difference between Spider and prior work is that Spider uses different databases across domains for training and testing, which can verify the generalization power of a semantic parsing model.",
  "y": "background"
 },
 {
  "id": "b9e9f358ace19da43bfe9e5bc380c5_4",
  "x": "There are originally 10181 questions from Spider, but only 9691 for the training and development sets are publicly available. We thus translated these sentences only. Following the database split setting of <cite>Yu et al. (2018b)</cite> , we make training, development and test sets split in a way that no database overlaps in them as shown in Table 1 .",
  "y": "uses"
 },
 {
  "id": "b9e9f358ace19da43bfe9e5bc380c5_5",
  "x": "Evaluation Metrics. We follow <cite>Yu et al. (2018b)</cite> , evaluating the results using two major 2 https://nlp.stanford.edu/projects/glove/ 3 https://ai.tencent.com/ailab/nlp/embedding.html types of metrics. The first is exact matching accuracy, namely the percentage of questions that have exactly the same SQL output as its reference.",
  "y": "uses"
 },
 {
  "id": "bbacc6539a7346e4f30a1ae42a636e_0",
  "x": "Chapter 2 focuses on how the statistical framework of a syntax-based SMT approach learns its model from a word-aligned and parsed parallel text. The first section explains how phrase pairs are extracted as translation rules from a word-aligned sentence pair in phrase-based SMT (Koehn, Och, and Marcu 2003) , highlighting the definition of a phrase as a sequence of words and the alignment-consistency property of a phrase pair as defined in Och and Ney (2004) . The remainder of the chapter introduces three predominant instantiations of syntax-based models: hierarchical phrase-based SMT (Hiero) <cite>(Chiang 2007)</cite> , which is a non-labeled syntax-based SMT approach arising from the phrase-based approach; syntax-augmented machine translation (SAMT), which introduces the notion of soft labels while keeping the nonlinguistic phrase notion; and GHKM (Galley et al. 2004 ), which only extracts translation rules consistent with constituency parse subtrees.",
  "y": "background"
 },
 {
  "id": "bbd8c1007b573758fc78a6d16e2b77_0",
  "x": "One of the main goals in statistical natural language parsing is to find the minimal set of statistical dependencies (between words and syntactic structures) which achieves maximal parse accuracy. Many stochastic parsing models use linguistic intuitions to find this minimal set, for example by restricting the statistical dependencies to the locality of headwords of constituents (Collins 1997<cite> (Collins , 1999</cite> Eisner 1997) , leaving it as an open question whether there exist important statistical dependencies that go beyond linguistically motivated dependencies. The Data Oriented Parsing model, on the other hand, takes a rather extreme view on this issue: it does not single out a narrowly predefined set of structures as the statistically significant ones; given an annotated corpus, all fragments (i.e., subtrees) seen in that corpus, regardless of size and lexicalization, are in principle taken to form a grammar (see Bod 1992 Bod , 1998 Bod & Kaplan 1998; Bonnema et al. 1997; Cormons 1999; Goodman 1996 Goodman , 1998 Kaplan 1996; de Pauw 2000; Scha 1990; Sima'an 1995 Sima'an , 1999 Way 1999) .",
  "y": "background"
 },
 {
  "id": "bbd8c1007b573758fc78a6d16e2b77_1",
  "x": "This distinguishes DOP1 from most other statistical parsing models that identify exactly one derivation for each parse tree and thus compute the probability of a tree by only one product of probabilities --see Collins (1997<cite> Collins ( , 1999</cite> , Charniak (1997 Charniak ( , 2000 and Eisner (1997 (Dempster et al. 1977) , but this resulted in a decrease in parse accuracy on the ATIS and OVIS corpora (Bod 2000a) , although it slightly improved the word error rate for OVIS word-graphs. Other ways for estimating DOP1's parameters have also been proposed: Bonnema et al. (1999) estimate the probability of a subtree as the probability that it has been involved in the derivation of a corpus tree, but it is not yet known how their estimator compares experimentally to DOP1's relative frequency estimator. Since the relative frequency estimator has not been surpassed by any other estimator for DOP1 (at least in natural language parsing), we will stick to this estimator for the rest of this paper.",
  "y": "background"
 },
 {
  "id": "bbd8c1007b573758fc78a6d16e2b77_2",
  "x": "**THE BASE LINE** For our base line parse accuracy, we used the now standard division of the WSJ (see Collins 1997 <cite>Collins , 1999</cite> Charniak 1997 Charniak , 2000 Ratnaparkhi 1999 ) with sections 2 through 21 for training (approx. 40,000 sentences) and section 23 for testing (2416 sentences \u2264 100 words); section 22 was used as development set. All trees were stripped off their semantic tags, co-reference information and quotation marks.",
  "y": "similarities"
 },
 {
  "id": "bbd8c1007b573758fc78a6d16e2b77_3",
  "x": "We may also raise the question as to whether we need almost arbitrarily large lexicalized subtrees (up to 12 words) to obtain our best results. It could be the case that DOP's gain in parse accuracy with increasing subtree depth is due to the model becoming sensitive to the influence of lexical heads higher in the tree, and that this gain could also be achieved by a more compact model which annotates the nonterminals with their headwords. Such \"head-lexicalized stochastic grammars\" have recently become increasingly popular (e.g. Collins 1997 <cite>Collins , 1999</cite> Charniak 1997 Charniak , 2000 and are based on Magerman's head-percolation scheme to determine the headword of each nonterminal (Magerman 1995) .",
  "y": "background"
 },
 {
  "id": "bbd8c1007b573758fc78a6d16e2b77_4",
  "x": "We should note, however, that most other stochastic parsers do include counts of single nonheadwords: they appear in the backed-off statistics of these parsers (see Collins 1997 <cite>Collins , 1999</cite> Charniak 1997; Goodman 1998) . But our parser is the first parser that also includes counts between two or more nonheadwords, to the best of our knowledge, and these counts lead to improved performance, as can be seen in the table above. ----------------------------------",
  "y": "background"
 },
 {
  "id": "bbd8c1007b573758fc78a6d16e2b77_5",
  "x": "A major difference between our approach and most other models tested on the WSJ is that the DOP model uses frontier lexicalization while most other models use, what we might call, constituent lexicalization (in that it associates each constituent nonterminal with its lexical head --see Collins 1997 <cite>Collins , 1999</cite> Charniak 1997; Eisner 1997) . The results in this paper, especially those of section 4.5, indicate that our use of frontier lexicalization is a more flexible and promising approach than the use of constituent lexicalization (although our CPU time per sentence is much longer). Our results also show that the linguistically motivated constraint which limits the statistical dependencies to the locality of headwords of constituents is too narrow.",
  "y": "differences"
 },
 {
  "id": "bbd8c1007b573758fc78a6d16e2b77_6",
  "x": "While earlier head-lexicalized models restricted fragments to the locality of headwords of constituents (e.g. Collins 1996; Eisner 1996) , later models showed the importance of including additional context from higher nodes in the tree, resulting in improved parse accuracy (Charniak 1997; Johnson 1998 ). This mirrors our result of the utility of fragments of depth 2 (and larger) which was already reported in Bod (1993) . The importance of including counts of (single) nonheadwords is now also quite uncontroversial (e.g. Collins 1997 <cite>Collins , 1999</cite> Charniak 2000) , and the current paper has shown the importance of including two and more nonheadwords.",
  "y": "similarities"
 },
 {
  "id": "bce5c3bf551a8aa211dfd962cde7a8_0",
  "x": "performance. This makes the task a nice testbed for the cross-fertilization of various language processing techniques. As an example of such work, <cite>Zhang et al. (2008)</cite> have shown in the past that deep linguistic parsing outputs can be integrated to help improve the performance of the English semantic role labeling task.",
  "y": "background"
 },
 {
  "id": "bce5c3bf551a8aa211dfd962cde7a8_1",
  "x": "The overall system architecture is shown in Figure 1 . It is similar to the architecture used by <cite>Zhang et al. (2008)</cite> . Three major components were involved.",
  "y": "similarities"
 },
 {
  "id": "bce5c3bf551a8aa211dfd962cde7a8_2",
  "x": "Comparing to <cite>Zhang et al. (2008)</cite> , this architecture simplified the syntactic component, and puts more focus on the integration of deep parsing outputs. While <cite>Zhang et al. (2008)</cite> only used seman- tic features from HPSG parsing in the SRL task, we added extra syntactic features from deep parsing to help both tasks. Flickinger, 2000) , German (GG; Crysmann, 2005) , Japanese (JaCY; Siegel & Bender, 2002) , and Spanish (SRG; Marimon, Bel, & Seghezzi, 2007) .",
  "y": "differences"
 },
 {
  "id": "bce5c3bf551a8aa211dfd962cde7a8_3",
  "x": "Comparing to <cite>Zhang et al. (2008)</cite> , this architecture simplified the syntactic component, and puts more focus on the integration of deep parsing outputs. While <cite>Zhang et al. (2008)</cite> only used seman- tic features from HPSG parsing in the SRL task, we added extra syntactic features from deep parsing to help both tasks. Flickinger, 2000) , German (GG; Crysmann, 2005) , Japanese (JaCY; Siegel & Bender, 2002) , and Spanish (SRG; Marimon, Bel, & Seghezzi, 2007) .",
  "y": "differences"
 },
 {
  "id": "bce5c3bf551a8aa211dfd962cde7a8_4",
  "x": "Deep Semantic Features Similar to <cite>Zhang et al. (2008)</cite> , we extract a set of features from the semantic outputs (MRS) of the HPSG parses. These features represent the basic predicate-argument structure, and provides a simplified semantic view on the target sentence. Deep Syntactic Dependency Features A HPSG derivation is a tree structure.",
  "y": "similarities"
 },
 {
  "id": "bce5c3bf551a8aa211dfd962cde7a8_5",
  "x": "The semantic role labeling component used in the submitted system is similar to the one described by <cite>Zhang et al. (2008)</cite> . Since predicates are indicated in the data, the predicate identification module is removed from this year's system. Argument identification, argument classification and predicate classification are the three sub-components in the pipeline.",
  "y": "similarities"
 },
 {
  "id": "bce5c3bf551a8aa211dfd962cde7a8_6",
  "x": "The most interesting observation is that even with grammars which only achieve very limited coverage, noticeable SRL improvements are obtained. Confirming the observation of <cite>Zhang et al. (2008)</cite> , the gain with HPSG features is more significant on outdomain tests, this time on German as well. The training of the syntactic parsing models for all seven languages with MST parser takes about 100 CPU hours with 10 iterations.",
  "y": "similarities"
 },
 {
  "id": "bce5c3bf551a8aa211dfd962cde7a8_7",
  "x": "Four hand-written HPSG grammars of a variety of scale have been applied to parse the datasets, and the outcomes were integrated as features into the semantic role labeler of the system. The results clearly show that the integration of HPSG parsing results in the semantic role labeling task brings substantial performance improvement. The conclusion of <cite>Zhang et al. (2008)</cite> has been reconfirmed on multiple languages for which we handbuilt HPSG grammars exist, even where grammatical coverage is low.",
  "y": "background"
 },
 {
  "id": "bcf19914bb67ded47785d298969a7a_0",
  "x": "However, whether or not these models are actually learning to address the tasks they are designed for is questionable. For example, Hodosh and Hockenmaier (2016) showed that IC models do not understand images sufficiently, as reflected by the generated captions. As a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and/or linguistic information<cite> (Shekhar et al., 2017b</cite>; Johnson et al., 2017; Antol et al., 2015; Chen et al., 2015; Gao et al., 2015; Yu et al., 2015; Zhu et al., 2016) .",
  "y": "background"
 },
 {
  "id": "bcf19914bb67ded47785d298969a7a_1",
  "x": "As a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and/or linguistic information<cite> (Shekhar et al., 2017b</cite>; Johnson et al., 2017; Antol et al., 2015; Chen et al., 2015; Gao et al., 2015; Yu et al., 2015; Zhu et al., 2016) . FOIL<cite> (Shekhar et al., 2017b</cite> ) is one such dataset. It was proposed to evaluate the ability of V2L models in understanding the interplay of objects and their attributes in the images and their relations in an image captioning framework.",
  "y": "background"
 },
 {
  "id": "bcf19914bb67ded47785d298969a7a_2",
  "x": "**BACKGROUND** In this section we describe the foiled caption classification task and dataset. We combine the tasks and data from<cite> Shekhar et al. (2017b)</cite> and Shekhar et al. (2017a) .",
  "y": "extends uses"
 },
 {
  "id": "bcf19914bb67ded47785d298969a7a_3",
  "x": "\u2022 Foiled Preposition: Prepositions are directly replaced with functionally similar prepositions. The Verb, Adjective, Adverb and Preposition subsets were obtained using a slightly different methodology (see Shekhar et al. (2017a) ) than that used for Nouns<cite> (Shekhar et al., 2017b)</cite> . Therefore, we evaluate these two groups separately.",
  "y": "background"
 },
 {
  "id": "bcf19914bb67ded47785d298969a7a_4",
  "x": "\u2022 Foiled Preposition: Prepositions are directly replaced with functionally similar prepositions. The Verb, Adjective, Adverb and Preposition subsets were obtained using a slightly different methodology (see Shekhar et al. (2017a) ) than that used for Nouns<cite> (Shekhar et al., 2017b)</cite> . Therefore, we evaluate these two groups separately.",
  "y": "uses"
 },
 {
  "id": "bcf19914bb67ded47785d298969a7a_5",
  "x": "**EXPERIMENTS** Data: We use the dataset for nouns from<cite> Shekhar et al. (2017b)</cite> 1 and the datasets for other parts of speech from Shekhar et al. (2017a) 2 . Statistics about the dataset are given in Table 1 .",
  "y": "uses"
 },
 {
  "id": "bcf19914bb67ded47785d298969a7a_7",
  "x": "bag of objects information are the best performing models across classifiers. We also note that the performance is better than human performance. We hypothesize the following reasons for this: (a) human responses were crowd-sourced, which could have resulted in some noisy annotations; (b) our gold object-based features closely resembles the information used for data-generation as described in<cite> Shekhar et al. (2017b)</cite> for the foil noun dataset.",
  "y": "uses"
 },
 {
  "id": "bcf19914bb67ded47785d298969a7a_8",
  "x": "We also note that the performance is better than human performance. We hypothesize the following reasons for this: (a) human responses were crowd-sourced, which could have resulted in some noisy annotations; (b) our gold object-based features closely resembles the information used for data-generation as described in<cite> Shekhar et al. (2017b)</cite> for the foil noun dataset. The models using Predicted bag of objects from a detector are very close to the performance of Gold.",
  "y": "similarities"
 },
 {
  "id": "bcf19914bb67ded47785d298969a7a_9",
  "x": "The Multimodal LSTM (MM-LSTM) has a slightly better performance than LSTM classifiers. In all cases, we observe that the performance is on par with human-level accuracy. Our overall accuracy is substantially higher than that reported in<cite> Shekhar et al. (2017b)</cite> .",
  "y": "differences"
 },
 {
  "id": "bcf19914bb67ded47785d298969a7a_10",
  "x": "The accuracy of our models is substantially higher than that reported in<cite> Shekhar et al. (2017b)</cite> , even for equivalent models. We note, however, that while the trends of image information is similar for other parts of speech datasets, the performance of BOW based models are lower than the performance of LSTM based models. The anomaly of improved performance of BOW based models seems heavily pronounced in the nouns dataset.",
  "y": "differences"
 },
 {
  "id": "bd12a9270c5f94056701b86eda9c8e_0",
  "x": "The rest of the corpus was automatically validated synthetic material using general data from Leipzig (Goldhahn et al., 2012 Engine customization The data was cleaned using the Bicleaner tool (S\u00e1nchez-Cartagena et al., 2018) . The data was lowercased and extra embeddings were added in order to keep the case information. The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE <cite>(Sennrich et al., 2016)</cite> approach.",
  "y": "uses"
 },
 {
  "id": "bd12a9270c5f94056701b86eda9c8e_1",
  "x": "The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE <cite>(Sennrich et al., 2016)</cite> approach. The models were trained with multi-domain data and we improved performance following a domainmixing approach (Britz et al., 2017) . The domain information was prepended with special tokens for each target sequence.",
  "y": "uses"
 },
 {
  "id": "bd64b244756259f18f7c3cb60989a2_0",
  "x": "Next, our experiments confirm that NMT translation quality for GA\u2192EN can be significantly improved using back-translation. Due to a lack of Irish monolingual data, backtranslation was less useful for EN\u2192GA NMT. Finally, a set of experiments was performed in which the synthetic parallel corpus, obtained via back-translation, was filtered with Bicleaner 2 <cite>(S\u00e1nchez-Cartagena et al. 2018</cite> ), a tool for misalignment detection.",
  "y": "uses"
 },
 {
  "id": "bd64b244756259f18f7c3cb60989a2_1",
  "x": "Finally, the misalignment detection tool Bicleaner <cite>(S\u00e1nchez-Cartagena et al. 2018</cite> ) was applied to these aligned sentences (the Bicleaner threshold was set to 0.5 8 ). We also used the ParaCrawl corpus as a bilingual resource. We used the Raw EN-GA ParaCrawl corpus v4.0 15 consisting of 156M sentence pairs.",
  "y": "uses"
 },
 {
  "id": "bd64b244756259f18f7c3cb60989a2_3",
  "x": "Finally, we presented a lightweight method for filtering synthetic sentence pairs obtained via back-translation, using a tool for misalignment detection, Bicleaner<cite> (S\u00e1nchez-Cartagena et al. 2018)</cite> . We show that our approach results in small increases in BLEU score, while requiring less training data. In future work we will investigate to what extent our proposed methodology can be applied to other languages with a similar amount of data available.",
  "y": "uses"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_0",
  "x": "In a previous paper<cite> [5]</cite> we have presented a methodology for the automatic summarization of documents, emitted by multiple sources, which describe the evolution of an event. At the heart of this methodology lies the identification of similarities and differences between the various documents, in two axes: the synchronic and the diachronic. This is achieved by the introduction of the notion of Synchronic and Diachronic Relations.",
  "y": "background"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_1",
  "x": "The creation of this graph can be considered as completing-as we have previously argued<cite> [5]</cite> -the Document Planning phase of a typical architecture of a Natural Language Generation (NLG) system [20] . Nevertheless, this graph can prove to be very large and thus the resulting summary can easily exceed the desired compression rate. In Section 4 we will present a brief sketch of a probabilistic model for the selection of the appropriate information-i.e.",
  "y": "background"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_2",
  "x": "In Section 2 we will briefly present this methodology, at the heart of which lies the notion of Synchronic and Diachronic Relations (SDRs) whose aim is the identification of the similarities and differences that exist between the documents in the synchronic and diachronic axes. The end result of this methodology is a graph whose vertices are the SDRs and whose nodes are some structures which we call messages. The creation of this graph can be considered as completing-as we have previously argued<cite> [5]</cite> -the Document Planning phase of a typical architecture of a Natural Language Generation (NLG) system [20] .",
  "y": "uses"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_3",
  "x": "At the heart of Multi-document Summarization (MDS) lies the process of identifying the similarities and differences that exist between the input documents. Although this holds true for the general case of Multi-document Summarization, for the case of summarizing evolving events the identification of the similarities and differences should be distinguished, as we have previously argued [1, 2, 4, <cite>5,</cite> 6] between two axes: the synchronic and the diachronic axes. In the synchronic axis we are mostly concerned with the degree of agreement or disagreement that the various sources exhibit, for the same time frame, whilst in the diachronic axis we are concerned with the actual evolution of an event, as this evolution is being described by one source.",
  "y": "background"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_4",
  "x": "Although this holds true for the general case of Multi-document Summarization, for the case of summarizing evolving events the identification of the similarities and differences should be distinguished, as we have previously argued [1, 2, 4, <cite>5,</cite> 6] between two axes: the synchronic and the diachronic axes. In the synchronic axis we are mostly concerned with the degree of agreement or disagreement that the various sources exhibit, for the same time frame, whilst in the diachronic axis we are concerned with the actual evolution of an event, as this evolution is being described by one source. The initial inspiration for the SDRs was provided by the Rhetorical Structure Theory (RST) of Mann & Thompson [1<cite>5,</cite> 16] .",
  "y": "uses"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_5",
  "x": "In the synchronic axis we are mostly concerned with the degree of agreement or disagreement that the various sources exhibit, for the same time frame, whilst in the diachronic axis we are concerned with the actual evolution of an event, as this evolution is being described by one source. The initial inspiration for the SDRs was provided by the Rhetorical Structure Theory (RST) of Mann & Thompson [1<cite>5,</cite> 16] . Rhetorical Structure Theory-which was initially developed in the context of \"computational text generation\" 3 [1<cite>5,</cite> 16, 22] -is trying to connect several units of analysis with relations that are semantic in nature and are supposed to capture the intentions of the author.",
  "y": "background"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_6",
  "x": "In the synchronic axis we are mostly concerned with the degree of agreement or disagreement that the various sources exhibit, for the same time frame, whilst in the diachronic axis we are concerned with the actual evolution of an event, as this evolution is being described by one source. The initial inspiration for the SDRs was provided by the Rhetorical Structure Theory (RST) of Mann & Thompson [1<cite>5,</cite> 16] . Rhetorical Structure Theory-which was initially developed in the context of \"computational text generation\" 3 [1<cite>5,</cite> 16, 22] -is trying to connect several units of analysis with relations that are semantic in nature and are supposed to capture the intentions of the author.",
  "y": "uses"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_7",
  "x": "Concerning the SDRs, in order to formally define a relation the following four fields ought to be defined (see also<cite> [5]</cite> ): The name of the relation carries semantic information which, along with the messages that are connected with the relation, are later being exploited by the NLG component (see<cite> [5]</cite> ) in order to produce the final summary.",
  "y": "background"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_8",
  "x": "Those constraints are expressed using the notation of first order logic. The name of the relation carries semantic information which, along with the messages that are connected with the relation, are later being exploited by the NLG component (see<cite> [5]</cite> ) in order to produce the final summary. 2 Due to space limitations this section contains a very brief introduction to a methodology for the creation of summaries from evolving events that we have earlier presented<cite> [5]</cite> .",
  "y": "background"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_9",
  "x": "The interested reader is encouraged to consult [1, 2, 4, <cite>5,</cite> 6 ] for more information. 3 Also referred to as Natural Language Generation (NLG). The methodology we propose consists of two main phases, the topic analysis phase and the implementation phase.",
  "y": "background"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_10",
  "x": "In Table 1 we present the statistics of the final messages and SDRs extraction stages for both case studies. The creation of the grid can be considered as completing-as we have previously argued<cite> [5]</cite> -the Document Planning phase of a typical architecture of an NLG system [20] . Nevertheless, this graph can prove to be very large and thus the resulting summary can easily exceed the desired compression rate.",
  "y": "background"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_11",
  "x": "Despite those objections, we make the claim in this paper that the proposed model can nevertheless be considered as a good starting point for the case of Multi-document Summarization of Evolving Events, at least in the framework we have described in Section 2. Concerning the first objection-i.e. the claim that the same trivial information might be contained in all the documents and thus such trivial information will have a high probability of being included in the final summary-this claim is rebuffed by the nature of the methodology that we have briefly presented in Section 2 and more fully exposed in [1] and<cite> [5]</cite> .",
  "y": "uses background"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_12",
  "x": "The use of an ontology and especially the use of the messages guarantee that the system will try to extract information whose nature, we know beforehand, will be non-trivial. Of course, this beneficial situation has its drawbacks as well. As we have argued in<cite> [5]</cite> the creation of the ontology and the specifications of the messages require a considerable amount of human labor.",
  "y": "motivation"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_13",
  "x": "As we have argued in<cite> [5]</cite> the creation of the ontology and the specifications of the messages require a considerable amount of human labor. Nevertheless, in Section 9 of<cite> [5]</cite> we present specific propositions of how this problem can be alleviated. Let us now come to the second objection.",
  "y": "uses"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_14",
  "x": "**CONCLUSIONS** In [1] and<cite> [5]</cite> we thoroughly presented a methodology (and applied it in two different case studies) which aims towards the creation of summaries from descriptions of evolving events which are emitted from multiple sources. The end result of this methodology is the computational extraction of a structure, which we called a grid.",
  "y": "background"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_15",
  "x": "In [1] and<cite> [5]</cite> we thoroughly presented a methodology (and applied it in two different case studies) which aims towards the creation of summaries from descriptions of evolving events which are emitted from multiple sources. The end result of this methodology is the computational extraction of a structure, which we called a grid. Nevertheless, it can be the case that the created grid can prove to be large enough in order for the final summary to exceed the required compression rate.",
  "y": "motivation"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_16",
  "x": "In Table 1 we present the statistics of the final messages and SDRs extraction stages for both case studies. The creation of the grid can be considered as completing-as we have previously argued<cite> [5]</cite> -the Document Planning phase of a typical architecture of an NLG system [20] . Nevertheless, this graph can prove to be very large and thus the resulting summary can easily exceed the desired compression rate.",
  "y": "motivation"
 },
 {
  "id": "bdd0ebe147e277f8f7f04fc351464a_17",
  "x": "Concerning the SDRs, in order to formally define a relation the following four fields ought to be defined (see also<cite> [5]</cite> ): 1. The relation's type (i.e. Synchronic or Diachronic). 2. The relation's name. 3. The set of pairs of message types that are involved in the relation. 4. The constraints that the corresponding arguments of each of the pairs of message types should have.",
  "y": "background"
 },
 {
  "id": "bdeffcf02a86d06f57dbfae979b098_0",
  "x": "Unlike some more exploratory applications of topic models, translation detection is easy to evaluate. The need for bilingual training data in many language pairs and domains also makes it attractive to mitigate the quadratic runtime of brute force translation detection. We begin in \u00a72 by extending the online variational Bayes approach of Hoffman et al. (2010) to polylingual topic models<cite> (Mimno et al., 2009)</cite> .",
  "y": "uses"
 },
 {
  "id": "bdeffcf02a86d06f57dbfae979b098_1",
  "x": "In this paper, we focus on the Polylingual Topic Model, introduced by <cite>Mimno et al. (2009)</cite> . Given a multilingual set of aligned documents, the PLTM assumes that across an aligned multilingual document tuple, there exists a single, tuple-specific, distribution across topics. In addition, PLTM assumes that for each language-topic pair, there exists a distribution over words in that language \u03b2 l .",
  "y": "uses"
 },
 {
  "id": "bdeffcf02a86d06f57dbfae979b098_2",
  "x": "In their original work <cite>Mimno et al. (2009)</cite> used the Gibbs sampling approach as a posterior inference algorithm to assign topics distributions over their test collection. While more straightforward to implement, this sampling approach is inherently slow when applied to large collections which makes the original PLTM work practically infeasible to be used on real-world data sets. In general, performing posterior inference over the latent variables of a Bayesian model is usually done with two of the three approximate approaches, Gibbs sampling, variational Bayes (VB) and expectation-propagation.",
  "y": "background"
 },
 {
  "id": "bdeffcf02a86d06f57dbfae979b098_3",
  "x": "In their original work <cite>Mimno et al. (2009)</cite> used the Gibbs sampling approach as a posterior inference algorithm to assign topics distributions over their test collection. While more straightforward to implement, this sampling approach is inherently slow when applied to large collections which makes the original PLTM work practically infeasible to be used on real-world data sets. In this paper we use Hoffman et al. (2010) approach.",
  "y": "differences"
 },
 {
  "id": "bdeffcf02a86d06f57dbfae979b098_4",
  "x": "**EFFICIENT APPROXIMATE TRANSLATION DETECTION** Mapping multilingual documents into a common, language-independent vector space for the purpose of improving machine translation (MT) and performing cross-language information retrieval (CLIR) tasks has been explored through various techniques. <cite>Mimno et al. (2009)</cite> introduced polylingual topic models (PLTM), an extension of latent Dirichlet allocation (LDA), and, more recently, Platt et al. (2010) proposed extensions of principal component analysis (PCA) and probabilistic latent semantic indexing (PLSI).",
  "y": "background"
 },
 {
  "id": "bdeffcf02a86d06f57dbfae979b098_5",
  "x": "<cite>Mimno et al. (2009)</cite> introduced polylingual topic models (PLTM), an extension of latent Dirichlet allocation (LDA), and, more recently, Platt et al. (2010) proposed extensions of principal component analysis (PCA) and probabilistic latent semantic indexing (PLSI). We use PLTM representations of bilingual documents.",
  "y": "uses"
 },
 {
  "id": "bdeffcf02a86d06f57dbfae979b098_6",
  "x": "**EXPERIMENTAL SETUP** We use Mallet's (McCallum, 2002) implementation of the PLTM to train and infer topics on the same data set used in Platt et al. (2010) . That paper used the Europarl (Koehn, 2005)<cite> (Mimno et al., 2009)</cite> , these performance comparisons are not done on the same training and test sets-a gap that we fill below.",
  "y": "background"
 },
 {
  "id": "bdeffcf02a86d06f57dbfae979b098_7",
  "x": "**EXPERIMENTAL SETUP** We use Mallet's (McCallum, 2002) implementation of the PLTM to train and infer topics on the same data set used in Platt et al. (2010) . That paper used the Europarl (Koehn, 2005)<cite> (Mimno et al., 2009)</cite> , these performance comparisons are not done on the same training and test sets-a gap that we fill below.",
  "y": "uses background"
 },
 {
  "id": "bdeffcf02a86d06f57dbfae979b098_8",
  "x": "In order to compare exactly the same topic distributions when computing speed vs. accuracy of various approximate and exhaustive all-pairs comparisons we focus only on one inference approach -the Gibbs sampling and ignore the online VB approach as it yields similar performance. For all four topic models, we use the same settings for PLTM (hyperparameter values and number of Gibbs sampling iterations) as in<cite> (Mimno et al., 2009)</cite> 2 . Topic distributions were then inferred on the test collection using the trained topics.",
  "y": "uses"
 },
 {
  "id": "be4144c60068cf242479ece304fd19_0",
  "x": "The last few years have seen an increased interest in narrative within the field of Natural Language Generation (Reiter et al., 2008; <cite>Elson and McKeown, 2010</cite>; Siddharthan et al., 2012; Lester, 2012) . Narrative is generally acknowledged as a fundamental mode of presenting and communicating information between humans, with different manifestations across media but with a very significant presence in textual form. Yet efforts in Natural Language Generation research have generally side stepped the issue.",
  "y": "background"
 },
 {
  "id": "be4144c60068cf242479ece304fd19_1",
  "x": "---------------------------------- **** The last few years have seen an increased interest in narrative within the field of Natural Language Generation (Reiter et al., 2008; <cite>Elson and McKeown, 2010</cite>; Siddharthan et al., 2012; Lester, 2012) .",
  "y": "background"
 },
 {
  "id": "be67496882917c2a44afb42e6f9f15_0",
  "x": "The sequence-to-sequence neural network framework (seq2seq) has been successful in a wide range of tasks in natural language processing, from machine translation (Bahdanau et al., 2014) and semantic parsing (Dong and Lapata, 2016) to summarization (Nallapati et al., 2016b; See et al., 2017) . Despite this success, seq2seq models are known to often exhibit an overall lack of fluency in the natural language output produced: problems include lexical repetition, under-generation in the form of partial phrases and lack of specificity (often caused by the gap between the input and output vocabularies) (Xie, 2017) . Recently, a number of taskspecific attention variants have been proposed to deal with these issues: See et al. (2017) introduced a coverage mechanism<cite> (Tu et al., 2016</cite> ) * Work performed while at Apple.",
  "y": "background"
 },
 {
  "id": "be67496882917c2a44afb42e6f9f15_1",
  "x": "to deal with repetition and over-copying in summarization, Hua and Wang (2018) introduced a method of attending over keyphrases to improve argument generation, and Kiddon et al. (2016) introduced a method that attends to an agenda of items to improve recipe generation. Perhaps not surprisingly, general-purpose attention mechanisms targeting individual problems from the list above have also begun to be developed. Copynet (Gu et al., 2016) and pointer-generator networks (Vinyals et al., 2015) , for example, aim to reduce input-output vocabulary mismatch and, thereby, improve specificity, while the coveragebased techniques of<cite> Tu et al. (2016)</cite> tackle repetition and under-generation.",
  "y": "background"
 },
 {
  "id": "be67496882917c2a44afb42e6f9f15_2",
  "x": "To demonstrate general(izable) improvements on conditional natural language generation problems broadly construed, we instantiate Scratchpad for three well-studied generation tasksMachine Translation, Question Generation, and Summarization -and evaluate it on a diverse set of datasets. These tasks exhibit a variety of input modalities (structured and unstructured language) and typically have required a variety of computational strategies to perform well (attention, pointing, copying). We find, for each task, that Scratchpad attains improvements over several strong baselines: Sequence-to-Sequence with attention Bahdanau et al., 2014) , copy-enhanced approaches (Gu et al., 2016; Vinyals et al., 2015) , and coverageenhanced approaches<cite> (Tu et al., 2016</cite>; See et al., 2017) .",
  "y": "background"
 },
 {
  "id": "be67496882917c2a44afb42e6f9f15_3",
  "x": "For IWSLT14 (Cettolo et al., 2015) , we compare to the models evaluated by He et al. (2018) , which includes a transformer (Vaswani et al., 2017) and RNN-based models (Bahdanau et al., 2014) . For IWSLT15, we primarily compare to GNMT (Wu et al., 2016) , which incorporates Coverage<cite> (Tu et al., 2016)</cite> . Table 1 shows BLEU scores of our approach on 3 IWSLT translation tasks along with reported results from previous work.",
  "y": "similarities"
 },
 {
  "id": "be67496882917c2a44afb42e6f9f15_4",
  "x": "We noticed that many tokens that appear in the logical form are also present in the natural language form for each example. In fact, nearly half of the tokens in the question appear in the corresponding SPARQL of the WebQuestionSP dataset (Yih et al., 2016) , implying that a network with the ability to copy from the input could see significant gains on the task. Accordingly, we compare our Scratchpad Mechanism against three baselines: (1) Seq2Seq, (2) Copynet and (3) Coverage, a method introduced by<cite> Tu et al. (2016)</cite> that aims to solve attention-related problems.",
  "y": "similarities uses"
 },
 {
  "id": "be67496882917c2a44afb42e6f9f15_5",
  "x": "Previous work based on coverage based approaches<cite> (Tu et al., 2016</cite>; See et al., 2017) either imposed an extra term to the loss function or used an extra vector to keep track of which parts of the input sequences had been attended to, thereby focusing the attention weights in subsequent steps on tokens that received little attention before. In other words, focusing the attention on the relevant parts of the input. Our proposed approach naturally learns to focus the attention on the important tokens, without a need for modifying the loss function or introducing coverage vectors.",
  "y": "differences"
 },
 {
  "id": "be67496882917c2a44afb42e6f9f15_6",
  "x": "Improvements over the approach followed, first by the introduction of attention (Bahdanau et al., 2014) which helped seq2seq translation to focus on certain tokens of the encoder outputs. Later on, many improvements were described in the Google neural machine translation system (Wu et al., 2016) , including utilizing coverage penalty<cite> (Tu et al., 2016)</cite> while decoding. The Transformer model was introduced to alleviate the dependence on RNNs in both the encoder and the decoder steps (Vaswani et al., 2017) .",
  "y": "background"
 },
 {
  "id": "be67496882917c2a44afb42e6f9f15_7",
  "x": "Both approaches operate on triplets, meaning they have limited capability beyond generating simple questions and cannot generate the far more complex compositional questions that our approach does by operating on the more expressive SPARQL query (logical form). In the question generation domain, there has been a recent surge in research on generating questions for a given paragraph of text (Song et al., 2017; Du et al., 2017; Yao et al., 2018) , with most of the work being a variant of the seq2seq approach. In Song et al. (2017) , a seq2seq model with copynet and a coverage mechanism<cite> (Tu et al., 2016</cite> ) is used to achieve state-of-the-art results.",
  "y": "similarities uses"
 },
 {
  "id": "be67496882917c2a44afb42e6f9f15_8",
  "x": "Attention Closest to our work, in the general paradigm of seq2seq learning, is the coverage mechanism introduced in<cite> Tu et al. (2016)</cite> and later adapted for summarization in See et al. (2017) . Both works try to minimize erroneous repetitions generated by a copy mechanism by introducing a new vector to keep track of what has been used from the encoder thus far. Tu et al. (2016) , for example, use an extra GRU to keep track of this information, whereas See et al. (2017) keep track of the sum of attention weights and add a penalty to the loss function based on it to discourage repetition.",
  "y": "similarities uses"
 },
 {
  "id": "c07bc362ac7ee1f64e149fe8907fee_0",
  "x": "Unsurprisingly, when NLP tools are applied directly to social media data, the results tend to be miserable when compared to data sets such as the Wall Street Journal component of the Penn Treebank. However, there have been recent successes in adapting parsers and POS taggers to social media data <cite>(Foster et al., 2011</cite>; Gimpel et al., 2011) . Additionally, lexical normalisation and other preprocessing strategies have been shown to enhance the performance of NLP tools over social media data (Lui and Baldwin, 2012; Han et al., to appear) .",
  "y": "background"
 },
 {
  "id": "c0d2bbf9dc7615040763019f5c668b_0",
  "x": "However, there have been recent successes in adapting parsers and POS taggers to social media data (Foster et al., 2011; Gimpel et al., 2011) . Additionally, lexical normalisation and other preprocessing strategies have been shown to enhance the performance of NLP tools over social media data <cite>(Lui and Baldwin, 2012</cite>; Han et al., to appear) . Furthermore, social media posts tend to be short and the content highly varied, meaning it is difficult to adapt a tool to the domain, or harness textual context to disambiguate the content.",
  "y": "background"
 },
 {
  "id": "c126f8b9a5fcb2687494a8c0b1e859_0",
  "x": "We train our classifiers on the exact training set defined by <cite>Zitouni et al. (2006)</cite> , a subpart of the third segment of the Penn Arabic Treebank (Maamouri et al., 2004 ) (\"ATB3-Train\", 288,000 words). We also (reluctantly) follow them in having a single set for development and testing (\"ATB3-Devtest\", 52,000 words), rather than separate development and test sets (as is common), in order to be able to compare our results to theirs. Up until this point, MADA-D has narrowed the list of possible analyses of a word (supplied by BAMA) down to a small number.",
  "y": "uses"
 },
 {
  "id": "c126f8b9a5fcb2687494a8c0b1e859_1",
  "x": "We review three approaches that are directly relevant to us; we refer to the excellent literature review in <cite>(Zitouni et al., 2006)</cite> for a general review. Vergyri and Kirchhoff (2004) follow an approach similar to ours in that they choose from the diacritizations proposed by BAMA. However, they train a single tagger using unannotated data and EM, which necessarily leads to a lower performance.",
  "y": "uses"
 },
 {
  "id": "c126f8b9a5fcb2687494a8c0b1e859_2",
  "x": "They use a word-based language model (using both diacritized and undiacritized words in the context) but back off to a character-based model for unseen words. They consult BAMA to narrow possible diacritizations for unseen words, but BAMA does not provide much improvement used in this manner. <cite>Zitouni et al. (2006)</cite> use a maximum entropy classifier to assign a set of diacritics to the letters of each word.",
  "y": "background"
 },
 {
  "id": "c126f8b9a5fcb2687494a8c0b1e859_3",
  "x": "<cite>Zitouni et al. (2006)</cite> use a maximum entropy classifier to assign a set of diacritics to the letters of each word. <cite>They</cite> use the output of a tokenizer (segmenter) and a part-of-speech tagger (which presumably tags the output of the tokenizer). <cite>They</cite> then use segment n-grams, segment position of the character being diacritized, the POS of the current segment, along with lexical features, including letter and word n-grams.",
  "y": "background"
 },
 {
  "id": "c126f8b9a5fcb2687494a8c0b1e859_4",
  "x": "<cite>Zitouni et al. (2006)</cite> use a maximum entropy classifier to assign a set of diacritics to the letters of each word. <cite>They</cite> then use segment n-grams, segment position of the character being diacritized, the POS of the current segment, along with lexical features, including letter and word n-grams.",
  "y": "background"
 },
 {
  "id": "c126f8b9a5fcb2687494a8c0b1e859_5",
  "x": "<cite>Zitouni et al. (2006)</cite> use a maximum entropy classifier to assign a set of diacritics to the letters of each word. Thus, while many of the same elements are used in <cite>their</cite> and our work (word n-grams, features related to morphological analysis), the basic approach is quite different: while we have one procedure that chooses a correct analysis (including to- Figure 1: Diacritization Results (all followed by single-choice-diac model); our best results are shown in boldface; Only-DLM-1 is the baseline; <cite>\"Zitouni\"</cite> is <cite>(Zitouni et al., 2006)</cite> kenization, morphological tag, and diacritization), they have a pipeline of processors.",
  "y": "similarities differences"
 },
 {
  "id": "c126f8b9a5fcb2687494a8c0b1e859_6",
  "x": "Thus, while many of the same elements are used in <cite>their</cite> and our work (word n-grams, features related to morphological analysis), the basic approach is quite different: while we have one procedure that chooses a correct analysis (including to- Figure 1: Diacritization Results (all followed by single-choice-diac model); our best results are shown in boldface; Only-DLM-1 is the baseline; <cite>\"Zitouni\"</cite> is <cite>(Zitouni et al., 2006)</cite> kenization, morphological tag, and diacritization), they have a pipeline of processors. Furthermore, <cite>Zitouni et al. (2006)</cite> do not use a morphological lexicon. To our knowledge, <cite>their</cite> system is the best performing currently, and we have set up our experiments to allow us to compare our results directly to <cite>their</cite> results.",
  "y": "background"
 },
 {
  "id": "c126f8b9a5fcb2687494a8c0b1e859_7",
  "x": "Furthermore, <cite>Zitouni et al. (2006)</cite> do not use a morphological lexicon. To our knowledge, <cite>their</cite> system is the best performing currently, and we have set up our experiments to allow us to compare our results directly to <cite>their</cite> results. ----------------------------------",
  "y": "uses background"
 },
 {
  "id": "c126f8b9a5fcb2687494a8c0b1e859_8",
  "x": "There are several ways of defining metrics for diacritization. In order to assure maximal comparability with the work of <cite>Zitouni et al. (2006)</cite> , we adopt their metric. 5 We count all words, including numbers and punctuation.",
  "y": "uses"
 },
 {
  "id": "c126f8b9a5fcb2687494a8c0b1e859_9",
  "x": "Finally, we give the results of <cite>Zitouni et al. (2006)</cite> on the last line, which we understand to be the best published results currently. We see that we improve on their results in all categories. We can see the effect of our different approaches to diacritization in the numbers: while for WER we reduce the <cite>Zitouni et al</cite> error by 17.2%, the DER error reduction is only 10.9%.",
  "y": "uses"
 },
 {
  "id": "c126f8b9a5fcb2687494a8c0b1e859_10",
  "x": "We see that we improve on their results in all categories. We can see the effect of our different approaches to diacritization in the numbers: while for WER we reduce the <cite>Zitouni et al</cite> error by 17.2%, the DER error reduction is only 10.9%. This is because we are choosing among complete diacritization options for white space-tokenized words, while <cite>Zitouni et al. (2006)</cite> make choices for each diacritic.",
  "y": "differences"
 },
 {
  "id": "c126f8b9a5fcb2687494a8c0b1e859_11",
  "x": "We see that we improve on their results in all categories. We can see the effect of our different approaches to diacritization in the numbers: while for WER we reduce the <cite>Zitouni et al</cite> error by 17.2%, the DER error reduction is only 10.9%. This is because we are choosing among complete diacritization options for white space-tokenized words, while <cite>Zitouni et al. (2006)</cite> make choices for each diacritic.",
  "y": "differences"
 },
 {
  "id": "c126f8b9a5fcb2687494a8c0b1e859_12",
  "x": "We have shown that a diacritizer that uses a lexical resource can outperform a highly optimized ad-hoc diacritization system that draws on a large number of features. We speculate that further work on WSD could further improve our results. We also note the issue of unknown words, which will affect our system much more than that of <cite>(Zitouni et al., 2006)</cite> .",
  "y": "differences"
 },
 {
  "id": "c182062efc486f83eb27f9a3859a9a_0",
  "x": "For example, Nobata et al. (2016) report that in their corpus of comments on Yahoo! articles collected between April 2014 and April 2015, the percentage of abusive comments is around 3.4% on Finance articles and 10.7% on News. Since the phenomenon is elusive, researchers often use lists of offensive terms to collect datasets with the aim to increase the likelihood of catching instances of hate speech<cite> Waseem and Hovy, 2016)</cite> . This filtering process, however, has the risk of producing corpora with a variety of biases, which may go undetected.",
  "y": "background"
 },
 {
  "id": "c182062efc486f83eb27f9a3859a9a_1",
  "x": "Thus, datasets on which research experiments are performed are ephemeral, which makes replication of results very difficult. In this paper, we focus on the latter two points. We consider a particular hate speech corpus -a Twitter corpus collected by <cite>Waseem and Hovy (2016)</cite> , which has been gaining traction as a resource for training hate speech detection models<cite> (Waseem and Hovy, 2016</cite>; Gamb\u00e4ck and Utpal, 2017; Park and Fung, 2017) -and analyse it critically to better understand its usefulness as a hate speech resource.",
  "y": "uses background"
 },
 {
  "id": "c182062efc486f83eb27f9a3859a9a_2",
  "x": "We consider a particular hate speech corpus -a Twitter corpus collected by <cite>Waseem and Hovy (2016)</cite> , which has been gaining traction as a resource for training hate speech detection models<cite> (Waseem and Hovy, 2016</cite>; Gamb\u00e4ck and Utpal, 2017; Park and Fung, 2017) -and analyse it critically to better understand its usefulness as a hate speech resource. In particular, we make the following contributions: \u2022 We report the outcome of a reproduction experiment, where we attempt to replicate the results by <cite>Waseem and Hovy (2016)</cite> on hate speech detection using their Twitter corpus.",
  "y": "uses"
 },
 {
  "id": "c182062efc486f83eb27f9a3859a9a_3",
  "x": "We aim to replicate the results on hate speech detection by <cite>Waseem and Hovy (2016)</cite> using the hate speech Twitter corpus created by the authors. 2 The dataset is a useful resource as it is one of few freely available corpora for hate speech research; it is manually annotated and distinguishes between two types of hate speech -sexism and racismwhich allows for more nuanced insight and analysis. Additionally, as a Twitter corpus, it provides opportunity for any type of analysis and feature examination typical for Twitter corpora, such as user and tweet metadata, user interaction, etc.",
  "y": "uses"
 },
 {
  "id": "c182062efc486f83eb27f9a3859a9a_4",
  "x": "The algorithm. <cite>Waseem and Hovy (2016)</cite> state that they use a logistic regression classifier for their hate speech prediction task. What is not mentioned is which implementation of the algorithm is used, how the model was fit to the data, whether the features were scaled, and whether any other additional parameters had been used. Due to its popularity and accessibility, we opt for the Scikitlearn (Pedregosa et al., 2011) Python implementation of the logistic regression algorithm.",
  "y": "background"
 },
 {
  "id": "c182062efc486f83eb27f9a3859a9a_5",
  "x": "**5** The features. <cite>Waseem and Hovy (2016)</cite> explore several feature types: they employ n-gram features -specifically, they find that character n-grams of lengths up to 4 perform best -and in addition, they combine them with gender information, geographic location information and tweet length, finding that combining n-gram features with gender features yields slightly better results than just n-gram features do, while mixing in any of the other features results in slightly lower scores.",
  "y": "background"
 },
 {
  "id": "c182062efc486f83eb27f9a3859a9a_6",
  "x": "---------------------------------- **NEW EXPERIMENT: POPULARITY PREDICTION** To date, most research on hate speech within the NLP community has been done in the area of automatic detection using a variety of techniques, from lists of prominent keywords (Warner and Hirschberg, 2012) to regression classifiers as seen in the previous section (Nobata et al., 2016;<cite> Waseem and Hovy, 2016)</cite> , naive Bayes, decision trees, random forests, and linear SVMs , as well as deep learning models with convolutional neural networks (Gamb\u00e4ck and Utpal, 2017; Park and Fung, 2017) .",
  "y": "background"
 },
 {
  "id": "c182062efc486f83eb27f9a3859a9a_7",
  "x": "To date, most research on hate speech within the NLP community has been done in the area of automatic detection using a variety of techniques, from lists of prominent keywords (Warner and Hirschberg, 2012) to regression classifiers as seen in the previous section (Nobata et al., 2016;<cite> Waseem and Hovy, 2016)</cite> , naive Bayes, decision trees, random forests, and linear SVMs , as well as deep learning models with convolutional neural networks (Gamb\u00e4ck and Utpal, 2017; Park and Fung, 2017) . Our intent in this section is to explore hate speech beyond just detection, using the Twitter corpus by <cite>Waseem and Hovy (2016)</cite> . Given that Twitter is a platform that enables sharing ideas, and given that extreme ideas have a tendency to intensely spread through social networks (Brady et al., 2017) , our question is: how does the fact that a tweet is a hate tweet affect its popularity?",
  "y": "uses"
 },
 {
  "id": "c182062efc486f83eb27f9a3859a9a_10",
  "x": "Qualitative observations on tweet content. According to the annotation guidelines devised by <cite>Waseem and Hovy (2016)</cite> for the purpose of annotating this corpus, a tweet is tagged as offensive if it: (1) uses a sexist or racial slur, (2) attacks a minority, (3) seeks to silence a minority, (4) criticizes a minority (without a well founded argument), (5) promotes, but does not directly use, hate speech or violent crime, (6) criticizes a minority and uses a straw man argument, (7) blatantly misrepresents truth or seeks to distort views on a minority with unfounded claims, (8) shows support of problematic hashtags (e.g. #BanIslam, #whori-ental, #whitegenocide), (9) negatively stereotypes a minority, (10) defends xenophobia or sexism, (11) the tweet is ambiguous (at best); and contains a screen name that is offensive as per the previous criteria; and is on a topic that satisfies any of the above criteria. Though at first glance specific and detailed, these criteria are quite broad and open to interpretation.",
  "y": "background"
 },
 {
  "id": "c1eefe276c0ed46d7cd50f3f3bc3f3_0",
  "x": "The main motivation for building neural network based systems over traditional systems for such tasks is that they do not require any feature engineering or domain-specific handcrafting of rules (Vinyals and Le, 2015) . Conversation modelling is one such domain where end-to-end trained systems have matched or surpassed traditional dialog systems in both open-ended (Dodge et al., 2016) and goal-oriented applications <cite>(Bordes et al., 2017</cite> ). An important yet unexplored aspect of dialog systems is the ability to personalize the bot's responses based on the profile or attributes of who it is interacting with (Serban et al., 2017) .",
  "y": "background"
 },
 {
  "id": "c1eefe276c0ed46d7cd50f3f3bc3f3_1",
  "x": "(Illustration taken from<cite> Bordes et al., 2017)</cite> as location, type of cuisine and price range. It should then make recommendations based on these variables as well as certain fixed attributes about the user (dietary preference, favorite food items, etc.). The register (or style) of the language used by the bot may also be influenced by certain characteristics of the user (age, gender, etc.) (Halliday et al., 1964) .",
  "y": "uses"
 },
 {
  "id": "c1eefe276c0ed46d7cd50f3f3bc3f3_2",
  "x": "The register (or style) of the language used by the bot may also be influenced by certain characteristics of the user (age, gender, etc.) (Halliday et al., 1964) . However, there are no open datasets which allow researchers to train end-to-end dialog systems where each conversation is influenced by a speaker's profile (Serban et al., 2017) . With the ultimate aim of creating such a dataset, this paper aims to be an extension of the bAbI dialog dataset introduced by<cite> Bordes et al. (2017)</cite> .",
  "y": "extends"
 },
 {
  "id": "c1eefe276c0ed46d7cd50f3f3bc3f3_3",
  "x": "**RELATED WORK** This work builds upon the bAbI dialog dataset described in<cite> Bordes et al. (2017)</cite> , which is aimed at testing end-to-end dialog systems in the goal-oriented domain of restaurant reservations. Their tasks are meant to complement the bAbI tasks for text understanding and reasoning described in Weston et al. (2015b) .",
  "y": "extends"
 },
 {
  "id": "c1eefe276c0ed46d7cd50f3f3bc3f3_4",
  "x": "Following<cite> Bordes et al. (2017)</cite> , we provide baselines on the modified dataset by evaluating several learning methods: rule-based systems, supervised embeddings, and end-to-end Memory networks. ---------------------------------- **RULE-BASED SYSTEMS**",
  "y": "uses"
 },
 {
  "id": "c1eefe276c0ed46d7cd50f3f3bc3f3_5",
  "x": "Although widely known for learning unsupervised embeddings from raw text like in Word2Vec (Mikolov et al., 2013) , embeddings can also be learned in a supervised manner specifically for a given task. Supervised word embedding models which score (conversation history, response) pairs have been shown to be a strong baseline for both open-ended and goal-oriented dialog (Dodge et al., 2016;<cite> Bordes et al., 2017)</cite> . We do not handcraft any special embeddings for the user profiles.",
  "y": "background"
 },
 {
  "id": "c1eefe276c0ed46d7cd50f3f3bc3f3_6",
  "x": "We do not handcraft any special embeddings for the user profiles. The embedding vectors are trained specifically for the task of predicting the next response given the previous conversation: a candidate response y is scored against the input (Dodge et al., 2016;<cite> Bordes et al., 2017)</cite> . For dialogs, the entire conversation history is stored in the memory component of the model.",
  "y": "uses"
 },
 {
  "id": "c1eefe276c0ed46d7cd50f3f3bc3f3_7",
  "x": "It can be iteratively read from to perform reasoning and select the best possible responses based on the context. Implementing the modifications to the Memory Network architecture described by<cite> Bordes et al. (2017)</cite> , we use the model as an end-to-end baseline and analyze its performance. The user profile information is stored in the memory of the model as if it were the first turn of the conversation history spoken by the user, i.e. the model builds an embedding of the profile by combining the values of the embeddings of each attribute in the profile.",
  "y": "uses"
 },
 {
  "id": "c1eefe276c0ed46d7cd50f3f3bc3f3_8",
  "x": "Implementing the modifications to the Memory Network architecture described by<cite> Bordes et al. (2017)</cite> , we use the model as an end-to-end baseline and analyze its performance. The user profile information is stored in the memory of the model as if it were the first turn of the conversation history spoken by the user, i.e. the model builds an embedding of the profile by combining the values of the embeddings of each attribute in the profile. Unlike<cite> Bordes et al. (2017)</cite>, we do not make use of any match type features.",
  "y": "differences"
 },
 {
  "id": "c1eefe276c0ed46d7cd50f3f3bc3f3_9",
  "x": "However, it is important to note that building a rule-based system for real conversations is not easy-our tasks use a restricted vocabulary and fixed speech patterns. Compared to results reported on the bAbI dialog tasks in<cite> Bordes et al. (2017)</cite> , supervised embeddings performed significantly worse on the modified tasks. The model was unable to complete any of the tasks successfully and had extremely low per-response accuracy for PT2-5.",
  "y": "differences"
 },
 {
  "id": "c1eefe276c0ed46d7cd50f3f3bc3f3_10",
  "x": "This paper aims to bridge a gap in research on neural conversational agents by introducing a new open dataset of goal-oriented dialogs with user profiles associated with each dialog. The dataset acts as a testbed for the training and analysis of end-to-end goal-oriented conversational agents which must personalize their conversation with the user based on attributes in the user's profile. As this work builds on top of the bAbI dialog dataset proposed by<cite> Bordes et al. (2017)</cite> , crucial aspects of goal-oriented conversation have been split into various synthetically generated tasks to evaluate the strengths and weaknesses of models in a systematic way before applying them on real data.",
  "y": "extends"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_0",
  "x": "In other words, the biases detected in <cite>Elazar and Goldberg (2018)</cite> seem restricted to <cite>their particular data sample</cite>, and would therefore not bias the decisions of the model on new samples, whether in-domain or out-of-domain. In light of this, we discuss better methodologies for detecting bias in our models. ----------------------------------",
  "y": "motivation"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_1",
  "x": "In other words, the biases detected in <cite>Elazar and Goldberg (2018)</cite> seem restricted to <cite>their particular data sample</cite>, and would therefore not bias the decisions of the model on new samples, whether in-domain or out-of-domain. In light of this, we discuss better methodologies for detecting bias in our models. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_2",
  "x": "Adversarial training has been used to learn data representations that are invariant to demographic attributes (Raff and Sylvester, 2018; Beutel et al., 2017; Li et al., 2018) , as well as representations invariant to domain differences (Ganin and Lempitsky, 2015) , clean or noisy speech (Sriram et al., 2018) , and invariant to differences between source languages in multi-lingual machine translation (Xie et al., 2017) . <cite>Elazar and Goldberg (2018)</cite> argue, however, that adversarial learning does not fully remove sensitive demographic traits from the data representations. This conclusion is based on the observation that a diagnostic classifier trained over the supposedly debiased data representations could still predict gender, age and race above chance level in <cite>their</cite> experimental setup.",
  "y": "motivation"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_3",
  "x": "The correlation is unexpected and particular to a finite data sample. This paper presents a follow-up to the experiments in <cite>Elazar and Goldberg (2018)</cite> and examines what kind of correlation the data representations in their models exhibit with demographic attributes: PREVALENT, SAMPLE-SPECIFIC or AC-CIDENTAL correlations. We do this by not only evaluating the diagnostic classifiers on in-sample data, but also on new samples, as well as across textual domains.",
  "y": "extends"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_4",
  "x": "**GENDER AGE** Contributions Our contributions are methodological. We show that the diagnostic classifiers used to establish gender and age bias in <cite>Elazar and Goldberg (2018)</cite> (a) rely only on samplespecific patterns to predict gender and age, and (b) do therefore not generalize to new samples or domains.",
  "y": "differences"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_5",
  "x": "Contributions Our contributions are methodological. We show that the diagnostic classifiers used to establish gender and age bias in <cite>Elazar and Goldberg (2018)</cite> (a) rely only on samplespecific patterns to predict gender and age, and (b) do therefore not generalize to new samples or domains. Surprisingly, we also show that this also holds for the biased baseline model in <cite>their experiments</cite>, suggesting that the particular representations induced for the mention detection task in <cite>Elazar and Goldberg (2018)</cite> were not biased with respect to protected demographic attributes.",
  "y": "differences"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_7",
  "x": "A model trained without the second objective is referred to as a non-adversarial model (-ADV). The architecture of <cite>Elazar and Goldberg (2018)</cite> consists of a single-layer LSTM encoder, and two multi-layer perceptrons -one for each task. The main task p(y|x) is learned as usual, with loss being backpropagated through the relevant perceptron and the encoder.",
  "y": "background"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_8",
  "x": "The language encoder is used to obtain representations of the input data, and the diagnostic classifier is trained to predict the protected attributes from these representations, without access to the encoder or to the original inputs. Since the dataset is balanced and the targets are binary, <cite>leakage</cite> is defined in <cite>Elazar and Goldberg (2018)</cite> as any demographic attribute prediction accuracy over 50.0% by the diagnostic classifier. Preprocessing Table 1 shows sizes for all used datasets.",
  "y": "background"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_9",
  "x": "Preprocessing Table 1 shows sizes for all used datasets. Our main dataset, PAN16 TWIT (Rangel et al., 2016) , is split into train and development, following <cite>Elazar and Goldberg (2018)</cite> . We further remove 10,000 sentences from their train data to use as a held-out test split, making sure there is no author overlap between training and test data.",
  "y": "similarities"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_10",
  "x": "We further remove 10,000 sentences from their train data to use as a held-out test split, making sure there is no author overlap between training and test data. This means that we have 12,000 fewer sentences in our train split than <cite>Elazar and Goldberg (2018)</cite>, but we report results on exactly the same development split as well as on the new held-out test split. PAN16 TWIT is balanced using undersampling with respect to main task and demographic attribute (gender and age respectively) which is why there are separate datasets for GENDER and AGE.",
  "y": "differences"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_11",
  "x": "Using random subsamples this way is common in machine learning, including bias detection studies <cite>(Elazar and Goldberg, 2018</cite>; Zhao et al., 2019) and probing studies (Ravfogel et al., 2018; Lin et al., 2019) , but is known to overestimate performance (Globerson and Roweis, 2016), in particular for highdimensional problems. Replication We start by replicating the experiment of <cite>Elazar and Goldberg (2018)</cite> using <cite>their code</cite> on PAN16 TWIT with <cite>their data splits</cite>. The main task is predicting the mentions of other Twitter users after removing all user names in the tweets.",
  "y": "background"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_12",
  "x": "Using random subsamples this way is common in machine learning, including bias detection studies <cite>(Elazar and Goldberg, 2018</cite>; Zhao et al., 2019) and probing studies (Ravfogel et al., 2018; Lin et al., 2019) , but is known to overestimate performance (Globerson and Roweis, 2016), in particular for highdimensional problems. Replication We start by replicating the experiment of <cite>Elazar and Goldberg (2018)</cite> using <cite>their code</cite> on PAN16 TWIT with <cite>their data splits</cite>. The main task is predicting the mentions of other Twitter users after removing all user names in the tweets.",
  "y": "similarities uses"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_13",
  "x": "The main task is predicting the mentions of other Twitter users after removing all user names in the tweets. The protected demographic attributes are age and gender, both with binary targets. Our development results (main and diagnostic classifier) which are comparable to <cite>Elazar and Goldberg (2018)</cite> are reported in Table 6 in the Appendix; test set results are also in Table 4 .",
  "y": "similarities"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_14",
  "x": "The protected demographic attributes are age and gender, both with binary targets. Our development results (main and diagnostic classifier) which are comparable to <cite>Elazar and Goldberg (2018)</cite> are reported in Table 6 in the Appendix; test set results are also in Table 4 . Our results remain comparable to those obtained in <cite>Elazar and Goldberg (2018)</cite> , albeit the diagnostic classifier is able to achieve 3.92 percentage points less leakage for gender and 2.59 percentage points for age, possibly due to the reduction in training data.",
  "y": "similarities"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_15",
  "x": "We also construct an artificial dataset from PAN16 TWIT where the main task labels are preserved but the demographic label is randomly shuffled (PAN16 RAND), allowing us to run experiments with no PREVALENT or SAMPLE-SPECIFIC gender correlations, only ACCIDENTAL. This experiment supplements our in-sample analysis in \u00a73 and shows that our models are not overly expressive, indicating that the sample-specific correlations detected in <cite>Elazar and Goldberg (2018)</cite> are relatively simple associations. The results in Table 4 show, however, that the performance and leakage of the adversarial models do not generalize well across domains, but for the most part the performance of the non-adversarial models doesn't either.",
  "y": "differences"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_16",
  "x": "The results in Table 4 show, however, that the performance and leakage of the adversarial models do not generalize well across domains, but for the most part the performance of the non-adversarial models doesn't either. This is the main result presented here: <cite>The leakage</cite> shown in <cite>Elazar and Goldberg (2018)</cite> does not transfer across domains, but also does not even generalize to a new sample within the same domain. This suggests that the leakage is merely spurious correlations in a small, finite sample of data.",
  "y": "differences"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_17",
  "x": "---------------------------------- **CONCLUSION AND DISCUSSION** We examined the methodology used in <cite>Elazar and Goldberg (2018)</cite> to establish bias in the representations of adversarial machine learning architectures designed to protect demographic attributes.",
  "y": "uses"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_19",
  "x": "Our contribution is mainly method-6 The methodology is not only found in <cite>Elazar and Goldberg (2018)</cite> . ological: What we have shown is that the methodology in <cite>Elazar and Goldberg (2018)</cite> , i.e., insample evaluation of diagnostic classifiers, is not sufficient to establish bias/leakage beyond the current data sample.",
  "y": "differences"
 },
 {
  "id": "c2633be695561ce3f5b39fccf38927_20",
  "x": "The same methodology can be found in other papers on detecting bias in machine learning models, for example, Zhao et al. (2019) , as well as in several papers probing neural network representations for linguistic knowledge, for example, Lin et al. (2019) . ological: What we have shown is that the methodology in <cite>Elazar and Goldberg (2018)</cite> , i.e., insample evaluation of diagnostic classifiers, is not sufficient to establish bias/leakage beyond the current data sample. Instead we propose out-ofsample and cross-domain evaluation, as well as more qualitative investigation of the induced diagnostic classifiers.",
  "y": "differences"
 },
 {
  "id": "c327812b2369a1dfc8e2ce4077b997_0",
  "x": "Already many sentence alignment techniques have been implemented for some languages pairs such as English-French <cite>(Gale and Church, 1993</cite>; Brown et al., 1991; Chen, 1993; Braune and Fraser 2010; Lamraoui and Langlais, 2013) , English-German<cite> (Gale and Church, 1993)</cite> English-Chinese (Wu, 1994; Chuang and Yeh, 2005) and Hungarian-English (Varga et al., 2005; T\u00f3th et al., 2008) . However, none of these techniques have been evaluated for Sinhala and Tamil, the two official languages in Sri Lanka. This paper presents the first ever study on automatically creating a sentence aligned parallel corpus for Sinhala and Tamil.",
  "y": "background"
 },
 {
  "id": "c327812b2369a1dfc8e2ce4077b997_1",
  "x": "Already many sentence alignment techniques have been implemented for some languages pairs such as English-French <cite>(Gale and Church, 1993</cite>; Brown et al., 1991; Chen, 1993; Braune and Fraser 2010; Lamraoui and Langlais, 2013) , English-German<cite> (Gale and Church, 1993)</cite> English-Chinese (Wu, 1994; Chuang and Yeh, 2005) and Hungarian-English (Varga et al., 2005; T\u00f3th et al., 2008) . However, none of these techniques have been evaluated for Sinhala and Tamil, the two official languages in Sri Lanka. This paper presents the first ever study on automatically creating a sentence aligned parallel corpus for Sinhala and Tamil.",
  "y": "differences"
 },
 {
  "id": "c327812b2369a1dfc8e2ce4077b997_2",
  "x": "With this limitation in mind, an extensive literature study was carried out to identify the applicable sentence alignment techniques for Sinhala and Tamil. We implemented six such methods, and evaluated their performance using a corpus of 1300 sentences based on the precision, recall, and F-measure using annual reports of Sri Lankan government departments as the source text. The highest F-measure value of 0.791 was obtained for Varga et al.'s (2005) Hunalign method, the hybrid method that combined the use of a bilingual dictionary with the statistical method by<cite> Gale and Church (1993)</cite> .",
  "y": "uses"
 },
 {
  "id": "c327812b2369a1dfc8e2ce4077b997_3",
  "x": "While the parameters such as mean and variance for<cite> Gale and Church's (1993)</cite> method are considered language independent for European languages, tuning these for non-'European language pairs has improved results (Zotti et al, 2014) . Both these methods have given good accuracy in alignment; however they require some form of initial alignment or anchor points. Method by Chuang and Yeh (2005) exploits the statistically ordered matching of punctuation marks in the two languages English and Chinese to achieve high accuracy in sentence alignment compared with using the length-based methods alone.",
  "y": "background"
 },
 {
  "id": "c327812b2369a1dfc8e2ce4077b997_4",
  "x": "The method used by Wu (1994) is a modification of<cite> Gale and Church's (1993)</cite> length-based statistical method for the task of aligning English with Chinese. It uses a bilingual external lexicon with lexicon cues to improve the alignment accuracy. Dynamic programming optimization has been used for the alignment of the lexicon extensions.",
  "y": "background"
 },
 {
  "id": "c327812b2369a1dfc8e2ce4077b997_5",
  "x": "Dynamic programming optimization has been used for the alignment of the lexicon extensions. However, the computation and memory costs grow linearly with the number of lexical cues. The method by Chen (1993) is a word-correspondence-based model that gives a better accuracy than length based methods, however, it was reported to be much slower than the algorithms of Brown et al., (1991) and<cite> Gale and Church (1993)</cite> .",
  "y": "background"
 },
 {
  "id": "c327812b2369a1dfc8e2ce4077b997_6",
  "x": "According to<cite> Gale and Church (1993)</cite> a considerably large parallel corpus having a small error percentage can be built without lexical constraints. According to the authors, lexical constraints might slow down the program and make it less useful in the first pass. Linguistic methods can produce better results if the performance of the system is not a concern.",
  "y": "background"
 },
 {
  "id": "c327812b2369a1dfc8e2ce4077b997_7",
  "x": "Automatic alignment of sentences has been attempted for few Indic language pairs from the South Asian subcontinent including Hindi-Urdu (Kaur and Kaur, 2012) and Hindi-Punjabi (Kumar and Goyal, 2010) . This research used the method proposed by<cite> Gale and Church (1993)</cite> citing the close linguistic similarities between languages of these pairs, causing parallel sentences to be of similar lengths. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c327812b2369a1dfc8e2ce4077b997_8",
  "x": "For example, methods by T\u00f3th et al. (2008) and M\u00fajdricza-Maydt et al. (2013) cannot be used because NER systems and comprehensive POS taggers are not fully developed for Sinhala (Dahanayaka and Weerasinghe, 2014; Manamini et al., 2016) and Tamil (Pandian et al., 2008; Vijayakrishna and Devi, 2008) . Also methods that align using the punctuations in the two languages similar to that of Chuang and Yeh (2005) cannot be used in this case because when extracting text from pdf, some punctuations are lost, and also the translators of the original text have not been consistent with the use of punctuations. Constrained by the available resources, we compared methods by<cite> Gale and Church (1993)</cite> , Moore (2002) , Varga et al. (2005) , Braune and Fraser (2010) , Lamraoui and Langlais (2013) , and Sennrich and Volk (2011) .",
  "y": "uses"
 },
 {
  "id": "c327812b2369a1dfc8e2ce4077b997_9",
  "x": "These close linguistic relationships include similarities in word or sentence length, similarities in sentence structure and in languages that use the character set, similarities between words. Linguistic similarities between Sinhala and Tamil include word and sentence length similarities and sentence structure similarity with both Sinhala and Tamil following a Subject-Object-Verb structure. The mean and variance for the number of Tamil characters per Sinhala was found and these values were used for the<cite> Gale and Church's (1993)</cite> method.",
  "y": "uses"
 },
 {
  "id": "c327812b2369a1dfc8e2ce4077b997_10",
  "x": "---------------------------------- **DISCUSSION** Most of the above methods <cite>(Gale and Church, 1993</cite>; Brown et al., 1991; Chen and S.F, 1993; Braune and Fraser, 2010) have been first used for English and French sentence alignment.",
  "y": "background"
 },
 {
  "id": "c327812b2369a1dfc8e2ce4077b997_11",
  "x": "We used<cite> Gale and Church (1993)</cite> method even though we could not align the paragraphs before aligning the sentences, due the dissimilarities among the text converted from pdfs. The length of Tamil sentences was comparatively higher than Sinhala sentences and the correlation between Sinhala and Tamil was comparatively low, hence we cannot consider mean and variance as language independent as suggested by<cite> Gale and Church (1993)</cite> . Therefore we calculated the mean and variance for Sinhala and Tamil using 700 sentences.",
  "y": "uses"
 },
 {
  "id": "c327812b2369a1dfc8e2ce4077b997_13",
  "x": "Even after changing the parameters for Sinhala and Tamil in the<cite> Gale and Church (1993)</cite> method, we obtained a comparatively low precision because this method does not only look at one to one alignments but also one to zero, many to one, one to many or many to many alignments. Also according to<cite> Gale and Church (1993)</cite> , in this method one to zero alignment is never handled correctly. Most misalignments arise due to one to zero, many to one to many or many to many alignments, resulting in methods that consider only one to one alignments to have better precision values.",
  "y": "extends differences"
 },
 {
  "id": "c34bbed419bddb6d63b3e3bccf595d_0",
  "x": [
   "**CONCLUSIONS** This paper aims to recognize bridging anaphora in written text. We develop discourse structure, lexicosemantic and genericity features based on linguistic intuition and corpus research."
  ],
  "y": "differences"
 },
 {
  "id": "c34bbed419bddb6d63b3e3bccf595d_1",
  "x": "In recent work, these two tasks have been tackled separately, with bridging recognition handled as part of information status (IS) classification<cite> (Markert et al., 2012</cite>; Cahill and Riester, 2012; Rahman and Ng, 2012) . Each mention in a text gets assigned one IS class that describes its accessibility to the reader at a given point in a text, bridging being one possible class. We stay within this framework.",
  "y": "background"
 },
 {
  "id": "c34bbed419bddb6d63b3e3bccf595d_2",
  "x": "We stay within this framework. Bridging recognition is a difficult task, so that we had to report very low results on this IS class in previous work<cite> (Markert et al., 2012)</cite> . This is due to the phenomenon's variety, leading to a lack of clear surface features for recognition.",
  "y": "background motivation"
 },
 {
  "id": "c34bbed419bddb6d63b3e3bccf595d_3",
  "x": "Bridging recognition is a difficult task, so that we had to report very low results on this IS class in previous work<cite> (Markert et al., 2012)</cite> . This is due to the phenomenon's variety, leading to a lack of clear surface features for recognition. Instead, we formulate in this paper novel discourse structure and lexicosemantic features as well as features that distinguish bridging from generics (see Section 3).",
  "y": "motivation"
 },
 {
  "id": "c34bbed419bddb6d63b3e3bccf595d_4",
  "x": "Instead, we formulate in this paper novel discourse structure and lexicosemantic features as well as features that distinguish bridging from generics (see Section 3). In addition, making up between 5% and 20% of definite descriptions (Gardent and Manu\u00e9lian, 2005; Caselli and Prodanof, 2006) and around 6% of all NPs<cite> (Markert et al., 2012)</cite> , bridging is still less frequent than many other IS classes and recognition of minority classes is well known to be more difficult. We therefore use a cascaded classification algorithm to address this problem (Omuya et al., 2013) .",
  "y": "motivation background"
 },
 {
  "id": "c34bbed419bddb6d63b3e3bccf595d_5",
  "x": "Instead, we formulate in this paper novel discourse structure and lexicosemantic features as well as features that distinguish bridging from generics (see Section 3). In addition, making up between 5% and 20% of definite descriptions (Gardent and Manu\u00e9lian, 2005; Caselli and Prodanof, 2006) and around 6% of all NPs<cite> (Markert et al., 2012)</cite> , bridging is still less frequent than many other IS classes and recognition of minority classes is well known to be more difficult. We therefore use a cascaded classification algorithm to address this problem (Omuya et al., 2013) .",
  "y": "motivation"
 },
 {
  "id": "c34bbed419bddb6d63b3e3bccf595d_6",
  "x": "**RELATED WORK** Most bridging research concentrates on antecedent selection only (Poesio and Vieira, 1998; Poesio et al., 2004a; Lassalle and Denis, 2011; Hou et al., 2013) , assuming that bridging recognition has already been performed. Previous work on recognition is either limited to definite NPs based on heuristics evaluated on small datasets (Hahn et al., 1996; Vieira and Poesio, 2000) , or models it as a subtask of learning fine-grained IS (Rahman and Ng, 2012;<cite> Markert et al., 2012</cite>; Cahill and Riester, 2012) .",
  "y": "motivation"
 },
 {
  "id": "c34bbed419bddb6d63b3e3bccf595d_7",
  "x": "Previous work on recognition is either limited to definite NPs based on heuristics evaluated on small datasets (Hahn et al., 1996; Vieira and Poesio, 2000) , or models it as a subtask of learning fine-grained IS (Rahman and Ng, 2012;<cite> Markert et al., 2012</cite>; Cahill and Riester, 2012) . Results within this latter framework for bridging have been mixed: We reported in<cite> Markert et al. (2012)</cite> low results for bridging in written news text whereas Rahman and Ng (2012) report high results for the four subcategories of bridging annotated in the Switchboard dialogue corpus by Nissim et al. (2004) . We believe this discrepancy to be due to differences in corpus size and genre as well as in bridging definition.",
  "y": "motivation"
 },
 {
  "id": "c34bbed419bddb6d63b3e3bccf595d_8",
  "x": "4 (2) The blicket couldn't be connected to the dax. The wug failed. Similarly, Clark (1975) distinguishes between bridging via necessary, probable and inducible parts/roles and argues that only in the first and maybe the second case the antecedent triggers the 3 See also the high results for our specific category for comparative anaphora<cite> (Markert et al., 2012)</cite> .",
  "y": "background"
 },
 {
  "id": "c34bbed419bddb6d63b3e3bccf595d_9",
  "x": "Experimental setup. We perform experiments on the corpus provided in<cite> Markert et al. (2012)</cite> 6 . It consists of 50 texts taken from the WSJ portion of the OntoNotes corpus (Weischedel et al., 2011) with almost 11,000 NPs annotated for information status including 663 bridging NPs and their antecedents.",
  "y": "uses"
 },
 {
  "id": "c34bbed419bddb6d63b3e3bccf595d_10",
  "x": "Reimplemented baseline system (rbls). rbls uses the same features as<cite> Markert et al. (2012)</cite> (Table 1) but replaces the local decision tree classifier with LibSVM as we will need to include lexical features. Tables 1 and 2 and apply them in order from rarest to more frequent category.",
  "y": "extends"
 },
 {
  "id": "c34bbed419bddb6d63b3e3bccf595d_11",
  "x": "**FEATURES** In<cite> Markert et al. (2012)</cite> we classify eight finegrained IS categories for NPs in written text: old, new and 6 mediated categories (syntactic, worldKnowledge, bridging, comparative, aggregate and function). This feature set (Table 1 , f 1-f 13) works well to identify old, new and several mediated categories.",
  "y": "extends background"
 },
 {
  "id": "c34bbed419bddb6d63b3e3bccf595d_12",
  "x": "(10) Friends are part of the glue that holds life and faith together. Bridging anaphora can have almost limitless variation. However, we observe that bridging anaphors are often licensed because of discourse structure<cite> Markert et al. (2012)</cite>",
  "y": "extends background"
 },
 {
  "id": "c3f71bea55f85633568c7ba57f6fd5_0",
  "x": "In order to promote research in this area, we propose a data generation paradigm adapted from CLEVR [<cite>11</cite>] . We generate acoustic scenes by leveraging a bank of elementary sounds. We also provide a number of functional programs that can be used to compose questions and answers that exploit the relationships between the attributes of the elementary sounds in each scene.",
  "y": "extends"
 },
 {
  "id": "c3f71bea55f85633568c7ba57f6fd5_1",
  "x": "Current question answering methods require large amounts of annotated data. In the visual domain, several strategies have been proposed to make this kind of data available to the community [<cite>11</cite>, 2, 25, 7] . Agrawal et al. [1] noted that the way the questions are created has a huge impact on what information a neural network uses to answer them (this is a well known problem that can arise with In what part of the scene is the clarinet playing a G note that is before the third violin sound?",
  "y": "background"
 },
 {
  "id": "c3f71bea55f85633568c7ba57f6fd5_2",
  "x": "Question answering (QA) problems have attracted increasing interest in the machine learning and artificial intelligence communities. These tasks usually involve interpreting and answering text based questions in the view of some contextual information, often expressed in a different modality. Text-based QA, use text corpora as context ( [19, 20, 17, 9, 10, 16] ); in visual question answering (VQA), instead, the questions are related to a scene depicted in still images (e.g. [<cite>11</cite>, 2, 25, 7, 1, 23, 8, 10, 16] .",
  "y": "background"
 },
 {
  "id": "c3f71bea55f85633568c7ba57f6fd5_3",
  "x": "Total 47 all neural network based systems). This motivated research [23, 8, <cite>11</cite>] on how to reduce the bias in VQA datasets. The complexity around gathering good labeled data forced some authors [23, 8] to constrain their work to yes/no questions.",
  "y": "background"
 },
 {
  "id": "c3f71bea55f85633568c7ba57f6fd5_4",
  "x": "The complexity around gathering good labeled data forced some authors [23, 8] to constrain their work to yes/no questions. <cite>Johnson et al. [11]</cite> made their way around this constraint by using synthetic data. To generate the questions, they first generate a semantic representation that describes the reasoning steps needed in order to answer the question.",
  "y": "background"
 },
 {
  "id": "c3f71bea55f85633568c7ba57f6fd5_5",
  "x": "Inspired by the work on <cite>CLEVR</cite> [<cite>11</cite>] , we propose an acoustical question answering (AQA) task by defining a synthetic dataset that comprises audio scenes composed by sequences of elementary sounds and questions relating properties of the sounds in each scene. We provide the adapted software for AQA data generation as well as a version of the dataset based on musical instrument sounds. We also report preliminary experiments using the FiLM architecture derived from the VQA domain.",
  "y": "motivation"
 },
 {
  "id": "c3f71bea55f85633568c7ba57f6fd5_6",
  "x": "To represent questions, we use the same semantic representation through functional programs that is proposed in [<cite>11</cite>, 12] . ---------------------------------- **SCENES AND ELEMENTARY SOUNDS**",
  "y": "uses"
 },
 {
  "id": "c3f71bea55f85633568c7ba57f6fd5_7",
  "x": "Questions are structured in a logical tree introduced in <cite>CLEVR</cite> [<cite>11</cite>] as a functional program. A functional program, defines the reasoning steps required to answer a question given a scene definition. We adapted the original work of <cite>Johnson et al. [11]</cite> to our acoustical context by updating the function catalog and the relationships between the objects of the scene.",
  "y": "uses"
 },
 {
  "id": "c3f71bea55f85633568c7ba57f6fd5_8",
  "x": "Questions are structured in a logical tree introduced in <cite>CLEVR</cite> [<cite>11</cite>] as a functional program. A functional program, defines the reasoning steps required to answer a question given a scene definition. We adapted the original work of <cite>Johnson et al. [11]</cite> to our acoustical context by updating the function catalog and the relationships between the objects of the scene.",
  "y": "extends"
 },
 {
  "id": "c3f71bea55f85633568c7ba57f6fd5_9",
  "x": "The same question would be considered degenerate if there is only one violin sound in the scene, because it could be answered without taking into account the relation \"after the trumpet\". A validation process [<cite>11</cite>] is responsible for rejecting both ill-posed and degenerate questions during the generation phase. Thanks to the functional representation we can use the reasoning steps of the questions to analyze the results.",
  "y": "uses"
 },
 {
  "id": "c3f71bea55f85633568c7ba57f6fd5_10",
  "x": "**PRELIMINARY EXPERIMENTS** To evaluate our dataset, we performed preliminary experiments with a FiLM network [15] . It is a good candidate as it has been shown to work well on the <cite>CLEVR VQA task</cite> [<cite>11</cite>] that shares the same structure of questions as our CLEAR dataset.",
  "y": "uses"
 },
 {
  "id": "c3f71bea55f85633568c7ba57f6fd5_11",
  "x": "We also propose a paradigm for data generation that is an extension of the <cite>CLEVR</cite> paradigm: The acoustic scenes are generated by combining a number of elementary sounds, and the corresponding questions and answers are generated based on the properties of those sounds and their mutual relationships. We generated a preliminary dataset comprising 50k acoustic scenes composed of 10 musical instrument sounds, and 2M corresponding questions and answers. We also tested the FiLM model on the preliminary dataset obtaining at best 89.97% accuracy predicting the right answer from the question and the scene.",
  "y": "extends"
 },
 {
  "id": "c437e447603ecdbe4053651169770a_0",
  "x": "This paper describes the Rouletabille participation to the Hyperpartisan News Detection task. We propose the use of different text classification methods for this task. Preliminary experiments using a similar collection used in<cite> Potthast et al. (2018)</cite> show that neural-based classification methods reach state-of-the art results.",
  "y": "background"
 },
 {
  "id": "c437e447603ecdbe4053651169770a_1",
  "x": "Identifying partisan preferences in news, based only on text content, has been shown to be a challenging task <cite>(Potthast et al., 2018)</cite> . This problem requires to identify if a news article was written in such a way that it includes an overrated appreciation of one of the participants in the news (a political party, a person, a company, etc.). Despite the fact that sharply polarized documents are not necessarily fake, it is an early problem to solve for the identification of fake content.",
  "y": "motivation background"
 },
 {
  "id": "c437e447603ecdbe4053651169770a_2",
  "x": "A recent paper <cite>(Potthast et al., 2018)</cite> claims that stylometric features are a key factor to tackle this task. In this paper, we present the description of our participation to the Hyperpartisan classification task at SemEval-2019 (Kiesel et al., 2019) . This task was composed of two subtasks, the first consist to identify hyperpartisan bias in documents classified by its individual content (bias of the writer or by-article category) and the second by the editorial house that published the article (bias of the editorial house or by-publisher category) as depicted in Figure 1 1 .",
  "y": "background"
 },
 {
  "id": "c437e447603ecdbe4053651169770a_3",
  "x": "Identifying partisan preferences in news, based only on text content, has been shown to be a challenging task <cite>(Potthast et al., 2018)</cite> . Despite the fact that sharply polarized documents are not necessarily fake, it is an early problem to solve for the identification of fake content.",
  "y": "motivation"
 },
 {
  "id": "c437e447603ecdbe4053651169770a_4",
  "x": "This task was composed of two subtasks, the first consist to identify hyperpartisan bias in documents classified by its individual content (bias of the writer or by-article category) and the second by the editorial house that published the article (bias of the editorial house or by-publisher category) as depicted in Figure 1 1 . To address this problem, we experimented with well-known models based on deep learning (Honnibal and Montani, 2017; Kim, 2014) . They achieve state-of-the-art results on a publicly available collection <cite>(Potthast et al., 2018)</cite> , showing that neural models can effectively address the task of hyperpartisan detection without including stylometric features.",
  "y": "background"
 },
 {
  "id": "c437e447603ecdbe4053651169770a_5",
  "x": "**EXPERIMENTAL SETUP** Experiments were performed using two collections, the ACL2018 collection <cite>(Potthast et al., 2018)</cite> and the SemEval19 collection . The first collection is composed of 1627 articles including 801 hyperpartisan and 826 2 Different to the classical training of the involved classifiers.",
  "y": "uses"
 },
 {
  "id": "c437e447603ecdbe4053651169770a_6",
  "x": "We only used the first fold produced by the authors' code 6 . As our results are not directly comparable with the values reported in<cite> Potthast et al. (2018)</cite> , we re-evaluated their approach on this single fold. ----------------------------------",
  "y": "uses differences"
 },
 {
  "id": "c437e447603ecdbe4053651169770a_7",
  "x": "Two state-of-the-art models (SpaCy and Kim (2014)) outperform the approach presented in<cite> Potthast et al. (2018)</cite> , showing that stylometric features are probably not necessary for the task. ---------------------------------- **RESULTS IN THE SEMEVAL2019 COLLECTION**",
  "y": "background"
 },
 {
  "id": "c437e447603ecdbe4053651169770a_8",
  "x": "Our experiments and participation to the Hyperpartisan task led us to conclude that: \u2022 stylometric features seem not to be necessary to achieve state-of-the-art results for hyperpartisan detection in the ACL2018 collection. This deserves a set of extra experiments to better understand the real contribution of stylometric features when combined with strong representations/classifiers to validate the work of<cite> Potthast et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_0",
  "x": "Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However,<cite> Finegan-Dollak et al. (2018)</cite> demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example.",
  "y": "background"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_1",
  "x": "Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However,<cite> Finegan-Dollak et al. (2018)</cite> demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example.",
  "y": "motivation"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_2",
  "x": "It requires a lot of examples and additional training to support new templates of SQL. On the other hand, the sequence-to-sequence model is unstable because of the large search space including outputs with SQL syntax errors. Moreover,<cite> Finegan-Dollak et al. (2018)</cite> demonstrated that the sequence-tosequence model also lack the ability to generate SQL queries of unseen templates.",
  "y": "background"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_3",
  "x": "On the other hand, the sequence-to-sequence model is unstable because of the large search space including outputs with SQL syntax errors. Moreover,<cite> Finegan-Dollak et al. (2018)</cite> demonstrated that the sequence-tosequence model also lack the ability to generate SQL queries of unseen templates. In this work, we propose an extension of a template-based model with one-shot learning, which can generate SQL queries of untrained templates based on a single example.",
  "y": "extends"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_4",
  "x": "The proposed model has three advantages. 2. It minimizes unnecessary search space, unlike sequence-to-sequence approaches (Iyer et al., 2017;<cite> Finegan-Dollak et al., 2018)</cite> ; thus, the model is guaranteed to be free of SQL syntax errors.",
  "y": "differences"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_5",
  "x": "We conducted experiments with four different text-to-SQL datasets on both of the questionbased split and query-based split <cite>(Finegan-Dollak et al., 2018)</cite> . In the question-based split, SQL queries of the same template appear in both training dataset and test dataset. With the questionbased split, we tested the effectiveness of the model at generating queries for the trained templates of SQL.",
  "y": "uses"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_6",
  "x": "Early works applied a sequence-to-sequence architecture that directly maps a natural language description to a sequence of the target code (Ling et al., 2016a; Jia and Liang, 2016) , but this approach does not guarantee syntax correctness. To overcome this limitation, tree-based approaches such as sequence-totree (Dong and Lapata, 2016) and Abstract Syntax Tree (AST) (Rabinovich et al., 2017) have been proposed to ensure syntax correctness. However,<cite> Finegan-Dollak et al. (2018)</cite> showed that the sequence-to-tree approach was inefficient when generating complex SQL queries from a natural language question.",
  "y": "background"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_7",
  "x": "Recently, WikiSQL dataset (Zhong et al., 2017) , a large dataset of natural language and SQL pairs, has been released, and a number of studies have proposed text-to-SQL approaches based on deep learning (Xu et al., 2017; Yu et al., 2018; Dong and Lapata, 2018; Huang et al., 2018) . However, as WikiSQL only contains simple SQL queries, most of the approaches are restricted to the simple queries alone. Iyer et al. (2017);<cite> Finegan-Dollak et al. (2018)</cite> focused on the dataset that contains more complex queries such as ATIS (Dahl et al., 1994) and GeoQuery (Zelle and Mooney, 1996) .",
  "y": "background"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_8",
  "x": "We propose a Candidate Search Network (CSN) for the selection of top-n relevant SQL templates within the candidate set C to build a support set S. Then, we find the template\u0177 using the Matching Network based on the support set S. plied a sequence-to-sequence approach with attention mechanism, and<cite> Finegan-Dollak et al. (2018)</cite> proposed a template-based model and another sequence-to-sequence model with a copy mechanism. However,<cite> Finegan-Dollak et al. (2018)</cite> showed that both approaches lack the ability to generate SQL of the unseen template in the training stage.",
  "y": "background"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_9",
  "x": "plied a sequence-to-sequence approach with attention mechanism, and<cite> Finegan-Dollak et al. (2018)</cite> proposed a template-based model and another sequence-to-sequence model with a copy mechanism. However,<cite> Finegan-Dollak et al. (2018)</cite> showed that both approaches lack the ability to generate SQL of the unseen template in the training stage. One-shot Learning/Matching Network Deep learning models usually require hundreds or thousands of examples in order to learn a class. To overcome this limitation, one-shot learning aims to learn a class from a single labeled example. We applied one-shot learning to the text-to-SQL task so that our model could learn a SQL template from just a few examples and adapt easily and promptly to the SQL of untrained templates.",
  "y": "motivation differences"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_10",
  "x": "We first classify an SQL template for a given natural language question and then, we fill the variable slots of the predicted template. This architecture is based on an idea similar to the template-based model of<cite> Finegan-Dollak et al. (2018)</cite> . However, the previous model requires a number of examples for each template and needs retraining to support new templates of SQL.",
  "y": "similarities"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_11",
  "x": "This architecture is based on an idea similar to the template-based model of<cite> Finegan-Dollak et al. (2018)</cite> . However, the previous model requires a number of examples for each template and needs retraining to support new templates of SQL. Conversely, we applied one-shot learning so that our model could learn a template with just a single example.",
  "y": "differences"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_13",
  "x": "We used a SQL version of the dataset processed by FineganDollak et al. (2018) . GeoQuery (Zelle and Mooney, 1996) Collection of questions on a US geography database. We used a SQL version of the dataset processed by<cite> Finegan-Dollak et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_14",
  "x": "The number of natural language questions, vocabularies, SQL templates, and the average number of variables per SQL template for each dataset is described in Table 1 . We used a template and variables for each SQL from the preprocessed versions provided by<cite> Finegan-Dollak et al. (2018)</cite> . For the question-based split, we used a 2:1:1 ratio for the train:dev:test split and ensured that every SQL template in the test set appeared at least once in the training set.",
  "y": "uses"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_15",
  "x": "For the question-based split, we used a 2:1:1 ratio for the train:dev:test split and ensured that every SQL template in the test set appeared at least once in the training set. For the query-based split, we used the same split as in<cite> Finegan-Dollak et al. (2018)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_16",
  "x": "We evaluated the query generation accuracy for both the question-based split and query-based split <cite>(Finegan-Dollak et al., 2018)</cite> . In the question-based split, SQL queries of the same template appear in both train and test sets. Through the question-based split, we tested how well the model could generate SQL of trained templates from natural language questions.",
  "y": "uses"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_17",
  "x": "---------------------------------- **BASELINES** We compare our results with three different previous approaches: a sequence-to-sequence model from Iyer et al. (2017) , template-based model, and another sequence-to-sequence model from<cite> Finegan-Dollak et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_18",
  "x": "**COMPARISON TO PREVIOUS APPROACHES** Question-based Split For the question-based split, our model outperformed the state-of-theart approaches in every benchmark. Our model shows 3-27% query generation accuracy gain, compared to the sequence-to-sequence model, 5-9% gain, compared to template-based model <cite>(Finegan-Dollak et al., 2018)</cite> , and 15-56% gain, compared to Iyer et al. (2017) .",
  "y": "differences"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_19",
  "x": "Our model shows 3-27% query generation accuracy gain, compared to the sequence-to-sequence model, 5-9% gain, compared to template-based model <cite>(Finegan-Dollak et al., 2018)</cite> , and 15-56% gain, compared to Iyer et al. (2017) . The result demonstrates that our model is more efficient in generating SQL of the trained templates than the previous approaches. Advising ATIS GeoQuery Scholar Model \"0\" \"1\" \"0\" \"1\" \"0\" \"1\" \"0\" \"1\" Table 3 : SQL generation accuracy for the query-based split in a zero-shot setting (\"0\" column) and a one-shot setting (\"1\" column)",
  "y": "differences"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_20",
  "x": "Query-based Split Although our approach cannot generate a SQL of unseen templates, we observed that it could adapt well to new templates of SQL given just a single example without additional training. Sequence-to-sequence models (Iyer et al., 2017;<cite> Finegan-Dollak et al., 2018)</cite> , as shown in the Table 3 , showed poor performance for the query-based split in the zero-shot setting. The model from<cite> Finegan-Dollak et al. (2018)</cite> showed accuracies of 0%, 32%, 20%, and 5% for each benchmark and accuracies of Iyer et al. (2017) showed 1%, 17%, 40%, and 3%, meaning that they also lack the capability to generate unseen templates of SQL.",
  "y": "background"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_21",
  "x": "Query-based Split Although our approach cannot generate a SQL of unseen templates, we observed that it could adapt well to new templates of SQL given just a single example without additional training. Sequence-to-sequence models (Iyer et al., 2017;<cite> Finegan-Dollak et al., 2018)</cite> , as shown in the Table 3 , showed poor performance for the query-based split in the zero-shot setting. The model from<cite> Finegan-Dollak et al. (2018)</cite> showed accuracies of 0%, 32%, 20%, and 5% for each benchmark and accuracies of Iyer et al. (2017) showed 1%, 17%, 40%, and 3%, meaning that they also lack the capability to generate unseen templates of SQL.",
  "y": "background"
 },
 {
  "id": "c4cc8d4013b0259eb626d06750e4ab_22",
  "x": "Sequence-to-sequence models (Iyer et al., 2017;<cite> Finegan-Dollak et al., 2018)</cite> , as shown in the Table 3 , showed poor performance for the query-based split in the zero-shot setting. The model from<cite> Finegan-Dollak et al. (2018)</cite> showed accuracies of 0%, 32%, 20%, and 5% for each benchmark and accuracies of Iyer et al. (2017) showed 1%, 17%, 40%, and 3%, meaning that they also lack the capability to generate unseen templates of SQL. In a one-shot setting, where an example is added for each new template, our approach outperformed previous ones against every benchmark. Our model outperforms the sequence-to-sequence model <cite>(Finegan-Dollak et al., 2018)</cite> by 1-60%, the template-based model <cite>(Finegan-Dollak et al., 2018)</cite> by 17-52%, Iyer et al. (2017) by 14-62%.",
  "y": "differences"
 },
 {
  "id": "c4e0e12362bd7d505f6887abad78d4_0",
  "x": "These conflicting system requirements make KWS an active area of research ever since its inception over 50 years ago [4] . Recently, with the renaissance of artificial neural networks in the form of deep learning algorithms, neural network (NN) based KWS has become very popular [5, 6, 7,<cite> 8]</cite> . Low power consumption requirement for keyword spotting systems make microcontrollers an obvious choice for deploying KWS in an always-on system.",
  "y": "background"
 },
 {
  "id": "c4e0e12362bd7d505f6887abad78d4_1",
  "x": "\u2022 We first train the popular KWS neural net models from the literature [5, 6, 7,<cite> 8]</cite> on Google speech commands dataset [9] and compare them in terms of accuracy, memory footprint and number of operations per inference. \u2022 In addition, we implement a new KWS model using depth-wise separable convolutions and point-wise convolutions, inspired by the success of resource-efficient MobileNet [10] in computer vision. This model outperforms the other prior models in all aspects of accuracy, model size and number of operations.",
  "y": "similarities uses"
 },
 {
  "id": "c4e0e12362bd7d505f6887abad78d4_2",
  "x": "Combining the strengths of CNNs and RNNs, convolutional recurrent neural network based KWS is investigated in [7] and demonstrate the robustness of the model to noise. While all the prior KWS neural networks are trained with cross entropy loss function, a max-pooling based loss function for training KWS model with long short-term memory (LSTM) is proposed in <cite>[8]</cite> , which achieves better accuracy than the DNNs and LSTMs trained with cross entropy loss. Although many neural network models for KWS are presented in literature, it is difficult to make a fair comparison between them as they are all trained and evaluated on different proprietary datasets (e.g. \"TalkType\" dataset in [7] , \"Alexa\" dataset in <cite>[8]</cite> , etc.) with different input speech features and audio duration.",
  "y": "background"
 },
 {
  "id": "c4e0e12362bd7d505f6887abad78d4_3",
  "x": "While all the prior KWS neural networks are trained with cross entropy loss function, a max-pooling based loss function for training KWS model with long short-term memory (LSTM) is proposed in <cite>[8]</cite> , which achieves better accuracy than the DNNs and LSTMs trained with cross entropy loss. Although many neural network models for KWS are presented in literature, it is difficult to make a fair comparison between them as they are all trained and evaluated on different proprietary datasets (e.g. \"TalkType\" dataset in [7] , \"Alexa\" dataset in <cite>[8]</cite> , etc.) with different input speech features and audio duration. Also, the primary focus of prior research has been to maximize the accuracy with a small memory footprint model, without explicit constraints of underlying hardware, such as limits on number of operations per inference.",
  "y": "background"
 },
 {
  "id": "c4e0e12362bd7d505f6887abad78d4_4",
  "x": "The training data is augmented with background noise and random time shift of up to 100ms. The trained models are evaluated based on the classification accuracy on the test set. Table 2 summarizes the accuracy, memory requirement and operations per inference for the network architectures for KWS from literature [5, 6, 7,<cite> 8]</cite> trained on Google speech commands dataset [9] .",
  "y": "similarities uses"
 },
 {
  "id": "c4e0e12362bd7d505f6887abad78d4_6",
  "x": "Figure 5 shows the number of operations per inference, memory requirement and test accuracy of neural network models from prior work [5, 6, 7,<cite> 8]</cite> trained on Google speech commands dataset overlayed with the memory and compute bounding boxes for the neural network classes from section 4. ---------------------------------- **RESOURCE CONSTRAINED NEURAL NETWORK ARCHITECTURE EXPLORATION**",
  "y": "background"
 },
 {
  "id": "c4e0e12362bd7d505f6887abad78d4_7",
  "x": "[5, 6, 7,<cite> 8]</cite> trained on the speech commands dataset [9] . As shown in Fig. 1 , from each input speech signal, T \u00d7 F features are extracted and the number of these features impact the model size, number of operations and accuracy. The key parameters in the feature extraction step that impact the model size, number of operations and accuracy are (1) number of MFCC features per frame (F) and (2) the frame stride (S).",
  "y": "background"
 },
 {
  "id": "c4e0e12362bd7d505f6887abad78d4_8",
  "x": "An efficient model would maximize accuracy using small T \u00d7 F , i.e., small F and/or large S. The neural network architectures and the corresponding hyperparameters explored in this work are summarized in Table 4 . The LSTM model mentioned in the table includes peephole connections and output projection layer similar to that in <cite>[8]</cite> , whereas basic LSTM model does not include those.",
  "y": "similarities"
 },
 {
  "id": "c4e2f43e223f61d81d81ac2c9aaa3f_0",
  "x": "---------------------------------- **INTRODUCTION** Multi-task learning (MTL) in deep neural networks is typically a result of parameter sharing between two networks (of usually the same dimensions)<cite> (Caruana 1993)</cite> .",
  "y": "background"
 },
 {
  "id": "c4e2f43e223f61d81d81ac2c9aaa3f_1",
  "x": "The weights \u03bb i determine the importance of the different tasks during training. We explicitly add inductive bias to the model via the regularizer \u2126 below, but our model also implicitly learns regularization through multi-task learning<cite> (Caruana 1993</cite> ) mediated by the \u03b1 parameters, while the \u03b2 parameters are used to learn the mixture functions f (\u00b7), as detailed in the following. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c4e2f43e223f61d81d81ac2c9aaa3f_2",
  "x": "where h A,k is a linear combination of the outputs that is fed to the k+1-th layer of task A, and a , b designates the stacking of two vectors a, b \u2208 R D to a matrix M \u2208 R 2\u00d7D . Subspaces (Virtanen, Klami, and Kaski 2011;<cite> Bousmalis et al. 2016</cite> ) should allow the model to focus on task-specific and shared features in different parts of its parameter space. Extending the \u03b1-layers to include subspaces, for 2 tasks and 2 subspaces, we obtain an \u03b1 matrix \u2208 R 4\u00d74 that not only controls the interaction between the layers of both tasks, but also between their subspaces:",
  "y": "background"
 },
 {
  "id": "c4e2f43e223f61d81d81ac2c9aaa3f_3",
  "x": "For now just observe that if all \u03b1-values are set to 0.25 (or any other constant), we obtain hard parameter sharing<cite> (Caruana 1993)</cite> , which is equivalent to a heavy L 0 matrix regularizer. Adding Inductive Bias Naturally, we can also add explicit inductive bias to sluice networks by partially constraining the regularizer or adding to the learned penalty. Inspired by work on shared-space component analysis (Salzmann et al. 2010 ), we add a penalty to enforce a division of labor and discourage redundancy between shared and task-specific subspaces.",
  "y": "similarities"
 },
 {
  "id": "c4e2f43e223f61d81d81ac2c9aaa3f_4",
  "x": "Inspired by work on shared-space component analysis (Salzmann et al. 2010 ), we add a penalty to enforce a division of labor and discourage redundancy between shared and task-specific subspaces. While the networks can theoretically learn such a separation, an explicit constraint empirically leads to better results and enables the sluice networks to take better advantage of subspace-specific \u03b1-values. We introduce an orthogonality constraint<cite> (Bousmalis et al. 2016</cite> ) between the layer-wise subspaces of each model:",
  "y": "uses"
 },
 {
  "id": "c4e2f43e223f61d81d81ac2c9aaa3f_5",
  "x": "Note that due to this set-up, our results are not directly comparable to the results reported in , who only train on the WSJ domain and use OntoNotes 4.0. Baseline Models As baselines, we compare against i) a single-task model only trained on chunking; ii) the low supervision model , which predicts the auxiliary task at the first layer; iii) an MTL model based on hard parameter sharing<cite> (Caruana 1993)</cite> ; and iv) cross-stitch networks (Misra et al. 2016) . We compare these against our complete sluice network with subspace constraints and learned \u03b1 and \u03b2 parameters.",
  "y": "uses"
 },
 {
  "id": "c4e2f43e223f61d81d81ac2c9aaa3f_6",
  "x": "On average, sluice networks significantly outperform all other model architectures on both in-domain and out-of-domain data and perform best for all domains, except for the telephone conversation (tc) domain, where they are outperformed by cross-stitch networks. The performance boost is particularly significant for the out-ofdomain setting, where sluice networks add more than 1 point in accuracy compared to hard parameter sharing and almost .5 compared to the strongest baseline on average, demonstrating that sluice networks are particularly useful to help a model generalize better. In contrast to previous studies on MTL (Mart\u00ednez Alonso and Plank 2017;<cite> Bingel and S\u00f8gaard 2017</cite>; Augenstein, Ruder, and S\u00f8gaard 2018) , our model also consistently outperforms single-task learning.",
  "y": "differences"
 },
 {
  "id": "c4e2f43e223f61d81d81ac2c9aaa3f_7",
  "x": "Joint model Most work on MTL for NLP uses a single auxiliary task <cite>(Bingel and S\u00f8gaard 2017</cite>; Mart\u00ednez Alonso and Plank 2017) . In this experiment, we use one sluice network to jointly learn our four tasks on the newswire domain and show results in Table 5 . Here, the low-level POS tagging and simplified SRL tasks are the only ones that benefit from hard parameter sharing highlighting that hard parameter sharing by itself is not sufficient for doing effective multi-task learning with semantic tasks.",
  "y": "background"
 },
 {
  "id": "c4e2f43e223f61d81d81ac2c9aaa3f_8",
  "x": "Joint model Most work on MTL for NLP uses a single auxiliary task <cite>(Bingel and S\u00f8gaard 2017</cite>; Mart\u00ednez Alonso and Plank 2017) . In this experiment, we use one sluice network to jointly learn our four tasks on the newswire domain and show results in Table 5 . Here, the low-level POS tagging and simplified SRL tasks are the only ones that benefit from hard parameter sharing highlighting that hard parameter sharing by itself is not sufficient for doing effective multi-task learning with semantic tasks.",
  "y": "differences"
 },
 {
  "id": "c4e2f43e223f61d81d81ac2c9aaa3f_9",
  "x": "Task Properties and Performance<cite> Bingel and S\u00f8gaard (2017)</cite> correlate meta-characteristics of task pairs and gains compared to hard parameter sharing across a large set of NLP task pairs. Similarly, we correlate various metacharacteristics with error reductions and \u03b1, \u03b2 values. Most importantly, we find that a) multi-task learning gains, also in sluice networks, are higher when there is less training data; and b) sluice networks learn to share more when there is more variance in the training data (cross-task \u03b1s are higher, intra-task \u03b1s lower).",
  "y": "background"
 },
 {
  "id": "c4e2f43e223f61d81d81ac2c9aaa3f_10",
  "x": "Task Properties and Performance<cite> Bingel and S\u00f8gaard (2017)</cite> correlate meta-characteristics of task pairs and gains compared to hard parameter sharing across a large set of NLP task pairs. Similarly, we correlate various metacharacteristics with error reductions and \u03b1, \u03b2 values. Most importantly, we find that a) multi-task learning gains, also in sluice networks, are higher when there is less training data; and b) sluice networks learn to share more when there is more variance in the training data (cross-task \u03b1s are higher, intra-task \u03b1s lower).",
  "y": "similarities"
 },
 {
  "id": "c4e2f43e223f61d81d81ac2c9aaa3f_11",
  "x": "Hard parameter sharing<cite> (Caruana 1993</cite> ) is easy to implement, reduces overfitting, but is only guaranteed to work for (certain types of) closely related tasks (Baxter 2000; Maurer 2007 ). Peng and Dredze (2016) apply a variation of hard parameter sharing to multi-domain multi-task sequence tagging with a shared CRF layer and domain-specific projection layers. Yang, Salakhutdinov, and Cohen (2016) use hard parameter sharing to jointly learn different sequence-tagging tasks across languages.",
  "y": "background"
 },
 {
  "id": "c5a10f46c253f0da005622661b12a1_0",
  "x": "While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011;<cite> Li et al., 2013)</cite> follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines.",
  "y": "background"
 },
 {
  "id": "c5a10f46c253f0da005622661b12a1_1",
  "x": "Using eventindependent features allows us to train the system only once, at system setup phase, requiring annotated triggers in a training set for just a few preselected event types. Then, whenever a new event type is introduced for labeling, we only need to collect a seed list for it from its description, and provide it as input to the system. We developed a seed-based system (Section 3), based on a state-of-the-art fully-supervised event extraction system<cite> (Li et al., 2013)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "c5a10f46c253f0da005622661b12a1_2",
  "x": "Therefore, we implemented our system by adapting the state-of-the-art fully-supervised event extraction system of<cite> Li et al. (2013)</cite> , modifying mechanisms relevant for features and for trigger labels, as described below. Hence the systems are comparable with respect to using the same preprocessing and machine learning infrastructure. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "c5a10f46c253f0da005622661b12a1_3",
  "x": "**THE FULLY-SUPERVISED SYSTEM** The event extraction system of<cite> Li et al. (2013)</cite> labels triggers and their arguments for a set of target event types L, for which annotated training documents are provided. The system utilizes a structured perceptron with beam search (Collins and Roark, 2004; .",
  "y": "similarities"
 },
 {
  "id": "c5a10f46c253f0da005622661b12a1_4",
  "x": "To implement the seed-based approach for trigger labeling, we adapt only the trigger classification part in the<cite> Li et al. (2013)</cite> fully-supervised system, ignoring arguments. Given a set of new target event types T we classify every test sentence once for each event type t \u2208 T . Hence, when classifying a sentence for t, the labeling of each token x i is binary, where y i \u2208 { , \u22a5} marks whether x i is a trigger of type t ( ) or not (\u22a5).",
  "y": "extends differences"
 },
 {
  "id": "c5a10f46c253f0da005622661b12a1_5",
  "x": "We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by<cite> Li et al. (2013)</cite> (Section 3) . To maintain comparability, we use the ACE-2005 documents with the same split as in (Ji and Grishman, 2008; Liao and Grishman, 2010b;<cite> Li et al., 2013)</cite> to 40 test documents and 559 training documents. However, some evaluation settings differ:<cite> Li et al. (2013)</cite> train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types.",
  "y": "similarities"
 },
 {
  "id": "c5a10f46c253f0da005622661b12a1_6",
  "x": "**SETTING** We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by<cite> Li et al. (2013)</cite> (Section 3) . To maintain comparability, we use the ACE-2005 documents with the same split as in (Ji and Grishman, 2008; Liao and Grishman, 2010b;<cite> Li et al., 2013)</cite> to 40 test documents and 559 training documents.",
  "y": "similarities uses"
 },
 {
  "id": "c5a10f46c253f0da005622661b12a1_7",
  "x": "We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by<cite> Li et al. (2013)</cite> (Section 3) . To maintain comparability, we use the ACE-2005 documents with the same split as in (Ji and Grishman, 2008; Liao and Grishman, 2010b;<cite> Li et al., 2013)</cite> to 40 test documents and 559 training documents. However, some evaluation settings differ:<cite> Li et al. (2013)</cite> train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types.",
  "y": "extends differences"
 },
 {
  "id": "c5a10f46c253f0da005622661b12a1_9",
  "x": "80.6 67.1 73.2 0.04<cite> Li et al. (2013)</cite> 73.7 62.3 67.5 - Ji and Grishman (2008) 67.6 53.5 59.7 - Table 2 shows our system's precision, recall and F 1 , 7 and the average variance of F 1 within the 10 runs of each test event type. The very low variance indicates that the system's performance does not depend much on the choice of training event types. We compare our system's performance to the published trigger classification results of the baseline system of <cite>(Li et al., 2013</cite> ) (its globally optimized run, when labeling both triggers and arguments).",
  "y": "similarities"
 },
 {
  "id": "c5e1debe3fcab509737e092505a29e_0",
  "x": "With reference to the morphology, Bengali, Marathi, Tamil, and Telugu are more agglutinative compared to Hindi. It is known that SMT produces more unknown words resulting in the bad translation quality if the morphological divergence between the source and target language is high. Koehn and Knight (2003) , Popovic and Ney (2004) and<cite> Popovi\u0107 et al. (2006)</cite> have demonstrated ways to handle this issue with morphological segmentation of words before training the SMT system.",
  "y": "background"
 },
 {
  "id": "c5e1debe3fcab509737e092505a29e_1",
  "x": "**REORDERING (RO)** It is based on the syntactic transformation of the English sentence parse tree according to the target language (Hindi) structure. We have used source side reordering developed by Patel et al. (2013), and <cite>Ramanathan et al. (2008</cite> We now discuss training and testing corpus from health, tourism and general domains for be-hi, mrhi, ta-hi, te-hi, and en-hi language pairs, followed by preprocessing, SMT system setup and evaluation metrics for experiments.",
  "y": "uses"
 },
 {
  "id": "c5e1debe3fcab509737e092505a29e_2",
  "x": "**PRE-PROCESSING** To tackle the morphological divergence between the source and target languages (Bengali/Marathi/Tamil/Telugu to Hindi), we used suffix separation and compound splitting, as explained in section 2. To handle the structural divergence for English-Hindi SMT, we exploited source side preordering (Patel et al., 2013;<cite> Ramanathan et al., 2008)</cite> .",
  "y": "uses"
 },
 {
  "id": "c5e1debe3fcab509737e092505a29e_3",
  "x": "Stemming was done using lightweight stemmer<cite> (Ramanathan and Rao, 2003)</cite> for Hindi. For English, we used porter stemmer (Minnen et al., 2001 ). ----------------------------------",
  "y": "uses"
 },
 {
  "id": "c5e1debe3fcab509737e092505a29e_4",
  "x": "---------------------------------- **IMPACT OF REORDERING** Preordering of the source language sentence helps in the better alignment and decoding for English to Indian language<cite> (Ramanathan et al., 2008</cite>; Patel et al., 2013; Kunchukuttan et al., 2014) SMT.",
  "y": "background"
 },
 {
  "id": "c608567abe72c75bbbc8eb917ab5d3_0",
  "x": "Many neural network methods have recently been exploited in various natural language processing (NLP) tasks, such as parsing , POS tagging (Lample et al., 2016) , relation extraction (dos Santos et al., 2015) , translation (Bahdanau et al., 2015) , and joint tasks<cite> (Miwa and Bansal, 2016)</cite> . However, Szegedy et al. (2014) observed that intentional small scale perturbations (i.e., adversarial examples) to the input of such models may lead to incorrect decisions (with high confidence). Goodfellow et al. (2015) proposed adversarial training (AT) (for image recognition) as a regularization method which uses a mixture of clean and adversarial examples to enhance the robustness of the model.",
  "y": "background"
 },
 {
  "id": "c608567abe72c75bbbc8eb917ab5d3_2",
  "x": "Joint entity and relation extraction: Joint models (Li and Ji, 2014; Miwa and Sasaki, 2014) that are based on manually extracted features have been proposed for performing both the named entity recognition (NER) and relation extraction subtasks at once. These methods rely on the availability of NLP tools (e.g., POS taggers) or manually designed features leading to additional complexity. Neural network methods have been exploited to overcome this feature design issue and usually involve RNNs and CNNs<cite> (Miwa and Bansal, 2016</cite>; Zheng et al., 2017) . Specifically, <cite>Miwa and Bansal (2016)</cite> as well as Li et al. (2017) apply bidirectional tree-structured RNNs for different contexts (i.e., news, biomedical) to capture syntactic information (using external dependency parsers).",
  "y": "background"
 },
 {
  "id": "c608567abe72c75bbbc8eb917ab5d3_3",
  "x": "We evaluate our models on four datasets, using the code as available from our github codebase. 1 Specifically, we follow the 5-fold crossvalidation defined by <cite>Miwa and Bansal (2016)</cite> for the ACE04 (Doddington et al., 2004) dataset. For the CoNLL04 (Roth and Yih, 2004 ) EC task (assuming boundaries are given), we use the same splits as in Gupta et al. (2016) ; Adel and Sch\u00fctze (2017) .",
  "y": "uses"
 },
 {
  "id": "c608567abe72c75bbbc8eb917ab5d3_4",
  "x": "This improvement can be explained by the use of: (i) multi-label head selection, (ii) CRF-layer and (iii) character level embeddings. Compared to <cite>Miwa and Bansal (2016)</cite> , who rely on NLP tools, the baseline performs within a reasonable margin (less than 1%) on the joint task. On the other hand, Li et al. (2017) use the same model for the ADE biomedical dataset, where we report a 2.5% overall improvement.",
  "y": "differences"
 },
 {
  "id": "c663b64c73f2e583ea631c054824d8_0",
  "x": "This ability has been tested in [12] and other studies with analogy question tests of the form \"A is to B as C is to \" or male/female relations. A recent improved method for generating word embeddings is Glove <cite>[15]</cite> which makes efficient use of global statistics of text words and preserves the linear substructure of Skip-gram word2vec, the other popular method. Authors report that Glove outperforms other methods such as Skip-gram in several tasks like word similarity, word analogy etc.",
  "y": "background"
 },
 {
  "id": "c663b64c73f2e583ea631c054824d8_1",
  "x": "It was created by authors of <cite>[15]</cite> to evaluate Glove performance. Wikipedia Dependency corpus is a collection of 1 billion tokens from Wikipedia. The method used for training it is a modified version of Skip-gram word2vec described in [7] .",
  "y": "background"
 },
 {
  "id": "c663b64c73f2e583ea631c054824d8_2",
  "x": "It was trained using Skipgram word2vec with negative sampling, windows size 5 and 300 dimensions. Even bigger is Common Crawl 840, a huge corpus of 840 billion tokens and 2.2 million word vectors also used at <cite>[15]</cite> . It contains data of Common Crawl (http://commoncrawl.org), a nonprofit organization that creates and maintains public datasets by crawling the web.",
  "y": "background"
 },
 {
  "id": "c663b64c73f2e583ea631c054824d8_3",
  "x": "In this section we present the different word embedding models that we compare. It was created by authors of <cite>[15]</cite> to evaluate Glove performance. Even bigger is Common Crawl 840, a huge corpus of 840 billion tokens and 2.2 million word vectors also used at <cite>[15]</cite> .",
  "y": "uses background"
 },
 {
  "id": "c67297dc1c4376dc715bf5c1c9132f_0",
  "x": "**INTRODUCTION** Contextualized word embeddings have played an essential role in many NLP tasks. One could expect considerable performance boosts by simply substituting distributional word embeddings with Flair (Akbik et al., 2018) , ELMo<cite> (Peters et al., 2018)</cite> , and BERT (Devlin et al., 2019) embeddings.",
  "y": "background"
 },
 {
  "id": "c67297dc1c4376dc715bf5c1c9132f_1",
  "x": "ELMo is a deep word-level bidirectional LSTM language model with character level convolution networks along with a final linear projection output layer<cite> (Peters et al., 2018)</cite> . Flair is a character-level bidirectional LSTM language model on sequences of characters (Akbik et al., 2018) . BERT has an architecture of a multi-layer bidirectional transformer encoder (Devlin et al., 2019) .",
  "y": "background"
 },
 {
  "id": "c67297dc1c4376dc715bf5c1c9132f_2",
  "x": "Depending on the evaluation dataset, the state-of-art in WSD varies. Raganato et al. (2017b) utilize bi-LSTM networks with attention mechanism and a softmax layer. Melamud et al. (2016) and <cite>Peters et al. (2018)</cite> also adopt bi-LSTM networks with KNN classifiers.",
  "y": "background"
 },
 {
  "id": "c67297dc1c4376dc715bf5c1c9132f_3",
  "x": "K-Nearest Neighbor (KNN) approach is adopted from both ELMo<cite> (Peters et al., 2018)</cite> and con-text2vec (Melamud et al., 2016) to establish strong baseline approaches. Sense-based KNN Adapted from ELMo<cite> (Peters et al., 2018)</cite> with k = 1, words that have the same senses are clustered together, and the average of that cluster is used as the sense vector, which is then fitted using a one KNN classifier. Unseen words from the test corpus fall back using the first sense from WordNet (Fellbaum, 1998) .",
  "y": "uses"
 },
 {
  "id": "c67297dc1c4376dc715bf5c1c9132f_4",
  "x": "Sense-based KNN Adapted from ELMo<cite> (Peters et al., 2018)</cite> with k = 1, words that have the same senses are clustered together, and the average of that cluster is used as the sense vector, which is then fitted using a one KNN classifier. Unseen words from the test corpus fall back using the first sense from WordNet (Fellbaum, 1998) . Word-based KNN Following context2vec (Akbik et al., 2018), a cluster of each lemma occurrences in the training set is formed.",
  "y": "uses"
 },
 {
  "id": "c6ae69051a6d9111dea1a6e8405ac9_0",
  "x": "In this paper, we propose a novel solution which is to re-estimate the models from the best BLEU translation of each source sentence in the bitext. An important contribution of our approach is that unlike previous approaches such as forced alignment<cite> (Wuebker et al., 2010)</cite> , reordering and language models can also be re-estimated. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "c6ae69051a6d9111dea1a6e8405ac9_1",
  "x": "The forced alignment technique of <cite>Wuebker et al. (2010)</cite> forms the main motivation for our work. In forced alignment, given a sentence pair (F, E), a decoder determines the best phrase segmentation and alignment which will result in a translation of F into E. The best segmentation is defined as the one which maximizes the probability of translating the source sentence into the given target sentence. At the end, the phrase table is re-estimated using the phrase pair segmentations obtained from forced decoding.",
  "y": "motivation"
 },
 {
  "id": "c6ae69051a6d9111dea1a6e8405ac9_2",
  "x": "In this work, we propose that aligning source sentences to their oracle BLEU translations provides a more realistic estimate of the models from the decoding perspective instead of aligning them to high quality human translations as in forced decoding. Another relevant line of research relates tuning (weight optimisation), where our work lies between forced decoding<cite> (Wuebker et al., 2010)</cite> and the bold updating approach of (Liang et al., 2006) . However, our approach specifically proposes a novel method for training models using oracle BLEU translations.",
  "y": "similarities"
 },
 {
  "id": "c6ae69051a6d9111dea1a6e8405ac9_3",
  "x": "Within forced decoding, <cite>Wuebker et al. (2010)</cite> address this problem by using a leave-one-out approach where they modify the phrase translation probabilities for each sentence pair by removing the counts of all phrases that were extracted from that particular sentence. However, in our approach, we do not impose a constraint to produce the exact translation, instead we use the highest BLEU translations which may be very different from the references. Thus it is not strictly necessary to apply leave-one-out in our approach as a solution to over-fitting.",
  "y": "background"
 },
 {
  "id": "c6ae69051a6d9111dea1a6e8405ac9_4",
  "x": "Re-estimation of the translation models from the n-best translation of the bitext could re-enforce the probabilities of the low frequency phrase pairs in the re-estimated models leading to over-fitting. Within forced decoding, <cite>Wuebker et al. (2010)</cite> address this problem by using a leave-one-out approach where they modify the phrase translation probabilities for each sentence pair by removing the counts of all phrases that were extracted from that particular sentence. However, in our approach, we do not impose a constraint to produce the exact translation, instead we use the highest BLEU translations which may be very different from the references.",
  "y": "differences"
 },
 {
  "id": "c6ae69051a6d9111dea1a6e8405ac9_5",
  "x": "The best improvements over the baseline are obtained by using only 1-best (n= 1) alignments as shown in Table 1 . Surprisingly, this is in contrast with forced decoding as discussed in <cite>Wuebker et al. (2010)</cite> , where the best improvements are obtained for n = 100. Table 2 provides a comparison between BLEU improvements achieved by forced decoding (n = 100 best) and our oracle-BLEU re-estimation approach (n = 1 best) over the baseline for different models.",
  "y": "differences"
 },
 {
  "id": "c6ae69051a6d9111dea1a6e8405ac9_6",
  "x": "However, improvements achieved with this interpolation did not surpass the best result obtained for the oracle-BLEU re-estimation. Additionally, we also compare oracle-BLEU re-estimation to forced decoding with leave-oneout<cite> (Wuebker et al., 2010)</cite> by evaluating both on a concatenation of 5 test sets (MT03, MT05-MT09). As shown in Table 3 , even with leaveone-out, forced decoding performance drops below the baseline by -0.3 BLEU.",
  "y": "uses"
 },
 {
  "id": "c7778abb2f1890ba896ccef2c3e13b_0",
  "x": "---------------------------------- **RECENT ADVANCEMENTS** Recently, with the arrival of large-scale collections of historic texts and online libraries such as Google books, a new paradigm has been added to this research area, whereby the prime interest is in identifying the temporal scope of a sense [10, 14, <cite>16,</cite> 25] which, in turn, can give further insights to the phenomenon of language evolution.",
  "y": "background"
 },
 {
  "id": "c7778abb2f1890ba896ccef2c3e13b_1",
  "x": "When we extract the novel senses by comparing the DTs from 1909-1953 and 2002-2005 , the precision obtained for these 100 words is as low as 0.32. Similarly if we extract the novel senses comparing the DTs of 1909-1953 with 2006-2008, the precision stands at 0.23. We then explore another unsupervised approach presented in Lau et al.<cite> [16]</cite> over the same Google books corpus 1 , apply topic modeling for sense induction and directly adapt their similarity measure to get the new senses.",
  "y": "uses"
 },
 {
  "id": "c7778abb2f1890ba896ccef2c3e13b_2",
  "x": "Efforts have been made by Cook et al. [3] to prepare the largest corpus-based dataset of diachronic sense differences. Attempts have been made by Lau et al. [17] where they first introduced their topic modeling based word sense induction method to automatically detect words with emergent novel senses and in a subsequent work, Lau et al.<cite> [16]</cite> extended this task by leveraging the concept of predominant sense. The first computational approach to track and detect statistically significant linguistic shifts of words has been proposed by Kulkarni et al. [15] .",
  "y": "background"
 },
 {
  "id": "c7778abb2f1890ba896ccef2c3e13b_3",
  "x": "The authors then apply multi-stage filtering in order to obtain meaningful candidate words. Baseline 2: Lau et al.<cite> [16]</cite> : The authors proposed an unsupervised approach based on topic modeling for sense induction, and showed novel sense identification as one of its applications. For a candidate word, Hierarchical Dirichlet Process [26] is run over a corpus to induce topics.",
  "y": "uses"
 },
 {
  "id": "c7778abb2f1890ba896ccef2c3e13b_4",
  "x": "**EXPERIMENTAL RESULTS** For experimental evaluation, we start with the 'birth' cases reported by Mitra et al. . We run Lau et al.<cite> [16]</cite> over these birth cases to detect 'novel' sense as per their algorithm.",
  "y": "uses"
 },
 {
  "id": "c7778abb2f1890ba896ccef2c3e13b_5",
  "x": "From both the time point pairs (T 1 and T 2 ), we take 100 random samples from the birth cases reported by Mitra et al. [19] and get these manually evaluated. For the same 100 random samples, we now use the outputs of Lau et al.<cite> [16]</cite> and the proposed approach, and estimate the precision as well as recall of these. To further evaluate the proposed algorithm, we perform two more evaluations.",
  "y": "uses"
 },
 {
  "id": "c7778abb2f1890ba896ccef2c3e13b_6",
  "x": "The first two cases belong to computer related sense of 'searches' and 'logging', which were absent from time point 1909-1953. On the other hand, the 'birth' cluster of 'pesticide' represents an old sense which was also present in 1909-1953. Similarly Table 4 shows manual evaluations results for 3 example cases, along with their novel sense as captured by Lau et al.<cite> [16]</cite> .",
  "y": "uses"
 },
 {
  "id": "c7778abb2f1890ba896ccef2c3e13b_7",
  "x": "Note that for these 100 random samples (that are all marked 'true' by Mitra et al. [19] ), it is possible to find an upper bound on the recall of Lau et al.<cite> [16]</cite> 's approach automatically. While the low recall might be justified because this is a different approach, even the precision is found to be in the same range as that of Mitra et al. [19] . Table 6 presents the evaluation results for the same set of 100 random samples after using the proposed SVM filtering.",
  "y": "uses"
 },
 {
  "id": "c7778abb2f1890ba896ccef2c3e13b_8",
  "x": "Further, we check if we can meaningfully combine the results reported by both the methods of Mitra et al. [19] and Lau et al.<cite> [16]</cite> for more accurate sense detection; and how does this compare with Table 8 ; both the senses look quite similar. Table 9 shows the accuracy results obtained using this approach. Only 6 and 2 words out of those 100 samples were flagged as 'birth' for the two time points T 1 and T 2 respectively.",
  "y": "uses"
 },
 {
  "id": "c796e11db9203d35c1fad61d1329ef_0",
  "x": "Indeed, this was the pattern demonstrated by<cite> Levy et al. (2015)</cite> : After tuning hyperparameters, word2vec performed best on similarity-based tasks while PPMI performed best on relatedness tasks. SVD-based models attempt to represent both statistical patterns.",
  "y": "similarities"
 },
 {
  "id": "c796e11db9203d35c1fad61d1329ef_1",
  "x": "---------------------------------- **DISTRIBUTIONAL METHODS** In our experiments, we use the implementations of word2vec Skip-Gram with Negative Sampling (SGNS) and PMI matrix factorization via Singular Value Decomposition (SVD) by<cite> Levy et al. (2015)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "c796e11db9203d35c1fad61d1329ef_2",
  "x": "The other parameter neg is only applicable to SGNS and indicates the number of negative samples (we try between zero and 6 negative samples). Finally, a parameter in SVD determines the asymmetry of factorization, which was simulated with 0, 0.5 and 1 eig (for more details refer to<cite> Levy et al., 2015)</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "c796e11db9203d35c1fad61d1329ef_3",
  "x": "Figure 1 shows that the inclusion of context vectors enhances the accuracy of both models in the syntagmatic task (red lines are on top of the blue lines). This enhancement is more pronounced in the SGNS model: more data increases the accuracy of syntagmatic similarity inference consistently when the w+c option is used. SVD also benefits from a w+c equivalent setting proposed by <cite>Levy & Goldberg (2015)</cite> in performing the syntagmatic task, however the enhancement is tightly bounded for this model.",
  "y": "differences"
 },
 {
  "id": "c796e11db9203d35c1fad61d1329ef_4",
  "x": "The equivalent post-processing of the matrices in SVD for explicit inclusion of firstorder similarity suggested by<cite> Levy et al. (2015)</cite> enhanced the performance of this model in the syntagmatic (relatedness) task only in the expense of making it worse for the paradigmatic (similarity) task. Our observations suggest that SVD has some limitations in populating the distributional space as evenly as SGNS; thus it always comes up with vectors that are on average closer to one another. Further study is needed to explain this finding in a fundamental way perhaps via mathematical derivations.",
  "y": "extends differences"
 },
 {
  "id": "c7c9266b5063ec85494fde45d1dce1_0",
  "x": "In this work, we focus on the problem of classifying a tweet as racist, sexist or neither. The task is quite challenging due to the inherent complexity of the natural language constructsdifferent forms of hatred, different kinds of targets, different ways of representing the same meaning. Most of the earlier work revolves either around manual feature extraction <cite>[6]</cite> or use representation learning methods followed by a linear classifier [1, 4] of complex problems in speech, vision and text applications.",
  "y": "background"
 },
 {
  "id": "c7c9266b5063ec85494fde45d1dce1_1",
  "x": "The feature spaces for these classifiers are in turn defined by task-specific embeddings learned using three deep learning architectures: FastText, Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs). As baselines, we compare with feature spaces comprising of char n-grams <cite>[6]</cite> , TF-IDF vectors, and Bag of Words vectors (BoWV). Main contributions of our paper are as follows: (1) We investigate the application of deep learning methods for the task of hate speech detection.",
  "y": "uses"
 },
 {
  "id": "c7c9266b5063ec85494fde45d1dce1_2",
  "x": "Baseline Methods: As baselines, we experiment with three broad representations. (1) Char n-grams: It is the state-ofthe-art method <cite>[6]</cite> which uses character n-grams for hate speech detection. (2) TF-IDF: TF-IDF are typical features used for text classification.",
  "y": "background"
 },
 {
  "id": "c7c9266b5063ec85494fde45d1dce1_3",
  "x": "**DATASET AND EXPERIMENTAL SETTINGS** We experimented with a dataset of 16K annotated tweets made available by <cite>the authors of</cite> <cite>[6]</cite> . Of the 16K tweets, 3383 are labeled as sexist, 1972 as racist, and the remaining are marked as neither sexist nor racist.",
  "y": "uses"
 },
 {
  "id": "c870d761c6fcd24de73f5bf98a9fd3_0",
  "x": "In this paper we focus on answering questions of the former type and investigate models that describe the effect of syntactic context on the meaning of a single word. The work described in this paper uses probabilistic latent variable models to describe patterns of syntactic interaction, building on the selectional preference models of\u00d3 S\u00e9aghdha (2010) and Ritter et al. (2010) and the lexical substitution models of<cite> Dinu and Lapata (2010)</cite> . We propose novel methods for incorporating information about syntactic context in models of lexical choice, yielding a probabilistic analogue to dependency-based models of contextual similarity.",
  "y": "uses"
 },
 {
  "id": "c870d761c6fcd24de73f5bf98a9fd3_1",
  "x": "In this paper we focus on answering questions of the former type and investigate models that describe the effect of syntactic context on the meaning of a single word. The work described in this paper uses probabilistic latent variable models to describe patterns of syntactic interaction, building on the selectional preference models of\u00d3 S\u00e9aghdha (2010) and Ritter et al. (2010) and the lexical substitution models of<cite> Dinu and Lapata (2010)</cite> . We propose novel methods for incorporating information about syntactic context in models of lexical choice, yielding a probabilistic analogue to dependency-based models of contextual similarity.",
  "y": "extends"
 },
 {
  "id": "c870d761c6fcd24de73f5bf98a9fd3_2",
  "x": "As described in Section 3 below,<cite> Dinu and Lapata (2010)</cite> propose an LDA-based model for lexical substitution; the techniques presented in this paper can be viewed as a generalisation of theirs. Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010) , word sense induction (Brody and Lapata, 2009 ) and modelling human judgements of semantic association (Griffiths et al., 2007) . ----------------------------------",
  "y": "background"
 },
 {
  "id": "c870d761c6fcd24de73f5bf98a9fd3_3",
  "x": "In this paper we consider generative models of lexical choice that assign a probability to a particular word appearing in a given linguistic context. In particular, we follow recent work <cite>(Dinu and Lapata, 2010</cite>; \u00d3 S\u00e9aghdha, 2010; Ritter et al., 2010) in assuming a latent variable model that associates contexts with distributions over a shared set of variables and associates each variable with a distribution over the vocabulary of word types: The set of latent variables Z is typically much smaller than the vocabulary size; this induces a (soft) clustering of the vocabulary.",
  "y": "uses"
 },
 {
  "id": "c870d761c6fcd24de73f5bf98a9fd3_4",
  "x": "With appropriate priors chosen for the distributions over words and latent variables, P (n|o, C) is a fully generative model of lexical substitution. A non-generative alternative is one that estimates the similarity of the latent variable distributions associated with seeing n and o in context C. The principle that similarity between topic distributions corresponds to semantic similarity is well-known in document modelling and was proposed in the context of lexical substitution by<cite> Dinu and Lapata (2010)</cite> . In terms of the equations presented above, we could compare the distributions P (z|o, C) with P (z|n, C) using equations (5) or (16).",
  "y": "background"
 },
 {
  "id": "c870d761c6fcd24de73f5bf98a9fd3_5",
  "x": "In terms of the equations presented above, we could compare the distributions P (z|o, C) with P (z|n, C) using equations (5) or (16). However, Thater et al. (2010) and<cite> Dinu and Lapata (2010)</cite> both observe that contextualising both o and n can degrade performance; in view of this we actually compare P (z|o, C) with P (z|n) and make the further simplifying assumption that P (z|n) \u221d P (n|z). The similarity measure we adopt is the Bhattacharyya coefficient, which is a natural measure of similarity between probability distributions and is closely related to the Hellinger distance used in previous work on topic modelling (Blei and Lafferty, 2007) :",
  "y": "background motivation"
 },
 {
  "id": "c870d761c6fcd24de73f5bf98a9fd3_6",
  "x": "In terms of the equations presented above, we could compare the distributions P (z|o, C) with P (z|n, C) using equations (5) or (16). However, Thater et al. (2010) and<cite> Dinu and Lapata (2010)</cite> both observe that contextualising both o and n can degrade performance; in view of this we actually compare P (z|o, C) with P (z|n) and make the further simplifying assumption that P (z|n) \u221d P (n|z). The similarity measure we adopt is the Bhattacharyya coefficient, which is a natural measure of similarity between probability distributions and is closely related to the Hellinger distance used in previous work on topic modelling (Blei and Lafferty, 2007) :",
  "y": "motivation"
 },
 {
  "id": "c870d761c6fcd24de73f5bf98a9fd3_7",
  "x": "Thater et al. (2010) discard sentences which their parser cannot parse and paraphrases absent from their training corpus and then optimise the parameters of their model through four-fold cross-validation. Here we aim for complete coverage on the dataset and do not perform any parameter tuning. We use two measures to evaluate performance: Generalised Averaged Precision (Kishida, 2005 ) and Kendall's \u03c4 b rank correlation coefficient, which were used for this task by Thater et al. (2010) and<cite> Dinu and Lapata (2010)</cite> , respectively.",
  "y": "uses"
 },
 {
  "id": "c870d761c6fcd24de73f5bf98a9fd3_8",
  "x": "For the syntactic models we extracted all dependencies and inverse dependencies between lemmas of the aforementioned POS types; in order to maximise the extraction yield, the dependency graph for each sentence was preprocessed using the transformations shown in Table 3 . For the window-based context model we follow<cite> Dinu and Lapata (2010)</cite> in treating each word within five words of a target as a member of its context set. It proved necessary to subsample the corpora in order to make LDA training tractable, especially for the window-based model where the training set of context-target counts is extremely dense (each instance of a word in the corpus contributes up to 10 context instances).",
  "y": "uses"
 },
 {
  "id": "c870d761c6fcd24de73f5bf98a9fd3_9",
  "x": "The choice of 70 for scaling Wikipedia counts is adopted from<cite> Dinu and Lapata (2010)</cite> , who used the same factor for the comparably sized English Gigaword corpus. As the dependency data is an order of magnitude smaller we downsampled the Wikipedia counts by 5 and left the BNC counts untouched. Finally, we created a larger corpus by combining the counts from the BNC and Wikipedia datasets.",
  "y": "uses"
 },
 {
  "id": "c870d761c6fcd24de73f5bf98a9fd3_10",
  "x": "As remarked in Section 3.1,<cite> Dinu and Lapata (2010)</cite> use a slightly different formulation of P (z|C, o). Using the window-based context model our formulation (5) outperforms (7) for both training corpora; the<cite> Dinu and Lapata (2010)</cite> Table 6 : Performance by part of speech Table 6 gives a breakdown of performance by target part of speech for the BNC+Wikipedia-trained W5 and W5 + T \u2194 C models, as well as figures provided by previous researchers. 7 W5 + T \u2194 C outperforms W5 on all parts of speech using both evaluation metrics.",
  "y": "differences"
 },
 {
  "id": "c870d761c6fcd24de73f5bf98a9fd3_11",
  "x": "Using the window-based context model our formulation (5) outperforms (7) for both training corpora; the<cite> Dinu and Lapata (2010)</cite> Table 6 : Performance by part of speech Table 6 gives a breakdown of performance by target part of speech for the BNC+Wikipedia-trained W5 and W5 + T \u2194 C models, as well as figures provided by previous researchers. 7 W5 + T \u2194 C outperforms W5 on all parts of speech using both evaluation metrics. As remarked above, previous researchers have used the corpus in slightly different ways; we believe that the results of<cite> Dinu and Lapata (2010)</cite> are fully comparable, while those of Thater et al. (2010) were attained on a slightly smaller dataset with parameters set through cross-validation.",
  "y": "differences"
 },
 {
  "id": "c870d761c6fcd24de73f5bf98a9fd3_12",
  "x": "Using the window-based context model our formulation (5) outperforms (7) for both training corpora; the<cite> Dinu and Lapata (2010)</cite> Table 6 : Performance by part of speech Table 6 gives a breakdown of performance by target part of speech for the BNC+Wikipedia-trained W5 and W5 + T \u2194 C models, as well as figures provided by previous researchers. 7 W5 + T \u2194 C outperforms W5 on all parts of speech using both evaluation metrics. As remarked above, previous researchers have used the corpus in slightly different ways; we believe that the results of<cite> Dinu and Lapata (2010)</cite> are fully comparable, while those of Thater et al. (2010) were attained on a slightly smaller dataset with parameters set through cross-validation.",
  "y": "similarities"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_0",
  "x": "Domain shift is a fundamental problem in machine learning, that has attracted a lot of attention in the natural language processing and vision communities [2, 6, 11,<cite> 13,</cite> 29, 30, 32, 37, 39, 40, 42] . To understand and address this problem, generated by the lack of labeled data in a target domain, researchers have studied the behavior of machine learning methods in cross-domain settings [12,<cite> 13,</cite> 29] and came up with various domain adaptation techniques [6, 11, 28, 39] . In crossdomain classification, a classifier is trained on data from a source domain and tested on data from a (different) target domain.",
  "y": "background"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_1",
  "x": "Domain shift is a fundamental problem in machine learning, that has attracted a lot of attention in the natural language processing and vision communities [2, 6, 11,<cite> 13,</cite> 29, 30, 32, 37, 39, 40, 42] . To understand and address this problem, generated by the lack of labeled data in a target domain, researchers have studied the behavior of machine learning methods in cross-domain settings [12,<cite> 13,</cite> 29] and came up with various domain adaptation techniques [6, 11, 28, 39] . In crossdomain classification, a classifier is trained on data from a source domain and tested on data from a (different) target domain.",
  "y": "background"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_2",
  "x": "Interestingly, some recent works<cite> [13,</cite> 18] indicate that string kernels can yield robust results in the cross-domain setting without any domain adaptation. In fact, methods based on string kernels have demonstrated impressive results in various text classification tasks ranging from native language identification [22] [23] [24] 36] and authorship identification [34] to dialect identification [4, 18, 21] , sentiment analysis<cite> [13,</cite> 35] and automatic essay scoring [7] . As long as a labeled training set is available, string kernels can reach state-of-the-art results in various languages including English [7,<cite> 13,</cite> 23] , Arabic [4, 17, 18, 24] , Chinese [35] and Norwegian [24] .",
  "y": "background"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_3",
  "x": "In fact, methods based on string kernels have demonstrated impressive results in various text classification tasks ranging from native language identification [22] [23] [24] 36] and authorship identification [34] to dialect identification [4, 18, 21] , sentiment analysis<cite> [13,</cite> 35] and automatic essay scoring [7] . As long as a labeled training set is available, string kernels can reach state-of-the-art results in various languages including English [7,<cite> 13,</cite> 23] , Arabic [4, 17, 18, 24] , Chinese [35] and Norwegian [24] . Different from all these recent approaches, we use unlabeled data from the test set in a transductive setting in order to significantly increase the performance of string kernels.",
  "y": "background"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_4",
  "x": "As long as a labeled training set is available, string kernels can reach state-of-the-art results in various languages including English [7,<cite> 13,</cite> 23] , Arabic [4, 17, 18, 24] , Chinese [35] and Norwegian [24] . Different from all these recent approaches, we use unlabeled data from the test set in a transductive setting in order to significantly increase the performance of string kernels. In our recent work [19] , we proposed two transductive learning approaches combined into a unified framework that improves the results of string kernels in two different tasks.",
  "y": "differences"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_5",
  "x": "In recent years, methods based on string kernels have demonstrated remarkable performance in various text classification tasks [7, 10,<cite> 13,</cite> 18, 23, 27, 34] . String kernels represent a way of using information at the character level by measuring the similarity of strings through character n-grams. Lodhi et al. [27] used string kernels for document categorization, obtaining very good results.",
  "y": "background"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_6",
  "x": "Gim\u00e9nez-P\u00e9rez et al. <cite>[13]</cite> have used string kernels for single-source and multi-source polarity classification. Remarkably, they obtain state-of-the-art performance without using knowledge from the target domain, which indicates that string kernels provide robust results in the cross-domain setting without any domain adaptation. Ionescu et al. [18] obtained the best performance in the Arabic Dialect Identification Shared Task of the 2017 VarDial Evaluation Campaign [41] , with an improvement of 4.6% over the second-best method.",
  "y": "background"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_7",
  "x": "Ionescu et al. [18] obtained the best performance in the Arabic Dialect Identification Shared Task of the 2017 VarDial Evaluation Campaign [41] , with an improvement of 4.6% over the second-best method. It is important to note that the training and the test speech samples prepared for the shared task were recorded in different setups [41] , or in other words, the training and the test sets are drawn from different distributions. Different from all these recent approaches<cite> [13,</cite> 18, 23] , we use unlabeled data from the target domain to significantly increase the performance of string kernels in cross-domain text classification, particularly in English polarity classification.",
  "y": "differences"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_8",
  "x": "Baselines. We compare our approach with several methods [3, 12,<cite> 13,</cite> 15, 32, 40] in two cross-domain settings. Using string kernels, Gim\u00e9nez-P\u00e9rez et al. <cite>[13]</cite> reported better performance than SST [3] and KE-Meta [12] in the multi-source domain setting.",
  "y": "uses"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_9",
  "x": "Baselines. We compare our approach with several methods [3, 12,<cite> 13,</cite> 15, 32, 40] in two cross-domain settings. Using string kernels, Gim\u00e9nez-P\u00e9rez et al. <cite>[13]</cite> reported better performance than SST [3] and KE-Meta [12] in the multi-source domain setting.",
  "y": "differences"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_11",
  "x": "In addition, we compare our approach with SFA [32] , CORAL [40] and TR-TrAdaBoost [15] in the single-source setting. Method DEK\u2192B BEK\u2192D BDK\u2192E BDE\u2192K SST [3] 76.3 78.3 83.9 85.2 KE-Meta [12] 77.9 80.4 78.9 82.5 K 0/1 <cite>[13]</cite> 82 Table 1 . Multi-source cross-domain polarity classification accuracy rates (in %) of our transductive approaches versus a state-of-the-art baseline based on string kernels <cite>[13]</cite> , as well as SST [3] and KE-Meta [12] .",
  "y": "uses"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_12",
  "x": "Evaluation procedure and parameters. We follow the same evaluation methodology of Gim\u00e9nez-P\u00e9rez et al. <cite>[13]</cite> , to ensure a fair comparison. Furthermore, we use the same kernels, namely the presence bits string kernel (K 0/1 ) and the intersection string kernel (K \u2229 ), and the same range of character n-grams (5) (6) (7) (8) . To compute the string kernels, we used the open-source code provided by Ionescu et al. [20, 23] .",
  "y": "uses"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_13",
  "x": "We choose Kernel Ridge Regression [38] as classifier and set its regularization parameter to 10 \u22125 in all our experiments. Although Gim\u00e9nez-P\u00e9rez et al. <cite>[13]</cite> used a different classifier, namely Kernel Discriminant Analysis, we observed that Kernel Ridge Regression produces similar results (\u00b10.1%) when we employ the same string kernels. As Gim\u00e9nez-P\u00e9rez et al. <cite>[13]</cite> , we evaluate our approach in two cross-domain settings.",
  "y": "similarities differences"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_14",
  "x": "As Gim\u00e9nez-P\u00e9rez et al. <cite>[13]</cite> , we evaluate our approach in two cross-domain settings. In the multi-source setting, we train the models on all domains, except the one used for testing. In the single-source setting, we train the models on one of the four domains and we independently test the models on the remaining three domains.",
  "y": "uses"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_15",
  "x": "For example, on the Books domain the accuracy of the transductive classifier based on the presence bits kernel (84.1%) is 2.1% above the best baseline (82.0%) represented by the intersection string kernel. Remark- Table 2 . Single-source cross-domain polarity classification accuracy rates (in %) of our transductive approaches versus a state-of-the-art baseline based on string kernels <cite>[13]</cite> , as well as SFA [32] , CORAL [40] and TR-TrAdaBoost [15] .",
  "y": "uses"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_16",
  "x": "The results for the single-source crossdomain polarity classification setting are presented in Table 2 . We considered all possible combinations of source and target domains in this experiment, and we improve the results in each and every case. Without exception, the accuracy rates reached by the transductive string kernels are significantly better than the best baseline string kernel <cite>[13]</cite> , according to the McNemar's test performed at a confidence level of 0.01.",
  "y": "differences"
 },
 {
  "id": "c897c2ea0d641f1f35072be4a5a7d3_17",
  "x": "We provided empirical evidence indicating that our framework can be successfully applied in cross-domain text classification, particularly in cross-domain English polarity classification. Indeed, the polarity classification experiments demonstrate that our framework achieves better accuracy rates than other state-ofthe-art methods [3, 12,<cite> 13,</cite> 15, 32, 40] . By using the same parameters across all the experiments, we showed that our transductive transfer learning framework can bring significant improvements without having to fine-tune the parameters for each individual setting.",
  "y": "differences"
 },
 {
  "id": "c932ba05eb5cb30094dd98739daa95_0",
  "x": "Figure 1 shows the predicates and arguments extracted by PredPatt from the sentence: \"Chris, the designer, wants to launch a new brand.\" The underlying predicate-argument structure constructed by PredPatt is a directed graph, where a special dependency ARG is built between a predicate head token and its arguments' head tokens, and the original UD relations are retained within predicate phrases and argument phrases. For example, Figure 2 shows the directed graph for the predicate-argument extraction (1) and (2) in Figure 1 . Compared to other existing systems for predicate-argument extraction (Banko et al., 2007; Fader et al., 2011; <cite>Angeli et al., 2015)</cite> , the use of manual language-agnostic patterns on UD makes PredPatt a well-founded component across languages.",
  "y": "background"
 },
 {
  "id": "c932ba05eb5cb30094dd98739daa95_1",
  "x": "3 These two corpora have all verbal predicates annotated, and are used to evaluate PredPatt in different perspectives: EWT is the corpus where the gold standard English UD Treebank is built over, which enables an evaluation and analysis of PredPatt patterns; WSJ is used to evaluate PredPatt in a real-world scenario where we run SyntaxNet Parser 4 (Andor et al., 2016) on the corpus to generate automated UD parses as input of PredPatt. Table 1 shows the statistics of the auto-converted gold annotations for predicate-argument extraction on EWT and WSJ. We convert the PropBank annotations for all verbal predicates in these two corpora, and ignore roles of directional (DIR), manner (MNR), modals (MOD), negation (NEG) and adverbials (ADV), as they aren't extracted as distinct argument but instead are folded into the complex predicate by PredPatt and other systems for predicate-argument extraction (Banko et al., 2007; Fader et al., 2011; <cite>Angeli et al., 2015)</cite> .",
  "y": "background"
 },
 {
  "id": "c932ba05eb5cb30094dd98739daa95_2",
  "x": "In this section, we evaluate the original PredPatt (PredPatt v1) and the improved PredPatt (PredPatt v2) on the English Web Treebank (EWT) and the Wall Street Journal corpus (WSJ), and compare their performance with four prominent Open IE systems: OpenIE 4, 6 OLLIE (Mausam et al., 2012) , ClausIE (Del Corro and Gemulla, 2013) , and Stanford Open IE<cite> (Angeli et al., 2015)</cite> . ---------------------------------- **PRECISION-RECALL CURVE**",
  "y": "uses"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_0",
  "x": "Natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity. Recently, <cite>Canny et al. (2013)</cite> presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU. In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU.",
  "y": "background"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_1",
  "x": "Recently, <cite>Canny et al. (2013)</cite> proposed a GPU implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that GPUs can provide. Their system uses a grammar based on the Berkeley parser (Petrov and Klein, 2007) (which is particularly amenable to GPU processing), \"compiling\" the grammar into a sequence of GPU kernels that are applied densely to every item in the parse chart. Together these kernels implement the Viterbi inside algorithm.",
  "y": "background"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_2",
  "x": "For comparison, the publicly available CPU implementation of Petrov and Klein (2007) parses approximately 7 sentences per second per core on a modern CPU. A further drawback of the dense approach in <cite>Canny et al. (2013)</cite> is that it only computes Viterbi parses. As with other grammars with a parse/derivation distinction, the grammars of Petrov and Klein (2007) only achieve their full accuracy using minimum-Bayes-risk parsing, with improvements of over 1.5 F1 over best-derivation Viterbi parsing on the Penn Treebank (Marcus et al., 1993) .",
  "y": "background"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_3",
  "x": "1 We should note that our experimental condition differs from that of <cite>Canny et al. (2013)</cite> : they evaluate on sentences of length \u2264 30. Furthermore, they 1 The implementation of <cite>Canny et al. (2013)</cite> cannot handle batches so large, and so we tested it on batches of 1200 sentences. Our reimplementation is approximately the same speed for the same batch sizes.",
  "y": "differences"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_4",
  "x": "1 We should note that our experimental condition differs from that of <cite>Canny et al. (2013)</cite> : they evaluate on sentences of length \u2264 30. Furthermore, they 1 The implementation of <cite>Canny et al. (2013)</cite> cannot handle batches so large, and so we tested it on batches of 1200 sentences. Our reimplementation is approximately the same speed for the same batch sizes.",
  "y": "differences uses"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_5",
  "x": "Our system uses a coarse-to-fine approach, where the coarse pass computes a pruning mask that is used by the CPU when deciding which items to queue during the fine pass. The original system of <cite>Canny et al. (2013)</cite> only used the fine pass, with no pruning. instructions in lockstep, differing only in their input data.",
  "y": "differences"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_6",
  "x": "**ANATOMY OF A DENSE GPU PARSER** This architecture environment puts very different constraints on parsing algorithms from a CPU environment. <cite>Canny et al. (2013)</cite> proposed an implementation of a PCFG parser that sacrifices standard sparse methods like coarse-to-fine pruning, focusing instead on maximizing the instruction and memory throughput of the parser.",
  "y": "background"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_7",
  "x": "Speedups are measured in reference to this reimplementation. See Section 7 for discussion of the clustering algorithms and Section 6 for a description of the pruning methods. The <cite>Canny et al. (2013)</cite> system is benchmarked on a batch size of 1200 sentences, the others on 20,000.",
  "y": "background"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_8",
  "x": "One important feature of <cite>Canny et al. (2013)</cite> 's system is grammar compilation. Because registers are so much faster than thread-local memory, it is critical to keep as many variables in registers as possible. One way to accomplish this is to unroll loops at compilation time.",
  "y": "background"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_9",
  "x": "However, register space is limited on GPUs. Because the Berkeley grammar is so large, the compiler is not able to efficiently schedule all of the operations in the grammar, resulting in register spills. <cite>Canny et al. (2013)</cite> found they had to partition the grammar into multiple different kernels.",
  "y": "background"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_10",
  "x": "All in all, <cite>Canny et al. (2013)</cite> 's system is able to compute Viterbi charts at 164 sentences per second, for sentences up to length 40. On larger batch sizes, our reimplementation of their approach is able to achieve 193 sentences per second on the same hardware. (See Table 1 .) 6 Pruning on a GPU Now we turn to the algorithmic and architectural changes in our approach.",
  "y": "differences"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_11",
  "x": "Once on the GPU, parse items are processed using the same style of compiled kernel as in <cite>Canny et al. (2013)</cite> . Because the entire partition (though not necessarily the entire grammar) is applied to each item in the queue, we still do not need to worry about warp divergence. At the top level, our system first computes pruning masks with a coarse grammar.",
  "y": "similarities uses"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_12",
  "x": "That way, we are more likely to be able to skip a subcluster, since fewer distinct symbols need to be \"off\" for a parse item to be skipped in a given subcluster. <cite>Canny et al. (2013)</cite> clustered symbols of the grammar using a sophisticated spectral clustering algorithm to obtain a permutation of the symbols. Then the rules of the grammar were laid out in a (sparse) three-dimensional tensor, with one dimension representing the parent of the rule, one representing the left child, and one representing the right child.",
  "y": "background"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_13",
  "x": "4 In order to subcluster, we divide up rules among subclusters so that each subcluster has the same number of active parent symbols. We found this approach to subclustering worked well in practice. Clustering using this method is labeled 'Parent' in Table 1 . Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to <cite>Canny et al. (2013)</cite> 's system, and nearly 50% over our reimplemented baseline.",
  "y": "differences"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_14",
  "x": "It turns out that this simple clustering algorithm produces relatively efficient kernels even in the unpruned case. The unpruned Viterbi computations in a fine grammar using the clustering method of <cite>Canny et al. (2013)</cite> yields a speed of 193 sentences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second. (See Table 1 .) This is not as efficient as <cite>Canny et al. (2013)</cite> 's highly tuned method, but it is still fairly fast, and much simpler to implement.",
  "y": "differences uses"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_15",
  "x": "(See Table 1 .) This is not as efficient as <cite>Canny et al. (2013)</cite> 's highly tuned method, but it is still fairly fast, and much simpler to implement. ---------------------------------- **PRUNING WITH FINER GRAMMARS**",
  "y": "differences"
 },
 {
  "id": "ca1391f1f908fc081589b1a7dd8229_16",
  "x": "---------------------------------- **RELATED WORK** Apart from the model of <cite>Canny et al. (2013)</cite> , there have been a few attempts at using GPUs in NLP contexts before.",
  "y": "background"
 },
 {
  "id": "ca7db62af4457ca887fe220c43b10e_0",
  "x": "**ABSTRACT** If we compare the widely used Conditional Random Fields (CRF) with newly proposed \"deep architecture\" sequence models<cite> (Collobert et al., 2011)</cite> , there are two things changing: from linear architecture to non-linear, and from discrete feature representation to distributional. It is unclear, however, what utility nonlinearity offers in conventional featurebased models.",
  "y": "differences"
 },
 {
  "id": "ca7db62af4457ca887fe220c43b10e_1",
  "x": "Most methods developed so far for sequence labeling employ generalized linear statistical models, meaning methods that describe the data as a combination of linear basis functions, either directly in the input variables space (e.g., SVM) or through some transformation of the probability distributions (e.g., \"log-linear\" models). Recently, <cite>Collobert et al. (2011)</cite> proposed \"deep architecture\" models for sequence labeling (named Sentence-level Likelihood Neural Nets, abbreviated as SLNN henceforth), and showed promising results on a range of tasks (POS tagging, NER, Chunking, and SRL). Two new changes were suggested: extending the model from a linear to non-linear architecture; and replacing discrete feature representations with distributional feature representations in a continuous space.",
  "y": "motivation"
 },
 {
  "id": "ca7db62af4457ca887fe220c43b10e_2",
  "x": "In neural network terminology, this architecture is called a single-layer Input-Output Neural Network (IONN). 1 Normalizing locally in a logistic regression is equivalent to adding a softmax layer to the output layer of the IONN, which was commonly done in neural networks, such as in <cite>Collobert et al. (2011)</cite> . We can add a hidden linear layer to this architecture to formulate a two-layer Linear Neural Network (LNN), as shown in the middle diagram of Figure 1 .",
  "y": "similarities"
 },
 {
  "id": "ca7db62af4457ca887fe220c43b10e_3",
  "x": "So far we have extended the potential function used in node cliques of a CRF to a non-linear DNN. And if we keep the potential function for edge cliques the same as before, then in fact we have arrived at an identical model to the SLNN in Collobert et al.<cite> (Collobert et al., 2011)</cite> . The difference between a SLNN and an ordinary DNN model is that we need to take into consideration the influence of edge cliques, and therefore we can no longer normalize the clique factors at each position to calculate the local marginals, as we would do in a logistic regression. The cardinality of the output variable vector y grows exponentially with respect to input sequence length.",
  "y": "similarities"
 },
 {
  "id": "ca7db62af4457ca887fe220c43b10e_4",
  "x": "It is also worth pointing out that this model has in fact been introduced a few times in prior literature. It was termed Conditional Neural Fields by Peng et al. (2009) , and later Neural Conditional Random Fields by Do and Artieres (2010) . Unfortunately, the connection to Collobert and Weston (2008) was not recognized in either of these two studies; vice versa, neither of the above were referenced in <cite>Collobert et al. (2011)</cite> .",
  "y": "differences"
 },
 {
  "id": "ca7db62af4457ca887fe220c43b10e_5",
  "x": "We did not explicitly tune the features used in CRF to optimize for performance, since feature engineering is not the focus of this study. However, overall we found that the feature set we used is competitive with CRF results from earlier literature (Turian et al., 2010;<cite> Collobert et al., 2011)</cite> . For models that embed hidden layers, we set the number of hidden nodes to 300.",
  "y": "similarities"
 },
 {
  "id": "ca7db62af4457ca887fe220c43b10e_6",
  "x": "For models that embed hidden layers, we set the number of hidden nodes to 300. 2 Results are reported on the standard evaluation metrics of entity/chunk precision, recall and F1 measure. For experiments with continuous space feature representations (a.k.a., word embeddings), we took the word embeddings (130K words, 50 dimensions) used in <cite>Collobert et al. (2011)</cite> , which were trained for 2 months over Wikipedia text.",
  "y": "similarities uses"
 },
 {
  "id": "ca7db62af4457ca887fe220c43b10e_7",
  "x": "We attempt to replicate the model described in <cite>Collobert et al. (2011)</cite> without task-specific fine-tuning, with a few exceptions: 1) we used the soft tanh activation function instead of hard tanh; 2) we use the BIO2 tagging scheme instead of BIOES; 3) we use L-BFGS optimization algorithm instead of stochastic gradient descent; 4) we did not use Gazetteer features; 5) <cite>Collobert et al. (2011)</cite> mentioned 5 binary features that look at the capitalization pattern of words to append to the embedding as additional dimensions, but only 4 were described in the paper, which we implemented accordingly. ---------------------------------- **RESULTS OF DISCRETE REPRESENTATION**",
  "y": "extends differences"
 },
 {
  "id": "ca7db62af4457ca887fe220c43b10e_8",
  "x": "Four binary features are also appended to each word embedding to capture capitalization patterns, as described in <cite>Collobert et al. (2011)</cite> . Results of the CRF and SLNN under this setting for the NER task is show in Table 3 . With a continuous space representation, the SLNN model works significantly better than a CRF, by as much as 7% on the CoNLL development set, and 3.7% on ACE dataset.",
  "y": "uses similarities"
 },
 {
  "id": "ca7db62af4457ca887fe220c43b10e_9",
  "x": "**CONCLUSION** We carefully compared and analyzed the nonlinear neural networks used in <cite>Collobert et al. (2011)</cite> and the widely adopted CRF, and revealed their close relationship. Through extensive experiments on NER and Syntactic Chunking, we have shown that non-linear architectures are effective in low dimensional continuous input spaces, but that they are not better suited for conventional highdimensional discrete input spaces.",
  "y": "similarities uses"
 },
 {
  "id": "ca98f16fa3a118f83b16586bba04c8_0",
  "x": "Open-domain human-computer dialog systems are attracting increasing attention in the NLP community. With the development of deep learning, sequence-to-sequence (Seq2Seq) neural networks or more generally encoder-decoder frameworks, are among the most popular models for text-based response generation in dialog systems [1, 2,<cite> 3,</cite> 4] . Historically, Seq2Seq-like models are first designed for machine translation [5, 6] and later widely applied to image captioning [7] , text summarization [8] , etc.",
  "y": "background"
 },
 {
  "id": "ca98f16fa3a118f83b16586bba04c8_1",
  "x": "Historically, Seq2Seq-like models are first designed for machine translation [5, 6] and later widely applied to image captioning [7] , text summarization [8] , etc. When adapted to text-based open-domain dialog systems, however, Seq2Seq models are less satisfactory. A severe problem is that the Seq2Seq model tends to generate short and meaningless replies, e.g., \"I don't know\" [2] and \"Me too\" <cite>[3]</cite> .",
  "y": "background"
 },
 {
  "id": "ca98f16fa3a118f83b16586bba04c8_2",
  "x": "They are universally relevant to most utterances, called universal replies in <cite>[3]</cite> , and hence less desired in real-world conversation systems. In previous studies, researchers have proposed a variety of approaches to address the problem of universal replies, ranging from heuristically modified training objectives [2] , diversified decoding algorithms [9] , to content-introducing approaches<cite> [3,</cite> 10] . Although the problems of universal replies have been alleviated to some extent, there lacks an empirical explanation to the curious question: Why does the same Seq2Seq model tend to generate shorter and less meaningful sentences in a dialog system than in a machine translation system?",
  "y": "background"
 },
 {
  "id": "ca98f16fa3a118f83b16586bba04c8_3",
  "x": "A severe problem is that the Seq2Seq model tends to generate short and meaningless replies, e.g., \"I don't know\" [2] and \"Me too\" <cite>[3]</cite> . They are universally relevant to most utterances, called universal replies in <cite>[3]</cite> , and hence less desired in real-world conversation systems. In previous studies, researchers have proposed a variety of approaches to address the problem of universal replies, ranging from heuristically modified training objectives [2] , diversified decoding algorithms [9] , to content-introducing approaches<cite> [3,</cite> 10] .",
  "y": "background"
 },
 {
  "id": "ca98f16fa3a118f83b16586bba04c8_4",
  "x": "This conjecture is casually expressed in previous work <cite>[3]</cite> , but is so far not supported by experiments. To verify our conjecture, we propose a method by mimicking the unaligned phenomenon on machine translation datasets, which is to shuffle the source and target sides of the translation pairs to artificially build a conditional distribution of target sentences with multiple plausible data points. We conduct experiments on a widely used translation dataset; we further conduct a simulation with some predefined distributions, serving as additional evidence.",
  "y": "background"
 },
 {
  "id": "ca98f16fa3a118f83b16586bba04c8_5",
  "x": "Other areas with low probabilities are nonsensical utterances that are either not fluent in spoken language or irrelevant to the previous utterance s. The above is, perhaps, the most salient difference between dialog and translation datasets. Although it is tempting to think of Seq2Seq's performance in this way <cite>[3]</cite> , barely a practical approach exists to verify the conjecture in the dialog setting alone.",
  "y": "background"
 },
 {
  "id": "ca98f16fa3a118f83b16586bba04c8_8",
  "x": "These metrics are used in previous work [4,<cite> 3]</cite> , and are related to our research question. We first compare the dialog system with machine translation, both in the un-shuffled setting. We observe that the dialog system does generate short and meaningless replies with lower length, NLL, and entropy metrics than references, as opposed to machine translation where Seq2Seq's generated sentences are comparable to references in terms of these statistics on both two datasets.",
  "y": "similarities uses"
 },
 {
  "id": "ca98f16fa3a118f83b16586bba04c8_9",
  "x": "However, the unaligned property is a salient difference, and by controlling this, we observe the desired phenomenon, demonstrating our conjecture. Our findings also explain why referring to additional information-including dialog context [19] , keywords <cite>[3]</cite> and knowledge bases [20]-helps dialog systems: the number of plausible target sentences decreases if the generation is conditioned on more information; this intuition is helpful for future development of text-based response generation in Seq2Seq dialog systems. Besides, our experiments suggest that Seq2Seq models are more suitable to applications where the source and target information is aligned.",
  "y": "similarities"
 },
 {
  "id": "cb81d56412d1e800074777687fb45a_0",
  "x": "Using the model of <cite>Yu et al. (2018a)</cite> , we compare several key model configurations. Results show that our human-translated dataset is significantly more reliable compared to a dataset composed of machine-translated questions. In addition, the overall accuracy for Chinese SQL semantic parsing can be comparable to that for English.",
  "y": "uses"
 },
 {
  "id": "cb81d56412d1e800074777687fb45a_1",
  "x": "To our knowledge, we are the first to release a Chinese SQL semantic parsing dataset. There has been a line of work improving the model of <cite>Yu et al. (2018a)</cite> since the release of the Spider dataset (Guo et al., 2019; Lin et al., 2019) . At the time of our investigation, however, the models are not published.",
  "y": "motivation background"
 },
 {
  "id": "cb81d56412d1e800074777687fb45a_2",
  "x": "---------------------------------- **MODEL** We use the neural semantic parsing method of <cite>Yu et al. (2018a)</cite> as the baseline model, which can be regarded as a sequence-to-tree model.",
  "y": "uses"
 },
 {
  "id": "cb81d56412d1e800074777687fb45a_3",
  "x": "Hyperparameters. Our hyperparameters are mostly taken from <cite>Yu et al. (2018a)</cite> , but tuned on the Chinese Spider development set. We use character and word embeddings from Tencent embedding; both of them are not fine-tuned during model training.",
  "y": "extends uses"
 },
 {
  "id": "cb81d56412d1e800074777687fb45a_4",
  "x": "Figure 2 shows F1 scores of several typical components, including 4 Note that the results are lower than those reported by <cite>Yu et al. (2018a)</cite> under their split due to different training/test splits. Our split has less training data and more test instances in the \"Hard\" category and less in \"Easy\" and \"Medium\". Table 4 .",
  "y": "differences"
 },
 {
  "id": "cb81d56412d1e800074777687fb45a_5",
  "x": "In this table, ENG represents the results of <cite>Yu et al. (2018a)</cite> 's model on their English dataset but under our split. C-ML and C-S denote the results of our Chinese models based on characters with multi-lingual embeddings and monolingual embeddings, respectively, while WY-ML, WY-S denote the wordbased models applying YZ segmentor with multilingual embeddings and monolingual embeddings, respectively. First, compared to the best results of human translation (C-ML and WY-ML), machine translation results show a large disadvantage (e.g. 7.1% vs 12.1% using C-ML). Out of the 100 translated Second, comparisons among C-ML, WY-ML and WJ-ML, and among C-S, WY-S and WJ-S show that multi-lingual embeddings give superior results compared to monolingual embeddings, which is likely because they bring a better connection between natural language questions and database columns.",
  "y": "differences"
 },
 {
  "id": "cbe9e36f371c072432ca25800c96d3_0",
  "x": "phonemes). Such features can be hand-crafted by leveraging prior knowledge [11, 12, 13, 14] , or they can be learned in a data-driven fashion. Furthermore, this learning can take place jointly with ASR [15] , or separately with some tasks that have aligned objectives [16,<cite> 17,</cite> 18] .",
  "y": "background"
 },
 {
  "id": "cbe9e36f371c072432ca25800c96d3_1",
  "x": "**EVALUATING TRANSFER LEARNING PERFORMANCE** To evaluate transfer learning performance, we consider three criteria: (1) inclusion of phonetic content, (2) exclusion of nuisance factors, and (3) transferrability across datasets. The first two are evaluated using a protocol similar to <cite>[17]</cite> , where an ASR model is trained on a set of domains, and evaluated on both in-domain and out-of-domain speech (relative to the training data).",
  "y": "similarities"
 },
 {
  "id": "cbe9e36f371c072432ca25800c96d3_2",
  "x": "For example, in natural language processing, dense word vector models such as word2vec [34] and GloVe [35] , or more advanced ones like ELMo [36] and BERT [37] have quickly replaced one-hot word representations in many tasks and pushed the state-of-theart forward on a variety of language understanding tasks. More recently, there is also an increasing interest in learning from multimodal data [38] and transfer learned representations from such tasks [39] In the field of speech recognition, low-resource speech recognition is a scenario which heavily benefits from transfer learning, for example in the form of training on multilingual datasets [40] . Other models capable of disentangling phonetic and domain information have recently been shown to learn acoustic features with a greater degree of domain invariance than traditional acoustic features [16, 7,<cite> 17]</cite> .",
  "y": "background"
 },
 {
  "id": "cbe9e36f371c072432ca25800c96d3_3",
  "x": "The set of conditions are divided into four groups: clean (A), noisy (B), channel (C), and noisy+channel (D). While recordings in A are recorded by one microphone in quiet environments, those in C are recorded with a different set of microphones than A. Recordings in B and D are created from A and C, respectively, with artificially added noises. Similar to <cite>[17]</cite> , we use the clean set (A) for training ASR systems, and test on the four groups separately.",
  "y": "similarities"
 },
 {
  "id": "cbe9e36f371c072432ca25800c96d3_4",
  "x": "The first one is FBank feature, which is the input to DAVEnet models and contains rich phonetic and domain information. The second one is the latent segment variable z1 from a model called factorized hierarchical variational autoencoder (FHVAE) [16] . FHVAE learns to encode sequence-level and segment-level information into separate latent variables without supervision by optimizing an evidence lower bound derived from a factorized graphical model, and has been shown effective for extracting domain invariant ASR features <cite>[17]</cite> .",
  "y": "background"
 },
 {
  "id": "cbe9e36f371c072432ca25800c96d3_5",
  "x": "While previous work investigated usage of FHVAE for ASR by training FHVAE models on all domains of the target task (e.g., Aurora-4 with all four conditions)<cite> [17,</cite> 8] , we also evaluate FHVAE models trained on PlacesAudCap to test cross-dataset transferability, and on the subset of domains used for ASR training. We use FHVAE models with two LSTM layers, each with 256 cells, for both the encoders and decoder. A discriminative weight of \u03b1 = 10 is applied for all models, and the scalable training algorithm proposed in [51] is used for training on PlacesAudCap dataset with a sequence batch size K = 5000, because the original algorithm cannot handle large-scale datasets.",
  "y": "background"
 },
 {
  "id": "cbed76ac8086637fe1d2e30f39c585_0",
  "x": "They are also capable of correcting complex errors which are difficult for classifier systems that target specific error types. The generalization of SMT-based GEC systems has been shown to improve further by adding neural network models <cite>(Chollampatt et al., 2016b)</cite> . Though SMT provides a strong framework for GEC, the traditional word-level SMT is weak in generalizing beyond patterns seen in the training data Rozovskaya and Roth, 2016) .",
  "y": "background"
 },
 {
  "id": "cbed76ac8086637fe1d2e30f39c585_1",
  "x": "Two of the top three teams used this approach in their systems. It later became the most widely used approach and was used in state-of-the-art GEC systems<cite> Chollampatt et al., 2016b</cite>; JunczysDowmunt and Grundkiewicz, 2016; Rozovskaya and Roth, 2016) . Neural machine translation approaches have also showed some promise (Xie et al., 2016; .",
  "y": "background"
 },
 {
  "id": "cbed76ac8086637fe1d2e30f39c585_2",
  "x": "Additionally, we use neural network joint models (Devlin et al., 2014) introduced in <cite>(Chollampatt et al., 2016b)</cite> and a character-level SMT component. Character-level SMT systems are used in transliteration and machine translation (Tiedemann, 2009; Nakov and Tiedemann, 2012; Durrani et al., 2014) . It has been previously used for spelling correction in Arabic (Bougares and Bouamor, 2015) and for pre-processing noisy input to an SMT system (Formiga and Fonollosa, 2012) .",
  "y": "background"
 },
 {
  "id": "cbed76ac8086637fe1d2e30f39c585_3",
  "x": "---------------------------------- **NEURAL NETWORK JOINT MODELS AND ADAPTATION** Following<cite> Chollampatt et al. (2016b)</cite> , we add a neural network joint model (NNJM) feature to further improve the SMT component.",
  "y": "uses"
 },
 {
  "id": "cbed76ac8086637fe1d2e30f39c585_4",
  "x": "Our word-level SMT-based GEC system utilizes task-specific features described in (JunczysDowmunt and Grundkiewicz, 2016) . We show in this paper that performance continues to improve further after adding neural network joint models (NNJMs), as introduced in <cite>(Chollampatt et al., 2016b)</cite> . NNJMs can leverage the continuous space representation of words and phrases and can capture a larger context from the source sentence, which enables them to make better predictions than traditional language models (Devlin et al., 2014) .",
  "y": "similarities background"
 },
 {
  "id": "cbfa4d71f40d8008ebd90026dc1bcd_0",
  "x": "In this paper we focus only on the streaming shallow processing part of the SUMMA project (the dark block in Fig.1 ), where the recently developed neural machine translation techniques (Sutskerev, <cite>Vinyals & Le, 2014</cite>; Bahdanau, Cho & Bengio, 2014) enable radically new end-to-end approach to machine translation and clustering of the incoming news stories. The approach is informed by our previous work on machine learning (Barzdins, Paikens, Gosko, 2013) , media monitoring (Barzdins et al.,2014) , and character-level neural translation (Barzdins & Gosko, 2016) . The key difference of the SUMMA project is that it has been incepted after the recent paradigm-shift (Manning, 2015) in the NLP community towards neural network inspired deep learning techniques such as end-to-end automatic speech recognition (Graves & Jaitly, 2014; Hannun et al., 2014; Amodei, 2015) , end-to-end machinetranslation (Sutskerev, <cite>Vinyals & Le, 2014</cite>; Bahdanau, Cho & Bengio, 2014; Luong et al., 2015) , efficient distributed vectorspace word embeddings (Mikolov et al., 2013) , image and video captioning Venugopalan et al., 2015) , unsupervised learning of document representations by autoencoders (Li, Luong & Jurafsky, 2015) .",
  "y": "uses background"
 },
 {
  "id": "cbfa4d71f40d8008ebd90026dc1bcd_1",
  "x": "The approach is informed by our previous work on machine learning (Barzdins, Paikens, Gosko, 2013) , media monitoring (Barzdins et al.,2014) , and character-level neural translation (Barzdins & Gosko, 2016) . The key difference of the SUMMA project is that it has been incepted after the recent paradigm-shift (Manning, 2015) in the NLP community towards neural network inspired deep learning techniques such as end-to-end automatic speech recognition (Graves & Jaitly, 2014; Hannun et al., 2014; Amodei, 2015) , end-to-end machinetranslation (Sutskerev, <cite>Vinyals & Le, 2014</cite>; Bahdanau, Cho & Bengio, 2014; Luong et al., 2015) , efficient distributed vectorspace word embeddings (Mikolov et al., 2013) , image and video captioning Venugopalan et al., 2015) , unsupervised learning of document representations by autoencoders (Li, Luong & Jurafsky, 2015) . These recent deep learning breakthroughs along with massively parallel GPU computing allow addressing the media monitoring tasks in the completely new end-toend manner rather than relying on the legacy NLP pipelines.",
  "y": "background"
 },
 {
  "id": "cbfa4d71f40d8008ebd90026dc1bcd_2",
  "x": "This shared vectorspace approach extends also to the unsupervised multi-task learning of language models from the large monolingual corpora (Fig.  3) , which is crucial for low-resourced languages: having a generic language model learned in parallel from the monolingual corpora reduces (Dai & Le, 2015) the need for large supervised parallel corpora to achieve the same translational accuracy for the Fig. 2 setup. The joint training of seventeen translational and samelanguage autoencoders with shared parameters ( Fig. 2 and Fig. 3 together) to our knowledge has not been attempted so far. Even training of a single state-of-the-art sentencelevel translational autoencoder requires days of GPU computing (Barzdins & Gosko, 2016) ) in TensorFlow (Abadi et al., 2015) seq2seq model (Sutskerev, <cite>Vinyals & Le, 2014</cite>; Bahdanau, Cho & Bengio, 2014) .",
  "y": "background"
 },
 {
  "id": "cbfa4d71f40d8008ebd90026dc1bcd_3",
  "x": "Moving from wordlevel to character-level neural translation makes it even harder to cope with long sentences presenting additional reason to employ the sliding-window translation approach. Table 2 illustrates the character-level neural translation from English to Latvian using modified 2 TensorFlow (Abadi et al., 2015) seq2seq (Sutskerev,<cite> Vinyals & Le, 2014)</cite> neural translation model. The character-level neural translation is enabled by forcing tokenizer to treat each input symbol as a separate \"word\" leading to the small and fixed \"vocabulary\" containing only 90 most frequently encountered characters.",
  "y": "uses"
 },
 {
  "id": "cc3d38692097020ee7f4f17cf9247d_0",
  "x": "These approaches were hindered by drawbacks such as limited feature space and excessive feature engineering. Kernel methods (Cortes and Vapnik, 1995; Cristianini and Shawe-Taylor, 2000) on the other hand can explore a much larger feature space very efficiently. Recent studies on relation extraction have shown that by combining kernels with Support-vector Machines (SVM), one can obtain results superior to feature-based methods (Bunescu and Mooney, 2005b;<cite> Bunescu and Mooney, 2005a</cite>; Culotta and Sorensen, 2004; Cumby and Roth, 2003; Zelenko et al., 2003; Zhang et al., 2006a; Zhang et al., 2006b; Zhao and Grishman, 2005) .",
  "y": "background"
 },
 {
  "id": "cc3d38692097020ee7f4f17cf9247d_1",
  "x": "Recent studies on relation extraction have shown that by combining kernels with Support-vector Machines (SVM), one can obtain results superior to feature-based methods (Bunescu and Mooney, 2005b;<cite> Bunescu and Mooney, 2005a</cite>; Culotta and Sorensen, 2004; Cumby and Roth, 2003; Zelenko et al., 2003; Zhang et al., 2006a; Zhang et al., 2006b; Zhao and Grishman, 2005) . Despite the large number of recently proposed kernels and their reported success, there lacks a clear understanding of their relative strength and weakness. In this study, we provide a systematic comparison and analysis of three such kernels -subsequence kernel (Bunescu and Mooney, 2005b) , dependency tree kernel (Culotta and Sorensen, 2004) and dependency path kernel <cite>(Bunescu and Mooney, 2005a)</cite> .",
  "y": "motivation background"
 },
 {
  "id": "cc3d38692097020ee7f4f17cf9247d_2",
  "x": "Recent studies on relation extraction have shown that by combining kernels with Support-vector Machines (SVM), one can obtain results superior to feature-based methods (Bunescu and Mooney, 2005b;<cite> Bunescu and Mooney, 2005a</cite>; Culotta and Sorensen, 2004; Cumby and Roth, 2003; Zelenko et al., 2003; Zhang et al., 2006a; Zhang et al., 2006b; Zhao and Grishman, 2005) . Despite the large number of recently proposed kernels and their reported success, there lacks a clear understanding of their relative strength and weakness. In this study, we provide a systematic comparison and analysis of three such kernels -subsequence kernel (Bunescu and Mooney, 2005b) , dependency tree kernel (Culotta and Sorensen, 2004) and dependency path kernel <cite>(Bunescu and Mooney, 2005a)</cite> .",
  "y": "uses"
 },
 {
  "id": "cc3d38692097020ee7f4f17cf9247d_3",
  "x": "In a later work also done by<cite> Bunescu & Mooney (2005a)</cite> , they proposed a kernel that computes similarities between nodes on the shortest dependency paths that connect the entities. Their kernel assigns no-match to paths that are of different length. And for paths that are of the same length, it simply computes the product of the similarity score of node pairs at each index.",
  "y": "background"
 },
 {
  "id": "cc3d38692097020ee7f4f17cf9247d_4",
  "x": "In this Section we first give a very brief introduction to kernel methods. We then present the algorithms behind three kernels that we are particularly interested in: subsequence kernel (Bunescu and Mooney, 2005b) , dependency tree kernel (Culotta and Sorensen, 2004) and shortest path dependency kernel <cite>(Bunescu and Mooney, 2005a)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "cc3d38692097020ee7f4f17cf9247d_5",
  "x": "---------------------------------- **SHORTEST PATH DEPENDENCY KERNEL** The shortest path dependency kernel proposed by<cite> Bunescu & Mooney (2005a)</cite> also works with dependency parse trees.",
  "y": "background"
 },
 {
  "id": "cd7bb4543828f915bc930841bb8d7c_0",
  "x": "One exception is<cite> (Goyal et al., 2016)</cite> , who employed a char-based seq2seq model where the input MR is simply represented as a character sequence, and the output is also generated char-by-char; this approach avoids the rare word problem, as the character vocabulary is very small. While<cite> (Goyal et al., 2016)</cite> used an additional finite-state mechanism to guide the production of well-formed (and input-motivated) character sequences, the performance of their basic char2char model was already quite good. We further explore how a recent out-of-the box seq2seq model would perform on E2E NLG Challenge, when used in a char-based mode.",
  "y": "background"
 },
 {
  "id": "cd7bb4543828f915bc930841bb8d7c_1",
  "x": "One exception is<cite> (Goyal et al., 2016)</cite> , who employed a char-based seq2seq model where the input MR is simply represented as a character sequence, and the output is also generated char-by-char; this approach avoids the rare word problem, as the character vocabulary is very small. While<cite> (Goyal et al., 2016)</cite> used an additional finite-state mechanism to guide the production of well-formed (and input-motivated) character sequences, the performance of their basic char2char model was already quite good. We further explore how a recent out-of-the box seq2seq model would perform on E2E NLG Challenge, when used in a char-based mode.",
  "y": "background"
 },
 {
  "id": "cd7bb4543828f915bc930841bb8d7c_2",
  "x": "One exception is<cite> (Goyal et al., 2016)</cite> , who employed a char-based seq2seq model where the input MR is simply represented as a character sequence, and the output is also generated char-by-char; this approach avoids the rare word problem, as the character vocabulary is very small. While<cite> (Goyal et al., 2016)</cite> used an additional finite-state mechanism to guide the production of well-formed (and input-motivated) character sequences, the performance of their basic char2char model was already quite good. We further explore how a recent out-of-the box seq2seq model would perform on E2E NLG Challenge, when used in a char-based mode.",
  "y": "extends background"
 },
 {
  "id": "cd7bb4543828f915bc930841bb8d7c_3",
  "x": "On the linguistic side, vast majority of the predictions were surprisingly grammatically perfect, while still being rather diverse and natural. In particular, and contrary to the findings of <cite>(Goyal et al., 2016</cite> ) (on a different dataset), our char-based model never produced non-words. On the adequacy side, we found that the only serious problem was the tendency (in about half of the evaluated cases) of the model to omit to render one (rarely two) slot(s); on the other end, it never hallucinated, and very rarely duplicated, material.",
  "y": "differences"
 },
 {
  "id": "cd7bb4543828f915bc930841bb8d7c_4",
  "x": "1. Among the utterances produced by the model in first position (Pred), the most prominent issue was that of omissions (underlined in example 2). There were no additions or non-words (which was one of the primary concerns for<cite> (Goyal et al., 2016)</cite> ). We observed only a couple of repetitions which were actually accompanied by omission of some slot(s) in the same utterance (repetition highlighted in bold in example 3).",
  "y": "background"
 },
 {
  "id": "ce86cf36ee3b359c34b68e5d82b563_0",
  "x": "1. Choose sets of words in the daughter languages that appear to be cognate; Frantz (1970) , Hewson (1974) , Wimbish (1989) , and Lowe and Mazandon (1994), but none of them have tackled the alignment step. <cite>Covington (1996)</cite> presents a workable alignment algorithm for comparing two languages. In this paper I extend that algorithm to handle more than two languages at once.",
  "y": "extends background"
 },
 {
  "id": "ce86cf36ee3b359c34b68e5d82b563_1",
  "x": "Of these, the second is etymologically correct, and the third would merit consideration if one did not know the etymology. The number of alignments rises exponentially with the length of the strings and the number of strings being aligned. Two ten-letter strings have anywhere from 26,797 to 8,079,453 different alignments depending on exactly what alignments are considered distinct<cite> (Covington 1996, Covington and</cite> Canfield 1996) .",
  "y": "background"
 },
 {
  "id": "ce86cf36ee3b359c34b68e5d82b563_2",
  "x": "Parts of the Comparative Method have been computerized by Frantz (1970) , Hewson (1974) , Wimbish (1989) , and Lowe and Mazandon (1994) , but none of them have tackled the alignment step. <cite>Covington (1996)</cite> presents a workable alignment algorithm for comparing two languages. In this paper I extend that algorithm to handle more than two languages at once.",
  "y": "extends background"
 },
 {
  "id": "ce86cf36ee3b359c34b68e5d82b563_3",
  "x": "The number of alignments rises exponentially with the length of the strings and the number of strings being aligned. Two ten-letter strings have anywhere from 26,797 to 8,079,453 different alignments depending on exactly what alignments are considered distinct<cite> (Covington 1996, Covington and</cite> Canfield 1996) . As for multiple strings, if two strings have A alignments then n strings have roughly A '~-1 alignments, assuming the alignments are generated by aligning the first two strings, then aligning the third string against the second, and so forth.",
  "y": "background"
 },
 {
  "id": "ce86cf36ee3b359c34b68e5d82b563_4",
  "x": "---------------------------------- **APPLYING AN EVALUATION METRIC** The phonetic similarity criterion used by <cite>Covington (1996)</cite> is shown in Table 1 .",
  "y": "background"
 },
 {
  "id": "ce86cf36ee3b359c34b68e5d82b563_5",
  "x": "This becomes even more apparent when more than three strings are being aligned. Accordingly, when computing badness I count each skip only once (assessing it 50 points), then ignore skips when comparing the segments against each other. I have not implemented the rule from <cite>Covington (1996)</cite> that gives a reduced penalty for adjacent skips in the same string to reflect the fact that affixes tend to be contiguous. 5 Searching the set of alignments The standard way to find the best alignment of two strings is a matrix-based technique known as dynamic programming (Ukkonen 1985 , Waterman 1995 .",
  "y": "differences motivation"
 },
 {
  "id": "ce86cf36ee3b359c34b68e5d82b563_6",
  "x": "Additionally, generalization of dynamic programming to multiple strings does not entirely appear to be a solved problem (cf. Kececioglu 1993). Accordingly, I follow <cite>Covington (1996)</cite> in recasting the problem as a tree search.",
  "y": "uses"
 },
 {
  "id": "ce86cf36ee3b359c34b68e5d82b563_7",
  "x": "Accordingly, I follow <cite>Covington (1996)</cite> in recasting the problem as a tree search. Consider the problem of aligning [el] with [le] . <cite>Covington (1996)</cite> treats this as a process that steps through both strings and, at each step, performs either a \"match\" (accepting a character from both strings), a \"skip-l\" (skipping a character in the first string), or a \"skip-2\" (skipping a character in the second string).",
  "y": "background"
 },
 {
  "id": "ce86cf36ee3b359c34b68e5d82b563_9",
  "x": "Following <cite>Covington (1996)</cite> , I implemented a very simple pruning strategy. The program keeps track of the badness of the best complete alignment found so far. Every branch in the search tree is abandoned as soon as its total badness exceeds that value.",
  "y": "uses"
 },
 {
  "id": "ce990a3d035b8e57fe86b8d84ca479_0",
  "x": "The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (Blei et al., 2010) , <cite>hierarchical Pitman-Yor process</cite> (<cite>Teh, 2006</cite>) , Indian buffet process (Ghahramani and Griffiths, 2005) , recurrent neural network (Mikolov et al., 2010; Van Den Oord et al., 2016) , long short-term memory (Hochreiter and Schmidhuber, 1997; , sequence-to-sequence model (Sutskever et al., 2014), variational auto-encoder (Kingma and Welling, 2014) , generative adversarial network (Goodfellow et al., 2014) , attention mechanism (Chorowski et al., 2015; Seo et al., 2016) , memory-augmented neural network (Graves et al., 2014; Graves et al., 2014) , stochastic neural network Miao et al., 2016) , predictive state neural network (Downey et al., 2017) , policy gradient (Yu et al., 2017) and reinforcement learning (Mnih et al., 2015) . We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language.",
  "y": "uses background"
 },
 {
  "id": "cf2cc67035107f5bdaab85a760e56e_0",
  "x": "Consequently, it is unknown if Indonesian word embeddings introduced in, e.g., (Al-Rfou et al., 2013) and (Grave et al., 2018) , capture syntactic or semantic information as measured by analogy tasks. Also, such embeddings are usually trained on Indonesian Wikipedia (Al-Rfou et al., 2013; <cite>Bojanowski et al., 2017</cite>) whose size is relatively small, approximately 60M tokens. Therefore, in this work, we introduce KaWAT (Kata Word Analogy Task), an Indonesian word analogy task dataset, and new Indonesian word embeddings pretrained on 160M tokens of online news corpus.",
  "y": "background motivation"
 },
 {
  "id": "cf2cc67035107f5bdaab85a760e56e_1",
  "x": "Despite the existence of various Indonesian pretrained word embeddings, there are no publicly available Indonesian analogy task datasets on which to evaluate these embeddings. Also, such embeddings are usually trained on Indonesian Wikipedia (Al-Rfou et al., 2013; <cite>Bojanowski et al., 2017</cite>) whose size is relatively small, approximately 60M tokens. Therefore, in this work, we introduce KaWAT (Kata Word Analogy Task), an Indonesian word analogy task dataset, and new Indonesian word embeddings pretrained on 160M tokens of online news corpus.",
  "y": "motivation"
 },
 {
  "id": "cf2cc67035107f5bdaab85a760e56e_2",
  "x": "Also, such embeddings are usually trained on Indonesian Wikipedia (Al-Rfou et al., 2013; <cite>Bojanowski et al., 2017</cite>) whose size is relatively small, approximately 60M tokens. Therefore, in this work, we introduce KaWAT (Kata Word Analogy Task), an Indonesian word analogy task dataset, and new Indonesian word embeddings pretrained on 160M tokens of online news corpus. We evaluated these embeddings on KaWAT, and also tested them on POS tagging and text summarization as representatives of syntactic and semantic downstream task respectively.",
  "y": "motivation"
 },
 {
  "id": "cf2cc67035107f5bdaab85a760e56e_3",
  "x": "1 One of the goals of this work is to evaluate and compare existing Indonesian pretrained word embeddings. We used <cite>fastText</cite> pretrained embeddings introduced in (<cite>Bojanowski et al., 2017</cite> ) and (Grave et al., 2018) , which have been trained on Indonesian Wikipedia and Indonesian Wikipedia plus Common Crawl data respectively. We refer to them as Wiki/fastText and CC/fastText hereinafter.",
  "y": "uses"
 },
 {
  "id": "cf2cc67035107f5bdaab85a760e56e_4",
  "x": "To train the word embeddings, we experimented with three algorithms: word2vec (Mikolov et al., 2013b) , <cite>fastText</cite> (<cite>Bojanowski et al., 2017</cite>) , and GloVe (Pennington et al., 2014) . We used gensim 3 to run word2vec and <cite>fastText</cite> and the original C implementation for GloVe.",
  "y": "uses"
 },
 {
  "id": "cf2cc67035107f5bdaab85a760e56e_5",
  "x": "To train the word embeddings, we experimented with three algorithms: word2vec (Mikolov et al., 2013b) , <cite>fastText</cite> (<cite>Bojanowski et al., 2017</cite>) , and GloVe (Pennington et al., 2014) . We used gensim 3 to run word2vec and <cite>fastText</cite> and the original C implementation for GloVe. 4 For all three, we used <cite>their</cite> default hyperparameters, i.e. no tuning was performed.",
  "y": "uses"
 },
 {
  "id": "cf2cc67035107f5bdaab85a760e56e_6",
  "x": [
   "---------------------------------- **CONCLUSION** We introduced KaWAT, a new dataset for Indonesian word analogy task, and evaluated several Indonesian pretrained word embeddings on it."
  ],
  "y": "uses"
 },
 {
  "id": "d06a49ad232f73328874282d91cde0_0",
  "x": "To maximally recover the linguistic knowledge from an unsuccessful parse, a proper selection model must be used. Also, the efficiency challenges usually presented by the selection model must be answered. Building on the work reported in <cite>Zhang et al. (2007a)</cite> , we further propose a new partial parsing model that splits the parsing process into two stages, both of which use the bottom-up chart-based parsing algorithm.",
  "y": "extends"
 },
 {
  "id": "d06a49ad232f73328874282d91cde0_1",
  "x": "Also, the cost of manually extending the grammar would be too high to be easily acceptable for other precision grammar-based parsing systems. In <cite>(Zhang et al., 2007a)</cite> , we have pointed out that most applications are only interested in certain aspects of parsing results. Full analyses are preferable, but not always necessary.",
  "y": "background"
 },
 {
  "id": "d06a49ad232f73328874282d91cde0_2",
  "x": "However, it is still unlikely for the specific precision large-scale grammar to achieve full coverage on unseen data without extra robust processing techniques. Also, the cost of manually extending the grammar would be too high to be easily acceptable for other precision grammar-based parsing systems. In <cite>(Zhang et al., 2007a)</cite> , we have pointed out that most applications are only interested in certain aspects of parsing results.",
  "y": "background"
 },
 {
  "id": "d06a49ad232f73328874282d91cde0_3",
  "x": "Take the DELPH-IN HPSG grammars, for instance: Minimal Recursion Semantics (MRS, Copestake et al. (2005) ) is used as the semantic representation in these grammars. For recording syntactic structures, derivation trees are usually used. Based on this fact, <cite>(Zhang et al., 2007a)</cite> have proposed to use partial parsing models to recover the most useful fragment analyses from the intermediate parsing results in cases of unsuccessful parses.",
  "y": "background"
 },
 {
  "id": "d06a49ad232f73328874282d91cde0_4",
  "x": "To this effect, two statistical partial parse selection models are formulated, implemented, and evaluated. Along the lines of the analysis presented in <cite>(Zhang et al., 2007a)</cite> , in this paper we propose a more elaborated par-tial parsing model, in order to further simplify the training procedure, so that full parse disambiguation models can be reused in partial parsing. Moreover, this new model enables us to obtain complete derivation trees, instead of a set of subtrees.",
  "y": "extends"
 },
 {
  "id": "d06a49ad232f73328874282d91cde0_5",
  "x": "The rest of the paper is structured as follows. Section 2. provides background knowledge about the DELPH-IN HPSG grammars, the semantic and syntactic representations, and the partial parsing model presented in Kasper et al. (1999) and <cite>Zhang et al. (2007a)</cite> . Section 3. presents the new proposed two-stage robust parsing model.",
  "y": "background"
 },
 {
  "id": "d06a49ad232f73328874282d91cde0_6",
  "x": "However, it is not clear (apart from some simple heuristics) how the estimation function can be acquired. Moreover, by its additive nature, the shortest-path, such a model makes an implicit independence assumption of the estimation function in different edge contexts. Based on a similar definition of partial parse, <cite>Zhang et al. (2007a)</cite> formulated the following statistical model:",
  "y": "background"
 },
 {
  "id": "d06a49ad232f73328874282d91cde0_7",
  "x": "Moreover, by its additive nature, the shortest-path, such a model makes an implicit independence assumption of the estimation function in different edge contexts. Based on a similar definition of partial parse, <cite>Zhang et al. (2007a)</cite> formulated the following statistical model: <cite>The above model</cite> contains two probabilistic components: i) P (\u2126|w) is the conditional probability of a segmentation \u2126 given the input sequence w; and ii) P (t i |w i ) is the conditional probability of an analysis t i for a given subsequence w i in the segmentation.",
  "y": "background"
 },
 {
  "id": "d06a49ad232f73328874282d91cde0_8",
  "x": "The evaluation was done using multiple metrics. While there is no gold-standard corpus for the purpose of partial parse evaluation, <cite>Zhang et al. (2007a)</cite> manually compared the parser's partial derivation trees with the Penn Treebank annotation for syntactic similarity. Furthermore, <cite>Zhang et al. (2007a)</cite> evaluated the fragment semantic outputs based on a practical estimation of RMRS similarities described by Dridan and Bond (2006) .",
  "y": "background"
 },
 {
  "id": "d06a49ad232f73328874282d91cde0_9",
  "x": "While there is no gold-standard corpus for the purpose of partial parse evaluation, <cite>Zhang et al. (2007a)</cite> manually compared the parser's partial derivation trees with the Penn Treebank annotation for syntactic similarity. Furthermore, <cite>Zhang et al. (2007a)</cite> evaluated the fragment semantic outputs based on a practical estimation of RMRS similarities described by Dridan and Bond (2006) . The semantic outputs of different partial parse selection models were compared to the RMRS outputs from the RASP system (Briscoe et al., 2006) .",
  "y": "background"
 },
 {
  "id": "d06a49ad232f73328874282d91cde0_10",
  "x": "---------------------------------- **A TWO-STAGE ROBUST PARSING MODEL** One common shortcoming of the partial parsing models proposed in both (Kasper et al., 1999) and <cite>(Zhang et al., 2007a)</cite> is that the results of partial parsing are sets of disjoint sub-analyses, either in the form of derivation subtrees, or in the form of MRS fragments.",
  "y": "background"
 },
 {
  "id": "d06a49ad232f73328874282d91cde0_11",
  "x": "As <cite>Zhang et al. (2007a)</cite> have also pointed out, the evaluation of a partial parser is a very difficult task as such, due to the lack of gold-standard annotation for sentences that are not fully analysed by the grammar. For the purpose of evaluation, <cite>Zhang et al. (2007a)</cite> compared the partial derivation tree to the Penn Treebank bracketing, and partial RMRS fragments to the RASP RMRS outputs. Although the results have shown that the proposed partial parsing model performs comparatively better than the baseline model, it is not convincing in relation i) to how informative it is to compare HPSG derivations with Penn Treebank bracketings; and ii) to whether RASP RMRS output should be considered for evaluation comparison in the first place at all.",
  "y": "background"
 },
 {
  "id": "d06a49ad232f73328874282d91cde0_12",
  "x": "As <cite>Zhang et al. (2007a)</cite> have also pointed out, the evaluation of a partial parser is a very difficult task as such, due to the lack of gold-standard annotation for sentences that are not fully analysed by the grammar. For the purpose of evaluation, <cite>Zhang et al. (2007a)</cite> compared the partial derivation tree to the Penn Treebank bracketing, and partial RMRS fragments to the RASP RMRS outputs. Although the results have shown that the proposed partial parsing model performs comparatively better than the baseline model, it is not convincing in relation i) to how informative it is to compare HPSG derivations with Penn Treebank bracketings; and ii) to whether RASP RMRS output should be considered for evaluation comparison in the first place at all.",
  "y": "background"
 },
 {
  "id": "d0c12613f09b36e071b9a842a4d844_0",
  "x": "The alignment of syntactic trees is the task of aligning the internal and leaf nodes of two sentences in different languages structured as trees. The output of the alignment can be used, for instance, as knowledge resource for learning translation rules (for rule-based machine translation systems) or models (for statistical machine translation systems). This paper presents some experiments carried out based on two syntactic tree alignment algorithms presented in<cite> [Lavie et al. 2008]</cite> and [Tinsley et al. 2007 ].",
  "y": "background"
 },
 {
  "id": "d0c12613f09b36e071b9a842a4d844_1",
  "x": "After the alignment of leaf nodes, the internal nodes are aligned following various approaches and distinct criteria. For instance, the method presented in <cite>[Lavie et al. 2008</cite> ] assigns a prime number to each pair of aligned leaf nodes in source and target trees based on the lexical alignment. This alignment is propagated to the highest nodes in a way that the ascendant nodes receive the product of their children, and the internal nodes of both trees with the same resultant value are aligned.",
  "y": "background"
 },
 {
  "id": "d0c12613f09b36e071b9a842a4d844_2",
  "x": "This alignment is propagated to the highest nodes in a way that the ascendant nodes receive the product of their children, and the internal nodes of both trees with the same resultant value are aligned. Similarly, in [Tinsley et al. 2007 ] the alignment of internal nodes is accomplished using the alignment probabilities of leaf nodes generated by GIZA++ [Och and Ney 2003] . In this case, the product of the probabilities of lexical alignment (not prime numbers as<cite> [Lavie et al. 2008]</cite> ) is assigned to parent nodes.",
  "y": "background"
 },
 {
  "id": "d0c12613f09b36e071b9a842a4d844_3",
  "x": "For the experiments presented in this paper, the baseline models were implemented based on<cite> [Lavie et al. 2008]</cite> and [Tinsley et al. 2007 ] mainly because they do not require rich resources such as [Marecek et al. 2008] neither use manually created composition rules as [Menezes and Richardson 2001] and [Groves et al. 2004 ]. ---------------------------------- **MODELS FOR ALIGNING PARALLEL SYNTACTIC TREES**",
  "y": "uses"
 },
 {
  "id": "d0c12613f09b36e071b9a842a4d844_4",
  "x": "The alignment produced by the automatic methods can be very useful for Machine Translation (MT). This paper, therefore, proposes the combination of two syntactic tree alignment methods - <cite>[Lavie et al. 2008</cite> ] (a bottom-up approach) and [Tinsley et al. 2007 ] (a topdown approach) -aiming at improving their performance evaluated on Brazilian Portuguese (pt) and English (en) pair of languages. Moreover, some lexical alignment filters are proposed to filter out the misaligned leaf nodes.",
  "y": "extends motivation"
 },
 {
  "id": "d0c12613f09b36e071b9a842a4d844_5",
  "x": "**MODELS FOR ALIGNING PARALLEL SYNTACTIC TREES** 3.1. Model 1 -Based on<cite> [Lavie et al. 2008]</cite> Following an idea similar to that described in<cite> [Lavie et al. 2008]</cite> , our implementation (model 1) assigns prime numbers to each pair of aligned terminal nodes 1 .",
  "y": "uses similarities"
 },
 {
  "id": "d13502d44435988822e59bcf66b635_0",
  "x": "During the TDT program, FSD was applied to news wire documents and solely focused on effectiveness, neglecting efficiency and scalability. The traditional approach to FSD<cite> (Petrovic et al., 2010)</cite> computes the distance of each incoming document 1 e.g. a natural disaster or a scandal 2 TDT by NIST -1998 NIST - -2004 . http://www.itl.nist.gov/ iad/mig/tests/tdt/resources.html (Last Update: 2008) 3 5,700 tweets per second https://about.twitter .com/company (last updated: March 31, 2015) to all previously seen documents and the minimum distance determines the novelty score.",
  "y": "background"
 },
 {
  "id": "d13502d44435988822e59bcf66b635_1",
  "x": "Each tweet was hashed, placing it into buckets that contain other similar tweets, which are subsequently compared. Operation in constant space was ensured by keeping the number of tweets per bucket constant. Because LSH alone performed ineffectively,<cite> Petrovic et al. (2010)</cite> additionally compared each incoming tweet with the k most recent tweets.",
  "y": "background"
 },
 {
  "id": "d13502d44435988822e59bcf66b635_3",
  "x": "It is known for its high effectiveness in the TDT2 and TDT3 competitions (Fiscus, 2001) and widely used as a benchmark for FSD systems<cite> (Petrovic et al., 2010</cite>; Kasiviswanathan et al., 2011; Petrovic 2013; ) . UMass makes use of an inverted index and k-nearest-neighbour clustering, which optimize the system for speed by ensuring a minimal number of comparisons. To maximise efficiency, we set-up UMass to operate in-memory by turning off its default memory mapping to disk.",
  "y": "background"
 },
 {
  "id": "d13502d44435988822e59bcf66b635_4",
  "x": "This ensures fair comparisons, as all algorithms operate in memory. LSH-FSD is a highly-scalable system by<cite> Petrovic et al. (2010)</cite> . It is based on Locality Sensitive Hashing (LSH) and claims to operate in constant time and space while performing on a comparable level of accuracy as UMass.",
  "y": "background"
 },
 {
  "id": "d13502d44435988822e59bcf66b635_5",
  "x": "It is based on Locality Sensitive Hashing (LSH) and claims to operate in constant time and space while performing on a comparable level of accuracy as UMass. We configure their system using the default parameters<cite> (Petrovic et al., 2010)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d13502d44435988822e59bcf66b635_6",
  "x": "In Figure 2 we compare the memory requirements of k-term and LSH-FSD at different points in the stream. Although<cite> Petrovic et al. (2010)</cite> designed their system (LSH-FSD) to operate in constant space, we found that the memory requirement gradually increases with the number of documents processed, as seen in Figure 3 . We hypothesise that this increase results from new terms added to the vocabulary.",
  "y": "differences"
 },
 {
  "id": "d178b55f8d5928867b481ba89e165c_0",
  "x": "There are only a few existing GIR benchmark datasets (e.g., Geo-CLEF [4] ) and they often lack in rich annotations as would be required for the examples above. Recently many datasets have been produced for reading comprehension such as SQuAD<cite> [5]</cite> . However, they do not have a spatial/platial component.",
  "y": "background"
 },
 {
  "id": "d1bff202991116a6a957aa61c05770_0",
  "x": "In order to better understand the components that lead to effective representations, we propose a lightweight version of InferSent<cite> (Conneau et al., 2017)</cite> , called InferLite, that does not use any recurrent layers and operates on a collection of pre-trained word embeddings. We show that a simple instance of our model that makes no use of context, word ordering or position can still obtain competitive performance on the majority of downstream prediction tasks, with most performance gaps being filled by adding local contextual information through temporal convolutions. Our models can be trained in under 1 hour on a single GPU and allows for fast inference of new representations.",
  "y": "extends"
 },
 {
  "id": "d1bff202991116a6a957aa61c05770_1",
  "x": "Much of the motivation behind this work is to mimic the successful use of feature transfer in computer vision. Recently, <cite>Conneau et al. (2017)</cite> showed that a bidirectional LSTM with max pooling trained to perform Natural Language Inference (NLI), called InferSent, outperforms several other encoding functions on a suite of downstream prediction tasks. This method could match or outperform existing models that learns generic embeddings in an unsupervised setting, often requiring several days or weeks to train (Kiros et al., 2015) .",
  "y": "background"
 },
 {
  "id": "d1bff202991116a6a957aa61c05770_2",
  "x": "Our method uses a controller to dynamically weight embeddings for each word followed by max pooling over components to obtain the final sentence representation. Despite its simplicity, our method obtains performances on par with InferSent<cite> (Conneau et al., 2017)</cite> when using Glove representations (Pennington et al., 2014) as the source of pre-trained word vectors. To our surprise, the majority of evaluations can be done competitively without any notion of context, word ordering or position.",
  "y": "similarities"
 },
 {
  "id": "d1bff202991116a6a957aa61c05770_3",
  "x": "<cite>Conneau et al. (2017)</cite> showed that similar or improved performance can be obtained using NLI datasets as a source of supervisory information. The state of the art sentence encoders utilize multi-task learning (Subramanian et al., 2018) by training an encoder to simultaneously do well on a collection of tasks such as NLI, next sentence prediction and translation. The use of gating for selecting word representations has been considered in previous work.",
  "y": "background"
 },
 {
  "id": "d1bff202991116a6a957aa61c05770_4",
  "x": "Our method operates on a collection of pre-trained word representations and is then trained on the concatenation of SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) datasets as in <cite>Conneau et al. (2017)</cite> . Table 1 summarizes the properties of the embeddings we consider. At a high level, our method takes as input a collection of embeddings for each word and learns a gated controller to decide how to weight each representation.",
  "y": "uses"
 },
 {
  "id": "d1bff202991116a6a957aa61c05770_5",
  "x": "The final reduction operation simply applies max pooling across tokens: resulting in a sentence vector s. This resulting vector corresponds to the embedding for which we evaluate all downstream tasks with. For training on NLI, we follow existing work and compute the concatenation of the embeddings of premise and hypothesis sentences along with their componentwise and absolute difference<cite> (Conneau et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "d1bff202991116a6a957aa61c05770_6",
  "x": "We use the SentEval toolkit (Conneau and Kiela, 2018) for evaluating our sentence embeddings. All of our models are trained to optimize performance on the concatenation of SNLI and MultiNLI, using the concatenated development sets for early stopping. We use 4096-dimensional embeddings as in <cite>Conneau et al. (2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "d1dce63d89e8cfc73962413734bf7b_0",
  "x": "Due to their dependence on the distributional hypothesis (Harris, 1954) , that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018 , inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; . This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; : for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation system (Mrk\u0161i\u0107 et al., 2016; Kim et al., 2016b) . Semantic specialization techniques are therefore leveraged to stress a relation of interest such as semantic similarity (Wieting et al., 2015;<cite> Ponti et al., 2018)</cite> or lexical entailment (Nguyen et al., 2017; over other types of semantic association in the word vector space.",
  "y": "background"
 },
 {
  "id": "d1dce63d89e8cfc73962413734bf7b_1",
  "x": "We observe large improvements over purely distributional word vectors for all target languages and in all tasks. Moreover, we show that the proposed specialization transfer method consistently outperforms the direct specialization transfer based on the composition of the crosslingual projection and the post-specialization function<cite> (Ponti et al., 2018)</cite> , with substantial gains across all experimental setups. In order to boost the integration of external lexical knowledge into distributional models beyond English, we will release our code and lists of WordNet-style lexical relations generated by our transfer method for all target languages at: https://github.com/ cambridgeltl/xling-postspec.",
  "y": "differences"
 },
 {
  "id": "d1dce63d89e8cfc73962413734bf7b_2",
  "x": "---------------------------------- **RELATED WORK** Conflating distinct (both paradigmatic and syntagmatic) lexico-semantic relations is a well-known property of distributional word vectors; semantic specialization of such spaces for a particular lexicosemantic relation (e.g., semantic similarity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking<cite> Ponti et al., 2018)</cite> , spoken language understanding (Kim et al., 2016b,a) , text simplification (Glava\u0161 and Vuli\u0107, 2018b;<cite> Ponti et al., 2018)</cite> , and cross-lingual transfer of resources (Vuli\u0107 et al., 2017a) .",
  "y": "background"
 },
 {
  "id": "d1dce63d89e8cfc73962413734bf7b_3",
  "x": "Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical constraints, the latter yield better downstream performance (Mrk\u0161i\u0107 et al., 2016) . Moreover, while the joint models are tightly coupled to a concrete word embedding objective, retrofitting models can be applied on top of any distributional vector space. Post-specialization<cite> Ponti et al., 2018</cite>; Kamath et al., 2019) is a generalization of retrofitting that specializes the entire distributional space: 1) it learns a global specialization function using before-and after-retrofitting vectors of words from lexical constraints as training examples and 2) it applies the global specialization functions to vectors of words unseen in lexical constraints.",
  "y": "background"
 },
 {
  "id": "d1dce63d89e8cfc73962413734bf7b_5",
  "x": "This allows for monolingual application of retrofitting or post-specialization in the target language. Our experiments show that the proposed specialization transfer via lexical relation induction (CLSRI) outperforms the previous state-of-the-art specialization transfer method of<cite> Ponti et al. (2018).</cite> 3 Methodology CLSRI in a Nutshell.",
  "y": "differences"
 },
 {
  "id": "d1dce63d89e8cfc73962413734bf7b_6",
  "x": "AR specializes only the words seen in the cleaned L t constraints. As the final step, we generalize AR's specialization to the entire target vocabulary with a post-specialization model<cite> (Ponti et al., 2018)</cite> that learns the global specialization function from pairs of distributional and ARspecialized vectors of words from L t constraints. A visual summary of our transfer model is presented in Figure 1 .",
  "y": "uses"
 },
 {
  "id": "d1dce63d89e8cfc73962413734bf7b_7",
  "x": "A visual summary of our transfer model is presented in Figure 1 . Our proposed CLSRI specialization conceptually differs from an existing cross-lingual specialization transfer methodology<cite> (Ponti et al., 2018</cite>; Glava\u0161 and Vuli\u0107, 2018b) , in which the global specialization function is learned in the source language L s and then transferred directly to the target language L s via a shared cross-lingual embedding space. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "d1dce63d89e8cfc73962413734bf7b_8",
  "x": "Post-specialization aims to learn a global specialization function G : X t \u2208 R d \u2192 X t \u2208 R d that approximates the perturbation patterns of AR as captured by changes in vectors of seen words from A t and R t . G is learned as a non-linear mapping between pairs (x i , y i ), where x i \u2208 X t is a distributional vectors of some constraint word (from A t or R t ) and y i is its corresponding AR-specialized vector. In line with and <cite>Ponti et al. (2018)</cite>, we implement this function as a deep feed-forward neural network with l hidden layers of size h and a final linear layer with weight W \u2208 R h\u00d7d .",
  "y": "similarities"
 },
 {
  "id": "d1dce63d89e8cfc73962413734bf7b_9",
  "x": "Note that with our proposed specialization approach CLSRI, we execute the retrofitting and postspecialization completely monolingually in the target language L t on the automatically induced constraints in the target language. In contrast, existing work Glava\u0161 and Vuli\u0107, 2018b;<cite> Ponti et al., 2018)</cite> transfers the post-specialization function learned for the source language L s to the target language L t via a cross-lingual vector space. This fundamental design difference is illustrated in Figure 1 and empirically validated in \u00a75.",
  "y": "differences"
 },
 {
  "id": "d1dce63d89e8cfc73962413734bf7b_10",
  "x": "**EXPERIMENTAL SETUP** Lexical Constraints. The assortment of English constraints for specialization is the same as in prior work (Zhang et al., 2014; Ono et al., 2015;<cite> Ponti et al., 2018)</cite> .",
  "y": "similarities"
 },
 {
  "id": "d1dce63d89e8cfc73962413734bf7b_11",
  "x": "Adagrad (Duchi et al., 2011) is employed to optimize the model parameters for 5 epochs, feeding batches of size |B A | = |B R | = 50, again as in prior work. of Skip-Gram with Negative Sampling (SGNS) that builds representations for each word's constituent character n-grams and sums them up to obtain the entire word's representation. 6 https://github.com/facebookresearch/ fastText/tree/master/alignment Owing to the difference in the amount of supervision, the post-specialization model has partially non-overlapping configurations for the baseline model of <cite>Ponti et al. (2018)</cite> and our CLSRI model.",
  "y": "similarities"
 },
 {
  "id": "d1dce63d89e8cfc73962413734bf7b_12",
  "x": "First, we evaluate the original Distributional vectors. X-PS refers to the baseline model of <cite>Ponti et al. (2018)</cite> based on direct cross-lingual post-specialization. CLSRI-AR denotes the variant of our model based on constraint induction in L t after running the initial AR retrofitting without post-specialization; CLSRI-PS refers to our full model with the postspecialization step.",
  "y": "differences"
 },
 {
  "id": "d1dce63d89e8cfc73962413734bf7b_13",
  "x": "We summarize the results for word similarity in Table 1 . The full CLSRI-PS model outperforms both the distributional vectors and the baseline method for cross-lingual specialization<cite> (Ponti et al., 2018)</cite> . In all languages but two (DE and RU) even the CLSRI-AR model without post-specialization is superior to both baselines, and the post-specialization step additionally improves the results, supporting the findings from prior work .",
  "y": "differences"
 },
 {
  "id": "d1dce63d89e8cfc73962413734bf7b_14",
  "x": "---------------------------------- **DIALOG STATE TRACKING** A standard language understanding evaluation task used in prior work on semantic specialization<cite> Ponti et al., 2018</cite>, inter alia) is dialog state tracking (DST) (Henderson et al., 2014; .",
  "y": "background"
 },
 {
  "id": "d1dce63d89e8cfc73962413734bf7b_15",
  "x": "First, as already confirmed in prior work<cite> Ponti et al., 2018)</cite> , vectors specialized for semantic similarity are indeed important for DST: we observe improvements with all specialized vectors. The highest gains are observed with the full CSLRI-PS model. This confirms two main intuitions: 1) our proposed specialization transfer via lexical induction in the target language is more robust than 15 Note that the original NBT framework in the English DST task has been recently surpassed by more intricate taskspecific architectures (Zhong et al., 2018; Ren et al., 2018) , but its lightweight design coupled with its strong dependence on input word vectors still makes it a convenient means to evaluate the effects of different specialization methods.",
  "y": "background"
 },
 {
  "id": "d1dce63d89e8cfc73962413734bf7b_17",
  "x": "Results and Analysis. The results are reported in Table 3 . As shown in previous work<cite> Ponti et al., 2018)</cite> , retrofitting (CLSRI-AR) and the cross-lingual post-specialization transfer (X-PS) are substantially better in the LS task than the original distributional space.",
  "y": "background"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_0",
  "x": "We analyze the performance of different sentiment classification models on syntacticallycomplex inputs like A-but-B sentences. The first contribution of this analysis addresses reproducible research: to meaningfully compare different models, their accuracies must be averaged over far more random seeds than what has traditionally been reported. With proper averaging in place, we notice that the distillation model described in <cite>Hu et al. (2016)</cite> , which incorporates explicit logic rules for sentiment classification, is ineffective.",
  "y": "differences motivation"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_1",
  "x": "Neural models that explicitly encode word order (Kim, 2014) , syntax (Socher et al., 2013; Tai et al., 2015) and semantic features (Li et al., 2017) have been proposed with the aim of improving performance on these more complicated sentences. Recently, <cite>Hu et al. (2016)</cite> incorporate logical rules into a neural model and show that these rules increase the model's accuracy on sentences containing contrastive conjunctions, while Peters et al. (2018a) demonstrate increased overall accuracy on sentiment analysis by initializing a model with representations from a language model trained on millions of sentences. In this work, we carry out an in-depth study of the effectiveness of the techniques in <cite>Hu et al. (2016)</cite> and Peters et al. (2018a) for sentiment classification of complex sentences.",
  "y": "background"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_2",
  "x": "In this work, we carry out an in-depth study of the effectiveness of the techniques in <cite>Hu et al. (2016)</cite> and Peters et al. (2018a) for sentiment classification of complex sentences. Part of our contribution is to identify an important gap in the methodology used in <cite>Hu et al. (2016)</cite> for performance measurement, which is addressed by averaging the experiments over several executions. With the averaging in place, we obtain three key findings: (1) the improvements in <cite>Hu et al. (2016)</cite> can almost entirely be attributed to just one of their two proposed mechanisms and are also less pronounced than previously reported; (2) contextualized word embeddings (Peters et al., 2018a) incorporate the \"A-but-B\" rules more effectively without explicitly programming for them; and (3) an analysis using crowdsourcing reveals a bigger picture where the errors in the automated systems have a striking correlation with the inherent sentiment-ambiguity in the data.",
  "y": "uses"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_3",
  "x": "Recently, <cite>Hu et al. (2016)</cite> incorporate logical rules into a neural model and show that these rules increase the model's accuracy on sentences containing contrastive conjunctions, while Peters et al. (2018a) demonstrate increased overall accuracy on sentiment analysis by initializing a model with representations from a language model trained on millions of sentences. In this work, we carry out an in-depth study of the effectiveness of the techniques in <cite>Hu et al. (2016)</cite> and Peters et al. (2018a) for sentiment classification of complex sentences. Part of our contribution is to identify an important gap in the methodology used in <cite>Hu et al. (2016)</cite> for performance measurement, which is addressed by averaging the experiments over several executions.",
  "y": "extends uses motivation"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_4",
  "x": "---------------------------------- **LOGIC RULES IN SENTIMENT CLASSIFICATION** Here we briefly review background from <cite>Hu et al. (2016)</cite> to provide a foundation for our reanalysis in the next section.",
  "y": "background"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_5",
  "x": "We focus on a logic rule for sentences containing an \"A-but-B\" structure (the only rule for which <cite>Hu et al. (2016)</cite> provide experimental results). Intuitively, the logic rule for such sentences is that the sentiment associated with the whole sentence should be the same as the sentiment associated with phrase \"B\". 1 More formally, let p \u03b8 (y|x) denote the probability assigned to the label y \u2208 {+, \u2212} for an input x by the baseline model using parameters \u03b8.",
  "y": "uses"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_6",
  "x": "The first contribution of this analysis addresses reproducible research: to meaningfully compare different models, their accuracies must be averaged over far more random seeds than what has traditionally been reported. With proper averaging in place, we notice that the distillation model described in <cite>Hu et al. (2016)</cite> , which incorporates explicit logic rules for sentiment classification, is ineffective. In contrast, using contextualized ELMo embeddings (Peters et al., 2018a) instead of logic rules yields significantly better performance.",
  "y": "uses motivation"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_7",
  "x": "With the averaging in place, we obtain three key findings: (1) the improvements in <cite>Hu et al. (2016)</cite> can almost entirely be attributed to just one of their two proposed mechanisms and are also less pronounced than previously reported; (2) contextualized word embeddings (Peters et al., 2018a) incorporate the \"A-but-B\" rules more effectively without explicitly programming for them; and (3) an analysis using crowdsourcing reveals a bigger picture where the errors in the automated systems have a striking correlation with the inherent sentiment-ambiguity in the data. ---------------------------------- **LOGIC RULES IN SENTIMENT CLASSIFICATION**",
  "y": "extends uses"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_8",
  "x": "For the case of A-but-B sentences, r \u03b8 (x, y) = p \u03b8 (y|B) if x has the structure A-but-B (and 1 otherwise). Next, we discuss the two techniques from <cite>Hu et al. (2016)</cite> for incorporating rules into models: projection, which directly alters a trained model, and distillation, which progressively adjusts the loss function during training. Projection.",
  "y": "uses"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_9",
  "x": "Following Hinton et al. (2015) , a \"student\" model p \u03b8 can learn from the \"teacher\" model q \u03b8 , by using a loss function \u03c0H(p \u03b8 , P true ) + (1 \u2212 \u03c0)H(p \u03b8 , q \u03b8 ) during training, where P true denotes the distribution implied by the ground truth, H(\u00b7, \u00b7) denotes the cross-entropy function, and \u03c0 is a hyperparameter. <cite>Hu et al. (2016)</cite> computes q \u03b8 after every gradient update by projecting the current p \u03b8 , as described above. Note that both mechanisms can be combined: After fully training p \u03b8 using the iterative distillation process above, the projection step can be applied one more time to obtain q \u03b8 which is then used as the trained model.",
  "y": "background"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_10",
  "x": "Note that both mechanisms can be combined: After fully training p \u03b8 using the iterative distillation process above, the projection step can be applied one more time to obtain q \u03b8 which is then used as the trained model. Dataset. All of our experiments (as well as those in <cite>Hu et al. (2016)</cite> ) use the SST2 dataset, a binarized subset of the popular Stanford Sentiment Treebank (SST) (Socher et al., 2013) .",
  "y": "uses"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_11",
  "x": "All of our experiments (as well as those in <cite>Hu et al. (2016)</cite> ) use the SST2 dataset, a binarized subset of the popular Stanford Sentiment Treebank (SST) (Socher et al., 2013) . The dataset includes phrase-level labels in addition to sentence-level labels (see Table 1 for detailed statistics); following <cite>Hu et al. (2016)</cite> , we use both types of labels for the comparisons in Section 3.2. In all other experiments, we use only sentencelevel labels, and our baseline model for all experiments is the CNN architecture from Kim (2014) .",
  "y": "uses"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_12",
  "x": "In this section we reanalyze the effectiveness of the techniques of <cite>Hu et al. (2016)</cite> and find that most of the performance gain is due to projection and not knowledge distillation. The discrepancy with the original analysis can be attributed to the relatively small dataset and the resulting variance across random initializations. We start by analyzing the baseline CNN by Kim (2014) to point out the need for an averaged analysis.",
  "y": "extends uses"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_14",
  "x": "---------------------------------- **PERFORMANCE OF <cite>HU ET AL. (2016)</cite>** We carry out an averaged analysis of the publicly available implementation 4 of <cite>Hu et al. (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_15",
  "x": "**PERFORMANCE OF <cite>HU ET AL. (2016)</cite>** We carry out an averaged analysis of the publicly available implementation 4 of <cite>Hu et al. (2016)</cite> . Our analysis reveals that the reported performance of <cite>their two mechanisms</cite> (projection and distillation) is in fact affected by the high variability across random seeds.",
  "y": "uses"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_16",
  "x": "In Figure 2 , the first two columns show the reported accuracies in <cite>Hu et al. (2016)</cite> for models trained with and without distillation (corresponding to using values \u03c0 = 1 and \u03c0 = 0.95 t in the t th epoch, respectively). The two rows show the results for models with and without a final projection into the rule-regularized space. We keep our hyper-parameters identical to <cite>Hu et al. (2016)</cite> .",
  "y": "uses"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_17",
  "x": "We keep our hyper-parameters identical to <cite>Hu et al. (2016)</cite> . 5 The baseline system (no-project, no-distill) is identical to the system of Kim (2014) . All the systems are trained on the phrase-level SST2 dataset 3 We use early stopping based on validation performance for all models in the density plot.",
  "y": "uses"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_18",
  "x": "We observe no significant gains by projecting a trained ELMo model into an A-but-B rule-regularized space, unlike the other models. We confirm that ELMo's predictions are much closer to the A-but-B rule's manifold than those of the other models by computing KL(q \u03b8 ||p \u03b8 ) where p \u03b8 and q \u03b8 are the original and projected distributions: Averaged across all A-but-B sentences and 100 seeds, this gives 0.27, 0.26 and 0.13 for the Kim (2014) , <cite>Hu et al. (2016)</cite> with distillation and ELMo systems respectively. Intra-sentence Similarity: To understand the information captured by ELMo embeddings for A-but-B sentences, we measure the cosine similarity between the word vectors of every pair of words within the A-but-B sentence (Peters et al., 2018b) .",
  "y": "uses"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_19",
  "x": "**CROWDSOURCED EXPERIMENTS** We conduct a crowdsourced analysis that reveals that SST2 data has significant levels of ambiguity even for human labelers. We discover that ELMo's performance improvements over the baseline are robust across varying levels of ambiguity, whereas the advantage of <cite>Hu et al. (2016)</cite> is reversed in sentences of low ambiguity (restricting to A-but-B style sentences).",
  "y": "extends uses differences"
 },
 {
  "id": "d3122aab8960a7c89afe87c73faa59_20",
  "x": "In Figure 4 we compare all the models on the A-but-B sentences in this set. Across all thresholds, we notice trends similar to previous sections: 1) ELMo performs the best among all models on A-but-B style sentences, and projection results in only a slight improvement; 2) models in <cite>Hu et al. (2016)</cite> (with and without distillation) benefit considerably from projection; but 3) distillation offers little improvement (with or without projection). Also, as the ambiguity threshold increases, we see decreasing gains from projection on all models.",
  "y": "extends differences"
 },
 {
  "id": "d5144370a9361ff870dd3cb2e064ff_0",
  "x": "Recent work examines the extent to which RNN-based models capture syntax-sensitive phenomena that are traditionally taken as evidence for the existence in hierarchical structure. In particular, in <cite>(Linzen et al., 2016)</cite> we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences. (Gulordava et al., 2018 ) also consider subject-verb agreement, but in a \"colorless green ideas\" setting in which content words in naturally occurring sentences are replaced with random words with the same partof-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues.",
  "y": "background"
 },
 {
  "id": "d5144370a9361ff870dd3cb2e064ff_1",
  "x": "The BERT model is based on the \"Transformer\" architecture (Vaswani et al., 2017) , which-in contrast to RNNs-relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute-position embedding. This reliance on attention may lead one 1 to expect decreased performance on syntax-sensitive tasks compared to RNN (LSTM) models that do model word order directly, and explicitly track states across the sentence. Indeed, Tran et al. (2018) finds that transformerbased models perform worse than LSTM models on the <cite>Linzen et al. (2016)</cite> agreement prediction dataset.",
  "y": "differences"
 },
 {
  "id": "d5144370a9361ff870dd3cb2e064ff_2",
  "x": "Indeed, Tran et al. (2018) finds that transformerbased models perform worse than LSTM models on the <cite>Linzen et al. (2016)</cite> agreement prediction dataset. In contrast, (Tang et al., 2018) find that self-attention performs on par with LSTM for syntax sensitive dependencies in the context of machine-translation, and performance on syntactic tasks is correlated with the number of attention heads in multi-head attention. I adapt the evaluation protocol and stimuli of <cite>Linzen et al. (2016)</cite> , Gulordava et al. (2018) and Marvin and Linzen (2018) to the bidirectional setting required by BERT, and evaluate the pretrained BERT models (both the LARGE and the BASE models).",
  "y": "uses"
 },
 {
  "id": "d5144370a9361ff870dd3cb2e064ff_3",
  "x": "**METHODOLOGY** I use the stimuli provided by (<cite>Linzen et al., 2016</cite>; Gulordava et al., 2018; Marvin and Linzen, 2018) , but change the experimental protocol to adapt it to the bidirectional nature of the BERT model. This requires discarding some of the stimuli, as described below.",
  "y": "extends"
 },
 {
  "id": "d5144370a9361ff870dd3cb2e064ff_4",
  "x": "All three previous work use uni-directional language-model-like models. <cite>Linzen et al. (2016)</cite> start with existing sentences from wikipedia that contain a present-tense verb. <cite>They</cite> feed each sentence word by word into an LSTM, stop right before the focus verb, and ask the model to predict a binary plural/singular decision (supervised setup) or compare the probability assigned by a pre-trained language model (LM) to the plural vs singular forms of the verb (LM setup).",
  "y": "background"
 },
 {
  "id": "d5144370a9361ff870dd3cb2e064ff_5",
  "x": "However, in order to control for the possibillity of the model learning to rely on \"semantic\" selectional-preferences cues rather than syntactic ones, they replace each content word with random words from the same part-ofspeech and inflection. This results in \"coloreless green ideas\" nonce sentences. The evaluation is then performed similarly to the LM setup of <cite>Linzen et al. (2016)</cite> : the sentence is fed into a pretraiend LSTM LM up to the focus verb, and the model is considered correct if the probability assigned to the correct inflection of the original verb form given the prefix is larger than that assigned to the incorrect inflection.",
  "y": "background"
 },
 {
  "id": "d5144370a9361ff870dd3cb2e064ff_7",
  "x": "and compare the scores predicted for is and are. This differs from <cite>Linzen et al. (2016)</cite> and Gulordava et al. (2018) by considering the entire sentence (excluding the verb) and not just its prefix leading to the verb, and differs from Marvin and Linzen (2018) by conditioning the focus verb on bidirectional context. I use the PyTorch implementation of BERT, with the pre-trained models supplied by Google.",
  "y": "differences"
 },
 {
  "id": "d5144370a9361ff870dd3cb2e064ff_8",
  "x": "Discarded Material The bi-directional setup precludes using using the NPI stimuli of Marvin and Linzen (2018) , in which the minimal pair differs in two words position, which I discard from the evaluation. I also discard the agreement cases involving the verbs is or are in <cite>Linzen et al. (2016)</cite> and in Gulordava et al. (2018) , because some of them are copular construction, in which strong agreement hints can be found also on the object following the verb. 5 This is not an issue in the manually constructed (Marvin and Linzen, 2018 ) stimuli due to the patterns they chose.",
  "y": "extends"
 },
 {
  "id": "d5144370a9361ff870dd3cb2e064ff_9",
  "x": "Finally, I discard stimuli in which the focus verb or its plural/singular inflection does not appear as a single word in the BERT wordpiece-based vocabulary (and hence cannot be predicted by the model). This include discarding Marvin and Linzen (2018) stimuli involving the words swims or admires, resulting in 23,368 discarded pairs (out of 152,300). I similarly discard 680 sentences from (<cite>Linzen et al., 2016</cite>) where the focus verb or its inflection were one of 108 out-ofvocabulary tokens, 6 and 28 sentence-pairs (8 tokens 7 ) from (Gulordava et al., 2018) .",
  "y": "extends"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_0",
  "x": "With advances in deep learning, neural models have given state-of-the-art results on many sequence labeling tasks (Ling et al., 2015; Lample et al., 2016; <cite>Ma and Hovy, 2016</cite>) . Words and characters are encoded in distributed representations (Mikolov et al., 2013) and sentence-level features are learned automatically during end-to-end training. Many existing state-of-the-art neural sequence labeling models utilize word-level Long Short-Term Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels Lample et al., 2016; <cite>Ma and Hovy, 2016</cite>; Peters et al., 2017) .",
  "y": "background"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_1",
  "x": "Words and characters are encoded in distributed representations (Mikolov et al., 2013) and sentence-level features are learned automatically during end-to-end training. Many existing state-of-the-art neural sequence labeling models utilize word-level Long Short-Term Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels Lample et al., 2016; <cite>Ma and Hovy, 2016</cite>; Peters et al., 2017) . As an alternative, Convolution Neural Network (CNN) (LeCun et al., 1989) has also been used for its ability of parallel computing, leading to an efficient training and decoding process.",
  "y": "background"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_2",
  "x": "For example, Reimers and Gurevych (2017b) conduct a large number of experiments using the code of <cite>Ma and Hovy (2016)</cite> , but cannot obtain comparable results as reported in the paper. Liu et al. (2018) report lower average F-scores on NER when reproducing the structure of Lample et al. (2016) , and on POS tagging when reproducing <cite>Ma and Hovy (2016)</cite> . Most literature compares results with others by citing the scores directly Lample et al., 2016) without re-implementing them under the same setting, resulting in less persuasiveness on the advantage of their models.",
  "y": "background motivation"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_3",
  "x": "Despite them being dominant in the research literature, reproducing published results for neural models can be challenging, even if the codes are available open source. For example, Reimers and Gurevych (2017b) conduct a large number of experiments using the code of <cite>Ma and Hovy (2016)</cite> , but cannot obtain comparable results as reported in the paper. Liu et al. (2018) report lower average F-scores on NER when reproducing the structure of Lample et al. (2016) , and on POS tagging when reproducing <cite>Ma and Hovy (2016)</cite> .",
  "y": "motivation"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_4",
  "x": "Liu et al. (2018) report lower average F-scores on NER when reproducing the structure of Lample et al. (2016) , and on POS tagging when reproducing <cite>Ma and Hovy (2016)</cite> . Most literature compares results with others by citing the scores directly Lample et al., 2016) without re-implementing them under the same setting, resulting in less persuasiveness on the advantage of their models. In addition, conclusions from different reports can be contradictory.",
  "y": "background motivation"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_5",
  "x": "Despite them being dominant in the research literature, reproducing published results for neural models can be challenging, even if the codes are available open source. Liu et al. (2018) report lower average F-scores on NER when reproducing the structure of Lample et al. (2016) , and on POS tagging when reproducing <cite>Ma and Hovy (2016)</cite> .",
  "y": "motivation"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_6",
  "x": "In addition, conclusions from different reports can be contradictory. For example, most work observes that stochastic gradient descent (SGD) gives best performance on NER task (Chiu and Nichols, 2016; Lample et al., 2016; <cite>Ma and Hovy, 2016</cite>) , while Reimers and Gurevych (2017b) report that SGD is the worst optimizer on the same datasets. The comparison between different deep neural models is challenging due to sensitivity on experimental settings.",
  "y": "background"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_7",
  "x": "Most literature compares results with others by citing the scores directly Lample et al., 2016) without re-implementing them under the same setting, resulting in less persuasiveness on the advantage of their models. In addition, conclusions from different reports can be contradictory. For example, most work observes that stochastic gradient descent (SGD) gives best performance on NER task (Chiu and Nichols, 2016; Lample et al., 2016; <cite>Ma and Hovy, 2016</cite>) , while Reimers and Gurevych (2017b) report that SGD is the worst optimizer on the same datasets.",
  "y": "motivation"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_8",
  "x": "We list six inconsistent configurations in literature, which lead to difficulties for fair comparison. \u2022 Dataset. Most work reports sequence labeling results on both CoNLL 2003 English NER (Tjong Kim Sang and De Meulder, 2003) and PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011; <cite>Ma and Hovy, 2016</cite>) .",
  "y": "background"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_9",
  "x": "We list six inconsistent configurations in literature, which lead to difficulties for fair comparison. \u2022 Dataset. Most work reports sequence labeling results on both CoNLL 2003 English NER (Tjong Kim Sang and De Meulder, 2003) and PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011; <cite>Ma and Hovy, 2016</cite>) . Ling et al. (2015) give results only on POS dataset, while some papers (Chiu and Nichols, 2016; Lample et al., 2016; Strubell et al., 2017) report results on the NER dataset only.",
  "y": "motivation"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_10",
  "x": "Most work uses the development set to select hyperparameters (Lample et al., 2016; <cite>Ma and Hovy, 2016</cite>) , while others add development set into training set (Chiu and Nichols, 2016; Peters et al., 2017) . Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences). Different from <cite>Ma and Hovy (2016)</cite> and Liu et al. (2018) , choose a different data split on the POS dataset.",
  "y": "background"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_11",
  "x": "We list six inconsistent configurations in literature, which lead to difficulties for fair comparison. Most work uses the development set to select hyperparameters (Lample et al., 2016; <cite>Ma and Hovy, 2016</cite>) , while others add development set into training set (Chiu and Nichols, 2016; Peters et al., 2017) . Different from <cite>Ma and Hovy (2016)</cite> and Liu et al. (2018) , choose a different data split on the POS dataset.",
  "y": "motivation"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_12",
  "x": "Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences). Different from <cite>Ma and Hovy (2016)</cite> and Liu et al. (2018) , choose a different data split on the POS dataset. Liu et al. (2018) and Hashimoto et al. (2017) use different development sets for chunking.",
  "y": "background"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_13",
  "x": "<cite>Ma and Hovy (2016)</cite> do not use preprocessing. \u2022 Features. Strubell et al. (2017) and Chiu and Nichols (2016) apply word spelling features and further integrate context features.",
  "y": "background"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_14",
  "x": "We list six inconsistent configurations in literature, which lead to difficulties for fair comparison. A typical data preprocessing step is to normize digit characters (Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Strubell et al., 2017) . <cite>Ma and Hovy (2016)</cite> do not use preprocessing.",
  "y": "motivation"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_15",
  "x": "Besides, Lample et al. (2016) and <cite>Ma and Hovy (2016)</cite> use end-to-end structure without handcrafted features. \u2022 Hyperparameters including learning rate, dropout rate (Srivastava et al., 2014) , number of layers, hidden size etc. can strongly affect the model performance. Chiu and Nichols (2016) search for the hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters.",
  "y": "background"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_16",
  "x": "We list six inconsistent configurations in literature, which lead to difficulties for fair comparison. Strubell et al. (2017) and Chiu and Nichols (2016) apply word spelling features and further integrate context features. Besides, Lample et al. (2016) and <cite>Ma and Hovy (2016)</cite> use end-to-end structure without handcrafted features.",
  "y": "motivation"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_17",
  "x": "We list six inconsistent configurations in literature, which lead to difficulties for fair comparison. Some literature reports results using mean and standard deviation under different random seeds (Chiu and Nichols, 2016; Peters et al., 2017; Liu et al., 2018) . Others report the best result among different trials (<cite>Ma and Hovy, 2016</cite>) , which cannot be compared directly.",
  "y": "motivation"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_18",
  "x": "Hammerton (2003) was the first to exploit LSTM for sequence labeling. built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (Lample et al., 2016; Liu et al., 2018) , GRU (Yang et al., 2016) , and CNN (Chiu and Nichols, 2016; <cite>Ma and Hovy, 2016</cite>) features. Yang et al. (2017a) proposed a neural reranking model to improve NER models.",
  "y": "background"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_19",
  "x": "2) Their system gives relatively low performances on standard benchmarks 2 , while ours can give comparable or better results with state-of-the-art models, rendering our observations more informative for practitioners. 3) Our findings are more consistent with most previous work on configurations such as usefulness of character information (Lample et al., 2016; <cite>Ma and Hovy, 2016</cite>) , optimizer (Chiu and Nichols, 2016; Lample et al., 2016; <cite>Ma and Hovy, 2016</cite>) and tag scheme (Ratinov and Roth, 2009; Dai et al., 2015) . In contrast, many results of Reimers and Gurevych (2017b) contradict existing reports.",
  "y": "similarities"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_20",
  "x": "Our neural sequence labeling framework contains three layers, i.e., a character sequence representation layer, a word sequence representation layer and an inference layer, as shown in Figure 1 . Character information has been proven to be critical for sequence labeling tasks (Chiu and Nichols, 2016; Lample et al., 2016; <cite>Ma and Hovy, 2016</cite>) , with LSTM and CNN being used to model character sequence information (\"Char Rep.\"). Similarly, on the word level, LSTM or CNN structures can be leveraged to capture long-term information or local features (\"Word Rep.\"), respectively.",
  "y": "background"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_21",
  "x": "**NEURAL SEQUENCE LABELING MODELS** Our neural sequence labeling framework contains three layers, i.e., a character sequence representation layer, a word sequence representation layer and an inference layer, as shown in Figure 1 . Character information has been proven to be critical for sequence labeling tasks (Chiu and Nichols, 2016; Lample et al., 2016; <cite>Ma and Hovy, 2016</cite>) , with LSTM and CNN being used to model character sequence information (\"Char Rep.\").",
  "y": "uses"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_22",
  "x": "Character features such as prefix, suffix and capitalization can be represented with embeddings through a feature-based lookup table (Collobert et al., 2011; Strubell et al., 2017) , or neural networks without human-defined features (Lample et al., 2016; <cite>Ma and Hovy, 2016</cite>) . In this work, we focus on neural character sequence representations without hand-engineered features. Character CNN.",
  "y": "background"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_23",
  "x": "**CHARACTER SEQUENCE REPRESENTATIONS** Character features such as prefix, suffix and capitalization can be represented with embeddings through a feature-based lookup table (Collobert et al., 2011; Strubell et al., 2017) , or neural networks without human-defined features (Lample et al., 2016; <cite>Ma and Hovy, 2016</cite>) . In this work, we focus on neural character sequence representations without hand-engineered features.",
  "y": "uses"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_24",
  "x": "Character CNN. Using a CNN structure to encode character sequences was firstly proposed by Santos and Zadrozny (2014), and followed by many subsequent investigations (dos Santos et al., 2015; Chiu and Nichols, 2016; <cite>Ma and Hovy, 2016</cite>) . In our experiments, we take the same structure as <cite>Ma and Hovy (2016)</cite> , using one layer CNN structure with max-pooling to capture character-level representations.",
  "y": "background"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_25",
  "x": "Character CNN. Using a CNN structure to encode character sequences was firstly proposed by Santos and Zadrozny (2014), and followed by many subsequent investigations (dos Santos et al., 2015; Chiu and Nichols, 2016; <cite>Ma and Hovy, 2016</cite>) . In our experiments, we take the same structure as <cite>Ma and Hovy (2016)</cite> , using one layer CNN structure with max-pooling to capture character-level representations.",
  "y": "uses"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_26",
  "x": "**WORD SEQUENCE REPRESENTATIONS** Similar to character sequences in words, we can model word sequence information through LSTM or CNN structures. LSTM has been widely used in sequence labeling (Lample et al., 2016; <cite>Ma and Hovy, 2016</cite>; Chiu and Nichols, 2016; Liu et al., 2018) .",
  "y": "background"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_27",
  "x": "The NER dataset has been standardly split in Tjong Kim Sang and De Meulder (2003 (Toutanova et al., 2003; Santos and Zadrozny, 2014; <cite>Ma and Hovy, 2016;</cite> Liu et al., 2018) , we adopt the standard splits by using sections 0-18 as training set, sections 19-21 as development set and sections 22-24 as test set. No preprocessing is performed on either dataset except for normalizing digits. The dataset statistics are listed in Table 2 .",
  "y": "uses background"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_28",
  "x": "The dataset statistics are listed in Table 2 . Hyperparameters. Table 3 shows the hyperparameters used in our experiments, which mostly follow <cite>Ma and Hovy (2016)</cite> , including the learning rate \u03b7 = 0.015 for word LSTM models.",
  "y": "similarities uses"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_29",
  "x": "We re-implement the structure of several reports (Chiu and Nichols, 2016; <cite>Ma and Hovy, 2016</cite>; Peters et al., 2017) , which take the CCNN+WLSTM+CRF architecture. Our reproduced models give slightly better performances. The results of Lample et al. (2016) can be reproduced by our CLSTM+WLSTM+CRF.",
  "y": "uses"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_30",
  "x": "Our Nochar+WCNN+CRF can give comparable results with Collobert et al. (2011) , even ours does not include character information. The results of the POS tagging task is shown in Table 6 . The results of Lample et al. (2016) , <cite>Ma and Hovy (2016)</cite> and Yang et al. (2017b) can be reproduced by our CLSTM+WLSTM+CRF and CCNN+WLSTM+CRF models.",
  "y": "similarities uses"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_31",
  "x": "The GloVe 100-dimension embeddings give higher F1-scores than SENNA (Collobert et al., 2011) on both models, which is consistent with the observation of <cite>Ma and Hovy (2016)</cite> . Tag scheme. We examine two different tag schemes: BIO and BIOES (Ratinov and Roth, 2009) .",
  "y": "similarities"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_32",
  "x": "Our observation is consistent with most literature (Chiu and Nichols, 2016; Lample et al., 2016; <cite>Ma and Hovy, 2016</cite>) . ---------------------------------- **ANALYSIS**",
  "y": "similarities"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_33",
  "x": "OOV error. We conduct error analysis on in-vocabulary and out-of-vocabulary words with the CRF based models 6 . Following <cite>Ma and Hovy (2016)</cite> , words in the test set are divided into four subsets: in-vocabulary words, out-of-training-vocabulary words (OOTV), out-of-embedding-vocabulary words (OOEV) and out-of-both-vocabulary words (OOBV).",
  "y": "uses"
 },
 {
  "id": "d52a1a26cbf8a6f528be5494f05e45_34",
  "x": "The OOV entities and chunks are categorized following <cite>Ma and Hovy (2016)</cite> . Table 7 shows the performances of different OOV splits on three benchmarks. The top three rows list the performances of word-based LSTM CRF models, followed by the word-based CNN CRF models.",
  "y": "uses"
 },
 {
  "id": "d576de5c19d7ff62a143b0d4d56135_0",
  "x": "**INTRODUCTION** An important goal in argument mining is to understand the structure in argumentative text (Persing and Ng, 2016; Peldszus and Stede, 2015;<cite> Stab and Gurevych, 2016</cite>; Nguyen and Litman, 2016) . One fundamental assumption when working with argumentative text is the presence of Arguments Components (ACs).",
  "y": "background"
 },
 {
  "id": "d576de5c19d7ff62a143b0d4d56135_1",
  "x": "The types of ACs are generally characterized as a claim or a premise (Govier, 2013) , with premises acting as support (or possibly attack) units for claims (though some corpora have further AC types, such as major claim Gurevych, 2016, 2014b) ). The task of processing argument structure encapsulates four distinct subtasks (our work focuses on subtasks 2 and 3): (1) Given a sequence of tokens that represents an entire argumentative text, determine the token subsequences that constitute non-intersecting ACs; (2) Given an AC, determine the type of AC (claim, premise, etc.); (3) Given a set/list of ACs, determine which ACs have directed links that encapsulate overall argument structure; (4) Given two linked ACs, determine whether the link is a supporting or attacking relation. This can be labeled as a 'micro' approach to argument mining<cite> (Stab and Gurevych, 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "d576de5c19d7ff62a143b0d4d56135_2",
  "x": "First, we assume subtask 1 has been completed, i.e. ACs have already been identified. Second, we follow previous work that assumes a tree structure for the linking of ACs (Palau and Moens, 2009; Cohen, 1987; Peldszus and Stede, 2015;<cite> Stab and Gurevych, 2016)</cite> . Specifically, a given AC can only have a single outgoing link, but can have numerous incoming links.",
  "y": "uses"
 },
 {
  "id": "d576de5c19d7ff62a143b0d4d56135_3",
  "x": "Depending on the corpus (see Section 4), an argument structure can be either a single tree or a forest, consisting of multiple trees. Figure 1 shows an example that we will use throughout the paper to concretely explain how our approach works. First, the left side of the figure presents the raw text of a paragraph in a persuasive essay<cite> (Stab and Gurevych, 2016)</cite> , with the ACs contained in square brackets.",
  "y": "uses"
 },
 {
  "id": "d576de5c19d7ff62a143b0d4d56135_4",
  "x": "Lastly, we note that the order of argument components can be a strong indicator of how components should relate. Linking to the first argument component can provide a competitive baseline heuristic (Peldszus and Stede, 2015;<cite> Stab and Gurevych, 2016)</cite> . Given the above considerations, we propose that sequence-to-sequence attention modeling, in the spirit of a Pointer Network (PN) (Vinyals et al., 2015b) , can be effective for predicting argument structure.",
  "y": "background"
 },
 {
  "id": "d576de5c19d7ff62a143b0d4d56135_5",
  "x": "Linking to the first argument component can provide a competitive baseline heuristic (Peldszus and Stede, 2015;<cite> Stab and Gurevych, 2016)</cite> . Given the above considerations, we propose that sequence-to-sequence attention modeling, in the spirit of a Pointer Network (PN) (Vinyals et al., 2015b) , can be effective for predicting argument structure. To the best of our knowledge, a clean, elegant implementation of a PN-based model has yet to be introduced for discourse parsing tasks.",
  "y": "uses"
 },
 {
  "id": "d576de5c19d7ff62a143b0d4d56135_6",
  "x": "We evaluate our models on the corpora of<cite> Stab and Gurevych (2016)</cite> and Peldszus (2014) , and compare our results with the results of the aformentioned authors. Our results show that (1) joint modeling is imperative for competitive performance on the link extraction task, (2) the presence of the second recurrence improves performance over a non-sequence-to-sequence model, and (3) the joint model can outperform models with heavy featureengineering and corpus-specific constraints. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d576de5c19d7ff62a143b0d4d56135_7",
  "x": "Various authors have also proposed to jointly model link extraction with other subtasks from the argument mining pipeline, using either an Integer Linear Programming (ILP) framework (Persing and Ng, 2016;<cite> Stab and Gurevych, 2016)</cite> or directly feeding previous subtask predictions into a tree-based parser. The former joint approaches are evaluated on an annotated corpus of persuasive essays Gurevych, 2014a, 2016) , and the latter on a corpus of microtexts (Peldszus, 2014) . The ILP framework is effective in enforcing a tree structure between ACs when predictions are made from otherwise naive base classifiers.",
  "y": "background"
 },
 {
  "id": "d576de5c19d7ff62a143b0d4d56135_8",
  "x": "We evaluate our models on the corpora of<cite> Stab and Gurevych (2016)</cite> and Peldszus (2014) , and compare our results with the results of the aformentioned authors. Our results show that (1) joint modeling is imperative for competitive performance on the link extraction task, (2) the presence of the second recurrence improves performance over a non-sequence-to-sequence model, and (3) the joint model can outperform models with heavy featureengineering and corpus-specific constraints. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "d576de5c19d7ff62a143b0d4d56135_9",
  "x": "Each AC is itself Figure 3: Architecture of the joint model applied to the example in Figure 1 . Note that D1 points to itself to denote that it has not outgoing link and is therefore the head of a tree. a sequence of tokens, similar to the QuestionAnswering dataset from Weston et al. (2015) . We follow the work of<cite> Stab and Gurevych (2016)</cite> and focus on three different types of features to represent our ACs: (1) Bag-of-Words of the AC; (2) Embedding representation based on GloVe embeddings (Pennington et al., 2014) , which uses average, max, and min pooling across the token embeddings; (3) Structural features: Whether or not the AC is the first AC in a paragraph, and whether the AC is in an opening, body, or closing paragraph.",
  "y": "uses"
 },
 {
  "id": "d576de5c19d7ff62a143b0d4d56135_10",
  "x": "We test the effectiveness of our proposed model on a dataset of persuasive essays (PEC)<cite> (Stab and Gurevych, 2016)</cite> , as well as a dataset of microtexts (MTC) (Peldszus, 2014) . The feature space for the PEC has roughly 3,000 dimensions, and the MTC feature space has between 2,500 and 3,000 dimensions, depending on the data split. The PEC contains a total of 402 essays, with a frozen set of 80 essays held out for testing.",
  "y": "uses"
 },
 {
  "id": "d576de5c19d7ff62a143b0d4d56135_11",
  "x": "We implement and compare four types of neural models: 1) The previously described joint model from In both corpora we compare against the following previously proposed models: Base Classifier <cite>(Stab and Gurevych, 2016</cite> ) is a feature-rich, taskspecific (AC type or link extraction) SVM classifier. Neither of these classifiers enforce structural or global constraints. Conversely, the ILP Joint Model<cite> (Stab and Gurevych, 2016)</cite> provides constraints by sharing prediction information between the base classifiers.",
  "y": "uses"
 },
 {
  "id": "d576de5c19d7ff62a143b0d4d56135_12",
  "x": "We implement and compare four types of neural models: 1) The previously described joint model from In both corpora we compare against the following previously proposed models: Base Classifier <cite>(Stab and Gurevych, 2016</cite> ) is a feature-rich, taskspecific (AC type or link extraction) SVM classifier. Neither of these classifiers enforce structural or global constraints. Conversely, the ILP Joint Model<cite> (Stab and Gurevych, 2016)</cite> provides constraints by sharing prediction information between the base classifiers.",
  "y": "motivation"
 },
 {
  "id": "d576de5c19d7ff62a143b0d4d56135_13",
  "x": "Neither of these classifiers enforce structural or global constraints. Conversely, the ILP Joint Model<cite> (Stab and Gurevych, 2016)</cite> provides constraints by sharing prediction information between the base classifiers. For example, the model attempts to enforce a tree structure among ACs within a given paragraph, as well as using incoming link predictions to better predict the type class claim.",
  "y": "background"
 },
 {
  "id": "d576de5c19d7ff62a143b0d4d56135_14",
  "x": "The ablation results also provide an interesting insight into the effectiveness of different pooling strategies for using individual token embeddings to create a multi-word embedding. The popular method of averaging embeddings (which is used by<cite> Stab and Gurevych (2016)</cite> in their system) is in fact the worst method, although its performance is still competitive with the previous state-of-the-art. Conversely, max pooling results are on par with the joint model results in Table 1 .",
  "y": "motivation"
 },
 {
  "id": "d576de5c19d7ff62a143b0d4d56135_15",
  "x": "We evaluate our models on two corpora: a corpus of persuasive essays<cite> (Stab and Gurevych, 2016)</cite> , and a corpus of microtexts (Peldszus, 2014) . The Joint Model records state-of-the-art results on the persuasive essay corpus, as well as achieving state-of-the-art results for link prediction on the microtext corpus. The results show that jointly modeling the two prediction tasks is critical for high performance.",
  "y": "uses"
 },
 {
  "id": "d5d81a4c7759f9a4ab81195819c6d9_0",
  "x": "Neural machine translation (briefly, NMT), which is built upon deep neural networks, has gained rapid progress in recent years (Bahdanau et al., 2015; Sutskever et al., 2014; Sennrich et al., 2016b; He et al., 2016a; Sennrich et al., 2016a; Xia et al., 2017; Wang et al., 2019) and achieved significant improvement in translation quality (Hassan et al., 2018) . Variants of network structures have been applied in NMT such as LSTM (Wu et al., 2016) , CNN (Gehring et al., 2017) and Transformer<cite> (Vaswani et al., 2017)</cite> . Training deep networks has always been a challenging problem, mainly due to the difficulties in optimization for deep architecture.",
  "y": "background"
 },
 {
  "id": "d5d81a4c7759f9a4ab81195819c6d9_1",
  "x": "Breakthroughs have been made in computer vision to enable deeper model construction via advanced initialization schemes (He et al., 2015) , multi-stage training strategy (Simonyan and Zisserman, 2015) , and Figure 1 : Performances of Transformer models with different number of encoder/decoder blocks (recorded on x-axis) on WMT14 En\u2192De translation task. \u2020 denotes the result reported in<cite> (Vaswani et al., 2017)</cite> . novel model architectures (Srivastava et al., 2015; He et al., 2016b) .",
  "y": "uses"
 },
 {
  "id": "d5d81a4c7759f9a4ab81195819c6d9_2",
  "x": "While constructing very deep neural networks with tens and even more than a hundred blocks have shown effectiveness in image recognition (He et al., 2016b) , question answering and text classification (Devlin et al., 2018; Radford et al., 2019) , scaling up model capacity with very deep network remains challenging for NMT. The NMT models are generally constructed with up to 6 encoder and decoder blocks in both state-of-the-art research work and champion systems of machine translation competition. For example, the LSTM-based models are usually stacked for 4 (Stahlberg et al., 2018 ) or 6 (Chen et al., 2018 blocks, and the state-of-the-art Transformer models are equipped with a 6-block encoder and decoder<cite> (Vaswani et al., 2017</cite>; JunczysDowmunt, 2018; Edunov et al., 2018) .",
  "y": "background"
 },
 {
  "id": "d5d81a4c7759f9a4ab81195819c6d9_3",
  "x": "We adopt the big transformer configuration following<cite> Vaswani et al. (2017)</cite> , with the dimension of word embeddings, hidden states and non-linear layer set as 1024, 1024 and 4096 respectively. The dropout rate is 0.3 for En\u2192De and 0.1 for En\u2192Fr. We set the number of encoder/decoder blocks for the bottom module as N = 6 following the common practice, and set the number of additionally stacked blocks of the top module as M = 2.",
  "y": "uses"
 },
 {
  "id": "d5d81a4c7759f9a4ab81195819c6d9_4",
  "x": "Our models are implemented based on the PyTorch implementation of Transformer 5 and the code can be found in the supplementary materials. Training We use Adam (Kingma and Ba, 2015) optimizer following the optimization settings and default learning rate schedule in<cite> Vaswani et al. (2017)</cite> for model training. All models are trained on 8 M40 GPUs.",
  "y": "uses"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_0",
  "x": "****HEAD-DRIVEN PARSING FOR WORD LATTICES**** **ABSTRACT** We present the first application of the head-driven statistical parsing model of <cite>Collins (1999)</cite> as a simultaneous language model and parser for largevocabulary speech recognition.",
  "y": "uses"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_1",
  "x": "Parse trees generated by this process will be useful for automated speech understanding, such as in higher semantic parsing (Ng and Zelle, 1997) . <cite>Collins (1999)</cite> presents three lexicalized models which consider long-distance dependencies within a sentence. Grammar productions are conditioned on headwords.",
  "y": "background"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_2",
  "x": "We find that the parsing model of <cite>Collins (1999)</cite> can be successfully adapted as a language model for speech recognition. In the following section, we present a review of recent works in high-level language modelling for speech recognition. We describe the word lattice parser developed in this work in Section 3.",
  "y": "extends"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_3",
  "x": "All possible sentence prefixes are considered at each extension step. Roark (2001) reports on the use of a lexicalized probabilistic top-down parser for word lattices, evaluated both on parse accuracy and WER. Our work is different from Roark (2001) in that we use a bottom-up parsing algorithm with dynamic programming based on the parsing model II of <cite>Collins (1999)</cite> .",
  "y": "uses"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_4",
  "x": "These models use much less conditioning information than the parsing models of <cite>Collins (1999)</cite> , and do not provide Penn Treebank format parse trees as output. In this section we outline the adaptation of the <cite>Collins (1999)</cite> parsing model to word lattices. The intended action of the parser is illustrated in Figure 1 , which shows parse trees built directly upon a word lattice.",
  "y": "background"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_5",
  "x": "In this section we outline the adaptation of the <cite>Collins (1999)</cite> parsing model to word lattices. The intended action of the parser is illustrated in Figure 1 , which shows parse trees built directly upon a word lattice. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_6",
  "x": "The parameterization of model II of <cite>Collins (1999)</cite> is used in our word lattice parser. Parameters are maximum likelihood estimates of conditional probabilities -the probability of some event of interest (e.g., a left-modifier attachment) given a context (e.g., parent non-terminal, distance, headword). One notable difference between the word lattice parser and the original implementation of <cite>Collins (1999)</cite> is the handling of part-of-speech (POS) tagging of unknown words (words seen fewer than 5 times in training).",
  "y": "uses"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_7",
  "x": "The parameterization of model II of <cite>Collins (1999)</cite> is used in our word lattice parser. Parameters are maximum likelihood estimates of conditional probabilities -the probability of some event of interest (e.g., a left-modifier attachment) given a context (e.g., parent non-terminal, distance, headword). One notable difference between the word lattice parser and the original implementation of <cite>Collins (1999)</cite> is the handling of part-of-speech (POS) tagging of unknown words (words seen fewer than 5 times in training).",
  "y": "differences"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_8",
  "x": "<cite>Collins (1999)</cite> falls back to the POS tagging of Ratnaparkhi (1996) for words seen fewer than 5 times in the training corpus. As the tagger of Ratnaparkhi (1996) cannot tag a word lattice, we cannot back off to this tagging. We rely on the tag assigned by the parsing model in all cases.",
  "y": "background"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_9",
  "x": "As the tagger of Ratnaparkhi (1996) cannot tag a word lattice, we cannot back off to this tagging. We rely on the tag assigned by the parsing model in all cases. Edges created by the bottom-up parsing are assigned a score which is the product of the inside and outside probabilities of the <cite>Collins (1999)</cite> model.",
  "y": "uses"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_10",
  "x": "The parameter estimation techniques (smoothing and back-off) of <cite>Collins (1999)</cite> are reimplemented. Additional techniques are required to prune the search space of possible parses, due to the complexity of the parsing algorithm and the size of the word lattices. The main technique we employ is a variation of the beam search of <cite>Collins (1999)</cite> to restrict the chart size by excluding low probability edges.",
  "y": "uses"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_11",
  "x": "Additional techniques are required to prune the search space of possible parses, due to the complexity of the parsing algorithm and the size of the word lattices. The main technique we employ is a variation of the beam search of <cite>Collins (1999)</cite> to restrict the chart size by excluding low probability edges. The total score (combined acoustic and language model scores) of candidate edges are compared against edge with the same span and category.",
  "y": "extends"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_12",
  "x": "In practice, these heuristics have a negative effect on parse accuracy, but the amount of pruning can be tuned to balance relative time and space savings against precision and recall degradation<cite> (Collins, 1999)</cite> . <cite>Collins (1999)</cite> uses a fixed size beam (10 000). We experiment with several variable beam (b) sizes, where the beam is some function of a base beam (b) and the edge width (the number of terminals dominated by an edge).",
  "y": "background"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_13",
  "x": "<cite>Collins (1999)</cite> uses a fixed size beam (10 000). We experiment with several variable beam (b) sizes, where the beam is some function of a base beam (b) and the edge width (the number of terminals dominated by an edge). The base beam starts at a low beam size and increases iteratively by a specified increment if no parse is found.",
  "y": "background"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_14",
  "x": "<cite>Collins (1999)</cite> uses a fixed size beam (10 000). We experiment with several variable beam (b) sizes, where the beam is some function of a base beam (b) and the edge width (the number of terminals dominated by an edge). The base beam starts at a low beam size and increases iteratively by a specified increment if no parse is found.",
  "y": "differences"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_15",
  "x": "Results show scores for parsing strings which are lower than the original implementation of <cite>Collins (1999)</cite> . The WER scores for this, the first application of the <cite>Collins (1999)</cite> model to parsing word lattices, are comparable to other recent work in syntactic language modelling, and better than a simple trigram model trained on the same data. 3 Parse trees are commonly scored with the PARSEVAL set of metrics (Black et al., 1991) .",
  "y": "differences"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_16",
  "x": "Results show scores for parsing strings which are lower than the original implementation of <cite>Collins (1999)</cite> . The WER scores for this, the first application of the <cite>Collins (1999)</cite> model to parsing word lattices, are comparable to other recent work in syntactic language modelling, and better than a simple trigram model trained on the same data. 3 Parse trees are commonly scored with the PARSEVAL set of metrics (Black et al., 1991) .",
  "y": "extends"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_17",
  "x": "The lattice parser was trained on sections 02-21 of the Wall Street Journal portion of the Penn Treebank (Taylor et al., 2003) Development testing was carried out on section 23 in order to select model thresholds and variable beam functions. Final testing was carried out on section 00, and the PARSEVAL measures (Black et al., 1991) were used to evaluate the performance. The scores for our experiments are lower than the scores of the original implementation of model II<cite> (Collins, 1999)</cite> .",
  "y": "differences"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_18",
  "x": "Tag accuracy for our model was 93.2%, whereas for the original implementation of <cite>Collins (1999)</cite> , model II achieved tag accuracy of 96.75%. In addition to different tagging strategies for unknown words, mentioned above, we restrict the tag-set considered by the parser for each word to those suggested by a simple first-stage tagger. 4 By reducing the tag-set considered by the parsing model, we reduce the search space and increase the speed.",
  "y": "differences"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_20",
  "x": "Two different corpora were used in training the parsing model on word lattices: sections<cite> (Collins, 1999)</cite> . section \"1987\" of the BLLIP corpus (Charniak et al., 1999) [20 million words] The BLLIP corpus is a collection of Penn Treebank-style parses of the three-year (1987) (1988) (1989) Wall Street Journal collection from the ACL/DCI corpus (approximately 30 million words).",
  "y": "uses"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_21",
  "x": "By contrast,<cite> (Collins, 1999)</cite> calculates parameter values by looking up event counts at run-time. The implementation was then optimized using a memory and processor profiler and debugger. Parsing the complete set of HUB-1 lattices (213 sentences, a total of 3,446 words) on average takes approximately 8 hours, on an Intel Pentium 4 (1.6GHz) Linux system, using 1GB memory.",
  "y": "differences"
 },
 {
  "id": "d636df7cc0eb06323ef159528caf49_22",
  "x": "**CONCLUSIONS** In this work we present an adaptation of the parsing model of <cite>Collins (1999)</cite> for application to ASR. The system was evaluated over two sets of data: strings and word lattices.",
  "y": "extends"
 },
 {
  "id": "d6c8b712c8fe3dd87d23886d575098_0",
  "x": "Out-ofdomain training data can hurt the translation performance on News test sets (Wang et al., 2017) and also significantly increase training time. Therefore, we trained neural language models on a large monolingual News corpus to perform data selection<cite> (Schamper et al., 2018)</cite> . Back-translation Large monolingual data in the News domain is provided for both German and 1 https://github.com/awslabs/sockeye English, which can be back-translated as additional parallel training data for our system (Sennrich et al., 2016a; Fadaee and Monz, 2018) .",
  "y": "motivation"
 },
 {
  "id": "d6c8b712c8fe3dd87d23886d575098_1",
  "x": "In-domain Fine-tuning The Transformer models were finally fine-tuned using the small in-domain parallel data provided for the News task (Luong and Manning, 2015; <cite>Schamper et al., 2018)</cite> . Note that the large back-translated parallel data is also in-domain, but it has relatively low quality due to translation errors. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "d6c8b712c8fe3dd87d23886d575098_2",
  "x": "**MODEL ENSEMBLE** We trained two Transformer models with different sizes, Transformer-base and Transformer-big. Our final submission is an ensemble of both models<cite> (Schamper et al., 2018)</cite> .",
  "y": "extends"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_0",
  "x": "The hierarchical lexical database approach can be reclassified into three groups according to usages of the database: gloss based method [5] , conceptual density based method [6, 7] and relative based method<cite> [8,</cite> 9, 10] . Since our method is a kind of the relative based method, this section describes the related works of the relative based method. <cite>[8]</cite> introduced the relative based method using International Roget's Thesaurus as a hierarchical lexical database.",
  "y": "background"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_1",
  "x": "The hierarchical lexical database approach can be reclassified into three groups according to usages of the database: gloss based method [5] , conceptual density based method [6, 7] and relative based method<cite> [8,</cite> 9, 10] . Since our method is a kind of the relative based method, this section describes the related works of the relative based method. <cite>[8]</cite> introduced the relative based method using International Roget's Thesaurus as a hierarchical lexical database.",
  "y": "similarities"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_2",
  "x": "Therefore, most of the example sentences of rail are not helpful for WSD of crane. His method has another problem in disambiguating senses of a large number of target words because it requires a great amount of time and storage space to collect example sentences of relatives of the target words. [9] followed the method of <cite>[8]</cite> , but tried to resolve the ambiguous relative problem by using just unambiguous relatives.",
  "y": "background"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_3",
  "x": "Another difference from <cite>[8]</cite> is on a lexical database: they utilized WordNet as a lexical database for acquiring relatives of target words instead of International Roget's Thesaurus. Since WordNet is freely available for research, various kinds of WSD studies based on WordNet can be compared with the method of [9] . They evaluated their method on 14 ambiguous nouns and achieved a good performance comparable to the methods based on the sense tagged corpus.",
  "y": "background"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_4",
  "x": "Since WordNet is freely available for research, various kinds of WSD studies based on WordNet can be compared with the method of [9] . They evaluated their method on 14 ambiguous nouns and achieved a good performance comparable to the methods based on the sense tagged corpus. However, the evaluation was conducted on a small part of senses of the target words like <cite>[8]</cite> .",
  "y": "background"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_5",
  "x": "Like <cite>[8]</cite> , the method also has a difficulty in disambiguating senses of many words because the method collects the example sentences of relatives of many words. [10] reimplemented the method of [9] using a web, which may be a very large corpus, in order to collect example sentences. They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as <cite>[8]</cite> and [9] .",
  "y": "background"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_6",
  "x": "[10] reimplemented the method of [9] using a web, which may be a very large corpus, in order to collect example sentences. They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as <cite>[8]</cite> and [9] . ----------------------------------",
  "y": "background"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_7",
  "x": "Our method makes use of ambiguous relatives as well as unambiguous relatives unlike [9] and hence overcomes the shortage problem of relatives and also reduces the problem of ambiguous relatives in <cite>[8]</cite> by handling relatives separately instead of putting example sentences of the relatives together into a pool. ---------------------------------- **RELATIVE SELECTION**",
  "y": "differences"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_8",
  "x": "Comparison with Other Relative Based Methods. We tried to compare our proposed method with the previous relative based methods. However, both of <cite>[8]</cite> and [9] did not evaluate their methods on a publicly available data.",
  "y": "background"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_9",
  "x": "However, both of <cite>[8]</cite> and [9] did not evaluate their methods on a publicly available data. We implemented their methods and compared our method with them on the same evaluation data. When both of the methods are implemented, it is practically difficult to collect example sentences of all target words in the evaluation data.",
  "y": "uses"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_10",
  "x": "When both of the methods are implemented, it is practically difficult to collect example sentences of all target words in the evaluation data. Instead, we implemented the previous methods to work with our CFM. WordNet was utilized as a lexical database to acquire relatives of target words and the sense disambiguation modules were implemented by using on Na\u00efve Bayesian classifier, which [9] adopted though <cite>[8]</cite> utilized International Roget's Thesaurus and other classifier similar to decision lists.",
  "y": "uses"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_11",
  "x": "The main difference between <cite>[8]</cite> and [9] is whether ambiguous relatives are utilized or not. Considering the difference, we implemented the method of <cite>[8]</cite> to include the ambiguous relatives into relatives, but the method of [9] to exclude the ambiguous relatives. Table 3 .",
  "y": "background"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_12",
  "x": "The main difference between <cite>[8]</cite> and [9] is whether ambiguous relatives are utilized or not. Considering the difference, we implemented the method of <cite>[8]</cite> to include the ambiguous relatives into relatives, but the method of [9] to exclude the ambiguous relatives. Table 3 .",
  "y": "uses"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_13",
  "x": "[5] 24.4% 32.8% . [17] . . 58.3% [18] . . 54.8% [19] . . 48.1% Our method 40.94% 45.12% 51.35% Table 2 shows the comparison results. 7 In the table, All Relatives and Unambiguous Relatives represent the results of the reimplemented methods of <cite>[8]</cite> and [9] , respectively.",
  "y": "uses"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_14",
  "x": "[17] . . 58.3% [18] . . 54.8% [19] . . 48.1% Our method 40.94% 45.12% 51.35% Table 2 shows the comparison results. 7 In the table, All Relatives and Unambiguous Relatives represent the results of the reimplemented methods of <cite>[8]</cite> and [9] , respectively. It is observed in the table that our proposed method achieves better performance on all evaluation data than the previous methods though the improvement is not large.",
  "y": "differences"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_15",
  "x": "It is observed in the table that our proposed method achieves better performance on all evaluation data than the previous methods though the improvement is not large. Hence, we may have an idea that our method handles relatives and in particular ambiguous relatives more effectively than <cite>[8]</cite> and [9] . Compared with [9] , <cite>[8]</cite> obtains a better performance, and the difference between the performance of them are totally more than 15 % on all of the evaluation data.",
  "y": "differences"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_16",
  "x": "[10] evaluated their method on nouns of lexical sample task of SENSEVAL-2. Their method achieved 49.8% recall. When evaluated on the same nouns of the lexical sample task, our proposed method achieved 47.26%, and the method of <cite>[8]</cite> 45.61%, and the method of [9] 38.03%.",
  "y": "differences"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_17",
  "x": "The experimental results show that the proposed method effectively disambiguates many ambiguous words in SemCor and in test data for SENSEVAL all words task, as well as a small number of ambiguous words in test data for SENSEVAL lexical sample task. Also our method more correctly disambiguates senses than <cite>[8]</cite> and [9] . Furthermore, the proposed method achieved comparable performance with the top 3 ranked systems at SENSEVAL-2 & 3.",
  "y": "differences"
 },
 {
  "id": "d6d8f08147e45acc0a61692abb37a9_18",
  "x": "Furthermore, the proposed method achieved comparable performance with the top 3 ranked systems at SENSEVAL-2 & 3. In consequence, our method has two advantages over the previous methods ( <cite>[8]</cite> and [9] ): our method 1) handles the ambiguous relatives and unambiguous relatives more effectively, and 2) utilizes only one co-occurrence matrix for disambiguating all contents words instead of collecting training data of the content words. However, our method did not achieve good performances.",
  "y": "differences"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_0",
  "x": "However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk et al., 2005; Liu et al., 2007; Zhang et al., 2007; <cite>Zhang et al., 2008a</cite>; Zhang et al., 2008b; Gildea, 2003; Galley et al., 2004; Bod, 2007) .",
  "y": "background motivation"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_1",
  "x": "The basic motivation behind <cite>syntax-based model</cite> is that the syntax information has the potential to model the structure reordering and discontiguous corresponding by the intrinsic structural generalization ability. Although remarkable progresses have been reported, the strict syntactic constraint (the both sides of the rules should strictly be a subtree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents. To alleviate this constraint, a few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks.",
  "y": "background motivation"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_2",
  "x": "<cite>Zhang et al. (2008a)</cite> made it possible to utilize the non-syntactic rules and even the phrases which are used in phrase based model by advancing a general tree sequence to tree sequence framework based on the tree-to-tree model presented in (Zhang et al., 2007) . In these models, various kinds of rules can be employed. For example, as shown in Figure 1 and Figure 2 , Figure 1 shows a Chinese-to-English sentence pair with syntax parses on both sides and the word alignments (dotted lines).",
  "y": "background"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_3",
  "x": "For example, as shown in Figure 1 and Figure 2 , Figure 1 shows a Chinese-to-English sentence pair with syntax parses on both sides and the word alignments (dotted lines). Figure 2 lists some of the rules which can be extracted from the sentence pair in Figure 1 by the system used in (<cite>Zhang et al., 2008a</cite>) . These rules includes not only conventional syntax rules but also the tree sequence rules (the multi-headed syntax rules ).",
  "y": "uses background"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_4",
  "x": "**RELATED WORKS** A few researches have made some exploratory investigations towards the effects of different rules by classifying the translation rules into different subcategories (Liu et al., 2007; <cite>Zhang et al., 2008a</cite>; DeNeefe et al., 2007) . Liu et al. (2007) differentiated the rules in their tree-to-string model which integrated with forest 1 -to-string into fully lexicalized rules, non-lexicalized rules and partial lexicalized rules according to the lexicalization levels.",
  "y": "background"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_5",
  "x": "Liu et al. (2007) differentiated the rules in their tree-to-string model which integrated with forest 1 -to-string into fully lexicalized rules, non-lexicalized rules and partial lexicalized rules according to the lexicalization levels. As an extension, <cite>Zhang et al. (2008a)</cite> proposed two more categories: <cite>Structure Reordering Rules</cite> (<cite>SRR</cite>) and <cite>Discontiguous Phrase Rules</cite> (<cite>DPR</cite>). The <cite>SRR</cite> stands for the rules which have at least two non-terminal leaf nodes with inverted order in the source and target side.",
  "y": "background"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_6",
  "x": "The <cite>SRR</cite> stands for the rules which have at least two non-terminal leaf nodes with inverted order in the source and target side. And <cite>DPR</cite> refers to the rules having at least one non-terminal leaf node between two terminal leaf nodes. (DeNeefe et al., 2007) made an illuminating breakdown of the different kinds of rules.",
  "y": "background"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_7",
  "x": "As an extension, <cite>Zhang et al. (2008a)</cite> proposed two more categories: <cite>Structure Reordering Rules</cite> (<cite>SRR</cite>) and <cite>Discontiguous Phrase Rules</cite> (<cite>DPR</cite>). The <cite>SRR</cite> stands for the rules which have at least two non-terminal leaf nodes with inverted order in the source and target side. And <cite>DPR</cite> refers to the rules having at least one non-terminal leaf node between two terminal leaf nodes.",
  "y": "background"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_8",
  "x": "Our proposed rule classification is inspired by <cite>these works</cite>. ---------------------------------- **RULES CLASSIFICATIONS**",
  "y": "motivation"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_9",
  "x": "Generally, the rules can be classified into two main groups according to whether syntax information is involved: bilingual phrases (Phrase) and syntax rules (Syntax). Further, the syntax rules can be divided into three categories according to the lexicalization levels (Liu et al., 2007; <cite>Zhang et al., 2008a</cite> source and target sides are non-lexicons (nonterminals) 3) Partially lexicalized (PLex): otherwise.",
  "y": "uses background"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_10",
  "x": "Then, the syntax rules can also fall into two categories according to whether equipping with generalization capability (Chiang, 2007; <cite>Zhang et al., 2008a</cite>) : 1) Initial rules (Initial): all leaf nodes of this rule are terminals. 2) Abstract rules (Abstract): otherwise, i.e. at least one leaf node is a non-terminal. A non-terminal leaf node in a rule is named an abstract node since it has the generalization capability.",
  "y": "uses background"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_11",
  "x": "For instance, the potentials to model the structure reordering and the discontiguous correspondence. <cite>The Structure Reordering Rules</cite> (<cite>SRR</cite>) and <cite>Discontiguous Phrase Rules</cite> (<cite>DPR</cite>) mentioned by (<cite>Zhang et al., 2008a</cite>) can be regarded as more in-depth classification of the syntax rules. In (<cite>Zhang et al., 2008a</cite>) , they are described as follows: Definition 1: The <cite>Structure Reordering Rule</cite> (<cite>SRR</cite>) refers to the structure reordering rule that has at least two non-terminal leaf nodes with inverted order in the source and target side.",
  "y": "background"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_12",
  "x": "In (<cite>Zhang et al., 2008a</cite>) , they are described as follows: Definition 1: The <cite>Structure Reordering Rule</cite> (<cite>SRR</cite>) refers to the structure reordering rule that has at least two non-terminal leaf nodes with inverted order in the source and target side. Definition 2: The <cite>Discontiguous Phrase Rule</cite> (<cite>DPR</cite>) refers to the rule having at least one nonterminal leaf node between two lexicalized leaf nodes. Based on <cite>these descriptions</cite>, R 7 , R 8 in Figure 2 belong to the category of <cite>SRR</cite> and R 6 , R 7 fall into the category of <cite>DPR</cite>.",
  "y": "background"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_13",
  "x": "In (<cite>Zhang et al., 2008a</cite>) , they are described as follows: Definition 1: The <cite>Structure Reordering Rule</cite> (<cite>SRR</cite>) refers to the structure reordering rule that has at least two non-terminal leaf nodes with inverted order in the source and target side. Definition 2: The <cite>Discontiguous Phrase Rule</cite> (<cite>DPR</cite>) refers to the rule having at least one nonterminal leaf node between two lexicalized leaf nodes. Based on <cite>these descriptions</cite>, R 7 , R 8 in Figure 2 belong to the category of <cite>SRR</cite> and R 6 , R 7 fall into the category of <cite>DPR</cite>.",
  "y": "background"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_14",
  "x": "Based on <cite>these descriptions</cite>, R 7 , R 8 in Figure 2 belong to the category of <cite>SRR</cite> and R 6 , R 7 fall into the category of <cite>DPR</cite>. Although <cite>these two definitions</cite> are easy implemented in practice, we argue that <cite>the definition</cite> of <cite>SRR</cite> is not complete. The reordering rules involving the reordering between content word terminals and non-terminal (such as R 5 in Figure  2 ) also can model the useful structure reorderings.",
  "y": "background"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_15",
  "x": "Definition 2: The <cite>Discontiguous Phrase Rule</cite> (<cite>DPR</cite>) refers to the rule having at least one nonterminal leaf node between two lexicalized leaf nodes. Based on <cite>these descriptions</cite>, R 7 , R 8 in Figure 2 belong to the category of <cite>SRR</cite> and R 6 , R 7 fall into the category of <cite>DPR</cite>. Although <cite>these two definitions</cite> are easy implemented in practice, we argue that <cite>the definition</cite> of <cite>SRR</cite> is not complete.",
  "y": "motivation"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_16",
  "x": "The definition of <cite>DPR</cite> in (<cite>Zhang et al., 2008a</cite> ) is explicit but somewhat rough and not very accurate. For example, in Figure 3(a) , non-terminal node pair ([0,' '] , [0,'love'] ) is surrounded by lexical terminals. According to Definition 2, it is a <cite>DPR</cite>.",
  "y": "motivation background"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_17",
  "x": "For example, in Figure 3(a) , non-terminal node pair ([0,' '] , [0,'love'] ) is surrounded by lexical terminals. According to Definition 2, it is a <cite>DPR</cite>. However, obviously it is not a discontiguous phrase actually.",
  "y": "differences"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_18",
  "x": "On the basis of the above analyses, we present a novel classification system for the abstract rules based on the crossings between the leaf node alignment links. Given an abstract rule r =< Note that the intersection of <cite>SRR</cite> NT 2 and <cite>SRR</cite> NT-T is not necessary an empty set, i.e. a rule can be both <cite>SRR</cite> NT 2 and <cite>SRR</cite> NT-T rule. The basic characteristic of the discontiguous translation is that the correspondence of one nonterminal N T is inserted among the correspondences of one phrase X. Figure 5 (a) illustrates this situation.",
  "y": "differences"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_19",
  "x": "Thus, the rule in Figure  5 (c) is a discontiguous phrase rule. Definition 3: Given an abstract rule r =< \u03be s , \u03be t , A T , A N T >, it is a Discontiguous Phrase iff \u2203 two links l t1 , l t2 from A T and a link l nt from A N T , satisfy: l t1 , l t2 are emitted from the same word and l t1 is crossed with l nt when l t2 is not crossed with l nt . Through Definition 3, we know that the <cite>DPR</cite> is a sub-set of the <cite>SRR</cite> NT-T.",
  "y": "differences"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_20",
  "x": "This novel classification system may supports the SMT research community with some helpful references. In the future works, aiming to analyze the rule contributions and the redundances issues using the presented rule classification based on some real translation systems, we plan to implement some synchronous grammar based syntax translation models such as the one presented in (Liu et al., 2007) or in (<cite>Zhang et al., 2008a</cite>) . Taking <cite>such a system</cite> as the experimental platform, we can perform comprehensive statistics about distributions of different rule categories.",
  "y": "future_work"
 },
 {
  "id": "d70b9838e8a32a8638d7aed0adc80a_21",
  "x": "This novel classification system may supports the SMT research community with some helpful references. In the future works, aiming to analyze the rule contributions and the redundances issues using the presented rule classification based on some real translation systems, we plan to implement some synchronous grammar based syntax translation models such as the one presented in (Liu et al., 2007) or in (<cite>Zhang et al., 2008a</cite>) . Taking <cite>such a system</cite> as the experimental platform, we can perform comprehensive statistics about distributions of different rule categories.",
  "y": "future_work"
 },
 {
  "id": "d70e69bb3eaa6b46ee3b7110126129_0",
  "x": "Word embeddings representation spaces are known to present geometrical phenomena mimicking relations and analogies between words (e.g. man is to woman as king is to queen). Following this property of finding relations or analogies, one popular example of gender bias is the word association between man to computer programmer as woman to homemaker<cite> (Bolukbasi et al., 2016)</cite> . Pre-trained word embeddings are used in many NLP downstream tasks, such as natural language inference (NLI), machine translation (MT) or question answering (QA).",
  "y": "background"
 },
 {
  "id": "d70e69bb3eaa6b46ee3b7110126129_1",
  "x": "Recent progress in word embedding techniques has been achieved with contextualized word embeddings (Peters et al., 2018) which provide different vector representations for the same word in different contexts. While gender bias has been studied, detected and partially addressed for standard word embeddings techniques<cite> (Bolukbasi et al., 2016</cite>; Zhao et al., 2018a; Gonen and Goldberg, 2019) , it is not the case for the latest techniques of contextualized word embeddings. Only just recently, Zhao et al. (2019) present a first analysis on the topic based on the proposed methods in<cite> Bolukbasi et al. (2016)</cite> .",
  "y": "background motivation"
 },
 {
  "id": "d70e69bb3eaa6b46ee3b7110126129_2",
  "x": "While gender bias has been studied, detected and partially addressed for standard word embeddings techniques<cite> (Bolukbasi et al., 2016</cite>; Zhao et al., 2018a; Gonen and Goldberg, 2019) , it is not the case for the latest techniques of contextualized word embeddings. Only just recently, Zhao et al. (2019) present a first analysis on the topic based on the proposed methods in<cite> Bolukbasi et al. (2016)</cite> . In this paper, we further analyse the presence of gender biases in contextualized word embeddings by means of the proposed methods in Gonen and Goldberg (2019) .",
  "y": "background"
 },
 {
  "id": "d70e69bb3eaa6b46ee3b7110126129_3",
  "x": "**DEBIASED WORD EMBEDDINGS** Human-generated corpora suffer from social biases. Those biases are reflected in the cooccurrence statistics, and therefore learned into word embeddings trained in those corpora, amplifying them<cite> (Bolukbasi et al., 2016</cite>; Caliskan et al., 2017) .",
  "y": "background motivation"
 },
 {
  "id": "d70e69bb3eaa6b46ee3b7110126129_4",
  "x": [
   "Bolukbasi et al. (2016) studied from a geometrical point of view the presence of gender bias in word embeddings. For this, they compute the subspace where the gender information concentrates by computing the principal components of the difference of vector representations of male and female gender-defining word pairs. With the gender subspace, the authors identify direct and indirect biases in profession words. Finally, they mitigate the bias by nullifying the information in the gender subspace for words that should not be associated to gender, and also equalize their distance to both elements of gender-defining word pairs."
  ],
  "y": "background"
 },
 {
  "id": "d70e69bb3eaa6b46ee3b7110126129_5",
  "x": "Once the embeddings are trained, the gender protected attribute can be simply removed from the vector representation, therefore eliminating any gender bias present in it. The transformations proposed by both<cite> Bolukbasi et al. (2016)</cite> and Zhao et al. (2018b) are downstream task-agnostic. This fact is used in the work of Gonen and Goldberg (2019) to showcase that, while apparently the embedding information is removed, there is still gender information remaining in the vector representations.",
  "y": "background motivation"
 },
 {
  "id": "d70e69bb3eaa6b46ee3b7110126129_6",
  "x": "To address these questions, we adapt and contrast with the evaluation measures proposed by<cite> Bolukbasi et al. (2016)</cite> and Gonen and Goldberg (2019) . ---------------------------------- **EXPERIMENTAL FRAMEWORK**",
  "y": "differences"
 },
 {
  "id": "d70e69bb3eaa6b46ee3b7110126129_7",
  "x": "To perform our analysis we used a set of lists from previous work<cite> (Bolukbasi et al., 2016</cite>; Gonen and Goldberg, 2019) . We refer to the list of definitional pairs 2 as 'Definitonal List' (e.g. shehe, girl-boy). We refer to the list of female and male professions 3 as 'Professional List' (e.g. accountant, surgeon). The 'Biased List' is the list used in the clustering experiment and it consists of biased male and female words (500 female biased tokens and 500 male biased token). This list is generated by taking the most biased words, where the bias of a word is computed by taking its projection on the gender direction ( \u2212 \u2192 he-\u2212\u2192 she) (e.g. breastfeeding, bridal and diet for female and hero, cigar and teammates for male). The 'Extended Biased List' is the list used in classification experiment , which contains 5000 male and female biased tokens, 2500 for each gender, generated in the same way of the Biased List 4 .",
  "y": "uses"
 },
 {
  "id": "d70e69bb3eaa6b46ee3b7110126129_8",
  "x": "The 'Extended Biased List' is the list used in classification experiment , which contains 5000 male and female biased tokens, 2500 for each gender, generated in the same way of the Biased List 4 . A note to be considered, is that the lists we used in our experiments (and obtained from<cite> Bolukbasi et al. (2016)</cite> and Gonen and Goldberg (2019) ) may contain words that are missing in our corpus and so we can not obtain contextualized embeddings for them. Among different approaches to contextualized word embeddings (mentioned in section 2), we choose ELMo (Peters et al., 2018) as contextualized word embedding approach.",
  "y": "uses"
 },
 {
  "id": "d70e69bb3eaa6b46ee3b7110126129_9",
  "x": "There is no standard measure for gender bias, and even less for such the recently proposed contextualized word embeddings. In this section, we adapt gender bias measures for word embedding methods from previous work<cite> (Bolukbasi et al., 2016)</cite> and (Gonen and Goldberg, 2019) to be applicable to contextualized word embeddings. This way, we first compute the gender subspace from the ELMo vector representations of genderdefining words, then identify the presence of direct bias in the contextualized representations.",
  "y": "extends"
 },
 {
  "id": "d70e69bb3eaa6b46ee3b7110126129_10",
  "x": "We then proceed to identify gender information by means of clustering and classifications techniques. We compare our results to previous results from debiased and non-debiased word embeddings<cite> (Bolukbasi et al., 2016)</cite> . Bolukbasi et al. (2016) propose to identify gender bias in word representations by computing the direction between representations of male and female word pairs from the Definitional List ( \u2212 \u2192 he-\u2212\u2192 she, \u2212\u2212\u2192 man-\u2212 \u2212\u2212\u2212\u2212 \u2192 woman) and computing their principal components.",
  "y": "background"
 },
 {
  "id": "d70e69bb3eaa6b46ee3b7110126129_11",
  "x": "In order to have a reference, we computed the principal components of representation of random words. Similarly to<cite> Bolukbasi et al. (2016)</cite> , figure 1 shows that the first eigenvalue is significantly larger than the rest and that there is also a single direction describing the majority of variance in these vectors, still the difference between the percentage of variances is less in case of contextualized embeddings, which may refer that there is less bias in such embeddings. We can easily note the difference in the case of random, where there is a smooth and gradual decrease in eigenvalues, and hence the variance percentage.",
  "y": "similarities background"
 },
 {
  "id": "d70e69bb3eaa6b46ee3b7110126129_12",
  "x": "We excluded the sentences that have both a professional token and definitional gender word to avoid the influence of the latter over the presence of bias in the former. We applied the definition of direct bias from<cite> Bolukbasi et al. (2016)</cite> on the ELMo representations of the professional words in these sentences. where N is the amount of gender neutral words, g the gender direction, and w the word vector of each profession.",
  "y": "similarities"
 },
 {
  "id": "d70e69bb3eaa6b46ee3b7110126129_13",
  "x": "We got direct bias of 0.03, compared to 0.08 from standard word2vec embeddings described in<cite> Bolukbasi et al. (2016)</cite> . This reduction on the direct bias confirms that the substantial component along the gender direction that is present in standard word embeddings is less for the contextualized word embeddings. Probably, this reduction comes from the fact that we are using different word embeddings for the same profession depending on the sentence which is a direct consequence and advantage of using contextualized embeddings.",
  "y": "differences background"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_0",
  "x": "1 The accuracy of identifying the fillers of such Zero cases remains only around 50% in terms of F 1 even if the task is restricted to the identification of intra-sentential predicate-argument relations (Matsubayashi and Inui, 2017) . One promising approach for addressing this problem is to model argument sharing across multiple predicates (Iida et al., 2015; Ouchi et al., 2015;<cite> Ouchi et al., 2017)</cite> . In Figure 1 , for example, one can find very limited syntactic clues for predicting the long-distance dative relation between answer and reporters.",
  "y": "background"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_1",
  "x": "Identifying PASs in Japanese text is a long-standing challenge chiefly due to the abundance of omitted (elliptical) arguments. One promising approach for addressing this problem is to model argument sharing across multiple predicates (Iida et al., 2015; Ouchi et al., 2015;<cite> Ouchi et al., 2017)</cite> . However, the relation must be easy to identify for human readers who know that the person who asks a question is likely to be answered; namely, the nominative argument of ask is likely to be shared with the dative argument of answer.",
  "y": "motivation"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_2",
  "x": "With this goal in mind, Iida et al. (2015) constructed a subject-shared predicate network with an accurate recognizer of subject-sharing relations and deterministically propagated the predicted subjects to the other predicates in the graph. However, this method is applied only to subject sharing, so it cannot take into account the relationships among multiple argument labels. More recently, as an end-to-end model considering multi-predicate dependencies, <cite>Ouchi et al. (2017)</cite> used Grid RNN to incorporate intermediate representations of the prediction for one predicate generated by an RNN layer into the inputs of the RNN layer for another predicate.",
  "y": "background"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_3",
  "x": "However, this method is applied only to subject sharing, so it cannot take into account the relationships among multiple argument labels. More recently, as an end-to-end model considering multi-predicate dependencies, <cite>Ouchi et al. (2017)</cite> used Grid RNN to incorporate intermediate representations of the prediction for one predicate generated by an RNN layer into the inputs of the RNN layer for another predicate. However, in this model, since the information of multiple predicates also propagates through the RNNs, the integration of the prediction information is influenced by word order and distance, which is not necessarily important for aspects of syntactic and semantic relations.",
  "y": "motivation"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_4",
  "x": "In this study, we follow the setting of Iida et al. (2015) , <cite>Ouchi et al. (2017)</cite> , and Matsubayashi and Inui (2017) , and focus only on analyzing arguments in a target sentence. In addition, we exclude argument instances that are in the same bunsetsu, a base phrase unit in Japanese, as the target predicate, following <cite>Ouchi et al. (2017)</cite> , which we will compare with the results in experiments. The semantic labels used in NTC may seem to be rather syntactic as they are named nominative, accusative, etc.",
  "y": "uses"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_5",
  "x": "In addition, we exclude argument instances that are in the same bunsetsu, a base phrase unit in Japanese, as the target predicate, following <cite>Ouchi et al. (2017)</cite> , which we will compare with the results in experiments. The semantic labels used in NTC may seem to be rather syntactic as they are named nominative, accusative, etc. However, this annotation task markedly differs from shallow syntactic parsing and is, in fact, more like a semantic role labeling (SRL) task including implicit argument prediction.",
  "y": "uses"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_6",
  "x": "---------------------------------- **BASE MODEL** Our proposed models extend end-to-end style SRL systems using deep bi-RNN (Zhou and Xu, 2015; He et al., 2017;<cite> Ouchi et al., 2017)</cite> to combine mechanisms that consider multiple predicate interactions.",
  "y": "extends uses"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_7",
  "x": "We employ gated recurrent units (GRUs) (Cho et al., 2014) for the RNNs. In addition, we use the residual connections (He et al., 2016) following <cite>Ouchi et al. (2017)</cite> . Then, a fourdimensional vector representing a probability p(c i,t |i, p, w) is obtained by applying a softmax layer to each output of the last RNN layer h K i,t .",
  "y": "uses"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_8",
  "x": "In order to capture dependencies between the arguments of multiple predicates, we apply two extensions to our base model: a multi-predicate input layer and three variants of interaction layers on top of the deep bi-RNNs. Figures 2b and 3 show the network structures of the extended models. In contrast to the Grid RNN model of <cite>Ouchi et al. (2017)</cite> , where the information of multiple predicates propagates through the RNNs, our interaction layers use pooling and attention mechanisms to directly associate the label prediction information for a target (predicate, word) pair with that for words strongly related to the target pair, without being disturbed by word order and distance.",
  "y": "differences"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_9",
  "x": "In order to strictly compare the impact of our extensions to the method used for integrating multiple pieces of predicate information in the state-of-the-art end-to-end model, in addition to our base model, we replicated the method of <cite>Ouchi et al. (2017)</cite> by modifying Equations (1) of our base model as follows: if 1 \u2264 i \u2264 q; otherwise, h k i,t = 0. The performance of this replicated model may not be strictly the same as that reported in <cite>Ouchi et al. (2017)</cite> due to discrepancies in the embeddings of inputs, hyperparameters (a training batch size, a hidden unit size, etc.), and training strategy (an optimizing algorithm, a regularization method, an early stopping method, etc.).",
  "y": "uses"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_10",
  "x": "if 1 \u2264 i \u2264 q; otherwise, h k i,t = 0. The performance of this replicated model may not be strictly the same as that reported in <cite>Ouchi et al. (2017)</cite> due to discrepancies in the embeddings of inputs, hyperparameters (a training batch size, a hidden unit size, etc.), and training strategy (an optimizing algorithm, a regularization method, an early stopping method, etc.). The predicate positions p = p 1 , ..., p q are arranged in ascending order.",
  "y": "differences"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_11",
  "x": "The third set of rows in Table 1 shows the reported performance of related studies. Grid RNN of <cite>Ouchi et al. (2017)</cite> is a state-of-the-art end-to-end model, designed to capture interactions among multiple predicate-argument relations. A comparison between their model and the proposed models was somewhat tricky because our replication of Grid RNN did not reproduce the reported performance on the same dataset (see the row of GRID in Table 1 ).",
  "y": "background"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_12",
  "x": "Grid RNN of <cite>Ouchi et al. (2017)</cite> is a state-of-the-art end-to-end model, designed to capture interactions among multiple predicate-argument relations. A comparison between their model and the proposed models was somewhat tricky because our replication of Grid RNN did not reproduce the reported performance on the same dataset (see the row of GRID in Table 1 ). Unlike the results reported in <cite>Ouchi et al. (2017)</cite> , the GRID model in our experiment did not clearly outperform the model without the grid architecture, i.e., the Base model.",
  "y": "differences"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_13",
  "x": "A comparison between their model and the proposed models was somewhat tricky because our replication of Grid RNN did not reproduce the reported performance on the same dataset (see the row of GRID in Table 1 ). Unlike the results reported in <cite>Ouchi et al. (2017)</cite> , the GRID model in our experiment did not clearly outperform the model without the grid architecture, i.e., the Base model. We first suspected that this might have resulted from the difference in dimensionality d r of RNN hidden states: d r = 32 in <cite>Ouchi et al. (2017)</cite> , whereas d r = 256 in our experiments.",
  "y": "differences"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_14",
  "x": "We first suspected that this might have resulted from the difference in dimensionality d r of RNN hidden states: d r = 32 in <cite>Ouchi et al. (2017)</cite> , whereas d r = 256 in our experiments. Specifically, we speculated that the base model with a low dimensionality left a larger margin for improvement and incorporating the Grid architecture derived positive effects. We thus trained our GRID model with <cite>Ouchi et al. (2017)</cite> 's settings (d r = 32 and K = 8) and the best performing hyperparameters; however, we were not able to reproduce the reported gain from Grid RNN (see the row of \"GRID (d r = 32, K = 8)\" in Table 1 ).",
  "y": "differences uses"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_15",
  "x": "For an end-to-end neural model, <cite>Ouchi et al. (2017)</cite> used a Grid RNN to capture multiple predicate interactions. Through experiments, we demonstrated that our proposed models outperformed these models in terms of the overall F 1 on a standard benchmark corpus. 4 To the best of our knowledge, there are few previous studies related to SRL considering multiple predicate interactions for languages other than Japanese.",
  "y": "background"
 },
 {
  "id": "d76946b009d67613326a8e7650ad36_16",
  "x": "For an end-to-end neural model, <cite>Ouchi et al. (2017)</cite> used a Grid RNN to capture multiple predicate interactions. Through experiments, we demonstrated that our proposed models outperformed these models in terms of the overall F 1 on a standard benchmark corpus. 4 To the best of our knowledge, there are few previous studies related to SRL considering multiple predicate interactions for languages other than Japanese.",
  "y": "differences"
 },
 {
  "id": "d785838888358a711fbf07c9dcf430_0",
  "x": "In this work we learn clusters of contextual annotations for non-terminals in the Penn Treebank. Perhaps the best way to think about this problem is to contrast our work with that of<cite> Klein and Manning (2003)</cite> . That research used treetransformations to create various grammars with different contextual annotations on the non-terminals. These grammars were then used in conjunction with a CKY parser. The authors explored the space of different annotation combinations by hand. Here we try to automate the process -to learn the \"right\" combination automatically. Our results are not quite as good as those carefully created by hand, but they are close (84.8 vs 85.7).",
  "y": "differences background"
 },
 {
  "id": "d785838888358a711fbf07c9dcf430_1",
  "x": "Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (Magerman, 1995; Collins, 1996; Collins, 1997; Johnson, 1998; Charniak, 2000; Henderson, 2003;<cite> Klein and Manning, 2003</cite>; Matsuzaki et al., 2005) (and others) . One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (Johnson, 1998;<cite> Klein and Manning, 2003)</cite> . Rather than imagining the parser roaming around the tree for picking up the information it needs, we rather relabel the nodes to directly encode this information.",
  "y": "background"
 },
 {
  "id": "d785838888358a711fbf07c9dcf430_2",
  "x": "Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (Magerman, 1995; Collins, 1996; Collins, 1997; Johnson, 1998; Charniak, 2000; Henderson, 2003;<cite> Klein and Manning, 2003</cite>; Matsuzaki et al., 2005) (and others) . One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (Johnson, 1998;<cite> Klein and Manning, 2003)</cite> . Rather than imagining the parser roaming around the tree for picking up the information it needs, we rather relabel the nodes to directly encode this information.",
  "y": "differences background"
 },
 {
  "id": "d785838888358a711fbf07c9dcf430_3",
  "x": "Thus rather than have the parser \"look\" to find out that, say, the parent of some N P is an S, we simply relabel the N P as an N P [S] . This viewpoint is even more compelling if one does not intend to smooth the probabilities. For example, consider p(N P \u2192 P RN | N P [S]) If we have no intention of backing off this probability to p(N P \u2192 P RN | N P ) we can treat N P [S] as an uninterpreted phrasal category and run all of the standard PCFG algorithms without change. The result is a vastly simplified parser. This is exactly what is done by<cite> Klein and Manning (2003)</cite> .",
  "y": "background motivation"
 },
 {
  "id": "d785838888358a711fbf07c9dcf430_4",
  "x": "The<cite> Klein and Manning (2003)</cite> parser is an unlexicalized PCFG with various carefully selected context annotations. Their model uses some parent annotations, and marks nodes which initiate or in certain cases conclude unary productions. They also propose linguistically motivated annotations for several tags, including V P , IN , CC,N P and S. This results in a reasonably accurate unlexicalized PCFG parser. The downside of this approach is that their features are very specific, applying different annotations to different treebank nonterminals. For instance, they mark right-recursive N P s and not V P s (i.e., an N P which is the right-most child of another N P ). This is because data sparsity issues preclude annotating the nodes in the treebank too liberally. The goal of our work is to automate the process a bit, by annotating with more general features that apply broadly, and by learning clus-ters of these annotations.",
  "y": "differences background motivation"
 },
 {
  "id": "d785838888358a711fbf07c9dcf430_5",
  "x": "They mark the highest scoring nonterminals that are part of these joint events in the treebank, and use the resulting PCFG. Coming to this problem from the standpoint of tree transformation, we naturally view our work as a descendent of Johnson (1998) and<cite> Klein and Manning (2003)</cite> . In retrospect, however, there are perhaps even greater similarities to that of (Magerman, 1995; Henderson, 2003; Matsuzaki et al., 2005) .",
  "y": "similarities background"
 },
 {
  "id": "d785838888358a711fbf07c9dcf430_6",
  "x": "**BACKGROUND** where V is a set of terminal symbols; M = {\u00b5 i } is a set of nonterminal symbols; \u00b5 0 is a start or root symbol; R is a set of productions of the form \u00b5 i \u2192 \u03c1, where \u03c1 is a sequence of terminals and nonterminals; and q is a family of probability distributions over rules conditioned on each rule's left-hand side. As in (Johnson, 1998) and<cite> (Klein and Manning, 2003)</cite> , we annotate the Penn treebank nonterminals with various context information.",
  "y": "uses"
 },
 {
  "id": "d785838888358a711fbf07c9dcf430_7",
  "x": "Extending on the notion of a Base NP, introduced by Collins (1996) , we mark any nonterminal that dominates only preterminals as Base. Collins inserts a unary NP over any base NPs without NP parents. However,<cite> Klein and Manning (2003)</cite> find that this hurts performance relative to just marking the NPs, and so our Base feature does not insert.",
  "y": "similarities"
 },
 {
  "id": "d785838888358a711fbf07c9dcf430_8",
  "x": "Related to this, we further noticed that several of<cite> Klein & Manning's (2003)</cite> features, such as marking N P s as right recursive or possessive have the property of annotating with the label of the rightmost child (when they are NP and POS respectively). We generalize this by marking all nodes both with their rightmost child and (an analogous feature) leftmost child. We also mark whether or not a node borders the end of a sentence, save for ending punctuation. (For instance, in this sentence, all the constituents with the second \"marked\" rightmost in their span would be marked).",
  "y": "extends background"
 },
 {
  "id": "d785838888358a711fbf07c9dcf430_9",
  "x": "Another<cite> Klein and Manning (2003)</cite> feature we try includes the temporal NP feature, where TMP markings in the treebank are retained, and propagated down the head inheritance path of the tree. It is worth mentioning that all the features here come directly from the treebank. For instance, the part of speech of the head feature has values only from the raw treebank tag set. When a preterminal cluster is split, this assignment does not change the value of this feature.",
  "y": "uses background"
 },
 {
  "id": "d785838888358a711fbf07c9dcf430_10",
  "x": "We perform this in a manner similar to<cite> Klein and Manning (2003)</cite> and Matsuzaki et al. (2005) Our mechanism lays out the unmarkovized intermediate rules in the same way, but we mostly use our clustering scheme to reduce sparsity. We do so by aligning the labels contained in the intermediate nodes in the order in which they would be added when increasing the markovization hori-1 The implementation we use was created by Mark Johnson and used for the research in (Johnson, 1998) . It is available at his homepage.",
  "y": "extends similarities"
 },
 {
  "id": "d785838888358a711fbf07c9dcf430_11",
  "x": "We also always keep the heir label as a feature, following<cite> Klein and Manning (2003</cite> (D, F, E, D, \u2212) , where the first item is the heir of the parent's head. The \"-\" indicates that the fourth item to be expanded is here non-existent. The clusterer would consider each of these five features as for a single possible split.",
  "y": "uses background"
 },
 {
  "id": "d785838888358a711fbf07c9dcf430_12",
  "x": "In order to ease comparison between our work and that of<cite> Klein and Manning (2003)</cite> , we follow their lead in smoothing no production probabilities save those going from preterminal to nonterminal. Our smoothing mechanism runs roughly along the lines of theirs. Table 2 : Parsing results for grammars generated using clusterer with different random seeds.",
  "y": "similarities"
 },
 {
  "id": "d785838888358a711fbf07c9dcf430_14",
  "x": "We have presented a scheme for automatically discovering phrasal categories for parsing with a standard CKY parser. The parser achieves 84.8% precision-recall f-measure on the standard testsection of the Penn WSJ-Treebank (section 23). While this is not as accurate as the hand-tailored grammar of<cite> Klein and Manning (2003)</cite> , it is close, and we believe there is room for improvement.",
  "y": "background"
 },
 {
  "id": "d7dba136667d6058bf46d6ede3f2ef_0",
  "x": "Most recently, several approaches based on cross-lingual entity embeddings (Hao et al., 2016; Chen et al., 2017; Sun, Hu, and Li, 2017) or graph neural networks <cite>Xu et al., 2019</cite>; Wu et al., 2019) have been proposed for this task. In particular,<cite> Xu et al. (2019)</cite> introduces the topic entity graph to capture the local context information of an entity within the KG, and further tackles this task as a graph matching problem by proposing a graph matching network. This work significantly advanced the state-of-theart accuracies across several datasets.",
  "y": "background"
 },
 {
  "id": "d7dba136667d6058bf46d6ede3f2ef_1",
  "x": "Most recently, several approaches based on cross-lingual entity embeddings (Hao et al., 2016; Chen et al., 2017; Sun, Hu, and Li, 2017) or graph neural networks <cite>Xu et al., 2019</cite>; Wu et al., 2019) have been proposed for this task. In particular,<cite> Xu et al. (2019)</cite> introduces the topic entity graph to capture the local context information of an entity within the KG, and further tackles this task as a graph matching problem by proposing a graph matching network. This work significantly advanced the state-of-theart accuracies across several datasets.",
  "y": "background"
 },
 {
  "id": "d7dba136667d6058bf46d6ede3f2ef_2",
  "x": "In particular, we analyze the results of<cite> Xu et al. (2019)</cite> and find that nearly 8% of the alignments are many-to-one mappings. One intuitive solution is to align these entities in a greedy fashion, that is, assign one alignment at each time with a constraint that all alignments are one-to-one mappings. However, this may introduce the error propagation, since each decision error may propagate to the future decisions.",
  "y": "uses"
 },
 {
  "id": "d7dba136667d6058bf46d6ede3f2ef_3",
  "x": "Instead of using TransE to derive entity embeddings from the knowledge graph, various GCN based methods Ye et al., 2019; Wu et al., 2019) that use the conventional GCN to encode the entities and relations have been proposed to perform the alignment. Different with those methods that still follow previous works that rely on learned entity embeddings to rank alignments,<cite> Xu et al. (2019)</cite> views this task as a graph matching problem and further proposes a graph matching neural network that additionally considers the matching information of an entity's neighborhood to perform the prediction. Despite these approaches achieve progressive results, all current works focus on encoding the entities and relations, while neglecting the fact that the decoding strategy may have a considerable impact over the final performance.",
  "y": "background"
 },
 {
  "id": "d7dba136667d6058bf46d6ede3f2ef_4",
  "x": "We analyze the alignment results of three baseline methods, i.e., Wang et al. (2018) ,<cite> Xu et al. (2019)</cite> and Wu et al. (2019) . Interestingly, we find that all these baselines could achieve at least 99.5% accuracy for those alignments with normalized probabilities over 0.9. This result is coherent with our expectation since a higher probability typically suggests that the model is more confident about the prediction and also indicates that this alignment is easier for the model to resolve.",
  "y": "uses"
 },
 {
  "id": "d7dba136667d6058bf46d6ede3f2ef_5",
  "x": "After establishing easy assignments in each decoding step, we need to incorporate them as additional knowledge into the alignment model for the next round decoding. This design heavily depends on alignment model architecture. In this paper, we use the state-of-the-art alignment model<cite> (Xu et al., 2019)</cite> as our baseline method and propose two ways to enhance this model by incorporating easy assignment information.",
  "y": "extends uses"
 },
 {
  "id": "d7dba136667d6058bf46d6ede3f2ef_6",
  "x": "Our Model. In contrast to<cite> Xu et al. (2019)</cite> that only takes two topic graphs as input, we can utilize additional information such as easy assignments found in previous decoding steps to resolve hard assignments. In particular, we introduce two ways to enhance this baseline model by explicitly integrating the easy assignment information into two layers of<cite> Xu et al. (2019)</cite> :",
  "y": "differences"
 },
 {
  "id": "d7dba136667d6058bf46d6ede3f2ef_7",
  "x": "In particular, we introduce two ways to enhance this baseline model by explicitly integrating the easy assignment information into two layers of<cite> Xu et al. (2019)</cite> : \u2022 Enhanced Input Representation Layer. In this layer,<cite> Xu et al. (2019)</cite> utilizes a GCN to learn entity embeddings from the topic graph, where the entity surface form has been proved to be a key feature in deriving their embeddings.",
  "y": "extends"
 },
 {
  "id": "d7dba136667d6058bf46d6ede3f2ef_8",
  "x": "As concluded in<cite> Xu et al. (2019)</cite> , the node-level matching layer has a significant impact on the matching performance, since it captures the local entity matching information. In the baseline model, the entity similarities are calculated based on the entity embeddings derived from the first GCN layer. Although in the enhanced input representation layer the aligned entities have the same surface forms, it can still not guarantee that their embeddings are close.",
  "y": "background"
 },
 {
  "id": "d7dba136667d6058bf46d6ede3f2ef_9",
  "x": "Comparison Models. We compare our approach against existing alignment methods: JE (Hao et al., 2016) , MTransE (Chen et al., 2017) , JAPE (Sun, Hu, and Li, 2017) , IPTransE (Zhu et al., 2017) , BootEA (Sun, Hu, and Li, 2017) , GCN , GM<cite> (Xu et al., 2019)</cite> and RDGCN (Wu et al., 2019) . Model Variants.",
  "y": "uses"
 },
 {
  "id": "d7dba136667d6058bf46d6ede3f2ef_10",
  "x": "(2) X-JEA: the baseline model X that only uses our proposed Joint Entity Alignment method; (3) X-EHD-JEA: the baseline model X that uses both of these two reasoning methods. Implementation details. For the configurations of the alignment model, we use the same settings as<cite> Xu et al. (2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "d8168f4596878807d22ddc7474ffc8_0",
  "x": "In addition, many of these polysynthetic languages are low-resource. We propose unsupervised approaches for morphological segmentation of low-resource polysynthetic languages based on Adaptor Grammars (AG) <cite>(Eskander et al., 2016)</cite> . We experiment with four languages from the Uto-Aztecan family.",
  "y": "uses"
 },
 {
  "id": "d8168f4596878807d22ddc7474ffc8_1",
  "x": "We propose unsupervised approaches for morphological segmentation of low-resource polysynthetic languages based on Adaptor Grammars (AG) <cite>(Eskander et al., 2016)</cite> . We experiment with four languages from the Uto-Aztecan family. Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages.",
  "y": "extends"
 },
 {
  "id": "d8168f4596878807d22ddc7474ffc8_2",
  "x": "We experiment with four UtoAztecan languages: Mexicanero (MX), Nahuatl (NH), Wixarika (WX) and Yorem Nokki (YN) (Kann et al., 2018) . Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (Johnson, 2008; Sirts and Goldwater, 2013;<cite> Eskander et al., 2016</cite> Eskander et al., , 2018 . Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal.",
  "y": "background"
 },
 {
  "id": "d8168f4596878807d22ddc7474ffc8_3",
  "x": "Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal. We use the datasets introduced by Kann et al. (2018) in an unsupervised fashion (unsegmented words). We design several AG learning setups: 1) use the best-on-average AG setup from<cite> Eskander et al. (2016)</cite> ; 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from Kann et al. (2018) ; 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3).",
  "y": "uses"
 },
 {
  "id": "d8168f4596878807d22ddc7474ffc8_4",
  "x": "An Adaptor Grammar is typically composed of a PCFG and an adaptor that adapts the probabilities of individual subtrees. For morphological segmentation, a PCFG is a morphological grammar that specifies word structure, where AGs learn latent tree structures given a list of words. In this paper, we experiment with the grammars and the learning setups proposed by<cite> Eskander et al. (2016)</cite> , which we outline briefly below.",
  "y": "uses"
 },
 {
  "id": "d8168f4596878807d22ddc7474ffc8_5",
  "x": "In this paper, we experiment with the grammars and the learning setups proposed by<cite> Eskander et al. (2016)</cite> , which we outline briefly below. Grammars. We use the nine grammars from<cite> Eskander et al. (2016</cite> Eskander et al. ( , 2018 that were designed based on three dimensions: 1) how the grammar models word structure (e.g., prefix-stem-suffix vs. morphemes), 2) the level of abstraction in nonterminals (e.g., compounds, morphemes and submorphemes) and 3) how the output boundaries are specified (see Table 2 for a sample grammars).",
  "y": "uses"
 },
 {
  "id": "d8168f4596878807d22ddc7474ffc8_6",
  "x": "We use the nine grammars from<cite> Eskander et al. (2016</cite> Eskander et al. ( , 2018 that were designed based on three dimensions: 1) how the grammar models word structure (e.g., prefix-stem-suffix vs. morphemes), 2) the level of abstraction in nonterminals (e.g., compounds, morphemes and submorphemes) and 3) how the output boundaries are specified (see Table 2 for a sample grammars). For example, the PrStSu+SM grammar models the Table 2 : Sample grammar setups used by Eskander et al. (2018<cite> Eskander et al. ( , 2016</cite> . Compound = Upper level representation of the word as a sequence of compounds; Morph = affix/morpheme representation as a sequence of morphemes.",
  "y": "uses"
 },
 {
  "id": "d8168f4596878807d22ddc7474ffc8_7",
  "x": "Learning Settings. The input to the learner is a grammar and a vocabulary of unsegmented words. We consider the three learning settings in <cite>(Eskander et al., 2016)</cite> : Standard, Scholarseeded Knowledge and Cascaded.",
  "y": "uses"
 },
 {
  "id": "d8168f4596878807d22ddc7474ffc8_8",
  "x": "Language-Independent Morphological Segmenter. LIMS is the best-on-average AG setup obtained by<cite> Eskander et al. (2016)</cite> when trained on six languages (English, German, Finnish, Estonian, Turkish and Zulu), which is the Cascaded PrStSu+SM configuration. We use this AG setup for each of the four languages.",
  "y": "uses"
 },
 {
  "id": "d8168f4596878807d22ddc7474ffc8_9",
  "x": "Language-Independent Morphological Segmenter. LIMS is the best-on-average AG setup obtained by<cite> Eskander et al. (2016)</cite> when trained on six languages (English, German, Finnish, Estonian, Turkish and Zulu), which is the Cascaded PrStSu+SM configuration. We use this AG setup for each of the four languages.",
  "y": "background"
 },
 {
  "id": "d8168f4596878807d22ddc7474ffc8_10",
  "x": "Best AG Configuration per Language. In this experimental setup, we consider all nine grammars from<cite> Eskander et al. (2016)</cite> using both the Standard and the Cascaded approaches and choosing the one that is best for each polysynthetic language by training on the training set and evaluating on the development set. We denote this system as AG BestL .",
  "y": "uses"
 },
 {
  "id": "d8168f4596878807d22ddc7474ffc8_11",
  "x": "We denote this system as AG BestL . Using Seeded Knowledge. To approximate the effect of Scholar-seeded-Knowledge in<cite> Eskander et al. (2016)</cite>, we used the training set to derive affixes and use them as scholar-seeded knowledge added to the grammars (before the learning happens).",
  "y": "similarities"
 },
 {
  "id": "d8168f4596878807d22ddc7474ffc8_12",
  "x": "Using Seeded Knowledge. To approximate the effect of Scholar-seeded-Knowledge in<cite> Eskander et al. (2016)</cite>, we used the training set to derive affixes and use them as scholar-seeded knowledge added to the grammars (before the learning happens). However, since affixes and stems are not distinguished in the training annotations from Kann et al. (2018) , we only consider the first and last morphemes that appear at least five times.",
  "y": "extends"
 },
 {
  "id": "d8168f4596878807d22ddc7474ffc8_13",
  "x": "The best AG setup learned for each of the four polysynthetic languages (AG BestL ) is the PrStSu+SM grammar using the Cascaded learning setup. This is an interesting finding as the Cascaded PrSTSu+SM setup is in fact AG LIM S -the best-on-average AG setup obtained by<cite> Eskander et al. (2016)</cite> Table 4 : Best AG results compared to supervised approaches from Kann et al. (2018) . Bold indicates best scores.",
  "y": "background"
 },
 {
  "id": "d8168f4596878807d22ddc7474ffc8_14",
  "x": "Table 3 shows the performance of our AG setups on the four languages. The best AG setup learned for each of the four polysynthetic languages (AG BestL ) is the PrStSu+SM grammar using the Cascaded learning setup. This is an interesting finding as the Cascaded PrSTSu+SM setup is in fact AG LIM S -the best-on-average AG setup obtained by<cite> Eskander et al. (2016)</cite> Table 4 : Best AG results compared to supervised approaches from Kann et al. (2018) .",
  "y": "similarities"
 },
 {
  "id": "d8168f4596878807d22ddc7474ffc8_15",
  "x": "**CONCLUSIONS** Unsupervised approaches based on Adaptor Grammars show promise for morphological segmentation of low-resource polysynthetic languages. We worked with the AG grammars developed by<cite> Eskander et al. (2016</cite> Eskander et al. ( , 2018 for languages that are not polysynthetic.",
  "y": "uses"
 },
 {
  "id": "d883c7bb1e95895cd4f57a8f430e35_0",
  "x": "Part-of-speech (POS) tagging has received a great deal of attention as it is a critical component of most natural language processing systems. In particular, there has been growing interest in both multilingual POS induction ) and cross-lingual POS induction via treebank projection (Yarowsky and Ngai, 2001; Xi and Hwa, 2005; <cite>Das and Petrov, 2011)</cite> .",
  "y": "background"
 },
 {
  "id": "d883c7bb1e95895cd4f57a8f430e35_1",
  "x": "When corpora with common tagsets are unavailable, a standard approach is to manually define a mapping from language and treebank specific fine-grained tagsets to a predefined universal set. This was the approach taken by<cite> Das and Petrov (2011)</cite> to evaluate their cross-lingual POS projection system for six different languages. To facilitate future research and to standardize best-practices, we propose a tagset that consists of twelve universal POS categories.",
  "y": "differences background"
 },
 {
  "id": "d883c7bb1e95895cd4f57a8f430e35_2",
  "x": "First, using our universal tagset and mapping, we run an experiment comparing POS tag accuracies for 25 different treebanks to evaluate POS tagging accuracy on a single tagset. Second, we combine the cross-lingual projection part-of-speech taggers of<cite> Das and Petrov (2011)</cite> with the grammar induction system of Naseem et al. (2010) -which requires a universal tagset -to produce a completely unsupervised grammar induction system for multiple languages, that does not require gold POS tags in the target language. ----------------------------------",
  "y": "motivation differences extends"
 },
 {
  "id": "d883c7bb1e95895cd4f57a8f430e35_3",
  "x": "In our experiments, we did not make use of refined categories, as the POS tags induced by<cite> Das and Petrov (2011)</cite> were all coarse. We present results on the same eight IndoEuropean languages as<cite> Das and Petrov (2011)</cite> , so that we can make use of their automatically projected POS tags. For all languages, we used the treebanks released as a part of the CoNLL-X (Buchholz and Marsi, 2006) shared task.",
  "y": "uses"
 },
 {
  "id": "d883c7bb1e95895cd4f57a8f430e35_4",
  "x": "We present results on the same eight IndoEuropean languages as<cite> Das and Petrov (2011)</cite> , so that we can make use of their automatically projected POS tags. For all languages, we used the treebanks released as a part of the CoNLL-X (Buchholz and Marsi, 2006) shared task. We only considered sentences of length 10 or less, after the removal of punctuations.",
  "y": "similarities uses"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_0",
  "x": "VQA Datasets: We conduct our analysis on a total of 459,861 visual questions and 4,598,610 answers coming from today's largest freely-available VQA benchmark <cite>[3]</cite> . The remaining 90,000 VQAs are about abstract scenes that were created with clipart and show 100 types of everyday objects often observed in real images <cite>[3]</cite> .",
  "y": "uses"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_1",
  "x": "Pilots could learn how many birds are in their path to decide whether to change course and so avoid costly, life-threatening collisions. These examples illustrate several of the interests from a visual question answering (VQA) system, including tackling problems that involve classification, detection, and counting. More generally, the goal for VQA is to have a single system that can accurately answer any natural language question about an image or video [2] , <cite>[3]</cite> , [4] .",
  "y": "background"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_2",
  "x": "Today's status quo is to assume a fixed number of human responses per visual question and so a fixed cost, delay, and potential diversity of answers for every visual question <cite>[3]</cite> , [1] , [5] . We instead propose to dynamically solicit the number of human responses based on each visual question. In particular, we aim to accrue additional costs and delays from collecting extra answers only when extra responses are needed to discover all plausible answers.",
  "y": "background"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_3",
  "x": "Today's status quo is to assume a fixed number of human responses per visual question and so a fixed cost, delay, and potential diversity of answers for every visual question <cite>[3]</cite> , [1] , [5] . We instead propose to dynamically solicit the number of human responses based on each visual question. In particular, we aim to accrue additional costs and delays from collecting extra answers only when extra responses are needed to discover all plausible answers.",
  "y": "differences background"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_4",
  "x": "Our work is also inspired by the goal to improve how to employ crowds to produce the information needed to train and evaluate automated methods. Specifically, researchers in fields as diverse as computer vision <cite>[3]</cite> , computational linguistics [2] , and machine learning [4] rely on large datasets to improve their VQA algorithms. These datasets include visual questions and human-supplied answers.",
  "y": "background"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_5",
  "x": "Such data is also critical for evaluating how well VQA algorithms perform. In general, \"bigger\" data is better. Current methods to create these datasets assume a fixed number of human answers per visual question <cite>[3]</cite> , [5] , thereby either compromising on quality by not collecting all plausible answers or cost by collecting additional answers when they are redundant.",
  "y": "background"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_6",
  "x": "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2] , <cite>[3]</cite> , [1] , [4] . Yet a commonality across these communities is they adopt a one-size-fits-all approach when deciding the number of answers for any visual question. For example, crowd-powered systems aim to supply a prespecified, fixed number of answers per visual question [1] and automated systems return a single answer for every visual question [2] , <cite>[3]</cite> , [4] .",
  "y": "background"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_7",
  "x": "For example, crowd-powered systems aim to supply a prespecified, fixed number of answers per visual question [1] and automated systems return a single answer for every visual question [2] , <cite>[3]</cite> , [4] . Inspired by the observation that there can be multiple plausible answers per visual question, we propose a richer representation of visual question answering that accounts for whether different people would agree on a single answer. We propose a system that automatically predicts whether humans will disagree.",
  "y": "background"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_8",
  "x": "We propose a system that automatically predicts whether humans will disagree. We demonstrate the predictive advantage of our system over relying on the uncertainty of a VQA algorithm in its predicted answer <cite>[3]</cite> . Answer Collection from a Crowd: Our work relates to methods that propose how to employ crowd workers to answer questions about images.",
  "y": "differences"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_9",
  "x": "Such approaches aim to collect a pre-specified, fixed number of answers per visual question. For those systems that treat response time as a first priority, a variable number of answers may arise but this is due to varying crowdsourcing conditions such as the available supply of workers [1] , [6] . Other systems ensure a fixed number of answers are collected per visual question <cite>[3]</cite> , [5] .",
  "y": "background"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_10",
  "x": "For those systems that treat response time as a first priority, a variable number of answers may arise but this is due to varying crowdsourcing conditions such as the available supply of workers [1] , [6] . Other systems ensure a fixed number of answers are collected per visual question <cite>[3]</cite> , [5] . Unlike prior work, our goal is to collect answers in a way that is both economical and complete in capturing the diversity of plausible answers for all visual questions.",
  "y": "differences background"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_11",
  "x": "VQA Datasets: We conduct our analysis on a total of 459,861 visual questions and 4,598,610 answers coming from today's largest freely-available VQA benchmark <cite>[3]</cite> . We chose this benchmark because it both represents a diversity of visual questions and includes many crowdsourced answers for every visual question. The benchmark consists of two datasets that reflect VQAs for real images and abstract scenes.",
  "y": "uses"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_12",
  "x": "Specifically, 80% (i.e., 369,861) of VQAs are about real images that show 91 types of objects that would be \"easily recognizable by a 4 year old\" in their natural context [21] . The remaining 90,000 VQAs are about abstract scenes that were created with clipart and show 100 types of everyday objects often observed in real images <cite>[3]</cite> . The benchmark includes a diversity of visual questions intentionally collected to be both grounded in images and taskindependent.",
  "y": "background"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_13",
  "x": "The remaining 90,000 VQAs are about abstract scenes that were created with clipart and show 100 types of everyday objects often observed in real images <cite>[3]</cite> . The benchmark includes a diversity of visual questions intentionally collected to be both grounded in images and taskindependent. Towards this aim, visual questions were collected by asking three Amazon Mechanical Turk (AMT) crowd workers to look at a given image and generate a text-based question about it that would \"stump a smart robot\" <cite>[3]</cite> .",
  "y": "uses"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_14",
  "x": "Three open-ended questions were collected about each of 153,287 images, resulting in a total of 459,861 visual questions. The benchmark also includes 10 open-ended natural language answers from 10 AMT crowd workers per visual question. Each answer was collected by showing a worker an image with associated question and asking him/her to respond with \"a brief phrase and not a complete sentence\" <cite>[3]</cite> .",
  "y": "uses"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_15",
  "x": "We pre-process each answer by converting all letters to lower case, converting numbers to digits, and removing punctuation and articles (i.e., \"a\", \"an\", \"the\"), as was done in prior work <cite>[3]</cite> . While this approach does not fully resolve all conceptually equivalent responses, it does reveal an upper bound of expected answer diversity. In other words, more lenient agreement schemes (e.g., employing sophisticated natural language processing methods) would lead to either the same or less answer diversity.",
  "y": "uses"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_16",
  "x": "In practice, prior work deems answers as 100% valid using blind trust (i.e., m = 1 person) [22] as well as more conservative answer validation schemes (i.e., m = 3 people) <cite>[3]</cite> . Measuring Answer Diversity: We now turn to the question of how much answer diversity is observed in practice for visual questions. Across all 459,861 visual questions, we tally how many visual questions yield k unique, valid answers where k = {1, 2, ..., 10}.",
  "y": "background"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_17",
  "x": "In other words, more lenient agreement schemes (e.g., employing sophisticated natural language processing methods) would lead to either the same or less answer diversity. We establish valid answers by tallying the number of times each unique answer is given and then only accepting answers observed from at least m people, where m is an application-specific parameter to set. In practice, prior work deems answers as 100% valid using blind trust (i.e., m = 1 person) [22] as well as more conservative answer validation schemes (i.e., m = 3 people) <cite>[3]</cite> .",
  "y": "differences background"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_18",
  "x": "We now describe our studies to assess the predictive power of our classification systems to decide whether visual questions will lead to answer (dis)agreement. We capitalize on today's largest visual question answering dataset <cite>[3]</cite> to evaluate our prediction system, which includes 369,861 visual questions about real images. Of these, 248,349 visual questions (i.e., Training questions 2015 v1.0) are kept for for training and the remaining 121,512 visual questions (i.e., Validation questions 2015 v1.0) are employed for testing our classification system.",
  "y": "uses"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_19",
  "x": "Therefore, we employ as a baseline a related VQA algorithm [25] , <cite>[3]</cite> which produces for a given visual question an answer with a confidence score. This system parallels the deep learning architecture we adapt. However, it predicts the system's uncertainty in its own answer, whereas we are interested in the humans' collective disagreement on the answer.",
  "y": "uses"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_20",
  "x": "Therefore, we employ as a baseline a related VQA algorithm [25] , <cite>[3]</cite> which produces for a given visual question an answer with a confidence score. However, it predicts the system's uncertainty in its own answer, whereas we are interested in the humans' collective disagreement on the answer.",
  "y": "uses differences"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_21",
  "x": "Figure 4a shows precision-recall curves for all prediction systems. Both our proposed classification systems outperform the VQA Algorithm <cite>[3]</cite> baseline; e.g., Ours -RF yields a 12 percentage point improvement with respect to AP. This is interesting because it shows there is value in learning the disagreement task specifically, rather than employing an algorithm's confidence in its answers.",
  "y": "differences"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_22",
  "x": "We observe that question-based features yield greater predictive performance than image-based features for all visual questions, when comparing AP scores for Q and I classification results ( Figure 5 ). In fact, image features contribute to performance improvements only for our random forest classifier for visual questions that lead to \"number\" answers, as illustrated by comparing AP scores for Our RF: Q+I and Our RF: Q (Figure 5b) . Our overall finding that most of the predictive power stems from language-based features parallels feature analysis findings in the automated VQA literature <cite>[3]</cite> , [22] .",
  "y": "similarities"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_23",
  "x": "We next present a novel resource allocation system for efficiently capturing the diversity of true answers for a batch of visual questions. Today's status quo is to either uniformly collect N answers for every visual question <cite>[3]</cite> or collect multiple answers where the number is determined by external crowdsourcing conditions [1] . Our system instead spends a human budget by predicting the number of answers to collect Fig. 6 : We propose a novel application of predicting the number of redundant answers to collect from the crowd per visual question to efficiently capture the diversity of all answers for all visual questions.",
  "y": "background"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_24",
  "x": "We next present a novel resource allocation system for efficiently capturing the diversity of true answers for a batch of visual questions. Today's status quo is to either uniformly collect N answers for every visual question <cite>[3]</cite> or collect multiple answers where the number is determined by external crowdsourcing conditions [1] . Our system instead spends a human budget by predicting the number of answers to collect Fig. 6 : We propose a novel application of predicting the number of redundant answers to collect from the crowd per visual question to efficiently capture the diversity of all answers for all visual questions.",
  "y": "differences background"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_26",
  "x": "We simulate answer collection by randomly selecting answers from the 10 crowd answers per visual question. Baselines: We compare our approach to the following baselines: VQA Algorithm <cite>[3]</cite> :",
  "y": "uses"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_27",
  "x": "Status Quo: The system randomly prioritizes which images receive redundancy. This predictor illustrates the best a user can achieve today with crowd-powered systems [1] , [6] or with current dataset collection methods <cite>[3]</cite> , [5] .",
  "y": "background"
 },
 {
  "id": "d8a9f0578e389f8cbe153783dc47b3_28",
  "x": "Our approach fills an important gap in the crowdsourcing answer collection literature for targeting the allocation of extra answers only to visual questions where a diversity of answers is expected. Figure 6b also illustrates the advantage of our system over a related VQA algorithm <cite>[3]</cite> for our novel application of costsensitive answer collection from a crowd. As observed, relying on an algorithm's confidence in its answer offers a valuable indicator over today's status quo of passively budgeting.",
  "y": "differences"
 },
 {
  "id": "d8c3d04514c0867d78a7603e49ea9b_0",
  "x": "For the other language pairs, the input material was 30,000 post-edited segments. The main part of the training corpora (approximately 75%) was part of Pangeanic's own repository harvested through web crawling and also OpenSubtitles (Tiedemann, 2012) . The rest of the corpus was automatically validated synthetic material using general data from Leipzig (Goldhahn et al., 2012 Engine customization The data was cleaned using the Bicleaner tool <cite>(S\u00e1nchez-Cartagena et al., 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "d8c3d04514c0867d78a7603e49ea9b_1",
  "x": "The main part of the training corpora (approximately 75%) was part of Pangeanic's own repository harvested through web crawling and also OpenSubtitles (Tiedemann, 2012) . The rest of the corpus was automatically validated synthetic material using general data from Leipzig (Goldhahn et al., 2012) . Engine customization The data was cleaned using the Bicleaner tool <cite>(S\u00e1nchez-Cartagena et al., 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "d8e3cdea7f61152ed37395c5f9393e_0",
  "x": "These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for Chang et al. (2009) , N = 5, whereas for<cite> Newman et al. (2010)</cite> , Aletras and Stevenson (2013) and Lau et al. (2014) , N = 10. The germ of this paper came when using the automatic word intrusion methodology (Lau et al., 2014) , and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction. This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence.",
  "y": "background"
 },
 {
  "id": "d8e3cdea7f61152ed37395c5f9393e_1",
  "x": "We also test the PMI methodology<cite> (Newman et al., 2010)</cite> and make the same observation. To remedy this, we show that performance can be substantially improved if system scores and human ratings are aggregated over different cardinality settings before computing the correlation. This has broad implications for topic model evaluation.",
  "y": "uses"
 },
 {
  "id": "d8e3cdea7f61152ed37395c5f9393e_2",
  "x": "To examine the relationship between topic cardinality and topic coherence, we require a dataset that has topics for a range of cardinality settings. Although there are existing datasets with human-annotated coherence scores<cite> (Newman et al., 2010</cite>; Aletras and Stevenson, 2013; Lau et al., 2014; Chang et al., 2009) , these topics were annotated using a fixed cardinality setting (e.g. 5 or 10). We thus develop a new dataset for this experiment.",
  "y": "background"
 },
 {
  "id": "d8e3cdea7f61152ed37395c5f9393e_3",
  "x": "Although there are existing datasets with human-annotated coherence scores<cite> (Newman et al., 2010</cite>; Aletras and Stevenson, 2013; Lau et al., 2014; Chang et al., 2009) , these topics were annotated using a fixed cardinality setting (e.g. 5 or 10). We thus develop a new dataset for this experiment. Following Lau et al. (2014) , we use two domains: (1) WIKI, a collection of 3.3 million English Wikipedia articles (retrieved November 28th 2009); and (2) NEWS, a collection of 1.2 million New York Times articles from 1994 to 2004 (English Gigaword).",
  "y": "differences"
 },
 {
  "id": "d8e3cdea7f61152ed37395c5f9393e_4",
  "x": "We then generate 300 LDA topics for each of the sub-sampled collection. 2 There are two primary approaches to assessing topic coherence: (1) via word intrusion (Chang et (2) by directly measuring observed coherence<cite> (Newman et al., 2010</cite>; Lau et al., 2014) . With the first method, Chang et al. (2009) injects an intruder word into the top-5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words.",
  "y": "background"
 },
 {
  "id": "d8e3cdea7f61152ed37395c5f9393e_5",
  "x": "2 There are two primary approaches to assessing topic coherence: (1) via word intrusion (Chang et (2) by directly measuring observed coherence<cite> (Newman et al., 2010</cite>; Lau et al., 2014) . As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities.",
  "y": "uses"
 },
 {
  "id": "d8e3cdea7f61152ed37395c5f9393e_6",
  "x": "As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities. 3 To collect the coherence judgements, we used Amazon Mechanical Turk and asked Turkers to rate topics in terms of coherence using a 3-point ordinal scale, where 1 indicates incoherent and 3 very coherent<cite> (Newman et al., 2010)</cite> . For each topic (600 topics in total) we experiment with 4 cardinality settings: N = {5, 10, 15, 20}. For example, for N = 5, we display the top-5 topic words for coherence judgement.",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_0",
  "x": "The model relies heavily on an adversarial, unsupervised alignment of word embedding spaces for bilingual dictionary induction (<cite>Conneau et al., 2018</cite>), which we examine here. Our results identify the limitations of current unsupervised MT: unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking, when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust induction, and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_1",
  "x": "Early cross-lingual word embedding models relied on large amounts of parallel data (Klementiev et al., 2012; Mikolov et al., 2013a) , but more recent approaches have tried to minimize the amount of supervision necessary Levy et al., 2017; Artetxe et al., 2017) . Some researchers have even presented unsupervised methods that do not rely on any form of cross-lingual supervision at all (Barone, 2016; <cite>Conneau et al., 2018</cite>; Zhang et al., 2017) . Unsupervised cross-lingual word embeddings hold promise to induce bilingual lexicons and machine translation models in the absence of dictionaries and translations (Barone, 2016; Zhang et al., 2017; Lample et al., 2018a) , and would therefore be a major step toward machine translation to, from, or even between low-resource languages.",
  "y": "background"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_2",
  "x": "Unsupervised cross-lingual word embeddings hold promise to induce bilingual lexicons and machine translation models in the absence of dictionaries and translations (Barone, 2016; Zhang et al., 2017; Lample et al., 2018a) , and would therefore be a major step toward machine translation to, from, or even between low-resource languages. Unsupervised approaches to learning crosslingual word embeddings are based on the assumption that monolingual word embedding graphs are approximately isomorphic, that is, after removing a small set of vertices (words) (Mikolov et al., 2013b; Barone, 2016; Zhang et al., 2017; <cite>Conneau et al., 2018</cite>) . In the words of Barone (2016):",
  "y": "background"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_3",
  "x": "Contributions We focus on the recent stateof-the-art unsupervised model of <cite>Conneau et al. (2018)</cite> . 1 Our contributions are: (a) In \u00a72, we show that the monolingual word embeddings used in <cite>Conneau et al. (2018)</cite> are not approximately isomorphic, using the VF2 algorithm (Cordella et al., 2001 ) and we therefore introduce a metric for quantifying the similarity of word embeddings, based on Laplacian eigenvalues. (b) In \u00a73, we identify circumstances under which the unsupervised <cite>bilingual dictionary induction (BDI)</cite> algorithm proposed in <cite>Conneau et al. (2018)</cite> does not lead to good performance.",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_4",
  "x": ". . . we hypothesize that, if languages are used to convey thematically similar information in similar contexts, these random processes should be approximately isomorphic between languages, and that this isomorphism can be learned from the statistics of the realizations of these processes, the monolingual corpora, in principle without any form of explicit alignment. Our results indicate this assumption is not true in general, and that approaches based on this assumption have important limitations. Contributions We focus on the recent stateof-the-art unsupervised model of <cite>Conneau et al. (2018)</cite> .",
  "y": "motivation"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_5",
  "x": "Contributions We focus on the recent stateof-the-art unsupervised model of <cite>Conneau et al. (2018)</cite> . 1 Our contributions are: (a) In \u00a72, we show that the monolingual word embeddings used in <cite>Conneau et al. (2018)</cite> are not approximately isomorphic, using the VF2 algorithm (Cordella et al., 2001 ) and we therefore introduce a metric for quantifying the similarity of word embeddings, based on Laplacian eigenvalues. (b) In \u00a73, we identify circumstances under which the unsupervised <cite>bilingual dictionary induction (BDI)</cite> algorithm proposed in <cite>Conneau et al. (2018)</cite> does not lead to good performance.",
  "y": "uses motivation"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_6",
  "x": "(c) We show that a simple trick, exploiting a weak supervision signal from words that are identical across languages, makes the algorithm much more robust. Our main finding is that the performance of unsupervised <cite>BDI</cite> depends heavily on all three factors: the language pair, the comparability of the monolingual corpora, and the parameters of the word embedding algorithms. 2 How similar are embeddings across languages?",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_7",
  "x": "As mentioned, recent work focused on unsupervised <cite>BDI</cite> assumes that monolingual word embedding spaces (or at least the subgraphs formed by the most frequent words) are approximately isomorphic. In this section, we show, by investigating the nearest neighbor graphs of word embedding spaces, that word embeddings are far from isomorphic. We therefore introduce a method for computing the similarity of non-isomorphic graphs.",
  "y": "background"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_8",
  "x": "2 How similar are embeddings across languages? As mentioned, recent work focused on unsupervised <cite>BDI</cite> assumes that monolingual word embedding spaces (or at least the subgraphs formed by the most frequent words) are approximately isomorphic. In this section, we show, by investigating the nearest neighbor graphs of word embedding spaces, that word embeddings are far from isomorphic.",
  "y": "motivation differences"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_9",
  "x": "In this section, we show, by investigating the nearest neighbor graphs of word embedding spaces, that word embeddings are far from isomorphic. We therefore introduce a method for computing the similarity of non-isomorphic graphs. In \u00a74.7, we correlate our similarity metric with performance on unsupervised <cite>BDI</cite>.",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_10",
  "x": "Isomorphism To motivate our study, we first establish that word embeddings are far from graph isomorphic 2 -even for two closely re-2 Two graphs that contain the same number of graph vertices connected in the same way are said to be isomorphic. In the context of weighted graphs such as word embeddings, a lated languages, English and German, and using embeddings induced from comparable corpora (Wikipedia) with the same hyper-parameters. If we take the top k most frequent words in English, and the top k most frequent words in German, and build nearest neighbor graphs for English and German using the monolingual word embeddings used in <cite>Conneau et al. (2018)</cite> , the graphs are of course very different.",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_11",
  "x": "We take this as evidence that word embeddings are not approximately isomorphic across languages. We also ran graph isomorphism checks on 10 random samples of frequent English nouns and their translations into Spanish, and only in 1/10 of the samples were the corresponding nearest neighbor graphs isomorphic. Eigenvector similarity Since the nearest neighbor graphs are not isomorphic, even for frequent translation pairs in neighboring languages, we want to quantify the potential for unsupervised <cite>BDI</cite> using a metric that captures varying degrees of graph similarity.",
  "y": "motivation"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_12",
  "x": "Note that \u2206 = 0 means the graphs are isospectral, and the metric goes to infinite. Thus, the higher \u2206 is, the less similar the graphs (i.e., their Laplacian spectra). We discuss the correlation between unsupervised <cite>BDI</cite> performance and approximate isospectrality or eigenvector similarity in \u00a74.7.",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_13",
  "x": "---------------------------------- **LEARNING SCENARIOS** Unsupervised neural machine translation relies on <cite>BDI</cite> using cross-lingual embeddings (Lample et al., 2018a; Artetxe et al., 2018) , which in turn relies on the assumption that word embedding graphs are approximately isomorphic.",
  "y": "background"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_14",
  "x": "Unsupervised neural machine translation relies on <cite>BDI</cite> using cross-lingual embeddings (Lample et al., 2018a; Artetxe et al., 2018) , which in turn relies on the assumption that word embedding graphs are approximately isomorphic. The work of <cite>Conneau et al. (2018)</cite> , which we focus on here, also makes several implicit assumptions that may or may not be necessary to achieve such isomorphism, and which may or may not scale to low-resource languages. <cite>The algorithms</cite> are not intended to be limited to learning scenarios where these assumptions hold, but since they do in the reported experiments, it is important to see to what extent these assumptions are necessary for <cite>the algorithms</cite> to produce useful embeddings or dictionaries.",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_15",
  "x": "<cite>The algorithms</cite> are not intended to be limited to learning scenarios where these assumptions hold, but since they do in the reported experiments, it is important to see to what extent these assumptions are necessary for <cite>the algorithms</cite> to produce useful embeddings or dictionaries. We focus on the work of <cite>Conneau et al. (2018)</cite> , who present a fully unsupervised approach to aligning monolingual word embeddings, induced using fastText (Bojanowski et al., 2017) . We describe the <cite>learning algorithm</cite> in \u00a73.2.",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_16",
  "x": "We focus on the work of <cite>Conneau et al. (2018)</cite> , who present a fully unsupervised approach to aligning monolingual word embeddings, induced using fastText (Bojanowski et al., 2017) . We describe the <cite>learning algorithm</cite> in \u00a73.2. <cite>Conneau et al. (2018)</cite> consider a specific set of learning scenarios:",
  "y": "background"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_17",
  "x": "3 We evaluate <cite>Conneau et al. (2018)</cite> on (English to) Estonian (ET), Finnish (FI), Greek (EL), Hungarian (HU), Polish (PL), and Turkish (TR) in \u00a74.2, to test whether the selection of languages in the original study introduces a bias. (b) The monolingual corpora in their experiments are comparable; Wikipedia corpora are used, except for an experiment in which they include Google Gigawords. We evaluate across different domains, i.e., on all combinations of Wikipedia, EuroParl, and the EMEA medical corpus, in \u00a74.3.",
  "y": "uses motivation"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_18",
  "x": "We believe such scenarios are more realistic for low-resource languages. (c) The monolingual embedding models are induced using the same algorithms with the same hyper-parameters. We evaluate <cite>Conneau et al. (2018)</cite> on pairs of embeddings induced with different hyper-parameters in \u00a74.4.",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_19",
  "x": "We also investigate the sensitivity of unsupervised <cite>BDI</cite> to the dimensionality of the monolingual word embeddings in \u00a74.5. The motivation for this is that dimensionality reduction will alter the geometric shape and remove characteristics of the embedding graphs that are important for alignment; but on the other hand, lower dimensionality introduces regularization, which will make the graphs more similar. Finally, in \u00a74.6, we investigate the impact of different types of query test words on performance, including how performance varies across part-of-speech word classes and on shared vocabulary items.",
  "y": "motivation"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_21",
  "x": "**SUMMARY OF <cite>CONNEAU ET AL. (2018)</cite>** We now introduce the method of <cite>Conneau et al. (2018)</cite> . 4 <cite>The approach</cite> builds on existing work on learning a mapping between monolingual word embeddings (Mikolov et al., 2013b; Xing et al., 2015) and consists of the following steps: 1) Monolingual word embeddings: An off-the-shelf word embedding algorithm (Bojanowski et al., 2017 ) is used to learn source and target language spaces X and Y .",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_22",
  "x": "**SUMMARY OF <cite>CONNEAU ET AL. (2018)</cite>** We now introduce the method of <cite>Conneau et al. (2018)</cite> . 4 <cite>The approach</cite> builds on existing work on learning a mapping between monolingual word embeddings (Mikolov et al., 2013b; Xing et al., 2015) and consists of the following steps: 1) Monolingual word embeddings: An off-the-shelf word embedding algorithm (Bojanowski et al., 2017 ) is used to learn source and target language spaces X and Y .",
  "y": "background"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_23",
  "x": "It requires frequent words to serve as reliable anchors for learning a translation matrix. In the experiments in <cite>Conneau et al. (2018)</cite> , as well as in ours, the iterative Procrustes refinement improves performance across the board. 4) Cross-domain similarity local scaling (CSLS) is used to expand high-density areas and condense low-density ones, for more accurate nearest neighbor calculation, CSLS reduces the hubness problem in high-dimensional spaces (Radovanovi\u0107 et al., 2010; Dinu et al., 2015) .",
  "y": "similarities"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_24",
  "x": "Instead of learning cross-lingual embeddings completely without supervision, we can extract inexpensive supervision signals by harvesting identically spelled words as in, e.g. (Artetxe et al., 2017; Smith et al., 2017) . Specifically, we use identically spelled words that occur in the vocabularies of both languages as bilingual seeds, without employing any additional transliteration or lemmatization/normalization methods. Using this seed dictionary, we then run the refinement step using <cite>Procrustes analysis</cite> of <cite>Conneau et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_25",
  "x": "**EXPERIMENTS** In the following experiments, we investigate the robustness of unsupervised cross-lingual word embedding learning, varying the language pairs, monolingual corpora, hyper-parameters, etc., to obtain a better understanding of when and why unsupervised <cite>BDI</cite> works. Task: Bilingual dictionary induction After the shared cross-lingual space is induced, given a list of N source language words x u,1 , . . . , x u,N , the task is to find a target language word t for each query word x u relying on the representations in the space.",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_26",
  "x": "t i is the target language word closest to the source language word x u,i in the induced cross-lingual space, also known as the cross-lingual nearest neighbor. The set of learned N (x u,i , t i ) pairs is then run against a gold standard dictionary. We use bilingual dictionaries compiled by <cite>Conneau et al. (2018)</cite> as gold standard, and adopt <cite>their</cite> evaluation procedure: each test set in each language consists of 1500 gold translation pairs.",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_27",
  "x": "We use bilingual dictionaries compiled by <cite>Conneau et al. (2018)</cite> as gold standard, and adopt <cite>their</cite> evaluation procedure: each test set in each language consists of 1500 gold translation pairs. We rely on CSLS for retrieving the nearest neighbors, as it consistently outperformed the cosine similarity in all our experiments. Following a standard evaluation practice (Vuli\u0107 and Moens, 2013; Mikolov et al., 2013b; <cite>Conneau et al., 2018</cite>) , we report Precision at 1 scores (P@1): how many times one of the correct translations of a source word w is retrieved as the nearest neighbor of w in the target language.",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_28",
  "x": "**EXPERIMENTAL SETUP** Our default experimental setup closely follows the setup of <cite>Conneau et al. (2018)</cite> . For each language we induce monolingual word embeddings for all languages from their respective tokenized and lowercased Polyglot Wikipedias (Al-Rfou et al., 2013) using fastText (Bojanowski et al., 2017) .",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_29",
  "x": "Our fastText setup relies on skip-gram with negative sampling (Mikolov et al., 2013a) with standard hyper-parameters: bag-of-words contexts with the window size 2, 15 negative samples, subsampling rate 10 \u22124 , and character n-gram length Table 2 : Bilingual dictionary induction scores (P@1\u00d7100%) using a) the unsupervised method with adversarial training; b) the supervised method with a bilingual seed dictionary consisting of identical words (shared between the two languages). The third columns lists eigenvector similarities between 10 randomly sampled source language nearest neighbor subgraphs of 10 nodes and the subgraphs of their translations, all from the benchmark dictionaries in <cite>Conneau et al. (2018)</cite> . 3-6.",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_31",
  "x": "**<cite>CONNEAU</cite>** Agglutinative languages with mixed or double marking show more morphological variance with content words, and we speculate whether unsupervised <cite>BDI</cite> is challenged by this kind of morphological complexity. To evaluate this, we experiment with Estonian and Finnish, and we include Greek, Hungarian, Polish, and Turkish to see how their approach fares on combinations of these two morphological traits.",
  "y": "motivation"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_32",
  "x": "To evaluate this, we experiment with Estonian and Finnish, and we include Greek, Hungarian, Polish, and Turkish to see how their approach fares on combinations of these two morphological traits. The results are quite dramatic. The approach achieves impressive performance for Spanish, one of the languages <cite>Conneau et al. (2018)</cite> include in <cite>their</cite> paper. For the languages we add here, performance is less impressive.",
  "y": "motivation extends"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_34",
  "x": "In general, unsupervised <cite>BDI</cite>, using the approach in <cite>Conneau et al. (2018)</cite> , seems challenged when pairing En-glish with languages that are not isolating and do not have dependent marking. 6 The promise of zero-supervision models is that we can learn cross-lingual embeddings even for low-resource languages. On the other hand, a similar distribution of embeddings requires languages to be similar.",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_35",
  "x": "Monolingual word embeddings used in <cite>Conneau et al. (2018)</cite> are induced from Wikipedia, a nearparallel corpus. In order to assess the sensitivity of unsupervised <cite>BDI</cite> to the comparability and domain similarity of the monolingual corpora, we replicate the experiments in <cite>Conneau et al. (2018)</cite> using combinations of word embeddings extracted from three different domains: 1) parliamentary proceedings from EuroParl.v7 (Koehn, 2005) , 2) Wikipedia (Al- Rfou et al., 2013) , and 3) the EMEA corpus in the medical domain (Tiedemann, 2009) . We report experiments with three language pairs: English{Spanish, Finnish, Hungarian}.",
  "y": "background"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_36",
  "x": "**IMPACT OF DOMAIN DIFFERENCES** Monolingual word embeddings used in <cite>Conneau et al. (2018)</cite> are induced from Wikipedia, a nearparallel corpus. In order to assess the sensitivity of unsupervised <cite>BDI</cite> to the comparability and domain similarity of the monolingual corpora, we replicate the experiments in <cite>Conneau et al. (2018)</cite> using combinations of word embeddings extracted from three different domains: 1) parliamentary proceedings from EuroParl.v7 (Koehn, 2005) , 2) Wikipedia (Al- Rfou et al., 2013) , and 3) the EMEA corpus in the medical domain (Tiedemann, 2009) .",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_37",
  "x": "For each pair of monolingual corpora, we compute their domain (dis)similarity by calculating the Jensen-Shannon divergence (El-Gamal, 1991) , based on term distributions. 8 We show the results of unsupervised <cite>BDI</cite> in Figures 2g-i .",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_38",
  "x": "<cite>Conneau et al. (2018)</cite> use the same hyperparameters for inducing embeddings for all languages. This is of course always practically possible, but we are interested in seeing whether <cite>their</cite> approach works on pre-trained embeddings induced with possibly very different hyper-parameters. We focus on two hyper-parameters: context windowsize (win) and the parameter controlling the number of n-gram features in the fastText model (chn), while at the same time varying the underlying algorithm: skip-gram vs. cbow.",
  "y": "background"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_39",
  "x": "<cite>Conneau et al. (2018)</cite> use the same hyperparameters for inducing embeddings for all languages. This is of course always practically possible, but we are interested in seeing whether <cite>their</cite> approach works on pre-trained embeddings induced with possibly very different hyper-parameters. We focus on two hyper-parameters: context windowsize (win) and the parameter controlling the number of n-gram features in the fastText model (chn), while at the same time varying the underlying algorithm: skip-gram vs. cbow.",
  "y": "motivation"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_41",
  "x": "**IMPACT OF EVALUATION PROCEDURE** <cite>BDI</cite> models are evaluated on a held-out set of query words. Here, we analyze the performance of the unsupervised approach across different parts-ofspeech, frequency bins, and with respect to query words that have orthographically identical counterparts in the target language with the same or a different meaning.",
  "y": "background"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_42",
  "x": "**IMPACT OF EVALUATION PROCEDURE** <cite>BDI</cite> models are evaluated on a held-out set of query words. Here, we analyze the performance of the unsupervised approach across different parts-ofspeech, frequency bins, and with respect to query words that have orthographically identical counterparts in the target language with the same or a different meaning.",
  "y": "motivation"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_43",
  "x": "---------------------------------- **EVALUATING EIGENVECTOR SIMILARITY** Finally, in order to get a better understanding of the limitations of unsupervised <cite>BDI</cite>, we correlate the graph similarity metric described in \u00a72 (right column of Table 2 ) with performance across languages (left column).",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_44",
  "x": "**EVALUATING EIGENVECTOR SIMILARITY** Finally, in order to get a better understanding of the limitations of unsupervised <cite>BDI</cite>, we correlate the graph similarity metric described in \u00a72 (right column of Table 2 ) with performance across languages (left column). Since we already established that the monolingual word embeddings are far from isomorphic-in contrast with the intuitions motivating previous work (Mikolov et al., 2013b; Barone, 2016; Zhang et al., 2017; <cite>Conneau et al., 2018</cite> )-we would like to establish another diagnostic metric that identifies embedding spaces for which the approach in <cite>Conneau et al. (2018)</cite> is likely to work.",
  "y": "differences motivation"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_46",
  "x": "Cross-lingual word embeddings Cross-lingual word embedding models typically, unlike <cite>Conneau et al. (2018)</cite> , require aligned words, sentences, or documents (Levy et al., 2017) . Most approaches based on word alignments learn an explicit mapping between the two embedding spaces (Mikolov et al., 2013b; Xing et al., 2015) . Recent approaches try to minimize the amount of supervision needed Artetxe et al., 2017; Smith et al., 2017) .",
  "y": "background"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_47",
  "x": "Unsupervised cross-lingual learning Haghighi et al. (2008) were first to explore unsupervised BDI, using features such as context counts and orthographic substrings, and canonical correlation analysis. Recent approaches use adversarial learning (Goodfellow et al., 2014) and employ a discriminator, trained to distinguish between the translated source and the target language space, and a generator learning a translation matrix (Barone, 2016). Zhang et al. (2017) , in addition, use different forms of regularization for convergence, while <cite>Conneau et al. (2018)</cite> uses additional steps to refine the induced embedding space.",
  "y": "background"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_48",
  "x": "Unsupervised machine translation Research on unsupervised machine translation (Lample et al., 2018a; Artetxe et al., 2018; Lample et al., 2018b) has generated a lot of interest recently with a promise to support the construction of MT systems for and between resource-poor languages. All unsupervised NMT methods critically rely on accurate unsupervised <cite>BDI</cite> and back-translation. Models are trained to reconstruct a corrupted version of the source sentence and to translate its translated version back to the source language.",
  "y": "background"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_49",
  "x": "**CONCLUSION** We investigated when unsupervised <cite>BDI</cite> (<cite>Conneau et al., 2018</cite> ) is possible and found that differences in morphology, domains or word embedding algorithms may challenge this approach. Further, we found eigenvector similarity of sampled nearest neighbor subgraphs to be predictive of unsupervised <cite>BDI</cite> performance.",
  "y": "uses"
 },
 {
  "id": "d91913d0c8e669153e8d477e19aef2_50",
  "x": "**CONCLUSION** We investigated when unsupervised <cite>BDI</cite> (<cite>Conneau et al., 2018</cite> ) is possible and found that differences in morphology, domains or word embedding algorithms may challenge this approach. Further, we found eigenvector similarity of sampled nearest neighbor subgraphs to be predictive of unsupervised <cite>BDI</cite> performance.",
  "y": "future_work"
 },
 {
  "id": "d9877fc29c2e4f20805076392a70d0_0",
  "x": "These two lines of research converge in prior work to show, e.g., the increasing association of the lexical item 'gay' with the meaning dimension of homosexuality (Kim et al., 2014;<cite> Kulkarni et al., 2015)</cite> . Neural word embeddings (Mikolov et al., 2013) are probably the most influential among all embedding types (see Section 2.1). Yet, we gathered evidence that the inherent randomness involved in their generation affects the reliability of word neighborhood judgments and demonstrate how this hampers qualitative conclusions based on such models.",
  "y": "background"
 },
 {
  "id": "d9877fc29c2e4f20805076392a70d0_1",
  "x": "Yet, we gathered evidence that the inherent randomness involved in their generation affects the reliability of word neighborhood judgments and demonstrate how this hampers qualitative conclusions based on such models. Our investigation was performed on both historical (for the time span of 1900 to 1904) and contemporary texts (for the time span of 2005 to 2009) in two languages, English and German. It is thus a continuation of prior work, in which we investigated historical English texts only (Hellrich and Hahn, 2016a) , and also influenced by the design decisions of Kim et al. (2014) and <cite>Kulkarni et al. (2015)</cite> which were the first to use word embeddings in diachronic studies.",
  "y": "uses background"
 },
 {
  "id": "d9877fc29c2e4f20805076392a70d0_2",
  "x": "These models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor (Kim et al., 2014; Hellrich and Hahn, 2016b) , or a mapping between models for different points in time must be calculated<cite> (Kulkarni et al., 2015</cite>; Hamilton et al., 2016) . The first approach cannot be performed in parallel and is thus rather time-consuming, if texts are not subsampled. We nevertheless discourage using samples instead of full corpora, as we observed extremely low reliability values between different samples (Hellrich and Hahn, 2016a) .",
  "y": "background"
 },
 {
  "id": "d9877fc29c2e4f20805076392a70d0_3",
  "x": "Word embeddings can be used rather directly for tracking semantic changes, namely by measuring the similarity of word representations generated for one word at different points in time-words which underwent semantic shifts will be dissimilar with themselves. These models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor (Kim et al., 2014; Hellrich and Hahn, 2016b) , or a mapping between models for different points in time must be calculated<cite> (Kulkarni et al., 2015</cite>; Hamilton et al., 2016) . The first approach cannot be performed in parallel and is thus rather time-consuming, if texts are not subsampled.",
  "y": "motivation"
 },
 {
  "id": "d9877fc29c2e4f20805076392a70d0_4",
  "x": "Following <cite>Kulkarni et al. (2015)</cite> , we trained our models on all 5-grams occurring during five consecutive years for the two time spans, 5 1900-1904 and 2005-2009 ; the number of 5-grams 6 for each time span is listed in Table 1 . The two languages share a similar number of 5-grams for 1900-1904, yet not for [2005] [2006] [2007] [2008] [2009] . 5-grams from both corpus parts were lower cased for training.",
  "y": "uses"
 },
 {
  "id": "d9877fc29c2e4f20805076392a70d0_5",
  "x": "During each epoch the learning rate was decreased from 0.025 to 0.0001. The averaged cosine values between word embeddings before and after an epoch are used as a convergence measure c (Kim et al., 2014;<cite> Kulkarni et al., 2015)</cite> . It is defined for a vocabulary with n words and a matrix W containing word embedding vectors (normalized to length 1) for words i from training epochs e and e-1:",
  "y": "uses"
 },
 {
  "id": "d9877fc29c2e4f20805076392a70d0_6",
  "x": "The convergence criterion proposed by <cite>Kulkarni et al. (2015)</cite> , i.e., c = 0.9999, was never reached (this observation might be explained by Kulkarni et al.'s decision not to reset the learning rate for each training epoch, as was done by us and Kim et al. (2014) ). SVD PPMI , which are conceptually not bothered by the reliability problems we discussed here, were not a good fit for the hyperparameters we adopted from <cite>Kulkarni et al. (2015)</cite> . Hamilton et al. (2016) reports similarity accuracy superior to SGNS, whereas for our set-up results in pretests were about 10 percent points worse than skip-gram embeddings, e.g., only 0.35 for 1900-1904 English Fiction.",
  "y": "differences"
 },
 {
  "id": "d9877fc29c2e4f20805076392a70d0_7",
  "x": "SVD PPMI , which are conceptually not bothered by the reliability problems we discussed here, were not a good fit for the hyperparameters we adopted from <cite>Kulkarni et al. (2015)</cite> . Hamilton et al. (2016) reports similarity accuracy superior to SGNS, whereas for our set-up results in pretests were about 10 percent points worse than skip-gram embeddings, e.g., only 0.35 for 1900-1904 English Fiction. Finally, to want to illustrate how this reliability problem affects qualitative conclusions.",
  "y": "differences uses"
 },
 {
  "id": "d987872352e4602fd48936cf2fdab8_0",
  "x": "**INTRODUCTION** Domain classification is a task that predicts the most relevant domain given an input utterance [1] . 1 It is becoming more challenging since recent conversational interaction systems such as Amazon Alexa, Google Assistant, and Microsoft Cortana support more than thousands of domains developed by external developers [4, 3,<cite> 5]</cite> .",
  "y": "background"
 },
 {
  "id": "d987872352e4602fd48936cf2fdab8_1",
  "x": "For example, if a system response of a domain for an input utterance is \"I don't know that one\", the domain is regarded as a negative ground-truth since it fails to handle the utterance. Evaluating on an annotated dataset from the user logs of a large-scale conversation interaction system, we show that the proposed approach significantly improves the domain classification especially when hypothesis reranking is used [14,<cite> 5]</cite> . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "d987872352e4602fd48936cf2fdab8_2",
  "x": "We take a hypothesis reranking approach, which is widely used in large-scale domain classification for higher scalabil- ity [14,<cite> 5]</cite> . Within the approach, a shortlister, which is a light-weighted domain classifier, suggests the most promising k domains as the hypotheses. We train the shortlister along with the added pseudo labels, leveraging negative system responses, and self-distillation, which are described in Section 3. Then a hypothesis reranker selects the final prediction from the k hypotheses enriched with additional input features, which is described in Section 4.",
  "y": "similarities uses"
 },
 {
  "id": "d987872352e4602fd48936cf2fdab8_3",
  "x": "\u03b2 is a hyperparameter for utilizing negative ground-truths, which is set to 0.0002<cite>5</cite> showing the best dev set performance. Figure 2 shows the overall architecture of the hypothesis reranker that is similar to<cite> [5]</cite> . First, we run intent classification and slot filling for the k most confident domains from the shortlister outputs to obtain additional information for those domains [1] .",
  "y": "similarities"
 },
 {
  "id": "d987872352e4602fd48936cf2fdab8_4",
  "x": "8 Then, we compose k hypotheses, each of which is a vector consists of the shortlister confidence score, intent score, Viterbi score of slot-filling, domain vector, intent vector, and the summation of the slot vectors. On top of the k hypothesis vectors, a BiLSTM is utilized for representing contextualized hypotheses and a shared feed-forward neural network is used to obtain final confidence score for each hypothesis. We set k=3 in our experiments following<cite> [5]</cite> .",
  "y": "similarities"
 },
 {
  "id": "d987872352e4602fd48936cf2fdab8_5",
  "x": "**DATASETS** We utilize utterances with explicit invocation patterns from an intelligent conversational system for the model training similarly to<cite> [5]</cite> and [18] . For example, given \"ask {AmbientSounds} to {play thunderstorm sound}\", we extract \"play thunderstorm\" as the input utterance and Ambient Sounds as the ground-truth.",
  "y": "uses"
 },
 {
  "id": "d9c0e641f8ceb61e5d6e416bfc6492_0",
  "x": "It has been done for constituency parsing for example by Collins (1999) but also for dependency parsing for example by <cite>Nilsson et al. (2007)</cite> . <cite>Nilsson et al. (2007)</cite> modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy. In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by <cite>Nilsson et al. (2007)</cite> on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD.",
  "y": "background"
 },
 {
  "id": "d9c0e641f8ceb61e5d6e416bfc6492_1",
  "x": "It has been done for constituency parsing for example by Collins (1999) but also for dependency parsing for example by <cite>Nilsson et al. (2007)</cite> . <cite>Nilsson et al. (2007)</cite> modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy. In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by <cite>Nilsson et al. (2007)</cite> on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD.",
  "y": "background"
 },
 {
  "id": "d9c0e641f8ceb61e5d6e416bfc6492_2",
  "x": "<cite>Nilsson et al. (2007)</cite> modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy. In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by <cite>Nilsson et al. (2007)</cite> on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD. have shown that modifying coordination constructions and verb groups from their representation in the Prague Dependency Treebank (henceforth PDT) to a representation described in Mel\u010duk (1988) (Mel'\u010duk style, henceforth MS) improves dependency parsing for Czech.",
  "y": "uses"
 },
 {
  "id": "d9c0e641f8ceb61e5d6e416bfc6492_3",
  "x": "3. Parse the test data. 4. Transform the parsed data back to the original representation (for comparison with the original gold standard). <cite>Nilsson et al. (2007)</cite> have shown that these same modifications as well as the modification of nonprojective structures helps parsing in four languages.",
  "y": "background"
 },
 {
  "id": "d9c0e641f8ceb61e5d6e416bfc6492_5",
  "x": "In the PDT, main verbs are the head of auxiliary dependencies, as in Figure 1 . <cite>Nilsson et al. (2007)</cite> show that making the auxiliary the head of the dependency as in Figure 2 is useful for parsing Czech and Slovenian. Schwartz et al. (2012) verb groups are easier to parse when the auxiliary is the head (as in PDT) than when the verb is the head (as in MS).",
  "y": "background"
 },
 {
  "id": "d9c0e641f8ceb61e5d6e416bfc6492_6",
  "x": "**GENERAL APPROACH** We will follow the methodology from <cite>Nilsson et al. (2007)</cite> , that is, to transform, parse and then detransform the data so as to compare the original and the transformed model on the original gold standard. The method from Schwartz et al. (2012) which consists in comparing the baseline and the transformed data on their respective gold standard is less relevant here because UD is believed to be a useful representation and that the aim will be to improve parsing within that representation.",
  "y": "uses"
 },
 {
  "id": "d9c0e641f8ceb61e5d6e416bfc6492_7",
  "x": "This is due to inconsistencies in the annotation: verb groups are annotated as a chain of dependency relations. This leaves us with a total of 25 out of the 37 treebanks. For comparability with the study in <cite>Nilsson et al. (2007)</cite> , and because we used a slightly modified version of their algorithm, we also tested the approach on the versions of the Czech and Slovenian treebanks that they worked on, respectively version 1.0 of the PDT (Hajic et al., 2001 ) and the 2006 version of SDT (Deroski et al., 2006) .",
  "y": "differences uses"
 },
 {
  "id": "d9c0e641f8ceb61e5d6e416bfc6492_8",
  "x": "In this paper, we have attempted to reproduce a study by <cite>Nilsson et al. (2007)</cite> that has shown that making auxiliaries heads in verb groups improves parsing but failed to show that those results port to parsing with Universal Dependencies. Contrary to expectations, the study has given evidence that main verbs should stay heads of auxiliary dependency relations for parsing with UD. The benefits of error analyses for such a study have been highlighted because they allow us to shed more light on the different ways in which the transformations affect the parsing output.",
  "y": "uses background"
 },
 {
  "id": "da2429450c8d1f1f3e72383c86ec73_0",
  "x": "Our approach was to build up on the system of the last year's winning approach by NRC Canada 2013 <cite>(Mohammad et al., 2013)</cite> , with some modifications and additions of features, and additional sentiment lexicons. Furthermore, we used a sparse ( 1 -regularized) SVM, instead of the more commonly used 2 -regularization, resulting in a very sparse linear classifier. ----------------------------------",
  "y": "extends"
 },
 {
  "id": "da2429450c8d1f1f3e72383c86ec73_1",
  "x": "This paper describes the classifier submitted to the SemEval-2014 competition (Task 9B). Our approach was to build up on the system of the last year's winning approach by NRC Canada 2013 <cite>(Mohammad et al., 2013)</cite> , with some modifications and additions of features, and additional sentiment lexicons. Furthermore, we used a sparse ( 1 -regularized) SVM, instead of the more commonly used 2 -regularization, resulting in a very sparse linear classifier.",
  "y": "extends uses"
 },
 {
  "id": "da2429450c8d1f1f3e72383c86ec73_2",
  "x": "An overview of the data, which we were able to download, is shown in Table 1 . (Tweets)  1417  494  286  637  Test: Twitter2014  1853  982  202  669  Test: Twitter2013  3813  1572  601  1640  Test: SMS2013  2093  492  394  1207  Test: Tw2014Sarcasm  86  33  40  13  Test: LiveJournal2014 1142  427  304  411 2 Description of Our Approach Compared to the previous NRC Canada 2013 approach <cite>(Mohammad et al., 2013)</cite> , our main changes are the following three: First we use sparse linear classifiers instead of classical dense ones. Secondly, we drop n-gram features completely, in favor of what we call part-of-speech n-grams, which are n-grams where up to two tokens are the original ones, and the rest of the tokens is replaced by their corresponding POS tag (noun, verb, punctuation etc) .",
  "y": "extends"
 },
 {
  "id": "da2429450c8d1f1f3e72383c86ec73_3",
  "x": "All changes are described in more detail in Sections 4 and 3 below. Performance. We tried to reproduce the same classifier as in <cite>(Mohammad et al., 2013)</cite> as a baseline for comparison.",
  "y": "uses"
 },
 {
  "id": "da2429450c8d1f1f3e72383c86ec73_4",
  "x": "Trying to quantify our contributions, when adding all our additional features and tricks described below, the score of our method increases from the baseline of 63.25 to 64.81 (on the Twitter-2013 test set), which is a gain of 1.56 points in F1. Baseline Approach by NRC Canada 2013. Unfortunately our replica system of<cite> Mohammad et al. (2013)</cite> only achieved an F1-score of 63.25 on the Twitter-2013 test set, while their score in the 2013 competition on the same test set was 69.02, nearly 6 points higher in F1.",
  "y": "differences"
 },
 {
  "id": "da2429450c8d1f1f3e72383c86ec73_5",
  "x": "\u2022 Instead of the score itself, we used the sigmoid value s(t) = 1/(1 + e \u2212t )) of each lexicon score. For each lexicon, the 4 scores were the same as in <cite>(Mohammad et al., 2013)</cite> , i.e. per tweet, we use the number of tokens appearing in the lexicon, the sum and the max of the scores, and the last non-zero score. We skipped some features from the baseline approach (because their effect was not significant in our setting): Elongated words (number of words with one character repeated more than two times), and word clustering.",
  "y": "similarities"
 },
 {
  "id": "da2429450c8d1f1f3e72383c86ec73_6",
  "x": "All text was transformed to lowercase (except for those features in <cite>(Mohammad et al., 2013)</cite> which use case information). As usual, URLs were normalized to http://someurl and twitter user IDs to @someuser. We also employed the usual marking of negated contexts of a sentence as in (Pang et al., 2002) , using the list of negation words from Christopher Potts' sentiment tutorial 1 .",
  "y": "uses"
 },
 {
  "id": "da2429450c8d1f1f3e72383c86ec73_7",
  "x": "A sentiment lexicon is a mapping from words (or n-grams) to an association score corresponding to positive or negative sentiment. Such lists can be constructed either from manually labeled data (supervised), or automatically labeled data (unsupervised) as for example tweets with a positive or negative smiley. We used the same set of lexicons as in <cite>(Mohammad et al., 2013)</cite> , with one addition:",
  "y": "extends uses"
 },
 {
  "id": "da2429450c8d1f1f3e72383c86ec73_8",
  "x": "To construct the lexicon, we extracted the POS n-grams (as we described in Section 3.1.1 above) from all texts. In comparison,<cite> Mohammad et al. (2013)</cite> used noncontiguous n-grams (unigram-unigram, unigrambigram, and bigram-bigram pairs) . We only used POS n-grams with 2 tokens kept original, and the remaining ones replaced by their POS tag, with n ranging from 3 to 6.",
  "y": "differences"
 },
 {
  "id": "da2429450c8d1f1f3e72383c86ec73_9",
  "x": "Building the Lexicon. While in <cite>(Mohammad et al., 2013)</cite> , the score for each n-gram was computed using point-wise mutual information (PMI) with the labels, we trained a linear classifier on the same labels instead. The lexicon weights are set as the resulting classifier weights for our (POS) n-grams.",
  "y": "differences"
 },
 {
  "id": "da2429450c8d1f1f3e72383c86ec73_10",
  "x": "Lexicons from Manually Labeled Data. We used the same 3 existing sentiment lexicons as in <cite>(Mohammad et al., 2013)</cite> . All lexicons give a single score for each word (if present in the lexicon).",
  "y": "similarities uses"
 },
 {
  "id": "da2429450c8d1f1f3e72383c86ec73_11",
  "x": "Lexicons from Automatically Labeled Data. The NRC hashtag sentiment lexicon was generated automatically from a set of 775k tweets containing a hashtag of a small predefined list of positive and negative hashtags <cite>(Mohammad et al., 2013)</cite> . Lexicon scores were trained via PMI (point-wise mutual information).",
  "y": "uses"
 },
 {
  "id": "da2429450c8d1f1f3e72383c86ec73_12",
  "x": [
   "---------------------------------- **CONCLUSION** We have described an SVM classifier to detect the sentiment of short texts such as tweets."
  ],
  "y": "extends uses"
 },
 {
  "id": "db1fd6f10a3ee22e22093d50395217_0",
  "x": "The best microaverage Fscore for each class or label for both Structured (S) and Unstructured (U) data are summarised in Table 3 . The other attempt of same 6 way PIBOSO classification on the same dataset is presented by <cite>(Verbeke et al., 2012)</cite> . In this method, the input sentences are pre-processed with a namedentity tagger and dependency parser.",
  "y": "background"
 },
 {
  "id": "db1fd6f10a3ee22e22093d50395217_1",
  "x": "The other attempt of same 6 way PIBOSO classification on the same dataset is presented by <cite>(Verbeke et al., 2012)</cite> . Unlike us and Kim et al. (2011) they have used SVM-HMM 2 for learning.",
  "y": "differences"
 },
 {
  "id": "db1fd6f10a3ee22e22093d50395217_2",
  "x": "Please note that the way we categorised an abstract as structured or unstructured might be a bit different from previous approaches by Kim et al. (2011) and<cite> Verbeke et al. 2012</cite> . If the first sentence in an abstract is a sentence ordering label then we considered the abstract as structured or else unstructured. There are 1000 abstracts containing 11616 sentences in total.",
  "y": "differences"
 },
 {
  "id": "db1fd6f10a3ee22e22093d50395217_3",
  "x": "In the unstructured testing data, we have divided sentences into four equal groups based on their position and mapped them to Aim, Method, Results and Conclusions in this order. Using sentence ordering labels for unstructured abstracts is the main difference compared to earlier methods (Kim et al., 2011;<cite> Verbeke et al., 2012)</cite> . We tried 6 combinations of features which will be discussed in Results section.",
  "y": "differences"
 },
 {
  "id": "db1fd6f10a3ee22e22093d50395217_4",
  "x": "Shared task organisers have used Receiver operating characteristic (ROC) to evaluate the scores. According to ROC our best system scored 93.78% (public board) and 92.16% (private board). However, we compare our results with (Kim et al., 2011) and <cite>(Verbeke et al., 2012)</cite> using the microaveraged F-scores as in Table 3 .",
  "y": "uses"
 },
 {
  "id": "db1fd6f10a3ee22e22093d50395217_5",
  "x": "According to ROC our best system scored 93.78% (public board) and 92.16% (private board). However, we compare our results with (Kim et al., 2011) and <cite>(Verbeke et al., 2012)</cite> using the microaveraged F-scores as in Table 3 . Our system outperformed previous works in unstructured abstracts (22% higher than state-of-the-art).",
  "y": "differences"
 },
 {
  "id": "db1fd6f10a3ee22e22093d50395217_6",
  "x": [
   "In this paper, we have presented a brief overview of our method to classify sentences to support EBM. We showed that structural and lexical features coupled with a CRF classifier is an effective method for dealing with sentence classification tasks. The best features in our setting are found to be words, lexical features such as part-of-speech information, sentence positional features, collocations and sentence ordering labels."
  ],
  "y": "differences"
 },
 {
  "id": "db6794da83b12336ab946e5777346d_0",
  "x": "**ABSTRACT** The title of our talk-an implicit reference to the English clich\u00e9 like a spider weaving her webintends to attract one's attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on Net-like lexical resources (Fellbaum, 1998; Baker et al., 2003;<cite> Gader et al., 2012)</cite> . Our claim is that the inherent graph structure of natural language lexicons not only determine vocabulary acquisition and use (Wolter, 2006) , but also lexicographic activity.",
  "y": "uses"
 },
 {
  "id": "db6794da83b12336ab946e5777346d_1",
  "x": "The title of our talk-an implicit reference to the English clich\u00e9 like a spider weaving her webintends to attract one's attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on Net-like lexical resources (Fellbaum, 1998; Baker et al., 2003;<cite> Gader et al., 2012)</cite> . Our claim is that the inherent graph structure of natural language lexicons not only determine vocabulary acquisition and use (Wolter, 2006) , but also lexicographic activity. In that respect, reflecting on new ways to implement the task of building lexical resources is essential for lexicographers themselves, but also for anyone interested is lexicons as mental structures.",
  "y": "uses"
 },
 {
  "id": "db6794da83b12336ab946e5777346d_2",
  "x": "In our talk, we take the above observations as given, including the fact that lexicography should indeed be targeting virtual dictionaries, generated from non-textual lexical models . We illustrate how the lexicographic process of building graph-based lexical models can benefit from tools that allow lexicographers to wade through the lexical web, following paradigmatic and syntagmatic paths, while simultaneously weaving new links and incrementing the lexical description. Work performed on the French Lexical Network <cite>(Gader et al., 2012</cite> ) will serve to demonstrate how the lexicographic process can be made closer to actual navigation through lexical knowledge by the speaker.",
  "y": "future_work"
 },
 {
  "id": "db6794da83b12336ab946e5777346d_3",
  "x": "The main theoretical and descriptive tool that makes such navigation possible is the system of lexical functions proposed by the Meaning-Text linguistic approach (Mel'\u010duk, 1996) . It induces the multidimensional and non-hierarchical graph structure of the FLN that, we believe, is far better suited for designing lexical resources than hyperonymy-based structures. Computational aspects of the work on the French Lexical Network are dealt with in<cite> (Gader et al., 2012)</cite> .",
  "y": "background"
 },
 {
  "id": "dc6d4eb1870ed5b0bbcbbf6686e5be_0",
  "x": "Understanding the temporal information in natural language text is an important NLP task (Verhagen et al., 2007 (Verhagen et al., , 2010 UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016 Bethard et al., , 2017 . A crucial component is temporal relation (TempRel; e.g., before or after) extraction (Mani et al., 2006; Bethard et al., 2007; Do et al., 2012; Mirza and Tonelli, 2016; <cite>Ning et al., 2017</cite> Ning et al., , 2018a . The TempRels in a document or a sentence can be conveniently modeled as a graph, where the nodes are events, and the edges are labeled by TempRels.",
  "y": "background"
 },
 {
  "id": "dc6d4eb1870ed5b0bbcbbf6686e5be_1",
  "x": "TimeBank (Pustejovsky et al., 2003 ) is a classic TempRel dataset, where the annotators were given a whole article and allowed to label TempRels between any pairs of events. Annotators in this setup usually focus only on salient relations but overlook some others. It has been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (Chambers, 2013;<cite> Ning et al., 2017)</cite> .",
  "y": "motivation"
 },
 {
  "id": "dc6d4eb1870ed5b0bbcbbf6686e5be_2",
  "x": "Annotators in this setup usually focus only on salient relations but overlook some others. It has been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (Chambers, 2013;<cite> Ning et al., 2017)</cite> . Consequently, we categorize TimeBank as a partially annotated dataset (P).",
  "y": "background"
 },
 {
  "id": "dc6d4eb1870ed5b0bbcbbf6686e5be_3",
  "x": "The first system on TBDense was proposed in . Two recent TempRel extraction systems (Mirza and Tonelli, 2016; <cite>Ning et al., 2017</cite> ) also reported their performances on TB-Dense (F) and on TempEval-3 (P) separately. However, there are no existing systems that jointly train on both.",
  "y": "motivation"
 },
 {
  "id": "dc6d4eb1870ed5b0bbcbbf6686e5be_4",
  "x": "As a result, edges in TB-Dense are considered as fully annotated in this paper. The first system on TBDense was proposed in . Two recent TempRel extraction systems (Mirza and Tonelli, 2016; <cite>Ning et al., 2017</cite> ) also reported their performances on TB-Dense (F) and on TempEval-3 (P) separately.",
  "y": "background"
 },
 {
  "id": "dc6d4eb1870ed5b0bbcbbf6686e5be_5",
  "x": "With Eq. (1), different implementations of Line 6 in Algorithm 1 can be described concisely as follows: (i) Local inference is performed by ignoring \"transitivity constraints\". (ii) Global inference can be performed by adding annotated edges in P as additional constraints. Note that Algorithm 1 is only for the learning step of TempRel extraction; as for the inference step of this task, we consistently adopt the standard method by solving Eq. (1), as was done by (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012;<cite> Ning et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "dc6d4eb1870ed5b0bbcbbf6686e5be_6",
  "x": "Then the ILP objective is formulated a\u015d where {r m 3 } is selected based on the general transitivity proposed in<cite> (Ning et al., 2017)</cite> . With Eq. (1), different implementations of Line 6 in Algorithm 1 can be described concisely as follows: (i) Local inference is performed by ignoring \"transitivity constraints\".",
  "y": "uses"
 },
 {
  "id": "dc6d4eb1870ed5b0bbcbbf6686e5be_7",
  "x": "A standard way to perform global inference is to formulate it as an Integer Linear Programming (ILP) problem (Roth and Yih, 2004 ) and enforce transitivity rules as constraints. Then the ILP objective is formulated a\u015d where {r m 3 } is selected based on the general transitivity proposed in<cite> (Ning et al., 2017)</cite> .",
  "y": "background"
 },
 {
  "id": "dc6d4eb1870ed5b0bbcbbf6686e5be_8",
  "x": "We believe that global inference makes better use of the information provided by P; in fact, as we show in Sec. 4, it does perform better than local inference. A standard way to perform global inference is to formulate it as an Integer Linear Programming (ILP) problem (Roth and Yih, 2004 ) and enforce transitivity rules as constraints. Then the ILP objective is formulated a\u015d where {r m 3 } is selected based on the general transitivity proposed in<cite> (Ning et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "dc6d4eb1870ed5b0bbcbbf6686e5be_9",
  "x": "Results are shown in Table 2 , where all systems were compared in terms of their performances on \"same sentence\" edges (both nodes are from the same sentence), \"nearby sentence\" edges, all edges, and the temporal awareness metric used by the TempEval3 workshop. The first part of Table 2 (Systems 1-5) refers to the baseline method proposed at the beginning of Sec. 3, i.e., simply treating P as F and training on their union. The second part (Systems 6-7) serves as an ablation study showing the effect of bootstrapping only. While System 7 can be regarded as a reproduction of<cite> Ning et al. (2017)</cite> , the original paper of<cite> Ning et al. (2017)</cite> achieved an overall score of P=43.0, R=46.4, F=44.7 and an awareness score of P=42.6, R=44.0, and F=43.3, and the proposed System 9 is also better than<cite> Ning et al. (2017)</cite> on all metrics.",
  "y": "differences"
 },
 {
  "id": "dc6d4eb1870ed5b0bbcbbf6686e5be_10",
  "x": "---------------------------------- **DISCUSSION** While incorporating transitivity constraints in inference is widely used,<cite> Ning et al. (2017)</cite> proposed to incorporate these constraints in the learning phase as well.",
  "y": "background"
 },
 {
  "id": "dc6d4eb1870ed5b0bbcbbf6686e5be_11",
  "x": "One of the algorithms proposed in<cite> Ning et al. (2017)</cite> is based on Chang et al. (2012) 's constraint-driven learning (CoDL), which is the same as our intermediate System 7 in Table 2 ; the fact that System 7 is better than System 1 can thus be considered as a reproduction of<cite> Ning et al. (2017)</cite> . Despite the technical similarity, this work is motivated differently and is set to achieve a different goal:<cite> Ning et al. (2017)</cite> tried to enforce the transitivity structure, while the current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data, and learn in the incidental supervision framework. The P used in this work is TBAQ, where only 12% of the edges are annotated.",
  "y": "uses"
 },
 {
  "id": "dc6d4eb1870ed5b0bbcbbf6686e5be_12",
  "x": "One of the algorithms proposed in<cite> Ning et al. (2017)</cite> is based on Chang et al. (2012) 's constraint-driven learning (CoDL), which is the same as our intermediate System 7 in Table 2 ; the fact that System 7 is better than System 1 can thus be considered as a reproduction of<cite> Ning et al. (2017)</cite> . Despite the technical similarity, this work is motivated differently and is set to achieve a different goal:<cite> Ning et al. (2017)</cite> tried to enforce the transitivity structure, while the current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data, and learn in the incidental supervision framework. The P used in this work is TBAQ, where only 12% of the edges are annotated.",
  "y": "similarities differences"
 },
 {
  "id": "dc6d4eb1870ed5b0bbcbbf6686e5be_13",
  "x": "Awareness: the temporal awareness metric used in the TempEval3 workshop, measuring how useful the predicted graphs are (UzZaman et al., 2013) . System 7 can also be considered as a reproduction of<cite> Ning et al. (2017)</cite> (see the discussion in Sec. 5 for details). Two bootstrapping algorithms (standard and constrained) are analyzed and the benefit of P, although with missing annotations, is shown on a benchmark dataset.",
  "y": "uses"
 },
 {
  "id": "dcc866dcfb5f9233170d633d052e8b_0",
  "x": "The use of various synchronous grammar based formalisms has been a trend for statistical machine translation (SMT) (Wu, 1997; Eisner, 2003; Galley et al., 2006; <cite>Chiang, 2007</cite>; Zhang et al., 2008) . The grammar formalism determines the intrinsic capacities and computational efficiency of the SMT systems. To evaluate the capacity of a grammar formalism, two factors, i.e. generative power and expressive power are usually considered (Su and Chang, 1990) .",
  "y": "background"
 },
 {
  "id": "dcc866dcfb5f9233170d633d052e8b_1",
  "x": "For instance, in our investigations for SMT (Section 3.1), the Formally SCFG based hierarchical phrase-based model (hereinafter FSCFG)<cite> (Chiang, 2007)</cite> has a better generalization capability than a Linguistically motivated STSSG based model (hereinafter LSTSSG) (Zhang et al., 2008) , with 5% rules of the former matched by NIST05 test set while only 3.5% rules of the latter matched by the same test set. However, from expressiveness point of view, the former usually results in more ambiguities than the latter. To combine the strengths of different synchronous grammars, this paper proposes a statistical machine translation model based on a synthetic synchronous grammar (SSG) which syncretizes FSCFG and LSTSSG.",
  "y": "uses background"
 },
 {
  "id": "dcc866dcfb5f9233170d633d052e8b_2",
  "x": "The rule extraction in current implementation can be considered as a combination of the ones in<cite> (Chiang, 2007)</cite> and (Zhang et al., 2008) . Given the sentence pair in Figure 1 , some SSG rules can be extracted as illustrated in Figure 2 . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "dcc866dcfb5f9233170d633d052e8b_3",
  "x": "For efficiency, our model approximately search for the single 'best' derivation using beam search as The major challenge for such a SSG-based decoder is how to apply the heterogeneous rules in a derivation. For example,<cite> (Chiang, 2007)</cite> adopts a CKY style span-based decoding while (Liu et al., 2006 ) applies a linguistically syntax node based bottom-up decoding, which are difficult to integrate.",
  "y": "background"
 },
 {
  "id": "dcc866dcfb5f9233170d633d052e8b_4",
  "x": "For significant test, we used Zhang's implementation (Zhang et al., 2004 )(confidence level of 95%). For comparisons, we used the following three baseline systems: LSTSSG An in-house implementation of linguistically motivated STSSG based model similar to (Zhang et al., 2008) . FSCFG An in-house implementation of purely formally SCFG based model similar to<cite> (Chiang, 2007)</cite> .",
  "y": "similarities"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_0",
  "x": "**ABSTRACT** This paper proposes a method for reordering words in a Japanese sentence based on concurrent execution with dependency parsing so that the sentence becomes more readable. Our contributions are summarized as follows: (1) we extend a probablistic model used in the <cite>previous work</cite> which concurrently performs word reordering and dependency parsing; (2) we conducted an evaluation experiment using our semi-automatically constructed evaluation data so that sentences in the data are more likely to be spontaneously written by natives than the automatically constructed evaluation data in the <cite>previous work</cite>.",
  "y": "extends"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_1",
  "x": "To solve the problem, we previously proposed a method for concurrently performing word reordering and dependency parsing and confirmed the effectiveness of their proposed method using evaluation data created by randomly changing the word order in newspaper article sentences <cite>(Yoshida et al., 2014)</cite> . However, since some of the just automatically created sentences are unlikely to be spontaneously written by a native, the evaluation is thought to be not enough. In addition, the probablistic model has room for improvement in targeting at sentences which a native is likely to spontaneously write.",
  "y": "background"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_2",
  "x": "However, since some of the just automatically created sentences are unlikely to be spontaneously written by a native, the evaluation is thought to be not enough. In addition, the probablistic model has room for improvement in targeting at sentences which a native is likely to spontaneously write. This paper proposes a new method on Japanese word reordering based on concurrent execution with dependency parsing by extending the probablistic model proposed by <cite>Yoshida et al. (2014)</cite> , and describes an evaluation experiment using our 1 Bunsetsu is a linguistic unit in Japanese that roughly corresponds to a basic phrase in English.",
  "y": "extends"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_3",
  "x": "We realize the concurrent execution of dependency parsing and word reordering by searching for the maximum-likelihood pattern of word order and dependency structure for an input sentence. We use the same search algorithm as one proposed by <cite>Yoshida et al. (2014)</cite> , which can efficiently find the approximate solution from a huge number of candidates of the pattern by extending CYK algorithm used in conventional dependency parsing. In this paper, we refine the probabilistic model proposed by <cite>Yoshida et al. (2014)</cite> to improve the accuracy.",
  "y": "uses"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_4",
  "x": "In this paper, we refine the probabilistic model proposed by <cite>Yoshida et al. (2014)</cite> to improve the accuracy. Note our method reorders bunsetsus in a sentence without paraphrasing and does not reorder morphemes within a bunsetsu. In addition, we assume there are not any inverted structures and commas in an input sentence.",
  "y": "extends"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_5",
  "x": "The structure S is defined as a tuple S = \u27e8O, D\u27e9 where In the probablistic model proposed by <cite>Yoshida et al. (2014)</cite> , P (S|B) was calculated as follows: We extend <cite>the above model</cite> and calculate P (S|B) as follows: where \u03b1 is a weight and 0 \u2264 \u03b1 \u2264 1. Formula (2) is obtained for the weighted geometric average 2 between the following two Formulas (3) and (4).",
  "y": "uses"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_6",
  "x": "We extend <cite>the above model</cite> and calculate P (S|B) as follows: where \u03b1 is a weight and 0 \u2264 \u03b1 \u2264 1. Formula (2) is obtained for the weighted geometric average 2 between the following two Formulas (3) and (4). Here, Formulas (3) and (4) are derived by expanding P (O, D|B) based on multiplication theorem.",
  "y": "extends"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_7",
  "x": "Conversely, if an input sentence has high adequacy of word order, it is probably better to perform word reordering after dependency parsing, and thus, we can think of calculating P (O, D|B) by Fomula (4). Therefore, we mix Formulas (3) and (4) by adjusting the weight \u03b1 depending on the adequacy of word order in an input sentence, instead of using the constant 0.5 in the previous model proposed by <cite>Yoshida et al. (2014)</cite> . Each factor in Formula (2) is estimated by the maximum entropy method in the same approximation procedure as that of <cite>Yoshida et al. (2014)</cite> .",
  "y": "differences"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_8",
  "x": "Conversely, if an input sentence has high adequacy of word order, it is probably better to perform word reordering after dependency parsing, and thus, we can think of calculating P (O, D|B) by Fomula (4). Therefore, we mix Formulas (3) and (4) by adjusting the weight \u03b1 depending on the adequacy of word order in an input sentence, instead of using the constant 0.5 in the previous model proposed by <cite>Yoshida et al. (2014)</cite> . Each factor in Formula (2) is estimated by the maximum entropy method in the same approximation procedure as that of <cite>Yoshida et al. (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_9",
  "x": "In addition, since spontaneously written sentences have many factors other than word order which decrease the readability, it is difficult to conduct the evaluation with a focus solely on word order. Therefore, <cite>our previous work</cite> (<cite>Yoshida et al., 2014</cite>) artificially generated sentences which were not easy to read, by just automatically changing the word order of newspaper article sentences in Kyoto Text Corpus 3 based on the dependency structure. However, just automatically changing the word order may create sentences which are unlikely to be written by a native.",
  "y": "background"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_10",
  "x": "Therefore, <cite>our previous work</cite> (<cite>Yoshida et al., 2014</cite>) artificially generated sentences which were not easy to read, by just automatically changing the word order of newspaper article sentences in Kyoto Text Corpus 3 based on the dependency structure. However, just automatically changing the word order may create sentences which are unlikely to be written by a native. To solve the problem, we semi-automatically constructed the evaluation data by adding human judgement.",
  "y": "motivation"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_11",
  "x": "To solve the problem, we semi-automatically constructed the evaluation data by adding human judgement. That is, if a subject judges that a sentence generated by automatically changing the word order in the same way as <cite>the previous work</cite> (<cite>Yoshida et al., 2014</cite> ) may have spontaneously written by a native. Our constructed data has 552 sentences including 4,906 bunsetsus.",
  "y": "background"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_12",
  "x": "In the evaluation of dependency parsing, we obtained the dependency accuracy (the percentage of correctly analyzed dependencies out of all dependencies) and sentency accuracy (the percentage of the sentences in which all the dependencies are analyzed correctly), which were defined by Uchimoto et al. (1999) . We compared our method to <cite>Yoshida</cite>'s method (<cite>Yoshida et al., 2014</cite>) and two conventional sequential methods. Both the sequential methods execute the dependency parsing primarily, and then, perform the word reordering by using the conventional word reordering method (Uchimoto et al., 1999) .",
  "y": "uses"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_13",
  "x": "The difference between the two is the method of dependency parsing. The sequential methods 1 and 2 use the dependency parsing method proposed by Uchimoto et al. (2000) and the dependency parsing tool CaboCha 5 , respectively. All of the methods used the same training features as those described in <cite>Yoshida et al. (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_14",
  "x": "Although the purpose of our method is reordering to improve readability, our method generates a dependency structure as a by-product. Here, for reference, we show the experimental results on dependency parsing in Table 2 . The dependency accuracy of our method was significantly lower than that of the two sequential methods, and was higher than that of <cite>Yoshida's method</cite> although there was no significant difference.",
  "y": "differences"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_15",
  "x": "On the other hand, the sentence accuracy of our method was highest among <cite>all the methods</cite> although there were no significant differences in them. As a result of analysis, especially, our method and <cite>Yoshida's method</cite> tended to improve the sentence accuracy very well in case of short sentences. On the other hand, CaboCha, which is a dependency parser in sequential 2, tended not to depend very well on the length of sentences.",
  "y": "differences"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_16",
  "x": "On the other hand, the sentence accuracy of our method was highest among <cite>all the methods</cite> although there were no significant differences in them. As a result of analysis, especially, our method and <cite>Yoshida's method</cite> tended to improve the sentence accuracy very well in case of short sentences. On the other hand, CaboCha, which is a dependency parser in sequential 2, tended not to depend very well on the length of sentences.",
  "y": "similarities"
 },
 {
  "id": "ddd23a034c366b62b53d15128edd45_17",
  "x": "**CONCLUSION** This paper proposed the method for reordering bunsetsus in a Japanese sentence based on executing concurrently with dependency parsing. Especially, we extended the probablistic model proposed by <cite>Yoshida et al. (2014)</cite> to deal with sentences spontaneously written by a native.",
  "y": "extends"
 },
 {
  "id": "debdaa202ebd856991e09e5e00a12b_0",
  "x": "For Subtask 2, a deep learning approach was taken. Specifically, a Bidirectional Long Short-Term Memory (BiLSTM) coupled with a Conditional Random Field (CRF) layer neural network architecture was used to perform Named Entity Recognition to identify the Adverse Drug Reaction mentions. This architecture has been empirically shown to perform well at Named Entity Recognition (NER) tasks <cite>(Lample et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "debdaa202ebd856991e09e5e00a12b_1",
  "x": "Overall, our systems for Tasks 1 and 2 consisted of a combination of (1) lexicon selection and domain-specific feature engineering; (2) classical machine learning techniques such as logistic regression; and (3) neural architectures, including BioBERT and BiLSTM-CRF models. We found simpler models consisting of lexicon selection and classical machine learning models (such as the logistic regression model discussed previously) performed better with limited datasets and offered explainability into feature importance. In the Named Entity Recognition task, we utilized a deep learning approach, given the demonstrated effectiveness of such an architecture in this domain <cite>(Lample et al., 2016)</cite> .",
  "y": "motivation uses"
 },
 {
  "id": "e0b72115e1905226d22876e72aa304_0",
  "x": "**CKB COMPLETION MODEL** The basic structure of our CKB completion model is similar to that of<cite> Li et al. (2016b)</cite> . The main difference between ours and theirs is that our method learns the CKB completion and generation tasks jointly.",
  "y": "similarities"
 },
 {
  "id": "e0b72115e1905226d22876e72aa304_1",
  "x": "The basic structure of our CKB completion model is similar to that of<cite> Li et al. (2016b)</cite> . The main difference between ours and theirs is that our method learns the CKB completion and generation tasks jointly. The completion model only considers the binary classification task, and therefore, it can be easily overfitted when there are not enough training data.",
  "y": "differences"
 },
 {
  "id": "e0b72115e1905226d22876e72aa304_2",
  "x": "Previous model<cite> Li et al. (2016b)</cite> defined a CKB completion model that estimates a confidence score of an arbitrary triple \u27e8t 1 , r, t 2 \u27e9. They used a simple neural network model to formulate score(t 1 , r, t 2 ) \u2208 R. where is a phrase representation of concatenating t 1 and t 2 .",
  "y": "background"
 },
 {
  "id": "e0b72115e1905226d22876e72aa304_3",
  "x": [
   "v r \u2208 R dr is a relation embedding for r. g is a nonlinear activation function. Note that we use ReLU for g. Our model Our CKB completion model is based on Li et al.'s (2016b) ."
  ],
  "y": "extends"
 },
 {
  "id": "e0b72115e1905226d22876e72aa304_4",
  "x": [
   "Li et al. (2016b) formulate the phrase embedding by using attention pooling of LSTM and a bilinear function. where J is the word length of phrase t i , u is a linear transformation vector for calculating the attention vector, x i j and h i j are the j th word embedding and hidden state of the LSTM for phrase t i , and v r is the relation embedding. Note that we calculated v 12 for DNN AVG and DNN LSTM by concatenating v 1 and v 2 ."
  ],
  "y": "background"
 },
 {
  "id": "e0b72115e1905226d22876e72aa304_5",
  "x": "**DATA** For the experiments with English, we used the ConceptNet 100K data released by<cite> Li et al. (2016b)</cite> 1 . The original ConceptNet is a largescale and multi-lingual CKB.",
  "y": "uses"
 },
 {
  "id": "e0b72115e1905226d22876e72aa304_7",
  "x": "**BASELINE METHOD** CKB completion As baselines, we used the DNN AVG and DNN LSTM models<cite> (Li et al., 2016b</cite> ) that were described in Section 3.1. To assess the effectiveness of joint learning, we compared our CKB completion model only (proposed w/o CKB generation) and the joint model (proposed w/ CKB generation).",
  "y": "uses"
 },
 {
  "id": "e0b72115e1905226d22876e72aa304_8",
  "x": "Moreover, we evaluated the effectiveness of simply adding augmentation data, as described in Section 4 to the training data (+auggen). We used the accuracy of binary classification as the evaluation measure. The threshold was determined by using the validation1 data to maximize the accuracy of binary classification for each method, as in<cite> (Li et al., 2016b)</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "e0b72115e1905226d22876e72aa304_9",
  "x": "Does joint learning method improve the accuracy of CKB completion? Table 2 shows the accuracy of the CKB completion model. The bottom two lines show the best performances reported in<cite> (Li et al., 2016b)</cite> . The results indicate that our method improved the accuracy of CKB completion compared with the previous method.",
  "y": "differences"
 },
 {
  "id": "e0b72115e1905226d22876e72aa304_11",
  "x": "First, we generated two types of query pairs: ones generated from ConceptNet (CN gen) and ones generated from Wikipedia (Wiki gen). In CN gen, we used all phrase and relation pairs \u27e8t, r\u27e9 appearing in the test data. In Wiki gen, we used triples extracted by using the POS tag sequence pattern for each relation according to<cite> Li et al. (2016b)</cite> and scored each triple with CKB completion scores.",
  "y": "uses"
 },
 {
  "id": "e0b72115e1905226d22876e72aa304_12",
  "x": "The quality score of each triple of CN gen was quite high. The quality score of Wiki gen was lower than that of CN gen. Since Wikipedia has lots of specific information, it is difficult to extract an input query that is useful for making commonsense knowledge. This tendency is similar to the results reported in Li et al.<cite> (Li et al., 2016b)</cite> .",
  "y": "similarities"
 },
 {
  "id": "e0b72115e1905226d22876e72aa304_13",
  "x": "In particular,<cite> Li et al. (2016b)</cite> and Socher et al. (2013) proposed a simple KBC model for CKB. The formulations of CKB completion in the two studies are the same, and we evaluated<cite> Li et al. (2016b)</cite> 's method as a baseline. Open Information Extraction Open Information Extraction (OpenIE) aims to extract triple knowledge from raw text.",
  "y": "background"
 },
 {
  "id": "e0b72115e1905226d22876e72aa304_14",
  "x": "In particular,<cite> Li et al. (2016b)</cite> and Socher et al. (2013) proposed a simple KBC model for CKB. The formulations of CKB completion in the two studies are the same, and we evaluated<cite> Li et al. (2016b)</cite> 's method as a baseline. Open Information Extraction Open Information Extraction (OpenIE) aims to extract triple knowledge from raw text.",
  "y": "uses"
 },
 {
  "id": "e0e21b4e473ad6fde28378b2dc4f34_0",
  "x": "Their method is a classical learning-to-rank setup where pairwise ranking is applied to a few hundred dense features. Methods to learn sparse word-based translation correspondences from supervised ranking signals have been presented by Bai et al. (2010) and<cite> Sokolov et al. (2013)</cite> . Both approaches work in a cross-lingual setting, the former on Wikipedia data, the latter on patents.",
  "y": "background"
 },
 {
  "id": "e0e21b4e473ad6fde28378b2dc4f34_1",
  "x": "Methods to learn sparse word-based translation correspondences from supervised ranking signals have been presented by Bai et al. (2010) and<cite> Sokolov et al. (2013)</cite> . Both approaches work in a cross-lingual setting, the former on Wikipedia data, the latter on patents. Our approach extends the work of<cite> Sokolov et al. (2013)</cite> by presenting an alternative learningto-rank approach that can be used for supervised model combination to integrate dense and sparse features, and by evaluating both approaches on cross-lingual retrieval for patents and Wikipedia.",
  "y": "extends differences"
 },
 {
  "id": "e0e21b4e473ad6fde28378b2dc4f34_2",
  "x": "is a non-negative importance function on tuples. The algorithm of<cite> Sokolov et al. (2013)</cite> combines batch boosting with bagging over a number of independently drawn bootstrap data samples from R. In each step, the single word pair feature is selected that provides the largest decrease of L exp . The found corresponding models are averaged.",
  "y": "background"
 },
 {
  "id": "e0e21b4e473ad6fde28378b2dc4f34_3",
  "x": "**MODEL COMBINATION** Combination by Borda Counts. The baseline consensus-based voting Borda Count procedure endows each voter with a fixed amount of voting points which he is free to distribute among the scored documents (Aslam and Montague, 2001; <cite>Sokolov et al., 2013)</cite> .",
  "y": "background"
 },
 {
  "id": "e0e21b4e473ad6fde28378b2dc4f34_4",
  "x": "We use BoostCLIR 1 , a Japanese-English (JP-EN) corpus of patent abstracts from the MAREC and NTCIR data<cite> (Sokolov et al., 2013)</cite> . It contains automatically induced relevance judgments for patent abstracts (Graf and Azzopardi, 2008) : EN patents are regarded as relevant with level (3) to a JP query patent, if they are in a family relationship (e.g., same invention), cited by the patent examiner (2), or cited by the applicant (1). Statistics on the ranking data are given in Table 1 .",
  "y": "uses"
 },
 {
  "id": "e0e21b4e473ad6fde28378b2dc4f34_5",
  "x": "For both tasks, DT and PSQ require an SMT baseline system trained on parallel corpora that are disjunct from the ranking data. A JP-EN system was trained on data described and preprocessed by<cite> Sokolov et al. (2013)</cite> , consisting of 1.8M parallel sentences from the NTCIR-7 JP-EN PatentMT subtask (Fujii et al., 2008) and 2k parallel sentences for parameter development from the NTCIR-8 test collection. For Wikipedia, we trained a DE-EN system on 4.1M parallel sentences from Europarl, Common Crawl, and NewsCommentary.",
  "y": "uses"
 },
 {
  "id": "e0e21b4e473ad6fde28378b2dc4f34_6",
  "x": "For the DE-EN system, a 4-gram model was built on the EN side of the training data and the EN Wikipedia documents. Weights for the standard feature set were optimized using cdec's MERT (JP-EN) and MIRA (DE-EN) implementations (Och, 2003; Chiang et al., 2008) . PSQ on patents reuses settings found by<cite> Sokolov et al. (2013)</cite> ; settings for Wikipedia were adjusted on its dev set (n=1000, \u03bb=0.4, L=0, C=1).",
  "y": "uses"
 },
 {
  "id": "e177758a227506bbf9de48f8f35715_0",
  "x": "Notice that these text corpora need not be aligned. The second step is to perform dictionary induction by learning a linear projection, in the form of a matrix, between language vector spaces <cite>(Mikolov et al., 2013b</cite>; Lazaridou et al., 2015) . Our key insight for Bantu languages is that one can create a single vector space for them, obviating the need for learning a projection matrix for each Bantu language.",
  "y": "background"
 },
 {
  "id": "e177758a227506bbf9de48f8f35715_1",
  "x": "In this work we use the skip-gram model with negative sampling to generate word vectors (Mikolov et al., 2013a) . It is one of the most competitive methods for generating word vector representations, as demonstrated by results on a various semantic tasks (Baroni et al., 2014;<cite> Mikolov et al., 2013b)</cite> . ----------------------------------",
  "y": "background"
 },
 {
  "id": "e177758a227506bbf9de48f8f35715_2",
  "x": "---------------------------------- **BILINGUAL DICTIONARY INDUCTION** To induce a bilingual dictionary for a pair of languages, we use the projection matrix approach <cite>(Mikolov et al., 2013b</cite>; Lazaridou et al., 2015) .",
  "y": "uses"
 },
 {
  "id": "e264c45391853fb008c838aa7ccca8_0",
  "x": "This paper proposes an extension of Sumida and Torisawa's method of acquiring hyponymy relations from hierachical layouts in Wikipedia<cite> (Sumida and Torisawa, 2008)</cite> . We extract hyponymy relation candidates (HRCs) from the hierachical layouts in Wikipedia by regarding all subordinate items of an item x in the hierachical layouts as x's hyponym candidates, while <cite>Sumida and Torisawa (2008)</cite> extracted only direct subordinate items of an item x as x's hyponym candidates. We then select plausible hyponymy relations from the acquired HRCs by running a filter based on machine learning with novel features, which even improve the precision of the resulting hyponymy relations.",
  "y": "extends"
 },
 {
  "id": "e264c45391853fb008c838aa7ccca8_1",
  "x": "This paper proposes an extension of Sumida and Torisawa's method of acquiring hyponymy relations from hierachical layouts in Wikipedia<cite> (Sumida and Torisawa, 2008)</cite> . We extract hyponymy relation candidates (HRCs) from the hierachical layouts in Wikipedia by regarding all subordinate items of an item x in the hierachical layouts as x's hyponym candidates, while <cite>Sumida and Torisawa (2008)</cite> extracted only direct subordinate items of an item x as x's hyponym candidates. We then select plausible hyponymy relations from the acquired HRCs by running a filter based on machine learning with novel features, which even improve the precision of the resulting hyponymy relations.",
  "y": "differences"
 },
 {
  "id": "e264c45391853fb008c838aa7ccca8_2",
  "x": "Many NLP researchers have attempted to automatically acquire hyponymy relations from texts (Hearst, 1992; Caraballo, 1999; Mann, 2002; Fleischman et al., 2003; Morin and Jacquemin, 2004; Shinzato and Torisawa, 2004; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Sumida et al., 2006;<cite> Sumida and Torisawa, 2008)</cite> . Most of these methods, however, require tera-scale documents (e.g., a web repository) and powerful computational resources to acquire a wide range of hyponymy relations that include concept-instance relations. On the other hand, <cite>Sumida and Torisawa (2008)</cite> have shown that you could easily obtain numerous hyponymy relations from Wikipedia; in particular, they have acquired more than 0.63 million hyponymy relations only from hierarchical layouts in the 2.2GB Japanese version of Wikipedia (e.g., Figure 1 shows a hierarchical structure of a Wikipedia article shown in Figure 2) .",
  "y": "background"
 },
 {
  "id": "e264c45391853fb008c838aa7ccca8_3",
  "x": "Most of these methods, however, require tera-scale documents (e.g., a web repository) and powerful computational resources to acquire a wide range of hyponymy relations that include concept-instance relations. On the other hand, <cite>Sumida and Torisawa (2008)</cite> have shown that you could easily obtain numerous hyponymy relations from Wikipedia; in particular, they have acquired more than 0.63 million hyponymy relations only from hierarchical layouts in the 2.2GB Japanese version of Wikipedia (e.g., Figure 1 shows a hierarchical structure of a Wikipedia article shown in Figure 2) . Although the reported precision (76.4%) is insufficient for practical applications, the hierarchical structures in Wikipedia are definitely a promising resource to mine hyponymy relations.",
  "y": "background"
 },
 {
  "id": "e264c45391853fb008c838aa7ccca8_4",
  "x": "They acquired more than 2.04 millions of hyponymy relations (relations SUBCLASSOF and TYPE in their paper) from 1.6 millions of Wikipedia articles with a precision of about 95%. Although the above studies extracted hyponymy relations from the English version of Wikipedia, <cite>Sumida and Torisawa (2008)</cite> extracted hyponymy relations from definition sentences, category labels, and hierarchical structures in Wikipedia articles. They reported that the number of hyponymy relations acquired from the hierarchical structures was larger than the number of hyponymy relations acquired from the other resources.",
  "y": "background"
 },
 {
  "id": "e264c45391853fb008c838aa7ccca8_5",
  "x": "Although the above studies extracted hyponymy relations from the English version of Wikipedia, <cite>Sumida and Torisawa (2008)</cite> extracted hyponymy relations from definition sentences, category labels, and hierarchical structures in Wikipedia articles. We thus focus on the hierarchical structures to acquire more hyponymy relations.",
  "y": "uses"
 },
 {
  "id": "e264c45391853fb008c838aa7ccca8_6",
  "x": "---------------------------------- **PROPOSED METHOD** Our method of acquiring hyponymy relations is an extension of the supervised method proposed by <cite>Sumida and Torisawa (2008)</cite> , but differs in the way of enumerating hyponymy relation candidates (hereafter, HRCs) from the hierarchical layouts, and in the features of machine learning.",
  "y": "extends differences"
 },
 {
  "id": "e264c45391853fb008c838aa7ccca8_7",
  "x": "**STEP 1: EXTRACTING HRCS FROM THE HIERARCHICAL STRUCTURES IN WIKIPEDIA ARTICLES** We obtain HRCs by considering the title of each marked-up item as a hypernym candidate, and titles of its all subordinate marked-up items as its hyponym candidates; for example, we extract 'England', 'France', 'Wedgwood', 'Lipton', and 'Fauchon' as hyponym candidates of 'Common tea brands' from the hierarchical structure in Figure 1 . Note that <cite>Sumida and Torisawa (2008)</cite> extracted HRCs by regarding the title of each marked-up item as a hypernym candidate and titles of its direct subordinate marked-up items as its hyponyms; for example, they extracted only 'England' and 'France' as hyponym candidates of 'Common tea brands' from the hierarchical structure in Figure 1 . They also employed patterns shown in Figure 3 (e.g., \"X \" (list of X)) to find plausible hypernyms denoted by X in the pattern.",
  "y": "differences"
 },
 {
  "id": "e264c45391853fb008c838aa7ccca8_8",
  "x": "We select proper hyponymy relations from the HRCs obtained in Step 1 by using SVMs (Vapnik, 1998) as a classifier. In what follows, we briefly review the features proposed by <cite>Sumida and Torisawa (2008)</cite> , and then explain the novel features introduced in this study. We expect that the readers will refer to the literature<cite> (Sumida and Torisawa, 2008)</cite> to see the effect of the features proposed by Sumida and Torisawa.",
  "y": "uses"
 },
 {
  "id": "e264c45391853fb008c838aa7ccca8_9",
  "x": "We expect that the readers will refer to the literature<cite> (Sumida and Torisawa, 2008)</cite> to see the effect of the features proposed by Sumida and Torisawa. In the following explanation, we refer to the hypernym candidate or the hyponym candidate of each HRC as hypernym or hyponym. ----------------------------------",
  "y": "background"
 },
 {
  "id": "e264c45391853fb008c838aa7ccca8_10",
  "x": "The last morpheme is mapped to the dimension that is different from that of the other morphemes. EXP The expression of a hypernym/hyponym itself is mapped to an element in a feature vector, and the corresponding element is set to 1. ATTR Using the attribute set created by <cite>Sumida and Torisawa (2008)</cite> , when a hypernym/hyponym is included as an element of the attribute set, we set a feature corresponding to the element to 1.",
  "y": "uses"
 },
 {
  "id": "e264c45391853fb008c838aa7ccca8_11",
  "x": "This feature reflects the tendency that HRCs acquired from items whose distance is d = 1 are more plausible than the other HRCs. PAT This feature is set to 1 when the hypernym of the given HRC is obtained from a hypernym that matches the patterns in Figure 3 . This reflects Sumida and Torisawa's observation that HRCs whose hypernym matches the patterns are likely to be correct<cite> (Sumida and Torisawa, 2008)</cite> .",
  "y": "similarities"
 },
 {
  "id": "e264c45391853fb008c838aa7ccca8_12",
  "x": "The row titled 'S & T (2008) ' shows the performance of the method proposed by <cite>Sumida and Torisawa (2008)</cite> . The following two rows show the precision of the HRCs acquired by the patterns in Figure 3 (PAT) 7 and that of the results of machine learning (ML). We successfully obtained more than 1.73 million hyponymy relations with 85.2% precision, which greatly outperformed the results of <cite>Sumida and Torisawa (2008)</cite> in terms of both the precision and the number of acquired hyponymy relations.",
  "y": "uses"
 },
 {
  "id": "e264c45391853fb008c838aa7ccca8_13",
  "x": "The row titled 'S & T (2008) ' shows the performance of the method proposed by <cite>Sumida and Torisawa (2008)</cite> . The following two rows show the precision of the HRCs acquired by the patterns in Figure 3 (PAT) 7 and that of the results of machine learning (ML). We successfully obtained more than 1.73 million hyponymy relations with 85.2% precision, which greatly outperformed the results of <cite>Sumida and Torisawa (2008)</cite> in terms of both the precision and the number of acquired hyponymy relations.",
  "y": "differences"
 },
 {
  "id": "e29c7551ea78cb425054963489e1b9_0",
  "x": "Where aligned speech and translations exist, data is typically clean speech clean text, as in news or TED talks, or disfluent speech disfluent translations, as in Fisher or meeting data, where disfluencies were faithfully included in the references for completeness. While some corpora with labeled disfluencies exist (Cho et al., 2014; Burger et al., 2002) , only subsets have been translated and/or released. <cite>Salesky et al. (2018)</cite> introduced a set of fluent references 1 for Fisher Spanish-English, enabling a new task: end-to-end training and evaluation against fluent references.",
  "y": "background"
 },
 {
  "id": "e29c7551ea78cb425054963489e1b9_1",
  "x": "Previous work on disfluency removal has treated it as a sequence labeling task using word or spanlevel labels. However, in some cases, simply removing disfluencies from an utterance can create ill-formed output. Further, corpora can have different translation and annotation schemes: for example for Fisher Spanish-English, translated using Mechanical Turk, <cite>Salesky et al. (2018)</cite> found 268 unique filler words due to spelling and casing.",
  "y": "background"
 },
 {
  "id": "e29c7551ea78cb425054963489e1b9_2",
  "x": "We present the first results translating directly from disfluent source speech to fluent target text. For our experiments, we use Fisher Spanish speech (Graff et al.) and with two sets of English translations<cite> (Salesky et al., 2018</cite>; Post et al., 2013) . The speech dataset comprises telephone conversations between mostly native Spanish speakers recorded in realistic noise conditions.",
  "y": "uses"
 },
 {
  "id": "e29c7551ea78cb425054963489e1b9_3",
  "x": "This data is conversational and disfluent. The original translations faithfully maintain and translate phenomena in the Spanish transcripts such as filler words and hesitations, discourse markers (you know, well, mm), repetitions, corrections and false starts, among others. <cite>Salesky et al. (2018)</cite> introduced a new set of fluent reference translations collected on Mechanical Turk.",
  "y": "background"
 },
 {
  "id": "e29c7551ea78cb425054963489e1b9_4",
  "x": "Using clean references for disfluent data collected by <cite>Salesky et al. (2018)</cite> , we extend their text baseline to speech input and provide first results for direct generation of fluent text from noisy disfluent speech. While fluent training data enables research on this task with end-to-end models, it is unlikely to have this resource for every corpus and domain and it is expensive to collect. In future work, we hope to reduce the dependence on fluent target data during training through decoder pretraining on external non-conversational corpora or multitask learning.",
  "y": "extends"
 },
 {
  "id": "e3ee86bbaca6ae00906e7ec64f0ac0_0",
  "x": "Zhiyuan Chen <cite>[2]</cite> ever proposed a approach to determine which domain dose a word have the sentiment orientation to achieve the goal of lifelong learning. He made a big progress but the supervised learning still is needed. Hence, we will make it forward to let the learning to star with supervised learning but continue with unsupervised learning in the future tasks.",
  "y": "background"
 },
 {
  "id": "e3ee86bbaca6ae00906e7ec64f0ac0_1",
  "x": "Zhiyuan Chen <cite>[2]</cite> ever proposed a approach to determine which domain dose a word have the sentiment orientation to achieve the goal of lifelong learning. He made a big progress but the supervised learning still is needed. Hence, we will make it forward to let the learning to star with supervised learning but continue with unsupervised learning in the future tasks.",
  "y": "motivation"
 },
 {
  "id": "e3ee86bbaca6ae00906e7ec64f0ac0_2",
  "x": "Efficient Lifelong Machine Learning (ELLA) [5] raised by Ruvolo and Eaton. Comparing with the multi-task learning [1] , ELLA is much more efficient. Zhiyuan and Bing <cite>[2]</cite> improved the sentiment classification by involving knowledge.",
  "y": "background"
 },
 {
  "id": "e3ee86bbaca6ae00906e7ec64f0ac0_3",
  "x": "**COMPONENTS OF LML** \u2022 Knowledge Base (KB): The knowledge Base <cite>[2]</cite> mainly used to maintain the previous knowledge. Based on the type of knowledge, it could be divided as Past Information Store (PIS), Meta-Knowledge Miner (MKM) and Meta-Knowledge Store (MKS).",
  "y": "background"
 },
 {
  "id": "e3ee86bbaca6ae00906e7ec64f0ac0_4",
  "x": "\u2022 Knowledge Reasoner (KR): The knowledge reasoner is designed to generate new knowledge upon the achieve knowledge by logic inference. A strict logic design is required so the most of the LML algorithms lack of the component. \u2022 Knowledge-Base Learner (KBL): The Knowledge-Based Learner <cite>[2]</cite> aims to retrieve and transfer previous knowledge to the current task.",
  "y": "background"
 },
 {
  "id": "e3ee86bbaca6ae00906e7ec64f0ac0_5",
  "x": "Previous classical paper <cite>[2]</cite> chose the sentiment classification as the learning target because it is could be regarded as a task as well as a group of subtasks in different domain. These sub-tasks related to each other but a model trained on a domain is unable to perform well in rest domains. The sub-tasked is related means that the knowledge transform among tasks is possible to improve performance. And the distribution of distribution is different requires that our algorithms could know when the knowledge can be used and when can not.",
  "y": "background"
 },
 {
  "id": "e3ee86bbaca6ae00906e7ec64f0ac0_6",
  "x": "Hong and etc. [3] ever discussed that the NLP field is most suitable for the researches of the lifelong learning due to it is easier to extract knowledge and be understood by human. Previous classical paper <cite>[2]</cite> chose the sentiment classification as the learning target because it is could be regarded as a task as well as a group of subtasks in different domain. These sub-tasks related to each other but a model trained on a domain is unable to perform well in rest domains.",
  "y": "motivation"
 },
 {
  "id": "e3ee86bbaca6ae00906e7ec64f0ac0_7",
  "x": "We also need to know well that some words may only have sentiment orientation in some specific domains. \"Lifelong Sentiment Classification\" (\"LSC\" for simple below) <cite>[2]</cite> records that which domain does a word have the sentiment orientation. If a word always has sentiment orientation or has significant orientation in current domain, a high weight will sign to it more than other words.",
  "y": "background"
 },
 {
  "id": "e3ee86bbaca6ae00906e7ec64f0ac0_8",
  "x": "---------------------------------- **CONTRIBUTION OF THIS PAPER** Although LSC <cite>[2]</cite> already raised a lifelong approach, it only aims to improve the classification accuracy.",
  "y": "motivation"
 },
 {
  "id": "e3ee86bbaca6ae00906e7ec64f0ac0_9",
  "x": "If a word has high probability with sentiment orientation, it also will leads to the document have higher probability of sentiment orientation based on the Na\u00efve Bayesian (NB) formula. NB text classification [4] will calculate the probability of each word w given a sentiment orientation (positive or negative). Use use the same formula as LSC <cite>[2]</cite> used below.",
  "y": "uses"
 },
 {
  "id": "e3ee86bbaca6ae00906e7ec64f0ac0_10",
  "x": "Ideally, if we know the P(c + ), P(c \u2212 ) and P(w |c j ) for all words, we can predict the sentiment orientation for all documents. However, above three key components are different in different domains. LSC <cite>[2]</cite> discussed a possible solution of P(w |c j ).",
  "y": "background"
 },
 {
  "id": "e3ee86bbaca6ae00906e7ec64f0ac0_11",
  "x": "---------------------------------- **LIFELONG SEMI-SUPERVISED LEARNING FOR SENTIMENT CLASSIFICATION** Although LSC <cite>[2]</cite> considered the difference among domains, it still is a typical supervised learning approach.",
  "y": "motivation"
 },
 {
  "id": "e3ee86bbaca6ae00906e7ec64f0ac0_12",
  "x": "**EXPERIMENT 6.1 DATASETS** In the experiment, we use the same datasets as LSC <cite>[2]</cite> used. It contains the reviews from 20 domains crawled from Amazon.com and each domain has 1,000 reviews (the distribution of positive and negative reviews is imbalanced).",
  "y": "uses"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_0",
  "x": "Most earlier studies on AMRs have focused on text understanding, i.e. processing texts in order to produce AMRs (Flanigan et al., 2014; Artzi et al., 2015) . However, recently the reverse process, i.e. the generation of texts from AMRs, has started to receive scholarly attention (Flanigan et al., 2016; Song et al., 2016; <cite>Pourdamghani et al., 2016</cite>; Song et al., 2017; Konstas et al., 2017) . We assume that in practical applications, conceptualisation models or dialogue managers (models which decide \"what to say\") output AMRs.",
  "y": "background"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_1",
  "x": "However, recently the reverse process, i.e. the generation of texts from AMRs, has started to receive scholarly attention (Flanigan et al., 2016; Song et al., 2016; <cite>Pourdamghani et al., 2016</cite>; Song et al., 2017; Konstas et al., 2017) . We assume that in practical applications, conceptualisation models or dialogue managers (models which decide \"what to say\") output AMRs. In this paper we study different ways in which these AMRs can be converted into natural language (deciding \"how to say it\").",
  "y": "motivation"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_2",
  "x": "Motivated by this similarity,<cite> Pourdamghani et al. (2016)</cite> proposed an AMR-to-text method that organises some of these concepts and edges in a flat representation, commonly known as Linearisation. Once the linearisation is complete,<cite> Pourdamghani et al. (2016)</cite> map the flat AMR into an English sentence using a Phrase-Based Machine Translation (PBMT) system. This method yields better results than Flanigan et al. (2016) on development and test set from the LDC2014T12 corpus.",
  "y": "background"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_3",
  "x": "In addition,<cite> Pourdamghani et al. (2016)</cite> use PBMT, which is devised for translation but also utilised in other NLP tasks, e.g. text simplification (Wubben et al., 2012; \u0160tajner et al., 2015) . However, these systems have the disadvantage of having many different feature functions, and finding optimal settings for all of them increases the complexity of the problem from an engineering point of view. An alternative MT model has been proposed: Neural Machine Translation (NMT).",
  "y": "background"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_4",
  "x": "To address this,<cite> Pourdamghani et al. (2016)</cite> look for special realisation component for names, dates and numbers in development and test sets and add them on the training set. On the other hand, similar to Konstas et al. (2017) , we delexicalised these constants, replacing the original information for tags (e.g., name1 , quant1 ). A list of tag-values is kept, aiming to identifying the position and to insert the original information in the sentence after the translation step is completed.",
  "y": "background"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_5",
  "x": "Following the aligner of Pourdamghani et al. (2014) ,<cite> Pourdamghani et al. (2016)</cite> clean an AMR by removing some nodes and edges independent of the context. Instead, we are using alignments that may relate a given node or edge to an English word according to the context. In Figure 1 for instance, the first edge :ARG1 is aligned to the preposition to from the sentence, whereas the second edge with a similar value is not aligned to any word in the sentence.",
  "y": "background"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_6",
  "x": "Following the aligner of Pourdamghani et al. (2014) ,<cite> Pourdamghani et al. (2016)</cite> clean an AMR by removing some nodes and edges independent of the context. Instead, we are using alignments that may relate a given node or edge to an English word according to the context. In Figure 1 for instance, the first edge :ARG1 is aligned to the preposition to from the sentence, whereas the second edge with a similar value is not aligned to any word in the sentence.",
  "y": "differences"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_7",
  "x": "---------------------------------- **LINEARISATION** After Compression, we flatten the AMR to serve as input to the translation step, similarly as proposed in<cite> Pourdamghani et al. (2016)</cite> .",
  "y": "similarities"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_8",
  "x": "After Compression, we flatten the AMR to serve as input to the translation step, similarly as proposed in<cite> Pourdamghani et al. (2016)</cite> . We perform a depthfirst search through the AMR, printing the elements according to their visiting order. In a second step, also following<cite> Pourdamghani et al. (2016)</cite>, we implemented a version of the 2-Step Classifier from Lerner and Petrov (2013) to preorder the elements from an AMR according to the target side.",
  "y": "uses"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_9",
  "x": "**MODELS FOR COMPARISON** We compare BLEU scores for some of the AMRto-text systems described in the literature (Flanigan et al., 2016; Song et al., 2016; <cite>Pourdamghani et al., 2016</cite>; Song et al., 2017; Konstas et al., 2017) . Since the models of Flanigan et al. (2016) and<cite> Pourdamghani et al. (2016)</cite> are publicly available, we also use them with the same training data as our models.",
  "y": "uses"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_10",
  "x": "We compare BLEU scores for some of the AMRto-text systems described in the literature (Flanigan et al., 2016; Song et al., 2016; <cite>Pourdamghani et al., 2016</cite>; Song et al., 2017; Konstas et al., 2017) . Since the models of Flanigan et al. (2016) and<cite> Pourdamghani et al. (2016)</cite> are publicly available, we also use them with the same training data as our models. For Flanigan et al. (2016) , we specifically use the version available on GitHub 2 .",
  "y": "uses"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_11",
  "x": "We compare BLEU scores for some of the AMRto-text systems described in the literature (Flanigan et al., 2016; Song et al., 2016; <cite>Pourdamghani et al., 2016</cite>; Song et al., 2017; Konstas et al., 2017) . Since the models of Flanigan et al. (2016) and<cite> Pourdamghani et al. (2016)</cite> are publicly available, we also use them with the same training data as our models. For Flanigan et al. (2016) , we specifically use the version available on GitHub 2 . For<cite> Pourdamghani et al. (2016)</cite> , we use the version available at the first author's website 3 .",
  "y": "uses"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_12",
  "x": "The rules used for the preordering model and the feature functions from the PBMT system are trained using alignments over AMR-sentence pairs from the training set obtained with the aligner described by Pourdamghani et al. (2014) . We do not use lexicalised reordering models as<cite> Pourdamghani et al. (2016)</cite> . Moreover, we tune the weights of the feature functions with MERT (Och, 2003) .",
  "y": "differences"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_13",
  "x": "For illustration, we depicted the BLEU scores of all the AMR-to-text systems described in the literature. The models of Flanigan et al. (2016) and<cite> Pourdamghani et al. (2016)</cite> were officially trained with 10,313 AMR-sentence pairs from the LDC2014T12 corpus, and with 36,521 AMR-sentence pairs from the LDC2016E25 in our study (as our models). The ones of Song et al. (2016) and Song et al. (2017) were trained with 16,833 pairs from the LDC2015E86 corpus.",
  "y": "differences"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_14",
  "x": "We report the results when their model were trained only with AMRsentence pairs from the corpus, and when improved with more 20 million sentences. Among the PBMT models, the Delexicalisation step (+Delex) does not seem to play a role in obtaining better sentences from AMRs. All the models with the preordering method in Linearisation Song et al. (2017) and introduce competitive results with<cite> Pourdamghani et al. (2016)</cite> .",
  "y": "similarities"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_15",
  "x": "Our best model (PBMT-Delex+Compress+Preorder) presents competitive results to<cite> Pourdamghani et al. (2016)</cite> with the advantage that no technique is necessary to overcome data sparsity. Compressing an AMR graph with a classifier shows improvements over a comparable model without compression, but not as strong as preordering the elements in the Linearisation step. In fact, preordering seems to be the most important preprocessing step across all three MT preprocessing metrics.",
  "y": "similarities"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_16",
  "x": "In fact, preordering seems to be the most important preprocessing step across all three MT preprocessing metrics. We note that the preordering success was expected, based on previous results<cite> (Pourdamghani et al., 2016)</cite> . Neural MT The first impression from our NMT experiments is that using Compression consistently deteriorates translations according to all metrics evaluated.",
  "y": "uses"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_17",
  "x": "PBMT models trained on small data sets clearly outperform NMT ones, e.g. Konstas et al. (2017) reported 22.0 BLEU, whereas<cite> Pourdamghani et al. (2016)</cite> 's best model achieved 26.9 BLEU, and our best model performs comparably (26.8 BLEU). Model comparison While the best PBMT models are comparable to the state-of-the-art AMR-totext systems, the current best results are reported by Konstas et al. (2017) , showing the potential of applying deep learning onto large amounts of training data with a 33.8 BLEU-score. However, this result crucially relies on the existence of a very large dataset.",
  "y": "similarities"
 },
 {
  "id": "e4452ce844b74c35f257c916aae120_18",
  "x": "However, this result crucially relies on the existence of a very large dataset. Interestingly, when applied in a situation with limited amounts of data, Konstas et al. (2017) report substantially lower performance scores. In such situations, our PBMT models, like<cite> Pourdamghani et al. (2016)</cite> , look appear to be a good alternative option.",
  "y": "similarities"
 },
 {
  "id": "e48a1eac39987cb2f504b66d135572_0",
  "x": "Advances in these contextual representa-tion based model have created new state-of-the-art performance in many NLP benchmark tasks. Recent studies have shown that training contextual representations in texts from specific domains improves power of the model by capturing domain specific linguistic characteristics. Texts from biomedical publications and electronic medical record have been used to pre-train BERT models for NLP task in this domain and showed considerable improvement in many downstream tasks (Lee et al., 2019;<cite> Alsentzer et al., 2019</cite>; Si et al., 2019) .",
  "y": "background motivation"
 },
 {
  "id": "e48a1eac39987cb2f504b66d135572_1",
  "x": "2019). Later in the year,<cite> Alsentzer et al. (2019)</cite> and Si et al. (2019) published almost at the same time BERT models pre-trained trained on publicly available clinical notes from MIMIC3 either starting from trained parameters of original BERT or BioBERT model and show improvement of clinical NLP tasks. There has been only limited work on federated NLP works in the clincal domain.",
  "y": "background"
 },
 {
  "id": "e48a1eac39987cb2f504b66d135572_2",
  "x": "Different type of clinical notes are available in this corpus including discharge summaries, nursing notes and so on. We included only discharge summaries in our study as previous studies have shown that performance of a model trained on only discharge summaries in this corpus is only marginally worse than model trained on all notes types<cite> (Alsentzer et al., 2019)</cite> . ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "e48a1eac39987cb2f504b66d135572_3",
  "x": "The prepossessing and tokenization pipeline from<cite> Alsentzer et al. 2019</cite><cite> (Alsentzer et al., 2019</cite> was adapted. To train the BERT model, we simulated sending out models with identical initial parameters to all silos. At each silo, a model was trained using only data from that site.",
  "y": "similarities uses"
 },
 {
  "id": "e48a1eac39987cb2f504b66d135572_4",
  "x": "Centralized fine-tuning of the i2b2 NER tasks plateaued after 4 epochs, with the learning rate set at 2e \u2212 5 and a batch size of 32<cite> (Alsentzer et al., 2019)</cite> . When conducting federated fine tuning using the same settings as centralized fine tuning, one epoch of training was conducted in each global cycle and a total of 6 global cycles were conducted when the performance plateaued. Each pre-training global cycle took around 4 hours on a Tesla K80 GPU which has a single precision GFLOPs of 55918736.",
  "y": "background"
 },
 {
  "id": "e48a1eac39987cb2f504b66d135572_5",
  "x": "**DOWNSTREAM TASKS** Clinical BERT pre-trained on MIMIC corpus has been reported to have superior performance on NER tasks in Inside-Outside-Beginning (IOB) format (Ramshaw and Marcus, 1999) using i2b2 2010 (Uzuner et al., 2011) and 2012 (Sun et al., 2013) data<cite> (Alsentzer et al., 2019)</cite> .Original training/development/test splits in the challenges were used. The NER tasks classify if a token is within a span of a class, outside spans of any classes or at beginning of a span of a class.",
  "y": "background"
 },
 {
  "id": "e48a1eac39987cb2f504b66d135572_6",
  "x": "In those cases, the parameters (checkpoint) from original BERT base model trained on Books Corpus and English Wikipedia (Zhu et al., 2015; Devlin et al., 2018) . We also looked at the scenarios where BERTbase model was pre-trained by MIMIC3 discharge summaries in a centralized manner<cite> (Alsentzer et al., 2019)</cite> . Lastly, we look at the scenarios where BERTbase model was pre-trained by MIMIC3 discharge summaries in a federated manner.",
  "y": "similarities"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_0",
  "x": "In another attempt, <cite>Jana and Goyal (2018b)</cite> proposed various complex network measures which can be used as features to build a supervised classifier model for co-hyponymy detection, and showed improvements over other baseline approaches. Recently, with the emergence of various network representation learning methods (Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016; Ribeiro et al., 2017) , attempts have been made to convert distributional thesauri network into low dimensional vector space. (Ferret, 2017) apply distributional thesaurus embedding for synonym extraction and expansion tasks whereas Jana and Goyal (2018a) use it to improve the state-of-theart performance of word similarity/relatedness tasks, word analogy task etc.",
  "y": "motivation"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_1",
  "x": "(Ferret, 2017) apply distributional thesaurus embedding for synonym extraction and expansion tasks whereas Jana and Goyal (2018a) use it to improve the state-of-theart performance of word similarity/relatedness tasks, word analogy task etc. Thus, a natural question arises as to whether network embeddings should be more effective than the handcrafted network features used by <cite>Jana and Goyal (2018b)</cite> for cohyponymy detection. Being motivated by this connection, we investigate how the information captured by network representation learning methodologies on distributional thesaurus can be used in discriminating word pairs having co-hyponymy relation from the word pairs having hypernymy, meronymy relation or any random pair of words.",
  "y": "motivation"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_2",
  "x": "This is well captured by the DT. In a recent work, <cite>Jana and Goyal (2018b)</cite> used network features extracted from the DT to detect co-hyponyms. In our approach, we attempt to use embeddings obtained through a network representation learning method such as node2vec (Grover and Leskovec, 2016) when applied over the DT network.",
  "y": "extends"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_3",
  "x": "---------------------------------- **EXPERIMENTAL RESULTS AND ANALYSIS** We perform experiments using three benchmark datasets for co-hyponymy detection (Weeds et al., 2014; Santus et al., 2016; <cite>Jana and Goyal, 2018b)</cite> .",
  "y": "uses"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_4",
  "x": "In a recent work, <cite>Jana and Goyal (2018b)</cite> used network features extracted from the DT to detect co-hyponyms. In our approach, we attempt to use embeddings obtained through a network representation learning method such as node2vec (Grover and Leskovec, 2016) when applied over the DT network. By choosing a flexible notion of a neighborhood and applying a biased random walk procedure, which efficiently explores diverse neighborhoods, node2vec learn representations for each node that organize nodes based on their network roles and/or communities.",
  "y": "uses"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_5",
  "x": "Evaluation results: We evaluate the usefulness of DT embeddings against three benchmark datasets for cohyponymy detection (Weeds et al., 2014; Santus et al., 2016; <cite>Jana and Goyal, 2018b)</cite> , following their experimental setup. We show that the network embeddings outperform the baselines by a huge margin throughout all the experiments, except for co-hyponyms vs. random pairs, where the baselines already have very high accuracy and network embeddings are able to match the results. arXiv:2002.11506v1 [cs.CL] 24 Feb 2020",
  "y": "uses"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_6",
  "x": "(Ferret, 2017) apply distributional thesaurus embedding for synonym extraction and expansion tasks whereas Jana and Goyal (2018a) use it to improve the state-of-theart performance of word similarity/relatedness tasks, word analogy task etc. Thus, a natural question arises as to whether network embeddings should be more effective than the handcrafted network features used by <cite>Jana and Goyal (2018b)</cite> for cohyponymy detection. Being motivated by this connection, we investigate how the information captured by network representation learning methodologies on distributional thesaurus can be used in discriminating word pairs having co-hyponymy relation from the word pairs having hypernymy, meronymy relation or any random pair of words. Evaluation results: We evaluate the usefulness of DT embeddings against three benchmark datasets for cohyponymy detection (Weeds et al., 2014; Santus et al., 2016; <cite>Jana and Goyal, 2018b)</cite> , following their experimental setup.",
  "y": "extends"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_7",
  "x": "We perform experiments using three benchmark datasets for co-hyponymy detection (Weeds et al., 2014; Santus et al., 2016; <cite>Jana and Goyal, 2018b)</cite> . For <cite>each of these</cite>, we follow the same experimental setup as discussed by the authors and compare our method with the method proposed by the author as well as the state-of-the-art models by <cite>Jana and Goyal (2018b)</cite> . We perform the analysis of three datasets to investigate the extent of overlap present in these publicly available benchmark datasets and find out that 45.7% word pairs of dataset prepared by Weeds et al. (2014) are present in dataset ROOT9 prepared by Santus et al. (2016) . This intersection set comprises 27.8% of the ROOT9 dataset. Similarly 36.7% word pairs of dataset prepared by Weeds et al. (2014) are present in the whole dataset prepared by <cite>Jana and Goyal (2018b)</cite> . This intersection set comprises 44.9% of the dataset prepared by <cite>Jana and Goyal (2018b)</cite> .",
  "y": "uses"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_8",
  "x": "The relation between word pair holds if the lin similarity (Lin, 1998) of the word vectors is greater than some threshold p Table 3 : Accuracy scores on a ten-fold cross validation for cohyponym BLESS dataset of our models along with the top two baseline models (one supervised, one semisupervised) described in (Weeds et al., 2014) and models described in <cite>(Jana and Goyal, 2018b )</cite> ---------------------------------- **METHOD**",
  "y": "uses"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_9",
  "x": "The results in Table 4 shows that, for the binary classification task of co-hyponymy vs random pairs, we achieve percentage F1 score of 99.0 with RF CC which is at par with the state-of-the-art models. More importantly, both RF CC and RF ADD beat the baselines with significant margins for the classification task of co-hyponymy vs hypernymy pairs. Table 5 : Accuracy scores on a ten-fold cross validation of models (svmSS, rfALL) proposed by <cite>Jana and Goyal (2018b)</cite> and our models for the dataset prepared by <cite>Jana and Goyal (2018b)</cite> .",
  "y": "uses"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_11",
  "x": "---------------------------------- **CO-HYP VS RANDOM** Co-Hyp vs Hyper (Santus et al., 2016) 97.8 95.7 <cite>(Jana and Goyal, 2018b)</cite> 99 Table 4 : Percentage F1 scores on a ten-fold cross validation of our models along with the best models described in (Santus et al., 2016) and <cite>(Jana and Goyal, 2018b)</cite> for ROOT9 dataset a set of baseline methodologies, the descriptions of which are presented in Table 1 .",
  "y": "uses"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_12",
  "x": "Co-Hyp vs Hyper (Santus et al., 2016) 97.8 95.7 <cite>(Jana and Goyal, 2018b)</cite> 99 Table 4 : Percentage F1 scores on a ten-fold cross validation of our models along with the best models described in (Santus et al., 2016) and <cite>(Jana and Goyal, 2018b)</cite> for ROOT9 dataset a set of baseline methodologies, the descriptions of which are presented in Table 1 . Here, the best model proposed by <cite>Jana and Goyal (2018b)</cite> uses SVM classifier which is fed with structural similarity of the words in the given word pair from the distributional thesaurus network.",
  "y": "uses"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_13",
  "x": "Here, the best model proposed by <cite>Jana and Goyal (2018b)</cite> uses SVM classifier which is fed with structural similarity of the words in the given word pair from the distributional thesaurus network. We see that all the 4 proposed methods perform at par or better than the baselines, and using RF CC gives a 15.4% improvement over the best results reported. 3.2. Experiment-2 (Santus et al., 2016) In the second experiment, we use ROOT9 dataset prepared by Santus et al. (2016) , containing 9,600 labeled pairs extracted from three datasets: EVALution (Santus et al., 2015) , Lenci/Benotto (?) and BLESS (Baroni and Lenci, 2011) .",
  "y": "differences"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_14",
  "x": "Table 4 represents the performance comparison of our models with the best stateof-the-art models reported in (Santus et al., 2016) and <cite>(Jana and Goyal, 2018b)</cite> . Here, the best model proposed by Santus et al. (2016) uses Random Forest classifier which is fed with nine corpus based features like frequency of words, co-occurrence frequency etc., and the best model proposed by <cite>Jana and Goyal (2018b)</cite> use Random Forest classifier which is fed with five complex network features like structural similarity, shortest path etc. computed from the distributional thesaurus network.",
  "y": "uses"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_15",
  "x": "Table 4 represents the performance comparison of our models with the best stateof-the-art models reported in (Santus et al., 2016) and <cite>(Jana and Goyal, 2018b)</cite> . Here, the best model proposed by Santus et al. (2016) uses Random Forest classifier which is fed with nine corpus based features like frequency of words, co-occurrence frequency etc., and the best model proposed by <cite>Jana and Goyal (2018b)</cite> use Random Forest classifier which is fed with five complex network features like structural similarity, shortest path etc. computed from the distributional thesaurus network.",
  "y": "uses"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_16",
  "x": "In the third experiment we use the dataset specifically build for co-hyponymy detection in one of the recent works by <cite>Jana and Goyal (2018b)</cite> . <cite>This dataset</cite> is extracted from BLESS (Baroni and Lenci, 2011) and divided into three small datasets-Co-Hypo vs Hyper, Co-Hypo vs Mero, Co-Hypo Vs Random. Each of these datasets are balanced, containing 1,000 co-hyponymy pairs and 1,000 pairs for the other class.",
  "y": "uses"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_17",
  "x": "Each of these datasets are balanced, containing 1,000 co-hyponymy pairs and 1,000 pairs for the other class. Following the same setup, we report accuracy scores for ten-fold cross validation for each of these three datasets of our models along with the best models (svmSS, rfALL) reported by <cite>Jana and Goyal (2018b)</cite> in Table 5 . <cite>Jana and Goyal (2018b)</cite> use SVM classifier with structural similarity between words in a word pair as feature to obtain svmSS and use Random Forest classifier with five complex network measures computed from distributional thesaurus network as features to obtain rfALL.",
  "y": "uses"
 },
 {
  "id": "e5886e138ce8d84a48e44db3f3d6a1_18",
  "x": "Each of these datasets are balanced, containing 1,000 co-hyponymy pairs and 1,000 pairs for the other class. Following the same setup, we report accuracy scores for ten-fold cross validation for each of these three datasets of our models along with the best models (svmSS, rfALL) reported by <cite>Jana and Goyal (2018b)</cite> in Table 5 . <cite>Jana and Goyal (2018b)</cite> use SVM classifier with structural similarity between words in a word pair as feature to obtain svmSS and use Random Forest classifier with five complex network measures computed from distributional thesaurus network as features to obtain rfALL.",
  "y": "background"
 },
 {
  "id": "e5ef75cd497dd94b4cf818291707df_0",
  "x": "Second, although the feature set is fundamentally a combination of those used in previous works <cite>(Zhang and Clark, 2010</cite>; Huang and Sagae, 2010) , to integrate them in a single incremental framework is not straightforward. Because we must perform decisions of three kinds (segmentation, tagging, and parsing) in an incremental framework, we must adjust which features are to be activated when, and how they are combined with which action labels. We have also found that we must balance the learning rate between features for segmentation and tagging decisions, and those for dependency parsing.",
  "y": "uses"
 },
 {
  "id": "e5ef75cd497dd94b4cf818291707df_1",
  "x": "In contrast, because the Chinese language does not have spaces between words, we fundamentally need to consider the lattice structure of the whole sentence. Therefore, we place no restriction on the segmentation possibilities to consider, and we assess the full potential of the joint segmentation and dependency parsing model. Among the many recent works on joint segmentation and POS tagging for Chinese, the linear-time incremental models by Zhang and Clark (2008) and<cite> Zhang and Clark (2010)</cite> largely inspired our model.",
  "y": "similarities uses"
 },
 {
  "id": "e5ef75cd497dd94b4cf818291707df_2",
  "x": "Zhang and Clark (2008) proposed an incremental joint segmentation and POS tagging model, with an effective feature set for Chinese. However, it requires to computationally expensive multiple beams to compare words of different lengths using beam search. More recently,<cite> Zhang and Clark (2010)</cite> proposed an efficient character-based decoder for their word-based model.",
  "y": "background"
 },
 {
  "id": "e5ef75cd497dd94b4cf818291707df_3",
  "x": "Based on the joint POS tagging and dependency parsing model by Hatori et al. (2011) , we build our joint model to solve word segmentation, POS tagging, and dependency parsing within a single framework. Particularly, we change the role of the shift action and additionally use the append action, inspired by the character-based actions used in the joint segmentation and POS tagging model by<cite> Zhang and Clark (2010)</cite> . The list of actions used is the following:",
  "y": "uses"
 },
 {
  "id": "e5ef75cd497dd94b4cf818291707df_4",
  "x": "Although SH(t) is similar to the one used in Hatori et al. (2011) , now it shifts the first character in the queue as a new word, instead of shifting a word. Following<cite> Zhang and Clark (2010)</cite> , the POS tag is assigned to the word when its first character is shifted, and the word-tag pairs observed in the training data and the closed-set tags (Xia, 2000) are used to prune unlikely derivations. Because 33 tags are defined in the CTB tag set (Xia, 2000) , our model exploits a total of 36 actions.",
  "y": "uses"
 },
 {
  "id": "e5ef75cd497dd94b4cf818291707df_5",
  "x": "In beam search, we use the step index that is associated with each state: the parser states in process are aligned according to the index, and the beam search pruning is applied to those states with the same index. Consequently, for the beam search to function effectively, all states with the same index must be comparable, and all terminal states should have the same step index. We can first think of using the number of shifted characters as the step index, as<cite> Zhang and Clark (2010)</cite> does.",
  "y": "uses"
 },
 {
  "id": "e5ef75cd497dd94b4cf818291707df_6",
  "x": "In our framework, because an action increases the step index by 1 (for SH(t) or RL/RR) or 2 (for A), we need to use two beams to store new states at each step. Theoretically, the computational time is greater than that with the character-based joint segmentation and tagging model by<cite> Zhang and Clark (2010)</cite> by a factor of 2.1, when the same beam size is used.",
  "y": "differences"
 },
 {
  "id": "e5ef75cd497dd94b4cf818291707df_7",
  "x": "**FEATURES** The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model<cite> (Zhang and Clark, 2010)</cite> and dependency parser (Huang and Sagae, 2010) , both of which are used as baseline models in our experiment. However, we must carefully adjust which features are to be activated and when, and how they are combined with which action labels, depending on the type of the features because we intend to perform three tasks in a single incremental framework.",
  "y": "uses"
 },
 {
  "id": "e5ef75cd497dd94b4cf818291707df_8",
  "x": "The list of the features used in our joint model is presented in Table 1 , where S01-S05, W01-W21, and T01-05 are taken from<cite> Zhang and Clark (2010)</cite> , and P01-P28 are taken from Huang and Sagae (2010) . Note that not all features are always considered: each feature is only considered if the action to be performed is included in the list of actions in the \"When to apply\" column. Because S01-S05 are used to represent the likelihood score of substring sequences, they are only used for A and SH(t) without being combined with any action label.",
  "y": "uses"
 },
 {
  "id": "e5ef75cd497dd94b4cf818291707df_9",
  "x": "These features will also be used in our reimplementation of the model by<cite> Zhang and Clark (2010)</cite> . ---------------------------------- **ADJUSTING THE LEARNING RATE OF FEATURES**",
  "y": "uses"
 },
 {
  "id": "e5ef75cd497dd94b4cf818291707df_10",
  "x": "We use the following baseline and proposed models for evaluation. \u2022 SegTag: our reimplementation of the joint segmentation and POS tagging model by<cite> Zhang and Clark (2010)</cite> . Table 5 shows that this reimplementation almost reproduces the accuracy of their implementation.",
  "y": "uses"
 },
 {
  "id": "e5ef75cd497dd94b4cf818291707df_11",
  "x": "We use the following baseline and proposed models for evaluation. tagging (Zhang and Clark, 2008;<cite> Zhang and Clark, 2010)</cite> and dependency parsing (Huang and Sagae, 2010) .",
  "y": "uses"
 },
 {
  "id": "e5ef75cd497dd94b4cf818291707df_12",
  "x": "Table 5 and Table 6 show a comparison of the segmentation and POS tagging accuracies with other state-of-the-art models. \"Zhang '10\" is the incremental model by<cite> Zhang and Clark (2010)</cite> .",
  "y": "uses"
 },
 {
  "id": "e608068f472e7045b682f979fd5295_0",
  "x": "Another closely related work is our own previously proposed method for leveraging on resources available for English to construct resources for a second language<cite> (Mihalcea et al., 2007)</cite> . That method assumed the availability of a bridge between languages, such as a bilingual lexicon or a parallel corpus. Instead, in the method proposed here, we rely exclusively on language-specific resources, and do not make use of any such bilingual resources which may not always be available.",
  "y": "differences background motivation"
 },
 {
  "id": "e608068f472e7045b682f979fd5295_1",
  "x": "More details about this data set are available in<cite> (Mihalcea et al., 2007)</cite> . The sentence-level subjectivity classification results are shown in Table 2 . By using the extracted lexicon alone, we were able to obtain a rule-based subjectivity classifier with an overall F-measure of 61.69%.",
  "y": "uses background"
 },
 {
  "id": "e608068f472e7045b682f979fd5295_3",
  "x": "By comparing the results in Tables 2 and 4 , we observe an absolute significant improvement of 18.03% in the overall F-measure when using the bootstrapping method introduced in this paper, as compared to the translated lexicon. Note that<cite> (Mihalcea et al., 2007</cite> ) also proposed a corpusbased method for subjectivity classification; however that method is supervised and thus not directly comparable with the approach introduced in this paper. Interestingly, the Fmeasure obtained for the classification of subjective sentences is more than double in the case of the bootstrapping method, reflecting the ability of our approach to identify reliable subjective clues.",
  "y": "differences"
 },
 {
  "id": "e68d09937d522dc5acac9637eb2a8b_0",
  "x": "However, it is not a baseNP since it contains two other noun phrases. Two baseNP data sets have been put forward by <cite>(Ramshaw and Marcus, 1995)</cite> . The main data set consist of four sections (15-18) of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) as training material and one section (20) as test material 1.",
  "y": "background"
 },
 {
  "id": "e68d09937d522dc5acac9637eb2a8b_1",
  "x": "A standard data set for this task was put forward at the CoNLL-99 workshop. The noun phrases in this data set are the same as in the Treebank and therefore the baseNPs in this data set are slightly different from the ones in the <cite>(Ramshaw and Marcus, 1995)</cite> data sets.",
  "y": "background"
 },
 {
  "id": "e68d09937d522dc5acac9637eb2a8b_2",
  "x": "First, with the percentage of detected noun phrases that are correct (precision). Second, with the percentage of noun phrases in the data that were found by the classifier (recall). And third, 1This <cite>(Ramshaw and Marcus, 1995)</cite> baseNP data set is available via ftp://ftp.cis.upenn.edu/pub/chunker/ 2Software for generating the data is available from http://lcg-www.uia.ac.be/conl199/npb/ with the FZ=I rate which is equal to (2*precision*recall)/(precision+recall).",
  "y": "background"
 },
 {
  "id": "e68d09937d522dc5acac9637eb2a8b_3",
  "x": "We have used the bracket representation (O+C) in combination with the second baseNP construction method. An alternative representation for baseNPs has been put forward by <cite>(Ramshaw and Marcus, 1995)</cite> . They have defined baseNP recognition as a tagging task: words can be inside a baseNP (1) or outside of baseNPs (O).",
  "y": "background"
 },
 {
  "id": "e68d09937d522dc5acac9637eb2a8b_4",
  "x": "(Tjong Kim Sang and Veenstra, 1999) have presented three variants of this tagging representation. They have used the <cite>(Ramshaw and Marcus, 1995)</cite> representation as well (IOB1).",
  "y": "background"
 },
 {
  "id": "e68d09937d522dc5acac9637eb2a8b_5",
  "x": "We have chosen to use majority voting because it does not require tuning data. We have applied it to the two data sets mentioned in <cite>(Ramshaw and Marcus, 1995)</cite> . The first data set uses WSJ sections 15-18 as training data (211727 tokens) and section 20 as test data (47377 tokens).",
  "y": "uses"
 },
 {
  "id": "e68d09937d522dc5acac9637eb2a8b_6",
  "x": "For this purpose we performed a 10-fold cross validation experiment on the baseNP training data, sections 15-18 of the WSJ part of the Penn Treebank (211727 tokens). Like the data used by <cite>(Ramshaw and Marcus, 1995)</cite> , this data was retagged by the Brill tagger in order to obtain realistic part-of-speech (POS) tags 3. The data was segmented into baseNP parts and non-baseNP parts in a similar fashion as the data used by <cite>(Ramshaw and Marcus, 1995)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "e68d09937d522dc5acac9637eb2a8b_7",
  "x": "<cite>(Ramshaw and Marcus, 1995)</cite> have build a chunker by applying transformation-based learning to sections of the Penn Treebank. Rather than working with bracket structures, they have represented the chunking task as a tagging problem. POS-like tags were used to account for the fact that words were inside or outside chunks.",
  "y": "background"
 },
 {
  "id": "e68d09937d522dc5acac9637eb2a8b_8",
  "x": "(Mufioz et al., 1999) use SNOW, a network of linear units, for recognizing baseNP phrases 6We have applied majority voting of five data representations to the Ramshaw and Marcus data set without using lexical information and the results were: accuracy O: 97.60%, accuracy C: 98.10%, precision: 92.19%, recall: 91.53% and F~=I: 91.86. and SV phrases. They compare two data representations and report that a representation with bracket structures outperforms the IOB tagging representation introduced by <cite>(Ramshaw and Marcus, 1995)</cite> . SNoW reaches the best performance on this task (Fz=I =92.8).",
  "y": "differences"
 },
 {
  "id": "e7c947a02bb0e81d6b6b4b9da74024_0",
  "x": "In this paper we focus on gender bias. Gender bias was demonstrated to be consistent and pervasive across different word embeddings. <cite>Bolukbasi et al. (2016b)</cite> show that using word embeddings for simple analogies surfaces many gender stereotypes.",
  "y": "background"
 },
 {
  "id": "e7c947a02bb0e81d6b6b4b9da74024_1",
  "x": "In addition, they demonstrate that word embeddings reflect actual gender gaps in reality by showing the correlation between the gender association of occupation words and labor-force participation data. Recently, some work has been done to reduce the gender bias in word embeddings, both as a post-processing step<cite> (Bolukbasi et al., 2016b)</cite> and as part of the training procedure (Zhao et al., 2018) . Both works substantially reduce the bias with respect to the same definition: the projection on the gender direction (i.e. \u2212 \u2192 he \u2212 \u2212\u2192 she), introduced in the former.",
  "y": "background"
 },
 {
  "id": "e7c947a02bb0e81d6b6b4b9da74024_2",
  "x": "**GENDER BIAS IN WORD EMBEDDINGS** In what follows we refer to words and their vectors interchangeably. <cite>Bolukbasi et al. (2016b)</cite> define the gender bias of a word w by its projection on the \"gender direction\": \u2212 \u2192 w \u00b7 ( \u2212 \u2192 he \u2212 \u2212\u2192 she), assuming all vectors are normalized.",
  "y": "background"
 },
 {
  "id": "e7c947a02bb0e81d6b6b4b9da74024_3",
  "x": "---------------------------------- **DEFINITION AND EXISTING DEBIASING METHODS** Both <cite>Bolukbasi et al. (2016b)</cite> and Zhao et al. (2018) propose methods for debiasing word embeddings, substantially reducing the bias according to the suggested definition.",
  "y": "background"
 },
 {
  "id": "e7c947a02bb0e81d6b6b4b9da74024_4",
  "x": "Both <cite>Bolukbasi et al. (2016b)</cite> and Zhao et al. (2018) propose methods for debiasing word embeddings, substantially reducing the bias according to the suggested definition. 2 In a seminal work, <cite>Bolukbasi et al. (2016b)</cite> use a post-processing debiasing method. Given a word embedding matrix, they make changes to the word vectors in order to reduce the gender bias as much as possible for all words that are not inherently gendered (e.g. mother, brother, queen).",
  "y": "background"
 },
 {
  "id": "e7c947a02bb0e81d6b6b4b9da74024_5",
  "x": "These works implicitly define what is good gender debiasing: according to <cite>Bolukbasi et al. (2016b)</cite> , there is no gender bias if each nonexplicitly gendered word in the vocabulary is in equal distance to both elements of all explicitly gendered pairs. In other words, if one cannot determine the gender association of a word by looking at its projection on any gendered pair. In Zhao et al. (2018) the definition is similar, but restricted to projections on the gender-direction.",
  "y": "background"
 },
 {
  "id": "e7c947a02bb0e81d6b6b4b9da74024_6",
  "x": "We refer to the word embeddings of the previous works as HARD-DEBIASED<cite> (Bolukbasi et al., 2016b)</cite> and GN-GLOVE (gender-neutral GloVe) counterparts in a predefined set. (Zhao et al., 2018) . For each debiased word embedding we quantify the hidden bias with respect to the biased version.",
  "y": "similarities"
 },
 {
  "id": "e7c947a02bb0e81d6b6b4b9da74024_7",
  "x": "For HARD-DEBIASED we compare to the embeddings before applying the debiasing procedure. For GN-GLOVE we compare to embedding trained with standard GloVe on the same corpus. 6 Unless otherwise specified, we follow <cite>Bolukbasi et al. (2016b)</cite> and use a reduced version of the vocabulary for both word embeddings: we take the most frequent 50,000 words and phrases and remove words with upper-case letters, digits, or punctuation, and words longer than 20 characters.",
  "y": "extends differences"
 },
 {
  "id": "e7c947a02bb0e81d6b6b4b9da74024_8",
  "x": "Male-and female-biased words cluster together We take the most biased words in the vocabulary according to the original bias (500 malebiased and 500 female-biased 8 ), and cluster them 6 We use the embeddings provided by <cite>Bolukbasi et al. (2016b)</cite> in https://github.com/tolga-b/ debiaswe and by Zhao et al. (2018) in https:// github.com/uclanlp/gn_glove. 7 For HARD-DEBIASED we use first three lists from: https://github.com/tolga-b/debiaswe/ tree/master/data and for GN-GLOVE we use the two lists from: https://github.com/uclanlp/gn_ glove/tree/master/wordlist 8 highest on the two lists for HARD-DEBIASED are 'petite', 'mums', 'bra', 'breastfeeding' and 'sassy' for female and 'rookie', 'burly', 'hero', 'training camp' and 'journeyman' for male. Lowest on the two lists are 'watchdogs', 'watercolors', 'sew', 'burqa', 'diets' for female and 'teammates', 'playable', 'grinning', 'knee surgery', 'impersonation' for male.",
  "y": "similarities uses"
 },
 {
  "id": "e7c947a02bb0e81d6b6b4b9da74024_9",
  "x": "Professions We consider the list of professions used in <cite>Bolukbasi et al. (2016b)</cite> and Zhao et al. (2018) 10 in light of the neighbours-based bias definition. Figure 2 plots the professions, with axis X being the original bias and axis Y being the number of male neighbors, before and after debiasing. For both methods, there is a clear correlation between the two variables.",
  "y": "uses similarities"
 },
 {
  "id": "e7f972baa73e7ababa28eded3adad9_0",
  "x": "There have been tremendous advancements in digitisation of ancient manuscripts in Sanskrit in the last decade. Numerous initiatives such as the Digital Corpus of Sanskrit 1 , GRETIL 2 , The Sanskrit Library 3 and others from the Sanskrit Linguistic and Computational Linguistic community is a fine example of such efforts (Goyal et al., 2012; <cite>Krishna et al., 2017)</cite> . The digitisation efforts have made the Sanskrit manuscripts easily available in the public domain.",
  "y": "background"
 },
 {
  "id": "e7f972baa73e7ababa28eded3adad9_1",
  "x": "Our approach will help to scale the segmentation process in comparison with the challenges posed by knowledge involved processes in the current systems <cite>(Krishna et al., 2017)</cite> . We only use parallel segmented and unsegmented sentences during training. At run-time, we only require the input sentence.",
  "y": "extends"
 },
 {
  "id": "e7f972baa73e7ababa28eded3adad9_2",
  "x": "To further catalyse the research in word segmentation for Sanskrit, <cite>Krishna et al. (2017)</cite> has released a dataset for the word segmentation task. <cite>The work</cite> releases a dataset of 119,000 sentences in Sanskrit along with the lexical and morphological analysis from a shallow parser. <cite>The work</cite> emphasises the need for not just predicting the inflected word form but also the prediction of the associated morphological information of the word.",
  "y": "background"
 },
 {
  "id": "e7f972baa73e7ababa28eded3adad9_3",
  "x": "<cite>The work</cite> emphasises the need for not just predicting the inflected word form but also the prediction of the associated morphological information of the word. The additional information will be beneficial in further processing of Sanskrit texts, such as Dependency parsing or summarisation <cite>(Krishna et al., 2017)</cite> .So far, no system successfully predicts the morphological information of the words in addition to the final word form. Though Krishna et al. (2016) has designed their system with this requirement in mind and outlined the possible extension of their system for the purpose, the system currently only predicts the final word-form.",
  "y": "future_work"
 },
 {
  "id": "e7f972baa73e7ababa28eded3adad9_4",
  "x": "While there are plenty of sandhied texts available for Sanskrit, it is hard to find parallel or unsandhied texts alone, as it is deterministic to get sandhied text from unsandhied texts. In our case we use 105,000 parallel strings from the Digital Corpus of Sanskrit as released in <cite>Krishna et al. (2017)</cite> . To handle the data sparsity, we adopt a purely engineering based approach for our model.",
  "y": "uses"
 },
 {
  "id": "e7f972baa73e7ababa28eded3adad9_5",
  "x": "---------------------------------- **DATASET** We used a dataset of 107,000 sentences from the <cite>Sanskrit Word Segmentation Dataset</cite> <cite>(Krishna et al., 2017)</cite> .",
  "y": "uses"
 },
 {
  "id": "e7f972baa73e7ababa28eded3adad9_6",
  "x": "This limits the scalability of those systems, as they cannot handle out of vocabulary words. Scalability of such systems is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing. The systems by Krishna et al. (2016) and <cite>Krishna et al. (2017)</cite> assume that the parser by Goyal et al. (2012) , identifies all the possible candidate chunks.",
  "y": "background"
 },
 {
  "id": "e7f972baa73e7ababa28eded3adad9_7",
  "x": "The systems by Krishna et al. (2016) and <cite>Krishna et al. (2017)</cite> assume that the parser by Goyal et al. (2012) , identifies all the possible candidate chunks. Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence. <cite>Krishna et al. (2017)</cite> states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit.",
  "y": "differences"
 },
 {
  "id": "e7f972baa73e7ababa28eded3adad9_8",
  "x": "<cite>Krishna et al. (2017)</cite> states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit. The segmentation task is seen as a means and not an end itself. Here, we overlook this aspect and see the segmentation task as an end in itself.",
  "y": "background"
 },
 {
  "id": "e7f972baa73e7ababa28eded3adad9_9",
  "x": "<cite>Krishna et al. (2017)</cite> states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit. The segmentation task is seen as a means and not an end itself. Here, we overlook this aspect and see the segmentation task as an end in itself.",
  "y": "differences"
 },
 {
  "id": "e7f972baa73e7ababa28eded3adad9_10",
  "x": "One possible solution is to learn 'GibberishVocab' on the set of words rather than sentences. But this leads to increased vocabulary at decoder which is not beneficial, given the scarcity of the data we have. Given the importance of morphological segmentation in morphologically rich languages such as Hebrew and Arabic (Seeker and \u00c7 etinoglu, 2015) , the same applies to the morphologically rich Sanskrit as well <cite>(Krishna et al., 2017)</cite> . But, we leave this work for future.",
  "y": "background"
 },
 {
  "id": "e7f972baa73e7ababa28eded3adad9_11",
  "x": "One possible solution is to learn 'GibberishVocab' on the set of words rather than sentences. But this leads to increased vocabulary at decoder which is not beneficial, given the scarcity of the data we have. Given the importance of morphological segmentation in morphologically rich languages such as Hebrew and Arabic (Seeker and \u00c7 etinoglu, 2015) , the same applies to the morphologically rich Sanskrit as well <cite>(Krishna et al., 2017)</cite> . But, we leave this work for future.",
  "y": "future_work"
 },
 {
  "id": "e831e058f208542af16c1ea236d2c9_0",
  "x": "We explore new feature spaces of a partof-speech (POS) hierarchy and relaxed for rough copy in the experiments. These steps result in an improvement of 43.98% percent relative error reduction in F-score over an earlier best result in edited detection when punctuation is included in both training and testing data<cite> [Charniak and Johnson 2001]</cite> , and 20.44% percent relative error reduction in F-score over the latest best result where punctuation is excluded from the training and testing data [Johnson and Charniak 2004] . ----------------------------------",
  "y": "differences"
 },
 {
  "id": "e831e058f208542af16c1ea236d2c9_1",
  "x": "Early work in this field is primarily based on small and proprietary corpora, which makes the comparison of the proposed methods difficult [Young and Matessa 1991 , Bear et al. 1992 , Heeman & Allen 1994 . Because of the availability of the Switchboard corpus [Godfrey et al. 1992] and other conversational telephone speech (CTS) corpora, there has been an increasing interest in improving the performance of identifying the edited regions for parsing disfluent sentences<cite> [Charniak and Johnson 2001</cite> , Johnson and Charniak 2004 , Liu et al. 2005 . In this paper we describe our effort towards the task of edited region identification with the intention of parsing disfluent sentences in the Switchboard corpus.",
  "y": "background"
 },
 {
  "id": "e831e058f208542af16c1ea236d2c9_2",
  "x": "In addition, we also explore new feature spaces of a part-of-speech hierarchy and extend candidate pools in the experiments. These steps result in a significant improvement in F-score over the earlier best result reported in<cite> [Charniak and Johnson 2001]</cite> , where punctuation is included in both the training and testing data of the Switchboard corpus, and a significant error reduction in F-score over the latest best result [Johnson and Charniak 2004] , where punctuation is ignored in both the training and testing data of the Switchboard corpus. In this paper, we follow the definition of [Shriberg 1994 ] and others for speech repairs: A speech repair is divided into three parts: the reparandum, the part that is repaired; the interregnum, the part that can be either empty or fillers; and the repair/repeat, the part that replaces or repeats the reparandum.",
  "y": "differences"
 },
 {
  "id": "e831e058f208542af16c1ea236d2c9_3",
  "x": "For the reparandum and repair types, we include their distributions with and without punctuation. We include the distributions with punctuation is to match with the baseline system reported in<cite> [Charniak and Johnson 2001]</cite> , where punctuation is included to identify the edited regions. Resent research showed that certain punctuation/prosody marks can be produced when speech signals are available [Liu et al. 2003 ].",
  "y": "similarities motivation"
 },
 {
  "id": "e831e058f208542af16c1ea236d2c9_4",
  "x": "---------------------------------- **FEATURE SPACE SELECTION FOR BOOSTING** We take as our baseline system the work by<cite> [Charniak and Johnson 2001]</cite> .",
  "y": "uses"
 },
 {
  "id": "e831e058f208542af16c1ea236d2c9_6",
  "x": "The weighting factors of the learners are adjusted accordingly. We re-implement the boosting algorithm reported by<cite> [Charniak and Johnson 2001]</cite> where \u03b1 i is the weight to be estimated for feature \u03c6 i .",
  "y": "uses"
 },
 {
  "id": "e831e058f208542af16c1ea236d2c9_7",
  "x": "---------------------------------- **CHARNIAK-JOHNSON APPROACH** In<cite> [Charniak and Johnson 2001]</cite> , identifying edited regions is considered as a classification problem, where each word is classified either as edited or normal.",
  "y": "background"
 },
 {
  "id": "e831e058f208542af16c1ea236d2c9_8",
  "x": "**RELAXING ROUGH COPY** We relax the definition for rough copy, because more than 94% of all edits have both reparandum and repair, while the rough copy defined in<cite> [Charniak and Johnson 2001]</cite> only covers 77.66% of such instances. Two methods are used to relax the rough copy definition.",
  "y": "differences"
 },
 {
  "id": "e831e058f208542af16c1ea236d2c9_9",
  "x": "We conducted a number of experiments to test the effectiveness of our feature space exploration. Since the original code from<cite> [Charniak and Johnson 2001]</cite> is not available, we conducted our first experiment to replicate the result of their baseline system described in section 3. We used the exactly same training and testing data from the Switchboard corpus as in<cite> [Charniak and Johnson 2001]</cite> .",
  "y": "motivation"
 },
 {
  "id": "e831e058f208542af16c1ea236d2c9_10",
  "x": "We used the exactly same training and testing data from the Switchboard corpus as in<cite> [Charniak and Johnson 2001]</cite> . The training subset consists of all files in the sections 2 and 3 of the Switchboard corpus. Section 4 is split into three approximately equal size subsets.",
  "y": "uses similarities"
 },
 {
  "id": "e831e058f208542af16c1ea236d2c9_11",
  "x": "When punctuation is included in both training and testing, the re-established baseline has the precision, recall, and F-score of 94.73%, 68.71% and 79.65%, respectively. These results are comparable with the results from <cite>[Charniak & Johnson 2001]</cite> , i.e., 95.2%, 67.8%, and 79.2% for precision, recall, and f-score, correspondingly. In the subsequent experiments, the set of additional feature spaces described in section 3 are added, step-by-step.",
  "y": "similarities"
 },
 {
  "id": "e834dadbcf08cf14e476b5f5cbf79e_0",
  "x": "The coffee break is arranged within this part. Next, the fourth part focuses on a variety of advanced studies which illustrate how deep Bayesian learning is developed to infer the sophisticated recurrent models for natural language understanding. In particular, the memory network Chien and Lin, 2018) , neural variational learning (<cite>Serban et al., 2017</cite>; Chung et al., 2015) , neural discrete representation (Jang et al., 2016; Maddison et al., 2016; van den Oord et al., 2017) , recurrent ladder network (Rasmus et al., 2015; Pr\u00e9mont-Schwarz et al., 2017; S\u00f8nderby et al., 2016) , stochastic neural network (Fraccaro et al., 2016; Goyal et al., 2017; Shabanian et al., 2017) , Markov recurrent neural network (Venkatraman et al., 2017; Kuo and Chien, 2018) , sequence GAN (Yu et al., 2017) and reinforcement learning (Tegho et al., 2017) are introduced in various deep models which open a window to more practical tasks, e.g. reading comprehension, sentence generation, dialogue system, question answering and machine translation.",
  "y": "background"
 },
 {
  "id": "e92c6b44f4482ca868221bff551d67_0",
  "x": "(Labels in parentheses will be explained later in the paper.) Output annotated with such informative labels underlies all domain-independent question an- swering or shallow semantic interpretation systems (Collins and Miller, 1998; Ge and Mooney, 2005) . We test the hypothesis that a current statistical parser can output such richer information without any degradation of the parser's accuracy on the original parsing task. Briefly, our method consists in augmenting a state-of-the-art statistical parser <cite>(Henderson, 2003)</cite> , whose architecture and properties make it particularly adaptive to new tasks.",
  "y": "extends"
 },
 {
  "id": "e92c6b44f4482ca868221bff551d67_1",
  "x": "Our approach maintains state-of-the-art results in parsing, while also reaching state-of-the-art results in function labelling, by suitably extending a Simple Synchrony Network (SSN) parser <cite>(Henderson, 2003)</cite> into a single integrated system. This is an interesting result, as a task combining function labelling and parsing is more complex than simple parsing. While the function of a constituent and its structural position are often correlated, they some-times diverge.",
  "y": "extends"
 },
 {
  "id": "e92c6b44f4482ca868221bff551d67_2",
  "x": "It is therefore important to choose a statistical parser that can model our augmented labelling problem. We use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers <cite>(Henderson, 2003)</cite> , which crucially do not make any explicit independence assumptions, and learn to smooth across rare feature combinations. They are therefore likely to adapt without much modification to the current problem.",
  "y": "uses"
 },
 {
  "id": "e92c6b44f4482ca868221bff551d67_3",
  "x": "SSN parsers, on the other hand, do not state any explicit independence assumptions: they induce a finite history representation of an unbounded sequence of moves, so that the representation of a move i \u2212 1 is included in the inputs to the represention of the next move i, as explained in more detail in <cite>(Henderson, 2003)</cite> . SSN parsers only impose soft inductive biases to capture relevant properties of the derivation, thereby exhibiting adaptivity to the input. The art of designing SSN parsers consists in selecting and introducing such biases.",
  "y": "background"
 },
 {
  "id": "e92c6b44f4482ca868221bff551d67_4",
  "x": "H03 indicates the model illustrated in <cite>(Henderson, 2003)</cite> . sections 2-21 from the Penn Treebank, validated on section 24, and tested on section 23. All models are trained on parse trees whose labels include function labels.",
  "y": "uses"
 },
 {
  "id": "e92c6b44f4482ca868221bff551d67_5",
  "x": "Both results taking function labels into account (FLABEL) and results not taking them into account (FLABEL-less) are reported. All our models, as well as the parser described in <cite>(Henderson, 2003)</cite> , are run only once. 6 These results are reported in Table 3 .",
  "y": "similarities"
 },
 {
  "id": "e92c6b44f4482ca868221bff551d67_6",
  "x": "The remaining pairs are constituted by words that were already in the original vocabulary and have been retagged, or by tags associated to unknown words. Second, this interpretation of the results is confirmed by comparing different ways of enlarging the vocabulary size input to the SSN. <cite>(Henderson, 2003)</cite> tested the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs.",
  "y": "background"
 },
 {
  "id": "e92c6b44f4482ca868221bff551d67_7",
  "x": "Second, this interpretation of the results is confirmed by comparing different ways of enlarging the vocabulary size input to the SSN. <cite>(Henderson, 2003)</cite> tested the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs. A frequency cutoff of 200 yields a vocabulary of 508 pairs, while a cut-off of 20 yields 4242 pairs, 3734 of which comprise new words.",
  "y": "uses"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_0",
  "x": "In this proposal track paper, we have presented a crowdsourcing-based word embedding evaluation technique that will be more reliable and linguistically justified. The method is designed for intrinsic evaluation and extends the approach proposed in (<cite>Schnabel et al., 2015</cite>) . Our improved evaluation technique captures word relatedness based on the word context.",
  "y": "extends"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_1",
  "x": "In (<cite>Schnabel et al., 2015</cite>) , crowdsourcingbased evaluation was proposed for synonyms or a word relatedness task where six word embedding techniques were evaluated. The <cite>crowdsourcingbased intrinsic evaluation</cite> which tests embeddings for semantic relationship between words focuses on a direct comparison of word embeddings with respect to individual queries. Although <cite>the method</cite> is promising for evaluating different word embeddings, <cite>it</cite> has some shortcomings.",
  "y": "background"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_2",
  "x": "In (<cite>Schnabel et al., 2015</cite>) , crowdsourcingbased evaluation was proposed for synonyms or a word relatedness task where six word embedding techniques were evaluated. The <cite>crowdsourcingbased intrinsic evaluation</cite> which tests embeddings for semantic relationship between words focuses on a direct comparison of word embeddings with respect to individual queries. Although <cite>the method</cite> is promising for evaluating different word embeddings, <cite>it</cite> has some shortcomings.",
  "y": "background"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_3",
  "x": "In (<cite>Schnabel et al., 2015</cite>) , crowdsourcingbased evaluation was proposed for synonyms or a word relatedness task where six word embedding techniques were evaluated. The <cite>crowdsourcingbased intrinsic evaluation</cite> which tests embeddings for semantic relationship between words focuses on a direct comparison of word embeddings with respect to individual queries. Although <cite>the method</cite> is promising for evaluating different word embeddings, <cite>it</cite> has some shortcomings.",
  "y": "motivation"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_4",
  "x": "The <cite>crowdsourcingbased intrinsic evaluation</cite> which tests embeddings for semantic relationship between words focuses on a direct comparison of word embeddings with respect to individual queries. Although <cite>the method</cite> is promising for evaluating different word embeddings, <cite>it</cite> has some shortcomings. Specifically, <cite>it</cite> does not explicitly consider word context.",
  "y": "motivation"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_5",
  "x": "As <cite>the approach</cite> relies on human interpretation of words, it is important to take into account how humans interpret or understand the meaning of a word. Humans usually understand semantic relatedness between words based on the context. Thus, if <cite>the approach</cite> is based only on the word without its context, it will be difficult for humans to understand the meaning of a particular word, and it could result in word sense ambiguity (WSA).",
  "y": "background"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_6",
  "x": "Humans usually understand semantic relatedness between words based on the context. Thus, if <cite>the approach</cite> is based only on the word without its context, it will be difficult for humans to understand the meaning of a particular word, and it could result in word sense ambiguity (WSA). In this paper, we show what are the consequences of the lack of the word context in (<cite>Schnabel et al., 2015</cite>) , and we discuss how to address the resulting challenge.",
  "y": "motivation"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_7",
  "x": "Thus, if <cite>the approach</cite> is based only on the word without its context, it will be difficult for humans to understand the meaning of a particular word, and it could result in word sense ambiguity (WSA). In this paper, we show what are the consequences of the lack of the word context in (<cite>Schnabel et al., 2015</cite>) , and we discuss how to address the resulting challenge. Specifically, we add a sentential context to mitigate word sense ambiguity, and this extension leads to an improved evaluation technique that explicitly accounts for multiple senses of a word.",
  "y": "motivation uses"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_8",
  "x": "---------------------------------- **DETAILS OF THE METHOD** <cite>The method</cite> in (<cite>Schnabel et al., 2015</cite>) started by creating a query inventory which is a pre-selected set of query terms and semantically related target words.",
  "y": "background"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_9",
  "x": "Although the experiments in (<cite>Schnabel et al., 2015</cite>) incorporated participants with adequate knowledge of English, the ambiguity is inherent in the language. This means that evaluations that ignore the context may have impact on the evaluation result. Also, the evaluated word embedding techniques in (<cite>Schnabel et al., 2015</cite>) except TSCCA (Dhillon et al., 2015)-generate one vector for each word, and that makes comparisons between two related words from two embedding techniques difficult.",
  "y": "background"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_10",
  "x": "This means that evaluations that ignore the context may have impact on the evaluation result. Also, the evaluated word embedding techniques in (<cite>Schnabel et al., 2015</cite>) except TSCCA (Dhillon et al., 2015)-generate one vector for each word, and that makes comparisons between two related words from two embedding techniques difficult. For example, the word 'bank' may be embedded by CBOW as a noun in the context of 'he cashed a cheque at the bank' where the related word according to nearest neighbours would be 'financial' or 'finance' whereas the TSCCA might embed the same 'bank' as a noun but in the context of 'they pulled the canoe up on the bank' where related word according to nearest neighbours would be 'slope' or 'incline'.",
  "y": "background"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_11",
  "x": "Before we introduce our extensions in the next section, we investigate how (<cite>Schnabel et al., 2015</cite>) accommodates word sense ambiguity. The Turker is presented with a query word and several related words to choose from. If the options presented to the Turker are from different contexts, the Turker has to choose from several correct senses.",
  "y": "uses"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_12",
  "x": "To achieve such an evaluation, we have first extended the work of (<cite>Schnabel et al., 2015</cite>) to include sentential context to avoid word sense ambiguity faced by a human tester. In our method, every query word is accompanied by a context sentence. We then extended <cite>the method</cite> further so that it is more suitable to evaluate embedding techniques designed for polysemous words with regard to their ability to embed diverse senses.",
  "y": "extends"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_13",
  "x": "We then extended <cite>the method</cite> further so that it is more suitable to evaluate embedding techniques designed for polysemous words with regard to their ability to embed diverse senses. ---------------------------------- **FIRST EXTENSION**",
  "y": "extends"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_14",
  "x": "**FIRST EXTENSION** Our chief idea is to extend the work of (<cite>Schnabel et al., 2015</cite>) by adding a context sentence for each query term. Using a context sentence for resolving word sense ambiguity is not a new concept, and it has been used by numerous researchers, such as (Melamud et al., 2015; Huang et al., 2012; Stetina et al., 1998; Biemann, 2013) .",
  "y": "extends"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_15",
  "x": "In particular, human judgement based approaches, such as (Huang et al., 2012) , have used the sentential context to determine the similarity between two words, and (Biemann, 2013) used sentential context for lexical substitution realising the importance of the word interpretation in the context for crowdsourcing-based evaluations. Due to limited and potentially insufficient embedded vocabulary used to identify a related sense of the query term, we are also proposing to provide another option of 'None of the above' along with the six words. In fact, (<cite>Schnabel et al., 2015</cite>) have already considered 'I don't know the meaning of one (or several) of the words'; however, when the context is in place, there may be a situation when none of the embeddings make a good match for the query term, and in that case 'None of the above' is more appropriate.",
  "y": "background motivation"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_16",
  "x": "Below, we show an example word 'bar' together with its context; the context is extracted from WordNet. To extend the evaluation for multi-sense embedding capabilities of the embedding techniques, we will extend the example setting above by adding multiple test cases for each query word representing different senses. Note that this is not needed in (<cite>Schnabel et al., 2015</cite>) where query words are not annotated.",
  "y": "background"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_17",
  "x": "**MERIT OF OUR EXTENSIONS** At the end of Sec. 2.2, we explained how word sense ambiguity is accommodated in (<cite>Schnabel et al., 2015</cite>) . We argued that <cite>their</cite> evaluation was in expectation with respect to subjective preferences of the Turkers.",
  "y": "background"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_18",
  "x": "At the end of Sec. 2.2, we explained how word sense ambiguity is accommodated in (<cite>Schnabel et al., 2015</cite>) . We argued that <cite>their</cite> evaluation was in expectation with respect to subjective preferences of the Turkers. Additionally, when the context is not provided, the Turkers may even forget about common senses of the query word.",
  "y": "uses background"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_19",
  "x": "---------------------------------- **CONCLUSION** In this paper, a crowdsourcing-based word embedding evaluation technique of (<cite>Schnabel et al., 2015</cite>) was extended to provide data-driven treatment of word sense ambiguity.",
  "y": "extends"
 },
 {
  "id": "e9404db1fbda5dd8c55a40711d06ec_20",
  "x": "In this paper, a crowdsourcing-based word embedding evaluation technique of (<cite>Schnabel et al., 2015</cite>) was extended to provide data-driven treatment of word sense ambiguity. The method of (<cite>Schnabel et al., 2015</cite>) relies on user's subjective and knowledge dependent ability to select 'preferred' meanings whereas our method would deal with this problem selecting explicit contexts for words. The selection is according to the real frequencies of meanings computed from data.",
  "y": "differences"
 },
 {
  "id": "e9779b09826d709f8851550d958df7_0",
  "x": "Multimodal corpora such as Flickr30k [1] or MSCOCO [2] containing images along with natural language captions were made available for research. They were soon extended with speech modality: speech recordings for the captions of Flickr8k were collected by [3] via crowdsourcing; spoken captions for MSCOCO were generated using Google Text-To-Speech (TTS) by [4] and using Voxygen TTS by [5] ; extensions of these corpora to other languages than English, such as Japanese, were also introduced by [6] . These corpora, as well as deep learning models, lead to contributions in multilingual language grounding and learning of shared and multimodal representations with neural networks [4, 7, <cite>8,</cite> 9, 10, 11, 12, 13] .",
  "y": "background"
 },
 {
  "id": "e9779b09826d709f8851550d958df7_1",
  "x": "We also show that our Japanese model developed a language-specific behaviour to detect relevant information by paying attention to particles, as Japanese toddlers do. Moreover, the bilingual corpus allows us to demonstrate that images can be used as pivots to automatically align spoken utterances in two different languages (English and Japanese) without using any transcripts. This preliminary result, in line with previous findings of<cite> [8]</cite> , confirms that neural speech-image models can capture a cross-lingual semantic signal, a first step in the perspective of learning speech-to-speech translation systems without text supervision.",
  "y": "similarities"
 },
 {
  "id": "e9779b09826d709f8851550d958df7_2",
  "x": "We have seen in previous section that attention focuses on nouns and Table 2 suggests that these nouns correspond to the main concept of the paired image. To confirm this trend, we experiment on a crosslingual speech-to-speech retrieval task using images as pivots. This possibility was introduced in<cite> [8]</cite> , but required training jointly or alternatively two speech encoders within the same architecture and a parallel bilingual speech dataset while we experiment with separately trained models for both languages.",
  "y": "extends differences"
 },
 {
  "id": "e9779b09826d709f8851550d958df7_3",
  "x": "In<cite> [8]</cite> , a parallel corpus was needed as the loss functions adopted try to minimise either the distance between captions in two languages or the distance between captions in two languages and the associated image as pivot. As our approach uses two monolingual models, we do not need a parallel corpus. Each monolingual model can be trained on its own dataset featuring images and their spoken description.",
  "y": "background"
 },
 {
  "id": "e9779b09826d709f8851550d958df7_4",
  "x": "To make sure no parallel dataset is used, we trained a new English model on the first half of the train set, and a new Japanese model on the second half. We evaluated our approach on 1k captions of our test corpus to be comparable with<cite> [8]</cite> . 11 At the time of the evaluation, given a speech query in language src which we know 10 Since both image encoders (from English and Japanese) are trained separately, they do not lead to the same representation of an image.",
  "y": "similarities uses"
 },
 {
  "id": "e9779b09826d709f8851550d958df7_5",
  "x": "For comparison, we report<cite> [8]</cite> 's results on English to Hindi (HI) and Hindi to English speech-to-speech retrieval. Chance scores are R@1=.001, R@5=.005, and R@10=.01. Chance for median rank r is 500.5.",
  "y": "similarities"
 },
 {
  "id": "e9779b09826d709f8851550d958df7_6",
  "x": "Chance scores are R@1=.001, R@5=.005, and R@10=.01. Chance for median rank r is 500.5. is paired with image I, we assess the ability of our approach to rank the matching spoken caption in language tgt paired with image I in the top 1, 5, and 10 results and give its median rank r. We report our results in Table 4 as well as results from<cite> [8]</cite> who performed speechto-speech retrieval using crowd-sourced spoken captions in English and Hindi.",
  "y": "similarities"
 },
 {
  "id": "e9779b09826d709f8851550d958df7_7",
  "x": "is paired with image I, we assess the ability of our approach to rank the matching spoken caption in language tgt paired with image I in the top 1, 5, and 10 results and give its median rank r. We report our results in Table 4 as well as results from<cite> [8]</cite> who performed speechto-speech retrieval using crowd-sourced spoken captions in English and Hindi. Our results are surprisingly high given the fact we did not train a bilingual model but used the output of two monolingual models never trained to solve such a task. Nevertheless, it is also important to mention that<cite> [8]</cite> experimented on real speech with multiple speakers while we used synthetic speech with only one voice.",
  "y": "differences"
 },
 {
  "id": "e9779b09826d709f8851550d958df7_8",
  "x": "**ACKNOWLEDGEMENTS** We thank G. Chrupa\u0142a and his team for sharing their code and dataset, as well as for helping us with technical issues. ces so that there would be only one target caption for each query in order to compare our results with<cite> [8]</cite> .",
  "y": "future_work"
 },
 {
  "id": "e99193f62a8f3a9e46dee3cadd786f_0",
  "x": "**DATASET AND FEATURES** Our dataset is a gold standard corpus of 1557 single-and multi-word disorder annotations <cite>(Ogren et al., 2008)</cite> . For training and testing the CRF and SVM models the IOB (inside-outside-begin) notation (Leaman, 2008) was applied.",
  "y": "uses"
 },
 {
  "id": "e99baf9c4b8650f29f410501c5165b_0",
  "x": "Moreover, it should be able to capture the relations between the objects/scenes and express them in a well-formed, humanunderstandable sentence. This challenge has the significance not only in academic research, but also in various applications such as information retrieval and visual question-answering. Some recent research of image captioning take inspiration from neural machine translation systems (NMT) [15] [16] [17] <cite>[18]</cite> that successfully use sequence-to-sequence learning for translation.",
  "y": "background"
 },
 {
  "id": "e99baf9c4b8650f29f410501c5165b_1",
  "x": "Moreover, the simple CNN+RNN pipeline squash the whole input image into a fixed-length embedding vector. This constitutes a major limitation of the basic encoder-decoder architecture. To overcome these limitations both for machine translation and image captioning, some new models were proposed by using the attention mechanism [3, 16,<cite> 18]</cite> .",
  "y": "motivation"
 },
 {
  "id": "e99baf9c4b8650f29f410501c5165b_2",
  "x": "For image captioning, attention mechanisms help the model focus on salient regions of the image while generating descriptions by dynamically updating the image representations at each time step. With this the input image is now represented as a sequence of context vector where the length of the sequence depends on the number of words in the sentence to be generated. Promising results has been published since attention was introduced in [16] then later refined in <cite>[18]</cite> .",
  "y": "background"
 },
 {
  "id": "e99baf9c4b8650f29f410501c5165b_3",
  "x": "We use the Luong style of attention <cite>[18]</cite> which is a refined version of attention mechanism and that to the best of our knowledge, there has not been any published work reporting the performance of an image captioning model that is built following only the encoder-decoder pipeline with Luong style of attention. Furthermore, to demonstrate the usefulness of the guiding LSTM, we also compare the performance of our model with and without the the guiding LSTM in experiments. ----------------------------------",
  "y": "extends differences"
 },
 {
  "id": "e99baf9c4b8650f29f410501c5165b_4",
  "x": "Inspired by the use of attention in sequence-to-sequence learning for machine translation [16,<cite> 18]</cite> , visual attention has been proved to be a very effective way of improving image captioning. Some early research follows this direction, e.g., the model proposed in [3] can focus on important parts of images while generating descriptions. The captioning model in [3] is very similar in spirit to that in [7] , in which visual representation is constructed for sentence parts while the description is being emitted.",
  "y": "motivation"
 },
 {
  "id": "e99baf9c4b8650f29f410501c5165b_5",
  "x": "In our work, we model the distribution p(w i t |X i , w i 1:t\u22121 ; \u03b8) with a LSTM cell wrapped with Luong-style attention mechanism <cite>[18]</cite> . A RNN cell with Luong's attention computes its hidden state h t at each time step based on the current input x t , the previous hidden state h t\u22121 and the previous attention vectorh t\u22121 . The current hidden state h t is combined with the image-side context vector c t to form the final output of the cell which is the current attention vectorh t .",
  "y": "similarities"
 },
 {
  "id": "e99baf9c4b8650f29f410501c5165b_6",
  "x": "**IMAGE FEATURES: CONVOLUTIONAL NEURAL NETWORK** where c t \u2208 R D and \u03b1 t 1 , \u03b1 t 2 , . . . , \u03b1 t K are alignment weights and the function \u03a6 is known as alignment function. In our model, we use the general form described in <cite>[18]</cite> :",
  "y": "extends differences"
 },
 {
  "id": "e99baf9c4b8650f29f410501c5165b_7",
  "x": "The form of the recursive function R (Equation 3) is a critical design choice for generating sequences. In this paper, we use a LSTM cell wrapped with the attention mechanism described in <cite>[18]</cite> to form R. LSTM [24] is a powerful form of recurrent neural network that is widely used now because of its ability to deal with issues like vanishing and exploding gradients. The final form of R is described by the following equations:",
  "y": "extends differences"
 },
 {
  "id": "e9f7d339ccda101000b53d89da4e49_0",
  "x": "Thus, we define our task as predicting the AMR tree structure, given a sequence of words in an NP. The previous method for AMR parsing takes a Train Dev Test  3504 463 398   Table 1 : Statistics of the extracted NP data two-step approach: first identifying distinct concepts (nodes) in the AMR graph, then defining the dependency relations between those concepts<cite> (Flanigan et al., 2014)</cite> . In the concept identification step, unlike POS tagging, one word is sometimes assigned with more than one concept, and the number of possible concepts is far more than the number of possible parts-of-speech.",
  "y": "background"
 },
 {
  "id": "e9f7d339ccda101000b53d89da4e49_1",
  "x": "We extract substructures (subtrees) corresponding to NPs from the AMR Bank (LDC2014T12). In the AMR Bank, there is no alignment between the words and the concepts (nodes) in the AMR graphs. We obtain this alignment by using the rule-based alignment tool by <cite>Flanigan et al. (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "e9f7d339ccda101000b53d89da4e49_2",
  "x": "---------------------------------- **PREVIOUS METHOD FOR AMR ANALYSIS** We adopt the method proposed by <cite>Flanigan et al. (2014)</cite> as our baseline, which is a two-step pipeline method of concept identification step and<cite> (Flanigan et al., 2014)</cite> for a retired plant worker.",
  "y": "uses"
 },
 {
  "id": "e9f7d339ccda101000b53d89da4e49_3",
  "x": "We use the implementation 2 of<cite> (Flanigan et al., 2014)</cite> as our baseline. For the baseline, we use the features of the default settings. The method by <cite>Flanigan et al. (2014)</cite> can only generate the concepts that appear in the training data.",
  "y": "uses"
 },
 {
  "id": "e9f7d339ccda101000b53d89da4e49_4",
  "x": "For the baseline, we use the features of the default settings. The method by <cite>Flanigan et al. (2014)</cite> can only generate the concepts that appear in the training data. On the other hand, our method can generate concepts that do not appear in the training data using the concept generation rules LEMMA, DICT PRED , and DICT NOUN in Table 3 .",
  "y": "differences"
 },
 {
  "id": "eab79e8aa2cbe6f3aeef0018129208_0",
  "x": "****ENHANCING THE INSIDE-OUTSIDE RECURSIVE NEURAL NETWORK RERANKER FOR DEPENDENCY PARSING**** **ABSTRACT** We propose solutions to enhance the Inside-Outside Recursive Neural Network (IORNN) reranker of<cite> Le and Zuidema (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "eab79e8aa2cbe6f3aeef0018129208_1",
  "x": "We propose solutions to enhance the Inside-Outside Recursive Neural Network (IORNN) reranker of<cite> Le and Zuidema (2014)</cite> . Replacing the original softmax function with a hierarchical softmax using a binary tree constructed by combining output of the Brown clustering algorithm and frequency-based Huffman codes, we significantly reduce the reranker's computational complexity. In addition, enriching contexts used in the reranker by adding subtrees rooted at (ancestors') cousin nodes, the accuracy is increased.",
  "y": "extends"
 },
 {
  "id": "eab79e8aa2cbe6f3aeef0018129208_2",
  "x": "For constituent parsing, Socher et al. (2013) using a recursive neural network (RNN) got an F1 score close to the state-of-the-art on the Penn WSJ corpus. For dependency parsing, the inside-outside recursive neural net (IORNN) reranker proposed by<cite> Le and Zuidema (2014)</cite> is among the top systems, including the Chen and Manning (2014)'s extremely fast transition-based parser employing a traditional feed-forward neural network. There are many reasons why neural-net-based systems perform very well.",
  "y": "background"
 },
 {
  "id": "eab79e8aa2cbe6f3aeef0018129208_3",
  "x": "Second, by comparing a countbased model with their neural-net-based model on perplexity,<cite> Le and Zuidema (2014)</cite> suggested that predicting with neural nets is an effective solution for the problem of data sparsity. Last but not least, as showed in the work of Socher and colleagues on RNNs, e.g. Socher et al. (2013) , neural networks are capable of 'semantic transfer', which is essential for disambiguation. We focus on how to enhance the IORNN reranker of<cite> Le and Zuidema (2014)</cite> by both reducing its computational complexity and increasing its accuracy.",
  "y": "extends"
 },
 {
  "id": "eab79e8aa2cbe6f3aeef0018129208_4",
  "x": "Second, by comparing a countbased model with their neural-net-based model on perplexity,<cite> Le and Zuidema (2014)</cite> suggested that predicting with neural nets is an effective solution for the problem of data sparsity. Last but not least, as showed in the work of Socher and colleagues on RNNs, e.g. Socher et al. (2013) , neural networks are capable of 'semantic transfer', which is essential for disambiguation. We focus on how to enhance the IORNN reranker of<cite> Le and Zuidema (2014)</cite> by both reducing its computational complexity and increasing its accuracy.",
  "y": "background"
 },
 {
  "id": "eab79e8aa2cbe6f3aeef0018129208_5",
  "x": "Last but not least, as showed in the work of Socher and colleagues on RNNs, e.g. Socher et al. (2013) , neural networks are capable of 'semantic transfer', which is essential for disambiguation. We focus on how to enhance the IORNN reranker of<cite> Le and Zuidema (2014)</cite> by both reducing its computational complexity and increasing its accuracy. Although this reranker is among the top systems in accuracy, its computation is very costly due to its softmax function used to compute probabilities of generating tokens: all possible words in the vocabulary are taken into account.",
  "y": "uses"
 },
 {
  "id": "eab79e8aa2cbe6f3aeef0018129208_6",
  "x": "**THE IORNN RERANKER** We firstly introduce the IORNN reranker <cite>(Le and Zuidema, 2014)</cite> . ----------------------------------",
  "y": "uses"
 },
 {
  "id": "eab79e8aa2cbe6f3aeef0018129208_7",
  "x": "Figure 1: The process to (a) generate y, (b) compute outer representation o y , given head h and sibling x. Black, grey, white boxes are respectively inner, partial outer, and outer representations. <cite>(Le and Zuidema, 2014)</cite> where r = 0 if y is the first dependent of h; oth- is the set of y's sisters generated before. And,",
  "y": "uses background"
 },
 {
  "id": "eab79e8aa2cbe6f3aeef0018129208_9",
  "x": "**CONCLUSION** Solutions to enhance the IORNN reranker of<cite> Le and Zuidema (2014)</cite> were proposed. We showed that, by replacing the original softmax function with a hierarchical softmax, the reranker's computational complexity significantly decreases.",
  "y": "extends"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_0",
  "x": "The design of such features requires entity-specific domain knowledge. These features can not fully capture relevant statistical dependencies and interactions. One recent notable work (<cite>Ganea and Hofmann 2017</cite>) instead pioneers to rely on pre-trained entity embeddings, learnable context representation and differentiable joint inference stage to learn basic features and their combinations from scratch.",
  "y": "background"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_1",
  "x": "One recent notable work (<cite>Ganea and Hofmann 2017</cite>) instead pioneers to rely on pre-trained entity embeddings, learnable context representation and differentiable joint inference stage to learn basic features and their combinations from scratch. <cite>Such model design</cite> allows to learn useful regularities in an end-to-end fashion and eliminates the need for extensive feature engineering. <cite>It</cite> also substantially outperforms In Milwaukee , Marc Newfield homered off Jose Parra leading off the bottom of the 12th as the Brewers rallied for a 5-4 victory over the Minnesota Twins .",
  "y": "background"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_2",
  "x": "---------------------------------- **WIKIPEDIA TITLE** Local context score Golden Figure 1 : One error case on AIDA-CoNLL development set of the full model of <cite>Ganea and Hofmann (2017)</cite> .",
  "y": "background"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_3",
  "x": "Such state-of-the-art entity linking models <cite>(Ganea and Hofmann 2017</cite>; Le and Titov 2018) employ attention-based bag-of-words context model and pre-trained entity embeddings bootstrapped from word embeddings to assess topic level context compatibility. However, the latent entity type information in the immediate context of the mention is neglected. We suspect this may sometimes cause the models link mentions to incorrect entities with incorrect type.",
  "y": "background"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_4",
  "x": "Such state-of-the-art entity linking models <cite>(Ganea and Hofmann 2017</cite>; Le and Titov 2018) employ attention-based bag-of-words context model and pre-trained entity embeddings bootstrapped from word embeddings to assess topic level context compatibility. However, the latent entity type information in the immediate context of the mention is neglected. We suspect this may sometimes cause the models link mentions to incorrect entities with incorrect type. To verify this, we conduct error analysis of the well known <cite>DeepED 1</cite> model <cite>(Ganea and Hofmann 2017)</cite> on the development set of AIDA-CoNLL (Hoffart et al. 2011) , and found that more than half of <cite>their</cite> error cases fall into the category of type errors where the predicted entity's type is different from the golden entity's type, although some predictive contextual cue for them can be found in <cite>their</cite> local context.",
  "y": "motivation"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_5",
  "x": "We suspect this may sometimes cause the models link mentions to incorrect entities with incorrect type. To verify this, we conduct error analysis of the well known <cite>DeepED 1</cite> model <cite>(Ganea and Hofmann 2017)</cite> on the development set of AIDA-CoNLL (Hoffart et al. 2011) , and found that more than half of <cite>their</cite> error cases fall into the category of type errors where the predicted entity's type is different from the golden entity's type, although some predictive contextual cue for them can be found in <cite>their</cite> local context. As shown in Fig. 1 , the full model of <cite>Ganea and Hofmann (2017)</cite> incorrectly links the mention \"Milwaukee\" to the entity MILWAUKEE BREWERS.",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_6",
  "x": "However, the prepo-sition \"In\" is a strong cue predictive of the type (location) of mention \"Milwaukee\" which is helpful for disambiguation. The reason why the local context model of <cite>Ganea and Hofmann (2017)</cite> couldn't capture such apparent cue is two folds. On one hand, the context encoding module adopts a bag-of-words encoding scheme which is position agnostic.",
  "y": "background"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_7",
  "x": "As shown in Fig. 1(b) , the attention mechanism is helpful for selecting predictive words (e.g. \"Milwaukee\", \"games\" etc.), but does not capture the pattern that the previous word \"In\" of the mention \"Milwaukee\" which very likely refers to an entity with location type. On the other hand, the pre-trained entity embedding of <cite>Ganea and Hofmann (2017)</cite> is not very sensitive to entity types. For example, as shown in Table 8 , when we query the most similar entities with the entity STEVE JOBS, the top one returned entity is APPLE INC., which is a different type but releated at topic level.",
  "y": "background"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_8",
  "x": "So it is natural for the model of <cite>Ganea and Hofmann (2017)</cite> to make type errors when it is trained to fit such entity embeddings. As is argued in (Zhou et al. 2018) , \"context consistency is a strong proxy for type compatibility\". Based on this claim, the mention's immediate context is a proxy of its type.",
  "y": "background"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_9",
  "x": "Pre-trained BERT models naturally fit our purpose to represent the entity context surrounding the [MASK] token as it is trained with masked language model objective. What's more, we integrate a BERT-based entity similarity feature into the local model of <cite>Ganea and Hofmann (2017)</cite> to better capture entity type information. This can leverage both the pre-trained entity embeddings from BERT and the domain adaption capability of BERT via fine-tuning.",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_10",
  "x": "Our model achieves an absolute improvement of 1.32% F1 on AIDA-CoNLL test set and average 0.80% F1 on five out-domain test sets over five different runs. In addition, we conduct detailed experiment analysis on AIDA-CoNLL development set which shows our proposed model can reduce 67.03% type errors of the state-of-the-art model <cite>(Ganea and Hofmann 2017)</cite> and more than 90% of the remaining type error cases are due to over estimation of prior and global modeling problem which we leave as the further work. Our contributions can be summarized as follows.",
  "y": "differences"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_11",
  "x": "\u2022 We integrate a BERT-based entity similarity into the local model of a SOTA model <cite>(Ganea and Hofmann 2017)</cite> . \u2022 We verify the effectiveness of our model on standard benchmark datasets and achieve significant improvement over the baseline. And the detailed experiment analysis demonstrates that our method truly corrects most of the type errors produced by the baseline. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_12",
  "x": "Next, we introduce the general formulation of entity linking problem with a focus on the well known <cite>DeepED</cite> model <cite>(Ganea and Hofmann 2017)</cite> . General Formulation An entity linking model integrating both local and global features can be formulated as a conditional random field. Formally, we can define a scoring function g to evaluate the entity assignment e 1 , ..., e n to mentions m 1 , ..., m n in a document D.",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_13",
  "x": "g(e 1 , ..., e n |D) where the first term scores how well an entity fits its local context and the second one measures the global coherence. Local Model Following <cite>Ganea and Hofmann (2017)</cite> , we instantiate the local model as an attention model based on pre-trained word and entity embeddings.",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_14",
  "x": "where B is a learnable diagonal matrix, x e is the embeddings of entity e, h(c) applies a hard attention mechanism to context words in c to obtain the representation of the context. Besides, <cite>Ganea and Hofmann (2017)</cite> combined this context score with the priorp(e|m) (computed by mixing mention-entity hyperlink count statistics from Wikipedia, a large Web corpus and YAGO. 2 ) using a two-layer feedforward neural network in the local model.",
  "y": "background"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_15",
  "x": "Previous work (Yamada et al. 2016 ; <cite>Ganea and Hofmann 2017)</cite> on learning entity representation are mostly extensions of the embedding methods proposed by (Mikolov et al. 2013 ). An entity's context is a bag-ofwords representation which mainly captures topic level entity relatedness rather than entity type relatedness. In contrast, we propose a simple method to build entity embeddings directly from pre-trained BERT (Devlin et al. 2019) which can better capture entity type information.",
  "y": "background"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_16",
  "x": "Previous work (Yamada et al. 2016 ; <cite>Ganea and Hofmann 2017)</cite> on learning entity representation are mostly extensions of the embedding methods proposed by (Mikolov et al. 2013 ). An entity's context is a bag-ofwords representation which mainly captures topic level entity relatedness rather than entity type relatedness. In contrast, we propose a simple method to build entity embeddings directly from pre-trained BERT (Devlin et al. 2019) which can better capture entity type information.",
  "y": "differences"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_17",
  "x": "Entity Representation For each entity e i \u2208 E, we randomly sample at most N anchor contexts {c i1 , c i2 , ..., c iN } from Wikipedia. Then the entity representation of e i is computed by aggregating all the context representation {c i1 , c i2 , ..., c iN } via average pooling. As will be shown in the analysis section, the entity embeddings from BERT better capture entity type information than those from <cite>Ganea and Hofmann (2017)</cite> .",
  "y": "differences"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_18",
  "x": "**BERT-BASED ENTITY SIMILARITY** The local context model of <cite>Ganea and Hofmann (2017)</cite> mainly captures the topic level entity relatedness information based on a long range bag-of-words context. To capture latent entity type information, we design a BERT-based entity similarity score \u03a8 BERT (e, c).",
  "y": "background"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_19",
  "x": "Then we define the BERT-based entity similarity as the cosine similarity 5 between the context representation c and the entity representation B e . Finally, as for the local disambiguation model, we integrate the BERT-based entity similarity \u03a8 BERT (e, c) with the local context score \u03a8 long (e, c) (defined in Equation 2) and the priorp(e|m i ) with two fully connected layers of 100 hidden units and ReLU non-linearities following the same feature composition methods as <cite>Ganea and Hofmann (2017)</cite> . As for the global disambiguation model, we firstly define the local context score \u03a8 localctx (e, c) by combining \u03a8 long (e, c) and \u03a8 BERT (e, c).",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_20",
  "x": "As for the global disambiguation model, we firstly define the local context score \u03a8 localctx (e, c) by combining \u03a8 long (e, c) and \u03a8 BERT (e, c). 6 Then we adopt exactly the same global model as <cite>Ganea and Hofmann (2017)</cite> which is already introduced in the Background section.",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_21",
  "x": "Specifically, we adopt <cite>loopy belief propagation (LBP)</cite> to estimate the max-marginal probability\u011d i (e|D) and then combine it with the priorp(e|m i ) using a two-layer neural network to get the final score \u03c1 i (e) for m i . ---------------------------------- **MODEL TRAINING**",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_22",
  "x": "For in-domain setting, we use AIDA-CoNLL dataset (Hoffart et al. 2011) for training, validation and testing. For out-domain setting, we evaluate the model trained with AIDA-CoNLL on five popular out-domain test sets: MSNBC, AQUAINT, ACE 2004 datasets cleaned and updated by Guo and Barbosa (2016) and WNED-CWEB (CWEB), WNED-WIKI (WIKI) automatically extracted from ClueWeb and Wikipedia (Guo and Barbosa 2016) . Following previous work <cite>(Ganea and Hofmann 2017)</cite>, we only consider in-KB mentions.",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_23",
  "x": "Besides, our candidate generation strategy follows that of <cite>Ganea and Hofmann (2017)</cite> to make our results comparable. ---------------------------------- **SETUP**",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_24",
  "x": "The main goal of this work is to introduce a BERT-based entity similarity to capture latent entity type information which is supplementary to existing SOTA local context model <cite>(Ganea and Hofmann 2017)</cite> . So we evaluate the performance when integrating the BERT-based entity similarity into the local context model of <cite>Ganea and Hofmann (2017)</cite> . We also evaluate our model with or without global modeling method of <cite>Ganea and Hofmann (2017)</cite> .",
  "y": "motivation"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_25",
  "x": "So we evaluate the performance when integrating the BERT-based entity similarity into the local context model of <cite>Ganea and Hofmann (2017)</cite> . We also evaluate our model with or without global modeling method of <cite>Ganea and Hofmann (2017)</cite> . In addition, we further compare our methods with other state-of-theart models (Yamada et al. 2016; Le and Titov 2018) .",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_26",
  "x": "In addition, we further compare our methods with other state-of-theart models (Yamada et al. 2016; Le and Titov 2018) . To verify the contribution of our proposed BERT-based entity embeddings, we also compare with a straightforward baseline which directly replaces the encoder of <cite>Ganea and Hofmann (2017)</cite> utilizing pre-trained BERT. To do so, we introduce a 768 \u00d7 300 dimensional matrix W which projects BERT-based context representation c into <cite>Ganea and Hofmann (2017)</cite>'s entity embeddings space when calculating the similarity score.",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_27",
  "x": "**HYPER-PARAMETER SETTING** The resources (word and entity embeddings) used to train the local context model of <cite>Ganea and Hofmann (2017)</cite> are obtained from <cite>DeepED 7</cite> . For each entity, we randomly sam-7 https://github.com/dalab/deep-ed/ Methods AIDA-B Local models priorp(e|m) 71.9 Lazic et al. (2015) 86.4 Globerson et al. (2016) 87.9 Yamada et al. (2016) 87.2 <cite>Ganea and Hofmann (2017)</cite> 88.8 <cite>Ganea and Hofmann (2017)</cite> (reproduce) 88.75 \u00b1 0.30 BERT-Entity-Sim (local)",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_29",
  "x": "Similar to <cite>Ganea and Hofmann (2017)</cite> , all the entity embeddings are fixed during fine-tuning. We randomly initialize the not BERT related parameters using Gaussian distribution N (0.0, 0.02) and the bias term is zeroed. Note that all the hyper-parameters used in the local context and global model of <cite>Ganea and Hofmann (2017)</cite> were set to the same values as theirs for direct comparison purpose.",
  "y": "similarities"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_30",
  "x": "We randomly initialize the not BERT related parameters using Gaussian distribution N (0.0, 0.02) and the bias term is zeroed. Note that all the hyper-parameters used in the local context and global model of <cite>Ganea and Hofmann (2017)</cite> were set to the same values as theirs for direct comparison purpose. Detailed hyper-parameters setting is described in the appendices.",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_31",
  "x": "The models are divided into two groups: local models and local & global models. As we can see, our proposed model, BERT-Entity-Sim, outperforms all previous methods. Our local model achieves a 1.31 improvement in terms of F1 over its corresponding baseline <cite>(Ganea and Hofmann 2017)</cite> , yielding a very competitive local model with an average 90.06 F1 score even surpassing the performance of four local & global models.",
  "y": "differences"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_32",
  "x": "As we can see, our proposed model, BERT-Entity-Sim, outperforms all previous methods. Our local model achieves a 1.31 improvement in terms of F1 over its corresponding baseline <cite>(Ganea and Hofmann 2017)</cite> , yielding a very competitive local model with an average 90.06 F1 score even surpassing the performance of four local & global models. Equipped with the global modeling method of <cite>Ganea and Hofmann (2017)</cite> , the performance of our model further increase to 93.54 with an average 1.32 improvement in terms of F1 over <cite>Ganea and Hofmann (2017)</cite> .",
  "y": "differences"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_33",
  "x": "Equipped with the global modeling method of <cite>Ganea and Hofmann (2017)</cite> , the performance of our model further increase to 93.54 with an average 1.32 improvement in terms of F1 over <cite>Ganea and Hofmann (2017)</cite> . In addition, our method outperforms Le and Titov (2018) model by 0.47 point. The model of Le and Titov (2018) is a multirelational extension of <cite>Ganea and Hofmann (2017)</cite>'s global modeling method while keeps exactly the same local context model.",
  "y": "extends"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_34",
  "x": "Our better local context model should be orthogonal with them and has potential more applications on short texts (e.g. tweets) where global modeling has little benefits. Moreover, BERT+G&Hs embeddings performs significantly worse than the baseline <cite>(Ganea and Hofmann 2017)</cite> and our proposed BERT-Entity-Sim model. The reason is that BERT-based context representation space and <cite>Ganea and Hofmanns entity embeddings space</cite> are heterogeneous.",
  "y": "similarities"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_35",
  "x": "Moreover, BERT+G&Hs embeddings performs significantly worse than the baseline <cite>(Ganea and Hofmann 2017)</cite> and our proposed BERT-Entity-Sim model. The reason is that BERT-based context representation space and <cite>Ganea and Hofmanns entity embeddings space</cite> are heterogeneous. <cite>Ganea and Hofmanns entity embeddings</cite> are bootstrapped from word embeddings which mainly capture topic level entity relatedness, while BERT-based context representation is derived from BERT which naturally captures type information.",
  "y": "background"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_36",
  "x": "To evaluate the robustness of our model, Table 2 shows the performance of our method and SOTA methods on five out-domain test sets. On average, our proposed model (BERT-Entity-Sim) outperforms the local & global version of <cite>Ganea and Hofmann</cite>; Le and Titov (2017; by an average 0.80 and 0.51 on F1. ----------------------------------",
  "y": "differences"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_39",
  "x": "Effectiveness of BERT-based and <cite>Ganea & Hofmann (2017)</cite> <cite>Ganea and Hofmann (2017)</cite> 93.7 \u00b1 0.1 88.5 \u00b1 0.4 88.5 \u00b1 0.3 77.9 \u00b1 0.1 77.5 \u00b1 0.1 85.22 Le and Titov (2018) 93.9 \u00b1 0.2 88.3 \u00b1 0.6 89.9 \u00b1 0.8 77. Table 5 : Performance of two state-of-the-art fine grained entity typing systems on AIDA-CoNLL development set order to verify our claim that the entity embeddings from BERT better capture entity type information than those from <cite>Ganea and Hofmann (2017)</cite> , we carry out an entity type prediction task based on its entity embedding. Specifically, we randomly sample 100K entities from Wikipedia, and randomly split them into training set (80K), development set (10K) and test set (10K).",
  "y": "differences"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_40",
  "x": "The entity type prediction model is a simple linear classification model 10 using the entity embedding of an entity as features; limiting its capacity enables us to focus on whether type information can be easily extracted from the entity embeddings. We evaluate the model using standard entity typing metrics: Strict Accuracy (Acc.), Micro F1 (F 1 mi ) and Macro F1 (F 1 ma ). As shown in Table 3 , our proposed entity embedding from BERT significantly outperforms the entity embedding proposed by <cite>Ganea and Hofmann (2017)</cite> on three typing sys-tems FIGER, BBN and OntoNotes fine .",
  "y": "differences"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_41",
  "x": "This demonstrates that our proposed entity embeddings from BERT indeed capture better latent entity type information than <cite>Ganea and Hofmann (2017)</cite> . Type Errors Correction As we have mentioned in the introduction section, more than half of the baseline model's errors on the AIDA-A dataset are type errors. Type errors are error cases 11 where (1) the predicted entity's type is different from the golden entity's type; (2) contextual cue predictive of the type of the mention exists; (3) errors are not due to annotation errors.",
  "y": "differences"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_42",
  "x": "Type errors are error cases 11 where (1) the predicted entity's type is different from the golden entity's type; (2) contextual cue predictive of the type of the mention exists; (3) errors are not due to annotation errors. By doing so, we collect 185 type error cases which cover 57.45% of all (322) error cases. This indicates that <cite>Ganea and Hofmann (2017)</cite> produces many type errors due to its inability to consider the entity type information in mention context.",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_44",
  "x": "Besides, there are 22.95% remaining type errors which are due to global modeling problem which shows the limitation of the global modeling method of <cite>Ganea and Hofmann (2017)</cite> . Finally, 9.84% type error cases are due to local context problem that our BERT-based solution cannot address. We leave this to future work.",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_45",
  "x": "It is nature to conjecture that we can also correct type errors by incorporating explicit type information into <cite>Ganea and Hofmann (2017)</cite> . We investigate this approach in this section. Assuming that we have types for each mention and candidate entity, we calculate the Jaccard similarity between them and use it as a feature for local disambiguation model.",
  "y": "extends"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_46",
  "x": "13 As shown in Table 5 , the ultra-fine entity typing system (Choi et al. 2018) only achieves 26.52% F 1 mi score while the ZOE system (Zhou et al. 2018 ) achieves 66.12% F 1 mi score 14 which are insufficient to improve state-of-the-art entity linking system with more than 92% F1 score. Better Global Model In order to investigate whether better global model can further boost the performance of our model, we incorporate the recent proposed Dynamic Context Augmentation (DCA) 15 (Yang et al. 2019) . DCA is a global entity linking model featuring better efficiency and effectiveness than that of <cite>Ganea and Hofmann (2017)</cite> by breaking the \"all-mention coherence\" assumption.",
  "y": "differences"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_48",
  "x": "---------------------------------- **NEAREST CONTEXTS** We follow Papernot and Mc-Daniel (2018) <cite>Ganea and Hofmann (2017)</cite> and BERT based entity representation space neighbour in the context representation space.",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_49",
  "x": "**NEAREST ENTITIES** We also retrieve nearest entities in the embedding space of <cite>Ganea and Hofmann (2017)</cite> and ours. As we can see, we query STEVE JOBS, the nearest entity in <cite>Ganea and Hofmann (2017)</cite> is APPLE INC.",
  "y": "uses"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_50",
  "x": "Another example is when we query NA-TIONAL BASKETBALL ASSOCIATION, the most similar entities in <cite>Ganea and Hofmann (2017)</cite> are NBA teams which are topically related, while the entities retrieved by our approach are all basketball leagues. ---------------------------------- **CONCLUSION**",
  "y": "differences"
 },
 {
  "id": "eb5ef34dd9c3845cd27c33242d5316_51",
  "x": "Then we integrate a BERT-based entity similarity into the local model of the state-of-the-art method by (<cite>Ganea and Hofmann 2017</cite>) . The experiment results show that our model significantly outperforms the baseline with an absolute improvement of 1.32% F1 on in-domain AIDA-CoNLL test set and average 0.80% F1 on five out-domain test datasets. The detailed experiment analysis shows that our method corrects most of the type errors produced by the baseline.",
  "y": "uses"
 },
 {
  "id": "ebd4488438579946c23904cc0f5932_0",
  "x": "In each FrameNet sentence, a single target predicate is identified and all of its relevant Frame Elements are tagged with their element-type (e.g., Agent, Judge), their syntactic Phrase Type (e.g., NP, PP), and their Grammatical Function (e.g., External Argument, Object Argument). Figure 1 shows an example of an annotated sentence and its appropriate semantic frame. To our knowledge, <cite>Gildea and Jurafsky (2000)</cite> is the only work that uses FrameNet to build a statistical semantic classifier.",
  "y": "similarities"
 },
 {
  "id": "ebd4488438579946c23904cc0f5932_1",
  "x": "In the identification phase, they use syntactic information extracted from a parse tree to learn the boundaries of Frame Elements in sentences. The work presented here, focuses only on the second phase: classification. <cite>Gildea and Jurafsky (2000)</cite> describe a system that uses completely syntactic features to classify the Frame Elements in a sentence.",
  "y": "background"
 },
 {
  "id": "ebd4488438579946c23904cc0f5932_2",
  "x": "She clapped her hands in inspiration. Frame: We extend <cite>Gildea and Jurafsky (2000)</cite> 's initial effort in three ways. First, we adopt a Maximum Entropy (ME) framework to better learn the feature weights associated with the classification model.",
  "y": "extends differences"
 },
 {
  "id": "ebd4488438579946c23904cc0f5932_3",
  "x": "---------------------------------- **2.2** Training (32,251 sentences), development (3,491 sentences), and held out test sets (3,398 sentences) were generated from the June 2002 FrameNet release following the divisions used in <cite>Gildea and Jurafsky (2000)</cite> 1 .",
  "y": "uses"
 },
 {
  "id": "ebd4488438579946c23904cc0f5932_4",
  "x": "Training (32,251 sentences), development (3,491 sentences), and held out test sets (3,398 sentences) were generated from the June 2002 FrameNet release following the divisions used in <cite>Gildea and Jurafsky (2000)</cite> 1 . Because human-annotated syntactic information could only be obtained for a subset of their data, the training, development, and test sets used here are approximately 10% smaller than those used in <cite>Gildea and Jurafsky (2000)</cite> . 2 There are on average 2.2 Frame Elements per sentence, falling into one of 126 unique classes.",
  "y": "differences"
 },
 {
  "id": "ebd4488438579946c23904cc0f5932_5",
  "x": "**P(R| PT, VOICE, POSITION, TARGET, GF, H)** Here r indicates the element type, pt the phrase type, gf the grammatical function, h the head word, and target the target predicate. Due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in <cite>Gildea and Jurafsky (2000)</cite> .",
  "y": "extends differences"
 },
 {
  "id": "ebd4488438579946c23904cc0f5932_6",
  "x": "1 Divisions given by Dan Gildea via personal communication. 2 <cite>Gildea and Jurafsky (2000)</cite> use 36995 training, 4000 development, and 3865 test sentences. They do not report results using hand annotated syntactic information.",
  "y": "differences"
 },
 {
  "id": "ebd4488438579946c23904cc0f5932_7",
  "x": "The difference in performance between the models under both human and extracted conditions was relatively consistent: averaging 8.7% with a standard deviation of 0.7. As a further analysis, we have examined the performance of our base ME model on the same test set as that used in <cite>Gildea and Jurafsky (2000)</cite> . Using only extracted information, we achieve an accuracy of 74.9%, two percent lower than their reported results.",
  "y": "similarities uses"
 },
 {
  "id": "ebd4488438579946c23904cc0f5932_8",
  "x": "Following <cite>Gildea and Jurafsky (2000)</cite> , automatic extraction of grammatical information here is limited to the governing category of a Noun Phrase. The FrameNet annotations, however, are much richer and include information about complements, modifiers, etc. We are looking at ways to include such information either by using alternative parsers (Hermjakob, 1997) or as a post processing task (Blaheta and Charniak, 2000) .",
  "y": "differences"
 },
 {
  "id": "ec3702a6b30057fcae65ca297656d2_0",
  "x": "Abdul-Rauf and Schwenk (2009), Uszkoreit et al. (2010) , and Gahbiche-Braham et al. (2011) , rather than trying to detect translated sentence pairs directly, translate the entire source language side of a comparable corpus into the target language with a baseline SMT system and then search for corresponding documents. On the other hand, there exist approaches that mine comparable corpora without any prior translation information or parallel data. Examples of this approach are rarer, and we briefly mention two: Enright and Kondrak (2007) use singleton words (hapax legomena) to represent documents in a bilingual collection for the task of detecting document translation pairs, and <cite>Krstovski and Smith (2011)</cite> construct a vocabulary of overlapping words to represent documents in multilingual collections.",
  "y": "background"
 },
 {
  "id": "ec3702a6b30057fcae65ca297656d2_1",
  "x": "Examples of this approach are rarer, and we briefly mention two: Enright and Kondrak (2007) use singleton words (hapax legomena) to represent documents in a bilingual collection for the task of detecting document translation pairs, and <cite>Krstovski and Smith (2011)</cite> construct a vocabulary of overlapping words to represent documents in multilingual collections. The latter approach demonstrates high precision vs. recall values on various language pairs from different languages and writing systems when detecting translation pairs on a document level such as Europarl sessions. Recently proposed approaches, such as (Klementiev et al., 2012) use monolingual corpora to estimate phrase-based SMT parameters. Unlike our paper, however, they do not demonstrate an end-toend SMT system trained without any parallel data.",
  "y": "differences"
 },
 {
  "id": "ec3702a6b30057fcae65ca297656d2_2",
  "x": "**BOOTSTRAPPING APPROACH** Our bootstrapping approach (Figure 1 ) is a twostage system that used the Overlapping Cosine Distance (OCD) approach of <cite>Krstovski and Smith (2011)</cite> as its first step. OCD outputs a ranked list of candidate document pairs, which are then fed through a sentence-alignment system (Moore, 2002) .",
  "y": "uses"
 },
 {
  "id": "ec3702a6b30057fcae65ca297656d2_3",
  "x": "Words found in both source (s) and target (t) languages are extracted and the overlapping list of words are then used as dimensions for constructing a feature vector template. Documents in both languages are then represented using the template vector whose dimensions are the tf\u00b7idf values computed on the overlapping words which we now consider as features. While the number of overlapping words is dependent on the families of the source and target languages and their orthography, <cite>Krstovski and Smith (2011)</cite> showed that this approach yields good results across language pairs from different families and writing systems such as English-Greek, English-Bulgarian and EnglishArabic where, as one would expect, most shared words are numbers and named entities.",
  "y": "background"
 },
 {
  "id": "ec5897c392b05cb8712feadfc6d2bf_0",
  "x": "<cite>Hatzivassiloglou and McKeown (1993)</cite> duster adjectives into partitions and present an interesting evaluation to compare the generated adjective classes against those provided by an expert. Their evaluation scheme bases the comparison between two classes on the presence or absence of pairs of words in them. Their approach involves filling in a YES-NO contingency table based on whether a pair of words (adjectives, in their case) is classified in the same class by the human expert and by the system.",
  "y": "background"
 },
 {
  "id": "ec5897c392b05cb8712feadfc6d2bf_1",
  "x": "<cite>Hatzivassiloglou and McKeown (1993)</cite> duster adjectives into partitions and present an interesting evaluation to compare the generated adjective classes against those provided by an expert. Their evaluation scheme bases the comparison between two classes on the presence or absence of pairs of words in them. Their approach involves filling in a YES-NO contingency table based on whether a pair of words (adjectives, in their case) is classified in the same class by the human expert and by the system. This method works very well for partitions. However, if it is used to evaluate sets of classes where the classes may be potentiaily overlapping, their technique yields a weaker measure since the same word pair could possibly be present in more than one class. An ideal scheme used to evaluate semantic classes should be able to handle overlapping classes (as o1>. posed to partitions) as well as hierarchies. The technique proposed by Hatzivassiloglou and McKeown does not do a good job of evaluating either of these. In this paper, we present an evaluation methodology which makes it possible to properly evaluate over- In the discussion that follows, the word \"clustering\" is used to refer to the set of classes that may be either provided by an expert or generated by the system, and the word \"class\" is used to refer to a single class in the clustering.",
  "y": "motivation"
 },
 {
  "id": "ec5897c392b05cb8712feadfc6d2bf_2",
  "x": "Before we delve deeper into the evaluation process, we must decide on some measure of \"closeness\" between a pair of classes. We have adopted the F-measure<cite> (Hatzivassiloglou and McKeown, 1993</cite>; Chincor, 1992) . In our computation of the Fmeasure, we construct a contingency table based on the presence or absence of individual elements in the two classes being compared, as opposed to basing it on pairs of words.",
  "y": "uses"
 },
 {
  "id": "ec5897c392b05cb8712feadfc6d2bf_3",
  "x": "Table 2 ). Once all the mapped classes have been incorporated into this contingency table, add every element of all unmapped classes generated by the system to the YES-NO cell and every element of all unmapped classes provided by the expert to the NO-YES cell of this table. Once all classes in the two clusterings have been accounted for, calculate the precision, recall, and F-measure as explained in<cite> (Hatzivassiloglou and McKeown, 1993)</cite> .",
  "y": "uses"
 },
 {
  "id": "ecb6e93a5254b86ef49a5ffd0a52a0_0",
  "x": "We focus on the first two tasks. A non-word is a sequence of letters that is not a possible word in the language in any context, e.g., English * thier. Once a sequence of letters has been determined to be a non-word, isolatedword error correction is the process of determining the appropriate word to substitute for the non-word. Given a sequence of letters, there are thus two main subtasks: 1) determine whether this is a nonword, 2) if so, select and rank candidate words as potential corrections to present to the writer. The first subtask can be accomplished by searching for the sequence of letters in a word list. The second subtask can be stated as follows<cite> (Brill and Moore, 2000)</cite> : Given an alphabet \u03a3, a word list D of strings \u2208 \u03a3 * , and a string r / \u2208 D and \u2208 \u03a3 * , find w \u2208 D such that w is the most likely correction.",
  "y": "uses"
 },
 {
  "id": "ecb6e93a5254b86ef49a5ffd0a52a0_1",
  "x": "The spelling error model proposed by <cite>Brill and Moore (2000)</cite> allows generic string edit operations up to a certain length. Each edit operation also has an associated probability that improves the ranking of candidate corrections by modeling how likely particular edits are. <cite>Brill and Moore (2000)</cite> estimate the probability of each edit from a corpus of spelling errors.",
  "y": "background"
 },
 {
  "id": "ecb6e93a5254b86ef49a5ffd0a52a0_2",
  "x": "<cite>Brill and Moore (2000)</cite> estimate the probability of each edit from a corpus of spelling errors. Toutanova and Moore (2002) extend <cite>Brill and Moore (2000)</cite> to consider edits over both letter sequences and sequences of phones in the pronunciations of the word and misspelling. They show that including pronunciation information improves performance as compared to <cite>Brill and Moore (2000)</cite> .",
  "y": "background"
 },
 {
  "id": "ecb6e93a5254b86ef49a5ffd0a52a0_3",
  "x": "<cite>Brill and Moore (2000)</cite> estimate the probability of each edit from a corpus of spelling errors. Toutanova and Moore (2002) extend <cite>Brill and Moore (2000)</cite> to consider edits over both letter sequences and sequences of phones in the pronunciations of the word and misspelling. They show that including pronunciation information improves performance as compared to <cite>Brill and Moore (2000)</cite> .",
  "y": "background"
 },
 {
  "id": "ecb6e93a5254b86ef49a5ffd0a52a0_4",
  "x": "They show that including pronunciation information improves performance as compared to <cite>Brill and Moore (2000)</cite> . ---------------------------------- **NOISY CHANNEL SPELLING CORRECTION**",
  "y": "background"
 },
 {
  "id": "ecb6e93a5254b86ef49a5ffd0a52a0_5",
  "x": "The spelling correction models from <cite>Brill and Moore (2000)</cite> and Toutanova and Moore (2002) use the noisy channel model approach to determine the types and weights of edit operations. The idea behind this approach is that a writer starts out with the intended word w in mind, but as it is being written the word passes through a noisy channel resulting in the observed non-word r. In order to determine how likely a candidate correction is, the spelling correction model determines the probability that the word w was the intended word given the misspelling r: P (w|r). To find the best correction, the word w is found for which P (w|r) is maximized: argmax w P (w|r).",
  "y": "background"
 },
 {
  "id": "ecb6e93a5254b86ef49a5ffd0a52a0_6",
  "x": "1 <cite>Brill and Moore (2000)</cite> allow all edit operations \u03b1 \u2192 \u03b2 where \u03a3 is the alphabet and \u03b1, \u03b2 \u2208 \u03a3 * , with a constraint on the length of \u03b1 and \u03b2. In order to consider all ways that a word w may generate r with the possibility that any, possibly empty, substring \u03b1 of w becomes any, possibly empty, substring \u03b2 of r, it is necessary to consider all ways that w and r may be partitioned into substrings. This error model over letters, called P L , is approximated by <cite>Brill and Moore (2000)</cite> as shown in Figure 1 by considering only the pair of partitions of w and r with the maximum product of the probabilities of individual substitutions.",
  "y": "background"
 },
 {
  "id": "ecb6e93a5254b86ef49a5ffd0a52a0_7",
  "x": "1 <cite>Brill and Moore (2000)</cite> allow all edit operations \u03b1 \u2192 \u03b2 where \u03a3 is the alphabet and \u03b1, \u03b2 \u2208 \u03a3 * , with a constraint on the length of \u03b1 and \u03b2. In order to consider all ways that a word w may generate r with the possibility that any, possibly empty, substring \u03b1 of w becomes any, possibly empty, substring \u03b2 of r, it is necessary to consider all ways that w and r may be partitioned into substrings. This error model over letters, called P L , is approximated by <cite>Brill and Moore (2000)</cite> as shown in Figure 1 by considering only the pair of partitions of w and r with the maximum product of the probabilities of individual substitutions.",
  "y": "background"
 },
 {
  "id": "ecb6e93a5254b86ef49a5ffd0a52a0_8",
  "x": "The parameters for P L (r|w) are estimated from a corpus of pairs of misspellings and target words. The method, which is described in detail in <cite>Brill and Moore (2000)</cite> , involves aligning the letters in pairs of words and misspellings, expanding each alignment with up to N neighboring alignments, and calculating the probability of each \u03b1 \u2192 \u03b2 alignment. Since we will be using a training corpus that consists solely of pairs of misspellings and words (see section 3), we would have lower probabilities for",
  "y": "background"
 },
 {
  "id": "ecb6e93a5254b86ef49a5ffd0a52a0_10",
  "x": "**EXTENDING TO PRONUNCIATION** Toutanova and Moore (2002) describe an extension to <cite>Brill and Moore (2000)</cite> where the same noisy channel error model is used to model phone sequences instead of letter sequences. Instead of the word w and the non-word r, the error model considers the pronunciation of the non-word r, pron r , and the pronunciation of the word w, pron w .",
  "y": "background"
 },
 {
  "id": "ecb6e93a5254b86ef49a5ffd0a52a0_11",
  "x": "The letter error model P L and the phone error model P P H are trained on the training set. The development set is used to tune the parameters introduced in previous sections. 7 In order to rank the words as candidate corrections for a misspelling r, P L (r|w) and P P HL (r|w) are calculated for each word in the word list using the algorithm described in <cite>Brill and Moore (2000)</cite> .",
  "y": "uses"
 },
 {
  "id": "ece5f95d3c616ceeb0b3061e606b41_0",
  "x": "Recently, several distributed word representation models have been introduced that have interesting properties regarding to the semantic information that they capture. In particular, we are interested in the word2vec package available in<cite> (Mikolov et al., 2013a)</cite> . These models proved to be robust and powerful for predicting semantic relations between words and even across languages.",
  "y": "motivation"
 },
 {
  "id": "ece5f95d3c616ceeb0b3061e606b41_1",
  "x": "**SEMANTIC MODELS USING CBOW** The basic architecture that we use to build our models is CBOW<cite> (Mikolov et al., 2013a)</cite> . The algorithm uses a neural network (NN) to predict a word taking into account its context, but without considering word order.",
  "y": "uses"
 },
 {
  "id": "ece5f95d3c616ceeb0b3061e606b41_2",
  "x": "Table 1 shows the results, both overall accuracy and accuracy over the known words for the models. Using the first 30, 000 entries of the model (the most frequent ones), we obtain 32% of accuracy for English (mono en) and 10% for Spanish (mono es). We chose these parameters for our system to obtain comparable results to the ones in<cite> (Mikolov et al., 2013a</cite> ) for a CBOW architecture but trained with 783 million words (50.4%).",
  "y": "motivation similarities"
 },
 {
  "id": "ee219d599e0e0c2bbebc1849863005_0",
  "x": "This mismatch of needs has motivated various proposals to reconstruct missing entries, in WALS and other databases, from known entries (Daum\u00e9 III and Campbell, 2007; Daum\u00e9 III, 2009; Coke et al., 2016; <cite>Littell et al., 2017)</cite> . In this study, we examine whether we can tackle the problem of inferring linguistic typology from parallel corpora, specifically by training a massively multi-lingual neural machine translation (NMT) system and using the learned representations to infer typological features for each language. This is motivated both by prior work in linguistics (Bugarski, 1991; Garc\u00eda, 2002) demonstrating strong links between translation studies and tools for contrastive linguistic analysis, work in inferring typology from bilingual data (\u00d6stling, 2015) and English as Second Language texts (Berzak et al., 2014) , as well as work in NLP (Shi et al., 2016; Kuncoro et al., 2017; Belinkov et al., 2017) showing that syntactic knowledge can be extracted from neural nets on the word-by-word or sentence-by-sentence level.",
  "y": "background"
 },
 {
  "id": "ee219d599e0e0c2bbebc1849863005_1",
  "x": "We calculate these feature vectors using an NMT model trained on 1017 languages, and use them for typlogy prediction both on their own and in composite with feature vectors from previous work based on the genetic and geographic distance between languages<cite> (Littell et al., 2017)</cite> . Results show that the extracted representations do in fact allow us to learn about the typology of languages, with particular gains for syntactic features like word order and the presence of case markers. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "ee219d599e0e0c2bbebc1849863005_2",
  "x": "---------------------------------- **DATASET AND EXPERIMENTAL SETUP** Typology Database: To perform our analysis, we use the URIEL language typology database<cite> (Littell et al., 2017)</cite> , which is a collection of binary features extracted from multiple typological, phylogenetic, and geographical databases such as WALS (World Atlas of Language Structures) (Collins and Kayne, 2011) , PHOIBLE (Moran et al., 2014) , Ethnologue (Lewis et al., 2015) , and Glottolog (Hammarstr\u00f6m et al., 2015) .",
  "y": "uses"
 },
 {
  "id": "ee219d599e0e0c2bbebc1849863005_3",
  "x": "As an alternative that does not necessarily require pre-existing knowledge of the typological features in the language at hand,<cite> Littell et al. (2017)</cite> have proposed a method for inferring typological features directly from the language's k nearest neighbors (k-NN) according to geodesic distance (distance on the Earth's surface) and genetic distance (distance according to a phylogenetic family tree). In our experiments, our baseline uses this method by taking the 3-NN for each language according to normalized geodesic+genetic distance, and calculating an average feature vector of these three neighbors. Typology Prediction: To perform prediction, we trained a logistic regression classifier 3 with the baseline k-NN feature vectors described above and the proposed NMT feature vectors described in the next section.",
  "y": "background"
 },
 {
  "id": "ee219d599e0e0c2bbebc1849863005_4",
  "x": "As an alternative that does not necessarily require pre-existing knowledge of the typological features in the language at hand,<cite> Littell et al. (2017)</cite> have proposed a method for inferring typological features directly from the language's k nearest neighbors (k-NN) according to geodesic distance (distance on the Earth's surface) and genetic distance (distance according to a phylogenetic family tree). In our experiments, our baseline uses this method by taking the 3-NN for each language according to normalized geodesic+genetic distance, and calculating an average feature vector of these three neighbors. Typology Prediction: To perform prediction, we trained a logistic regression classifier 3 with the baseline k-NN feature vectors described above and the proposed NMT feature vectors described in the next section.",
  "y": "uses"
 },
 {
  "id": "eebf1edb6dbd3e58a904eff309f548_0",
  "x": "The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a) , and dependencybased compositional semantics (Liang et al., 2011; <cite>Berant et al., 2013</cite>; Berant and Liang, 2014) . We characterize semantic parsing as the task of deriving a representation of meaning from language, sufficient for a given task.",
  "y": "background"
 },
 {
  "id": "eebf1edb6dbd3e58a904eff309f548_1",
  "x": "Our question is then: what is the difference between an IE system with access to syntax, as compared to a semantic parser, when both are targeting a factoid-extraction style task? While our conclusions should hold generally for similar KBs, we will focus on Freebase, such as explored by Krishnamurthy and Mitchell (2012) , and then others such as Cai and Yates (2013a) and<cite> Berant et al. (2013)</cite> . We compare two open-source, state-ofthe-art systems on the task of Freebase QA: the semantic parsing system SEMPRE<cite> (Berant et al., 2013)</cite> , and the IE system jacana-freebase (Yao and Van Durme, 2014) .",
  "y": "uses"
 },
 {
  "id": "eebf1edb6dbd3e58a904eff309f548_2",
  "x": "We compare two open-source, state-ofthe-art systems on the task of Freebase QA: the semantic parsing system SEMPRE<cite> (Berant et al., 2013)</cite> , and the IE system jacana-freebase (Yao and Van Durme, 2014) . We find that these two systems are on par with each other, with no significant differences in terms of accuracy between them. A major distinction between the work of<cite> Berant et al. (2013)</cite> and Yao and Van Durme (2014) is the ability of the former to represent, and compose, aggregation operators (such as argmax, or count), as well as integrate disparate pieces of information.",
  "y": "uses"
 },
 {
  "id": "eebf1edb6dbd3e58a904eff309f548_3",
  "x": "We find that these two systems are on par with each other, with no significant differences in terms of accuracy between them. A major distinction between the work of<cite> Berant et al. (2013)</cite> and Yao and Van Durme (2014) is the ability of the former to represent, and compose, aggregation operators (such as argmax, or count), as well as integrate disparate pieces of information. This representational capability was important in previous, closed-domain tasks such as GeoQuery.",
  "y": "background"
 },
 {
  "id": "eebf1edb6dbd3e58a904eff309f548_4",
  "x": "In this way the association between the question and answer type is enforced. Thus during decoding, for instance, if there is a who question, the nodes with a person property would be ranked higher as the answer candidate. SEMPRE 3 is an open-source system for training semantic parsers, that has been utilized to train a semantic parser against Freebase by<cite> Berant et al. (2013)</cite> .",
  "y": "background"
 },
 {
  "id": "eebf1edb6dbd3e58a904eff309f548_5",
  "x": "Both<cite> Berant et al. (2013)</cite> and Yao and Van Durme (2014) tested their systems on the WEBQUESTIONS dataset, which contains 3778 training questions and 2032 test questions collected from the Google Suggest API. Each question came with a standard answer from Freebase annotated by Amazon Mechanical Turk. Berant et al. (2013) reported a score of 31.4% in terms of accuracy (with partial credit if inexact match) on the test set and later in Berant and Liang (2014) revised it to 35.7%.",
  "y": "background"
 },
 {
  "id": "ef6bd5e57196c013d7d0436e5b0ca5_0",
  "x": "The increasing amounts of textual information on the Web have brought demands to develop techniques to extract and verify a fact. The Fact Extraction and VERification (FEVER) task <cite>(Thorne et al., 2018)</cite> focuses on verification of textual claims against evidence. In the FEVER shared task, a given claim is classified as SUPPORTED, REFUTED, or NOTENOUGHINFO (NEI).",
  "y": "background"
 },
 {
  "id": "ef6bd5e57196c013d7d0436e5b0ca5_1",
  "x": "The Fact Extraction and VERification (FEVER) task <cite>(Thorne et al., 2018)</cite> focuses on verification of textual claims against evidence. This paper describes our participating system in the FEVER shared task.",
  "y": "uses background"
 },
 {
  "id": "ef6bd5e57196c013d7d0436e5b0ca5_2",
  "x": "The evidence is not given and must be retrieved from Wikipedia. This paper describes our participating system in the FEVER shared task. The architecture of our system is designed by following the official baseline system <cite>(Thorne et al., 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "ef6bd5e57196c013d7d0436e5b0ca5_3",
  "x": "As RTE component, we adopt DEISTE (Deep Explorations of Inter-Sentence interactions for Textual Entailment) model that is the state-of-the-art in RTE tasks (Yin et al., 2018) . RTE component is trained on labeled claims paired with sentencelevel evidence. To build the model, we utilize the NEARESTP dataset described in<cite> Thorne et al. (2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "ef6bd5e57196c013d7d0436e5b0ca5_4",
  "x": "**DATASET** We used official training dataset for training RTE component. For parameter tuning and performance evaluation, we used a development and test datasets used in <cite>(Thorne et al., 2018)</cite> .",
  "y": "uses"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_0",
  "x": "We show that a neural approach to the task of non-factoid answer reranking can benefit from the inclusion of tried-and-tested handcrafted features. We present a novel neural network architecture based on a combination of recurrent neural networks that are used to encode questions and answers, and a multilayer perceptron. We show how this approach can be combined with additional features, in particular, the discourse features presented by<cite> Jansen et al. (2014)</cite> .",
  "y": "uses"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_1",
  "x": "Most previous attempts to perform non-factoid answer reranking on CQA data are supervised, feature-based, learning-to-rank approaches<cite> (Jansen et al., 2014</cite>; Fried et al., 2015; Sharp et al., 2015) . These methods represent the candidate answers as meaningful handcrafted features based on syntactic, semantic and discourse parses (Surdeanu et al., 2011; <cite>Jansen et al., 2014)</cite> , web correlation (Surdeanu et al., 2011) , and translation probabilities (Fried et al., 2015; Surdeanu et al., 2011) . The resulting feature vectors are then passed to a supervised ranking algorithm, such as SVMrank (Joachims, 2006) , which ranks the candidates.",
  "y": "background"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_2",
  "x": "Given a question, it aims to rearrange the answers in order to boost the community-selected best answer to the top position. Most previous attempts to perform non-factoid answer reranking on CQA data are supervised, feature-based, learning-to-rank approaches<cite> (Jansen et al., 2014</cite>; Fried et al., 2015; Sharp et al., 2015) . These methods represent the candidate answers as meaningful handcrafted features based on syntactic, semantic and discourse parses (Surdeanu et al., 2011; <cite>Jansen et al., 2014)</cite> , web correlation (Surdeanu et al., 2011) , and translation probabilities (Fried et al., 2015; Surdeanu et al., 2011) .",
  "y": "background"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_3",
  "x": "First, we present a novel neural approach to answer reranking that achieves competitive results on a public dataset of Yahoo! Answers (YA) that was previously introduced by<cite> Jansen et al. (2014)</cite> and later used in several other studies (Fried et al., 2015; Sharp et al., 2015; Bogdanova and Foster, 2016) . Our approach is based on a combination of recurrent neural networks (RNN) and a multilayer perceptron (MLP) that receives the encodings produced by the RNNs and interaction transformation features that are based on the outputs of the RNNs and which aim to represent the semantic interaction between the encoded sequences. We also show how this approach can be combined with discourse features previously shown to be beneficial for the task of answer reranking.",
  "y": "uses"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_4",
  "x": "These are the subjective nature of both the questions and the user choice of the best answer. The main contributions of this paper are as follows: 1) we propose a novel neural approach for non-factoid answer reranking that achieves state-4 http://askubuntu.com of-the-art performance on a public dataset of Yahoo! Answers; 2) we combine this approach with an approach based on discourse features that was introduced by<cite> Jansen et al. (2014)</cite> , with the hybrid approach outperforming the neural approach and the previous state-of-the-art; 3) we introduce a new dataset of Ask Ubuntu questions and answers. This paper is organized as follows: an overview of previous work on non-factoid question answering is provided in Section 2, our neural architecture is introduced in Section 3, the discourse features that are incorporated into our neural approach are described in Section 4, the results of our experiments with these new models are presented and analysed in Section 5, and suggestions for further research are provided in Section 6.",
  "y": "extends"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_5",
  "x": "In particular, they show how discourse information can complement distributed lexical semantic information obtained with a skip-gram model (Mikolov et al., 2013) . In this paper we use their features (discussed in detail in Section 4) in combination with a neural approach. Fried et al. (2015) improve on the lexical semantic models of<cite> Jansen et al. (2014)</cite> by exploiting indirect associations between words using higher-order models.",
  "y": "background"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_6",
  "x": "**DISCOURSE FEATURES** Based on the intuition that modelling questionanswer structure both within and across sentences could be useful,<cite> Jansen et al. (2014)</cite> propose an answer reranking model based on discourse features combined with lexical semantics. We experimentally evaluate these discourse features -added to our model described in Section 3 (the additional features x ext ) and on their own.",
  "y": "background"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_7",
  "x": "Based on the intuition that modelling questionanswer structure both within and across sentences could be useful,<cite> Jansen et al. (2014)</cite> propose an answer reranking model based on discourse features combined with lexical semantics. We experimentally evaluate these discourse features -added to our model described in Section 3 (the additional features x ext ) and on their own. We reuse their discourse marker model (DMM) combined with their lexical semantics model (LS).",
  "y": "uses background"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_8",
  "x": "For English, these markers include by, as, because, but, and, for and of -the full list can be found in Appendix B in (Marcu, 1998) . We illustrate the feature extraction process of<cite> Jansen et al. (2014)</cite> in Figure 2 . First, the answer is searched for discourse markers.",
  "y": "uses"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_9",
  "x": "To assign values to each feature, the similarity between the question and each of the two arguments is computed, and the average similarity is assigned as the value of the feature. Jansen et al. (2014) use cosine similarity over tf.idf and over the vector space built with a skipgram model (Mikolov et al., 2013) . Further details can be found in<cite> (Jansen et al., 2014)</cite> .",
  "y": "background"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_10",
  "x": "In our experiments, we use two datasets from different CQAs. For comparability, we use the dataset created by<cite> Jansen et al. (2014)</cite> which contains 10K how questions from Yahoo! Answers. 50% of it is used for training, 25% for development and 25% for testing. Each question in this dataset contains at least four user-generated answers.",
  "y": "uses"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_11",
  "x": "Some examples can be found in Table 1 . Further details about this dataset can be found in<cite> (Jansen et al., 2014)</cite> . To evaluate our approach on a more technical domain, we create a dataset of Ask Ubuntu (AU) questions containing 13K questions, of which 10K are used for training, 0.5K for development and 2.5K for testing.",
  "y": "background"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_12",
  "x": "For comparability, we use the dataset created by<cite> Jansen et al. (2014)</cite> which contains 10K how questions from Yahoo! Answers. 50% of it is used for training, 25% for development and 25% for testing. Further details about this dataset can be found in<cite> (Jansen et al., 2014)</cite> .",
  "y": "uses background"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_13",
  "x": "Following<cite> Jansen et al. (2014)</cite> and Fried et al. (2015) , we implement two baselines: the baseline that selects an answer randomly and the candidate retrieval (CR) baseline. The CR baseline uses the same scoring as in<cite> Jansen et al. (2014)</cite> : the questions and the candidate answers are represented using tf-idf over lemmas; the candidate answers are ranked according to their cosine similarity to the respective question. Additionally, we evaluate the discourse features described in Section 4 alone: we use them as the representation of the question-answer pairs that are then used as the input to a multilayer perceptron with five hidden layers.",
  "y": "uses"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_14",
  "x": "Following<cite> Jansen et al. (2014)</cite> and Fried et al. (2015) , we implement two baselines: the baseline that selects an answer randomly and the candidate retrieval (CR) baseline. The CR baseline uses the same scoring as in<cite> Jansen et al. (2014)</cite> : the questions and the candidate answers are represented using tf-idf over lemmas; the candidate answers are ranked according to their cosine similarity to the respective question. Additionally, we evaluate the discourse features described in Section 4 alone: we use them as the representation of the question-answer pairs that are then used as the input to a multilayer perceptron with five hidden layers.",
  "y": "uses background"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_15",
  "x": "The CR baseline uses the same scoring as in<cite> Jansen et al. (2014)</cite> : the questions and the candidate answers are represented using tf-idf over lemmas; the candidate answers are ranked according to their cosine similarity to the respective question. Additionally, we evaluate the discourse features described in Section 4 alone: we use them as the representation of the question-answer pairs that are then used as the input to a multilayer perceptron with five hidden layers. On the YA dataset, we also compare our results to the ones reported by<cite> Jansen et al. (2014)</cite> and by Bogdanova and Foster (2016) .",
  "y": "uses"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_16",
  "x": "All neural networks use the rectified linear activation function (ReLU). The word embeddings are initialized randomly, no pretrained embeddings are used. We use the software provided by<cite> Jansen et al. (2014)</cite> 7 to extract the discourse features described in Section 4 and referred to as x ext in Section 3.",
  "y": "uses"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_17",
  "x": "These discourse features require that word embeddings be trained in order to calculate the similarity. Following<cite> Jansen et al. (2014)</cite> , we train them using the skip-gram model (Mikolov et al., 2013) We use the L6 Yahoo dataset 8 to train the skip-gram model for the YA dataset and the Ask Ubuntu September 2015 data dump for the AU dataset. The neural model described in Section 3 does not require pretraining of word embeddings, the embeddings are used only to extract external discourse features.",
  "y": "uses"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_18",
  "x": "9 Table 3 shows that the discourse features on their own with an MLP (MLP-Discourse) outperform the random and the CR baselines for both datasets. They also perform better than the approach of<cite> Jansen et al. (2014)</cite> who used SVMrank with a linear kernel. This might be due to the ability of the MLP to model non-linear dependencies.",
  "y": "differences"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_19",
  "x": "The GRU-MLP systems does not use any external data, and learns only from the small training set. The system enriched with the interaction matrix, GRU-MLP-Sim, clearly outperforms all the baselines on both datasets, including the MLPDiscourse system. On the YA dataset, the results are better than<cite> Jansen et al. (2014)</cite> and very similar to Bogdanova and Foster (2016) .",
  "y": "differences"
 },
 {
  "id": "ef742defff1c2bdf145f72796cf3af_20",
  "x": "recurrent neural networks, then the interaction matrix is calculated, concatenated with external features, and passed as an input to a multilayer perceptron. As external features, we evaluate the discourse features that were found useful for this task by<cite> Jansen et al. (2014)</cite> . The combined approach achieves new state-of-the-art results on two CQA datasets.",
  "y": "uses"
 },
 {
  "id": "f2925513a7cce2e80ade1f948164d0_0",
  "x": "In the domain of text, many modern approaches often begin by embedding the input text data into an embedding space that is used as the first layer in a subsequent deep network [4] , [14] . These word embeddings have been shown to contain the same biases <cite>[3]</cite> , due to the source data from which they are trained. In effect, biases from the source data, such as in the differences in representation for men and women, that have been found in many different large-scale studies [5] , [10] , [12] , carry through to the semantic relations in the word embeddings, which become baked into the learning systems that are built on top of them.",
  "y": "background motivation"
 },
 {
  "id": "f2925513a7cce2e80ade1f948164d0_1",
  "x": "First we propose a new version of the Word Embedding Association Tests (WEATs) studied in <cite>[3]</cite> , designed to demonstrate and quantify bias in word embeddings, which puts them on a firm foundation by using the Linguistic Inquiry and Word Count (LIWC) lexica [17] to systematically detect and measure embedding biases. With this improved experimental setting, we find that European-American names are viewed more positively than African-American names, male names are more associated with work while female names are more associated with family, and that the academic disciplines of science and maths are more associated with male terms than the arts, which are more associated with female terms. Using this new methodology, we then find that there is a gender bias in the way different occupations are represented by the embedding.",
  "y": "differences background motivation"
 },
 {
  "id": "f2925513a7cce2e80ade1f948164d0_2",
  "x": "We first propose a new version of the Word Embedding Association Tests studied in <cite>[3]</cite> by using the LIWC lexica to systematically detect and measure the biases within the embedding, keeping the tests comparable with the same set of target words. We further extend this work using additional sets of target words, and compare sentiment across male and female names. Furthermore, we investigate gender bias in words that represent different occupations, comparing these associations with UK national employment statistics.",
  "y": "extends background"
 },
 {
  "id": "f2925513a7cce2e80ade1f948164d0_3",
  "x": "We begin by using the target words from <cite>[3]</cite> which were originally used in [8] , allowing us to directly compare our findings with the original WEAT. Our approach differs from that of <cite>[3]</cite> in that while we use the same set of target words in each test, we use an expanded set of attribute words, allowing us to perform a more rigorous, systematic study of the associations found within the word embeddings. For this, we use attribute words sourced from the LIWC lexica [17] .",
  "y": "extends"
 },
 {
  "id": "f2925513a7cce2e80ade1f948164d0_4",
  "x": "The categories specified in the LIWC lexica are based on many factors, including emotions, thinking styles, and social concerns. For each of the original word categories used in <cite>[3]</cite> , we matched them with their closest equivalent within the LIWC categories, for example matching the word lists for 'career' and 'family' with the 'work' and 'family' LIWC categories. We tested the association between each target word and the set of attribute words using the method described in Sec. II-B, focussing on the differences in association between sentimental terms and European-and African-American names, subject disciplines to each of the genders, career and family terms with gendered names, as well as looking at the association between gender and sentiment.",
  "y": "extends"
 },
 {
  "id": "f2925513a7cce2e80ade1f948164d0_5",
  "x": "**1) ASSOCIATION OF EUROPEAN AND AFRICAN-AMERICAN NAMES WITH SENTIMENT :** Taking the list of target European-American and African-American names used in <cite>[3]</cite> , we tested each of them for their associated with the positive and negative emotion concepts found in [17] by using the methodology described by Eq. 3 in Sec. II-B, replacing the short list of words used to originally represent pleasant and unpleasant attribute sets. Our test found that while both European-American names and African-American names are more associated with positive emotions than negative emotions, the test showed that European-American names are more associated with positive emotions than their African-American counterparts, as shown in Fig. 1a .",
  "y": "uses"
 },
 {
  "id": "f2925513a7cce2e80ade1f948164d0_6",
  "x": "Our test found that while both European-American names and African-American names are more associated with positive emotions than negative emotions, the test showed that European-American names are more associated with positive emotions than their African-American counterparts, as shown in Fig. 1a . This finding supports the association test in <cite>[3]</cite> , where they also found that European-American names were more pleasant than African-American names. ----------------------------------",
  "y": "similarities"
 },
 {
  "id": "f2925513a7cce2e80ade1f948164d0_8",
  "x": "A further test was conducted to find the association between words related to different subject disciplines (e.g. arts, maths, science) with each of the genders using the 'he' and 'she' categories from LIWC [17] . The results of our test again support the findings of <cite>[3]</cite> , with Maths and Science terms being more closely associated with males, while Arts terms are more closely associated with females, as shown in Fig. 1b. 3) Association of Gender with Career and Family : Taking the list of target gendered names used in <cite>[3]</cite> , we tested each of them for their associated with the career and family concepts using the categories of 'work' and 'family' found in LIWC [17] .",
  "y": "uses"
 },
 {
  "id": "f2925513a7cce2e80ade1f948164d0_9",
  "x": "As shown in Fig. 1c , we found that the set of male names was more associated with the concept of work, while the female names were more associated with family, mirroring the results found in <cite>[3]</cite> . Extending this test, we generated a much larger set of male and female target names from an online list of baby names 1 . Repeating the same test on this larger set of names, we found that male and female names were much less separated than suggested by previous results, with only minor differences between the two, as shown in Fig. 1d .",
  "y": "extends"
 },
 {
  "id": "f2925513a7cce2e80ade1f948164d0_10",
  "x": "We found that there is a strong, significant correlation (\u03c1 = 0.57, p-value < 10 \u22126 ) between the word embedding association between gender and occupation and the number of people of each gender in the United Kingdom working in those roles. This supports a similar finding for U.S. employment statistics using an independent set of occupations found in <cite>[3]</cite> . ----------------------------------",
  "y": "similarities background"
 },
 {
  "id": "f2925513a7cce2e80ade1f948164d0_12",
  "x": "In this paper, we have introduced the LIWC-WEAT, a set of objective tests extending the association tests in <cite>[3]</cite> by using the LIWC lexica to measure bias within word embeddings. We found bias in both the associations of gender and race, as first described in <cite>[3]</cite> , while additionally finding that male names have a slightly higher positive association than female names. Biases found in the embedding were also shown to reflect biases in the real world and the media, where we found a correlation between the number of men and women in an occupation and its association with each set of male and female names.",
  "y": "background motivation"
 },
 {
  "id": "f2925513a7cce2e80ade1f948164d0_13",
  "x": "We found bias in both the associations of gender and race, as first described in <cite>[3]</cite> , while additionally finding that male names have a slightly higher positive association than female names. Biases found in the embedding were also shown to reflect biases in the real world and the media, where we found a correlation between the number of men and women in an occupation and its association with each set of male and female names. Finally, using a projection algorithm [2] , we were able to reduce the gender bias shown in the embeddings, resulting in a decrease in the difference between associations for all tests based upon gender.",
  "y": "differences background"
 },
 {
  "id": "f2db88c0d4e0ec4c34fc295a5d59ba_1",
  "x": "These constraints can be lexicalized (Collins, 1999; Charniak, 2000) , unlexicalized (Johnson, 1998; Klein and Manning, 2003b) or automatically learned (Matsuzaki et al., 2005; <cite>Petrov et al., 2006)</cite> . The constraints serve the purpose of weakening the independence assumptions, and reduce the number of possible (but incorrect) parses. Here, we focus on the latent variable approach of<cite> Petrov et al. (2006)</cite> , where an Expectation Maximization (EM) algorithm is used to induce a hierarchy of increasingly more refined grammars.",
  "y": "background"
 },
 {
  "id": "f2db88c0d4e0ec4c34fc295a5d59ba_4",
  "x": "Latent variable grammars split the coarse (but observed) grammar categories of a treebank into more fine-grained (but hidden) subcategories, which are better suited for modeling the syntax of natural languages (e.g. NP becomes NP 1 through NP k ). Accordingly, each grammar production A\u2192BC over observed categories A,B,C is split into a set of productions A x \u2192B y C z over hidden categories A x ,B y ,C z . Computing the joint likelihood of the observed parse trees T and sentences w requires summing over all derivations t over split subcategories: Matsuzaki et al. (2005) derive an EM algorithm for maximizing the joint likelihood, and<cite> Petrov et al. (2006)</cite> extend this algorithm to use a split&merge procedure to adaptively determine the optimal number of subcategories for each observed category.",
  "y": "background"
 },
 {
  "id": "f2db88c0d4e0ec4c34fc295a5d59ba_5",
  "x": "While the split&merge procedure described above is shown in<cite> Petrov et al. (2006)</cite> to reduce the variance in final performance, we found after closer examination that there are substantial differences in the patterns learned by the grammars. Since the initialization is not systematically biased in any way, one can obtain different grammars by simply changing the seed of the random number generator. We trained 16 different grammars by initializing the random number generator with seed values 1 through 16, but without biasing the initialization in any other way.",
  "y": "differences"
 },
 {
  "id": "f2db88c0d4e0ec4c34fc295a5d59ba_6",
  "x": "While the parsing accuracies are consistently high, 2 there is only a weak correlation between the accuracies on the two evaluation sets (Pearson coefficient 0.34). This suggests that no single grammar should be preferred over the others. In previous work<cite> (Petrov et al., 2006</cite>; Petrov and Klein, 2007 ) the final grammar was chosen based on its performance on a held-out set (section 22), and corresponds to the second best grammar in Figure 3 (because only 8 different grammars were trained).",
  "y": "background"
 },
 {
  "id": "f2db88c0d4e0ec4c34fc295a5d59ba_7",
  "x": "Using weights learned on a held-out set and rescoring 50-best lists from Charniak (2000) and<cite> Petrov et al. (2006)</cite> , they obtain an F 1 score of 91.0 (which they further improve to 91.4 using a voting scheme). We replicated their experiment, but used an unweighted product of the two model scores. Using TREE-LEVEL inference, we obtained an F 1 score of 91.6, suggesting that weighting is not so important in the product case, as long as the classifiers are of comparable quality.",
  "y": "background"
 },
 {
  "id": "f2db88c0d4e0ec4c34fc295a5d59ba_8",
  "x": "The parameters of each latent variable grammar are typically smoothed in a linear fashion to prevent excessive overfitting<cite> (Petrov et al., 2006)</cite> . While all the experiments so far used smoothed grammars, we reran the experiments also with a set of unsmoothed grammars. The individual unsmoothed grammars have on average an 1.2% lower accuracy.",
  "y": "background"
 },
 {
  "id": "f2db88c0d4e0ec4c34fc295a5d59ba_9",
  "x": "Furthermore, those techniques have largely relied on different voting schemes in the past (Henderson and Brill, 1999; Sagae and Lavie, 2006) , and only more recently have started using actual posteriors from the underlying models (Fossum and Knight, 2009; Zhang et al., 2009) . Even then, those methods operate only over k-best lists, and we are the first to work directly with parse forests from multiple grammars. It is also interesting to note that the best results in Zhang et al. (2009) are achieved by combining kbest lists from a latent variable grammar of<cite> Petrov et al. (2006)</cite> with the self-trained reranking parser of McClosky et al. (2006) .",
  "y": "background"
 },
 {
  "id": "f2ff155003d139b3677f746baf3807_0",
  "x": "In this paper, we followed the line of predicting ICD codes from unstructured text of the MIMIC dataset (Johnson et al. 2016 ), because it is widely studied and publicly available. The state-of-the-art model for this line of work is the combination of the convolutional neural network (CNN) and the attention mechanism<cite> (Mullenbach et al. 2018)</cite> . However, this model only contains one convolutional layer to build document representations for subsequent layers to predict ICD codes.",
  "y": "background motivation"
 },
 {
  "id": "f2ff155003d139b3677f746baf3807_1",
  "x": "In this paper, we proposed a Multi-Filter Residual Convolutional Neural Network (MultiResCNN) for ICD coding using clinical discharge summaries. Our Mul-arXiv:1912.00862v1 [cs.CL] 25 Nov 2019 tiResCNN model is composed of five layers: the input layer leverages word embeddings pre-trained by word2vec (Mikolov et al. 2013) ; the multi-filter convolutional layer consists of multiple convolutional filters (Kim 2014); the residual convolutional layer contains multiple residual blocks (He et al. 2016) ; the attention layer keeps the interpretability for the model following<cite> (Mullenbach et al. 2018)</cite> ; the output layer utilizes the sigmoid function to predict the probability of each ICD code. Our main contribution is that we proposed a novel CNN architecture that combines the multi-filter CNN (Kim 2014) and residual CNN (He et al. 2016) .",
  "y": "similarities background"
 },
 {
  "id": "f2ff155003d139b3677f746baf3807_2",
  "x": "To evaluate our model, we employed the MIMIC dataset (Johnson et al. 2016 ) which has been widely used for automated ICD coding. Compared with 5 existing and stateof-the-art models (Perotte et al. 2013; Prakash et al. 2017; Shi et al. 2017; Baumel et al. 2018;<cite> Mullenbach et al. 2018)</cite> , our model outperformed them in nearly all the evaluation metrics (i.e., macro-and micro-AUC, macro-and micro-F1, precision at K). Concretely, in the MIMIC-III experiment using full codes, our model outperformed these models in macro-AUC, micro-F1 and precision at 8 and 15.",
  "y": "background"
 },
 {
  "id": "f2ff155003d139b3677f746baf3807_3",
  "x": "Baumel et al. (2018) proposed a hierarchical gated recurrent unit (GRU) network, which encodes sentences and documents with two stacked layers, to assign multiple ICD codes to discharge summaries of the MIMIC II and III datasets. <cite>Mullenbach et al. (2018)</cite> incorporated the convolutional neural network (CNN) with per-label attention mechanism. Their model achieved the state-of-the-art performance among the work using only unstructured text of the MIMIC dataset.",
  "y": "background"
 },
 {
  "id": "f2ff155003d139b3677f746baf3807_4",
  "x": "**ATTENTION LAYER** Following <cite>Mullenbach et al. (2018)</cite> , we employed the perlabel attention mechanism to make each ICD code attend to different parts of the document representation H. The attention layer is formalized as: where U \u2208 R (m\u00d7d p )\u00d7l represents the parameter matrix of the attention layer, A \u2208 R n\u00d7l represents the attention weights for each pair of an ICD code and a word, V \u2208 R l\u00d7(m\u00d7d p ) represents the output of the attention layer.",
  "y": "uses"
 },
 {
  "id": "f2ff155003d139b3677f746baf3807_5",
  "x": "For training, we treated the ICD coding task as a multi-label classification problem following previous work (McCallum 1999;<cite> Mullenbach et al. 2018)</cite> . The training objective is to minimize the binary cross entropy loss between the prediction\u1ef9 and the target y: where w denotes the input word sequence and \u03b8 denotes all the parameters.",
  "y": "uses"
 },
 {
  "id": "f2ff155003d139b3677f746baf3807_6",
  "x": "**EXPERIMENTS** Datasets MIMIC-III In this paper, we employed the third version of Medical Information Mart for Intensive Care (MIMIC-III) (Johnson et al. 2016 ) as the first dataset to evaluate our models. Following <cite>Mullenbach et al. (2018)</cite> , we used discharge summaries, split them by patient IDs, and conducted experiments using the full codes as well as the top-50 most frequent codes.",
  "y": "uses"
 },
 {
  "id": "f2ff155003d139b3677f746baf3807_8",
  "x": "Preprocessing Following previous work<cite> (Mullenbach et al. 2018)</cite> , the text was tokenized, and each token were transformed into its lowercase. The tokens that contain no alphabetic characters were removed such as numbers and punctuations. The maximum length of a token sequence is 2,500 and the one that exceeds this length will be truncated.",
  "y": "uses"
 },
 {
  "id": "f2ff155003d139b3677f746baf3807_9",
  "x": "**HYPER-PARAMETER TUNING** Since our model has a number of hyper-parameters, it is infeasible to search optimal values for all hyper-parameters. Therefore, some hyper-parameter values were chosen empirically or following prior work<cite> (Mullenbach et al. 2018</cite> ).",
  "y": "uses background"
 },
 {
  "id": "f2ff155003d139b3677f746baf3807_10",
  "x": "To explore a better configuration for the filter number m and the kernel sizes k 1 , k 2 , ..., k m in the multi-filter convolutional layer, and the residual block number p in the residual convolutional layer, we conducted the following experiments. First, we developed three variations: \u2022 CNN, which only has one convolutional filter and is equivalent to the CAML model<cite> (Mullenbach et al. 2018</cite> ).",
  "y": "uses"
 },
 {
  "id": "f2ff155003d139b3677f746baf3807_11",
  "x": "CAML & DR-CAML The Convolutional Attention network for Multi-Label classification (CAML) was proposed by <cite>Mullenbach et al. (2018)</cite> . It has achieved the state-of-theart results on the MIMIC-III and MIMIC-II datasets among the models using unstructured text. It consists of one convolutional layer and one attention layer to generate label-aware features for multi-label classification (McCallum 1999). The Description Regularized CAML (DR-CAML) is an extension of CAML and incorporates the text description of each code to regularize the model.",
  "y": "background"
 },
 {
  "id": "f2ff155003d139b3677f746baf3807_13",
  "x": "For CAML, we used the optimal hyper-parameter setting reported in their paper<cite> (Mullenbach et al. 2018)</cite> . For Mul-tiResCNN, we used six filters and 1 residual block, which obtained the best result in our hyper-parameter tuning experiments. The batch size, learning rate and dropout rate are identical in every experiment.",
  "y": "uses"
 },
 {
  "id": "f3012301e42a4075ed6d4d2b39b528_0",
  "x": "The past work in sarcasm detection involves rule-based and statistical approaches using: (a) unigrams and pragmatic features (such as emoticons, etc.) (Gonzalez-Ibanez et al., 2011; Carvalho et al., 2009; Barbieri et al., 2014) , (b) extraction of common patterns, such as hashtag-based sentiment (Maynard and Greenwood, 2014; Liebrecht et al., 2013) , a positive verb being followed by a negative situation<cite> (Riloff et al., 2013)</cite> , or discriminative n-grams (Tsur et al., 2010a; Davidov et al., 2010) . Thus, the past work detects sarcasm with specific indicators. However, we believe that it is time that sarcasm detection is based on well-studied linguistic theories.",
  "y": "background"
 },
 {
  "id": "f3012301e42a4075ed6d4d2b39b528_1",
  "x": "\u2022 We present a sarcasm detection system that is grounded on a linguistic theory, the theory of context incongruity in our case. Sarcasm detection research can push the frontiers by taking help of well-studied linguistic theories. \u2022 Our sarcasm detection system outperforms two state-of-art sarcasm detection systems <cite>(Riloff et al., 2013</cite>; Maynard and Greenwood, 2014) .",
  "y": "differences"
 },
 {
  "id": "f3012301e42a4075ed6d4d2b39b528_2",
  "x": "Our approach is architecturally similar to Tsur et al. (2010b) who use a semi-supervised pattern acquisition followed by classification. Our feature engineering is based on<cite> Riloff et al. (2013)</cite> and Ramteke et al. (2013) . Riloff et al. (2013) state that sarcasm is a contrast between positive sentiment word and a negative situation.",
  "y": "similarities uses"
 },
 {
  "id": "f3012301e42a4075ed6d4d2b39b528_3",
  "x": "These phrases are sentiment-bearing verb and noun phrases, the latter being situations with implied sentiment (e.g. 'getting late for work'). For this, we modify the algorithm given in<cite> Riloff et al. (2013)</cite> in two ways: (a) they extract only positive verbs and negative noun situation phrases. We generalize it to both polarities, (b) they remove subsumed phrases (i.e. 'being ignored' subsumes 'being ignored by a friend') while we retain both phrases.",
  "y": "extends differences"
 },
 {
  "id": "f3012301e42a4075ed6d4d2b39b528_4",
  "x": "2. Tweet-B (2278 tweets, 506 sarcastic): This dataset was manually labeled for<cite> Riloff et al. (2013</cite> To extract the implicit incongruity features, we run the iterative algorithm described in Section 4.2, on a dataset of 4000 tweets (50% sarcastic) (also created using hashtag-based supervision). The algorithm results in a total of 79 verb phrases and 202 noun phrases. We train our classifiers for different feature combinations, using LibSVM with RBF kernel (Chang and Lin, 2011) , and report average 5-fold cross-validation values.",
  "y": "uses similarities"
 },
 {
  "id": "f3012301e42a4075ed6d4d2b39b528_5",
  "x": "Table 2 : Comparative results for Tweet-A using rule-based algorithm and statistical classifiers using our feature combinations 6 Evaluation Table 2 shows the performance of our classifiers in terms of Precision (P), Recall (R) and F-score<cite> Riloff et al. (2013)</cite> 's two rule-based algorithms: the ordered version predicts a tweet as sarcastic if it has a positive verb phrase followed by a negative situation/noun phrase, while the unordered does so if the two are present in any order. We see that all statistical classifiers surpass the rule-based algorithms. The best F-score obtained is 0.8876 when all four kinds of features are used.",
  "y": "differences"
 },
 {
  "id": "f3012301e42a4075ed6d4d2b39b528_6",
  "x": "This is an improvement of about 5% over the baseline, and 40% over the algorithm by<cite> Riloff et al. (2013)</cite> . Table 3 shows that even in the case of the Discussion-A dataset, our features result in an improved performance. The F-score increases from 0.568 to 0.640, an improvement of about 8% in case of discussion forum posts, when all features are used.",
  "y": "extends differences"
 },
 {
  "id": "f3012301e42a4075ed6d4d2b39b528_7",
  "x": "For example, we reimplement their algorithm but do not have Table 4 : Comparison of our system with two past works, for Tweet-B access to their exact extracted phrases. Table 4 shows that we achieve a 10% higher F-score than the best reported F-score of<cite> Riloff et al. (2013)</cite> . This value is also 20% higher than our re-implementation of Maynard and Greenwood (2014) that uses their hashtag retokenizer and rulebased algorithm.",
  "y": "differences"
 },
 {
  "id": "f3012301e42a4075ed6d4d2b39b528_8",
  "x": "The corresponding improvement in case of discussion forum posts is 8%. Our system also outperforms two past works <cite>(Riloff et al., 2013</cite>; Maynard and Greenwood, 2014) with 10-20% improvement in F-score. Finally, to improve the performance for discussion forum posts, we introduce a novel approach to use elicitor posts for sarcasm detection.",
  "y": "differences"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_0",
  "x": "Following <cite>Gong et al. (2018)</cite> , we consider two document collections heterogeneous if <cite>their</cite> documents differ systematically with respect to vocabulary and / or level of abstraction. With these defining differences, there often also comes a difference in length, which, however, by itself does not make document collections heterogeneous. Examples include collections in which expert answers are mapped to non-expert questions (e.g. InsuranceQA by Feng et al. (2015) ), but also so-called community QA collections (Blooma and Kurian (2011) ), where the lexical mismatch between Q and A documents is often less pronounced than the length difference.",
  "y": "motivation background"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_1",
  "x": "We demonstrate our method with the Concept-Project matching task (<cite>Gong et al. (2018)</cite> ), which is described in the next section. ---------------------------------- **TASK, DATA SET, AND ORIGINAL APPROACH**",
  "y": "uses"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_2",
  "x": "The annotation was done by undergrad engineering students. <cite>Gong et al. (2018)</cite> do not provide any specification, or annotation guidelines, of the semantics of the 'matches' relation to be annotated. Instead, <cite>they</cite> create gold standard annotations based on a majority vote of three manual annotations.",
  "y": "differences"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_3",
  "x": "The concept labels can be very specific, potentially introducing vocabulary that is not present in the actual concept descriptions. The extent to which this information is used by <cite>Gong et al. (2018)</cite> is not entirely clear, so we experiment with several setups (cf. Section 4). ----------------------------------",
  "y": "motivation extends"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_4",
  "x": "---------------------------------- **<cite>GONG ET AL. (2018)</cite>'S APPROACH** The approach by <cite>Gong et al. (2018)</cite> is based on the idea that the longer document in the pair is reduced to a set of topics which capture the essence of the document in a way that eliminates the effect of a potential length difference.",
  "y": "background"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_5",
  "x": "In order to overcome the vocabulary mismatch, these topics are not based on words and their distributions (as in LSI (Deerwester et al. (1990) ) or LDA (Blei et al. (2003) )), but on word embedding vectors. Then, basically, matching is done by measuring the cosine similarity between the topic vectors and the short document words. <cite>Gong et al. (2018)</cite> motivate <cite>their</cite> approach mainly with the length mismatch argument, which <cite>they</cite> claim makes approaches relying on document representations (incl. vector averaging) unsuitable.",
  "y": "background"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_6",
  "x": "<cite>Gong et al. (2018)</cite> motivate <cite>their</cite> approach mainly with the length mismatch argument, which <cite>they</cite> claim makes approaches relying on document representations (incl. vector averaging) unsuitable. Accordingly, <cite>they</cite> use Doc2Vec (Le and Mikolov (2014) ) as one of their baselines, and show that its performance is inferior to <cite>their</cite> method. <cite>They</cite> do not, however, provide a much simpler averaging-based baseline.",
  "y": "background"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_7",
  "x": "Accordingly, <cite>they</cite> use Doc2Vec (Le and Mikolov (2014) ) as one of their baselines, and show that its performance is inferior to <cite>their</cite> method. <cite>They</cite> do not, however, provide a much simpler averaging-based baseline. As a second baseline, <cite>they</cite> use Word Mover's Distance (Kusner et al. (2015) ), which is based on word-level distances, rather than distance of global document representations, but which also fails to be competitive with <cite>their</cite> topic-based method.",
  "y": "background"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_8",
  "x": "Accordingly, <cite>they</cite> use Doc2Vec (Le and Mikolov (2014) ) as one of their baselines, and show that its performance is inferior to <cite>their</cite> method. <cite>They</cite> do not, however, provide a much simpler averaging-based baseline. As a second baseline, <cite>they</cite> use Word Mover's Distance (Kusner et al. (2015) ), which is based on word-level distances, rather than distance of global document representations, but which also fails to be competitive with <cite>their</cite> topic-based method.",
  "y": "background"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_9",
  "x": "<cite>They</cite> do not, however, provide a much simpler averaging-based baseline. As a second baseline, <cite>they</cite> use Word Mover's Distance (Kusner et al. (2015) ), which is based on word-level distances, rather than distance of global document representations, but which also fails to be competitive with <cite>their</cite> topic-based method. <cite>Gong et al. (2018)</cite> use two different sets of word embeddings: One (topic wiki) was trained on a full English Wikipedia dump, the other (wiki science) on a smaller subset of the former dump which only contained science articles.",
  "y": "background"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_10",
  "x": "We implement this standard measure (AVG COS SIM) as a baseline for both our method and for the method by <cite>Gong et al. (2018)</cite> . It yields a single scalar similarity score. The core idea of our alternative method is to turn the above process upside down, by computing the cosine similarity of selected pairs of words from c and p first, and to average over the similarity scores afterwards (cf. also Section 6).",
  "y": "extends"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_11",
  "x": "Parameter tuning experiments were performed on a random subset of 20% of our data set (54% positive). Note that <cite>Gong et al. (2018)</cite> used only 10% of <cite>their</cite> 537 instances data set as tuning data. The tuning data results of the best-performing parameter values for each setup can be found in Tables 1 and 2 .",
  "y": "differences"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_12",
  "x": "While the difference to the Google result for Label is only minimal, it is striking that the best overall score is again produced using the 'richest' setting, i.e. the one involving both TF and IDF weighting and the most informative input. We then selected the best performing parameter settings for every concept input and ran experiments on the held-out test data. Since the original data split used by <cite>Gong et al. (2018)</cite> is unknown, we cannot exactly replicate <cite>their</cite> settings, but we also perform ten runs using randomly selected 10% of our 408 instances test data set, and report average P, R, F, and standard deviation.",
  "y": "differences"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_14",
  "x": "The first interesting finding is that the AVG COS SIM measure again performs very well: In all three settings, it beats both the system based on general-purpose embeddings (topic wiki) and the one that is adapted to the science domain (topic science), with again the Both setting yielding the best overall result (.926). Note that our Both setting is probably the one most similar to the concept input used by <cite>Gong et al. (2018)</cite> . This result corroborates our findings on the tuning data, and clearly contradicts the (implicit) claim made by <cite>Gong et al. (2018)</cite> regarding the infeasibility of document-level matching for documents of different lengths.",
  "y": "similarities"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_15",
  "x": "This result corroborates our findings on the tuning data, and clearly contradicts the (implicit) claim made by <cite>Gong et al. (2018)</cite> regarding the infeasibility of document-level matching for documents of different lengths. The second, more important finding is that our proposed TOP n COS SIM AVG measure is also very competitive, as it also outperforms both systems by <cite>Gong et al. (2018)</cite> in two out of three settings. It only fails in the setting using only the Description input.",
  "y": "differences"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_16",
  "x": "The second, more important finding is that our proposed TOP n COS SIM AVG measure is also very competitive, as it also outperforms both systems by <cite>Gong et al. (2018)</cite> in two out of three settings. It only fails in the setting using only the Description input. 8 This is the more important as we exclusively employ off-the-shelf, general-purpose embeddings, while <cite>Gong et al. (2018)</cite> reach <cite>their</cite> best results with a much more sophisticated system and with embeddings that were custom-trained for the science domain.",
  "y": "similarities"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_17",
  "x": "It only fails in the setting using only the Description input. 8 This is the more important as we exclusively employ off-the-shelf, general-purpose embeddings, while <cite>Gong et al. (2018)</cite> reach <cite>their</cite> best results with a much more sophisticated system and with embeddings that were custom-trained for the science domain. Thus, while the performance of our proposed TOP n COS SIM AVG method is superior to the approach by <cite>Gong et al. (2018)</cite> , it is itself outperformed by the 'baseline' AVG COS SIM method with appropriate weighting.",
  "y": "differences"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_18",
  "x": "8 This is the more important as we exclusively employ off-the-shelf, general-purpose embeddings, while <cite>Gong et al. (2018)</cite> reach <cite>their</cite> best results with a much more sophisticated system and with embeddings that were custom-trained for the science domain. Thus, while the performance of our proposed TOP n COS SIM AVG method is superior to the approach by <cite>Gong et al. (2018)</cite> , it is itself outperformed by the 'baseline' AVG COS SIM method with appropriate weighting. However, apart from raw classification performance, our method also aims at providing human-interpretable information on how a classification was done.",
  "y": "differences"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_19",
  "x": "**CONCLUSION AND FUTURE WORK** We presented a simple method for semantic matching of documents from heterogeneous collections as a solution to the Concept-Project matching task by <cite>Gong et al. (2018)</cite> . Although much simpler, our method clearly outperformed the original system in most input settings.",
  "y": "motivation"
 },
 {
  "id": "f3282df3adadf78320e99c09d8384f_20",
  "x": "Although much simpler, our method clearly outperformed the original system in most input settings. Another result is that, contrary to the claim made by <cite>Gong et al. (2018)</cite> , the standard averaging approach does indeed work very well even for heterogeneous document collections, if appropriate weighting is applied. Due to its simplicity, we believe that our method can also be applied to other text matching tasks, including more 'standard' ones which do not necessarily involve heterogeneous document collections.",
  "y": "differences"
 },
 {
  "id": "f3f61d50929f862e263e3f658852bc_0",
  "x": "This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (Wachsmuth et al., 2017a) and one with 17 reasons for quality differences phrased spontaneously in practice<cite> (Habernal and Gurevych, 2016a)</cite> . In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4).",
  "y": "background"
 },
 {
  "id": "f3f61d50929f862e263e3f658852bc_1",
  "x": "---------------------------------- **OVERALL** Conv A is more convincing than B. Table 2 : The 17+1 practical reason labels given in the corpus of <cite>Habernal and Gurevych (2016a)</cite> .",
  "y": "background"
 },
 {
  "id": "f3f61d50929f862e263e3f658852bc_2",
  "x": "Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study<cite> (Habernal and Gurevych, 2016a)</cite> , these reasons were used to derive a hierarchical annotation scheme. 9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- Wachsmuth et al. (2017a) given for each of the 17+1 reason labels of <cite>Habernal and Gurevych (2016a)</cite> . Bold/gray: Highest/lowest value in each column.",
  "y": "background"
 },
 {
  "id": "f3f61d50929f862e263e3f658852bc_3",
  "x": "Argument pairs (A, B) from a debate portal were classified as to which argument is more convincing. Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study<cite> (Habernal and Gurevych, 2016a)</cite> , these reasons were used to derive a hierarchical annotation scheme. 9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- Wachsmuth et al. (2017a) given for each of the 17+1 reason labels of <cite>Habernal and Gurevych (2016a)</cite> .",
  "y": "background"
 },
 {
  "id": "f3f61d50929f862e263e3f658852bc_4",
  "x": "---------------------------------- **CORRELATIONS OF DIMENSIONS AND REASONS** For Hypotheses 1 and 2, we consider all 736 pairs of arguments from <cite>Habernal and Gurevych (2016a)</cite> where both have been annotated by Wachsmuth et al. (2017a) .",
  "y": "similarities uses"
 },
 {
  "id": "f3f61d50929f862e263e3f658852bc_5",
  "x": "The high \u03c4 's of 8-5 (more credible) for local acceptability (.73) and of 9-4 (well thought through) for cogency (.75) confirm the match assumed in Section 1. Also, the values of 5-3 (unclear) for clarity (.91) and of 7-2 (non-sense) for reasonableness (.94) as well as the weaker correlation of 8-4 (objective) for emotional appeal (.35) makes sense. Only the comparably low \u03c4 of 6-1 (no credible evidence) for local acceptability (.49) and credibility (.52) seem really unexpected. Besides, the descriptions of 6-2 and 6-3 sound like local but cor- Table 4 : The mean rating for each quality dimension of those arguments from Wachsmuth et al. (2017a) given for each reason label<cite> (Habernal and Gurevych, 2016a)</cite> .",
  "y": "background"
 },
 {
  "id": "f3f61d50929f862e263e3f658852bc_6",
  "x": "**ABSOLUTE RATINGS FOR RELATIVE DIFFERENCES** The correlations found imply that the relative quality differences captured are reflected in absolute differences. For explicitness, we computed the mean rating for each quality dimension of all arguments from Wachsmuth et al. (2017a) with a particular reason label from <cite>Habernal and Gurevych (2016a)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "f3f61d50929f862e263e3f658852bc_7",
  "x": "For all dimensions, the maximum and minimum value are higher for the positive than for the negative labels-a clear support of Hypothesis 3. 3 Also, Table 4 reveals which reasons predict absolute differences most: The mean ratings of 7-3 (off-topic) are very low, indicating a strong negative impact, while 6-3 (irrelevant reasons) still shows rather 3 While the differences seem not very large, this is expected, as in many argument pairs from <cite>Habernal and Gurevych (2016a)</cite> both arguments are strong or weak respectively. high values.",
  "y": "background"
 },
 {
  "id": "f3f61d50929f862e263e3f658852bc_8",
  "x": "Still, we conclude that the crowd generally handles the theory-based quality assessment almost as well as the experts. However, the complexity of the assessment is underlined by the generally limited agreement, suggesting that either simplification or stricter guidelines are needed. Regarding simplification, the most common practical reasons of <cite>Habernal and Gurevych (2016a)</cite> imply what to focus on.",
  "y": "background"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_0",
  "x": "The task has been studied for almost two decades. Early work on this task mostly relies on syntactic information such as constituency-based parse trees to help decide what to prune from a sentence or how to re-write a sentence (Jing, 2000; Knight and Marcu, 2000) . Recently, there has been much interest in applying neural network models to solve the problem, where little or no linguistic analysis is performed except for tokenization<cite> (Filippova et al., 2015</cite>; Rush et al., 2015; Chopra et al., 2016) .",
  "y": "background"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_1",
  "x": "For example,<cite> Filippova et al. (2015)</cite> used close to two Figure 1 : Examples of in-domain and out-ofdomain results by a standard abstractive sequenceto-sequence model trained on the Gigaword corpus. The first input sentence comes from the Gigaword corpus while the second input sentence comes from the written news corpus used by Clarke and Lapata (2008) . million sentence pairs to train an LSTM-based sentence compression model.",
  "y": "background"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_2",
  "x": "Recently, there has been much interest in applying neural network models to solve the problem, where little or no linguistic analysis is performed except for tokenization<cite> (Filippova et al., 2015</cite>; Rush et al., 2015; Chopra et al., 2016) . Although neural network-based models have achieved good performance on this task recently, they tend to suffer from two problems: (1) They require a large amount of data for training. For example,<cite> Filippova et al. (2015)</cite> used close to two Figure 1 : Examples of in-domain and out-ofdomain results by a standard abstractive sequenceto-sequence model trained on the Gigaword corpus.",
  "y": "motivation background"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_3",
  "x": "To this end, we extend the deletionbased LSTM model for sentence compression by<cite> Filippova et al. (2015)</cite> . Although deletion-based sentence compression is not as flexible as abstractive sentence compression, we chose to work on deletion-based sentence compression for the following reason. Abstractive sentence compression allows new words to be used in a compressed sentence, i.e., words that do not occur in the input sentence.",
  "y": "extends"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_4",
  "x": "Specifically, we propose two major changes to the model by<cite> Filippova et al. (2015)</cite> : (1) We explicitly introduce POS embeddings and dependency relation embeddings into the neural network model. (2) Inspired by a previous method (Clarke and Lapata, 2008) , we formulate the final predictions as an Integer Linear Programming problem to incorporate constraints based on syntactic relations between words and expected lengths of the compressed sentences. In addition to the two major changes above, we also use bi-directional LSTM to include contextual information from both directions into the model.",
  "y": "extends"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_5",
  "x": "(2) Inspired by a previous method (Clarke and Lapata, 2008) , we formulate the final predictions as an Integer Linear Programming problem to incorporate constraints based on syntactic relations between words and expected lengths of the compressed sentences. In addition to the two major changes above, we also use bi-directional LSTM to include contextual information from both directions into the model. We evaluate our method using around 10,000 sentence pairs released by<cite> Filippova et al. (2015)</cite> and two other data sets representing out-ofdomain data.",
  "y": "uses"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_6",
  "x": "**PROBLEM DEFINITION** Recall that we focus on deletion-based sentence compression. Our problem setup is the same as that by<cite> Filippova et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_7",
  "x": "We first introduce our base model, which uses LSTM to perform sequence labeling. This base model is largely based on the model by<cite> Filippova et al. (2015)</cite> with some differences, which will be explained below. We assume that each word in the vocabulary has a d-dimensional embedding vector.",
  "y": "differences uses"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_8",
  "x": "We use a standard bi-directional LSTM model to process these embedding vectors sequentially from both directions to obtain a sequence of hidden vectors (h 1 , h 2 , . . . , h n ), where h i \u2208 R h . We omit the details of the bi-LSTM and refer the interested readers to the work by Graves et al. (2013) for further explanation. Following<cite> Filippova et al. (2015)</cite> , our bi-LSTM has three layers, as shown in Figure 2 .",
  "y": "uses"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_9",
  "x": "where W \u2208 R 2\u00d7h and b \u2208 R 2 are a weight matrix and a weight vector to be learned. There are some differences between our base model and the LSTM model by<cite> Filippova et al. (2015)</cite> . (1)<cite> Filippova et al. (2015)</cite> first encoded the input sentence in its reverse order using the same LSTM before processing the sentence for sequence labeling.",
  "y": "differences"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_10",
  "x": "where W \u2208 R 2\u00d7h and b \u2208 R 2 are a weight matrix and a weight vector to be learned. There are some differences between our base model and the LSTM model by<cite> Filippova et al. (2015)</cite> . (1)<cite> Filippova et al. (2015)</cite> first encoded the input sentence in its reverse order using the same LSTM before processing the sentence for sequence labeling.",
  "y": "background"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_11",
  "x": "There are some differences between our base model and the LSTM model by<cite> Filippova et al. (2015)</cite> . (1)<cite> Filippova et al. (2015)</cite> first encoded the input sentence in its reverse order using the same LSTM before processing the sentence for sequence labeling. (2)<cite> Filippova et al. (2015)</cite> used only a single-directional LSTM while we use bi-LSTM to capture contextual information from both directions.",
  "y": "differences"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_12",
  "x": "(2)<cite> Filippova et al. (2015)</cite> used only a single-directional LSTM while we use bi-LSTM to capture contextual information from both directions. (3) Although<cite> Filippova et al. (2015)</cite> did not use any syntactic information in their basic model, they introduced some features based on dependency parse trees in their advanced models. Here we follow their basic model because later we will introduce more explicit syntaxbased features.",
  "y": "differences"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_13",
  "x": "(3) Although<cite> Filippova et al. (2015)</cite> did not use any syntactic information in their basic model, they introduced some features based on dependency parse trees in their advanced models. Here we follow their basic model because later we will introduce more explicit syntaxbased features. (4)<cite> Filippova et al. (2015)</cite> combined the predicted y i\u22121 with w i to help predict y i .",
  "y": "background"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_14",
  "x": "There are some differences between our base model and the LSTM model by<cite> Filippova et al. (2015)</cite> . (3) Although<cite> Filippova et al. (2015)</cite> did not use any syntactic information in their basic model, they introduced some features based on dependency parse trees in their advanced models.",
  "y": "differences"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_15",
  "x": "(3) Although<cite> Filippova et al. (2015)</cite> did not use any syntactic information in their basic model, they introduced some features based on dependency parse trees in their advanced models. Here we follow their basic model because later we will introduce more explicit syntaxbased features. (4)<cite> Filippova et al. (2015)</cite> combined the predicted y i\u22121 with w i to help predict y i .",
  "y": "background"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_16",
  "x": "(4)<cite> Filippova et al. (2015)</cite> combined the predicted y i\u22121 with w i to help predict y i . This adds some dependency between consecutive labels. We do not do this because later we will introduce an ILP layer to introduce dependencies among labels.",
  "y": "differences"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_17",
  "x": "Although the method above has explicitly incorporated some syntactic information into the bi-LSTM model, the syntactic information is used in a soft manner through the learned model weights. We hypothesize that there are also hard constraints we can impose on the compressed sentences. For example, the method above as well as the original method by<cite> Filippova et al. (2015)</cite> cannot impose any length constraint on the compressed sentences.",
  "y": "motivation"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_18",
  "x": "Here we use three datasets. Google News: The first dataset contains 10,000 sentence pairs collected and released by<cite> Filippova et al. (2015)</cite> 2 . The sentences were automatically acquired from the web through Google News using a method introduced by Filippova and Altun (2013) .",
  "y": "uses"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_19",
  "x": "We compare our methods with a few baselines: LSTM: This is the basic LSTM-based deletion method proposed by<cite> (Filippova et al., 2015)</cite> . We report both the performance they achieved using close to two million training sentence pairs and the performance of our re-implementation of their model trained on the 8,000 sentence pairs. LSTM+: This is advanced version of the model proposed by<cite> Filippova et al. (2015)</cite> , where the authors incorporated some dependency parse tree information into the LSTM model and used the prediction on the previous word to help the prediction on the current word.",
  "y": "uses"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_20",
  "x": "We report both the performance they achieved using close to two million training sentence pairs and the performance of our re-implementation of their model trained on the 8,000 sentence pairs. LSTM+: This is advanced version of the model proposed by<cite> Filippova et al. (2015)</cite> , where the authors incorporated some dependency parse tree information into the LSTM model and used the prediction on the previous word to help the prediction on the current word. Traditional ILP: This is the ILP-based method proposed by Clarke and Lapata (2008) .",
  "y": "background"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_21",
  "x": "We compare our methods with a few baselines: LSTM: This is the basic LSTM-based deletion method proposed by<cite> (Filippova et al., 2015)</cite> . LSTM+: This is advanced version of the model proposed by<cite> Filippova et al. (2015)</cite> , where the authors incorporated some dependency parse tree information into the LSTM model and used the prediction on the previous word to help the prediction on the current word.",
  "y": "uses"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_22",
  "x": "With the two datasets Google News and BNC News that have the ground truth compressed sentences, we can perform automatic evaluation. We first split the Google News dataset into a training set, a validation set and a test set. We took the first 1,000 sentence pairs from Google News as the test set, following the same practice as<cite> Filippova et al. (2015)</cite> .",
  "y": "uses"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_23",
  "x": "We can see that indeed this abstractive method performed poorly in cross-domain settings. (2) In the in-domain setting, with the same amount of training data (8,000), our BiLSTM method with syntactic features (BiLSTM+SynFeat and BiLSTM+SynFeat+ILP) performs similarly to or better than the LSTM+ method proposed by<cite> Filippova et al. (2015)</cite> , in terms of both F1 and accuracy. This shows that our method is comparable to the LSTM+ method in the in-domain setting.",
  "y": "differences"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_24",
  "x": "**MANUAL EVALUATION** The evaluation above does not look at the readability of the compressed sentences. In order to evaluate whether sentences generated by our method are readable, we adopt the manual evaluation procedure by<cite> Filippova et al. (2015)</cite> to compare our method with LSTM+ and Traditional ILP in terms of readability and informativeness.",
  "y": "uses"
 },
 {
  "id": "f4becae9cd7eeaa7fd3085ff904aaa_25",
  "x": "Our work is based on the deletion-based LSTM model for sentence compression by<cite> Filippova et al. (2015)</cite> . There has also been much interest in applying sequence-to-sequence models for abstractive sentence compression (Rush et al., 2015; Chopra et al., 2016) . As we pointed out in Section 1, in a cross-domain setting, abstractive sentence compression may not be suitable.",
  "y": "uses"
 },
 {
  "id": "f58555aa8fc78903df83af8309a3d7_0",
  "x": "Both tools are restrictive in terms of functionalities and do not support collaborative annotation and provide no means of representing complex sentential structures. Our representation model is built on the functionalities of Annotation Graph <cite>[7]</cite> and the underlying storage scheme is conceptually similar to Standoff XML format [9] , but we opted for a relational database structure built with an object-oriented design for efficiency, reusability and versatility. Several web-based annotation tools such as Serengeti [10] , a tool for annotating anaphoric relations and lexical chains, are limited to a particular domain and cannot be used for annotating and visualizing complex structural information without substantial modification.",
  "y": "differences uses"
 },
 {
  "id": "f58555aa8fc78903df83af8309a3d7_1",
  "x": "This gives the annotator fine-grained control over the annotating process and facilitates clear division of labor among different annotators. In addition, all annotators collaborating on the same step get notified of the relevant changes in annotation in real time once a modification has been made. The tagger is built on a generic, multifunctional relational database similar to the annotation graph model <cite>[7]</cite> that has been demonstrated to be capable of representing virtually all sorts of common linguistic annotations.",
  "y": "similarities"
 },
 {
  "id": "f59a8c650583343fe372db42fc109a_0",
  "x": "This paper describes the DFKI-NMT submission to the WMT19 News translation task. We participated in both English-to-German and German-toEnglish directions. We trained Transformer models <cite>(Vaswani et al., 2017)</cite> using Sockeye 1 (Hieber et al., 2017) .",
  "y": "uses"
 },
 {
  "id": "f59a8c650583343fe372db42fc109a_1",
  "x": "We selected the top 15M scored sentence pairs from out-ofdomain data for training our systems. The neural language models trained for data selection in our experiments are based on selfattention networks which can be trained very fast. Figure 1 (a) shows the structure of the standard Transformer translation model <cite>(Vaswani et al., 2017)</cite> and we removed the encoder and the attention layer in the decoder from the Transformer translation model to create our Transformer language model as shown in Figure 1 (b) .",
  "y": "extends"
 },
 {
  "id": "f59a8c650583343fe372db42fc109a_2",
  "x": "Then we randomly selected 10M sentences which contain difficult words for back-translation. The mod- els used for back-translating monolingual data are baseline Transformers <cite>(Vaswani et al., 2017)</cite> trained on the bilingual data after data selection as described before. During back-translation, we used greedy search instead of beam search for efficiency.",
  "y": "uses"
 },
 {
  "id": "f59a8c650583343fe372db42fc109a_3",
  "x": "We trained two Transformer models for each translation task as Transformer-base and Transformer-big. The settings of Transformerbase is the same as the baseline Transformer in<cite> Vaswani et al. (2017)</cite> 's work. For Transformerbig, we changed word embedding size into 1024 and kept other parameters unchanged.",
  "y": "similarities uses"
 },
 {
  "id": "f60796ff05156e81c4b183cdcb05ae_0",
  "x": "An earlier work proposed training a single multi-task deep learning model covering all domains, providing implicit shared feature learning across domains [6] . The approach showed significantly better overall semantic template level performance. Similar experiments on using shared feature extraction layers for slot-filling across several domains have demonstrated significant performance improvements relative to single-domain baselines, especially in low data regimes<cite> [9]</cite> .",
  "y": "background"
 },
 {
  "id": "f60796ff05156e81c4b183cdcb05ae_1",
  "x": "Test sets were constructed using the same framework. All the collected data was tokenized using a standard tokenizer and lower-cased before use since capitalization was seen to be indicative of slot values. All digits were replaced with special \"#\" tokens following<cite> [9]</cite> .",
  "y": "uses"
 },
 {
  "id": "f60796ff05156e81c4b183cdcb05ae_3",
  "x": "Both LSTM layers are shared across all domains, fol- lowed by domain specific softmax layers, following<cite> [9]</cite> . The model was trained using a batch size of 100 with alternating batches from different domains. To avoid over-training the model on the larger domains, the number of batches chosen from each domain was proportional to the logarithm of the number of training samples from the domain.",
  "y": "uses"
 },
 {
  "id": "f6a35ed1ec0c01d3e9faa1ec3d8478_0",
  "x": "We build on previous work in our lab on disagreement detection, classifying stance, identifying high quality arguments, measuring the properties and the persuasive effects of factual vs. emotional arguments, and clustering arguments into their facets or frames related to a particular topic [9, 1, 13, 18, <cite>16,</cite> 12, 15] . In this work, we present Debbie, a novel arguing bot, that uses retrieval from existing conversations in order to argue with users. Debbie's main aim is to keep the conversation going, by successfully producing arguments and counter-arguments that will keep the user talking about the topic.",
  "y": "uses"
 },
 {
  "id": "f6a35ed1ec0c01d3e9faa1ec3d8478_1",
  "x": "Social media conversations are a good source of argumentative data but many sentences either do not express an argument or cannot be understood out of context and hence cannot be used to build Debbie's response pool. Swanson et al.(2015) created a large corpus consisting of 109,074 posts on the topics gay marriage (GM, 22425 posts), gun control (GC, 38102 posts), death penalty (DP, 5283 posts) by combining the Internet Argument Corpus(IAC) [17] , with dialogues from online debate forums 1<cite> [16]</cite> . It includes topic annotations, response characterizations (4forums), and stance.",
  "y": "background"
 },
 {
  "id": "f6a35ed1ec0c01d3e9faa1ec3d8478_2",
  "x": "note that a threshold of predicted AQ >0.55 maintained both diversity and quality in the arguments [12] . For example, the sentence The death penalty is also discriminatory in its application what i mean is that through out the world the death penalty is disproportionately used against disadvantaged people was given a score of 0.98. We started with the Argument Quality (AQ) regressor from<cite> [16]</cite> , which predicts a quality score for each sentence.",
  "y": "uses"
 },
 {
  "id": "f6a35ed1ec0c01d3e9faa1ec3d8478_3",
  "x": "The stance for these argument segments is obtained from IAC [1] . We keep only stance bearing statements from the above dataset. had improved upon the AQ predictor from<cite> [16]</cite> , giving a much larger and diverse corpus [12] .",
  "y": "extends"
 },
 {
  "id": "f7255360eacc4e2a4e8bea2f6ab1b0_0",
  "x": "Most of them are based on the following assumption: the thematic coherence of a text segment finds expression at the lexical level. <cite>Hearst (1997)</cite> and Nomoto and Nitta (1994) detect this coherence through patterns of lexical cooccurrence. Morris and Hirst (1991) and Kozima (1993) find topic boundaries in the texts by using lexical cohesion.",
  "y": "background"
 },
 {
  "id": "f7255360eacc4e2a4e8bea2f6ab1b0_1",
  "x": "---------------------------------- **RESULTS** A first qualitative evaluation of the method has been done with about 20 texts but without a formal protocol as in<cite> (Hearst, 1997)</cite> .",
  "y": "differences"
 },
 {
  "id": "f7255360eacc4e2a4e8bea2f6ab1b0_2",
  "x": "In order to have a more objective evaluation, the method has been applied to the \"classical\" task of discovering boundaries between concatened texts. Results are shown in Table 1 . As in<cite> (Hearst, 1997)</cite> , boundaries found by the method are weighted and sorted in decreasing order.",
  "y": "uses"
 },
 {
  "id": "f7255360eacc4e2a4e8bea2f6ab1b0_3",
  "x": "For the first from the corpus used for building the collocation network. Each text was 80 words long on average. Each boundary, which is a minimum of the cohesion graph, was weighted by the sum of the differences between its value and the values of the two maxima around it, as in<cite> (Hearst, 1997)</cite> .",
  "y": "uses"
 },
 {
  "id": "f797e7439bd78af2ef86271214f991_0",
  "x": "Both the natural language processing engineer (who needs a coreference resolution system for the problem at hand) and the coreference resolution researcher need tools to facilitate and support system development, comparison and analysis. In <cite>Martschat and Strube (2014)</cite> , we propose a framework for error analysis for coreference resolution. In this paper, we present cort 1 , an implementation of this framework, and show how it can be useful for engineers and researchers.",
  "y": "motivation"
 },
 {
  "id": "f797e7439bd78af2ef86271214f991_1",
  "x": "**ERROR ANALYSIS FRAMEWORK** Due to the set-based nature of coreference resolution, it is not clear how to extract errors when an entity is not correctly identified. The idea underlying the analysis framework of <cite>Martschat and Strube (2014)</cite> is to employ spanning trees in a graph-based entity representation.",
  "y": "background"
 },
 {
  "id": "f797e7439bd78af2ef86271214f991_2",
  "x": "The analysis algorithm is parametrized only by the spanning tree algorithm employed: different algorithms lead to different notions of errors. In <cite>Martschat and Strube (2014)</cite> , we propose an algorithm based on Ariel's accessibility theory (Ariel, 1990) for reference entities. For system entity spanning trees, we take each output pair as an edge.",
  "y": "similarities uses"
 },
 {
  "id": "f797e7439bd78af2ef86271214f991_3",
  "x": "**COREFERENCE** cort ships with two coreference resolution approaches. First, it includes multigraph, which is a deterministic approach using a few strong features<cite> (Martschat and Strube, 2014)</cite> .",
  "y": "background"
 },
 {
  "id": "f797e7439bd78af2ef86271214f991_4",
  "x": "The user can define own spanning tree algorithms to extract errors. We already implemented the algorithms discussed in <cite>Martschat and Strube (2014)</cite> . Furthermore, this module provides functionality to",
  "y": "similarities uses"
 },
 {
  "id": "f797e7439bd78af2ef86271214f991_5",
  "x": "Our system also supports other use cases, such as the cross-system analysis described in <cite>Martschat and Strube (2014)</cite> . ---------------------------------- **USE CASE: IMPROVING A COREFERENCE RESOLUTION SYSTEM**",
  "y": "similarities uses"
 },
 {
  "id": "f797e7439bd78af2ef86271214f991_6",
  "x": "Hence, following <cite>Martschat and Strube (2014)</cite> , we categorize all errors by coarse mention type of anaphor and antecedent (proper name, noun, pronoun, demonstrative pronoun or verb) 4 . ---------------------------------- **BOTH NAME**",
  "y": "similarities uses"
 },
 {
  "id": "f797e7439bd78af2ef86271214f991_7",
  "x": "---------------------------------- **RELATED WORK** Compared to our original implementation of the error analysis framework<cite> (Martschat and Strube, 2014)</cite> , we made the analysis interface more userfriendly and provide more analysis functionality.",
  "y": "differences"
 },
 {
  "id": "f861e6590ff57225395e7d480c66e8_0",
  "x": "We start from a baseline joint model that performs the tasks of named entity recognition (NER) and relation extraction at once. Previously proposed models (summarized in Section 2) exhibit several issues that the neural network-based baseline approach (detailed in Section 3.1) overcomes: (i) our model uses automatically extracted features without the need of external parsers nor manually extracted features (see <cite>Gupta et al. (2016)</cite> ; Miwa and Bansal (2016) ; Li et al. (2017) ), (ii) all entities and the corresponding relations within the sentence are extracted at once, instead of examining one pair of entities at a time (see Adel and Sch\u00fctze (2017) ), and (iii) we model relation extraction in a multi-label setting, allowing multiple relations per entity (see Katiyar and Cardie (2017) ; Bekoulis et al. (2018a) ). The core contribution of the paper is the use of AT as an extension in the training procedure for the joint extraction task (Section 3.2).",
  "y": "background motivation"
 },
 {
  "id": "f861e6590ff57225395e7d480c66e8_1",
  "x": "Joint entity and relation extraction: Joint models (Li and Ji, 2014; Miwa and Sasaki, 2014) that are based on manually extracted features have been proposed for performing both the named entity recognition (NER) and relation extraction subtasks at once. These methods rely on the availability of NLP tools (e.g., POS taggers) or manually designed features leading to additional complexity. <cite>Gupta et al. (2016)</cite> propose the use of various manually extracted features along with RNNs.",
  "y": "background motivation"
 },
 {
  "id": "f861e6590ff57225395e7d480c66e8_2",
  "x": "We evaluate our models on four datasets, using the code as available from our github codebase. 1 Specifically, we follow the 5-fold crossvalidation defined by Miwa and Bansal (2016) for the ACE04 (Doddington et al., 2004) dataset. For the CoNLL04 (Roth and Yih, 2004 ) EC task (assuming boundaries are given), we use the same splits as in <cite>Gupta et al. (2016)</cite> ; Adel and Sch\u00fctze (2017) .",
  "y": "uses"
 },
 {
  "id": "f861e6590ff57225395e7d480c66e8_3",
  "x": "For the CoNLL04 dataset, we use two evaluation settings. We use the relaxed evaluation similar to <cite>Gupta et al. (2016)</cite> ; Adel and Sch\u00fctze (2017) on the EC task. The baseline model outperforms the state-of-the-art models that do not rely on manually extracted features (>4% improvement for both tasks), since we directly model the whole sentence, instead of just considering pairs of entities.",
  "y": "differences uses"
 },
 {
  "id": "f881f6c65301fdfe2fffe7a18e05c4_0",
  "x": "**INTRODUCTION** Words and phrases that may directly mark the structure of a discourse have been termed CUE PttR.ASES, CLUE WORDS, DISCOURSE MAI:tKERS~ arid DISCOURSE PARTICLES [3,<cite> 4,</cite> 14, 17, 19] . Some exarnpies are 'now', which marks the introduction of a new subtopic or return to a previous one, 'incidentally' and 'by the way', which indicate the beginning of a digression, and 'anyway' and 'in any case', which indicate return from a digression.",
  "y": "background"
 },
 {
  "id": "f881f6c65301fdfe2fffe7a18e05c4_1",
  "x": "For example, by indicating the presence of a structural boundary or a relationship between parts of a discourse, cue phrases caa assist in the resolution of anaphora [5,<cite> 4,</cite> 17] and in the identification of rhetorical relations [10, 12, 17] . Cue phrases have also been used to reduce the complexity of discourse processing and to increase textual coherence [3, 11, 21] . In Example (1) 1, interpretation of the anaphor 'it' as (correctly) co-indexed with THE SYSTEM is facilitated by the presence of the cue phrases 'say' and 'then', marking potential antecedents in '... as an EXPERT DATABASE for AN EXPERT SYSTEM ...' as structurally unavailable.",
  "y": "background"
 },
 {
  "id": "f881f6c65301fdfe2fffe7a18e05c4_2",
  "x": "Previous attempts to define the set of cue phrases have typically been extensional, 3 with such lists of cue phrases then further classified as to their discourse function. For example, Cohen [3] uses a taxonomy of connectives based on that of Quirk [16] to associate with each class of cue phrases a semantic function with respect to a model of argument understanding. Grosz and Sidner<cite> [4]</cite> classify cue phrases based on changes to the attentional stack and intentional structure found in their theory of discourse.",
  "y": "background"
 },
 {
  "id": "f881f6c65301fdfe2fffe7a18e05c4_3",
  "x": "Finally, Zukerman [21] presents a taxonomy of cue phrases based on three functions relevant to her work in language generation: knowledge organization, knowledge acquisition, and affect maintenance. Once a cue phrase has been identified, however, it is not always clear whether to interpret it as a discourse marker or not [6,<cite> 4,</cite> 8, 18] . The texts in Exampie (2) are potentially ambiguous between a temporal reading of 'now' and a discourse interpretation:",
  "y": "background"
 },
 {
  "id": "f881f6c65301fdfe2fffe7a18e05c4_4",
  "x": "8Our set of cue phrases was derived from extensional definitions provided by ourselves and othel~ [3,<cite> 4,</cite> 17, 18, 21] . Tim following lexicel items, although also cue phrases, are not present in the portion of the axlch-ess examined to date: 9The address was transcribed independently of our study by a meraber of the text processing pool at AT&T Bell Laboratories. We found that 20 cite phrases had been omitted by the traalscriber: 'and', 'now', 'ok', 'so', and 'well'.",
  "y": "extends differences"
 },
 {
  "id": "f8fc3634684ff37ab3d29cee910443_0",
  "x": "Given only supervision in the form of sentences paired with relevant but ambiguous perceptual contexts, a system should learn to interpret and/or generate language describing situations and events in the world. For example, systems have learned to commentate simulated robot soccer games by learning from sample sportscasts (Chen and Mooney, 2008; Liang et al., 2009; <cite>B\u00f6rschinger et al., 2011)</cite> , or understand navigation instructions by learning from action traces produced when following the directions (Chen and Mooney, 2011; Tellex et al., 2011) . B\u00f6rschinger et al. (2011) recently introduced an approach to grounded language learning using unsupervised induction of probabilistic context free grammars (PCFGs) to learn from ambiguous contextual supervision.",
  "y": "background"
 },
 {
  "id": "f8fc3634684ff37ab3d29cee910443_1",
  "x": "**EXISTING PCFG APPROACH** Our approach extends that of<cite> B\u00f6rschinger et al. (2011)</cite> , which in turn was inspired by a series of previous techniques (Lu et al., 2008; Liang et al., 2009; following the idea of constructing correspondences between NL and MR in a single probabilistic generative framework. Particularly, their approach automatically constructs a PCFG that generates NL sentences from MRs, which indicates how atomic MR constituents are probabilistically related to NL words.",
  "y": "extends"
 },
 {
  "id": "f8fc3634684ff37ab3d29cee910443_2",
  "x": "2 Like<cite> B\u00f6rschinger et al. (2011)</cite> , our approach learns a semantic parser directly from ambiguous supervision, specifically NL instructions paired with their complete landmarks plans as context. Our method incorporates the semantic lexemes as building blocks to find correspondences between NL words and semantic concepts represented by the lexeme MRs, instead of building connections between NL words and every possible MR constituent as in B\u00f6rschinger et al.'s approach. Particularly, we utilize the hierarchical subgraph relationships between the MRs in the learned semantic lexicon to produce a smaller, more focused set of PCFG rules.",
  "y": "similarities uses"
 },
 {
  "id": "f8fc3634684ff37ab3d29cee910443_3",
  "x": "We basically follow the scheme of<cite> B\u00f6rschinger et al. (2011)</cite> , but instead of generating NL words from each atomic MR, words are generated from each lexeme MR, Figure 6 : Summary of the rule generation process. NLs refer to the set of NL words in the corpus. Lexeme rules come from the schemata of<cite> B\u00f6rschinger et al. (2011)</cite> , and allow every lexeme MR to generate one or more NL words.",
  "y": "extends differences"
 },
 {
  "id": "f8fc3634684ff37ab3d29cee910443_4",
  "x": "Lexeme rules come from the schemata of<cite> B\u00f6rschinger et al. (2011)</cite> , and allow every lexeme MR to generate one or more NL words. Note that pseudo-lexeme nodes do not produce NL words. and smaller lexeme MRs are generated from more complex ones as given by the LHGs.",
  "y": "uses"
 },
 {
  "id": "f8fc3634684ff37ab3d29cee910443_5",
  "x": "When a leaf-node is reached, it marks all of the nodes in its MR. After traversing all of its children, 5 We used the implementation available at http://web. science.mq.edu.au/\u02dcmjohnson/Software.htm which was also used by<cite> B\u00f6rschinger et al. (2011)</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "f8fc3634684ff37ab3d29cee910443_6",
  "x": "Our approach improves on<cite> B\u00f6rschinger et al. (2011)</cite> 's method in the following ways: \u2022 The building blocks for associating NL and MR are semantic lexemes instead of atomic MR constituents. This prevents the number of constructed PCFG rules from becoming intractably large as happens with B\u00f6rschinger et al.'s approach.",
  "y": "extends"
 },
 {
  "id": "f8fc3634684ff37ab3d29cee910443_7",
  "x": "Several recent approaches have investigated grounded learning from ambiguous supervision extracted from perceptual context. A number of approaches (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al., 2010; <cite>B\u00f6rschinger et al., 2011)</cite> assume training data consisting of a set of sentences each associated with a small set of MRs, one of which is usually the correct meaning of the sentence. Many of these approaches (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al., 2010) disambiguate the data and match NL sentences to their correct MR by iteratively retraining a supervised semantic parser.",
  "y": "background"
 },
 {
  "id": "f8fc3634684ff37ab3d29cee910443_8",
  "x": "They train this model on ambiguous data using EM. As previously discussed,<cite> B\u00f6rschinger et al. (2011)</cite> use a PCFG generative model and also train it on ambiguous data using EM. Liang et al. (2009) assume each sentence maps to one or more semantic records (i.e. MRs) and trains a hierarchical semi-Markov generative model using EM, and then finds a Viterbi alignment between NL words and records and their constituents.",
  "y": "background"
 },
 {
  "id": "f8fc3634684ff37ab3d29cee910443_9",
  "x": "Our model enhances<cite> B\u00f6rschinger et al. (2011)</cite> 's approach to reducing the problem of grounded learning of semantic parsers to PCFG induction. We use a learned semantic lexicon to aid the construction of a smaller and more focused set of PCFG productions. This allows the approach to scale to complex MR languages that define a large (potentially infinite) space of representations for capturing the meaning of sentences.",
  "y": "extends"
 },
 {
  "id": "fa00b8bac394b48bf950f154c65216_0",
  "x": "Large pre-trained language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019;<cite> Yang et al., 2019</cite>; improved the state-of-the-art of various natural language understanding (NLU) tasks such as question answering (e.g., SQuAD; Rajpurkar et al. 2016) , natural language inference (e.g., MNLI; Bowman et al. 2015) as well as text classification (Zhang et al., 2015) . These models (i.e., large LSTMs; Hochreiter and Schmidhuber 1997 or Transformers; Vaswani et al. 2017 ) are pre-trained on large scale unlabeled text with language modeling (Peters et al., 2018; Radford et al., 2018) , masked lan-guage modeling (Devlin et al., 2019; and permutation language modeling objectives. In NLU tasks, pre-trained language models are mostly used as text encoders.",
  "y": "background"
 },
 {
  "id": "fa00b8bac394b48bf950f154c65216_2",
  "x": "The SEQ2SEQ Transformer has an encoder and a decoder Transformer. Abstractive summarization requires both encoding of an input document and generation of a summary usually containing multiple sentences. As mentioned earlier, we can take advantage of recent pre-trained Transformer encoders for the document encoding part as in<cite> Liu and Lapata (2019)</cite> .",
  "y": "uses similarities"
 },
 {
  "id": "fa00b8bac394b48bf950f154c65216_3",
  "x": "Abstractive summarization requires both encoding of an input document and generation of a summary usually containing multiple sentences. As mentioned earlier, we can take advantage of recent pre-trained Transformer encoders for the document encoding part as in<cite> Liu and Lapata (2019)</cite> . However,<cite> Liu and Lapata (2019)</cite> leave the decoder randomly initialized.",
  "y": "differences"
 },
 {
  "id": "fa00b8bac394b48bf950f154c65216_4",
  "x": "Very recently, the feature learning part was replaced again with pretrained transformers (Zhang et al., 2019;<cite> Liu and Lapata, 2019</cite> ) that lead to another huge improvement of summarization performance. However, extractive models have their own limitations. For example, the extracted sentences might be too long and redundant.",
  "y": "background"
 },
 {
  "id": "fa00b8bac394b48bf950f154c65216_5",
  "x": "Following previous work (See et al., 2017; Zhang et al., 2019;<cite> Liu and Lapata, 2019)</cite> , we use the non-anonymized version of CNNDM. Specifically, we preprocess the dataset with the publicly available scripts 5 provided by See et al. (2017) and obtain 287,226 document-summary pairs for training, 13,368 for validation and 11,490 for test. NYT The NYT dataset is a collection of articles along with multi-sentence summaries written by library scientists.",
  "y": "uses"
 },
 {
  "id": "fa00b8bac394b48bf950f154c65216_6",
  "x": "Specifically, we preprocess the dataset with the publicly available scripts 5 provided by See et al. (2017) and obtain 287,226 document-summary pairs for training, 13,368 for validation and 11,490 for test. NYT The NYT dataset is a collection of articles along with multi-sentence summaries written by library scientists. We closely follow the preprocessing procedures described in Durrett et al. (2016) and<cite> Liu and Lapata (2019)</cite> .",
  "y": "uses"
 },
 {
  "id": "fa00b8bac394b48bf950f154c65216_9",
  "x": "Lead3 is a baseline which simply takes the first three sentences of a document as its summary. BERTExt<cite> (Liu and Lapata, 2019</cite> ) is an extractive model fine-tuning on BERT (Devlin et al., 2019) that outperforms other extractive systems. PTGen (See et al., 2017) , DRM (Paulus et al., 2018) , and DCA (Celikyilmaz et al., 2018) are sequence-to-sequence learning based models extended with copy and coverage mechanism, reinforcement learning, and deep communicating agents individually.",
  "y": "background"
 },
 {
  "id": "fa00b8bac394b48bf950f154c65216_10",
  "x": "BERTAbs<cite> (Liu and Lapata, 2019)</cite> and UniLM (Dong et al., 2019) are both pre-training based SEQ2SEQ summarization models. We also implemented three abstractive models as our baselines. Transformer-S2S is 6-layer SEQ2SEQ Transformer (Vaswani et al., 2017) with random initialization.",
  "y": "background"
 },
 {
  "id": "fa3d20d5975ec59454abfca68f8935_0",
  "x": "Document summarization has been an active area of research, especially on the CNN/DailyMail dataset. Even with recent progress (Gehrmann et al., 2018;<cite> Chen and Bansal, 2018)</cite> , there is still some work to be done in the field. Although extractive summarization seem to be less challenging because new words are not generated, identifying salient parts of the document without any guide in the form of a query, is a substantial problem to tackle.",
  "y": "motivation"
 },
 {
  "id": "fa3d20d5975ec59454abfca68f8935_1",
  "x": "However, it is met with major challenges like grammatical correctness and repetition of words especially when generating long-worded sentences. Nonetheless remarkable progress have been achieved with the use of seq2seq models (Gehrmann et al., 2018; See et al., 2017; Chopra et al., 2016; Rush et al., 2015) and a reward instead of loss function via deep-reinforcement learning<cite> (Chen and Bansal, 2018</cite>; Paulus et al., 2017; Ranzato et al., 2015) . We see abstractive summarization in same light as several other authors<cite> (Chen and Bansal, 2018</cite>; Hsu et al., 2018; Liu et al., 2018 ) -extract salient sentences and then abstract; thus sharing similar advantages as the popular divide-and-conquer algorithm.",
  "y": "background"
 },
 {
  "id": "fa3d20d5975ec59454abfca68f8935_2",
  "x": "However, it is met with major challenges like grammatical correctness and repetition of words especially when generating long-worded sentences. Nonetheless remarkable progress have been achieved with the use of seq2seq models (Gehrmann et al., 2018; See et al., 2017; Chopra et al., 2016; Rush et al., 2015) and a reward instead of loss function via deep-reinforcement learning<cite> (Chen and Bansal, 2018</cite>; Paulus et al., 2017; Ranzato et al., 2015) . We see abstractive summarization in same light as several other authors<cite> (Chen and Bansal, 2018</cite>; Hsu et al., 2018; Liu et al., 2018 ) -extract salient sentences and then abstract; thus sharing similar advantages as the popular divide-and-conquer algorithm.",
  "y": "uses"
 },
 {
  "id": "fa3d20d5975ec59454abfca68f8935_3",
  "x": "**EXTRACTIVE TRAINING** Filtering Currently, no extractive summarization dataset exists. Hence it is customary to create one from the abstractive ground-truth summaries<cite> (Chen and Bansal, 2018</cite>; Nallapati et al., 2017) .",
  "y": "uses"
 },
 {
  "id": "fa3d20d5975ec59454abfca68f8935_4",
  "x": "Different from Nallapati et al. (2017) 's approach to greedily add sentences to the summary that maximizes the ROUGE score, our approach is more similar to <cite>Chen and Bansal (2018)</cite>'s model that calculates the individual reference sentence-level score as per its similarity with each sentence in the corresponding document. However, our sentence-level similarity score is based on its bigram overlap: for each t th sentence in the reference summary, R j , per i th sentence in document D j , in contrast to <cite>Chen and Bansal (2018)</cite> 's that uses ROUGE-L recall score.",
  "y": "similarities"
 },
 {
  "id": "fa3d20d5975ec59454abfca68f8935_5",
  "x": "for each t th sentence in the reference summary, R j , per i th sentence in document D j , in contrast to <cite>Chen and Bansal (2018)</cite> 's that uses ROUGE-L recall score. Additionally, for every time both words in the set of bigrams-overlap are stopwords, we decrement the similarity score by 1, for example, (on, the) is an invalid bigram-overlap while (the, President) is valid. We do this, to capture more important similarities instead of trivial ones.",
  "y": "differences"
 },
 {
  "id": "fa3d20d5975ec59454abfca68f8935_6",
  "x": "Additionally during training, we synthetically balance the labels, by forcing some random sentences to be labelled as 1 and subsequently masking their weights. Number of sentences to extract The number of extracted sentences is not trivial, as this significantly affects the summary length and hence evaluation scores. <cite>Chen and Bansal (2018)</cite> introduced a stop criterion in their reinforcement learning process.",
  "y": "background"
 },
 {
  "id": "fa3d20d5975ec59454abfca68f8935_9",
  "x": "---------------------------------- **EVALUATION** Following previous works (See et al., 2017; Nallapati et al., 2017;<cite> Chen and Bansal, 2018)</cite> , we evaluate both datasets on standard ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004) .",
  "y": "uses"
 },
 {
  "id": "fa3d20d5975ec59454abfca68f8935_10",
  "x": "Identifying the likely most salient part of the text as summary-worthy is very crucial. Some authors have employed integer linear programming (Martins and Smith, 2009; Gillick and Favre, 2009; Boudin et al., 2015) , graph concepts (Erkan and Radev, 2004; , ranking with reinforcement learning (Narayan et al., 2018) and mostly related to our work -binary classification (Shen et al., 2007; Nallapati et al., 2017;<cite> Chen and Bansal, 2018)</cite> Our binary classification architecture differs significantly from existing models because it uses a transformer as the building block instead of a bidirectional GRU-RNN (Nallapati et al., 2017) , or bidirectional LSTM-RNN<cite> (Chen and Bansal, 2018)</cite> . To the best of our knowledge, our utilization of the transformer encoder model as a building block for binary classification is novel, although the transformer has been successfully used for language understanding (Devlin et al., 2018) , machine translation (MT) (Vaswani et al., 2017) and paraphrase generation .",
  "y": "differences"
 },
 {
  "id": "fa3d20d5975ec59454abfca68f8935_11",
  "x": "Since the system generated summaries are usually evaluated on ROUGE, its been beneficial to directly optimize this metric during training via a suitable policy using reinforcement learning (Paulus et al., 2017; Celikyilmaz et al., 2018) . Similar to Rush et al. (2015) ; <cite>Chen and Bansal (2018)</cite> we abstract by simplifying our extracted sentences. We jointly learn to paraphrase and compress, but different from existing models purely based on RNN, we implement a blend of two proven efficient models -transformer encoder and GRU-RNN.",
  "y": "similarities"
 },
 {
  "id": "fa7475b6025d010dd6814dfb3905ef_0",
  "x": "In this way, the knowledge embedded in the teacher model can be transferred into the smaller model. However, the retained performance of the student model relies on a well-designed distillation loss function which forces the student model to behave as the teacher. Recent studies on KD<cite> [33,</cite> 15] even leverage more sophisticated model-specific distillation loss functions for better performance.",
  "y": "background"
 },
 {
  "id": "fa7475b6025d010dd6814dfb3905ef_1",
  "x": "Recent studies on KD<cite> [33,</cite> 15] even leverage more sophisticated model-specific distillation loss functions for better performance. Different from previous KD studies which explicitly exploit a distillation loss to minimize the distance between the teacher model and the student model, we propose a new genre of model compression. Inspired by the famous thought experiment \"Ship of Theseus 3 \" in Philosophy, where all components of a ship are gradually replaced by new ones until no original component exists, we propose Theseus Compression for BERT (BERT-of-Theseus), which progressively substitutes modules of BERT with modules of fewer parameters.",
  "y": "differences"
 },
 {
  "id": "fa7475b6025d010dd6814dfb3905ef_2",
  "x": "However, KD-based methods use task-specific loss, together with one or multiple distillation losses as its optimization objective. The use of only one loss function throughout the whole compression process allows us to unify the different phases and keep the compression in a total end-to-end fashion. Also, selecting various loss functions and balancing the weights of each loss for different tasks and datasets are always laborious<cite> [33,</cite> 28] .",
  "y": "background"
 },
 {
  "id": "fa7475b6025d010dd6814dfb3905ef_3",
  "x": "However, KD-based methods use task-specific loss, together with one or multiple distillation losses as its optimization objective. The use of only one loss function throughout the whole compression process allows us to unify the different phases and keep the compression in a total end-to-end fashion. Also, selecting various loss functions and balancing the weights of each loss for different tasks and datasets are always laborious<cite> [33,</cite> 28] .",
  "y": "differences"
 },
 {
  "id": "fa7475b6025d010dd6814dfb3905ef_4",
  "x": "Tang et al. [36] used a BiLSTM architecture to extract task-specific knowledge from BERT. DistilBERT [28] applies a naive Knowledge Distillation on the same corpus used to pretrain BERT. Patient Knowledge Distillation (PKD)<cite> [33]</cite> designs multiple distillation losses between the module hidden states of the teacher and student models.",
  "y": "background"
 },
 {
  "id": "fa7475b6025d010dd6814dfb3905ef_5",
  "x": "KD explicitly defines a loss to measure the similarity of the teacher and student. However, the performance greatly relies on the design of the loss function [14,<cite> 33,</cite> 15] . This loss function needs to be combined with taskspecific loss<cite> [33,</cite> 17] .",
  "y": "background"
 },
 {
  "id": "fa7475b6025d010dd6814dfb3905ef_6",
  "x": "KD explicitly defines a loss to measure the similarity of the teacher and student. However, the performance greatly relies on the design of the loss function [14,<cite> 33,</cite> 15] . This loss function needs to be combined with taskspecific loss<cite> [33,</cite> 17] .",
  "y": "background"
 },
 {
  "id": "fa7475b6025d010dd6814dfb3905ef_7",
  "x": "We test our approach under a task-specific compression setting<cite> [33,</cite> 37] instead of a pretraining compression setting [28, 34] . That is to say, we use no external unlabeled corpus but only the training set of each task in GLEU to compress the model. The reason why we test our model under such a setting is that we intend to straightforwardly verify the effectiveness of our generic compression approach.",
  "y": "uses"
 },
 {
  "id": "fa7475b6025d010dd6814dfb3905ef_8",
  "x": "Formally, we define the task of compression as trying to retain as much performance as possible when compressing the officially released BERT-base (uncased) 5 to a 6-layer compact model with the same hidden size, following the settings in [28,<cite> 33,</cite> 37] . Under this setting, the compressed model has 24M parameters for the token embedding (identical to the original model) and 42M parameters for the Transformer layers and obtains a 1.94\u00d7 speed-up for inference. ----------------------------------",
  "y": "uses"
 },
 {
  "id": "fa7475b6025d010dd6814dfb3905ef_9",
  "x": "**TRAINING DETAILS** We fine-tune BERT-base as the predecessor model for each task with the batch size of 32, the learning rate of 2 \u00d7 10 \u22125 , and the number of epochs as 4. As a result, we are able to obtain a predecessor model with comparable performance with that reported in previous studies [28,<cite> 33,</cite> 15] .",
  "y": "similarities"
 },
 {
  "id": "fa7475b6025d010dd6814dfb3905ef_10",
  "x": "Afterward, for training successor models, following [28, <cite>33]</cite> , we use the first 6 layers of BERT-base to initialize the successor model since the over-parameterized nature of Transformer [38] could cause the model unable to converge while training on small datasets. During module replacing, We fix the batch size as 32 for all evaluated tasks to reduce the search space. All r variables only sample once for a training batch.",
  "y": "uses"
 },
 {
  "id": "fa7475b6025d010dd6814dfb3905ef_11",
  "x": "**BASELINES** As shown in Table 1 , we compare the layer numbers, parameter numbers, loss function, external data usage and model agnosticism of our proposed approach to existing methods. We set up a baseline of vanilla Knowledge Distillation [14] as in<cite> [33]</cite> .",
  "y": "uses"
 },
 {
  "id": "fa7475b6025d010dd6814dfb3905ef_12",
  "x": "We set up a baseline of vanilla Knowledge Distillation [14] as in<cite> [33]</cite> . Additionally, we directly fine-tune an initialized 6-layer BERT model on GLUE tasks to obtain a natural fine-tuning baseline. Under the setting of compressing 12-layer BERT-base to a 6-layer compact model, we choose BERT-PKD<cite> [33]</cite> , PD-BERT [37] , and DistillBERT [28] as strong baselines.",
  "y": "uses"
 },
 {
  "id": "fa7475b6025d010dd6814dfb3905ef_13",
  "x": "Overall, our BERT-of-Theseus retains 98.4% and 98.3% of the BERT-base performance on GLUE dev set and test set, respectively. On every task of GLUE, our model dramatically outperforms the fine-tuning baseline, indicating that with the same loss function, our proposed approach can effectively transfer knowledge from the predecessor to the successor. Also, our model obviously outperforms the vanilla KD [14] and Patient Knowledge Distillation (PKD)<cite> [33]</cite> , showing its supremacy over the KD-based compression approaches.",
  "y": "differences"
 },
 {
  "id": "fb75198b7c9e569932dfd486ba6c0a_0",
  "x": "Current research on applying WSD to specific domains has been evaluated on three available lexicalsample datasets (Ng and Lee, 1996; Weeber et al., 2001;<cite> Koeling et al., 2005)</cite> . This kind of dataset contains hand-labeled examples for a handful of selected target words. As the systems are evaluated on a few words, the actual performance of the systems over complete texts can not be measured.",
  "y": "background"
 },
 {
  "id": "fb75198b7c9e569932dfd486ba6c0a_1",
  "x": "<cite>Koeling et al. (2005)</cite> present a corpus were the examples are drawn from the balanced BNC corpus (Leech, 1992) and the SPORTS and FINANCES sections of the newswire Reuters corpus (Rose et al., 2002) , comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns. The nouns were selected because they were salient in either the SPORTS or FINANCES domains, or because they had senses linked to those domains. The occurrences were hand-tagged with the senses from WordNet version 1.7.1 (Fellbaum, 1998) .",
  "y": "background"
 },
 {
  "id": "fb75198b7c9e569932dfd486ba6c0a_2",
  "x": "They found that the source corpus did not help when tagging the target corpus, showing that tagged corpora from each domain would suffice, and concluding that hand tagging a large general corpus would not guarantee robust broad-coverage WSD. Agirre and Mart\u00ednez (2000) used the same DSO corpus and showed that training on the subset of the source corpus that is topically related to the target corpus does allow for domain adaptation, obtaining better results than training on the target data alone. In (Agirre and Lopez de Lacalle, 2008) , the authors also show that state-of-the-art WSD systems are not able to adapt to the domains in the context of the <cite>Koeling et al. (2005)</cite> dataset.",
  "y": "background"
 },
 {
  "id": "fb75198b7c9e569932dfd486ba6c0a_3",
  "x": "For instance, (Daum\u00e9 III, 2007) shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks. His method improves or equals over previously explored more sophisticated methods (Daum\u00e9 III and Marcu, 2006; Chelba and Acero, 2004) . In contrast, ) reimplemented this method and showed that the improvement on WSD in the<cite> (Koeling et al., 2005)</cite> data was marginal.",
  "y": "background"
 },
 {
  "id": "fb75198b7c9e569932dfd486ba6c0a_4",
  "x": "In ) the authors report successful adaptation on the<cite> (Koeling et al., 2005</cite> ) dataset on supervised setting. Their method is based on the use of unlabeled data, reducing the feature space with SVD, and combination of features using an ensemble of kernel methods. They report 22% error reduction when using both source and target data compared to a classifier trained on target the target data alone, even when the full dataset is used.",
  "y": "background"
 },
 {
  "id": "fb75198b7c9e569932dfd486ba6c0a_5",
  "x": "The predominant sense acquisition method was succesfully applied to specific domains in<cite> (Koeling et al., 2005)</cite> . The methos has two steps: In the first, a corpus of untagged text from the target domain is used to construct a thesaurus of similar words. In the second, each target word is disambiguated using pairwise WordNet-based similarity measures, taking as pairs the target word and each of the most related words according to the thesaurus up to a certain threshold.",
  "y": "uses"
 },
 {
  "id": "fb75198b7c9e569932dfd486ba6c0a_6",
  "x": "This method aims to obtain, for each target word, the sense which is the most predominant for the target corpus. When a general corpus is used, the most predominant sense in general is obtained, and when a domain-specific corpus is used, the most predominant sense for that corpus is obtained<cite> (Koeling et al., 2005)</cite> . The main motivation of the authors is that the most frequent sense is a very powerful baseline, but it is one which requires hand-tagging text, while their method yields similar information automatically.",
  "y": "background"
 },
 {
  "id": "fb75198b7c9e569932dfd486ba6c0a_7",
  "x": "When a general corpus is used, the most predominant sense in general is obtained, and when a domain-specific corpus is used, the most predominant sense for that corpus is obtained<cite> (Koeling et al., 2005)</cite> . The main motivation of the authors is that the most frequent sense is a very powerful baseline, but it is one which requires hand-tagging text, while their method yields similar information automatically. The results show that they are able to obtain good results.",
  "y": "similarities"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_0",
  "x": "Lexical Simplification (LS) aims at replacing complex words with simpler alternatives, which can help various groups of people, including children [De Belder and Moens, 2010] , non-native speakers<cite> [Paetzold and Specia, 2016]</cite> , people with cognitive disabilities [Feng, 2009; Saggion, 2017] , to understand text better. The popular LS systems still predominantly use a set of rules for substituting complex words with their frequent synonyms from carefully handcrafted databases (e.g., WordNet) or automatically induced from comparable corpora [Devlin and Tait, 1998; De Belder and Moens, 2010] . Linguistic databases like WordNet are used to produce simple synonyms of a complex word.",
  "y": "background"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_1",
  "x": "Parallel corpora like Wikipedia-Simple Wikipedia corpus were also used to extract complex-to-simple word correspondences [Biran et al., 2011; Yatskar et al., 2010; Horn et al., 2014] . However, linguistic resources that are scarce or expensive to produce, such as WordNet and Simple Wikipedia, and it is impossible to obtain all possible simplification rules from them. For avoiding the need for resources such as databases or parallel corpora, recent work utilizes word embedding models to extract simplification candidates for complex words [Glava\u0161 and\u0160tajner, 2015;<cite> Paetzold and Specia, 2016</cite>; <cite>Paetzold and Specia, 2017a]</cite> .",
  "y": "background"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_2",
  "x": "For avoiding the need for resources such as databases or parallel corpora, recent work utilizes word embedding models to extract simplification candidates for complex words [Glava\u0161 and\u0160tajner, 2015;<cite> Paetzold and Specia, 2016</cite>; <cite>Paetzold and Specia, 2017a]</cite> . Glava\u0161 et al. [2015] adopted the Figure 1: Comparison of simplification candidates of complex words using three methods. Given one sentence \"John composed these verses.\" and complex words 'composed' and 'verses', the top three simplification candidates for each complex word are generated by our method BERT-LS and the state-of-the-art two baselines based word embeddings (Glava\u0161[Glava\u0161 and\u0160tajner, 2015] and Paetzold-NE<cite> [Paetzold and Specia, 2017a]</cite> ).",
  "y": "uses"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_3",
  "x": "Given one sentence \"John composed these verses.\" and complex words 'composed' and 'verses', the top three simplification candidates for each complex word are generated by our method BERT-LS and the state-of-the-art two baselines based word embeddings (Glava\u0161[Glava\u0161 and\u0160tajner, 2015] and Paetzold-NE<cite> [Paetzold and Specia, 2017a]</cite> ). original word embeddings trained on text, and<cite> Paetzold et al. (2017)</cite> used a retrofitted context-aware word embedding model trained on text with the POS tag. Given a complex word, they extracted from the word embedding model the simplification candidates whose vectors are closer in terms of cosine similarity with the complex word.",
  "y": "background"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_4",
  "x": "For complex words 'composed' and 'verses' in the sentence \"John composed these verses.\", the top three substitution candidates of the two complex words generated by the LS systems based on word embeddings [Glava\u0161 and\u0160tajner, 2015; <cite>Paetzold and Specia, 2017a]</cite> are only related with the complex words itself without without paying attention to the original sentence. The top three substitution candidates generated by BERT-LS are not only related with the complex words, but also can fit for the original sentence very well. Then, by considering the frequency or order of each candidate, we can easily choose 'wrote' as the replacement of 'composed and 'poems' as the replacement of 'verses'.",
  "y": "background"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_5",
  "x": "Here, we give an example shown in Figure 1 to illustrate the generation process using BERT. For complex words 'composed' and 'verses' in the sentence \"John composed these verses.\", the top three substitution candidates of the two complex words generated by the LS systems based on word embeddings [Glava\u0161 and\u0160tajner, 2015; <cite>Paetzold and Specia, 2017a]</cite> are only related with the complex words itself without without paying attention to the original sentence. The top three substitution candidates generated by BERT-LS are not only related with the complex words, but also can fit for the original sentence very well.",
  "y": "differences background"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_6",
  "x": "**RELATED WORK** Lexical simplification (LS) contains identifying complex words and finding the best candidate substitution for these complex words [Shardlow, 2014;<cite> Paetzold and Specia, 2017b]</cite> . The best substitution needs to be more simplistic while preserving the sentence grammatically and keeping its meaning as much as possible, which is a very challenging task.",
  "y": "background"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_7",
  "x": "Lexical simplification (LS) contains identifying complex words and finding the best candidate substitution for these complex words [Shardlow, 2014;<cite> Paetzold and Specia, 2017b]</cite> . The best substitution needs to be more simplistic while preserving the sentence grammatically and keeping its meaning as much as possible, which is a very challenging task. The popular lexical simplification (LS) approaches are rule-based, which each rule contain a complex word and its simple synonyms<cite> [Lesk, 1986</cite>; Pavlick and Callison-Burch, 2016;<cite> Maddela and Xu, 2018]</cite> .",
  "y": "background"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_8",
  "x": "Afterward, they further extracted candidates for complex word by combining word embeddings with WordNet and parallel corpora<cite> [Paetzold and Specia, 2017a]</cite> . After examining existing LS methods ranging from rulesbased to embedding-based, the major challenge is that they generated simplification candidates for the complex word regardless of the context of the complex word, which will inevitably produce a large number of spurious candidates that can confuse the systems employed in the subsequent steps. In this paper, we will first present a BERT-based LS approach that requires only a sufficiently large corpus of regular text without any manual efforts.",
  "y": "background"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_9",
  "x": "Instead of a traditional word embedding model, Paetzold and Specia (2016) adopted context-aware word embeddings trained on a large dataset where each word is annotated with the POS tag. Afterward, they further extracted candidates for complex word by combining word embeddings with WordNet and parallel corpora<cite> [Paetzold and Specia, 2017a]</cite> . After examining existing LS methods ranging from rulesbased to embedding-based, the major challenge is that they generated simplification candidates for the complex word regardless of the context of the complex word, which will inevitably produce a large number of spurious candidates that can confuse the systems employed in the subsequent steps.",
  "y": "background motivation"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_10",
  "x": "Afterward, they further extracted candidates for complex word by combining word embeddings with WordNet and parallel corpora<cite> [Paetzold and Specia, 2017a]</cite> . After examining existing LS methods ranging from rulesbased to embedding-based, the major challenge is that they generated simplification candidates for the complex word regardless of the context of the complex word, which will inevitably produce a large number of spurious candidates that can confuse the systems employed in the subsequent steps. In this paper, we will first present a BERT-based LS approach that requires only a sufficiently large corpus of regular text without any manual efforts.",
  "y": "differences background motivation"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_11",
  "x": "The substitution ranking of the lexical simplification pipeline is to decide which of the candidate substitutions that fit the context of a complex word is the simplest<cite> [Paetzold and Specia, 2017b]</cite>. We rank the candidate substitutions based on the following features. Each of the features captures one aspect of the suitability of the candidate word to replace the complex word.",
  "y": "background"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_12",
  "x": "The overall simplification algorithm BERT-LS is shown in Algorithm 1. In this paper, we are not focused on identifying complex words<cite> [Paetzold and Specia, 2017b]</cite> , which is a separate task. For each complex word, we first get simplification candidates using BERT after preprocessing the original sequence (lines 1-4).",
  "y": "differences"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_14",
  "x": "We use three widely used lexical simplification datasets to do experiments. (2) BenchLS 6<cite> [Paetzold and Specia, 2016]</cite> .",
  "y": "uses"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_15",
  "x": "Comparison Systems. We choose the following eight baselines to evaluation: Devlin [Devlin and Tait, 1998 ], Biran [Biran et al., 2011 ], Yamamoto [Kajiwara et al., 2013 , Horn [Horn et al., 2014] , Glava\u0161 [Glava\u0161 and\u0160tajner, 2015] , SimplePPDB [Pavlick and Callison-Burch, 2016] , Paetzold-CA<cite> [Paetzold and Specia, 2016]</cite> , and Paetzold-NE<cite> [Paetzold and Specia, 2017a]</cite> . The substitution generation strategies of these baselines generate candidates from WordNet, EW and SEW parallel corpus, Merriam dictionary, EW and SEW parallel corpus, word embeddings, context-aware word embeddings, combining the Newsela parallel corpus and contextaware word embeddings.",
  "y": "uses"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_16",
  "x": "The following three widely used metrics are used for evaluation<cite> [Paetzold and Specia, 2015</cite>;<cite> Paetzold and Specia, 2016</cite>;<cite> Paetzold and Specia, 2017b]</cite> . Precision: The proportion of generated candidates that are in the gold standard. Recall: The proportion of gold-standard substitutions that are included in the generated substitutions.",
  "y": "uses"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_17",
  "x": "The results are shown in Table 1 . The results of the baselines on LexMTurk are from<cite> [Paetzold and Specia, 2017b]</cite> and the results on BenchLS and NNSeval are from<cite> [Paetzold and Specia, 2017a]</cite> . As can be seen, despite being entirely unsupervised, our model BERT-LS obtains F1 scores on three 7 http://ghpaetzold.github.io/data/NNSeval.zip datasets, largely outperforming the previous best baselines.",
  "y": "uses"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_18",
  "x": "The results of the baselines on LexMTurk are from<cite> [Paetzold and Specia, 2017b]</cite> and the results on BenchLS and NNSeval are from<cite> [Paetzold and Specia, 2017a]</cite> . As can be seen, despite being entirely unsupervised, our model BERT-LS obtains F1 scores on three 7 http://ghpaetzold.github.io/data/NNSeval.zip datasets, largely outperforming the previous best baselines. The baseline Paetzold-NE by combining the Newsela parallel corpus and context-aware word embeddings obtains better results on Precision metric, which demonstrates that substitution generation tends to benefit from the combination of different resources.",
  "y": "differences"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_19",
  "x": "Here, we randomly choose 10 short sentences from LexMTurk dataset as examples. Table 2 shows the top six candidates generated by the three approaches. We chooses the state-of-the-art two baselines based word embeddings (Glava\u0161[Glava\u0161 and\u0160tajner, 2015] and Paetzold-NE<cite> [Paetzold and Specia, 2017a]</cite>) as comparison.",
  "y": "uses"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_20",
  "x": "We chooses the state-of-the-art two baselines based word embeddings (Glava\u0161[Glava\u0161 and\u0160tajner, 2015] and Paetzold-NE<cite> [Paetzold and Specia, 2017a]</cite>) as comparison. From Table 2 , we observe that BERT-LS achieves the best simplification candidates for complex words compared with the two baselines based word embeddings.",
  "y": "differences"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_21",
  "x": "In this section, we evaluate the performance of various complete LS systems. We adopt the following two well-known metrics used by these work [Horn et al., 2014;<cite> Paetzold and Specia, 2017b]</cite> . Precision: The proportion with which the replacement of the original word is either the original word itself or is in the gold standard Accuracy: The proportion with which the replacement of the original word is not the original word and is in the gold standard.",
  "y": "uses"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_22",
  "x": "The results are shown in Table 3 . The results of the baselines on LexMTurk are from<cite> [Paetzold and Specia, 2017b]</cite> and the results on BenchLS and NNSeval are from<cite> [Paetzold and Specia, 2017a]</cite> . We can see that our method BERT-LS attains the highest Accuracy on three datasets, which has an average increase of 11.6% over the former state-of-theart baseline (Paetzold-NE).",
  "y": "uses"
 },
 {
  "id": "fb87be2081ce1515dd8dbda46b4f3f_23",
  "x": "The results are shown in Table 3 . The results of the baselines on LexMTurk are from<cite> [Paetzold and Specia, 2017b]</cite> and the results on BenchLS and NNSeval are from<cite> [Paetzold and Specia, 2017a]</cite> . We can see that our method BERT-LS attains the highest Accuracy on three datasets, which has an average increase of 11.6% over the former state-of-theart baseline (Paetzold-NE).",
  "y": "differences"
 },
 {
  "id": "fbd028e073459b1b4c2d8d99173e15_0",
  "x": "On the FrameNet 1.5 data, presented additional semi-supervised experiments using gold targets, which was recently outperformed by an approach presented by <cite>Hermann et al. (2014)</cite> that made use of distributed word representations. Johansson and Nugues (2007) presented the system that resulted in the best F 1 score on the SemEval 2007 task of collectively identifying frameevoking targets, a disambiguated frame for each target, and the set of role-labeled arguments for each frame. The system contained a set of rulebased heuristics to identify targets followed by a cascade of three learned models as mentioned in \u00a71.",
  "y": "differences background"
 },
 {
  "id": "fbd028e073459b1b4c2d8d99173e15_1",
  "x": "On the FrameNet 1.5 data, presented additional semi-supervised experiments using gold targets, which was recently outperformed by an approach presented by <cite>Hermann et al. (2014)</cite> that made use of distributed word representations. ---------------------------------- **CURRENT STATE OF THE ART**",
  "y": "differences background"
 },
 {
  "id": "fbd028e073459b1b4c2d8d99173e15_2",
  "x": "Subsequently, <cite>Hermann et al. (2014)</cite> used a very similar framework but presented a novel method using distributed word representations for better frame identification, outperforming the aforementioned update to SEMAFOR. Table 1 shows the performance in terms of F 1 score for frames and arguments given gold targets. Recent work on the FrameNet corpora, including the aforementioned two papers have used gold targets to measure the performance of statistical methods because the distribution of annotated targets in the data varied significantly across documents and domains, making it difficult to build a learnable system for target identification.",
  "y": "differences"
 },
 {
  "id": "fbd028e073459b1b4c2d8d99173e15_3",
  "x": "Given the wide body of work in frame-semantic analysis of text, and recent interest in using framesemantic parsers in NLP applications, the future directions of research look exciting. First and foremost, to improve the quality of automatic frame-semantic parsers, the coverage of the FrameNet lexicon on free English text, and the number of annotated targets needs to increase. For example, the training dataset used for the state-ofthe-art system of <cite>Hermann et al. (2014)</cite> contains only 4,458 labeled targets, which is approximately 40 times less than the number of annotated targets in Ontonotes 4.0 (Hovy et al., 2006) , a standard NLP dataset, containing PropBank-style verb annotations.",
  "y": "background"
 },
 {
  "id": "fbd028e073459b1b4c2d8d99173e15_4",
  "x": "Given the wide body of work in frame-semantic analysis of text, and recent interest in using framesemantic parsers in NLP applications, the future directions of research look exciting. First and foremost, to improve the quality of automatic frame-semantic parsers, the coverage of the FrameNet lexicon on free English text, and the number of annotated targets needs to increase. For example, the training dataset used for the state-ofthe-art system of <cite>Hermann et al. (2014)</cite> contains only 4,458 labeled targets, which is approximately 40 times less than the number of annotated targets in Ontonotes 4.0 (Hovy et al., 2006) , a standard NLP dataset, containing PropBank-style verb annotations.",
  "y": "future_work background"
 },
 {
  "id": "fc3775c0d23292160f5c5eb86861be_0",
  "x": "The dataset we used is a Romanian language resource containing a total of 480,722 inflected forms of Romanian nouns and adjectives. It was extracted from the text form of the morphological dictionary RoMorphoDict<cite> (Barbu, 2008)</cite> , which was also used by Nastase and Popescu (2009) for their Romanian classifier, where every entry has the following structure: Here, 'form' denotes the inflected form and 'description', the morphosyntactic description, encoding part of speech, gender, number, and case.",
  "y": "uses"
 },
 {
  "id": "fc4b56c865c8a9d0f6a7f5ae37ba96_0",
  "x": "The importance of ParFDA increases with the proliferation of training material available for building SMT systems. Table 1 presents the statistics of the available training and LM corpora for the constrained (C) systems in WMT15 <cite>(Bojar et al., 2015)</cite> as well as the statistics of the ParFDA selected training and LM data. ParFDA (Bi\u00e7ici, 2013; Bi\u00e7ici et al., 2014) runs separate FDA5 (Bi\u00e7ici and Yuret, 2015) models on randomized subsets of the training data and combines the selections afterwards.",
  "y": "uses"
 },
 {
  "id": "fc4b56c865c8a9d0f6a7f5ae37ba96_1",
  "x": "We run ParFDA SMT experiments using Moses (Koehn et al., 2007) in all language pairs in WMT15 <cite>(Bojar et al., 2015)</cite> and obtain SMT performance close to the top constrained Moses systems. ParFDA allows rapid prototyping of SMT systems for a given target domain or task. We use ParFDA for selecting parallel training data and LM data for building SMT systems.",
  "y": "uses similarities"
 },
 {
  "id": "fc4b56c865c8a9d0f6a7f5ae37ba96_2",
  "x": "We run ParFDA SMT experiments for all language pairs in both directions in the WMT15 translation task <cite>(Bojar et al., 2015)</cite> , which include English-Czech (en-cs), English-German (en-de), English-Finnish (en-fi), English-French (en-fr), and English-Russian (en-ru). We truecase all of the corpora, set the maximum sentence length to 126, use 150-best lists during tuning, set the LM order to a value in [7, 10] for all language pairs, and train the LM using SRILM (Stolcke, 2002) with -unk option. For GIZA++ (Och and Ney, 2003) , max-fertility is set to 10, with the number of iterations set to 7,3,5,5,7 for IBM models 1,2,3,4, and the HMM model, and 70 word 74 classes are learned over 3 iterations with the mkcls tool during training.",
  "y": "uses"
 },
 {
  "id": "fc4b56c865c8a9d0f6a7f5ae37ba96_3",
  "x": "The importance of ParFDA increases with the proliferation of training material available for building SMT systems. Table 1 presents the statistics of the available training and LM corpora for the constrained (C) systems in WMT15 <cite>(Bojar et al., 2015)</cite> as well as the statistics of the ParFDA selected training and LM data. ParFDA (Bi\u00e7ici, 2013; Bi\u00e7ici et al., 2014) runs separate FDA5 (Bi\u00e7ici and Yuret, 2015 ) models on randomized subsets of the training data and combines the selections afterwards.",
  "y": "uses"
 },
 {
  "id": "fc4b56c865c8a9d0f6a7f5ae37ba96_4",
  "x": "We run ParFDA SMT experiments using Moses (Koehn et al., 2007) in all language pairs in WMT15 <cite>(Bojar et al., 2015)</cite> and obtain SMT performance close to the top constrained Moses systems. ParFDA allows rapid prototyping of SMT systems for a given target domain or task. We use ParFDA for selecting parallel training data and LM data for building SMT systems.",
  "y": "uses"
 },
 {
  "id": "fc4b56c865c8a9d0f6a7f5ae37ba96_5",
  "x": "We run ParFDA SMT experiments for all language pairs in both directions in the WMT15 translation task <cite>(Bojar et al., 2015)</cite> , which include English-Czech (en-cs), English-German (en-de), English-Finnish (en-fi), English-French (en-fr), and English-Russian (en-ru). We truecase all of the corpora, set the maximum sentence length to 126, use 150-best lists during tuning, set the LM order to a value in [7, 10] for all language pairs, and train the LM using SRILM (Stolcke, 2002) with -unk option. For GIZA++ (Och and Ney, 2003) , max-fertility is set to 10, with the number of iterations set to 7,3,5,5,7 for IBM models 1,2,3,4, and the HMM model, and 70 word Table 1 : Data statistics for the available training and LM corpora in the constrained (C) setting compared with the ParFDA selected training and LM data.",
  "y": "uses"
 },
 {
  "id": "fc5de471ba4cc82a2156ed25d2c78b_0",
  "x": "**INTRODUCTION** Low-resource automatic speech-to-text translation (AST) has recently gained traction as a way to bring NLP tools to under-represented languages. An end-to-end approach [1] [2] [3] [4] <cite>[5]</cite> [6] [7] is particularly appealing for source languages with no written form, or for endangered languages where translations into a high-resource language may be easier to collect than transcriptions [8] .",
  "y": "background"
 },
 {
  "id": "fc5de471ba4cc82a2156ed25d2c78b_1",
  "x": "Low-resource automatic speech-to-text translation (AST) has recently gained traction as a way to bring NLP tools to under-represented languages. An end-to-end approach [1] [2] [3] [4] <cite>[5]</cite> [6] [7] is particularly appealing for source languages with no written form, or for endangered languages where translations into a high-resource language may be easier to collect than transcriptions [8] . However, building high-quality endto-end AST with little parallel data is challenging, and has led researchers to explore how other sources of data could be used to help.",
  "y": "motivation background"
 },
 {
  "id": "fc5de471ba4cc82a2156ed25d2c78b_2",
  "x": "For example, Bansal et al. <cite>[5]</cite> showed that pre-training on either English or French ASR improved their Spanish-English AST system (trained on 20 hours of parallel data) and Tian [10] got improvements on an 8-hour Swahili-English AST dataset using English ASR pretraining. Overall these results show that pretraining helps, but leave open the question of what factors affect the degree of improvement. For example, does language relatedness play a role, or simply the amount of pretraining data?",
  "y": "background"
 },
 {
  "id": "fc5de471ba4cc82a2156ed25d2c78b_3",
  "x": "To answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. <cite>[5]</cite> , but pretrain the encoder using a number of different ASR datasets: the 150hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data. We find that pretraining on a larger amount of data from an unrelated language is much better than pretraining on a smaller amount of data from a related language. Moreover, even when controlling for the amount of data, the WER of the ASR model from pretraining seems to be a better predictor of final AST performance than does language relatedness.",
  "y": "extends uses"
 },
 {
  "id": "fc5de471ba4cc82a2156ed25d2c78b_4",
  "x": "---------------------------------- **METHODOLOGY** For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure 1 : the encoder-decoder model from <cite>[5]</cite> , which itself is adapted from [2] , [4] and [3] .",
  "y": "uses"
 },
 {
  "id": "fc5de471ba4cc82a2156ed25d2c78b_5",
  "x": "Previous experiments <cite>[5]</cite> showed that the encoder accounts for most of the benefits of transferring the parameters. Transferring also the decoder and attention mechanism does bring some improvements, but is only feasible when the ASR pretraining language is the same as the AST target language, which is not true in most of our experiments. In addition to pretraining, we experimented with data augmentation.",
  "y": "background"
 },
 {
  "id": "fc5de471ba4cc82a2156ed25d2c78b_6",
  "x": "To compare to the GlobalPhone data, we also created a 20-hour subset of the Romanized AISHELL (zh-ai-small) by randomly selecting utterances from a subset of the speakers (81, roughly the number present in most of the GlobalPhone datasets). Finally, to reproduce one of the experiments from <cite>[5]</cite> , we pretrained one model using 300 hours of Switchboard English [18] . This data is the most similar to the AST speech data in terms of style and channel (both are conversational telephone speech).",
  "y": "uses"
 },
 {
  "id": "fc5de471ba4cc82a2156ed25d2c78b_7",
  "x": "This data is the most similar to the AST speech data in terms of style and channel (both are conversational telephone speech). However, as noted by <cite>[5]</cite> , the Fisher Spanish speech contains many words that are actually in English (code-switching), so pretraining on English may provide an unfair advantage relative to other languages. ----------------------------------",
  "y": "motivation background"
 },
 {
  "id": "fc5de471ba4cc82a2156ed25d2c78b_8",
  "x": "Following the architecture and training procedure described in <cite>[5]</cite> , input speech features are fed into a stack of two CNN layers. In each CNN layer we stride the input with a factor of 2 along time, apply The points in the circled group come from different runs on the same dataset but with different BPE or learning rate schedules. The Spearman rank correlation of these points is -0.97; the correlation is -0.92 when using test sets to compute both ASR and BLEU.",
  "y": "uses"
 },
 {
  "id": "fc5de471ba4cc82a2156ed25d2c78b_9",
  "x": "For decoding, we use the predicted token 20% of the time and the training token 80% of the time [23] as input to a 128-dimensional embedding layer followed by a three-layer LSTM, with 256 hidden layer dimensions, and combine this with the output from the attention mechanism [24] to predict the word at the current time step. We use code and hyperparameter settings from <cite>[5]</cite> 4 : the Adam optimizer [25] with an initial learning rate of 0.001 and decay it by a factor of 0.5 based on the dev set BLEU score. When training AST models, we regularize using dropout [26] with a ratio of 0.3 over the embedding and LSTM layers [27] ; weight decay with a rate of 0.0001; and, after the first 20 epochs, 30% of the time we replace the predicted output word by a random word from the target vocabulary.",
  "y": "uses"
 },
 {
  "id": "fc5de471ba4cc82a2156ed25d2c78b_10",
  "x": "Our baseline 20-hour AST system obtains a BLEU score of 10.3 ( Table 1 , first row), 0.5 BLEU point lower than that reported by <cite>[5]</cite> . This discrepancy might be due to differences in subsampling from the 160-hour AST dataset to create the 20-hour subset, or from Kaldi parameters when computing the MFCCs. WERs for our pre-trained models ( Table 1 ) vary from 22.5 for the large AISHELL dataset with Romanized transcript to 80.5 for Portuguese GlobalPhone.",
  "y": "differences"
 },
 {
  "id": "fc5de471ba4cc82a2156ed25d2c78b_11",
  "x": "These results make it clear that language relatedness does not play a strong role in predicting AST improvements, since on the similar-sized GlobalPhone datasets, the two languages most related to Spanish (French and Portuguese) yield the highest and lowest improvements, respectively. Moreover, pretraining on the large Chinese dataset yields a bigger improvement than either of these-4.3 BLEU points. This is nearly as much as the 6 point improvement reported by <cite>[5]</cite> when pretraining on 100 hours of English data, which is especially surprising given not only that Chinese is very different from Spanish, but also that the Spanish data contains some English words.",
  "y": "similarities"
 },
 {
  "id": "fc5de471ba4cc82a2156ed25d2c78b_12",
  "x": "Moreover, pretraining on the large Chinese dataset yields a bigger improvement than either of these-4.3 BLEU points. This is nearly as much as the 6 point improvement reported by <cite>[5]</cite> when pretraining on 100 hours of English data, which is especially surprising given not only that Chinese is very different from Spanish, but also that the Spanish data contains some English words. This finding seems to suggest that data size is more important than language relatedness for predicting the effects of pretraining.",
  "y": "similarities"
 },
 {
  "id": "fca75d394e9f7007e1f674c7b99794_0",
  "x": "All rights reserved. frequency words and topic words. Litman et al. <cite>(2016)</cite> found significant group-level differences in pitch, jitter and shimmer between first and second halves of conversation.",
  "y": "background"
 },
 {
  "id": "fca75d394e9f7007e1f674c7b99794_1",
  "x": "We hypothesize that both entrainment and team characteristics, specifically team size, gender composition and the diversity of a team, are associated with the perception of team social outcomes. We use hierarchical regression models to examine the unique contribution of multiparty entrainment in explaining perceived team social outcomes above and beyond team characteristics. Finally, to support our studies, we have developed an innovative representation of multi-party entrainment by extending the measurement from Litman et al. <cite>(2016)</cite> and adapting it to study the feature of linguistic style from Pennebaker and King (1999) .",
  "y": "extends"
 },
 {
  "id": "fca75d394e9f7007e1f674c7b99794_2",
  "x": "The freely available Teams Corpus<cite> (Litman et al. 2016)</cite> The corpus also includes survey data. A pre-game survey collected personal information such as age, gender, and eight options for ethnicity. While each participant could choose multiple options, in this paper we categorize each speaker into nine exclusive categories: Caucasian (150), East Asian (12), South Asian (11), Pacific Islander (0), Black (15), Native American (0), Hispanic (3), Middle Eastern (2), and Multiple Ethnicity (20) for participants who chose more than one of the other categories.",
  "y": "uses"
 },
 {
  "id": "fca75d394e9f7007e1f674c7b99794_3",
  "x": "Gonzales, Hancock, and Pennebaker (2010) developed a method to perform linguistic style matching based on multi-party speech, but they only focused on a global measure rather than on the degree of change. Recently, Litman et al. <cite>(2016)</cite> proposed a method to compute multi-party entrainment on acoustic-prosodic features based on the same Teams Corpus as used here. Their method highlighted feature change over time, which is more relevant to linguistic style entrainment.",
  "y": "similarities"
 },
 {
  "id": "fca75d394e9f7007e1f674c7b99794_4",
  "x": "In our study, linguistic style is a single feature with multiple categories, so we converted their calculation of pair differences by summing up all the category differences. Moreover, we weighted category differences by the frequency of categories. More specifically, T Dif f unw (unweighted team difference) converts the team difference of Litman et al. <cite>(2016)</cite> to deal with multiple feature categories.",
  "y": "uses"
 },
 {
  "id": "fca75d394e9f7007e1f674c7b99794_5",
  "x": "The formulas are shown in Equations 1, 2, and 3, where F, K, and |team size| respectively refer to the function word category set, an arbitrary function word category, and the team size. KDif f ij refers to the weighted category difference of category K between speakers i and j. Litman et al. <cite>(2016)</cite> then define convergence, a type of entrainment measuring increase in feature similarity, by comparing the T Dif f of two non-overlapping temporal intervals of a game as in Equation 4.",
  "y": "similarities"
 },
 {
  "id": "fca75d394e9f7007e1f674c7b99794_6",
  "x": "Many studies defined n as two so that the conversation is evenly divided into two halves (Levitan and Hirschberg 2011; Rahimi et al. 2017 ). Since Litman et al. <cite>(2016)</cite> previously found that in the Teams corpus the highest acoustic-prosodic convergence occurred within the first and last three minutes, we used this finding to define our n. We evenly divided each game, which was limited to 30 minutes, into ten intervals, so each interval is less than three minutes. Since our focus is on measure development in this paper, methods for optimally tuning this temporal parameter are left for future work.",
  "y": "uses"
 },
 {
  "id": "fdfb8fbdb8544dca17b1aeba768124_0",
  "x": "In this paper, we compare <cite>CFG filtering techniques for LTAG</cite> (Harbusch, 1990; <cite>Poller and Becker, 1998</cite>) and HPSG (Torisawa et al., 2000; Kiefer and Krieger, 2000) , following an approach to parsing comparison among different grammar formalisms ). The key idea of the approach is to use strongly equivalent grammars, which generate equivalent parse results for the same input, obtained by a grammar conversion as demonstrated by . The parsers with CFG filtering predict possible parse trees by a CFG approximated from a given grammar.",
  "y": "uses"
 },
 {
  "id": "fdfb8fbdb8544dca17b1aeba768124_1",
  "x": "An empirical comparison of <cite>CFG filtering techniques for LTAG</cite> and HPSG is presented. We demonstrate that an approximation of HPSG produces a more effective CFG filter than that of LTAG. We also investigate the reason for that difference.",
  "y": "uses"
 },
 {
  "id": "fdfb8fbdb8544dca17b1aeba768124_2",
  "x": "Investigating the difference between the ways of context-free (CF) approximation of LTAG and HPSG will thereby enlighten a way of further optimization for both techniques. We performed a comparison between the existing <cite>CFG filtering techniques for LTAG</cite> (<cite>Poller and Becker, 1998</cite>) and HPSG (Torisawa et al., 2000) , using strongly equivalent grammars obtained by converting LTAGs extracted from the Penn Treebank (Marcus et al., 1993) into HPSG-style. We compared the parsers with respect to the size of the approximated CFG and its effectiveness as a filter.",
  "y": "uses"
 },
 {
  "id": "fdfb8fbdb8544dca17b1aeba768124_3",
  "x": "Comparison of those parsers are interesting because effective CFG filters allow us to bring the empirical time complexity of the parsers close to that of CFG parsing. Investigating the difference between the ways of context-free (CF) approximation of LTAG and HPSG will thereby enlighten a way of further optimization for both techniques. We performed a comparison between the existing <cite>CFG filtering techniques for LTAG</cite> (<cite>Poller and Becker, 1998</cite>) and HPSG (Torisawa et al., 2000) , using strongly equivalent grammars obtained by converting LTAGs extracted from the Penn Treebank (Marcus et al., 1993) into HPSG-style.",
  "y": "motivation"
 },
 {
  "id": "fdfb8fbdb8544dca17b1aeba768124_4",
  "x": "---------------------------------- **BACKGROUND** In this section, we introduce a grammar conversion ) and <cite>CFG filtering</cite> (Harbusch, 1990; <cite>Poller and Becker, 1998</cite>; Torisawa et al., 2000; Kiefer and Krieger, 2000) .",
  "y": "uses background"
 },
 {
  "id": "fdfb8fbdb8544dca17b1aeba768124_5",
  "x": "---------------------------------- **<cite>CFG FILTERING</cite> TECHNIQUES** An initial offline step of <cite>CFG filtering</cite> is performed to approximate a given grammar with a CFG.",
  "y": "background"
 },
 {
  "id": "fdfb8fbdb8544dca17b1aeba768124_6",
  "x": "---------------------------------- **<cite>CFG FILTERING</cite> TECHNIQUES** An initial offline step of <cite>CFG filtering</cite> is performed to approximate a given grammar with a CFG.",
  "y": "background"
 },
 {
  "id": "fdfb8fbdb8544dca17b1aeba768124_7",
  "x": "The obtained CFG is used as an efficient device to compute the necessary conditions for parse trees. The <cite>CFG filtering</cite> generally consists of two steps. In phase 1, the parser first predicts possible parse trees using the approximated CFG, and then filters out irrelevant edges by a top-down traversal starting from roots of successful context-free derivations.",
  "y": "background"
 },
 {
  "id": "fdfb8fbdb8544dca17b1aeba768124_8",
  "x": "We call the remaining edges that are used for the phase 2 parsing essential edges. The parsers with <cite>CFG filtering</cite> used in our experiments follow the above parsing strategy, but are different in the way the CF approximation and the elimination of impossible parse trees in phase 2 are performed. In the following sections, we briefly describe the CF approximation and the elimination of impossible parse trees in each realization.",
  "y": "extends uses differences"
 },
 {
  "id": "fdfb8fbdb8544dca17b1aeba768124_9",
  "x": "---------------------------------- **CF APPROXIMATION OF LTAG** In <cite>CFG filtering techniques for LTAG</cite> (Harbusch, 1990; <cite>Poller and Becker, 1998</cite>) , every branching of elementary trees in a given grammar is extracted as a CFG rule as shown in Figure 1 .",
  "y": "uses background"
 },
 {
  "id": "fdfb8fbdb8544dca17b1aeba768124_10",
  "x": "In this section, we compare a pair of <cite>CFG filtering techniques for LTAG</cite> (<cite>Poller and Becker, 1998</cite>) and HPSG (Torisawa et al., 2000) described in Section 2.2.1 and 2.2.2. We hereafter refer to PB and TNT for the C++ implementations of the former and a valiant 1 of the latter, respectively. 2 We first acquired LTAGs by a method proposed in Miyao et al. (2003) from Sections 2-21 of the Wall Street Journal (WSJ) in the Penn Treebank (Marcus et al., 1993) and its subsets.",
  "y": "uses"
 },
 {
  "id": "fdfb8fbdb8544dca17b1aeba768124_11",
  "x": "We then recursively apply substitution and adjunction on that traversal to an elementary tree and a generated tree structure. Because the processed portions of generated tree structures are no longer used later, we regard the unprocessed portions of the tree structures as nonterminals of CFG. We can thereby construct another <cite>CFG filtering for LTAG</cite> by combining this CFG filter with an existing LTAG parsing algorithm (van Noord, 1994) .",
  "y": "uses"
 },
 {
  "id": "fdfb8fbdb8544dca17b1aeba768124_12",
  "x": "We then recursively apply substitution and adjunction on that traversal to an elementary tree and a generated tree structure. Because the processed portions of generated tree structures are no longer used later, we regard the unprocessed portions of the tree structures as nonterminals of CFG. We can thereby construct another <cite>CFG filtering for LTAG</cite> by combining this CFG filter with an existing LTAG parsing algorithm (van Noord, 1994) .",
  "y": "uses"
 },
 {
  "id": "fdfb8fbdb8544dca17b1aeba768124_13",
  "x": "Experimental results showed that the existing CF approximation of HPSG (Torisawa et al., 2000) produced a more effective filter than that of LTAG (<cite>Poller and Becker, 1998</cite>) . By investigating the different ways of CF approximation, we concluded that the global constraints in a given grammar is essential to obtain an effective filter. We are going to integrate the advantage of the CF approximation of HPSG into that of LTAG in order to establish another <cite>CFG filtering for LTAG</cite>.",
  "y": "differences"
 },
 {
  "id": "fdfb8fbdb8544dca17b1aeba768124_14",
  "x": "**CONCLUSION AND FUTURE DIRECTION** We are going to integrate the advantage of the CF approximation of HPSG into that of LTAG in order to establish another <cite>CFG filtering for LTAG</cite>.",
  "y": "future_work"
 },
 {
  "id": "fdfb8fbdb8544dca17b1aeba768124_15",
  "x": "Experimental results showed that the existing CF approximation of HPSG (Torisawa et al., 2000) produced a more effective filter than that of LTAG (<cite>Poller and Becker, 1998</cite>) . By investigating the different ways of CF approximation, we concluded that the global constraints in a given grammar is essential to obtain an effective filter. We are going to integrate the advantage of the CF approximation of HPSG into that of LTAG in order to establish another <cite>CFG filtering for LTAG</cite>.",
  "y": "uses future_work"
 },
 {
  "id": "fe3e71020dfb32927f5c348a6fdcfc_0",
  "x": "In fact, the reward function is one of the most handcoded aspects in RL (Paek, 2006) . In this paper we propose a new method for meta-evaluation of the objective function. We bring together two strands of research: one strand uses Reinforcement Learning to automatically optimise dialogue strategies, e.g. (Singh et al., 2002) , (Henderson et al., 2008) , (Rieser and Lemon, 2008a;<cite> Rieser and Lemon, 2008b)</cite> ; the other other focuses on automatic evaluation of dialogue strategies, e.g. the PARADISE framework (Walker et al., 1997) , and meta-evaluation of dialogue metrics, e.g. (Engelbrecht and M\u00f6ller, 2007; Paek, 2007) .",
  "y": "uses"
 },
 {
  "id": "fe3e71020dfb32927f5c348a6fdcfc_1",
  "x": "Furthermore, it is not clear how they perform when being used for automatic strategy optimisation within the RL framework. In the following we evaluate different aspects of an objective function obtained from Wizard-of-Oz (WOZ) data<cite> (Rieser and Lemon, 2008b)</cite> . We proceed as follows: The next Section shortly summarises the overall dialogue system design.",
  "y": "uses"
 },
 {
  "id": "fe3e71020dfb32927f5c348a6fdcfc_2",
  "x": "The structure of information seeking dialogues consists of an information acquisition dialogue and an information presentation sub-dialogue (see Figure 1) . For information acquisition the task of the dialogue policy is to gather 'enough' search constraints from the user, and then, 'at the right time', to start the information presentation phase where the task is to present 'the right amount' of information -either on the screen or listing the items verbally. What this actually means depends on the dialogue context and the preferences of our users as reflected in the objective function. We therefore formulate dialogue learning as a hierarchical optimisation problem<cite> (Rieser and Lemon, 2008b)</cite> .",
  "y": "uses"
 },
 {
  "id": "fe3e71020dfb32927f5c348a6fdcfc_3",
  "x": "**METHOD** In the following the overall method is shortly summarised. Please see <cite>(Rieser and Lemon, 2008b</cite>; Rieser, 2008) for details. 1. We obtain an objective function from the WOZ data of (Rieser et al., 2005) according to the PARADISE framework.",
  "y": "background"
 },
 {
  "id": "fe3e71020dfb32927f5c348a6fdcfc_4",
  "x": "We choose Task Ease as the ultimate measure to be optimised following (Clark, 1996) 's principle of the least effort which says: \"All things being equal, agents try to minimize their effort in doing what they intend to do\". The PARADISE regression model is constructed from 3 different corpora: the SAMMIE WOZ experiment (Rieser et al., 2005) , and the iTalk system used for the user tests<cite> (Rieser and Lemon, 2008b)</cite> running the supervised baseline policy and the RL-based policy. By replicating the regression model on different data sets we test whether the automatic estimate of Task Ease generalises beyond the conditions and assumptions of a particular experimental design.",
  "y": "uses"
 },
 {
  "id": "fe3e71020dfb32927f5c348a6fdcfc_5",
  "x": "In previous work we showed that the RL-based policy significantly outperforms the supervised policy in terms of improved user ratings and dialogue performance measures<cite> (Rieser and Lemon, 2008b)</cite> . Here, we test the relationship between improved user ratings and dialogue behaviour, i.e. we investigate which factors lead the users to give higher scores, and whether this was correctly reflected in the original reward function. We concentrate on the information presentation phase, since there is a simple two-way relationship between user scores and the number of presented items.",
  "y": "background"
 },
 {
  "id": "fe3e71020dfb32927f5c348a6fdcfc_6",
  "x": "For multimodal presentation the WOZ objective function has a turning point at 14.8 (see Figure 3) . The RL-based policy learned to maximise the returned reward by displaying no more than 15 items. The SL policy, in contrast, did not learn an upper boundary for when to show items on the screen (since the wizards did not follow a specific pattern,<cite> (Rieser and Lemon, 2008b)</cite> ).",
  "y": "differences"
 },
 {
  "id": "fe443d5e13b525cbdfa58dafb83162_0",
  "x": "Johnson (2002) studied emptyelement recovery in English, followed by several others (Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006) ; the best results we are aware of are due to Schmid (2006) . Recently, empty-element recovery for Chinese has begun to receive attention: <cite>Yang and Xue (2010)</cite> treat it as classification problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with it, not added in afterwards.",
  "y": "background"
 },
 {
  "id": "fe443d5e13b525cbdfa58dafb83162_1",
  "x": "Our method makes use of a strong syntactic model, the PCFGs with latent annotation of Petrov et al. (2006) , which we extend to predict empty cate-gories by the use of lattice parsing. The method is language-independent and performs very well on both languages we tested it on: for English, it outperforms the best published method we are aware of (Schmid, 2006) , and for Chinese, it outperforms the method of <cite>Yang and Xue (2010)</cite> . 1",
  "y": "differences"
 },
 {
  "id": "fe443d5e13b525cbdfa58dafb83162_2",
  "x": "<cite>Yang and Xue (2010)</cite> simply count unlabeled empty elements: items are (i, i) for each empty element, where i is its position. If multiple empty elements occur at the same position, they only count the last one. The metric originally proposed by Johnson (2002) counts labeled empty brackets: items are (X/t, i, i) for each empty nonterminal node, where X is its label and t is the type of the empty element it dominates, but also (t, i, i) for each empty element not dominated by an empty nonterminal node.",
  "y": "background"
 },
 {
  "id": "fe443d5e13b525cbdfa58dafb83162_4",
  "x": "We selected the 6th split-merge cycle based on the labeled empty elements F 1 measure. The unlabeled empty elements column shows that our system outperforms the baseline system of <cite>Yang and Xue (2010)</cite> . We also analyzed the emptyelement recall by type (Table 3) .",
  "y": "differences"
 },
 {
  "id": "fe443d5e13b525cbdfa58dafb83162_5",
  "x": "The unlabeled empty elements column shows that our system outperforms the baseline system of <cite>Yang and Xue (2010)</cite> . We also analyzed the emptyelement recall by type (Table 3) . Our system outperformed that of <cite>Yang and Xue (2010)</cite> especially on *pro*, used for dropped arguments, and *T*, used for relative clauses and topicalization.",
  "y": "differences"
 },
 {
  "id": "fe8d369d4a6f940a1eb25aa7c9b4fe_0",
  "x": "where x and y are the source and target sentences, and P(y j |y <j , x) is the probability of generating the j-th word y j given the previously-generated words y <j and the source sentence x. However, the straightforward implementation of this model suffers from many problems, the most obvious one being the bias that the system tends to choose shorter translations because the log-probability is added over time steps. The situation is worse when we use beam search where the shorter translations have more chances to beat the longer ones. It is in general to normalize the model score by translation length (say length normalization) to eliminate this system bias<cite> (Wu et al., 2016)</cite> .",
  "y": "background"
 },
 {
  "id": "fe8d369d4a6f940a1eb25aa7c9b4fe_1",
  "x": "Alternatively, one can rerank the n-best outputs by coverage-sensitive models, but this method just affects the final output list which has a very limited scope<cite> (Wu et al., 2016)</cite> . In this paper we present a simple and effective approach by introducing a coverage-based feature into NMT. Unlike previous studies, we do not resort to developing extra models nor reranking the limited n-best translations.",
  "y": "background"
 },
 {
  "id": "fe8d369d4a6f940a1eb25aa7c9b4fe_2",
  "x": "In principle, the coverage score should be high if the translation covers most words in source sentence, and low if it covers only a few of them. Given a source position i, we define its coverage as the sum of the past attention probabilities c i = |y| j a ij <cite>(Wu et al., 2016</cite>; Tu et al., 2016) . Then, the coverage score of the sentence pair (x, y) is defined as the sum of the truncated coverage over all positions (See Figure 1 for an 1 As the discussion of the attention mechanism is out of the scope of this work, we refer the reader to Bahdanau et al. (2015) ; Luong et al. (2015) for more details.",
  "y": "similarities"
 },
 {
  "id": "fe8d369d4a6f940a1eb25aa7c9b4fe_3",
  "x": "The truncation with the lowest value \u03b2 can ensure that the coverage score has a reasonable value. Here \u03b2 is similar to model warm-up, which makes the model easy to run in the first few decoding steps. Note that our way of truncation is different from<cite> Wu et al. (2016)</cite> 's, where they clip the coverage into [0, 1] and ignore the fact that a source word may be translated into multiple target words and its coverage should be of a value larger than 1.",
  "y": "differences"
 },
 {
  "id": "fe8d369d4a6f940a1eb25aa7c9b4fe_4",
  "x": "30k entries for both source and target vocabularies. For the English-German task, BPE (Sennrich et al., 2016) was used for better performance. For comparison, we re-implemented the length normalization (LN) and coverage penalty (CP) methods<cite> (Wu et al., 2016)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "fe8d369d4a6f940a1eb25aa7c9b4fe_5",
  "x": "For the English-German task, BPE (Sennrich et al., 2016) was used for better performance. For comparison, we re-implemented the length normalization (LN) and coverage penalty (CP) methods<cite> (Wu et al., 2016)</cite> . We used grid search to tune all hyperparameters on the development set as<cite> Wu et al. (2016)</cite> .",
  "y": "similarities uses"
 },
 {
  "id": "fe8d369d4a6f940a1eb25aa7c9b4fe_6",
  "x": "In NMT, several good methods have been developed. The simplest of these is length normalization which penalizes short translations in decoding<cite> (Wu et al., 2016)</cite> . More sophisticated methods focus on modeling the coverage problem with extra sub-modules in NMT and require a training process (Tu et al., 2016; Mi et al., 2016) .",
  "y": "background"
 },
 {
  "id": "fe8d369d4a6f940a1eb25aa7c9b4fe_7",
  "x": "More sophisticated methods focus on modeling the coverage problem with extra sub-modules in NMT and require a training process (Tu et al., 2016; Mi et al., 2016) . Perhaps the most related work to this paper is<cite> Wu et al. (2016)</cite> . In their work, the coverage problem can be interpreted in a probability story.",
  "y": "background"
 },
 {
  "id": "fe8d369d4a6f940a1eb25aa7c9b4fe_8",
  "x": "However, it fails to account for the cases that one source word is translated into multiple target words and is thus of a total attention score > 1. To address this issue, we remove the probability constraint and make the coverage score interpretable for different cases. Another difference lies in that our coverage model is applied to every beam search step, while<cite> Wu et al. (2016)</cite> 's model affects only a small number of translation outputs.",
  "y": "differences"
 },
 {
  "id": "febb64368c09d03932742fc557f3d3_0",
  "x": "We applied the monolingual sentence alignment algorithm of <cite>Barzilay and Elhadad (2003)</cite> . The algorithm has two main features: Firstly, it uses a hierarchical approach by assigning paragraphs to clusters and learning mapping rules. Secondly, it aligns sentences despite low lexical similarity if the context suggests an alignment.",
  "y": "uses"
 },
 {
  "id": "febb64368c09d03932742fc557f3d3_1",
  "x": "<cite>Barzilay and Elhadad (2003)</cite> additionally considered every word starting with a capital letter inside a sentence to be a proper name. In German, all nouns (i.e., regular nouns as well as proper names) are capitalized; thus, this approach does not work. We used a list of 61,228 first names to remove at least part of the proper names.",
  "y": "extends background"
 },
 {
  "id": "febb64368c09d03932742fc557f3d3_2",
  "x": "We adapted the hierarchical completelink clustering method of <cite>Barzilay and Elhadad (2003)</cite> : While the authors claimed to have set a specific number of clusters, we believe this is not generally possible in hierarchical agglomerative clustering. Therefore, we used the largest number of clusters in which all paragraph pairs had a cosine similarity strictly greater than zero. Following the formation of the clusters, lexical similarity between all paragraphs of corresponding AS and LS texts was computed to establish probable mappings between the two sets of clusters.",
  "y": "extends background"
 },
 {
  "id": "febb64368c09d03932742fc557f3d3_3",
  "x": "<cite>Barzilay and Elhadad (2003)</cite> used the boosting tool Boostexter (Schapire and Singer, 2000) . All possible cross-combinations of paragraphs from the parallel training data served as training instances. An instance consisted of the cosine similarity of the two paragraphs and a string combining the two cluster IDs. The classification result was extracted from the manual alignments. In order for an AS and an LS paragraph to be aligned, at least one sentence from the LS paragraph had to be aligned to one sentence in the AS paragraph.",
  "y": "background"
 },
 {
  "id": "febb64368c09d03932742fc557f3d3_4",
  "x": "The classification result was extracted from the manual alignments. In order for an AS and an LS paragraph to be aligned, at least one sentence from the LS paragraph had to be aligned to one sentence in the AS paragraph. Like <cite>Barzilay and Elhadad (2003)</cite> , we performed 200 iterations in Boostexter.",
  "y": "similarities uses"
 },
 {
  "id": "febb64368c09d03932742fc557f3d3_5",
  "x": "This was implemented by local sequence alignment. We set the mismatch penalty to 0.02, as a higher mismatch penalty would have reduced recall. We set the skip penalty to 0.001 conforming to the value of <cite>Barzilay and Elhadad (2003)</cite> .",
  "y": "similarities"
 },
 {
  "id": "febb64368c09d03932742fc557f3d3_6",
  "x": "Adapted algorithm of <cite>Barzilay and Elhadad (2003)</cite> 27.7% 5.0% 8.5% Baseline I: First sentence 88.1% 4.8% 9.3% Baseline II: Word in common 2.2% 8.2% 3.5% Table 2 : Alignment results on test set 1. Aligning only the first sentence of each text (\"First sentence\") 2. Aligning every sentence with a cosine similarity greater than zero (\"Word in common\") As can be seen from Table 2 , by applying the sentence alignment algorithm of <cite>Barzilay and Elhadad (2003)</cite> we were able to extract only 5% of all reference alignments, while precision was below 30%.",
  "y": "extends"
 },
 {
  "id": "febb64368c09d03932742fc557f3d3_7",
  "x": "Adapted algorithm of <cite>Barzilay and Elhadad (2003)</cite> 27.7% 5.0% 8.5% Baseline I: First sentence 88.1% 4.8% 9.3% Baseline II: Word in common 2.2% 8.2% 3.5% Table 2 : Alignment results on test set 1. Aligning only the first sentence of each text (\"First sentence\") 2. Aligning every sentence with a cosine similarity greater than zero (\"Word in common\") As can be seen from Table 2 , by applying the sentence alignment algorithm of <cite>Barzilay and Elhadad (2003)</cite> we were able to extract only 5% of all reference alignments, while precision was below 30%.",
  "y": "uses"
 },
 {
  "id": "febb64368c09d03932742fc557f3d3_8",
  "x": "In conclusion, none of the three approaches (adapted algorithm of <cite>Barzilay and Elhadad (2003)</cite> , two baselines \"First sentence\" and \"Word in common\") performed well on our test set. We analyzed the characteristics of our data that hampered high-quality automatic alignment. ----------------------------------",
  "y": "differences background"
 },
 {
  "id": "febb64368c09d03932742fc557f3d3_9",
  "x": "---------------------------------- **DISCUSSION** Compared with the results of <cite>Barzilay and Elhadad (2003)</cite> , who achieved 77% precision at 55.8% recall for their data, our alignment scores were considerably lower (27.7% precision, 5% recall).",
  "y": "differences"
 },
 {
  "id": "febb64368c09d03932742fc557f3d3_10",
  "x": "In what follows, we discuss each reason in more detail. While <cite>Barzilay and Elhadad (2003)</cite> aligned English/Simple English texts, we dealt with German/Simple German data. As mentioned in Section 3.2, in German nouns (regular nouns as well as proper names) are capitalized.",
  "y": "differences"
 },
 {
  "id": "febb64368c09d03932742fc557f3d3_11",
  "x": "In terms of domain, <cite>Barzilay and Elhadad (2003)</cite> used city descriptions from an encyclopedia for their experiments. For these descriptions clustering worked well because all articles had the same structure (paragraphs about culture, sports, etc.). The domain of our corpus was broader: It included information about housing, work, and events for people with disabilities as well as information about the organizations behind the respective websites. Apart from language and domain challenges we observed heavy transformations from AS to LS in our data (Figure 1 shows a sample article in AS and LS). As a result, LS paragraphs were typically very short and the clustering process returned many singleton clusters.",
  "y": "differences background"
 },
 {
  "id": "febb64368c09d03932742fc557f3d3_12",
  "x": "The process of creating a parallel corpus for use in machine translation involves sentence alignment. Sentence alignment algorithms for bilingual corpora differ from those for monolingual corpora. Since all of our data was from the same language, we applied the monolingual sentence alignment approach of <cite>Barzilay and Elhadad (2003)</cite> .",
  "y": "similarities"
 },
 {
  "id": "febb64368c09d03932742fc557f3d3_13",
  "x": "For example, named entity recognition, a preprocessing step to clustering, is harder for German than for English, the language <cite>Barzilay and Elhadad (2003)</cite> worked with. Moreover, German features richer morphology than English, which leads to less lexical overlap when working on the word form level. The domain of our corpus was also broader than that of <cite>Barzilay and Elhadad (2003)</cite> , who used city descriptions from an encyclopedia for their experiments.",
  "y": "background"
 },
 {
  "id": "febb64368c09d03932742fc557f3d3_14",
  "x": "Moreover, German features richer morphology than English, which leads to less lexical overlap when working on the word form level. The domain of our corpus was also broader than that of <cite>Barzilay and Elhadad (2003)</cite> , who used city descriptions from an encyclopedia for their experiments. This made it harder to identify common article structures that could be exploited in clustering.",
  "y": "differences background"
 }
]